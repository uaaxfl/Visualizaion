2020.acl-main.490,D19-1287,0,0.0243916,"rb-phrase coordination: a. The woman laughs and talks/*talk. b. My friends play tennis every week and then get/*gets ice cream. In computational linguistics, acceptability judgments have been used extensively to assess the grammatical abilities of LMs (Linzen et al., 2016; Lau et al., 2017). For the minimal pair paradigm, this is done by determining whether the LM assigns a higher probability to the grammatical member of 5524 the minimal pair than to the ungrammatical member. This paradigm has been applied to a range of constructions, including subject-verb agreement (Marvin and Linzen, 2018; An et al., 2019), negative polarity item licensing (Marvin and Linzen, 2018; Jumelet and Hupkes, 2018), filler-gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018), argument structure (Kann et al., 2019), and several others (Warstadt et al., 2019a). To the extent that the acceptability contrast relies on a single word in a particular location, as in (2), this approach can be extended to bidirectional word prediction systems such as BERT, even though they do not assign a probability to the sentence (Goldberg, 2019). As we describe below, the current version of CLAMS only includes contrasts of"
2020.acl-main.490,C18-1012,0,0.021915,"n computational linguistics, acceptability judgments have been used extensively to assess the grammatical abilities of LMs (Linzen et al., 2016; Lau et al., 2017). For the minimal pair paradigm, this is done by determining whether the LM assigns a higher probability to the grammatical member of 5524 the minimal pair than to the ungrammatical member. This paradigm has been applied to a range of constructions, including subject-verb agreement (Marvin and Linzen, 2018; An et al., 2019), negative polarity item licensing (Marvin and Linzen, 2018; Jumelet and Hupkes, 2018), filler-gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018), argument structure (Kann et al., 2019), and several others (Warstadt et al., 2019a). To the extent that the acceptability contrast relies on a single word in a particular location, as in (2), this approach can be extended to bidirectional word prediction systems such as BERT, even though they do not assign a probability to the sentence (Goldberg, 2019). As we describe below, the current version of CLAMS only includes contrasts of this category. An alternative use of acceptability judgments in NLP involves training an encoder to classify sentences into acceptable and una"
2020.acl-main.490,N19-1423,0,0.625194,"icher morphology. Multilingual models generally underperformed monolingual models. Multilingual BERT showed high syntactic accuracy on English, but noticeable deficiencies in other languages. 1 (1) The key to the cabinets is/*are next to the coins. Introduction Neural networks can be trained to predict words from their context with much greater accuracy than the architectures used for this purpose in the past. This has been shown to be the case for both recurrent neural networks (Mikolov et al., 2010; Sundermeyer et al., 2012; Jozefowicz et al., 2016) and non-recurrent attention-based models (Devlin et al., 2019; Radford et al., 2019). To gain a better understanding of these models’ successes and failures, in particular in the domain of syntax, proposals have been made for testing the † Work done while at Johns Hopkins University. Now in the University of British Columbia’s Linguistics Department. To correctly predict the form of the verb (underlined), the model needs to determine that the head of the subject of the sentence—an abstract, structurally defined notion—is the word key rather than cabinets or coins. The approach of sampling challenging sentences from a test corpus has its limitations. Exa"
2020.acl-main.490,W18-5453,0,0.0188572,"cceptable sentences; by contrast, the prediction approach we adopt can be used to evaluate any word prediction model without additional training. 2.3 Grammatical Evaluation Beyond English Most of the work on grammatical evaluation of word prediction models has focused on English. However, there are a few exceptions, which we discuss in this section. To our knowledge, all of these studies have used sentences extracted from a corpus rather than a controlled challenge set, as we propose. Gulordava et al. (2018) extracted English, Italian, Hebrew, and Russian evaluation sentences from a treebank. Dhar and Bisazza (2018) trained a multilingual LM on a concatenated French and Italian corpus, and tested whether grammatical abilities transfer across languages. Ravfogel et al. (2018) reported an in-depth analysis of LSTM LM performance on agreement prediction in Basque, and Ravfogel et al. (2019) investigated the effect of different syntactic properties of a language on RNNs’ agreement prediction accuracy by creating synthetic variants of English. Finally, grammatical evaluation has been proposed for machine translation systems for languages such as German and French (Sennrich, 2017; Isabelle et al., 2017). 3 Gra"
2020.acl-main.490,N19-1004,0,0.10445,"Missing"
2020.acl-main.490,D17-1263,0,0.0708785,"Missing"
2020.acl-main.490,Q16-1037,1,0.85534,"et al., 2019), we focus our evaluation on the pre-trained English BERT and multilingual BERT. English, French, German and Russian are all IndoEuropean languages, and (Modern) Hebrew syntax exhibits European areal influence (for different perspectives, see Wexler 1990; Zuckermann 2006; Zeldes 2013). 2 https://github.com/aaronmueller/clams (2) Verb-phrase coordination: a. The woman laughs and talks/*talk. b. My friends play tennis every week and then get/*gets ice cream. In computational linguistics, acceptability judgments have been used extensively to assess the grammatical abilities of LMs (Linzen et al., 2016; Lau et al., 2017). For the minimal pair paradigm, this is done by determining whether the LM assigns a higher probability to the grammatical member of 5524 the minimal pair than to the ungrammatical member. This paradigm has been applied to a range of constructions, including subject-verb agreement (Marvin and Linzen, 2018; An et al., 2019), negative polarity item licensing (Marvin and Linzen, 2018; Jumelet and Hupkes, 2018), filler-gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018), argument structure (Kann et al., 2019), and several others (Warstadt et al., 2019a). To t"
2020.acl-main.490,D18-1151,1,0.0767055,"otion—is the word key rather than cabinets or coins. The approach of sampling challenging sentences from a test corpus has its limitations. Examples of relevant constructions may be difficult to find in the corpus, and naturally occurring sentences often contain statistical cues (confounds) that make it possible for the model to predict the correct form of the verb without an adequate syntactic analysis (Gulordava et al., 2018). To address these limitations, a growing number of studies have used constructed materials, which improve experimental control and coverage of syntactic constructions (Marvin and Linzen, 2018; Wilcox et al., 2018; Futrell et al., 2019; Warstadt et al., 2019a). Existing experimentally controlled data sets—in particular, those targeting subject-verb agreement— have largely been restricted to English. As such, we have a limited understanding of the effect of the cross-linguistic variability in neural networks’ syntactic prediction abilities. In this paper, we introduce the Cross-Linguistic Assessment of Models on Syntax (CLAMS) data set, which extends the subject-verb agreement component of the Marvin and Linzen (2018) challenge set to French, German, Hebrew and Russian. By focusing"
2020.acl-main.490,2020.acl-main.309,0,0.0171083,"es without attractors—Simple Agreement and Short VP Coordination—was close to perfect in all languages. This suggests that all monolingual models learned the basic facts of agreement, and were able to apply them to the vocabulary items in our materials. At the other end of the spectrum, performance was only slightly higher than chance in the Across an Object Relative Clause condition for all languages except German, suggesting that LSTMs tend to struggle with center embedding—that is, when a subject-verb dependency is nested within another dependency of the same kind (Marvin and Linzen, 2018; Noji and Takamura, 2020). There was higher variability across languages in the remaining three constructions. The German models had almost perfect accuracy in Long VP Coordination and Across Prepositional Phrase, compared to accuracies ranging between 0.76 and 0.87 for other languages in those constructions. The Hebrew, Russian, and German models showed very high performance on the Across Subject Relative Clause condition: ≥ 0.88 compared to 0.6–0.71 5527 7 https://github.com/yoavg/bert-syntax English French German Hebrew Russian Mono Multi Mono Multi Mono Multi Mono Multi Mono Multi Test Perplexity 57.90 66.13 35.48"
2020.acl-main.490,N19-1356,1,0.896022,"Missing"
2020.acl-main.490,W18-5412,0,0.0565845,"valuation Beyond English Most of the work on grammatical evaluation of word prediction models has focused on English. However, there are a few exceptions, which we discuss in this section. To our knowledge, all of these studies have used sentences extracted from a corpus rather than a controlled challenge set, as we propose. Gulordava et al. (2018) extracted English, Italian, Hebrew, and Russian evaluation sentences from a treebank. Dhar and Bisazza (2018) trained a multilingual LM on a concatenated French and Italian corpus, and tested whether grammatical abilities transfer across languages. Ravfogel et al. (2018) reported an in-depth analysis of LSTM LM performance on agreement prediction in Basque, and Ravfogel et al. (2019) investigated the effect of different syntactic properties of a language on RNNs’ agreement prediction accuracy by creating synthetic variants of English. Finally, grammatical evaluation has been proposed for machine translation systems for languages such as German and French (Sennrich, 2017; Isabelle et al., 2017). 3 Grammar Framework To construct our challenge sets, we use a lightweight grammar engineering framework that we term attribute-varying grammars (AVGs). This framework"
2020.acl-main.490,D19-1592,1,0.907687,"Missing"
2020.acl-main.490,E17-2060,0,0.0366035,"nces from a treebank. Dhar and Bisazza (2018) trained a multilingual LM on a concatenated French and Italian corpus, and tested whether grammatical abilities transfer across languages. Ravfogel et al. (2018) reported an in-depth analysis of LSTM LM performance on agreement prediction in Basque, and Ravfogel et al. (2019) investigated the effect of different syntactic properties of a language on RNNs’ agreement prediction accuracy by creating synthetic variants of English. Finally, grammatical evaluation has been proposed for machine translation systems for languages such as German and French (Sennrich, 2017; Isabelle et al., 2017). 3 Grammar Framework To construct our challenge sets, we use a lightweight grammar engineering framework that we term attribute-varying grammars (AVGs). This framework provides more flexibility than the hard-coded templates of Marvin and Linzen (2018) while avoiding the unbounded embedding depth of sentences generated from a recursive contextfree grammar (CFG, Chomsky 1956). This is done using templates, which consist of preterminals (which have attributes) and terminals. A vary statement specifies which preterminal attributes are varied to generate ungrammatical sente"
2020.acl-main.490,D18-1503,0,0.0329714,"Missing"
2020.acl-main.490,Q19-1040,0,0.138694,"sampling challenging sentences from a test corpus has its limitations. Examples of relevant constructions may be difficult to find in the corpus, and naturally occurring sentences often contain statistical cues (confounds) that make it possible for the model to predict the correct form of the verb without an adequate syntactic analysis (Gulordava et al., 2018). To address these limitations, a growing number of studies have used constructed materials, which improve experimental control and coverage of syntactic constructions (Marvin and Linzen, 2018; Wilcox et al., 2018; Futrell et al., 2019; Warstadt et al., 2019a). Existing experimentally controlled data sets—in particular, those targeting subject-verb agreement— have largely been restricted to English. As such, we have a limited understanding of the effect of the cross-linguistic variability in neural networks’ syntactic prediction abilities. In this paper, we introduce the Cross-Linguistic Assessment of Models on Syntax (CLAMS) data set, which extends the subject-verb agreement component of the Marvin and Linzen (2018) challenge set to French, German, Hebrew and Russian. By focusing on a single lin5523 Proceedings of the 58th Annual Meeting of the"
2020.acl-main.490,E17-2102,0,\N,Missing
2020.acl-main.490,W16-4117,0,\N,Missing
2020.acl-main.490,P18-1132,0,\N,Missing
2020.coling-main.255,P17-1183,0,0.31789,"For example, given the input pair {run, V.PTCP;PRS}, a successful system should produce the present participial form: running. The shared tasks have illuminated several idiosyncrasies of inflection. Although the state-of-the-art methods have been inspired by Neural Machine Translation (NMT), inflection is, in many ways, a more straightforward task. Often, a majority of characters can be copied directly from input to output, and reordering of tokens is not necessary to the extent that it is in translation. Thus, systems with copymechanisms (Makarov et al., 2017), and hard, monotonic attention (Aharoni and Goldberg, 2017) tend to perform well, even when data is scarce. Recurrent neural architectures require that the output from a previous time step be fed back into the model. During training, existing systems all use a variant of teacher forcing, which feeds gold-standard tokens into the model at time t + 1. This methodology differs significantly from how the model progresses at test-time, where silver tokens must be used. This problem has been dubbed exposure bias by Wiseman and Rush (2016). We hypothesize that exposure bias can lead models to overfit the training data, particularly in low-resource settings."
2020.coling-main.255,K17-2001,0,0.0534173,"Missing"
2020.coling-main.255,K18-3001,1,0.942093,"different training and test conditions, exposure bias increases the likelihood that a system too closely models its training data. Experiments show that teacher-forced models struggle to recover when they enter unknown territory. However, a simple modification to the training algorithm to more closely mimic test conditions creates models that are better able to generalize to unseen environments. 1 Introduction Morphological inflection has gained substantial interest in recent years due to a number of shared tasks focusing on morphology learning (Cotterell et al., 2016; Cotterell et al., 2017; Cotterell et al., 2018; McCarthy et al., 2019; Vylomova et al., 2020; Kann et al., 2020). Systems for inflection are trained on a sequence of pairs: {Lemma, MorphoSyntactic Descriptor (MSD)}, free of context, and must produce as output an inflected word form. For example, given the input pair {run, V.PTCP;PRS}, a successful system should produce the present participial form: running. The shared tasks have illuminated several idiosyncrasies of inflection. Although the state-of-the-art methods have been inspired by Neural Machine Translation (NMT), inflection is, in many ways, a more straightforward task. Often, a ma"
2020.coling-main.255,P17-2058,0,0.0204612,"is continuously increased during training which they call scheduling. Our work can be seen as an application of this approach except that we make the choice between gold standard context and the model suggestion at the example-level instead of the level of individual characters. According to our preliminary experiments, this delivered superior results in the task of inflection. Another difference is that we fix the probability for choosing the model suggestion throughout training and treat this as a hyperparameter. We do this because we did not observe consistent improvements from scheduling. Goyal et al. (2017) present a modification of the approach by Bengio et al. (2015) for named-entity recognition and machine translation. They replace the argmax model prediction with an average of all output embeddings, weighted by the prediction scores. Also related to scheduled sampling is the approach by Wiseman and Rush (2016) who experiment with beam search. The downside of this approach is that it can substantially increase training times. 4 Data and Experiments In this section, we lay out our experiment schedule, and describe the system architectures, hyperparameters, and data specifications. We also moti"
2020.coling-main.255,2020.sigmorphon-1.3,1,0.729929,"kelihood that a system too closely models its training data. Experiments show that teacher-forced models struggle to recover when they enter unknown territory. However, a simple modification to the training algorithm to more closely mimic test conditions creates models that are better able to generalize to unseen environments. 1 Introduction Morphological inflection has gained substantial interest in recent years due to a number of shared tasks focusing on morphology learning (Cotterell et al., 2016; Cotterell et al., 2017; Cotterell et al., 2018; McCarthy et al., 2019; Vylomova et al., 2020; Kann et al., 2020). Systems for inflection are trained on a sequence of pairs: {Lemma, MorphoSyntactic Descriptor (MSD)}, free of context, and must produce as output an inflected word form. For example, given the input pair {run, V.PTCP;PRS}, a successful system should produce the present participial form: running. The shared tasks have illuminated several idiosyncrasies of inflection. Although the state-of-the-art methods have been inspired by Neural Machine Translation (NMT), inflection is, in many ways, a more straightforward task. Often, a majority of characters can be copied directly from input to output,"
2020.coling-main.255,P17-4012,0,0.0358293,"l of “dog”, accuracy will assign the same score to a system that produces “doggs“, and one that produces “elephants”. While we do not claim that inflectional models make mistakes of this magnitude, a high edit-distance paired with a high accuracy may suggest significant mistakes are being made in incorrect predictions. Furthermore, student forcing is applicable to other learning tasks where all-or-nothing evaluations are less frequent - BLEU score, for example, has much more in common with edit distance than accuracy. Our pointer-generator network uses the PyTorch implementation from OpenNMT (Klein et al., 2017). They were trained for 10,000 steps, except for the “high” setting, which was trained for 20,000, upon observation that 10,000 steps was not enough for convergence. Model checkpoints were saved every 500 steps, and the model with the highest development accuracy was used for inference. Under the low setting, the RNN hidden dimension was set to 50, while in the medium and high settings, the RNN size was set to 200. All models were run on 10 different random seeds, using general copy attention. All other system parameters were set to the OpenNMT defaults. Our HA model was trained using the best"
2020.coling-main.255,D18-1314,0,0.0740245,"ral inflection models: the Pointer-Generator network (PG), a soft attention model originally introduced for document summarization (See et al., 2017) and subsequently applied to inflection by Sharma et al. (2018), a neural transducer utilizing hard attention trained on aligned lemmata and inflected word forms (Aharoni and Goldberg, 2017; Makarov et al., 2017) (HA) and modified with an explicit copy mechanism, and a later development of the HA system which applies minimum risk training in order to avoid relying on an explicit aligner for the lemma and inflected word form during training (MRT) (Makarov and Clematide, 2018a). 2.1 Student forcing Teacher and student forcing are illustrated in Figure 1. On the left, the teacher-forced system feeds the gold character from time t into the decoder to predict the character at time t + 1. The student forcing system, on the right, instead feeds the predicted token from the model. Both systems in this toy example make a mistake in their predictions, but must learn different correction strategies – the teacher-forced system must learn to “get back on track” after an error, while the student-forced system must only be conditioned on its previous prediction, regardless of"
2020.coling-main.255,C18-1008,0,0.0331679,"ral inflection models: the Pointer-Generator network (PG), a soft attention model originally introduced for document summarization (See et al., 2017) and subsequently applied to inflection by Sharma et al. (2018), a neural transducer utilizing hard attention trained on aligned lemmata and inflected word forms (Aharoni and Goldberg, 2017; Makarov et al., 2017) (HA) and modified with an explicit copy mechanism, and a later development of the HA system which applies minimum risk training in order to avoid relying on an explicit aligner for the lemma and inflected word form during training (MRT) (Makarov and Clematide, 2018a). 2.1 Student forcing Teacher and student forcing are illustrated in Figure 1. On the left, the teacher-forced system feeds the gold character from time t into the decoder to predict the character at time t + 1. The student forcing system, on the right, instead feeds the predicted token from the model. Both systems in this toy example make a mistake in their predictions, but must learn different correction strategies – the teacher-forced system must learn to “get back on track” after an error, while the student-forced system must only be conditioned on its previous prediction, regardless of"
2020.coling-main.255,W19-4226,1,0.844022,"est conditions, exposure bias increases the likelihood that a system too closely models its training data. Experiments show that teacher-forced models struggle to recover when they enter unknown territory. However, a simple modification to the training algorithm to more closely mimic test conditions creates models that are better able to generalize to unseen environments. 1 Introduction Morphological inflection has gained substantial interest in recent years due to a number of shared tasks focusing on morphology learning (Cotterell et al., 2016; Cotterell et al., 2017; Cotterell et al., 2018; McCarthy et al., 2019; Vylomova et al., 2020; Kann et al., 2020). Systems for inflection are trained on a sequence of pairs: {Lemma, MorphoSyntactic Descriptor (MSD)}, free of context, and must produce as output an inflected word form. For example, given the input pair {run, V.PTCP;PRS}, a successful system should produce the present participial form: running. The shared tasks have illuminated several idiosyncrasies of inflection. Although the state-of-the-art methods have been inspired by Neural Machine Translation (NMT), inflection is, in many ways, a more straightforward task. Often, a majority of characters ca"
2020.coling-main.255,P19-2049,0,0.0278694,"Missing"
2020.coling-main.255,P17-1099,0,0.076512,"Missing"
2020.coling-main.255,D16-1137,0,0.364547,"that it is in translation. Thus, systems with copymechanisms (Makarov et al., 2017), and hard, monotonic attention (Aharoni and Goldberg, 2017) tend to perform well, even when data is scarce. Recurrent neural architectures require that the output from a previous time step be fed back into the model. During training, existing systems all use a variant of teacher forcing, which feeds gold-standard tokens into the model at time t + 1. This methodology differs significantly from how the model progresses at test-time, where silver tokens must be used. This problem has been dubbed exposure bias by Wiseman and Rush (2016). We hypothesize that exposure bias can lead models to overfit the training data, particularly in low-resource settings. In this paper, we compare teacher forcing to silver label propagation, or student forcing, where model predictions are fed into the decoder during training instead of gold standard labels. Our experiments on a geographically and linguistically diverse set of 10 languages show that student forcing typically outperforms teacher forcing in a low-resource setting. Our analysis of the results suggests that teacher forced models are, indeed, overfitting the training data by ignori"
2020.lrec-1.352,P19-1310,0,0.0676769,"Missing"
2020.lrec-1.352,P15-2044,0,0.0825655,"Missing"
2020.lrec-1.352,D17-1011,0,0.0843599,"Missing"
2020.lrec-1.352,W08-0336,0,0.0231984,"Missing"
2020.lrec-1.352,D19-1632,0,0.0692809,"Missing"
2020.lrec-1.352,L18-1293,1,0.886792,"Missing"
2020.lrec-1.352,2005.mtsummit-papers.11,0,0.16262,"Missing"
2020.lrec-1.352,E17-1072,0,0.0251337,"eat map of the 66 Bible books’ presence by language. (Twenty non-canon books which appear in only a handful of languages are omitted.) Nearly all languages have a complete New Testament, and several also have a complete Old Testament. 5. Bibles as a Low-Resource Asset employ the fact that multiple interpretations can be used in tandem. Exploiting the Bible, Agi´c et al. (2015) learn POS taggers for 100 languages and evaluate on 25 languages with test sets. Parallel Bibles also aid a variety of cross-lingual tasks, e.g., dependency parsing (Schlichtkrull and Søgaard, 2017), sentence embedding (Levy et al., 2017), verbal morphology induction (Yarowsky et al., 2001), and multilingual optical character recognition (Kanungo et al., 2005). None of these By contrast, Xia and Yarowsky (2017) leverage 27 English translations of the Bible. They use alignment and consensus to transfer dependency parses across languages. Nicolai and Yarowsky (2019) build on this, using projection with the same 27 English translations to develop morphosyntactic analyzers for low-resource languages. Nicolai et al. (2020) 2886 Family Tai-Kadai Arai (Left May) Khoe-Kwadi South-Central Papuan Kiowa-Tanoan Eskimo-Aleut Aymaran Tungus"
2020.lrec-1.352,E17-2002,0,0.02835,"not because we did not segment the text in languages that do not indicate word boundaries with spaces.)) take this a step further, releasing fine-grained morphosyntactic analysis and generation tools for more than 1,000 languages. Additional practical resources can be derived from the Bible: translation matrices of named entities (Wu et al., 2018), lowresource transliteration tools (Wu and Yarowsky, 2018), and bitext for multilingual translation (Mueller et al., 2020). Feature Compilation In order to compile a typological description of this corpus, we utilize the URIEL typological database (Littell et al., 2017) and supplement this with a surface-level parser of Ethnologue (Eberhard et al., 2019) typology descriptions in order to leverage the most recent entries. This parser functions similarly to the original parser described by Littell et al. (2017)—using simple Boolean logic on contents of Ethnologue descriptions. As most Ethnologue entries have similar phrasing or terminology, a language can be classified with a particular typological feature if the description contains one of a handful of phrases used on Ethnologue to describe that feature, but does not contain any of the handful of phrases used"
2020.lrec-1.352,mayer-cysouw-2014-creating,0,0.0764406,"treat the verse alignment problem within a chapter as an instance of the longest common subsequence problem. Here, in a chapter with a pre-known number of verses m, the sequence of verse numbers A = [1, 2, . . . , m] is matched to the sequence B of n numbers extracted from the chapter text. The longest common subsequence is going to give the best explanation of the numbers B seen in text, “explaining” them either as a verse ID or a number seen in text. It can be solved efficiently using dynamic programming in O(m × n) time (Wagner and Fischer, 1974). 3.2. Verse alignment Our re-release of the Mayer and Cysouw (2014) Bibles and the web-scraped Bibles (Asgari and Sch¨utze, 2017) is already verse-aligned. The CMU Wilderness Bibles (Black, 2019) are verse-aligned using the dynamic programming approach explained in §3.1. Normalization We normalize all characters to their canonical (Unicode NFKC) form.3 Cross-references, footnote markers, and explanatory parentheticals are stripped. We replace archaisms in the King James Version of the English Bible (‘thou’ forms; ‘-est’ and ‘-eth’ verb forms) with their modern equivalents. Tokenization We preserve the tokenization of the Mayer and Cysouw (2014)–derived Bibles"
2020.lrec-1.352,2020.lrec-1.483,1,0.701077,"individual Bibles in a language that is a member of each family. The percent denotes the percent of the JHUBC Bibles that come from that language family. and clusivity. We leverage the parallelism of the Bible to annotate pronouns in the English Bibles for these features which are otherwise unmarked in English. We first collect a pronoun list for a small set of languages that mark pronouns for a number of different phenomena such as number, gender, plurality, and clusivity, and annotated these lists with UniMorph-style inflectional information (Sylak-Glassman et al., 2015; Kirov et al., 2018; McCarthy et al., 2020). Next, we word-align the English Bibles with the Bibles in those languages. Projecting from the source onto English, we obtain source–English pronoun hypotheses for each English pronoun. For each feature, we vote among the languages to arrive at a final annotation for English. To mitigate the ubiquity of certain feature values over others (i.e., the nominative case is much more prevalent in our languages than the essive case), we normalize each feature by the number of languages in which it is present. We can then use these annotations to identify pronouns in other languages via alignment. Bi"
2020.lrec-1.352,2020.lrec-1.458,1,0.792992,"here are several translations within a single language, we average these first. (Tai-Kadai would be expected to have a low type–token ratio. It does not because we did not segment the text in languages that do not indicate word boundaries with spaces.)) take this a step further, releasing fine-grained morphosyntactic analysis and generation tools for more than 1,000 languages. Additional practical resources can be derived from the Bible: translation matrices of named entities (Wu et al., 2018), lowresource transliteration tools (Wu and Yarowsky, 2018), and bitext for multilingual translation (Mueller et al., 2020). Feature Compilation In order to compile a typological description of this corpus, we utilize the URIEL typological database (Littell et al., 2017) and supplement this with a surface-level parser of Ethnologue (Eberhard et al., 2019) typology descriptions in order to leverage the most recent entries. This parser functions similarly to the original parser described by Littell et al. (2017)—using simple Boolean logic on contents of Ethnologue descriptions. As most Ethnologue entries have similar phrasing or terminology, a language can be classified with a particular typological feature if the d"
2020.lrec-1.352,P19-1172,1,0.803869,"sed in tandem. Exploiting the Bible, Agi´c et al. (2015) learn POS taggers for 100 languages and evaluate on 25 languages with test sets. Parallel Bibles also aid a variety of cross-lingual tasks, e.g., dependency parsing (Schlichtkrull and Søgaard, 2017), sentence embedding (Levy et al., 2017), verbal morphology induction (Yarowsky et al., 2001), and multilingual optical character recognition (Kanungo et al., 2005). None of these By contrast, Xia and Yarowsky (2017) leverage 27 English translations of the Bible. They use alignment and consensus to transfer dependency parses across languages. Nicolai and Yarowsky (2019) build on this, using projection with the same 27 English translations to develop morphosyntactic analyzers for low-resource languages. Nicolai et al. (2020) 2886 Family Tai-Kadai Arai (Left May) Khoe-Kwadi South-Central Papuan Kiowa-Tanoan Eskimo-Aleut Aymaran Tungusic Dravidian Eyak-Athabaskan Algic Mixed language Piawi Iroquoian Har´akmbut Pauwasi Quechuan Unclassified Maipurean East Geelvink Bay Yuat Senagi Tequistlatecan Pidgin Turkic Chibchan Chipaya-Uru Mapudungu Tacanan Mixe-Zoquean Uralic Constructed language Witotoan Muskogean Panoan Huavean North Bougainville Nakh-Daghestanian Jivar"
2020.lrec-1.352,2020.lrec-1.488,1,0.718874,"a variety of cross-lingual tasks, e.g., dependency parsing (Schlichtkrull and Søgaard, 2017), sentence embedding (Levy et al., 2017), verbal morphology induction (Yarowsky et al., 2001), and multilingual optical character recognition (Kanungo et al., 2005). None of these By contrast, Xia and Yarowsky (2017) leverage 27 English translations of the Bible. They use alignment and consensus to transfer dependency parses across languages. Nicolai and Yarowsky (2019) build on this, using projection with the same 27 English translations to develop morphosyntactic analyzers for low-resource languages. Nicolai et al. (2020) 2886 Family Tai-Kadai Arai (Left May) Khoe-Kwadi South-Central Papuan Kiowa-Tanoan Eskimo-Aleut Aymaran Tungusic Dravidian Eyak-Athabaskan Algic Mixed language Piawi Iroquoian Har´akmbut Pauwasi Quechuan Unclassified Maipurean East Geelvink Bay Yuat Senagi Tequistlatecan Pidgin Turkic Chibchan Chipaya-Uru Mapudungu Tacanan Mixe-Zoquean Uralic Constructed language Witotoan Muskogean Panoan Huavean North Bougainville Nakh-Daghestanian Jivaroan Indo-European Arauan Guaykuruan Austro-Asiatic Misumalpan Karaj´a Tarascan South Bougainville Matacoan TTR 0.621 0.590 0.535 0.439 0.427 0.426 0.370 0.35"
2020.lrec-1.352,N18-2084,0,0.0326875,"Missing"
2020.lrec-1.352,E17-1021,0,0.0207306,"r 1 John 2 John 3 John Jude Revelation 0.0 Figure 1: Heat map of the 66 Bible books’ presence by language. (Twenty non-canon books which appear in only a handful of languages are omitted.) Nearly all languages have a complete New Testament, and several also have a complete Old Testament. 5. Bibles as a Low-Resource Asset employ the fact that multiple interpretations can be used in tandem. Exploiting the Bible, Agi´c et al. (2015) learn POS taggers for 100 languages and evaluate on 25 languages with test sets. Parallel Bibles also aid a variety of cross-lingual tasks, e.g., dependency parsing (Schlichtkrull and Søgaard, 2017), sentence embedding (Levy et al., 2017), verbal morphology induction (Yarowsky et al., 2001), and multilingual optical character recognition (Kanungo et al., 2005). None of these By contrast, Xia and Yarowsky (2017) leverage 27 English translations of the Bible. They use alignment and consensus to transfer dependency parses across languages. Nicolai and Yarowsky (2019) build on this, using projection with the same 27 English translations to develop morphosyntactic analyzers for low-resource languages. Nicolai et al. (2020) 2886 Family Tai-Kadai Arai (Left May) Khoe-Kwadi South-Central Papuan"
2020.lrec-1.352,P15-2111,1,0.772899,"contacting the authors. 9. Table 3: The count of individual Bibles in a language that is a member of each family. The percent denotes the percent of the JHUBC Bibles that come from that language family. and clusivity. We leverage the parallelism of the Bible to annotate pronouns in the English Bibles for these features which are otherwise unmarked in English. We first collect a pronoun list for a small set of languages that mark pronouns for a number of different phenomena such as number, gender, plurality, and clusivity, and annotated these lists with UniMorph-style inflectional information (Sylak-Glassman et al., 2015; Kirov et al., 2018; McCarthy et al., 2020). Next, we word-align the English Bibles with the Bibles in those languages. Projecting from the source onto English, we obtain source–English pronoun hypotheses for each English pronoun. For each feature, we vote among the languages to arrive at a final annotation for English. To mitigate the ubiquity of certain feature values over others (i.e., the nominative case is much more prevalent in our languages than the essive case), we normalize each feature by the number of languages in which it is present. We can then use these annotations to identify p"
2020.lrec-1.352,tiedemann-2012-parallel,0,0.219573,"Missing"
2020.lrec-1.352,L18-1150,1,0.836665,"averaged over each language family, sorted by this ratio. When there are several translations within a single language, we average these first. (Tai-Kadai would be expected to have a low type–token ratio. It does not because we did not segment the text in languages that do not indicate word boundaries with spaces.)) take this a step further, releasing fine-grained morphosyntactic analysis and generation tools for more than 1,000 languages. Additional practical resources can be derived from the Bible: translation matrices of named entities (Wu et al., 2018), lowresource transliteration tools (Wu and Yarowsky, 2018), and bitext for multilingual translation (Mueller et al., 2020). Feature Compilation In order to compile a typological description of this corpus, we utilize the URIEL typological database (Littell et al., 2017) and supplement this with a surface-level parser of Ethnologue (Eberhard et al., 2019) typology descriptions in order to leverage the most recent entries. This parser functions similarly to the original parser described by Littell et al. (2017)—using simple Boolean logic on contents of Ethnologue descriptions. As most Ethnologue entries have similar phrasing or terminology, a language"
2020.lrec-1.352,L18-1263,1,0.837513,"morphological richness. We report type–token ratios, averaged over each language family, sorted by this ratio. When there are several translations within a single language, we average these first. (Tai-Kadai would be expected to have a low type–token ratio. It does not because we did not segment the text in languages that do not indicate word boundaries with spaces.)) take this a step further, releasing fine-grained morphosyntactic analysis and generation tools for more than 1,000 languages. Additional practical resources can be derived from the Bible: translation matrices of named entities (Wu et al., 2018), lowresource transliteration tools (Wu and Yarowsky, 2018), and bitext for multilingual translation (Mueller et al., 2020). Feature Compilation In order to compile a typological description of this corpus, we utilize the URIEL typological database (Littell et al., 2017) and supplement this with a surface-level parser of Ethnologue (Eberhard et al., 2019) typology descriptions in order to leverage the most recent entries. This parser functions similarly to the original parser described by Littell et al. (2017)—using simple Boolean logic on contents of Ethnologue descriptions. As most Ethnologu"
2020.lrec-1.352,2020.lrec-1.519,1,0.719738,"Missing"
2020.lrec-1.352,I17-2076,1,0.841713,"New Testament, and several also have a complete Old Testament. 5. Bibles as a Low-Resource Asset employ the fact that multiple interpretations can be used in tandem. Exploiting the Bible, Agi´c et al. (2015) learn POS taggers for 100 languages and evaluate on 25 languages with test sets. Parallel Bibles also aid a variety of cross-lingual tasks, e.g., dependency parsing (Schlichtkrull and Søgaard, 2017), sentence embedding (Levy et al., 2017), verbal morphology induction (Yarowsky et al., 2001), and multilingual optical character recognition (Kanungo et al., 2005). None of these By contrast, Xia and Yarowsky (2017) leverage 27 English translations of the Bible. They use alignment and consensus to transfer dependency parses across languages. Nicolai and Yarowsky (2019) build on this, using projection with the same 27 English translations to develop morphosyntactic analyzers for low-resource languages. Nicolai et al. (2020) 2886 Family Tai-Kadai Arai (Left May) Khoe-Kwadi South-Central Papuan Kiowa-Tanoan Eskimo-Aleut Aymaran Tungusic Dravidian Eyak-Athabaskan Algic Mixed language Piawi Iroquoian Har´akmbut Pauwasi Quechuan Unclassified Maipurean East Geelvink Bay Yuat Senagi Tequistlatecan Pidgin Turkic"
2020.lrec-1.352,H01-1035,1,0.451858,"ge. (Twenty non-canon books which appear in only a handful of languages are omitted.) Nearly all languages have a complete New Testament, and several also have a complete Old Testament. 5. Bibles as a Low-Resource Asset employ the fact that multiple interpretations can be used in tandem. Exploiting the Bible, Agi´c et al. (2015) learn POS taggers for 100 languages and evaluate on 25 languages with test sets. Parallel Bibles also aid a variety of cross-lingual tasks, e.g., dependency parsing (Schlichtkrull and Søgaard, 2017), sentence embedding (Levy et al., 2017), verbal morphology induction (Yarowsky et al., 2001), and multilingual optical character recognition (Kanungo et al., 2005). None of these By contrast, Xia and Yarowsky (2017) leverage 27 English translations of the Bible. They use alignment and consensus to transfer dependency parses across languages. Nicolai and Yarowsky (2019) build on this, using projection with the same 27 English translations to develop morphosyntactic analyzers for low-resource languages. Nicolai et al. (2020) 2886 Family Tai-Kadai Arai (Left May) Khoe-Kwadi South-Central Papuan Kiowa-Tanoan Eskimo-Aleut Aymaran Tungusic Dravidian Eyak-Athabaskan Algic Mixed language Pia"
2020.lrec-1.458,N19-1388,0,0.327459,"ecurrent sequence-to-sequence neural networks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) in combination with attention (Bahdanau et al., 2015; Luong et al., 2015); even better performance has been achieved using self-attention (Vaswani et al., 2017). The neural approach has been effective because of the high fluency and adequacy of its output, as well as its language-agnostic methods. Indeed, given enough data, one need not know any linguistic features of one’s source and target languages to effectively translate.1 With the success of multilingual training (Johnson et al., 2017; Aharoni et al., 2019), the trend seems clear: more data generally results in better models for neural machine translation, regardless of the languages (or combinations thereof) used. These results, however, have primarily been found for languages possessing large amounts of aligned data, and few such languages exist among the world’s 7,000+ languages (Lewis et al., 2015). While high neural machine translation performance has been observed in some bilingual lowresource2 contexts (Sennrich and Zhang, 2019), we have access to a multi-way aligned corpus of Bibles that allow us to perform a more in-depth analysis of th"
2020.lrec-1.458,1983.tc-1.13,0,0.318512,"Missing"
2020.lrec-1.458,P18-1073,0,0.0150898,"ora (Koehn and Knowles, 2017; Lample et al., 2018), NMT has achieved performance exceeding phrase-based MT in such settings by exploiting the same neural architectures as high-resource systems— primarily LSTMs (Hochreiter and Schmidhuber, 1997) and Transformers (Vaswani et al., 2017)—and by performing fine-grained hyperparameter tuning (Sennrich and Zhang, 2019). Another proposed strategy in for low-resource NMT is learning a latent variable NMT model with variational inference (McCarthy et al., 2019), though this has not been tried in a low-resource setting similar to ours. Unsupervised NMT (Artetxe et al., 2018) was suggested as a solution to the scarcity of parallel data for many languages, though this was found to be generally ineffective for lowresource and morphologically rich languages (Guzmán et al., 2019). Unsupervised phrase-based and neural models have also been proposed (Lample et al., 2018) and are reasonably effective when parallel data does not exist, though this is perhaps unrealistic given that the Bible exists in over 1,000 languages. Consequently, we aim to translate in a more supervised fashion. One surprisingly effective approach in high-resource settings has been to train translat"
2020.lrec-1.458,D17-1011,0,0.25729,"ed languages will reduce the bottleneck effect and increase BLEU compared to using more unrelated languages. We aim to investigate this effect for our particular context in a more largescale manner; we perform multilingual NMT by concatenating many low-resource corpora derived from the Bible, up to over 1,000 languages. 3. Data Preparation The monolingual Bibles used in this study are from the Bible corpus of McCarthy et al. (2020), which contains over 4,000 translations (of varying lengths) in over 1,000 languages. This corpus is an aggregation of prior Bible corpora (Mayer and Cysouw, 2014; Asgari and Schütze, 2017; Black, 2019) and web-scraped Bible data, all postprocessed to be in the same format. Namely, these monolingual corpora are all verse-aligned,3 normalized to Unicode NFKC, modified such that archaic English forms are replaced with their contemporary equivalents (e.g., “thou” is changed to “you”, and “-est” and “-eth” verb inflections are replaced with their modern “-es” or “-s” forms), tokenized,4 and deduplicated. The Old Testament (OT) contains approximately 31,000 verses, and the New Testament (NT) contains approximately 8,000 verses. Some translations have the entire OT and NT, while othe"
2020.lrec-1.458,W09-0106,0,0.0125839,"using either bilingual models, 5-language models, or 10-language models. Moreover, the relatedness of the multiple languages certainly has an effect on performance, though whether this effect is positive or negative is language-dependent. This implies that the idea of simply adding more data to a training corpus regardless of language will not always result in the best performance—though it sometimes does when done carefully. This highlights the need for multiple evaluation languages when working in the multilingual setting, for methods that work in one language may not generalize to others (Bender, 2009; Bender, 2011). If one is pursuing the best model for a specific low-resource language pair, it is imperative to try a variety of source language sets when training multilingually and to train a good bilingual baseline. This variable cross-lingual performance could be due to the bottleneck effect mentioned in §2 or the destructive interference described in §6. Future work could treat the manylanguage corpora as high-resource datasets and use more typical high-resource hyperparameters instead of the lowresource hyperparameters used here. One could also investigate a larger variety of source la"
2020.lrec-1.458,N18-1032,0,0.0183431,"lel corpus, up 103 languages. These approaches have used exclusively high-resource or a mix of high-resource and lowresource languages. By comparison, we do not include auxiliary data to perform a controlled study in a limited setting; we just use the Bible, which is the same size and domain across all of our experiments. Of note, Arivazhagan et al. (2019) point to the difficulty of balancing data, which is less of an issue in our multi-parallel low-resource setting. Recent studies have investigated transfer learning in the low-resource multilingual setting using multiple unrelated languages (Gu et al., 2018; Zoph et al., 2016), and some have done the similar work using multiple related languages (Nguyen and Chiang, 2017). These approaches employed a small number of helper languages during training, but no more than 5 to 10 at once. Prior work suggests that this could either result in transfer learning across languages, leading to increased performance (as in the abovecited works), or perhaps it could result in a bottleneck where there are too few parameters for too many languages (Sachan and Neubig, 2018; Wang et al., 2018). These latter works suggest that using more closely related languages wi"
2020.lrec-1.458,D19-1632,0,0.0280165,"STMs (Hochreiter and Schmidhuber, 1997) and Transformers (Vaswani et al., 2017)—and by performing fine-grained hyperparameter tuning (Sennrich and Zhang, 2019). Another proposed strategy in for low-resource NMT is learning a latent variable NMT model with variational inference (McCarthy et al., 2019), though this has not been tried in a low-resource setting similar to ours. Unsupervised NMT (Artetxe et al., 2018) was suggested as a solution to the scarcity of parallel data for many languages, though this was found to be generally ineffective for lowresource and morphologically rich languages (Guzmán et al., 2019). Unsupervised phrase-based and neural models have also been proposed (Lample et al., 2018) and are reasonably effective when parallel data does not exist, though this is perhaps unrealistic given that the Bible exists in over 1,000 languages. Consequently, we aim to translate in a more supervised fashion. One surprisingly effective approach in high-resource settings has been to train translation models on multiple language pairs at once (Johnson et al., 2017). This is done by concatenating the bitexts for multiple language pairs together into one large parallel corpus. Such approaches have en"
2020.lrec-1.458,Q17-1024,0,0.320519,"e pairs by employing recurrent sequence-to-sequence neural networks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) in combination with attention (Bahdanau et al., 2015; Luong et al., 2015); even better performance has been achieved using self-attention (Vaswani et al., 2017). The neural approach has been effective because of the high fluency and adequacy of its output, as well as its language-agnostic methods. Indeed, given enough data, one need not know any linguistic features of one’s source and target languages to effectively translate.1 With the success of multilingual training (Johnson et al., 2017; Aharoni et al., 2019), the trend seems clear: more data generally results in better models for neural machine translation, regardless of the languages (or combinations thereof) used. These results, however, have primarily been found for languages possessing large amounts of aligned data, and few such languages exist among the world’s 7,000+ languages (Lewis et al., 2015). While high neural machine translation performance has been observed in some bilingual lowresource2 contexts (Sennrich and Zhang, 2019), we have access to a multi-way aligned corpus of Bibles that allow us to perform a more"
2020.lrec-1.458,D13-1176,0,0.300674,"ges to a training set is often better, but too many harms performance—the best number depends on the source language. Furthermore, training on related languages can improve or degrade performance, depending on the language. As there is no one-size-fits-most answer, we find that it is critical to tailor one’s approach to the source language and its typology. Keywords: neural machine translation, low-resource, multilinguality, Bible 1. Introduction Recently, machine translation (MT) has made significant progress in many language pairs by employing recurrent sequence-to-sequence neural networks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) in combination with attention (Bahdanau et al., 2015; Luong et al., 2015); even better performance has been achieved using self-attention (Vaswani et al., 2017). The neural approach has been effective because of the high fluency and adequacy of its output, as well as its language-agnostic methods. Indeed, given enough data, one need not know any linguistic features of one’s source and target languages to effectively translate.1 With the success of multilingual training (Johnson et al., 2017; Aharoni et al., 2019), the trend seems clear: more data generally results in"
2020.lrec-1.458,W17-3204,0,0.0254985,"of-the-art phrase-based statistical approach (Koehn et al., 2003) in Sutskever et al. (2014) and Bahdanau et al. (2015). Its success was due to the combination of the sequence-to-sequence neural network, the use of the LSTM (Hochreiter and Schmidhuber, 1997), and the introduction and refinement of the attention mechanism (Luong et al., 2015). Although the main focus of investigation and improvement in NMT has been high-resource settings with millions of sentences, NMT has made great strides in low-resource settings as well. Initially considered ineffective without very large parallel corpora (Koehn and Knowles, 2017; Lample et al., 2018), NMT has achieved performance exceeding phrase-based MT in such settings by exploiting the same neural architectures as high-resource systems— primarily LSTMs (Hochreiter and Schmidhuber, 1997) and Transformers (Vaswani et al., 2017)—and by performing fine-grained hyperparameter tuning (Sennrich and Zhang, 2019). Another proposed strategy in for low-resource NMT is learning a latent variable NMT model with variational inference (McCarthy et al., 2019), though this has not been tried in a low-resource setting similar to ours. Unsupervised NMT (Artetxe et al., 2018) was su"
2020.lrec-1.458,N03-1017,0,0.0560702,"nding that the best approach depends to a large extent on the language pair. We hope that these results will help elucidate best practices for leveraging massively multilingual low-resource parallel datasets in future MT work. 2. Related Work Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013) has become the state-of-the-art approach to MT in recent years, employing new innovations in machine learning to achieve high performance in many language pairs. This approach was first shown to be effective with respect to the previously state-of-the-art phrase-based statistical approach (Koehn et al., 2003) in Sutskever et al. (2014) and Bahdanau et al. (2015). Its success was due to the combination of the sequence-to-sequence neural network, the use of the LSTM (Hochreiter and Schmidhuber, 1997), and the introduction and refinement of the attention mechanism (Luong et al., 2015). Although the main focus of investigation and improvement in NMT has been high-resource settings with millions of sentences, NMT has made great strides in low-resource settings as well. Initially considered ineffective without very large parallel corpora (Koehn and Knowles, 2017; Lample et al., 2018), NMT has achieved p"
2020.lrec-1.458,D18-1549,0,0.0191999,"tatistical approach (Koehn et al., 2003) in Sutskever et al. (2014) and Bahdanau et al. (2015). Its success was due to the combination of the sequence-to-sequence neural network, the use of the LSTM (Hochreiter and Schmidhuber, 1997), and the introduction and refinement of the attention mechanism (Luong et al., 2015). Although the main focus of investigation and improvement in NMT has been high-resource settings with millions of sentences, NMT has made great strides in low-resource settings as well. Initially considered ineffective without very large parallel corpora (Koehn and Knowles, 2017; Lample et al., 2018), NMT has achieved performance exceeding phrase-based MT in such settings by exploiting the same neural architectures as high-resource systems— primarily LSTMs (Hochreiter and Schmidhuber, 1997) and Transformers (Vaswani et al., 2017)—and by performing fine-grained hyperparameter tuning (Sennrich and Zhang, 2019). Another proposed strategy in for low-resource NMT is learning a latent variable NMT model with variational inference (McCarthy et al., 2019), though this has not been tried in a low-resource setting similar to ours. Unsupervised NMT (Artetxe et al., 2018) was suggested as a solution"
2020.lrec-1.458,D15-1166,0,0.37457,"nguage. Furthermore, training on related languages can improve or degrade performance, depending on the language. As there is no one-size-fits-most answer, we find that it is critical to tailor one’s approach to the source language and its typology. Keywords: neural machine translation, low-resource, multilinguality, Bible 1. Introduction Recently, machine translation (MT) has made significant progress in many language pairs by employing recurrent sequence-to-sequence neural networks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) in combination with attention (Bahdanau et al., 2015; Luong et al., 2015); even better performance has been achieved using self-attention (Vaswani et al., 2017). The neural approach has been effective because of the high fluency and adequacy of its output, as well as its language-agnostic methods. Indeed, given enough data, one need not know any linguistic features of one’s source and target languages to effectively translate.1 With the success of multilingual training (Johnson et al., 2017; Aharoni et al., 2019), the trend seems clear: more data generally results in better models for neural machine translation, regardless of the languages (or combinations thereof)"
2020.lrec-1.458,mayer-cysouw-2014-creating,0,0.653267,"using more closely related languages will reduce the bottleneck effect and increase BLEU compared to using more unrelated languages. We aim to investigate this effect for our particular context in a more largescale manner; we perform multilingual NMT by concatenating many low-resource corpora derived from the Bible, up to over 1,000 languages. 3. Data Preparation The monolingual Bibles used in this study are from the Bible corpus of McCarthy et al. (2020), which contains over 4,000 translations (of varying lengths) in over 1,000 languages. This corpus is an aggregation of prior Bible corpora (Mayer and Cysouw, 2014; Asgari and Schütze, 2017; Black, 2019) and web-scraped Bible data, all postprocessed to be in the same format. Namely, these monolingual corpora are all verse-aligned,3 normalized to Unicode NFKC, modified such that archaic English forms are replaced with their contemporary equivalents (e.g., “thou” is changed to “you”, and “-est” and “-eth” verb inflections are replaced with their modern “-es” or “-s” forms), tokenized,4 and deduplicated. The Old Testament (OT) contains approximately 31,000 verses, and the New Testament (NT) contains approximately 8,000 verses. Some translations have the en"
2020.lrec-1.458,2020.lrec-1.352,1,0.91034,"ine translation, regardless of the languages (or combinations thereof) used. These results, however, have primarily been found for languages possessing large amounts of aligned data, and few such languages exist among the world’s 7,000+ languages (Lewis et al., 2015). While high neural machine translation performance has been observed in some bilingual lowresource2 contexts (Sennrich and Zhang, 2019), we have access to a multi-way aligned corpus of Bibles that allow us to perform a more in-depth analysis of the performance of multilingual low-resource NMT. This parallel corpus of Bible texts (McCarthy et al., 2020) contains a set of “monoApproximately multi-parallel corpus Similar or random subset of languages Align Target × Source language text NMT Model Figure 1: Our process for constructing training sets from a multi-parallel corpus. We select some n − 1 languages that share either phylogeny or script or randomly sample n − 1 languages to augment the source–target pair. Using a multi-parallel corpus controls for variations in data quality or domain; this lets us more cleanly assess our scientific questions, but it is not an engineering requirement for MT systems that use several helper languages. Mor"
2020.lrec-1.458,I17-2050,0,0.0186778,"e and lowresource languages. By comparison, we do not include auxiliary data to perform a controlled study in a limited setting; we just use the Bible, which is the same size and domain across all of our experiments. Of note, Arivazhagan et al. (2019) point to the difficulty of balancing data, which is less of an issue in our multi-parallel low-resource setting. Recent studies have investigated transfer learning in the low-resource multilingual setting using multiple unrelated languages (Gu et al., 2018; Zoph et al., 2016), and some have done the similar work using multiple related languages (Nguyen and Chiang, 2017). These approaches employed a small number of helper languages during training, but no more than 5 to 10 at once. Prior work suggests that this could either result in transfer learning across languages, leading to increased performance (as in the abovecited works), or perhaps it could result in a bottleneck where there are too few parameters for too many languages (Sachan and Neubig, 2018; Wang et al., 2018). These latter works suggest that using more closely related languages will reduce the bottleneck effect and increase BLEU compared to using more unrelated languages. We aim to investigate"
2020.lrec-1.458,N19-4009,0,0.0159382,"o do so, we compare the performance of neural models trained on our various multilingual corpora when translating from our evaluation languages to English. This is essentially the same as the many-to-one approach of Johnson et al. (2017). All corpora have their tokens split into subwords using BPE (Sennrich et al., 2016). This is run jointly on all languages in the dataset, so each multilingual corpus will have different subword splits. We use 32,000 merge operations for all corpora despite their varying sizes; this results in more aggressive word splitting for larger corpora. We use fairseq (Ott et al., 2019) to run both training and inference. Separate Transformer-based models are trained for each of our multilingual corpora using the low-resource 5. Results Table 3 contains BLEU scores for the translation task for all models trained on varying numbers of source languages. All scores are for translations from the given evaluation language into English. We score on detokenized output translations using SacreBLEU (Post, 2018). We first note that the BLEU increase/decrease with respect to the number of training languages is not uniform across evaluation languages. Indeed, for the 5-language model, w"
2020.lrec-1.458,W18-6319,0,0.0125836,"rd splits. We use 32,000 merge operations for all corpora despite their varying sizes; this results in more aggressive word splitting for larger corpora. We use fairseq (Ott et al., 2019) to run both training and inference. Separate Transformer-based models are trained for each of our multilingual corpora using the low-resource 5. Results Table 3 contains BLEU scores for the translation task for all models trained on varying numbers of source languages. All scores are for translations from the given evaluation language into English. We score on detokenized output translations using SacreBLEU (Post, 2018). We first note that the BLEU increase/decrease with respect to the number of training languages is not uniform across evaluation languages. Indeed, for the 5-language model, we see notable gains over bilingual models for Arabic, Turkish, 5 1-layer encoder, 1-layer decoder, hidden size 1024, embedding size 512, tied decoder embeddings, hidden and embedding dropout 0.5, source and target word dropout 0.3, label smoothing 0.2, batch size 1000, initial learning rate 0.0005, Adam optimizer. 6 More data in general may help for a variety of reasons that are not well understood, especially in low-res"
2020.lrec-1.458,W18-6327,0,0.0162549,"gated transfer learning in the low-resource multilingual setting using multiple unrelated languages (Gu et al., 2018; Zoph et al., 2016), and some have done the similar work using multiple related languages (Nguyen and Chiang, 2017). These approaches employed a small number of helper languages during training, but no more than 5 to 10 at once. Prior work suggests that this could either result in transfer learning across languages, leading to increased performance (as in the abovecited works), or perhaps it could result in a bottleneck where there are too few parameters for too many languages (Sachan and Neubig, 2018; Wang et al., 2018). These latter works suggest that using more closely related languages will reduce the bottleneck effect and increase BLEU compared to using more unrelated languages. We aim to investigate this effect for our particular context in a more largescale manner; we perform multilingual NMT by concatenating many low-resource corpora derived from the Bible, up to over 1,000 languages. 3. Data Preparation The monolingual Bibles used in this study are from the Bible corpus of McCarthy et al. (2020), which contains over 4,000 translations (of varying lengths) in over 1,000 languages."
2020.lrec-1.458,P19-1021,0,0.29208,"ce and target languages to effectively translate.1 With the success of multilingual training (Johnson et al., 2017; Aharoni et al., 2019), the trend seems clear: more data generally results in better models for neural machine translation, regardless of the languages (or combinations thereof) used. These results, however, have primarily been found for languages possessing large amounts of aligned data, and few such languages exist among the world’s 7,000+ languages (Lewis et al., 2015). While high neural machine translation performance has been observed in some bilingual lowresource2 contexts (Sennrich and Zhang, 2019), we have access to a multi-way aligned corpus of Bibles that allow us to perform a more in-depth analysis of the performance of multilingual low-resource NMT. This parallel corpus of Bible texts (McCarthy et al., 2020) contains a set of “monoApproximately multi-parallel corpus Similar or random subset of languages Align Target × Source language text NMT Model Figure 1: Our process for constructing training sets from a multi-parallel corpus. We select some n − 1 languages that share either phylogeny or script or randomly sample n − 1 languages to augment the source–target pair. Using a multi-p"
2020.lrec-1.458,P16-1162,0,0.0216115,"relationship between the number of languages in a multilingual dataset and the performance of NMT into English in a low-resource setting. We observe whether the performance increase is monotonic with respect to the number of languages, as well as at what point we begin to see diminishing returns. To do so, we compare the performance of neural models trained on our various multilingual corpora when translating from our evaluation languages to English. This is essentially the same as the many-to-one approach of Johnson et al. (2017). All corpora have their tokens split into subwords using BPE (Sennrich et al., 2016). This is run jointly on all languages in the dataset, so each multilingual corpus will have different subword splits. We use 32,000 merge operations for all corpora despite their varying sizes; this results in more aggressive word splitting for larger corpora. We use fairseq (Ott et al., 2019) to run both training and inference. Separate Transformer-based models are trained for each of our multilingual corpora using the low-resource 5. Results Table 3 contains BLEU scores for the translation task for all models trained on varying numbers of source languages. All scores are for translations fr"
2020.lrec-1.458,D18-1326,0,0.0184802,"n the low-resource multilingual setting using multiple unrelated languages (Gu et al., 2018; Zoph et al., 2016), and some have done the similar work using multiple related languages (Nguyen and Chiang, 2017). These approaches employed a small number of helper languages during training, but no more than 5 to 10 at once. Prior work suggests that this could either result in transfer learning across languages, leading to increased performance (as in the abovecited works), or perhaps it could result in a bottleneck where there are too few parameters for too many languages (Sachan and Neubig, 2018; Wang et al., 2018). These latter works suggest that using more closely related languages will reduce the bottleneck effect and increase BLEU compared to using more unrelated languages. We aim to investigate this effect for our particular context in a more largescale manner; we perform multilingual NMT by concatenating many low-resource corpora derived from the Bible, up to over 1,000 languages. 3. Data Preparation The monolingual Bibles used in this study are from the Bible corpus of McCarthy et al. (2020), which contains over 4,000 translations (of varying lengths) in over 1,000 languages. This corpus is an ag"
2020.lrec-1.458,D19-3043,0,0.057393,"the 5-language models for German and Tagalog therein). The similar-language Xhosa model obtains more modest gains over the unrelated-language model, especially considering the extent of the BLEU gains on German and Tagalog. Once again, it seems that the effectiveness of any particular multilingual approach is highly language-dependent. 6. Qualitative Analysis Metrics do not fully encapsulate translation performance. They may not capture critical phenomena, and may not 7 We go into further detail on these possibilities and suggest improvements for future work in §7. align with human judgments (Wang et al., 2019). Further, they do not give a clear understanding of patterns of errors. Thus, this section focuses on the analysis of system outputs.8 We provide examples of translations from the multilingual experiments and the similar-language and similarscript experiments in Table 5. First, we note that fluency is strongly preferred over adequacy, especially for the models that do not degenerate. That is, taking the perspective of the decoder as a conditional language model, in some settings we have developed strong English language models which largely ignore their source text. This effect is clearer for"
2020.lrec-1.458,D16-1163,0,0.0183329,"3 languages. These approaches have used exclusively high-resource or a mix of high-resource and lowresource languages. By comparison, we do not include auxiliary data to perform a controlled study in a limited setting; we just use the Bible, which is the same size and domain across all of our experiments. Of note, Arivazhagan et al. (2019) point to the difficulty of balancing data, which is less of an issue in our multi-parallel low-resource setting. Recent studies have investigated transfer learning in the low-resource multilingual setting using multiple unrelated languages (Gu et al., 2018; Zoph et al., 2016), and some have done the similar work using multiple related languages (Nguyen and Chiang, 2017). These approaches employed a small number of helper languages during training, but no more than 5 to 10 at once. Prior work suggests that this could either result in transfer learning across languages, leading to increased performance (as in the abovecited works), or perhaps it could result in a bottleneck where there are too few parameters for too many languages (Sachan and Neubig, 2018; Wang et al., 2018). These latter works suggest that using more closely related languages will reduce the bottle"
2020.lrec-1.483,P19-1310,0,0.0958144,"Missing"
2020.lrec-1.483,C12-2009,1,0.843263,"Missing"
2020.lrec-1.483,P19-1156,0,0.050149,"Missing"
2020.lrec-1.483,P16-1156,1,0.881371,"Missing"
2020.lrec-1.483,K17-2001,1,0.904106,"Missing"
2020.lrec-1.483,N07-1048,0,0.0363457,"Missing"
2020.lrec-1.483,P08-1115,0,0.0323664,"Missing"
2020.lrec-1.483,K19-1014,1,0.901227,"Missing"
2020.lrec-1.483,N12-1032,0,0.0791982,"Missing"
2020.lrec-1.483,D19-1328,0,0.0253837,"Missing"
2020.lrec-1.483,S19-1026,1,0.819906,"Missing"
2020.lrec-1.483,L16-1498,1,0.928068,"Missing"
2020.lrec-1.483,L18-1293,1,0.890225,"Missing"
2020.lrec-1.483,W18-6011,1,0.883111,"Missing"
2020.lrec-1.483,W19-4226,1,0.885665,"Missing"
2020.lrec-1.483,D14-1095,0,0.0393603,"Missing"
2020.lrec-1.483,2020.lrec-1.488,1,0.822461,"Missing"
2020.lrec-1.483,L16-1262,0,0.126329,"Missing"
2020.lrec-1.483,Q15-1026,0,0.0701072,"Missing"
2020.lrec-1.483,W18-1813,1,0.887977,"Missing"
2020.lrec-1.483,P15-2111,1,0.855194,"Missing"
2020.lrec-1.483,A94-1008,0,0.317255,"Missing"
2020.lrec-1.483,N01-1026,1,0.569711,"Missing"
2020.lrec-1.488,P15-2044,0,0.154424,"Missing"
2020.lrec-1.488,P16-1184,0,0.0447663,"Missing"
2020.lrec-1.488,K17-2001,1,0.86632,"Missing"
2020.lrec-1.488,K18-3001,1,0.931236,"n this paper, we describe a resource intended to aid with the inflectional sparsity problem: a set of inflectional analyzers and generators for more than 1000 languages. Often, inflectional tools are created on an “as-needed” basis — a researcher will create and distribute a tool for a particular language of interest, or a small number of languages are incrementally added to an existing tool — the Porter stemmer (Porter, 1980), which evolved into the Snowball stemming suite3 is one such example. With the exception of the SIGMORPHON shared tasks (Cotterell et al., 2016; Cotterell et al., 2017; Cotterell et al., 2018a; McCarthy et al., 2019), we are unaware of any efforts to produce a large number of inflectional tools across a large number of languages. Leveraging a large, multi-way parallel corpus of Bible texts (McCarthy et al., 2020), we exploit languages with highaccuracy annotation tools to hypothesize and project inflectional morphology across an induced alignment (Figure 1. We hope that providing these tools to the community will www.ethnologue.com internetworldstats.com/stats7.htm 3 3963 https://snowballstem.org/ encourage and facilitate research in a much wider range of the world’s languages. Th"
2020.lrec-1.488,I05-1075,0,0.0876687,"Missing"
2020.lrec-1.488,P17-1044,0,0.0182987,"rators and analyzers are trained on inflectional paradigms extracted from the Bible corpus of McCarthy et al. (under review), which contains 4032 parallel translations in 1108 languages. Among these are 27 English translations, whose archaic forms have been replaced with their modern equivalents (i.e., “believeth” is replaced with “believes”). The English and target Bibles have been aligned using the Berkeley aligner (Liang et al., 3966 2006), POS-tagged and syntactically-parsed using the Stanford NLP toolkit (Manning et al., 2014), and semanticallyparsed using the Deep Semantic Role Labeler (He et al., 2017). If a target Bible has multiple translations, they are all used to extract inflectional paradigms. The Bible corpus contains languages representing a wide spectrum of morphological phenomena, including languages that do not inflect at all. Many Polynesian languages, such as Indonesian, Tangoa, and Balinese, for example, have very little inflectional morphology on nouns, verbs, and adjectives. Likewise, the Sino-Tibetan language family, containing languages like Mandarin Chinese, observes very little inflectional morphology (cf. Tibetan). The URIEL typological database (Littel et al., 2016) ca"
2020.lrec-1.488,N10-1103,0,0.0610536,"Missing"
2020.lrec-1.488,E17-2018,0,0.0137588,"ual dictionary. Fossum and Abney (2005) and Agi´c et al. (2015) exploit the parallel nature of the Bible to project POS tags to lower-resources languages, using this projection to then train POS-taggers. Buys and Botha (2016) extend this tagging paradigm to morphological tagging, projecting morphological tags onto a low-resource language, and training a tagger on these morphologically-aware tags. In the op3964 Figure 3: Projecting induced morphological categories and lemmas across an alignment from English to Finnish. Red arrows indicate a right-to-left syntactic dependency. posite direction, Kirov et al. (2017) uses a morphologically rich language to train a morphologically-aware tagger in English. Instead of tags, Soricut and Och (2015) induce morphological transformation rules in an unsupervised manner, recovering the lemma from inflected forms. We expand upon the work of Nicolai and Yarowsky (2019), who first richly annotate the English side of a bitext before projecting morphological information onto the lowresource text. The paucity of English morphological tagging is augmented through the heuristic interpretation of syntactic and semantic parses, as well as a reverse projection of a number of"
2020.lrec-1.488,L18-1293,1,0.899678,"Missing"
2020.lrec-1.488,D18-2012,0,0.0611284,"Missing"
2020.lrec-1.488,N06-1014,0,0.280767,"Missing"
2020.lrec-1.488,C18-1008,0,0.0196104,"7.1. before describing our novel type-to-token conversion, which allows an approximation of token-level accuracy when annotated corpora are unavailable. 7.1. Type Accuracy We first evaluate our systems on type accuracy: given a morphological dictionary, we report the percentage of instances that are correctly analyzed. We present the type accuracy for generation, lemmatization, and analysis in Table 5. We report the average over all 50 evaluation languages. We observe that ensembling and reranking is much more successful going from inflections to lemmas than the reverse. The neural system of Makarov and Clematide (2018) was specifically designed for inflection generation, and is very successful at producing inflected forms, even with noisy training data - the non-neural system has little to add in an ensemble. Likewise, the inflectional sparsity problem means that most inflectional forms will not be observed in the corpus used for reranking, and thus few forms are promoted. On the lemmatization side, the ensemble clearly improves over either individual system. Lemmatization is an exerSystem DTL@1 M&C@1 Ensemble@1 Ensemble+RR@1 DTL@5 M&C@5 Ensemble@5 Ensemble+RR@5 DTL@50 M&C@50 Ensemble@50 Ensemble+RR@50 Gene"
2020.lrec-1.488,P14-5010,0,0.00247993,"op of the list while preserving the order of the hypotheses. 5. Data All of our generators and analyzers are trained on inflectional paradigms extracted from the Bible corpus of McCarthy et al. (under review), which contains 4032 parallel translations in 1108 languages. Among these are 27 English translations, whose archaic forms have been replaced with their modern equivalents (i.e., “believeth” is replaced with “believes”). The English and target Bibles have been aligned using the Berkeley aligner (Liang et al., 3966 2006), POS-tagged and syntactically-parsed using the Stanford NLP toolkit (Manning et al., 2014), and semanticallyparsed using the Deep Semantic Role Labeler (He et al., 2017). If a target Bible has multiple translations, they are all used to extract inflectional paradigms. The Bible corpus contains languages representing a wide spectrum of morphological phenomena, including languages that do not inflect at all. Many Polynesian languages, such as Indonesian, Tangoa, and Balinese, for example, have very little inflectional morphology on nouns, verbs, and adjectives. Likewise, the Sino-Tibetan language family, containing languages like Mandarin Chinese, observes very little inflectional mo"
2020.lrec-1.488,W19-4226,1,0.863383,"a resource intended to aid with the inflectional sparsity problem: a set of inflectional analyzers and generators for more than 1000 languages. Often, inflectional tools are created on an “as-needed” basis — a researcher will create and distribute a tool for a particular language of interest, or a small number of languages are incrementally added to an existing tool — the Porter stemmer (Porter, 1980), which evolved into the Snowball stemming suite3 is one such example. With the exception of the SIGMORPHON shared tasks (Cotterell et al., 2016; Cotterell et al., 2017; Cotterell et al., 2018a; McCarthy et al., 2019), we are unaware of any efforts to produce a large number of inflectional tools across a large number of languages. Leveraging a large, multi-way parallel corpus of Bible texts (McCarthy et al., 2020), we exploit languages with highaccuracy annotation tools to hypothesize and project inflectional morphology across an induced alignment (Figure 1. We hope that providing these tools to the community will www.ethnologue.com internetworldstats.com/stats7.htm 3 3963 https://snowballstem.org/ encourage and facilitate research in a much wider range of the world’s languages. This paper progresses as fo"
2020.lrec-1.488,2020.lrec-1.352,1,0.850705,"basis — a researcher will create and distribute a tool for a particular language of interest, or a small number of languages are incrementally added to an existing tool — the Porter stemmer (Porter, 1980), which evolved into the Snowball stemming suite3 is one such example. With the exception of the SIGMORPHON shared tasks (Cotterell et al., 2016; Cotterell et al., 2017; Cotterell et al., 2018a; McCarthy et al., 2019), we are unaware of any efforts to produce a large number of inflectional tools across a large number of languages. Leveraging a large, multi-way parallel corpus of Bible texts (McCarthy et al., 2020), we exploit languages with highaccuracy annotation tools to hypothesize and project inflectional morphology across an induced alignment (Figure 1. We hope that providing these tools to the community will www.ethnologue.com internetworldstats.com/stats7.htm 3 3963 https://snowballstem.org/ encourage and facilitate research in a much wider range of the world’s languages. This paper progresses as follows: Section 2. gives a brief overview of inflectional morphology, and describes the key operations covered by our inflectional tools. Section 3. establishes the current state of affairs in computat"
2020.lrec-1.488,P19-1172,1,0.689801,"ing morphological tags onto a low-resource language, and training a tagger on these morphologically-aware tags. In the op3964 Figure 3: Projecting induced morphological categories and lemmas across an alignment from English to Finnish. Red arrows indicate a right-to-left syntactic dependency. posite direction, Kirov et al. (2017) uses a morphologically rich language to train a morphologically-aware tagger in English. Instead of tags, Soricut and Och (2015) induce morphological transformation rules in an unsupervised manner, recovering the lemma from inflected forms. We expand upon the work of Nicolai and Yarowsky (2019), who first richly annotate the English side of a bitext before projecting morphological information onto the lowresource text. The paucity of English morphological tagging is augmented through the heuristic interpretation of syntactic and semantic parses, as well as a reverse projection of a number of other high-resource languages. Our main contribution over their work is the expansion of the language set by a factor of 40 (from 26 languages to more than 1000). We also augment the set of inflectional features covered by their methods and incorporate frequency statistics into their learning mo"
2020.lrec-1.488,N15-1186,0,0.0223024,"o lower-resources languages, using this projection to then train POS-taggers. Buys and Botha (2016) extend this tagging paradigm to morphological tagging, projecting morphological tags onto a low-resource language, and training a tagger on these morphologically-aware tags. In the op3964 Figure 3: Projecting induced morphological categories and lemmas across an alignment from English to Finnish. Red arrows indicate a right-to-left syntactic dependency. posite direction, Kirov et al. (2017) uses a morphologically rich language to train a morphologically-aware tagger in English. Instead of tags, Soricut and Och (2015) induce morphological transformation rules in an unsupervised manner, recovering the lemma from inflected forms. We expand upon the work of Nicolai and Yarowsky (2019), who first richly annotate the English side of a bitext before projecting morphological information onto the lowresource text. The paucity of English morphological tagging is augmented through the heuristic interpretation of syntactic and semantic parses, as well as a reverse projection of a number of other high-resource languages. Our main contribution over their work is the expansion of the language set by a factor of 40 (from"
2020.lrec-1.488,P15-2111,1,0.811235,"t of a single column of acceptable word forms. 7. Evaluation We construct both inflection generators and morphological analyzers for more than 1000 languages. In this section, we evaluate the quality of the tools that we present to the community. It is not possible to collect evaluation sets for each and every language represented in this dataset — the evaluation of morphological tools requires annotated inflectional dictionaries, which do not exist for a majority of languages. We instead evaluate on a subset of the languages for which we do have inflecional dictionaries. We turn to UniMorph (Sylak-Glassman et al., 2015; Kirov et al., 2018), a collection of morphological dictionaries that spans more than 100 languages. Of these languages, 50 overlap with our languages, and can be used as a test set. For each language, we extract a validation set of 500 randomly-sampled tuples of the form {LEMMA, INFLECTED, MORPH}, and a test set of 1000 instances. For example, an English instance of the word “played” would appear as {play, played, PST}. The validation set is used to tune hyper-parameters, and for early stopping of the neural models. The languages are listed in Table 4. From these tuples, we construct test se"
2020.lrec-1.488,H01-1035,1,0.579966,"Missing"
2020.lrec-1.519,C10-3010,0,0.0316292,"one 4. dog 7. eye 10. blood 13. bone 16. tooth 19. die 22. hear 25. mouth 28. eat 31. smoke 34. black 37. man 40. three 43. liver 46. hide 49. drink 52. good 55. fat 58. cloud 61. neck 64. cold 67. earth 70. go 73. that 76. mother 79. sit 82. five 85. what 88. root 91. grind 94. who 97. house 100. back 103. little 106. know 109. short 112. female 115. old 118. sky 121. ash 124. six 127. stick 130. dull 133. eight 136. he 139. the 142. near 145. this 148. where Construction For the construction of our core vocabulary, we utilize LanguageNet1 , a multilingual lexicon that is a subset of PanLex (Baldwin et al., 2010), a freely available multilingual dictionary. PanLex contains lexical translations across thousands of the world’s languages and has recently garnered interest in the multilingual research community. Its lexical translations are sourced from existing dictionaries and thesauri such as Wiktionary and WordNet. LanguageNet, as of September 2019, contains 1895 languages. We employ a simple procedure: using English as a pivot, we collect counts of how many languages have a translation for each English concept. (This dictionary pivoting strategy has previously been applied to model color terminology"
2020.lrec-1.519,L16-1379,0,0.0282135,"you moon Dictionaries are available for most of the world’s languages, but coverage can be sparse for those with fewer resources. In sparse dictionaries, many entries are core vocabulary words from lists such as the Swadesh list (Swadesh, 1952; Swadesh, 1955), probably the most wellknown formulation of a core vocabulary containing around 100–200 words, depending on the version. This list of basic words is used in historical comparative linguistics to determine the relationships between languages, and there have been many attempts to revise or expand these concept lists for this purpose. (See List et al. (2016) for a recent survey and compilation of such lists.) Morris Swadesh chose the words in the Swadesh lists based on certain criteria: the words should be culturally universal, stable over time (not likely to change meaning), and not likely to be borrowed. Swadesh lists now exist in over 1000 languages and can be used as a dictionary to perform lexical translations. However, in a low-resource setting, the ability to translate a mere 100 concepts is insufficient for understanding in a language. In addition, the Swadesh list, like many other lists, was manually created and revised through years of"
2020.lrec-1.519,macleod-etal-2000-american,0,0.0433208,"Missing"
2020.lrec-1.519,D19-1229,1,0.828749,"a freely available multilingual dictionary. PanLex contains lexical translations across thousands of the world’s languages and has recently garnered interest in the multilingual research community. Its lexical translations are sourced from existing dictionaries and thesauri such as Wiktionary and WordNet. LanguageNet, as of September 2019, contains 1895 languages. We employ a simple procedure: using English as a pivot, we collect counts of how many languages have a translation for each English concept. (This dictionary pivoting strategy has previously been applied to model color terminology (McCarthy et al., 2019).) The concepts are then sorted in decreasing order by this count, resulting in our core vocabulary list. Up until recently, such a computational procedure would have been impossible without the computing resources and datasets available today. Figure 1 shows the top 30 concepts along with the number of dictionaries that contain them.2 The fact that so many languages’ dictionaries contain these words is a strong indicator of the coreness of these words. This point is even more salient for dictionaries of low-resource languages: that so many lexicographers have included these words in their lan"
2020.lrec-1.519,2020.lrec-1.488,1,0.591366,"g being cognates in related languages and often not being compositional. In addition, the core words span multiple domains and cover high frequency concepts which ought to be translatable in any language. We employed a cognate prediction model to translate the core vocabulary words with promising results. Based on the consensus of thousands of lexicographers across the world’s languages, in constructing dictionaries for low-resource languages, translations of these core words can be elicited by field linguists or computationally via a cognate methods or inflectional generation methods such as Nicolai et al. (2020). Code and data used in this paper, including the full list of core vocabulary words, is available at https://github.com/wswu/corevoc. 8. pounding model of Wu and Yarowsky (2018c). They analyze words by splitting the word into two component parts. By accumulating counts of the these components across all languages, they derive “recipes” for a concept, e.g. the concept of hospital is often realized as a compound of sick and house in many languages, even those unrelated to each other. We use this compounding model to analyze translations of our core vocabulary across languages. We find 278 conce"
2020.lrec-1.519,L18-1150,1,0.881392,".17 0.19 0.57 0.51 0.73 0.45 0.61 0.23 0.67 0.51 0.27 0.28 0.59 0.63 0.94 0.56 0.75 Figure 4: Coverage of lists over various corpora. The number of types and tokens for each corpus is in Table 4. Comparisons are only valid between same size lists, i.e. between columns 1 and 2, 3 and 4, and 5 and 6. Corpus Types Tokens Bible UDHR BNC ANC GNG 8,674 197 5,464 10,000 10,000 790K 1,773 62M 20M 341B Table 4: Corpus sizes phylogenetic relationships between languages. Thus if two languages are related, their respective Swadesh words are likely to be cognates. In this section, we expand on the work of Wu and Yarowsky (2018b), who devised a cognate translation method for the bilingual lexicon induction task. They discovered cognates from a multilingual dictionary in an unsupervised manner by using English as a pivot and then clustered these translations into cognate groups based on edit distance. Taking the Cartesian product of words in each cluster as word pairs, they run an aligner to extract character insertion, deletion, and substitution probabilities to be used as costs in a weighted edit distance in a second clustering iteration. The results of the second clustering were used to train characterbased machin"
2020.lrec-1.519,L18-1538,1,0.77089,".17 0.19 0.57 0.51 0.73 0.45 0.61 0.23 0.67 0.51 0.27 0.28 0.59 0.63 0.94 0.56 0.75 Figure 4: Coverage of lists over various corpora. The number of types and tokens for each corpus is in Table 4. Comparisons are only valid between same size lists, i.e. between columns 1 and 2, 3 and 4, and 5 and 6. Corpus Types Tokens Bible UDHR BNC ANC GNG 8,674 197 5,464 10,000 10,000 790K 1,773 62M 20M 341B Table 4: Corpus sizes phylogenetic relationships between languages. Thus if two languages are related, their respective Swadesh words are likely to be cognates. In this section, we expand on the work of Wu and Yarowsky (2018b), who devised a cognate translation method for the bilingual lexicon induction task. They discovered cognates from a multilingual dictionary in an unsupervised manner by using English as a pivot and then clustered these translations into cognate groups based on edit distance. Taking the Cartesian product of words in each cluster as word pairs, they run an aligner to extract character insertion, deletion, and substitution probabilities to be used as costs in a weighted edit distance in a second clustering iteration. The results of the second clustering were used to train characterbased machin"
2020.lrec-1.519,L18-1612,1,0.925934,".17 0.19 0.57 0.51 0.73 0.45 0.61 0.23 0.67 0.51 0.27 0.28 0.59 0.63 0.94 0.56 0.75 Figure 4: Coverage of lists over various corpora. The number of types and tokens for each corpus is in Table 4. Comparisons are only valid between same size lists, i.e. between columns 1 and 2, 3 and 4, and 5 and 6. Corpus Types Tokens Bible UDHR BNC ANC GNG 8,674 197 5,464 10,000 10,000 790K 1,773 62M 20M 341B Table 4: Corpus sizes phylogenetic relationships between languages. Thus if two languages are related, their respective Swadesh words are likely to be cognates. In this section, we expand on the work of Wu and Yarowsky (2018b), who devised a cognate translation method for the bilingual lexicon induction task. They discovered cognates from a multilingual dictionary in an unsupervised manner by using English as a pivot and then clustered these translations into cognate groups based on edit distance. Taking the Cartesian product of words in each cluster as word pairs, they run an aligner to extract character insertion, deletion, and substitution probabilities to be used as costs in a weighted edit distance in a second clustering iteration. The results of the second clustering were used to train characterbased machin"
2020.lt4hala-1.18,P12-1000,0,0.22836,"Missing"
2020.lt4hala-1.18,K17-2001,0,0.0652804,"Missing"
2020.lt4hala-1.18,K18-3001,1,0.840376,"erator into a contextual tagger. These modifications are described in Section 2. We also experiment with a neural machine translation system with no modifications. Our results indicate that out-of-the-box tools already perform at a very high level for Latin, but that small boosts in performance can be observed through simple modifications and ensembling of different learning algorithms. We discuss our results in more detail in Section 5. 2. System Description Since 2016, SIGMORPHON has hosted a series of Shared Tasks in morphological inflection (Cotterell et al., 2016; Cotterell et al., 2017; Cotterell et al., 2018; McCarthy et al., 2019). Increasingly, the tasks have become dominated by neural encoder-decoder architectures with heavy copybiasing. Originally borrowed from the neural machine translation (NMT) community (Bahdanau et al., 2014), the systems have converged around hard-attentional transducers over edit actions (Aharoni and Goldberg, 2017). Figure 1: The difference between inflection generation and contextual tagging. 2.1. System 1: Seq-to-seq morphological analysis As our starting point, we take the system of Makarov and Clematide (2018), the highest performing system in the 2018 shared task"
2020.lt4hala-1.18,P17-4012,0,0.0295879,"e decoder, a hard attention mechanism feeds a decoder that generates edit actions (either “copy”, “step”, or “insert-x”), before producing the final output: lego+VERB. The dual decoder produces the lemma in the same way, but uses a second decoder with a global attention mechanism to produce a single POS tag. 2.2. System 2: Neural Machine Translation Our second system submission is meant to serve as a strong baseline to compare with System 1. Treating the lemmatization and POS tagging tasks as a sequence prediction problem, we employ an off-the-shelf neural machine translation toolkit OpenNMT (Klein et al., 2017) with modifications to the data preprocessing. For both tasks, the input is the Latin word with its previous and next words in the sentence (including sentence boundary tokens). We train a SentencePiece (Kudo and Richardson, 2018) model with a vocabulary size of 8000 and apply it on both the input and output for lemmatization, and only the input for POS tagging. An example is shown in Table 1. 2.3. Ensembling In addition to producing multiple individual systems, we ensemble each system, using a linear combination of each Input: Output (lemma): Output (POS): Table 1: Data format for System 2 af"
2020.lt4hala-1.18,D18-2012,0,0.0273835,"t uses a second decoder with a global attention mechanism to produce a single POS tag. 2.2. System 2: Neural Machine Translation Our second system submission is meant to serve as a strong baseline to compare with System 1. Treating the lemmatization and POS tagging tasks as a sequence prediction problem, we employ an off-the-shelf neural machine translation toolkit OpenNMT (Klein et al., 2017) with modifications to the data preprocessing. For both tasks, the input is the Latin word with its previous and next words in the sentence (including sentence boundary tokens). We train a SentencePiece (Kudo and Richardson, 2018) model with a vocabulary size of 8000 and apply it on both the input and output for lemmatization, and only the input for POS tagging. An example is shown in Table 1. 2.3. Ensembling In addition to producing multiple individual systems, we ensemble each system, using a linear combination of each Input: Output (lemma): Output (POS): Table 1: Data format for System 2 after processing with SentencePiece. system’s confidence scores from the decoder2 . To aid the ensemble, we produce 10-best lists for each system, which requires a small modification to the beam search: each decoder produces a 10-be"
2020.lt4hala-1.18,C18-1008,0,0.125728,"ggers. Additionally, its word order is much more fluid than languages like English, handicapping n-gram taggers such as HMMs that rely on language modeling to produce tag sequences. We consider lemmatization to be a special case of morphological reinflection (Cotterell et al., 2017), which takes as input one inflected form of a word and produces another, given the desired morpho-syntactic description (MSD) of the output form. Likewise, POS-tagging is a special case of morphological tagging but with a greatly reduced tagset. Beginning with the state-of-the-art neural morphological generator of Makarov and Clematide (2018), we make several small modifications to both its input representation and its learning algorithm to transform it from a context-free generator into a contextual tagger. These modifications are described in Section 2. We also experiment with a neural machine translation system with no modifications. Our results indicate that out-of-the-box tools already perform at a very high level for Latin, but that small boosts in performance can be observed through simple modifications and ensembling of different learning algorithms. We discuss our results in more detail in Section 5. 2. System Description"
2020.lt4hala-1.18,W19-4226,1,0.82768,"tagger. These modifications are described in Section 2. We also experiment with a neural machine translation system with no modifications. Our results indicate that out-of-the-box tools already perform at a very high level for Latin, but that small boosts in performance can be observed through simple modifications and ensembling of different learning algorithms. We discuss our results in more detail in Section 5. 2. System Description Since 2016, SIGMORPHON has hosted a series of Shared Tasks in morphological inflection (Cotterell et al., 2016; Cotterell et al., 2017; Cotterell et al., 2018; McCarthy et al., 2019). Increasingly, the tasks have become dominated by neural encoder-decoder architectures with heavy copybiasing. Originally borrowed from the neural machine translation (NMT) community (Bahdanau et al., 2014), the systems have converged around hard-attentional transducers over edit actions (Aharoni and Goldberg, 2017). Figure 1: The difference between inflection generation and contextual tagging. 2.1. System 1: Seq-to-seq morphological analysis As our starting point, we take the system of Makarov and Clematide (2018), the highest performing system in the 2018 shared task. Note, however, that th"
2020.lt4hala-1.18,2020.lt4hala-1.16,0,0.0392698,"the single encoder, our simple context incorporation method does lead to modest improvements. Furthermore, the implementation of student-forcing, which approximates a test-time environment during training time, is also beneficial. Error analysis reveals that the majority of the mistakes made by our system are due to a confusion of affixes across parts-of-speech. Keywords: evalatin, morphology, encoder-decoder, lemmatization, pos-tagging 1. Introduction In this paper, we describe our system as participants in the EvaLatin Shared Task on lemmatization and part-of-speech (POS) tagging of Latin (Sprugnoli et al., 2020). Latin represents an interesting challenge for POS taggers — unlike English, its substantial inflectional morphology leads to significant data sparsity, resulting in large numbers of out-ofvocabulary (OOV) words for type-based taggers. Additionally, its word order is much more fluid than languages like English, handicapping n-gram taggers such as HMMs that rely on language modeling to produce tag sequences. We consider lemmatization to be a special case of morphological reinflection (Cotterell et al., 2017), which takes as input one inflected form of a word and produces another, given the des"
2020.sigmorphon-1.1,K18-3001,1,0.899071,"logical reinflection, we specifically focus on typological diversity and aim to investigate systems’ ability to generalize across typologically distinct languages many of which are low-resource. For example, if a neural network architecture works well for a sample of IndoEuropean languages, should the same architecture also work well for Tupi–Guarani languages (where nouns are “declined” for tense) or Austronesian languages (where verbal morphology is frequently prefixing)? 2 Task Description The 2020 iteration of our task is similar to CoNLL-SIGMORPHON 2017 (Cotterell et al., 2017) and 2018 (Cotterell et al., 2018) in that participants are required to design a model that learns to generate inflected forms from a lemma and a set of morphosyntactic features that derive the desired target form. For each language we provide a separate training, development, and test set. More historically, all of these tasks resemble the classic “wug”-test that Berko (1958) developed to test child and human knowledge of English nominal morphology. Unlike the task from earlier years, this year’s task proceeds in three phases: a Development Phase, a Generalization Phase, and an Evaluation Phase, in which each phase introduces"
2020.sigmorphon-1.1,K17-2001,1,0.928876,"e SIGMORPHON shared task on morphological reinflection, we specifically focus on typological diversity and aim to investigate systems’ ability to generalize across typologically distinct languages many of which are low-resource. For example, if a neural network architecture works well for a sample of IndoEuropean languages, should the same architecture also work well for Tupi–Guarani languages (where nouns are “declined” for tense) or Austronesian languages (where verbal morphology is frequently prefixing)? 2 Task Description The 2020 iteration of our task is similar to CoNLL-SIGMORPHON 2017 (Cotterell et al., 2017) and 2018 (Cotterell et al., 2018) in that participants are required to design a model that learns to generate inflected forms from a lemma and a set of morphosyntactic features that derive the desired target form. For each language we provide a separate training, development, and test set. More historically, all of these tasks resemble the classic “wug”-test that Berko (1958) developed to test child and human knowledge of English nominal morphology. Unlike the task from earlier years, this year’s task proceeds in three phases: a Development Phase, a Generalization Phase, and an Evaluation Pha"
2020.sigmorphon-1.1,W09-0106,0,0.0385713,"n variably surface as prefixes, suffixes, infixes, or circumfixes (Dryer, 2013). Most Eurasian and Australian languages strongly favor suffixation, and the same holds true, but to a lesser extent, for South American and New Guinean languages (Dryer, 2013). In Mesoamerican languages and African languages spoken below the Sahara, prefixation is dominant instead. These are just three dimensions of variation in morphology, and the cross-linguistic variation is already considerable. Such cross-lingual variation makes the development of natural language processing (NLP) applications challenging. As Bender (2009, 2016) notes, many current architectures and training and tuning algorithms still present language-specific biases. The most commonly used language for developing NLP applications is English. Along the above dimensions, English is productively concatenative, a mixture of analytic and synthetic, and largely suffixing in its inflectional morphology. With respect to languages that exhibit inflectional morphology, English is relatively impoverished.1 Importantly, English is just one morphological system among many. A larger goal of natural language processing is that the system work for any prese"
2020.sigmorphon-1.1,2020.lrec-1.344,1,0.878264,"Missing"
2020.sigmorphon-1.1,2020.sigmorphon-1.15,0,0.0565092,"Missing"
2020.sigmorphon-1.1,2020.sigmorphon-1.14,0,0.0439232,"Missing"
2020.sigmorphon-1.1,L16-1379,0,0.0190826,"Missing"
2020.sigmorphon-1.1,K17-2010,1,0.837279,"l baselines were based on a neural transducer (Wu and Cotterell, 2019), which is essentially a hard monotonic attention model (mono-*). The second baseline is a transformer (Vaswani et al., 2017) adopted for character-level tasks that currently holds the state-of-the-art on the 2017 SIGMORPHON shared task data (Wu et al., 2020, trm-*). Both models take the lemma and morphological tags as input and output the target inflection. The baseline is further expanded to include the data augmentation technique used by Anastasopoulos and Neubig (2019, -aug-) (conceptually similar to the one proposed by Silfverberg et al. (2017)). Relying on a simple characterlevel alignment between lemma and form, this technique replaces shared substrings of length &gt; 3 with random characters from the language’s alphabet, producing hallucinated lemma–tag–form triples. Both neural baselines were trained in mono- (*-single) and multilingual (shared parameters among the same family, *-shared) settings. 6 Many teams based their models on the transformer architecture. NYU-CUBoulder experimented with a vanilla transformer model (NYU-CUBoulder-04-0), a pointer-generator transformer that allows for a copy mechanism (NYU-CUBoulder-02-0), and"
2020.sigmorphon-1.1,2020.sigmorphon-1.4,0,0.0612223,"Missing"
2020.sigmorphon-1.1,W19-4207,0,0.0125147,"c attention model with improved alignment strategy. This model is further improved (flexica-03-1) by introducing a data hallucination technique which is based on phonotactic modelling of extremely low-resource languages (Shcherbakov et al., 2016). LTI focused on their earlier model (Anastasopoulos and Neubig, 2019), a neural multi-source encoder–decoder with two-step attention architecture, training it with hallucinated data, cross-lingual transfer, and romanization of scripts to improve performance on low-resource languages. DeepSpin reimplemented gated sparse two-headed attention model from Peters and Martins (2019) and trained it on all languages at once (massively multilingual). The team experimented with two modifications of the softmax function: sparsemax (Martins and Astudillo, 2016, deepspin-02-1) and 1.5-entmax (Peters et al., 2019, deepspin-01-1). Neural Neural baselines were based on a neural transducer (Wu and Cotterell, 2019), which is essentially a hard monotonic attention model (mono-*). The second baseline is a transformer (Vaswani et al., 2017) adopted for character-level tasks that currently holds the state-of-the-art on the 2017 SIGMORPHON shared task data (Wu et al., 2020, trm-*). Both"
2020.sigmorphon-1.1,P19-1146,0,0.0351308,"Missing"
2020.sigmorphon-1.1,P19-1148,1,0.838088,"lti-source encoder–decoder with two-step attention architecture, training it with hallucinated data, cross-lingual transfer, and romanization of scripts to improve performance on low-resource languages. DeepSpin reimplemented gated sparse two-headed attention model from Peters and Martins (2019) and trained it on all languages at once (massively multilingual). The team experimented with two modifications of the softmax function: sparsemax (Martins and Astudillo, 2016, deepspin-02-1) and 1.5-entmax (Peters et al., 2019, deepspin-01-1). Neural Neural baselines were based on a neural transducer (Wu and Cotterell, 2019), which is essentially a hard monotonic attention model (mono-*). The second baseline is a transformer (Vaswani et al., 2017) adopted for character-level tasks that currently holds the state-of-the-art on the 2017 SIGMORPHON shared task data (Wu et al., 2020, trm-*). Both models take the lemma and morphological tags as input and output the target inflection. The baseline is further expanded to include the data augmentation technique used by Anastasopoulos and Neubig (2019, -aug-) (conceptually similar to the one proposed by Silfverberg et al. (2017)). Relying on a simple characterlevel alignme"
2020.sigmorphon-1.1,2020.sigmorphon-1.5,0,0.0487999,"Missing"
2020.sigmorphon-1.25,N10-1103,0,0.0281455,"l et al., 2016, 2017, 2018; McCarthy et al., 2019), and typically consists of a modified sequence-to-sequence model with attention (Makarov and Clematide, 2018). However, these systems are fully supervised, and hand-curated morphological dictionaries often do not exist. We instead turn to the methods of Nicolai and Yarowsky (2019), who use English annotation as distant supervision to induce target language morphology, using a widelytranslated, verse-parallel text: the Bible. Starting from the inflection pairs extracted by their method, we ensemble generators trained using an RNN and DirecTL+ (Jiampojamarn et al., 2010). For each lemma in the respective UniMorph, we generate hypotheses for each feature bundle, ensembling via a linear combination of confidence scores. This gives us a set of inflections for each of the lexemes in the evaluation set which can then be searched for in the speech. The lexeme-set KWS Pipeline The pipeline starts with a lexeme of interest from the evaluation set (§2.1). Inflections of the lexeme are generated using some generation tool or manual resource (§2.2). These inflections are then converted to a phonemic representation (§2.3) before being added to the lexicon used in speech"
2020.sigmorphon-1.25,P19-1172,1,0.838134,"eely available online.2 2 Inflection generation is the task of producing an inflection, given a lemma and a bundle of morphosyntactic features. For example, run + {P RES ;3;S G} 7→ “runs”. The state of the art in inflection generation has arisen from the CoNLL– SIGMORPHON Shared Tasks (Cotterell et al., 2016, 2017, 2018; McCarthy et al., 2019), and typically consists of a modified sequence-to-sequence model with attention (Makarov and Clematide, 2018). However, these systems are fully supervised, and hand-curated morphological dictionaries often do not exist. We instead turn to the methods of Nicolai and Yarowsky (2019), who use English annotation as distant supervision to induce target language morphology, using a widelytranslated, verse-parallel text: the Bible. Starting from the inflection pairs extracted by their method, we ensemble generators trained using an RNN and DirecTL+ (Jiampojamarn et al., 2010). For each lemma in the respective UniMorph, we generate hypotheses for each feature bundle, ensembling via a linear combination of confidence scores. This gives us a set of inflections for each of the lexemes in the evaluation set which can then be searched for in the speech. The lexeme-set KWS Pipeline"
2020.sigmorphon-1.25,L18-1293,1,0.898687,"Missing"
2020.sigmorphon-1.25,N06-1030,0,0.0609829,"other methods. The result is an evaluation set tailored to morphologically salient word forms, with 1250 Turkish paradigms and 59 Bengali paradigms. The set of evaluation languages that can be extended to other languages in the Babel set for which we have ground truth paradigms. 2 Inflection Generation Grapheme-to-Phoneme Conversion To include hypothesized inflections in the KWS pipeline, orthographic forms of inflections must be mapped to a phonemic form consistent with the units used by the acoustic model (Maskey et al., 2004; Chen et al., 2016; Mortensen et al., 2018; Schultz et al., 2007; Kominek and Black, 2006; Deri and Knight, 2016; Trmal et al., 2017). We use a finite-state transducer model trained with Phonetisaurus3 on 5,000 word forms in the target language. 2.4 Keyword Search After generating inflections of lemmas in the evaluation set, these inflections are then included in the lexicon used in KWS. The KWS involves decoding the speech into lattices, and assessing lattice’s inclusion of the keyword of interest. Our pipeline builds on the Kaldi OpenKWS system (Trmal et al., 2017), which uses the standard lattice indexing approach of (Can and Saraclar, 2011). We 3 github.com/AdolfVonKleist/ Pho"
2020.sigmorphon-1.25,K18-3001,1,0.853286,"mponents serves as a novel downstream evaluation of inflection generation approaches, as well the other components in the pipeline. We make this recipe and evaluation set freely available online.2 2 Inflection generation is the task of producing an inflection, given a lemma and a bundle of morphosyntactic features. For example, run + {P RES ;3;S G} 7→ “runs”. The state of the art in inflection generation has arisen from the CoNLL– SIGMORPHON Shared Tasks (Cotterell et al., 2016, 2017, 2018; McCarthy et al., 2019), and typically consists of a modified sequence-to-sequence model with attention (Makarov and Clematide, 2018). However, these systems are fully supervised, and hand-curated morphological dictionaries often do not exist. We instead turn to the methods of Nicolai and Yarowsky (2019), who use English annotation as distant supervision to induce target language morphology, using a widelytranslated, verse-parallel text: the Bible. Starting from the inflection pairs extracted by their method, we ensemble generators trained using an RNN and DirecTL+ (Jiampojamarn et al., 2010). For each lemma in the respective UniMorph, we generate hypotheses for each feature bundle, ensembling via a linear combination of co"
2020.sigmorphon-1.25,W19-4226,1,0.893738,"Missing"
2020.sigmorphon-1.25,L18-1429,0,0.0156979,"ructed evaluation sets to compare against our other methods. The result is an evaluation set tailored to morphologically salient word forms, with 1250 Turkish paradigms and 59 Bengali paradigms. The set of evaluation languages that can be extended to other languages in the Babel set for which we have ground truth paradigms. 2 Inflection Generation Grapheme-to-Phoneme Conversion To include hypothesized inflections in the KWS pipeline, orthographic forms of inflections must be mapped to a phonemic form consistent with the units used by the acoustic model (Maskey et al., 2004; Chen et al., 2016; Mortensen et al., 2018; Schultz et al., 2007; Kominek and Black, 2006; Deri and Knight, 2016; Trmal et al., 2017). We use a finite-state transducer model trained with Phonetisaurus3 on 5,000 word forms in the target language. 2.4 Keyword Search After generating inflections of lemmas in the evaluation set, these inflections are then included in the lexicon used in KWS. The KWS involves decoding the speech into lattices, and assessing lattice’s inclusion of the keyword of interest. Our pipeline builds on the Kaldi OpenKWS system (Trmal et al., 2017), which uses the standard lattice indexing approach of (Can and Sarac"
2020.sigmorphon-1.25,P15-2111,1,0.858481,"Missing"
2020.sigmorphon-1.25,D14-1095,0,0.0254514,"inclusion of the keyword of interest. Our pipeline builds on the Kaldi OpenKWS system (Trmal et al., 2017), which uses the standard lattice indexing approach of (Can and Saraclar, 2011). We 3 github.com/AdolfVonKleist/ Phonetisaurus https://github.com/oadams/inflection-kws 211 use augmented pronunciation lexicons for KWS, which has been shown to outperform proxy KWS, a popular alternative (Chen et al., 2013). The novel problem of lexeme-set KWS is related to work on out-of-vocabulary KWS, which has been approached by handling sub-word units such as syllables and morphemes (Trmal et al., 2014; Narasimhan et al., 2014; van Heerden et al., 2017; He et al., 2016). In contrast to KWS with sub-word granularity, our approach is to generate likely full-word inflections given a lemma. For language modeling, we used a 4-gram modified Kneser-Ney baseline (Kneser and Ney, 1995). We compare using as training data the indomain Babel text to the Bible, a resource available for many languages, and which was the resource used for cross-lingual distant supervision for inflection generation described in Section 2.2. Hypothesized inflections not seen in the training data receive some probability mass in language model smoot"
2020.sigmorphon-1.3,P03-1036,0,0.427615,"Missing"
2020.sigmorphon-1.3,2020.sigmorphon-1.11,0,0.0613575,"Missing"
2020.sigmorphon-1.3,N15-1107,1,0.866875,"riori (MAP) estimations to determine segmentation points, or minimum description length (MDL)-based approaches. However, they tended to make assumptions regarding how morphemes are combined, and worked best for purely concatenative morphology. Furthermore, these methods had no productive method of handling allomorphy—morphemic variance was simply treated as separate morphemes. tently word embeddings change between related word forms, with the goal of providing useful word embeddings for unseen words. Our task further differs from traditional paradigm completion (e.g., Dreyer and Eisner, 2011; Ahlberg et al., 2015) in that no seed paradigms are observed. Thus, no information is being provided regarding the paradigm size, inflectional features, or relationships between lemmas and inflected forms. Other recent work (Nicolai and Yarowsky, 2019; Nicolai et al., 2020) learned fine-grained morphosyntactic tools from the Bible, though they leveraged supervision projected from higher-resource languages (Yarowsky et al., 2001; T¨ackstr¨om et al., 2013). The task of unsupervised morphological paradigm completion concerns more than just segmentation: besides capturing how morphology is reflected in the word form,"
2020.sigmorphon-1.3,D11-1057,0,0.0701913,"(MLE) or maximum a posteriori (MAP) estimations to determine segmentation points, or minimum description length (MDL)-based approaches. However, they tended to make assumptions regarding how morphemes are combined, and worked best for purely concatenative morphology. Furthermore, these methods had no productive method of handling allomorphy—morphemic variance was simply treated as separate morphemes. tently word embeddings change between related word forms, with the goal of providing useful word embeddings for unseen words. Our task further differs from traditional paradigm completion (e.g., Dreyer and Eisner, 2011; Ahlberg et al., 2015) in that no seed paradigms are observed. Thus, no information is being provided regarding the paradigm size, inflectional features, or relationships between lemmas and inflected forms. Other recent work (Nicolai and Yarowsky, 2019; Nicolai et al., 2020) learned fine-grained morphosyntactic tools from the Bible, though they leveraged supervision projected from higher-resource languages (Yarowsky et al., 2001; T¨ackstr¨om et al., 2013). The task of unsupervised morphological paradigm completion concerns more than just segmentation: besides capturing how morphology is refle"
2020.sigmorphon-1.3,J01-2001,0,0.785887,"Missing"
2020.sigmorphon-1.3,P82-1020,0,0.795212,"Missing"
2020.sigmorphon-1.3,2020.acl-main.598,1,0.850896,"ne of the submitted systems was able to improve over the baseline on average over all 9 test languages. Only on 3 languages did a submitted system obtain the best results. This shows that unsupervised morphological paradigm completion is still largely unsolved. We present an analysis here, so that this shared task will ground further research on the topic. 1 guess1 guess2 lemma2 lemma2 guess guess guess lemma 1 1 3 guess 2 4 guess guess 1 guess 5 guess 2 guess 3 guess 4 6 lemman guess guess guess 3 5 guess 4 6 guess5 guess6 Figure 1: The task of unsupervised morphological paradigm completion (Jin et al., 2020) consists of generating complete inflectional paradigms for given lemmas, with the only additional available information being a corpus without annotations. ing have not yet been developed. We anticipate that such systems will be extremely useful, as they will open the possibility of rapid development of first-pass inflectional paradigms in a large set of languages. These can be utilized both in se for generation and as a starting point for elicitation (Sylak-Glassman et al., 2016), thus aiding the development of low-resource human language technologies (Christianson et al., 2018). In this pap"
2020.sigmorphon-1.3,K18-3001,1,0.0792627,"Missing"
2020.sigmorphon-1.3,P16-2090,1,0.901708,"Missing"
2020.sigmorphon-1.3,K17-2001,1,0.925404,"Missing"
2020.sigmorphon-1.3,D18-1363,1,0.921705,"Missing"
2020.sigmorphon-1.3,L18-1293,1,0.871732,"Missing"
2020.sigmorphon-1.3,D18-2012,0,0.0402026,"Missing"
2020.sigmorphon-1.3,P19-1172,1,0.84237,"native morphology. Furthermore, these methods had no productive method of handling allomorphy—morphemic variance was simply treated as separate morphemes. tently word embeddings change between related word forms, with the goal of providing useful word embeddings for unseen words. Our task further differs from traditional paradigm completion (e.g., Dreyer and Eisner, 2011; Ahlberg et al., 2015) in that no seed paradigms are observed. Thus, no information is being provided regarding the paradigm size, inflectional features, or relationships between lemmas and inflected forms. Other recent work (Nicolai and Yarowsky, 2019; Nicolai et al., 2020) learned fine-grained morphosyntactic tools from the Bible, though they leveraged supervision projected from higher-resource languages (Yarowsky et al., 2001; T¨ackstr¨om et al., 2013). The task of unsupervised morphological paradigm completion concerns more than just segmentation: besides capturing how morphology is reflected in the word form, it also requires correctly clustering transformations into paradigm slots and, finally, generation of unobserved forms. Past shared tasks. This task extends a tradition of SIGMORPHON shared tasks concentrating on inflectional morp"
2020.sigmorphon-1.3,W10-2211,0,0.0931197,"Missing"
2020.sigmorphon-1.3,P17-1099,0,0.0243433,"stom inflection system. The IMS–CUBoulder team relied on LSTM (Hochreiter and Schmidhuber, 1997) sequence-tosequence models for inflection. In IMS-CUB-1, the generation component is based on the architecture by Bahdanau et al. (2015), but with fewer parameters, as suggested by Kann and Sch¨utze (2016). This model – as well as all other inflection components used for systems in this category – receives the sequence of the lemma’s characters and the paradigm slot number as input and produces a sequence of output characters. Their second system, IMS-CUB-2, uses an LSTM pointer-generator network (See et al., 2017) instead. This architecture has originally been proposed for low-resource morphological inflection by Sharma et al. (2018). The NYU–CUBoulder team also substituted the baseline’s generation component. Their morphological inflection models are ensembles of dif5 5.1 Results and Analysis Results on Development Languages To encourage reproducibility, we first report the performance of all systems on the development languages in the upper part of Table 4. Although participants were not evaluated on these languages, the results provide insight and enable future researchers to benchmark their progres"
2020.sigmorphon-1.3,2020.sigmorphon-1.9,1,0.755428,"Missing"
2020.sigmorphon-1.3,D18-1314,0,0.0742252,"ieval component is a list of inflected forms with their lemmas, annotated with a paradigm slot number. The generation component receives this output and prepares the data to train an inflectional generator. First, identified inflections are divided into a training and development split, and missing paradigm slots are identified. The generator is trained on the discovered inflections, and new forms are predicted for each missing slot. We used two morphological inflection systems for the two variants of our baseline: the non-neural baseline from Cotterell et al. (2017) and the model proposed by Makarov and Clematide (2018). Both are highly suitable for the low-resource setting. 4.2 ferent combinations of transformer sequence-tosequence models (Vaswani et al., 2017) and pointergenerator transformers, a model they introduced for the task. NYU-CUB-1 is an ensemble of 6 pointergenerator transformers, while NYU-CUB-2 is an ensemble of 6 vanilla transformers. Their last system, NYU-CUB-3, is an ensemble of all 12 models. 4.3 Submitted Systems: Segment+Conquer The KU–CST team did not modify the baseline directly, but, nevertheless, was heavily inspired by it. Their system first employs a charactersegmentation algorith"
2020.sigmorphon-1.3,W19-4226,1,0.657183,"tions, requiring participants to generate an inflection solely utilizing a provided lemma and sentential cues. This task further imitated language learners, but extended beyond morphological learning to morphosyntactic incorporation. Furthermore, removing the requirement of an inflectional feature vector more closely approximated the generation step in our task. However, it was still supervised in that participants were provided with lemma–inflection pairs in context during training. We, in contrast, made no assumption of the existence of such pairs. Finally, the fourth iteration of the task (McCarthy et al., 2019) again concentrated on lesssupervised inflection. Cross-lingual training allowed low-resource inflectors to leverage information from high-resource languages, while a contextual analysis task flipped the previous year’s contextual task on its head—tagging a sentence with inflectional information. This process is very similar to the retrieval portion of our task. We extended this effort to not only identify the paradigm slot of particular word, but to combine learned information from each class to extend and complete existing paradigms. Furthermore, we lifted the requirement of named inflection"
2020.sigmorphon-1.3,P08-1084,0,0.108806,"Missing"
2020.sigmorphon-1.3,N15-1186,0,0.0841628,"previously unencountered word forms, after having studied thousands of other types. The second task (Cotterell et al., 2017) extended While Xu et al. (2018) did discover something similar to paradigms, those paradigms were a means to a segmentation end and the shape or size of the paradigms was not a subject of their research. Moon et al. (2009) similarly uses segmentation and clustering of affixes to group words into conflation sets, groups of morphologically related words, in an unsupervised way. Their work assumes prefixing and suffixing morphology. In a more task-driven line of research, Soricut and Och (2015) develop an approach to learn morphological transformation rules from observing how consis58 6.2 the first task from 10 to 52 languages and started to encourage the development of tools for the lowresource setting. While the first shared task approximated an adult learner with experience with thousands of word forms, low-resource inflection was closer to the language learner that has only studied a small number of inflections—however, it was closer to L2 learning than L1, as it still required training sets with lemma–inflection–slot triplets. The 2017 edition of the shared task also introduced"
2020.sigmorphon-1.3,2020.lrec-1.352,1,0.595113,"rms for all lemmas in both the predictions and the ground truth before evaluating. Provided Resources We provided data for 5 development and 9 test languages. The development languages were available for system development and hyperparameter tuning, while the test languages were released shortly before the shared task deadline. For the test languages, no ground truth data was available before system submission. This setup emulated a realworld scenario with the goal to create a system for languages about which we have no information. For the raw text corpora, we leveraged the JHU Bible Corpus (McCarthy et al., 2020). This resource covers 1600 languages, which will enable future work to quickly produce systems for a large set of languages. Additionally, using the Bible allowed for a fair comparison of models across languages without potential confounds such as domain mismatch. 7 of the languages have only the New Testament available (approximately 8k sentences), and 7 have both the New and Old Testaments (approximately 31k sentences). All morphological information was taken from UniMorph (Sylak-Glassman et al., 2015; Kirov et al., 2018), a resource which contains paradigms for more than 100 languages. How"
2020.sigmorphon-1.3,L16-1497,0,0.0222648,"6 lemman guess guess guess 3 5 guess 4 6 guess5 guess6 Figure 1: The task of unsupervised morphological paradigm completion (Jin et al., 2020) consists of generating complete inflectional paradigms for given lemmas, with the only additional available information being a corpus without annotations. ing have not yet been developed. We anticipate that such systems will be extremely useful, as they will open the possibility of rapid development of first-pass inflectional paradigms in a large set of languages. These can be utilized both in se for generation and as a starting point for elicitation (Sylak-Glassman et al., 2016), thus aiding the development of low-resource human language technologies (Christianson et al., 2018). In this paper, we present the SIGMORPHON 2020 shared task on unsupervised morphological paradigm completion (SIGMORPHON 2020 Task 2). We asked participants to produce systems that can learn to inflect in an unsupervised fashion: given a small corpus (the Bible) together with a list of lemmas for each language, systems for the shared task should output all corresponding inflected forms. In their output, systems had to mark which forms expressed the same morphosyntactic features, e.g., demonstr"
2020.sigmorphon-1.3,D09-1070,0,0.0920434,"Missing"
2020.sigmorphon-1.3,P15-2111,0,0.0214204,"bout which we have no information. For the raw text corpora, we leveraged the JHU Bible Corpus (McCarthy et al., 2020). This resource covers 1600 languages, which will enable future work to quickly produce systems for a large set of languages. Additionally, using the Bible allowed for a fair comparison of models across languages without potential confounds such as domain mismatch. 7 of the languages have only the New Testament available (approximately 8k sentences), and 7 have both the New and Old Testaments (approximately 31k sentences). All morphological information was taken from UniMorph (Sylak-Glassman et al., 2015; Kirov et al., 2018), a resource which contains paradigms for more than 100 languages. However, this information was only accessible to the participants for the development languages. UniMorph paradigms were further used internally for evaluation on the test languages—this data was then released after the conclusion of the shared task. Example. Assume our gold standard is (1) (the complete, 5-slot English paradigms for the verbs walk and listen) and a system outputs the following, including an error in the fourth row: 3.2 Languages During the development phase of the shared task, we released"
2020.sigmorphon-1.3,Q13-1001,0,0.0712336,"Missing"
2020.sigmorphon-1.3,C18-1005,0,0.0169383,"rms. Past shared tasks. This task extends a tradition of SIGMORPHON shared tasks concentrating on inflectional morphology. The first such task (Cotterell et al., 2016) encouraged participants to create inflectional tools in a typologically diverse group of 10 languages. The task was fully-supervised, requiring systems to learn inflectional morphology from a large annotated database. This task is similar to human learners needing to generate inflections of previously unencountered word forms, after having studied thousands of other types. The second task (Cotterell et al., 2017) extended While Xu et al. (2018) did discover something similar to paradigms, those paradigms were a means to a segmentation end and the shape or size of the paradigms was not a subject of their research. Moon et al. (2009) similarly uses segmentation and clustering of affixes to group words into conflation sets, groups of morphologically related words, in an unsupervised way. Their work assumes prefixing and suffixing morphology. In a more task-driven line of research, Soricut and Och (2015) develop an approach to learn morphological transformation rules from observing how consis58 6.2 the first task from 10 to 52 languages"
2020.sigmorphon-1.3,H01-1035,0,0.0415472,"between related word forms, with the goal of providing useful word embeddings for unseen words. Our task further differs from traditional paradigm completion (e.g., Dreyer and Eisner, 2011; Ahlberg et al., 2015) in that no seed paradigms are observed. Thus, no information is being provided regarding the paradigm size, inflectional features, or relationships between lemmas and inflected forms. Other recent work (Nicolai and Yarowsky, 2019; Nicolai et al., 2020) learned fine-grained morphosyntactic tools from the Bible, though they leveraged supervision projected from higher-resource languages (Yarowsky et al., 2001; T¨ackstr¨om et al., 2013). The task of unsupervised morphological paradigm completion concerns more than just segmentation: besides capturing how morphology is reflected in the word form, it also requires correctly clustering transformations into paradigm slots and, finally, generation of unobserved forms. Past shared tasks. This task extends a tradition of SIGMORPHON shared tasks concentrating on inflectional morphology. The first such task (Cotterell et al., 2016) encouraged participants to create inflectional tools in a typologically diverse group of 10 languages. The task was fully-super"
2021.computel-1.1,N19-1388,0,0.0226523,"e and target set. Training Settings Preliminary experiments showed that multilingual systems trained on a single target corpus, i.e. the English Bible in our case, have a tendency to completely disregard the source sentence during test time and instead generate an unrelated English sentence as output. We dub this target overfitting. To counter this tendency, we employ four specialized training strategies: (1) Single Source translation (1Src) limits the number of training source texts to one even when we have multiple Bible translations in the same language 7 . (2) Heterogeneous batching (HB) (Aharoni et al., 2019) constructs minibatches by uniformly sampling sentences from the entire training data into each minibatch. In contrast, the common practice is to construct minibatches from training examples with similar length.8 (3) We increase the amount of English target data available to the model by adding monolingual English training examples where the source and target sentence are identical (E2E).9 (4) Finally, following Aharoni et al. (2019) we transform our many-to-English models into many-to-many models (M2M) by reversing the source and target language of our Bibles and combining the resulting data"
2021.computel-1.1,mayer-cysouw-2014-creating,0,0.0685248,"Missing"
2021.computel-1.1,2020.lrec-1.352,1,0.839547,"Missing"
2021.computel-1.1,2020.lrec-1.458,1,0.721072,"d low-resource languages. be less than beneficial for languages with high numbers of morphemes in each word. When we reduce the BPE vocabulary, we see a large increase in translation quality for all monolingual experiments, as the system sees many more short sequences. Unfortunately, we fail to leverage the increase in data as we add more languages from the same family, with the Algic (Cree) and Athabaskan (Navajo) family models still collapsing, and the Inuit-Aleut slightly decreasing. This result is not entirely unforeseen, although we didn’t expect it with such a small number of languages. Mueller et al. (2020) report that their models also completely devolved into translations that, while structurally fluent, were completely inadequate at representing the source translation. However, they did see small gains when the number of added languages was small. We hypothesize that our results degrade because of a lack of complete Bible translations. Mueller et al. (2020) start with complete translations, and the numbers only start failing as incomplete translations are added. We see small gains for the Inuit family, for which we have multiple complete Bibles. We hypothesize that many copies of an identical"
2021.computel-1.1,N19-4009,0,0.0184388,"ains a better BLEU score. When we extend our experiments to the entire Inuit-Aleut family, we see modest gains for both the Latin and Non-Latin languages. However, we also note that the translation quality collapses for the other language families. We suspect this may be due to a large BPE vocabulary - the Inuit-Aleut family, containing two scripts, is more likely to split words; the single script Athabaskan and Algic families, on the other hand, can simply memorize entire words, which may Model Details We use transformer systems for translation and train our models using the Fairseq toolkit (Ott et al., 2019), with 3 encoding and decoding layers, 4 attention heads, an embedding size of 512, and a maximum of 2000 tokens per batch6 . Models are trained for 100 epochs. We set aside the book of Revelation as an evaluation set: the first 100 verses serve as a validation set, and the final 304 verses form a held-out test set. 7 Discussions of dialects and languages aside, we include the largest source which contains the language name - thus, we choose one source only from Western, Eastern, Plains, and Moose Cree, for example. 8 According to our preliminary experiments, length-based batching can seriousl"
2021.computel-1.1,L18-1472,0,0.0256998,"Yes Yes No Yes No OT No No No No No No No No No No No Yes No No No No Yes No No No No No Yes No No No Yes Yes No No No No Books 27 1 27 27 27 30 41 8 2 2 27 67 28 1 28 27 67 27 27 27 1 4 67 27 5 27 67 67 27 17 27 2 Family Algic Athabaskan Inuit-Aleut Iroquoian Uto-Aztecan English Weighted TTR 12.74 9.12 36.92 22.04 8.04 2.22 Table 2: Weighted Type-to-Token ratios of collected language families. the same size. The languages that we collect exhibit a wide range of interesting linguistic phenomena. Several of the languages are predominantly SVO languages (if all arguments occur in the sentence) (Schmirler et al., 2018) but we also include languages like Haida where SOV constructions are prevalent (Enrico, 2003). We also have examples of both nominative-accusative alignment and ergative-absolutive alignment exemplified by Inuktitut in the Inuit-Aleut family (Nowak, 2011). Additionally, the languages display a large variety of interesting morphological features. We find examples of predominantly suffixing morphology in the Algic languages and extensive use of prefixes encountered in Athabaskan languages. Furthermore, animacy is an important grammatical category which is morphologically marked in Plains Cree ("
2021.naacl-main.435,P18-1198,0,0.0303327,"of the speech signal: specifically the presence or absence of the fricative [s] in the output of the network and the amplitude of frication. They show that manipulation of the variables changes these features in a predictable manner. Similarly to our work, Beguˇs (2020b) also scales state activations and observes the effect on the output of the network. In a related investigation of reduplication, Beguˇs (2020a) train GAN models on speech and identify variables which trigger reduplication in the speech signal. Extensive work exists on linguistic probing experiments for neural representations (Conneau et al., 2018a,b; Clark et al., 2019). A recent probing paper by Torroba Hennigen et al. (2020) is more directly related to our work. They present a decomposable probe for finding small sets of hidden states which encode for linguistically relevant information, particularly morphosyntactic information. Our work shares the aim of not only identifying if information is present in a neural system, but also examining how it is represented. However, we additionally perform experiments on manipulating network activations and examine how such manipulations influence the outputs of the network. Interpretation of n"
2021.naacl-main.435,D18-1269,0,0.0139971,"of the speech signal: specifically the presence or absence of the fricative [s] in the output of the network and the amplitude of frication. They show that manipulation of the variables changes these features in a predictable manner. Similarly to our work, Beguˇs (2020b) also scales state activations and observes the effect on the output of the network. In a related investigation of reduplication, Beguˇs (2020a) train GAN models on speech and identify variables which trigger reduplication in the speech signal. Extensive work exists on linguistic probing experiments for neural representations (Conneau et al., 2018a,b; Clark et al., 2019). A recent probing paper by Torroba Hennigen et al. (2020) is more directly related to our work. They present a decomposable probe for finding small sets of hidden states which encode for linguistically relevant information, particularly morphosyntactic information. Our work shares the aim of not only identifying if information is present in a neural system, but also examining how it is represented. However, we additionally perform experiments on manipulating network activations and examine how such manipulations influence the outputs of the network. Interpretation of n"
2021.naacl-main.435,K17-2001,1,0.890174,"Missing"
2021.naacl-main.435,2020.scil-1.5,0,0.0342658,"Missing"
2021.naacl-main.435,W19-4828,0,0.0254706,"cifically the presence or absence of the fricative [s] in the output of the network and the amplitude of frication. They show that manipulation of the variables changes these features in a predictable manner. Similarly to our work, Beguˇs (2020b) also scales state activations and observes the effect on the output of the network. In a related investigation of reduplication, Beguˇs (2020a) train GAN models on speech and identify variables which trigger reduplication in the speech signal. Extensive work exists on linguistic probing experiments for neural representations (Conneau et al., 2018a,b; Clark et al., 2019). A recent probing paper by Torroba Hennigen et al. (2020) is more directly related to our work. They present a decomposable probe for finding small sets of hidden states which encode for linguistically relevant information, particularly morphosyntactic information. Our work shares the aim of not only identifying if information is present in a neural system, but also examining how it is represented. However, we additionally perform experiments on manipulating network activations and examine how such manipulations influence the outputs of the network. Interpretation of neural representations in"
2021.naacl-main.435,2020.emnlp-main.15,0,0.0345602,"in the output of the network and the amplitude of frication. They show that manipulation of the variables changes these features in a predictable manner. Similarly to our work, Beguˇs (2020b) also scales state activations and observes the effect on the output of the network. In a related investigation of reduplication, Beguˇs (2020a) train GAN models on speech and identify variables which trigger reduplication in the speech signal. Extensive work exists on linguistic probing experiments for neural representations (Conneau et al., 2018a,b; Clark et al., 2019). A recent probing paper by Torroba Hennigen et al. (2020) is more directly related to our work. They present a decomposable probe for finding small sets of hidden states which encode for linguistically relevant information, particularly morphosyntactic information. Our work shares the aim of not only identifying if information is present in a neural system, but also examining how it is represented. However, we additionally perform experiments on manipulating network activations and examine how such manipulations influence the outputs of the network. Interpretation of neural representations in recurrent neural models has been an active area of resear"
2021.naacl-main.435,W16-2010,0,0.0450767,"Missing"
2021.naacl-main.435,W18-1817,0,0.0614385,"Missing"
2021.naacl-main.435,W19-4219,0,0.045635,"Missing"
2021.naacl-main.435,W97-1012,0,0.577375,"rticularly morphosyntactic information. Our work shares the aim of not only identifying if information is present in a neural system, but also examining how it is represented. However, we additionally perform experiments on manipulating network activations and examine how such manipulations influence the outputs of the network. Interpretation of neural representations in recurrent neural models has been an active area of research over a long period of time starting with Elman (1990). However, representations in models of phonology have received less attention than many other subfields of NLP. Rodd (1997) investigates learning of Turkish vowel harmony by a character-based RNN language model trained on Our approach was inspired by the now-classic word forms. The paper investigates hidden state ac- paper on visualization and interpretation of recurtivations of RNN models while varying the hidden rent networks by Karpathy et al. (2015) in that we state dimensionality between 1 and 4. It presents also seek individual interpretable dimensions. The evidence that RNN hidden states can capture Turk- work by Dalvi et al. (2019) on analyzing individish vowel harmony patterns when a sufficient num- ual n"
2021.naacl-main.435,W18-0314,1,0.84302,". The paper investigates hidden state ac- paper on visualization and interpretation of recurtivations of RNN models while varying the hidden rent networks by Karpathy et al. (2015) in that we state dimensionality between 1 and 4. It presents also seek individual interpretable dimensions. The evidence that RNN hidden states can capture Turk- work by Dalvi et al. (2019) on analyzing individish vowel harmony patterns when a sufficient num- ual neurons in networks trained for linguistic tasks ber of hidden dimensions are available. In a similar (POS tagging as well as semantic and morphologivein, Silfverberg et al. (2018) investigate phoneme cal tagging) is more closely related to the present representations for Finnish, Spanish and Turkish work. They present a general methodology for finding correlations between embedding represen- uncovering neurons which encode linguistic infortations and phonological distinctive features. Ko- mation by training a classifier to predict linguistic lachina and Magyar (2019) present an investiga- features of the input based on the representations tion of phone embeddings learned using word2vec generated by the network. They also show that it is (Mikolov et al., 2013) for simul"
2021.sigmorphon-1.11,N15-1107,0,0.0201075,"2020). However, paradigm clustering systems do not infer missing forms in paradigms. Our system resembles the baseline system for the paradigm completion task (Jin et al., 2020) which also extracts transformation rules, however, in the form of edit trees (Chrupala et al., 2008). Several approaches to unsupervised or minimally supervised morphology learning, which share characteristics with our system, have been proposed. Our rules are essentially identical to the FST rules used by Beemer et al. (2020) for the task of supervised morphological inflection. Likewise, Durrett and DeNero (2013) and Ahlberg et al. (2015) both extract inflectional rules after aligning forms from known paradigms. Yarowsky and Wicentowski (2000) also generate rules for morphological transformations but their system for minimally supervised morphological analysis requires additional information in the form of a list of morphemes as input. Erdmann et al. (2020) present a task called the paradigm discovery problem which is quite similar to the unsupervised paradigm clustering task. In their formulation of the task, inflected forms are clustered into paradigms and corre3.2 Transformation rules Our approach builds on the baseline par"
2021.sigmorphon-1.11,Q17-1010,0,0.254555,"Missing"
2021.sigmorphon-1.11,chrupala-etal-2008-learning,0,0.150233,"Missing"
2021.sigmorphon-1.11,K18-3001,1,0.913747,"Missing"
2021.sigmorphon-1.11,K17-2001,0,0.131853,"Missing"
2021.sigmorphon-1.11,N13-1138,0,0.187348,"digm completion (Kann et al., 2020). However, paradigm clustering systems do not infer missing forms in paradigms. Our system resembles the baseline system for the paradigm completion task (Jin et al., 2020) which also extracts transformation rules, however, in the form of edit trees (Chrupala et al., 2008). Several approaches to unsupervised or minimally supervised morphology learning, which share characteristics with our system, have been proposed. Our rules are essentially identical to the FST rules used by Beemer et al. (2020) for the task of supervised morphological inflection. Likewise, Durrett and DeNero (2013) and Ahlberg et al. (2015) both extract inflectional rules after aligning forms from known paradigms. Yarowsky and Wicentowski (2000) also generate rules for morphological transformations but their system for minimally supervised morphological analysis requires additional information in the form of a list of morphemes as input. Erdmann et al. (2020) present a task called the paradigm discovery problem which is quite similar to the unsupervised paradigm clustering task. In their formulation of the task, inflected forms are clustered into paradigms and corre3.2 Transformation rules Our approach"
2021.sigmorphon-1.11,2020.acl-main.695,0,0.169931,"ly supervised morphology learning, which share characteristics with our system, have been proposed. Our rules are essentially identical to the FST rules used by Beemer et al. (2020) for the task of supervised morphological inflection. Likewise, Durrett and DeNero (2013) and Ahlberg et al. (2015) both extract inflectional rules after aligning forms from known paradigms. Yarowsky and Wicentowski (2000) also generate rules for morphological transformations but their system for minimally supervised morphological analysis requires additional information in the form of a list of morphemes as input. Erdmann et al. (2020) present a task called the paradigm discovery problem which is quite similar to the unsupervised paradigm clustering task. In their formulation of the task, inflected forms are clustered into paradigms and corre3.2 Transformation rules Our approach builds on the baseline paradigms discovered in the previous step. We start by extracting transformation rules between all word forms in a single baseline paradigm. For each pair of strings like dog and dogs belonging to a paradigm, we generate a rule like ?+ 0:s which translates the first form into the second one. From a paradigm of size n, we can t"
2021.sigmorphon-1.11,2020.acl-main.598,0,0.343372,"ers (Wiemerslage et al., 2021). Here all forms sharing a given substring of length n are clustered into a paradigm. Duplicate paradigms are removed. The hyperparameter n can be tuned on validation data if such data is available (we use n = 5 in all our experiments). Related Work The unsupervised paradigm clustering task is closely related to the 2020 SIGMORPHON shared task on unsupervised morphological paradigm completion (Kann et al., 2020). However, paradigm clustering systems do not infer missing forms in paradigms. Our system resembles the baseline system for the paradigm completion task (Jin et al., 2020) which also extracts transformation rules, however, in the form of edit trees (Chrupala et al., 2008). Several approaches to unsupervised or minimally supervised morphology learning, which share characteristics with our system, have been proposed. Our rules are essentially identical to the FST rules used by Beemer et al. (2020) for the task of supervised morphological inflection. Likewise, Durrett and DeNero (2013) and Ahlberg et al. (2015) both extract inflectional rules after aligning forms from known paradigms. Yarowsky and Wicentowski (2000) also generate rules for morphological transforma"
2021.sigmorphon-1.11,2020.sigmorphon-1.3,1,0.867261,"uages with a high morpheme-to-word ratio like Basque. 2 Methods 3.1 Baseline As a baseline, we use the character n-gram clustering method provided by the shared task organizers (Wiemerslage et al., 2021). Here all forms sharing a given substring of length n are clustered into a paradigm. Duplicate paradigms are removed. The hyperparameter n can be tuned on validation data if such data is available (we use n = 5 in all our experiments). Related Work The unsupervised paradigm clustering task is closely related to the 2020 SIGMORPHON shared task on unsupervised morphological paradigm completion (Kann et al., 2020). However, paradigm clustering systems do not infer missing forms in paradigms. Our system resembles the baseline system for the paradigm completion task (Jin et al., 2020) which also extracts transformation rules, however, in the form of edit trees (Chrupala et al., 2008). Several approaches to unsupervised or minimally supervised morphology learning, which share characteristics with our system, have been proposed. Our rules are essentially identical to the FST rules used by Beemer et al. (2020) for the task of supervised morphological inflection. Likewise, Durrett and DeNero (2013) and Ahlbe"
2021.sigmorphon-1.11,W19-4226,1,0.867098,"utilize word embeddings when extracting rules due to the very small size of the shared task datasets. In addition to prefix and suffix rules, we also experiment with more general discontinuous transformation rules which can apply transformations to infixes as well as prefixes and suffixes. For example, the rule Introduction Supervised sequence-to-sequence models for word inflection have delivered impressive results in the past few years and a number of shared tasks on supervised learning of morphology have helped to raise the state of the art of this task (Cotterell et al., 2016, 2017, 2018; McCarthy et al., 2019; Vylomova et al., 2020). In contrast, unsupervised approaches to morphology have received far less attention in recent years. Nevertheless, the question of whether the morphological system of a language can be discovered from raw text data alone is certainly an interesting one. This paper describes the submission of the CU-UBC team for the SIGMORPHON 2021 Shared Task 2: Unsupervised morphological paradigm clustering (Wiemerslage et al., 2021).1 The objective of this task is to group 1 ?+ i:0 ?+ e:i ?+ 0:t would transform the input form gidem (‘to bite’ in Maltese) to gdimt. Our results github"
2021.sigmorphon-1.11,2020.lrec-1.352,1,0.495796,"other words in the paradigm, we then compare their cosine similarity r0 to one of the reference forms. Forms fail the embedding-similarity test if r0 &lt; 0.5 and r − r0 > 0.3. 4 Experiments and Results In this section, we describe experiments on the shared task development and test languages. 4.1 Data and Resources The shared task uses two data resources. Corpus data for the four development languages (Maltese, Persian, Russian and Swedish) and nine test languages (Basque Bulgarian, English, Finnish, German, Kannada, Navajo, Spanish and Turkish) are sourced from the Johns Hopkins Bible Corpus (McCarthy et al., 2020b). For most of the languages, complete Bibles were provided but for some of them, we only had access to a subset (see Wiemerslage et al. (2021) for details). Gold standard paradigms were automatically generated using the Unimorph 3.0 database (McCarthy et al., 2020a). 4.2 Experiments on validation languages Since our transformation rules are generated from paradigms discovered by the baseline system, which contain incorrect items, it is to be expected that some incorrect rules are generated. We filter out infrequent rules, as they are less likely to represent genuine morphological transformat"
2021.sigmorphon-1.11,N15-1186,0,0.023322,"best overall performance is delivered by prefix and suffix rules but more general transformation rules perform better for languages with templatic morphology and very high morpheme-to-word ratios. 1 [w a l k e d] .o. [?+ e:i d:n 0:g] will result in an output form w a l k i n g. We cluster forms into the same paradigm if we can find morphological transformation rules which map one of the forms into the other. Our approach is illustrated in Figure 2. We experiment with two methods for discovering rules, described in Section 3.3. Our first approach is inspired by work on morphology discovery by Soricut and Och (2015), who generate prefix and suffix transformations between similar strings. This idea closely parallels our approach for extracting rules. Unlike Soricut and Och (2015), however, we do not utilize word embeddings when extracting rules due to the very small size of the shared task datasets. In addition to prefix and suffix rules, we also experiment with more general discontinuous transformation rules which can apply transformations to infixes as well as prefixes and suffixes. For example, the rule Introduction Supervised sequence-to-sequence models for word inflection have delivered impressive re"
2021.sigmorphon-1.11,2020.sigmorphon-1.1,1,0.787901,"Missing"
2021.sigmorphon-1.11,P00-1027,0,0.0621708,"esembles the baseline system for the paradigm completion task (Jin et al., 2020) which also extracts transformation rules, however, in the form of edit trees (Chrupala et al., 2008). Several approaches to unsupervised or minimally supervised morphology learning, which share characteristics with our system, have been proposed. Our rules are essentially identical to the FST rules used by Beemer et al. (2020) for the task of supervised morphological inflection. Likewise, Durrett and DeNero (2013) and Ahlberg et al. (2015) both extract inflectional rules after aligning forms from known paradigms. Yarowsky and Wicentowski (2000) also generate rules for morphological transformations but their system for minimally supervised morphological analysis requires additional information in the form of a list of morphemes as input. Erdmann et al. (2020) present a task called the paradigm discovery problem which is quite similar to the unsupervised paradigm clustering task. In their formulation of the task, inflected forms are clustered into paradigms and corre3.2 Transformation rules Our approach builds on the baseline paradigms discovered in the previous step. We start by extracting transformation rules between all word forms"
2021.sigmorphon-1.15,K18-3001,1,0.845415,"ani et al., 2017), transformer-based architectures have begun looming large (e.g., Yolchuyeva et al., 2019). Recent years have also seen works that capitalize on multilingual data to train a single model with grapheme-phoneme pairs from multiple languages. For example, various systems from last year’s shared task submissions learned from a multilingual signal (e.g., ElSaadany and Suter, 2020; Peters and Martins, 2020; Vesik et al., 2020). 3.3 Baselines The official baselines for individual languages are based on an ensembled neural transducer trained with the imitation learning (IL) paradigm (Makarov and Clematide, 2018a). The baseline WERs are tabulated in Table 3. In what follows, we overview this baseline neural-transducer system, as our models are built on top of this system. The detailed formal description of the baseline system can be found in Makarov and Clematide (2018a,b,c, 2020). The neural transducer in question defines a conditional distribution over edit actions, such as copy, deletion, insertion, and substitution: pθ (y, a|x) = 3 The Low-resource Subtask pθ (aj |a<j , x), j=1 This section provides relevant information concerning the low-resource subtask. 3.1 Task Data The provided data in the l"
2021.sigmorphon-1.15,2020.sigmorphon-1.7,0,0.0155741,"n broken (Yao and Zweig, 2015). Attention further improved the performance, as attentional encoder-decoders (Toshniwal and Livescu, 2016) learned to focus on specific input sequences. As attention became “all that was needed” (Vaswani et al., 2017), transformer-based architectures have begun looming large (e.g., Yolchuyeva et al., 2019). Recent years have also seen works that capitalize on multilingual data to train a single model with grapheme-phoneme pairs from multiple languages. For example, various systems from last year’s shared task submissions learned from a multilingual signal (e.g., ElSaadany and Suter, 2020; Peters and Martins, 2020; Vesik et al., 2020). 3.3 Baselines The official baselines for individual languages are based on an ensembled neural transducer trained with the imitation learning (IL) paradigm (Makarov and Clematide, 2018a). The baseline WERs are tabulated in Table 3. In what follows, we overview this baseline neural-transducer system, as our models are built on top of this system. The detailed formal description of the baseline system can be found in Makarov and Clematide (2018a,b,c, 2020). The neural transducer in question defines a conditional distribution over edit actions, suc"
2021.sigmorphon-1.15,N10-1112,0,0.034379,"(VP), which includes additional penalties for vowels and diacritics, however, does outperform the baselines in several languages. Furthermore, the macro WER average not only outperforms the baseline, but all other submitted systems. Table 1: Typical errors in the development set that involve vowels from Khmer (khm), Latvian (lat), and Slovene (slv) additional penalties. Each incorrectly-predicted vowel incurs this penalty. The penalty acts as a regularizer that forces the model to expend more effort on learning vowels. This modification is in the same spirit as the softmax-margin objective of Gimpel and Smith (2010), which penalizes highcost outputs more heavily, but our approach is even simpler—we merely supplement the loss with additional penalties for vowels and diacritics. We fine-tuned the vowel and diacritic penalties using a grid search on the development data, incrementing each by 0.1, from 0 to 0.5. In the cases of ties, we skewed higher as the penalties generally worked better at higher values. The final values used to generate predictions for the test data are listed in Table 2. We also note that the vowel penalty had significantly more impact than the diacritic penalty. WER Language ady gre i"
2021.sigmorphon-1.15,P10-1080,0,0.2993,"subtasks is referred to Ashby et al. (this volume) for an overview. Previous Work on G2P conversion The techniques for performing G2P conversion have long been coupled with contemporary machine learning advances. Early paradigms utilize joint sequence models that rely on the alignment between grapheme and phoneme, usually with variants of the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). The resulting sequences of graphones (i.e., joint graphemephoneme tokens) are then modeled with n-gram models or Hidden Markov Models (e.g., Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn and Kondrak, 2010). A variant of this paradigm includes weighted finite-state transducers trained on such graphone sequences (Novak et al., 2012, 2015). With the rise of various neural network techniques, neural-based methods have dominated the scene ever since. For example, bidirectional long short-term memory-based (LSTM) networks using a connectionist temporal classification layer produce comparable results to earlier n-gram models (Rao et al., 2015). By incorporating alignment information into the model, the ceiling set by n-gram 131 Proceedings of the Seventeenth SIGMORPHON Workshop on Computational Resear"
2021.sigmorphon-1.15,N07-1047,0,0.546188,"tion 3.1; the reader interested in the other two subtasks is referred to Ashby et al. (this volume) for an overview. Previous Work on G2P conversion The techniques for performing G2P conversion have long been coupled with contemporary machine learning advances. Early paradigms utilize joint sequence models that rely on the alignment between grapheme and phoneme, usually with variants of the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). The resulting sequences of graphones (i.e., joint graphemephoneme tokens) are then modeled with n-gram models or Hidden Markov Models (e.g., Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn and Kondrak, 2010). A variant of this paradigm includes weighted finite-state transducers trained on such graphone sequences (Novak et al., 2012, 2015). With the rise of various neural network techniques, neural-based methods have dominated the scene ever since. For example, bidirectional long short-term memory-based (LSTM) networks using a connectionist temporal classification layer produce comparable results to earlier n-gram models (Rao et al., 2015). By incorporating alignment information into the model, the ceiling set by n-gram 131 Proceedings of the"
2021.sigmorphon-1.15,2020.lrec-1.521,0,0.260446,"Missing"
2021.sigmorphon-1.15,D18-1314,0,0.173432,"ani et al., 2017), transformer-based architectures have begun looming large (e.g., Yolchuyeva et al., 2019). Recent years have also seen works that capitalize on multilingual data to train a single model with grapheme-phoneme pairs from multiple languages. For example, various systems from last year’s shared task submissions learned from a multilingual signal (e.g., ElSaadany and Suter, 2020; Peters and Martins, 2020; Vesik et al., 2020). 3.3 Baselines The official baselines for individual languages are based on an ensembled neural transducer trained with the imitation learning (IL) paradigm (Makarov and Clematide, 2018a). The baseline WERs are tabulated in Table 3. In what follows, we overview this baseline neural-transducer system, as our models are built on top of this system. The detailed formal description of the baseline system can be found in Makarov and Clematide (2018a,b,c, 2020). The neural transducer in question defines a conditional distribution over edit actions, such as copy, deletion, insertion, and substitution: pθ (y, a|x) = 3 The Low-resource Subtask pθ (aj |a<j , x), j=1 This section provides relevant information concerning the low-resource subtask. 3.1 Task Data The provided data in the l"
2021.sigmorphon-1.15,C18-1008,0,0.0124713,"ani et al., 2017), transformer-based architectures have begun looming large (e.g., Yolchuyeva et al., 2019). Recent years have also seen works that capitalize on multilingual data to train a single model with grapheme-phoneme pairs from multiple languages. For example, various systems from last year’s shared task submissions learned from a multilingual signal (e.g., ElSaadany and Suter, 2020; Peters and Martins, 2020; Vesik et al., 2020). 3.3 Baselines The official baselines for individual languages are based on an ensembled neural transducer trained with the imitation learning (IL) paradigm (Makarov and Clematide, 2018a). The baseline WERs are tabulated in Table 3. In what follows, we overview this baseline neural-transducer system, as our models are built on top of this system. The detailed formal description of the baseline system can be found in Makarov and Clematide (2018a,b,c, 2020). The neural transducer in question defines a conditional distribution over edit actions, such as copy, deletion, insertion, and substitution: pθ (y, a|x) = 3 The Low-resource Subtask pθ (aj |a<j , x), j=1 This section provides relevant information concerning the low-resource subtask. 3.1 Task Data The provided data in the l"
2021.sigmorphon-1.15,W12-6208,0,0.0265552,"rsion have long been coupled with contemporary machine learning advances. Early paradigms utilize joint sequence models that rely on the alignment between grapheme and phoneme, usually with variants of the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). The resulting sequences of graphones (i.e., joint graphemephoneme tokens) are then modeled with n-gram models or Hidden Markov Models (e.g., Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn and Kondrak, 2010). A variant of this paradigm includes weighted finite-state transducers trained on such graphone sequences (Novak et al., 2012, 2015). With the rise of various neural network techniques, neural-based methods have dominated the scene ever since. For example, bidirectional long short-term memory-based (LSTM) networks using a connectionist temporal classification layer produce comparable results to earlier n-gram models (Rao et al., 2015). By incorporating alignment information into the model, the ceiling set by n-gram 131 Proceedings of the Seventeenth SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology,pages 131–140 August 5, 2021. ©2 models has since been broken (Yao and Zweig, 2015)"
2021.sigmorphon-1.15,2020.sigmorphon-1.16,0,0.278537,"roved the performance, as attentional encoder-decoders (Toshniwal and Livescu, 2016) learned to focus on specific input sequences. As attention became “all that was needed” (Vaswani et al., 2017), transformer-based architectures have begun looming large (e.g., Yolchuyeva et al., 2019). Recent years have also seen works that capitalize on multilingual data to train a single model with grapheme-phoneme pairs from multiple languages. For example, various systems from last year’s shared task submissions learned from a multilingual signal (e.g., ElSaadany and Suter, 2020; Peters and Martins, 2020; Vesik et al., 2020). 3.3 Baselines The official baselines for individual languages are based on an ensembled neural transducer trained with the imitation learning (IL) paradigm (Makarov and Clematide, 2018a). The baseline WERs are tabulated in Table 3. In what follows, we overview this baseline neural-transducer system, as our models are built on top of this system. The detailed formal description of the baseline system can be found in Makarov and Clematide (2018a,b,c, 2020). The neural transducer in question defines a conditional distribution over edit actions, such as copy, deletion, insertion, and substitutio"
2021.sigmorphon-1.21,E09-2008,0,0.328041,"he base form and morphosyntactic description of inflected word forms: a word gupdiit ‘they ate’ is annotated gup-TR-3PL. Our Gitksan analyzer is based on two core documentary resources: a wordlist spanning approximately 1250 tokens, and an 18,000 token interlinear-annotated text collection. Due to the scarcity of available lexical and corpus resources, we take a rule-based approach to modeling of morphology which is less dependent on large datasets than machine learning methods. Our analyzer is based on finite-state technology (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009b). Our work has three central goals: (1) We want to build a flexible morphological analyzer to supplement lexical and textual resources in support of language learning. Such an analyzer can support learners in identifying the base-form of inflected words where the morpheme-to-word ratio might be particularly high, in a way not addressed by a traditional dictionary. It may also productively generate inflected forms of words. (2) We want to facilitate ongoing efforts to expand the aforementioned 1250 token wordlist into a broad-coverage dictionary of the Gitksan language. Running our analyzer o"
2021.sigmorphon-1.21,L18-1416,0,0.0169016,"s in broad use today across the Gitxsan community for all diIi ii get, al’algaltgathl CVC-algal-t=gat=hl get CCNJ PL -watch-3. II = REPORT = CN people ‘And they stood by and watched,’ The analyzed corpus provides insight into the use of clitics in running speech, and is the dataset against which we test the results of the analyzer. 190 3 Related Work While considering different approaches to computational modeling of Gitksan morphology, finitestate morphology arose as a natural choice. At the present time, finite-state methods are quite widely applied for Indigenous languages of the Americas. Chen and Schwartz (2018) present a morphological analyzer for St. Lawrence Island / Central Siberian Yupik for aid in language preservation and revitalization work. Strunk (2020) present another analyzer for Central Alaskan Yupik. Snoek et al. (2014) present a morphological analyzer for Plains Cree nouns and Harrigan et al. (2017) present one for Plains Cree verbs. Littell (2018) build a finitestate analyzer for Kwak’wala. All of the above are languages which present similar challenges to the ones encountered in the case of Gitksan: word forms consisting of a large number of morphemes, both prefixing and suffixing mo"
2021.sigmorphon-1.21,N19-4021,0,0.0149726,"ic lexical resource for the Gitksan language. Littell et al. (2017) present an electronic dictionary interface Waldayu for endangered languages and apply it to Gitksan. The model is capable of performing fuzzy dictionary search which is an important extension in the presence of orthographic variation which widely occurs in Gitksan. While this represents an important development for computational lexicography for Gitksan, the method cannot model productive inflection which is important particularly for language learners who might not be able to easily deduce the base-form of an inflected word (Hunt et al., 2019). As mentioned earlier, our model can analyze inflected forms of lexemes. We extend the coverage of our finite-state analyzers by incorporating a neural morphological guesser which can be used to analyze word forms which are rejected by the finite-state analyzer. Similar mechanisms have been explored for other American Indigenous languages. Micher (2017) use segmental recurrent neural networks (Kong et al., 2015) to augment a finite-state morphological analyzer for Inuktitut.2 These jointly segment the 2 input word into morphemes and label each morpheme with one or more grammatical tags. Very"
2021.sigmorphon-1.21,W18-4803,0,0.024875,"ng different approaches to computational modeling of Gitksan morphology, finitestate morphology arose as a natural choice. At the present time, finite-state methods are quite widely applied for Indigenous languages of the Americas. Chen and Schwartz (2018) present a morphological analyzer for St. Lawrence Island / Central Siberian Yupik for aid in language preservation and revitalization work. Strunk (2020) present another analyzer for Central Alaskan Yupik. Snoek et al. (2014) present a morphological analyzer for Plains Cree nouns and Harrigan et al. (2017) present one for Plains Cree verbs. Littell (2018) build a finitestate analyzer for Kwak’wala. All of the above are languages which present similar challenges to the ones encountered in the case of Gitksan: word forms consisting of a large number of morphemes, both prefixing and suffixing morphology and morphophonological alternations. Finite-state morphology is well-suited for dealing with these challenges. It is noteworthy that similarly to Gitksan, a number of the aforementioned languages are also undergoing active documentation efforts. While we present the first morphological analyzer for Gitksan which is capable of productive inflection"
2021.sigmorphon-1.21,W17-0119,0,0.0282912,"ich present similar challenges to the ones encountered in the case of Gitksan: word forms consisting of a large number of morphemes, both prefixing and suffixing morphology and morphophonological alternations. Finite-state morphology is well-suited for dealing with these challenges. It is noteworthy that similarly to Gitksan, a number of the aforementioned languages are also undergoing active documentation efforts. While we present the first morphological analyzer for Gitksan which is capable of productive inflection, this is not the first electronic lexical resource for the Gitksan language. Littell et al. (2017) present an electronic dictionary interface Waldayu for endangered languages and apply it to Gitksan. The model is capable of performing fuzzy dictionary search which is an important extension in the presence of orthographic variation which widely occurs in Gitksan. While this represents an important development for computational lexicography for Gitksan, the method cannot model productive inflection which is important particularly for language learners who might not be able to easily deduce the base-form of an inflected word (Hunt et al., 2019). As mentioned earlier, our model can analyze inf"
2021.sigmorphon-1.21,C18-1008,0,0.0624235,"Missing"
2021.sigmorphon-1.21,W17-0114,0,0.0238523,"portant development for computational lexicography for Gitksan, the method cannot model productive inflection which is important particularly for language learners who might not be able to easily deduce the base-form of an inflected word (Hunt et al., 2019). As mentioned earlier, our model can analyze inflected forms of lexemes. We extend the coverage of our finite-state analyzers by incorporating a neural morphological guesser which can be used to analyze word forms which are rejected by the finite-state analyzer. Similar mechanisms have been explored for other American Indigenous languages. Micher (2017) use segmental recurrent neural networks (Kong et al., 2015) to augment a finite-state morphological analyzer for Inuktitut.2 These jointly segment the 2 input word into morphemes and label each morpheme with one or more grammatical tags. Very silmilarly to the approach that we adopt, Schwartz et al. (2019) and Moeller et al. (2018) use attentional LSTM encoder-decoder models to augment morphological analyzers for extending morphological analyzers for St. Lawrence Island / Central Siberian Yupik and Arapaho, respectively. 4 The Model Our morphological analyzer was designed with several conside"
2021.sigmorphon-1.21,W18-4802,0,0.0442155,"mes. We extend the coverage of our finite-state analyzers by incorporating a neural morphological guesser which can be used to analyze word forms which are rejected by the finite-state analyzer. Similar mechanisms have been explored for other American Indigenous languages. Micher (2017) use segmental recurrent neural networks (Kong et al., 2015) to augment a finite-state morphological analyzer for Inuktitut.2 These jointly segment the 2 input word into morphemes and label each morpheme with one or more grammatical tags. Very silmilarly to the approach that we adopt, Schwartz et al. (2019) and Moeller et al. (2018) use attentional LSTM encoder-decoder models to augment morphological analyzers for extending morphological analyzers for St. Lawrence Island / Central Siberian Yupik and Arapaho, respectively. 4 The Model Our morphological analyzer was designed with several considerations in mind. First, given the small amount of data at our disposal, we chose to construct a rule-based finite state transducer, built from a predefined lexicon and morphological description. The dependence of this type of analyzer on a lexicon supports one of the major goals of this project: lexical discovery from texts. Words w"
2021.sigmorphon-1.21,N19-4009,0,0.0256833,"Missing"
2021.sigmorphon-1.21,W19-6012,0,0.0201879,"yze inflected forms of lexemes. We extend the coverage of our finite-state analyzers by incorporating a neural morphological guesser which can be used to analyze word forms which are rejected by the finite-state analyzer. Similar mechanisms have been explored for other American Indigenous languages. Micher (2017) use segmental recurrent neural networks (Kong et al., 2015) to augment a finite-state morphological analyzer for Inuktitut.2 These jointly segment the 2 input word into morphemes and label each morpheme with one or more grammatical tags. Very silmilarly to the approach that we adopt, Schwartz et al. (2019) and Moeller et al. (2018) use attentional LSTM encoder-decoder models to augment morphological analyzers for extending morphological analyzers for St. Lawrence Island / Central Siberian Yupik and Arapaho, respectively. 4 The Model Our morphological analyzer was designed with several considerations in mind. First, given the small amount of data at our disposal, we chose to construct a rule-based finite state transducer, built from a predefined lexicon and morphological description. The dependence of this type of analyzer on a lexicon supports one of the major goals of this project: lexical dis"
2021.sigmorphon-1.21,W14-2205,0,0.0329881,"se of clitics in running speech, and is the dataset against which we test the results of the analyzer. 190 3 Related Work While considering different approaches to computational modeling of Gitksan morphology, finitestate morphology arose as a natural choice. At the present time, finite-state methods are quite widely applied for Indigenous languages of the Americas. Chen and Schwartz (2018) present a morphological analyzer for St. Lawrence Island / Central Siberian Yupik for aid in language preservation and revitalization work. Strunk (2020) present another analyzer for Central Alaskan Yupik. Snoek et al. (2014) present a morphological analyzer for Plains Cree nouns and Harrigan et al. (2017) present one for Plains Cree verbs. Littell (2018) build a finitestate analyzer for Kwak’wala. All of the above are languages which present similar challenges to the ones encountered in the case of Gitksan: word forms consisting of a large number of morphemes, both prefixing and suffixing morphology and morphophonological alternations. Finite-state morphology is well-suited for dealing with these challenges. It is noteworthy that similarly to Gitksan, a number of the aforementioned languages are also undergoing a"
2021.sigmorphon-1.25,K18-3001,1,0.679933,"l marker used for expressing case, familiarity, plurality, and (sometimes) gender within animate nouns. Pronouns are marked for different cases and honorificity levels. These paradigms are generated on the basis of a manually annotated corpus of Magahi folktales. We used a raw dataset from the literary domain. First, we annotated the dataset with the Universal Dependency morphological feature tags at token level using the CoNLL-U editor (Heinecke, 2019). We then converted the annotated dataset into the UniMorph schema using the script available for converting UD data into the UniMorph tagset (McCarthy et al., 2018). To finalize the data, we manually validated the dataset against the UniMorph schema (Sylak-Glassman et al., 2015a). Brajbhasha, or Braj is one of the Indo-Aryan languages spoken in the Western Indian states of Uttar Pradesh, Madhya Pradesh, and Rajasthan. Grierson (1908) groups Brajbhasha under Western Hindi of the Central Group in the Indo-Aryan family, along with other languages like Hindustani, Bangaru, Kannauji, and Bundeli. Braj is not generally used in education or for any official purposes in any Braj spoken state, but it has a very rich literary tradition. Also in order to preserve,"
2021.sigmorphon-1.25,K17-2001,1,0.858355,"Missing"
2021.sigmorphon-1.25,U19-1001,1,0.893844,"nje-ng ‘1/3PL-again-wrong-BENmeat-cook-PP’ (“I cooked the wrong meat for them again”). As shown, the form has several prefixes and suffixes attached to the stem. As in other Australian languages, long vowels are typically represented by double characters, and trills with “rr”.3 According to Evans’ (2003) analysis, the verb template contains 12 affix slots which include two incorporated noun classes, and derivational affixes such as the benefactive and comitative. The data included in this set are verbs extracted from the Kunwinjku translation of the Bible using the morphological analyzer from Lane and Bird (2019) and manually verified by human annotators. 3.2 Afro-Asiatic The Afro-Asiatic language family is represented by the Semitic subgroup. 3.2.1 Semitic: Classical Syriac Classical Syriac is a dialect of the Aramaic language and is attested as early as the 1st century CE. As with most Semitic languages, it displays non-concatenative morphology involving primarily tri-consonantal roots. Syriac nouns and adjectives are conventionally classified into three ‘states’— Emphatic, Absolute, Construct—which loosely correlate with the syntactic features of definiteness, indeterminacy and the genitive. There"
2021.sigmorphon-1.25,U19-1005,1,0.782664,"respect to morphology and realized in the UniMorph schema (Sylak-Glassman et al., 2015b). Morphosyntactic features (such as “the dative case” or “the past tense”) in the UniMorph occupy an intermediate position between the descriptive categories and comparative concepts. The set of features was initially established on the basis of analysis of typological literature, and refined with the addition of new languages to the UniMorph database (Kirov et al., 2018; McCarthy et al., 2020). Since 2016, SIGMORPHON organized shared tasks on morphological reinflection (Cotterell et al., 2016, 2017, 2018; McCarthy et al., 2019; Vylomova et al., 2020) that aimed at evaluating contemporary systems. Parallel to that, they also served as a platform for enriching the UniMorph database with new languages. For instance, the 2020 shared task (Vylomova et al., 2020) featured 90 typologically diverse languages derived from various linguistic resources. This year, we are bringing many under-resourced languages (languages of Peru, Russia, India, Australia, Papua New Guinea) and dialects (e.g., for Arabic and Kurdish). The sample is highly diverse: it contains languages with templatic, concatenative (fusional and agglutinative)"
2021.sigmorphon-1.25,W02-0604,0,0.0878472,"Missing"
2021.sigmorphon-1.25,U08-1018,0,0.0643919,"through affixation, compounding, or reduplication. The four types of Indonesian affixes are prefixes, suffixes, circumfixes (combination of prefixes and suffixes), and infixes (inside the base form). Indonesian uses both full and partial reduplication processes to form words. Full reduplication is often used to express the plural forms of nouns, while partial reduplication is typically used to derive forms that might have a different category than their base forms. Unlike English, the distinction between inflectional and derivational morphological processes in Indonesian is not always clear (Pisceldo et al., 2008). In this shared task, the Indonesian data is created by bootstrapping the data from an Indonesian Wikipedia dump. Using a list of possible Indonesian affixes, we collect unique word forms from Wikipedia and analyze them using MorphInd (Larasati et al., 2011), a morphological analyzer tool for Indonesian based on an FST. We manually create a mapping between the MorphInd tagset and the UniMorph schema. We then use this mapping and apply some additional rule-based formulas created by Indonesian linguists to build the final dataset (Table 9). 3.9.2 Malayo-Polynesian: Kodi/Kodhi Kodi or Kodhi [koâ"
2021.sigmorphon-1.25,N19-1119,0,0.0213266,"ugmentation technique presented by Anastasopoulos and Neubig (2019). More specifically, the team implemented an encoder–decoder model with an attention mechanism. The encoder processes a character sequence using an LSTM-based RNN with attention. Tags are encoded with a selfattention (Vaswani et al., 2017) position-invariant module. The decoder is an LSTM with separate attention mechanisms for the lemma and the tags. GUClasp focus their efforts on exploring strategies for training a multilingual model, in particular, they implement the following strategies: curriculum learning with competence (Platanios et al., 2019) based on character frequency and L BME GUClasp afb amh ara arz heb syc ame cni ind kod aym ckt itl gup bra bul ces ckb deu kmr mag nld pol por rus spa see ail evn sah tyv krl lud olo vep 92.39 98.16 99.76 95.27 97.46 21.71 82.46 99.5 81.31 94.62 99.98 44.74 32.4 14.75 58.52 98.9 98.03 99.46 97.98 98.21 70.2 98.28 99.54 99.85 98.07 99.82 78.28 6.84 51.9 99.95 99.97 99.88 59.46 99.72 99.72 81.71 93.81 94.86 87.12 89.93 10.57 55.94 93.36 55.68 87.1 99.97 52.63 31.28 21.31 56.91 96.46 94.00 96.60 91.94 98.09 72.24 94.91 98.52 99.11 94.32 97.65 40.97 6.46 51.5 99.69 99.78 98.50 59.46 98.2 97.05 sj"
2021.sigmorphon-1.25,2020.acl-main.597,1,0.915066,"arget inflected form—and removed all forms other than verbs, nouns, or adjectives. We then capped the dataset sizes to a maximum of 100,000 instances per language, subsampling when necessary. Finally, we create a 70–10–20 train–dev–test split per language, splitting the data across these sets at the instance level (as opposed to, e.g., the lemma one). As such, the information about a lemma’s declension or inflection class is spread out across these train, dev and test sets, making this task much simpler than if one had to predict the entire class from the lemma’s form alone, as done by, e.g., Williams et al. (2020) and Liu and Hulden (2021). 5 Baseline Systems The organizers provide four neural systems as baselines, a product of two models and optional data augmentation. The first model is a transformer (Vaswani et al., 2017, TRM), and the second model is an adaption of the transformer to character-level transduction tasks (Wu et al., 2021, CHR-TRM), which holds the state-of-the-art on the 2017 SIGMORPHON shared task data. Both models follow the hyperparameters of Wu et al. (2021). The optional data augmentation follows the technique proposed by Anastasopoulos and Neubig (2019). Rely14 The new languages"
2021.sigmorphon-1.8,K17-2001,1,0.900551,"Missing"
2021.sigmorphon-1.8,W16-2002,1,0.895884,"Missing"
2021.sigmorphon-1.8,N13-1138,0,0.0713896,"Missing"
2021.sigmorphon-1.8,2020.acl-main.695,1,0.883705,"Missing"
2021.sigmorphon-1.8,2021.sigmorphon-1.12,1,0.8311,"Missing"
2021.sigmorphon-1.8,Q17-1010,0,0.106213,"Missing"
2021.sigmorphon-1.8,2020.acl-main.598,1,0.881267,"Missing"
2021.sigmorphon-1.8,K18-3001,1,0.886365,"Missing"
2021.sigmorphon-1.8,2020.lrec-1.497,0,0.0616438,"Missing"
2021.sigmorphon-1.8,2020.sigmorphon-1.3,1,0.815511,"Missing"
2021.sigmorphon-1.8,2021.sigmorphon-1.10,0,0.061232,"Missing"
2021.sigmorphon-1.8,2020.acl-demos.14,0,0.0404485,"Missing"
2021.sigmorphon-1.8,W19-4226,1,0.902221,"Missing"
2021.sigmorphon-1.8,2021.sigmorphon-1.11,1,0.696729,"Missing"
2021.sigmorphon-1.8,2020.lrec-1.352,1,0.814695,"Missing"
2021.sigmorphon-1.8,2021.sigmorphon-1.9,0,0.0885046,"Missing"
E17-2034,N07-1047,1,0.798716,"rd-form l¨ufte. 2.1 c c c c c c We say that a lemma+tag analysis generated from a word-form satisfies the affix-match constraint if and only if the resulting affix-tag pair occurs in the alignment of the training data. Table 2 shows the alignments of five possible analyses to the corresponding word-form schreibet, of which three satisfy the affix-match constraint. Only analysis #2 (in bold) is correct. Alignment For the training of the string transduction models, we need aligned source-target pairs. Monotonic alignments are inferred with a modified version of the M2M (many-to-many) aligner of Jiampojamarn et al. (2007), which maximizes the joint likelihood of the aligned source and target substring pairs using the Expectation-Maximization algorithm. A transduction from a word-form which happens to be shorter than its lemma (e.g. l¨ufte/l¨uften) could be achieved by including an insertion operation (e.g.  → n). However, in order to avoid a prohibitively expensive transduction model, we model insertion as a many-to-many alignment, which bounds the transduction operation to its context. We modify the M2M aligner by allowing the alignment to learn the likelihood of a generalized identity alignment (i.e., i → i"
E17-2034,N10-1103,1,0.83254,"t, which is typically aligned to a substring in the word-form that involves the corresponding affix.2 We allow the maximum length of the alignment substring to be longer for the tag alignment than for the individual characters in the lemma. After aligning the training data we record all substring alignments that involve affixes and tags. At test time, the source-target alignment is implied by the substring transduction sequence. 2.2 Transduction We train transduction models for transforming the word-forms into analyses on the aligned sourcetarget pairs using a modified version of D I REC TL+ (Jiampojamarn et al., 2010). D IREC TL+ is a feature-rich, discriminative character transducer, which searches for a model-optimal sequence of character transformation rules for its input. The core of the engine is a dynamic programming algorithm capable of transducing many consecutive characters in a single operation, also known as a semi-Markov model. Using a structured version of the MIRA algorithm (McDonald et al., 2005), training attempts to assign weights to each feature so that its linear model separates the gold-standard derivation from all others in its search space. D IREC TL+ uses a number of feature template"
E17-2034,P05-1012,0,0.108293,"ring transduction sequence. 2.2 Transduction We train transduction models for transforming the word-forms into analyses on the aligned sourcetarget pairs using a modified version of D I REC TL+ (Jiampojamarn et al., 2010). D IREC TL+ is a feature-rich, discriminative character transducer, which searches for a model-optimal sequence of character transformation rules for its input. The core of the engine is a dynamic programming algorithm capable of transducing many consecutive characters in a single operation, also known as a semi-Markov model. Using a structured version of the MIRA algorithm (McDonald et al., 2005), training attempts to assign weights to each feature so that its linear model separates the gold-standard derivation from all others in its search space. D IREC TL+ uses a number of feature templates to assess the quality of a rule: source context, target n-gram, and joint n-gram features. Context features conjoin the rule with indicators for all source character n-grams within a fixed window of where the rule is being applied. Target n-grams provide indicators on target character sequences, describing the shape of the target as it is being produced, and may also be conjoined with our source"
E17-2034,D13-1032,0,0.0672395,"Missing"
E17-2034,D15-1272,0,0.038454,"Missing"
E17-2034,W15-1844,0,0.0627562,"Missing"
E17-2034,chrupala-etal-2008-learning,0,0.0935092,"Missing"
E17-2034,P09-1055,0,0.0361397,"present the word-forms of the languages that we consider in this paper. The only exception is the circumfix of the German past participle. 212 Source schreiben + 2PKA schreiben + 2PKE schreiben + 3SIA schrieben + 2PKE schreiben + 2PIA Target schriebet schreibet schrieb schriebet schriebt × X × × × 1 2 3 4 5 6 7 8 9 Table 3: Example source-target pairs of the inflector model. The check marks indicate which of the analyses of the German word-form schreibet satisfy the mirror constraint. Type binary real real binary binary binary binary binary binary Table 4: Features of the re-ranker. Following Toutanova and Cherry (2009), we modify the out-of-the-box version of D IREC TL+ by augmenting it with an abstract copy feature that indicates when a rule simply copies its source characters into the target, e.g. b → b. The copy feature has the effect of biasing the transducer towards preserving the source characters during transduction. In addition to training an analyzer model that transforms a word-form into an analysis, we also train an inflector model that converts an analysis back into a word-form. This opposite transformation corresponds to the task of morphological inflection (Cotterell et al., 2016). By deriving"
E17-2098,D13-1173,0,0.0134931,"me that the languages are sufficiently closely related to allow some translation pairs to be identified on the basis of orthographic similarity. Our setting is completely unsupervised: we extract the bilingual lexicons from non-parallel monolingual corpora representing the same domain. By contrast, most of the prior work depend on parallel data in the form of a small bitext (Genzel, 2005), a gold seed lexicon (Mikolov et al., 2013b), or document-aligned comparable corpora (Vuli´c and Moens, 2015). Other prior work assumes access to additional resources or features, such as dependency parsers (Dou and Knight, 2013; Dou et al., 2014), temporal and web-based features (Irvine and Callison-Burch, 2013), or BabelNet (Wang and Sitbon, 2014). Our approach consists of two stages: we first create a seed set of translation pairs, and then iteratively expand the lexicon with a bootstrapping 2 Methods In this section, we describe the two components of our approach: seed lexicon extraction, and lexicon expansion via bootstrapping. 2.1 Seed Lexicon Extraction Our seed extraction algorithm is aimed at identifying cross-lingual word pairs that exhibit high orthographic similarity, and have comparable frequency, both f"
E17-2098,D14-1061,0,0.0465802,"Missing"
E17-2098,H05-1110,0,0.0139885,"ctive of bilingual lexicon induction is to find translation pairs between two languages. Specifically, we aim to pair each word in the source vocabulary with its translation in the target vocabulary. In this paper, we assume that the languages are sufficiently closely related to allow some translation pairs to be identified on the basis of orthographic similarity. Our setting is completely unsupervised: we extract the bilingual lexicons from non-parallel monolingual corpora representing the same domain. By contrast, most of the prior work depend on parallel data in the form of a small bitext (Genzel, 2005), a gold seed lexicon (Mikolov et al., 2013b), or document-aligned comparable corpora (Vuli´c and Moens, 2015). Other prior work assumes access to additional resources or features, such as dependency parsers (Dou and Knight, 2013; Dou et al., 2014), temporal and web-based features (Irvine and Callison-Burch, 2013), or BabelNet (Wang and Sitbon, 2014). Our approach consists of two stages: we first create a seed set of translation pairs, and then iteratively expand the lexicon with a bootstrapping 2 Methods In this section, we describe the two components of our approach: seed lexicon extraction,"
E17-2098,P08-1088,0,0.287045,"fy a certain number of correct translation pairs. Adding the high-confidence pairs to the seed lexicon allows us to refine the cross-lingual transformation matrix. We proceed to iteratively expand our lexicon by alternating the two steps of translation pair identification, and transformation induction. We conduct a series of experiments on English, French, and Spanish. The results demonstrate a substantial error reduction with respect to a word-vector-based method of Mikolov et al. (2013b), when using the same word vectors on six source-target pairs. We also improve on the results reported by Haghighi et al. (2008) with both automatically-extracted and gold seed lexicons. The task of unsupervised lexicon induction is to find translation pairs across monolingual corpora. We develop a novel method that creates seed lexicons by identifying cognates in the vocabularies of related languages on the basis of their frequency and lexical similarity. We apply bidirectional bootstrapping to a method which learns a linear mapping between context-based vector spaces. Experimental results on three language pairs show consistent improvement over prior work. 1 Introduction The objective of bilingual lexicon induction i"
E17-2098,W13-2233,0,0.0249413,"lation pairs to be identified on the basis of orthographic similarity. Our setting is completely unsupervised: we extract the bilingual lexicons from non-parallel monolingual corpora representing the same domain. By contrast, most of the prior work depend on parallel data in the form of a small bitext (Genzel, 2005), a gold seed lexicon (Mikolov et al., 2013b), or document-aligned comparable corpora (Vuli´c and Moens, 2015). Other prior work assumes access to additional resources or features, such as dependency parsers (Dou and Knight, 2013; Dou et al., 2014), temporal and web-based features (Irvine and Callison-Burch, 2013), or BabelNet (Wang and Sitbon, 2014). Our approach consists of two stages: we first create a seed set of translation pairs, and then iteratively expand the lexicon with a bootstrapping 2 Methods In this section, we describe the two components of our approach: seed lexicon extraction, and lexicon expansion via bootstrapping. 2.1 Seed Lexicon Extraction Our seed extraction algorithm is aimed at identifying cross-lingual word pairs that exhibit high orthographic similarity, and have comparable frequency, both factors being indicators of translations (Kondrak, 2013). For each language, represente"
E17-2098,2005.mtsummit-papers.11,0,0.212768,"Missing"
E17-2098,J03-1002,0,0.029112,"Missing"
E17-2098,C94-1027,0,0.656183,"Missing"
E17-2098,tiedemann-2012-parallel,0,0.119449,"Missing"
E17-2098,P15-2118,0,0.141656,"Missing"
E17-2098,U14-1003,0,0.0299604,"rthographic similarity. Our setting is completely unsupervised: we extract the bilingual lexicons from non-parallel monolingual corpora representing the same domain. By contrast, most of the prior work depend on parallel data in the form of a small bitext (Genzel, 2005), a gold seed lexicon (Mikolov et al., 2013b), or document-aligned comparable corpora (Vuli´c and Moens, 2015). Other prior work assumes access to additional resources or features, such as dependency parsers (Dou and Knight, 2013; Dou et al., 2014), temporal and web-based features (Irvine and Callison-Burch, 2013), or BabelNet (Wang and Sitbon, 2014). Our approach consists of two stages: we first create a seed set of translation pairs, and then iteratively expand the lexicon with a bootstrapping 2 Methods In this section, we describe the two components of our approach: seed lexicon extraction, and lexicon expansion via bootstrapping. 2.1 Seed Lexicon Extraction Our seed extraction algorithm is aimed at identifying cross-lingual word pairs that exhibit high orthographic similarity, and have comparable frequency, both factors being indicators of translations (Kondrak, 2013). For each language, represented by a raw monolingual corpus, we fir"
K17-2008,P05-1012,0,0.154648,"r each language: a discriminative transduction system, an ensemble of neural encoder-decoder mod2.1 String transduction We perform string transduction with a modified version of D IREC TL+, a tool originally designed for grapheme-to-phoneme conversion.1 D IREC TL+ is a feature-rich, discriminative character string transducer that searches for a modeloptimal sequence of character transformation rules for its input. The core of the engine is a dynamic programming algorithm capable of transducing many consecutive characters in a single operation. Using a structured version of the MIRA algorithm (McDonald et al., 2005), training attempts to assign weights to each feature so that its linear model separates the gold-standard derivation from all others in its search space. From aligned source-target pairs, our version of D IREC TL+ extracts statistically-supported feature templates: source context, target n-gram, and joint 1 https://github.com/GarrettNicolai/DTL 79 Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection, pages 79–84, c Vancouver, Canada, August 3–4, 2017. 2017 Association for Computational Linguistics n-gram features. Context features conjoin the rule with in"
K17-2008,N15-1093,1,0.882714,"n SVM reranker. The results demonstrate that our transduction approach is strongly competitive in the low-resource setting. Further gains can be obtained via tag reordering heuristics and system combination. We describe our approach and experiments in the context of the CoNLLSIGMORPHON 2017 Shared Task on Universal Morphological Reinflection. We combine a discriminative transduction system with neural models. The results on five languages show that our approach works well in the low-resource setting. We also investigate adaptations designed to handle small training sets. 1 2 Methods We follow Nicolai et al. (2015, 2016) in approaching inflection generation as discriminative string transduction. After aligning source lemmas to target word forms, conversion operations are extracted and applied to transform a lemma-tag sequence into an inflected form. In this section, we describe several novel adaptations to the lowresource setting, as well as the system combination methods. Introduction In this paper, we describe our system as participants in the CoNLL-SIGMORPHON 2017 Shared Task on Universal Morphological Reinflection (Cotterell et al., 2017). Our focus is on the sub-task of inflection generation under"
K17-2008,W16-2005,1,0.600658,"set, and select the one that results in the highest accuracy on the development set. Training instances in the inflection generation task consist of a lemma and a tag sequence which specifies the inflection slot. Tag sequences consist of smaller units, which we refer to as subtags, that determine specific aspects of the inflection. For example, the tag sequence “V;PTCP;PST;FEM;SG” indicates that the target form is a verbal (V) feminine (FEM) singular (SG) past (PST) participle (PTCP). In the small training data scenario, it is not practical to treat tag sequences as atomic units, as we did in Nicolai et al. (2016), because many tag sequences may be represented by only a single training instance, or not at all. We follow Kann and Sch¨utze (2016) in separating each tag sequence into its component subtags, in order to share information across inflection slots. Our system treats each subtag as an indivisible atomic symbol. An example is shown in Figure 1. From the linguistic point of view, tag splitting may seem counter-intuitive, as composite inflectional affixes in fusional languages can rarely be separated into individual morphemes. However, on the character level, many affixes share letter substrings a"
K17-2008,E17-2034,1,0.880038,"Missing"
K17-2008,Q16-1006,1,0.793015,"d subtags that correspond to different parts of speech (e.g., V:SG vs. N:SG). 2.3 Tag splitting Subtag reordering Because our alignment and transduction systems are monotonic, tag splitting introduces the issue of subtag ordering. The provided data files are not always consistent in terms of the relative order in which subtags appear in sequence. We enforce the consistency by establishing a global ordering of all subtags in a given language. Our objective is to make as few changes as possible with respect to the original tag sequences. We achieve this by adapting the set ordering algorithm of Hauer and Kondrak (2016), which uses a beam search to minimize the number of subtag swaps within the tag sequences. We then reorder all tag sequences that are inconsistent with the resulting ordering. Our development experiments suggest that the consistent ordering never leads to a decrease in accuracy with respect to the original ordering. We also investigate ways of optimizing the subtag order. For example, it would make sense for the gender subtag to precede the number subtag in Spanish past participles (e.g., cortadas). Since the number of possible orderings is exponential, testing a separate transduction model f"
K17-2008,P08-1103,1,0.867487,"ovel adaptations to the lowresource setting, as well as the system combination methods. Introduction In this paper, we describe our system as participants in the CoNLL-SIGMORPHON 2017 Shared Task on Universal Morphological Reinflection (Cotterell et al., 2017). Our focus is on the sub-task of inflection generation under the lowresource scenario, in which the training data is limited to 100 labeled examples, with and without monolingual corpora. Our principal approach follows Nicolai et al. (2015), performing discriminative string transduction with a modified version of the D IREC TL+ program (Jiampojamarn et al., 2008). Taking into account the results of the SIGMORPHON 2016 Shared Task on Morphological Reinflection (Cotterell et al., 2016), we investigate ways to combine the strengths of D IREC TL+ with those of neural models. In addition, we experiment with various adaptations designed to handle small training sets, such as splitting and reordering morphological tags, and synthetic training data. We derive inflection models for five languages: English, German, Persian, Polish, and Spanish. These languages display varying degrees of inflectional complexity, but are mostly suffixing, fusional languages. We c"
K17-2008,N07-1047,1,0.783989,"ators for all source character n-grams within a fixed window of where the rule is being applied. Target n-grams provide indicators on target character sequences, describing the shape of the target as it is being produced, and may also be conjoined with our source context features. Joint n-grams build indicators on rule sequences, combining source and target context, and memorizing frequently-used rule patterns. We also add an abstract copy feature that corresponds to preserving the source characters unchanged. We perform source-target pair alignment with a modified version of the M2M aligner (Jiampojamarn et al., 2007). The program applies the Expectation-Maximization algorithm with the objective to maximize the conditional likelihood of its aligned source and target pairs. In order to encourage alignments between identical characters, we modify the aligner to generalize all identity transformations into a single match operation, which corresponds to the transduction copy feature. 2.2 desperdiciar + V;COND;3;SG = desperdiciaría desperdiciar + V + COND + 3 + SG = desperdiciaría Figure 1: Splitting a tag into subtags to mitigate data sparsity. lav+e+mos, where the three substrings correspond to the stem, the"
K18-3001,K18-3001,1,0.103672,"Missing"
K18-3001,P16-2090,1,0.838493,"Missing"
K18-3001,K17-2003,1,0.836665,"Missing"
K18-3001,K17-2010,1,0.734971,"Missing"
K18-3001,W18-6011,1,0.913422,"and a target UniMorph sentence is shown in Figure 3. Since the selection of languages in task 2 is small and we do not attempt to correct annotation errors in the UD source materials, conversion between UD and UniMorph morphosyntactic descriptions is generally straightforward.11 However, UD descriptions are more fine-grained than their UniMorph equivalents. For example, UD denotes lexical features such as noun gender which are inherent features of a lexeme possessed by all of its word forms. Such inherent features are missing from UniMorph which exclusively annotates inflectional morphology (McCarthy et al., 2018). Therefore, UD fea$ → sta$ ti$ → dista$ koti$ → kodista$ i$ → ista$ oti$ → odista$ Such rules are then extracted from each example inflection in the training data. At generation time, the longest matching left hand side of a rule is identified and applied to the citation form. For example, if the Finnish noun luoti ‘bullet’ were to be inflected in the elative (N;IN+ABL;SG) using only the extracted rules given above, the transformation oti$ → odista$ would be triggered, producing the output luodista. In case there are multiple candidate rules of equally long left hand sides that all match, tie"
K18-3001,K18-3012,0,0.30177,"Missing"
K18-3001,K18-3015,0,0.276525,"Missing"
K18-3001,P15-2111,1,\N,Missing
K18-3001,K17-2002,1,\N,Missing
K18-3001,K17-3001,0,\N,Missing
K18-3001,W17-4110,1,\N,Missing
K18-3001,N18-2087,1,\N,Missing
K18-3001,P18-1245,1,\N,Missing
K18-3001,L18-1293,1,\N,Missing
K18-3001,K18-3004,0,\N,Missing
K18-3001,K18-3010,0,\N,Missing
K18-3001,K18-3013,0,\N,Missing
K18-3001,K18-3003,0,\N,Missing
K18-3001,K18-3016,0,\N,Missing
K18-3001,K18-3005,0,\N,Missing
K18-3001,W16-2006,0,\N,Missing
K18-3001,K17-2008,1,\N,Missing
K18-3001,K17-2005,0,\N,Missing
K18-3001,K18-3008,0,\N,Missing
K18-3001,K18-3007,0,\N,Missing
N15-1056,P09-1015,1,0.81394,"en et al., 1995), which includes morphological analysis of words, and the Combilex speech lexicon (Richmond et al., 2009), which contains high-quality phonemic transcriptions. After intersecting the lexicons, and pruning it of proper nouns, function words, duplicate forms, and multi-word entries, we are left with approximately 51,000 word-forms that are annotated both morphologically and phonemically. In order to segment phonemic representations into constituent morphemes, we apply a high-precision phonetic aligner (Kondrak, 2000) to link letters and phonemes using the procedure described in (Dwyer and Kondrak, 2009). In rare cases where the phonetic aligner fails to produce an alignment, we backoff to alignment generated with m2m-aligner (Jiampojamarn et al., 2007), an unsupervised EMbased algorithm. We found that this approach worked better for our purposes than relying on the alignments provided in Combilex. We use the same approach to align variant phonemic representations of morphemes as described in Section 3.1. The morphological information contained in CELEX is incomplete for our purposes, and requires further processing. For example, the word amputate is listed as monomorphemic, but in fact conta"
N15-1056,N07-1047,1,0.64397,"phonemic transcriptions. After intersecting the lexicons, and pruning it of proper nouns, function words, duplicate forms, and multi-word entries, we are left with approximately 51,000 word-forms that are annotated both morphologically and phonemically. In order to segment phonemic representations into constituent morphemes, we apply a high-precision phonetic aligner (Kondrak, 2000) to link letters and phonemes using the procedure described in (Dwyer and Kondrak, 2009). In rare cases where the phonetic aligner fails to produce an alignment, we backoff to alignment generated with m2m-aligner (Jiampojamarn et al., 2007), an unsupervised EMbased algorithm. We found that this approach worked better for our purposes than relying on the alignments provided in Combilex. We use the same approach to align variant phonemic representations of morphemes as described in Section 3.1. The morphological information contained in CELEX is incomplete for our purposes, and requires further processing. For example, the word amputate is listed as monomorphemic, but in fact contains the suffix -ate. However, amputee is analyzed as This allows us to identify the stem as amput, which in turn implies the segmentations amput·ee, amp"
N15-1056,N10-1103,1,0.827919,"tains the suffix -ate. However, amputee is analyzed as This allows us to identify the stem as amput, which in turn implies the segmentations amput·ee, amput·ate, and amput·at·ion. Another issue that requires special handling in CELEX involves recovering reduced geminate consonants. For example, the word interrelate is pronounced with a single [r] phoneme at the morpheme boundary. However, when segmenting the phoneme sequence, we need to include [r] both at the end of inter- and at the beginning of relate. Predictor The role of the predictor mentioned in Section 3.2 is performed by D IREC TL+ (Jiampojamarn et al., 2010), a publicly available discriminative string transducer. It takes as input a sequence of common morpheme representations, determined using the method described above, and produces the predicted word pronunciation. Since D IREC TL+ tends to make mistakes related to the unstressed vowel reduction phenomenon in English, we refrain from replacing the “underlying” phonemes with either [@] or [I]. An example derivation is shown in Table 3, where the Underlying string represents the input to D I REC TL+, Predicted is its output, Surface is the actual pronunciation, and Respelling is the spelling gene"
N15-1056,N06-1030,0,0.185247,"ng Science University of Alberta {nicolai,gkondrak}@ualberta.ca Abstract In spite of the apparent irregularity of the English spelling system, Chomsky and Halle (1968) characterize it as “near optimal”. We investigate this assertion using computational techniques and resources. We design an algorithm to generate word spellings that maximize both phonemic transparency and morphological consistency. Experimental results demonstrate that the constructed system is much closer to optimality than the traditional English orthography. 1 Introduction English spelling is notorious for its irregularity. Kominek and Black (2006) estimate that it is about 3 times more complex than German, and 40 times more complex than Spanish. This is confirmed by lower accuracy of letter-to-phoneme systems on English (Bisani and Ney, 2008). A survey of English spelling (Carney, 1994) devotes 120 pages to describe phoneme-to-letter correspondences, and lists 226 letter-to-phoneme rules, almost all of which admit exceptions. Numerous proposals have been put forward for spelling reforms over the years, ranging from small changes affecting a limited set of words to complete overhauls based on novel writing scripts (Venezky, 1970). In sp"
N15-1056,A00-2038,1,0.586723,"ary information from two different resources: the CELEX lexical database (Baayen et al., 1995), which includes morphological analysis of words, and the Combilex speech lexicon (Richmond et al., 2009), which contains high-quality phonemic transcriptions. After intersecting the lexicons, and pruning it of proper nouns, function words, duplicate forms, and multi-word entries, we are left with approximately 51,000 word-forms that are annotated both morphologically and phonemically. In order to segment phonemic representations into constituent morphemes, we apply a high-precision phonetic aligner (Kondrak, 2000) to link letters and phonemes using the procedure described in (Dwyer and Kondrak, 2009). In rare cases where the phonetic aligner fails to produce an alignment, we backoff to alignment generated with m2m-aligner (Jiampojamarn et al., 2007), an unsupervised EMbased algorithm. We found that this approach worked better for our purposes than relying on the alignments provided in Combilex. We use the same approach to align variant phonemic representations of morphemes as described in Section 3.1. The morphological information contained in CELEX is incomplete for our purposes, and requires further"
N15-1093,E14-1060,0,0.208948,"ing levels of morphological complexity. In each experiment we either match or improve over the state of the art reported in previous work. In addition to providing a detailed comparison of the available inflection prediction systems, we also contribute four new inflection datasets composed of Dutch and French verbs, and Czech verbs and nouns, which are made available for future research. 923 2 Inflection generation Durrett and DeNero (2013) formulate the specific task of supervised generation of inflected forms for a given base-form based on a large number of training inflection tables, while Ahlberg et al. (2014) test their alternative method on the same Wiktionary dataset. In this section, we compare their work to our approach with respect to the following three subtasks: 1. character-wise alignment of the word-forms in an inflection table (Section 2.1), 2. extraction of rules from aligned forms (2.2), 3. matching of rules to new base-forms (2.3). 2.1 Table alignment The first step in supervised paradigm learning is the alignment of related inflected forms in a table. Though technically a multiple-alignment problem, this can also be addressed by aligning each inflected form to a base-form. Durrett &"
N15-1093,P11-1004,0,0.0483563,"depending on their role in a sentence, and adjectives agree with the nouns that they modify. For such languages, many forms will not be attested even in a large corpus. However, different lemmas often exhibit the same inflectional patterns, called paradigms, which are based on phonological, semantic, or morphological criteria. The paradigm of a given lemma can be identified and used to generate unseen forms. Inflection prediction has the potential to improve Statistical Machine Translation (SMT) into morphologically complex languages. In order to address data sparsity in the training bitext, Clifton and Sarkar (2011) and Fraser et al. (2012) reduce diverse inflected forms in the target language into the corresponding base forms, or lemmas. At test time, they predict an abstract inflection tag for each translated lemma, which is then transformed into a proper word-form. Unfortunately, hand-crafted morphological generators such as the ones that they use for this purpose are available only for a small number of languages, and are expensive to create from scratch. The supervised inflection generation models that we investigate in this paper can instead be trained on publicly available inflection tables. The t"
N15-1093,D11-1057,0,0.0194676,"Missing"
N15-1093,N13-1138,0,0.573516,"n publicly available inflection tables. The task of an inflection generator is to produce an inflected form given a base-form (e.g., an infinitive) and desired inflection, which can be specified as an abstract inflectional tag. The generator is trained on a number of inflection tables, such as the one in Figure 1, which enumerate inflection forms for a given lemma. At test time, the generator predicts inflections for previously unseen base-forms. For example, given the input atmen + 1SIA, where the tag stands for “first person singular indicative preterite,” it should output atmete. Recently, Durrett and DeNero (2013) and Ahlberg 922 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 922–931, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics et al. (2014) have proposed to model inflection generation as a two-stage process: an input base-form is first matched with rules corresponding to a paradigm seen during training, which is then used to generate all inflections for that base-form simultaneously. Although their methods are quite different, both systems account for paradigm-wide regularities by creating rules that"
N15-1093,E12-1068,0,0.0159665,"sentence, and adjectives agree with the nouns that they modify. For such languages, many forms will not be attested even in a large corpus. However, different lemmas often exhibit the same inflectional patterns, called paradigms, which are based on phonological, semantic, or morphological criteria. The paradigm of a given lemma can be identified and used to generate unseen forms. Inflection prediction has the potential to improve Statistical Machine Translation (SMT) into morphologically complex languages. In order to address data sparsity in the training bitext, Clifton and Sarkar (2011) and Fraser et al. (2012) reduce diverse inflected forms in the target language into the corresponding base forms, or lemmas. At test time, they predict an abstract inflection tag for each translated lemma, which is then transformed into a proper word-form. Unfortunately, hand-crafted morphological generators such as the ones that they use for this purpose are available only for a small number of languages, and are expensive to create from scratch. The supervised inflection generation models that we investigate in this paper can instead be trained on publicly available inflection tables. The task of an inflection gene"
N15-1093,N07-1047,1,0.80485,"ed on all inflected word-forms, we derive tag-specific models for each type of inflection. Development experiments showed the general model to be slightly more accurate overall, but we use both types of models in our reranker. 3.3 String alignment D IREC TL+ training requires a set of aligned pairs of source and target strings. The alignments account for every input and output character without the use of insertion. Derivations that transform the input substrings into the desired output substrings are then extracted from the alignments. We induce the alignments by adapting the M2M aligner of (Jiampojamarn et al., 2007), which uses Expectation-Maximization to maximize the joint likelihood of its input under a pairwise alignment scheme. Previous work creates alignments based upon entire inflection tables, while ours considers each inflection paired with its base form independently. M2M goes beyond linking single characters by aligning entire substrings instead. In practice, the base-form serves as a pivot for the entire inflection table, leading to consistent multiple alignments. We modify the M2M aligner to differentiate between stems and affixes. The alignments between stem letters rarely require more than"
N15-1093,N10-1103,1,0.404007,"es into empty strings. During development, we experimented with an alternative method, in which affixes are represented by a default allomorph. Allomorphic representations have the potential advantage of reducing the complexity of transductions by the virtue of being similar to the correct form of the affix. However, we found that allomorphic affixes tend to obfuscate differences between distinct inflections, so we decided to employ abstract tags instead. 3.2 String transduction We perform string transduction adapting the tool D IREC TL+, originally designed for grapheme-tophoneme conversion (Jiampojamarn et al., 2010). D IREC TL+ is a feature-rich, discriminative character transducer, which searches for a model-optimal sequence of character transformation rules for its input. The core of the engine is a dynamic programming algorithm capable of transducing many consecutive characters in a single operation, also known as a semi-Markov model. Using a structured version of the MIRA algorithm (McDonald et al., 2005), training attempts to assign weights to each feature so that its linear model separates the gold-standard derivation from all others in its search space. D IREC TL+ uses a number of feature template"
N15-1093,P05-1012,0,0.0152706,"ctions, so we decided to employ abstract tags instead. 3.2 String transduction We perform string transduction adapting the tool D IREC TL+, originally designed for grapheme-tophoneme conversion (Jiampojamarn et al., 2010). D IREC TL+ is a feature-rich, discriminative character transducer, which searches for a model-optimal sequence of character transformation rules for its input. The core of the engine is a dynamic programming algorithm capable of transducing many consecutive characters in a single operation, also known as a semi-Markov model. Using a structured version of the MIRA algorithm (McDonald et al., 2005), training attempts to assign weights to each feature so that its linear model separates the gold-standard derivation from all others in its search space. D IREC TL+ uses a number of feature templates to assess the quality of a rule: source context, target n-gram, and joint n-gram features. Context features conjoin the rule with indicators for all source character n-grams within a fixed window of where the rule is being applied. Target n-grams provide indi925 cators on target character sequences, describing the shape of the target as it is being produced, and may also be conjoined with our sou"
N15-1093,P09-1055,1,0.358775,"he rule with indicators for all source character n-grams within a fixed window of where the rule is being applied. Target n-grams provide indi925 cators on target character sequences, describing the shape of the target as it is being produced, and may also be conjoined with our source context features. Joint n-grams build indicators on rule sequences, combining source and target context, and memorizing frequently-used rule patterns. Durrett & DeNero also use source context features, but we are the first group to account for features that consider rule sequences or target word shape. Following Toutanova and Cherry (2009), we modify the out-of-the-box version of D IREC TL+ by implementing an abstract copy feature that indicates when a rule simply copies its source characters into the target, e.g. p → p. The copy feature has the effect of biasing the transducer towards preserving the base-form within the inflected form. In addition to the general model that is trained on all inflected word-forms, we derive tag-specific models for each type of inflection. Development experiments showed the general model to be slightly more accurate overall, but we use both types of models in our reranker. 3.3 String alignment D"
N15-1093,N04-1033,0,0.0594358,"than those of our predecessors, which makes it easy to get statistical support for these additional features. Finally, since our rules are not bound by paradigm structure, we employ a reranking step to account for intra-paradigm regularities. 3 Discriminative Transduction In this section, we describe the details of our approach, including the affix representation, the string alignment and transduction, and the paradigm reranking. 3.1 Affix representation Our inflection generation engine is a discriminative semi-Markov model, similar to a monotonic phrasebased decoder from machine translation (Zens and Ney, 2004). This system cannot insert characters, except as a part of a phrasal substitution, so when inflecting a base form, we add an abstract affix representation to both provide an insertion site and to indicate the desired inflection. Abstract tags are separated from their lemmas with a single ‘+’ character. Marking the morpheme boundary in such a way allows the transducer to generalize the context of a morpheme boundary. For example, the third person singular indicative present of the verb atmen is represented as atmen+3SIE. We use readable tags throughout this paper, but they are presented to the"
P14-2138,W13-1726,0,0.0339334,"Missing"
P14-2138,W13-1727,0,0.0325771,"Missing"
P14-2138,W13-1711,0,0.0343355,"Missing"
P14-2138,W13-1728,0,0.0234327,"Missing"
P14-2138,W13-1712,0,0.033807,"Missing"
P14-2138,W13-1713,0,0.178783,"Missing"
P14-2138,W13-1719,0,0.0430528,"Missing"
P14-2138,W13-1730,0,0.112684,"Missing"
P14-2138,W13-1706,0,0.117366,"Missing"
P14-2138,W13-1714,0,0.096944,"Missing"
P14-2138,W07-0602,0,0.630036,"s by BigramScore and perform two experiments to quantify their impact on the NLI task. The results of the first experiment demonstrate that the removal of a relatively small set of discriminative words from the training data significantly impairs the accuracy of a bigram-based classifier. The results of the second experiment reveal that the most indicative bigrams are quite similar across different language sets. We conclude that character bigrams are effective in determining L1 of the author because they reflect differences in L2 word usage that are unrelated to the phonology of L1. 2 Method Tsur and Rappoport (2007) report that character bigrams are more effective for the NLI task than either unigrams or trigrams. We are interested in identifying the character bigrams that are indicative of the most discriminative words in order to quantify their impact on the bigram-based classifier. We follow both Koppel et al. (2005) and Tsur and Rappoport (2007) in using a multi-class SVM classifier for the NLI task. The classifier computes a weight for each feature coupled with each L1 language by attempting to maximize the overall accuracy on the training set. For example, if we train the classifier using words as"
P14-2138,W13-1736,0,0.0464777,"Missing"
P14-2138,W13-1732,0,0.0543384,"Missing"
P14-2138,W13-1715,0,0.0463684,"Missing"
P14-2138,W13-1734,0,0.0384191,"Missing"
P14-2138,W13-1716,0,0.102272,"Missing"
P14-2138,W13-1717,0,0.0218954,"Missing"
P14-2138,W13-1718,1,0.88466,"Missing"
P14-2138,W13-1735,0,0.0547663,"Missing"
P14-2138,W13-1710,0,\N,Missing
P14-2138,W13-1725,0,\N,Missing
P16-1108,chrupala-etal-2008-learning,0,0.314797,"Missing"
P16-1108,K15-1017,0,0.0445668,"Missing"
P16-1108,W02-0603,0,0.186419,"mmatization. 2.1 Present Imperfect Preterite Future Stemming and Segmentation Stemming is a sub-task of the larger problem of morphological segmentation. Because of the scarcity of morphologically-annotated data, many segmentation algorithms are unsupervised or rulebased. The Porter stemmer (Porter, 1980) and its derivatives, such as Snowball, apply hand-crafted context rules to strip affixes from a word. Creation of such rule-based programs requires significant effort and expert knowledge. We use structured inflection tables to create training data for a discriminative transducer. Morfessor (Creutz and Lagus, 2002) and Linguistica (Goldsmith, 2001) are unsupervised word segmenters, which divide words into regularly occurring sub-sequences by applying the minimum description length (MDL) principle. While these methods are good at identifying common morphemes, they make no distinction between stems and affixes, and thus cannot be used for stemming. Morfessor Categories-MAP (Creutz and Lagus, 2004; Creutz and Lagus, 2005) distinguishes between stems and affixes, but not between derivational and inflectional affixes. We adapt a more recent version (Gr¨onroos et al., 2014) to be used as an approximate stemme"
P16-1108,W04-0106,0,0.109265,"Missing"
P16-1108,N13-1138,0,0.0973036,"rform a precise comparison between the supervised and unsupervised systems, we extract the inflection tables from CELEX, disregarding the segmentation information. Each system is represented by a single stemming model that works on nouns, verbs, and adjectives. Due to differences in representation, the number of training instances vary slightly between models, but the number of words is constant (Table 5). In order to demonstrate that our unsupervised methods require no segmentation information, we create additional German training sets using the inflection tables extracted from Wiktionary by Durrett and DeNero (2013). The sets contain 18,912 noun forms and 43,929 verb forms. We derive separate models for verbs and nouns in order to compare the difficulty of stemming different parts of speech. The test sets for both CELEX and Wiktionary data come from CELEX, and consist of 5252, 6155, and 9817 unique forms for English, Dutch, and German, respectively. The German test set contains 2620 nouns, 3837 verbs, and 3360 adjectives. Chipmunk3 requires training data in which ev2 Morfessor is applied to the union of the training and test data. 3 http://cistern.cis.lmu.de/chipmunk EN 98.5 82.3 94.6 50.0 65.2 NL 96.0 8"
P16-1108,P09-1055,0,0.465531,"l tags in inflection tables, but segmentation and tag alignment are performed in an unsupervised way. 2.2 Lemmatization Unlike stemmers, which can be unsupervised, lemmatizers typically require annotated training data. In addition, some lemmatizers assume access to the morphological tag of the word, and/or the surrounding words in the text. Our focus is on context-free lemmatization, which could later be combined with a contextual disambiguation module. Lemmatization is often part of the morphological analysis task, which aims at annotating each word-form with its lemma and morphological tag. Toutanova and Cherry (2009) learn a joint model for contextual lemmatization and part-of-speech prediction from a morphologically annotated lexicon. Their transduction model is tightly integrated with the POS information, which makes comparison difficult. However, in Section 6, we evaluate our approach against two other fully-supervised morphological analyzers: Morfette (Chrupała et al., 2008) and Lemming (M¨uller et al., 2015). Both of these systems perform lemmatization and morphological analysis in context, but can be trained to learn non-contextual models. Morfette requires morphological tags during training, while"
P16-1108,J01-2001,0,0.0215349,"ite Future Stemming and Segmentation Stemming is a sub-task of the larger problem of morphological segmentation. Because of the scarcity of morphologically-annotated data, many segmentation algorithms are unsupervised or rulebased. The Porter stemmer (Porter, 1980) and its derivatives, such as Snowball, apply hand-crafted context rules to strip affixes from a word. Creation of such rule-based programs requires significant effort and expert knowledge. We use structured inflection tables to create training data for a discriminative transducer. Morfessor (Creutz and Lagus, 2002) and Linguistica (Goldsmith, 2001) are unsupervised word segmenters, which divide words into regularly occurring sub-sequences by applying the minimum description length (MDL) principle. While these methods are good at identifying common morphemes, they make no distinction between stems and affixes, and thus cannot be used for stemming. Morfessor Categories-MAP (Creutz and Lagus, 2004; Creutz and Lagus, 2005) distinguishes between stems and affixes, but not between derivational and inflectional affixes. We adapt a more recent version (Gr¨onroos et al., 2014) to be used as an approximate stemmer. Poon et al. (2009) abandons the"
P16-1108,C14-1111,0,0.0393432,"Missing"
P16-1108,N07-1047,1,0.849813,"M|2SIE PP|STEM|PP geb|en gab|gib|st ge|geb|en setz|en setz|te setz|t ge|setz|t tu|n tat|tu|st ge|ta|n Table 3: Stemming of the training data based on the patterns of regularity in inflectional tables. Stemmas are shown in bold. Character Alignment The training of a transduction model requires a set of aligned pairs of source and target strings. The alignment involves every input and output character; the insertion and deletion operations are disallowed. Atomic character transformations are then extracted from the alignments. We infer the alignment with a modified version of the M2M aligner of Jiampojamarn et al. (2007). The program applies the ExpectationMaximization algorithm with the objective to maximize the joint likelihood of its aligned source and target pairs. For our task, the source and target strings are nearly identical, except that the target includes stem-affix boundary markers. In order to account for every character in the target, which is usually longer than the source, we allow one-tomany alignment. This has the effect of tying the markers to the edge of a stem or affix. In order to encourage alignments between identical characters, we modify the aligner to generalize all identity transform"
P16-1108,N10-1103,1,0.876225,"alignments between identical characters, we modify the aligner to generalize all identity transformations into a single match operation. 3.2 STEM|1SIA Supervised Transduction Once we have aligned the source and target pairs, we proceed to train a word-to-stem transduction model for stemming unseen test instances. The word-to-stem model learns where to insert boundary markers. We refer to a model that is trained on annotated morphological segmentations as our supervised method. We perform string transduction by adapting D I REC TL+, a tool originally designed for graphemeto-phoneme conversion (Jiampojamarn et al., 2010). D IREC TL+ is a feature-rich, discriminative character transducer that searches for a modeloptimal sequence of character transformation rules for its input. The core of the engine is a dynamic programming algorithm capable of transducing many consecutive characters in a single operation. Using a structured version of the MIRA algorithm (McDonald et al., 2005), training attempts to assign weights to each feature so that its linear model separates the gold-standard derivation from all others in its search space. D IREC TL+ uses a number of feature templates to assess the quality of a rule: sou"
P16-1108,P05-1012,0,0.100568,"ry markers. We refer to a model that is trained on annotated morphological segmentations as our supervised method. We perform string transduction by adapting D I REC TL+, a tool originally designed for graphemeto-phoneme conversion (Jiampojamarn et al., 2010). D IREC TL+ is a feature-rich, discriminative character transducer that searches for a modeloptimal sequence of character transformation rules for its input. The core of the engine is a dynamic programming algorithm capable of transducing many consecutive characters in a single operation. Using a structured version of the MIRA algorithm (McDonald et al., 2005), training attempts to assign weights to each feature so that its linear model separates the gold-standard derivation from all others in its search space. D IREC TL+ uses a number of feature templates to assess the quality of a rule: source context, target n-gram, and joint n-gram features. Context features conjoin the rule with indicators for all source character n-grams within a fixed window of where the rule is being applied. Target n-grams provide indicators on target character sequences, describing the shape of the target as it is being produced, and may also be conjoined with our source"
P16-1108,D13-1032,0,0.0629347,"Missing"
P16-1108,D15-1272,0,0.0703143,"Missing"
P16-1108,N09-1024,0,0.0162847,"d Linguistica (Goldsmith, 2001) are unsupervised word segmenters, which divide words into regularly occurring sub-sequences by applying the minimum description length (MDL) principle. While these methods are good at identifying common morphemes, they make no distinction between stems and affixes, and thus cannot be used for stemming. Morfessor Categories-MAP (Creutz and Lagus, 2004; Creutz and Lagus, 2005) distinguishes between stems and affixes, but not between derivational and inflectional affixes. We adapt a more recent version (Gr¨onroos et al., 2014) to be used as an approximate stemmer. Poon et al. (2009) abandons the generative model of Morfessor for a log-linear model that predicts segmentations in sequence. The discriminative approach allows for the incorporation of several priors that minimize over-segmentation. Their unsupervised model outperforms Morfessor, and they are also able to report semi- and fullysupervised results. We also approach the problem using a discriminative method, but by aligning structured inflection tables, we can take advantage of linguistic knowledge, without requiring costly annotation. Ruokolainen et al. (2014) obtain further improvements by combining a structure"
P16-1108,E14-4017,0,0.0188254,"¨onroos et al., 2014) to be used as an approximate stemmer. Poon et al. (2009) abandons the generative model of Morfessor for a log-linear model that predicts segmentations in sequence. The discriminative approach allows for the incorporation of several priors that minimize over-segmentation. Their unsupervised model outperforms Morfessor, and they are also able to report semi- and fullysupervised results. We also approach the problem using a discriminative method, but by aligning structured inflection tables, we can take advantage of linguistic knowledge, without requiring costly annotation. Ruokolainen et al. (2014) obtain further improvements by combining a structured perceptron CRF with letter successor variety (LSV), and the unsupervised features of Creutz and Lagus (2004). Their system is inherently supervised, while our stem annotations are derived in an unsupervised manner. Cotterell et al. (2015) introduce Chipmunk, a Singular 1st 2nd doy das daba dabas di diste dar´e dar´as 3rd da daba dio dar´a Plural 1st damos d´abamos dimos daramos Table 2: A partial inflection table for the Spanish verb dar “to give”. fully-supervised system for labeled morphological segmentation. Extending the sequence-predi"
P16-1108,W16-2002,0,\N,Missing
P19-1172,P15-2044,0,0.198831,"Missing"
P19-1172,P16-1184,0,0.0512551,"(2015) exploit the parallel nature of the Bible to project POS tags and train taggers in the target languages, leveraging the signal from multiple languages to improve the tagger accuracy. We focus, instead, on the induction of detailed morphological categories. Soricut and Och (2015) induce morphological transformation rules in an unsupervised manner. While this is analogous to lemmatization, part of our motivation is to also produce detailed morphological features that might be useful to train lowresource taggers, or to more richly annotate morphologically sparse languages such as English. Buys and Botha (2016) train morphological taggers in morphologically rich languages from an English projection. However, their method is dependent upon an English corpus tagged with more morphologically aware tags than are typically produced by an off-the-shelf English POS tagger. We instead argue that much of this information is recoverable from syntactic and semantic parses, allowing us to use massively-parallel corpora such as the Bible. Kirov et al. (2017) notes the morphological sparsity of English, and reverses our setup, projecting morphologically rich tags from Czech into English. Rather than add another p"
P19-1172,I05-1075,0,0.717276,"and translations of religious texts. Thus, documents such as the Christian Bible are among the most translated documents in the world (Mayer and Cysouw, 2014). Furthermore, the Bible consists of short, numbered chapters and verses consisting of a small number of sentences. Although not parallel to the standard required in fields such as machine translation, the structure of the Bible means that different Bibles are approximately parallel across verses. We follow a tradition of projecting POS tags from a high-resource language onto a language with fewer available tools (Yarowsky et al., 2001; Fossum and Abney, 2005; Agi´c et al., 2015; Buys 1765 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1765–1774 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics and Botha, 2016). Our contributions, however, lie on the level of morphology and morphosyntax. With no further resources in the target language than a Bible translation and a dictionary, we project English POS tags, dependency relations, and semantic labels across the alignment. Leveraging the alignment and a collaboration of annotations, we are able to hypothesize bot"
P19-1172,P17-1044,0,0.0254566,"he data and tools that we use to label our English Bibles and generate our morphological analyses. We also outline our evaluation metrics and describe our experimental results. Our Bible data is obtained from the corpus of Mayer and Cysouw (2014), which consists of verse-parallel Bible data across 591 languages, including 27 English Bibles. The English and target Bibles are aligned using the Berkeley aligner (Liang et al., 2006), and POS tagged and syntactically parsed using the Stanford NLP toolkit (Manning et al., 2014). We semantically parse the Bibles using the Deep Semantic Role Labeler (He et al., 2017). The alignment filter is implemented using M2M aligner (Jiampojamarn et al., 2007), and our dictionaries come from PanLex (Kamholz et al., 2014); statistics concerning dictionary and training sizes are contained in the appendix. 4 For languages such as Arabic and Hebrew, where the citation form is not an attested word, we use the unmarked nominative singular form, instead. 1769 To evaluate the quality of the lexica that are produced, we extract gold validation and heldout sets from UniMorph (Kirov et al., 2018). Using the URIEL typological database (Littel et al., 2016), we limit the language"
P19-1172,N10-1103,0,0.275632,"Missing"
P19-1172,N07-1047,0,0.0475751,"r morphological analyses. We also outline our evaluation metrics and describe our experimental results. Our Bible data is obtained from the corpus of Mayer and Cysouw (2014), which consists of verse-parallel Bible data across 591 languages, including 27 English Bibles. The English and target Bibles are aligned using the Berkeley aligner (Liang et al., 2006), and POS tagged and syntactically parsed using the Stanford NLP toolkit (Manning et al., 2014). We semantically parse the Bibles using the Deep Semantic Role Labeler (He et al., 2017). The alignment filter is implemented using M2M aligner (Jiampojamarn et al., 2007), and our dictionaries come from PanLex (Kamholz et al., 2014); statistics concerning dictionary and training sizes are contained in the appendix. 4 For languages such as Arabic and Hebrew, where the citation form is not an attested word, we use the unmarked nominative singular form, instead. 1769 To evaluate the quality of the lexica that are produced, we extract gold validation and heldout sets from UniMorph (Kirov et al., 2018). Using the URIEL typological database (Littel et al., 2016), we limit the languages to those that include affixing verbal and nominal inflection, and that distinctly"
P19-1172,kamholz-etal-2014-panlex,0,0.0208061,"d describe our experimental results. Our Bible data is obtained from the corpus of Mayer and Cysouw (2014), which consists of verse-parallel Bible data across 591 languages, including 27 English Bibles. The English and target Bibles are aligned using the Berkeley aligner (Liang et al., 2006), and POS tagged and syntactically parsed using the Stanford NLP toolkit (Manning et al., 2014). We semantically parse the Bibles using the Deep Semantic Role Labeler (He et al., 2017). The alignment filter is implemented using M2M aligner (Jiampojamarn et al., 2007), and our dictionaries come from PanLex (Kamholz et al., 2014); statistics concerning dictionary and training sizes are contained in the appendix. 4 For languages such as Arabic and Hebrew, where the citation form is not an attested word, we use the unmarked nominative singular form, instead. 1769 To evaluate the quality of the lexica that are produced, we extract gold validation and heldout sets from UniMorph (Kirov et al., 2018). Using the URIEL typological database (Littel et al., 2016), we limit the languages to those that include affixing verbal and nominal inflection, and that distinctly mark plurality and temporality.5 Our evaluation set consists"
P19-1172,L18-1293,1,0.681645,"et al., 2014). We semantically parse the Bibles using the Deep Semantic Role Labeler (He et al., 2017). The alignment filter is implemented using M2M aligner (Jiampojamarn et al., 2007), and our dictionaries come from PanLex (Kamholz et al., 2014); statistics concerning dictionary and training sizes are contained in the appendix. 4 For languages such as Arabic and Hebrew, where the citation form is not an attested word, we use the unmarked nominative singular form, instead. 1769 To evaluate the quality of the lexica that are produced, we extract gold validation and heldout sets from UniMorph (Kirov et al., 2018). Using the URIEL typological database (Littel et al., 2016), we limit the languages to those that include affixing verbal and nominal inflection, and that distinctly mark plurality and temporality.5 Our evaluation set consists of 26 languages belonging to several language families such as Semitic, Germanic, Italic, Slavic, Uralic, and Bantu. For each of these languages, we randomly select a validation set of 5000 instances, and 1000 heldout instances.6 For our declension experiments, we approximate case from a majority of higher-resource morphological dictionaries, as described in Section 3.3"
P19-1172,N06-1014,0,0.090606,"d that one iteration of supplementing the training data was beneficial across our languages; subsequent iterations led to little further gain. 4 Experiments In this section, we describe the data and tools that we use to label our English Bibles and generate our morphological analyses. We also outline our evaluation metrics and describe our experimental results. Our Bible data is obtained from the corpus of Mayer and Cysouw (2014), which consists of verse-parallel Bible data across 591 languages, including 27 English Bibles. The English and target Bibles are aligned using the Berkeley aligner (Liang et al., 2006), and POS tagged and syntactically parsed using the Stanford NLP toolkit (Manning et al., 2014). We semantically parse the Bibles using the Deep Semantic Role Labeler (He et al., 2017). The alignment filter is implemented using M2M aligner (Jiampojamarn et al., 2007), and our dictionaries come from PanLex (Kamholz et al., 2014); statistics concerning dictionary and training sizes are contained in the appendix. 4 For languages such as Arabic and Hebrew, where the citation form is not an attested word, we use the unmarked nominative singular form, instead. 1769 To evaluate the quality of the lex"
P19-1172,W16-2701,0,0.0266243,"instead argue that much of this information is recoverable from syntactic and semantic parses, allowing us to use massively-parallel corpora such as the Bible. Kirov et al. (2017) notes the morphological sparsity of English, and reverses our setup, projecting morphologically rich tags from Czech into English. Rather than add another potentially noisy projection step (i.e., Czech to English to LRL), we instead leverage dependency and semantic parses to more richly tag English. In the area of contraint-based discovery, our methodology most closely resembles the constrained discovery systems of Lin et al. (2016) and particularly Upadhyay et al. (2018). Starting from a high-quality seed, a learning algorithm generalizes observed patterns, iteratively increasing the seed data with confident examples, while discarding examples that fail to pass certain heuristics. However, unlike previous work, we assume no gold seed annotations for our system - our seed is extracted exclusively from a noisy bitext word alignment. 3 Methods In this section, we describe our methods for inducing lemmas and morphological features pertaining to plurality, temporality, and case from aligned English-target Bibles. Our process"
P19-1172,C18-1008,0,0.0874434,"Missing"
P19-1172,P14-5010,0,0.00324872,"subsequent iterations led to little further gain. 4 Experiments In this section, we describe the data and tools that we use to label our English Bibles and generate our morphological analyses. We also outline our evaluation metrics and describe our experimental results. Our Bible data is obtained from the corpus of Mayer and Cysouw (2014), which consists of verse-parallel Bible data across 591 languages, including 27 English Bibles. The English and target Bibles are aligned using the Berkeley aligner (Liang et al., 2006), and POS tagged and syntactically parsed using the Stanford NLP toolkit (Manning et al., 2014). We semantically parse the Bibles using the Deep Semantic Role Labeler (He et al., 2017). The alignment filter is implemented using M2M aligner (Jiampojamarn et al., 2007), and our dictionaries come from PanLex (Kamholz et al., 2014); statistics concerning dictionary and training sizes are contained in the appendix. 4 For languages such as Arabic and Hebrew, where the citation form is not an attested word, we use the unmarked nominative singular form, instead. 1769 To evaluate the quality of the lexica that are produced, we extract gold validation and heldout sets from UniMorph (Kirov et al.,"
P19-1172,mayer-cysouw-2014-creating,0,0.134548,"otate the French words. Note that the projection is not lossless: the aligner could not find a French translation of “doubtless”, and has thus been unable to project the RB tag or advmod relation into French. Parallel corpora are rare, and even when they do exist, they often only exist between specific pairs of languages. However, the documentation of a language often begins with the creation of several important documents, including a dictionary of key terms, and translations of religious texts. Thus, documents such as the Christian Bible are among the most translated documents in the world (Mayer and Cysouw, 2014). Furthermore, the Bible consists of short, numbered chapters and verses consisting of a small number of sentences. Although not parallel to the standard required in fields such as machine translation, the structure of the Bible means that different Bibles are approximately parallel across verses. We follow a tradition of projecting POS tags from a high-resource language onto a language with fewer available tools (Yarowsky et al., 2001; Fossum and Abney, 2005; Agi´c et al., 2015; Buys 1765 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1765–1774"
P19-1172,N15-1186,0,0.062909,"al morphology, and deploy a new set of machine learning techniques to do so. Futhermore, we significantly expand the languages included in our test set, from 3 to 26 typologically diverse languages, substantially increasing the range of morphosyntactic phenomena covered and assessed. Similarly, Fossum and Abney (2005) and Agi´c et al. (2015) exploit the parallel nature of the Bible to project POS tags and train taggers in the target languages, leveraging the signal from multiple languages to improve the tagger accuracy. We focus, instead, on the induction of detailed morphological categories. Soricut and Och (2015) induce morphological transformation rules in an unsupervised manner. While this is analogous to lemmatization, part of our motivation is to also produce detailed morphological features that might be useful to train lowresource taggers, or to more richly annotate morphologically sparse languages such as English. Buys and Botha (2016) train morphological taggers in morphologically rich languages from an English projection. However, their method is dependent upon an English corpus tagged with more morphologically aware tags than are typically produced by an off-the-shelf English POS tagger. We i"
P19-1172,D18-1046,0,0.0232662,"formation is recoverable from syntactic and semantic parses, allowing us to use massively-parallel corpora such as the Bible. Kirov et al. (2017) notes the morphological sparsity of English, and reverses our setup, projecting morphologically rich tags from Czech into English. Rather than add another potentially noisy projection step (i.e., Czech to English to LRL), we instead leverage dependency and semantic parses to more richly tag English. In the area of contraint-based discovery, our methodology most closely resembles the constrained discovery systems of Lin et al. (2016) and particularly Upadhyay et al. (2018). Starting from a high-quality seed, a learning algorithm generalizes observed patterns, iteratively increasing the seed data with confident examples, while discarding examples that fail to pass certain heuristics. However, unlike previous work, we assume no gold seed annotations for our system - our seed is extracted exclusively from a noisy bitext word alignment. 3 Methods In this section, we describe our methods for inducing lemmas and morphological features pertaining to plurality, temporality, and case from aligned English-target Bibles. Our process is outlined in Figure 2. After annotati"
P19-1172,H01-1035,1,0.847298,"ictionary of key terms, and translations of religious texts. Thus, documents such as the Christian Bible are among the most translated documents in the world (Mayer and Cysouw, 2014). Furthermore, the Bible consists of short, numbered chapters and verses consisting of a small number of sentences. Although not parallel to the standard required in fields such as machine translation, the structure of the Bible means that different Bibles are approximately parallel across verses. We follow a tradition of projecting POS tags from a high-resource language onto a language with fewer available tools (Yarowsky et al., 2001; Fossum and Abney, 2005; Agi´c et al., 2015; Buys 1765 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1765–1774 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics and Botha, 2016). Our contributions, however, lie on the level of morphology and morphosyntax. With no further resources in the target language than a Bible translation and a dictionary, we project English POS tags, dependency relations, and semantic labels across the alignment. Leveraging the alignment and a collaboration of annotations, we are"
P19-1172,E17-2018,0,0.0710215,"led morphological features that might be useful to train lowresource taggers, or to more richly annotate morphologically sparse languages such as English. Buys and Botha (2016) train morphological taggers in morphologically rich languages from an English projection. However, their method is dependent upon an English corpus tagged with more morphologically aware tags than are typically produced by an off-the-shelf English POS tagger. We instead argue that much of this information is recoverable from syntactic and semantic parses, allowing us to use massively-parallel corpora such as the Bible. Kirov et al. (2017) notes the morphological sparsity of English, and reverses our setup, projecting morphologically rich tags from Czech into English. Rather than add another potentially noisy projection step (i.e., Czech to English to LRL), we instead leverage dependency and semantic parses to more richly tag English. In the area of contraint-based discovery, our methodology most closely resembles the constrained discovery systems of Lin et al. (2016) and particularly Upadhyay et al. (2018). Starting from a high-quality seed, a learning algorithm generalizes observed patterns, iteratively increasing the seed da"
W13-1718,P07-1083,1,0.940831,"-L dictionary. We use the following algorithm: 1. For each misspelled English word m found in a document, identify the most likely intended word e using a spell-checking program. 2. For each language L: (a) Look up the translation f of the intended word e in language L. (b) Compute the orthographic edit distance D between the words. (c) If D(e, f ) &lt; t then f is assumed to be a cognate of e. (d) If f is a cognate and D(m, f ) &lt; D(e, f ) then we consider it as a clue that L = L1. We use a simple method of computing orthographic distance with threshold t = 0.58 defined as the baseline method by Bergsma and Kondrak (2007). However, more accurate methods of cognate identification discussed in that paper could also be used. Misspellings can betray cognate interference even if the misspelled word has no direct cognate in language L1. For example, a Spanish speaker might spell the word quick as cuick because of the existence of numerous cognates such as question/cuesti´on. Our misspelling features can detect such phenomena at the character level; in this case, qu:cu corresponds to an individual misspelling feature. 4.7 0.3 0.4 Meta-features We included a number of document-specific metafeatures as suggested by Ber"
W13-1718,N12-1033,0,0.0480618,"Missing"
W13-1718,N07-1047,1,0.849432,"ent to distinguish the spelling characteristics of writers from 11 different languages. We extract the spelling error features from character-level alignments between the misspelled word and the intended word. For example, if the word abstract is identified as the intended spelling of a misspelling abustruct, the character alignments are as follows: a | a bu | b s | s t | t ru | ra ct | ct Only the alignments of the misspelled parts, i.e. (bu,b) and (ru,ra) in this case, are used as features. The spell-checker we use is aspell1 , and the character-level alignments are generated by m2maligner (Jiampojamarn et al., 2007). 4.6 Cognate Interference Cognates are words that share their linguistic origin. For example, English become and German bekommen have evolved from the same word in a common ancestor language. Other cognates are words that have been transfered between languages; for example, English system comes from the Greek word συστ ηµα via Latin and French. On average, pairs of cognates exhibit higher orthographic similarity than unrelated translation pairs (Kondrak, 2013). Cognate interference may cause an L1-speaker to use a cognate word instead of a correct English translation (for example, become inst"
W13-1718,P03-1054,0,0.00831794,"bigram across all training documents. Then, during feature extraction, we again determine the relative frequency of each character bigram across documents. We then use binary features to indicate if the frequency of a bigram is higher than the average frequency. Experiments conducted on the development set showed that although this modified frequency was out-performed by the original relative frequency on its own, our method performed better when further features were incorporated into the classifier. 4.3 Part-of-speech n-grams All documents are tagged with POS tags using the Stanford parser (Klein and Manning, 2003), From the documents in the training data, a list of all POS bigrams was generated, and documents were represented by binary indicators of the presence or absence of a bigram in the document. As with character bigrams, we did not simply use the most common bigrams, but rather considered all bigrams that appeared in the training data. 4.4 Syntax Production Rules After generating syntactic parse trees with the Stanford Parser. we extract all possible production rules from each document, including lexicalized rules. The features are binary; if a production rule occurs in an essay, its value is se"
W13-1718,C12-1158,0,0.0240562,"of their frequency. An early concern with token bigrams was that they were both large in number, and sparse. In an attempt to reduce the number of bigrams, we conducted experiments on the development set with different numbers of bigrams that exhibited the highest information gain. It was found that using all combinations of word bigrams improved predictive accuracy the most, and did not lead to a significant cost to the SVM. Thus, for experiments on the test set, all token bigrams that were encountered in the training set were used as features. 4.2 4 Word n-grams Character n-grams Following Tetreault et al. (2012), we utilize all character bigrams that occur in the training data, rather than only the most frequent ones. However, where the literature uses either binary indicators or relative frequency of bigrams as features, we use a modified form of the relative frequency in our classifier. In a pre-processing step, we calculate the average frequency of each character bigram across all training documents. Then, during feature extraction, we again determine the relative frequency of each character bigram across documents. We then use binary features to indicate if the frequency of a bigram is higher tha"
W13-1718,W13-1706,0,0.0629267,"ond language (L2). Speakers and writers of the same L1 can sometimes be identified by similar L2 errors. The weak Contrastive Analysis Hypothesis (Jarvis and Crossley, 2012) suggests that these errors may be a result of L1 causing linguistic interference; that is, common tendencies of a speaker’s L1 are superimposed onto their L2. Native Language Identification, or NLI, is an attempt to exploit these errors in order to identify the L1 of the speaker from texts written in L2. Our group at the University of Alberta was unfamiliar with the NLI research prior to the announcement of a shared task (Tetreault et al., 2013). However, we saw it as an opportunity to apply our expertise in character-level NLP to a new task. Our goal was to propose novel features, and to combine them with other features that have been previously shown to work well for language identification. In the end, we managed to define two feature sets that are based on spelling errors made by L2 writers. Cognate features relate a spelling mistake to cognate interference with the writer’s L1. Misspelling features identify common mistakes that may be indicative of the writer’s L1. Both feature sets are meant to exploit the Contrastive Analysis"
W13-1718,W07-0602,0,0.241271,"Missing"
W13-1718,D11-1148,0,0.0199766,"absence of a bigram in the document. As with character bigrams, we did not simply use the most common bigrams, but rather considered all bigrams that appeared in the training data. 4.4 Syntax Production Rules After generating syntactic parse trees with the Stanford Parser. we extract all possible production rules from each document, including lexicalized rules. The features are binary; if a production rule occurs in an essay, its value is set to 1, and 0 otherwise. For each language, we use information gain for feature selection to select the most informative production rules as suggested by Wong and Dras (2011). Experiments on the development set indicated that the information gain is superior to raw frequency for the purpose of syntax feature selection. Since the accuracy increased as we added more production rules, the feature set for final testing includes all production rules encountered in the training set. The majority of the rules are of the form POS ⇒ terminal. We hypothesized that most of the information contained in these rules may be already captured by the word unigram features. However, experiments on the development set suggested that the lexicalized rules contain information that is n"
W15-1518,W13-3520,0,0.0233018,"xity. In this paper, we replicate their syntactic experiments on four languages that are more morphologically complex than English: Dutch, French, German, and Spanish. 2 Replication Experiments In order to to validate our methodology, we first replicate the results of Mikolov et al. (2013b) on English syntactic analogies. 2.1 Training Corpus for Word Vectors The vectors of Mikolov et al. (2013b) were trained on 320M tokens of broadcast news data, as described by Mikolov et al. (2011). Since we have no access to this data, we instead train English vectors on a corpus from the Polyglot project (Al-Rfou et al., 2013), which contains tokenized Wikipedia dumps intended for the training of vector-space models. For comparison with the results of Mikolov et al. (2013b), we limit the data to the first 320M lowercased tokens of the corpus. 129 Proceedings of NAACL-HLT 2015, pages 129–134, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics Mikolov et al. (2013b) obtain their best results with vectors of size 1600 that combine several models, but do not elaborate how this composite model was constructed. Instead, we take as a point of reference their second-best model, which"
W15-1518,N13-1138,0,0.0331844,"gy for the other languages. 3.2 Test Sets In order to make results between multiple languages comparable, we made several changes to the construction of syntactic analogy questions. We follow the methodology of Mikolov et al. (2013b) in limiting analogy questions to the 100 most frequent verbs or nouns. The frequencies are obtained from corpora tagged by T REE TAGGER (Schmid, 1994). We identify inflections using manually constructed inflection tables from several sources. Spanish and German verbal inflections, as well as German nominal inflections, are from a Wiktionary data set introduced by Durrett and DeNero (2013).4 Dutch verbal inflections and English verbal and nominal inflections are from the CELEX database (Baayen et al., 1995). French verbal inflections are from Verbiste, an online French conjugation dictionary.5 Whereas Mikolov et al. create analogies from various inflectional forms, we require the analogies to always include the base dictionary form: the infinitive for verbs, and the nominative singular for nouns. In other words, all analogies are limited to 4 We exclude Finnish because of its high morphological complexity and the small size of the corresponding Polyglot corpus. 5 http://perso.b"
W15-1518,N13-1090,0,0.683358,"treal Road Ottawa, ON, K1A 0R6, Canada Colin.Cherry@nrc-cnrc.gc.ca Abstract We replicate the syntactic experiments of Mikolov et al. (2013b) on English, and expand them to include morphologically complex languages. We learn vector representations for Dutch, French, German, and Spanish with the W ORD 2V EC tool, and investigate to what extent inflectional information is preserved across vectors. We observe that the accuracy of vectors on a set of syntactic analogies is inversely correlated with the morphological complexity of the language. 1 Figure 1: An example of vector offsets. Introduction Mikolov et al. (2013b) demonstrate that vector representations of words obtained from a neural network language model provide a way of capturing both semantic and syntactic regularities in language. They observe that by manipulating vector offsets between pairs of words, it is possible to derive an approximation of vectors representing other words, such as queen ≈ king − man + woman. Similarly, an abstract relationship between the present and past tense may be computed by subtracting the base form eat from the past form ate; the result of composing such an offset with the base form cook may turn out to be similar"
W15-1518,D14-1113,0,0.0871772,"Missing"
W15-3911,N12-1044,1,0.914253,"− minScore) (maxScore − minScore) where minScore is the confidence score of the n-th best prediction, and maxScore is the confidence score of the best prediction. Predictions that do not occur in a specific system’s n-best predictions are also given a score of 0 for combination. n is set to 10 in all of our experiments. If an n-best list contains less than 10 predictions, minScore is set to the score of the last prediction in the list. Our development experiments indicated that this method of combination was more accurate than a simpler method that uses only the prediction ranks. 4.2 R ERANK Bhargava and Kondrak (2012) propose a reranking approach to transliteration to leverage supplemental representations, such as phonetic transcriptions and transliterations from other languages. The reranker utilizes many features, including the similarity of the candidate outputs to the supplemental 73 representations, several types of n-gram features, and the confidence scores of the base system itself. Once a feature vector is created for each output, weights are learned with an SVM reranker. Bhargava et al. (2011) apply the reranking approach (R ERANK) to system combination. The idea is to rerank the n-best list outpu"
W15-3911,W11-3206,1,0.901865,"ansliterations from other languages, in order to test whether this can improve the overall results. We obtain state-of-the-art results on most language pairs. 2 Base Systems In this section, we describe our three base systems: D IREC TL+, S EQUITUR, and SMT. 2.1 DirecTL+ D IREC TL+ is a publicly-available1 discriminative string transduction tool, which was initially developed for grapheme-to-phoneme conversion (Jiampojamarn et al., 2008). D IREC TL+ was successfully applied to transliteration in the previous NEWS shared tasks by our team (Jiampojamarn et al., 2009; Jiampojamarn et al., 2010b; Bhargava et al., 2011; Kondrak et al., 2012), as well as by other teams (Okuno, 2012; Wu et al., 2012). We make use of all features described by Jiampojamarn et al. (2010a). We perform source-target pair alignment with mpaligner (Kubo et al., 2011) because it performed slightly better in our development experiments than M 2 M-aligner (Jiampojamarn et al., 2007). The parameters of the transducer and the aligner were tuned separately for each language pair. 2.2 S EQUITUR S EQUITUR is a joint n-gram-based string transduction system2 originally designed for grapheme-to-phoneme transduction (Bisani and Ney, 2008), whic"
W15-3911,W10-2406,0,0.117958,"and the aligner were tuned separately for each language pair. 2.2 S EQUITUR S EQUITUR is a joint n-gram-based string transduction system2 originally designed for grapheme-to-phoneme transduction (Bisani and Ney, 2008), which is also applicable to a wide 1 2 72 https://code.google.com/p/directl-p http://www-i6.informatik.rwth-aachen.de/web/Software Proceedings of the Fifth Named Entity Workshop, joint with 53rd ACL and the 7th IJCNLP, pages 72–77, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics variety of monotone translation tasks including transliteration (Finch and Sumita, 2010; Nejad et al., 2011). Unlike D IREC TL+, which requires aligned source-target pairs, S EQUITUR directly trains a joint n-gram model for transduction from unaligned data. Higher order n-gram models are trained iteratively: a unigram model is created first; this model is then used to train a bigram model, which is then in turn used to train a trigram model, and so on. The order of the model trained is a parameter tuned on a development set. An important limitation of S EQUITUR is that both the source and target character sets are limited to a maximum of 255 symbols each. This precludes a direct"
W15-3911,N07-1047,1,0.854506,"ction tool, which was initially developed for grapheme-to-phoneme conversion (Jiampojamarn et al., 2008). D IREC TL+ was successfully applied to transliteration in the previous NEWS shared tasks by our team (Jiampojamarn et al., 2009; Jiampojamarn et al., 2010b; Bhargava et al., 2011; Kondrak et al., 2012), as well as by other teams (Okuno, 2012; Wu et al., 2012). We make use of all features described by Jiampojamarn et al. (2010a). We perform source-target pair alignment with mpaligner (Kubo et al., 2011) because it performed slightly better in our development experiments than M 2 M-aligner (Jiampojamarn et al., 2007). The parameters of the transducer and the aligner were tuned separately for each language pair. 2.2 S EQUITUR S EQUITUR is a joint n-gram-based string transduction system2 originally designed for grapheme-to-phoneme transduction (Bisani and Ney, 2008), which is also applicable to a wide 1 2 72 https://code.google.com/p/directl-p http://www-i6.informatik.rwth-aachen.de/web/Software Proceedings of the Fifth Named Entity Workshop, joint with 53rd ACL and the 7th IJCNLP, pages 72–77, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics variety of monotone translation"
W15-3911,P08-1103,1,0.878928,"translation toolkits (SMT). In an effort to harness the strengths of each system, we explore various techniques of combining their outputs. Furthermore, we experiment with leveraging transliterations from other languages, in order to test whether this can improve the overall results. We obtain state-of-the-art results on most language pairs. 2 Base Systems In this section, we describe our three base systems: D IREC TL+, S EQUITUR, and SMT. 2.1 DirecTL+ D IREC TL+ is a publicly-available1 discriminative string transduction tool, which was initially developed for grapheme-to-phoneme conversion (Jiampojamarn et al., 2008). D IREC TL+ was successfully applied to transliteration in the previous NEWS shared tasks by our team (Jiampojamarn et al., 2009; Jiampojamarn et al., 2010b; Bhargava et al., 2011; Kondrak et al., 2012), as well as by other teams (Okuno, 2012; Wu et al., 2012). We make use of all features described by Jiampojamarn et al. (2010a). We perform source-target pair alignment with mpaligner (Kubo et al., 2011) because it performed slightly better in our development experiments than M 2 M-aligner (Jiampojamarn et al., 2007). The parameters of the transducer and the aligner were tuned separately for e"
W15-3911,W09-3504,1,0.899505,"outputs. Furthermore, we experiment with leveraging transliterations from other languages, in order to test whether this can improve the overall results. We obtain state-of-the-art results on most language pairs. 2 Base Systems In this section, we describe our three base systems: D IREC TL+, S EQUITUR, and SMT. 2.1 DirecTL+ D IREC TL+ is a publicly-available1 discriminative string transduction tool, which was initially developed for grapheme-to-phoneme conversion (Jiampojamarn et al., 2008). D IREC TL+ was successfully applied to transliteration in the previous NEWS shared tasks by our team (Jiampojamarn et al., 2009; Jiampojamarn et al., 2010b; Bhargava et al., 2011; Kondrak et al., 2012), as well as by other teams (Okuno, 2012; Wu et al., 2012). We make use of all features described by Jiampojamarn et al. (2010a). We perform source-target pair alignment with mpaligner (Kubo et al., 2011) because it performed slightly better in our development experiments than M 2 M-aligner (Jiampojamarn et al., 2007). The parameters of the transducer and the aligner were tuned separately for each language pair. 2.2 S EQUITUR S EQUITUR is a joint n-gram-based string transduction system2 originally designed for grapheme-t"
W15-3911,N10-1103,1,0.917539,"xperiment with leveraging transliterations from other languages, in order to test whether this can improve the overall results. We obtain state-of-the-art results on most language pairs. 2 Base Systems In this section, we describe our three base systems: D IREC TL+, S EQUITUR, and SMT. 2.1 DirecTL+ D IREC TL+ is a publicly-available1 discriminative string transduction tool, which was initially developed for grapheme-to-phoneme conversion (Jiampojamarn et al., 2008). D IREC TL+ was successfully applied to transliteration in the previous NEWS shared tasks by our team (Jiampojamarn et al., 2009; Jiampojamarn et al., 2010b; Bhargava et al., 2011; Kondrak et al., 2012), as well as by other teams (Okuno, 2012; Wu et al., 2012). We make use of all features described by Jiampojamarn et al. (2010a). We perform source-target pair alignment with mpaligner (Kubo et al., 2011) because it performed slightly better in our development experiments than M 2 M-aligner (Jiampojamarn et al., 2007). The parameters of the transducer and the aligner were tuned separately for each language pair. 2.2 S EQUITUR S EQUITUR is a joint n-gram-based string transduction system2 originally designed for grapheme-to-phoneme transduction (Bis"
W15-3911,W10-2405,1,0.873324,"xperiment with leveraging transliterations from other languages, in order to test whether this can improve the overall results. We obtain state-of-the-art results on most language pairs. 2 Base Systems In this section, we describe our three base systems: D IREC TL+, S EQUITUR, and SMT. 2.1 DirecTL+ D IREC TL+ is a publicly-available1 discriminative string transduction tool, which was initially developed for grapheme-to-phoneme conversion (Jiampojamarn et al., 2008). D IREC TL+ was successfully applied to transliteration in the previous NEWS shared tasks by our team (Jiampojamarn et al., 2009; Jiampojamarn et al., 2010b; Bhargava et al., 2011; Kondrak et al., 2012), as well as by other teams (Okuno, 2012; Wu et al., 2012). We make use of all features described by Jiampojamarn et al. (2010a). We perform source-target pair alignment with mpaligner (Kubo et al., 2011) because it performed slightly better in our development experiments than M 2 M-aligner (Jiampojamarn et al., 2007). The parameters of the transducer and the aligner were tuned separately for each language pair. 2.2 S EQUITUR S EQUITUR is a joint n-gram-based string transduction system2 originally designed for grapheme-to-phoneme transduction (Bis"
W15-3911,P07-2045,0,0.0218725,"meter tuned on a development set. An important limitation of S EQUITUR is that both the source and target character sets are limited to a maximum of 255 symbols each. This precludes a direct application of S EQUITUR to scripts such as Chinese, Korean, and Japanese Kanji. Ultimately, it was a factor in our decision to leave out the datasets that involve these languages. 2.3 SMT We frame the transliteration task as a machine translation task by treating individual characters as words, and sequences of characters as phrases. We align the word pairs with GIZA++ (Och and Ney, 2003), and use Moses (Koehn et al., 2007), a phrase-based SMT system, to generate transliterations. The decoder’s log-linear model includes a standard feature set. Four translation model features encode phrase translation probabilities and lexical scores in both directions. Both alignment and generation are monotonic, i.e. reordering is disabled, with distortion limit set to zero. We train a KN-smoothed 5-gram language model on the target side of the parallel training data with SRILM (Stolcke, 2002). If a source word is provided with several target transliterations, we select the first one. The decoder’s log-linear model is tuned wit"
W15-3911,W12-4411,1,0.935495,"er languages, in order to test whether this can improve the overall results. We obtain state-of-the-art results on most language pairs. 2 Base Systems In this section, we describe our three base systems: D IREC TL+, S EQUITUR, and SMT. 2.1 DirecTL+ D IREC TL+ is a publicly-available1 discriminative string transduction tool, which was initially developed for grapheme-to-phoneme conversion (Jiampojamarn et al., 2008). D IREC TL+ was successfully applied to transliteration in the previous NEWS shared tasks by our team (Jiampojamarn et al., 2009; Jiampojamarn et al., 2010b; Bhargava et al., 2011; Kondrak et al., 2012), as well as by other teams (Okuno, 2012; Wu et al., 2012). We make use of all features described by Jiampojamarn et al. (2010a). We perform source-target pair alignment with mpaligner (Kubo et al., 2011) because it performed slightly better in our development experiments than M 2 M-aligner (Jiampojamarn et al., 2007). The parameters of the transducer and the aligner were tuned separately for each language pair. 2.2 S EQUITUR S EQUITUR is a joint n-gram-based string transduction system2 originally designed for grapheme-to-phoneme transduction (Bisani and Ney, 2008), which is also applicable to"
W15-3911,W11-3214,0,0.0230646,"ned separately for each language pair. 2.2 S EQUITUR S EQUITUR is a joint n-gram-based string transduction system2 originally designed for grapheme-to-phoneme transduction (Bisani and Ney, 2008), which is also applicable to a wide 1 2 72 https://code.google.com/p/directl-p http://www-i6.informatik.rwth-aachen.de/web/Software Proceedings of the Fifth Named Entity Workshop, joint with 53rd ACL and the 7th IJCNLP, pages 72–77, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics variety of monotone translation tasks including transliteration (Finch and Sumita, 2010; Nejad et al., 2011). Unlike D IREC TL+, which requires aligned source-target pairs, S EQUITUR directly trains a joint n-gram model for transduction from unaligned data. Higher order n-gram models are trained iteratively: a unigram model is created first; this model is then used to train a bigram model, which is then in turn used to train a trigram model, and so on. The order of the model trained is a parameter tuned on a development set. An important limitation of S EQUITUR is that both the source and target character sets are limited to a maximum of 255 symbols each. This precludes a direct application of S EQU"
W15-3911,J03-1002,0,0.00745369,"rder of the model trained is a parameter tuned on a development set. An important limitation of S EQUITUR is that both the source and target character sets are limited to a maximum of 255 symbols each. This precludes a direct application of S EQUITUR to scripts such as Chinese, Korean, and Japanese Kanji. Ultimately, it was a factor in our decision to leave out the datasets that involve these languages. 2.3 SMT We frame the transliteration task as a machine translation task by treating individual characters as words, and sequences of characters as phrases. We align the word pairs with GIZA++ (Och and Ney, 2003), and use Moses (Koehn et al., 2007), a phrase-based SMT system, to generate transliterations. The decoder’s log-linear model includes a standard feature set. Four translation model features encode phrase translation probabilities and lexical scores in both directions. Both alignment and generation are monotonic, i.e. reordering is disabled, with distortion limit set to zero. We train a KN-smoothed 5-gram language model on the target side of the parallel training data with SRILM (Stolcke, 2002). If a source word is provided with several target transliterations, we select the first one. The dec"
W15-3911,P03-1021,0,0.0379389,"se-based SMT system, to generate transliterations. The decoder’s log-linear model includes a standard feature set. Four translation model features encode phrase translation probabilities and lexical scores in both directions. Both alignment and generation are monotonic, i.e. reordering is disabled, with distortion limit set to zero. We train a KN-smoothed 5-gram language model on the target side of the parallel training data with SRILM (Stolcke, 2002). If a source word is provided with several target transliterations, we select the first one. The decoder’s log-linear model is tuned with MERT (Och, 2003). We use BLEU score (Papineni et al., 2002) as an evaluation metric during tuning. 3 Language-specific Preprocessing Our development experiments showed that romanization of Chinese and Japanese characters can be helpful. For the alignment of English and Chinese (EnCh) names, we convert the Chinese names in the training data into Pinyin romanization, as described in Kondrak et al. (2012). This set of training pairs is aligned using our many-tomany aligner, and the resulting alignment links are projected onto Chinese characters. In cases where alignments split individual Chinese characters, they"
W15-3911,W12-4409,0,0.0173922,"prove the overall results. We obtain state-of-the-art results on most language pairs. 2 Base Systems In this section, we describe our three base systems: D IREC TL+, S EQUITUR, and SMT. 2.1 DirecTL+ D IREC TL+ is a publicly-available1 discriminative string transduction tool, which was initially developed for grapheme-to-phoneme conversion (Jiampojamarn et al., 2008). D IREC TL+ was successfully applied to transliteration in the previous NEWS shared tasks by our team (Jiampojamarn et al., 2009; Jiampojamarn et al., 2010b; Bhargava et al., 2011; Kondrak et al., 2012), as well as by other teams (Okuno, 2012; Wu et al., 2012). We make use of all features described by Jiampojamarn et al. (2010a). We perform source-target pair alignment with mpaligner (Kubo et al., 2011) because it performed slightly better in our development experiments than M 2 M-aligner (Jiampojamarn et al., 2007). The parameters of the transducer and the aligner were tuned separately for each language pair. 2.2 S EQUITUR S EQUITUR is a joint n-gram-based string transduction system2 originally designed for grapheme-to-phoneme transduction (Bisani and Ney, 2008), which is also applicable to a wide 1 2 72 https://code.google.com/p"
W15-3911,P02-1040,0,0.0922654,"e transliterations. The decoder’s log-linear model includes a standard feature set. Four translation model features encode phrase translation probabilities and lexical scores in both directions. Both alignment and generation are monotonic, i.e. reordering is disabled, with distortion limit set to zero. We train a KN-smoothed 5-gram language model on the target side of the parallel training data with SRILM (Stolcke, 2002). If a source word is provided with several target transliterations, we select the first one. The decoder’s log-linear model is tuned with MERT (Och, 2003). We use BLEU score (Papineni et al., 2002) as an evaluation metric during tuning. 3 Language-specific Preprocessing Our development experiments showed that romanization of Chinese and Japanese characters can be helpful. For the alignment of English and Chinese (EnCh) names, we convert the Chinese names in the training data into Pinyin romanization, as described in Kondrak et al. (2012). This set of training pairs is aligned using our many-tomany aligner, and the resulting alignment links are projected onto Chinese characters. In cases where alignments split individual Chinese characters, they are expanded to include the entire charact"
W15-3911,W12-4408,0,0.0147674,"rall results. We obtain state-of-the-art results on most language pairs. 2 Base Systems In this section, we describe our three base systems: D IREC TL+, S EQUITUR, and SMT. 2.1 DirecTL+ D IREC TL+ is a publicly-available1 discriminative string transduction tool, which was initially developed for grapheme-to-phoneme conversion (Jiampojamarn et al., 2008). D IREC TL+ was successfully applied to transliteration in the previous NEWS shared tasks by our team (Jiampojamarn et al., 2009; Jiampojamarn et al., 2010b; Bhargava et al., 2011; Kondrak et al., 2012), as well as by other teams (Okuno, 2012; Wu et al., 2012). We make use of all features described by Jiampojamarn et al. (2010a). We perform source-target pair alignment with mpaligner (Kubo et al., 2011) because it performed slightly better in our development experiments than M 2 M-aligner (Jiampojamarn et al., 2007). The parameters of the transducer and the aligner were tuned separately for each language pair. 2.2 S EQUITUR S EQUITUR is a joint n-gram-based string transduction system2 originally designed for grapheme-to-phoneme transduction (Bisani and Ney, 2008), which is also applicable to a wide 1 2 72 https://code.google.com/p/directl-p http://"
W15-3911,N15-1095,1,0.918642,"L+ and S EQUITUR as the base and supplemental system, respectively. For this shared task, we investigated two modifications of R ERANK. First, we attempted to extend the original approach to take advantage of more than one supplemental system. For this purpose, we experimented with cascaded reranking, in which the n-best list is reranked using the top outputs of both supplemental systems in turn. Second, in an attempt to emulate the effectiveness of the linear combination approach, we experimented with restricting the set of features to confidence scores from the individual systems. 4.3 JOINT Yao and Kondrak (2015) propose a JOINT generation approach that can incorporate multiple transliterations as input, and show that it outperforms the reranking approach of Bhargava and Kondrak (2012). The JOINT system is a modified version of D IREC TL+ that utilizes aligned supplemental transliterations to learn additional features. Supplemental transliterations are then provided to the system at test time, in order to generate the final output. For this shared task, we performed two sets of experiments with the JOINT system. While the JOINT system was designed to incorporate additional transliterations as suppleme"
W16-2005,N07-1047,1,0.836744,"res conjoin the rule with indicators for all source character n-grams within a fixed window of where the rule is being applied. Target n-grams provide indicators on target character sequences, describing the shape of the target as it is being produced, and may also be conjoined with our source context features. Joint ngrams build indicators on rule sequences, combining source and target context, and memorizing frequently-used rule patterns. We train separate models for each part of speech in the training data. We perform source-target pair alignment with a modified version of the M2M aligner (Jiampojamarn et al., 2007). The program applies the Expectation-Maximization algorithm with the obIntroduction Many languages have complex morphology with dozens of different word-forms for any given lemma. It is often beneficial to reduce the data sparsity introduced by morphological variation in order to improve the applicability of methods that rely on textual regularity. The task of inflection generation (Task 1) is to produce an inflected form given a lemma and desired inflection, which is specified as an abstract tag. The task of labelled reinflection (Task 2) replaces the input lemma with a morphologically-tagge"
W16-2005,P08-1103,1,0.911432,"roduce an inflected form given a lemma and desired inflection, which is specified as an abstract tag. The task of labelled reinflection (Task 2) replaces the input lemma with a morphologically-tagged inflected form. Finally, the task of unlabelled reinflection (Task 3) differs from Task 2 in that the input lacks the inflection tag. In this paper, we describe our system as participants in the SIGMORPHON 2016 Shared Task on Morphological Reinflection (Cotterell et al., 2016). Our approach is based on discriminative string transduction performed with a modified version of the D IREC TL+ program (Jiampojamarn et al., 2008). We perform Task 1 using the inflection generation approach of Nicolai et al. (2015), which we refer to as the lemma-to-word model. We also derive a reverse word-to-lemma (lemmatization) model from the Task 1 data. We perform Task 3 by composing the word-to-lemma and lemma-to-word models. We reduce Task 2 to Task 3 by simply ignoring the input inflection tag. 1 https://code.google.com/p/directl-p 31 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 31–35, c Berlin, Germany, August 11, 2016. 2016 Association for Computat"
W16-2005,P05-1012,0,0.118038,"rm well on typologically diverse languages. We also discuss language-specific heuristics and errors. 1 Methods 2.1 String Transduction We perform string transduction by adapting D I REC TL+, a tool originally designed for graphemeto-phoneme conversion.1 D IREC TL+ is a featurerich, discriminative character string transducer that searches for a model-optimal sequence of character transformation rules for its input. The core of the engine is a dynamic programming algorithm capable of transducing many consecutive characters in a single operation. Using a structured version of the MIRA algorithm (McDonald et al., 2005), training attempts to assign weights to each feature so that its linear model separates the gold-standard derivation from all others in its search space. From aligned source-target pairs, D IREC TL+ extracts statistically-supported feature templates: source context, target n-gram, and joint n-gram features. Context features conjoin the rule with indicators for all source character n-grams within a fixed window of where the rule is being applied. Target n-grams provide indicators on target character sequences, describing the shape of the target as it is being produced, and may also be conjoine"
W16-2005,P16-1108,1,0.842851,"o Task 3 by simply ignoring the input inflection tag. 1 https://code.google.com/p/directl-p 31 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 31–35, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics side. In order to avoid the problem of unbounded insertions, we place a dummy null character at the boundaries of the word, effectively turning insertion into substitution. Lemmatization is not the only method of inflection simplification; we experimented with three alternative approaches (Nicolai and Kondrak, 2016): jective to maximize the joint likelihood of its aligned source and target pairs. In order to encourage alignments between identical characters, we modify the aligner to generalize all identity transformations into a single match operation. 2.2 Task 1: Inflection For Task 1, we derive a lemma-to-word model, which transforms the lemma along with an inflection tag into the inflected form. Our method models affixation with atomic morphological tags. For example, the training instance corresponding to the past participle dado of the Spanish verb dar “to give” consists of the source dar+PP and the"
W16-2005,N15-1093,1,0.752338,"stract tag. The task of labelled reinflection (Task 2) replaces the input lemma with a morphologically-tagged inflected form. Finally, the task of unlabelled reinflection (Task 3) differs from Task 2 in that the input lacks the inflection tag. In this paper, we describe our system as participants in the SIGMORPHON 2016 Shared Task on Morphological Reinflection (Cotterell et al., 2016). Our approach is based on discriminative string transduction performed with a modified version of the D IREC TL+ program (Jiampojamarn et al., 2008). We perform Task 1 using the inflection generation approach of Nicolai et al. (2015), which we refer to as the lemma-to-word model. We also derive a reverse word-to-lemma (lemmatization) model from the Task 1 data. We perform Task 3 by composing the word-to-lemma and lemma-to-word models. We reduce Task 2 to Task 3 by simply ignoring the input inflection tag. 1 https://code.google.com/p/directl-p 31 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 31–35, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics side. In order to avoid the problem of unbounded insertions, we pla"
W16-2016,N13-1072,1,0.842677,"iderations we focus here on orthographic syllabification, which is also referred to as hyphenation. Some dictionaries include hyphenation information to indicate where words may be broken for end-of-line divisions, and to assist the reader in recovering the correct pronunciation. In many languages the orthographic and phonological representations of a word are closely related. Orthographic syllabification has a number of computational applications. Incorporation of the syllable boundaries between letters benefits grapheme-to-phoneme conversion (Damper et al., 2005), and respelling generation (Hauer and Kondrak, 2013). Hyphenation of out-of-dictionary words is also important in text processing (Trogkanis and Elkan, 2010). Because of the productive nature of language, a dictionary look-up process for syllabification is inadequate. Rule-based systems are generally outperformed on out-ofdictionary words by data-driven methods, such as those of Daelemans et al. (1997), Demberg (2006), Marchand and Damper (2007), and Trogkanis and Elkan (2010). 1 We denote syllable boundaries with ‘-’, and morpheme boundaries with ‘+’. 99 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics,"
W16-2016,N10-1103,1,0.819523,", while the N tags indicate the distance from the previous boundary. For example, the word syl-lab-i-fy is annotated as: N1 N2 B N1 N2 B B N1 N2. The feature vectors consist of all n-grams around the current focus character, up to size 5. These n-grams are composed of context letters, and word-boundary markers that are added at the beginning and end of each word. While supervised methods typically require large amounts of annotated training data, they can perform segmentation of unseen (out-of-dictionary) words. As our fully-supervised segmenter, we use the discriminative string transducer of Jiampojamarn et al. (2010). The transducer is trained on aligned source-target pairs, one pair per word; the target is identical to the source except that it includes characters that represent morphological breaks. Using source and target context, the transducer learns to insert these breaks into words. 2.2 2.2.3 2.2.2 Distantly-supervised Whereas morphologically-annotated lexicons are rare, websites such as Wiktionary contain crowdgenerated inflection tables for many languages. A distantly-supervised segmenter can be trained on semi-structured inflection tables to divide words into stems and affixes without explicit s"
W16-2016,P16-1108,1,0.837915,"get pairs, one pair per word; the target is identical to the source except that it includes characters that represent morphological breaks. Using source and target context, the transducer learns to insert these breaks into words. 2.2 2.2.3 2.2.2 Distantly-supervised Whereas morphologically-annotated lexicons are rare, websites such as Wiktionary contain crowdgenerated inflection tables for many languages. A distantly-supervised segmenter can be trained on semi-structured inflection tables to divide words into stems and affixes without explicit segmentation annotation. We adopt the approach of Nicolai and Kondrak (2016), which combines unsupervised alignment with a discriminative string transduction algorithm, An important limitation of this approach is that it can only identify inflectional morpheme boundaries. Morphological information Unsupervised Unsupervised methods have the advantage of requiring no training data. We investigate the applicability of two unsupervised segmenters: Morfessor (Creutz and Lagus, 2005) and Morpheme++ (Dasgupta and Ng, 2007). Morfessor uses the minimum description length (MDL) principle to predict a word as a likely sequence of morphemes. Since the baseline version of Morfesso"
W16-2016,W13-3504,0,0.0314428,"l Segmentation Can Improve Syllabification Garrett Nicolai Lei Yao Grzegorz Kondrak Department of Computing Science University of Alberta {nicolai,lyao1,gkondrak}@ualberta.ca Abstract Morphological segmentation is the task of dividing words into morphemes, the smallest meaning-bearing units in the word (Goldsmith, 2001). For example the morpheme over occurs in words like hold+over, lay+over, and skip+over.1 Roots combine with derivational (e.g. refut+able) and inflectional affixes (e.g. hold+ing). Computational segmentation approaches can be divided into rule-based (Porter, 1980), supervised (Ruokolainen et al., 2013), semi-supervised (Gr¨onroos et al., 2014), and unsupervised (Creutz and Lagus, 2002). Bartlett et al. (2008) observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification. In this paper, we demonstrate that the accuracy of orthographic syllabification can be improved by considering morphology. We augment the syllabification approach of Bartlett et al. (2008), with features encoding morphological segmentation of words. We investigate the"
W16-2016,P08-1065,1,0.908689,"nce University of Alberta {nicolai,lyao1,gkondrak}@ualberta.ca Abstract Morphological segmentation is the task of dividing words into morphemes, the smallest meaning-bearing units in the word (Goldsmith, 2001). For example the morpheme over occurs in words like hold+over, lay+over, and skip+over.1 Roots combine with derivational (e.g. refut+able) and inflectional affixes (e.g. hold+ing). Computational segmentation approaches can be divided into rule-based (Porter, 1980), supervised (Ruokolainen et al., 2013), semi-supervised (Gr¨onroos et al., 2014), and unsupervised (Creutz and Lagus, 2002). Bartlett et al. (2008) observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification. In this paper, we demonstrate that the accuracy of orthographic syllabification can be improved by considering morphology. We augment the syllabification approach of Bartlett et al. (2008), with features encoding morphological segmentation of words. We investigate the degree of overlap between the morphological and syllable boundaries. The results of our experiments on Engli"
W16-2016,P10-1038,0,0.0214047,"e dictionaries include hyphenation information to indicate where words may be broken for end-of-line divisions, and to assist the reader in recovering the correct pronunciation. In many languages the orthographic and phonological representations of a word are closely related. Orthographic syllabification has a number of computational applications. Incorporation of the syllable boundaries between letters benefits grapheme-to-phoneme conversion (Damper et al., 2005), and respelling generation (Hauer and Kondrak, 2013). Hyphenation of out-of-dictionary words is also important in text processing (Trogkanis and Elkan, 2010). Because of the productive nature of language, a dictionary look-up process for syllabification is inadequate. Rule-based systems are generally outperformed on out-ofdictionary words by data-driven methods, such as those of Daelemans et al. (1997), Demberg (2006), Marchand and Damper (2007), and Trogkanis and Elkan (2010). 1 We denote syllable boundaries with ‘-’, and morpheme boundaries with ‘+’. 99 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 99–103, c Berlin, Germany, August 11, 2016. 2016 Association for Comput"
W16-2016,W02-0603,0,0.100796,"artment of Computing Science University of Alberta {nicolai,lyao1,gkondrak}@ualberta.ca Abstract Morphological segmentation is the task of dividing words into morphemes, the smallest meaning-bearing units in the word (Goldsmith, 2001). For example the morpheme over occurs in words like hold+over, lay+over, and skip+over.1 Roots combine with derivational (e.g. refut+able) and inflectional affixes (e.g. hold+ing). Computational segmentation approaches can be divided into rule-based (Porter, 1980), supervised (Ruokolainen et al., 2013), semi-supervised (Gr¨onroos et al., 2014), and unsupervised (Creutz and Lagus, 2002). Bartlett et al. (2008) observe that some of the errors made by their otherwise highly-accurate system, such as hol-dov-er and coad-ju-tors, can be attributed to the lack of awareness of morphological boundaries, which influence syllabification. In this paper, we demonstrate that the accuracy of orthographic syllabification can be improved by considering morphology. We augment the syllabification approach of Bartlett et al. (2008), with features encoding morphological segmentation of words. We investigate the degree of overlap between the morphological and syllable boundaries. The results of"
W16-2016,N07-1020,0,0.0302045,"trained on semi-structured inflection tables to divide words into stems and affixes without explicit segmentation annotation. We adopt the approach of Nicolai and Kondrak (2016), which combines unsupervised alignment with a discriminative string transduction algorithm, An important limitation of this approach is that it can only identify inflectional morpheme boundaries. Morphological information Unsupervised Unsupervised methods have the advantage of requiring no training data. We investigate the applicability of two unsupervised segmenters: Morfessor (Creutz and Lagus, 2005) and Morpheme++ (Dasgupta and Ng, 2007). Morfessor uses the minimum description length (MDL) principle to predict a word as a likely sequence of morphemes. Since the baseline version of Morfessor tends to over-segment rare words, we instead apply Morfessor FlatCat (Gr¨onroos et al., 2014), which reduces over-segmentation through the use of a hidden Markov model. Morpheme++ is another system that is capable of distinguishing between prefixes, suffixes, and stems by taking advantage of the regularity of affixes. We incorporate available morphological information by adding morpheme boundary markers into the input words. The extracted"
W16-2016,J01-2001,0,0.274286,"Missing"
W18-5805,P17-1183,0,0.0152042,"ms by a large margin thanks to its ability to leverage a target word list. Additional results are reported by Najafi et al. (2018b). 4.3 Average 40.7 49.0 40.9 Inflection generation Inflection generation is the task of producing an inflected word-form, given a citation form and a set of morphological features. For example, given the Spanish infinitive liberar, with the tag V;IND;FUT;2;SG, the word-form liberarás should be produced. In recent years, inflection generation has attracted much interest (Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015). Aharoni and Goldberg (2017) propose an RNN augmented with hard attention and explicit alignments for inflection, but have difficulty consistently improving upon the results of DTL, even on larger datasets. Furthermore, their system cannot be applied to tasks where the source and target are different languages, due to shared embeddings between the encoder and decoder. Ruzsics and Samardzic (2017) incorporate a language model into the decoder of 48 System EN-DE RU-PL DTL 4.3 23.5 DTL+RR 7.1 32.8 DTLM 17.7 43.9 RNN 2.2 1.7 SEQ 9.2 22.3 with the sparse corpora that such inventories supply, we see notable gains over DTL. We"
W18-5805,N15-1107,0,0.0184167,"erforms the other systems by a large margin thanks to its ability to leverage a target word list. Additional results are reported by Najafi et al. (2018b). 4.3 Average 40.7 49.0 40.9 Inflection generation Inflection generation is the task of producing an inflected word-form, given a citation form and a set of morphological features. For example, given the Spanish infinitive liberar, with the tag V;IND;FUT;2;SG, the word-form liberarás should be produced. In recent years, inflection generation has attracted much interest (Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015). Aharoni and Goldberg (2017) propose an RNN augmented with hard attention and explicit alignments for inflection, but have difficulty consistently improving upon the results of DTL, even on larger datasets. Furthermore, their system cannot be applied to tasks where the source and target are different languages, due to shared embeddings between the encoder and decoder. Ruzsics and Samardzic (2017) incorporate a language model into the decoder of 48 System EN-DE RU-PL DTL 4.3 23.5 DTL+RR 7.1 32.8 DTLM 17.7 43.9 RNN 2.2 1.7 SEQ 9.2 22.3 with the sparse corpora that such inventories supply, we se"
W18-5805,D11-1057,0,0.034818,"he same as for the low-resource experiments. Table 2 shows that DTLM outperforms the other systems by a large margin thanks to its ability to leverage a target word list. Additional results are reported by Najafi et al. (2018b). 4.3 Average 40.7 49.0 40.9 Inflection generation Inflection generation is the task of producing an inflected word-form, given a citation form and a set of morphological features. For example, given the Spanish infinitive liberar, with the tag V;IND;FUT;2;SG, the word-form liberarás should be produced. In recent years, inflection generation has attracted much interest (Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015). Aharoni and Goldberg (2017) propose an RNN augmented with hard attention and explicit alignments for inflection, but have difficulty consistently improving upon the results of DTL, even on larger datasets. Furthermore, their system cannot be applied to tasks where the source and target are different languages, due to shared embeddings between the encoder and decoder. Ruzsics and Samardzic (2017) incorporate a language model into the decoder of 48 System EN-DE RU-PL DTL 4.3 23.5 DTL+RR 7.1 32.8 DTLM 17.7 43.9 RNN 2.2 1.7 S"
W18-5805,P86-1009,0,0.700389,"rmations. The unsupervised M2M aligner (Jiampojamarn et al., 2007) employs the Expectation-Maximization (EM) algorithm with the objective of maximizing the joint likelihood of its aligned source and target pairs. The alignment involves every source and target character. The pairs of aligned substrings may contain multiple characters on both the source and target sides, yielding many-to-many (M-M) alignment links. DTL excludes insertions from its set of edit operations because they greatly increase the complexity of the generation process, to the point of making it computationally intractable (Barton, 1986). Therefore, the M2M aligner is forced to avoid nulls on the source side by incorporating them into many-to-many links during the alignment of the training data. Although many-tomany alignment models are more flexible than 1-1 models, they also generally require larger parallel datasets to produce correct alignments. In lowdata scenarios, especially when the target strings tend to be longer than the source strings, this approach often yields sub-optimal alignments (e.g., the leftmost alignment in Figure 2). 2.3 Reranking The target language modeling of DTL is limited to a set of binary ?-gram"
W18-5805,N13-1138,0,0.0258593,"source experiments. Table 2 shows that DTLM outperforms the other systems by a large margin thanks to its ability to leverage a target word list. Additional results are reported by Najafi et al. (2018b). 4.3 Average 40.7 49.0 40.9 Inflection generation Inflection generation is the task of producing an inflected word-form, given a citation form and a set of morphological features. For example, given the Spanish infinitive liberar, with the tag V;IND;FUT;2;SG, the word-form liberarás should be produced. In recent years, inflection generation has attracted much interest (Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015). Aharoni and Goldberg (2017) propose an RNN augmented with hard attention and explicit alignments for inflection, but have difficulty consistently improving upon the results of DTL, even on larger datasets. Furthermore, their system cannot be applied to tasks where the source and target are different languages, due to shared embeddings between the encoder and decoder. Ruzsics and Samardzic (2017) incorporate a language model into the decoder of 48 System EN-DE RU-PL DTL 4.3 23.5 DTL+RR 7.1 32.8 DTLM 17.7 43.9 RNN 2.2 1.7 SEQ 9.2 22.3 with the spars"
W18-5805,I13-1112,0,0.168055,"Missing"
W18-5805,N10-1103,1,0.860489,"s based on discriminative string transduction, where a learning algorithm assigns weights to features defined on aligned source and target pairs. At test time, an input sequence is converted into the highest-scoring output sequence. Advantages of discriminative transduction include an aptitude to derive effective models from small training sets, as wells as the capability to incorporate diverse sets of features. Specifically, we build Figure 1: Illustration of four character-level sequenceto-sequence prediction tasks. In each case, the output is a word in the target language. upon D IREC TL+ (Jiampojamarn et al., 2010), a string transduction tool which was originally designed for grapheme-to-phoneme conversion. We present a new system, DTLM, that combines discriminative transduction with character and word language models (LMs) derived from large unannotated corpora. Target language modeling is particularly important in low-data scenarios, where the limited transduction models often produce many ill-formed output candidates. We avoid the error propagation problem which is inherent in pipeline approaches by incorporating the LM feature sets directly into the transducer. In addition, we bolster the quality of"
W18-5805,D09-1111,0,0.034292,"original features of DTL, so that the high unigram count of piece is not sufficient to make it the top prediction on the right Corpus frequency counts We also extend DTL with a feature set that can be described as a unigram word-level language model. The objective is to bias the model towards generating output sequences that correspond to words observed in a large corpus. Since an output sequence can only be matched against a word list after the generation process is complete, we propose to estimate the final frequency count for each prefix considered during the generation process. Following Cherry and Suzuki (2009) we use a prefix trie to store partial words for reference in the generation phase. We modify their solution by also storing the count of each prefix, calculated as the sum of all of the words in which the prefix occurs. As with our language model features, unigram features are binned. A unigram feature fires if the 46 source and target ?-grams (Bisani and Ney, 2008), and a character-level neural model (RNN). The neural model uses the encoder-decoder architecture typically used for NMT (Sutskever et al., 2014). The encoder is a bi-directional RNN applied to randomly initialized character embed"
W18-5805,N07-1047,1,0.853065,"py feature generalizes the identity function from source to target, which is useful if there is an overlap between the input and output symbol sets. Baseline methods In this section, we describe the baseline methods, including the alignment of the training data, the feature sets of DirecTL+ (henceforth DTL), and reranking as a way of incorporating corpus statistics. 2.1 wɔkəz Alignment Before a transduction model can be derived from the training data, the pairs of source and target strings need to be aligned, in order to identify atomic substring transformations. The unsupervised M2M aligner (Jiampojamarn et al., 2007) employs the Expectation-Maximization (EM) algorithm with the objective of maximizing the joint likelihood of its aligned source and target pairs. The alignment involves every source and target character. The pairs of aligned substrings may contain multiple characters on both the source and target sides, yielding many-to-many (M-M) alignment links. DTL excludes insertions from its set of edit operations because they greatly increase the complexity of the generation process, to the point of making it computationally intractable (Barton, 1986). Therefore, the M2M aligner is forced to avoid nulls"
W18-5805,K18-3001,1,0.867407,"Missing"
W18-5805,K17-2001,0,0.13538,"Missing"
W18-5805,P05-1012,0,0.0464658,"thods. (5) Three new datasets for cognate projection. 2 wɔ_k_əz wɔk əz walkers walkers walkers Figure 2: Examples of different alignments in phoneme-to-letter conversion. The underscore denotes a null substring. 2.2 Features DTL is a feature-rich, discriminative character transducer, which searches for a model-optimal sequence of character transformation operations for its input. The core of the engine is a dynamic programming algorithm capable of transducing many consecutive characters in a single operation, also known as a semi-Markov model. Using a structured version of the MIRA algorithm (McDonald et al., 2005), the training process assigns weights to each feature, in order to achieve maximum separation of the gold-standard output from all others in the search space. DTL uses a number of feature templates to assess the quality of an operation: source context, target ?-gram, and joint ?-gram features. Context features conjoin the rule with indicators for all source character ?-grams within a fixed window of where the rule is being applied. Target n-grams provide indicators on target character sequences, describing the shape of the target as it is being produced, and may also be conjoined with the sou"
W18-5805,K18-3015,1,0.882215,"Missing"
W18-5805,W18-2412,1,0.442669,"Missing"
W18-5805,N15-1093,1,0.953652,"they also generally require larger parallel datasets to produce correct alignments. In lowdata scenarios, especially when the target strings tend to be longer than the source strings, this approach often yields sub-optimal alignments (e.g., the leftmost alignment in Figure 2). 2.3 Reranking The target language modeling of DTL is limited to a set of binary ?-gram features, which are based exclusively on the target sequences from the parallel training data. This shortcoming can be remedied by taking advantage of large unannotated corpora that contain thousands of examples of valid target words. Nicolai et al. (2015) propose to leverage corpus statistics by reranking the ?-best list of candidates generated by the transducer. They report consistent modest gains by applying an SVM-based 44 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: reranker, with features including a word unigram corpus presence indicator, a normalized character language model score, and the rank and normalized confidence score generated by DTL. However, such a pipeline approach suffers from error propagation, and is unable to produce output forms that are not already present in the ?-best list. In addition, training"
W18-5805,J96-3003,0,0.640465,"Missing"
W18-5805,K17-1020,0,0.021006,"e tag V;IND;FUT;2;SG, the word-form liberarás should be produced. In recent years, inflection generation has attracted much interest (Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015). Aharoni and Goldberg (2017) propose an RNN augmented with hard attention and explicit alignments for inflection, but have difficulty consistently improving upon the results of DTL, even on larger datasets. Furthermore, their system cannot be applied to tasks where the source and target are different languages, due to shared embeddings between the encoder and decoder. Ruzsics and Samardzic (2017) incorporate a language model into the decoder of 48 System EN-DE RU-PL DTL 4.3 23.5 DTL+RR 7.1 32.8 DTLM 17.7 43.9 RNN 2.2 1.7 SEQ 9.2 22.3 with the sparse corpora that such inventories supply, we see notable gains over DTL. We also see large gains for languages such as Northern Sami and Navajo that have relatively small Wikipedias (fewer than 10,000 articles). DTLM was also evaluated as a non-standard submission in the low-data track of the 2018 Shared Task on Universal Morphological Inflection (Cotterell et al., 2018). The results reported by Najafi et al. (2018a) confirm that DTLM substant"
W18-5805,P07-3005,0,\N,Missing
W18-5805,P18-1180,0,\N,Missing
W18-5805,K17-2004,0,\N,Missing
W19-4226,P17-2107,0,0.0357656,"Missing"
W19-4226,N18-1126,0,0.0330462,"gs (BERT) provided by Google (Devlin et al., 2019). CBNU1 used a mix of pre-trained embeddings from the CoNLL 2017 shared task and fastText. Further, some teams trained their own embeddings to aid performance. CUNI–Malta performs lemmatization as operations over edit actions with LSTM and ReLU. Tagging is a bidirectional LSTM augmented by the edit actions (i.e., two-stage decoding), predicting features separately. The Edinburgh system is a character-based LSTM encoder-decoder with attention, implemented in OpenNMT. It can be seen as an extension of the contextual lemmatization system Lematus (Bergmanis and Goldwater, 2018) to include morphological tagging, or alternatively as an adaptation of the morphological re-inflection system MED (Kann and Sch¨utze, 2016) to incorporate context and perform analysis rather than re-inflection. Like these systems it uses a completely generic encoderdecoder architecture with no specific adaptation to the morphological processing task other than the form of the input. In the submitted version of the system, the input is split into short chunks corresponding to the target word plus one word of context on either side, and the system is trained to output the corresponding lemmas a"
W19-4226,K18-3001,1,0.746475,"previous shared task (Cotterell et al., 2018), training a neural network on unambiguous forms to estimate the distribution over all, even ambiguous, forms. We then sampled 12,000 triples without replacement from this distribution. The first 100 were taken as training data for low-resource settings. The first 10,000 were used as high-resource training sets. As these sets are nested, the highest-count triples tend to appear in the smaller training sets.3 Data conversion The morphological annotations for the 2019 shared task were converted to the UniMorph schema (Kirov et al., 2018) according to McCarthy et al. (2018), who provide a deterministic mapping that increases agreement across languages. This also moves the part of speech into the bundle of morphological features. We do not attempt to individually correct any errors in the UD source material. Further, some languages received additional pre-processing. In the Finnish data, we removed morpheme boundaries that were present in the lemmata (e.g., puhe#kieli 7→ puhekieli ‘spoken+language’). Russian lemmata in the GSD treebank were presented in all uppercase; to match Swahili. Likewise, the low-resource language Telugu had fewer than 100 forms. 4 When su"
W19-4226,K17-2001,1,0.692718,"Missing"
W19-4226,N19-1155,1,0.843499,"ive teams participated in the first Task, with a variety of methods aimed at leveraging the crosslingual data to improve system performance. The University of Alberta (UAlberta) performed a focused investigation on four language pairs, training cognate-projection systems from external cognate lists. Two methods were considered: one which trained a high-resource neural encoderdecoder, and projected the test data into the HRL, and one that projected the HRL data into the LRL, and trained a combined system. Results demonstrated that certain language pairs may be amenable to such methods. Neural (Malaviya et al., 2019): This is a stateof-the-art neural model that also performs joint morphological tagging and lemmatization, but also accounts for the exposure bias with the application of maximum likelihood (MLE). The model stitches the tagger and lemmatizer together with the use of jackknifing (Agi´c and Schluter, 2017) to expose the lemmatizer to the errors made by the tagger model during training. The morphological tagger is based on a character-level biLSTM embedder that produces the embedding for a word, 4 HRL–LRL adyghe–kabardian albanian–breton arabic–classical-syriac arabic–maltese arabic–turkmen armen"
W19-4226,N19-1423,0,0.0153778,"only applied to a subset of languages, making scores incomparable. † indicates that additional external resources were used for training, and ‡ indicates that training data were shared across languages or treebanks. ging). Although they predict complete tags, they use the individual features to regularize the decoder. Small gains are also obtained from joining multilingual corpora and ensembling. improve their lemmatization and feature analysis. Several teams made use of pre-trained embeddings. CHARLES-SAARLAND-2 and UFALPRAGUE1 used pretrained contextual embeddings (BERT) provided by Google (Devlin et al., 2019). CBNU1 used a mix of pre-trained embeddings from the CoNLL 2017 shared task and fastText. Further, some teams trained their own embeddings to aid performance. CUNI–Malta performs lemmatization as operations over edit actions with LSTM and ReLU. Tagging is a bidirectional LSTM augmented by the edit actions (i.e., two-stage decoding), predicting features separately. The Edinburgh system is a character-based LSTM encoder-decoder with attention, implemented in OpenNMT. It can be seen as an extension of the contextual lemmatization system Lematus (Bergmanis and Goldwater, 2018) to include morpholo"
W19-4226,Q18-1032,0,0.0456659,"Missing"
W19-4226,W17-4110,0,0.124781,"Missing"
W19-4226,D15-1272,1,0.898864,"Missing"
W19-4226,W16-2010,0,0.0549216,"Missing"
W19-4226,D18-1103,0,0.0261867,"he system, the input is split into short chunks corresponding to the target word plus one word of context on either side, and the system is trained to output the corresponding lemmas and tags for each three-word chunk. 6 Future Directions In general, the application of typology to natural language processing (e.g., Gerz et al., 2018; Ponti et al., 2018) provides an interesting avenue for multilinguality. Further, our shared task was designed to only leverage a single helper language, though many may exist with lexical or morphological overlap with the target language. Techniques like those of Neubig and Hu (2018) may aid in designing universal inflection architectures. Neither task this year included unannotated monolingual corpora. Using such data is well-motivated from an L1-learning point of view, and may affect the performance of low-resource data settings. In the case of inflection an interesting future topic could involve departing from orthographic representation and using more IPA-like representations, i.e. transductions over pronunciations. DifferSeveral teams relied on external resources to 8 Table 6: Task 2 Lemma Accuracy scores 9 UD UD UD UD UD UD UD UD UD UD UD UD UD UD UD UD UD UD UD UD"
W19-4226,L18-1293,1,0.829575,"previous shared task (Cotterell et al., 2018), training a neural network on unambiguous forms to estimate the distribution over all, even ambiguous, forms. We then sampled 12,000 triples without replacement from this distribution. The first 100 were taken as training data for low-resource settings. The first 10,000 were used as high-resource training sets. As these sets are nested, the highest-count triples tend to appear in the smaller training sets.3 Data conversion The morphological annotations for the 2019 shared task were converted to the UniMorph schema (Kirov et al., 2018) according to McCarthy et al. (2018), who provide a deterministic mapping that increases agreement across languages. This also moves the part of speech into the bundle of morphological features. We do not attempt to individually correct any errors in the UD source material. Further, some languages received additional pre-processing. In the Finnish data, we removed morpheme boundaries that were present in the lemmata (e.g., puhe#kieli 7→ puhekieli ‘spoken+language’). Russian lemmata in the GSD treebank were presented in all uppercase; to match Swahili. Likewise, the low-resource language Telugu had fewer than 100 forms. 4 When su"
W19-4226,D15-1166,0,0.0843899,"ani Kurdish were created as part of the Alexina project (Walther et al., 2010; Walther and Sagot, 2010). 2 These datasets can be obtained from https:// sigmorphon.github.io/sharedtasks/2019/ 3 Several high-resource languages had necessarily fewer, but on a similar order of magnitude. Bengali, Uzbek, Kannada, 3 the 2018 shared task, we lowercased these. In development and test data, all fields except for form and index within the sentence were struck. 4 4.1 Team Baselines Task 1 Baseline We include four neural sequence-to-sequence models mapping lemma into inflected word forms: soft attention (Luong et al., 2015), non-monotonic hard attention (Wu et al., 2018), monotonic hard attention and a variant with offset-based transition distribution (Wu and Cotterell, 2019). Neural sequenceto-sequence models with soft attention (Luong et al., 2015) have dominated previous SIGMORPHON shared tasks (Cotterell et al., 2017). Wu et al. (2018) instead models the alignment between characters in the lemma and the inflected word form explicitly with hard attention and learns this alignment and transduction jointly. Wu and Cotterell (2019) shows that enforcing strict monotonicity with hard attention is beneficial in tas"
W19-4226,P18-1247,1,0.817396,"ciently prescribed by the lemma, as with the Romanian verbal inflection classes or nominal gender in German. As we move toward multilingual models for morphology, it becomes important to understand which representations are critical or irrelevant for adapting to new languages; this may be probed in the style of (Thompson et al., 2018), and it can be used as a first step toward designing systems that avoid “catastrophic forgetting” as they learn to inflect new languages (Thompson et al., 2019). Future directions for Task 2 include exploring cross-lingual analysis—in stride with both Task 1 and Malaviya et al. (2018)—and leveraging these analyses in downstream tasks. 7 In the second task, several methods were implemented by multiple groups, with the most successful systems implementing variations of multiheaded attention, multi-level encoding, multiple decoders, and ELMo and BERT contextual embeddings. We have released the training, development, and test sets, and expect these datasets to provide a useful benchmark for future research into learning of inflectional morphology and string-to-string transduction. Acknowledgments MS has received funding from the European Research Council (ERC) under the Europe"
W19-4226,P15-2111,1,0.881319,"Missing"
W19-4226,N19-1209,0,0.0249795,"tasks. One pertinent facet of this is information about inflectional categories—often the inflectional information is insufficiently prescribed by the lemma, as with the Romanian verbal inflection classes or nominal gender in German. As we move toward multilingual models for morphology, it becomes important to understand which representations are critical or irrelevant for adapting to new languages; this may be probed in the style of (Thompson et al., 2018), and it can be used as a first step toward designing systems that avoid “catastrophic forgetting” as they learn to inflect new languages (Thompson et al., 2019). Future directions for Task 2 include exploring cross-lingual analysis—in stride with both Task 1 and Malaviya et al. (2018)—and leveraging these analyses in downstream tasks. 7 In the second task, several methods were implemented by multiple groups, with the most successful systems implementing variations of multiheaded attention, multi-level encoding, multiple decoders, and ELMo and BERT contextual embeddings. We have released the training, development, and test sets, and expect these datasets to provide a useful benchmark for future research into learning of inflectional morphology and str"
W19-4226,W18-6313,1,0.850382,"sentangle.8 Creating new data sets that accurately reflect learner exposure (whether L1 or L2) is also an important consideration in the design of future shared tasks. One pertinent facet of this is information about inflectional categories—often the inflectional information is insufficiently prescribed by the lemma, as with the Romanian verbal inflection classes or nominal gender in German. As we move toward multilingual models for morphology, it becomes important to understand which representations are critical or irrelevant for adapting to new languages; this may be probed in the style of (Thompson et al., 2018), and it can be used as a first step toward designing systems that avoid “catastrophic forgetting” as they learn to inflect new languages (Thompson et al., 2019). Future directions for Task 2 include exploring cross-lingual analysis—in stride with both Task 1 and Malaviya et al. (2018)—and leveraging these analyses in downstream tasks. 7 In the second task, several methods were implemented by multiple groups, with the most successful systems implementing variations of multiheaded attention, multi-level encoding, multiple decoders, and ELMo and BERT contextual embeddings. We have released the t"
W19-4226,W18-5818,1,0.888961,"Missing"
W19-4226,P19-1148,1,0.888951,"sigmorphon.github.io/sharedtasks/2019/ 3 Several high-resource languages had necessarily fewer, but on a similar order of magnitude. Bengali, Uzbek, Kannada, 3 the 2018 shared task, we lowercased these. In development and test data, all fields except for form and index within the sentence were struck. 4 4.1 Team Baselines Task 1 Baseline We include four neural sequence-to-sequence models mapping lemma into inflected word forms: soft attention (Luong et al., 2015), non-monotonic hard attention (Wu et al., 2018), monotonic hard attention and a variant with offset-based transition distribution (Wu and Cotterell, 2019). Neural sequenceto-sequence models with soft attention (Luong et al., 2015) have dominated previous SIGMORPHON shared tasks (Cotterell et al., 2017). Wu et al. (2018) instead models the alignment between characters in the lemma and the inflected word form explicitly with hard attention and learns this alignment and transduction jointly. Wu and Cotterell (2019) shows that enforcing strict monotonicity with hard attention is beneficial in tasks such as morphological inflection where the transduction is mostly monotonic. The encoder is a biLSTM while the decoder is a left-to-right LSTM. All mode"
W19-4226,D18-1473,1,0.906631,"Missing"
W19-4226,D16-1163,0,0.0280678,"larger number of examples in either a related or unrelated language. Each test example asked participants to produce some other inflected form when given a lemma and a bundle of morphosyntactic features as input. The goal, thus, is to perform morphological inflection in the low-resource language, having hopefully exploited some similarity to the high-resource language. Models which perform well here can aid downstream tasks like machine translation in lowresource settings. All datasets were resampled from UniMorph, which makes them distinct from past years. The mode of the task is inspired by Zoph et al. (2016), who fine-tune a model pre-trained on a high-resource language to perform well on a lowresource language. We do not, though, require that models be trained by fine-tuning. Joint modeling or any number of methods may be explored instead. Task 2: Morphological analysis in context Although inflection of words in a context-agnostic manner is a useful evaluation of the morphological quality of a system, people do not learn morphology in isolation. In 2018, the second task of the CoNLL– SIGMORPHON Shared Task (Cotterell et al., 2018) required submitting systems to complete an inflectional cloze tas"
W19-4226,W18-6011,1,\N,Missing
