2021.mtsummit-research.8,Surprise Language Challenge: Developing a Neural Machine Translation System between {P}ashto and {E}nglish in Two Months,2021,-1,-1,1,1,5031,alexandra birch,Proceedings of Machine Translation Summit XVIII: Research Track,0,In the media industry and the focus of global reporting can shift overnight. There is a compelling need to be able to develop new machine translation systems in a short period of time and in order to more efficiently cover quickly developing stories. As part of the EU project GoURMET and which focusses on low-resource machine translation and our media partners selected a surprise language for which a machine translation system had to be built and evaluated in two months(February and March 2021). The language selected was Pashto and an Indo-Iranian language spoken in Afghanistan and Pakistan and India. In this period we completed the full pipeline of development of a neural machine translation system: data crawling and cleaning and aligning and creating test sets and developing and testing models and and delivering them to the user partners. In this paperwe describe rapid data creation and experiments with transfer learning and pretraining for this low-resource language pair. We find that starting from an existing large model pre-trained on 50languages leads to far better BLEU scores than pretraining on one high-resource language pair with a smaller model. We also present human evaluation of our systems and which indicates that the resulting systems perform better than a freely available commercial system when translating from English into Pashto direction and and similarly when translating from Pashto into English.
2021.findings-acl.261,Exploring Unsupervised Pretraining Objectives for Machine Translation,2021,-1,-1,3,0.833333,8137,christos baziotis,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.69,{C}o{PHE}: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification,2021,-1,-1,3,0,8773,matuvs falis,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Large-Scale Multi-Label Text Classification (LMTC) includes tasks with hierarchical label spaces, such as automatic assignment of ICD-9 codes to discharge summaries. Performance of models in prior art is evaluated with standard precision, recall, and F1 measures without regard for the rich hierarchical structure. In this work we argue for hierarchical evaluation of the predictions of neural LMTC models. With the example of the ICD-9 ontology we describe a structural issue in the representation of the structured label space in prior art, and propose an alternative representation based on the depth of the ontology. We propose a set of metrics for hierarchical evaluation using the depth-based representation. We compare the evaluation scores from the proposed metrics with previously used metrics on prior art LMTC models for ICD-9 coding in MIMIC-III. We also propose further avenues of research involving the proposed ontological representation."
2021.emnlp-main.87,Cross-lingual Intermediate Fine-tuning improves Dialogue State Tracking,2021,-1,-1,3,0,8806,nikita moghe,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Recent progress in task-oriented neural dialogue systems is largely focused on a handful of languages, as annotation of training data is tedious and expensive. Machine translation has been used to make systems multilingual, but this can introduce a pipeline of errors. Another promising solution is using cross-lingual transfer learning through pretrained multilingual models. Existing methods train multilingual models with additional code-mixed task data or refine the cross-lingual representations through parallel ontologies. In this work, we enhance the transfer learning process by intermediate fine-tuning of pretrained multilingual models, where the multilingual models are fine-tuned with different but related data and/or tasks. Specifically, we use parallel and conversational movie subtitles datasets to design cross-lingual intermediate tasks suitable for downstream dialogue tasks. We use only 200K lines of parallel data for intermediate fine-tuning which is already available for 1782 language pairs. We test our approach on the cross-lingual dialogue state tracking task for the parallel MultiWoZ (English -{\textgreater} Chinese, Chinese -{\textgreater} English) and Multilingual WoZ (English -{\textgreater} German, English -{\textgreater} Italian) datasets. We achieve impressive improvements ({\textgreater} 20{\%} on joint goal accuracy) on the parallel MultiWoZ dataset and the Multilingual WoZ dataset over the vanilla baseline with only 10{\%} of the target language task data and zero-shot setup respectively."
2021.eacl-main.90,Few-shot learning through contextual data augmentation,2021,-1,-1,3,0,10630,farid arthaud,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Machine translation (MT) models used in industries with constantly changing topics, such as translation or news agencies, need to adapt to new data to maintain their performance over time. Our aim is to teach a pre-trained MT model to translate previously unseen words accurately, based on very few examples. We propose (i) an experimental setup allowing us to simulate novel vocabulary appearing in human-submitted translations, and (ii) corresponding evaluation metrics to compare our approaches. We extend a data augmentation approach using a pretrained language model to create training examples with similar contexts for novel words. We compare different fine-tuning and data augmentation approaches and show that adaptation on the scale of one to five examples is possible. Combining data augmentation with randomly selected training sentences leads to the highest BLEU score and accuracy improvements. Impressively, with only 1 to 5 examples, our model reports better accuracy scores than a reference system trained with on average 313 parallel examples."
2020.wmt-1.5,The {U}niversity of {E}dinburgh{'}s {E}nglish-{T}amil and {E}nglish-{I}nuktitut Submissions to the {WMT}20 News Translation Task,2020,-1,-1,2,0.564417,7687,rachel bawden,Proceedings of the Fifth Conference on Machine Translation,0,"We describe the University of Edinburgh{'}s submissions to the WMT20 news translation shared task for the low resource language pair English-Tamil and the mid-resource language pair English-Inuktitut. We use the neural machine translation transformer architecture for all submissions and explore a variety of techniques to improve translation quality to compensate for the lack of parallel training data. For the very low-resource English-Tamil, this involves exploring pretraining, using both language model objectives and translation using an unrelated high-resource language pair (German-English), and iterative backtranslation. For English-Inuktitut, we explore the use of multilingual systems, which, despite not being part of the primary submission, would have achieved the best results on the test set."
2020.ngt-1.1,Findings of the Fourth Workshop on Neural Generation and Translation,2020,-1,-1,8,0,10335,kenneth heafield,Proceedings of the Fourth Workshop on Neural Generation and Translation,0,"We describe the finding of the Fourth Workshop on Neural Generation and Translation, held in concert with the annual conference of the Association for Computational Linguistics (ACL 2020). First, we summarize the research trends of papers presented in the proceedings. Second, we describe the results of the three shared tasks 1) efficient neural machine translation (NMT) where participants were tasked with creating NMT systems that are both accurate and efficient, and 2) document-level generation and translation (DGT) where participants were tasked with developing systems that generate summaries from structured data, potentially with assistance from text in another language and 3) STAPLE task: creation of as many possible translations of a given input text. This last shared task was organised by Duolingo."
2020.lrec-1.471,Multiword Expression aware Neural Machine Translation,2020,-1,-1,2,0,17623,andrea zaninello,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Multiword Expressions (MWEs) are a frequently occurring phenomenon found in all natural languages that is of great importance to linguistic theory, natural language processing applications, and machine translation systems. Neural Machine Translation (NMT) architectures do not handle these expressions well and previous studies have rarely addressed MWEs in this framework. In this work, we show that annotation and data augmentation, using external linguistic resources, can improve both translation of MWEs that occur in the source, and the generation of MWEs on the target, and increase performance by up to 5.09 BLEU points on MWE test sets. We also devise a MWE score to specifically assess the quality of MWE translation which agrees with human evaluation. We make available the MWE score implementation {--} along with MWE-annotated training sets and corpus-based lists of MWEs {--} for reproduction and extension."
2020.iwltp-1.3,"Architecture of a Scalable, Secure and Resilient Translation Platform for Multilingual News Media",2020,-1,-1,5,0,18867,susie coleman,Proceedings of the 1st International Workshop on Language Technology Platforms,0,"This paper presents an example architecture for a scalable, secure and resilient Machine Translation (MT) platform, using components available via Amazon Web Services (AWS). It is increasingly common for a single news organisation to publish and monitor news sources in multiple languages. A growth in news sources makes this increasingly challenging and time-consuming but MT can help automate some aspects of this process. Building a translation service provides a single integration point for news room tools that use translation technology allowing MT models to be integrated into a system once, rather than each time the translation technology is needed. By using a range of services provided by AWS, it is possible to architect a platform where multiple pre-existing technologies are combined to build a solution, as opposed to developing software from scratch for deployment on a single virtual machine. This increases the speed at which a platform can be developed and allows the use of well-maintained services. However, a single service also provides challenges. It is key to consider how the platform will scale when handling many users and how to ensure the platform is resilient."
2020.emnlp-main.187,Bridging Linguistic Typology and Multilingual Machine Translation with Multi-View Language Representations,2020,36,0,3,0,1368,arturo oncevay,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Sparse language vectors from linguistic typology databases and learned embeddings from tasks like multilingual machine translation have been investigated in isolation, without analysing how they could benefit from each other{'}s language characterisation. We propose to fuse both views using singular vector canonical correlation analysis and study what kind of information is induced from each source. By inferring typological features and language phylogenies, we observe that our representations embed typology and strengthen correlations with language relationships. We then take advantage of our multi-view language vector space for multilingual machine translation, where we achieve competitive overall translation accuracy in tasks that require information about language similarities, such as language clustering and ranking candidates for multilingual transfer. With our method, we can easily project and assess new languages without expensive retraining of massive multilingual or ranking models, which are major disadvantages of related approaches."
2020.emnlp-main.615,Language Model Prior for Low-Resource Neural Machine Translation,2020,45,0,3,0.833333,8137,christos baziotis,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM). Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM {``}disagrees{''} with the LM. This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language. The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis of the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data."
W19-6723,Global Under-Resourced Media Translation ({G}o{URMET}),2019,-1,-1,1,1,5031,alexandra birch,"Proceedings of Machine Translation Summit XVII: Translator, Project and User Tracks",0,None
W19-5304,The {U}niversity of {E}dinburgh{'}s Submissions to the {WMT}19 News Translation Task,2019,24,0,7,0.833333,7687,rachel bawden,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"The University of Edinburgh participated in the WMT19 Shared Task on News Translation in six language directions: EnglishâGujarati, EnglishâChinese, GermanâEnglish, and EnglishâCzech. For all translation directions, we created or used back-translations of monolingual data in the target language as additional synthetic training data. For EnglishâGujarati, we also explored semi-supervised MT with cross-lingual language model pre-training, and translation pivoting through Hindi. For translation to and from Chinese, we investigated character-based tokenisation vs. sub-word segmentation of Chinese text. For GermanâEnglish, we studied the impact of vast amounts of back-translated training data on translation quality, gaining a few additional insights over Edunov et al. (2018). For EnglishâCzech, we compared different preprocessing and tokenisation regimes."
D19-5601,Findings of the Third Workshop on Neural Generation and Translation,2019,25,1,3,1,4384,hiroaki hayashi,Proceedings of the 3rd Workshop on Neural Generation and Translation,0,"This document describes the findings of the Third Workshop on Neural Generation and Translation, held in concert with the annual conference of the Empirical Methods in Natural Language Processing (EMNLP 2019). First, we summarize the research trends of papers presented in the proceedings. Second, we describe the results of the two shared tasks 1) efficient neural machine translation (NMT) where participants were tasked with creating NMT systems that are both accurate and efficient, and 2) document generation and translation (DGT) where participants were tasked with developing systems that generate summaries from structured data, potentially with assistance from text in another language."
D19-5619,On the Importance of Word Boundaries in Character-level Neural Machine Translation,2019,23,2,5,0,1387,duygu ataman,Proceedings of the 3rd Workshop on Neural Generation and Translation,0,"Neural Machine Translation (NMT) models generally perform translation using a fixed-size lexical vocabulary, which is an important bottleneck on their generalization capability and overall translation quality. The standard approach to overcome this limitation is to segment words into subword units, typically using some external tools with arbitrary heuristics, resulting in vocabulary units not optimized for the translation task. Recent studies have shown that the same approach can be extended to perform NMT directly at the level of characters, which can deliver translation accuracy on-par with subword-based models, on the other hand, this requires relatively deeper networks. In this paper, we propose a more computationally-efficient solution for character-level NMT which implements a hierarchical decoding architecture where translations are subsequently generated at the level of words and characters. We evaluate different methods for open-vocabulary NMT in the machine translation task from English into five languages with distinct morphological typology, and show that the hierarchical decoding model can reach higher translation accuracy than the subword-level NMT model using significantly fewer parameters, while demonstrating better capacity in learning longer-distance contextual and grammatical dependencies than the standard character-level NMT model."
W18-6320,Exploring gap filling as a cheaper alternative to reading comprehension questionnaires when evaluating machine translation for gisting,2018,0,0,5,0,5037,mikel forcada,Proceedings of the Third Conference on Machine Translation: Research Papers,0,"A popular application of machine translation (MT) is \textit{gisting}: MT is consumed \textit{as is} to make sense of text in a foreign language. Evaluation of the usefulness of MT for gisting is surprisingly uncommon. The classical method uses \textit{reading comprehension questionnaires} (RCQ), in which informants are asked to answer professionally-written questions in their language about a foreign text that has been machine-translated into their language. Recently, \textit{gap-filling} (GF), a form of \textit{cloze} testing, has been proposed as a cheaper alternative to RCQ. In GF, certain words are removed from reference translations and readers are asked to fill the gaps left using the machine-translated text as a hint. This paper reports, for the first time, a comparative evaluation, using both RCQ and GF, of translations from multiple MT systems for the same foreign texts, and a systematic study on the effect of variables such as gap density, gap-selection strategies, and document context in GF. The main findings of the study are: (a) both RCQ and GF clearly identify MT to be useful; (b) global RCQ and GF rankings for the MT systems are mostly in agreement; (c) GF scores vary very widely across informants, making comparisons among MT systems hard, and (d) unlike RCQ, which is framed around documents, GF evaluation can be framed at the sentence level. These findings support the use of GF as a cheaper alternative to RCQ."
W18-2701,Findings of the Second Workshop on Neural Machine Translation and Generation,2018,28,1,1,1,5031,alexandra birch,Proceedings of the 2nd Workshop on Neural Machine Translation and Generation,0,"This document describes the findings of the Second Workshop on Neural Machine Translation and Generation, held in concert with the annual conference of the Association for Computational Linguistics (ACL 2018). First, we summarize the research trends of papers presented in the proceedings, and note that there is particular interest in linguistic structure, domain adaptation, data augmentation, handling inadequate resources, and analysis of models. Second, we describe the results of the workshop{'}s shared task on efficient neural machine translation, where participants were tasked with creating MT systems that are both accurate and efficient."
P18-4020,{M}arian: Fast Neural Machine Translation in {C}++,2018,8,28,12,0,3523,marcin junczysdowmunt,"Proceedings of {ACL} 2018, System Demonstrations",0,"We present Marian, an efficient and self-contained Neural Machine Translation framework with an integrated automatic differentiation engine based on dynamic computation graphs. Marian is written entirely in C++. We describe the design of the encoder-decoder framework and demonstrate that a research-friendly toolkit can achieve high training and translation speed."
N18-1118,Evaluating Discourse Phenomena in Neural Machine Translation,2018,0,37,3,0.833333,7687,rachel bawden,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"For machine translation to tackle discourse phenomena, models must have access to extra-sentential linguistic context. There has been recent interest in modelling context in neural machine translation (NMT), but models have been principally evaluated with standard automatic metrics, poorly adapted to evaluating discourse phenomena. In this article, we present hand-crafted, discourse test sets, designed to test the models{'} ability to exploit previous source and target sentences. We investigate the performance of recently proposed multi-encoder NMT models trained on subtitles for English to French. We also explore a novel way of exploiting context from the previous sentence. Despite gains using BLEU, multi-encoder models give limited improvement in the handling of discourse phenomena: 50{\%} accuracy on our coreference test set and 53.5{\%} for coherence/cohesion (compared to a non-contextual baseline of 50{\%}). A simple strategy of decoding the concatenation of the previous and current sentence leads to good performance, and our novel strategy of multi-encoding and decoding of two sentences leads to the best performance (72.5{\%} for coreference and 57{\%} for coherence/cohesion), highlighting the importance of target-side context."
W17-4707,Predicting Target Language {CCG} Supertags Improves Neural Machine Translation,2017,0,19,7,1,25421,maria nuadejde,Proceedings of the Second Conference on Machine Translation,0,None
W17-4710,Deep architectures for Neural Machine Translation,2017,8,7,5,0,5033,antonio barone,Proceedings of the Second Conference on Machine Translation,0,"It has been shown that increasing model depth improves the quality of neural machine translation. However, different architectural variants to increase model depth have been proposed, and so far, there has been no thorough comparative study. n In this work, we describe and evaluate several existing approaches to introduce depth in neural machine translation. Additionally, we explore novel architectural variants, including deep transition RNNs, and we vary how attention is used in the deep decoder. We introduce a novel BiDeep RNN architecture that combines deep transition RNNs and stacked RNNs. n Our evaluation is carried out on the English to German WMT news translation dataset, using a single-GPU machine for both training and inference. We find that several of our proposed architectures improve upon existing approaches in terms of speed and translation quality. We obtain best improvements with a BiDeep RNN of combined depth 8, obtaining an average improvement of 1.5 BLEU over a strong shallow baseline. n We release our code for ease of adoption."
W17-4739,The {U}niversity of {E}dinburgh{'}s Neural {MT} Systems for {WMT}17,2017,6,37,2,0.273102,2690,rico sennrich,Proceedings of the Second Conference on Machine Translation,0,"This paper describes the University of Edinburgh's submissions to the WMT17 shared news translation and biomedical translation tasks. We participated in 12 translation directions for news, translating between English and Czech, German, Latvian, Russian, Turkish and Chinese. For the biomedical task we submitted systems for English to Czech, German, Polish and Romanian. Our systems are neural machine translation systems trained with Nematus, an attentional encoder-decoder. We follow our setup from last year and build BPE-based models with parallel and back-translated monolingual training data. Novelties this year include the use of deep architectures, layer normalization, and more compact models due to weight tying and improvements in BPE segmentations. We perform extensive ablative experiments, reporting on the effectivenes of layer normalization, deep architectures, and different ensembling techniques."
E17-3017,{N}ematus: a Toolkit for Neural Machine Translation,2017,5,124,4,0.273102,2690,rico sennrich,Proceedings of the Software Demonstrations of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present Nematus, a toolkit for Neural Machine Translation. The toolkit prioritizes high translation accuracy, usability, and extensibility. Nematus has been used to build top-performing submissions to shared translation tasks at WMT and IWSLT, and has been used to train systems for production environments."
E17-3029,The {SUMMA} Platform Prototype,2017,8,1,4,0,28433,renars liepins,Proceedings of the Software Demonstrations of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present the first prototype of the SUMMA Platform: an integrated platform for multilingual media monitoring. The platform contains a rich suite of low-level and high-level natural language processing technologies: automatic speech recognition of broadcast media, machine translation, automated tagging and classification of named entities, semantic parsing to detect relationships between entities, and automatic construction / augmentation of factual knowledge bases. Implemented on the Docker platform, it can easily be deployed, customised, and scaled to large volumes of incoming media streams."
W16-2323,{E}dinburgh Neural Machine Translation Systems for {WMT} 16,2016,9,119,3,0.273102,2690,rico sennrich,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions: English Czech, English German, English Romanian and English Russian. Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary. We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3--11.2 BLEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which we participated."
W16-2204,Modeling Selectional Preferences of Verbs and Nouns in String-to-Tree Machine Translation,2016,26,2,2,1,25421,maria nuadejde,"Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers",0,None
P16-1009,Improving Neural Machine Translation Models with Monolingual Data,2016,15,397,3,0.273102,2690,rico sennrich,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
P16-1162,Neural Machine Translation of Rare Words with Subword Units,2016,34,1423,3,0.273102,2690,rico sennrich,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English!German and English!Russian by up to 1.1 and 1.3 BLEU, respectively."
N16-1005,Controlling Politeness in Neural Machine Translation via Side Constraints,2016,12,82,3,0.273102,2690,rico sennrich,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
D16-1134,{HUME}: Human {UCCA}-Based Evaluation of Machine Translation,2016,9,11,1,1,5031,alexandra birch,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
W15-3013,The {E}dinburgh/{JHU} Phrase-based Machine Translation Systems for {WMT} 2015,2015,30,13,3,0.233554,5032,barry haddow,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper describes the submission of the University of Edinburgh and the Johns Hopkins University for the shared translation task of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation (WMT 2015). We set up phrase-based statistical machine translation systems for all ten language pairs of this yearxe2x80x99s evaluation campaign, which are English paired with Czech, Finnish, French, German, and Russian in both translation directions. Novel research directions we investigated include: neural network language models and bilingual neural network language models, a comprehensive use of word classes, and sparse lexicalized reordering features."
2015.mtsummit-papers.19,Mixed domain vs. multi-domain statistical machine translation,2015,45,6,2,0,5061,matthias huck,Proceedings of Machine Translation Summit XV: Papers,0,"Domain adaptation boosts translation quality on in-domain data, but translation quality for domain adapted systems on out-of-domain data tends to suffer. Users of web-based translation services expect high quality translation across a wide range of diverse domains, and what makes the task even more difficult is that no domain label is provided with the translation request. In this paper we present an approach to domain adaptation which results in large-scale, general purpose machine translation systems. First, we tune our translation models to multiple individual domains. Then, by means of source-side domain classification, we are able to predict the domain of individual input sentences and thereby select the appropriate domain-specific model parameters. We call this approach multi-domain translation. We develop state-of-the-art, domain-adapted translation engines for three broadly-defined domains: TED talks, Europarl, and News. Our results suggest that multi-domain translation performs better than a mixed-domain approach, which deploys a system that has been tuned on a development set composed of samples from many domains."
2015.iwslt-evaluation.4,The {E}dinburgh machine translation systems for {IWSLT} 2015,2015,55,3,2,0,5061,matthias huck,Proceedings of the 12th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the University of Edinburghxe2x80x99s machine translation (MT) systems for the IWSLT 2015 evaluation campaign. Our submissions are based on preliminary systems which are under development for the purpose of lecture translation in the TraMOOC project, 1 funded by the European Union. We participated in the English!Chinese and the English!German translation tasks in the MT track, utilizing only data supplied by the organizers or listed as permissible. We built phrase-based translation systems for both tasks. For English!German, we furthermore made use of syntax-based translation and system combination."
E14-1014,Generalizing a Strongly Lexicalized Parser using Unlabeled Data,2014,26,5,3,0,5203,tejaswini deoskar,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Statistical parsers trained on labeled data suffer from sparsity, both grammatical and lexical. For parsers based on strongly lexicalized grammar formalisms (such as CCG, which has complex lexical categories but simple combinatory rules), the problem of sparsity can be isolated to the lexicon. In this paper, we show that semi-supervised Viterbi-EM can be used to extend the lexicon of a generative CCG parser. By learning complex lexical entries for low-frequency and unseen words from unlabeled data, we obtain improvements over our supervised model for both indomain (WSJ) and out-of-domain (questions and Wikipedia) data. Our learnt lexicons when used with a discriminative parser such as C&C also significantly improve its performance on unseen words."
2014.iwslt-evaluation.6,{E}dinburgh {SLT} and {MT} system description for the {IWSLT} 2014 evaluation,2014,48,14,1,1,5031,alexandra birch,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the University of Edinburgh{'}s spoken language translation (SLT) and machine translation (MT) systems for the IWSLT 2014 evaluation campaign. In the SLT track, we participated in the GermanâEnglish and EnglishâFrench tasks. In the MT track, we participated in the GermanâEnglish, EnglishâFrench, ArabicâEnglish, FarsiâEnglish, HebrewâEnglish, SpanishâEnglish, and Portuguese-BrazilâEnglish tasks. For our SLT submissions, we experimented with comparing operation sequence models with bilingual neural network language models. For our MT submissions, we explored using unsupervised transliteration for languages which have a different script than English, in particular for Arabic, Farsi, and Hebrew. We also investigated syntax-based translation and system combination."
2014.iwslt-evaluation.7,Combined spoken language translation,2014,55,6,6,0,3519,markus freitag,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"EU-BRIDGE is a European research project which is aimed at developing innovative speech translation technology. One of the collaborative efforts within EU-BRIDGE is to produce joint submissions of up to four different partners to the evaluation campaign at the 2014 International Workshop on Spoken Language Translation (IWSLT). We submitted combined translations to the GermanâEnglish spoken language translation (SLT) track as well as to the GermanâEnglish, EnglishâGerman and EnglishâFrench machine translation (MT) tracks. In this paper, we present the techniques which were applied by the different individual translation systems of RWTH Aachen University, the University of Edinburgh, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler. We then show the combination approach developed at RWTH Aachen University which combined the individual systems. The consensus translations yield empirical gains of up to 2.3 points in BLEU and 1.2 points in TER compared to the best individual system."
W13-2203,The Feasibility of {HMEANT} as a Human {MT} Evaluation Metric,2013,27,11,1,1,5031,alexandra birch,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"There has been a recent surge of interest in semantic machine translation, which standard automatic metrics struggle to evaluate. A family of measures called MEANT has been proposed which uses semantic role labels (SRL) to overcome this problem. The human variant, HMEANT, has largely been evaluated using correlation with human contrastive evaluations, the standard human evaluation metric for the WMT shared tasks. In this paper we claim that for a human metric to be useful, it needs to be evaluated on intrinsic properties. It needs to be reliable; it needs to work across different language pairs; and it needs to be lightweight. Most importantly, however, a human metric must be discerning. We conclude that HMEANT is a step in the right direction, but has some serious flaws. The reliance on verbs as heads of frames, and the assumption that annotators need minimal guidelines are particularly problematic."
2013.iwslt-evaluation.3,{E}nglish {SLT} and {MT} system description for the {IWSLT} 2013 evaluation,2013,40,12,1,1,5031,alexandra birch,Proceedings of the 10th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper gives a description of the University of Edinburgh{'}s (UEDIN) systems for IWSLT 2013. We participated in all the MT tracks and the German-to-English and Englishto-French SLT tracks. Our SLT submissions experimented with including ASR uncertainty into the decoding process via confusion networks, and looked at different ways of punctuating ASR output. Our MT submissions are mainly based on a system used in the recent evaluation campaign at the Workshop on Statistical Machine Translation [1]. We additionally explored the use of generalized representations (Brown clusters, POS and morphological tags) translating out of English into European languages."
2013.iwslt-evaluation.22,The {UEDIN} {E}nglish {ASR} system for the {IWSLT} 2013 evaluation,2013,18,8,5,0,10879,peter bell,Proceedings of the 10th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the University of Edinburgh (UEDIN) English ASR system for the IWSLT 2013 Evaluation. Notable features of the system include deep neural network acoustic models in both tandem and hybrid configuration, cross-domain adaptation with multi-level adaptive networks, and the use of a recurrent neural network language model. Improvements to our system since the 2012 evaluation {--} which include the use of a significantly improved n-gram language model {--} result in a 19{\%} relative WER reduction on the tst2012 set."
W11-2916,Simple Semi-Supervised Learning for Prepositional Phrase Attachment,2011,26,3,2,0,44136,gregory coppola,Proceedings of the 12th International Conference on Parsing Technologies,0,"Prepositional phrase attachment is an important subproblem of parsing, performance on which suffers from limited availability of labelled data. We present a semi-supervised approach. We show that a discriminative lexical model trained from labelled data, and a generative lexical model learned via Expectation Maximization from unlabelled data can be combined in a product model to yield a PP-attachment model which is better than either is alone, and which outperforms the modern parser of Petrov and Klein (2007) by a significant margin. We show that, when learning from unlabelled data, it can be beneficial to model the generation of modifiers of a head collectively, rather than individually. Finally, we suggest that our pair of models will be interesting to combine using new techniques for discriminatively constraining EM."
P11-1103,Reordering Metrics for {MT},2011,21,31,1,1,5031,alexandra birch,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"One of the major challenges facing statistical machine translation is how to model differences in word order between languages. Although a great deal of research has focussed on this problem, progress is hampered by the lack of reliable metrics. Most current metrics are based on matching lexical items in the translation and the reference, and their ability to measure the quality of word order has not been demonstrated. This paper presents a novel metric, the LRscore, which explicitly measures the quality of word order by using permutation distance metrics. We show that the metric is more consistent with human judgements than other metrics, including the Bleu score. We also show that the LRscore can successfully be used as the objective function when training translation model parameters. Training with the LRscore leads to output which is preferred by humans. Moreover, the translations incur no penalty in terms of Bleu scores."
D11-1079,Soft Dependency Constraints for Reordering in Hierarchical Phrase-Based Translation,2011,48,28,3,0,7649,yang gao,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"Long-distance reordering remains one of the biggest challenges facing machine translation. We derive soft constraints from the source dependency parsing to directly address the reordering problem for the hierarchical phrase-based model. Our approach significantly improves Chinese--English machine translation on a large-scale task by 0.84 BLEU points on average. Moreover, when we switch the tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering."
W10-1749,{LR}score for Evaluating Lexical and Reordering Quality in {MT},2010,19,38,1,1,5031,alexandra birch,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"The ability to measure the quality of word order in translations is an important goal for research in machine translation. Current machine translation metrics do not adequately measure the reordering performance of translation systems. We present a novel metric, the LRscore, which directly measures reordering success. The reordering component is balanced by a lexical metric. Capturing the two most important elements of translation success in a simple combined metric with only one parameter results in an intuitive, shallow, language independent metric."
W09-0434,A Quantitative Analysis of Reordering Phenomena,2009,16,31,1,1,5031,alexandra birch,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"Reordering is a serious challenge in statistical machine translation. We propose a method for analysing syntactic reordering in parallel corpora and apply it to understanding the differences in the performance of SMT systems. Results at recent large-scale evaluation campaigns show that synchronous grammar-based statistical machine translation models produce superior results for language pairs such as Chinese to English. However, for language pairs such as Arabic to English, phrase-based approaches continue to be competitive. Until now, our understanding of these results has been limited to differences in Bleu scores. Our analysis shows that current state-of-the-art systems fail to capture the majority of reorderings found in real data."
2009.mtsummit-papers.7,462 Machine Translation Systems for {E}urope,2009,9,65,2,0.0487485,4417,philipp koehn,Proceedings of Machine Translation Summit XII: Papers,0,"We built 462 machine translation systems for all language pairs of the Acquis Communautaire corpus. We report and analyse the performance of these system, and compare them against pivot translation and a number of system combination methods (multi-pivot, multisource) that are possible due to the available systems."
D08-1078,Predicting Success in Machine Translation,2008,13,50,1,1,5031,alexandra birch,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"The performance of machine translation systems varies greatly depending on the source and target languages involved. Determining the contribution of different characteristics of language pairs on system performance is key to knowing what aspects of machine translation to improve and which are irrelevant. This paper investigates the effect of different explanatory variables on the performance of a phrase-based system for 110 European language pairs. We show that three factors are strong predictors of performance in isolation: the amount of reordering, the morphological complexity of the target language and the historical relatedness of the two languages. Together, these factors contribute 75% to the variability of the performance of the system."
W07-0702,{CCG} Supertags in Factored Statistical Machine Translation,2007,22,82,1,1,5031,alexandra birch,Proceedings of the Second Workshop on Statistical Machine Translation,0,Combinatorial Categorial Grammar (CCG) supertags present phrase-based machine translation with an opportunity to access rich syntactic information at a word level. The challenge is incorporating this information into the translation process. Factored translation models allow the inclusion of supertags as a factor in the source or target language. We show that this results in an improvement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings.
P07-2045,{M}oses: Open Source Toolkit for Statistical Machine Translation,2007,13,3819,3,0.0487485,4417,philipp koehn,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks."
W06-3123,"Constraining the Phrase-Based, Joint Probability Statistical Translation Model",2006,20,45,1,1,5031,alexandra birch,Proceedings on the Workshop on Statistical Machine Translation,0,"The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT). The model's usefulness is, however, limited by the computational complexity of estimating parameters at the phrase level. We present the first model to use word alignments for constraining the space of phrasal alignments searched during Expectation Maximization (EM) training. Constraining the joint model improves performance, showing results that are very close to state-of-the-art phrase-based models. It also allows it to scale up to larger corpora and therefore be more widely applicable."
2006.amta-papers.2,"Constraining the Phrase-Based, Joint Probability Statistical Translation Model",2006,20,45,1,1,5031,alexandra birch,Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"The Joint Probability Model proposed by Marcu and Wong (2002) provides a probabilistic framework for modeling phrase-based statistical machine transla- tion (SMT). The model{'}s usefulness is, however, limited by the computational complexity of estimating parameters at the phrase level. We present a method of constraining the search space of the Joint Probability Model based on statistically and linguistically motivated word align- ments. This method reduces the complexity and size of the Joint Model and allows it to display performance superior to the standard phrase-based models for small amounts of training material."
