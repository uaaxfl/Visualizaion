2021.jeptalnrecital-taln.3,Exploration des relations s{\\'e}mantiques sous-jacentes aux plongements contextuels de mots (Exploring semantic relations underlying contextual word embeddings),2021,-1,-1,1,1,5589,olivier ferret,Actes de la 28e Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conf{\\'e}rence principale,0,"De nombreuses {\'e}tudes ont r{\'e}cemment {\'e}t{\'e} r{\'e}alis{\'e}es pour {\'e}tudier les propri{\'e}t{\'e}s des mod{\`e}les de langue contextuels mais, de mani{\`e}re surprenante, seules quelques-unes d{'}entre elles se concentrent sur les propri{\'e}t{\'e}s de ces mod{\`e}les en termes de similarit{\'e} s{\'e}mantique. Dans cet article, nous proposons d{'}abord, en nous appuyant sur le principe distributionnel de substituabilit{\'e}, une m{\'e}thode permettant d{'}utiliser ces mod{\`e}les pour ordonner un ensemble de mots cibles en fonction de leur similarit{\'e} avec un mot source. Nous appliquons d{'}abord cette m{\'e}thode pour l{'}anglais comme m{\'e}canisme de sondage pour explorer les propri{\'e}t{\'e}s s{\'e}mantiques des mod{\`e}les ELMo et BERT du point de vue des relations paradigmatiques de WordNet et dans le contexte contr{\^o}l{\'e} du corpus SemCor. Dans un second temps, nous la transposons {\`a} l{'}{\'e}tude des diff{\'e}rences entre ces mod{\`e}les contextuels et un mod{\`e}le de plongement statique."
2021.jeptalnrecital-taln.17,Int{\\'e}r{\\^e}t des mod{\\`e}les de caract{\\`e}res pour la d{\\'e}tection d{'}{\\'e}v{\\'e}nements (The interest of character-level models for event detection),2021,-1,-1,3,0,5625,emanuela boros,Actes de la 28e Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conf{\\'e}rence principale,0,"Cet article aborde la t{\^a}che de d{\'e}tection d{'}{\'e}v{\'e}nements, visant {\`a} identifier et cat{\'e}goriser les mentions d{'}{\'e}v{\'e}nements dans les textes. Une des difficult{\'e}s de cette t{\^a}che est le probl{\`e}me des mentions d{'}{\'e}v{\'e}nements correspondant {\`a} des mots mal orthographi{\'e}s, tr{\`e}s sp{\'e}cifiques ou hors vocabulaire. Pour analyser l{'}impact de leur prise en compte par le biais de mod{\`e}les de caract{\`e}res, nous proposons d{'}int{\'e}grer des plongements de caract{\`e}res, qui peuvent capturer des informations morphologiques et de forme sur les mots, {\`a} un mod{\`e}le convolutif pour la d{\'e}tection d{'}{\'e}v{\'e}nements. Plus pr{\'e}cis{\'e}ment, nous {\'e}valuons deux strat{\'e}gies pour r{\'e}aliser une telle int{\'e}gration et montrons qu{'}une approche de fusion tardive surpasse {\`a} la fois une approche de fusion pr{\'e}coce et des mod{\`e}les int{\'e}grant des informations sur les caract{\`e}res ou les sous-mots tels que ELMo ou BERT."
2020.lrec-1.528,Building a Multimodal Entity Linking Dataset From Tweets,2020,-1,-1,3,0,17726,omar adjali,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The task of Entity linking, which aims at associating an entity mention with a unique entity in a knowledge base (KB), is useful for advanced Information Extraction tasks such as relation extraction or event detection. Most of the studies that address this problem rely only on textual documents while an increasing number of sources are multimedia, in particular in the context of social media where messages are often illustrated with images. In this article, we address the Multimodal Entity Linking (MEL) task, and more particularly the problem of its evaluation. To this end, we propose a novel method to quasi-automatically build annotated datasets to evaluate methods on the MEL task. The method collects text and images to jointly build a corpus of tweets with ambiguous mentions along with a Twitter KB defining the entities. We release a new annotated dataset of Twitter posts associated with images. We study the key characteristics of the proposed dataset and evaluate the performance of several MEL approaches on it."
2020.lrec-1.713,Extrinsic Evaluation of {F}rench Dependency Parsers on a Specialized Corpus: Comparison of Distributional Thesauri,2020,-1,-1,3,0.271773,10279,ludovic tanguy,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We present a study in which we compare 11 different French dependency parsers on a specialized corpus (consisting of research articles on NLP from the proceedings of the TALN conference). Due to the lack of a suitable gold standard, we use each of the parsers{'} output to generate distributional thesauri using a frequency-based method. We compare these 11 thesauri to assess the impact of choosing a parser over another. We show that, without any reference data, we can still identify relevant subsets among the different parsers. We also show that the similarity we identify between parsers is confirmed on a restricted distributional benchmark."
2020.jeptalnrecital-taln.4,Repr{\\'e}sentation dynamique et sp{\\'e}cifique du contexte textuel pour l{'}extraction d{'}{\\'e}v{\\'e}nements (Dynamic and specific textual context representation for event extraction),2020,-1,-1,3,0,18573,dorian kodelja,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 2 : Traitement Automatique des Langues Naturelles",0,"Dans cet article, focalis{\'e} sur l{'}extraction supervis{\'e}e de mentions d{'}{\'e}v{\'e}nements dans les textes, nous proposons d{'}{\'e}tendre un mod{\`e}le op{\'e}rant au niveau phrastique et reposant sur une architecture neuronale de convolution de graphe exploitant les d{\'e}pendances syntaxiques. Nous y int{\'e}grons pour ce faire un contexte plus large au travers de la repr{\'e}sentation de phrases distantes s{\'e}lectionn{\'e}es sur la base de relations de cor{\'e}f{\'e}rence entre entit{\'e}s. En outre, nous montrons l{'}int{\'e}r{\^e}t d{'}une telle int{\'e}gration au travers d{'}{\'e}valuations men{\'e}es sur le corpus de r{\'e}f{\'e}rence TAC Event 2015."
2020.jeptalnrecital-taln.35,Mod{\\`e}le neuronal pour la r{\\'e}solution de la cor{\\'e}f{\\'e}rence dans les dossiers m{\\'e}dicaux {\\'e}lectroniques (Neural approach for coreference resolution in electronic health records ),2020,-1,-1,2,1,18601,julien tourille,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 2 : Traitement Automatique des Langues Naturelles",0,"La r{\'e}solution de la cor{\'e}f{\'e}rence est un {\'e}l{\'e}ment essentiel pour la constitution automatique de chronologies m{\'e}dicales {\`a} partir des dossiers m{\'e}dicaux {\'e}lectroniques. Dans ce travail, nous pr{\'e}sentons une approche neuronale pour la r{\'e}solution de la cor{\'e}f{\'e}rence dans des textes m{\'e}dicaux {\'e}crits en anglais pour les entit{\'e}s g{\'e}n{\'e}rales et cliniques en nous {\'e}valuant dans le cadre de r{\'e}f{\'e}rence pour cette t{\^a}che que constitue la t{\^a}che 1C de la campagne i2b2 2011."
2020.computerm-1.4,Which Dependency Parser to Use for Distributional Semantics in a Specialized Domain?,2020,-1,-1,2,0,18056,pauline brunet,Proceedings of the 6th International Workshop on Computational Terminology,0,"We present a study whose objective is to compare several dependency parsers for English applied to a specialized corpus for building distributional count-based models from syntactic dependencies. One of the particularities of this study is to focus on the concepts of the target domain, which mainly occur in documents as multi-terms and must be aligned with the outputs of the parsers. We compare a set of ten parsers in terms of syntactic triplets but also in terms of distributional neighbors extracted from the models built from these triplets, both with and without an external reference concerning the semantic relations between concepts. We show more particularly that some patterns of proximity between these parsers can be observed across our different evaluations, which could give insights for anticipating the performance of a parser for building distributional models from a given corpus"
2020.coling-main.609,{C}haracter{BERT}: Reconciling {ELM}o and {BERT} for Word-Level Open-Vocabulary Representations From Characters,2020,-1,-1,2,1,8589,hicham boukkouri,Proceedings of the 28th International Conference on Computational Linguistics,0,"Due to the compelling improvements brought by BERT, many recent representation models adopted the Transformer architecture as their main building block, consequently inheriting the wordpiece tokenization system despite it not being intrinsically linked to the notion of Transformers. While this system is thought to achieve a good balance between the flexibility of characters and the efficiency of full words, using predefined wordpiece vocabularies from the general domain is not always suitable, especially when building models for specialized domains (e.g., the medical domain). Moreover, adopting a wordpiece tokenization shifts the focus from the word level to the subword level, making the models conceptually more complex and arguably less convenient in practice. For these reasons, we propose CharacterBERT, a new variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters. We show that this new model improves the performance of BERT on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations."
P19-2041,Embedding Strategies for Specialized Domains: Application to Clinical Entity Recognition,2019,0,1,2,1,8589,hicham boukkouri,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"Using pre-trained word embeddings in conjunction with Deep Learning models has become the {``}de facto{''} approach in Natural Language Processing (NLP). While this usually yields satisfactory results, off-the-shelf word embeddings tend to perform poorly on texts from specialized domains such as clinical reports. Moreover, training specialized word representations from scratch is often either impossible or ineffective due to the lack of large enough in-domain data. In this work, we focus on the clinical domain for which we study embedding strategies that rely on general-domain resources only. We show that by combining off-the-shelf contextual embeddings (ELMo) with static word2vec embeddings trained on a small in-domain corpus built from the task data, we manage to reach and sometimes outperform representations learned from a large corpus in the medical domain."
2019.jeptalnrecital-long.3,Comparaison qualitative et extrins{\\`e}que d{'}analyseurs syntaxiques du fran{\\c{c}}ais : confrontation de mod{\\`e}les distributionnels sur un corpus sp{\\'e}cialis{\\'e} (Extrinsic evaluation of {F}rench dependency parsers on a specialised corpus : comparison of distributional thesauri ),2019,-1,-1,3,0.271773,10279,ludovic tanguy,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume I : Articles longs,0,"Nous pr{\'e}sentons une {\'e}tude visant {\`a} comparer 11 diff{\'e}rents analyseurs en d{\'e}pendances du fran{\c{c}}ais sur un corpus sp{\'e}cialis{\'e} (constitu{\'e} des archives des articles de la conf{\'e}rence TALN). En l{'}absence de gold standard, nous utilisons chacune des sorties de ces analyseurs pour construire des th{\'e}saurus distributionnels en utilisant une m{\'e}thode {\`a} base de fr{\'e}quence. Nous comparons ces 11 th{\'e}saurus afin de proposer un premier aper{\c{c}}u de l{'}impact du choix d{'}un analyseur par rapport {\`a} un autre."
W18-5622,Evaluation of a Sequence Tagging Tool for Biomedical Texts,2018,0,2,3,1,18601,julien tourille,Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis,0,"Many applications in biomedical natural language processing rely on sequence tagging as an initial step to perform more complex analysis. To support text analysis in the biomedical domain, we introduce Yet Another SEquence Tagger (YASET), an open-source multi purpose sequence tagger that implements state-of-the-art deep learning algorithms for sequence tagging. Herein, we evaluate YASET on part-of-speech tagging and named entity recognition in a variety of text genres including articles from the biomedical literature in English and clinical narratives in French. To further characterize performance, we report distributions over 30 runs and different sizes of training datasets. YASET provides state-of-the-art performance on the CoNLL 2003 NER dataset (F1=0.87), MEDPOST corpus (F1=0.97), MERLoT corpus (F1=0.99) and NCBI disease corpus (F1=0.81). We believe that YASET is a versatile and efficient tool that can be used for sequence tagging in biomedical and clinical texts."
P18-2056,Using pseudo-senses for improving the extraction of synonyms from word embeddings,2018,0,0,1,1,5589,olivier ferret,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"The methods proposed recently for specializing word embeddings according to a particular perspective generally rely on external knowledge. In this article, we propose Pseudofit, a new method for specializing word embeddings according to semantic similarity without any external knowledge. Pseudofit exploits the notion of pseudo-sense for building several representations for each word and uses these representations for making the initial embeddings more generic. We illustrate the interest of Pseudofit for acquiring synonyms and study several variants of Pseudofit according to this perspective."
2018.jeptalnrecital-long.10,Int{\\'e}gration de contexte global par amor{\\c{c}}age pour la d{\\'e}tection d{'}{\\'e}v{\\'e}nements (Integrating global context via bootstrapping for event detection),2018,-1,-1,3,0,18573,dorian kodelja,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"Les approches neuronales obtiennent depuis plusieurs ann{\'e}es des r{\'e}sultats int{\'e}ressants en extraction d{'}{\'e}v{\'e}nements. Cependant, les approches d{\'e}velopp{\'e}es dans ce cadre se limitent g{\'e}n{\'e}ralement {\`a} un contexte phrastique. Or, si certains types d{'}{\'e}v{\'e}nements sont ais{\'e}ment identifiables {\`a} ce niveau, l{'}exploitation d{'}indices pr{\'e}sents dans d{'}autres phrases est parfois n{\'e}cessaire pour permettre de d{\'e}sambigu{\""\i}ser des {\'e}v{\'e}nements. Dans cet article, nous proposons ainsi l{'}int{\'e}gration d{'}une repr{\'e}sentation d{'}un contexte plus large pour am{\'e}liorer l{'}apprentissage d{'}un r{\'e}seau convolutif. Cette repr{\'e}sentation est obtenue par amor{\c{c}}age en exploitant les r{\'e}sultats d{'}un premier mod{\`e}le convolutif op{\'e}rant au niveau phrastique. Dans le cadre d{'}une {\'e}valuation r{\'e}alis{\'e}e sur les donn{\'e}es de la campagne TAC 2017, nous montrons que ce mod{\`e}le global obtient un gain significatif par rapport au mod{\`e}le local, ces deux mod{\`e}les {\'e}tant eux-m{\^e}mes comp{\'e}titifs par rapport aux r{\'e}sultats de TAC 2017. Nous {\'e}tudions {\'e}galement en d{\'e}tail le gain de performance de notre nouveau mod{\`e}le au travers de plusieurs exp{\'e}riences compl{\'e}mentaires."
2018.jeptalnrecital-court.17,"Utilisation de Repr{\\'e}sentations Distribu{\\'e}es de Relations pour la D{\\'e}sambigu{\\\\\i}sation d{'}Entit{\\'e}s Nomm{\\'e}es (Exploiting Relation Embeddings to Improve Entity Linking )""",2018,-1,-1,3,0,30992,nicolas wagner,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"L{'}identification des entit{\'e}s nomm{\'e}es dans un texte est une {\'e}tape fondamentale pour de nombreuses t{\^a}ches d{'}extraction d{'}information. Pour avoir une identification compl{\`e}te, une {\'e}tape de d{\'e}sambigu{\""\i}sation des entit{\'e}s similaires doit {\^e}tre r{\'e}alis{\'e}e. Celle-ci s{'}appuie souvent sur la seule description textuelle des entit{\'e}s. Or, les bases de connaissances contiennent des informations plus riches, sous la forme de relations entre les entit{\'e}s : cette information peut {\'e}galement {\^e}tre exploit{\'e}e pour am{\'e}liorer la d{\'e}sambigu{\""\i}sation des entit{\'e}s. Nous proposons dans cet article une approche d{'}apprentissage de repr{\'e}sentations distribu{\'e}es de ces relations et leur utilisation pour la t{\^a}che de d{\'e}sambigu{\""\i}sation d{'}entit{\'e}s nomm{\'e}es. Nous montrons le gain de cette m{\'e}thode sur un corpus d{'}{\'e}valuation standard, en anglais, issu de la t{\^a}che de d{\'e}sambigu{\""\i}sation d{'}entit{\'e}s de la campagne TAC-KBP."
2018.jeptalnrecital-court.19,Des pseudo-sens pour am{\\'e}liorer l{'}extraction de synonymes {\\`a} partir de plongements lexicaux (Pseudo-senses for improving the extraction of synonyms from word embeddings),2018,-1,-1,1,1,5589,olivier ferret,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"Au-del{\`a} des mod{\`e}les destin{\'e}s {\`a} construire des plongements lexicaux {\`a} partir de corpus, des m{\'e}thodes de sp{\'e}cialisation de ces repr{\'e}sentations selon diff{\'e}rentes orientations ont {\'e}t{\'e} propos{\'e}es. Une part importante d{'}entre elles repose sur l{'}utilisation de connaissances externes. Dans cet article, nous proposons Pseudofit, une nouvelle m{\'e}thode de sp{\'e}cialisation de plongements lexicaux focalis{\'e}e sur la similarit{\'e} s{\'e}mantique et op{\'e}rant sans connaissances externes. Pseudofit s{'}appuie sur la notion de pseudo-sens afin d{'}obtenir plusieurs repr{\'e}sentations pour un m{\^e}me mot et utilise cette pluralit{\'e} pour rendre plus g{\'e}n{\'e}riques les plongements initiaux. Nous illustrons l{'}int{\'e}r{\^e}t de Pseudofit pour l{'}extraction de synonymes et nous explorons dans ce cadre diff{\'e}rentes variantes visant {\`a} en am{\'e}liorer les r{\'e}sultats."
W17-4211,Unsupervised Event Clustering and Aggregation from Newswire and Web Articles,2017,11,1,2,0,31706,swen ribeiro,Proceedings of the 2017 {EMNLP} Workshop: Natural Language Processing meets Journalism,0,"In this paper, we present an unsupervised pipeline approach for clustering news articles based on identified event instances in their content. We leverage press agency newswire and monolingual word alignment techniques to build meaningful and linguistically varied clusters of articles from the web in the perspective of a broader event type detection task. We validate our approach on a manually annotated corpus of Web articles."
S17-2098,{LIMSI}-{COT} at {S}em{E}val-2017 Task 12: Neural Architecture for Temporal Information Extraction from Clinical Narratives,2017,0,5,2,1,18601,julien tourille,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"In this paper we present our participation to SemEval 2017 Task 12. We used a neural network based approach for entity and temporal relation extraction, and experimented with two domain adaptation strategies. We achieved competitive performance for both tasks."
P17-2035,Neural Architecture for Temporal Relation Extraction: A {B}i-{LSTM} Approach for Detecting Narrative Containers,2017,20,23,2,1,18601,julien tourille,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We present a neural architecture for containment relation identification between medical events and/or temporal expressions. We experiment on a corpus of de-identified clinical notes in English from the Mayo Clinic, namely the THYME corpus. Our model achieves an F-measure of 0.613 and outperforms the best result reported on this corpus to date."
I17-2035,Taking into account Inter-sentence Similarity for Update Summarization,2017,19,2,3,1,32866,maali mnasri,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Following Gillick and Favre (2009), a lot of work about extractive summarization has modeled this task by associating two contrary constraints: one aims at maximizing the coverage of the summary with respect to its information content while the other represents its size limit. In this context, the notion of redundancy is only implicitly taken into account. In this article, we extend the framework defined by Gillick and Favre (2009) by examining how and to what extent integrating semantic sentence similarity into an update summarization system can improve its results. We show more precisely the impact of this strategy through evaluations performed on DUC 2007 and TAC 2008 and 2009 datasets."
I17-1028,Turning Distributional Thesauri into Word Vectors for Synonym Extraction and Expansion,2017,34,4,1,1,5589,olivier ferret,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"In this article, we propose to investigate a new problem consisting in turning a distributional thesaurus into dense word vectors. We propose more precisely a method for performing such task by associating graph embedding and distributed representation adaptation. We have applied and evaluated it for English nouns at a large scale about its ability to retrieve synonyms. In this context, we have also illustrated the interest of the developed method for three different tasks: the improvement of already existing word embeddings, the fusion of heterogeneous representations and the expansion of synsets."
E17-2117,Temporal information extraction from clinical text,2017,15,5,2,1,18601,julien tourille,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"In this paper, we present a method for temporal relation extraction from clinical narratives in French and in English. We experiment on two comparable corpora, the MERLOT corpus and the THYME corpus, and show that a common approach can be used for both languages."
2017.jeptalnrecital-long.7,Construire des repr{\\'e}sentations denses {\\`a} partir de th{\\'e}saurus distributionnels (Distributional Thesaurus Embedding and its Applications),2017,-1,-1,1,1,5589,olivier ferret,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 - Articles longs,0,"Dans cet article, nous nous int{\'e}ressons {\`a} un nouveau probl{\`e}me, appel{\'e} plongement de th{\'e}saurus, consistant {\`a} transformer un th{\'e}saurus distributionnel en une repr{\'e}sentation dense de mots. Nous proposons de traiter ce probl{\`e}me par une m{\'e}thode fond{\'e}e sur l{'}association d{'}un plongement de graphe et de l{'}injection de relations dans des repr{\'e}sentations denses. Nous avons appliqu{\'e} et {\'e}valu{\'e} cette m{\'e}thode pour un large ensemble de noms en anglais et montr{\'e} que les repr{\'e}sentations denses produites obtiennent de meilleures performances, selon une {\'e}valuation intrins{\`e}que, que les repr{\'e}sentations denses construites selon les m{\'e}thodes de l{'}{\'e}tat de l{'}art sur le m{\^e}me corpus. Nous illustrons aussi l{'}int{\'e}r{\^e}t de la m{\'e}thode d{\'e}velopp{\'e}e pour am{\'e}liorer les repr{\'e}sentations denses existantes {\`a} la fois de fa{\c{c}}on endog{\`e}ne et exog{\`e}ne."
S16-1175,{LIMSI}-{COT} at {S}em{E}val-2016 Task 12: Temporal relation identification using a pipeline of classifiers,2016,9,1,2,1,18601,julien tourille,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"SemEval 2016 Task 12 addresses temporal reasoning in the clinical domain. In this paper, we present our participation for relation extraction based on gold standard entities (subtasks DR and CR). We used a supervised approach comparing plain lexical features to word embeddings for temporal relation identification, and obtained above-median scores."
L16-1307,A Dataset for Open Event Extraction in {E}nglish,2016,13,4,3,1,6565,kiemhieu nguyen,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This article presents a corpus for development and testing of event schema induction systems in English. Schema induction is the task of learning templates with no supervision from unlabeled texts, and to group together entities corresponding to the same role in a template. Most of the previous work on this subject relies on the MUC-4 corpus. We describe the limits of using this corpus (size, non-representativeness, similarity of roles across templates) and propose a new, partially-annotated corpus in English which remedies some of these shortcomings. We make use of Wikinews to select the data inside the category Laws {\&} Justice, and query Google search engine to retrieve different documents on the same events. Only Wikinews documents are manually annotated and can be used for evaluation, while the others can be used for unsupervised learning. We detail the methodology used for building the corpus and evaluate some existing systems on this new data."
2016.jeptalnrecital-poster.19,Extraction de relations temporelles dans des dossiers {\\'e}lectroniques patient (Extracting Temporal Relations from Electronic Health Records),2016,-1,-1,2,1,18601,julien tourille,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters),0,"L{'}analyse temporelle des documents cliniques permet d{'}obtenir des repr{\'e}sentations riches des informations contenues dans les dossiers {\'e}lectroniques patient. Cette analyse repose sur l{'}extraction d{'}{\'e}v{\'e}nements, d{'}expressions temporelles et des relations entre eux. Dans ce travail, nous consid{\'e}rons que nous disposons des {\'e}v{\'e}nements et des expressions temporelles pertinents et nous nous int{\'e}ressons aux relations temporelles entre deux {\'e}v{\'e}nements ou entre un {\'e}v{\'e}nement et une expression temporelle. Nous pr{\'e}sentons des mod{\`e}les de classification supervis{\'e}e pour l{'}extraction de des relations en fran{\c{c}}ais et en anglais. Les performances obtenues sont comparables dans les deux langues, sugg{\'e}rant ainsi que diff{\'e}rents domaines cliniques et diff{\'e}rentes langues pourraient {\^e}tre abord{\'e}s de mani{\`e}re similaire."
2016.jeptalnrecital-poster.22,Int{\\'e}gration de la similarit{\\'e} entre phrases comme crit{\\`e}re pour le r{\\'e}sum{\\'e} multi-document (Integrating sentence similarity as a constraint for multi-document summarization),2016,-1,-1,3,1,32866,maali mnasri,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters),0,"multi-document Ma{\^a}li Mnasri1, 2 Ga{\""e}l de Chalendar1 Olivier Ferret1 (1) CEA, LIST, Laboratoire Vision et Ing{\'e}nierie des Contenus, Gif-sur-Yvette, F-91191, France. (2) Universit{\'e} Paris-Sud, Universit{\'e} Paris-Saclay, F-91405 Orsay, France. maali.mnasri@cea.fr, gael.de-chalendar@cea.fr, olivier.ferret@cea.fr R {\'E}SUM{\'E} {\`A} la suite des travaux de Gillick {\&} Favre (2009), beaucoup de travaux portant sur le r{\'e}sum{\'e} par extraction se sont appuy{\'e}s sur une mod{\'e}lisation de cette t{\^a}che sous la forme de deux contraintes antagonistes : l{'}une vise {\`a} maximiser la couverture du r{\'e}sum{\'e} produit par rapport au contenu des textes d{'}origine tandis que l{'}autre repr{\'e}sente la limite du r{\'e}sum{\'e} en termes de taille. Dans cette approche, la notion de redondance n{'}est prise en compte que de fa{\c{c}}on implicite. Dans cet article, nous reprenons le cadre d{\'e}fini par Gillick {\&} Favre (2009) mais nous examinons comment et dans quelle mesure la prise en compte explicite de la similarit{\'e} s{\'e}mantique des phrases peut am{\'e}liorer les performances d{'}un syst{\`e}me de r{\'e}sum{\'e} multi-document. Nous v{\'e}rifions cet impact par des {\'e}valuations men{\'e}es sur les corpus DUC 2003 et 2004."
2016.jeptalnrecital-long.22,"Utilisation des relations d{'}une base de connaissances pour la d{\\'e}sambigu{\\\\\i}sation d{'}entit{\\'e}s nomm{\\'e}es (Using the Relations of a Knowledge Base to Improve Entity Linking )""",2016,-1,-1,3,1,5626,romaric besanccon,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Articles longs),0,"L{'}identification des entit{\'e}s nomm{\'e}es dans un texte est une t{\^a}che essentielle des outils d{'}extraction d{'}information dans de nombreuses applications. Cette identification passe par la reconnaissance d{'}une mention d{'}entit{\'e} dans le texte, ce qui a {\'e}t{\'e} tr{\`e}s largement {\'e}tudi{\'e}, et par l{'}association des entit{\'e}s reconnues {\`a} des entit{\'e}s connues, pr{\'e}sentes dans une base de connaissances. Cette association repose souvent sur une mesure de similarit{\'e} entre le contexte textuel de la mention de l{'}entit{\'e} et un contexte textuel de description des entit{\'e}s de la base de connaissances. Or, ce contexte de description n{'}est en g{\'e}n{\'e}ral pas pr{\'e}sent pour toutes les entit{\'e}s. Nous proposons d{'}exploiter les relations de la base de connaissances pour ajouter un indice de d{\'e}sambigu{\""\i}sation pour ces entit{\'e}s. Nous {\'e}valuons notre travail sur des corpus d{'}{\'e}valuation standards en anglais issus de la t{\^a}che de d{\'e}sambigu{\""\i}sation d{'}entit{\'e}s de la campagne TAC-KBP."
P15-2077,Early and Late Combinations of Criteria for Reranking Distributional Thesauri,2015,29,1,1,1,5589,olivier ferret,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"In this article, we first propose to exploit a new criterion for improving distributional thesauri. Following a bootstrapping perspective, we select relations between the terms of similar nominal compounds for building in an unsupervised way the training set of a classifier performing the reranking of a thesaurus. Then, we evaluate several ways to combine thesauri reranked according to different criteria and show that exploiting the complementary information brought by these criteria leads to significant improvements."
P15-1019,Generative Event Schema Induction with Entity Disambiguation,2015,31,26,3,1,6565,kiemhieu nguyen,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"This paper presents a generative model to event schema induction. Previous methods in the literature only use head words to represent entities. However, elements other than head words contain useful information. For instance, an armed man is more discriminative than man. Our model takes into account this information and precisely represents it using probabilistic topic distributions. We illustrate that such information plays an important role in parameter estimation. Mostly, it makes topic distributions more coherent and more discriminative. Experimental results on benchmark dataset empirically confirm this enhancement."
2015.jeptalnrecital-long.7,"D{\\'e}sambigu{\\\\\i}sation d{'}entit{\\'e}s pour l{'}induction non supervis{\\'e}e de sch{\\'e}mas {\\'e}v{\\'e}nementiels""",2015,-1,-1,3,1,6565,kiemhieu nguyen,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article pr{\'e}sente un mod{\`e}le g{\'e}n{\'e}ratif pour l{'}induction non supervis{\'e}e d{'}{\'e}v{\'e}nements. Les pr{\'e}c{\'e}dentes m{\'e}thodes de la litt{\'e}rature utilisent uniquement les t{\^e}tes des syntagmes pour repr{\'e}senter les entit{\'e}s. Pourtant, le groupe complet (par exemple, {''}un homme arm{\'e}{''}) apporte une information plus discriminante (que {''}homme{''}). Notre mod{\`e}le tient compte de cette information et la repr{\'e}sente dans la distribution des sch{\'e}mas d{'}{\'e}v{\'e}nements. Nous montrons que ces relations jouent un r{\^o}le important dans l{'}estimation des param{\`e}tres, et qu{'}elles conduisent {\`a} des distributions plus coh{\'e}rentes et plus discriminantes. Les r{\'e}sultats exp{\'e}rimentaux sur le corpus de MUC-4 confirment ces progr{\`e}s."
2015.jeptalnrecital-long.13,D{\\'e}classer les voisins non s{\\'e}mantiques pour am{\\'e}liorer les th{\\'e}saurus distributionnels,2015,-1,-1,1,1,5589,olivier ferret,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"La plupart des m{\'e}thodes d{'}am{\'e}lioration des th{\'e}saurus distributionnels se focalisent sur les moyens {--} repr{\'e}sentations ou mesures de similarit{\'e} {--} de mieux d{\'e}tecter la similarit{\'e} s{\'e}mantique entre les mots. Dans cet article, nous proposons un point de vue inverse : nous cherchons {\`a} d{\'e}tecter les voisins s{\'e}mantiques associ{\'e}s {\`a} une entr{\'e}e les moins susceptibles d{'}{\^e}tre li{\'e}s s{\'e}mantiquement {\`a} elle et nous utilisons cette information pour r{\'e}ordonner ces voisins. Pour d{\'e}tecter les faux voisins s{\'e}mantiques d{'}une entr{\'e}e, nous adoptons une approche s{'}inspirant de la d{\'e}sambigu{\""\i}sation s{\'e}mantique en construisant un classifieur permettant de diff{\'e}rencier en contexte cette entr{\'e}e des autres mots. Ce classifieur est ensuite appliqu{\'e} {\`a} un {\'e}chantillon des occurrences des voisins de l{'}entr{\'e}e pour rep{\'e}rer ceux les plus {\'e}loign{\'e}s de l{'}entr{\'e}e. Nous {\'e}valuons cette m{\'e}thode pour des th{\'e}saurus construits {\`a} partir de cooccurrents syntaxiques et nous montrons l{'}int{\'e}r{\^e}t de la combiner avec les m{\'e}thodes d{\'e}crites dans (Ferret, 2013b) selon une strat{\'e}gie de type vote."
W14-6603,Using a generic neural model for lexical substitution (Utiliser un mod{\\`e}le neuronal g{\\'e}n{\\'e}rique pour la substitution lexicale) [in {F}rench],2014,0,0,1,1,5589,olivier ferret,{TALN}-{RECITAL} 2014 Workshop {S}em{D}is 2014 : Enjeux actuels de la s{\\'e}mantique distributionnelle ({S}em{D}is 2014: Current Challenges in Distributional Semantics),0,None
garcia-fernandez-etal-2014-evaluation,Evaluation of different strategies for domain adaptation in opinion mining,2014,12,2,2,0.909091,39320,anne garciafernandez,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The work presented in this article takes place in the field of opinion mining and aims more particularly at finding the polarity of a text by relying on machine learning methods. In this context, it focuses on studying various strategies for adapting a statistical classifier to a new domain when training data only exist for one or several other domains. This study shows more precisely that a self-training procedure consisting in enlarging the initial training corpus with texts from the target domain that were reliably classified by the classifier is the most successful and stable strategy for the tested domains. Moreover, this strategy gets better results in most cases than (Blitzer et al., 2007){'}s method on the same evaluation corpus while it is more simple."
F14-1003,Event Role Labelling using a Neural Network Model ({\\'E}tiquetage en r{\\^o}les {\\'e}v{\\'e}nementiels fond{\\'e} sur l{'}utilisation d{'}un mod{\\`e}le neuronal) [in {F}rench],2014,-1,-1,3,0,40021,emanuela borocs,Proceedings of TALN 2014 (Volume 1: Long Papers),0,None
F14-1020,Exploring the neighbor graph to improve distributional thesauri (Explorer le graphe de voisinage pour am{\\'e}liorer les th{\\'e}saurus distributionnels) [in {F}rench],2014,-1,-1,3,0,5590,vincent claveau,Proceedings of TALN 2014 (Volume 1: Long Papers),0,None
D14-1199,Event Role Extraction using Domain-Relevant Word Representations,2014,35,2,3,0,40021,emanuela borocs,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"The efficiency of Information Extraction systems is known to be heavily influenced by domain-specific knowledge but the cost of developing such systems is considerably high. In this article, we consider the problem of event extraction and show that learning word representations from unlabeled domain-specific data and using them for representing event roles enable to outperform previous state-of-the-art event extraction models on the MUC-4 data set."
C14-1067,Improving distributional thesauri by exploring the graph of neighbors,2014,35,6,3,0,5590,vincent claveau,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"In this paper, we address the issue of building and improving a distributional thesaurus. We first show that existing tools from the information retrieval domain can be directly used in order to build a thesaurus with state-of-the-art performance. Secondly, we focus more specifically on improving the obtained thesaurus, seen as a graph of k-nearest neighbors. By exploiting information about the neighborhood contained in this graph, we propose several contributions. 1) We show how the lists of neighbors can be globally improved by examining the reciprocity of the neighboring relation, that is, the fact that a word can be close of another and vice-versa. 2) We also propose a method to associate a confidence score to any lists of nearest neighbors (i.e. any entry of the thesaurus). 3) Last, we demonstrate how these confidence scores can be used to reorder the closest neighbors of a word. These different contributions are validated through experiments and offer significant improvement over the state-of-the-art."
P13-1055,Identifying Bad Semantic Neighbors for Improving Distributional Thesauri,2013,27,11,1,1,5589,olivier ferret,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Distributional thesauri are now widely used in a large number of Natural Language Processing tasks. However, they are far from containing only interesting semantic relations. As a consequence, improving such thesaurus is an important issue that is mainly tackled indirectly through the improvement of semantic similarity measures. In this article, we propose a more direct approach focusing on the identification of the neighbors of a thesaurus entry that are not semantically linked to this entry. This identification relies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts. Its bad neighbors are found by applying this classifier to a representative set of occurrences of each of these neighbors. We evaluate the interest of this method for a large set of English nouns with various frequencies."
F13-1004,Unsupervised selection of semantic relations for improving a distributional thesaurus (S{\\'e}lection non supervis{\\'e}e de relations s{\\'e}mantiques pour am{\\'e}liorer un th{\\'e}saurus distributionnel) [in {F}rench],2013,0,0,1,1,5589,olivier ferret,Proceedings of TALN 2013 (Volume 1: Long Papers),0,None
F13-1026,Semantic relation clustering for unsupervised information extraction (Regroupement s{\\'e}mantique de relations pour l{'}extraction d{'}information non supervis{\\'e}e) [in {F}rench],2013,0,0,3,0.477128,4596,wei wang,Proceedings of TALN 2013 (Volume 1: Long Papers),0,None
wang-etal-2012-evaluation,Evaluation of Unsupervised Information Extraction,2012,13,4,3,0.56498,4596,wei wang,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Unsupervised methods gain more and more attention nowadays in information extraction area, which allows to design more open extraction systems. In the domain of unsupervised information extraction, clustering methods are of particular importance. However, evaluating the results of clustering remains difficult at a large scale, especially in the absence of reliable reference. On the basis of our experiments on unsupervised relation extraction, we first discuss in this article how to evaluate clustering quality without a reference by relying on internal measures. Then we propose a method, supported by a dedicated annotation tool, for building a set of reference clusters of relations from a corpus. Moreover, we apply it to our experimental framework and illustrate in this way how to build a significant reference for unsupervised relation extraction, more precisely made of 80 clusters gathering more than 4,000 relation instances, in a short time. Finally, we present how such reference is exploited for the evaluation of clustering with external measures and analyze the results of the application of these measures to the clusters of relations produced by our unsupervised relation extraction system."
besancon-etal-2012-evaluation,Evaluation of a Complex Information Extraction Application in Specific Domain,2012,18,2,2,1,5626,romaric besanccon,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Operational intelligence applications in specific domains are developed using numerous natural language processing technologies and tools. A challenge for this integration is to take into account the limitations of each of these technologies in the global evaluation of the application. We present in this article a complex intelligence application for the gathering of information from the Web about recent seismic events. We present the different components needed for the development of such system, including Information Extraction, Filtering and Clustering, and the technologies behind each component. We also propose an independent evaluation of each component and an insight of their influence in the overall performance of the system."
F12-2003,Une m{\\'e}thode d{'}extraction d{'}information fond{\\'e}e sur les graphes pour le remplissage de formulaires (A Graph-Based Method for Template Filling in Information Extraction) [in {F}rench],2012,-1,-1,3,1,34825,ludovic jeanlouis,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
F12-2032,Etude de diff{\\'e}rentes strat{\\'e}gies d{'}adaptation {\\`a} un nouveau domaine en fouille d{'}opinion (Study of various strategies for adapting an opinion classifier to a new domain) [in {F}rench],2012,0,0,2,0.909091,39320,anne garciafernandez,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
I11-1081,Text Segmentation and Graph-based Method for Template Filling in Information Extraction,2011,20,12,3,1,34825,ludovic jeanlouis,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"In event-based Information Extraction systems, a major task is the automated filling from unstructured texts of a template gathering information related to a particular event. Such template filling may be a hard task when the information is scattered throughout the text and mixed with similar pieces of information relative to a different event. We propose in this paper a twostep approach for template filling: first, an event-based segmentation is performed to select the parts of the text related to the target event; then, a graph-based method is applied to choose the most relevant entities in these parts for characterizing the event. An evaluation of this model based on an annotated corpus for earthquake events shows that we achieve a 77% F1-measure for the template-filling task."
2011.jeptalnrecital-long.4,Une approche faiblement supervis{\\'e}e pour l{'}extraction de relations {\\`a} large {\\'e}chelle (A weakly supervised approach to large scale relation extraction),2011,-1,-1,3,1,34825,ludovic jeanlouis,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Les syst{\`e}mes d{'}extraction d{'}information traditionnels se focalisent sur un domaine sp{\'e}cifique et un nombre limit{\'e} de relations. Les travaux r{\'e}cents dans ce domaine ont cependant vu {\'e}merger la probl{\'e}matique des syst{\`e}mes d{'}extraction d{'}information {\`a} large {\'e}chelle. {\`A} l{'}instar des syst{\`e}mes de question-r{\'e}ponse en domaine ouvert, ces syst{\`e}mes se caract{\'e}risent {\`a} la fois par le traitement d{'}un grand nombre de relations et par une absence de restriction quant aux domaines abord{\'e}s. Dans cet article, nous pr{\'e}sentons un syst{\`e}me d{'}extraction d{'}information {\`a} large {\'e}chelle fond{\'e} sur un apprentissage faiblement supervis{\'e} de patrons d{'}extraction de relations. Cet apprentissage repose sur la donn{\'e}e de couples d{'}entit{\'e}s en relation dont la projection dans un corpus de r{\'e}f{\'e}rence permet de constituer la base d{'}exemples de relations support de l{'}induction des patrons d{'}extraction. Nous pr{\'e}sentons {\'e}galement les r{\'e}sultats de l{'}application de cette approche dans le cadre d{'}{\'e}valuation d{\'e}fini par la t{\^a}che KBP de l{'}{\'e}valuation TAC 2010."
2011.jeptalnrecital-court.8,Filtrage de relations pour l{'}extraction d{'}information non supervis{\\'e}e (Filtering relations for unsupervised information extraction),2011,-1,-1,3,0.56498,4596,wei wang,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Le domaine de l{'}extraction d{'}information s{'}est r{\'e}cemment d{\'e}velopp{\'e} en limitant les contraintes sur la d{\'e}finition des informations {\`a} extraire, ouvrant la voie {\`a} des applications de veille plus ouvertes. Dans ce contexte de l{'}extraction d{'}information non supervis{\'e}e, nous nous int{\'e}ressons {\`a} l{'}identification et la caract{\'e}risation de nouvelles relations entre des types d{'}entit{\'e}s fix{\'e}s. Un des d{\'e}fis de cette t{\^a}che est de faire face {\`a} la masse importante de candidats pour ces relations lorsque l{'}on consid{\`e}re des corpus de grande taille. Nous pr{\'e}sentons dans cet article une approche pour le filtrage des relations combinant m{\'e}thode heuristique et m{\'e}thode par apprentissage. Nous {\'e}valuons ce filtrage de mani{\`e}re intrins{\`e}que et par son impact sur un regroupement s{\'e}mantique des relations."
2011.jeptalnrecital-court.26,Utiliser l{'}amor{\\c{c}}age pour am{\\'e}liorer une mesure de similarit{\\'e} s{\\'e}mantique (Using bootstrapping to improve a measure of semantic similarity),2011,-1,-1,1,1,5589,olivier ferret,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Les travaux sur les mesures de similarit{\'e} s{\'e}mantique de nature distributionnelle ont abouti {\`a} un certain consensus quant {\`a} leurs performances et ont montr{\'e} notamment que leurs r{\'e}sultats sont surtout int{\'e}ressants pour des mots de forte fr{\'e}quence et une similarit{\'e} s{\'e}mantique {\'e}tendue, non restreinte aux seuls synonymes. Dans cet article, nous proposons une m{\'e}thode d{'}am{\'e}lioration d{'}une mesure de similarit{\'e} classique permettant de r{\'e}{\'e}quilibrer ses r{\'e}sultats pour les mots de plus faible fr{\'e}quence. Cette m{\'e}thode est fond{\'e}e sur un m{\'e}canisme d{'}amor{\c{c}}age : un ensemble d{'}exemples et de contre-exemples de mots s{\'e}mantiquement li{\'e}s sont s{\'e}lectionn{\'e}s de fa{\c{c}}on non supervis{\'e}e {\`a} partir des r{\'e}sultats de la mesure initiale et servent {\`a} l{'}entra{\^\i}nement d{'}un classifieur supervis{\'e}. Celui-ci est ensuite utilis{\'e} pour r{\'e}ordonner les voisins s{\'e}mantiques initiaux. Nous {\'e}valuons l{'}int{\'e}r{\^e}t de ce r{\'e}ordonnancement pour un large ensemble de noms anglais couvrant diff{\'e}rents domaines fr{\'e}quentiels."
grappy-etal-2010-corpus,A Corpus for Studying Full Answer Justification,2010,7,6,3,0,42529,arnaud grappy,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Question answering (QA) systems aim at retrieving precise information from a large collection of documents. To be considered as reliable by users, a QA system must provide elements to evaluate the answer. This notion of answer justification can also be useful when developping a QA system in order to give criteria for selecting correct answers. An answer justification can be found in a sentence, a passage made of several consecutive sentences or several passages of a document or several documents. Thus, we are interesting in pinpointing the set of information that allows to verify the correctness of the answer in a candidate passage and the question elements that are missing in this passage. Moreover, the relevant information is often given in texts in a different form from the question form: anaphora, paraphrases, synonyms. In order to have a better idea of the importance of all the phenomena we underlined, and to provide enough examples at the QA developer's disposal to study them, we decided to build an annotated corpus."
besancon-etal-2010-lima,{LIMA} : A Multilingual Framework for Linguistic Analysis and Linguistic Resources Development and Evaluation,2010,17,31,3,1,5626,romaric besanccon,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The increasing amount of available textual information makes necessary the use of Natural Language Processing (NLP) tools. These tools have to be used on large collections of documents in different languages. But NLP is a complex task that relies on many processes and resources. As a consequence, NLP tools must be both configurable and efficient: specific software architectures must be designed for this purpose. We present in this paper the LIMA multilingual analysis platform, developed at CEA LIST. This configurable platform has been designed to develop NLP based industrial applications while keeping enough flexibility to integrate various processes and resources. This design makes LIMA a linguistic analyzer that can handle languages as different as French, English, German, Arabic or Chinese. Beyond its architecture principles and its capabilities as a linguistic analyzer, LIMA also offers a set of tools dedicated to the test and the evaluation of linguistic modules and to the production and the management of new linguistic resources."
ferret-2010-testing,Testing Semantic Similarity Measures for Extracting Synonyms from a Corpus,2010,18,23,1,1,5589,olivier ferret,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The definition of lexical semantic similarity measures has been the subject of lots of works for many years. In this article, we focus more specifically on distributional semantic similarity measures. Although several evaluations of this kind of measures were already achieved for determining if they actually catch semantic relatedness, it is still difficult to determine if a measure that performs well in an evaluation framework can be applied more widely with the same success. In the work we present here, we first select a semantic similarity measure by testing a large set of such measures against the WordNet-based Synonymy Test, an extended TOEFL test proposed in (Freitag et al., 2005), and we show that its accuracy is comparable to the accuracy of the best state of the art measures while it has less demanding requirements. Then, we apply this measure for extracting automatically synonyms from a corpus and we evaluate the relevance of this process against two reference resources, WordNet and the Moby thesaurus. Finally, we compare our results in details to those of (Curran and Moens, 2002)."
2010.jeptalnrecital-long.4,Similarit{\\'e} s{\\'e}mantique et extraction de synonymes {\\`a} partir de corpus,2010,16,5,1,1,5589,olivier ferret,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"La d{\'e}finition de mesures s{\'e}mantiques au niveau lexical a fait l{'}objet de nombreux travaux depuis plusieurs ann{\'e}es. Dans cet article, nous nous focalisons plus sp{\'e}cifiquement sur les mesures de nature distributionnelle. Bien que diff{\'e}rentes {\'e}valuations ont {\'e}t{\'e} r{\'e}alis{\'e}es les concernant, il reste difficile {\`a} {\'e}tablir si une mesure donnant de bons r{\'e}sultats dans un cadre d{'}{\'e}valuation peut {\^e}tre appliqu{\'e}e plus largement avec le m{\^e}me succ{\`e}s. Dans le travail pr{\'e}sent{\'e}, nous commen{\c{c}}ons par s{\'e}lectionner une mesure de similarit{\'e} sur la base d{'}un test de type TOEFL {\'e}tendu. Nous l{'}appliquons ensuite au probl{\`e}me de l{'}extraction de synonymes {\`a} partir de corpus en comparant nos r{\'e}sultats avec ceux de (Curran {\&} Moens, 2002). Enfin, nous testons l{'}int{\'e}r{\^e}t pour cette t{\^a}che d{'}extraction de synonymes d{'}une m{\'e}thode d{'}am{\'e}lioration de la qualit{\'e} des donn{\'e}es distributionnelles propos{\'e}e dans (Zhitomirsky-Geffet {\&} Dagan, 2009)."
2010.jeptalnrecital-long.24,Utilisation d{'}indices temporels pour la segmentation {\\'e}v{\\'e}nementielle de textes,2010,13,0,3,1,34825,ludovic jeanlouis,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans le domaine de l{'}Extraction d{'}Information, une place importante est faite {\`a} l{'}extraction d{'}{\'e}v{\'e}nements dans des d{\'e}p{\^e}ches d{'}actualit{\'e}, particuli{\`e}rement justifi{\'e}e dans le contexte d{'}applications de veille. Or il est fr{\'e}quent qu{'}une d{\'e}p{\^e}che d{'}actualit{\'e} {\'e}voque plusieurs {\'e}v{\'e}nements de m{\^e}me nature pour les comparer. Nous proposons dans cet article d{'}{\'e}tudier des m{\'e}thodes pour segmenter les textes en s{\'e}parant les {\'e}v{\'e}nements, dans le but de faciliter le rattachement des informations pertinentes {\`a} l{'}{\'e}v{\'e}nement principal. L{'}id{\'e}e est d{'}utiliser des mod{\`e}les d{'}apprentissage statistique exploitant les marqueurs temporels pr{\'e}sents dans les textes pour faire cette segmentation. Nous pr{\'e}sentons plus pr{\'e}cis{\'e}ment deux mod{\`e}les (HMM et CRF) entra{\^\i}n{\'e}s pour cette t{\^a}che et, en faisant une {\'e}valuation de ces mod{\`e}les sur un corpus de d{\'e}p{\^e}ches traitant d{'}{\'e}v{\'e}nements sismiques, nous montrons que les m{\'e}thodes propos{\'e}es permettent d{'}obtenir des r{\'e}sultats au moins aussi bons que ceux d{'}une approche ad hoc, avec une approche beaucoup plus g{\'e}n{\'e}rique."
2010.jeptalnrecital-court.30,Adapter un syst{\\`e}me de question-r{\\'e}ponse en domaine ouvert au domaine m{\\'e}dical,2010,-1,-1,2,1,36099,mehdi embarek,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Dans cet article, nous pr{\'e}sentons Esculape, un syst{\`e}me de question-r{\'e}ponse en fran{\c{c}}ais d{\'e}di{\'e} aux m{\'e}decins g{\'e}n{\'e}ralistes et {\'e}labor{\'e} {\`a} partir d{'}OEdipe, un syst{\`e}me de question-r{\'e}ponse en domaine ouvert. Esculape ajoute {\`a} OEdipe la capacit{\'e} d{'}exploiter la structure d{'}un mod{\`e}le du domaine, le domaine m{\'e}dical dans le cas pr{\'e}sent. Malgr{\'e} l{'}existence d{'}un grand nombre de ressources dans ce domaine (UMLS, MeSH ...), il n{'}est pas possible de se reposer enti{\`e}rement sur ces ressources, et plus sp{\'e}cifiquement sur les relations qu{'}elles abritent, pour r{\'e}pondre aux questions. Nous montrons comment surmonter cette difficult{\'e} en apprenant de fa{\c{c}}on supervis{\'e}e des patrons linguistiques d{'}extraction de relations et en les appliquant {\`a} l{'}extraction de r{\'e}ponses."
R09-1017,Improving Text Segmentation by Combining Endogenous and Exogenous Methods,2009,21,7,1,1,5589,olivier ferret,Proceedings of the International Conference {RANLP}-2009,0,"Topic segmentation was addressed by a large amount of work from which it is not easy to draw conclusions, especially about the need for knowledge. In this article, we propose to combine in the same framework two methods for improving the results of a topic segmenter based on lexical reiteration. The first one is endogenous and exploits the distributional similarity of words in a document for discovering its topics. These topics are then used to facilitate the detection of topical similarity between discourse units. The second approach achieves the same goal by relying on external resources. Two resources are tested: a network of lexical co-occurrences built from a large corpus and a set of word senses induced from this network. An evaluation of the two approaches and their combination is performed in a reference framework and shows the interest of this combination both for French and English."
2009.jeptalnrecital-court.46,Utiliser des sens de mots pour la segmentation th{\\'e}matique ?,2009,-1,-1,1,1,5589,olivier ferret,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"La segmentation th{\'e}matique est un domaine de l{'}analyse discursive ayant donn{\'e} lieu {\`a} de nombreux travaux s{'}appuyant sur la notion de coh{\'e}sion lexicale. La plupart d{'}entre eux n{'}exploitent que la simple r{\'e}currence lexicale mais quelques uns ont n{\'e}anmoins explor{\'e} l{'}usage de connaissances rendant compte de cette coh{\'e}sion lexicale. Celles-ci prennent g{\'e}n{\'e}ralement la forme de r{\'e}seaux lexicaux, soit construits automatiquement {\`a} partir de corpus, soit issus de dictionnaires {\'e}labor{\'e}s manuellement. Dans cet article, nous examinons dans quelle mesure une ressource d{'}une nature un peu diff{\'e}rente peut {\^e}tre utilis{\'e}e pour caract{\'e}riser la coh{\'e}sion lexicale des textes. Il s{'}agit en l{'}occurrence de sens de mots induits automatiquement {\`a} partir de corpus, {\`a} l{'}instar de ceux produits par la t{\^a}che Word Sense Induction and Discrimination  de l{'}{\'e}valuation SemEval 2007. Ce type de ressources apporte une structuration des r{\'e}seaux lexicaux au niveau s{\'e}mantique dont nous {\'e}valuons l{'}apport pour la segmentation th{\'e}matique."
embarek-ferret-2008-learning,Learning Patterns for Building Resources about Semantic Relations in the Medical Domain,2008,17,29,2,1,36099,mehdi embarek,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In this article, we present a method for extracting automatically semantic relations from texts in the medical domain using linguistic patterns. These patterns refer to three levels of information about words: inflected form, lemma and part-of-speech. The method we present consists first in identifying the entities that are part of the relations to extract, that is to say diseases, exams, treatments, drugs or symptoms. Thereafter, sentences that contain couples of entities are extracted and the presence of a semantic relation is validated by applying linguistic patterns. These patterns were previously learnt automatically from a manually annotated corpus by relying onan algorithm based on the edit distance. We first report the results of an evaluation of our medical entity tagger for the five types of entities we have mentioned above and then, more globally, the results of an evaluation of our extraction method for four relations between these entities. Both evaluations were done for French."
P07-1061,Finding document topics for improving topic segmentation,2007,20,15,1,1,5589,olivier ferret,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"Topic segmentation and identification are often tackled as separate problems whereas they are both part of topic analysis. In this article, we study how topic identification can help to improve a topic segmenter based on word reiteration. We first present an unsupervised method for discovering the topics of a text. Then, we detail how these topics are used by segmentation for finding topical similarities between text segments. Finally, we show through the results of an evaluation done both for French and English the interest of the method we propose."
2007.jeptalnrecital-long.3,Une exp{\\'e}rience d{'}extraction de relations s{\\'e}mantiques {\\`a} partir de textes dans le domaine m{\\'e}dical,2007,-1,-1,2,1,36099,mehdi embarek,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans cet article, nous pr{\'e}sentons une m{\'e}thode permettant d{'}extraire {\`a} partir de textes des relations s{\'e}mantiques dans le domaine m{\'e}dical en utilisant des patrons linguistiques. La premi{\`e}re partie de cette m{\'e}thode consiste {\`a} identifier les entit{\'e}s entre lesquelles les relations vis{\'e}es interviennent, en l{'}occurrence les maladies, les examens, les m{\'e}dicaments et les sympt{\^o}mes. La pr{\'e}sence d{'}une des relations s{\'e}mantiques vis{\'e}es dans les phrases contenant un couple de ces entit{\'e}s est ensuite valid{\'e}e par l{'}application de patrons linguistiques pr{\'e}alablement appris de mani{\`e}re automatique {\`a} partir d{'}un corpus annot{\'e}. Nous rendons compte de l{'}{\'e}valuation de cette m{\'e}thode sur un corpus en Fran{\c{c}}ais pour quatre relations."
P06-1036,Enhancing Electronic Dictionaries with an Index Based on Associations,2006,21,21,1,1,5589,olivier ferret,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"A good dictionary contains not only many entries and a lot of information concerning each one of them, but also adequate means to reveal the stored information. Information access depends crucially on the quality of the index. We will present here some ideas of how a dictionary could be enhanced to support a speaker/writer to find the word s/he is looking for. To this end we suggest to add to an existing electronic resource an index based on the notion of association. We will also present preliminary work of how a subset of such associations, for example, topical associations, can be acquired by filtering a network of lexical co-occurrences extracted from a corpus."
ferret-2006-building,Building a network of topical relations from a corpus,2006,19,6,1,1,5589,olivier ferret,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Lexical networks such as WordNet are known to have a lack of topical relations although these relations are very useful for tasks such as text summarization or information extraction. In this article, we present a method for automatically building from a large corpus a lexical network whose relations are preferably topical ones. As it does not rely on resources such as dictionaries, this method is based on self-bootstrapping: a network of lexical cooccurrences is first built from a corpus and then, is filtered by using the words of the corpus that are selected by the initial network. We report an evaluation about topic segmentation showing that the results got with the filtered network are the same as the results got with the initial network although the first one is significantly smaller than the second one."
C04-1194,Discovering word senses from a network of lexical cooccurrences,2004,14,29,1,1,5589,olivier ferret,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Lexico-semantic networks such as WordNet have been criticized about the nature of the senses they distinguish as well as on the way they define these senses. In this article, we present a possible solution to overcome these limits by defining the sense of words from the way they are used. More precisely, we propose to differentiate the senses of a word from a network of lexical cooccurrences built from a large corpus. This method was tested both for French and English and was evaluated for English by comparing its results with WordNet."
2004.jeptalnrecital-long.5,D{\\'e}couvrir des sens de mots {\\`a} partir d{'}un r{\\'e}seau de cooccurrences lexicales,2004,-1,-1,1,1,5589,olivier ferret,Actes de la 11{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Les r{\'e}seaux lexico-s{\'e}mantiques de type WordNet ont fait l{'}objet de nombreuses critiques concernant la nature des sens qu{'}ils distinguent ainsi que la fa{\c{c}}on dont ils caract{\'e}risent ces distinctions de sens. Cet article pr{\'e}sente une solution possible {\`a} ces limites, solution consistant {\`a} d{\'e}finir les sens des mots {\`a} partir de leur usage. Plus pr{\'e}cis{\'e}ment, il propose de diff{\'e}rencier les sens d{'}un mot {\`a} partir d{'}un r{\'e}seau de cooccurrences lexicales construit sur la base d{'}un large corpus. Cette m{\'e}thode a {\'e}t{\'e} test{\'e}e {\`a} la fois pour le fran{\c{c}}ais et pour l{'}anglais et a fait l{'}objet dans ce dernier cas d{'}une premi{\`e}re {\'e}valuation par comparaison avec WordNet."
2003.jeptalnrecital-poster.9,Filtrage th{\\'e}matique d{'}un r{\\'e}seau de collocations,2003,-1,-1,1,1,5589,olivier ferret,Actes de la 10{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Les r{\'e}seaux lexicaux de type WordNet pr{\'e}sentent une absence de relations de nature th{\'e}matique, relations pourtant tr{\`e}s utiles dans des t{\^a}ches telles que le r{\'e}sum{\'e} automatique ou l{'}extraction d{'}information. Dans cet article, nous proposons une m{\'e}thode visant {\`a} construire automatiquement {\`a} partir d{'}un large corpus un r{\'e}seau lexical dont les relations sont pr{\'e}f{\'e}rentiellement th{\'e}matiques. En l{'}absence d{'}utilisation de ressources de type dictionnaire, cette m{\'e}thode se fonde sur un principe d{'}auto-amor{\c{c}}age : un r{\'e}seau de collocations est d{'}abord construit {\`a} partir d{'}un corpus puis filtr{\'e} sur la base des mots du corpus que le r{\'e}seau initial a permis de s{\'e}lectionner. Nous montrons au travers d{'}une {\'e}valuation portant sur la segmentation th{\'e}matique que le r{\'e}seau final, bien que de taille bien inf{\'e}rieure au r{\'e}seau initial, permet d{'}obtenir les m{\^e}mes performances que celui-ci pour cette t{\^a}che."
2003.jeptalnrecital-long.9,Confronter des sources de connaissances diff{\\'e}rentes pour obtenir une r{\\'e}ponse plus fiable,2003,-1,-1,3,0.6,16752,gael chalendar,Actes de la 10{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"La fiabilit{\'e} des r{\'e}ponses qu{'}il propose, ou un moyen de l{'}estimer, est le meilleur atout d{'}un syst{\`e}me de question-r{\'e}ponse. A cette fin, nous avons choisi d{'}effectuer des recherches dans des ensembles de documents diff{\'e}rents et de privil{\'e}gier des r{\'e}sultats qui sont trouv{\'e}s dans ces diff{\'e}rentes sources. Ainsi, le syst{\`e}me QALC travaille {\`a} la fois sur une collection finie d{'}articles de journaux et sur le Web."
ferret-etal-2002-building,Building domain specific lexical hierarchies from corpora,2002,16,2,1,1,5589,olivier ferret,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"In this article, we present a new algorithm for building domain specific lexical hierarchies from texts. The basic elements of such a hierarchy are the normalized terms - mono and multi-word terms - extracted from a large corpus by a terminological extractor. The algorithm relies on collocations for representing the meaning of these terms, finding hierarchical relations between them and finally, organizing them into a hierarchy. Moreover, it takes into account the polysemy of terms while it builds the hierarchy. We also present the results of its application on a part of the corpus designed for the ARC A3 of the Francil network and we go through its possible applications."
C02-1033,Using Collocations for Topic Segmentation and Link Detection,2002,16,54,1,1,5589,olivier ferret,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"We present in this paper a method for achieving in an integrated way two tasks of topic analysis: segmentation and link detection. This method combines word repetition and the lexical cohesion stated by a collocation network to compensate for the respective weaknesses of the two approaches. We report an evaluation of our method for segmentation on two corpora, one in French and one in English, and we propose an evaluation measure that specifically suits that kind of systems."
2002.jeptalnrecital-long.13,Segmenter et structurer th{\\'e}matiquement des textes par l{'}utilisation conjointe de collocations et de la r{\\'e}currence lexicale,2002,14,3,1,1,5589,olivier ferret,Actes de la 9{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,Nous exposons dans cet article une m{\'e}thode r{\'e}alisant de fa{\c{c}}on int{\'e}gr{\'e}e deux t{\^a}ches de l{'}analyse th{\'e}matique : la segmentation et la d{\'e}tection de liens th{\'e}matiques. Cette m{\'e}thode exploite conjointement la r{\'e}currence des mots dans les textes et les liens issus d{'}un r{\'e}seau de collocations afin de compenser les faiblesses respectives des deux approches. Nous pr{\'e}sentons son {\'e}valuation concernant la segmentation sur un corpus en fran{\c{c}}ais et un corpus en anglais et nous proposons une mesure d{'}{\'e}valuation sp{\'e}cifiquement adapt{\'e}e {\`a} ce type de syst{\`e}mes.
2002.jeptalnrecital-long.28,Recherche de la r{\\'e}ponse fond{\\'e}e sur la reconnaissance du focus de la question,2002,-1,-1,1,1,5589,olivier ferret,Actes de la 9{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Le syst{\`e}me de question-r{\'e}ponse QALC utilise les documents s{\'e}lectionn{\'e}s par un moteur de recherche pour la question pos{\'e}e, les s{\'e}pare en phrases afin de comparer chaque phrase avec la question, puis localise la r{\'e}ponse soit en d{\'e}tectant l{'}entit{\'e} nomm{\'e}e recherch{\'e}e, soit en appliquant des patrons syntaxiques d{'}extraction de la r{\'e}ponse, sortes de sch{\'e}mas fig{\'e}s de r{\'e}ponse pour un type donn{\'e} de question. Les patrons d{'}extraction que nous avons d{\'e}finis se fondent sur la notion de focus, qui est l{'}{\'e}l{\'e}ment important de la question, celui qui devra se trouver dans la phrase r{\'e}ponse. Dans cet article, nous d{\'e}crirons comment nous d{\'e}terminons le focus dans la question, puis comment nous l{'}utilisons dans l{'}appariement question-phrase et pour la localisation de la r{\'e}ponse dans les phrases les plus pertinentes retenues."
W01-1207,Terminological Variants for Document Selection and Question/Answer Matching,2001,10,11,1,1,5589,olivier ferret,Proceedings of the {ACL} 2001 Workshop on Open-Domain Question Answering,0,"Answering precise questions requires applying Natural Language techniques in order to locate the answers inside retrieved documents. The QALC system, presented in this paper, participated to the Question Answering track of the TREC8 and TREC9 evaluations. QALC exploits an analysis of documents based on the search for multi-word terms and their variations. These indexes are used to select a minimal number of documents to be processed and to give indices when comparing question and sentence representations. This comparison also takes advantage of a question analysis module and recognition of numeric and named entities in the documents."
W01-0909,A Cross-Comparison of Two Clustering Methods,2001,11,1,3,0,52095,michele jardino,Proceedings of the {ACL} 2001 Workshop on Evaluation Methodologies for Language and Dialogue Systems,0,"Many Natural Language Processing applications require semantic knowledge about topics in order to be possible or to be efficient. So we developed a system, SEGAPSITH, that acquires it automatically from text segments by using an unsupervised and incremental clustering method. In such an approach, an important problem consists of the validation of the learned classes. To do that, we applied another clustering method, that only needs to know the number of classes to build, on the same subset of text segments and we reformulate our evaluation problem in comparing the two classifications. So, we established different criteria to compare them, based either on the words as class descriptors or on the thematic units. Our first results lead to show a great correlation between the two classifications."
2001.jeptalnrecital-long.13,Utilisation des entit{\\'e}s nomm{\\'e}es et des variantes terminologiques dans un syst{\\`e}me de question-r{\\'e}ponse,2001,-1,-1,1,1,5589,olivier ferret,Actes de la 8{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous pr{\'e}sentons dans cet article le syst{\`e}me QALC qui a particip{\'e} {\`a} la t{\^a}che Question Answering de la conf{\'e}rence d{'}{\'e}valuation TREC. Ce syst{\`e}me repose sur un ensemble de modules de Traitement Automatique des Langues (TAL) intervenant essentiellement en aval d{'}un moteur de recherche op{\'e}rant sur un vaste ensemble de documents : typage des questions, reconnaissance des entit{\'e}s nomm{\'e}es, extraction et reconnaissance de termes, simples et complexes, et de leurs variantes. Ces traitements permettent soit de mieux s{\'e}lectionner ces documents, soit de d{\'e}cider quelles sont les phrases susceptibles de contenir la r{\'e}ponse {\`a} une question."
2001.jeptalnrecital-long.14,Rep{\\'e}rage de structures th{\\'e}matiques dans des textes,2001,-1,-1,1,1,5589,olivier ferret,Actes de la 8{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Afin d{'}am{\'e}liorer les performances des syst{\`e}mes de r{\'e}sum{\'e} automatique ou de filtrage s{\'e}mantique concernant la prise en charge de la coh{\'e}rence th{\'e}matique, nous proposons un mod{\`e}le faisant collaborer une m{\'e}thode d{'}analyse statistique qui identifie les ruptures th{\'e}matiques avec un syst{\`e}me d{'}analyse linguistique qui identifie les cadres de discours."
P98-2243,How to Thematically Segment Texts by using Lexical Cohesion?,1998,5,5,1,1,5589,olivier ferret,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,This article outlines a quantitative method for segmenting texts into thematically coherent units. This method relies on a network of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words. We also present the results of an experiment about locating boundaries between a series of concatened texts.
P98-1065,Thematic Segmentation of Texts: Two Methods for Two Kind of Texts,1998,8,15,1,1,5589,olivier ferret,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"To segment texts in thematic units, we present here how a basic principle relying on word distribution can be applied on different kind of texts. We start from an existing method well adapted for scientific texts, and we propose its adaptation to other kinds of texts by using semantic links between words. These relations are found in a lexical network, automatically built from a large corpus. We will compare their results and give criteria to choose the more suitable method according to text characteristics."
C98-2238,How to thematically segment texts by using lexical cohesion?,1998,5,5,1,1,5589,olivier ferret,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,This article outlines a quantitative method for segmenting texts into thematically coherent units. This method relies on a network of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words. We also present the results of an experiment about locating boundaries between a series of concatened texts.
C98-1062,Thematic segmentation of texts: two methods for two kinds of texts,1998,8,15,1,1,5589,olivier ferret,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"To segment texts in thematic units, we present here how a basic principle relying on word distribution can be applied on different kind of texts. We start from an existing method well adapted for scientific texts, and we propose its adaptation to other kinds of texts by using semantic links between words. These relations are found in a lexical network, automatically built from a large corpus. We will compare their results and give criteria to choose the more suitable method according to text characteristics."
