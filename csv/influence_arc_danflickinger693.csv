1995.tmi-1.2,J94-4004,0,0.160124,"ppo et al&apos;s encoding of a neoDavidsonian semantics by use of proto-roles is not significant. We show a linearized equivalent of their encoding of the semantics for nadando8 and the corresponding MRS representation below: [e2][while(e2, el) ∧ nadar(e2) ∧ p-agt-cause-move-manner(e2, x1) ∧ P(e1)} A simplified form of the MRS representations for (15a) and (15b) are shown in Figure 6. We assume that strict intransitive manner of motion verbs such as swim are related 7 For some other approaches to this mismatch problem see, for example, Isabelle et al, 1988; Beaven, 1992; Nirenburg and Levin, 1992; Dorr, 1994. 8 We have omitted the classification of events which Sanfilippo et al adopt — this is an important part of the analysis but is not relevant to the comparison with MRS since we could implement it in exactly the same way, by typing event variables. Apart from this, we have essentially preserved the analysis, but we do not reproduce the original FS here, since it is multiply nested, due to the use of a binary and and over 20 lines of text. 27 Figure 6: Simplified MRS representations for Kim swam across the river and Kim cruzó el río nadando 28 to entries subcategorized for a PP expressing a pat"
1995.tmi-1.2,C88-1053,0,0.0220099,". For current purposes, the distinction between MRS and Sanfilippo et al&apos;s encoding of a neoDavidsonian semantics by use of proto-roles is not significant. We show a linearized equivalent of their encoding of the semantics for nadando8 and the corresponding MRS representation below: [e2][while(e2, el) ∧ nadar(e2) ∧ p-agt-cause-move-manner(e2, x1) ∧ P(e1)} A simplified form of the MRS representations for (15a) and (15b) are shown in Figure 6. We assume that strict intransitive manner of motion verbs such as swim are related 7 For some other approaches to this mismatch problem see, for example, Isabelle et al, 1988; Beaven, 1992; Nirenburg and Levin, 1992; Dorr, 1994. 8 We have omitted the classification of events which Sanfilippo et al adopt — this is an important part of the analysis but is not relevant to the comparison with MRS since we could implement it in exactly the same way, by typing event variables. Apart from this, we have essentially preserved the analysis, but we do not reproduce the original FS here, since it is multiply nested, due to the use of a binary and and over 20 lines of text. 27 Figure 6: Simplified MRS representations for Kim swam across the river and Kim cruzó el río nadando 2"
1995.tmi-1.2,P95-1035,0,0.0506489,"ose with the appropriate coindexation. The advantage of this approach is that the transfer component contains no information about the monolingual grammar, since it merely relates existing lexical entries, and that the problem of LF equivalence is to a large extent circumvented, because the transfer component only indicates the relation of the lexical signs by coindexation. One major disadvantage of Shake-and-Bake as originally described is lack of efficiency, since the generator/parser has to consider a number of possibilities which is factorial in the number of signs in the target sentence. Poznanski et al (1995) show 16 that polynomial complexity in Shake-and-Bake generation can be achieved by including extra information from the target language grammar that constrains the possibilities tried by the generator. Although this potentially dilutes the original ideal of independent grammars, it still has advantages over alternative approaches since it could be made clear that this is control information. Even so, Shake-and-Bake (and related systems such as those described by Sanfilippo et al (1992) and Trujillo (1995)) have other disadvantages inherent to strongly lexicalist approaches. Representing phras"
1995.tmi-1.2,1992.tmi-1.1,1,0.513766,"r has to consider a number of possibilities which is factorial in the number of signs in the target sentence. Poznanski et al (1995) show 16 that polynomial complexity in Shake-and-Bake generation can be achieved by including extra information from the target language grammar that constrains the possibilities tried by the generator. Although this potentially dilutes the original ideal of independent grammars, it still has advantages over alternative approaches since it could be made clear that this is control information. Even so, Shake-and-Bake (and related systems such as those described by Sanfilippo et al (1992) and Trujillo (1995)) have other disadvantages inherent to strongly lexicalist approaches. Representing phrasal equivalences is cumbersome, since they have to be stated in terms of sets of monolingual entries. The requirement that grammars be absolutely lexicalist restricts the frameworks for which it is appropriate and is even in conflict with recent proposals within HPSG (Sag 1995). As we will illustrate in §3.1, some translation equivalences cannot be encoded simply by relating indices in a conventional semantic framework, and therefore cannot be treated without expansion and complication o"
1995.tmi-1.2,J93-1008,0,0.0225423,"essions. For simplicity, in the examples in this paper we will take the object language to be predicate calculus, but MRS is intended to be compatible with DRT. The advantages of allowing various types of semantic underspecification for translation purposes are well-known (see e.g. Alshawi et al (1991), Kay et al (1994)), so here we concentrate on the advantages of flatness or minimal structure in a semantic representation language. The problem of ensuring that a grammar can generate from a particular semantic representation is well-known: it has been discussed in the context of generation by Shieber (1993) and in machine translation by Landsbergen (1987) and Whitelock (1992) 15 among others. One difficulty is the problem of logical form equivalence: even though the grammar may accept a logical form (LF) logically equivalent to a particular LF which is input to the generator, there is no guarantee that it will generate from that syntactic form of the input LF. To take a trivial example, an English grammar might naturally produce the logical form in (la) from fierce black cat, while a straightforward transfer from the natural Spanish representation of gato negro y feroz shown in (1b) would produc"
1995.tmi-1.2,C92-2117,0,0.583056,"the object language to be predicate calculus, but MRS is intended to be compatible with DRT. The advantages of allowing various types of semantic underspecification for translation purposes are well-known (see e.g. Alshawi et al (1991), Kay et al (1994)), so here we concentrate on the advantages of flatness or minimal structure in a semantic representation language. The problem of ensuring that a grammar can generate from a particular semantic representation is well-known: it has been discussed in the context of generation by Shieber (1993) and in machine translation by Landsbergen (1987) and Whitelock (1992) 15 among others. One difficulty is the problem of logical form equivalence: even though the grammar may accept a logical form (LF) logically equivalent to a particular LF which is input to the generator, there is no guarantee that it will generate from that syntactic form of the input LF. To take a trivial example, an English grammar might naturally produce the logical form in (la) from fierce black cat, while a straightforward transfer from the natural Spanish representation of gato negro y feroz shown in (1b) would produce the LF in ( 1 c ) , which the English grammar probably would not all"
2004.tmi-1.2,P92-1005,0,0.0634629,"nent communication is in terms of sets of MRSs and, thus, can easily be managed in a distributed and (potentially) parallel client – server set-up. Both the analysis and generation grammars ‘publish’ their interface to transfer—i.e. the inventory and synopsis of semantic predicates— in the form of a Semantic Interface specification (‘SEM-I’), such that transfer can operate without knowledge about grammar internals. In practical terms SEM-Is are an important 1 In this respect, MRS is closely related to a tradition of underspecified semantics reflected in, among others, Quasi-Logical Form (QLF; Alshawi & Crouch, 1992), Underspecified Discourse Representation Theory (UDRT; Reyle, 1993), Hole Semantics (Bos, 1995), and the Constraint Language for Lambda Structures (CLLS; Egg, Koller, & Niehren, 2001). development tool (facilitating wellformedness testing of interface representations at all levels), but they also have interesting theoretical status with regard to transfer. The SEM-Is for the Norwegian analysis and English generation grammars, respectively, provide an exhaustive enumeration of legitimate semantic predicates (i.e. the transfer vocabulary) and ‘terms of use’, i.e. for each predicate its set of a"
2004.tmi-1.2,C96-1023,0,0.0187876,"o multiple phases and optionally apply output filters upon the completion of each phase. The LOGON transfer grammar, for example, includes two sets of language-specific MTRs to accommodate grammar-specific idiosyncrasies before and after the core transfer phase, in some cases simply suppressing superfluous information (e.g. predicates introduced by selected-for prepositions and some aspectual markers), in others re-arranging or augmenting semantics to facilitate English generation. Transfer outputs incorporating plural mass nouns, for example, require the insertion of a suitable ‘classifier’ (Bond, Ogura, & Ikehara, 1996), in order to generate, say, two pieces of information instead of the ungrammatical ∗ two informations. temp loc at p temp in p temp temp abstr on p temp afternoon n day n ··· year n Figure 4: Excerpt from predicate hierarchies provided by English SEM-I. Temporal, directional, and other usages of prepositions give rise to distinct, but potentially related, semantic predicates. Likewise, the SEM-I incorporates some ontological information, e.g. a classification of temporal entities, though crucially only to the extent that is actually grammaticized in the language proper. create a non-determin"
2004.tmi-1.2,1995.tmi-1.2,1,0.818746,"Missing"
2004.tmi-1.2,P00-1061,0,0.0112111,"Missing"
2005.eamt-1.27,2004.tmi-1.2,1,0.743699,"Missing"
2005.eamt-1.27,C02-2025,1,0.798577,"y size remains limited to a minimum breadth (Flickinger, Nerbonne, Sag, & Wasow, 1987). Test suites typically include some degree of linguistic and extra-linguistic annotation. In the case of LOGON, these consist primarily of an internal break-down by linguistic phenomena and the determination of the intended or preferred reading(s) per test item, as with grammars of relatively broad coverage some ambiguities are inevitable. The latter annotation, in turn, is part of our regular treebanking (or MRS banking) of LOGON test suites and domain corpora (in the spirit of the LinGO Redwoods approach; Oepen et al., 2002). It is not only needed to maintain training material for stochastic processes (like parse and realization ranking), but at the same time makes regression testing a lot more concise. A general ability to track all changes in system outputs or intermediate analyses is very useful already, but the added information about which hypotheses are actually in-focus clearly facilitates more targeted diagnostics. The two main test suites used so far exemplify aspects of semantic composition (e.g. variation in complementation and linking, modiﬁcation, quantiﬁcation, scopal relations, et al.) and various"
2005.eamt-1.27,2003.mtsummit-systems.14,0,0.0263188,"Missing"
2005.mtsummit-papers.22,W02-1503,1,0.809591,"ationship between the two formalisms in general, while our task goes beyond that. We need to interrelate speciﬁc f-structures and MRS representations which are not only well-formed, but which also satisfy further, mutually independent constraints. In the ﬁrst place, already the fact that f-structures are syntactic representations and MRSs semantic representations designed to capture translational relations frequently motivates diﬀerent packagings of information on the two levels. Furthermore, the NorGram f-structures meet the requirements for f-structures developed within the ParGram project (Butt, Dyvik, King, Masuichi, & Rohrer, 2002; Dyvik, 2003), while the NorGram MRS representations are constructed according to the same general principles as the MRS representations of the target ERG grammar. As a result the f-structure and MRS analyses of the same sentence are not always in a simple structural correspondence with each other. One example is nominal phrases with several speciﬁers in the f-structure, and phrases with no speciﬁers (Norwegian bare singulars), contrasted with the MRS requirement that a variable must always be bound by one single quantiﬁer; other examples involve diﬀerent dominance relations among predicates"
2005.mtsummit-papers.22,1995.tmi-1.2,1,0.840886,"ally grounded component of each grammar, capturing several classes of lexical regularities while also serving the crucial engineering function of supplying a reliable and complete speciﬁcation of the elementary predications the grammar can realize. We make extensive use of underspeciﬁcation and type hierarchies to maximize generality and precision. 1 Introduction In this paper we introduce two interesting features of the Norwegian-to-English machine translation system LOGON. (1) It is the ﬁrst system to use the full power of Minimal Recursion Semantics in translation (originally introduced by Copestake, Flickinger, Malouf, Riehemann, & Sag, 1995). (2) The transfer modules use the SEM-I, an interface speciﬁcation designed to allow the use of deep grammars in various applications without knowledge of the grammar internals (Copestake & Flickinger, 2003). The motivation for the SEM-I is threefold. First, it allows the semantic representation to be underspeciﬁed. In the LOGON system, if the analysis system does not have enough information to commit to an interpretation then the ambiguity is retained. Doing this by naively expanding all interpretations is so ineﬃcient as to be unworkable. Second, it exposes only the information that is rel"
2005.mtsummit-papers.22,P97-1052,0,0.0715544,"Missing"
2005.mtsummit-papers.22,1997.tmi-1.18,0,0.00828586,"ular count {IND +} nouns, we will generate for example She wrote a good paper but not *She wrote good paper. The underspeciﬁed ﬁrst entry above will be useful in translating between English and, say, German, where in both languages one word can be used for both senses, and where for some sentences no disambiguation is required, as in That paper is good. This approach diﬀers from many semantic-transfer based systems, particularly knowledge-based MT. In these systems lexical semantic information (including word senses, semantic classes and selectional preferences) is used to disambiguate input (Mahesh et al., 1997; Ikehara et al., 1996). The interface between transfer and generation is then a single fully speciﬁed semantic representation (although often with scope issues ignored), rather than an underspeciﬁed one. The SEM-I is compatible with such an approach — it would then link the grammars to the ontology of lexical semantic information and provide the input to the sense disambiguation module. 5.2 Productive Derivation Languages typically include productive derivational processes which result in multiple words whose syntactic and semantic properties are related but distinct. One such common process"
2005.mtsummit-papers.22,2004.tmi-1.2,1,0.832796,"Missing"
2006.eamt-1.16,W97-1502,0,0.035145,"d its analyses provide precise, fine-grained syntactic and semantic information; Minimal Recursion Semantics (MRS; Copestake, Flickinger, Pollard, & Sag, To appear) is the general framework used for meaning representation. Building on an array of existing software tools for processing with the erg (and similar grammars), the Redwoods Treebank was constructed by parsing selected domain corpora and subsequently hand-inspecting analyses and selecting the intended reading(s) for each input item. Annotation (i.e. manual parse selection) in Redwoods builds on the notion of elementary discriminants (Carter, 1997), basic properties of sub-constituents in the parse forest that account for contrasts (i.e. local sources of ambiguity) among analyses. Discriminants—competing lexical entries, for example, or a choice of using the head – complement vs. head – adjunct schema to build a token phrase—are fairly easy to judge, even for non-experts, and enable annotators to navigate the parse forest quickly. Using a specialized tool, each annotator decision on accepting or rejecting a discriminant directly results in the elimination of large parts of the parse forest, so that a small number of local decisions typi"
2006.eamt-1.16,C02-2025,1,0.833963,"ng an explicit mapping between the annotations and the phenomena. Applying this method to the 90,000-word English section of the development corpus used in the LOGON Norwegian-English machine translation project (Lønning et al., 2004) results in a linguistic profile which should highlight a number of development opportunities for the project’s deep grammar resources, and thereby improve the end-to-end performance of the demonstrator system. 2 Redwoods Treebank and the ERG Developed initially within the LinGO (Linguistic Grammars Online) laboratory at Stanford University, the Redwoods Treebank Oepen et al., 2002 is a treebank comprised entirely of analyses derived from a broad-coverage computational grammar, the LinGO English Resource Grammar (erg; Flickinger, 2000). The erg is a large-scale hpsg implementation, actively developed at Stanford since 1993, and its analyses provide precise, fine-grained syntactic and semantic information; Minimal Recursion Semantics (MRS; Copestake, Flickinger, Pollard, & Sag, To appear) is the general framework used for meaning representation. Building on an array of existing software tools for processing with the erg (and similar grammars), the Redwoods Treebank was c"
2007.tmi-papers.18,I05-1015,1,0.851038,"as a feature in the model. TRANSFER METRICS Two additional features capture information about the transfer step: the total number of transfer rules that were invoked (as a measure of transfer granularity, e.g. where idiomatic transfer of a larger cluster of EPs contrasts with stepwise transfer of component EPs), as well as the ratio of EP counts, |E|/|F |. SEMANTIC DISTANCE Generation proceeds in two phases: a chart-based bottom-up search enumerates candidate realizations, of which a final semantic compatiblity test selects the one(s) whose MRS is subsumed by the original generator input MRS (Carroll & Oepen, 2005). Given an imperfect input (or error in the generation grammar), it is possible for none of the candidate outputs to fulfill the semantic compatiblity test. In this case, the generator will gradually relax MRS com150 parison, going through seven pre-defined levels of semantic mismatch, which we encode as one integer-valued feature in the re-ranking model. Training the Model While batch translating, the LOGON controller records all candidate translations, intermediate semantic representations, and a large number of processing and resource consumption properties in a database, which we call a pr"
2007.tmi-papers.18,W97-1502,0,0.0125593,"on to the 109 items that translate in both configurations, our BLEU score over the first translation drops from 37.41 to 30.29.1 MRS { prpstn m[MARG recommend v] recommend v[ARG1 pron, ARG2 hike n] a q[ARG0 hike n] around p[ARG1 hike n, ARG2 source n] implicit q[ARG0 source n] poss[ARG1 waterway n, ARG2 source n] def q[ARG0 waterway n] } Figure 4: Variable-free reduction of the MRS for the utterance ‘We recommend a hike around the waterway’s sources’. ing set; this could be done automatically. The second approach is motivated by the hypothesis that discriminants, as used in manual annotation (Carter, 1997), represent promising alternative feature functions to the predefined templates. Initial tests (see table 2) show that the discriminant approach (which is not yet used in the LOGON system) scores better than the templatebased approach. 5 Ranking Transfer Outputs While MRS formulae are highly structured graphs, Oepen & Lønning (2006) suggest a reduction into a variable-free form that resembles elementary dependency structures. For the ranking of transfer outputs, MRSs are broken down into basic dependency triples, whose probabilities are estimated by adaptation of standard n-gram sequence model"
2007.tmi-papers.18,P99-1069,0,0.0438269,"nd a given feature can even itself be a separate statistical model. In the following we first give a brief high-level presentation of conditional log-linear modeling, and then we go on to present the actual feature functions in our setup. Given a set of m real-valued features, each pair of source sentence f and target sentence e are represented as a feature vector Φ(f, e) ∈ ℜm . A vector of weights λ ∈ ℜm is then fitted to optimize some objective function of the training data. For the experiments reported in this paper the weights are fitted to maximize the conditional (or pseudo) likelihood (Johnson, Geman, Canon, Chi, & Riezler, 1999).3 In other words, for each input source sentence in the training data we seek to maximize 3 For estimation we use the TADM open-source toolkit (Malouf, 2002), using its limited-memory variable metric as the optimization method. As is standard practice, the model is regularized by including a zero-mean Gaussian prior on the feature weights to reduce the risk of overfitting. 149 the probability of its annotated reference translation relative to the other competing candidates. However, for future work we plan to also experiment with optimizing the scores of a given evaluation metric (e.g. BLEU)"
2007.tmi-papers.18,koen-2004-pharaoh,0,0.0252225,"ise word-to-word probabilities. STRING PROBABILITY Although a part of the (conditional) realization ranker already, we include the string probability (according to the tri4 Of these, 9,410 sentences are taken from the LOGON development data, while an additional 12,946 sentences are from the English-Norwegian Parallel Corpus (Oksefjell, 1999). 5 The ML estimation of the lexical probabilities, as well as the final word alignments produced from the output of GIZA++ , are carried out using the training scripts provided by Phillip Koehn, and as distributed with the phrase-based SMT module Pharaoh (Koehn, 2004). gram language model trained on the BNC) of candidate translations ek as an independent indicator of output fluency. DISTORTION Elementary predications (EPs) in our MRS are linked to corresponding surface elements, i.e. sub-string pointers. Surface links are preserved in transfer, such that post-generation, for each EP—or group of EPs, as transfer need not be a one-to-one mapping—there is information about its original vs. its output sub-string span. To gauge reordering among constituents, for both the generator input and output, each EP is compared pairwise to other EPs in the same MRS, and"
2007.tmi-papers.18,W98-1426,0,0.119221,"Missing"
2007.tmi-papers.18,W02-2018,0,0.0150677,"resent the actual feature functions in our setup. Given a set of m real-valued features, each pair of source sentence f and target sentence e are represented as a feature vector Φ(f, e) ∈ ℜm . A vector of weights λ ∈ ℜm is then fitted to optimize some objective function of the training data. For the experiments reported in this paper the weights are fitted to maximize the conditional (or pseudo) likelihood (Johnson, Geman, Canon, Chi, & Riezler, 1999).3 In other words, for each input source sentence in the training data we seek to maximize 3 For estimation we use the TADM open-source toolkit (Malouf, 2002), using its limited-memory variable metric as the optimization method. As is standard practice, the model is regularized by including a zero-mean Gaussian prior on the feature weights to reduce the risk of overfitting. 149 the probability of its annotated reference translation relative to the other competing candidates. However, for future work we plan to also experiment with optimizing the scores of a given evaluation metric (e.g. BLEU) directly, following the Minimum Error Rate approach of Och (2003). The three most fundamental features that are supplied in our log-linear re-ranker correspon"
2007.tmi-papers.18,P03-1021,0,0.0280051,"e training data we seek to maximize 3 For estimation we use the TADM open-source toolkit (Malouf, 2002), using its limited-memory variable metric as the optimization method. As is standard practice, the model is regularized by including a zero-mean Gaussian prior on the feature weights to reduce the risk of overfitting. 149 the probability of its annotated reference translation relative to the other competing candidates. However, for future work we plan to also experiment with optimizing the scores of a given evaluation metric (e.g. BLEU) directly, following the Minimum Error Rate approach of Och (2003). The three most fundamental features that are supplied in our log-linear re-ranker correspond to the three ranking modules of the baseline system, as described in Sections § 4, § 5, and § 6 above. In other words, these features record the scores of the parse ranker, the MRS ranker, and the realization ranker, respectively. But our re-ranker also includes several other features that are not part of the baseline model. Other Features Our experiments so far have taken into account another eight properties of the translation process, in some cases observing internal features of individual compone"
2007.tmi-papers.18,P02-1038,0,0.0471966,"mony between the string lengths of the source and target. Log-linear models provide a very flexible framework for discriminative modeling that allows us to combine disparate and overlapping sources of information in a single model without running the risk of making unwarranted independence assumptions. In this section we describe a model that directly estimates the posterior translation probability Pλ (e|f ), for a given source sentence f and translation e. Although the re-ranker we describe here is built on top of a hybrid baseline system, the overall approach is similar to that described by Och & Ney (2002) in the context of SMT. Log-Linear Models A log-linear model is given in terms of (a) a set of specified features that describe properties of the data, and (b) an associated set of learned weights that determine the contribution of each feature. One advantage of working with a discriminative re-ranking setup is that the model can use global features that the baseline system would not be able to incorporate. The information that the feature functions record can be arbitrarily complex, and a given feature can even itself be a separate statistical model. In the following we first give a brief hig"
2007.tmi-papers.18,2005.eamt-1.27,1,0.855742,"Missing"
2007.tmi-papers.18,2004.tmi-1.2,1,0.736592,"Missing"
2007.tmi-papers.18,oepen-lonning-2006-discriminant,1,0.843509,"waterway n] } Figure 4: Variable-free reduction of the MRS for the utterance ‘We recommend a hike around the waterway’s sources’. ing set; this could be done automatically. The second approach is motivated by the hypothesis that discriminants, as used in manual annotation (Carter, 1997), represent promising alternative feature functions to the predefined templates. Initial tests (see table 2) show that the discriminant approach (which is not yet used in the LOGON system) scores better than the templatebased approach. 5 Ranking Transfer Outputs While MRS formulae are highly structured graphs, Oepen & Lønning (2006) suggest a reduction into a variable-free form that resembles elementary dependency structures. For the ranking of transfer outputs, MRSs are broken down into basic dependency triples, whose probabilities are estimated by adaptation of standard n-gram sequence modeling techniques. The actual training is done using the freely available CMU SLM toolkit (Clarkson & Rosenfeld, 1997). Based on a training set of some 8,500 indomain MRSs, viz. the treebanked version of the English translations of the (full) LOGON development corpus, our target language ‘semantic model’ is defined as a smoothed tri-gr"
2007.tmi-papers.18,C00-1085,0,0.0240505,"xample by assessing the relative contribution of individual features, fine-tuning parameter estimation, and including additional properties. Our current maximum likelihood training of the log-linear model is based on a binarized empirical distribution, where for each input we consider the candidate translation(s) with maximum NEVA score(s) as preferred, and all others as dis-preferred. Obviously, however, the degradation in quality among alternate candidates is continuous (rather than absolute), and we have started experimentation with a graded empirical distribution, adapting the approach of Osborne (2000) to the re-ranking task. Finally, in a parallel refinement cycle, we aim to contrast our current ( LL) re-ranking model with Minimum Error Rate (MER) training, a method that aims to estimate model parameters to directly optimize BLEU scores (or another quality metric) as its objective function. Trading coverage for increased output quality may be economic for a range of tasks—say as a complement to other tools in the workbench of a professional translator. Our re-ranking approach, with access to rich intermediate representations, probabilities, and confidence measures, provides a fertile envir"
2007.tmi-papers.18,P02-1040,0,0.0737594,"the speaker had intended. Our argument for the first translation can be illustrated within our earlier example of a wordlevel noun vs. verb ambiguity in analysis. The many different realizations of the noun in the target language may fall into classes of near synonyms, in which case it does not matter for the quality of the result which synonym is chosen. Even though each of the individual realizations has a low probability, it may be a good translation. Observe here also that an automatic evaluation measure—measuring the similarities to a set of reference translations, like the BLEU metric (Papineni, Roukos, Ward, & Zhu, 2002)—will favor the view of most likely translation. We conjecture, however, that a human evaluation will correspond better to the first translation. From a theoretical point of view, it seems most correct to go for the first translation. But it presupposes that we choose the correct interpretation of the source sentence, which we cannot expect to always do. In cases where we have chosen an incorrect analysis, this might be revealed by trying to translate it into the target language and consider the result. If all the candidate translations sound bad—or have a very low probability—in the target l"
2007.tmi-papers.18,N06-1032,0,0.0685787,"Missing"
2007.tmi-papers.18,W04-3223,0,0.0275221,"t matches and matches among the five top-ranked analyses. Figures in parentheses show a random choice baseline. Both models were trained on seven of nine treebanked texts and evaluated on the two remaining texts. was used to build a treebank for the LOGON development corpus. Parse selection in LOGON uses training data from this treebank; all sentences with full parses with low ambiguity (fewer than 100 readings) were at least partially disambiguated. The parse selection method employed in the LOGON demonstrator uses the stochastic disambiguation scheme and training software developed at PARC (Riezler & Vasserman, 2004). The XLE system provides a set of parameterized feature function templates that must be expanded in accordance with the grammar or the training set at hand. Application of these feature functions to the training data yields feature forests for both the labeled data (the partially disambiguated parse forests) and the unlabeled data (the full parse forests). These feature forests are the input to the statistical estimation algorithm, which generates a property weights file that is used to rank solutions. One of the challenges in applying the probability model to a given grammar and training set"
adolphs-etal-2008-fine,callmeier-etal-2004-deepthought,1,\N,Missing
adolphs-etal-2008-fine,W01-1815,0,\N,Missing
adolphs-etal-2008-fine,2004.tmi-1.2,1,\N,Missing
adolphs-etal-2008-fine,J93-2004,0,\N,Missing
adolphs-etal-2008-fine,W07-1219,1,\N,Missing
adolphs-etal-2008-fine,W06-2714,0,\N,Missing
adolphs-etal-2008-fine,P99-1061,1,\N,Missing
adolphs-etal-2008-fine,P02-1056,1,\N,Missing
adolphs-etal-2008-fine,P03-1014,1,\N,Missing
adolphs-etal-2008-fine,P02-1035,0,\N,Missing
adolphs-etal-2008-fine,P04-1014,0,\N,Missing
adolphs-etal-2008-fine,waldron-etal-2006-preprocessing,1,\N,Missing
baldwin-etal-2004-road,copestake-flickinger-2000-open,1,\N,Missing
baldwin-etal-2004-road,C02-2025,1,\N,Missing
baldwin-etal-2004-road,P03-1059,1,\N,Missing
C02-2025,J97-4005,0,0.0188522,"d information about the words. Well-understood statistical part-of-speech tagging technology is sufficient for this approach. In order to use more information about the parse, we might examine the entire derivation of the string. Most probabilistic parsing research – including, for example, work by by Collins (1997), and Charniak (1997) – is based on branching process models (Harris, 1963). The HPSG derivations that the treebank makes available can be viewed as just such a branching process, and a stochastic model of the trees can be built as a probabilistic context-free grammar (PCFG) model. Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). These models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax. Nevertheless, the naive PCFG approach has the advantage of simplicity, so we pursue it and the tagging approach to parse ranking in these pr"
C02-2025,W97-1502,0,0.122641,"nks, there is no need to define a (new) form of grammatical representation specific to the treebank. Instead, the treebank records complete syntactosemantic analyses as defined by the LinGO ERG and provide tools to extract different types of linguistic information at varying granularity. The treebanking environment, building on the [incr tsdb()] profiling environment (Oepen & Callmeier, 2000), presents annotators, one sentence at a time, with the full set of analyses produced by the grammar. Using a pre-existing tree comparison tool in the LKB (similar in kind to the SRI Cambridge TreeBanker; Carter, 1997), annotators can quickly navigate through the parse forest and identify the correct or preferred analysis in the current context (or, in rare cases, reject all analyses proposed by the grammar). The tree selection tool presents users, who need little expert knowledge of the underlying grammar, with a range of basic properties that distinguish competing analyses and that are relatively easy to judge. All disambiguating decisions made by annotators are recorded in the [incr tsdb()] database and thus become available for (i) later dynamic extraction from the annotated profile or (ii) dynamic prop"
C02-2025,P97-1003,0,0.0128863,"t the simplest end, we might look only at the lexical type sequence assigned to the words by each parse and rank the parse based on the likelihood of that sequence. These lexical types – the preterminals in the derivation – are essentially part-of-speech tags, but encode considerably finer-grained information about the words. Well-understood statistical part-of-speech tagging technology is sufficient for this approach. In order to use more information about the parse, we might examine the entire derivation of the string. Most probabilistic parsing research – including, for example, work by by Collins (1997), and Charniak (1997) – is based on branching process models (Harris, 1963). The HPSG derivations that the treebank makes available can be viewed as just such a branching process, and a stochastic model of the trees can be built as a probabilistic context-free grammar (PCFG) model. Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). The"
C02-2025,P01-1019,1,0.256862,"with an enhanced version of the grammar in an automated fashion, viz. by re-applying the disambiguating decisions on the corpus with an updated version of the grammar. Depth of Representation and Transformation of Information Internally, the [incr tsdb()] database records analyses in three different formats, viz. (i) as a derivation tree composed of identifiers of lexical items and constructions used to build the analysis, (ii) as a traditional phrase structure tree labeled with an inventory of some fifty atomic labels (of the type ‘S’, ‘NP’, ‘VP’ et al.), and (iii) as an underspecified MRS (Copestake, Lascarides, & Flickinger, 2001) meaning representation. While representation (ii) will in many cases be similar to the representation found in the Penn Treebank, representation (iii) subsumes the functor – argument (or tectogrammatical) structure advocated in the Prague Dependency Treebank or the German TiGer corpus. Most importantly, however, representation (i) provides all the information required to replay the full HPSG analysis (using the original grammar and one of the open-source HPSG processing environments, e.g., the LKB or PET, which already have been interfaced to [incr tsdb()]). Using the latter approach, users"
C02-2025,W00-1908,0,0.0559015,"Missing"
C02-2025,P99-1069,0,0.0373806,"including, for example, work by by Collins (1997), and Charniak (1997) – is based on branching process models (Harris, 1963). The HPSG derivations that the treebank makes available can be viewed as just such a branching process, and a stochastic model of the trees can be built as a probabilistic context-free grammar (PCFG) model. Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). These models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax. Nevertheless, the naive PCFG approach has the advantage of simplicity, so we pursue it and the tagging approach to parse ranking in these proof-of-concept experiments (more recently, we have begun work on building log-linear models over HPSG signs (Toutanova & Manning, 2002)). The learned models were used to rank possible parses of unseen test sentences according to the probabilities they assign to them. We report parse selection perfo"
C02-2025,2000.iwpt-1.19,1,0.696136,"SG framework and a generally-available broad-coverage grammar of English, the LinGO English Resource Grammar (Flickinger, 2000) as implemented with the LKB grammar development environment (Copestake, 2002). Unlike existing treebanks, there is no need to define a (new) form of grammatical representation specific to the treebank. Instead, the treebank records complete syntactosemantic analyses as defined by the LinGO ERG and provide tools to extract different types of linguistic information at varying granularity. The treebanking environment, building on the [incr tsdb()] profiling environment (Oepen & Callmeier, 2000), presents annotators, one sentence at a time, with the full set of analyses produced by the grammar. Using a pre-existing tree comparison tool in the LKB (similar in kind to the SRI Cambridge TreeBanker; Carter, 1997), annotators can quickly navigate through the parse forest and identify the correct or preferred analysis in the current context (or, in rare cases, reject all analyses proposed by the grammar). The tree selection tool presents users, who need little expert knowledge of the underlying grammar, with a range of basic properties that distinguish competing analyses and that are relat"
C02-2025,W02-2030,1,0.780762,"ermining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). These models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax. Nevertheless, the naive PCFG approach has the advantage of simplicity, so we pursue it and the tagging approach to parse ranking in these proof-of-concept experiments (more recently, we have begun work on building log-linear models over HPSG signs (Toutanova & Manning, 2002)). The learned models were used to rank possible parses of unseen test sentences according to the probabilities they assign to them. We report parse selection performance as percentage of test sentences for which the correct parse was highest ranked by the model. (We restrict attention in the test corpus to sentences that are ambiguous according to the grammar, that is, for which the parse selection task is nontrivial.) We examine four models: an HMM tagging model, a simple PCFG, a PCFG with grandparent annotation, and a hybrid model that combines predictions from the PCFG and the tagger. Thes"
copestake-etal-2002-multiword,W98-0707,0,\N,Missing
copestake-etal-2002-multiword,P97-1018,1,\N,Missing
copestake-etal-2004-lexicon,copestake-flickinger-2000-open,1,\N,Missing
copestake-etal-2004-lexicon,villavicencio-etal-2004-multilingual,1,\N,Missing
copestake-etal-2004-lexicon,copestake-etal-2002-multiword,1,\N,Missing
copestake-etal-2004-lexicon,W02-1210,0,\N,Missing
copestake-flickinger-2000-open,1996.amta-1.6,0,\N,Missing
copestake-flickinger-2000-open,A00-2022,0,\N,Missing
copestake-flickinger-2000-open,J99-1002,1,\N,Missing
copestake-flickinger-2000-open,C94-1042,0,\N,Missing
copestake-flickinger-2000-open,A92-1012,1,\N,Missing
copestake-flickinger-2000-open,C96-2120,0,\N,Missing
copestake-flickinger-2000-open,P99-1061,0,\N,Missing
copestake-flickinger-2000-open,P97-1028,0,\N,Missing
copestake-flickinger-2000-open,P99-1052,0,\N,Missing
copestake-flickinger-2000-open,2000.iwpt-1.19,0,\N,Missing
D11-1037,P06-4020,0,0.051008,"Missing"
D11-1037,W08-1705,0,0.023705,"Missing"
D11-1037,cer-etal-2010-parsing,0,0.0398474,"Missing"
D11-1037,P05-1022,0,0.0267785,"ngs from the realm of natural language database interfaces directly with semantic representations in lambda calculus. These were hand-written on the basis of database query statements distributed with the original datasets. English factored model which combines the preferences of unlexicalized PCFG phrase structures and of lexical dependencies, trained on sections 02–21 of the WSJ portion of the PTB. We chose Stanford Parser from among the state-of-the-art PTB-derived parsers for its support for grammatical relations as an alternate interface representation. Charniak&Johnson Reranking Parser (Charniak and Johnson, 2005) is a two-stage PCFG parser with a lexicalized generative model for the firststage, and a discriminative MaxEnt reranker for the second-stage. The models we evaluate are also trained on sections 02–21 of the WSJ. Top-50 readings were used for the reranking stage. The output constituent trees were then converted into Stanford Dependencies. According to Cer et al. (2010), this combination gives the best parsing accuracy in terms of Stanford dependencies on the PTB. Enju (Miyao et al., 2004) is a probabilistic HPSG parser, combining a hand-crafted core grammar with automatically acquired lexical"
D11-1037,J07-4004,0,0.0417408,". (2010), this combination gives the best parsing accuracy in terms of Stanford dependencies on the PTB. Enju (Miyao et al., 2004) is a probabilistic HPSG parser, combining a hand-crafted core grammar with automatically acquired lexical types from the PTB.3 The model we evaluate is trained on the same material from the WSJ sections of the PTB, but the treebank is first semi-automatically converted into HPSG derivations, and the annotation is enriched with typed feature structures for each constituent. In addition to HPSG derivation trees, Enju also produces predicate argument structures. C&C (Clark and Curran, 2007) is a statistical CCG parser. Abstractly similar to the approach of Enju, the grammar and lexicon are automatically induced from CCGBank (Hockenmaier and Steedman, 2007), a largely automatic projection of (the WSJ portion of) PTB trees into the CCG framework. In addition to CCG derivations, the C&C parser can directly output a variant of grammatical relations. RASP (Briscoe et al., 2006) is an unlexicalized robust parsing system, with a hand-crafted “tag sequence” grammar at its core. The parser thus analyses a lattice of PoS tags, building a parse forest from which the most probable syntactic"
D11-1037,W01-0713,0,0.0205191,"ive examples along these dimensions, as these help to position the parsers we subsequently evaluate.1 2.1 Approaches to parsing Source of linguistic knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH - IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak"
D11-1037,flickinger-etal-2010-wikiwoods,1,0.827502,"omparable to what they report for the Brown Corpus (but not the WSJ portion of the PTB). 4.2 Annotation format We annotated up to two dependency triples per phenomenon instance, identifying the heads and dependents by the surface form of the head words in the sentence suffixed with a number indicating word position (see Table 2).6 Some strings contain more than one instance of the phenomenon they illustrate; in these cases, multiple sets of dependencies are We processed 900 million tokens of Wikipedia text using the October 2010 release of the ERG, following the work of the WikiWoods project (Flickinger et al., 2010). Using the top-ranked ERG deriva6 tion trees as annotations over this corpus and simAs the parsers differ in tokenization strategies, our evaluaple patterns using names of ERG-specific construc- tion script treats these position IDs as approximate indicators. 402 Item ID 1011079100200 1011079100200 1011079100200 Phenomenon absol absol absol Polarity 1 1 1 Dependency having-2|been-3|passed-4 ARG act-1 withdrew-9 MOD having-2|been-3|passed-4 carried+on-12 MOD having-2|been-3|passed-4 Table 2: Sample annotations for sentence # 1011079100200: The-0 act-1 having-2 been-3 passed-4 in-5 that-6 year-"
D11-1037,P06-1111,0,0.012084,"c knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH - IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowled"
D11-1037,J07-3004,0,0.221976,"type “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic disambiguation. Design of representations Approaches to parsing also differ fundamentally in the style of representation assigned to strings. These vary both in their 1 Additional sources of variation among extant parsing technologies include (a) the behavior with respect to ungrammatical inputs and (b) the relationship between probabilistic and symbolic knowled"
D11-1037,W07-2416,0,0.109783,"with a hand-coded grammar at their core typically also incorporate an automatically trained probabilistic disambiguation component. 398 formal nature and the “granularity” of linguistic information (i.e. the number of distinctions assumed), encompassing variants of constituent structure, syntactic dependencies, or logical-form representations of semantics. Parser interface representations range between the relatively simple (e.g. phrase structure trees with a limited vocabulary of node labels as in the PTB, or syntactic dependency structures with a limited vocabulary of relation labels as in Johansson and Nugues (2007)) and the relatively complex, as for example elaborate syntactico-semantic analyses produced by the ParGram or DELPH - IN grammars. There tends to be a correlation between the methodology used in the acquisition of linguistic knowledge and the complexity of representations: in the creation of a mostly hand-crafted treebank like the PTB, representations have to be simple enough for human annotators to reliably manipulate. Deriving more complex representations typically presupposes further computational support, often involving some hand-crafted linguistic knowledge—which can take the form of ma"
D11-1037,N04-1013,0,0.0162081,"Missing"
D11-1037,P04-1061,0,0.0139015,"along these dimensions, as these help to position the parsers we subsequently evaluate.1 2.1 Approaches to parsing Source of linguistic knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH - IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Pe"
D11-1037,J93-2004,0,0.0463107,"verage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic di"
D11-1037,H05-1066,0,0.0471313,"equence” grammar at its core. The parser thus analyses a lattice of PoS tags, building a parse forest from which the most probable syntactic trees and sets of corresponding grammatical relations can be extracted. Unlike other parsers in our mix, RASP did not build on PTB data in either its PoS tagging 3 This hand-crafted grammar is distinct from the ERG, despite sharing the general framework of HPSG. The ERG is not included in our evaluation, since it was used in the extraction of the original examples and thus cannot be fairly evaluated. 399 or syntactic disambiguation components. MSTParser (McDonald et al., 2005) is a datadriven dependency parser. The parser uses an edgefactored model and searches for a maximal spanning tree that connects all the words in a sentence into a dependency tree. The model we evaluate is the second-order projective model trained on the same WSJ corpus, where the original PTB phrase structure annotations were first converted into dependencies, as established in the CoNLL shared task 2009 (Johansson and Nugues, 2007). XLE/ParGram (Riezler et al., 2002, see also Cahill et al., 2008) applies a hand-built Lexical Functional Grammar for English and a stochastic parse selection mod"
D11-1037,C10-1094,0,0.115638,"Missing"
D11-1037,P04-1047,0,0.0766051,"Missing"
D11-1037,P06-1055,0,0.00651761,"4), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic disambiguation. Design of representations Approaches to parsing also differ fundamentally in the style of representation assigned to strings. These vary both in"
D11-1037,P02-1035,0,0.037496,"anguage Processing, pages 397–408, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics unlabeled training data. A related dimension of variation is the type of representations manipulated by the parser. We briefly review some representative examples along these dimensions, as these help to position the parsers we subsequently evaluate.1 2.1 Approaches to parsing Source of linguistic knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH - IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all"
D11-1037,D09-1085,0,0.273521,"Missing"
D11-1037,P08-1039,0,0.028581,"Missing"
D11-1037,N10-1034,0,\N,Missing
D11-1037,J03-4003,0,\N,Missing
flickinger-etal-2010-wikiwoods,zhang-kordoni-2008-robust,0,\N,Missing
flickinger-etal-2010-wikiwoods,2004.tmi-1.2,1,\N,Missing
flickinger-etal-2010-wikiwoods,A00-2022,1,\N,Missing
flickinger-etal-2010-wikiwoods,W97-1502,0,\N,Missing
flickinger-etal-2010-wikiwoods,adolphs-etal-2008-fine,1,\N,Missing
flickinger-etal-2014-towards,J93-2004,0,\N,Missing
flickinger-etal-2014-towards,flickinger-etal-2010-wikiwoods,1,\N,Missing
flickinger-etal-2014-towards,P82-1014,0,\N,Missing
flickinger-etal-2014-towards,D12-1035,0,\N,Missing
flickinger-etal-2014-towards,D11-1037,1,\N,Missing
flickinger-etal-2014-towards,P13-1042,0,\N,Missing
flickinger-etal-2014-towards,kouylekov-oepen-2014-semantic,1,\N,Missing
flickinger-etal-2014-towards,D13-1161,0,\N,Missing
flickinger-etal-2014-towards,oepen-lonning-2006-discriminant,1,\N,Missing
flickinger-etal-2014-towards,adolphs-etal-2008-fine,1,\N,Missing
I05-2035,W02-1503,0,0.19159,"s that result in automatically generated language-specific software. We illustrate the approach for several phenomena and explore the interdependence of the modules. 2 The Grammar Matrix 1 Introduction Manual development of precise broad-coverage grammar implementations, useful in a range of natural language processing/understanding tasks, is a labor-intensive undertaking, requiring many years of work by highly trained linguists. Many recent efforts toward reducing the time and level of expertise needed to produce a new grammar have focused on adapting an existing grammar of another language (Butt et al., 2002; Kim et al., 2003; Bateman et al., ip). Our work on the ‘Grammar Matrix’ has pursued an alternative approach, identifying a set of language-independent grammar constraints to which language-specific constraints can be added (Bender et al., 2002). This approach has the hitherto unexploited potential to benefit from the substantial theoretical work on language typology. In this paper, we present 203 Wide-coverage grammars representing deep linguistic analysis exist in several frameworks, including Head-Driven Phrase Structure Grammar (HPSG), Lexical-Functional Grammar, and Lexicalized Tree Adjo"
I05-2035,P01-1019,1,0.826322,"Missing"
I05-2035,C96-2106,0,0.189913,"tionnaire requires orthographic forms and predicate names. Note that the forms are assumed to be fully inflected (modulo negation), support morphological processes awaiting future work. We use this information and the knowledge base to produce a set of lexical types inheriting from the types defined in the core Matrix and specifying appropriate language-specific constraints, and a set of lexical entries. 5 Limits of modularity Recent computational work in HPSG has asked whether different parts of a single grammar can be abstracted into separate, independent mod206 ules, either for processing (Kasper and Krieger, 1996; Theofilidis et al., 1997) or grammar development (Keˇselj, 2001). Our work is most similar to Keˇselj’s though we are pursuing different goals: Keˇselj is looking to support a division of labor among multiple individuals working on the same grammar and to support variants of a single grammar for different domains. His modules each have private and public features and types, and he illustrates the approach with a small-scale question answering system. In contrast, we are approaching this issue from the perspective of reuse of grammar code in the context of multilingual grammar engineering (a"
I05-2035,W02-1210,1,0.802364,"can be added (Bender et al., 2002). This approach has the hitherto unexploited potential to benefit from the substantial theoretical work on language typology. In this paper, we present 203 Wide-coverage grammars representing deep linguistic analysis exist in several frameworks, including Head-Driven Phrase Structure Grammar (HPSG), Lexical-Functional Grammar, and Lexicalized Tree Adjoining Grammar. In HPSG (P. and Sag, 1994), the most extensive grammars are those of English (Flickinger, 2000), German (Hinrichs et al., 1997; M¨uller and Kasper, 2000; Crysmann, ip), and Japanese (Siegel, 2000; Siegel and Bender, 2002). The Grammar Matrix is an attempt to distill the wisdom of existing grammars and document it in a form that can be used as the basis for new grammars. The main goals of the project are: (i) to develop in detail semantic representations and the syntax-semantics interface, consistent with other work in HPSG; (ii) to represent generalizations across linguistic objects and across languages; and (iii) to allow for very quick start-up as the Matrix is applied to new languages. The original Grammar Matrix consisted of types defining the basic feature geometry, types associated with Minimal Recursion"
I05-2035,W02-1508,1,\N,Missing
I11-1028,P09-1006,0,0.050545,"Missing"
I11-1028,P02-1035,0,0.1593,"Missing"
I11-1028,P05-1041,1,0.936429,"new parse forest, and checking inter-annotator agreement on the overlap, we annotated the remaining sentences. All accuracy figures we report are over the data set of 669 trees complete at the time of experimentation. 3.3 4 Blazing In §2, we reviewed work that uses linguistic information from superficially incompatible formalisms for treebanking or parse selection. Our experiments here use syntactic information from the GTB to partially disambiguate the parse forest produced by the ERG. We do this by disallowing certain candidate ERG trees on the basis of GTBderived information, and we follow Tanaka et al. (2005) in denoting this process “blazing”.1 As detailed below, we can use this partially disambiguated forest: (1) to train parse selection models; and (2) to reduce treebanking effort, abstractly similarly to Tanaka et al. (2005). The goal is not to apply all constraints from the GTB to the ERG parse trees; rather, we want to apply the minimal amount of constraints possible, while still sufficiently restricting the parse forest for our target application. We call the set of trees remaining after blazing silver trees, to represent the fact that they are not gold standard, but are generally of better"
I11-1028,W10-3007,1,0.800767,"scriminants, which are easier to identify. This process happens with all discriminants for a sentence simultaneously, so it is possible to rule out all parse trees. This may indicate that none of the candidate parses are desirable, or that the imperfect blazing process is not completely successful. The blazing module is given the GTB XML source for the tree, and a set of discriminants, each of which includes the name of the rule or lexical Biomedical parsing setup We parsed sentences using the ERG with the PET parser (Callmeier, 2000), which uses POS tags to constrain unknown words. Following Velldal et al. (2010), we primarily use the biomedically trained GENIA tagger (Tsuruoka et al., 2005), but defer to TnT (Brants, 2000) for tagging nominal elements, because it makes a useful distinction between common and proper nouns. Biomedical text poses a unique set of challenges, mostly relating to named entities, such as proteins, DNA and cell lines. To address this, we used the GENIA tagger as a named-entity (NE) recogniser, treating named entities as atomic lex1 248 Which is a term in forestry: marking trees for removal. NP entry, as well as the corresponding character span in the source tree. It applies s"
I11-1028,P94-1034,0,0.142853,"Missing"
I11-1028,W07-2207,1,0.929339,"Missing"
I11-1028,A00-1031,0,0.578191,"it is possible to rule out all parse trees. This may indicate that none of the candidate parses are desirable, or that the imperfect blazing process is not completely successful. The blazing module is given the GTB XML source for the tree, and a set of discriminants, each of which includes the name of the rule or lexical Biomedical parsing setup We parsed sentences using the ERG with the PET parser (Callmeier, 2000), which uses POS tags to constrain unknown words. Following Velldal et al. (2010), we primarily use the biomedically trained GENIA tagger (Tsuruoka et al., 2005), but defer to TnT (Brants, 2000) for tagging nominal elements, because it makes a useful distinction between common and proper nouns. Biomedical text poses a unique set of challenges, mostly relating to named entities, such as proteins, DNA and cell lines. To address this, we used the GENIA tagger as a named-entity (NE) recogniser, treating named entities as atomic lex1 248 Which is a term in forestry: marking trees for removal. NP entry, as well as the corresponding character span in the source tree. It applies some pre-configured transformations to the GTB tree, and examines each discriminant for whether it should be ruled"
I11-1028,W02-1503,0,0.354299,"Missing"
I11-1028,W97-1502,0,0.655547,"Missing"
I11-1028,W11-2927,1,0.695034,"rsing using a pure WeScience model. Other configurations used models trained from the same training sentence parse forest, setting a pseudo-gold tree either randomly, selftrained (best from a WeScience model), or blazing (highest-ranked of the silver trees, other silver trees discarded). The gold WeScience data is also used for training. Significance figures are against “WeSc only”, ( *: p &lt; 0.05; ***:p &lt; 0.001), and “Self-train”, ( ††: p &lt; 0.01; †††: p &lt; 0.001) Experimental Configuration how ‘right’ or ‘wrong’ the top analysis is. To supplement AccN , we use Elementary Dependency Match (EDM: Dridan and Oepen (2011)). This is based on triples extracted from the semantic output of the parser, providing a more granular measure of the quality of the analyses. We use the EDMN A configuration that is arguably the most compatible with other dependency-based parser evaluation, although we make no claims of direct comparability. We create a parse forest by parsing 10747 sentences from a GTB subset not overlapping with the test corpus, using the WeScience model to determine the top 500 parses. The best-performing method we found to create parse selection models from this parse forest was to apply the blazing conf"
I11-1028,W01-0521,0,0.056251,"s stochastically adapted to a new domain, using unlabelled data from the new domain (Daum´e III and Marcu, 2006); and (2) annotation projection, where the labels in a pre-existing resource are semi-automatically translated into an independent formalism, e.g. in translating the PTB into the CCG formalism (Hockenmaier and Steedman, 2002). This paper looks at both of these approaches: domain adaptation from unannotated 2 Related Work Domain adaptation is an active research area, triggered by the observation that parsers trained on one domain show decreased performance when used in other domains (Gildea, 2001). Much domain-adaptation work involves some small amount of in-domain data to tune a model, but McClosky et al. (2006) showed “self-training” using unannotated in-domain data could achieve significant improvements in parser accuracy. In parsing-related research that has used annotated data, but in an incompatible format, we see two main use cases. The first uses an existing treebank to create a treebank for some completely different linguistic framework, generally to induce a grammar in that framework. Xia (1999) presents work on transforming Penn Treebank (PTB) trees into Lexicalized Tree Adj"
I11-1028,hockenmaier-steedman-2002-acquiring,0,0.18136,"n option, however, for finegrained tasks which require an expert understanding of a theory or domain, such as syntactic treebanking or discourse annotation. Two main approaches have been adopted to efficiently create new resources: (1) domain adaptation, where a trained model from one domain is stochastically adapted to a new domain, using unlabelled data from the new domain (Daum´e III and Marcu, 2006); and (2) annotation projection, where the labels in a pre-existing resource are semi-automatically translated into an independent formalism, e.g. in translating the PTB into the CCG formalism (Hockenmaier and Steedman, 2002). This paper looks at both of these approaches: domain adaptation from unannotated 2 Related Work Domain adaptation is an active research area, triggered by the observation that parsers trained on one domain show decreased performance when used in other domains (Gildea, 2001). Much domain-adaptation work involves some small amount of in-domain data to tune a model, but McClosky et al. (2006) showed “self-training” using unannotated in-domain data could achieve significant improvements in parser accuracy. In parsing-related research that has used annotated data, but in an incompatible format, w"
I11-1028,W02-2018,0,0.0442808,"Missing"
I11-1028,P06-1043,0,0.197318,"06); and (2) annotation projection, where the labels in a pre-existing resource are semi-automatically translated into an independent formalism, e.g. in translating the PTB into the CCG formalism (Hockenmaier and Steedman, 2002). This paper looks at both of these approaches: domain adaptation from unannotated 2 Related Work Domain adaptation is an active research area, triggered by the observation that parsers trained on one domain show decreased performance when used in other domains (Gildea, 2001). Much domain-adaptation work involves some small amount of in-domain data to tune a model, but McClosky et al. (2006) showed “self-training” using unannotated in-domain data could achieve significant improvements in parser accuracy. In parsing-related research that has used annotated data, but in an incompatible format, we see two main use cases. The first uses an existing treebank to create a treebank for some completely different linguistic framework, generally to induce a grammar in that framework. Xia (1999) presents work on transforming Penn Treebank (PTB) trees into Lexicalized Tree Adjoining Grammar (LTAG) structures. The work of Hockenmaier and Steedman (2002) is roughly parallel, but targets Combina"
I11-1028,W05-0603,0,0.0611187,"Missing"
I11-1028,I05-2038,0,\N,Missing
J92-3002,1991.iwpt-1.17,0,0.0389413,"Missing"
J92-3002,C90-3052,0,0.100734,"ubject of intensive implementation activity during the past several years; we know of implementations at Hewlett-Packard Laboratories, The German AI Center (DFKI), Stanford University, Carnegie Mellon University, The Ohio State University, Simon Fraser University, University of Edinburgh, ICOT, University of Stuttgart, the IBM LILOG project in Stuttgart, and ATR. We may therefore safely refer the reader to documentations of those implementations, even if these are less generally available than the theoretical literature: Proudian and Pollard (1985), Nerbonne and Proudian (1987), Franz (1990), Emele and Zajac (1990), and Carpenter, Pollard, and Franz (1991). 274 Dan Flickinger and John Nerbonne Inheritance and Complernentation representation in Appendix A. The fundamental data we shall be concerned with are repeated in (2): (2) a. b. C. d. Bill is easy to talk to. It is easy to talk to Bill. Bill is easy for Mary to talk to. It is easy for Mary to talk to Bill. Other adjectives that show this same distribution include the following: (3) amusing annoying boring comfortable confusing depressing difficult exhausting fun good great hard important impossible impressive nice painful tiresome terrible tough Giv"
J92-3002,E89-1009,0,0.152177,"ve the lexicon describe feature structures directly. But this does not correspond to our implementation, nor are w e clear on how, e.g., information on lexical rules and their application ought to be r e n d e r e d in features. Perhaps lexical entries must be structured so that one c o m p o n e n t of a lexical entry is a feature structure, while others are not. 2° Can inheritance be exploited in the specification of inflectional variation? This appears to be a promising area of application, since in general, one can view inflected elements as further specifications of abstract lexemes (cf. Evans and Gazdar 1989 for an intriguing proposal). Can derivational lexical rules be treated more satisfactorily? For example, it is clear that at least some lexical rules relate not merely a pair of w o r d classes, but rather entire lexical substructures (involving several classes) to one another. Can the techniques of inheritance be applied here, so that exceptional elements m a y be easily accommodated? Acknowledgments We are indebted to Mark Gawron, Masayo Iida, Bill Ladusaw, Joachim Laubsch, Carl Pollard, and Tom Wasow for frequent conversations about this analysis. We are also grateful to Anthony Kroch, the"
J92-3002,P85-1032,0,0.190373,"lexicon. The remainder of the paper is structured as follows: Section 2 summarizes the aspects of HPSG that are important to our proposal, and Section 3 develops the fundamental analysis that Section 4 illustrates in a series of analytical &quot;snapshots&quot; of a single example. Section 5 suggests extensions of the fundamental analysis, especially to further lexical classes (developing the argument that structured lexicons are easily maintained and extended), and a final section summarizes and suggests directions for future work. Appendix A presents the framework for lexical description developed in Flickinger et al. (1985) and Flickinger (1987). The framework is convenient for featurebased grammars, but it allows the specification of other lexical properties as well. This Appendix presents a notation that is precise while avoiding redundancy, e.g., in characterizing the kinds of complements that these adjectives permit, and in expressing the relationships that hold between pairs like the easy of (la) and that of (lb). Since a fundamental claim of hierarchical lexicons is that they eliminate redundancy and thus improve modifiability, there is a second appendix, Appendix B, which demonstrates the modifiability of"
J92-3002,P85-1021,0,0.0213358,"s for lexical 3 It is also worth mentioning that HPSG has also been the subject of intensive implementation activity during the past several years; we know of implementations at Hewlett-Packard Laboratories, The German AI Center (DFKI), Stanford University, Carnegie Mellon University, The Ohio State University, Simon Fraser University, University of Edinburgh, ICOT, University of Stuttgart, the IBM LILOG project in Stuttgart, and ATR. We may therefore safely refer the reader to documentations of those implementations, even if these are less generally available than the theoretical literature: Proudian and Pollard (1985), Nerbonne and Proudian (1987), Franz (1990), Emele and Zajac (1990), and Carpenter, Pollard, and Franz (1991). 274 Dan Flickinger and John Nerbonne Inheritance and Complernentation representation in Appendix A. The fundamental data we shall be concerned with are repeated in (2): (2) a. b. C. d. Bill is easy to talk to. It is easy to talk to Bill. Bill is easy for Mary to talk to. It is easy for Mary to talk to Bill. Other adjectives that show this same distribution include the following: (3) amusing annoying boring comfortable confusing depressing difficult exhausting fun good great hard impo"
K19-2003,K19-2008,0,0.0425329,"al. (2007) and was trained on Sections 00– 20 of the Redwoods Ninth Growth using the Maximum Entropy estimation toolkit of Malouf (2002). We use the LOGON distribution as of August 2019 2 The ERS-to-EDS converter of Oepen and Lønning (2006) is part of the LOGON distribution, as is the converter of Ivanova et al. (2012) for further simplification to bi-lexical DM. Exact command-line incantations for all tools and their parameterization are specified as part of the submission archive in the MRP 2019 data release. 42 overall: HIT-SCIR (Che et al., 2019), Peking (Chen et al., 2019)3 , SJTU–NICT (Bai and Zhao, 2019), and SUDA–Alibaba (Zhang et al., 2019). In contrast to the ERG parser, all of these systems are purely data-driven, in the sense that they do not incorporate manually curated linguistic knowledge (beyond finite-state tokenization rules, maybe) but rather learn all their parameters exclusively from the shared task training data. By and large, the data-driven parsers are competitive to the ERG, in particular the SJTU–NICT and HIT-SCIR systems for DM, and the Peking parser for EDS. For some structural types of graph components (tops and edges), the ERG is in fact outperformed by some submissions"
K19-2003,W15-0128,1,0.876913,"all utterances in standard corpora, including newspaper text, the English Wikipedia, or bio-medical research literature (Flickinger et al., 2017). Parsing times for these data sets measure in seconds per sentence, time comparable to human production or comprehension. Second, since around 2001 the ERG has been accompanied by a selection of development corIntroduction Two of the target representations in the 2019 Shared Task on Cross-Framework Meaning Representation Parsing (MRP 2019; Oepen et al., 2019) derive from the framework dubbed English Resource Semantics (ERS; Flickinger et al., 2014; Bender et al., 2015). ERS instantiates the designer logic for scopally underspecified meaning representation called Minimal Recursion Semantics (MRS; Copestake et al., 2005); in and of themselves, ERS terms are logic- rather than graph-based, i.e. require conversion into graph-structured representations of meaning in the context of the MRP shared task. Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) and DELPH-IN MRS Bi-Lexical Dependencies (DM; Ivanova et al., 2012) achieve simplification of ERS into labeled directed graphs by elimination of most of the information regarding scope underspecificati"
K19-2003,I05-1015,1,0.518071,". (2000), Erbach (1991), Kiefer et al. (1999), and Oepen and Callmeier (2000). The parser achieves exact inference by constructing the complete parse forest, factoring local ambiguity under feature structure subsumption (a technique termed retroactive packing by Oepen and Carroll, 2000) and subsequently enumerating n-best full derivations from the forest according to a discriminative parse ranking model in the tradition of Johnson et al. (1999) and Toutanova et al. (2005). Despite the non-local nature of features (of ERG derivations) used in parse ranking, the selective unpacking procedure of Carroll and Oepen (2005) The Redwoods treebank is built exclusively from ERG analyses, i.e. full HPSG syntactico-semantic signs. Annotation in Redwoods amounts to disambiguation among the candidate analyses derived by the grammar (identifying the intended parse) and, of course, analytical validation of the final result. To make this task practical, a specialized tree selection tool extracts a set of what are called discriminants from the complete set of analyses. Discriminants encode contrasts among alternate analyses—for example whether to treat a word like crop as nominal or verbal, or where to attach a preposition"
K19-2003,W97-1502,0,0.137023,"changes in the underlying grammar, as well as ‘picking up’ analyses for previously out-of-scope inputs and new development corpora. Since mid-2016, the current version of Redwoods (dubbed Ninth Growth, corresponding to ERG release 1214) encompasses gold-standard analyses for some 85,400 utterances (or close to 1.3 million tokens) of running text from half a dozen different genres and domains, including the first 22 sections of the venerable Wall Street Journal (WSJ) text in the Penn Treebank (PTB; Marcus et al., 1993). the most comprehensive such effort, complementing the original proposal by Carter (1997) with the notion of dynamic treebanking, in two senses of this term. First, different views can be projected from the multi-stratal HPSG analyses at the core of the treebank, highlighting subsets of the syntactic or semantic properties of each analysis, e.g. HPSG derivations, more conventional phrase structure trees, full logical-form meaning representations, and various variable-free forms of semantic dependency graphs—including EDS and DM. Second, the dynamic treebank is extended and refined over time. As the grammar (the core repository of knowledge about derivation and composition) evolves"
K19-2003,K19-2007,0,0.116833,"nouns and adjectives) typically enable the grammar to derive complete syntactico-semantic analyses for 85 – 95 percent of all utterances in standard corpora, including newspaper text, the English Wikipedia, or bio-medical research literature (Flickinger et al., 2017). Parsing times for these data sets measure in seconds per sentence, time comparable to human production or comprehension. Second, since around 2001 the ERG has been accompanied by a selection of development corIntroduction Two of the target representations in the 2019 Shared Task on Cross-Framework Meaning Representation Parsing (MRP 2019; Oepen et al., 2019) derive from the framework dubbed English Resource Semantics (ERS; Flickinger et al., 2014; Bender et al., 2015). ERS instantiates the designer logic for scopally underspecified meaning representation called Minimal Recursion Semantics (MRS; Copestake et al., 2005); in and of themselves, ERS terms are logic- rather than graph-based, i.e. require conversion into graph-structured representations of meaning in the context of the MRP shared task. Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) and DELPH-IN MRS Bi-Lexical Dependencies (DM; Ivanova et al., 2012)"
K19-2003,K19-2016,0,0.0976457,"feature configuration of Zhang et al. (2007) and was trained on Sections 00– 20 of the Redwoods Ninth Growth using the Maximum Entropy estimation toolkit of Malouf (2002). We use the LOGON distribution as of August 2019 2 The ERS-to-EDS converter of Oepen and Lønning (2006) is part of the LOGON distribution, as is the converter of Ivanova et al. (2012) for further simplification to bi-lexical DM. Exact command-line incantations for all tools and their parameterization are specified as part of the submission archive in the MRP 2019 data release. 42 overall: HIT-SCIR (Che et al., 2019), Peking (Chen et al., 2019)3 , SJTU–NICT (Bai and Zhao, 2019), and SUDA–Alibaba (Zhang et al., 2019). In contrast to the ERG parser, all of these systems are purely data-driven, in the sense that they do not incorporate manually curated linguistic knowledge (beyond finite-state tokenization rules, maybe) but rather learn all their parameters exclusively from the shared task training data. By and large, the data-driven parsers are competitive to the ERG, in particular the SJTU–NICT and HIT-SCIR systems for DM, and the Peking parser for EDS. For some structural types of graph components (tops and edges), the ERG is in fac"
K19-2003,J93-2004,0,0.0670511,"of the treebank has been produced, manually validating and updating existing analyses to reflect changes in the underlying grammar, as well as ‘picking up’ analyses for previously out-of-scope inputs and new development corpora. Since mid-2016, the current version of Redwoods (dubbed Ninth Growth, corresponding to ERG release 1214) encompasses gold-standard analyses for some 85,400 utterances (or close to 1.3 million tokens) of running text from half a dozen different genres and domains, including the first 22 sections of the venerable Wall Street Journal (WSJ) text in the Penn Treebank (PTB; Marcus et al., 1993). the most comprehensive such effort, complementing the original proposal by Carter (1997) with the notion of dynamic treebanking, in two senses of this term. First, different views can be projected from the multi-stratal HPSG analyses at the core of the treebank, highlighting subsets of the syntactic or semantic properties of each analysis, e.g. HPSG derivations, more conventional phrase structure trees, full logical-form meaning representations, and various variable-free forms of semantic dependency graphs—including EDS and DM. Second, the dynamic treebank is extended and refined over time."
K19-2003,K19-2001,1,0.788811,"Missing"
K19-2003,flickinger-etal-2014-towards,1,0.841676,"es for 85 – 95 percent of all utterances in standard corpora, including newspaper text, the English Wikipedia, or bio-medical research literature (Flickinger et al., 2017). Parsing times for these data sets measure in seconds per sentence, time comparable to human production or comprehension. Second, since around 2001 the ERG has been accompanied by a selection of development corIntroduction Two of the target representations in the 2019 Shared Task on Cross-Framework Meaning Representation Parsing (MRP 2019; Oepen et al., 2019) derive from the framework dubbed English Resource Semantics (ERS; Flickinger et al., 2014; Bender et al., 2015). ERS instantiates the designer logic for scopally underspecified meaning representation called Minimal Recursion Semantics (MRS; Copestake et al., 2005); in and of themselves, ERS terms are logic- rather than graph-based, i.e. require conversion into graph-structured representations of meaning in the context of the MRP shared task. Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) and DELPH-IN MRS Bi-Lexical Dependencies (DM; Ivanova et al., 2012) achieve simplification of ERS into labeled directed graphs by elimination of most of the information regarding"
K19-2003,2000.iwpt-1.19,1,0.558011,"re needed. 3 Parsing with the ERG There are several highly engineered implementations of the DELPH-IN feature structure reference formalism; for our experiments we used the PET parser of Callmeier (2002), as bundled in the open-source distribution of DELPH-IN resources called LOGON (Lønning and Oepen, 2006).1 At its core, PET is a classic, agenda-driven chart parser (Kay, 1986), synthesizing a large body of algorithm design for efficient feature structure manipulation and unification-based parsing by among others Tomabechi (1995), Malouf et al. (2000), Erbach (1991), Kiefer et al. (1999), and Oepen and Callmeier (2000). The parser achieves exact inference by constructing the complete parse forest, factoring local ambiguity under feature structure subsumption (a technique termed retroactive packing by Oepen and Carroll, 2000) and subsequently enumerating n-best full derivations from the forest according to a discriminative parse ranking model in the tradition of Johnson et al. (1999) and Toutanova et al. (2005). Despite the non-local nature of features (of ERG derivations) used in parse ranking, the selective unpacking procedure of Carroll and Oepen (2005) The Redwoods treebank is built exclusively from ERG"
K19-2003,P82-1014,0,0.708302,"een under continuous development for multiple decades now, as part of the world-wide Deep Linguistic Processing with HPSG Initiative (DELPH-IN; http://delph-in.net). First, the LinGO English Resource Grammar (ERG; Flickinger, 2000) is an implementation of the grammatical theory of Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) for English, i.e. a computational grammar that can be used for parsing and generation. Development of the ERG started in 1993, building conceptually on earlier work on unification-based grammar engineering for English at Hewlett Packard Laboratories (Gawron et al., 1982). The ERG has continuously evolved through a series of R&D projects (and a small handful of commercial applications) and today allows the grammatical analysis of running text across domains and genres. The handbuilt ERG lexicon of some 38,000 lemmata (for 27,000 distinct citation forms) aims for complete coverage of function words and open-class words with ‘non-standard’ syntactic properties (e.g. argument structure). Built-in support for light-weight named entity recognition and an unknown word mechanism combining statistical PoS tagging and on-the-fly lexical instantiation for ‘standard’ ope"
K19-2003,A00-2022,1,0.48413,"led in the open-source distribution of DELPH-IN resources called LOGON (Lønning and Oepen, 2006).1 At its core, PET is a classic, agenda-driven chart parser (Kay, 1986), synthesizing a large body of algorithm design for efficient feature structure manipulation and unification-based parsing by among others Tomabechi (1995), Malouf et al. (2000), Erbach (1991), Kiefer et al. (1999), and Oepen and Callmeier (2000). The parser achieves exact inference by constructing the complete parse forest, factoring local ambiguity under feature structure subsumption (a technique termed retroactive packing by Oepen and Carroll, 2000) and subsequently enumerating n-best full derivations from the forest according to a discriminative parse ranking model in the tradition of Johnson et al. (1999) and Toutanova et al. (2005). Despite the non-local nature of features (of ERG derivations) used in parse ranking, the selective unpacking procedure of Carroll and Oepen (2005) The Redwoods treebank is built exclusively from ERG analyses, i.e. full HPSG syntactico-semantic signs. Annotation in Redwoods amounts to disambiguation among the candidate analyses derived by the grammar (identifying the intended parse) and, of course, analytic"
K19-2003,W12-3602,1,0.948803,"tion Parsing (MRP 2019; Oepen et al., 2019) derive from the framework dubbed English Resource Semantics (ERS; Flickinger et al., 2014; Bender et al., 2015). ERS instantiates the designer logic for scopally underspecified meaning representation called Minimal Recursion Semantics (MRS; Copestake et al., 2005); in and of themselves, ERS terms are logic- rather than graph-based, i.e. require conversion into graph-structured representations of meaning in the context of the MRP shared task. Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) and DELPH-IN MRS Bi-Lexical Dependencies (DM; Ivanova et al., 2012) achieve simplification of ERS into labeled directed graphs by elimination of most of the information regarding scope underspecification and, in the case of DM, further reduction into pure bi-lexical graphs. Oepen et al. (2019) provide additional background on these representations. This paper gives some linguistic and technical background on ERS parsing (§2), summarizes the processes used in deriving EDS and DM graphs for the MRP evaluation data 40 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 40–44 c Hong Kong, November 3, 2019. 201"
K19-2003,P99-1069,0,0.647476,"y to mostly automatically update the Redwoods treebank, to for example add detail to the linguistic analyses or apply targeted error correction while minimizing any loss of manual input from previous annotation cycles. Although we can by no means quantify precisely the effort devoted to ERG and Redwoods development to date, we estimate that in excess of thirty person years have been accumulated between 1993 and 2019. The original motivation for treebanking ERG analyses was to enable training discriminative parse ranking models, i.e. a conditional probability distribution over ERG derivations (Johnson et al., 1999). For this purpose, the treebank must disambiguate at the same level of granularity as is maintained in the grammar, i.e. encode its exact linguistic distinctions. Furthermore, to train discriminative (i.e. conditional) stochastic models, both the intended as well as the dispreferred analyses are needed. 3 Parsing with the ERG There are several highly engineered implementations of the DELPH-IN feature structure reference formalism; for our experiments we used the PET parser of Callmeier (2002), as bundled in the open-source distribution of DELPH-IN resources called LOGON (Lønning and Oepen, 20"
K19-2003,P99-1061,0,0.0220515,"he dispreferred analyses are needed. 3 Parsing with the ERG There are several highly engineered implementations of the DELPH-IN feature structure reference formalism; for our experiments we used the PET parser of Callmeier (2002), as bundled in the open-source distribution of DELPH-IN resources called LOGON (Lønning and Oepen, 2006).1 At its core, PET is a classic, agenda-driven chart parser (Kay, 1986), synthesizing a large body of algorithm design for efficient feature structure manipulation and unification-based parsing by among others Tomabechi (1995), Malouf et al. (2000), Erbach (1991), Kiefer et al. (1999), and Oepen and Callmeier (2000). The parser achieves exact inference by constructing the complete parse forest, factoring local ambiguity under feature structure subsumption (a technique termed retroactive packing by Oepen and Carroll, 2000) and subsequently enumerating n-best full derivations from the forest according to a discriminative parse ranking model in the tradition of Johnson et al. (1999) and Toutanova et al. (2005). Despite the non-local nature of features (of ERG derivations) used in parse ranking, the selective unpacking procedure of Carroll and Oepen (2005) The Redwoods treeban"
K19-2003,W07-2207,1,0.51326,"esulting graphs are guaranteed to reflect the composition algebra of the ERG, recursively building larger fragments of meaning from smaller parts. 4 Experimental Results Parsing accuracies for PET and the ERG are summarized in Table 1, for both the DM (top) and EDS (bottom) evaluation graphs. The table compares ERG parsing results to a selection of ‘real’ submissions to the shared task, viz. the top performers within each framework and for the task For parsing the MRP evaluation data, we applied ERG release 1214 with its bundled WSJ parse ranking model, which uses the feature configuration of Zhang et al. (2007) and was trained on Sections 00– 20 of the Redwoods Ninth Growth using the Maximum Entropy estimation toolkit of Malouf (2002). We use the LOGON distribution as of August 2019 2 The ERS-to-EDS converter of Oepen and Lønning (2006) is part of the LOGON distribution, as is the converter of Ivanova et al. (2012) for further simplification to bi-lexical DM. Exact command-line incantations for all tools and their parameterization are specified as part of the submission archive in the MRP 2019 data release. 42 overall: HIT-SCIR (Che et al., 2019), Peking (Chen et al., 2019)3 , SJTU–NICT (Bai and Zha"
K19-2003,P06-4014,1,0.427681,"Johnson et al., 1999). For this purpose, the treebank must disambiguate at the same level of granularity as is maintained in the grammar, i.e. encode its exact linguistic distinctions. Furthermore, to train discriminative (i.e. conditional) stochastic models, both the intended as well as the dispreferred analyses are needed. 3 Parsing with the ERG There are several highly engineered implementations of the DELPH-IN feature structure reference formalism; for our experiments we used the PET parser of Callmeier (2002), as bundled in the open-source distribution of DELPH-IN resources called LOGON (Lønning and Oepen, 2006).1 At its core, PET is a classic, agenda-driven chart parser (Kay, 1986), synthesizing a large body of algorithm design for efficient feature structure manipulation and unification-based parsing by among others Tomabechi (1995), Malouf et al. (2000), Erbach (1991), Kiefer et al. (1999), and Oepen and Callmeier (2000). The parser achieves exact inference by constructing the complete parse forest, factoring local ambiguity under feature structure subsumption (a technique termed retroactive packing by Oepen and Carroll, 2000) and subsequently enumerating n-best full derivations from the forest ac"
K19-2003,K19-2014,0,0.0946981,"00– 20 of the Redwoods Ninth Growth using the Maximum Entropy estimation toolkit of Malouf (2002). We use the LOGON distribution as of August 2019 2 The ERS-to-EDS converter of Oepen and Lønning (2006) is part of the LOGON distribution, as is the converter of Ivanova et al. (2012) for further simplification to bi-lexical DM. Exact command-line incantations for all tools and their parameterization are specified as part of the submission archive in the MRP 2019 data release. 42 overall: HIT-SCIR (Che et al., 2019), Peking (Chen et al., 2019)3 , SJTU–NICT (Bai and Zhao, 2019), and SUDA–Alibaba (Zhang et al., 2019). In contrast to the ERG parser, all of these systems are purely data-driven, in the sense that they do not incorporate manually curated linguistic knowledge (beyond finite-state tokenization rules, maybe) but rather learn all their parameters exclusively from the shared task training data. By and large, the data-driven parsers are competitive to the ERG, in particular the SJTU–NICT and HIT-SCIR systems for DM, and the Peking parser for EDS. For some structural types of graph components (tops and edges), the ERG is in fact outperformed by some submissions, whereas it holds at times commanding"
K19-2003,W02-2018,0,0.00929824,"smaller parts. 4 Experimental Results Parsing accuracies for PET and the ERG are summarized in Table 1, for both the DM (top) and EDS (bottom) evaluation graphs. The table compares ERG parsing results to a selection of ‘real’ submissions to the shared task, viz. the top performers within each framework and for the task For parsing the MRP evaluation data, we applied ERG release 1214 with its bundled WSJ parse ranking model, which uses the feature configuration of Zhang et al. (2007) and was trained on Sections 00– 20 of the Redwoods Ninth Growth using the Maximum Entropy estimation toolkit of Malouf (2002). We use the LOGON distribution as of August 2019 2 The ERS-to-EDS converter of Oepen and Lønning (2006) is part of the LOGON distribution, as is the converter of Ivanova et al. (2012) for further simplification to bi-lexical DM. Exact command-line incantations for all tools and their parameterization are specified as part of the submission archive in the MRP 2019 data release. 42 overall: HIT-SCIR (Che et al., 2019), Peking (Chen et al., 2019)3 , SJTU–NICT (Bai and Zhao, 2019), and SUDA–Alibaba (Zhang et al., 2019). In contrast to the ERG parser, all of these systems are purely data-driven, i"
L16-1630,D11-1031,0,0.0192561,"linguistic properties of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where subst"
L16-1630,W13-2322,0,0.201241,"ns as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer t"
L16-1630,W08-2222,0,0.0201912,"and sources, comprising gold-standard annotations as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks and to encourage broader use of this standardized collec"
L16-1630,Q15-1040,1,0.86691,"banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional manual annotation was perf"
L16-1630,J93-2004,0,0.0550061,"ies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional manual annotation was performed). CCD: Combinatory Categorial Grammar Dependencies Hockenmaier & Steedman (2007) construct CCGbank from a combination of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Journal (WSJ) Corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bi-lexical dependency triples. The latter “includ"
L16-1630,S14-2082,0,0.0694129,"information. We envision that general availability of a standardized and comprehensive set of semantic dependency graphs and associated tools will stimulate more research in this sub-area of semantic parsing. To date, reported ‘parsing success’ 7 To seek to relate these different approaches to the encoding of lexical valency, one can multiply out the DM frame identifiers with verb lemmata, which yields a count of some 4,600 distinct combinations, i.e. slightly less than the set of observed sense distinctions in PSD. measures in terms of dependency F1 range between the high seventies for PSD (Martins & Almeida, 2014) and high eighties to low nineties for CCD, DM, and PAS (Du et al., 2015; Miyao et al., 2014). Such variation may in principle be owed to differences in the number and complexity of linguistic distinctions made, to homogeneity and consistency of training and test data, and of course to the cumulative effort that has gone into pushing the state of the art on individual target representations. A deeper understanding of these parameters, as well as of contentful vs. superficial linguistic differences across frameworks, will be a prerequisite to judging the relative suitability of different resour"
L16-1630,J07-4004,0,0.0382917,"bank from a combination of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Journal (WSJ) Corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bi-lexical dependency triples. The latter “include most semantically relevant non-anaphoric local and longrange dependencies” and are suggested by the CCGbank creators as a proxy for predicate–argument structure. While these have mainly been used for contrastive parser evaluation (Clark & Curran, 2007; Fowler & Penn, 2010; inter alios), recent parsing work as mentioned above views each set of triples as a directed graph and parses directly into these target representations. Our CCD graphs combine the CCGbank dependency triples with information gleaned from the CCG syntactic derivations, notably the part of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannot"
L16-1630,copestake-flickinger-2000-open,1,0.723309,"al trees), and with corresponding ‘companion’ syntactic analyses from a broad variety of frameworks and sources, comprising gold-standard annotations as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic propert"
L16-1630,S14-2056,1,0.955729,"s). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional manual annotation was performed). CCD: Combinatory Categorial Grammar Dependencies Hockenmaier & Steedman (2007) construct CCGbank from a combination of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Jo"
L16-1630,P15-1149,0,0.063476,"s of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional"
L16-1630,flickinger-etal-2014-towards,1,0.859802,"ing ‘companion’ syntactic analyses from a broad variety of frameworks and sources, comprising gold-standard annotations as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks a"
L16-1630,P10-1035,0,0.0195635,"n of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Journal (WSJ) Corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bi-lexical dependency triples. The latter “include most semantically relevant non-anaphoric local and longrange dependencies” and are suggested by the CCGbank creators as a proxy for predicate–argument structure. While these have mainly been used for contrastive parser evaluation (Clark & Curran, 2007; Fowler & Penn, 2010; inter alios), recent parsing work as mentioned above views each set of triples as a directed graph and parses directly into these target representations. Our CCD graphs combine the CCGbank dependency triples with information gleaned from the CCG syntactic derivations, notably the part of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannotation, dubbed DeepBan"
L16-1630,hajic-etal-2012-announcing,1,0.917423,"Missing"
L16-1630,S15-2153,1,0.925074,"Missing"
L16-1630,S14-2008,1,0.928327,"Missing"
L16-1630,oepen-lonning-2006-discriminant,1,0.921801,"rivations, notably the part of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannotation, dubbed DeepBank2 , of Sections 00–21 of the WSJ Corpus with syntactico-semantic analyses from the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). Native ERG semantics take the form of underspecified logical forms, which Oepen et al. (2002); Oepen & Lønning (2006); and Ivanova et al. (2012) map onto the DM bi-lexical semantic dependencies in a twostep conversion pipeline.3 For this target representation, top nodes designate the highest-scoping (non-quantificational) predicate in the graph, e.g. the scopal adverb almost in Figure 1 below. PAS: Enju Predicate–Argument Structures The Enju Treebank4 is derived from automatic HPSG-style reannotation of the PTB (Miyao, 2006). Our PAS graphs stem from the Enju Treebank, without contentful conversion, and from the application of the same basic techniques to the Penn Chinese Treebank (CTB; Xue et al., 2005). To"
L16-1630,J07-3004,0,0.0404021,"hon tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Tree"
L16-1630,W12-3602,1,0.888177,"of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannotation, dubbed DeepBank2 , of Sections 00–21 of the WSJ Corpus with syntactico-semantic analyses from the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). Native ERG semantics take the form of underspecified logical forms, which Oepen et al. (2002); Oepen & Lønning (2006); and Ivanova et al. (2012) map onto the DM bi-lexical semantic dependencies in a twostep conversion pipeline.3 For this target representation, top nodes designate the highest-scoping (non-quantificational) predicate in the graph, e.g. the scopal adverb almost in Figure 1 below. PAS: Enju Predicate–Argument Structures The Enju Treebank4 is derived from automatic HPSG-style reannotation of the PTB (Miyao, 2006). Our PAS graphs stem from the Enju Treebank, without contentful conversion, and from the application of the same basic techniques to the Penn Chinese Treebank (CTB; Xue et al., 2005). Top nodes in this representat"
N12-1070,adolphs-etal-2008-fine,1,0.831861,"h both the PROS and GEST features are appropriate. gesture_lexrule := phrase_or_lexrule & [ ORTH [ PROS #pros ], ARGS &lt;[ ORTH [ GEST gesture-form, PROS p-word & #pros ]]>]. Figure 2: Definition of gesture lexrule This robust pre-processing step is sufficient since the only temporal relation required by the grammar is overlap, an abstraction over more fined-grained relations between speech (S) and gesture (G) such as (precedence(start(S), start(G)) ∧ identity (end(S), end(G))). The linking of gesture to its temporally overlapping speech segment happens prior to parsing via chart-mapping rules (Adolphs et al., 2008) which involve re-writing chart items into FSs. The gesture-unary-rule (see Fig.1) rewrites an input (I) speech token in the context (C) of a gesture token into a combined speech+gesture token where the + GEST and + PROS values of the speech and gesture tokens are copied onto the output (O). gesture-unary-rule := cm_rule & [+CONTEXT &lt;gesture_token & [+GEST #gest]>, +INPUT &lt;speech_token & [+PROS #pros]>, +OUTPUT &lt;speech+gesture_token & [+GEST #gest, +PROS #pros]>, +POSITION ""O1@I1, I1@C1"" ]. Figure 1: Definition of gesture-unary-rule The + PROS attribute contains prosodic information and the +"
N12-1070,C96-2120,0,0.0408966,"picting gesture along with “Anna”. Figure 3: Definition of depicting lexrule by the gestural modality [G]. The rule therefore introduces in RELS a label (here #larg1) for an EP which is in qeq constraints with [G]. The instantiation of the particular EPs comes from the gestural lexical entry. In the real implementation, the number of these labels corresponds to the number of features. They are designed in the same way and we thus forego any details about the rest. 4 Evaluation The evaluation was performed against a test suite designed in analogy to the traditional phenomenonbased test-suites (Lehmann et al., 1996): manuallycrafted to ensure coverage of well-formed and illformed data, but inspired by an examination of natural data. We systematically tested syntactic phenomena (intransitivity, transitivity, complex NPs, coordination, negation and modification) over well-formed and ill-formed examples where the ill-formed items were derived by means of the following operations: prosodic permutation (varying the prosodic markedness, e.g., from (4a) we derive (4b) to reflect intuitions of native speakers); gesture variation (testing distinct gesture types) and temporal permutation 585 The test set contained"
N16-4001,W13-5707,1,0.867199,"Missing"
N16-4001,S14-1003,0,0.0330419,"Missing"
N16-4001,W15-2205,0,0.0150817,"ing and parse selection algorithms. With high parsing accuracy with rich semantic representations, English Resource Semantics is a valuable source of information for many semanticallysensitive NLP tasks. ERSbased systems have achieved stateoftheart results in various tasks, including the identification of speculative or negated event mentions in biomedical text (MacKinlay et al 2011), question generation (Yao et al 2012), detecting the scope of negation (Packard et al 2014), relating natural language to robot control language (Packard 2014), and recognizing textual entailment (PETE task; Lien & Kouylekov 2015). ERS representations have also been beneficial in semantic transferbased MT (Oepen et al 2007, Bond et al 2011), ontology acquisition (Herbelot 2006), extraction of glossary sentences 1 Proceedings of NAACL-HLT 2016, pages 1–5, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (Reiplinger et al 2012), sentiment analysis (Kramer & Gordon 2014), and the ACL Anthology Searchbench (Schäfer et al 2011). The goal of this tutorial is to make this resource more accessible to the ACL community. Specifically, we take as our learning goals that tutorial participa"
N16-4001,S14-2056,0,0.127666,"Missing"
N16-4001,2007.tmi-papers.18,1,0.651881,"glish Resource Semantics is a valuable source of information for many semanticallysensitive NLP tasks. ERSbased systems have achieved stateoftheart results in various tasks, including the identification of speculative or negated event mentions in biomedical text (MacKinlay et al 2011), question generation (Yao et al 2012), detecting the scope of negation (Packard et al 2014), relating natural language to robot control language (Packard 2014), and recognizing textual entailment (PETE task; Lien & Kouylekov 2015). ERS representations have also been beneficial in semantic transferbased MT (Oepen et al 2007, Bond et al 2011), ontology acquisition (Herbelot 2006), extraction of glossary sentences 1 Proceedings of NAACL-HLT 2016, pages 1–5, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (Reiplinger et al 2012), sentiment analysis (Kramer & Gordon 2014), and the ACL Anthology Searchbench (Schäfer et al 2011). The goal of this tutorial is to make this resource more accessible to the ACL community. Specifically, we take as our learning goals that tutorial participants will learn how to: (1) set up the ERGbased parsing stack, including preprocessing; (2) acc"
N16-4001,S14-2144,1,0.861772,"or by processing new text with the ERG and its associated parsing and parse selection algorithms. With high parsing accuracy with rich semantic representations, English Resource Semantics is a valuable source of information for many semanticallysensitive NLP tasks. ERSbased systems have achieved stateoftheart results in various tasks, including the identification of speculative or negated event mentions in biomedical text (MacKinlay et al 2011), question generation (Yao et al 2012), detecting the scope of negation (Packard et al 2014), relating natural language to robot control language (Packard 2014), and recognizing textual entailment (PETE task; Lien & Kouylekov 2015). ERS representations have also been beneficial in semantic transferbased MT (Oepen et al 2007, Bond et al 2011), ontology acquisition (Herbelot 2006), extraction of glossary sentences 1 Proceedings of NAACL-HLT 2016, pages 1–5, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (Reiplinger et al 2012), sentiment analysis (Kramer & Gordon 2014), and the ACL Anthology Searchbench (Schäfer et al 2011). The goal of this tutorial is to make this resource more accessible to the ACL communi"
N16-4001,P14-1007,1,0.844733,"al 2004) and DeepBank (Wall Street Journal corpus: Flickinger et al 2012) or by processing new text with the ERG and its associated parsing and parse selection algorithms. With high parsing accuracy with rich semantic representations, English Resource Semantics is a valuable source of information for many semanticallysensitive NLP tasks. ERSbased systems have achieved stateoftheart results in various tasks, including the identification of speculative or negated event mentions in biomedical text (MacKinlay et al 2011), question generation (Yao et al 2012), detecting the scope of negation (Packard et al 2014), relating natural language to robot control language (Packard 2014), and recognizing textual entailment (PETE task; Lien & Kouylekov 2015). ERS representations have also been beneficial in semantic transferbased MT (Oepen et al 2007, Bond et al 2011), ontology acquisition (Herbelot 2006), extraction of glossary sentences 1 Proceedings of NAACL-HLT 2016, pages 1–5, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (Reiplinger et al 2012), sentiment analysis (Kramer & Gordon 2014), and the ACL Anthology Searchbench (Schäfer et al 2011). The goal of this"
N16-4001,W12-3206,0,0.0581575,"Missing"
N16-4001,P11-4002,0,0.0764826,"Missing"
P01-1019,P89-1005,0,0.0742803,"notes an element of H × . . . H × P(G), where the Hs (= L × I) are the new hook and holes. Note that the language Σ is first order, and we do not use λ-abstraction over higher order elements.6 For example, in the standard Montagovian view, a quantifier such as every SEMENTs 5 Note every is a predicate rather than a quantifier in this language, since MRSs are partial descriptions of logical forms in a base language. 6 Even though we do not use λ-calculus for composition, we could make use of λ-abstraction as a representation device, for instance for dealing with adjectives such as former, cf., Moore (1989). is represented by the higher-order expression λP λQ∀x(P (x), Q(x)). In our framework, however, every is the following (using qeq conditions, as in the LinGO ERG): [hf , x]{[]subj , []comp1 , [h0 , x]spec , . . .} [he : every(x, hr , hs )][hr =q h0 ]{} and dog is: [hd , y]{[]subj , []comp1 , []spec , . . .}[hd : dog(y)][]{} So these composes via opspec to yield every dog: [hf , x]{[]subj , []comp1 , []spec , . . .} [he : every(x, hr , hs ), hd : dog(y)] [hr =q h0 ]{h0 = hd , x = y} This SEMENT is semantically equivalent to: [hf , x]{[]subj , []comp1 , []spec , . . .} [he : every(x, hr , hs ),"
read-etal-2012-wesearch,flickinger-etal-2010-wikiwoods,1,\N,Missing
read-etal-2012-wesearch,bird-etal-2008-acl,0,\N,Missing
read-etal-2012-wesearch,P00-1061,0,\N,Missing
read-etal-2012-wesearch,D08-1050,0,\N,Missing
read-etal-2012-wesearch,W00-0901,0,\N,Missing
read-etal-2012-wesearch,I11-1100,0,\N,Missing
read-etal-2012-wesearch,W07-2202,0,\N,Missing
read-etal-2012-wesearch,W11-2925,0,\N,Missing
read-etal-2012-wesearch,P11-4002,0,\N,Missing
read-etal-2012-wesearch,W10-0508,0,\N,Missing
S14-2008,D12-1133,0,0.0149346,"ained or otherwise derived from WSJ Section 21. This restriction implies that typical off-the-shelf syntactic parsers had to be re-trained, as many datadriven parsers for English include this section of the PTB in their default training data. To simplify participation in the open track, the organizers prepared ready-to-use ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in two formats, viz. PTB-style phrase structure trees obtained from the parser of Petrov et al. (2006) and Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). 6 Submissions and Results From 36 teams who had registered for the task, test runs were submitted for nine systems. Each team submitted one or two test runs per track. In total, there were ten runs submitted to the closed track and nine runs to the open track. Three teams submitted to both the closed and the open track. The main results are summarized and ranked in Table 4. The ranking is based on the average LF score across all three target representations, which is given in the LF column. In cases where a team submitted two runs to a track, only the highestranked score is included in the t"
S14-2008,W06-2920,0,0.101128,"em in comparison to other sub-tasks in computational language analysis, introduce the semantic dependency target representations used, reflect on high-level commonalities and differences between these representations, and summarize the task setup, participating systems, and main results. 1 Background and Motivation Syntactic dependency parsing has seen great advances in the past decade, in part owing to relatively broad consensus on target representations, and in part reflecting the successful execution of a series of shared tasks at the annual Conference for Natural Language Learning (CoNLL; Buchholz & Marsi, 2006; Nivre et al., 2007; inter alios). From this very active research area accurate and efficient syntactic parsers have developed for a wide range of natural languages. However, the predominant data structure in dependency parsing to date are trees, in the formal sense that every node in the dependency graph is reachable from a distinguished root node by exactly one directed path. (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans, and rice. Semantically, technique arguably is dependent on the determiner (the quantificational locus), the modifier simil"
S14-2008,de-marneffe-etal-2006-generating,0,0.0702807,"Missing"
S14-2008,oepen-lonning-2006-discriminant,1,0.758648,"antic dependency graphs originate in a manual re-annotation of Sections 00– 21 of the WSJ Corpus with syntactico-semantic analyses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czec"
S14-2008,W09-1201,1,0.855031,"Missing"
S14-2008,J05-1004,0,0.129381,"ependency graphs for Example (1). uous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002). In much previous work, however, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena— for example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in o"
S14-2008,P06-1055,0,0.0124328,"e of the gold-standard syntactic or semantic analyses of the SDP 2014 test data, i.e. were directly or indirectly trained or otherwise derived from WSJ Section 21. This restriction implies that typical off-the-shelf syntactic parsers had to be re-trained, as many datadriven parsers for English include this section of the PTB in their default training data. To simplify participation in the open track, the organizers prepared ready-to-use ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in two formats, viz. PTB-style phrase structure trees obtained from the parser of Petrov et al. (2006) and Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). 6 Submissions and Results From 36 teams who had registered for the task, test runs were submitted for nine systems. Each team submitted one or two test runs per track. In total, there were ten runs submitted to the closed track and nine runs to the open track. Three teams submitted to both the closed and the open track. The main results are summarized and ranked in Table 4. The ranking is based on the average LF score across all three target representations, which is given i"
S14-2008,hajic-etal-2012-announcing,1,0.772691,"Missing"
S14-2008,W12-3602,1,0.885947,"yses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czech translations. Similarly to other treebanks in the Prague family, there are two layers of syntactic annotation: analytical ("
S14-2008,C08-1095,0,0.478753,".27 to 75.89 and the corresponding scores across systems are 88.64 for PAS, 84.95 for DM, and 67.52 for PCEDT. While these scores are consistently higher than in the closed track, the differences are small. In fact, for each of the three teams that submitted to both tracks (Alpage, Potsdam, and Priberam) improvements due to the use of additional resources in the open track do not exceed two points LF. 7 dencies), while the others apply post-processing to recover non-tree structures. The second strategy is to use a parsing algorithm that can directly generate graph structures (in the spirit of Sagae & Tsujii, 2008; Titov et al., 2009). In many cases such algorithms generate restricted types of graph structures, but these restrictions appear feasible for our target representations. The last approach is more machine learning–oriented; they apply classifiers or scoring methods (e.g. edge-factored scores), and find the highest-scoring structures by some decoding method. It is difficult to tell which approach is the best; actually, the top three systems in the closed and open tracks selected very different approaches. A possible conclusion is that exploiting existing systems or techniques for dependency par"
S14-2008,P10-5006,0,0.0693315,"ple inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Another difference to common interpretations of SRL is that the SDP 2014 task definition does not encompass predicate disambiguation, a design decision in part owed to our goal to focus on parsing-oriented, i.e. structural, analysis, and in part to lacking consensus on sense inventories for all content words. Finally, a third closely related area of much current interest is often dubbed ‘semantic parsing’, which Kate and Wong (2010) define as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” In contrast to most work in this tradition, our SDP target representations aim to be task- and domainindependent, though at least part of this generality comes at the expense of ‘completeness’ in the above sense; i.e. there are aspects of sentence meaning that arguably remain implicit. 2 Target Representations We use three distinct target representations for semantic dependencies. As is evident in our running example (Figure"
S14-2008,J93-2004,0,0.0590496,"t of multiple predicates (i.e. have more than one incoming arc), and it will often be desirable to leave nodes corresponding to semantically vacuous word classes unattached (with no incoming arcs). Thus, Task 8 at SemEval 2014, Broad-Coverage Semantic Dependency Parsing (SDP 2014),1 seeks to stimulate the dependency parsing community to move towards more general graph processing, to thus enable a more direct analysis of Who did What to Whom? For English, there exist several independent annotations of sentence meaning over the venerable Wall Street Journal (WSJ) text of the Penn Treebank (PTB; Marcus et al., 1993). These resources constitute parallel semantic annotations over the same common text, but to date they have not been related to each other and, in fact, have hardly been applied for training and testing of datadriven parsers. In this task, we have used three different such target representations for bi-lexical semantic dependencies, as demonstrated in Figure 1 below for the WSJ sentence: Task 8 at SemEval 2014 defines BroadCoverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate–argument relationships for all content words, i.e. the semantic structure"
S14-2008,P07-1031,0,0.0115637,"ersely, in PCEDT the last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected to the graph.7 A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure 2. Much noun phrase–internal structure is not made explicit in the PTB, and the Enju Treebank from which our PAS representation derives predates the bracketing work of Vadas and Curran (2007). In the four-way nominal compounding example of Figure 2, thus, PAS arrives at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of the compound either; PCEDT, on the other hand, annotates both the actual compound-internal bracketing and the assignment of roles, e.g. making stock the PAT(ient) of investment. In this spirit, the PCEDT annotations could be directly paraphrased along the lines of plans by employees for investment in stocks. In a middle position between the other two, DM disambiguates the bracketing but, by design, merely ass"
S14-2008,meyers-etal-2004-annotating,0,0.0465249,"Example (1). uous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002). In much previous work, however, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena— for example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Another"
S14-2008,S14-2056,1,0.893424,"pBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czech translations. Similarly to other treebanks in the Prague family, there are two layers of syntactic annotation: analytical (a-trees) and tectogrammatical (t-trees). PCEDT bi-lexical dependencies in this task have been extracted from the t-trees. The specifics of the PCE"
S14-2008,C10-1011,0,\N,Missing
S14-2008,S14-2080,0,\N,Missing
S14-2008,S14-2082,0,\N,Missing
S14-2008,J02-3001,0,\N,Missing
S14-2008,W15-0128,1,\N,Missing
S14-2008,D07-1096,0,\N,Missing
S14-2008,cinkova-2006-propbank,0,\N,Missing
S15-2153,D12-1133,0,0.036307,"semantic dependencies distributed for the task. Systems in the open track, on the other hand, could use additional resources, such as a syntactic parser, for example—provided that they make sure to not use any tools or resources that encompass knowledge of the gold-standard syntactic or semantic analyses of the SDP 2015 test data.11 To simplify participation in the open track, the organizers prepared ready-touse ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in the form of Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). Finally, to more directly gauge the the contributions of syntactic structure on the semantic dependency parsing problem, an idealized gold track was introduced in SDP 2015. For this track, gold-standard syntactic companion files were provided in a varity of formats, viz. (a) Stanford Basic dependencies, derived from the PTB, (b) HPSG syntactic dependencies in the form called DM by Ivanova et al. (2012), derived from DeepBank, and (c) HPSG syntactic dependencies derived from the Enju Treebank. 6 Submissions and Results From almost 40 teams who had registered for the task, twelve teams obtaine"
S15-2153,cinkova-2006-propbank,1,0.772792,".6993 .5743 .6719 − .5630 .5675 .5490 − Table 2: Pairwise F1 similarities, including punctuation (upper right diagonals) or not (lower left). Frame or sense distinctions are a new property in SDP 2015 and currently are only available for the English DM and PSD data. Table 1 reveals a stark difference in granularity: DM limits itself to argument structure distinctions that are grammaticized, e.g. causative vs. inchoative contrasts or differences in the arity or coarse semantic typing of argument frames; PSD, on the other hand, draws on the much richer sense inventory of the EngValLex database (Cinková, 2006). Accordingly, the two target representations represent quite different challenges for the predicate disambiguation sub-task of SDP 2015. Finally, in Table 2 we seek to quantify pairwise structural similarity between the three representations in terms of unlabeled dependency F1 (dubbed UF in Section 5 below). We provide four variants of this metric, (a) taking into account the directionality of edges or not and (b) including edges involving punctuation marks or not. On this view, DM and PAS are structurally much closer to each other than either of the two is to PSD, even more so when discardin"
S15-2153,de-marneffe-etal-2006-generating,0,0.0580061,"Missing"
S15-2153,S14-2080,0,0.42538,"ably because the additional dependency parser they used was trained on data from the target domain. 7 Overview of Approaches Table 5 shows a summary of the tracks in which each submitted system participated, and Table 6 shows an overview of approaches and additionally used resources. All the teams except In-House submitted results for cross-lingual data (Czech and Chinese). Teams except Lisbon also tackled with predicate disambiguation. Only Turku participated in the Gold track. The submitted teams explored a variety of approaches. Riga and Peking relied on the graph-to-tree transformation of Du et al. (2014) as a basis. This method converts semantic dependency graphs into tree structures. Training data of semantic dependency 12 Please see the task web page at the address indicated above for full labeled and unlabeled scores. Team In-House Lisbon Minsk Peking Riga Turku Closed X X X X Open X X X Cross-Lingual Predicate Disambiguation Gold X X X X X X X X X X X Table 5: Summary of tracks in which submitted systems participated Team Approach Resources In-House Lisbon Minsk Peking Riga Turku grammar-based parsing (Miyao et al., 2014) graph parsing with dual decomposition (Martins & Almeida, 2014) tra"
S15-2153,hajic-etal-2012-announcing,1,0.91158,"Missing"
S15-2153,W12-3602,1,0.93906,"ctionality of edges or not and (b) including edges involving punctuation marks or not. On this view, DM and PAS are structurally much closer to each other than either of the two is to PSD, even more so when discarding punctuation. While relaxing the comparison to ignore edge directionality also increases similarity scores for this pair, the effect is much more pronounced when comparing either to PSD. This suggests that directionality of semantic dependencies is a major source of diversion between DM and PAS on the one hand, and PSD on the other hand. Linguistic Comparison Among other aspects, Ivanova et al. (2012) categorize a range of syntactic and semantic dependency annotation schemes according to the role that functional elements take. In Figure 1 and the discussion of Table 1 above, we already observed that PAS differs from the other representations in integrating into the graph auxiliaries, the infinitival marker, the case-marking preposition introducing the argument of apply (to), and most punctuation marks;9 while these (and other functional elements, e.g. complementizers) are analyzed as semantically vacuous in DM and PSD, they function as predicates in PAS, though do not always serve as ‘loca"
S15-2153,P10-5006,0,0.0554855,"R-arg ACT-arg PAT-arg RSTR EXT RSTR CONJ.m APPS.m ADDR-arg APPS.m CONJ.m A similar technique is almost impossible to apply to other crops , such as cotton , soybeans _ _ _ ev-w218f2 _ _ _ ev-w119f2 _ _ _ _ _ _ _ _ _ CONJ.m and _ rice . _ _ (c) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PSD). Figure 1: Sample semantic dependency graphs for Example (1). sentence’ semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Finally, a third related area of much interest is often dubbed ‘semantic parsing’, which Kate and Wong (2010) define as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” In contrast to much work in this tradition, our SDP target representations aim to be task- and domain-independent. 2 Target Representations We use three distinct target representations for semantic dependencies. As is evident in our running example (Figure 1), showing what are called the DM, PAS, and PSD semantic dependencies, there are contentful differences among these annotations, and there is of course not one obvious (o"
S15-2153,J93-2004,0,0.0679778,"stitute of Informatics, Tokyo Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics • Stanford University, Center for the Study of Language and Information ♠ ◦ sdp-organizers@emmtee.net Abstract more general graph processing, to thus enable a more direct analysis of Who did What to Whom? Extending the very similar predecessor task SDP 2014 (Oepen et al., 2014), we make use of three distinct, parallel semantic annotations over the same common texts, viz. the venerable Wall Street Journal (WSJ) and Brown segments of the Penn Treebank (PTB; Marcus et al., 1993) for English, as well as comparable resources for Chinese and Czech. Figure 1 below shows example target representations, bi-lexical semantic dependency graphs in all cases, for the WSJ sentence: Task 18 at SemEval 2015 defines BroadCoverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate–argument relationships for all content words, i.e. the semantic structure constituting the relational core of sentence meaning. In this task description, we position the problem in comparison to other language analysis sub-tasks, introduce and compare the semantic de"
S15-2153,S14-2082,0,0.0634842,"ormation of Du et al. (2014) as a basis. This method converts semantic dependency graphs into tree structures. Training data of semantic dependency 12 Please see the task web page at the address indicated above for full labeled and unlabeled scores. Team In-House Lisbon Minsk Peking Riga Turku Closed X X X X Open X X X Cross-Lingual Predicate Disambiguation Gold X X X X X X X X X X X Table 5: Summary of tracks in which submitted systems participated Team Approach Resources In-House Lisbon Minsk Peking Riga Turku grammar-based parsing (Miyao et al., 2014) graph parsing with dual decomposition (Martins & Almeida, 2014) transition-based dependency graph parsing in the spirit of Titov et al. (2009) (Du et al., 2014) extended with weighted tree approximation, parser ensemble (Du et al., 2014)’s graph-to-tree transformation, Mate, C6.0, parser ensemble sequence labeling for argument detection for each predicate, SVM classifiers for top node recognition and sense prediction ERG & Enju companion — — — companion Table 6: Overview of approaches and additional resources used (if any). graphs are converted into tree structures, and wellestablished parsing methods for tree structures are applied to converted structure"
S15-2153,meyers-etal-2004-annotating,0,0.0555434,"the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node re-entrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. Besides its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002).2 However, we require parsers to identify ‘full2 In much previous SRL work, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena—for example negation 915 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 915–926, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics top BV ARG3 ARG2 ARG1 ARG1 ARG1 ARG1 ARG1 mwe ARG2 conj _and_c A similar technique is almost impossible to apply to other crops , such as cotton, soybeans and rice . q:i-h-h a_to:e-i n:x _ a:e-h a_for:e-h-i _ v_to:e-i-p-i _ a:e-i n:x _ p:e-u-i p:e-u-i n:x n:x _ n:"
S15-2153,S14-2056,1,0.858363,"ndencies, there are contentful differences among these annotations, and there is of course not one obvious (or even objective) truth. Advancing in-depth comparison of representations and underlying design decisions, in fact, is among the moand other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—often remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inference-based techniques. 916 tivations for the SDP task series. Please see Oepen et al. (2014) and Miyao et al. (2014) for additional background. DM: DELPH-IN MRS-Derived Bi-Lexical Dependencies These semantic dependency graphs originate in a manual re-annotation, dubbed DeepBank, of Sections 00–21 of the WSJ Corpus and of selected parts of the Brown Corpus with syntacticosemantic analyses of the LinGO English Resource Grammar (Flickinger, 2000; Flickinger et al., 2012). For this target representation, top nodes designate the highest-scoping (non-quantifier) predicate in the graph, e.g. the (scopal) adverb almost in Figure 1.3 PAS: Enju Predicate–Argument Structures The Enju Treebank and parser4 are derived f"
S15-2153,S14-2008,1,0.363094,"Missing"
S15-2153,J05-1004,0,0.359612,"s preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node re-entrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. Besides its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002).2 However, we require parsers to identify ‘full2 In much previous SRL work, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena—for example negation 915 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 915–926, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics top BV ARG3 ARG2 ARG1 ARG1 ARG1 ARG1 ARG1 mwe ARG2 conj _and_c A similar technique is almost impossible to apply to other crops , such as cotton, soybeans and rice . q:i-h-h a_to:e-i n:x _ a:e-h a_for:e-h-i _ v_to:e-i-p-i _ a:e-i n:x _ p:e-u-"
S15-2153,P07-1031,0,0.0187644,"ersely, in PSD the last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected to the graph.10 A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure 2. Much noun phrase– internal structure is not made explicit in the PTB, and the Enju Treebank from which our PAS representation derives predates the bracketing work of Vadas and Curran (2007). In the four-way nominal compounding example of Figure 2, thus, PAS arrives at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of the compound either; PSD, on the other hand, annotates both the actual compound-internal bracketing and the assignment of roles, e.g. making stock the PAT(ient) of investment. In this spirit, the PSD annotations could be directly paraphrased along the lines of plans by employees for investment in stocks. In a middle position between the other two, DM disambiguates the bracketing but, by design, merely assigns"
U05-1001,W01-0906,0,0.015343,"development, whereby these laborintensive, long-lived resources undergo periodic modification intended to improve corpus coverage, or processing efficiency, or linguistic precision, or more typically all three at once (Oepen and Flickinger, 1998). Such a methodology must incorporate a set of disciplined procedures for validation of the resulting grammar, ensuring that it meets the expectations of its engineers, and explicitly communicates the interface specifications for its use in applications. In this talk I describe an instantiation of a method for natural language grammar validation (cf. (Barr and Klavans, 2001)) to identify and motivate the multiple dimensions of the procedure, and to illustrate how many of the the tools and techniques proposed in the NLP evaluation literature are being exploited by engineers of large deep grammars. For concreteness, I present the method used in maintaining and extending the English Resource Grammar (ERG (Flickinger, 2000), a semantically precise, broad-coverage Head-driven Phrase Structure Grammar (HPSG) implementation used for both parsing and generation in several NLP applications, being developed within the Deep Linguistic Processing with HPSG Initiative (DELPH-"
U05-1001,W04-3317,0,0.0121749,"ammar to the next release. There is by now a substantial literature on many aspects of the evaluation of NLP software, including grammars, much of it focused around ’black box’ or functional evaluation within some specific task domain(Hirschman and Thompson, 1998), (Sparck Jones and Galliers, 1998). Methods and tools have also been developed for ’glass box’ or structural evaluation (EAGLES, 1996), with one line of work analyzing the internal formal coherence of such deep grammars, for example to flag inconsistencies or identify unused rules or constraints in the grammar code (Broeker, 2000), (Barr and Siefring, 2004), and another line of work using paraphrase generation to illuminate properties of a bi-directional grammar not easily detected when parsing (Dymetman and Isabelle, 1988). And finally, there has been work on developing a methodology of sustained grammar development, whereby these laborintensive, long-lived resources undergo periodic modification intended to improve corpus coverage, or processing efficiency, or linguistic precision, or more typically all three at once (Oepen and Flickinger, 1998). Such a methodology must incorporate a set of disciplined procedures for validation of the resultin"
U05-1001,C00-1018,0,0.0304975,"e release of a grammar to the next release. There is by now a substantial literature on many aspects of the evaluation of NLP software, including grammars, much of it focused around ’black box’ or functional evaluation within some specific task domain(Hirschman and Thompson, 1998), (Sparck Jones and Galliers, 1998). Methods and tools have also been developed for ’glass box’ or structural evaluation (EAGLES, 1996), with one line of work analyzing the internal formal coherence of such deep grammars, for example to flag inconsistencies or identify unused rules or constraints in the grammar code (Broeker, 2000), (Barr and Siefring, 2004), and another line of work using paraphrase generation to illuminate properties of a bi-directional grammar not easily detected when parsing (Dymetman and Isabelle, 1988). And finally, there has been work on developing a methodology of sustained grammar development, whereby these laborintensive, long-lived resources undergo periodic modification intended to improve corpus coverage, or processing efficiency, or linguistic precision, or more typically all three at once (Oepen and Flickinger, 1998). Such a methodology must incorporate a set of disciplined procedures for"
U05-1001,1988.tmi-1.12,0,0.265883,"black box’ or functional evaluation within some specific task domain(Hirschman and Thompson, 1998), (Sparck Jones and Galliers, 1998). Methods and tools have also been developed for ’glass box’ or structural evaluation (EAGLES, 1996), with one line of work analyzing the internal formal coherence of such deep grammars, for example to flag inconsistencies or identify unused rules or constraints in the grammar code (Broeker, 2000), (Barr and Siefring, 2004), and another line of work using paraphrase generation to illuminate properties of a bi-directional grammar not easily detected when parsing (Dymetman and Isabelle, 1988). And finally, there has been work on developing a methodology of sustained grammar development, whereby these laborintensive, long-lived resources undergo periodic modification intended to improve corpus coverage, or processing efficiency, or linguistic precision, or more typically all three at once (Oepen and Flickinger, 1998). Such a methodology must incorporate a set of disciplined procedures for validation of the resulting grammar, ensuring that it meets the expectations of its engineers, and explicitly communicates the interface specifications for its use in applications. In this talk I"
U05-1001,2005.mtsummit-papers.22,1,0.822396,"initions, and robust interpretation of transcribed conversations via enriched annotations of rhetorical relations. Finally, for the grammar to be of use to application developers without an overly intimate knowledge of it, each release must be accom2 panied by (10) external interface specifications whose currency must be validated. The content of these specifications is in part automatically generated from the implemented representations of lexical entries, lexical types and grammar rules, and in part manually maintained. Principal among these specifications is the SEM-I (semantic interface) (Flickinger et al., 2005), an exhaustive listing of each lexical semantic predicate and its salient properties, which should precisely determine the observed variation in the elementary predications within the Minimal Recursion Semantics (MRS (Copestake et al., 2006)) representations that the grammar produces or accepts as input. A second essential specification provides the set of available lexical entry types, particularly those for open-class words which will inevitably need to be added for each new application, whether automatically guessed via part-of-speech tagging or entered into the lexicon manually. In the ta"
W01-1512,W99-0802,0,0.127485,"been used for teaching. Besides the LKB, typed feature structure environments have been used at many universities, though unlike the systems cited above, most have only been used with small grammars and may not scale up. Hands on courses using various systems have been run at many recent summer schools including ESSLLI 99 (using the Xerox XLE, see Butt et al, 1999) and ESSLLI 97 and the 1999 LSA summer school (both using ConTroll, see Hinrichs and Meurers, 1999). Very little seems to have been formally published describing experiences in teaching with grammar development environments, though Bouma (1999) describes material for teaching a computational linguistics course that includes exercises using the Hdrug unification-based enviroment to extend a grammar. Despite this rich variety of tools, we believe that the LKB system has a combination of features which make it distinctive and give it a useful niche in teaching. The most important points are that its availability as open source, combined with scale and efficiency, allow advanced projects to be supported as well as introductory courses. As far as we are aware, it is the only system freely available with a broad coverage grammar that supp"
W01-1512,copestake-flickinger-2000-open,1,0.684868,"tware University of Groningen, CSLI, Stanford University Ventura Hall, Postbus 716, 9700 AS Groningen, 110 Pioneer Way Stanford, USA The Netherlands Mountain View, USA danf@csli.stanford.edu malouf@let.rug.nl Abstract We demonstrate the open-source LKB system which has been used to teach the fundamentals of constraint-based grammar development to several groups of students. 1 Overview of the LKB system The LKB system is a grammar development environment that is distributed as part of the open source LinGO tools (http://wwwcsli.stanford.edu/˜aac/lkb.html and http://lingo.stanford.edu, see also Copestake and Flickinger, 2000). It is an open-source grammar development environment implemented in Common Lisp, distributed not only as source but also as a standalone application that can be run on Linux, Solaris and Windows (see the website for specific requirements). It will also run under Macintosh Common Lisp, but for this a license is required. The LKB includes a parser, generator, support for large-scale inheritance hierarchies (including the use of defaults), various tools for manipulating semantic representations, a rich set of graphical tools for analyzing and debugging grammars, and extensive on-line documentat"
W02-1502,C00-1004,0,0.00894855,"system developed at Tokyo University (Makino, Yoshida, Torisawa, & Tsujii, 1998), and a parallel processing system developed in Objective C at Delft University (The Netherlands; van Lohuizen, 2002). As part of the matrix package, sample configuration files and documentation will be provided for at least some of these additional platforms. Existing pre-processing packages can also significantly reduce the effort required to develop a new grammar, particularly for coping with the morphology/syntax interface. For example, the ChaSen package for segmenting Japanese input into words and morphemes (Asahara & Matsumoto, 2000) has been linked to at least the LKB and PET systems. Support for connecting implementations of language-specific pre-processing packages of this kind will be preserved and extended as the matrix develops. Likewise, configuration files are included to support generation, at least within the LKB, provided that the grammar conforms to certain assumptions about semantic representation using the Minimal Recursion Semantics framework. Finally, a methodology is under development for constructing and using test suites organized around a typology of linguistic phenomena, using the implementation platf"
W02-1502,1995.tmi-1.2,1,0.665663,"t constituent structure (in one view, Norwegian exhibits a VSO topology in the main clause). The user groups have suggested refinements and extensions of the basic inventory, and it is expected that general solutions, as they are identified jointly, will propagate into the existing grammars too. 3 A Detailed Example As an example of the level of detail involved in the grammar matrix, in this section we consider the analysis of intersective and scopal modification. The matrix is built to give Minimal Recursion Semantics (MRS; Copestake et al., 2001; Copestake, Flickinger, Sag, & Pollard, 1999; Copestake, Flickinger, Malouf, Riehemann, & Sag, 1995) representations. The two English examples in (1) exemplify the difference between intersective and scopal modification:1 (1) a. Keanu studied Kung Fu on a spaceship. b. Keanu probably studied Kung Fu. The MRSs for (1a-b) (abstracting away from agreement information) are given in (2) and (3). The MRSs are ordered tuples consisting of a top handle (h1 in both cases), an instance or event variable (e in both cases), a bag of elementary predications (eps), and a bag of scope constraints (in these cases, QEQ constraints or ‘equal modulo quantifiers’). In a well-formed MRS, the handles can be 1 Th"
W02-1502,P01-1019,1,0.872623,"t we foresee and an evaluation methodology for the matrix proper. 2 Preliminary Development of Matrix We have produced a preliminary version of the grammar matrix relying heavily on the LinGO project’s English Resource Grammar, and to a lesser extent on the Japanese grammar developed jointly between DFKI Saarbr¨ucken (Germany) and YY Technologies (Mountain View, CA). This early version of the matrix comprises the following components:  Types defining the basic feature geometry and technical devices (e.g., for list manipulation).  Types associated with Minimal Recursion Semantics (see, e.g., Copestake, Lascarides, & Flickinger, 2001), a meaning representation language which has been shown to be wellsuited for semantic composition in typed feature structure grammars. This portion of the grammar matrix includes a hierarchy of relation types, types and constraints for the propagation of semantic information through the phrase structure tree, a representation of illocutionary force, and provisions for grammar rules which make semantic contributions.  General classes of rules, including derivational and inflectional (lexical) rules, unary and binary phrase structure rules, headed and non-headed rules, and head-initial and he"
W02-1502,P98-2132,0,0.049922,"Missing"
W02-1502,2000.iwpt-1.19,1,0.705383,"ecting implementations of language-specific pre-processing packages of this kind will be preserved and extended as the matrix develops. Likewise, configuration files are included to support generation, at least within the LKB, provided that the grammar conforms to certain assumptions about semantic representation using the Minimal Recursion Semantics framework. Finally, a methodology is under development for constructing and using test suites organized around a typology of linguistic phenomena, using the implementation platform of the [incr tsdb()] profiling package (Oepen & Flickinger, 1998; Oepen & Callmeier, 2000). These test suites will enable better communication about current coverage of a given grammar built using the matrix, and serve as the basis for identifying additional phenomena that need to be addressed cross-linguistically within the matrix. Of course, the development of the typology of phenomena is itself a major undertaking for which a systematic cross-linguistic approach will be needed, a discussion of which is outside the scope of this report. But the intent is to seed this classification scheme with a set of relatively coarse-grained phenomenon classes drawn from the existing grammars,"
W02-1502,C02-2025,1,0.16955,"t are important to sync to (e.g., changes that affect MRS outputs, fundamental changes to important analyses), (ii) develop a methodology for communicating changes in the matrix, their motivation and their implementation to the user community, and (iii) develop tools for semi-automating resynching of existing grammars to upgrades of the matrix. These tools could use the type hierarchy to predict where conflicts are likely to arise and bring these to the engineer’s attention, possibly inspired by the approach under development at CSLI for the dynamic maintenance of the LinGO Redwoods treebank (Oepen et al., 2002). Finally, while initial development of the matrix has been and will continue to be highly centralized, we hope to provide support for proposed matrix improvements from the user community. User feedback will already come in the form of case studies for the library as discussed in Section 5 above, but also potentially in proposals for modification of the matrix drawing on experiences in grammar development. In order to provide users with some cross-linguistic context in which to develop and evaluate such proposals themselves, we intend to provide some sample matrix-derived grammars and correspo"
W02-1502,W02-1210,1,0.532935,"necessary for robust natural language processing and the precision parses and semantic representations necessary for natural language understanding. 1 Introduction The past decade has seen the development of wide-coverage implemented grammars representing deep linguistic analysis of several languages in several frameworks, including Head-Driven Phrase Structure Grammar (HPSG), LexicalFunctional Grammar (LFG), and Lexicalized Tree Adjoining Grammar (LTAG). In HPSG, the most extensive grammars are those of English (Flickinger, 2000), German (M¨uller & Kasper, 2000), and Japanese (Siegel, 2000; Siegel & Bender, 2002). Despite being couched in the same general framework and in some cases being written in the same formalism and consequently being compatible with the same parsing and generation software, these grammars were developed more or less independently of each other. They each represent between 5 and 15 person years of research efforts, and comprise 35–70,000 lines of code. Unfortunately, most of that research is undocumented and the accumulated analyses, best practices for grammar engineering, and tricks of the trade are only available through painstaking inspection of the grammars and/or consultati"
W02-1502,C98-2128,0,\N,Missing
W02-1508,C00-1004,0,0.015174,"ammarians shifted their engineering focus on ‘tightening’ the grammar, i.e. the elimination of spurious ambiguity and overgeneration (see Siegel & Bender, 2002, for details on the grammar). Another view on grammar evolution is presented in Figure 3, depicting the ‘size’ of the Japanese grammar over the same five-month development cycle. Although measuring the size of 2 Quantifying input complexity for Japanese is a nontrivial task, as the count of the number of input words would depend on the approach to string segmentation used in a specific system (the fairly aggressive tokenizer of ChaSen, Asahara & Matsumoto, 2000, in our case); to avoid potential for confusion, we report input complexity in the (overtly systemspecific) number of lexical items stipulated by the grammar instead: around 50 and 80, on average, for the ‘banking’ and ‘trading’ data sets, respectively (as of February 2002). Grammar Size 10200 10000 9800 9600 9400 9200 9000 8800  •••••• • ••• •  • ••• • •• • •••   •••   •   •         • ••• •• ••  ••••••  ••••   ••    •• • (generated   by [incr tsdb()] at 30-jun-2002 (16:09 h)) •• • • — types  — rules 106 104 102 100 98 96 94 92 90"
W02-1508,P98-2132,0,0.02331,"on the following components and modules: • test and reference data stored with annotations in a structured database; annotations can range from minimal information (unique test item identifier, item origin, length et al.) to fine-grained linguistic classifications (e.g. regarding grammaticality and linguistic phenomena presented in an item), as they are represented in the TSNLP test suites, for example (Oepen, Netter, & Klein, 1997); • tools to browse the available data, identify suitable subsets and feed them through the analysis component of processing systems like the LKB and PET, LiLFeS (Makino, Yoshida, Torisawa, & Tsujii, 1998), TRALE (Penn, 2000), PAGE (Uszkoreit et al., 1994), and others; • the ability to gather a multitude of precise and fine-grained (grammar) competence and (system) performance measures—like the number of readings obtained per test item, various time and memory usage statistics, ambiguity and non-determinism metrics, and salient properties of the result structures—and store them in a uniform, platform-independent data format as a competence and performance profile; and • graphical facilities to inspect the resulting profiles, analyze system competence (i.e. grammatical coverage and overgenerati"
W02-1508,2000.iwpt-1.19,1,0.834837,"mmar development cannot be isolated from measurements of system resource consumption and overall performance (specific properties of a grammar may trigger idiosyncrasies or software bugs in a particular version of the processing system); therefore, and to enable exchange of reference points and comparability of experiments, grammarians and system developers alike should use the same, homogenuous set of relevant parameters. 3 Integrated Competence and Performance Profiling The integrated competence and performance profiling methodology and associated engineering platform, dubbed [incr tsdb()] (Oepen & Callmeier, 2000)1 and reviewed in the remainder of this sec1 See ‘http://www.coli.uni-sb.de/itsdb/’ for the (draft) [incr tsdb()] user manual, pronunciation rules, and instructions on obtaining and installing the package. tion, was designed to meet all of the requirements identified in the DFKI – YY case study. Generally speaking, the [incr tsdb()] environment is an integrated package for diagnostics, evaluation, and benchmarking in practical grammar and system engineering. The toolkit implements an approach to grammar development and system optimization that builds on precise empirical data and systematic ex"
W02-1508,W02-1210,1,0.838186,"overall average of readings assigned to each sentence varies around relatively small numbers. For the moderately complex email data2 the grammar often assigns less than ten analyses, rarely more than a few dozens. However, not surprisingly the addition of grammatical coverage comes with a sharp increase in ambiguity (which may indicate overgeneration): the graphs in Figure 2 clearly show that, once coverage on the ‘trading’ data was above eighty per cent, grammarians shifted their engineering focus on ‘tightening’ the grammar, i.e. the elimination of spurious ambiguity and overgeneration (see Siegel & Bender, 2002, for details on the grammar). Another view on grammar evolution is presented in Figure 3, depicting the ‘size’ of the Japanese grammar over the same five-month development cycle. Although measuring the size of 2 Quantifying input complexity for Japanese is a nontrivial task, as the count of the number of input words would depend on the approach to string segmentation used in a specific system (the fairly aggressive tokenizer of ChaSen, Asahara & Matsumoto, 2000, in our case); to avoid potential for confusion, we report input complexity in the (overtly systemspecific) number of lexical items s"
W02-1508,C94-1072,1,0.847612,"ored with annotations in a structured database; annotations can range from minimal information (unique test item identifier, item origin, length et al.) to fine-grained linguistic classifications (e.g. regarding grammaticality and linguistic phenomena presented in an item), as they are represented in the TSNLP test suites, for example (Oepen, Netter, & Klein, 1997); • tools to browse the available data, identify suitable subsets and feed them through the analysis component of processing systems like the LKB and PET, LiLFeS (Makino, Yoshida, Torisawa, & Tsujii, 1998), TRALE (Penn, 2000), PAGE (Uszkoreit et al., 1994), and others; • the ability to gather a multitude of precise and fine-grained (grammar) competence and (system) performance measures—like the number of readings obtained per test item, various time and memory usage statistics, ambiguity and non-determinism metrics, and salient properties of the result structures—and store them in a uniform, platform-independent data format as a competence and performance profile; and • graphical facilities to inspect the resulting profiles, analyze system competence (i.e. grammatical coverage and overgeneration) and performance (e.g. cpu time and memory usage,"
W02-1508,C96-2120,1,\N,Missing
W02-1508,P94-1040,0,\N,Missing
W02-1508,C98-2128,0,\N,Missing
W06-0502,C04-1185,0,0.0423415,"Missing"
W06-0502,1991.mtsummit-papers.16,0,0.0610952,"Missing"
W06-0502,W02-2016,0,0.013304,"part-of-speech tagger, ChaSen (Matsumoto et al., 2000) was used for shallow processing of Japanese. Predicate names were produced by transliterating the pronunciation field and mapping the part-of-speech codes to the RMRS super types. The part-of-speech codes were also used to judge whether predicates were real or grammatical. Since Japanese is a head-final language, the hook value was set to be the handle of the right-most real predicate. This is easy to do for Japanese, but difficult for English. Medium Parser (CaboCha-RMRS) For Japanese, we produce RMRS from the dependency parser Cabocha (Kudo and Matsumoto, 2002). The method is similar to that of Spreyer and Frank (2005), who produce RMRS from detailed German dependencies. CaboCha provides fairly minimal dependencies: there are three links (dependent, parallel, apposition) and they link base phrases (Japanese bunsetsu), marked with the syntactic and semantic head. The CaboChaRMRS parser uses this information, along with heuristics based on the parts-of-speech, to produce underspecified RMRSs. CaboCha-RMRS is capable of making use of HPSG resources, including verbal case frames, to further enrich its output. This allows it to produce RMRS that approach"
W06-0502,P98-2180,0,0.0441011,"yet to be seen how the rules will scale when deeper semantic relations are extracted. In comparison, as we will demonstrate, our system produces comparable results while the framework is immediately applicable to any language with the resources to produce RMRS. Advances in the state-of-the-art in parsing have made it practical to use deep processing systems that produce rich syntactic and semantic analyses to parse lexicons. This high level of semantic information makes it easy to identify the relations between words that make up an ontology. Such an approach was taken by the MindNet project (Richardson et al., 1998). However, deep parsing systems often suffer from small lexicons and large amounts of parse ambiguity, making it difficult to apply this knowledge broadly. Our ontology extraction system uses Robust Minimal Recursion Semantics (RMRS), a formalism that provides a high level of detail while, at the same time, allowing for the flexibility of underspecification. RMRS encodes syntactic information in a general enough manner to make processing of and extraction from syntactic phenomena including coordination, relative clause analyIn this paper, we outline the development of a system that automatical"
W06-0502,I05-6001,0,0.0259576,"used for shallow processing of Japanese. Predicate names were produced by transliterating the pronunciation field and mapping the part-of-speech codes to the RMRS super types. The part-of-speech codes were also used to judge whether predicates were real or grammatical. Since Japanese is a head-final language, the hook value was set to be the handle of the right-most real predicate. This is easy to do for Japanese, but difficult for English. Medium Parser (CaboCha-RMRS) For Japanese, we produce RMRS from the dependency parser Cabocha (Kudo and Matsumoto, 2002). The method is similar to that of Spreyer and Frank (2005), who produce RMRS from detailed German dependencies. CaboCha provides fairly minimal dependencies: there are three links (dependent, parallel, apposition) and they link base phrases (Japanese bunsetsu), marked with the syntactic and semantic head. The CaboChaRMRS parser uses this information, along with heuristics based on the parts-of-speech, to produce underspecified RMRSs. CaboCha-RMRS is capable of making use of HPSG resources, including verbal case frames, to further enrich its output. This allows it to produce RMRS that approaches the granularity of the analyses given by 3 Ontology Cons"
W06-0502,W04-2209,0,0.0703215,"Missing"
W06-0502,callmeier-etal-2004-deepthought,0,0.0344792,"Missing"
W06-0502,C98-2175,0,\N,Missing
W08-1304,E03-1025,0,0.0195004,"lard and Sag (1994)). We even find variation in the assignments of part-of-speech tags for individual tokens, for example with words like “missionary” or “classical” treated as adjectives in some of the annotations and as nouns in others. Furthermore, a simple labelled bracketing of surface tokens obscures the fact that a single syntactic constituent can fill multiple roles in the logical structure expressed by a sentence, as with controlled subjects, relative clauses, appositives, coordination, etc. More detailed discussions of the obstacles to directly comparing syntactic structures include Preiss (2003), Clark and Curran (2007), and most recently Sagae et al. (2008). Since it is this underlying logical content that we seek when parsing a sentence, the target annotation for cross-framework comparison should not include marking of syntactic constituents, but focus instead on the predicate argument structures determined by the syntactic analysis, as proposed ten years ago by Carroll et al. (1998). Several of 1 Introduction Efficient and precise comparison of parser results across frameworks will require a negotiated agreement on a target representation which embodies a good balance of three com"
W08-1304,W03-1013,0,0.0284037,"ap in the content of the various schemes for these sentences, no one of the schemes is ideal. This paper presents some desiderata for a negotiated target annotation scheme for which straightforward mappings can be constructed from each of the supplied annotation schemes. Competing linguistic frameworks can vary dramatically in the syntactic structures they assign to sentences, and this variation makes cross-framework comparison of labelled bracketings difficult and in the limit uninteresting. The syntactic structures of Combinatory Categorial Grammar (CCG: Steedman (2000), Hockenmaier (2003), Clark and Curran (2003)), for example, contrast sharply with those of the Penn Treebank Marcus et al. (1993), and the PTB structures differ in many less dramatic though equally important details from those assigned in Lexical Functional Grammar (LFG: Bresnan and Kaplan (1982)) or Head-driven Phrase Structure Grammar (HPSG: Pollard and Sag (1994)). We even find variation in the assignments of part-of-speech tags for individual tokens, for example with words like “missionary” or “classical” treated as adjectives in some of the annotations and as nouns in others. Furthermore, a simple labelled bracketing of surface tok"
W08-1304,P07-1032,0,0.0136727,"nd variation in the assignments of part-of-speech tags for individual tokens, for example with words like “missionary” or “classical” treated as adjectives in some of the annotations and as nouns in others. Furthermore, a simple labelled bracketing of surface tokens obscures the fact that a single syntactic constituent can fill multiple roles in the logical structure expressed by a sentence, as with controlled subjects, relative clauses, appositives, coordination, etc. More detailed discussions of the obstacles to directly comparing syntactic structures include Preiss (2003), Clark and Curran (2007), and most recently Sagae et al. (2008). Since it is this underlying logical content that we seek when parsing a sentence, the target annotation for cross-framework comparison should not include marking of syntactic constituents, but focus instead on the predicate argument structures determined by the syntactic analysis, as proposed ten years ago by Carroll et al. (1998). Several of 1 Introduction Efficient and precise comparison of parser results across frameworks will require a negotiated agreement on a target representation which embodies a good balance of three competing dimensions: consis"
W08-1304,J93-2004,0,\N,Missing
W12-3602,branco-etal-2010-developing,0,0.0137066,"Specifically, the Deep Linguistic Processing with HPSG Initiative (DELPH-IN1 ) has produced both manually and automatically annotated resources making available comparatively fine-grained syntactic and semantic analyses in the framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard & Sag, 1994). For English, the so-called LinGO Redwoods Treebank (Oepen et al., 2004) contains gold-standard annotations for some 45,000 utterances in five broad genres and domains; comparable resources exist for Japanese (Bond et al., 2004) and are currently under construction for Portuguese and Spanish (Branco et al., 2010; Marimon, 2010). We develop an automated, parameterizable conversion procedure for these resources that maps HPSG analyses into either syntactic or semantic bilexical dependencies. Similar conversion procedures have recently been formulated for functional structures within the LFG framework (Øvrelid et al., 2009; Cetinoglu et al., 2010). In the design of this unidirectional (i.e. lossy) mapping, we apply and corroborate the cross-framework observations made in the more linguistic part of this study. The paper has the following structure: Section 2 introduces the corpus and annotations we take"
W12-3602,P05-1067,0,0.0176179,"ilar information can take quite different forms across frameworks. We further seek to shed light on the representational ‘distance’ between pure bilexical dependencies, on the one hand, and full-blown logical-form propositional semantics, on the other hand. Furthermore, we propose a fully automated conversion procedure from (logical-form) meaning representation to bilexical semantic dependencies.† 1 Introduction—Motivation Dependency representations have in recent years received considerable attention from the NLP community, and have proven useful in diverse tasks such as Machine Translation (Ding & Palmer, 2005), Semantic Search (Poon & Domingos, 2009), and Sentiment Analysis (Wilson et al., 2009). Dependency representations are often claimed to be more ‘semantic’ in spirit, in the sense that they directly express predicate–argument relations, i.e. Who did What to Whom? Several of the shared tasks of the Conference on Natural Language Learning (CoNLL) in the past years have focused on datadriven dependency parsing—producing both syntactic (Nivre et al., 2007) and semantic dependencies (Hajiˇc et al., 2009)—and have made available gold † We are indebted to Emily Bender, Rebecca Dridan, and four anonym"
W12-3602,flickinger-etal-2010-wikiwoods,1,0.847029,"Missing"
W12-3602,W09-1201,0,0.0731308,"Missing"
W12-3602,W07-2416,0,0.121834,"e in recent years received considerable attention from the NLP community, and have proven useful in diverse tasks such as Machine Translation (Ding & Palmer, 2005), Semantic Search (Poon & Domingos, 2009), and Sentiment Analysis (Wilson et al., 2009). Dependency representations are often claimed to be more ‘semantic’ in spirit, in the sense that they directly express predicate–argument relations, i.e. Who did What to Whom? Several of the shared tasks of the Conference on Natural Language Learning (CoNLL) in the past years have focused on datadriven dependency parsing—producing both syntactic (Nivre et al., 2007) and semantic dependencies (Hajiˇc et al., 2009)—and have made available gold † We are indebted to Emily Bender, Rebecca Dridan, and four anonymous reviewers for their feedback on this work. standard data sets (dependency banks) for a range of different languages. These data sets have enabled rigorous evaluation of parsers and have spurred considerable progress in the field of data-driven dependency parsing (McDonald & Nivre, 2011). Despite widespread use, dependency grammar does not represent a unified grammatical framework and there are large representational differences across communities,"
W12-3602,P03-1054,0,0.00636733,"he representation does not adhere to the formal constraints posed above; it lacks a designated root node, the graph is not connected, and the graph is not acyclic. The choices with respect to head status are largely substantive. The dependency relations employed for this representation are PropBank semantic roles, such as A0 (proto-agent), A1 (protopatient), and various modifier roles. Stanford Basic Dependencies (SB) The Stanford Dependency scheme, a popular alternative to CoNLL-style syntactic dependencies (CD), was originally provided as an additional output format for the Stanford parser (Klein & Manning, 2003). It is a result of a conversion from PTB-style phrase structure trees (be they gold standard or automatically produced)—combining ‘classic’ head finding rules with rules that target specific linguistic constructions, such as passives or attributive adjectives (Marneffe et al., 2006). The so-called basic format provides a dependency graph which conforms to the criteria listed above, and the heads are largely content rather than function words. The grammatical relations are organized in a hierarchy, rooted in the generic relation ‘dependent’ and containing 56 different relations (Marneffe & Man"
W12-3602,J93-2004,0,0.0479705,"pendency banks) for a range of different languages. These data sets have enabled rigorous evaluation of parsers and have spurred considerable progress in the field of data-driven dependency parsing (McDonald & Nivre, 2011). Despite widespread use, dependency grammar does not represent a unified grammatical framework and there are large representational differences across communities, frameworks, and languages. Moreover, many of the gold-standard dependency banks were created by automated conversion from pre-existing constituency treebanks— notably the venerable Penn Treebank for English (PTB; Marcus et al., 1993)—and there exist several conversion toolkits which convert from constituent structures to dependency structures. This conversion is not always trivial, and the outputs can differ notably in choices concerning head status, relation inventories, and formal graph properties of the resulting depedency structure. Incompatibilty of representations and differences in the ‘granularity’ of linguistic information hinder the evaluation of parsers across communities (Sagae et al., 2008). In this paper, we pursue theoretical as well as practical goals. First, we hope to shed more light on commonalities and"
W12-3602,marimon-2010-spanish,0,0.0134134,"ep Linguistic Processing with HPSG Initiative (DELPH-IN1 ) has produced both manually and automatically annotated resources making available comparatively fine-grained syntactic and semantic analyses in the framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard & Sag, 1994). For English, the so-called LinGO Redwoods Treebank (Oepen et al., 2004) contains gold-standard annotations for some 45,000 utterances in five broad genres and domains; comparable resources exist for Japanese (Bond et al., 2004) and are currently under construction for Portuguese and Spanish (Branco et al., 2010; Marimon, 2010). We develop an automated, parameterizable conversion procedure for these resources that maps HPSG analyses into either syntactic or semantic bilexical dependencies. Similar conversion procedures have recently been formulated for functional structures within the LFG framework (Øvrelid et al., 2009; Cetinoglu et al., 2010). In the design of this unidirectional (i.e. lossy) mapping, we apply and corroborate the cross-framework observations made in the more linguistic part of this study. The paper has the following structure: Section 2 introduces the corpus and annotations we take as 1 See http:/"
W12-3602,de-marneffe-etal-2006-generating,0,0.00979249,"on are PropBank semantic roles, such as A0 (proto-agent), A1 (protopatient), and various modifier roles. Stanford Basic Dependencies (SB) The Stanford Dependency scheme, a popular alternative to CoNLL-style syntactic dependencies (CD), was originally provided as an additional output format for the Stanford parser (Klein & Manning, 2003). It is a result of a conversion from PTB-style phrase structure trees (be they gold standard or automatically produced)—combining ‘classic’ head finding rules with rules that target specific linguistic constructions, such as passives or attributive adjectives (Marneffe et al., 2006). The so-called basic format provides a dependency graph which conforms to the criteria listed above, and the heads are largely content rather than function words. The grammatical relations are organized in a hierarchy, rooted in the generic relation ‘dependent’ and containing 56 different relations (Marneffe & Manning, 2008), largely based on syntactic functions. sb-hd_mc_c { e12 sp-hd_n_c hd-cmp_u_c v_prd_is_le d_-_sg-nmd_le 1 :_a_q( BV x 6 ) e9 :_similar_a_to(ARG1 x 6 ) x 6 :_technique_n_1 e12 :_almost_a_1(ARG1 e3 ) e3 :_impossible_a_for(ARG1 e18 ) e18 :_apply_v_to(ARG2 x 6 , ARG3 x 19 ) 2"
W12-3602,W08-1301,0,0.126868,"Missing"
W12-3602,J11-1007,0,0.0165324,"ral of the shared tasks of the Conference on Natural Language Learning (CoNLL) in the past years have focused on datadriven dependency parsing—producing both syntactic (Nivre et al., 2007) and semantic dependencies (Hajiˇc et al., 2009)—and have made available gold † We are indebted to Emily Bender, Rebecca Dridan, and four anonymous reviewers for their feedback on this work. standard data sets (dependency banks) for a range of different languages. These data sets have enabled rigorous evaluation of parsers and have spurred considerable progress in the field of data-driven dependency parsing (McDonald & Nivre, 2011). Despite widespread use, dependency grammar does not represent a unified grammatical framework and there are large representational differences across communities, frameworks, and languages. Moreover, many of the gold-standard dependency banks were created by automated conversion from pre-existing constituency treebanks— notably the venerable Penn Treebank for English (PTB; Marcus et al., 1993)—and there exist several conversion toolkits which convert from constituent structures to dependency structures. This conversion is not always trivial, and the outputs can differ notably in choices conc"
W12-3602,J05-1004,0,0.0583595,"ST collection were obtained by converting PTB trees with the PennConverter software (Johansson & Nugues, 2007), which relies on head finding rules (Magerman, 1994; Collins, 1999) and the functional anno3 Among others, annotations in the Prague Dependency format would be interesting to compare to, but currently these are unfortunately not among the formats represented in the PEST corpus. 4 CoNLL PropBank Semantics (CP) For the 2008 CoNLL shared task on joint learning of syntactic and semantic dependencies (Surdeanu et al., 2008), the PropBank and NomBank annotations ‘on top’ of the PTB syntax (Palmer et al., 2005; Meyers et al., 2004) were converted to bilexical dependency form. This conversion was based on the dependency syntax already obtained for the same data set (CD, above) and heuristics which identify the semantic head of an argument with its syntactic head.The conversion further devotes special attention to arguments with several syntactic heads, discontinuous arguments, and empty categories (Surdeanu et al., 2008). The representation does not adhere to the formal constraints posed above; it lacks a designated root node, the graph is not connected, and the graph is not acyclic. The choices wit"
W12-3602,W04-2705,0,0.0333196,"tained by converting PTB trees with the PennConverter software (Johansson & Nugues, 2007), which relies on head finding rules (Magerman, 1994; Collins, 1999) and the functional anno3 Among others, annotations in the Prague Dependency format would be interesting to compare to, but currently these are unfortunately not among the formats represented in the PEST corpus. 4 CoNLL PropBank Semantics (CP) For the 2008 CoNLL shared task on joint learning of syntactic and semantic dependencies (Surdeanu et al., 2008), the PropBank and NomBank annotations ‘on top’ of the PTB syntax (Palmer et al., 2005; Meyers et al., 2004) were converted to bilexical dependency form. This conversion was based on the dependency syntax already obtained for the same data set (CD, above) and heuristics which identify the semantic head of an argument with its syntactic head.The conversion further devotes special attention to arguments with several syntactic heads, discontinuous arguments, and empty categories (Surdeanu et al., 2008). The representation does not adhere to the formal constraints posed above; it lacks a designated root node, the graph is not connected, and the graph is not acyclic. The choices with respect to head stat"
W12-3602,D09-1001,0,0.0165104,"nt forms across frameworks. We further seek to shed light on the representational ‘distance’ between pure bilexical dependencies, on the one hand, and full-blown logical-form propositional semantics, on the other hand. Furthermore, we propose a fully automated conversion procedure from (logical-form) meaning representation to bilexical semantic dependencies.† 1 Introduction—Motivation Dependency representations have in recent years received considerable attention from the NLP community, and have proven useful in diverse tasks such as Machine Translation (Ding & Palmer, 2005), Semantic Search (Poon & Domingos, 2009), and Sentiment Analysis (Wilson et al., 2009). Dependency representations are often claimed to be more ‘semantic’ in spirit, in the sense that they directly express predicate–argument relations, i.e. Who did What to Whom? Several of the shared tasks of the Conference on Natural Language Learning (CoNLL) in the past years have focused on datadriven dependency parsing—producing both syntactic (Nivre et al., 2007) and semantic dependencies (Hajiˇc et al., 2009)—and have made available gold † We are indebted to Emily Bender, Rebecca Dridan, and four anonymous reviewers for their feedback on this"
W12-3602,read-etal-2012-wesearch,1,0.863524,"Missing"
W12-3602,oepen-lonning-2006-discriminant,1,0.569818,"ries. In Section 4 below, we convert DELPH-IN derivations into syntactic bilexical dependencies. DELPH-IN Minimal Recursion Semantics (DM) As part of the full HPSG sign, the ERG also makes available a logical-form representation of propositional semantics in the format of Minimal Recursion Semantics (MRS; Copestake et al., 2005). While MRS proper utilizes a variant of predicate calculus that affords underspecification of scopal relations, for our goal of projecting semantic forms onto bilexical dependencies, we start from the reduction of MRS into the Elementary Dependency Structures (EDS) of Oepen & Lønning (2006), as shown in Figure 2. EDS is a lossy (i.e. non-reversible) conversion from MRS into a variable-free dependency graph; graph nodes (one per line in Figure 2) correspond to elementary predications from the original logical form and are connected by arcs labeled with MRS argument indices: ARG1, ARG2, etc. (where BV is reserved for what is the bound variable of a quantifier in the full MRS).6 Note that, while EDS already brings us relatively close to the other formats, there are graph nodes that do not correspond to individual words from our running example, for example the underspecified quanti"
W12-3602,P09-2010,1,0.844405,"glish, the so-called LinGO Redwoods Treebank (Oepen et al., 2004) contains gold-standard annotations for some 45,000 utterances in five broad genres and domains; comparable resources exist for Japanese (Bond et al., 2004) and are currently under construction for Portuguese and Spanish (Branco et al., 2010; Marimon, 2010). We develop an automated, parameterizable conversion procedure for these resources that maps HPSG analyses into either syntactic or semantic bilexical dependencies. Similar conversion procedures have recently been formulated for functional structures within the LFG framework (Øvrelid et al., 2009; Cetinoglu et al., 2010). In the design of this unidirectional (i.e. lossy) mapping, we apply and corroborate the cross-framework observations made in the more linguistic part of this study. The paper has the following structure: Section 2 introduces the corpus and annotations we take as 1 See http://www.delph-in.net for background. 3 our point of departure; Section 3 contrasts analyses of select linguistic phenomena by example; and Section 4 develops an automated conversion from HPSG analyses to bilexical dependencies. 2 The Multi-Annotated PEST Corpus At the 2008 Conference on Computational"
W12-3602,W08-2121,0,0.0449557,"syntactic or semantic bilexical relations. For English, syntactic dependencies in the PEST collection were obtained by converting PTB trees with the PennConverter software (Johansson & Nugues, 2007), which relies on head finding rules (Magerman, 1994; Collins, 1999) and the functional anno3 Among others, annotations in the Prague Dependency format would be interesting to compare to, but currently these are unfortunately not among the formats represented in the PEST corpus. 4 CoNLL PropBank Semantics (CP) For the 2008 CoNLL shared task on joint learning of syntactic and semantic dependencies (Surdeanu et al., 2008), the PropBank and NomBank annotations ‘on top’ of the PTB syntax (Palmer et al., 2005; Meyers et al., 2004) were converted to bilexical dependency form. This conversion was based on the dependency syntax already obtained for the same data set (CD, above) and heuristics which identify the semantic head of an argument with its syntactic head.The conversion further devotes special attention to arguments with several syntactic heads, discontinuous arguments, and empty categories (Surdeanu et al., 2008). The representation does not adhere to the formal constraints posed above; it lacks a designate"
W12-3602,J09-3003,0,0.0079248,"hed light on the representational ‘distance’ between pure bilexical dependencies, on the one hand, and full-blown logical-form propositional semantics, on the other hand. Furthermore, we propose a fully automated conversion procedure from (logical-form) meaning representation to bilexical semantic dependencies.† 1 Introduction—Motivation Dependency representations have in recent years received considerable attention from the NLP community, and have proven useful in diverse tasks such as Machine Translation (Ding & Palmer, 2005), Semantic Search (Poon & Domingos, 2009), and Sentiment Analysis (Wilson et al., 2009). Dependency representations are often claimed to be more ‘semantic’ in spirit, in the sense that they directly express predicate–argument relations, i.e. Who did What to Whom? Several of the shared tasks of the Conference on Natural Language Learning (CoNLL) in the past years have focused on datadriven dependency parsing—producing both syntactic (Nivre et al., 2007) and semantic dependencies (Hajiˇc et al., 2009)—and have made available gold † We are indebted to Emily Bender, Rebecca Dridan, and four anonymous reviewers for their feedback on this work. standard data sets (dependency banks) fo"
W12-3602,J03-4003,0,\N,Missing
W12-3602,D07-1096,0,\N,Missing
W13-3609,A00-1031,0,0.071546,"tries that license instances of certain error types, using the mal-rule approach of (Schneider and McCoy, 1998), adapted and extended for the ERG as described in (Bender et al., 2004). For parsing each sentence with this grammar, we use the relatively efficient PET parser (Callmeier, 2002), along with a parse-ranking method based on a model trained on a manually disambiguated treebank, so far consisting only of parses of well-formed sentences. In addition to using the manually constructed 37,000word lexicon included in the ERG, we accommodate unknown words by mapping POS tags produced by TnT (Brants, 2000) to generic lexical entry types on the fly. The bottom-up chart parser then exhaustively applies the rules of the grammar to the lexical entries introduced by the tokens in the input sentence, producing a packed forest of analyses (derivations) ranked by likelihood, and then presents the most likely derivation for postprocessing. The post-processor is a script which uses the derivation tree to identify the type and location of each error, and then takes appropriate action, which in the course is an instructional mesOverview As a participating group in the 2013 CoNLL Shared Task on Grammatical"
W13-3609,N12-1067,0,0.0313162,"Missing"
W13-3609,W13-1703,0,0.0624847,"derivation for postprocessing. The post-processor is a script which uses the derivation tree to identify the type and location of each error, and then takes appropriate action, which in the course is an instructional mesOverview As a participating group in the 2013 CoNLL Shared Task on Grammatical Error Correction, we adapted an existing system for error detection in a simpler closed-vocabulary domain to meet the additional demands of accommodating an open vocabulary and producing corrections for the errors identified. The training and test data for this shared task are from the NUCLE corpus (Dahlmeier et al., 2013), which consists of about one million words of short essays written by relatively competent English language learners. Each sentence has been manually annotated to identify and correct a wide range of grammatical and stylistic error types, though the shared task focused only on correcting instances of five of these types. Following standard procedure for such shared tasks, the organizers supplied most of the annotated data as a development corpus, and held out a 1381-sentence test corpus which was used for the evaluation of system output. 68 Proceedings of the Seventeenth Conference on Computa"
W13-3609,flickinger-etal-2010-wikiwoods,1,0.83529,"acy of the resulting analyses, at some expense to robustness. Its initial use was for generation within the German-English machine translation prototype developed in the Verbmobil project (Wahlster, 2000), so constraining the grammar to avoid overgeneration was a necessary design requirement that fit well with the broader aims of its developers. Applications using the grammar since then have included automatic processing of e-commerce customer support email messages, a second machine translation system (LOGON: (Lnning et al., 2004)), and information extraction over the full English Wikipedia (Flickinger et al., 2010). At present, the ERG consists of a rich hierarchy of types encoding regularities both in the lexicon and in the syntactic constructions of English. The lexicon contains 40,000 manually constructed lexeme entries, each assigned to one of 975 lexical types at the leaves of this hierarchy, where the types encode idiosyncracies of subcategorization, modification targets, exceptional behavior with respect to lexical rules, etc. The grammar also includes 70 derivational and inflectional rules which apply to these lexemes (or to each other’s outputs) to produce the words as they appear in text. The"
W13-3609,P98-2196,0,0.82477,"ncluded in the EPGY (Education Program for Gifted Youth) course offerings (Suppes et al., 2012). This error detection engine consists of a grammar, a parser, and a post-processing script that interprets the error codes in the derivation tree for each parsed sentence. Both the grammar and the parser are open-source resources developed and distributed as part of the DELPH-IN consortium (www.delph-in.net). We use the English Resource Grammar, described below, which we have augmented with both rules and lexical entries that license instances of certain error types, using the mal-rule approach of (Schneider and McCoy, 1998), adapted and extended for the ERG as described in (Bender et al., 2004). For parsing each sentence with this grammar, we use the relatively efficient PET parser (Callmeier, 2002), along with a parse-ranking method based on a model trained on a manually disambiguated treebank, so far consisting only of parses of well-formed sentences. In addition to using the manually constructed 37,000word lexicon included in the ERG, we accommodate unknown words by mapping POS tags produced by TnT (Brants, 2000) to generic lexical entry types on the fly. The bottom-up chart parser then exhaustively applies t"
W13-3609,N10-2012,0,\N,Missing
W13-3609,C98-2191,0,\N,Missing
W13-5707,adolphs-etal-2008-fine,1,0.821468,"al., 2009). Table 1 provides exact sentence, token, and type counts for these data sets. Tokenization Conventions A relevant peculiarity of the DeepBank and Redwoods annotations in this context is the ERG approach to tokenization. Three aspects in Figure 1 deviate from the widely used PTB conventions: (a) hyphens (and slashes) introduce token boundaries; (b) whitespace in multi-word lexical units (like ad hoc, of course, or Mountain View) does not force token boundaries; and (c) punctuation marks are attached as ‘pseudo-affixes’ to adjacent words, reflecting the rules of standard orthography. Adolphs et al. (2008) offer some linguistic arguments for this approach to tokenization, but for our purposes it suffices to note that these differences to PTB tokenization may in part counter-balance each other, but do increase the types-per-tokens ratio somewhat. This property of the DeepBank annotations, arguably, makes English look somewhat similar to languages with moderate inflectional morphology. To take advantage of the finegrained ERG lexical categories, most of our experiments assume ERG tokenization. In two calibration experiments, however, we also investigate the effects of tokenization differences on"
W13-5707,H91-1060,0,0.216164,"ency parsing), instantiating a comparatively diverse range of domains and genres (Oepen et al., 2004). Adding this data to our setup for additional cross-domain testing, we seek to document not only what trade-offs apply in terms of dependency accuracy vs. parser efficiency, but also how these trade-offs are affected by domain and genre variation, and, more generally, how resilient the different approaches are to variation in parser inputs. 63 2 Related Work Comparing between parsers from different frameworks has long been an area of active interest, ranging from the original PARSEVAL design (Black et al., 1991), to evaluation against ‘formalism-independent’ dependency banks (King et al., 2003; Briscoe & Carroll, 2006), to dedicated workshops (Bos et al., 2008). Grammatical Relations (GRs; Briscoe & Carroll, 2006) have been the target of a number of benchmarks, but they require a heuristic mapping from ‘native’ parser outputs to the target representations for evaluation, which makes results hard to interpret. Clark and Curran (2007) established an upper bound by running the mapping process on gold-standard data, to put into perspective the mapped results from their CCG parser proper. When Miyao et al"
W13-5707,D12-1133,0,0.176439,"bserve the strongest correspondence between DT and CoNLL (at a Jaccard index of 0.49, compared to 0.32 for DT and Stanford, and 0.43 between CoNLL and Stanford). posed to its developers until the grammar and disambiguation model were finalized and frozen for this release. Ivanova et al. (2013) complement this comparison of dependency schemes through an empirical assesment in terms of ‘parsability’, i.e. accuracy levels available for the different target representations when training and testing a range of state-of-the-art parsers on the same data sets. In their study, the dependency parser of Bohnet and Nivre (2012), henceforth B&N, consistently performs best for all schemes and output configurations. Furthermore, parsability differences between the representations are generally very small. Based on these observations, we conjecture that DT is as suitable a target representation for parser comparison as any of the others. Furthermore, two linguistic factors add to the attractiveness of DT for our study: it is defined in terms of a formal (and implemented) theory of grammar; and it makes available more finegrained lexical categories, ERG lexical types, than is common in PTB-derived dependency banks. Cross"
W13-5707,P06-2006,0,0.0126994,"dding this data to our setup for additional cross-domain testing, we seek to document not only what trade-offs apply in terms of dependency accuracy vs. parser efficiency, but also how these trade-offs are affected by domain and genre variation, and, more generally, how resilient the different approaches are to variation in parser inputs. 63 2 Related Work Comparing between parsers from different frameworks has long been an area of active interest, ranging from the original PARSEVAL design (Black et al., 1991), to evaluation against ‘formalism-independent’ dependency banks (King et al., 2003; Briscoe & Carroll, 2006), to dedicated workshops (Bos et al., 2008). Grammatical Relations (GRs; Briscoe & Carroll, 2006) have been the target of a number of benchmarks, but they require a heuristic mapping from ‘native’ parser outputs to the target representations for evaluation, which makes results hard to interpret. Clark and Curran (2007) established an upper bound by running the mapping process on gold-standard data, to put into perspective the mapped results from their CCG parser proper. When Miyao et al. (2007) carried out the same experiment for a number of different parsers, they showed that the loss of accu"
W13-5707,W97-1502,0,0.0106444,"s (e.g. subject–head or head–complement: sbhd_mc_c and hd-cmp_u_c, respectively; see below for more details on unary rules). Preterminals are labeled with fine-grained lexical categories, dubbed ERG lexical types, that augment common parts of speech with additional information, for example argument structure or the distinction between count, mass, and proper nouns. In total, the ERG distinguishes about 250 construction types and 1000 lexical types. DeepBank annotations were created by combining the native ERG parser, dubbed PET (Callmeier, 2002), with a discriminant-based tree selection tool (Carter, 1997; Oepen et al., 2004), thus making it possible for annotators to navigate the large space of possible analyses efficiently, identify and validate the intended reading, and record its full HPSG analysis in the treebank. Owing to this setup, DeepBank in its current version 1.0 lacks analyses for some 15 percent of the WSJ sentences, for which either the ERG parser failed to suggest a set of candidates (within certain bounds on time and memory usage), or the annotators found none of the available parses acceptable.3 Furthermore, DeepBank annotations to date only comprise the first 21 sections of"
W13-5707,cer-etal-2010-parsing,0,0.0534845,"Missing"
W13-5707,P07-1032,0,0.0143626,"n in parser inputs. 63 2 Related Work Comparing between parsers from different frameworks has long been an area of active interest, ranging from the original PARSEVAL design (Black et al., 1991), to evaluation against ‘formalism-independent’ dependency banks (King et al., 2003; Briscoe & Carroll, 2006), to dedicated workshops (Bos et al., 2008). Grammatical Relations (GRs; Briscoe & Carroll, 2006) have been the target of a number of benchmarks, but they require a heuristic mapping from ‘native’ parser outputs to the target representations for evaluation, which makes results hard to interpret. Clark and Curran (2007) established an upper bound by running the mapping process on gold-standard data, to put into perspective the mapped results from their CCG parser proper. When Miyao et al. (2007) carried out the same experiment for a number of different parsers, they showed that the loss of accuracy due to the mapping process can swamp any actual parser differences. As long as heuristic conversion is required before evaluation, cross-framework comparison inevitably includes a level of fuzziness. An alternative approach is possible when there is enough data available in a particular representation, and convers"
W13-5707,W08-1301,0,0.0449293,"Missing"
W13-5707,D13-1120,1,0.827329,"arser for unification grammars. PET constructs a complete parse forest, using subsumption-based ambiguity factoring (Oepen & Carroll, 2000), and then extracts from the forest n-best lists of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–19 of DeepBank and otherwise used the default configuration (which corresponds to the environment used by the DeepBank and Redwoods developers), which is optimized for accuracy. This parser, performing exact inference, we will call ERGa . In recent work, Dridan (2013) augments ERG parsing with lattice-based sequence labeling over lexical types and lexical rules. Pruning the parse chart prior to forest construction yields greatly improved efficiency at a moderate accuracy loss. Her lexical pruning model is trained on DeepBank 00–19 too, hence compatible with our setup. We include the bestperforming configuration of Dridan (2013) in our experiments, a variant henceforth referred to as ERGe . Unlike the other parsers in our study, PET internally operates over an ambiguous token lattice, and there is no easy interface to feed the parser pre-tokenized inputs. W"
W13-5707,I11-1100,0,0.0285816,"Missing"
W13-5707,P10-1035,0,0.0184554,"rocess can swamp any actual parser differences. As long as heuristic conversion is required before evaluation, cross-framework comparison inevitably includes a level of fuzziness. An alternative approach is possible when there is enough data available in a particular representation, and conversion (if any) is deterministic. Cer et al. (2010) used Stanford Dependencies (de Marneffe & Manning, 2008) to evaluate a range of statistical parsers. Pre- or post-converting from PTB phrase structure trees to the Stanford dependency scheme, they were able to evaluate a large number of different parsers. Fowler and Penn (2010) formally proved that a range of Combinatory Categorial Grammars (CCGs) are context-free. They trained the PCFG Berkeley parser on CCGBank, the CCG annotation of the PTB WSJ text (Hockenmaier & Steedman, 2007), advancing the state of the art in terms of supertagging accuracy, PARSEVAL measures, and CCG dependency accuracy. In other words, a specialized CCG parser is not necessarily more accurate than the generalpurpose Berkeley parser; this study, however, fails to also take parser efficiency into account. In related work for Dutch, Plank and van Noord (2010) suggest that, intuitively, one sho"
W13-5707,W01-0521,0,0.0808273,"Missing"
W13-5707,J07-3004,0,0.0193412,"h is possible when there is enough data available in a particular representation, and conversion (if any) is deterministic. Cer et al. (2010) used Stanford Dependencies (de Marneffe & Manning, 2008) to evaluate a range of statistical parsers. Pre- or post-converting from PTB phrase structure trees to the Stanford dependency scheme, they were able to evaluate a large number of different parsers. Fowler and Penn (2010) formally proved that a range of Combinatory Categorial Grammars (CCGs) are context-free. They trained the PCFG Berkeley parser on CCGBank, the CCG annotation of the PTB WSJ text (Hockenmaier & Steedman, 2007), advancing the state of the art in terms of supertagging accuracy, PARSEVAL measures, and CCG dependency accuracy. In other words, a specialized CCG parser is not necessarily more accurate than the generalpurpose Berkeley parser; this study, however, fails to also take parser efficiency into account. In related work for Dutch, Plank and van Noord (2010) suggest that, intuitively, one should expected that a grammar-driven system can be more resiliant to domain shifts than a purely data-driven parser. In a contrastive study on parsing into Dutch syntactic dependencies, they substantiated this e"
W13-5707,P13-3005,1,0.838136,"petitions (Nivre et al., 2007) and the ‘basic’ variant of Stanford Dependencies. They observe that the three dependency representations are broadly comparable in granularity and that there are substantial structural correspondences between the schemes. Measured as average Jaccard similarity over unlabeled dependencies, they observe the strongest correspondence between DT and CoNLL (at a Jaccard index of 0.49, compared to 0.32 for DT and Stanford, and 0.43 between CoNLL and Stanford). posed to its developers until the grammar and disambiguation model were finalized and frozen for this release. Ivanova et al. (2013) complement this comparison of dependency schemes through an empirical assesment in terms of ‘parsability’, i.e. accuracy levels available for the different target representations when training and testing a range of state-of-the-art parsers on the same data sets. In their study, the dependency parser of Bohnet and Nivre (2012), henceforth B&N, consistently performs best for all schemes and output configurations. Furthermore, parsability differences between the representations are generally very small. Based on these observations, we conjecture that DT is as suitable a target representation fo"
W13-5707,W12-3602,1,0.908199,"hrase structure and dependency parsers alike. Two recent developments make it possible to broaden the range of parsing approaches that can be assessed empirically on the task of deriving bi-lexical syntactic dependencies. Flickinger et al. (2012) make available another annotation layer over the same WSJ text, ‘deep’ syntacto-semantic analyses in the linguistic framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard & Sag, 1994; Flickinger, 2000). This resource, dubbed DeepBank, is available since late 2012. For the type of HPSG analyses recorded in DeepBank, Zhang and Wang (2009) and Ivanova et al. (2012) define a reduction into bi-lexical syntactic dependencies, which they call Derivation TreeDerived Dependencies (DT). Through application of the converter of Ivanova et al. (2012) to DeepBank, we can thus obtain a DT-annotated version of the standard WSJ text, to train and test a data-driven dependency and phrase structure parser, respectively, and to compare parsing results to a hybrid, grammar-driven HPSG parser. Furthermore, we can draw on a set of additional corpora annotated in the same HPSG format (and thus amenable to conversion for both phrase structure and dependency parsing), instant"
W13-5707,W03-2401,0,0.0464787,"en et al., 2004). Adding this data to our setup for additional cross-domain testing, we seek to document not only what trade-offs apply in terms of dependency accuracy vs. parser efficiency, but also how these trade-offs are affected by domain and genre variation, and, more generally, how resilient the different approaches are to variation in parser inputs. 63 2 Related Work Comparing between parsers from different frameworks has long been an area of active interest, ranging from the original PARSEVAL design (Black et al., 1991), to evaluation against ‘formalism-independent’ dependency banks (King et al., 2003; Briscoe & Carroll, 2006), to dedicated workshops (Bos et al., 2008). Grammatical Relations (GRs; Briscoe & Carroll, 2006) have been the target of a number of benchmarks, but they require a heuristic mapping from ‘native’ parser outputs to the target representations for evaluation, which makes results hard to interpret. Clark and Curran (2007) established an upper bound by running the mapping process on gold-standard data, to put into perspective the mapped results from their CCG parser proper. When Miyao et al. (2007) carried out the same experiment for a number of different parsers, they sh"
W13-5707,J93-2004,0,0.042212,"reduction in linguistic detail. 2 In contrast, much earlier work on cross-framework comparison involved post-processing parser outputs in form and content, into a target representation for which gold-standard annotations were available. In § 2 below, we argue that such conversion inevitably introduces blur into the comparison. ited to purely data-driven (or statistical) parsers, i.e. systems where linguistic knowledge is exclusively acquired through supervised machine learning from annotated training data. For English, the venerable Wall Street Journal (WSJ) portion of the Penn Treebank (PTB; Marcus et al., 1993) has been the predominant source of training data, for phrase structure and dependency parsers alike. Two recent developments make it possible to broaden the range of parsing approaches that can be assessed empirically on the task of deriving bi-lexical syntactic dependencies. Flickinger et al. (2012) make available another annotation layer over the same WSJ text, ‘deep’ syntacto-semantic analyses in the linguistic framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard & Sag, 1994; Flickinger, 2000). This resource, dubbed DeepBank, is available since late 2012. For the type of HPSG a"
W13-5707,D07-1013,0,0.0391918,"nd (c) the distribution of LAS over different lexical categories. Among the different dependency types, we observe that the notion of an adjunct is difficult for all three parsers. One of the hardest dependency labels is hdn-aj (post-adjunction to a nominal head), the relation employed for relative clauses and prepositional phrases attaching to a nominal head. The most common error for this relation is verbal attachment. It has been noted that dependency parsers may exhibit systematic performance differences with respect to dependency length (i.e. the distance between a head and its argument; McDonald & Nivre, 2007). In our experiments, we find that the parsers perform comparably on longer dependency arcs (upwards of fifteen words), with ERGa constantly showing the highest accuracy, and Berkeley holding a slight edge over B&N as dependency length increases. In Figure 3, one can eyeball accuracy levels per lexical category, where conjunctions (c) and various types of prepositions (p and pp) are the most difficult for all three parsers. That the DT analysis of coordination is challenging is unsurprising. Schwartz et al. 68 CB SC VM (2012) show that choosing conjunctions as heads in coordinate structures is"
W13-5707,A00-2022,1,0.62018,"epBank annotations, arguably, makes English look somewhat similar to languages with moderate inflectional morphology. To take advantage of the finegrained ERG lexical categories, most of our experiments assume ERG tokenization. In two calibration experiments, however, we also investigate the effects of tokenization differences on our parser comparison. PET: Native HPSG Parsing The parser most commonly used with the ERG is called PET (Callmeier, 2002), a highly engineered chart parser for unification grammars. PET constructs a complete parse forest, using subsumption-based ambiguity factoring (Oepen & Carroll, 2000), and then extracts from the forest n-best lists of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–19 of DeepBank and otherwise used the default configuration (which corresponds to the environment used by the DeepBank and Redwoods developers), which is optimized for accuracy. This parser, performing exact inference, we will call ERGa . In recent work, Dridan (2013) augments ERG parsing with lattice-based sequence labeling over lexical types and lexical rules. Pruning the parse chart prior"
W13-5707,P06-1055,0,0.114573,"Missing"
W13-5707,W10-2105,0,0.0233349,"Missing"
W13-5707,C12-1147,0,0.0515236,"Missing"
W13-5707,W11-2923,0,0.0197785,"enization mismatches local to some sub-segment of the input will not ‘throw off’ token correspondences in other parts of the string.5 We will refer to this character-based variant of the standard CoNLL metrics as LASc and UASc . 4 PCFG Parsing of HPSG Derivations Formally, the HPSG analyses in the DeepBank and Redwoods treebanks transcend the class of contextfree grammars, of course. Nevertheless, one can pragmatically look at an ERG derivation as if it were a context-free phrase structure tree. On this view, standard, off-the-shelf PCFG parsing techniques are applicable to the ERG treebanks. Zhang and Krieger (2011) explore this space experimentally, combining the ERG, Redwoods (but not DeepBank), and massive collections of automatically parsed text. Their study, however, does not consider parser efficiency.6 . In contrast, our goal is to reflect on practical tradeoffs along multiple dimensions. We therefore focus on Berkeley, as one of the currently best-performing (and relatively efficient) PCFG engines. Due to its ability to internally rewrite node labels, this parser should be expected to adapt well also to ERG derivations. Compared to the phrase structure annotations in the PTB, there are two struct"
W13-5707,W07-2207,1,0.846594,"finegrained ERG lexical categories, most of our experiments assume ERG tokenization. In two calibration experiments, however, we also investigate the effects of tokenization differences on our parser comparison. PET: Native HPSG Parsing The parser most commonly used with the ERG is called PET (Callmeier, 2002), a highly engineered chart parser for unification grammars. PET constructs a complete parse forest, using subsumption-based ambiguity factoring (Oepen & Carroll, 2000), and then extracts from the forest n-best lists of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–19 of DeepBank and otherwise used the default configuration (which corresponds to the environment used by the DeepBank and Redwoods developers), which is optimized for accuracy. This parser, performing exact inference, we will call ERGa . In recent work, Dridan (2013) augments ERG parsing with lattice-based sequence labeling over lexical types and lexical rules. Pruning the parse chart prior to forest construction yields greatly improved efficiency at a moderate accuracy loss. Her lexical pruning model is trained on DeepBank 00–"
W13-5707,P09-1043,0,0.474619,"ce of training data, for phrase structure and dependency parsers alike. Two recent developments make it possible to broaden the range of parsing approaches that can be assessed empirically on the task of deriving bi-lexical syntactic dependencies. Flickinger et al. (2012) make available another annotation layer over the same WSJ text, ‘deep’ syntacto-semantic analyses in the linguistic framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard & Sag, 1994; Flickinger, 2000). This resource, dubbed DeepBank, is available since late 2012. For the type of HPSG analyses recorded in DeepBank, Zhang and Wang (2009) and Ivanova et al. (2012) define a reduction into bi-lexical syntactic dependencies, which they call Derivation TreeDerived Dependencies (DT). Through application of the converter of Ivanova et al. (2012) to DeepBank, we can thus obtain a DT-annotated version of the standard WSJ text, to train and test a data-driven dependency and phrase structure parser, respectively, and to compare parsing results to a hybrid, grammar-driven HPSG parser. Furthermore, we can draw on a set of additional corpora annotated in the same HPSG format (and thus amenable to conversion for both phrase structure and de"
W15-0128,adolphs-etal-2008-fine,1,0.866719,"Missing"
W15-0128,P98-1013,0,0.191125,"tic representations and deploying them at scale to create a large sembank including diverse genres. Such representations can be created compositionally, where the content and internal structure of the representations is constrained by syntactic structure, or non-compositionally, where annotators encode their understanding of a sentence directly. The latter category is exemplified by Abstract Meaning Representation (AMR; Langkilde and Knight 1998; Banarescu et al. 2013). In the former category, we find both manual annotation projects, such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998), which annotate semantic information with reference to syntactic structure, and grammar-based annotation initiatives such as the Redwoods Treebank (Oepen et al., 2004), TREPIL (Rosén et al., 2005), and the Groningen Meaning Bank (Basile et al., 2012). We argue here that a grammar-based, compositional approach is critical to achieving this long-range goal, in particular because it supports more comprehensive representations (§4.1), produced with better consistency (§4.2) and greater scalability (§4.3). The drawback to a grammar-based approach is that it cannot, in itself, include information t"
W15-0128,W13-2322,0,0.651957,"emantics either for parsing or for generation. 4 Benefits of Compositionality We are concerned here with the goal of designing task-independent semantic representations and deploying them at scale to create a large sembank including diverse genres. Such representations can be created compositionally, where the content and internal structure of the representations is constrained by syntactic structure, or non-compositionally, where annotators encode their understanding of a sentence directly. The latter category is exemplified by Abstract Meaning Representation (AMR; Langkilde and Knight 1998; Banarescu et al. 2013). In the former category, we find both manual annotation projects, such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998), which annotate semantic information with reference to syntactic structure, and grammar-based annotation initiatives such as the Redwoods Treebank (Oepen et al., 2004), TREPIL (Rosén et al., 2005), and the Groningen Meaning Bank (Basile et al., 2012). We argue here that a grammar-based, compositional approach is critical to achieving this long-range goal, in particular because it supports more comprehensive representations (§4.1), produced with bett"
W15-0128,basile-etal-2012-developing,0,0.204582,"ture, or non-compositionally, where annotators encode their understanding of a sentence directly. The latter category is exemplified by Abstract Meaning Representation (AMR; Langkilde and Knight 1998; Banarescu et al. 2013). In the former category, we find both manual annotation projects, such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998), which annotate semantic information with reference to syntactic structure, and grammar-based annotation initiatives such as the Redwoods Treebank (Oepen et al., 2004), TREPIL (Rosén et al., 2005), and the Groningen Meaning Bank (Basile et al., 2012). We argue here that a grammar-based, compositional approach is critical to achieving this long-range goal, in particular because it supports more comprehensive representations (§4.1), produced with better consistency (§4.2) and greater scalability (§4.3). The drawback to a grammar-based approach is that it cannot, in itself, include information that is not compositional, but as we will develop further in §6 below, it is possible to have the best of both worlds, adding non-compositional information as additional annotation layers over grammar-produced semantic representations. 4.1 Comprehensiv"
W15-0128,P11-1059,0,0.0201877,"ication, rather than a strong claim about lexical meaning. Another kind of non-compositional meaning layer is that which requires some sort of further computation over linguistic structure. This can be seen as purely monotonic addition of further constraints on underspecified meaning representations, but it is not compositional in the sense that it is never (strictly) constrained by grammatical structure. In this category, we find quantifier scope ambiguity resolution (e.g. Higgins and Sadock 2003), coreference resolution (e.g. Hobbs 1979), and the determination of the focus of negation (e.g. Blanco and Moldovan 2011). All of these build on partial constraints provided by the grammar, but in all cases, the interpretation of particular sentences in context will correspond to one (or a subset) of the possibilities allowed by the grammar. The next layer of meaning annotation to consider corresponds to discourse processing. This includes the calculation of presupposition projection (e.g. Van der Sandt 1992; Zaenen and Karttunen 2013; Venhuizen et al. 2013), coherence relations/rhetorical structure (e.g. Marcu 1997), and the annotation of discourse moves/adjacency pairs (Shriberg et al., 2004). These aspects of"
W15-0128,W97-1502,0,0.546635,"ee and ERS for each of these sentences are made available as the Redwoods Treebank; at the end of 2014, the current version of Redwoods encompasses gold-standard ERG analyses for 85,000 utterances (∼1.5 million tokens) of running text from half a dozen different genres and domains. In more detail, the task of annotation for a sentence consists of making binary decisions about the set of discriminants each of which partitions the parse forest into two: all of the analyses which employ the particular rule or lexical entry, and the rest of the analyses which do not. This method, originating with Carter 1997, enables the human annotator to rapidly discard analyses in order to isolate the intended analysis, or to conclude that the correct analysis is unavailable. As a reference point for speed of annotation using this method, an expert treebanker using the current ‘1214’ version of the ERG annotated 2400 sentences (37,200 words) from the Brown corpus in 1400 minutes, for an average rate of 1.7 sentences per minute.3 Annotations produced by this method of choosing among the candidate analyses licensed by a grammar will thus record those components of sentence meaning which are constrained by the gr"
W15-0128,P01-1019,1,0.580225,"nnotation. As Szabó (2013) points out, there are many different interpretations of the principle of compositionality in the literature. Since we are concerned with annotation, the issue is compositionality of meaning representations (rather than denotation, for instance). In order to ask which aspects of meaning are compositional, we provide the following working definition:1 1 In Szabó’s terms, our definition of compositionality is local, distributive, and language-bound and furthermore consistent with the rule-to-rule principle. It is also consistent with the notion of compositionality from Copestake et al. (2001) and implemented in the ERG, which furthermore adds the constraint that the function for determining meanings of complex expressions must be monotonic in the sense that it cannot remove or overwrite any information contributed by the constituents. 240 (1) A meaning system (or subsystem) is compositional if: • it consists of a finite (but possibly very large) number of arbitrary atomic symbol-meaning pairings; • it is possible to create larger symbol-meaning pairings by combining the atomic pairings through a finite set of rules; • the meaning of any non-atomic symbol-meaning pairing is a funct"
W15-0128,W11-2927,1,0.716728,"Missing"
W15-0128,flickinger-etal-2010-wikiwoods,1,0.898233,"Missing"
W15-0128,P82-1014,0,0.754755,"fits. We then report on a small inter-annotator agreement study to quantify the consistency of semantic representations produced via this grammar-based method. 1 Introduction Kate and Wong (2010) define ‘semantic parsing’ as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” At this level of generality, semantic parsing has been a cornerstone of NLU from its early days, including work seeking to support dialogue systems, database interfaces, or machine translation (Woods et al., 1972; Gawron et al., 1982; Alshawi, 1992, inter alios). What distinguishes most current work in semantic parsing from such earlier landmarks of old-school NLU is (a) the use of (highly) taskand domain-specific meaning representations (e.g. the RoboCup or GeoQuery formal language) and (b) a lack of emphasis on natural language syntax, i.e. a tacit expectation to map (more or less) directly from a linguistic surface form to an abstract representation of its meaning. This approach risks conflating a distinction that has long played an important role in the philosophy of language and theoretical linguistics (Quine, 1960;"
W15-0128,J03-1004,0,0.0864457,"Missing"
W15-0128,W07-1501,0,0.0215267,"entions. Some of the information we would like to see in such annotations is grammatically constrained, and we have argued that representations of those aspects of meaning are best built compositionally. However, there are further aspects of meaning which are closely tied to the linguistic signal but are not constrained by sentencelevel grammar (or only partially so constrained). We agree here with Basile et al. (2012) and Banarescu et al. (2013) that a single resource that combines multiple different types of semantic annotations, all applied to the same text, will be most valuable (see also Ide and Suderman 2007). However, just because some aspects of the desired representations cannot be created in a grammar-based fashion does not mean that what can be done with a grammar has no value. To get the best of both worlds, one should start from grammar-derived semantic annotations and then either add further layers of annotation (e.g. word sense, coreference) or, should larger paraphrase sets be desired, systematically simplify aspects of the grammar-derived representations, effectively ‘bleaching’ some of the contrasts. In moving from the current state of the art towards more comprehensive representations"
W15-0128,P10-5006,0,0.0583505,"al itself. We further argue that compositional construction of such sentence meaning representations affords better consistency, more comprehensiveness, greater scalability, and less duplication of effort for each new NLP application. For concreteness, we describe one well-tested grammar-based method for producing sentence meaning representations which is efficient for annotators, and which exhibits many of the above benefits. We then report on a small inter-annotator agreement study to quantify the consistency of semantic representations produced via this grammar-based method. 1 Introduction Kate and Wong (2010) define ‘semantic parsing’ as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” At this level of generality, semantic parsing has been a cornerstone of NLU from its early days, including work seeking to support dialogue systems, database interfaces, or machine translation (Woods et al., 1972; Gawron et al., 1982; Alshawi, 1992, inter alios). What distinguishes most current work in semantic parsing from such earlier landmarks of old-school NLU is (a) the use of (highly) taskand domain-"
W15-0128,kingsbury-palmer-2002-treebank,0,0.223431,"e goal of designing task-independent semantic representations and deploying them at scale to create a large sembank including diverse genres. Such representations can be created compositionally, where the content and internal structure of the representations is constrained by syntactic structure, or non-compositionally, where annotators encode their understanding of a sentence directly. The latter category is exemplified by Abstract Meaning Representation (AMR; Langkilde and Knight 1998; Banarescu et al. 2013). In the former category, we find both manual annotation projects, such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998), which annotate semantic information with reference to syntactic structure, and grammar-based annotation initiatives such as the Redwoods Treebank (Oepen et al., 2004), TREPIL (Rosén et al., 2005), and the Groningen Meaning Bank (Basile et al., 2012). We argue here that a grammar-based, compositional approach is critical to achieving this long-range goal, in particular because it supports more comprehensive representations (§4.1), produced with better consistency (§4.2) and greater scalability (§4.3). The drawback to a grammar-based approach is that it cannot"
W15-0128,P98-1116,0,0.0854027,"equire the computation of semantics either for parsing or for generation. 4 Benefits of Compositionality We are concerned here with the goal of designing task-independent semantic representations and deploying them at scale to create a large sembank including diverse genres. Such representations can be created compositionally, where the content and internal structure of the representations is constrained by syntactic structure, or non-compositionally, where annotators encode their understanding of a sentence directly. The latter category is exemplified by Abstract Meaning Representation (AMR; Langkilde and Knight 1998; Banarescu et al. 2013). In the former category, we find both manual annotation projects, such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998), which annotate semantic information with reference to syntactic structure, and grammar-based annotation initiatives such as the Redwoods Treebank (Oepen et al., 2004), TREPIL (Rosén et al., 2005), and the Groningen Meaning Bank (Basile et al., 2012). We argue here that a grammar-based, compositional approach is critical to achieving this long-range goal, in particular because it supports more comprehensive representations (§"
W15-0128,I11-1028,1,0.818925,"to abstract away from task-irrelevant details of linguistic expression, task-independent representations only have that luxury when the variation is truly (sentence) meaning preserving. A task-independent semantic representation should capture exactly the meaning encoded in the linguistic signal itself, as it is is not possible to know, a priori, which parts of that sentence meaning will be critical to determining speaker meaning in any given application. 3 This rate is roughly consistent with an earlier experiment using the same Redwoods treebanking method where annotation times were noted: MacKinlay et al. (2011) report a somewhat slower mean annotation time by an expert annotator of 0.6 sentences per minute, but this difference can be attributed to the greater average sentence length (and hence increased number of discriminants to be determined) for that biomedical corpus: 23.4 tokens compared with 15.5 for the Brown data. 243 h h1 , h h1 , h4 :personh0 : 6i(ARG0 x 5 ), h6 :_no_qh0 : 6i(ARG0 x 5 , RSTR h7 , BODY h8 ), h2 :_eat_v_1h7 : 11i(ARG0 e3 , ARG1 x 5 , ARG2 i 9 ) { h1 =q h2 , h7 =q h4 } i h4 :_every_qh0 : 5i(ARG0 x 6 , RSTR h7 , BODY h5 ), h8 :_person_n_1h6 : 12i(ARG0 x 6 ), h2 :_fail_v_1h13 :"
W15-0128,P97-1013,0,0.0300129,"resolution (e.g. Hobbs 1979), and the determination of the focus of negation (e.g. Blanco and Moldovan 2011). All of these build on partial constraints provided by the grammar, but in all cases, the interpretation of particular sentences in context will correspond to one (or a subset) of the possibilities allowed by the grammar. The next layer of meaning annotation to consider corresponds to discourse processing. This includes the calculation of presupposition projection (e.g. Van der Sandt 1992; Zaenen and Karttunen 2013; Venhuizen et al. 2013), coherence relations/rhetorical structure (e.g. Marcu 1997), and the annotation of discourse moves/adjacency pairs (Shriberg et al., 2004). These aspects of meaning clearly build on information provided during sentence-level processing, including lexically determined veridicality contexts (e.g. (2a) vs. (2b)) as well as discourse connectives. In both cases, the grammatical structure links embedded clauses to the relevant lexical predicates. 2 We assume here that word sense is a property of roots, rather than fully inflected forms. Productive derivational morphology supports compositionally built-up meanings for morphologically complex words. Semi-prod"
W15-0128,J94-1007,0,0.713929,"act representation of its meaning. This approach risks conflating a distinction that has long played an important role in the philosophy of language and theoretical linguistics (Quine, 1960; Grice, 1968), viz. the contrast between those aspects of meaning that are determined by the linguistic signal alone (called ‘timeless’, ‘conventional’, ‘standing’, or ‘sentence’ meaning), on the one hand, and aspects of meaning that are particular to a context of use (‘utterer’, ‘speaker’, or ‘occasion’ meaning, or ‘interpretation’), on the other hand. Relating this tradition to computational linguistics, Nerbonne (1994, p. 134) notes: Linguistic semantics does not furnish a characterization of the interpretation of utterances in use, which is what one finally needs for natural language understanding applications—rather, it (mostly) provides a characterization of conventional content, that part of meaning determined by linguistic form. Interpretation is not determined by 239 Proceedings of the 11th International Conference on Computational Semantics, pages 239–249, c London, UK, April 15-17 2015. 2015 Association for Computational Linguistics form, however, nor by its derivative content. In order to interpre"
W15-0128,oepen-lonning-2006-discriminant,1,0.897769,"Missing"
W15-0128,W04-2319,0,0.0143789,"negation (e.g. Blanco and Moldovan 2011). All of these build on partial constraints provided by the grammar, but in all cases, the interpretation of particular sentences in context will correspond to one (or a subset) of the possibilities allowed by the grammar. The next layer of meaning annotation to consider corresponds to discourse processing. This includes the calculation of presupposition projection (e.g. Van der Sandt 1992; Zaenen and Karttunen 2013; Venhuizen et al. 2013), coherence relations/rhetorical structure (e.g. Marcu 1997), and the annotation of discourse moves/adjacency pairs (Shriberg et al., 2004). These aspects of meaning clearly build on information provided during sentence-level processing, including lexically determined veridicality contexts (e.g. (2a) vs. (2b)) as well as discourse connectives. In both cases, the grammatical structure links embedded clauses to the relevant lexical predicates. 2 We assume here that word sense is a property of roots, rather than fully inflected forms. Productive derivational morphology supports compositionally built-up meanings for morphologically complex words. Semi-productive morphological processes and frozen or lexicalized complex forms complica"
W15-0128,W13-0122,0,0.0367255,"scope ambiguity resolution (e.g. Higgins and Sadock 2003), coreference resolution (e.g. Hobbs 1979), and the determination of the focus of negation (e.g. Blanco and Moldovan 2011). All of these build on partial constraints provided by the grammar, but in all cases, the interpretation of particular sentences in context will correspond to one (or a subset) of the possibilities allowed by the grammar. The next layer of meaning annotation to consider corresponds to discourse processing. This includes the calculation of presupposition projection (e.g. Van der Sandt 1992; Zaenen and Karttunen 2013; Venhuizen et al. 2013), coherence relations/rhetorical structure (e.g. Marcu 1997), and the annotation of discourse moves/adjacency pairs (Shriberg et al., 2004). These aspects of meaning clearly build on information provided during sentence-level processing, including lexically determined veridicality contexts (e.g. (2a) vs. (2b)) as well as discourse connectives. In both cases, the grammatical structure links embedded clauses to the relevant lexical predicates. 2 We assume here that word sense is a property of roots, rather than fully inflected forms. Productive derivational morphology supports compositionally bu"
W15-0128,W08-0606,0,0.0207628,"As this level of processing concerns relationships both within and across sentences, it is clearly not compositional with respect to sentence grammar. We consider it an open question whether there are compositional processes at higher levels of structure that constrain these aspects of meaning in analogous ways, but we note that in presupposition processing at least, a notion of defeasibility is required (Asher and Lascarides, 2011). Finally, there are semantic annotations that attempt to capture what speakers are trying to do with their speech acts. This includes tasks like hedge detection (Vincze et al., 2008) and the annotation of social acts such as authority claims and alignment moves (Morgan et al., 2013) or the pursuit of power in dialogue (Swayamdipta and Rambow, 2012). While in some cases there are keywords that have a strong association with particular categories in these annotation schemes, these aspects of meaning are clearly not anchored in the structure of sentences but rather relate to the goals that speakers have in uttering sentences. Lacking a firm link to the structure of sentences, they do not appear to be compositional. We have seen in this (necessarily brief) section that existi"
W15-0128,W13-0505,0,0.01847,"tegory, we find quantifier scope ambiguity resolution (e.g. Higgins and Sadock 2003), coreference resolution (e.g. Hobbs 1979), and the determination of the focus of negation (e.g. Blanco and Moldovan 2011). All of these build on partial constraints provided by the grammar, but in all cases, the interpretation of particular sentences in context will correspond to one (or a subset) of the possibilities allowed by the grammar. The next layer of meaning annotation to consider corresponds to discourse processing. This includes the calculation of presupposition projection (e.g. Van der Sandt 1992; Zaenen and Karttunen 2013; Venhuizen et al. 2013), coherence relations/rhetorical structure (e.g. Marcu 1997), and the annotation of discourse moves/adjacency pairs (Shriberg et al., 2004). These aspects of meaning clearly build on information provided during sentence-level processing, including lexically determined veridicality contexts (e.g. (2a) vs. (2b)) as well as discourse connectives. In both cases, the grammatical structure links embedded clauses to the relevant lexical predicates. 2 We assume here that word sense is a property of roots, rather than fully inflected forms. Productive derivational morphology sup"
W15-0128,C98-1112,0,\N,Missing
W15-0128,C98-1013,0,\N,Missing
W16-0511,I05-1015,0,0.0959366,"Missing"
W16-0511,W16-0506,0,0.0347315,"f grammatical errors in compositions from a variety of genres. In this year’s task, the organizers invited participants to “analyze the linguistic characteristics of scientific writing to promote the development of automated writing evaluation tools that can assist authors in writing scientific papers. The task is to predict whether a given sentence requires editing to ensure its ’fit’ within the scientific writing genre.” This Automated Evaluation of Scientific Writing (AESW) Shared Task 2016 is described in more detail, along with descriptions of the seven teams and their system results, in Daudaravicius et al. (2016). This paper provides a description of the system adapted for this task by a team of collaborators from the University of Washington and from Stan2 Resources and methods Our basic approach to this task has much in common with the one used in the Stanford system participating in the 2013 CoNLL Shared Task on Grammatical Error Correction (Flickinger and Yu, 2013), again employing a current version of the English Resource Grammar (ERG: Flickinger (2000), Flickinger (2011)) and a task-specific inventory of mal-rules, but this time using a more efficient parser (ACE: moin.delph-in.net/AceTop). As w"
W16-0511,W13-3609,1,0.88987,"Missing"
W16-0511,A00-2022,0,0.189864,"Missing"
W16-0511,P98-2196,0,0.355617,"Missing"
W16-0511,C98-2191,0,\N,Missing
