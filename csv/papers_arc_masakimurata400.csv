2020.lrec-1.87,Relation between Degree of Empathy for Narrative Speech and Type of Responsive Utterance in Attentive Listening,2020,-1,-1,2,0,16787,koichiro ito,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Nowadays, spoken dialogue agents such as communication robots and smart speakers listen to narratives of humans. In order for such an agent to be recognized as a listener of narratives and convey the attitude of attentive listening, it is necessary to generate responsive utterances. Moreover, responsive utterances can express empathy to narratives and showing an appropriate degree of empathy to narratives is significant for enhancing speaker{'}s motivation. The degree of empathy shown by responsive utterances is thought to depend on their type. However, the relation between responsive utterances and degrees of the empathy has not been explored yet. This paper describes the classification of responsive utterances based on the degree of empathy in order to explain that relation. In this research, responsive utterances are classified into five levels based on the effect of utterances and literature on attentive listening. Quantitative evaluations using 37,995 responsive utterances showed the appropriateness of the proposed classification."
Y16-3001,Retrieval Term Prediction Using Deep Learning Methods,2016,27,0,3,0,33252,qing ma,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Posters",0,None
Y14-1040,Retrieval Term Prediction Using Deep Belief Networks,2014,24,1,3,0,33252,qing ma,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,"This paper presents a method to predict retrieval terms from relevant/surrounding words or descriptive texts in Japanese by using deep belief networks (DBN), one of two typical types of deep learning. To determine the effectiveness of using DBN for this task, we tested it along with baseline methods using examplebased approaches and conventional machine learning methods, i.e., multi-layer perceptron (MLP) and support vector machines (SVM), for comparison. The data for training and testing were obtained from the Web in manual and automatic manners. Automatically created pseudo data was also used. A grid search was adopted for obtaining the optimal hyperparameters of these machine learning methods by performing cross-validation on training data. Experimental results showed that (1) using DBN has far higher prediction precisions than using baseline methods and higher prediction precisions than using either MLP or SVM; (2) adding automatically gathered data and pseudo data to the manually gathered data as training data is an effective measure for further improving the prediction precisions; and (3) DBN is able to deal with noisier training data than MLP, i.e., the prediction precision of DBN can be improved by adding noisy training data, but that of MLP cannot be."
W14-4506,Automatic Detection and Analysis of Impressive {J}apanese Sentences Using Supervised Machine Learning,2014,18,0,2,0,38393,daiki hazure,Proceedings of the First {AHA}!-Workshop on Information Discovery in Text,0,"It is important to write sentences that impress the listener or reader (xe2x80x9cimpressive sentencesxe2x80x9d) in many cases, such as when drafting political speeches. The study reported here provides useful information for writing such sentences in Japanese. Impressive sentences in Japanese are collected and examined for characteristic words. A number of such words are identified that often appear in impressive sentences, including jinsei (human life), hitobito (people), koufuku (happiness), yujou (friendliness), seishun (youth), and renxe2x80x99ai (love). Sentences using these words are likely to impress the listener or reader. Machine learning (SVM) is also used to automatically extract impressive sentences. It is found that the use of machine learning enables impressive sentences to be extracted from a large amount of Web documents with higher precision than that obtained with a baseline method, which extracts all sentences as impressive sentences."
Y11-1053,System for Flexibly Judging the Misuse of Honorifics in {J}apanese,2011,1,2,3,0,43966,tamotsu shirado,"Proceedings of the 25th Pacific Asia Conference on Language, Information and Computation",0,"We propose a system for flexibly judging the misusage of honorifics in Japanese sentence. The system can point out misused words and phrases, and can also indicate how they are misused. The system uses judgment rules whose degrees of validity in modern Japanese society are quantified by psychological experiments. The system can judge sentences flexibly based on the learner's linguistic level by tuning thresholds regarding the degree of validity. The proposed system is expected to be applied in practical computeraided education."
Y11-1062,"Extraction of Broad-Scale, High-Precision {J}apanese-{E}nglish Parallel Translation Expressions Using Lexical Information and Rules",2011,8,3,3,0,33252,qing ma,"Proceedings of the 25th Pacific Asia Conference on Language, Information and Computation",0,"Extraction was attempted of broad-scale, high-precision Japanese-English paral- lel translation expressions from large aligned parallel corpora. To acquire broad-scale parallel translation expressions, a new method was used to extract single Japanese and English word n-grams, by which as many parallel translation expressions as possible could then be ex- tracted. To achieve high extraction precision, first, hand-crafted rules were used to prune the unnecessary words often found in expressions extracted on the basis of word n-grams, and lexical information was used to refine the parallel translation expressions. Computer exper- iments with aligned parallel corpora consisting of about 280,000 pairs of Japanese-English parallel sentences found that more than 125,000 pairs of parallel translation expressions could be extracted with a precision of 0.96. These figures show that the proposed methods for ex- tracting a broad range of parallel translation expressions have reached a level high enough for practical use."
Y10-1073,Detection of Users Suspected of Pretending to Be Other Users in a Community Site by Using Messages Submitted to Non-Target Categories,2010,6,0,4,0,45042,naoki ishikawa,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"Some users abuse the anonymity and disrupt communications in a community site. Authorship identication based on analyzing stylistic features of messages is effective in detecting these inadequate users. However, in this method, the scope of target users was often limited because the criteria for selecting learning examples were strict. To relax the criteria and extend the scope of target users, we propose a method of detecting users suspected of pretending to be other users in a certain category of a community site by using their messages submitted to other (non-target) categories. Also, we show the accuracy of user identication when we relax the criteria of selecting learning examples and extend the scope of target users."
Y10-1075,Using Various Features in Machine Learning to Obtain High Levels of Performance for Recognition of {J}apanese Notational Variants,2010,17,2,2,0,44118,masahiro kojima,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"We proposed a method of using machine learning with various features for the recognition of Japanese notational variants. We increased 0.06 at the F-measure by specific features using existing dictionaries and character pairs useful for recognizing notational variants and obtained 0.91 at the F-measure for the recognition of notational variants. By using the method, we could extract 160 thousand word pairs with a precision rate of 0.9. We also constructed a method using patterns in addition to machine learning and observed that we could extract 4.2 million notational variant pairs with a precision rate of 0.78. We confirmed that our method was much better than an existing method through experiments."
Y10-1080,Generation of Summaries that Appropriately and Adequately Express the Contents of Original Documents Using Word-Association Knowledge,2010,15,0,2,0,45050,kazuki takigawa,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"In this study, our purpose was to make a short summary for sentences. For example, we aimed to make a short summary xe2x80x9cterrorxe2x80x9d for sentences xe2x80x9cA bomb went off. Some people were killed. This was triggered by rebel campaign.xe2x80x9d In this study, we proposed a new method that generates summaries that can appropriately and adequately express the contents of their respective original documents using word-association knowledge. In this method, we assumed that a good summary comprises words that can express the contents of the original document and does not contain words that are unable to express the contents of the original document. Using statistical tests, we confirmed that the use of elements in our method was beneficial. Our method obtained 0.75 as the ratio where the top 10 summaries for each document include a correct summary and 0.45 as the mean reciprocal rank (MRR) in the xe2x80x9clenientxe2x80x9d case of experiments."
P10-1026,A {B}ayesian Method for Robust Estimation of Distributional Similarities,2010,19,27,4,0.319474,41587,junichi kazama,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Existing word similarity measures are not robust to data sparseness since they rely only on the point estimation of words' context profiles obtained from a limited amount of data. This paper proposes a Bayesian method for robust distributional word similarities. The method uses a distribution of context profiles obtained by Bayesian estimation and takes the expectation of a base similarity measure under that distribution. When the context profiles are multinomial distributions, the priors are Dirichlet, and the base measure is the Bhattacharyya coefficient, we can derive an analytical form that allows efficient calculation. For the task of word similarity estimation using a large amount of Web data in Japanese, we show that the proposed measure gives better accuracies than other well-known similarity measures."
murata-etal-2010-construction,Construction of Chunk-Aligned Bilingual Lecture Corpus for Simultaneous Machine Translation,2010,7,0,1,1,16788,masaki murata,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"With the development of speech and language processing, speech translation systems have been developed. These studies target spoken dialogues, and employ consecutive interpretation, which uses a sentence as the translation unit. On the other hand, there exist a few researches about simultaneous interpreting, and recently, the language resources for promoting simultaneous interpreting research, such as the publication of an analytical large-scale corpus, has been prepared. For the future, it is necessary to make the corpora more practical toward realization of a simultaneous interpreting system. In this paper, we describe the construction of a bilingual corpus which can be used for simultaneous lecture interpreting research. Simultaneous lecture interpreting systems are required to recognize translation units in the middle of a sentence, and generate its translation at the proper timing. We constructed the bilingual lecture corpus by the following steps. First, we segmented sentences in the lecture data into semantically meaningful units for the simultaneous interpreting. And then, we assigned the translations to these units from the viewpoint of the simultaneous interpreting. In addition, we investigated the possibility of automatically detecting the simultaneous interpreting timing from our corpus."
ishikawa-etal-2010-detection,Detection of submitters suspected of pretending to be someone else in a community site,2010,5,1,5,0,45042,naoki ishikawa,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"One of the essential factors in community sites is anonymous submission. This is because anonymity gives users chances to submit messages (questions, problems, answers, opinions, etc.) without regard to shame and reputation. However, some users abuse the anonymity and disrupt communications in a community site. These users and their submissions discourage other users, keep them from retrieving good communication records, and decrease the credibility of the communication site. To solve this problem, we conducted an experimental study to detect submitters suspected of pretending to be someone else to manipulate communications in a community site by using machine learning techniques. In this study, we used messages in the data of Yahoo! chiebukuro for data training and examination."
D10-1087,Automatic Comma Insertion for {J}apanese Text Generation,2010,9,3,1,1,16788,masaki murata,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"This paper proposes a method for automatically inserting commas into Japanese texts. In Japanese sentences, commas play an important role in explicitly separating the constituents, such as words and phrases, of a sentence. The method can be used as an elemental technology for natural language generation such as speech recognition and machine translation, or in writing-support tools for non-native speakers. We categorized the usages of commas and investigated the appearance tendency of each category. In this method, the positions where commas should be inserted are decided based on a machine learning approach. We conducted a comma insertion experiment using a text corpus and confirmed the effectiveness of our method."
P09-1060,Linefeed Insertion into {J}apanese Spoken Monologue for Captioning,2009,11,5,2,0,16789,tomohiro ohno,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"To support the real-time understanding of spoken monologue such as lectures and commentaries, the development of a captioning system is required. In monologues, since a sentence tends to be long, each sentence is often displayed in multi lines on one screen, it is necessary to insert linefeeds into a text so that the text becomes easy to read. This paper proposes a technique for inserting linefeeds into a Japanese spoken monologue text as an elemental technique to generate the readable captions. Our method appropriately inserts linefeeds into a sentence by machine learning, based on the information such as dependencies, clause boundaries, pauses and line length. An experiment using Japanese speech data has shown the effectiveness of our technique."
D09-1097,Hypernym Discovery Based on Distributional Similarity and Hierarchical Structures,2009,21,40,5,0,300,ichiro yamada,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a new method of developing a large-scale hyponymy relation database by combining Wikipedia and other Web documents. We attach new words to the hyponymy database extracted from Wikipedia by using distributional similarity calculated from documents on the Web. For a given target word, our algorithm first finds k similar words from the Wikipedia database. Then, the hypernyms of these k similar words are assigned scores by considering the distributional similarities and hierarchical distances in the Wikipedia database. Finally, new hyponymy relations are output according to the scores. In this paper, we tested two distributional similarities. One is based on raw verb-noun dependencies (which we call RVD), and the other is based on a large-scale clustering of verb-noun dependencies (called CVD). Our method achieved an attachment accuracy of 91.0% for the top 10,000 relations, and an attachment accuracy of 74.5% for the top 100,000 relations when using CVD. This was a far better outcome compared to the other baseline approaches. Excluding the region that had very high scores, CVD was found to be more effective than RVD. We also confirmed that most relations extracted by our method cannot be extracted merely by applying the well-known lexico-syntactic patterns to Web documents."
D09-1122,Large-Scale Verb Entailment Acquisition from the {W}eb,2009,17,33,5,0,26950,chikara hashimoto,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Textual entailment recognition plays a fundamental role in tasks that require indepth natural language understanding. In order to use entailment recognition technologies for real-world applications, a large-scale entailment knowledge base is indispensable. This paper proposes a conditional probability based directional similarity measure to acquire verb entailment pairs on a large scale. We targeted 52,562 verb types that were derived from 108 Japanese Web documents, without regard for whether they were used in daily life or only in specific fields. In an evaluation of the top 20,000 verb entailment pairs acquired by previous methods and ours, we found that our similarity measure outperformed the previous ones. Our method also worked well for the top 100,000 results."
ma-etal-2008-selection,Selection of {J}apanese-{E}nglish Equivalents by Integrating High-quality Corpora and Huge Amounts of Web Data,2008,5,1,3,0.785558,33252,qing ma,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"As a first step to developing systems that enable non-native speakers to output near-perfect English sentences for given mixed English-Japanese sentences, we propose new approaches for selecting English equivalents by using the number of hits for various contexts in large English corpora. As the large English corpora, we not only used the huge amounts of Web data but also the manually compiled large, high-quality English corpora. Using high-quality corpora enables us to accurately select equivalents, and using huge amounts of Web data enables us to resolve the problem of the shortage of hits that normally occurs when using only high-quality corpora. The types and lengths of contexts used to select equivalents are variable and optimally determined according to the number of hits in the corpora, so that performance can be further refined. Computer experiments showed that the precision of our methods was much higher than that of the existing methods for equivalent selection."
I08-2100,Non-Factoid {J}apanese Question Answering through Passage Retrieval that Is Weighted Based on Types of Answers,2008,16,2,1,1,16788,masaki murata,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"We constructed a system for answering nonfactoid Japanese questions. We used various methods of passage retrieval for the system. We extracted paragraphs based on terms from an input question and output them as the preferred answers. We classified the non-factoid questions into six categories. We used a particular method for each category. For example, we increased the scores of paragraphs including the word xe2x80x9creasonxe2x80x9d for questions including the word xe2x80x9cwhy.xe2x80x9d We participated at NTCIR-6 QAC-4, where our system obtained the most correct answers out of all the eight participating teams. The rate of accuracy was 0.77, which indicates that our methods were effective."
Y07-1034,{J}apanese Expressions that Include {E}nglish Expressions,2007,0,0,1,1,16788,masaki murata,"Proceedings of the 21st Pacific Asia Conference on Language, Information and Computation",0,"We extracted English expressions that appear in Japanese sentences in newspaper articles and on the Internet. The results obtained from the newspaper articles showed that the preposition in has been regularly used for more than ten years, and it is still regularly used now. The results obtained from the Internet articles showed there were many kinds of English expressions from various parts of speech. We extracted some interesting expressions that included English prepositions and verb phrases. These were interesting because they had different word orders to the normal order in Japanese expressions. Comparing the extracted English and katakana expressions, we found that the expressions that are commonly used in Japanese are often written in the katakana syllabary and that the expressions that are not so often used in Japanese, such as prepositions, are hardly ever written in the katakana syllabary. Keyword: English expression, katakana expression, newspaper article, Internet"
Y06-1039,Construction of Adverb Dictionary that Relates to Speaker Attitudes and Evaluation of Its Effectiveness,2006,3,1,2,1,48624,toshiyuki kanamaru,"Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation",0,"Adverbs can express a speakerxe2x80x99s attitude in a given situation on a specific matter. We constructed an adverb dictionary in which the attitude of the speaker is described. We also looked into whether or not adverbs could effectively be used as basic data to analyze reputations. We conducted three kinds of experiments to verify how effective and precise our dictionary was. First, we calculated the coverage ratio of the dictionary by comparing the ratios of appearances of all adverbs to the ratios of appearances of our dictionary items. Next, we attached a tag to 988 adverbs, and found that the coverage ratio of the tagged adverbs was 97.76% in the open data. Finally, we classified whether sentences were positively or negatively represented using the adverb dictionary and calculated that the accuracy of the classification was 86.5%."
W06-0201,Development of an Automatic Trend Exploration System using the {M}u{ST} Data Collection,2006,5,3,1,1,16788,masaki murata,Proceedings of the Workshop on Information Extraction Beyond The Document,0,"The automatic extraction of trend information from text documents such as newspaper articles would be useful for exploring and examining trends. To enable this, we used data sets provided by a workshop on multimodal summarization for trend information (the MuST Workshop) to construct an automatic trend exploration system. This system first extracts units, temporals, and item expressions from newspaper articles, then it extracts sets of expressions as trend information, and finally it arranges the sets and displays them in graphs. For example, when documents concerning the politics are given, the system extracts % and Cabinet approval rating as a unit and an item expression including temporal expressions. It next extracts values related to %. Finally, it makes a graph where temporal expressions are used for the horizontal axis and the value of percentage is shown on the vertical axis. This graph indicates the trend of Cabinet approval rating and is useful for investigating Cabinet approval rating. Graphs are obviously easy to recognize and useful for understanding information described in documents. In experiments, when we judged the extraction of a correct graph as the top output to be correct, the system accuracy was 0.2500 in evaluation A and 0.3334 in evaluation B. (In evaluation A, a graph where 75% or more of the points were correct was judged to be correct; in evaluation B, a graph where 50% or more of the points were correct was judged to be correct.) When we judged the extraction of a correct graph in the top five outputs to be correct, accuracy rose to 0.4167 in evaluation A and 0.6250 in evaluation B. Our system is convenient and effective because it can output a graph that includes trend information at these levels of accuracy when given only a set of documents as input."
P06-2076,Machine-Learning-Based Transformation of Passive {J}apanese Sentences into Active by Separating Training Data into Each Input Particle,2006,6,1,1,1,16788,masaki murata,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"We developed a new method of transforming Japanese case particles when transforming Japanese passive sentences into active sentences. It separates training data into each input particle and uses machine learning for each particle. We also used numerous rich features for learning. Our method obtained a high rate of accuracy (94.30%). In contrast, a method that did not separate training data for any input particles obtained a lower rate of accuracy (92.00%). In addition, a method that did not have many rich features for learning used in a previous study (Murata and Isahara, 2003) obtained a much lower accuracy rate (89.77%). We confirmed that these improvements were significant through a statistical test. We also conducted experiments utilizing traditional methods using verb dictionaries and manually prepared heuristic rules and confirmed that our method obtained much higher accuracy rates than traditional methods."
kanamaru-etal-2006-creation,Creation of a {J}apanese Adverb Dictionary that Includes Information on the Speaker{'}s Communicative Intention Using Machine Learning,2006,2,0,2,1,48624,toshiyuki kanamaru,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Japanese adverbs are classified as either declarative or normal; the former declare the communicative intention of the speaker, while the latter convey a manner of action, a quantity, or a degree by which the adverb modifies the verb or adjective that it accompanies. We have automatically classified adverbs as either declarative or not declarative using a machine-learning method such as the maximum entropy method. We defined adverbs having positive or negative connotations as the positive data. We classified adverbs in the EDR dictionary and IPADIC used by Chasen using this result and built an adverb dictionary that contains descriptions of the communicative intentions of the speaker."
Y05-1014,"Analysis of Machine Translation Systems{'} Errors in Tense, Aspect, and Modality",2005,10,8,1,1,16788,masaki murata,"Proceedings of the 19th Pacific Asia Conference on Language, Information and Computation",0,"Errors of the translation of tense, aspect, and modality by machine translation systems were analyzed for six translation systems on the market and our new systems for translating tense, aspect, and modality. The results showed that our systems outperformed the other systems. They also showed that the other systems often produced progressive forms rather than the correct present forms. Our systems rarely made this mistake. Translation systems on the market could thus be improved by incorporating the methods used in our systems. Moreover, error analysis of the translation systems on the market identified information that would be useful for improving them."
W05-0629,Semantic Role Labeling Using Support Vector Machines,2005,7,12,2,0,50819,tomohiro mitsumori,Proceedings of the Ninth Conference on Computational Natural Language Learning ({C}o{NLL}-2005),0,"In this paper, we describe our systems for the CoNLL-2005 shared task. The aim of the task is semantic role labeling using a machine-learning algorithm. We apply the Support Vector Machines to the task. We added new features based on full parses and manually categorized words. We also report on system performance and what effect the newly added features had."
I05-6002,Obtaining {J}apanese Lexical Units for Semantic Frames from {B}erkeley {F}rame{N}et Using a Bilingual Corpus,2005,10,2,2,1,48624,toshiyuki kanamaru,Proceedings of the Sixth International Workshop on Linguistically Interpreted Corpora ({LINC}-2005),0,None
I05-2024,Information Retrieval Capable of Visualization and High Precision,2005,7,2,3,0.97967,33252,qing ma,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,"We present a neural-network based selforganizing approach that enables visualization of the information retrieval while at the same time improving its precision. In computer experiments, two-dimensional documentary maps in which queries and documents were mapped in topological order according to their similarities were created. The ranking of the results retrieved using the maps was better than that of the results obtained using a conventional TFIDF method. Furthermore, the precision of the proposed method was much higher than that of the conventional TFIDF method when the process was focused on retrieving highly relevant documents, suggesting that the proposed method might be especially suited to information retrieval tasks in which precision is more critical than recall."
I05-2043,Trend Survey on {J}apanese Natural Language Processing Studies over the Last Decade,2005,0,3,1,1,16788,masaki murata,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,"Using natural language processing, we carried out a trend survey on Japanese natural language processing studies that have been done over the last ten years. We determined the changes in the number of papers published for each research organization and on each research area as well as the relationship between research organizations and research areas. This paper is useful for both recognizing trends in Japanese NLP and constructing a method of supporting trend surveys using NLP."
Y04-1032,Three {E}nglish Learner Assistance Systems Using Automatic Paraphrasing Techniques,2004,9,1,1,1,16788,masaki murata,"Proceedings of the 18th Pacific Asia Conference on Language, Information and Computation",0,"We developed three systems based on automatic paraphrasing techniques to help English learners and English-language beginners. One system extracts personal error patterns in the userxe2x80x99s English usage. The second transforms English sentences containing the letters xe2x80x9clxe2x80x9d and xe2x80x9crxe2x80x9d into sentences containing fewer instances of these letters, which Japanese people have trouble pronouncing properly in English. This system could be used, for example, to transform a draft of a presentation that a Japanese speaker was to present to an audience. The third is an annotation system that provides definition sentences of difficult English words, making them easier to understand. We believe that these systems will be useful both for learners of English and in studies on second-language acquisition."
W04-2208,Multilingual Aligned Parallel Treebank Corpus Reflecting Contextual Information and Its Applications,2004,13,18,4,0.571794,30019,kiyotaka uchimoto,Proceedings of the Workshop on Multilingual Linguistic Resources,0,"This paper describes Japanese-English-Chinese aligned parallel treebank corpora of newspaper articles. They have been constructed by translating each sentence in the Penn Treebank and the Kyoto University text corpus into a corresponding natural sentence in a target language. Each sentence is translated so as to reflect its contextual information and is annotated with morphological and syntactic structures and phrasal alignment. This paper also describes the possible applications of the parallel corpus and proposes a new framework to aid in translation. In this framework, parallel translations whose source language sentence is similar to a given sentence can be semi-automatically generated. In this paper we show that the framework can be achieved by using our aligned parallel treebank corpus."
kanzaki-etal-2004-extraction,Extraction of Hyperonymy of Adjectives from Large Corpora by Using the Neural Network Model,2004,1,2,4,0.869565,15923,kyoko kanzaki,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this research, we extract hierarchical abstract concepts of adjectives automatically from large corpora by using the Neural Network Model. We show the hierarchies on the Semantic Map and compare the hierarchies in the Semantic Map and a manually prepared thesaurus. We recognized five types of distributions on the map. By comparing the Semantic Map and a manual thesaurus, we found that the word that the abstract noun belongs to, whether a person, thing or event, is introduced as the standard of classification in the manual thesaurus. On the other hand, in the Semantic Map, we found that abstract nouns belonging to people or events are distributed together. We also found that the hierarchies of sokumen (side), imi (meaning), and kanten (viewpoint) are necessary for a category of adjectives."
W03-1714,Semantic Maps for Word Alignment in Bilingual Parallel Corpora,2003,11,4,3,1,33252,qing ma,Proceedings of the Second {SIGHAN} Workshop on {C}hinese Language Processing,0,"Effective self-organizing techniques for constructing monolingual semantic maps of Japanese and Chinese have already been developed. By extending the monolingual map to a bilingual semantic map, we have proposed a semantics-based approach for word alignment in a Japanese/Chinese bilingual corpus."
W02-1107,Classification of Adjectival and Non-adjectival Nouns Based on their Semantic Behavior by Using a Self-Organizing Semantic Map,2002,7,3,3,0.869565,15923,kyoko kanzaki,{COLING}-02: {SEMANET}: Building and Using Semantic Networks,0,"We treat nouns that behave adjectively, which we call adjectival nouns, extracted from large corpora. For example, in financial world and world of finance, financial and finance are different parts of speech, but their semantic behaviors are similar to each other. We investigate how adjectival nouns are similar to adjectives and different from non-adjectival nouns by using self-organizing semantic maps. We create five kinds of semantic maps, i.e., semantic maps of abstract nouns organized via (1) adjectives, (2) adjectival nouns, (3) non-adjectival nouns and (4) adjectival and adjectival nouns and a semantic map of adjectives, adjectival nouns and non-adjectival nouns organized via collocated abstract nouns, and compare them with each other to find similarities and differences."
murata-isahara-2002-automatic,"Automatic extraction of differences between spoken and written languages, and automatic translation from the written to the spoken language",2002,0,9,1,1,16788,masaki murata,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,None
C02-1060,Self-Organizing {C}hinese and {J}apanese Semantic Maps,2002,14,3,3,1,33252,qing ma,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"This paper describes a corpus-based connectionist approach to the development of self-organizing Chinese and Japanese semantic maps, proposing an improved coding method using TFIDF term-weighting and newly introducing a numerical evaluation for objectively judging the results. The adaption of TFIDF term-weighting is proved to be effective by experimental comparisons with five other coding methods. The effectiveness and necessity of the proposed method for creating semantic maps are clarified by comparisons with a conventional clustering technique and multivariate statistical analysis."
2002.tmi-papers.14,Correction of errors in a modality corpus used for machine translation using machine-learning,2002,-1,-1,1,1,16788,masaki murata,Proceedings of the 9th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,None
W01-1415,"Using a Support-Vector Machine for {J}apanese-to-{E}nglish Translation of Tense, Aspect, and Modality",2001,7,12,1,1,16788,masaki murata,Proceedings of the {ACL} 2001 Workshop on Data-Driven Methods in Machine Translation,0,"This paper describes experiments carried out using a variety of machine-learning methods, including the k-nearest neighborhood method that was used in a previous study, for the translation of tense, aspect, and modality. It was found that the support-vector machine method was the most precise of all the methods tested."
S01-1033,{J}apanese Word Sense Disambiguation using the Simple {B}ayes and Support Vector Machine Methods,2001,2,16,1,1,16788,masaki murata,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"We submitted four systems to the Japanese dictionary-based lexical-sample task of Senseval-2. They were i) the support vector machine method ii) the simple Bayes method, iii) a method combining the two, and iv) a method combining two kinds of each. The combined methods obtained the best precision among the submitted systems. After the contest, we tuned the parameter used in the simple Bayes method, and it obtained higher precision. An explanation of these systems used in Japanese word sense disambiguation was provided."
S01-1038,Word Translation Based on Machine Learning Models Using Translation Memory and Corpora,2001,1,0,3,0.930233,30019,kiyotaka uchimoto,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"Senseval-2 was held in Spring, 2001. It consisted of several tasks in various languages. In this paper, we describe our system used for one of these tasks: the Japanese translation task. With an accuracy of 63.4%, our system was the third best system in the contest among nine systems developed by seven groups."
P00-1042,Named Entity Extraction Based on A Maximum Entropy Model and Transformation Rules,2000,13,63,3,0.930233,30019,kiyotaka uchimoto,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes named entity (NE) extraction based on a maximum entropy (M. E.) model and transformation rules. There are two types of named entities when focusing on the relationship between morphemes and NEs as defined in the NE task of the IREX competition held in 1999. Each NE consists of one or more morphemes, or includes a substring of a morpheme. We extract the former type of NE by using the M. E. model. We then extract the latter type of NE by applying transformation rules to the text."
C00-2126,Word Order Acquisition from Corpora,2000,2,18,2,0.930233,30019,kiyotaka uchimoto,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"In this paper we describe a method of acquiring word order from corpora. Word order is defined as the order of modifiers, or the order of phrasal units called 'bunsetsu' which depend on the same modifiee. The method uses a model which automatically discovers what the tendency of the word order in Japanese is by using various kinds of information in and around the target bunsetsus. This model shows us to what extent each piece of information contributes to deciding the word order and which word order tends to be selected when several kinds of information conflict. The contribution rate of each piece of information in deciding word order is efficiently learned by a model within a maximum entropy framework. The performance of this trained model can be evaluated by checking how many instances of word order selected by the model agree with those in the original text. In this paper, we show that even a raw corpus that has not been tagged can be used to train the model, if it is first analyzed by a parser. This is possible because the word order of the text in the corpus is correct."
C00-2128,A Statistical Approach to the Processing of Metonymy,2000,16,17,2,0,127,masao utiyama,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"This paper describes a statistical approach to the interpretation of metonymy. A metonymy is received as an input, then its possible interpretations are ranked by applying a statistical measure. The method has been tested experimentally. It correctly interpreted 53 out of 75 metonymies in Japanese."
C00-1074,Hybrid Neuro and Rule-Based Part of Speech Taggers,2000,6,13,2,1,33252,qing ma,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"A hybrid system for tagging part of speech is described that consists of a neuro tagger and a rule-based corrector. The neuro tagger is an initial-state annotator that uses different lengths of context based on longest context priority. Its inputs are weighted by information gains that are obtained by information maximization. The rule-based corrector is constructed by a set of transformation rules to make up for the shortcomings of the neuro tagger. Computer experiments show that almost 20% of the errors made by the neuro tagger are corrected by these transformation rules, so that the hybrid system can reach an accuracy of 95.5% counting only the ambiguous words and 99.1% counting all words when a small Thai corpus with 22,311 ambiguous words is used for training. This accuracy is far higher than that using an HMM and is also higher than that using a rule-based model."
C00-1082,Bunsetsu Identification Using Category-Exclusive Rules,2000,11,8,1,1,16788,masaki murata,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"This paper describes two new bunsetsu identification methods using supervised learning. Since Japanese syntactic analysis is usually done after bunsetsu identification, bunsetsu identification is important for analyzing Japanese sentences. In experiments comparing the four previously available machine-learning methods (decision tree, maximum-entropy method, example-based approach and decision list) and two new methods using category-exclusive rules, the new method using the category-exclusive rules with the highest similarity performed best."
2000.iwpt-1.43,Dependency Model using Posterior Context,2000,-1,-1,2,0.930233,30019,kiyotaka uchimoto,Proceedings of the Sixth International Workshop on Parsing Technologies,0,"We describe a new model for dependency structure analysis. This model learns the relationship between two phrasal units called bunsetsus as three categories; {`}between{'}, {`}dependent{'}, and {`}beyond{'}, and estimates the dependency likelihood by considering not only the relationship between two bunsetsus but also the relationship between the left bunsetsu and all of the bunsetsus to its right. We implemented this model based on the maximum entropy model. When using the Kyoto University corpus, the dependency accuracy of our model was 88{\%}, which is about 1{\%} higher than that of the conventional model using exactly the same features."
W99-0205,Resolution of Indirect Anaphora in {J}apanese Sentences Using Examples: {``}{X} no {Y} ({Y} of {X}){''},1999,5,9,1,1,16788,masaki murata,Coreference and Its Applications,0,"A noun phrase can indirectly refer to an entity that has already been mentioned. For example, I went into an old house last night. The roof was leaking badly and .... indicates that the roof is associated with an old house, which was mentioned in the previous sentence. This kind of reference (indirect anaphora) has not been studied well in natural language processing, but is important for coherence resolution, language understanding, and machine translation. In order to analyze indirect anaphora, we need a case frame dictionary for nouns that contains knowledge of the relationships between two nouns but no such dictionary presently exists. Therefore, we are forced to use examples of X no Y (Y of X) and a verb case frame dictionary instead. We tried estimating indirect anaphora using this information and obtained a recall rate of 63% and a precision rate of 68% on test sentences. This indicates that the information of X no Y is useful to a certain extent when we cannot make use of a noun case frame dictionary. We estimated the results that would be given by a noun case frame dictionary, and obtained recall and precision rates of 71% and 82% respectively. Finally, we proposed a way to construct a noun case frame dictionary by using examples of X no Y."
W99-0206,Pronoun Resolution in {J}apanese Sentences Using Surface Expressions and Examples,1999,10,10,1,1,16788,masaki murata,Coreference and Its Applications,0,"In this paper, we present a method of estimating referents of demonstrative pronouns, personal pronouns, and zero pronouns in Japanese sentences using examples, surface expressions, topics and foci. Unlike conventional work which was semantic markers for semantic constraints, we used examples for semantic constraints and showed in our experiments that examples are as useful as semantic markers. We also propose many new methods for estimating referents of pronouns. For example, we use the form X of Y for estimating referents of demonstrative adjectives. In addition to our new methods, we used many conventional methods. As a result, experiments using these methods obtained a precision rate of 87% in estimating referents of demonstrative pronouns, personal pronouns, and zero pronouns for training sentences, and obtained a precision rate of 78% for test sentences."
1999.tmi-1.7,"An example-based approach to {J}apanese-to-{E}nglish translation of tense, aspect, and modality",1999,3,16,1,1,16788,masaki murata,Proceedings of the 8th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,"We have developed a new method for Japanese-to-English translation of tense, aspect, and modality that uses an example-based method. In this method the similarity between input and example sentences is defined as the degree of semantic matching between the expressions at the ends of the sentences. Our method also uses the k-nearest neighbor method in order to exclude the effects of noise; for example, wrongly tagged data in the bilingual corpora. Experiments show that our method can translate tenses, aspects, and modalities more accurately than the top-level MT software currently available on the market can. Moreover, it does not require hand-craft rules."
W98-0605,Construction of {J}apanese Nominal Semantic Dictionary using {``}A {NO} {B}{''} Phrases in Corpora,1998,1,1,2,0,297,sadao kurohashi,The Computational Treatment of Nominals,0,None
P98-2150,An Estimate of Referent of Noun Phrases in {J}apanese Sentences,1998,2,7,1,1,16788,masaki murata,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"In machine translation and man-machine dialogue, it is important to clarify referents of noun phrases. We present a method for determining the referents of noun phrases in Japanese sentences by using the referential properties, modifiers, and possessors of noun phrases. Since the Japanese language has no articles, it is difficult to decide whether a noun phrase has an antecedent or not. We had previously estimated the referential properties of noun phrases that correspond to articles by using clue words in the sentences (Murata and Nagao 1993). By using these referential properties, our system determined the referents of noun phrases in Japanese sentences. Furthermore we used the modifiers and possessors of noun phrases in determining the referents of noun phrases. As a result, on training sentences we obtained a precision rate of 82% and a recall rate of 85% in the determination of the referents of noun phrases that have antecedents. On test sentences, we obtained a precision rate of 79% and a recall rate of 77%."
C98-2145,An Estimate of Referent of Noun Phrases in {J}apanese Sentences,1998,2,7,1,1,16788,masaki murata,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"In machine translation and man-machine dialogue, it is important to clarify referents of noun phrases. We present a method for determining the referents of noun phrases in Japanese sentences by using the referential properties, modifiers, and possessors of noun phrases. Since the Japanese language has no articles, it is difficult to decide whether a noun phrase has an antecedent or not. We had previously estimated the referential properties of noun phrases that correspond to articles by using clue words in the sentences (Murata and Nagao 1993). By using these referential properties, our system determined the referents of noun phrases in Japanese sentences. Furthermore we used the modifiers and possessors of noun phrases in determining the referents of noun phrases. As a result, on training sentences we obtained a precision rate of 82% and a recall rate of 85% in the determination of the referents of noun phrases that have antecedents. On test sentences, we obtained a precision rate of 79% and a recall rate of 77%."
C96-2134,Document Classification Using Domain Specific Kanji Characters Extracted by X2 Method,1996,6,6,2,0.254856,44119,yasuhiko watanabe,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"In this paper we describe a method of classifying Japanese text documents using domain specific kanji characters. Text documents are generally classified by significant words (keywords) of the documents. However, it is difficult to extract these significant words from Japanese text, because Japanese texts are written without using blank spaces, such as delimiters, and must be segmented into words. Therefore, instead of words, we used domain specific kanji characters which appear more frequently in one domain than the other. We extracted these domain specific kanji characters by X2 method. Then, using these domain specific kanji characters, we classified editorial columns TENSEI JINGO, editorial articles, and articles in Scientific American (in Japanese). The correct recognition scores for them were 47%, 74%, and 85%, respectively."
1993.tmi-1.18,Determination of Referential Property and Number of Nouns in {J}apanese Sentences for Machine Translation into {E}nglish,1993,-1,-1,1,1,16788,masaki murata,Proceedings of the Fifth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
1993.tmi-1.19,Translation into {E}nglish,1993,-1,-1,1,1,16788,masaki murata,Proceedings of the Fifth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
