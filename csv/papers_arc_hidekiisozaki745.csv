W15-3058,Dependency Analysis of Scrambled References for Better Evaluation of {J}apanese Translation,2015,18,2,1,1,36870,hideki isozaki,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"In English-to-Japanese translation, BLEU (Papineni et al., 2002), the de facto standard evaluation metric for machine translation (MT), has very weak correlation with human judgments (Goto et al., 2011; Goto et al., 2013). Therefore, RIBES (Isozaki et al., 2010; Hirao et al., 2014) was proposed. RIBES measures similarity of the word order of a machine-translated sentence and that of a corresponding human-translated reference sentence. RIBES has much stronger correlation than BLEU but most Japanese sentences have alternative word orders (scrambling), and one reference sentence is not sufficient for fair evaluation. Isozaki et al. (2014) proposed a solution to this problem. This solution generates semantically equivalent word orders of reference sentences. Automatically generated word orders are sometimes incomprehensible or misleading, and they introduced a heuristic rule that filters out such bad sentences. However, their rule is too conservative and generated alternative word orders for only 30% of reference sentences. In this paper, we present a rule-free method that uses a dependency parser to check scrambled sentences and generated alternatives for 80% of sentences. The experimental results show that our method improves sentence-level correlation with human judgments. In addition, strong system-level correlation of single reference RIBES is not damaged very much. We expect this method can be applied to other languages such as German, Korean,"
W14-3335,Dependency-based Automatic Enumeration of Semantically Equivalent Word Orders for Evaluating {J}apanese Translations,2014,12,2,1,1,36870,hideki isozaki,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"Scrambling is acceptable reordering of verb arguments in languages such as Japanese and German. In automatic evaluation of translation quality, BLEU is the de facto standard method, but BLEU has only very weak correlation with human judgements in case of Japanese-toEnglish/English-to-Japanese translations. Therefore, alternative methods, IMPACT and RIBES, were proposed and they have shown much stronger correlation than BLEU. Now, RIBES is widely used in recent papers on Japanese-related translations. RIBES compares word order of MT output with manually translated reference sentences but it does not regard scrambling at all. In this paper, we present a method to enumerate scrambled sentences from dependency trees of reference sentences. Our experiments based on NTCIR Patent MT data show that the method improves sentence-level correlation between RIBES and human-judged adequacy."
2012.iwslt-keynotes.1,Head finalization: translation from {SVO} to {SOV},2012,2,0,1,1,36870,hideki isozaki,Proceedings of the 9th International Workshop on Spoken Language Translation: Keynotes,0,None
P11-2112,Learning Condensed Feature Representations from Large Unsupervised Data Sets for Supervised Learning,2011,20,14,2,1,9188,jun suzuki,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"This paper proposes a novel approach for effectively utilizing unsupervised data in addition to supervised data for supervised learning. We use unsupervised data to generate informative 'condensed feature representations' from the original feature set used in supervised NLP systems. The main contribution of our method is that it can offer dense and low-dimensional feature spaces for NLP tasks while maintaining the state-of-the-art performance provided by the recently developed high-performance semi-supervised learning technique. Our method matches the results of current state-of-the-art systems with very few features, i.e., F-score 90.72 with 344 features for CoNLL-2003 NER data, and UAS 93.55 with 12.5K features for dependency parsing data derived from PTB-III."
W10-1736,Head Finalization: A Simple Reordering Rule for {SOV} Languages,2010,18,64,1,1,36870,hideki isozaki,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"English is a typical SVO (Subject-Verb-Object) language, while Japanese is a typical SOV language. Conventional Statistical Machine Translation (SMT) systems work well within each of these language families. However, SMT-based translation from an SVO language to an SOV language does not work well because their word orders are completely different. Recently, a few groups have proposed rule-based preprocessing methods to mitigate this problem (Xu et al., 2009; Hong et al., 2009). These methods rewrite SVO sentences to derive more SOV-like sentences by using a set of handcrafted rules. In this paper, we propose an alternative single reordering rule: Head Finalization. This is a syntax-based preprocessing approach that offers the advantage of simplicity. We do not have to be concerned about part-of-speech tags or rule weights because the powerful Enju parser allows us to implement the rule at a general level. Our experiments show that its result, Head Final English (HFE), follows almost the same order as Japanese. We also show that this rule improves automatic evaluation scores."
W10-1757,N-Best Reranking by Multitask Learning,2010,37,9,4,0,5136,kevin duh,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"We propose a new framework for N-best reranking on sparse feature sets. The idea is to reformulate the reranking problem as a Multitask Learning problem, where each N-best list corresponds to a distinct task.n n This is motivated by the observation that N-best lists often show significant differences in feature distributions. Training a single reranker directly on this heteroge-nous data can be difficult.n n Our proposed meta-algorithm solves this challenge by using multitask learning (such as e1/e2 regularization) to discover common feature representations across N-best lists. This meta-algorithm is simple to implement, and its modular approach allows one to plug-in different learning algorithms from existing literature. As a proof of concept, we show statistically significant improvements on a machine translation system involving millions of features."
D10-1092,Automatic Evaluation of Translation Quality for Distant Language Pairs,2010,14,155,1,1,36870,hideki isozaki,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"Automatic evaluation of Machine Translation (MT) quality is essential to developing high-quality MT systems. Various evaluation metrics have been proposed, and BLEU is now used as the de facto standard metric. However, when we consider translation between distant language pairs such as Japanese and English, most popular metrics (e.g., BLEU, NIST, PER, and TER) do not work well. It is well known that Japanese and English have completely different word orders, and special care must be paid to word order in translation. Otherwise, translations with wrong word order often lead to misunderstanding and incomprehensibility. For instance, SMT-based Japanese-to-English translators tend to translate 'A because B' as 'B because A.' Thus, word order is the most important problem for distant language translation. However, conventional evaluation metrics do not significantly penalize such word order mistakes. Therefore, locally optimizing these metrics leads to inadequate translations. In this paper, we propose an automatic evaluation metric based on rank correlation coefficients modified with precision. Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics."
W09-3917,Analysis of Listening-Oriented Dialogue for Building Listening Agents,2009,9,21,5,0,40255,toyomi meguro,Proceedings of the {SIGDIAL} 2009 Conference,0,"Our aim is to build listening agents that can attentively listen to the user and satisfy his/her desire to speak and have himself/herself heard. This paper investigates the characteristics of such listening-oriented dialogues so that such a listening process can be achieved by automated dialogue systems. We collected both listening-oriented dialogues and casual conversation, and analyzed them by comparing the frequency of dialogue acts, as well as the dialogue flows using Hidden Markov Models (HMMs). The analysis revealed that listening-oriented dialogues and casual conversation have characteristically different dialogue flows and that it is important for listening agents to self-disclose before asking questions and to utter more questions and acknowledgment than in casual conversation to be good listeners."
P09-2086,A Succinct N-gram Language Model,2009,10,9,3,1,128,taro watanabe,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"Efficient processing of tera-scale text data is an important research topic. This paper proposes lossless compression of N-gram language models based on LOUDS, a succinct data structure. LOUDS succinctly represents a trie with M nodes as a 2M  1 bit string. We compress it further for the N-gram language model structure. We also use 'variable length coding' and 'block-wise compression' to compress values associated with nodes. Experimental results for three large-scale N-gram compression tasks achieved a significant compression rate without any loss."
P09-1093,A Syntax-Free Approach to {J}apanese Sentence Compression,2009,17,5,3,1,3632,tsutomu hirao,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"Conventional sentence compression methods employ a syntactic parser to compress a sentence without changing its meaning. However, the reference compressions made by humans do not always retain the syntactic structures of the original sentences. Moreover, for the goal of on-demand sentence compression, the time spent in the parsing stage is not negligible. As an alternative to syntactic parsing, we propose a novel term weighting technique based on the positional information within the original sentence and a novel language model that combines statistics from the original sentence and a general corpus. Experiments that involve both human subjective evaluations and automatic evaluations show that our method outperforms Hori's method, a state-of-the-art conventional technique. Because our method does not use a syntactic parser, it is 4.3 times faster than Hori's method."
D09-1058,An Empirical Study of Semi-supervised Structured Conditional Models for Dependency Parsing,2009,25,73,2,1,9188,jun suzuki,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"This paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning approach. We describe an extension of semi-supervised structured conditional models (SS-SCMs) to the dependency parsing problem, whose framework is originally proposed in (Suzuki and Isozaki, 2008). Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with another semi-supervised approach, described in (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, such as those described in (Carreras, 2007), using a two-stage semi-supervised learning approach. We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections: the Penn Treebank for English, and the Prague Dependency Tree-bank for Czech. Our best results on test data in the above datasets achieve 93.79% parent-prediction accuracy for English, and 88.05% for Czech."
2009.iwslt-papers.3,Structural support vector machines for log-linear approach in statistical machine translation,2009,26,7,4,0,12296,katsuhiko hayashi,Proceedings of the 6th International Workshop on Spoken Language Translation: Papers,0,"Minimum error rate training (MERT) is a widely used learning method for statistical machine translation. In this paper, we present a SVM-based training method to enhance generalization ability. We extend MERT optimization by maximizing the margin between the reference and incorrect translations under the L2-norm prior to avoid overfitting problem. Translation accuracy obtained by our proposed methods is more stable in various conditions than that obtained by MERT. Our experimental results on the French-English WMT08 shared task show that degrade of our proposed methods is smaller than that of MERT in case of small training data or out-of-domain test data."
P08-1076,Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data,2008,19,113,2,1,9188,jun suzuki,Proceedings of ACL-08: HLT,1,"This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition. We first propose a simple yet powerful semi-supervised discriminative model appropriate for handling large scale unlabeled data. Then, we describe experiments performed on widely used test collections, namely, PTB III data, CoNLLxe2x80x9900 and xe2x80x9903 shared task data for the above three NLP tasks, respectively. We incorporate up to 1G-words (one billion tokens) of unlabeled data, which is the largest amount of unlabeled data ever used for these tasks, to investigate the performance improvement. In addition, our results are superior to the best reported results for all of the above test collections."
I08-2116,Multi-label Text Categorization with Model Combination based on F1-score Maximization,2008,12,13,2,1,42688,akinori fujino,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"Text categorization is a fundamental task in natural language processing, and is generally defined as a multi-label categorization problem, where each text document is assigned to one or more categories. We focus on providing good statistical classifiers with a generalization ability for multi-label categorization and present a classifier design method based on model combination and F1-score maximization. In our formulation, we first design multiple models for binary classification per category. Then, we combine these models to maximize the F1-score of a training dataset. Our experimental results confirmed that our proposed method was useful especially for datasets where there were many combinations of category labels."
I08-1055,Corpus-based Question Answering for why-Questions,2008,27,50,2,1,1442,ryuichiro higashinaka,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"This paper proposes a corpus-based approach for answering why-questions. Conventional systems use hand-crafted patterns to extract and evaluate answer candidates. However, such hand-crafted patterns are likely to have low coverage of causal expressions, and it is also difficult to assign suitable weights to the patterns by hand. In our approach, causal expressions are automatically collected from corpora tagged with semantic relations. From the collected expressions, features are created to train an answer candidate ranker that maximizes the QA performance with regards to the corpus of why-questions and answers. NAZEQA, a Japanese why-QA system based on our approach, clearly outperforms a baseline that uses hand-crafted patterns with a Mean Reciprocal Rank (top-5) of 0.305, making it presumably the best-performing fully implemented why-QA system."
2008.iwslt-evaluation.13,{NTT} statistical machine translation system for {IWSLT} 2008.,2008,18,3,5,1,1440,katsuhito sudoh,Proceedings of the 5th International Workshop on Spoken Language Translation: Evaluation Campaign,0,The NTT Statistical Machine Translation System consists of two primary components: a statistical machine translation decoder and a reranker. The decoder generates k-best translation canditates using a hierarchical phrase-based translation based on synchronous context-free grammar. The decoder employs a linear feature combination among several real-valued scores on translation and language models. The reranker reorders the k-best translation candidates using Ranking SVMs with a large number of sparse features. This paper describes the two components and presents the results for the evaluation campaign of IWSLT 2008.
P07-2030,Learning to Rank Definitions to Generate Quizzes for Interactive Information Presentation,2007,10,9,3,1,1442,ryuichiro higashinaka,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"This paper proposes the idea of ranking definitions of a person (a set of biographical facts) to automatically generate Who is this? quizzes. The definitions are ordered according to how difficult they make it to name the person. Such ranking would enable users to interactively learn about a person through dialogue with a system with improved understanding and lasting motivation, which is useful for educational systems. In our approach, we train a ranker that learns from data the appropriate ranking of definitions based on features that encode the importance of keywords in a definition as well as its content. Experimental results show that our approach is significantly better in ranking definitions than baselines that use conventional information retrieval measures such as tf*idf and pointwise mutual information (PMI)."
D07-1080,Online Large-Margin Training for Statistical Machine Translation,2007,23,176,4,1,128,taro watanabe,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,We achieved a state of the art performance in statistical machine translation by using a large number of features with an online large-margin training algorithm. The millions of parameters were tuned only on a small development set consisting of less than 1K sentences. Experiments on Arabic-toEnglish translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features.
D07-1083,Semi-Supervised Structured Output Learning Based on a Hybrid Generative and Discriminative Approach,2007,23,33,3,1,9188,jun suzuki,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"This paper proposes a framework for semi-supervised structured output learning (SOL), specifically for sequence labeling, based on a hybrid generative and discriminative approach. We define the objective function of our hybrid model, which is written in log-linear form, by discriminatively combining discriminative structured predictor(s) with generative model(s) that incorporate unlabeled data. Then, unlabeled data is used in a generative manner to increase the sum of the discriminant functions for all outputs during the parameter estimation. Experiments on named entity recognition (CoNLL-2003) and syntactic chunking (CoNLL-2000) data show that our hybrid model significantly outperforms the stateof-the-art performance obtained with supervised SOL methods, such as conditional random fields (CRFs)."
2007.iwslt-1.16,Larger feature set approach for machine translation in {IWSLT} 2007,2007,28,2,5,1,128,taro watanabe,Proceedings of the Fourth International Workshop on Spoken Language Translation,0,"The NTT Statistical Machine Translation System employs a large number of feature functions. First, k-best translation candidates are generated by an efficient decoding method of hierarchical phrase-based translation. Second, the k-best translations are reranked. In both steps, sparse binary features {---} of the order of millions {---} are integrated during the search. This paper gives the details of the two steps and shows the results for the Evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2007."
W06-3115,{NTT} System Description for the {WMT}2006 Shared Task,2006,10,6,3,1,128,taro watanabe,Proceedings on the Workshop on Statistical Machine Translation,0,"We present two translation systems experimented for the shared-task of Workshop on Statistical Machine Translation, a phrase-based model and a hierarchical phrase-based model. The former uses a phrasal unit for translation, whereas the latter is conceptualized as a synchronous-CFG in which phrases are hierarchically combined using non-terminals. Experiments showed that the hierarchical phrase-based model performed very comparable to the phrase-based model. We also report a phrase/rule extraction technique differentiating tokenization of corpora."
P06-1028,Training Conditional Random Fields with Multivariate Evaluation Measures,2006,22,45,3,1,9188,jun suzuki,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"This paper proposes a framework for training Conditional Random Fields (CRFs) to optimize multivariate evaluation measures, including non-linear measures such as F-score. Our proposed framework is derived from an error minimization approach that provides a simple solution for directly optimizing any evaluation measure. Specifically focusing on sequential segmentation tasks, i.e. text chunking and named entity recognition, we introduce a loss function that closely reflects the target evaluation measure for these tasks, namely, segmentation F-score. Our experiments show that our method performs better than standard CRF training."
P06-1078,Incorporating Speech Recognition Confidence into Discriminative Named Entity Recognition of Speech Data,2006,18,16,3,1,1440,katsuhito sudoh,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"This paper proposes a named entity recognition (NER) method for speech recognition results that uses confidence on automatic speech recognition (ASR) as a feature. The ASR confidence feature indicates whether each word has been correctly recognized. The NER model is trained using ASR results with named entity (NE) labels as well as the corresponding transcriptions with NE labels. In experiments using support vector machines (SVMs) and speech data from Japanese newspaper articles, the proposed method outperformed a simple application of text-based NER to ASR results in NER F-measure by improving precision. These results show that the proposed method is effective in NER for noisy inputs."
P06-1098,Left-to-Right Target Generation for Hierarchical Phrase-Based Translation,2006,14,45,3,1,128,taro watanabe,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"We present a hierarchical phrase-based statistical machine translation in which a target sentence is efficiently generated in left-to-right order. The model is a class of synchronous-CFG with a Greibach Normal Form-like structure for the projected production rule: The paired target-side of a production rule takes a phrase prefixed form. The decoder for the target-normalized form is based on an Early-style top down parser on the source side. The target-normalized form coupled with our top down parser implies a left-to-right generation of translations which enables us a straightforward integration with ngram language models. Our model was experimented on a Japanese-to-English newswire translation task, and showed statistically significant performance improvements against a phrase-based translation system."
2006.iwslt-evaluation.14,{NTT} statistical machine translation for {IWSLT} 2006,2006,15,15,4,1,128,taro watanabe,Proceedings of the Third International Workshop on Spoken Language Translation: Evaluation Campaign,0,"We present the NTT translation system that is experimented for the evaluation campaign of xe2x80x9cInternational Workshop on Spoken Language Translation (IWSLT).xe2x80x9d The system consists of two primary components: a hierarchical phrase-based statistical machine translation system and a reranking sys tem. The former is conceptualized as a synchronous-CFG in which phrases are hierarchically combined using nonterminals. The latter uses a modified voted perceptron approach with large number of features. Experiments showed that our hierarchical phrase-based model outperformed a conventional phrase-based model. In addition, our reranking algorithm further boosted the performance."
P05-1024,Boosting-based Parse Reranking with Subtree Features,2005,21,47,3,0,29083,taku kudo,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"This paper introduces a new application of boosting for parse reranking. Several parsers have been proposed that utilize the all-subtrees representation (e.g., tree kernel and data oriented parsing). This paper argues that such an all-subtrees representation is extremely redundant and a comparable accuracy can be achieved using just a small set of subtrees. We show how the boosting algorithm can be applied to the all-subtrees representation and how it selects a small and relevant feature set efficiently. Two experiments on parse reranking show that our method achieves comparable or even better performance than kernel methods and also improves the testing efficiency."
H05-1019,Kernel-based Approach for Automatic Evaluation of Natural Language Generation Technologies: Application to Automatic Summarization,2005,11,5,3,1,3632,tsutomu hirao,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"In order to promote the study of automatic summarization and translation, we need an accurate automatic evaluation method that is close to human evaluation. In this paper, we present an evaluation method that is based on convolution kernels that measure the similarities between texts considering their substructures. We conducted an experiment using automatic summarization evaluation data developed for Text Summarization Challenge 3 (TSC-3). A comparison with conventional techniques shows that our method correlates more closely with human evaluations and is more robust."
2005.iwslt-1.15,The {NTT} Statistical Machine Translation System for {IWSLT}2005,2005,19,4,5,1,38029,hajime tsukada,Proceedings of the Second International Workshop on Spoken Language Translation,0,"This paper reports the NTT statistical translation system participating in the evaluation campaign of IWSLT 2005. The NTT system is based on a phrase translation model and utilizes a large number of features with a log-linear model. We studied the various features recently developed in this research field and evaluate the system using supplied data as well as publicly available Chinese, Japanese, and English data. Despite domain mismatch, additional data helped improve translation accuracy."
W04-1014,Evaluation Measures Considering Sentence Concatenation for Automatic Summarization by Sentence or Word Extraction,2004,-1,-1,3,1,40303,chiori hori,Text Summarization Branches Out,0,None
P04-1016,Convolution Kernels with Feature Selection for Natural Language Processing Tasks,2004,15,34,2,1,9188,jun suzuki,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"Convolution kernels, such as sequence and tree kernels, are advantageous for both the concept and accuracy of many natural language processing (NLP) tasks. Experiments have, however, shown that the over-fitting problem often arises when these kernels are used in NLP tasks. This paper discusses this issue of convolution kernels, and then proposes a new approach based on statistical feature selection that avoids this issue. To enable the proposed method to be executed efficiently, it is embedded into an original kernel calculation process by using sub-structure mining algorithms. Experiments are undertaken on real NLP tasks to confirm the problem with a conventional method and to compare its performance with that of the proposed method."
C04-1040,A Deterministic Word Dependency Analyzer Enhanced With Preference Learning,2004,22,10,1,1,36870,hideki isozaki,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Word dependency is important in parsing technology. Some applications such as Information Extraction from biological documents benefit from word dependency analysis even without phrase labels. Therefore, we expect an accurate dependency analyzer trainable without using phrase labels is useful. Although such an English word dependency analyzer was proposed by Yamada and Matsumoto, its accuracy is lower than state-of-the-art phrase structure parsers because of the lack of top-down information given by phrase labels. This paper shows that the dependency analyzer can be improved by introducing a Root-Node Finder and a Prepositional-Phrase Attachment Resolver. Experimental results show that these modules based on Preference Learning give better scores than Collins' Model 3 parser for these subproblems. We expect this method is also applicable to phrase structure parsers."
C04-1064,Dependency-based Sentence Alignment for Multiple Document Summarization,2004,13,23,3,1,3632,tsutomu hirao,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In this paper, we describe a method of automatic sentence alignment for building extracts from abstracts in automatic summarization research. Our method is based on two steps. First, we introduce the dependency tree path (DTP). Next, we calculate the similarity between DTPs based on the ESK (Extended String Subsequence Kernel), which considers sequential patterns. By using these procedures, we can derive one-to-many or many-to-one correspondences among sentences. Experiments using different similarity measures show that DTP consistently improves the alignment accuracy and that ESK gives the best performance."
W03-1024,{J}apanese Zero Pronoun Resolution based on Ranking Rules and Machine Learning,2003,19,26,1,1,36870,hideki isozaki,Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,0,"Anaphora resolution is one of the most important research topics in Natural Language Processing. In English, overt pronouns such as she and definite noun phrases such as the company are anaphors that refer to preceding entities (antecedents). In Japanese, anaphors are often omitted, and these omissions are called zero pronouns. There are two major approaches to zero pronoun resolution: the heuristic approach and the machine learning approach. Since we have to take various factors into consideration, it is difficult to find a good combination of heuristic rules. Therefore, the machine learning approach is attractive, but it requires a large amount of training data. In this paper, we propose a method that combines ranking rules and machine learning. The ranking rules are simple and effective, while machine learning can take more factors into account. From the results of our experiments, this combination gives better performance than either of the two previous approaches."
P03-2028,Spoken Interactive {ODQA} System: {SPIQA},2003,9,14,4,1,40303,chiori hori,The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,0,"We have been investigating an interactive approach for Open-domain QA (ODQA) and have constructed a spoken interactive ODQA system, SPIQA. The system derives disambiguating queries (DQs) that draw out additional information. To test the efficiency of additional information requested by the DQs, the system reconstructs the user's initial question by combining the addition information with question. The combination is then used for answer extraction. Experimental results revealed the potential of the generated DQs."
C02-1053,Extracting Important Sentences with Support Vector Machines,2002,16,62,2,1,3632,tsutomu hirao,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"Extracting sentences that contain important information from a document is a form of text summarization. The technique is the key to the automatic generation of summaries similar to those written by humans. To achieve such extraction, it is important to be able to integrate heterogeneous pieces of information. One approach, parameter tuning by machine learning, has been attracting a lot of attention. This paper proposes a method of sentence extraction based on Support Vector Machines (SVMs). To confirm the method's performance, we conduct experiments that compare our method to three existing methods. Results on the Text Summarization Challenge (TSC) corpus show that our method offers the highest accuracy. Moreover, we clarify the different features effective for extracting different document genres."
C02-1054,Efficient Support Vector Classifiers for Named Entity Recognition,2002,22,217,1,1,36870,hideki isozaki,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"Named Entity (NE) recognition is a task in which proper nouns and numerical information are extracted from documents and are classified into categories such as person, organization, and date. It is a key technology of Information Extraction and Open-Domain Question Answering. First, we show that an NE recognizer based on Support Vector Machines (SVMs) gives better scores than conventional systems. However, off-the-shelf SVM classifiers are too inefficient for this task. Therefore, we present a method that makes the system substantially faster. This approach can also be applied to other similar tasks such as chunking and part-of-speech tagging. We also present an SVM-based feature selection method and an efficient training method."
P01-1041,{J}apanese Named Entity Recognition based on a Simple Rule Generator and Decision Tree Learning,2001,16,42,1,1,36870,hideki isozaki,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"Named entity (NE) recognition is a task in which proper nouns and numerical information in a document are detected and classified into categories such as person, organization, location, and date. NE recognition plays an essential role in information extraction systems and question answering systems. It is well known that hand-crafted systems with a large set of heuristic rules are difficult to maintain, and corpus-based statistical approaches are expected to be more robust and require less human intervention. Several statistical approaches have been reported in the literature. In a recent Japanese NE workshop, a maximum entropy (ME) system outperformed decision tree systems and most hand-crafted systems. Here, we propose an alternative method based on a simple rule generator and decision tree learning. Our experiments show that its performance is comparable to the ME approach. We also found that it can be trained more efficiently with a large set of training data and that it improves readability."
