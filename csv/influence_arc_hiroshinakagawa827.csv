C08-1100,W02-0908,0,0.0849305,"Various methods (Hindle, 1990; Lin, 1998) of automatically acquiring synonyms have been proposed. They are usually based on the distributional hypothesis (Harris, 1985), which states that semantically similar words share similar contexts, and they can be roughly viewed as the combinations of two steps: context extraction and similarity calculation. The former extracts useful features from the contexts of words, such as surrounding words or dependency structure. The latter calculates how semantically similar two given words are based on similarity or distance metrics. Many studies (Lee, 1999; Curran and Moens, 2002; Weeds et al., 2004) have investigated similarity calculation, and a variety of distance/similarity measures have already been compared and discussed. Weeds et al.’s work is especially useful because it investigated the characteristics of metrics based on a few criteria such as the relative frequency of acquired synonyms and clarified the correlation between word frequency, distributional generality, and semantic generality. However, all of the existing research conducted only a posteriori comparison, and as Weeds et al. pointed out, there is no one best measure for all applications. Therefor"
C08-1100,I08-1072,1,0.779656,"Missing"
C08-1100,P90-1034,0,0.299936,"pplying this metric to synonym acquisition. We conclude in section 7. 2 Prior Work As this paper is based on two different lines of research, we first review the work in synonym acquisition, and then review the work in generic metric learning. To the best of the authors’ knowledge, none of the metric learning algorithms have been applied to automatic synonym acquisition. Synonym relation is important lexical knowledge for many natural language processing tasks including automatic thesaurus construction (Croach and Yang, 1992; Grefenstette, 1994) and IR (Jing and Croft, 1994). Various methods (Hindle, 1990; Lin, 1998) of automatically acquiring synonyms have been proposed. They are usually based on the distributional hypothesis (Harris, 1985), which states that semantically similar words share similar contexts, and they can be roughly viewed as the combinations of two steps: context extraction and similarity calculation. The former extracts useful features from the contexts of words, such as surrounding words or dependency structure. The latter calculates how semantically similar two given words are based on similarity or distance metrics. Many studies (Lee, 1999; Curran and Moens, 2002; Weeds"
C08-1100,W06-1605,0,0.0232276,"compared a number of metrics in our experiments, and the results show that the proposed metric has a higher mean average precision than other metrics. 1 Introduction Accurately estimating the semantic distance between words in context has applications for machine translation, information retrieval (IR), speech recognition, and text categorization (Budanitsky and Hirst, 2006), and it is becoming clear that a combination of corpus statistics can be used with a dictionary, thesaurus, or other knowledge source such as WordNet or Wikipedia, to increase the accuracy of semantic distance estimation (Mohammad and Hirst, 2006). Although compiling such resources is labor intensive and achieving wide coverage is difficult, these resources to some extent explicitly capture semantic structures c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. of concepts and words. In contrast, corpus statistics achieve wide coverage, but the semantic structure of a concept is only implicitly represented in the context. Assuming that two words are semantically closer if they occur in similar contexts, statisti"
C08-1100,O97-1002,0,0.0856242,"Missing"
C08-1100,P98-2127,0,0.349086,"etric to synonym acquisition. We conclude in section 7. 2 Prior Work As this paper is based on two different lines of research, we first review the work in synonym acquisition, and then review the work in generic metric learning. To the best of the authors’ knowledge, none of the metric learning algorithms have been applied to automatic synonym acquisition. Synonym relation is important lexical knowledge for many natural language processing tasks including automatic thesaurus construction (Croach and Yang, 1992; Grefenstette, 1994) and IR (Jing and Croft, 1994). Various methods (Hindle, 1990; Lin, 1998) of automatically acquiring synonyms have been proposed. They are usually based on the distributional hypothesis (Harris, 1985), which states that semantically similar words share similar contexts, and they can be roughly viewed as the combinations of two steps: context extraction and similarity calculation. The former extracts useful features from the contexts of words, such as surrounding words or dependency structure. The latter calculates how semantically similar two given words are based on similarity or distance metrics. Many studies (Lee, 1999; Curran and Moens, 2002; Weeds et al., 2004"
C08-1100,C04-1146,0,\N,Missing
C08-1100,P99-1004,0,\N,Missing
C08-1100,P06-4020,0,\N,Missing
C08-1100,J06-1003,0,\N,Missing
C08-1100,C98-2122,0,\N,Missing
C12-1049,C10-2032,0,0.0336674,"nt in terms of native language. In computational linguistics, Kireyev and Landauer (2011) proposed an extension of word difficulty called “word maturity” by focusing on the analysis of child development in terms of native language. Their extension was aimed at “track the degree of knowledge of each word at different stages of language learning” using latent semantic analysis. Thus, both their purpose and method of extending word difficulty differ from ours. While few have studied the vocabulary prediction task, prediction of text readability has been of great focus (François and Fairon, 2012; Feng et al., 2010; Kate et al., 2010) in computational 811 linguistics. The relationship between vocabulary knowledge and text readability has been thoroughly studied by educational experts (Nation, 2006). A substantial amount of work has been done by mainly SLA experts in estimating vocabulary size. Two major testing approaches have been proposed: multiple-choice, (Nation, 1990), and Yes/No (Meara and Buxton, 1987). For Yes/No tests, Eyckmans (2004) studied the validity and relation to readability prediction. In the field of psychology, the shared difficulty model (Ehara et al., 2010) is almost mathematically"
C12-1049,D12-1043,0,0.0179622,"analysis of child development in terms of native language. In computational linguistics, Kireyev and Landauer (2011) proposed an extension of word difficulty called “word maturity” by focusing on the analysis of child development in terms of native language. Their extension was aimed at “track the degree of knowledge of each word at different stages of language learning” using latent semantic analysis. Thus, both their purpose and method of extending word difficulty differ from ours. While few have studied the vocabulary prediction task, prediction of text readability has been of great focus (François and Fairon, 2012; Feng et al., 2010; Kate et al., 2010) in computational 811 linguistics. The relationship between vocabulary knowledge and text readability has been thoroughly studied by educational experts (Nation, 2006). A substantial amount of work has been done by mainly SLA experts in estimating vocabulary size. Two major testing approaches have been proposed: multiple-choice, (Nation, 1990), and Yes/No (Meara and Buxton, 1987). For Yes/No tests, Eyckmans (2004) studied the validity and relation to readability prediction. In the field of psychology, the shared difficulty model (Ehara et al., 2010) is al"
C12-1049,C10-1062,0,0.0265187,"ve language. In computational linguistics, Kireyev and Landauer (2011) proposed an extension of word difficulty called “word maturity” by focusing on the analysis of child development in terms of native language. Their extension was aimed at “track the degree of knowledge of each word at different stages of language learning” using latent semantic analysis. Thus, both their purpose and method of extending word difficulty differ from ours. While few have studied the vocabulary prediction task, prediction of text readability has been of great focus (François and Fairon, 2012; Feng et al., 2010; Kate et al., 2010) in computational 811 linguistics. The relationship between vocabulary knowledge and text readability has been thoroughly studied by educational experts (Nation, 2006). A substantial amount of work has been done by mainly SLA experts in estimating vocabulary size. Two major testing approaches have been proposed: multiple-choice, (Nation, 1990), and Yes/No (Meara and Buxton, 1987). For Yes/No tests, Eyckmans (2004) studied the validity and relation to readability prediction. In the field of psychology, the shared difficulty model (Ehara et al., 2010) is almost mathematically identical to the li"
C12-1049,P11-1031,0,0.0298035,"d models, let alone the generalization of the likelihood of the Rasch model. Strictly speaking, these two models differ from our model in that they do not include the Rasch and shared difficulty models (Ehara et al., 2010) as special cases while our proposed model does. We extended word difficulty to learner-specific word difficulty by focusing on the analysis of the vocabulary knowledge of adult second language learners. Aside from second languages, study of vocabulary knowledge is also important for the analysis of child development in terms of native language. In computational linguistics, Kireyev and Landauer (2011) proposed an extension of word difficulty called “word maturity” by focusing on the analysis of child development in terms of native language. Their extension was aimed at “track the degree of knowledge of each word at different stages of language learning” using latent semantic analysis. Thus, both their purpose and method of extending word difficulty differ from ours. While few have studied the vocabulary prediction task, prediction of text readability has been of great focus (François and Fairon, 2012; Feng et al., 2010; Kate et al., 2010) in computational 811 linguistics. The relationship"
C88-2097,P83-1017,0,\N,Missing
C88-2097,P84-1031,0,\N,Missing
C88-2097,P84-1054,0,\N,Missing
C92-1051,C90-2047,0,0.505614,"ferred to Retaining is preferred to Shifting-1 is preferred to Shifting. These states are characterized as follows. Here Cp(U~) means the highest ranked Cf(Ui). Continuing: Cb(U/_I) = Cb(Ui) = Cp(U/) Retaining: Cb(U,_I) = Cb(Ui) # Cp(Ui) Shifting-l: Cb(U,_t) # Cb(U,) = Cp(U,) Shifting: Cb(U;_~) # Cb(Ui) # Cp(Ui) Actually, the antecedent of zero pronoun is determined under satisfying all these constraints and rules. This centering mechanism can account for a very broad range of Japanese zero pronoun anaphora. Almost the same centering mechanism applies to Italian pronominal system successfully [2]. PROC.OFCOLING-92,NANTES,AU~.23-28, 1992 Another important theory is Property sharing theory proposed by Kameyama [4, 5]. Her theory concerns the interaction between Cb and zero pronouns. Consider adjacent uttermaces or a sentence including a subordinated clause. Two zero pronouns appearing distinct utterances or clauses can retain the same Cb if they share one of the following properties: 1) IDENT-SUBJECT, 2) IDENT alone, 3) S U B J E C T alone, 4) bott, nonIDENT and non-SUBJECT, 5)non-IDENT only, 6)non-SUBJ only, where the descending order means preference, and IDENT is almost the same as,"
C92-1051,P86-1031,0,0.283767,"re Cp(U~) means the highest ranked Cf(Ui). Continuing: Cb(U/_I) = Cb(Ui) = Cp(U/) Retaining: Cb(U,_I) = Cb(Ui) # Cp(Ui) Shifting-l: Cb(U,_t) # Cb(U,) = Cp(U,) Shifting: Cb(U;_~) # Cb(Ui) # Cp(Ui) Actually, the antecedent of zero pronoun is determined under satisfying all these constraints and rules. This centering mechanism can account for a very broad range of Japanese zero pronoun anaphora. Almost the same centering mechanism applies to Italian pronominal system successfully [2]. PROC.OFCOLING-92,NANTES,AU~.23-28, 1992 Another important theory is Property sharing theory proposed by Kameyama [4, 5]. Her theory concerns the interaction between Cb and zero pronouns. Consider adjacent uttermaces or a sentence including a subordinated clause. Two zero pronouns appearing distinct utterances or clauses can retain the same Cb if they share one of the following properties: 1) IDENT-SUBJECT, 2) IDENT alone, 3) S U B J E C T alone, 4) bott, nonIDENT and non-SUBJECT, 5)non-IDENT only, 6)non-SUBJ only, where the descending order means preference, and IDENT is almost the same as, or very near, the empathy proposed by Kuno [7]. Although these theories cover a fairly large part of Japanese zero anapho"
C92-1051,P87-1022,0,\N,Missing
C94-2107,E89-1013,0,\N,Missing
C94-2107,P87-1022,0,\N,Missing
C96-2132,P87-1022,0,0.284132,"Missing"
C96-2132,P92-1016,0,\N,Missing
C98-2148,A94-1029,0,0.0646696,"Missing"
C98-2148,A92-1037,0,0.0204936,"Basili et al. (1994) use document structnres and semantic information by means of natural language processing technique to set hyl)erlinks on plain texts. The essential point in the research of automatic hypertext authoring is the way to find semantically relevant parts where each part is characterized by a nmnbcr of key words. Actually it is very similar 929 with information retrieval, IR henceforth, especially with the so called passage retrieval (Salton et al., 1993). J.Green (1996) does hypertext authoring of newspaper articles by word's lexical chains which are calculated using WordNet. Kurohashi et al. (1992) made a hypertext dictionary of tile field of inlbrmation science. They use linguistic patterns that are used for definition of terminology as well ~Ls thesaurus based on words' similarity. Furner-llines and WiIlett (1994) experimentally evaluate and compare tile performance of several hnman hyper linkers. In general, however, we have not yet paid enough attention to a flfll-automatic hyper linker system, that is what we pursue in this paper. The new ideas in our system are tile following points: 1. Our target is a lmdti-volmne manual that describes the same hardware or soft.ware but is difflu"
D07-1129,W06-1615,0,0.698516,"e that a model trained in the source domain using this common feature representation will generalize better to the target domain, and focus on using unlabeled data from both the source and target domains to learn a common feature representation that is meaningful across both domains. The paper is structured as follows: in section 2, we review the decoding and learning aspects of (McDonald et al., 2005), in section 3, structural correspondence learning applied to dependency parsing, and in section 4, we describe the experiments and the features needed for the CoNLL 2006 shared task. Following (Blitzer et al., 2006), we present an application of structural correspondence learning to non-projective dependency parsing (McDonald et al., 2005). To induce the correspondences among dependency edges from different domains, we looked at every two tokens in a sentence and examined whether or not there is a preposition, a determiner or a helping verb between them. Three binary linear classifiers were trained to predict the existence of a preposition, etc, on unlabeled data and we used singular value decomposition to induce new features. During the training, the parser was trained with these additional features in"
D07-1129,H05-1091,0,0.0376128,"to induce new features. During the training, the parser was trained with these additional features in addition to these described in (McDonald et al., 2005). We discriminatively trained our parser in an on-line fashion using a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004; Crammer and Singer, 2003). 2 Non-Projective Dependency Parsing 2.1 1 Introduction We have recently seen growing popularity of dependency parsing. It is no longer rare to see dependency relations used as features, in tasks such as machine translation (Ding and Palmer, 2005) and relation extraction (Bunescu and Mooney, 2005). However, there is one factor that prevents the use of dependency parsing: sparseness of annotated corpora outside Wall Street Journal. In many situations we need to parse sentences from a target domain with no labeled data, which is a different distribution from a Dependency Structure Let us define x to be a generic sequence of input tokens together with their POS tags and other morphological features, and y to be a generic dependency structure, that is, a set of edges for x. A labeled edge is a tuple hDEPREL, i → ji where i is the start point of the edge, j is the end point, and DEPREL is t"
D07-1129,P04-1015,0,0.188912,"y edges from different domains, we looked at every two tokens in a sentence and examined whether or not there is a preposition, a determiner or a helping verb between them. Three binary linear classifiers were trained to predict the existence of a preposition, etc, on unlabeled data and we used singular value decomposition to induce new features. During the training, the parser was trained with these additional features in addition to these described in (McDonald et al., 2005). We discriminatively trained our parser in an on-line fashion using a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004; Crammer and Singer, 2003). 2 Non-Projective Dependency Parsing 2.1 1 Introduction We have recently seen growing popularity of dependency parsing. It is no longer rare to see dependency relations used as features, in tasks such as machine translation (Ding and Palmer, 2005) and relation extraction (Bunescu and Mooney, 2005). However, there is one factor that prevents the use of dependency parsing: sparseness of annotated corpora outside Wall Street Journal. In many situations we need to parse sentences from a target domain with no labeled data, which is a different distribution from a Depende"
D07-1129,W02-1001,0,0.177953,"among dependency edges from different domains, we looked at every two tokens in a sentence and examined whether or not there is a preposition, a determiner or a helping verb between them. Three binary linear classifiers were trained to predict the existence of a preposition, etc, on unlabeled data and we used singular value decomposition to induce new features. During the training, the parser was trained with these additional features in addition to these described in (McDonald et al., 2005). We discriminatively trained our parser in an on-line fashion using a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004; Crammer and Singer, 2003). 2 Non-Projective Dependency Parsing 2.1 1 Introduction We have recently seen growing popularity of dependency parsing. It is no longer rare to see dependency relations used as features, in tasks such as machine translation (Ding and Palmer, 2005) and relation extraction (Bunescu and Mooney, 2005). However, there is one factor that prevents the use of dependency parsing: sparseness of annotated corpora outside Wall Street Journal. In many situations we need to parse sentences from a target domain with no labeled data, which is a different di"
D07-1129,P05-1067,0,0.0270491,"ed data and we used singular value decomposition to induce new features. During the training, the parser was trained with these additional features in addition to these described in (McDonald et al., 2005). We discriminatively trained our parser in an on-line fashion using a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004; Crammer and Singer, 2003). 2 Non-Projective Dependency Parsing 2.1 1 Introduction We have recently seen growing popularity of dependency parsing. It is no longer rare to see dependency relations used as features, in tasks such as machine translation (Ding and Palmer, 2005) and relation extraction (Bunescu and Mooney, 2005). However, there is one factor that prevents the use of dependency parsing: sparseness of annotated corpora outside Wall Street Journal. In many situations we need to parse sentences from a target domain with no labeled data, which is a different distribution from a Dependency Structure Let us define x to be a generic sequence of input tokens together with their POS tags and other morphological features, and y to be a generic dependency structure, that is, a set of edges for x. A labeled edge is a tuple hDEPREL, i → ji where i is the start poi"
D07-1129,W07-2416,0,0.0242216,"ose of the resulting unitary matrix, U ⊤ which maps the original data to the space spanned by the principal components, and applied it to the feature vector of every  potential edge. The original feature vector is fsubset freminder . We argument the feature vector with the additional feature induced by ! U ⊤ . The augmented feature vectors fsubset freminder U ⊤ fsubset were used throughout the training and testing of the dependency parser. 4 Experiments Our experiments were conducted on CoNLL-2007 shared task domain adaptation track (Nivre et al., 2007) using treebanks (Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004). 1168 Given an edge hDEPREL, i, ji head−1 = wordi−1 head = wordi head+1 = wordi+1 child−1 = wordj−1 child = wordj child+1 = wordj+1 Table 2: Binary Features for Pivot Predictors 4.1 Dependency Relation The CLE algorithm works on a directed graph with unlabeled edges. Since the CoNLL shared task requires the labeling of edges, as a preprocessing stage, we created a directed complete graph. Then we labeled each edge with the highest scoring dependency relation. This complete graph was given to the CLE algorithm and the edge labels were never altered in the course of findin"
D07-1129,W04-3111,0,0.0298455,"matrix, U ⊤ which maps the original data to the space spanned by the principal components, and applied it to the feature vector of every  potential edge. The original feature vector is fsubset freminder . We argument the feature vector with the additional feature induced by ! U ⊤ . The augmented feature vectors fsubset freminder U ⊤ fsubset were used throughout the training and testing of the dependency parser. 4 Experiments Our experiments were conducted on CoNLL-2007 shared task domain adaptation track (Nivre et al., 2007) using treebanks (Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004). 1168 Given an edge hDEPREL, i, ji head−1 = wordi−1 head = wordi head+1 = wordi+1 child−1 = wordj−1 child = wordj child+1 = wordj+1 Table 2: Binary Features for Pivot Predictors 4.1 Dependency Relation The CLE algorithm works on a directed graph with unlabeled edges. Since the CoNLL shared task requires the labeling of edges, as a preprocessing stage, we created a directed complete graph. Then we labeled each edge with the highest scoring dependency relation. This complete graph was given to the CLE algorithm and the edge labels were never altered in the course of finding the maximum spanning"
D07-1129,J93-2004,0,0.0274514,"then took the transpose of the resulting unitary matrix, U ⊤ which maps the original data to the space spanned by the principal components, and applied it to the feature vector of every  potential edge. The original feature vector is fsubset freminder . We argument the feature vector with the additional feature induced by ! U ⊤ . The augmented feature vectors fsubset freminder U ⊤ fsubset were used throughout the training and testing of the dependency parser. 4 Experiments Our experiments were conducted on CoNLL-2007 shared task domain adaptation track (Nivre et al., 2007) using treebanks (Marcus et al., 1993; Johansson and Nugues, 2007; Kulick et al., 2004). 1168 Given an edge hDEPREL, i, ji head−1 = wordi−1 head = wordi head+1 = wordi+1 child−1 = wordj−1 child = wordj child+1 = wordj+1 Table 2: Binary Features for Pivot Predictors 4.1 Dependency Relation The CLE algorithm works on a directed graph with unlabeled edges. Since the CoNLL shared task requires the labeling of edges, as a preprocessing stage, we created a directed complete graph. Then we labeled each edge with the highest scoring dependency relation. This complete graph was given to the CLE algorithm and the edge labels were never alt"
D07-1129,H05-1066,0,0.0609964,"Missing"
D07-1129,D07-1096,0,\N,Missing
D13-1069,P10-1084,0,0.258111,"Missing"
D13-1069,P08-1080,0,0.431429,"Missing"
D13-1069,N06-1059,0,0.0487376,"Missing"
D13-1069,N03-1020,0,0.0886251,"gth learned by our BNP model. This result partly demonstrates our length determination is rational and it can be used as the recommended length for some constant-length summarization systems, such as the Linear . Ϭϯϱ Ϭϰϱ ZŽƵŐĞͲ> Ϭϯ ZĂŶĚŽŵ ϬϮϱ DDZ ϬϮ >ĞǆƌĂŶŬ >ŝŶĞĂƌ Ϭϭϱ Ϭϰ Ϭϯϱ Ϭϯ Ϭϯ EW ZĂƚĞͲĚŝƐƚ ZŽƵŐĞͲϭ ϬϮϱ Ϭϭ Ϭ ϭϬϬ ϮϬϬ ϯϬϬ ϰϬϬ ZŽƵŐĞͲ> ^ƵŵŵĂƌǇ>ĞŶŐƚŚ;EƵŵďĞƌŽĨtŽƌĚƐͿ ϬϮ ZŽƵŐĞͲϮ ZŽƵŐĞͲ^h Ϭϭϱ Figure 3: Rouge-L values on DUC2004 dataset. Ϭϭ All the compared systems are implemented at different predeﬁned lengths from 50 to 300 words. Then we evaluate the summaries with ROUGE4 tools (Lin and Hovy, 2003) in terms of the f-measure 4 ϬϬϱ Ϭ ϭϬϬ ϮϬϬ ϯϬϬ ϰϬϬ ^ƵŵŵĂƌǇtŽƌĚŽƵŶƚ Figure 4: Rate-dist value V.S. summary word length. we use ROUGE1.5.5 in this work. 742 The Rouge evaluation requires golden standard summaries as the base. However, in many cases we cannot get the reference summaries. For example, when we implement experiments on our expanded datasets (the separate and combined clusters of documents), we do not have exact reference summaries. Louis and Nenkova (2009) advanced an automatic summary evaluation without human models. They used the Jensen-Shannon divergence(JSD) between the inpu"
D13-1069,D09-1032,0,0.0940107,"ed systems are implemented at different predeﬁned lengths from 50 to 300 words. Then we evaluate the summaries with ROUGE4 tools (Lin and Hovy, 2003) in terms of the f-measure 4 ϬϬϱ Ϭ ϭϬϬ ϮϬϬ ϯϬϬ ϰϬϬ ^ƵŵŵĂƌǇtŽƌĚŽƵŶƚ Figure 4: Rate-dist value V.S. summary word length. we use ROUGE1.5.5 in this work. 742 The Rouge evaluation requires golden standard summaries as the base. However, in many cases we cannot get the reference summaries. For example, when we implement experiments on our expanded datasets (the separate and combined clusters of documents), we do not have exact reference summaries. Louis and Nenkova (2009) advanced an automatic summary evaluation without human models. They used the Jensen-Shannon divergence(JSD) between the input documents and the summaries as a feature, and got high correlation with human evaluations and the rouge metric. Unfortunately, it was designed for comparison at a constant-length, which cannot meet our needs. To extend the JSD evaluation to compare varying-length summaries, we propose a new measure based on information theory, the ratedistortion (Cover and Thomas, 2006). Rate-Distortion: The distortion function d(x, x ˆ) is a measure of the cost of representing the sym"
D14-1143,C12-1049,1,0.369758,"is essential to support them when they are reading. Educational experts have been continuously studying methods for measuring the size of a learner’s vocabulary, i.e., the number of words ∗ The main body of this work was done when the first author was a Ph.D. candidate in the University of Tokyo and the paper was later greatly revised when the first author was a JSPS (Japan Society for the Promotion of Science) research fellow (PD) at National Institute of Informatics. See http: //yoehara.com/ for details. the learner knows, over the decades (Meara and Buxton, 1987; Laufer and Nation, 1999). Ehara et al. (2012) formalized a more fine-grained measurement task called vocabulary prediction. The goal of this task is to predict whether a learner knows a given word based on only a relatively small portion of his/her vocabulary. This vocabulary prediction task can be further used for predicting the readability of texts. By predicting vocabulary unknown to readers and showing the meaning of those specific words to readers, Ehara et al. (2013) showed that the number of documents that learners can read increases. Word sampling is essential for vocabulary prediction. Because of the large size of language vocab"
E09-1069,J97-4005,0,0.0599797,"(i) deterministic shift-reduce parsing for unification-based grammars, and (ii) best-first shift-reduce parsing with beam thresholding for unification-based grammars. Deterministic parsing cannot simply be applied to unification-based grammar parsing, which often fails because of its hard constraints. Therefore, it is developed by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in grammars. 1 Introduction Over the last few decades, probabilistic unification-based grammar parsing has been investigated intensively. Previous studies (Abney, 1997; Johnson et al., 1999; Kaplan et al., 2004; Malouf and van Noord, 2004; Miyao and Tsujii, 2005; Riezler et al., 2000) defined a probabilistic model of unification-based grammars, including head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a dynamic programming algorithm for estimating the probabilities of all possible parse candidates. A feature fores"
E09-1069,J96-1002,0,0.0115771,"ification, which almost always succeeds in unification by overwriting inconsistent constraints in grammars. 1 Introduction Over the last few decades, probabilistic unification-based grammar parsing has been investigated intensively. Previous studies (Abney, 1997; Johnson et al., 1999; Kaplan et al., 2004; Malouf and van Noord, 2004; Miyao and Tsujii, 2005; Riezler et al., 2000) defined a probabilistic model of unification-based grammars, including head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a dynamic programming algorithm for estimating the probabilities of all possible parse candidates. A feature forest can estimate the model parameters without unpacking the parse forest, i.e., the chart and its edges. Feature forests have been used successfully for probabilistic HPSG and CCG (Clark and Curran, 2004b; Miyao and Tsujii, 2005), and its parsing is empirically known to be fast and accurate, especially with supertagging (Clark and Curran, 2004a; Ninomiya et"
E09-1069,J93-1002,0,0.164709,"e feature structure nodes are not unified but merged as a set of types. Then, all types marked as “strict” are unified into one type for each node. If this fails, the default unification also returns unification failure as its result. Finally, each node is assigned a single type, which is the result of type unification for all types marked as both “default” and “strict” if it succeeds or all types marked only as “strict” otherwise. 4 Shift-reduce parsing for unificationbased grammars Non-deterministic shift-reduce parsing for unification-based grammars has been studied by Briscoe and Carroll (Briscoe and Carroll, 1993). Their algorithm works non-deterministically with the mechanism of the packed parse forest, and hence it has the problem of locality in the packed parse forest. This section explains our shiftreduce parsing algorithms, which are based on deterministic shift-reduce CFG parsing (Sagae and Lavie, 2005) and best-first shift-reduce CFG parsing (Sagae and Lavie, 2006). Sagae’s parser selects the most probable shift/reduce actions and non-terminal symbols without assuming explicit CFG rules. Therefore, his parser can proceed deterministically without failure. However, in Binary Reduce Features [Sw(0"
E09-1069,P05-1022,0,0.0606157,"–611, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 603 segmented constituents are instantiated. This is because values in parse trees can propagate anywhere throughout the parse tree by unification. For example, values may propagate from the root node to terminal nodes, and the final form of the terminal nodes is unknown until the parser finishes constructing the whole parse tree. Consequently, the design of grammars, semantic structures, and feature functions becomes complex. To solve the problem of locality, several approaches, such as reranking (Charniak and Johnson, 2005), shift-reduce parsing (Yamada and Matsumoto, 2003), search optimization learning (Daumé and Marcu, 2005) and sampling methods (Malouf and van Noord, 2004; Nakagawa, 2007), were studied. In this paper, we investigate shift-reduce parsing approach for unification-based grammars without the mechanisms of the packed parse forest. Shift-reduce parsing for CFG and dependency parsing have recently been studied (Nivre and Scholz, 2004; Ratnaparkhi, 1997; Sagae and Lavie, 2005, 2006; Yamada and Matsumoto, 2003), through approaches based essentially on deterministic parsing. These techniques, however,"
E09-1069,C04-1041,0,0.0267427,"including head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a dynamic programming algorithm for estimating the probabilities of all possible parse candidates. A feature forest can estimate the model parameters without unpacking the parse forest, i.e., the chart and its edges. Feature forests have been used successfully for probabilistic HPSG and CCG (Clark and Curran, 2004b; Miyao and Tsujii, 2005), and its parsing is empirically known to be fast and accurate, especially with supertagging (Clark and Curran, 2004a; Ninomiya et al., 2007; Ninomiya et al., 2006). Both estimation and parsing with the packed parse forest, however, have several inherent problems deriving from the restriction of locality. First, feature functions can be defined only for local structures, which limit the parser’s performance. This is because parsers segment parse trees into constituents and factor equivalent constituents into a single constituent (edge) in a chart to avoid the same cal"
E09-1069,P04-1014,0,0.0235272,"including head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a dynamic programming algorithm for estimating the probabilities of all possible parse candidates. A feature forest can estimate the model parameters without unpacking the parse forest, i.e., the chart and its edges. Feature forests have been used successfully for probabilistic HPSG and CCG (Clark and Curran, 2004b; Miyao and Tsujii, 2005), and its parsing is empirically known to be fast and accurate, especially with supertagging (Clark and Curran, 2004a; Ninomiya et al., 2007; Ninomiya et al., 2006). Both estimation and parsing with the packed parse forest, however, have several inherent problems deriving from the restriction of locality. First, feature functions can be defined only for local structures, which limit the parser’s performance. This is because parsers segment parse trees into constituents and factor equivalent constituents into a single constituent (edge) in a chart to avoid the same cal"
E09-1069,P02-1036,0,0.0201586,"n unification by overwriting inconsistent constraints in grammars. 1 Introduction Over the last few decades, probabilistic unification-based grammar parsing has been investigated intensively. Previous studies (Abney, 1997; Johnson et al., 1999; Kaplan et al., 2004; Malouf and van Noord, 2004; Miyao and Tsujii, 2005; Riezler et al., 2000) defined a probabilistic model of unification-based grammars, including head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a dynamic programming algorithm for estimating the probabilities of all possible parse candidates. A feature forest can estimate the model parameters without unpacking the parse forest, i.e., the chart and its edges. Feature forests have been used successfully for probabilistic HPSG and CCG (Clark and Curran, 2004b; Miyao and Tsujii, 2005), and its parsing is empirically known to be fast and accurate, especially with supertagging (Clark and Curran, 2004a; Ninomiya et al., 2007; Ninomiya et al., 2006). Both estim"
E09-1069,P03-1046,0,0.0772058,"Missing"
E09-1069,P99-1069,0,0.0417252,"stic shift-reduce parsing for unification-based grammars, and (ii) best-first shift-reduce parsing with beam thresholding for unification-based grammars. Deterministic parsing cannot simply be applied to unification-based grammar parsing, which often fails because of its hard constraints. Therefore, it is developed by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in grammars. 1 Introduction Over the last few decades, probabilistic unification-based grammar parsing has been investigated intensively. Previous studies (Abney, 1997; Johnson et al., 1999; Kaplan et al., 2004; Malouf and van Noord, 2004; Miyao and Tsujii, 2005; Riezler et al., 2000) defined a probabilistic model of unification-based grammars, including head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a dynamic programming algorithm for estimating the probabilities of all possible parse candidates. A feature forest can estimate the mod"
E09-1069,N04-1013,0,0.0459961,"ing for unification-based grammars, and (ii) best-first shift-reduce parsing with beam thresholding for unification-based grammars. Deterministic parsing cannot simply be applied to unification-based grammar parsing, which often fails because of its hard constraints. Therefore, it is developed by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in grammars. 1 Introduction Over the last few decades, probabilistic unification-based grammar parsing has been investigated intensively. Previous studies (Abney, 1997; Johnson et al., 1999; Kaplan et al., 2004; Malouf and van Noord, 2004; Miyao and Tsujii, 2005; Riezler et al., 2000) defined a probabilistic model of unification-based grammars, including head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a dynamic programming algorithm for estimating the probabilities of all possible parse candidates. A feature forest can estimate the model parameters without"
E09-1069,P05-1011,0,0.788301,"-first shift-reduce parsing with beam thresholding for unification-based grammars. Deterministic parsing cannot simply be applied to unification-based grammar parsing, which often fails because of its hard constraints. Therefore, it is developed by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in grammars. 1 Introduction Over the last few decades, probabilistic unification-based grammar parsing has been investigated intensively. Previous studies (Abney, 1997; Johnson et al., 1999; Kaplan et al., 2004; Malouf and van Noord, 2004; Miyao and Tsujii, 2005; Riezler et al., 2000) defined a probabilistic model of unification-based grammars, including head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a dynamic programming algorithm for estimating the probabilities of all possible parse candidates. A feature forest can estimate the model parameters without unpacking the parse forest, i.e., the chart and its"
E09-1069,D07-1100,0,0.0167214,"propagate anywhere throughout the parse tree by unification. For example, values may propagate from the root node to terminal nodes, and the final form of the terminal nodes is unknown until the parser finishes constructing the whole parse tree. Consequently, the design of grammars, semantic structures, and feature functions becomes complex. To solve the problem of locality, several approaches, such as reranking (Charniak and Johnson, 2005), shift-reduce parsing (Yamada and Matsumoto, 2003), search optimization learning (Daumé and Marcu, 2005) and sampling methods (Malouf and van Noord, 2004; Nakagawa, 2007), were studied. In this paper, we investigate shift-reduce parsing approach for unification-based grammars without the mechanisms of the packed parse forest. Shift-reduce parsing for CFG and dependency parsing have recently been studied (Nivre and Scholz, 2004; Ratnaparkhi, 1997; Sagae and Lavie, 2005, 2006; Yamada and Matsumoto, 2003), through approaches based essentially on deterministic parsing. These techniques, however, cannot simply be applied to unification-based grammar parsing because it can fail as a result of its hard constraints in the grammar. Therefore, in this study, we propose"
E09-1069,W07-2208,1,0.834096,"t al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a dynamic programming algorithm for estimating the probabilities of all possible parse candidates. A feature forest can estimate the model parameters without unpacking the parse forest, i.e., the chart and its edges. Feature forests have been used successfully for probabilistic HPSG and CCG (Clark and Curran, 2004b; Miyao and Tsujii, 2005), and its parsing is empirically known to be fast and accurate, especially with supertagging (Clark and Curran, 2004a; Ninomiya et al., 2007; Ninomiya et al., 2006). Both estimation and parsing with the packed parse forest, however, have several inherent problems deriving from the restriction of locality. First, feature functions can be defined only for local structures, which limit the parser’s performance. This is because parsers segment parse trees into constituents and factor equivalent constituents into a single constituent (edge) in a chart to avoid the same calculation. This also means that the semantic structures must be segmented. This is a crucial problem when we think of designing semantic structures other than predicat"
E09-1069,W06-1619,1,0.806997,"Missing"
E09-1069,C02-1100,1,0.829183,"the phrasestructure rules and the lexical entries are represented by feature structures (Carpenter, 1992), and constraints in the grammar are forced by unification. Among the phrase-structure rules, a binary rule is a partial function: ℱ × ℱ → ℱ, 3 Default unification Default unification was originally investigated in a series of studies of lexical semantics, in order to deal with default inheritance in a lexicon. It is also desirable, however, for robust processing, because (i) it almost always succeeds and (ii) a feature structure is relaxed such that the amount of information is maximized (Ninomiya et al., 2002). In our experiments, we tested a simplified version of Copestake’s default unification. Before explaining it, we first explain Carpenter’s 604 two definitions of default unification (Carpenter, 1993). procedure forced_unification(p, q) queue := {〈p, q〉}; while( queue is not empty ) 〈p, q〉 := shift(queue); p := deref(p); q := deref(q); if p ≠ q θ(p) ≔ θ(p) ∪ θ(q); θ(q) ≔ ptr(p); forall f ∈ feat(p)⋃ feat(q) if f ∈ feat(p) ∧ f ∈ feat(q) queue := queue ∪ 〈δ(f, p), δ(f, q)〉; if f ∉ feat(p) ∧ f ∈ feat(q) δ(f, p) ≔ δ(f, q); procedure mark(p, m) p := deref(p); if p has not been visited θ(p) := {〈θ(p)"
E09-1069,C04-1010,0,0.290997,"ently, the design of grammars, semantic structures, and feature functions becomes complex. To solve the problem of locality, several approaches, such as reranking (Charniak and Johnson, 2005), shift-reduce parsing (Yamada and Matsumoto, 2003), search optimization learning (Daumé and Marcu, 2005) and sampling methods (Malouf and van Noord, 2004; Nakagawa, 2007), were studied. In this paper, we investigate shift-reduce parsing approach for unification-based grammars without the mechanisms of the packed parse forest. Shift-reduce parsing for CFG and dependency parsing have recently been studied (Nivre and Scholz, 2004; Ratnaparkhi, 1997; Sagae and Lavie, 2005, 2006; Yamada and Matsumoto, 2003), through approaches based essentially on deterministic parsing. These techniques, however, cannot simply be applied to unification-based grammar parsing because it can fail as a result of its hard constraints in the grammar. Therefore, in this study, we propose deterministic parsing for unification-based grammars by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in the grammars. We further pursue best-first shift-reduce parsing for unificationbased gramm"
E09-1069,W97-0301,0,0.0427544,"mmars, semantic structures, and feature functions becomes complex. To solve the problem of locality, several approaches, such as reranking (Charniak and Johnson, 2005), shift-reduce parsing (Yamada and Matsumoto, 2003), search optimization learning (Daumé and Marcu, 2005) and sampling methods (Malouf and van Noord, 2004; Nakagawa, 2007), were studied. In this paper, we investigate shift-reduce parsing approach for unification-based grammars without the mechanisms of the packed parse forest. Shift-reduce parsing for CFG and dependency parsing have recently been studied (Nivre and Scholz, 2004; Ratnaparkhi, 1997; Sagae and Lavie, 2005, 2006; Yamada and Matsumoto, 2003), through approaches based essentially on deterministic parsing. These techniques, however, cannot simply be applied to unification-based grammar parsing because it can fail as a result of its hard constraints in the grammar. Therefore, in this study, we propose deterministic parsing for unification-based grammars by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in the grammars. We further pursue best-first shift-reduce parsing for unificationbased grammars. Sections 2 and"
E09-1069,P00-1061,0,0.217224,"ing with beam thresholding for unification-based grammars. Deterministic parsing cannot simply be applied to unification-based grammar parsing, which often fails because of its hard constraints. Therefore, it is developed by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in grammars. 1 Introduction Over the last few decades, probabilistic unification-based grammar parsing has been investigated intensively. Previous studies (Abney, 1997; Johnson et al., 1999; Kaplan et al., 2004; Malouf and van Noord, 2004; Miyao and Tsujii, 2005; Riezler et al., 2000) defined a probabilistic model of unification-based grammars, including head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model (Berger et al., 1996). Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a feature forest, which is a dynamic programming algorithm for estimating the probabilities of all possible parse candidates. A feature forest can estimate the model parameters without unpacking the parse forest, i.e., the chart and its edges. Feature forests"
E09-1069,W05-1513,0,0.758564,"uctures, and feature functions becomes complex. To solve the problem of locality, several approaches, such as reranking (Charniak and Johnson, 2005), shift-reduce parsing (Yamada and Matsumoto, 2003), search optimization learning (Daumé and Marcu, 2005) and sampling methods (Malouf and van Noord, 2004; Nakagawa, 2007), were studied. In this paper, we investigate shift-reduce parsing approach for unification-based grammars without the mechanisms of the packed parse forest. Shift-reduce parsing for CFG and dependency parsing have recently been studied (Nivre and Scholz, 2004; Ratnaparkhi, 1997; Sagae and Lavie, 2005, 2006; Yamada and Matsumoto, 2003), through approaches based essentially on deterministic parsing. These techniques, however, cannot simply be applied to unification-based grammar parsing because it can fail as a result of its hard constraints in the grammar. Therefore, in this study, we propose deterministic parsing for unification-based grammars by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in the grammars. We further pursue best-first shift-reduce parsing for unificationbased grammars. Sections 2 and 3 explain unification-"
E09-1069,P06-2089,0,0.0575817,"ct” if it succeeds or all types marked only as “strict” otherwise. 4 Shift-reduce parsing for unificationbased grammars Non-deterministic shift-reduce parsing for unification-based grammars has been studied by Briscoe and Carroll (Briscoe and Carroll, 1993). Their algorithm works non-deterministically with the mechanism of the packed parse forest, and hence it has the problem of locality in the packed parse forest. This section explains our shiftreduce parsing algorithms, which are based on deterministic shift-reduce CFG parsing (Sagae and Lavie, 2005) and best-first shift-reduce CFG parsing (Sagae and Lavie, 2006). Sagae’s parser selects the most probable shift/reduce actions and non-terminal symbols without assuming explicit CFG rules. Therefore, his parser can proceed deterministically without failure. However, in Binary Reduce Features [Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)] [Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)] [Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)] [Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2] [pi-1] [pi] [pi+1] [pi+2] [pi+3] [d,c,hw,hp,hl] [d,c,hw,hp] [d, c, hw, hl] [d, c, sy, hw] [c, sp, hw, hp, hl] [c, sp, hw, hp] [c, sp, hw,hl] [c, s"
E09-1069,P07-1079,0,0.0302476,"Missing"
E09-1069,W03-3023,0,0.213876,"09 Association for Computational Linguistics 603 segmented constituents are instantiated. This is because values in parse trees can propagate anywhere throughout the parse tree by unification. For example, values may propagate from the root node to terminal nodes, and the final form of the terminal nodes is unknown until the parser finishes constructing the whole parse tree. Consequently, the design of grammars, semantic structures, and feature functions becomes complex. To solve the problem of locality, several approaches, such as reranking (Charniak and Johnson, 2005), shift-reduce parsing (Yamada and Matsumoto, 2003), search optimization learning (Daumé and Marcu, 2005) and sampling methods (Malouf and van Noord, 2004; Nakagawa, 2007), were studied. In this paper, we investigate shift-reduce parsing approach for unification-based grammars without the mechanisms of the packed parse forest. Shift-reduce parsing for CFG and dependency parsing have recently been studied (Nivre and Scholz, 2004; Ratnaparkhi, 1997; Sagae and Lavie, 2005, 2006; Yamada and Matsumoto, 2003), through approaches based essentially on deterministic parsing. These techniques, however, cannot simply be applied to unification-based gramm"
H05-2010,W05-0203,1,0.781715,"oshi Nakagawa Ayako Hoshino Information Technology Center Interfaculty Initiative in Information Studies University of Tokyo University of Tokyo 7-3-1 Hongo, Bunkyo, Tokyo, 7-3-1 Hongo, Bunkyo, Tokyo, 113-0033, JAPAN 113-0033, JAPAN hoshino,nakagawa@dl.itc.u-tokyo.ac.jp 1 Aim Automatic generation of multiple-choice questions is an emerging topic in application of natural language processing. Particularly, applying it to language testing has been proved to be useful (Sumita et al., 2005). This demo presents an novel approach of question generation using machine learning we have introduced in (Hoshino and Nakagawa, 2005). Our study aims to generate TOEIC-like 1 multiple choice, fillin-the-blank questions from given text using a classifier trained on a set of human-made questions. The system comprises of a question pool, which is a database of questions, an instance converter which does feature extraction, etc. for machine learning and a question generator. Each step of learning and generation is conducted through a web-browser. parameters, to demonstrate our method of question generation by showing the result of each steps, and to collect the data (training data and the students’ answers) from multiple users"
H05-2010,W03-0203,0,0.0488156,"Missing"
H05-2010,W05-0210,0,0.0316139,"Missing"
I05-1024,C94-2167,0,0.255109,"arison with other methods. 1 Introduction Term extraction, which is the task of extracting terminology (or technical terms) from a set of documents, is one of major topics in natural language processing. It has a wide variety of applications including book indexing, dictionary generation, and keyword extraction for information retrieval systems. Most automatic term extraction systems make a sorted list of candidate terms extracted from a given corpus according to the “importance” scores of the terms, so they require scores of “importance” for the terms. Existing scores include TF-IDF, C-Value [1], and FLR [9]. In this paper, we propose a new method that involves revising the deﬁnition of the FLR method in a more sophisticated way. One of the advantages of the FLR method is its size-robustness, i.e, it can be applied to small corpus with less signiﬁcant drop in performance than other standard methods like TF and IDF, because it is deﬁned using more ﬁne-grained features called term units. Our new method, called FPP, inherit this property while exhibiting better performance than FLR. At the same time, we also propose a new scheme for evaluating term extraction systems. Our idea is to use"
I05-1024,C00-1004,0,0.0450964,"Missing"
kiyota-nakagawa-2006-domain,J94-4001,0,\N,Missing
kiyota-nakagawa-2006-domain,H01-1043,0,\N,Missing
kiyota-nakagawa-2006-domain,P90-1034,0,\N,Missing
nakagawa-etal-2004-terminal,W02-1411,0,\N,Missing
nakagawa-etal-2004-terminal,W03-1602,0,\N,Missing
nakagawa-etal-2004-terminal,P01-1008,0,\N,Missing
P03-2016,W02-1407,1,\N,Missing
P03-2016,C96-1009,0,\N,Missing
P03-2016,W01-1413,0,\N,Missing
P03-2016,W02-1030,0,\N,Missing
P05-3031,P96-1024,0,0.0426841,"They are collected by retrieving all user pages on one server of a Japanese ISP. 6 from 1,000 to 10,000 bytes 7 Src tags indicate inclusion of image files, java codes, etc Algorithm OUR ALGORITHM NO-CL NO-EM PREV Recall 0.477 0.178 0.389 0.144 Precision 0.266 0.119 0.211 0.615 F-measure 0.329 0.139 0.265 0.202 5 Conclusions and Future Work Table 1: Macro-Averaged Recall, Precision, and Fmeasure on Test Documents to evaluate machine-made bracketings. The performance of web-page structuring algorithms can be evaluated via the nested-list form of tree by bracketed recall and bracketed precision (Goodman, 1996). Recall is the rate that bracketing given by hand are also given by machine, and precision is the rate that bracketing given by machine are also given by hand. F-measure is a harmonic mean of recall and precision that is used as a combined measure. Recall and precision were evaluated for each test document and they were averaged across all test documents. These averaged values are called macro-average recall, precision, and f-measure (Yang, 1999). We implemented our algorithm and the following three ones as baselines. NO-CL does not perform block clustering. NO-EM does not perform the EM-para"
P12-1076,P08-1004,0,0.0168136,"Missing"
P12-1076,D11-1142,0,0.0549241,"Missing"
P12-1076,P10-1030,0,0.0107919,"tion performance than the original DS (Mintz et al., 2009) and MultiR (Hoffmann et al., 2011), which is a state-of-the-art multiinstance learning system for relation extraction (see Section 7). 2 Related Work The increasingly popular approach, called distant supervision (DS), or weak supervision, utilizes a knowledge base to heuristically label a corpus (Wu and Weld, 2007; Bellare and McCallum, 2007; Pal 722 et al., 2007). Our work was inspired by Mintz et al. (2009) who used Freebase as a knowledge base by making the DS assumption and trained relation extractors on Wikipedia. Previous works (Hoffmann et al., 2010; Yao et al., 2010) have pointed out that the DS assumption generates noisy labeled data, but did not directly address the problem. Wang et al. (2011) applied a rule-based method to the problem by using popular entity types and keywords for each relation. In (Bellare and McCallum, 2007; Riedel et al., 2010; Hoffmann et al., 2011), they used multiinstance learning, which deals with uncertainty of labels, to relax the DS assumption. However, the relaxed assumption can fail when a labeled entity pair is mentioned only once in a corpus (Riedel et al., 2010). Our approach relies on neither of these"
P12-1076,P11-1055,0,0.802283,"el et al. (2010)’s, the model predicts whether each pattern expresses each relation via hidden variables (see Section 5). • Our variational inference for our generative model lets us automatically calibrate parameters for each relation, which are sensitive to the performance (see Section 6). • We applied our method to Wikipedia articles using Freebase as a knowledge base and found that (i) our model identified patterns expressing a given relation more accurately than baseline methods and (ii) our method led to better extraction performance than the original DS (Mintz et al., 2009) and MultiR (Hoffmann et al., 2011), which is a state-of-the-art multiinstance learning system for relation extraction (see Section 7). 2 Related Work The increasingly popular approach, called distant supervision (DS), or weak supervision, utilizes a knowledge base to heuristically label a corpus (Wu and Weld, 2007; Bellare and McCallum, 2007; Pal 722 et al., 2007). Our work was inspired by Mintz et al. (2009) who used Freebase as a knowledge base by making the DS assumption and trained relation extractors on Wikipedia. Previous works (Hoffmann et al., 2010; Yao et al., 2010) have pointed out that the DS assumption generates no"
P12-1076,D08-1106,0,0.0140471,"tion are given in order to select reliable patterns, which are used to extract new entity pairs. To avoid the selection of unreliable patterns, bootstrapping introduces scoring functions for each pattern candidate. This can be applied to our approach, which seeks to reduce the number of unreliable patterns by using a set of given entity pairs. However, the bootstrappinglike approach suffers from sensitive parameters that are critical to its performance. Ideally, the parameters such as a threshold for scoring function should be determined for each relation, but there are no principled methods (Komachi et al., 2008). In our approach, parameters are calibrated for each relation by maximizing the likelihood of our generative model. 3 Knowledge-based Distant Supervision In this section, we describe DS for relation extraction. We use the term relation as the relation between two entities. A relation instance is a tuple consisting of two entities and relation r. For example, place of birth(Michael Jackson, Gary) in Figure 1 is a relation instance. Relation extraction seeks to extract relation instances from text. An entity is mentioned as a named entity in text. We extract a relation instance from a single se"
P12-1076,P09-1113,0,0.835613,"0 1 89&apos;:4;*6;)-+&lt;= Figure 1: Automatic labeling by distant supervision. Upper sentence: correct labeling; lower sentence: incorrect labeling. Introduction Machine learning approaches have been developed to address relation extraction, which is the task of extracting semantic relations between entities expressed in text. Supervised approaches are limited in scalability because labeled data is expensive to produce. A particularly attractive approach, called distant supervision (DS), creates labeled data by heuristically aligning entities in text with those in a knowledge base, such as Freebase (Mintz et al., 2009). With DS it is assumed that if a sentence contains an entity pair in a knowledge base, such a sentence actually expresses the corresponding relation in the knowledge base. However, the DS assumption can fail, which results in noisy labeled data and this causes poor extraction performance. An entity pair in a target text generally expresses more than one relation while a knowledge base stores a subset of the relations. The assumption ignores this possibility. For instance, consider the place of birth relation between Michael Jackson and Gary in Figure 1. The upper sentence indeed expresses the"
P12-1076,P06-1015,0,0.00712574,"abeled data, but did not directly address the problem. Wang et al. (2011) applied a rule-based method to the problem by using popular entity types and keywords for each relation. In (Bellare and McCallum, 2007; Riedel et al., 2010; Hoffmann et al., 2011), they used multiinstance learning, which deals with uncertainty of labels, to relax the DS assumption. However, the relaxed assumption can fail when a labeled entity pair is mentioned only once in a corpus (Riedel et al., 2010). Our approach relies on neither of these assumptions. Bootstrapping for relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006; Carlson et al., 2010) is related to our method. In bootstrapping, seed entity pairs of the target relation are given in order to select reliable patterns, which are used to extract new entity pairs. To avoid the selection of unreliable patterns, bootstrapping introduces scoring functions for each pattern candidate. This can be applied to our approach, which seeks to reduce the number of unreliable patterns by using a set of given entity pairs. However, the bootstrappinglike approach suffers from sensitive parameters that are critical to its performance. Ideally, the parameters such as a thre"
P12-1076,D11-1132,0,0.0517063,"for relation extraction (see Section 7). 2 Related Work The increasingly popular approach, called distant supervision (DS), or weak supervision, utilizes a knowledge base to heuristically label a corpus (Wu and Weld, 2007; Bellare and McCallum, 2007; Pal 722 et al., 2007). Our work was inspired by Mintz et al. (2009) who used Freebase as a knowledge base by making the DS assumption and trained relation extractors on Wikipedia. Previous works (Hoffmann et al., 2010; Yao et al., 2010) have pointed out that the DS assumption generates noisy labeled data, but did not directly address the problem. Wang et al. (2011) applied a rule-based method to the problem by using popular entity types and keywords for each relation. In (Bellare and McCallum, 2007; Riedel et al., 2010; Hoffmann et al., 2011), they used multiinstance learning, which deals with uncertainty of labels, to relax the DS assumption. However, the relaxed assumption can fail when a labeled entity pair is mentioned only once in a corpus (Riedel et al., 2010). Our approach relies on neither of these assumptions. Bootstrapping for relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006; Carlson et al., 2010) is related to our m"
P12-1076,P09-1115,0,0.00690762,"assifier with a labeled corpus cleaned by Algorithm 1 using the negative pattern list predicted by the generative model. 7.1 Dataset Following Mintz et al. (2009), we carried out our experiments using Wikipedia as the target corpus and Freebase (September, 2009, (Google, 2009)) as the knowledge base. We used more than 1,300,000 Wikipedia articles in the wex dump data (September, 2009, (Metaweb Technologies, 2009)). The properties of our data are shown in Table 1. In Wikipedia articles, named entities were identified by anchor text linking to another article and starting with a capital letter (Yan et al., 2009). We applied Open NLP POS tagger5 and MaltParser (Nivre et al., 2007) to sentences containing more 4 The first-order information becomes zero in this case. 5 http://opennlp.sourceforge.net/ 726 Table 1: Properties of Wikipedia dataset documents entity pairs (matched to Freebase) (with entity types) frequent patterns relations 1,303,000 2,017,000 129,000 913,000 3,084 24 than one named entity. We then extracted sentences containing related entity pairs with the method explained in Section 3. To match entity pairs, we used ID mapping between the dump data and Freebase. We used the most frequent"
P12-1076,D10-1099,0,0.0152214,"he original DS (Mintz et al., 2009) and MultiR (Hoffmann et al., 2011), which is a state-of-the-art multiinstance learning system for relation extraction (see Section 7). 2 Related Work The increasingly popular approach, called distant supervision (DS), or weak supervision, utilizes a knowledge base to heuristically label a corpus (Wu and Weld, 2007; Bellare and McCallum, 2007; Pal 722 et al., 2007). Our work was inspired by Mintz et al. (2009) who used Freebase as a knowledge base by making the DS assumption and trained relation extractors on Wikipedia. Previous works (Hoffmann et al., 2010; Yao et al., 2010) have pointed out that the DS assumption generates noisy labeled data, but did not directly address the problem. Wang et al. (2011) applied a rule-based method to the problem by using popular entity types and keywords for each relation. In (Bellare and McCallum, 2007; Riedel et al., 2010; Hoffmann et al., 2011), they used multiinstance learning, which deals with uncertainty of labels, to relax the DS assumption. However, the relaxed assumption can fail when a labeled entity pair is mentioned only once in a corpus (Riedel et al., 2010). Our approach relies on neither of these assumptions. Boots"
P98-2153,A94-1029,0,0.0240314,"of several separated volumes. In a case of such a large manual, the same topic appears at several places in different volumes. One of them is an introductory explanation for a novice. Another is a precise explanation for an advanced user. It is very useful to jump from one of them to another of them directly by just clicking a button of mouse in reading a manual text on a browser like NetScape. This type of access is realized by linking them in hypertext format by hypertext authoring. Automatic hypertext authoring has been focused on in these years, and much work has been done. For instance, Basili et al. (1994) use document structures and semantic information by means of natural language processing technique to set hyperlinks on plain texts. The essential point in the research of automatic hypertext authoring is the way to find semantically relevant parts where each part is characterized by a number of key words. Actually it is very similar 929 with information retrieval, IR henceforth, especially with the so called passage retrieval (Salton et al., 1993). J.Green (1996) does hypertext authoring of newspaper articles by word&apos;s lexical chains which are calculated using WordNet. Kurohashi et al. (1992"
P98-2153,A92-1037,0,0.0227765,", Basili et al. (1994) use document structures and semantic information by means of natural language processing technique to set hyperlinks on plain texts. The essential point in the research of automatic hypertext authoring is the way to find semantically relevant parts where each part is characterized by a number of key words. Actually it is very similar 929 with information retrieval, IR henceforth, especially with the so called passage retrieval (Salton et al., 1993). J.Green (1996) does hypertext authoring of newspaper articles by word&apos;s lexical chains which are calculated using WordNet. Kurohashi et al. (1992) made a hypertext dictionary of the field of information science. They use linguistic patterns that are used for definition of terminology as well as thesaurus based on words&apos; similarity. Furner-Hines and Willett (1994) experimentally evaluate and compare the performance of several human hyper linkers. In general, however, we have not yet paid enough attention to a full-automatic hyper linker system, that is what we pursue in this paper. The new ideas in our system are the following points: 1. Our target is a multi-volume manual that describes the same hardware or software but is different in"
W02-1407,C96-1009,0,0.347817,"Background 1.1 Candidates Extraction The first thing to do in ATR is to extract term candidates from the given text corpus. Here we only focus on nouns, more precisely a single-noun and a compound noun, which are exactly the targets of the NTCIR1 TMREC task(Kageura et al 1999). To extract compound nouns which are promising term candidates and at the same time to exclude undesirable strings such as “is a” or “of the”, the frequently used method is to filter out the words that are members of a stop-word-list. More complex structures like noun phrases, collocations and so on, become focused on (Frantzi and Ananiadou 1996). All of these are good term candidates in a corpus of a specific domain because all of them have a strong unithood (Kageura&Umino96) which refers to the degree of strength or stability of syntagmatic combinations or collocations. We assume the following about compound nouns or collocations: Terms having complex structure are to be made of existing simple terms Assumption The structure of complex terms is another important factor for automatic term candidates extraction. It is expressed syntactically or semantically. As a syntactic structure, dependency structures that are the results of parsi"
W02-1407,C00-1058,0,\N,Missing
W02-1407,P90-1032,0,\N,Missing
W02-1407,C00-1047,0,\N,Missing
W04-1112,C94-2167,0,0.0853932,"Missing"
W04-1112,W03-1711,0,0.0339018,"Missing"
W04-1112,W03-1704,0,0.104151,"Missing"
W04-1112,C96-1009,0,\N,Missing
W04-1112,P90-1032,0,\N,Missing
W04-1112,C00-1047,0,\N,Missing
W04-1112,W03-1730,0,\N,Missing
W04-1112,W03-1713,0,\N,Missing
W05-0203,W03-0203,0,0.26229,"Missing"
W10-3020,W02-1001,0,0.443412,"(x, yj0 ) Pn−1 0 )). + j=1 w> φ(x, yj0 → yj+1 = argmaxy0 = argmaxy0 Optimization We now turn to the optimization of the weight parameter w. We compare three approaches – Perceptron, Bayes Point Machines and Conditional Random Fields, using our c++ library for structured output prediction 1 . Perceptron is an online update scheme that leaves the weights unchanged when the predicted output matches the target, and changes them when it does not. The update is: wk := wk − φ(xi , y) + φ(xi , yi ). Despite its seemingly simple update scheme, perceptron is known for its effectiveness and performance (Collins, 2002). Conditional Random Fields (CRF) is a conditional model P (y|x) = 1 exp(w> φ(x, y)) Zx where w is the weight for each feature and Zx is a normalization constant for each x. Zx = X exp(w> φ(x, y)) y 1 139 Available at http://soplib.sourceforge.net/ for structured output prediction. To fit the weight vector w using the training set {(xi , yi )}ni=1 , we use a standard gradient-descent method to find the weight vector that maximizes the log likelihood Pn i i i logP (y |x ) (Sha and Pereira, 2003). To avoid overfitting, the log likelihood is often penalized with a spherical Gaussian weight prior:"
W10-3020,N06-1021,0,0.0133049,"ach x. Zx = X exp(w> φ(x, y)) y 1 139 Available at http://soplib.sourceforge.net/ for structured output prediction. To fit the weight vector w using the training set {(xi , yi )}ni=1 , we use a standard gradient-descent method to find the weight vector that maximizes the log likelihood Pn i i i logP (y |x ) (Sha and Pereira, 2003). To avoid overfitting, the log likelihood is often penalized with a spherical Gaussian weight prior: Pn C||w|| i i i logP (y |x ) − 2 . We also evaluated this penalized version, varying the trade-off parameter C. Bayes Point Machines (BPM) for structured prediction (Corston-Oliver et al., 2006) is an ensemble learning algorithm that attempts to set the weight w to be the Bayes Point which approximates to Bayesian inference for linear classifiers. Assuming a uniform prior distribution over w, we revise our belief of w after observing the training data and produce a posterior distribution. We create the final wbpm for classification using a posterior distribution as follows: wbpm = Ep(w|D) [w] = |V (D)| X i=1 Algorithm 3.1: BPM(K, T, {(xi , yi )}ni=1 ) wbpm := 0; for k := 1 to K Randomly shuffle the sequential order of samples {(xi , yi )}ni=1 wk := 0; for t := 1 to T # Perceptron ite"
W10-3020,W10-3001,0,0.0504101,"Missing"
W10-3020,I08-1072,0,0.0255722,"ter, 0 otherwise. Base Features For each sentence x, we have state features, represented by a binary vector φ(x, yj0 ) and transition 0 features, again a binary vector φ(x, yj0 → yj+1 ). For transition features, we do not utilize lexicalized features. Thus, each dimension of φ(x, yj0 → F 5 1 if the token contains a hyphen, 0 otherwise. 2 Available at: http:// www-tsujii.is.s.u-tokyo.ac.jp/ GENIA/ tagger/ 3 A tokenizer is available at: http:// www.cis.upenn.edu/ treebank/ tokenization.html 140 F 6 first letter in the token. This metric is known to perform quite well for near-synonym discovery (Hagiwara et al., 2008). Given a stem and its similarities to different hedge cues, we took the maximum similarity and discretized it. F 7 first two letters in the token. F 8 first three letters in the token. F 9 last letter in the token. F 16 1 if similarity is bigger than 0.9, 0 otherwise. F 10 last two letters in the token. ... F 11 last three letters in the token. F 19 1 if similarity is bigger than 0.6, 0 otherwise. The features F 0 to F 11 are known to be useful for POS tagging. We postulated that since most frequent hedge cues tend not to be nouns, these features might help identify them. The following three"
W10-3020,W09-1304,0,0.0720009,"Missing"
W10-3020,N03-1028,0,0.21713,"Missing"
W13-5007,C92-2082,0,0.0289569,"ur as realistic data. . 2 Related Work Kozareva and Hovy (2010) recently shed light on the problem of improving the seed set for bootstrapping. They defined several goodness of seeds and proposed a method to predict these measures using 45 support vector regression (SVR) for their doubly anchored pattern (DAP) system. However, Kozareva and Hovy (2010) does not show how effective the seed set selected by the goodness of seeds that they defined was for the bootstrapping process while they show how accurately they could predict the goodness of seeds. Early work on bootstrapping includes that of (Hearst, 1992) and that of (Yarowsky, 1995). Abney (2004) extended self-training algorithms including that of (Yarowsky, 1995), forming a theory different from that of (Komachi et al., 2008). We chose to extend the theory of (Komachi et al., 2008) because it can actually explain recent graph-based algorithms including that of (Pantel and Pennacchiotti, 2006). The theory of Komachi et al. (2008) is also newer and simpler than that of (Abney, 2004). The iterative seeding framework can be regarded as an example of active learning on graph-based semi-supervised learning. Selecting seed sets corresponds to sampl"
W13-5007,D08-1106,0,0.0605666,"ods or random seed sets. 1 • criteria that support iterative updates of goodness of seeds for seed candidate unlabeled instances. • iterative update of similarity “score” to the seeds. Introduction Bootstrapping has recently drawn a great deal of attention in natural language processing (NLP) research. We define bootstrapping as a method for harvesting “instances” similar to given “seeds” by recursively harvesting “instances” and “patterns” by turns over corpora using the distributional hypothesis (Harris, 1954). This definition follows the definitions of bootstrapping in existing NLP papers (Komachi et al., 2008; Talukdar and Pereira, 2010; Kozareva et al., 2011). Bootstrapping can greatly To invent a “criterion” that captures the characteristics of a dataset, we need to measure the influence of the unlabeled instances to the model. This model, however, is not explicit in usual bootstrapping algorithms’ notations. Thus, we need to reveal the model parameters of bootstrapping algorithms for explicit model notations. To this end, we first reduced bootstrapping algorithms to label propagation using Komachi et al. 44 Proceedings of the TextGraphs-8 Workshop, pages 44–52, c Seattle, Washington, USA, 18 Oc"
W13-5007,N10-1087,0,0.52694,"nformation Science and Technology ‡ Information Technology Center The University of Tokyo / 7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan ∗ JSPS Research Fellow Kojimachi Business Center Building, 5-3-1 Kojimachi, Chiyoda-ku, Tokyo, Japan {ehara@r., sato@r., oiwa@r., nakagawa@}dl.itc.u-tokyo.ac.jp Abstract reduce the cost of labeling instances, which is especially needed for tasks with high labeling costs. The performance of bootstrapping algorithms, however, depends on the selection of seeds. Although various bootstrapping algorithms have been proposed, randomly chosen seeds are usually used instead. Kozareva and Hovy (2010) recently reports that the performance of bootstrapping algorithms depends on the selection of seeds, which sheds light on the importance of selecting a good seed set. Especially a method to select a seed set considering the characteristics of the dataset remains largely unaddressed. To this end, we propose an “iterative seeding” framework, where the algorithm iteratively ranks the goodness of seeds in response to current human labeling and the characteristics of the dataset. For iterative seeding, we added the following two properties to the bootstrapping; Bootstrapping has recently become th"
W13-5007,D11-1011,0,0.0136695,"iterative updates of goodness of seeds for seed candidate unlabeled instances. • iterative update of similarity “score” to the seeds. Introduction Bootstrapping has recently drawn a great deal of attention in natural language processing (NLP) research. We define bootstrapping as a method for harvesting “instances” similar to given “seeds” by recursively harvesting “instances” and “patterns” by turns over corpora using the distributional hypothesis (Harris, 1954). This definition follows the definitions of bootstrapping in existing NLP papers (Komachi et al., 2008; Talukdar and Pereira, 2010; Kozareva et al., 2011). Bootstrapping can greatly To invent a “criterion” that captures the characteristics of a dataset, we need to measure the influence of the unlabeled instances to the model. This model, however, is not explicit in usual bootstrapping algorithms’ notations. Thus, we need to reveal the model parameters of bootstrapping algorithms for explicit model notations. To this end, we first reduced bootstrapping algorithms to label propagation using Komachi et al. 44 Proceedings of the TextGraphs-8 Workshop, pages 44–52, c Seattle, Washington, USA, 18 October 2013. 2013 Association for Computational Lingu"
W13-5007,P06-1015,0,0.815094,"2013 Association for Computational Linguistics (2008)’s theorization. Komachi et al. (2008) shows that simple bootstrapping algorithms can be interpreted as label propagation on graphs (Komachi et al., 2008). This accords with the fact that many papers such as (Talukdar and Pereira, 2010; Kozareva et al., 2011) suggest that graph-based semi-supervised learning, or label propagation, is another effective method for this harvesting task. Their theorization starts from a simple bootstrapping scheme that can model many bootstrapping algorithms so far proposed, including the “Espresso” algorithm (Pantel and Pennacchiotti, 2006), which was the most cited among the Association for Computational Linguistics (ACL) 2006 papers. After reducing bootstrapping algorithms to label propagation, next, we will reveal the model parameters of a bootstrapping algorithm by taking the dual problem of bootstrapping formalization of (Komachi et al., 2008). By revealing the model parameters, we can obtain an interpretation of selecting seeds which helps us to create criteria for the iterative seeding framework. Namely, we propose expected model rotation (EMR) criterion that works well on realistic, and not well-separated data. The contr"
W13-5007,P10-1149,0,0.0464848,"Missing"
W13-5007,P95-1026,0,0.128157,"elated Work Kozareva and Hovy (2010) recently shed light on the problem of improving the seed set for bootstrapping. They defined several goodness of seeds and proposed a method to predict these measures using 45 support vector regression (SVR) for their doubly anchored pattern (DAP) system. However, Kozareva and Hovy (2010) does not show how effective the seed set selected by the goodness of seeds that they defined was for the bootstrapping process while they show how accurately they could predict the goodness of seeds. Early work on bootstrapping includes that of (Hearst, 1992) and that of (Yarowsky, 1995). Abney (2004) extended self-training algorithms including that of (Yarowsky, 1995), forming a theory different from that of (Komachi et al., 2008). We chose to extend the theory of (Komachi et al., 2008) because it can actually explain recent graph-based algorithms including that of (Pantel and Pennacchiotti, 2006). The theory of Komachi et al. (2008) is also newer and simpler than that of (Abney, 2004). The iterative seeding framework can be regarded as an example of active learning on graph-based semi-supervised learning. Selecting seed sets corresponds to sampling a data point in active le"
W13-5007,J04-3004,0,\N,Missing
W97-1302,C96-2132,1,0.758715,"their manuals into other languages, maintaining consistency between the description in manuals and the actual behavior of the machines. To solve these problems, we have to have a computer assisted system for processing manual sentences. In processing instruction manuals written in Japanese, however, it is problematic that almost all subjects are omitted. They are called &quot;zero subjects.&quot; For example, machine translation systems have to supply appropriate subjects to translate sentences. Therefore, we have focused on anaphora resolution of zero subjects in Japanese manual sentences. Mori et al.(Mori and Nakagawa, 1996) show that properties of Japanese conditionals can be used to resolve them. In this paper, we propose new constraints and defaults based on properties of linguistic expressions, which are useful to estimate omitted subjects in addition to the constraints and defaults proposed by Mori et al. A large number of researchers have come to grip with the method of understanding some types of text including instruction manuals(Abe et al., 1988; Nomura, 1992; Eugenio, 1992). One of the most important matters of concern in these types of system is how we can resolve ambiguities in semantic representation"
W97-1302,C96-2137,0,0.0393977,"Missing"
W97-1302,P92-1016,0,0.0705757,"Missing"
W97-1302,P87-1022,0,\N,Missing
W97-1407,J94-2003,0,\N,Missing
W97-1407,P87-1022,0,\N,Missing
Y12-1054,P10-1115,0,0.0309413,"elated Works Wang et al. (2007) studied how to detect correlated bursty topic patterns across multiple text streams such as multilingual news streams, where their method concentrated on detecting correlated bursty topic patterns based on the similarity of temporal distribution of tokens. Unlike the method of Wang et al. (2007), in this paper, we do not utilize burst detection techniques, but employ a time series topic model and cross-lingually align time series topics utilizing translation knowledge automatically extracted from Wikipedia. Boyd-Graber and Blei (2009), De Smet and Moens (2009), Zhang et al. (2010), and Jagarlamudi and Daum´e III (2010) concentrated on applying variants of topic models which have certain functions of bridging cross-lingual gaps by exploiting clues such as translation knowledge from bilingual lexicon or distribution of named entities. Compared with those previous works, the approach we take in this paper is different in that we focus on a time series topic model and align time series topics across two languages. It is one of our future works to introduce those other models and compare them with 506 our proposed framework in terms of effectiveness of aligning time series"
