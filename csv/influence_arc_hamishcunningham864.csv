A97-1035,C96-2187,1,0.703341,", morphological analysers, discourse planning modules, etc, - be readily available for experimentation and reuse. But the pressure towards theoretical diversity means that there is no point attempting to gain agreement, in the short term, on what set of component technologies should be developed or on the informational content or syntax of representations that these components should require or produce. Our response to these considerations has been to design and implement a software environment called GATE - a General Architecture for Text Engineering (Cunningham, Gaizauskas, and Wilks, 1995; Cunningham, Wilks, and Gaizauskas, 1996) - which attempts to meet the following objectives: 1. support information interchange between LE modules at the highest common level possible without prescribing theoretical approach (though it allows modules which share theoretical presuppositions to pass data in a mutually accepted common form); â€¢ maturing NLP technology which is now able, for some tasks, to achieve high levels of accuracy repeatedly on real data. Aside from the host of fundamental theoretical problems that remain to be answered in NLP, language engineering faces a variety of problems of its own. Two features of the curren"
A97-1035,C96-1082,0,0.0553257,"in SGML for processing by LT-NSL tools, and convert the SGML results back into native format. Work is underway to integrate the LT-NSL API with G A T E and provide SGML I / O for TIPS T E R (and we acknowledge valuable assistance from colleagues at Edinburgh in this task). 2.6 ICE ICE, the Intarc Communication Environment (Amtrup, 1995), is an 'environment for the development of distributed AI systems'. As part of the Verbmobil real-time speech-to-speech translation project ICE has addressed two key problems for this type of system, viz. distributed processing and incremental interpretation (Gorz et al., 1996): distribution to contribute to processing speed in what is a very compute-intensive application area; incremental interpretation both for speed reasons and to facilitate feedback of results from downstream modules to upstream ones (e.g. to inform the selection of word interpretations from phone lattices using partof-speech information). ICE provides a distribution and communication layer based on PVM (Parallel Virtual Machine). The infrastructure that ICE delivers doesn't fit into our tripartite classification because the communication channels do not use data structures specific to NLP needs"
A97-1035,C96-1079,0,0.0690796,"action-based: A fourth category might be added to cater for those systems that provide communication and control infrastructure without addressing the text-specific needs of NLP (e.g. Verbmobil's ICE architecture (Amtrup, 1995)). We begin by reviewing examples of the three approaches we sketched above (and a system that falls into the fourth category). Next we discuss current trends in the field and motivate a set of requirements that have formed the design brief for G A T E , which is then described. The initial distribution of the system includes a MUC-6 (Message Understanding Conference 6 (Grishman and Sundheim, 1996)) style information extraction (IE) system and an overview 1These texts may sometimes be the results of automatic speech recognition - see section 2.6. 237 of these modules is given. G A T E is now available for research purposes - see http ://ul;w.dcs. shef. ac. u_k/research/groups/ nlp/gate/ for details of how to obtain the system. It is written in C + + and T c l / T k and currently runs on UNIX (SunOS, Solaris, Irix, Linux and AIX are known to work); a Windows N T version is in preparation. 2 2.1 Managing Abstraction Information about Text Approaches T h e abstraction-based approach to man"
A97-1035,A97-1034,0,0.137842,"Missing"
A97-1035,J92-2002,0,0.010504,"UNIX (SunOS, Solaris, Irix, Linux and AIX are known to work); a Windows N T version is in preparation. 2 2.1 Managing Abstraction Information about Text Approaches T h e abstraction-based approach to managing information a b o u t texts is primarily motivated by theories of the nature of the information to be represented. One such position is t h a t declarative, constraint-based representations using featurestructure matrices manipulated under unification are an appropriate vehicle by which &quot;many technical problems in language description and computer manipulation of language can be solved&quot; (Shieber, 1992). Information in these models may be characterised as abstract in our present context as there is no requirement to tie d a t a elements back to the original text - these models represent abstractions from the text. One recent example of an infrastructure project based on abstraction is A L E P - the Advanced Language Engineering Platform (Simkins, 1994). A L E P aims to provide &quot;the NLP research and engineering community in Europe with an open, versatile, and general-purpose development environment&quot;. ALEP, while in principle open, is primarily an advanced system for developing and manipulatin"
A97-1035,A97-1036,0,0.074113,"al access to data (McKelvie, Brew, and Thompson, 1997). 5. T I P S T E R can easily support multi-level access control via a database's protection mechanisms - this is again not straightforward in SGML. 6. Distributed control is easy to implement in a database-centred system like T I P S T E R - the DB can act as a blackboard, and implementations can take advantage of well-understood access control (locking) technology. How to do distributed control in LT-NSL is not obvious. We plan to provide this type of control in G A T E via collaboration with the Corelli project at CRL, New Mexico - see (Zajac, 1997) for more details. 2.5 Combining Addition and Reference We believe the above comparison demonstrates that there are significant advantages to the T I P S T E R model and it is this model that we have chosen for GATE. We also believe that SGML and the T E I must remain central to any serious text processing strategy. 240 The points above do not contradict this view, but indicate that SGML should not form the central representation format of every text processing system. Input from SGML text and T E I conformant output are becoming increasingly necessary for LE applications as more and more publ"
A97-1035,M95-1017,0,\N,Missing
A97-1035,A97-2017,1,\N,Missing
A97-2017,A97-1035,1,0.70965,"s the best practical support that can be given to advance the field? Clearly, the pressure to build on the efforts of others demands that LE tools or component technologies be readily available for experimentation and reuse. But the pressure towards theoretical diversity means that there is no point attempting to gain agreement, in the short term, on what set of component technologies should be developed or on the informational content or syntax of representations that these components should require or produce. Our response has been to design and implement a software environment called GATE (Cunninham et al., 1997), which we will demonstrate at ANLP. GATE attempts to meet the following objectives: 29 ing modules are active buttons: clicking on them will, if conditions are right, cause the module to be executed. The paths through the graph indicate the dependencies amongst the various modules making up this subsystem. At any point in time, the state of execution of the system, or, more accurately, the availability of data from various modules, is depicted through colour-coding of the module boxes. After execution, the results of completed modules are available for viewing by clicking again on the module"
A97-2017,M95-1017,0,0.0521059,"escribed above. In addition, modules can be &apos;reset&apos;, i.e. their results removed from the GDM, to allow the user to pick another path through the graph, or re-execute having altered some tailorable data-resource (such as a grammar or lexicon) interpreted by the module at run-time. (Modules running as external executables might also be recompiled between runs.) To illustrate the process of converting pre-existing LE systems into GATE-compatible C R E O L E sets we use as an example the creation of VIE (Vanilla Information Extraction system) from LaSIE (LargeScale Information Extraction system) (Gaizauskas et al., 1995), Sheffield&apos;s entry in the MUC-6 system evaluations. LaSIE module interfaces were not standardised when originally produced and its CREOLEization gives a good indication of the ease of integrating other LE tools into GATE. The work took around 2 person-months. The resulting system, VIE, is distributed with GATE. developed from scratch for the architecture - in either case the object provides a standardised API to the underlying resources which allows access via GGI and I / O via GDM. Tile CREOLE APIs may also be used for programming new objects. When the user initiates a particular C R E O L E"
A97-2017,J93-2004,0,\N,Missing
A97-2017,C94-1070,0,\N,Missing
A97-2017,A97-1034,0,\N,Missing
A97-2017,A97-1036,0,\N,Missing
A97-2017,A97-1051,0,\N,Missing
A97-2017,C96-1079,0,\N,Missing
baker-etal-2002-emille,W00-1501,1,\N,Missing
baker-etal-2002-emille,mcenery-etal-2000-corpus,1,\N,Missing
C96-2187,C94-1070,0,0.025987,"s of the M U L T E X T project. G D M provides a central repository or server t h a t stores all the information an LE system generates about the texts it processes. All communication between the components of an LE system goes through GDM, insulating parts fi&apos;om each other and providing a uniform A P I (applications p r o g r a m m e r interface) for manipulating the data produced by the system. 3 Benefits of this approach include the ability to exploit the maturity and efficiency of database technology, easy modelling of blackboard-type distributed control regimes (of the type proposed by: (Boitet and Seligman, 1994) and in the section on control in (Black ed., 1991)) and reduced interdependence of components. G G I is in development at Sheffield. It is a graphical launchpad for LE subsystems, and provides various facilities for viewing and testing results and playing software lego with LE components - interactively assembling objects into different system configurations. All the real work of analysing texts (and m a y b e producing summaries of them, or translations, or SQL statements, etc.) in a GATE-based LE system is done by C R E O L E modules. Note that we use the terms module and object rather loos"
C96-2187,J93-2004,0,0.0245392,"nefficency have been extensively studied, and a number of solutions are now available (Prieto-Diaz and t~h&apos;eeman, 1987; Prieto-Diaz, 1993). Similarly, the Natural Language Engineering (NLE l) community has identified the potential benefits of reducing repetition, and work has been flmded to promote reuse. This work concerns either reusable resources which are primarily data or those which are primarily algorithmic (i.e. processing &apos;tools&apos;, or programs, or code libraries). Successflfl examples of reuse of data resources include: the WordNet thesaurus (Miller el; al., 1993); the Penn Tree Bank (Marcus et al., 1993); the Longmans Dictionary of Contemporary English (Summers, 1995). A large number of papers report results relative to these and other resources, and these successes have spawned a numOur view is that succesful algorithmic reuse in NLE will require the provision of support software for NLE in the form of a general architecture and development environment which is specifically designed for text processing systems. Under EPSRC 2 grant GR/K25267 the NLP group at, the University of Sheffield are developing a system that aims to implement this new approach. The system is called GATE - the General A"
cunningham-etal-2000-software,C92-2123,0,\N,Missing
cunningham-etal-2000-software,bird-etal-2000-atlas,0,\N,Missing
cunningham-etal-2000-software,W99-0301,0,\N,Missing
cunningham-etal-2000-software,C96-1082,0,\N,Missing
cunningham-etal-2000-software,C94-1070,0,\N,Missing
cunningham-etal-2000-software,A97-1034,0,\N,Missing
cunningham-etal-2000-software,W94-0319,0,\N,Missing
cunningham-etal-2000-software,A97-1035,1,\N,Missing
cunningham-etal-2000-software,W98-1310,0,\N,Missing
cunningham-etal-2000-software,A97-1054,0,\N,Missing
cunningham-etal-2000-software,P98-2126,0,\N,Missing
cunningham-etal-2000-software,C98-2121,0,\N,Missing
cunningham-etal-2000-software,P98-1099,0,\N,Missing
cunningham-etal-2000-software,C98-1096,0,\N,Missing
cunningham-etal-2000-software,P98-1024,0,\N,Missing
cunningham-etal-2000-software,C98-1024,0,\N,Missing
cunningham-etal-2000-software,C98-2126,0,\N,Missing
cunningham-etal-2000-software,W97-1500,0,\N,Missing
cunningham-etal-2000-software,W98-1102,0,\N,Missing
cunningham-etal-2000-software,M93-1009,0,\N,Missing
cunningham-etal-2000-software,A97-2017,1,\N,Missing
damljanovic-etal-2010-identification,damljanovic-etal-2008-text,1,\N,Missing
damljanovic-etal-2010-identification,W03-1310,0,\N,Missing
damljanovic-etal-2010-identification,P00-1071,0,\N,Missing
damljanovic-etal-2010-identification,J03-4003,0,\N,Missing
damljanovic-etal-2010-identification,J07-1006,0,\N,Missing
damljanovic-etal-2010-identification,P07-1121,0,\N,Missing
damljanovic-etal-2010-identification,P09-1110,0,\N,Missing
damljanovic-etal-2010-identification,P09-1069,0,\N,Missing
E03-2009,baker-etal-2002-emille,1,0.827736,"enabled graphical user interface (GUI) needs to address two main issues: the capability to display text and the ability to enter text in other languages than the default one. It also provides a means of entering text in a variety of languages and scripts, using virtual keyboards where the language is not supported by the underlying operating platform (Java itself does not support input in many languages covered by Unicode, although it supports Unicode representation). Figure 1 depicts text in various scripts displayed in GATE. The facilities have been developed as part of the EMILLE project (Baker et al., 2002), designed to construct a 63 million word corpus of South Asian languages. There are currently 28 languages supported in GATE, and more are planned for the future. Since GATE is an open architecture, new virtual keyboards can easily be defined by users and added to the system. Apart from the input methods, GUK also provides a simple Unicode-aware text editor which is important because not all platforms provide one by default or the users may not know which one of the already installed editors is Unicode-aware. Besides providing text visualisation and editing facilities, the GUK editor also per"
E03-2009,W02-2025,0,0.0384946,"Missing"
E03-2009,pastra-etal-2002-feasible,1,0.699012,"e development and testing of GATE in a cross-platform environment, while the ability to handle Unicode enables applications developed 220 within GATE to be easily ported to new languages. 4 The future isn't English Robust tools for multilingual information extraction are becoming increasingly sought after now that we have capabilities for processing texts in different languages and scripts. While the default IE system is English-specific, some of the modules can be reused directly (e.g. the Unicode-based tokeniser can handle IndoEuropean languages), and/or easily customised for new languages (Pastra et al., 2002). So far, ANNIE has been adapted to do IE in Bulgarian, Romanian, Bengali, Greek, Spanish, Swedish, German, Italian, and French, and we are currently porting it to Arabic, Chinese and Russian, as part of the MUSE project 2 . IMMFis AnnotatiooidA Din surnarui Nr.336/1111110.2002 â€¢ Gnidul calatoriilorfa4a vita in ball Bilandul activitadii poobor buzodani in anul 111.(partea bâ€¢Polidistii au posibOtatea legislativa de aid indeplini mai blue sarcinileii misnimle cc le revinâ€¢ A incep-ut cnn nou an de lupta cu infractorii: Printre primii la calatorli fara vita, 5eribedn Nadine din a font prinsa pa ae"
E03-2013,W03-2805,1,0.791342,"entences to a set of &quot;correct&quot; extracted sentences, then co-selection is measured by precision, recall and F-score. Gate&apos;s AnnotationDiff tool enables two sets of annotations on a document to be quantitative compared (i,e. two summaries produced by two summarisation configurations). We are making use of human annotated corpus (source documents and sets of extracts) (Saggion et al., 2002b) in order to evaluate different system configurations and to identify experimentally the best feature combination. Processing resources for content-based evaluation have already been integrated in the system (Pastra and Saggion, 2003). Future work will include the use of document-summary (non extractive) pairs (from the Document Understanding Conferences Corpus as well as from the HKNews Corpus (Saggion et al., 2002a)) and machine learning algorithms to obtain the best combination of the summarisation features, where &apos;extracts&apos; will be learn based on the automatic alignment between the non-extractive summaries and their source documents. The summarisation 238 system presented here provides a framework for experimentation in text summarisation research. The summariser combines two orthogonal approaches in a simple way takin"
E03-2013,J02-4005,1,0.748078,"summarisation addressing the need for user adaptation. 1 Introduction Two approaches are generally considered in automatic text summarisation research: the shallow sentence extraction approach and the deep, understand and generate approach (Mani, 2000). Sentence extraction methods are quite robust, but sentence extracts suffer from lack of cohesion and coherence. Methods that identify the essential information of the document by either information extraction or text understanding and that use the key information to produce a new text, lead to high-quality summarisation (Paice and Jones, 1993; Saggion and Lapalme, 2002) but suffer from the knowledge-bottleneck problem: adapting information extraction rules, templates, and generation grammars to new tasks or domains is time consuming. An alternative to these approaches is to use combination of robust techniques for semantic tagging together with statistical methods (Saggion, 2002). Here, we present a summarisation system that makes use of robust components for semantic tagging and coreference resolution provided by GATE (Cunningham et al., 2002). Our system combines GATE components with well established statistical techniques developed for the purpose of text"
E03-2013,C02-1073,1,0.68697,"Missing"
E03-2013,saggion-etal-2002-developing,1,0.791192,"rch projects make use of in-house evaluation, making it difficult to replicate experiments, to compare results, or to use evaluation data for training purposes. When text summarisation systems are evaluated by comparing extracted sentences to a set of &quot;correct&quot; extracted sentences, then co-selection is measured by precision, recall and F-score. Gate&apos;s AnnotationDiff tool enables two sets of annotations on a document to be quantitative compared (i,e. two summaries produced by two summarisation configurations). We are making use of human annotated corpus (source documents and sets of extracts) (Saggion et al., 2002b) in order to evaluate different system configurations and to identify experimentally the best feature combination. Processing resources for content-based evaluation have already been integrated in the system (Pastra and Saggion, 2003). Future work will include the use of document-summary (non extractive) pairs (from the Document Understanding Conferences Corpus as well as from the HKNews Corpus (Saggion et al., 2002a)) and machine learning algorithms to obtain the best combination of the summarisation features, where &apos;extracts&apos; will be learn based on the automatic alignment between the non-e"
E03-2014,J98-3005,0,0.0411612,"e definitions; and template generation and filling component that uses the domain lexicon and linguistic output of the first step as a guidance to fill-in the templates. The systems takes advantage of the information extracted from formal texts (e.g., lists of players) in order to carry out the analysis of tickers. 3 Merging or Cross-document Event Coreference The merging component in MUMIS combines the partial information as extracted from various sources, such that more complete annotations can be obtained. Information extraction and merging from multiple sources has been tried in the past (Radev and McKeown, 1998) but only for single events, the novelty of our approach consists on applying merging to multiple-events extracted from multiple sources. As an example consider the following situation (Netherlands-Yugoslavia match): One of the IE components extracted from document A that in the 30th minute of the match a free-kick was taken, but did not discover who took it. It did find the names of two players, though: Mihajlovic (a Yugoslavian player) and Van der Sar (the Dutch keeper). From document B a save in the 31st minute was extracted by the IE component, and the names of the same two players were re"
guthrie-etal-2004-large,H93-1051,0,\N,Missing
guthrie-etal-2004-large,A97-1035,1,\N,Missing
guthrie-etal-2004-large,J01-3001,0,\N,Missing
guthrie-etal-2004-large,C96-1079,0,\N,Missing
I05-3023,W03-0422,0,0.0415066,"Missing"
I05-3023,W03-1721,0,0.0312292,"ne particular context should have the same segmentation. 2 Character Based Chinese Word Segmentation We adopted the character based methodology for Chinese word segmentation, in which every character in a sentence was checked one by one to see if it was a word on its own or it was beginning, middle, or end character of a multi-character word. In contrast, another commonly used strategy, the word based methodology segments a Chinese sentence into the words in a pre-defined word list possibly with probability information about each word, according to some maximum probability criteria ( see e.g. Chen (2003)). The performance of word based segmentation is dependent upon the quality of word list used, while the character based method does not need any word list â€“ it segments a sentence only based on the characters in the sentence. Using character based methodology, we transform the word segmentation problem into four binary classification problems, corresponding to single-character word, the beginning, middle and end character of multi-character word, respectively. For each of the four classes a classifier was learnt from training set using the one vs. all others paradigm, in which every character"
I05-3023,W05-0610,1,0.837393,"ing example a value (before thresholding) larger than a predefined parameter (margin). The margin Perceptron has better generalisation capability than the standard Perceptron. Li et al. (2002) proposed the Perceptron algorithm with uneven margins (PAUM) by introducing two margin parameters Ï„+ and Ï„âˆ’ into the update rules for the positive and negative examples, respectively. Two margin parameters allow the PAUM to handle imbalanced datasets better than both the standard Perceptron and the margin Perceptron. PAUM has been successfully used for document classification and information extraction (Li et al., 2005). We used the PAUM algorithm to train a classifier for each of four classes for Chinese word segmentation. For one test example, the output of the Perceptron classifier before thresholding was used for comparison among the four classifiers. The important parameters of the learning algorithm are the uneven margins parameters Ï„ + and Ï„âˆ’ . In all our experiments Ï„+ = 20 and Ï„âˆ’ = 1 were used. Table 1 presents the results for each of the four classification problems, obtained from 4-fold cross-validation on training set. Not surprisingly, the classification for middle character of multicharacter wo"
I05-3023,W03-1728,0,0.0930721,"the four classes a classifier was learnt from training set using the one vs. all others paradigm, in which every character in the training data belonging to the class considered was regarded as positive example and all other characters were negative examples. After learning, we applied the four classifiers to each character in test text and assigned the character the class which classifier had the maximal output among the four. This kind of strategy has been widely used in the applications of machine learning to named entity recognition and has also 154 been used in Chinese word segmentation (Xue and Shen, 2003). Finally a word delimiter (often a blank space, depending on particular corpus) was added to the right of one character if it was not the last character of a sentence and it was predicted as end character of word or as a single character word. 3 Learning Algorithm Perceptron is a simple and effective learning algorithm. For a binary classification problem, it checks the training examples one by one by predicting their labels. If the prediction is correct, the example is passed; otherwise, the example is used to correct the model. The algorithm stops when the model classifies all training exam"
maynard-etal-2004-automatic,A97-1028,0,\N,Missing
maynard-etal-2004-automatic,E99-1001,0,\N,Missing
maynard-etal-2004-automatic,W03-1301,0,\N,Missing
maynard-etal-2004-automatic,W03-1505,1,\N,Missing
P02-1022,bird-etal-2000-atlas,0,\N,Missing
P02-1022,X98-1004,0,\N,Missing
P02-1022,A97-1054,0,\N,Missing
P13-4004,W02-0302,0,0.0294913,"vices, so that users can quickly restrict which types of services they are after and then be shown only the relevant subset. At the time of writing, there are services of the following kinds: â€¢ Part-of-Speech-Taggers for English, German, Dutch, and Hungarian. â€¢ Chunking: the GATE NP and VP chunkers and the OpenNLP ones; â€¢ Parsing: currently the Stanford Parser 4 , but more are under integration; â€¢ Stemming in 15 languages, via the Snowball stemmer; â€¢ Named Entity Recognition: in English, German, French, Arabic, Dutch, Romanian, and Bulgarian; â€¢ Biomedical taggers: the PennBio5 and the AbGene (Tanabe and Wilbur, 2002) taggers; â€¢ Twitter-specific NLP: language detection, tokenisation, normalisation, POS tagging, and developerâ€™s stand-alone computer to be deployed seamlessly on distributed hardware resources (the compute cloud) with the aim of processing large amounts of data in a timely fashion. This process needs to be resilient in the face of failures at the level of the cloud infrastructure, the network communication, errors in the processing pipeline and in the input data. The platform layer determines the optimal number of virtual machines for running a given NLP application, given the size of the docu"
P13-4004,P02-1022,1,0.911579,"nt and to create high quality training and evaluation datasets. It is common to use double or triple annotation, where several people perform the annotation task independently and we then measure their level of agreement (Inter-Annotator Agreement, or IAA) to quantify and control the quality of this data (Hovy, 2010). The AnnoMarket platform was therefore designed to offer full methodological support for all stages of the text analysis development lifecycle: 1. Create an initial prototype of the NLP pipeline, testing on a small document collection, using the desktop-based GATE user interface (Cunningham et al., 2002); 2. If required, collect a gold-standard corpus for evaluation and/or training, using the GATE Teamware collaborative corpus annotation service (Bontcheva et al., 2013), running in AnnoMarket; 3. Evaluate the performance of the automatic pipeline on the gold standard (either locally in the GATE development environment or on the cloud). Return to step 1 for further development and evaluation cycles, as needed. 4. Upload the large datasets and deploy the NLP pipeline on the AnnoMarket PaaS; 5. Run the large-scale NLP experiment and download the results as XML or a standard linguistic annotation"
P13-4004,P10-5004,0,\N,Missing
P13-4004,ide-romary-2002-standards,0,\N,Missing
saggion-etal-2002-extracting,X98-1004,0,\N,Missing
saggion-etal-2002-extracting,P00-1036,0,\N,Missing
saggion-etal-2002-extracting,J95-4004,0,\N,Missing
saggion-etal-2002-extracting,W01-1017,1,\N,Missing
saggion-etal-2002-extracting,O98-4002,1,\N,Missing
tablan-etal-2002-unicode,pastra-etal-2002-feasible,1,\N,Missing
tablan-etal-2002-unicode,gamback-olsson-2000-experiences,0,\N,Missing
tablan-etal-2006-user,C92-2090,0,\N,Missing
W00-1501,gamback-olsson-2000-experiences,0,\N,Missing
W00-1501,A97-1034,0,\N,Missing
W00-1501,A97-1035,1,\N,Missing
W00-1501,A97-1051,0,\N,Missing
W00-1501,E99-1035,0,\N,Missing
W00-1501,cunningham-etal-2000-software,1,\N,Missing
W00-1501,C96-1079,0,\N,Missing
W00-1503,A97-1035,1,\N,Missing
W01-1004,J92-4003,0,0.0165778,"Missing"
W01-1004,M98-1007,0,0.0295886,"Missing"
W01-1004,W01-1002,0,0.0327066,"Missing"
W01-1004,J96-2003,0,0.0220405,"Missing"
W01-1004,C00-2136,0,0.0501815,"Missing"
W01-1017,W00-1503,1,0.871131,"Missing"
W01-1017,M98-1007,0,0.07732,"Missing"
W01-1017,C96-2116,0,0.0408392,"Missing"
W01-1017,P98-2143,0,0.0201447,"Missing"
W01-1017,A97-1031,0,0.04498,"Missing"
W01-1017,C96-1021,0,\N,Missing
W01-1017,A00-1033,0,\N,Missing
W01-1017,C94-2144,0,\N,Missing
W01-1017,M95-1017,0,\N,Missing
W01-1017,C98-2138,0,\N,Missing
W02-0108,W02-0109,0,0.302305,"For courses where the emphasis is more on linguistic annotation and corpus work, GATE can be used as a corpus annotation environment (see http://gate.ac.uk/talks/tutorial3/). The annotation can be done completely manually or it can be bootstrapped by running some of GATEâ€™s processing resources over the corpus and then correcting/adding new annotations manually. These facilities can also be used in courses and assignments where the students need to learn how to create data for quantitative evaluation of NLP systems. If evaluated against the requirements for teaching environments discussed in (Loper and Bird, 2002), GATE covers them all quite well. The graphical development environment and the JAPE language facilitate otherwise difficult tasks. Inter-module consistency is achieved by using the annotations model to hold language data, while extensibility and modularity are the very reason why GATE has been successfully used in many research projects (Maynard et al., 2000). In addition, GATE also offers robustness and scalability, which allow students to experiment with big corpora, such as the British National Corpus (approx. 4GB). In the following subsections we will provide further detail about these a"
W02-0108,tablan-etal-2002-unicode,1,0.875493,"Missing"
W02-0108,X98-1004,0,\N,Missing
W02-0108,P06-4018,0,\N,Missing
W02-0403,tablan-etal-2002-unicode,1,\N,Missing
W02-0403,X98-1004,0,\N,Missing
W02-0403,P00-1036,0,\N,Missing
W02-0403,W00-0401,1,\N,Missing
W02-0403,A97-1051,0,\N,Missing
W03-0101,E03-2009,1,\N,Missing
W03-0803,baker-etal-2002-emille,1,0.880531,"Missing"
W03-0803,ma-etal-2002-models,0,0.063299,"Missing"
W03-0803,daelemans-hoste-2002-evaluation,0,0.0294896,"rs for the chosen learning method (e.g., thresholds, smoothing values). The classes to be learnt (e.g., Person, Organisation) are provided as part of the user profile, which can be edited on a dedicated page. All ML methods compatible with OLLIE have a uniform way of describing attributes and classes (see Section 3.1 for more details on the ML integration); this makes possible the use of a single user interface for all the ML algorithms available. The fine-tuning parameters are specific to each ML method and, although the ML methods can be run with their default parameters, as established by (Daelemans and Hoste, 2002), substantial variation in performance can be obtained by changing algorithm options. The graphical interface facilities provided by a web browser could be used to design an interface for annotating documents but that would mean stretching them beyond their intended use and it is hard to believe that such an interface would rate very high on a usability scale. In order to provide a more ergonomic interface, OLLIE uses a Java applet that integrates seamlessly with the page displayed by the browser. Apart from better usability, this allows for greater range of options for the user. Since OLLIE n"
W03-0803,gamback-olsson-2000-experiences,0,0.0526719,"Missing"
W03-0803,ide-etal-2000-xces,0,\N,Missing
W03-0803,tablan-etal-2002-unicode,1,\N,Missing
W03-0803,P02-1062,0,\N,Missing
W03-1505,pastra-etal-2002-feasible,1,\N,Missing
W03-1505,A97-1028,0,\N,Missing
W03-1505,E03-2009,1,\N,Missing
W05-0610,C02-1054,0,0.16269,"our algorithms on small amounts of training data and show their learning curve. The learning algorithms for IE can be classified broadly into two main categories: rule learning and statistical learning. The former induces a set of rules from training examples. There are many rule based learning systems, e.g. SRV (Freitag, 1998), RAPIER (Califf, 1998), WHISK (Soderland, 1999), BWI (Freitag and Kushmerick, 2000), and (LP ) 2 (Ciravegna, 2001). Statistical systems learn a statistical model or classifiers, such as HMMs (Freigtag and McCallum, 1999), Maximal Entropy (Chieu and Ng., 2002), the SVM (Isozaki and Kazawa, 2002; Mayfield et al., 2003), and Perceptron (Carreras et al., 2003). IE systems also differ from each other in the NLP features that they use. These include simple features such as token form and capitalisation information, linguistic features such as part-ofspeech, semantic information from gazetteer lists, and genre-specific information such as document structure. In general, the more features the system uses, the better performance it can achieve. This paper concentrates on classifier-based learning for IE, which typically converts the recognition of each information entity into a set of class"
W05-0610,Y03-1024,1,0.697666,"balanced data. In this paper we explore the application of PAUM to IE. The rest of the paper is structured as follows. Section 2 describes the uneven margins SVM and Perceptron algorithms. Sections 3.1 and 3.2 discuss the classifier-based framework for IE and the experimental datasets we used, respectively. We compare our systems to other state-of-the-art systems on three benchmark datasets in Section 3.3. Section 3.4 discusses the effects of the uneven margins parameter on the SVM and Perceptronâ€™s performances. Finally, Section 4 provides some conclusions. 2 Uneven Margins SVM and Perceptron Li and Shawe-Taylor (2003) introduced an uneven margins parameter into the SVM to deal with imbalanced classification problems. They showed that the SVM with uneven margins outperformed the standard SVM on document classification problem with imbalanced training data. Formally, given a training set Z = ((x1 , y1 ), . . . , (xm , ym )),where xi is the ndimensional input vector and yi (= +1 or âˆ’1) its label, the SVM with uneven margins is obtained by solving the quadratic optimisation problem: minw, b, Î¾ hw, wi + C m X i=1 Î¾i s.t. hw, xi i + Î¾i + b â‰¥ 1 if yi = +1 3 Experiments hw, xi i âˆ’ Î¾i + b â‰¤ âˆ’Ï„ if yi = âˆ’1 3.1 Î¾i â‰¥ 0"
W05-0610,W03-0422,0,0.140468,"Missing"
W05-0610,W03-0425,0,0.027395,"Missing"
W05-0610,W03-0429,0,0.276145,"ounts of training data and show their learning curve. The learning algorithms for IE can be classified broadly into two main categories: rule learning and statistical learning. The former induces a set of rules from training examples. There are many rule based learning systems, e.g. SRV (Freitag, 1998), RAPIER (Califf, 1998), WHISK (Soderland, 1999), BWI (Freitag and Kushmerick, 2000), and (LP ) 2 (Ciravegna, 2001). Statistical systems learn a statistical model or classifiers, such as HMMs (Freigtag and McCallum, 1999), Maximal Entropy (Chieu and Ng., 2002), the SVM (Isozaki and Kazawa, 2002; Mayfield et al., 2003), and Perceptron (Carreras et al., 2003). IE systems also differ from each other in the NLP features that they use. These include simple features such as token form and capitalisation information, linguistic features such as part-ofspeech, semantic information from gazetteer lists, and genre-specific information such as document structure. In general, the more features the system uses, the better performance it can achieve. This paper concentrates on classifier-based learning for IE, which typically converts the recognition of each information entity into a set of classification problems. In t"
W98-1208,W97-0211,0,0.040889,"Missing"
W98-1208,H92-1022,0,0.0159555,"en words belonging to a list of stop words s are removed. The words which have not been identified as part of a named entity or removed because it is a stop word are considered by the system to be ambiguous words and those are the words which are disambignated. For each of the ambiguous words, its set of possible senses is extracted from LDOCE and stored. Each sense in LDOCE contains a short textual definition (such as those shown in figure 5) which, when extracted from the dictionary, is processed to remove stop words and stem the remaining words. . The text is tagged using the Brill tagger (Brill, 1992) and a translation is carried out using a manually defined mapping from the syntactic tags assigned by Brill (Penn Tree Bank tags (Marcus, Santorini, and Marcinkiewicz, 1993)) onto the simpler part-of-speech categories associated with LDOCE senses 6. We then remove from consideration any of the senses whose partof-speech is not consistent with the one assigned by the tagger, if none of the senses are consistent with the part-of-speech we assume the tagger has made an error and leave the set of senses for that word unaltered. Preprocessing â€¢ Named-entity identification â€¢ Dictionary look-up Disa"
W98-1208,C92-4189,0,0.0422652,"Missing"
W98-1208,1997.tmi-1.18,0,0.0520242,"Missing"
W98-1208,J93-2004,0,0.0296192,"it is a stop word are considered by the system to be ambiguous words and those are the words which are disambignated. For each of the ambiguous words, its set of possible senses is extracted from LDOCE and stored. Each sense in LDOCE contains a short textual definition (such as those shown in figure 5) which, when extracted from the dictionary, is processed to remove stop words and stem the remaining words. . The text is tagged using the Brill tagger (Brill, 1992) and a translation is carried out using a manually defined mapping from the syntactic tags assigned by Brill (Penn Tree Bank tags (Marcus, Santorini, and Marcinkiewicz, 1993)) onto the simpler part-of-speech categories associated with LDOCE senses 6. We then remove from consideration any of the senses whose partof-speech is not consistent with the one assigned by the tagger, if none of the senses are consistent with the part-of-speech we assume the tagger has made an error and leave the set of senses for that word unaltered. Preprocessing â€¢ Named-entity identification â€¢ Dictionary look-up Disarnbiguation â€¢ Part-of-speech filtering â€¢ Dictionary definition overlap 4. Our next module is based on a proposal by Lesk (Lesk, 1986) that words in a sentence could be disam"
W98-1208,J92-1001,0,0.0322514,"case means 4LDOCE senses have additional information such as subject categories, subcategorisation information and selectional restrictions which we do not show here. Implementing a Sense Tagger m m m m m m m m m II senses in texts. A natural extension to this observation is to create a disambiguation system which makes use of several of these independent knowledge sources and combines their results in an intelligent way. Our system is based on a set of partial taggers, each of which uses a different knowledge source, with their results being combined. Our system is in the tradition of McRoy (McRoy, 1992), who also made use of several knowledge sources for word sense disambiguation, although the information sources she used were not independent, making it difficult to evaluate the contribution of each component. Our system makes use of strictly independent knowledge sources and is implemented within GATE whose plug-and-play architecture makes the evaluation of individual components more straightforward. At the moment the sense tagger consists of six stages (shown in figure 6), the first two preprocess the text which is to be disambiguated while the remaining four carry out the disambiguation."
W98-1208,P96-1006,0,0.0345721,"Missing"
W98-1208,W97-0322,0,0.0781595,"Missing"
W98-1208,C96-1071,1,0.873626,"Missing"
W98-1208,W97-0208,1,0.780338,"Missing"
W98-1208,H93-1052,0,0.0544604,"Missing"
W98-1208,P95-1026,0,0.0366988,"Missing"
W98-1208,A97-1036,0,0.0262743,"for the task graphs, and the diffculty of managing these graphs as the number of modules in the system grows. The graphs currently make two main contributions to the system: they give a graphical representation of control flow, and allow the user to manipulate execution of modules; they give a graphical entry point to results visualisation. These benefits will have to be balanced against their disadvantages in future versions of the Cunningham, Stevenson and Wilks II 69 system. Another problem may arise when the architecture includes facilities for distributed processing (Zajac et al., 1997; Zajac, 1997), as it is not obvious how the linear model currently embodied in the graphs could be extended to support non-linear control strucures. 8 Conclusion The previous section indicates that GATE version 1 goes a long way to meeting it's design goals (noted in section 2). The reuse of components we have experienced in the sense tagging project and a number of other local and collaborative projects is in itself justification of the development effort spent on the system, and, hopefully, these savings will be multiplied accross other users of the system. Future versions will address the problems we un"
W98-1208,A97-1035,1,\N,Missing
W98-1208,P94-1020,0,\N,Missing
W98-1208,H92-1045,0,\N,Missing
W98-1208,C96-1079,0,\N,Missing
W98-1208,A97-2017,1,\N,Missing
yankova-etal-2008-framework,P98-1012,0,\N,Missing
yankova-etal-2008-framework,C98-1012,0,\N,Missing
yankova-etal-2008-framework,W03-0405,0,\N,Missing
yankova-etal-2008-framework,I08-1020,1,\N,Missing
