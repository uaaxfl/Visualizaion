2021.emnlp-main.400,Zero-Shot Dialogue Disentanglement by Self-Supervised Entangled Response Selection,2021,-1,-1,2,0,9546,tachung chi,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Dialogue disentanglement aims to group utterances in a long and multi-participant dialogue into threads. This is useful for discourse analysis and downstream applications such as dialogue response selection, where it can be the first step to construct a clean context/response set. Unfortunately, labeling all \textit{reply-to} links takes quadratic effort w.r.t the number of utterances: an annotator must check all preceding utterances to identify the one to which the current utterance is a reply. In this paper, we are the first to propose a \textbf{zero-shot} dialogue disentanglement solution. Firstly, we train a model on a multi-participant response selection dataset harvested from the web which is not annotated; we then apply the trained model to perform zero-shot dialogue disentanglement. Without any labeled data, our model can achieve a cluster F1 score of 25. We also fine-tune the model using various amounts of labeled data. Experiments show that with only 10{\%} of the data, we achieve nearly the same performance of using the full dataset."
2020.lrec-1.51,Adjusting Image Attributes of Localized Regions with Low-level Dialogue,2020,29,0,2,0,4683,tzuhsiang lin,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Natural Language Image Editing (NLIE) aims to use natural language instructions to edit images. Since novices are inexperienced with image editing techniques, their instructions are often ambiguous and contain high-level abstractions which require complex editing steps. Motivated by this inexperience aspect, we aim to smooth the learning curve by teaching the novices to edit images using low-level command terminologies. Towards this end, we develop a task-oriented dialogue system to investigate low-level instructions for NLIE. Our system grounds language on the level of edit operations, and suggests options for users to choose from. Though compelled to express in low-level terms, user evaluation shows that 25{\%} of users found our system easy-to-use, resonating with our motivation. Analysis shows that users generally adapt to utilizing the proposed low-level language interface. We also identified object segmentation as the key factor to user satisfaction. Our work demonstrates advantages of low-level, direct language-action mapping approach that can be applied to other problem domains beyond image editing such as audio editing or industrial design."
W16-3608,A {W}izard-of-{O}z Study on A Non-Task-Oriented Dialog Systems That Reacts to User Engagement,2016,7,22,4,0,1417,zhou yu,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,None
W16-3649,Strategy and Policy Learning for Non-Task-Oriented Conversational Systems,2016,14,32,4,0,1417,zhou yu,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,None
L16-1499,{A}pp{D}ialogue: Multi-App Dialogues for Intelligent Assistants,2016,9,4,6,0,26039,ming sun,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Users will interact with an individual app on smart devices (e.g., phone, TV, car) to fulfill a specific goal (e.g. find a photographer), but users may also pursue more complex tasks that will span multiple domains and apps (e.g. plan a wedding ceremony). Planning and executing such multi-app tasks are typically managed by users, considering the required global context awareness. To investigate how users arrange domains/apps to fulfill complex tasks in their daily life, we conducted a user study on 14 participants to collect such data from their Android smart phones. This document 1) summarizes the techniques used in the data collection and 2) provides a brief statistical description of the data. This data guilds the future direction for researchers in the fields of conversational agent and personal assistant, etc. This data is available at http://AppDialogue.com."
W15-4604,Miscommunication Recovery in Physically Situated Dialogue,2015,34,9,2,1,1549,matthew marge,Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"We describe an empirical study that crowdsourced human-authored recovery strategies for various problems encountered in physically situated dialogue. The purpose was to investigate the strategies that people use in response to requests that are referentially ambiguous or impossible to execute. Results suggest a general preference for including specific kinds of visual information when disambiguating referents, and for volunteering alternative plans when the original instruction was not possible to carry out."
P15-1047,Matrix Factorization with Knowledge Graph Propagation for Unsupervised Spoken Language Understanding,2015,45,23,4,1,53,yunnung chen,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Spoken dialogue systems (SDS) typically require a predefined semantic ontology to train a spoken language understanding (SLU) module. In addition to the annotation cost, a key challenge for designing such an ontology is to define a coherent slot set while considering their complex relations. This paper introduces a novel matrix factorization (MF) approach to learn latent feature vectors for utterances and semantic elements without the need of corpus annotations. Specifically, our model learns the semantic slots for a domain-specific SDS in an unsupervised fashion, and carries out semantic parsing using latent MF techniques. To further consider the global semantic structure, such as inter-word and inter-slot relations, we augment the latent MF-based model with a knowledge graph propagation model based on a slot-based semantic graph and a word-based lexical graph. Our experiments show that the proposed MF approaches produce better SLU models that are able to predict semantic slots and word patterns taking into account their relations and domain-specificity in a joint manner."
N15-1064,Jointly Modeling Inter-Slot Relations by Random Walk on Knowledge Graphs for Unsupervised Spoken Language Understanding,2015,36,20,3,1,53,yunnung chen,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"A key challenge of designing coherent semantic ontology for spoken language understanding is to consider inter-slot relations. In practice, however, it is difficult for domain experts and professional annotators to define a coherent slot set, while considering various lexical, syntactic, and semantic dependencies. In this paper, we exploit the typed syntactic dependency theory for unsupervised induction and filling of semantics slots in spoken dialogue systems. More specifically, we build two knowledge graphs: a slot-based semantic graph, and a word-based lexical graph. To jointly consider word-to-word, word-toslot, and slot-to-slot relations, we use a random walk inference algorithm to combine the two knowledge graphs, guided by dependency grammars. The experiments show that considering inter-slot relations is crucial for generating a more coherent and compete slot set, resulting in a better spoken language understanding model, while enhancing the interpretability of semantic slots."
W14-4414,Two-Stage Stochastic Email Synthesizer,2014,6,2,2,1,53,yunnung chen,Proceedings of the 8th International Natural Language Generation Conference ({INLG}),0,"This paper presents the design and implementation details of an email synthesizer using two-stage stochastic natural language generation, where the first stage structures the emails according to sender style and topic structure, and the second stage synthesizes text content based on the particulars of an email structure element and the goals of a given communication for surface realization. The synthesized emails reflect sender style and the intent of communication, which can be further used as synthetic evidence for developing other applications."
W14-4425,Two-Stage Stochastic Natural Language Generation for Email Synthesis by Modeling Sender Style and Topic Structure,2014,10,6,2,1,53,yunnung chen,Proceedings of the 8th International Natural Language Generation Conference ({INLG}),0,"This paper describes a two-stage process for stochastic generation of email, in which the first stage structures the emails according to sender style and topic structure (high-level generation), and the second stage synthesizes text content based on the particulars of an email element and the goals of a given communication (surface-level realization). Synthesized emails were rated in a preliminary experiment. The results indicate that sender style can be detected. In addition we found that stochastic generation performs better if applied at the word level than at an original-sentence level (xe2x80x9ctemplate-basedxe2x80x9d) in terms of email coherence, sentence fluency, naturalness, and preference."
W14-4326,Knowledge Acquisition Strategies for Goal-Oriented Dialog Systems,2014,16,6,2,1,10646,aasish pappu,Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ({SIGDIAL}),0,"Many goal-oriented dialog agents are expected to identify slot-value pairs in a spoken query, then perform lookup in a knowledge base to complete the task. When the agent encounters unknown slotvalues, it may ask the user to repeat or reformulate the query. But a robust agent can proactively seek new knowledge from a user, to help reduce subsequent task failures. In this paper, we propose knowledge acquisition strategies for a dialog agent and show their effectiveness. The acquired knowledge can be shown to subsequently contribute to task completion."
W14-0211,Conversational Strategies for Robustly Managing Dialog in Public Spaces,2014,22,0,4,1,10646,aasish pappu,Proceedings of the {EACL} 2014 Workshop on Dialogue in Motion,0,Open environments present an attention management challenge for conversational systems. We describe a kiosk system (based on Ravenclawxe2x80x90Olympus) that uses simple auditory and visual information to interpret human presence and manage the systemxe2x80x99s attention. The system robustly differentiates intended interactions from unintended ones at an accuracy of 93% and provides similar task completion rates in both a quiet room and a public space.
W13-4038,Predicting Tasks in Goal-Oriented Spoken Dialog Systems using Semantic Knowledge Bases,2013,27,17,2,1,10646,aasish pappu,Proceedings of the {SIGDIAL} 2013 Conference,0,"Goal-oriented dialog agents are expected to recognize user-intentions from an utterance and execute appropriate tasks. Typically, such systems use a semantic parser to solve this problem. However, semantic parsers could fail if user utterances contain out-of-grammar words/phrases or if the semantics of uttered phrases did not match the parserxe2x80x99s expectations. In this work, we have explored a more robust method of task prediction. We define task prediction as a classification problem, rather than xe2x80x9cparsingxe2x80x9d and use semantic contexts to improve classification accuracy. Our classifier uses semantic smoothing kernels that can encode information from knowledge bases such as Wordnet, NELL and Freebase.com. Our experiments on two spoken language corpora show that augmenting semantic information from these knowledge bases gives about 30% absolute improvement in task prediction over a parserbased method. Our approach thus helps make a dialog agent more robust to user input and helps reduce number of turns required to detected intended tasks."
W12-1613,The Structure and Generality of Spoken Route Instructions,2012,20,9,2,1,10646,aasish pappu,Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"A robust system that understands route instructions should be able to process instructions generated naturally by humans. Also desirable would be the ability to handle repairs and other modifications to existing instructions. To this end, we collected a corpus of spoken instructions (and modified instructions) produced by subjects provided with an origin and a destination. We found that instructions could be classified into four categories, depending on their intent such as imperative, feedback, or meta comment. We asked a different set of subjects to follow these instructions to determine the usefulness and comprehensibility of individual instructions. Finally, we constructed a semantic grammar and evaluated its coverage. To determine whether instruction-giving forms a predictable sub-language, we tested the grammar on three corpora collected by others and determined that this was largely the case. Our work suggests that predictable sub-languages may exist for well-defined tasks."
W10-4318,Towards Improving the Naturalness of Social Conversations with Dialogue Systems,2010,9,8,4,1,1549,matthew marge,Proceedings of the {SIGDIAL} 2010 Conference,0,"We describe an approach to improving the naturalness of a social dialogue system, Talkie, by adding disfluencies and other content-independent enhancements to synthesized conversations. We investigated whether listeners perceive conversations with these improvements as natural (i.e., human-like) as human-human conversations. We also assessed their ability to correctly identify these conversations as between humans or computers. We find that these enhancements can improve the perceived naturalness of conversations for observers overhearing the dialogues."
W10-4328,Comparing Spoken Language Route Instructions for Robots across Environment Representations,2010,30,14,2,1,1549,matthew marge,Proceedings of the {SIGDIAL} 2010 Conference,0,"Spoken language interaction between humans and robots in natural environments will necessarily involve communication about space and distance. The current study examines people's close-range route instructions for robots and how the presentation format (schematic, virtual or natural) and the complexity of the route affect the content of instructions. We find that people have a general preference for providing metric-based instructions. At the same time, presentation format appears to have less impact on the formulation of these instructions. We conclude that understanding of spatial language requires handling both landmark-based and metric-based expressions."
W10-0716,Using the {A}mazon {M}echanical {T}urk to Transcribe and Annotate Meeting Speech for Extractive Summarization,2010,11,32,3,1,1549,matthew marge,Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with {A}mazon{'}s Mechanical Turk,0,"Due to its complexity, meeting speech provides a challenge for both transcription and annotation. While Amazon's Mechanical Turk (MTurk) has been shown to produce good results for some types of speech, its suitability for transcription and annotation of spontaneous speech has not been established. We find that MTurk can be used to produce high-quality transcription and describe two techniques for doing so (voting and corrective). We also show that using a similar approach, high quality annotations useful for summarization systems can also be produced. In both cases, accuracy is comparable to that obtained using trained personnel."
W09-3910,Detecting the Noteworthiness of Utterances in Human Meetings,2009,9,7,2,1,44378,satanjeev banerjee,Proceedings of the {SIGDIAL} 2009 Conference,0,"Our goal is to make note-taking easier in meetings by automatically detecting noteworthy utterances in verbal exchanges and suggesting them to meeting participants for inclusion in their notes. To show feasibility of such a process we conducted a Wizard of Oz study where the Wizard picked automatically transcribed utterances that he judged as noteworthy, and suggested their contents to the participants as notes. Over 9 meetings, participants accepted 35% of these suggestions. Further, 41.5% of their notes at the end of the meeting contained Wizard-suggested text. Next, in order to perform noteworthiness detection automatically, we annotated a set of 6 meetings with a 3-level noteworthiness annotation scheme, which is a break from the binary in summary/not in summary labeling typically used in speech summarization. We report Kappa of 0.44 for the 3-way classification, and 0.58 when two of the 3 labels are merged into one. Finally, we trained an SVM classifier on this annotated data; this classifier's performance lies between that of trivial baselines and inter-annotator agreement."
W09-2813,Non-textual Event Summarization by Applying Machine Learning to Template-based Language Generation,2009,28,7,4,0,46946,mohit kumar,Proceedings of the 2009 Workshop on Language Generation and Summarisation ({UCNLG}+{S}um 2009),0,"We describe a learning-based system that creates draft reports based on observation of people preparing such reports in a target domain (conference replanning). The reports (or briefings) are based on a mix of text and event data. The latter consist of task creation and completion actions, collected from a wide variety of sources within the target environment. The report drafting system is part of a larger learning-based cognitive assistant system that improves the quality of its assistance based on an opportunity to learn from observation. The system can learn to accurately predict the briefing assembly behavior and shows significant performance improvements relative to a non-learning system, demonstrating that it's possible to create meaningful verbal descriptions of activity from event streams."
P09-2023,Predicting Barge-in Utterance Errors by using Implicitly-Supervised {ASR} Accuracy and Barge-in Rate per User,2009,10,6,2,0,14947,kazunori komatani,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"Modeling of individual users is a promising way of improving the performance of spoken dialogue systems deployed for the general public and utilized repeatedly. We define implicitly-supervised ASR accuracy per user on the basis of responses following the system's explicit confirmations. We combine the estimated ASR accuracy with the user's barge-in rate, which represents how well the user is accustomed to using the system, to predict interpretation errors in barge-in utterances. Experimental results showed that the estimated ASR accuracy improved prediction performance. Since this ASR accuracy and the barge-in rate are obtainable at runtime, they improve prediction performance without the need for manual labeling."
W08-0805,Mixture Pruning and Roughening for Scalable Acoustic Models,2008,5,3,2,1,21627,david hugginsdaines,Proceedings of the {ACL}-08: {HLT} Workshop on Mobile Language Processing,0,"In an automatic speech recognition system using a tied-mixture acoustic model, the main cost in CPU time and memory lies not in the evaluation and storage of Gaussians themselves but rather in evaluating the mixture likelihoods for each state output distribution. Using a simple entropy-based technique for pruning the mixture weight distributions, we can achieve a significant speedup in recognition for a 5000-word vocabulary with a negligible increase in word error rate. This allows us to achieve real-time connected-word dictation on an ARM-based mobile device."
P08-4005,Interactive {ASR} Error Correction for Touchscreen Devices,2008,5,20,2,1,21627,david hugginsdaines,Proceedings of the {ACL}-08: {HLT} Demo Session,0,We will demonstrate a novel graphical interface for correcting search errors in the output of a speech recognizer. This interface allows the user to visualize the word lattice by pulling apart regions of the hypothesis to reveal a cloud of words simlar to the tag clouds popular in many Web applications. This interface is potentially useful for dictation on portable touchscreen devices such as the Nokia N800 and other mobile Internet devices.
I08-1035,Automatic Extraction of Briefing Templates,2008,16,2,3,0.833333,6251,dipanjan das,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"An approach to solving the problem of automatic briefing generation from non-textual events can be segmenting the task into two major steps, namely, extraction of briefing templates and learning aggregators that collate information from events and automatically fill up the templates. In this paper, we describe two novel unsupervised approaches for extracting briefing templates from human written reports. Since the problem is non-standard, we define our own criteria for evaluating the approaches and demonstrate that both approaches are effective in extracting domain relevant templates with promising accuracies."
D08-1100,Acquiring Domain-Specific Dialog Information from Task-Oriented Human-Human Interaction through an Unsupervised Learning,2008,26,3,2,0,45964,ananlada chotimongkol,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"We describe an approach for acquiring the domain-specific dialog knowledge required to configure a task-oriented dialog system that uses human-human interaction data. The key aspects of this problem are the design of a dialog information representation and a learning approach that supports capture of domain information from in-domain dialogs. To represent a dialog for a learning purpose, we based our representation, the form-based dialog structure representation, on an observable structure. We show that this representation is sufficient for modeling phenomena that occur regularly in several dissimilar task-oriented domains, including information-access and problem-solving. With the goal of ultimately reducing human annotation effort, we examine the use of unsupervised learning techniques in acquiring the components of the form-based representation (i.e. task, subtask, and concept). These techniques include statistical word clustering based on mutual information and Kullback-Liebler distance, TextTiling, HMM-based segmentation, and bisecting K-mean document clustering. With some modifications to make these algorithms more suitable for inferring the structure of a spoken dialog, the unsupervised learning algorithms show promise."
W07-0305,{O}lympus: an open-source framework for conversational spoken language interface research,2007,20,113,5,1,36641,dan bohus,Proceedings of the Workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technologies,0,"We introduce Olympus, a freely available framework for research in conversational interfaces. Olympus' open, transparent, flexible, modular and scalable nature facilitates the development of large-scale, real-world systems, and enables research leading to technological and scientific advances in conversational spoken language interfaces. In this paper, we describe the overall architecture, several systems spanning different domains, and a number of current research efforts supported by Olympus."
N07-2019,Implicitly Supervised Language Model Adaptation for Meeting Transcription,2007,9,9,2,1,21627,david hugginsdaines,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,"We describe the use of meeting metadata, acquired using a computerized meeting organization and note-taking system, to improve automatic transcription of meetings. By applying a two-step language model adaptation process based on notes and agenda items, we were able to reduce perplexity by 9% and word error rate by 4% relative on a set of ten meetings recorded in-house. This approach can be used to leverage other types of metadata."
2007.sigdial-1.46,Implicitly-supervised Learning in Spoken Language Interfaces: an Application to the Confidence Annotation Problem,2007,14,9,2,1,36641,dan bohus,Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue,0,"In this paper we propose the use of a novel learning paradigm in spoken language interfaces xe2x80x93 implicitly-supervised learning. The central idea is to extract a supervision signal online, directly from the user, from certain patterns that occur naturally in the conversation. The approach eliminates the need for developer supervision and facilitates online learning and adaptation. As a first step towards better understanding its properties, advantages and limitations, we have applied the proposed approach to the problem of confidence annotation. Experimental results indicate that we can attain performance similar to that of a fully supervised model, without any manual labeling. In effect, the system learns from its own experiences with the users. *"
W06-3404,You Are What You Say: Using Meeting Participants{'} Speech to Detect their Roles and Expertise,2006,12,10,2,1,44378,satanjeev banerjee,Proceedings of the Analyzing Conversations in Text and Speech,0,"Our goal is to automatically detect the functional roles that meeting participants play, as well as the expertise they bring to meetings. To perform this task, we build decision tree classifiers that use a combination of simple speech features (speech lengths and spoken keywords) extracted from the participants' speech in meetings. We show that this algorithm results in a role detection accuracy of 83% on unseen test data, where the random baseline is 33.3%. We also introduce a simple aggregation mechanism that combines evidence of the participants' expertise from multiple meetings. We show that this aggregation mechanism improves the role detection accuracy from 66.7% (when aggregating over a single meeting) to 83% (when aggregating over 5 meetings)."
N06-4003,{S}mart{N}otes: Implicit Labeling of Meeting Data through User Note-Taking and Browsing,2006,4,5,2,1,44378,satanjeev banerjee,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Demonstrations",0,"We have implemented SmartNotes, a system that automatically acquires labeled meeting data as users take notes during meetings and browse the notes afterwards. Such data can enable meeting understanding components such as topic and action item detectors to automatically improve their performance over a sequence of meetings. The SmartNotes system consists of a laptop based note taking application, and a web based note retrieval system. We shall demonstrate the functionalities of this system, and will also demonstrate the labeled data obtained during typical meetings and browsing sessions."
H05-1029,Error Handling in the {R}aven{C}law Dialog Management Architecture,2005,0,10,2,1,36641,dan bohus,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,None
2005.sigdial-1.14,Sorry and {I} Didn{'}t Catch That! - An Investigation of Non-understanding Errors and Recovery Strategies,2005,21,96,2,1,36641,dan bohus,Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue,0,"We present results from an extensive empirical analysis of non-understanding errors and ten non-understanding recovery strategies, based on a corpus of dialogs collected with a spoken dialog system that handles conference room reservations. More specifically, the issues we investigate are: what are the main sources of non-understanding errors? What is the impact of these errors on global performance? How do various strategies for recovery from non-understandings compare to each other? What are the relationships between these strategies and subsequent user response types, and which response types are more likely to lead to successful recovery? Can dialog performance be improved by using a smarter policy for engaging the non-understanding recovery strategies? If so, can we learn such a policy from data? Whenever available, we compare and contrast our results with other studies in the literature. Finally, we summarize the lessons learned and present our plans for future work inspired by this analysis."
W02-0711,Speech Translation on a Tight Budget without Enough Data,2002,11,3,4,0.405196,39652,robert frederking,Proceedings of the {ACL}-02 Workshop on Speech-to-Speech Translation: Algorithms and Systems,0,"The Tongues speech-to-speech translation system was developed for the US Army chaplains, with fairly stringent constraints on time, budget, and available data. The resulting prototype was required to undergo a quite realistic field test. We describe the development and architecture of the system, the field test, and our analysis of its results. The system performed quite well, especially given its development constraints."
W00-0306,Stochastic Language Generation for Spoken Dialogue Systems,2000,10,132,2,0,7069,alice oh,ANLP-NAACL 2000 Workshop: Conversational Systems,0,"The two current approaches to language generation, template-based and rule-based (linguistic) NLG, have limitations when applied to spoken dialogue systems, in part because they were developed for text generation. In this paper, we propose a new corpus-based approach to natural language generation, specifically designed for spoken dialogue systems."
W00-0309,Task-based dialog management using an agenda,2000,-1,-1,2,0,4068,wei xu,ANLP-NAACL 2000 Workshop: Conversational Systems,0,None
1999.mtsummit-1.82,A new approach to the translating telephone,1999,15,1,3,0.776172,39652,robert frederking,Proceedings of Machine Translation Summit VII,0,"The Translating Telephone has been a major goal of speech translation for many years. Previous approaches have attempted to work from limited-domain, fully-automatic translation towards broad-coverage, fully-automatic translation. We are approaching the problem from a different direction: starting with a broad-coverage but not fully-automatic system, and working towards full automation. We believe that working in this direction will provide us with better feedback, by observing users and collecting language data under realistic conditions, and thus may allow more rapid progress towards the same ultimate goal. Our initial approach relies on the wide-spread availability of Internet connections and web browsers to provide a user interface. We describe our initial work, which is an extension of the Diplomat wearable speech translator."
W97-0409,Interactive Speech Translation in the {DIPLOMAT} Project,1997,-1,-1,2,0.776172,39652,robert frederking,Spoken Language Translation,0,None
H94-1010,Expanding the Scope of the {ATIS} Task: The {ATIS}-3 Corpus,1994,6,178,8,0,55257,deborah dahl,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"The Air Travel Information System (ATIS) domain serves as the common evaluation task for ARPA spoken language system developers. To support this task, the Multi-Site ATIS Data COllection Working group (MADCOW) coordinates data collection activities. This paper describes recent MADCOW activities. In particular, this paper describes the migration of the ATIS task to a richer relational database and development corpus (ATIS-3) and describes the ATIS-3 corpus. The expanded database, which includes information on 46 US and Canadian cities and 23,457 flights, was released in the fall of 1992, and data collection for the ATIS-3 corpus began shortly thereafter. The ATIS-3 corpus now consists of a total of 8297 released training utterances and 3211 utterances reserved for testing, collected at BBN, CMU, MIT, NIST and SRI. 2906 of the training utterances have been annotated with the correct information from the database. This paper describes the ATIS-3 corpus in detail, including breakdowns of data by type (e.g. context-independent, context-dependent, and unevaluable)and variations in the data collected at different sites. This paper also includes a description of the ATIS-3 database. Finally, we discuss future data collection and evaluation plans."
H93-1002,Session 1: Spoken Language Systems,1993,-1,-1,1,1,9547,alexander rudnicky,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,None
H93-1074,Mode preference in a simple data-retrieval task,1993,9,28,1,1,9547,alexander rudnicky,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"This paper describes some recent experiments that assess user behavior in a multi-modal environment in which actions can be performed with equivalent effect in speech, keyboard or scroiler modes. Results indicate that users freely choose speech over other modalities, even when it is less efficient in objective terms, such as time-to-completion or input error."
H90-1045,A Comparison of Speech and Typed Input,1990,16,18,2,0,3847,alexander hauptmann,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"Meaningful evaluation of spoken language interfaces must be based on detailed comparisons with an alternate, well-understood input modality, such as the keyboard. This paper presents an empirical study in which users were asked to enter digit strings into the computer by voice and by keyboard. Two different ways of verifying and correcting the spoken input were also examined using either voice or keyboard. Timing analyses were performed to determine which aspects of the interface were critical to speedy completion of the task. The results show that speech is preferable for strings that require more than a few keystrokes. The results emphasize the need for fast and accurate speech recognition, but also demonstrate how error correction and input validation are crucial components of a speech interface."
H90-1046,The design of a spoken language interface,1990,4,19,2,0,57553,jeanmichel lunati,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"Fast and accurate speech recognition systems systems bring with them the possibility of designing effective voice driven applications. Efforts to this date have involved the construction of monolithic systems, necessitating repetition of effort as each new system is implemented. In this paper, we describe an initial implementation of a general spoken language interface, the Carnegie Mellon Spoken Language Shell (CM-SLS) which provides voice interface services to a variable number of applications running on the same computer. We also present a system built using CM-SLS, the Office Manager, which provides the user with voice access to facilities such as an appointment calendar, a personal database, and voice mail."
H89-2021,Evaluating spoken language interaction,1989,8,9,1,1,9547,alexander rudnicky,"Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",0,"To study the spoken language interface in the context of a complex problem-solving task, a group of users were asked to perform a spreadsheet task, alternating voice and keyboard input. A total of 40 tasks were performed by each participant, the first thirty in a group (over several days), the remaining ones a month later. The voice spreadsheet program used in this study was extensively instrumented to provide detailed information about the components of the interaction. These data, as well as analysis of the participants's utterances and recognizer output, provide a fairly detailed picture of spoken language interaction.Although task completion by voice took longer than by keyboard, analysis shows that users would be able to perform the spreadsheet task faster by voice, if two key criteria could be met: recognition occurs in real-time, and the error rate is sufficiently low. This initial experience with a spoken language system also allows us to identify several metrics, beyond those traditionally associated with speech recognition, that can be used to characterize system performance."
H89-1015,The design of voice-driven interfaces,1989,2,10,1,1,9547,alexander rudnicky,"Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",0,"This paper presents some issues that arise in building voice-driven interfaces to complex applications and describes some of the approaches that we have developed for this purpose. To test these approaches, we have implemented a voice spreadsheet and have begun observation of users interacting with it."
