2021.naacl-industry.27,L16-1503,0,0.120887,"Missing"
2021.naacl-industry.27,D14-1181,0,0.00293987,"ven appear sequentially, for example, the cancel intent is often followed by the refund intent, as users tend to request a can3.1.1 Convolutional Baseline cellation first and then ask for a refund. Therefore, we modeled our multi-intent system as a sequence The first approach was to assume that the feature tagging problem, where intent spans are encoded labels for an intent are local to that intent span, and, 216 therefore, each intent span can be fed into a classifier independently of the other intent spans. Under this assumption, we used a convolutional neural network with parallel filters (Kim, 2014), as it is a strong baseline used in several of our production systems. We used parallel filters of size 3, 4, and 5 with 100 filters each. Max-over-time pooling was used to produce a final span representation, which is projected into the label space. This model was trained using Adadelta (Zeiler, 2012) with an initial learning rate of 1.0 and a batch size of 50. However, this approach misses possible dependencies across spans. Some features (such as “tense”) are naturally co-dependent among spans; the use of a past tense verb in one span dictates that all spans in the utterance are past tense"
2021.naacl-industry.27,D17-1018,0,0.0541183,"Missing"
2021.naacl-industry.27,2020.findings-emnlp.166,1,0.824666,"Missing"
2021.naacl-industry.27,J00-3003,0,0.646951,"Missing"
2021.naacl-industry.27,P16-1101,0,0.0426155,"f just marking an event as “Asserted” or “Other”, our version of Modality distinguishes between different aspects of hypothetical events. 3 Modeling There are four different model types we explored for intent features that we detail below. However, before we can annotate an intent with a feature, we need to have an intent span. First, we describe our intent span extraction model whose predictions are used as intent spans. 3.1 Multi-Intent as Annotatable Spans as token level annotations with the IOBES tagging scheme (Ratinov and Roth, 2009). We used a standard BiLSTM-CRF architecture following Ma and Hovy (2016). Each input token is represented both as a character composition, by running a small convolutional neural network with a filter size of 3 over the characters and doing max-over-time pooling as in Dos Santos and Zadrozny (2014), and as a word embedding. We use the concatenation of multiple word embeddings, GloVe embeddings (Pennington et al., 2014), as well as 100 dimensional, in-domain embeddings trained in-house, following Lester et al. (2020a). The token sequence is then fed into an bidirectional LSTM (Graves et al., 2005), where the LSTM (Hochreiter and Schmidhuber, 1997) in each direction"
2021.naacl-industry.27,D14-1162,0,0.085986,"ribe our intent span extraction model whose predictions are used as intent spans. 3.1 Multi-Intent as Annotatable Spans as token level annotations with the IOBES tagging scheme (Ratinov and Roth, 2009). We used a standard BiLSTM-CRF architecture following Ma and Hovy (2016). Each input token is represented both as a character composition, by running a small convolutional neural network with a filter size of 3 over the characters and doing max-over-time pooling as in Dos Santos and Zadrozny (2014), and as a word embedding. We use the concatenation of multiple word embeddings, GloVe embeddings (Pennington et al., 2014), as well as 100 dimensional, in-domain embeddings trained in-house, following Lester et al. (2020a). The token sequence is then fed into an bidirectional LSTM (Graves et al., 2005), where the LSTM (Hochreiter and Schmidhuber, 1997) in each direction has a size of 200, and projected to the final label space. Finally a Conditional Random Field (CRF) (Lafferty et al., 2001) with constrained decoding (Lester et al., 2020b) is used to produce the final sequence of intents. This model was trained using SGD with momentum using 0.0015 as the learning rate, 0.9 for momentum, and a batch size of 10. Mo"
2021.naacl-industry.27,W18-2506,1,0.85666,"ssed by the local pooling function separately. Doing this efficiently in a batched computing environment, like TensorFlow (Abadi et al., 2015), is slightly tricky to implement. A much simpler model would feed the global utterance and the span separately, to be encoded and processed independently. Our ablations in the “– Shared Embedding” row of Table 4 shows that using a shared embedding space does yield performance gains, but it can be removed for the sake of easier model deployment and still maintain superior performance over the span-level model. All models were trained with Mead-Baseline (Pressel et al., 2018), an open-source library for the development, training, and export for deep neural networks for NLP. 6 Deployment We have deployed a NLU component of a taskoriented, production dialogue system that produces intent features. The dialogue system deals with cus5 Experiments tomer service in the retail software domain. The The F1 scores for these models are reported in Ta- dialogue manager currently makes use of several ble 3. The BiLSTM-CRF tagger without any infor- intent features. The easier feature to use is negamation about the intent boundaries has the lowest tion and it is critical to under"
2021.naacl-industry.27,H90-1020,0,0.176757,". Like us, they have regions on interest embedded in a larger context. However, our models differ in several key ways: their span representation is a hand-crafted combination of token features while ours is a learned pooling of token representations. Also, their model is restricted to operating on contiguous spans (possibly due to unavailability of spans a priori, or that noncontiguous spans would lead to a combinatorial explosion), while our model has no such restriction. 7 8 Previous Work Conclusion Most popular intent taxonomies such as ATIS Improvements in the complexity of conversations (Price, 1990) are domain-specific. Dialog Acts that a dialogue system can handle have put tremen(DA) (Stolcke et al., 2000) are more formalized dous pressure on NLU systems to capture fineand generalized versions of intents. The interna- grained and domain-specific information. Diffitional standard for DA annotations (Bunt et al., culty in the data generation process means the abil2010, 2012, 2016) defined the concept of commu- ity to share data across clients is critical. We define 219 intent features, a core set of general annotations, on intents that provide context and clarity on the exact nature of th"
2021.naacl-industry.27,W09-1119,0,0.0480731,"t in event extraction datasets like ACE 2005 (Consortium, 2005), but instead of just marking an event as “Asserted” or “Other”, our version of Modality distinguishes between different aspects of hypothetical events. 3 Modeling There are four different model types we explored for intent features that we detail below. However, before we can annotate an intent with a feature, we need to have an intent span. First, we describe our intent span extraction model whose predictions are used as intent spans. 3.1 Multi-Intent as Annotatable Spans as token level annotations with the IOBES tagging scheme (Ratinov and Roth, 2009). We used a standard BiLSTM-CRF architecture following Ma and Hovy (2016). Each input token is represented both as a character composition, by running a small convolutional neural network with a filter size of 3 over the characters and doing max-over-time pooling as in Dos Santos and Zadrozny (2014), and as a word embedding. We use the concatenation of multiple word embeddings, GloVe embeddings (Pennington et al., 2014), as well as 100 dimensional, in-domain embeddings trained in-house, following Lester et al. (2020a). The token sequence is then fed into an bidirectional LSTM (Graves et al., 2"
C10-2118,D09-1036,0,0.0226129,"Missing"
C10-2118,P02-1047,0,0.106414,"Missing"
C10-2118,J93-2004,0,0.03453,"tation of 624 tokens of AltLex in the PDTB. We turn to our analysis of these expressions in the next section. 3 What is found in AltLex? Several questions arise when considering the AltLex annotations. What kind of expressions are they? What can we learn from their syntax? Do they project discourse relations of a different sort than connectives? How can they be identified, both during manual annotation and automatically? To address these questions, we examined the AltLex annotation for annotated senses, and for common lexico-syntactic patterns extracted using alignment with the Penn Treebank (Marcus et al., 1993).4 3.1 Lexico-syntactic Characterization We found that we could partition AltLex annotation into three groups by (a) whether or not they belonged to one of the syntactic classes admitted as explicit connectives in the PDTB, and (b) whether the expression was frozen (ie, blocking free substitution, modification or deletion of any of its parts) or open-ended. The three groups are shown in Table 1 and discussed below. 4 The source texts of the PDTB come from the Penn Treebank (PTB) portion of the Wall Street Journal corpus. The PDTB corpus provides PTB tree alignments of all its text span annotat"
C10-2118,W09-3029,1,0.821637,"Missing"
C10-2118,P09-2004,0,0.00784831,"verbs, and prepositional phrases. Thus the literature presents lists of DRMs, which researchers try to make as complete as possible for their chosen language. In annotating lexicalized discourse relations of the Penn Discourse Treebank (Prasad et al., 2008), this same assumption drove the initial phase of annotation. A list of “explicit connectives” was collected from various sources and provided to annotators, who then searched for these expressions in the text and annotated them, along with their arguments and senses. The same assumption underlies methods for automatically identifying DRMs (Pitler and Nenkova, 2009). Since expressions functioning as DRMs can also have non-DRM functions, the task is framed as one of classifying given individual tokens as DRM or not DRM. In this paper, we argue that placing such syntactic and lexical restrictions on DRMs limits a proper understanding of discourse relations, which can be realized in other ways as well. For example, one should recognize that the instantiation (or exemplification) relation between the two sentences in Ex. (3) is explicitly signalled in the second sentence by the phrase Probably the most egregious example is, which is sufficient to express the"
C10-2118,P09-1077,0,0.0325156,"Missing"
C10-2118,W07-2314,0,0.011281,"ntra-clausal, he does not observe that verbalized discourse relations can hold across sentences as well, where a verb and one of its arguments function similarly to a discourse adverbial, and in the end, he does not provide a proposal for how to systematically identify these alternative realizations. Le Huong et al. (2003), in developing an algorithm for recognizing discourse relations, consider non-verbal realizations (called NP cues) in addition to verbal realizations (called VP cues). However, they provide only one example of such a cue (“the result”). Like Kibble (1999), Danlos (2006) and Power (2007) also focus only on identifying verbalizations of discourse relations, although they do consider cases where such relations hold across sentences. What has not been investigated in prior work is the basis for the alternation between connectives and AltLex’s, although there are several accounts of why a language may provide more than one connective that conveys the same relation. For example, the alternation in Dutch between dus (“so”), daardoor (“as a result”), and daarom (“that’s why”) is explained by Pander Maat and Sanders (2000) as having its basis in “subjectivity”. class expressions is p"
C10-2118,prasad-etal-2008-penn,1,0.419254,"tify explicit signals of discourse relations, exemplified in Ex. (1). To refer to all such signals, we use the term “discourse relation markers” (DRMs). Past research (e.g., (Halliday and Hasan, 1976; Martin, 1992; Knott, 1996), among others) has assumed that DRMs are frozen or fixed expressions from a few welldefined syntactic classes, such as conjunctions, adverbs, and prepositional phrases. Thus the literature presents lists of DRMs, which researchers try to make as complete as possible for their chosen language. In annotating lexicalized discourse relations of the Penn Discourse Treebank (Prasad et al., 2008), this same assumption drove the initial phase of annotation. A list of “explicit connectives” was collected from various sources and provided to annotators, who then searched for these expressions in the text and annotated them, along with their arguments and senses. The same assumption underlies methods for automatically identifying DRMs (Pitler and Nenkova, 2009). Since expressions functioning as DRMs can also have non-DRM functions, the task is framed as one of classifying given individual tokens as DRM or not DRM. In this paper, we argue that placing such syntactic and lexical restriction"
C10-2118,miltsakaki-etal-2004-penn,1,\N,Missing
C10-2118,D08-1021,0,\N,Missing
C16-2026,al-saif-markert-2010-leeds,0,0.0966919,"overview of the PDTB Framework and discusses the tool’s features, setup requirements and how it can also be used for adjudication. 1 Introduction In recent years, discourse relations have become a topic of some interest and there has in effect been a rise in the number of corpora annotated for discourse relations. Following the release of the Penn Discourse TreeBank (PDTB) in 2008 (Prasad et al., 2008), a number of comparable corpora have since adapted the PDTB framework (Prasad et al., 2014), including the Hindi Discourse Relation Bank (Oza et al., 2009), the Leeds Arabic Discourse TreeBank (Al-Saif and Markert, 2010), the Biomedical Discourse Relation Bank (Prasad et al., 2011), the Chinese Discourse TreeBank (Zhou and Xue, 2012), the Turkish Discourse Bank (Zeyrek et al., 2013), the discourse layer of the Prague Dependency Treebank 3.0 (Bejˇcek et al, 2013) and the TED-Multilingual Discourse Bank (TED-MDB) (Zeyrek et al., 2016). Groups starting new discourse annotation projects have sought an openly available resource to support their work. To address this for annotation in the PDTB framework, we have packaged an updated version of our annotation tool - the PDTB Annotator - for use by the research commun"
C16-2026,C14-2008,0,0.0367071,"Missing"
C16-2026,J05-1004,0,0.0819207,"Missing"
C16-2026,W16-1704,1,0.881334,"72) This work has been supported by the National Science Foundation under grants RI 1422186 and RI 1421067. It is licensed under a Creative Commons Attribution 4.0 International Licence. License details: http://creativecommons.org/ licenses/by/4.0/ 121 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations, pages 121–125, Osaka, Japan, December 11-17 2016. All relations are taken to have two arguments - Arg1 (shown in italics) and Arg2 (in bold). As per the revised argument-naming conventions in recent ongoing work on PDTB enrichment (Webber et al., 2016), the Arg2 in syntactically coordinated relations follows (i.e. is to the right of) Arg1, while the Arg2 in syntactically subordinated relations is (syntactically) subordinate to Arg1, regardless of textual order. Discourse relations are not always realized as Explicit connectives. In such cases, a connective is left to be inferred by the annotator, who lexically encodes this inferred relation. This is shown in (2), where a Reason relation between the two adjacent sentences is annotated with because as the Implicit connective: (2) Also unlike Mr. Ruder, Mr. Breeden appears to be in a position"
C16-2026,P12-1008,0,0.0196254,"ication. 1 Introduction In recent years, discourse relations have become a topic of some interest and there has in effect been a rise in the number of corpora annotated for discourse relations. Following the release of the Penn Discourse TreeBank (PDTB) in 2008 (Prasad et al., 2008), a number of comparable corpora have since adapted the PDTB framework (Prasad et al., 2014), including the Hindi Discourse Relation Bank (Oza et al., 2009), the Leeds Arabic Discourse TreeBank (Al-Saif and Markert, 2010), the Biomedical Discourse Relation Bank (Prasad et al., 2011), the Chinese Discourse TreeBank (Zhou and Xue, 2012), the Turkish Discourse Bank (Zeyrek et al., 2013), the discourse layer of the Prague Dependency Treebank 3.0 (Bejˇcek et al, 2013) and the TED-Multilingual Discourse Bank (TED-MDB) (Zeyrek et al., 2016). Groups starting new discourse annotation projects have sought an openly available resource to support their work. To address this for annotation in the PDTB framework, we have packaged an updated version of our annotation tool - the PDTB Annotator - for use by the research community. Some of the potential benefits of using the PDTB Annotator include the following: i) the tool is Java-based an"
C16-2026,prasad-etal-2008-penn,1,\N,Missing
C16-2026,W03-2120,0,\N,Missing
hastie-etal-2002-automatic,H92-1005,0,\N,Missing
hastie-etal-2002-automatic,H92-1009,0,\N,Missing
hastie-etal-2002-automatic,W02-0221,1,\N,Missing
hastie-etal-2002-automatic,P01-1066,1,\N,Missing
hastie-etal-2002-automatic,bonneau-maynard-etal-2000-predictive,0,\N,Missing
I08-7010,I08-2099,1,0.820437,"Missing"
I08-7010,W05-0312,0,0.26675,"e connectives. 1 (1) The federal government suspended sales of U.S. savings bonds because Congress hasn’t lifted the ceiling on government debt. One of the questions that arises is how the PDTB style annotation can be carried over to languages other than English. It may prove to be a challenge cross-linguistically, as the guidelines and methodology appropriate for English may not apply as well or directly to other languages, especially when they differ greatly in syntax and morphology. To date, cross-linguistic investigations of connectives in this direction have been carried out for Chinese (Xue, 2005) and Turkish (Deniz and Webber, 2008). This paper explores discourse relation annotation in Hindi, a language with rich morphology and free word order. We describe our study of “explicit connectives” in a small corpus of Hindi texts, discussing them from two perspectives. First, we consider the type and distribution of Hindi connectives, proposing to annotate a wider range Introduction An increasing interest in human language technologies such as textual summarization, question answering, natural language generation has recently led to the development of several discourse annotation projects a"
I08-7010,I08-7009,0,0.404376,"Missing"
I08-7010,J03-4002,1,\N,Missing
J14-4007,C12-1163,0,0.0216544,"s for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDTB-Group 2008) and presents what we have learned since release of the corpus in 2008. Secondly, for those researchers who have used th"
J14-4007,W12-4703,0,0.0192018,"s for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDTB-Group 2008) and presents what we have learned since release of the corpus in 2008. Secondly, for those researchers who have used th"
J14-4007,W13-2610,0,0.105535,"Missing"
J14-4007,W01-1605,0,0.526226,"Missing"
J14-4007,F12-2042,0,0.10177,"nd Xue (in press)), the Turkish Discourse Bank or TDB (Zeyrek et al. 2008, 2009; Aktas¸, Bozs¸ahin, and Zeyrek 2010; Zeyrek et al. 2010; Demirsahin et al. 2013; Zeyrek et al. 2013), the Hindi Discourse Relation Bank (Oza et al. 2009; Kolachina et al. 2012; Sharma et al. 2013), and the Prague Discourse TreeBank, or PDiT (Mladov´a, Zik´anov´a, and Hajiˇcov´a 2008; J´ınov´a, M´ırovsky, ´ and Pol´akov´a 2012; Rysov´a 2012; Pol´akov´a et al. 2013), now part of the Prague Dependency TreeBank, version 3.0, PDT 3.0 (Bejˇcek et al. 2013). (A comparable discourse treebank is being developed for French (Danlos et al. 2012), but it has not yet been released and the information needed to compare it to the other corpora in Table 4 is not available.) Although these comparable corpora differ in ways to be discussed subsequently, they all adhere to the key ideas of PDTB annotation (Section 2) in being neutral to any discourse structure beyond the argument structure of individual discourse relations and in grounding discourse relations in lexical expressions. Where they annotate implicit discourse relations (Table 4), these comparable corpora follow the PDTB in annotating an inferred lexical grounding. All of the corp"
J14-4007,W13-2315,0,0.0301067,"Missing"
J14-4007,W05-0305,1,0.280638,"Missing"
J14-4007,I11-1120,0,0.0122955,"e outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the ai"
J14-4007,ghosh-etal-2012-improving,0,0.0782143,"Missing"
J14-4007,J93-3003,0,0.567299,"er, and Joshi 2006). This set was then enlarged as new connectives were found in the WSJ corpus itself. Also identified during this phase were productive modifiers of explicit connectives such as apparently, at least partly, in large part, even, only, and so on, which were then annotated as connective modifiers.2 What were not taken to be discourse connectives were adverbial cue phrases, including sentence-initial Now (Example (3)), Well (Example (4)), So (Example (5)), and OK (Example (6)), because they signal topic changes such as the beginning of a subtopic or a return to a previous topic (Hirschberg and Litman 1993), rather than relating particular discourse elements. (3) Now why, you have to ask yourself, would intelligent beings haul a bunch of rocks around the universe? [wsj 0550] (4) Well, mankind can rest easier for now. [wsj 1272] (5) So, OK kids, everybody on stage for “Carry On Trading.” [wsj 2402] (6) When Mr. Jacobson walked into the office at 7:30 a.m. EDT, he announced: “OK, buckle up.” [wsj 1171] We did not intend to annotate as discourse connectives pragmatic markers such as actually and in fact, which serve to signal the conversational role of the speaker’s matrix utterance—specifically, t"
J14-4007,W12-4704,0,0.0298796,"Missing"
J14-4007,kolachina-etal-2012-evaluation,1,0.889811,"Missing"
J14-4007,J93-2004,0,0.0567604,"Missing"
J14-4007,P11-3009,0,0.0119687,"n both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDT"
J14-4007,W12-0117,0,0.0254165,"age technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDTB-Group 2008) and presents wha"
J14-4007,W13-3303,1,0.724081,"istics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDTB-Group 2008) and presents what we have learned since"
J14-4007,W04-2703,1,0.292051,"Missing"
J14-4007,mladova-etal-2008-sentence,0,0.117353,"Missing"
J14-4007,J88-2003,0,0.713677,"Missing"
J14-4007,J05-1004,0,0.126043,"Missing"
J14-4007,pareti-2012-database,0,0.0241257,"cessary for the discourse relation in Example (20).5 (19) Defense contractors “cannot continue to get contracts on that basis,” said Howard Rubel, an analyst with C.J. Lawrence, Morgan Grenfell Inc. in New York. (implicit=because) “The pain is too great.” [wsj 0673] 5 The PDTB also annotates attribution relations, capturing their textual signal and semantic features over each discourse relation and each of its arguments. For a full description of attribution and its annotation, the reader is referred to Prasad et al. (2007). Attribution is now being annotated as a separate layer over the WSJ (Pareti 2012), building on the PDTB attribution scheme, but aiming to capture the phenomena more comprehensively than in the PDTB. 929 Computational Linguistics (20) Volume 40, Number 4 Mr. Asman is also annoyed that Mr. Castro has resisted collaboration with U.S. officials, even though by his own account that collaboration has been devised essentially as a mechanism for acts directly hostile to the Cuban regime, such as facilitating defections. [wsj 1416] Attribution differs from supplementary information in that, when its polarity is negative, it can interact with discourse relations. (Sup has no such in"
J14-4007,D13-1094,0,0.452089,"ns (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDTB-Group 2008) and presents what we have learned since release of the corpus in 2008. Secondly, for those researchers who have used the PDTB, Section 3 aims to point out significant featur"
J14-4007,P09-2004,0,0.0946201,"it=as a result) By 1997, almost all remaining uses of cancer-causing asbestos will be outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create compar"
J14-4007,C08-2022,1,0.898974,"of asbestos. (implicit=as a result) By 1997, almost all remaining uses of cancer-causing asbestos will be outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of an"
J14-4007,I13-1011,0,0.176202,"Missing"
J14-4007,prasad-etal-2008-penn,1,0.9379,"on that may have weakened previous results or the performance of decision procedures induced from the data; (3) to explain variations seen in the annotation of comparable resources in other languages and genres, which should allow developers of future comparable resources to recognize whether the variations are relevant to them; and (4) to enumerate and explain relationships between PDTB annotation and complementary annotation of other linguistic phenomena. The paper draws on work done by ourselves and others since the corpus was released. 1. Introduction The Penn Discourse TreeBank, or PDTB (Prasad et al. 2008; PDTB-Group 2008) is the largest manually annotated resource of discourse relations. This annotation has been added to the million-word Wall Street Journal portion of the Penn Treebank (PTB) corpus ∗ Department of Health Informatics and Administration, University of Wisconsin-Milwaukee, 2025 E. Newport Ave (NWQB), Milwaukee WI 53211. E-mail: prasadr@uwm.edu. ∗∗ School of Informatics, University of Edinburgh, 10 Crichton Street (IF4.29), Edinburgh UK EH8 9AB. E-mail: bonnie.webber@ed.ac.uk. † Institute for Research in Cognitive Science, University of Pennsylvania, 3401 Walnut Street (Suite 400"
J14-4007,prasad-etal-2010-exploiting,1,0.857193,"aining uses of cancer-causing asbestos will be outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages"
J14-4007,C10-2118,1,0.673052,"aining uses of cancer-causing asbestos will be outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages"
J14-4007,W05-0302,0,0.0584237,"Missing"
J14-4007,rysova-2012-alternative,0,0.0860411,"Missing"
J14-4007,W13-0124,1,0.948187,"ve begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDTB-Group 2008) and presents what we have learned since"
J14-4007,D07-1010,0,0.00884648,"ction Agency imposed a gradual ban on virtually all uses of asbestos. (implicit=as a result) By 1997, almost all remaining uses of cancer-causing asbestos will be outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Othe"
J14-4007,W05-0312,0,0.038729,"Missing"
J14-4007,W09-3006,0,0.0495351,"Missing"
J14-4007,W10-1844,0,0.0503983,"Missing"
J14-4007,P12-1008,0,0.0294487,"nature and the sources of this variation, so that people contemplating development of comparable resources in additional languages and/or genres will recognize variation that is appropriate to their situation, while avoiding unnecessary variation that prevents inter-operability of these comparable corpora (Bunt, Prasad, and Joshi 2012). Table 4 identifies the corpora we will discuss and the extent of their current annotation: the BioDRB (Prasad et al. 2011), the Leeds Arabic Discourse TreeBank, or LADTB (Al-Saif and Markert 2010, 2011; Al-Saif 2012), the Chinese Discourse TreeBank (Xue 2005; Zhou and Xue 2012; Zhou and Xue (in press)), the Turkish Discourse Bank or TDB (Zeyrek et al. 2008, 2009; Aktas¸, Bozs¸ahin, and Zeyrek 2010; Zeyrek et al. 2010; Demirsahin et al. 2013; Zeyrek et al. 2013), the Hindi Discourse Relation Bank (Oza et al. 2009; Kolachina et al. 2012; Sharma et al. 2013), and the Prague Discourse TreeBank, or PDiT (Mladov´a, Zik´anov´a, and Hajiˇcov´a 2008; J´ınov´a, M´ırovsky, ´ and Pol´akov´a 2012; Rysov´a 2012; Pol´akov´a et al. 2013), now part of the Prague Dependency TreeBank, version 3.0, PDT 3.0 (Bejˇcek et al. 2013). (A comparable discourse treebank is being developed for"
J14-4007,C10-2172,0,0.133404,"ng asbestos will be outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). W"
J14-4007,W10-1832,0,\N,Missing
J14-4007,D11-1068,0,\N,Missing
J14-4007,W09-3029,1,\N,Missing
J14-4007,al-saif-markert-2010-leeds,0,\N,Missing
J14-4007,W04-0211,0,\N,Missing
J14-4007,W11-1401,0,\N,Missing
K15-2001,P09-1075,0,0.0149588,"course of the sixteen CoNLL shared 1 http://www.seas.upenn.edu/˜pdtb 1 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative a"
K15-2001,P12-1007,0,0.0215894,"omputational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the com"
K15-2001,P14-1048,0,0.0131111,"tational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “d"
K15-2001,W12-1622,0,0.047129,"anguage Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain cruci"
K15-2001,P14-1002,0,0.0686762,"re generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “deep” representation learn"
K15-2001,K15-2004,0,0.0849821,"Missing"
K15-2001,P13-2013,0,0.189614,"uly 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard”"
K15-2001,P13-1047,0,0.0359978,"ociation for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based lear"
K15-2001,W14-4320,0,0.0531135,"of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “deep” representation learning techniques. The re"
K15-2001,K15-2006,0,0.17442,"Missing"
K15-2001,P14-1003,0,0.00827391,"or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “deep” representation learning techniques. The rest of this overvi"
K15-2001,K15-2007,0,0.107459,"Missing"
K15-2001,D09-1036,1,0.729252,"Missing"
K15-2001,P14-5010,0,0.00276217,", 2014). In addition, to be competitive in the discourse parsing task, one also has to process the data with syntactic and possibly semantic parsers, which may also be trained on data that is outside the training set. As a compromise, therefore, we allowed participants to use the following linguistic resources in the closed track, other than the trainBrown clusters VerbNet Sentiment lexicon Word embeddings (word2vec) • Phrase structure parses (predicted using the Berkeley parser (Petrov and Klein, 2007)) • Dependency parses (converted from phrase structure parses using the Stanford converter (Manning et al., 2014)) As it turned out, all of the teams this year chose to participate in the closed track. 4.2 Evaluation Platform: TIRA We use a new web service called TIRA as the platform for system evaluation (Gollub et al., 2012; Potthast et al., 2014). Traditionally, participating teams were asked to manually run their system on the blind test set without the gold standard labels, and submit the output for evaluation. This year, however, we shifted this evaluation paradigm, asking participants to deploy their systems on a remote virtual machine, and to use the TIRA web platform (tira.io) to run their syste"
K15-2001,W11-1901,1,0.0252812,"Nianwen Xue∗ Hwee Tou Ng† Sameer Pradhan‡ Rashmi Prasad3 Christopher Bryant† Attapol T. Rutherford∗ ∗ Brandeis University xuen,tet@brandeis.edu † National University of Singapore nght,bryant@comp.nus.edu.sg ‡ Boulder Language Technologies pradhan@bltek.com 3 University of Wisconsin-Milwaukee prasadr@uwm.edu Abstract tasks organized over the past two decades, progressing gradually to tackle phenomena at the word and phrase level phenomena and then the sentence and extra-sentential level, it was only very recently that discourse level processing has been addressed, with coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012). The 2015 shared task takes the community a step further in that direction, with the potential to impact scores of richer language applications (Webber et al., 2012). Given an English newswire text as input, the goal of the shared task is to detect and categorize discourse relations between discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks ("
K15-2001,J93-2004,0,0.058942,"if present; 2. Identify the spans of text that serve as the two arguments for each relation; 3. Label the arguments as (Arg1 or Arg2) to indicate the order of the arguments; 4. Predict the sense of the discourse relation (e.g., “Cause”, “Condition”, “Contrast”). 3 Data 3.1 Training and Development The training data for the CoNLL-2015 Shared Task was adapted from the Penn Discourse TreeBank 2.0. (PDTB-2.0.) (Prasad et al., 2008; Prasad et al., 2014), annotated over the one million word Wall Street Journal (WSJ) corpus that has also been annotated with syntactic structures (the Penn TreeBank) (Marcus et al., 1993) and propositions (the Proposition Bank) (Palmer et al., 2005). The PDTB annotates discourse relations that hold between eventualities and propositions mentioned in text. Following a lexically grounded approach to annotation, the PDTB annotates relations realized explicitly by discourse connectives drawn from syntactically well-defined classes, as well as implicit relations between adjacent sentences when no explicit connective exists to relate the two. A limited but well-defined set of implicit relations are also annotated within sentences. Arguments of relations are annotated in each case, f"
K15-2001,W12-4501,1,0.544745,"Ng† Sameer Pradhan‡ Rashmi Prasad3 Christopher Bryant† Attapol T. Rutherford∗ ∗ Brandeis University xuen,tet@brandeis.edu † National University of Singapore nght,bryant@comp.nus.edu.sg ‡ Boulder Language Technologies pradhan@bltek.com 3 University of Wisconsin-Milwaukee prasadr@uwm.edu Abstract tasks organized over the past two decades, progressing gradually to tackle phenomena at the word and phrase level phenomena and then the sentence and extra-sentential level, it was only very recently that discourse level processing has been addressed, with coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012). The 2015 shared task takes the community a step further in that direction, with the potential to impact scores of richer language applications (Webber et al., 2012). Given an English newswire text as input, the goal of the shared task is to detect and categorize discourse relations between discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012; Webber et"
K15-2001,W04-2703,1,0.817359,"Missing"
K15-2001,W15-0210,1,0.806958,"shared task takes the community a step further in that direction, with the potential to impact scores of richer language applications (Webber et al., 2012). Given an English newswire text as input, the goal of the shared task is to detect and categorize discourse relations between discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012; Webber et al., 2012; Prasad and Bunt, 2015). For example, the RST-DT Corpus (Carlson et al., 2003) is based on the Rhetorical Structure Theory of Mann and Thompson (1988) and produces a complete treestructured RST analysis of a text, whereas the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014) provides a shallow representation of discourse structure, in that each discourse relation is annotated independently of other discourse relations, leaving room for a high-level analysis that may attempt to connect them. For the CoNLL-2015 shared task, we chose to use the PDTB, as it is currently the largest data set annot"
K15-2001,K15-2010,0,0.14262,"Missing"
K15-2001,prasad-etal-2008-penn,1,0.768574,"course relations between discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012; Webber et al., 2012; Prasad and Bunt, 2015). For example, the RST-DT Corpus (Carlson et al., 2003) is based on the Rhetorical Structure Theory of Mann and Thompson (1988) and produces a complete treestructured RST analysis of a text, whereas the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014) provides a shallow representation of discourse structure, in that each discourse relation is annotated independently of other discourse relations, leaving room for a high-level analysis that may attempt to connect them. For the CoNLL-2015 shared task, we chose to use the PDTB, as it is currently the largest data set annotated with discourse relations.1 The CoNLL-2015 Shared Task is on Shallow Discourse Parsing, a task focusing on identifying individual discourse relations that are present in a natural language text. A discourse relation can be expressed explicitly or imp"
K15-2001,J05-1004,0,0.00719659,"wo arguments for each relation; 3. Label the arguments as (Arg1 or Arg2) to indicate the order of the arguments; 4. Predict the sense of the discourse relation (e.g., “Cause”, “Condition”, “Contrast”). 3 Data 3.1 Training and Development The training data for the CoNLL-2015 Shared Task was adapted from the Penn Discourse TreeBank 2.0. (PDTB-2.0.) (Prasad et al., 2008; Prasad et al., 2014), annotated over the one million word Wall Street Journal (WSJ) corpus that has also been annotated with syntactic structures (the Penn TreeBank) (Marcus et al., 1993) and propositions (the Proposition Bank) (Palmer et al., 2005). The PDTB annotates discourse relations that hold between eventualities and propositions mentioned in text. Following a lexically grounded approach to annotation, the PDTB annotates relations realized explicitly by discourse connectives drawn from syntactically well-defined classes, as well as implicit relations between adjacent sentences when no explicit connective exists to relate the two. A limited but well-defined set of implicit relations are also annotated within sentences. Arguments of relations are annotated in each case, following the minimality principle for selecting all and only t"
K15-2001,J14-4007,1,0.868703,"een discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012; Webber et al., 2012; Prasad and Bunt, 2015). For example, the RST-DT Corpus (Carlson et al., 2003) is based on the Rhetorical Structure Theory of Mann and Thompson (1988) and produces a complete treestructured RST analysis of a text, whereas the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014) provides a shallow representation of discourse structure, in that each discourse relation is annotated independently of other discourse relations, leaving room for a high-level analysis that may attempt to connect them. For the CoNLL-2015 shared task, we chose to use the PDTB, as it is currently the largest data set annotated with discourse relations.1 The CoNLL-2015 Shared Task is on Shallow Discourse Parsing, a task focusing on identifying individual discourse relations that are present in a natural language text. A discourse relation can be expressed explicitly or implicitly, and takes two"
K15-2001,W12-1614,0,0.349783,"ared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the rela"
K15-2001,E14-1068,1,0.369296,"s. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “deep” representation learning techniques. The rest of this overview paper is structured as follows. In Section"
K15-2001,N07-1051,0,0.00516358,"like discourse parsing where external resources such as Brown clusters have proved to be useful (Rutherford and Xue, 2014). In addition, to be competitive in the discourse parsing task, one also has to process the data with syntactic and possibly semantic parsers, which may also be trained on data that is outside the training set. As a compromise, therefore, we allowed participants to use the following linguistic resources in the closed track, other than the trainBrown clusters VerbNet Sentiment lexicon Word embeddings (word2vec) • Phrase structure parses (predicted using the Berkeley parser (Petrov and Klein, 2007)) • Dependency parses (converted from phrase structure parses using the Stanford converter (Manning et al., 2014)) As it turned out, all of the teams this year chose to participate in the closed track. 4.2 Evaluation Platform: TIRA We use a new web service called TIRA as the platform for system evaluation (Gollub et al., 2012; Potthast et al., 2014). Traditionally, participating teams were asked to manually run their system on the blind test set without the gold standard labels, and submit the output for evaluation. This year, however, we shifted this evaluation paradigm, asking participants t"
K15-2001,K15-2012,0,0.0750505,"Missing"
K15-2001,C08-2022,0,0.230737,"arsing (SDP). In the course of the sixteen CoNLL shared 1 http://www.seas.upenn.edu/˜pdtb 1 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques o"
K15-2001,P09-1077,0,0.665554,"seas.upenn.edu/˜pdtb 1 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared tas"
K15-2001,N09-1064,0,0.0519034,"Missing"
K15-2001,K15-2002,0,0.195871,"Standard “shallow” architectures typically make use of discrete features while neural networks generally use continuous real-valued features such as word and paragraph embeddings. For discourse connective and argument extraction, token level features extracted from a fixed window centered on the target word token are generally used, and so are features extracted from syntactic parses. Distributional representations such as Brown clusters have generally been used to determine the senses (Chiarcos and Schenk, 2015; Devi et al., 2015; Kong et al., 2015; Song et al., 2015; Stepanov et al., 2015; Wang and Lan, 2015; Wang et al., 2015; Yoshida et al., 2015), although one team also used them in the sequence labeling task for argument extraction (Nguyen et al., 2015). Additional resources used by some systems for sense determination include word embeddings (Chiarcos and Schenk, 2015; Wang et al., 2015), VerbNet classes (Devi et al., 2015; Kong et al., 2015), and the MPQA polarity lexicon (Devi et al., 2015; Kong et al., 2015; Wang and Lan, 2015). Table 4 provides a summary of the different approaches. 6 Results Table 5 shows the performance of all participating systems across the three test evaluation sets"
K15-2001,C12-1168,0,0.04565,"c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and"
K15-2001,K15-2014,0,0.132851,"Missing"
K15-2001,K15-2015,0,0.0995344,"Missing"
K15-2001,C10-2172,0,0.24464,"nth Conference on Computational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an idea"
K15-2001,W01-1605,0,\N,Missing
K15-2001,K15-2003,0,\N,Missing
kolachina-etal-2012-evaluation,tonelli-etal-2010-annotation,1,\N,Missing
kolachina-etal-2012-evaluation,W04-2703,1,\N,Missing
kolachina-etal-2012-evaluation,mladova-etal-2008-sentence,0,\N,Missing
kolachina-etal-2012-evaluation,W09-3029,1,\N,Missing
kolachina-etal-2012-evaluation,al-saif-markert-2010-leeds,0,\N,Missing
kolachina-etal-2012-evaluation,W09-3036,1,\N,Missing
kolachina-etal-2012-evaluation,prasad-etal-2008-penn,1,\N,Missing
kolachina-etal-2012-evaluation,W11-0414,1,\N,Missing
kolachina-etal-2012-evaluation,I08-7009,0,\N,Missing
kolachina-etal-2012-evaluation,P12-1008,0,\N,Missing
kolachina-etal-2012-evaluation,I08-2099,1,\N,Missing
miltsakaki-etal-2004-penn,kingsbury-palmer-2002-treebank,0,\N,Missing
miltsakaki-etal-2004-penn,J93-2004,0,\N,Missing
miltsakaki-etal-2004-penn,W04-2703,1,\N,Missing
miltsakaki-etal-2004-penn,J03-4002,1,\N,Missing
miltsakaki-etal-2004-penn,P02-1045,0,\N,Missing
P02-1049,H01-1028,0,0.0216091,"Missing"
P02-1049,P98-1122,0,0.0638596,"Missing"
P02-1049,P99-1040,1,0.891002,"Missing"
P02-1049,W02-0221,1,0.829319,"us labelling was done because we found that systems had augmented their inventory of named entities and utterance patterns from 2000 to 2001, and these were not accounted for by the 2000 tagger database. For the extension, we collected a fresh set of vocabulary lists from the sites and augmented the pattern database with additional 800 labelled utterance patterns. We also implemented a contextual rule-based postprocessor that takes any remaining unlabelled utterances and attempts to label them by looking at their surrounding DATE labels. More details about the extended tagger can be found in (Prasad and Walker, 2002). On the 2001 corpus, we were able to label 98.4 of the data. A hand evaluation of 10 randomly selected dialogues from each system shows that we achieved a classification accuracy of 96 at the utterance level. For User Satisfaction Prediction, we found that the distribution of DATE acts were better captured by using the frequency normalized over the total number of dialogue acts. In addition to these unigram proportions, the bigram frequencies of the DATE dialogue acts were also calculated. In the following two sections, we discuss which DATE labels are discriminatory for predicting Task Compl"
P02-1049,P01-1066,1,0.851463,"ialogues that provide training data for further system development. As a spoken dialogue system is developed, it is first tested as a prototype, then fielded in a limited setting, possibly running with human supervision (Gorin et al., 1997), and finally deployed. At each stage from research prototype to deployed commercial application, the system is constantly undergoing further development. When a system is prototyped in house or first tested in the field, human subjects are often paid to use the system and give detailed feedback on task completion and user satisfaction (Baggia et al., 1998; Walker et al., 2001). Even when a system is deployed, it often keeps evolving, either because customers want to do different things with it, or because new tasks arise out of developments in the underlying application. However, real customers of a deployed system may not be willing to give detailed feedback. Thus, the widespread use of these systems has created a data management and analysis problem. System designers need to constantly track system performance, identify problems, and fix them. System modules such as automatic speech recognition (ASR), natural language understanding (NLU) and dialogue management m"
P02-1049,C98-1117,0,\N,Missing
P04-1011,A00-2023,0,0.0311426,"es portability across domains and dialog contexts by using general rules for each generation module. However, the quality of the output for a particular domain, or a particular dialog context, may be inferior to that of a templatebased system unless domain-specific rules are developed or general rules are tuned for the particular domain. Furthermore, full NLG may be too slow for use in dialog systems. A third, more recent, approach is trainable generation: techniques for automatically training NLG modules, or hybrid techniques that adapt NLG modules to particular domains or user groups, e.g. (Langkilde, 2000; Mellish, 1998; Walker, Rambow and Rogati, 2002). Open questions about the trainable approach include (1) whether the output quality is high enough, and (2) whether the techniques work well across domains. For example, the training method used in SPoT (Sentence Planner Trainable), as described in (Walker, Rambow and Rogati, 2002), was only shown to work in the travel domain, for the information gathering phase of the dialog, and with simple content plans involving no rhetorical relations. This paper describes trainable sentence planning for information presentation in the MATCH (Multimodal Ac"
P04-1011,P02-1048,1,0.318096,"d Rogati, 2002). Open questions about the trainable approach include (1) whether the output quality is high enough, and (2) whether the techniques work well across domains. For example, the training method used in SPoT (Sentence Planner Trainable), as described in (Walker, Rambow and Rogati, 2002), was only shown to work in the travel domain, for the information gathering phase of the dialog, and with simple content plans involving no rhetorical relations. This paper describes trainable sentence planning for information presentation in the MATCH (Multimodal Access To City Help) dialog system (Johnston et al., 2002). We provide evidence that the trainable approach is feasible by showing (1) that the training technique used for SPoT can be extended to a new domain (restaurant information); (2) that this technique, previously used for informationgathering utterances, can be used for information presentations, namely recommendations and comparisons; and (3) that the quality of the output is comparable to that of a template-based generator previously developed and experimentally evaluated with MATCH users (Walker et al., 2002; Stent et al., 2002). Section 2 describes SPaRKy (Sentence Planning with Rhetorical"
P04-1011,A97-1039,0,0.0665551,"ts a set of text plan trees (tp-trees), consisting of a set of speech acts to be communicated and the rhetorical relations that hold between them. For example, the two tp-trees in Figure 6 are generated for the content plan in Figure 2. Sentence plans such as alternative 25 in Figure 4 are avoided; it is clearly worse than alternatives 12, 13 and 20 since it neither combines information based on a restaurant entity (e.g Babbo) nor on an attribute (e.g. decor). The top ranked sentence plan output by the SPR is input to the RealPro surface realizer which produces a surface linguistic utterance (Lavoie and Rambow, 1997). A prosody assignment module uses the prior levels of linguistic representation to determine the appropriate prosody for the utterance, and passes a markedup string to the text-to-speech module. Sentence Plan Generation As in SPoT, the basis of the SPG is a set of clause-combining operations that operate on tptrees and incrementally transform the elementary predicate-argument lexico-structural representations (called DSyntS (Melcuk, 1988)) associated with the speech-acts on the leaves of the tree. The operations are applied in a bottom-up left-to-right fashion and the resulting representation"
P04-1011,W98-1411,0,0.0360157,"Missing"
P04-1011,A92-1006,0,0.089344,"tructuring phase, called infer, which holds for combinations of speech acts for which there is no rhetorical relation expressed in the content plan, as in (Marcu, 1997). By explicitly representing the discourse structure of the information presentation, we can generate information presentations with considerably more internal complexity than those generated in (Walker, Rambow and Rogati, 2002) and eliminate those that violate certain coherence principles, as described in Section 2. The clause-combining operations are general operations similar to aggregation operations used in other research (Rambow and Korelsky, 1992; Danlos, 2000). The operations and the 1 Although the probability distribution here is handcrafted based on assumed preferences for operations such as merge, relative-clause and with-reduction, it might also be possible to learn this probability distribution from the data by training in two phases. elaboration infer nucleus:&lt;1&gt;assert-com-list_exceptional contrast contrast nucleus:&lt;2&gt;assert-com-decor nucleus:&lt;4&gt;assert-com-service nucleus:&lt;3&gt;assert-com-decor contrast nucleus:&lt;6&gt;assert-com-cuisine nucleus:&lt;5&gt;assert-com-service nucleus:&lt;7&gt;assert-com-cuisine elaboration nucleus:&lt;1&gt;assert-com-list_"
P04-1011,P01-1056,1,0.92581,"Missing"
P04-1011,W02-2110,0,0.123126,"mation presentation in the MATCH (Multimodal Access To City Help) dialog system (Johnston et al., 2002). We provide evidence that the trainable approach is feasible by showing (1) that the training technique used for SPoT can be extended to a new domain (restaurant information); (2) that this technique, previously used for informationgathering utterances, can be used for information presentations, namely recommendations and comparisons; and (3) that the quality of the output is comparable to that of a template-based generator previously developed and experimentally evaluated with MATCH users (Walker et al., 2002; Stent et al., 2002). Section 2 describes SPaRKy (Sentence Planning with Rhetorical Knowledge), an extension of SPoT that uses rhetorical relations. SPaRKy consists of a randomized sentence plan generator (SPG) and a trainable sentence plan ranker (SPR); these are described in Sections 3 strategy:recommend items: Chanpen Thai relations:justify(nuc:1;sat:2); justify(nuc:1;sat:3); justify(nuc:1;sat:4) content: 1. assert(best(Chanpen Thai)) 2. assert(has-att(Chanpen Thai, decor(decent))) 3. assert(has-att(Chanpen Thai, service(good)) 4. assert(has-att(Chanpen Thai, cuisine(Thai))) Figure 1: A co"
P04-1011,P87-1022,0,\N,Missing
P06-1034,W02-1022,0,0.0412888,"Missing"
P06-1034,W02-2103,0,\N,Missing
P06-1034,W98-1428,0,\N,Missing
P06-1034,J02-4007,0,\N,Missing
P06-1034,H05-1042,0,\N,Missing
P06-1034,W00-0306,0,\N,Missing
P06-1034,N06-1046,0,\N,Missing
P06-1034,C02-1138,1,\N,Missing
P06-1034,H05-2017,0,\N,Missing
P06-1034,H05-1043,0,\N,Missing
P06-1034,H92-1022,0,\N,Missing
P06-1034,A92-1021,0,\N,Missing
P06-1034,P01-1008,0,\N,Missing
P06-1034,P06-2059,0,\N,Missing
P06-1034,P07-1063,1,\N,Missing
P06-1034,N03-1003,0,\N,Missing
P06-1034,P01-1056,1,\N,Missing
P06-1034,P04-1011,1,\N,Missing
P06-1034,J02-3001,0,\N,Missing
P06-1034,P02-1053,0,\N,Missing
P06-1034,P05-1015,0,\N,Missing
P06-1034,P87-1023,0,\N,Missing
P06-1034,W04-2302,0,\N,Missing
P06-1034,N04-1041,0,\N,Missing
P06-1034,H01-1047,0,\N,Missing
P06-1034,A97-1039,0,\N,Missing
P06-1034,W06-1650,0,\N,Missing
prasad-etal-2008-penn,W98-0315,1,\N,Missing
prasad-etal-2008-penn,J93-2004,0,\N,Missing
prasad-etal-2008-penn,W04-2703,1,\N,Missing
prasad-etal-2008-penn,W06-0305,1,\N,Missing
prasad-etal-2008-penn,W05-0305,1,\N,Missing
prasad-etal-2008-penn,W01-1605,0,\N,Missing
prasad-etal-2008-penn,J03-4002,1,\N,Missing
prasad-etal-2008-penn,J05-1004,0,\N,Missing
prasad-etal-2010-exploiting,poesio-artstein-2008-anaphoric,0,\N,Missing
prasad-etal-2010-exploiting,J97-1003,0,\N,Missing
prasad-etal-2010-exploiting,W05-0305,1,\N,Missing
prasad-etal-2010-exploiting,C00-1031,0,\N,Missing
prasad-etal-2010-exploiting,J95-2003,1,\N,Missing
prasad-etal-2010-exploiting,P09-1076,1,\N,Missing
prasad-etal-2010-exploiting,prasad-etal-2008-penn,1,\N,Missing
prasad-etal-2010-exploiting,W04-2322,0,\N,Missing
S13-2110,N03-1033,0,\N,Missing
tonelli-etal-2010-annotation,pareti-prodanof-2010-annotating,0,\N,Missing
tonelli-etal-2010-annotation,J93-2004,0,\N,Missing
tonelli-etal-2010-annotation,mladova-etal-2008-sentence,0,\N,Missing
tonelli-etal-2010-annotation,W05-0312,0,\N,Missing
tonelli-etal-2010-annotation,W09-3029,1,\N,Missing
tonelli-etal-2010-annotation,W09-0505,1,\N,Missing
tonelli-etal-2010-annotation,prasad-etal-2008-penn,1,\N,Missing
tonelli-etal-2010-annotation,I08-7009,0,\N,Missing
W02-0221,P98-1052,0,0.0660896,"Missing"
W02-0221,hastie-etal-2002-automatic,1,0.82665,"are available. 1 Introduction Recent research on dialogue is based on the assumption that dialogue acts provide a useful way of characterizing dialogue behaviors in both humanhuman (HH) and human-computer (HC) dialogue (Isard and Carletta, 1995; Shriberg et al., 2000; Di Eugenio et al., 1998; Cattoni et al., 2001). Previous research has used dialogue act tagging for tasks such as improving recognition performance (Shriberg et al., 2000), identifying important parts of a dialogue (Finke et al., 1998), evaluating and comparing spoken dialogue systems (Walker et al., 2001c; Cattoni et al., 2001; Hastie et al., 2002), as a constraint on nominal expression generation (Jordan, 2000), and for comparing HH to HC dialogues (Doran et al., 2001). Our work builds directly on the previous application of the DATE (Dialogue Act Tagging for Evaluation) tagging scheme to the evaluation and comparison of DARPA Communicator dialogues. The hypothesis underlying the use of dialogue act tagging in spoken dialogue evaluation is that a system’s dialogue behaviors have a strong effect on its usability. Because Communicator systems have unique dialogue strategies, and a unique way of representing and achieving particular commu"
W02-0221,P98-2188,0,0.0727287,"me domain with high accuracy; (2) A DATE tagger trained on data from an earlier version of the system only achieves moderate accuracy on a later version of the system without a small amount labelled training data from that later version; (3) Labelled training data from HC dialogues can improve the performance of a DATE tagger for HH dialogue when only a small amount of HH training data is available. Previous work has also reported results for dialogue act taggers, using similar features to those we use, with accuracies ranging from 62 to 75 (Reithinger and Klesen, 1997; Shriberg et al., 2000; Samuel et al., 1998). Our best accuracy for the HC data is 98 . The best performance for the HH corpus is 76 accuracy for the cross-validation study using only HH data. However, accuracies reported for previous work are not directly comparable to ours for several reasons. First, some of our results concern labelling the system side of utterances in HC dialogues for the purpose of automatic evaluation of system performance. It is much easier to develop a high accuracy tagger for HC dialogue than it is for HH dialogue. We also applied the DATE tagger to HH dialogue, and focused on the travel agent side of the dialo"
W02-0221,P97-1035,1,0.718575,"que way of representing and achieving particular communicative goals, DATE was developed to consistently label dialogue behaviors across systems so that the potential utility of dialogue act tagging could be explored. In previous work, Walker and Passonneau defined the DATE scheme, and labelled the system utterances in the June 2000 data collection of 663 dialogues from nine participating Communicator systems (Walker et al., 2001c; Walker et al., 2001a). They then derived dialogue act metrics from the DATE tags and showed that when these metrics were used in the PARADISE evaluation framework (Walker et al., 1997) that they improved models of user satisfaction by an absolute 5 , and that the new metrics could be used to understand which system’s dialogue strategies were most effective. A major part of evaluation effort using dialogue act tagging, however, is to actually label the dialogues with the dialogue act tags. In previous work (Walker et al., 2001c), the DATE labelling of the June-2000 corpus was done using a semi-automatic method that involved collection of a large number of utterance patterns from the different sites participating in the collection and subsequent hand labelling of these patter"
W02-0221,W01-1607,0,0.242704,"of characterizing dialogue behaviors in both humanhuman (HH) and human-computer (HC) dialogue (Isard and Carletta, 1995; Shriberg et al., 2000; Di Eugenio et al., 1998; Cattoni et al., 2001). Previous research has used dialogue act tagging for tasks such as improving recognition performance (Shriberg et al., 2000), identifying important parts of a dialogue (Finke et al., 1998), evaluating and comparing spoken dialogue systems (Walker et al., 2001c; Cattoni et al., 2001; Hastie et al., 2002), as a constraint on nominal expression generation (Jordan, 2000), and for comparing HH to HC dialogues (Doran et al., 2001). Our work builds directly on the previous application of the DATE (Dialogue Act Tagging for Evaluation) tagging scheme to the evaluation and comparison of DARPA Communicator dialogues. The hypothesis underlying the use of dialogue act tagging in spoken dialogue evaluation is that a system’s dialogue behaviors have a strong effect on its usability. Because Communicator systems have unique dialogue strategies, and a unique way of representing and achieving particular communicative goals, DATE was developed to consistently label dialogue behaviors across systems so that the potential utility of"
W02-0221,N01-1003,1,0.902299,"small amounts of human-human training data are available. 1 Introduction Recent research on dialogue is based on the assumption that dialogue acts provide a useful way of characterizing dialogue behaviors in both humanhuman (HH) and human-computer (HC) dialogue (Isard and Carletta, 1995; Shriberg et al., 2000; Di Eugenio et al., 1998; Cattoni et al., 2001). Previous research has used dialogue act tagging for tasks such as improving recognition performance (Shriberg et al., 2000), identifying important parts of a dialogue (Finke et al., 1998), evaluating and comparing spoken dialogue systems (Walker et al., 2001c; Cattoni et al., 2001; Hastie et al., 2002), as a constraint on nominal expression generation (Jordan, 2000), and for comparing HH to HC dialogues (Doran et al., 2001). Our work builds directly on the previous application of the DATE (Dialogue Act Tagging for Evaluation) tagging scheme to the evaluation and comparison of DARPA Communicator dialogues. The hypothesis underlying the use of dialogue act tagging in spoken dialogue evaluation is that a system’s dialogue behaviors have a strong effect on its usability. Because Communicator systems have unique dialogue strategies, and a unique way o"
W02-0221,P01-1066,1,0.911577,"Missing"
W02-0221,C98-2183,0,\N,Missing
W02-0221,C98-1051,0,\N,Missing
W04-0212,P97-1011,0,0.0334976,"Missing"
W04-0212,kingsbury-palmer-2002-treebank,0,0.0252442,"Penn TreeBank (syntactic structure) and PropBank (verbs and their arguments), which adds value for both linguistic discovery and discourse modeling. Here we describe the PDTB and some experiments in linguistic discovery based on the PDTB alone, as well as on the linked PTB and PDTB corpora. 1 Introduction Large scale annotated corpora such as the Penn TreeBank (Marcus et al., 1993) have played a central role in speech and natural language research. However, with the demand for more powerful NLP applications comes a need for greater richness in annotation – hence, the development of PropBank (Kingsbury and Palmer, 2002), which adds basic semantics to the PTB in the form of verb predicateargument annotation and eventually similar annotation of nominalizations. We have been developing yet another annotation layer above these both. The Penn Discourse TreeBank (PDTB) adds low-level discourse structure and semantics through the annotation of discourse connectives and their arguments, using connective-specific semantic role labels. With this added knowledge, the PDTB (together with the PTB and PropBank) should support more in-depth NLP research and more powerful applications. Work on the PDTB is grounded in a lexi"
W04-0212,J93-2004,0,0.0345992,"nk (PDTB) is a new resource built on top of the Penn Wall Street Journal corpus, in which discourse connectives are annotated along with their arguments. Its use of standoff annotation allows integration with a stand-off version of the Penn TreeBank (syntactic structure) and PropBank (verbs and their arguments), which adds value for both linguistic discovery and discourse modeling. Here we describe the PDTB and some experiments in linguistic discovery based on the PDTB alone, as well as on the linked PTB and PDTB corpora. 1 Introduction Large scale annotated corpora such as the Penn TreeBank (Marcus et al., 1993) have played a central role in speech and natural language research. However, with the demand for more powerful NLP applications comes a need for greater richness in annotation – hence, the development of PropBank (Kingsbury and Palmer, 2002), which adds basic semantics to the PTB in the form of verb predicateargument annotation and eventually similar annotation of nominalizations. We have been developing yet another annotation layer above these both. The Penn Discourse TreeBank (PDTB) adds low-level discourse structure and semantics through the annotation of discourse connectives and their ar"
W04-0212,W03-2608,1,0.888723,"or a state, and discourse deictics that denote an abstract object. What we describe to annotators as arguments to discourse connectives are actually the textual span from which the argument is derived (Webber et al., 1999a; Webber et al., 2003). This is especially clear in the case of the first argument of instead in (3), which does not actually include the negation, although it is part of the selected text.3 2 For a more detailed discussion of how discourse adverbials can be distinguished from clausal adverbials, see Forbes (2003). 3 For a corpus-based study of the arguments of instead, see (Miltsakaki et al., 2003). (3) [No price for the new shares has been set]. Instead, [the companies will leave it up to the marketplace to decide]. How far does an argument extend? One particularly significant addition to the guidelines came as a result of differences among annotators as to how large a span constituted the argument of a connective. During pilot annotations, annotators used three annotation tags: CONN for the connective and ARG1 and ARG2 for the two arguments. To this set, we have added two optional tags, SUP1 and SUP2 (supplementary), for cases when the annotator wants to mark textual spans s/he consid"
W04-0212,W04-2703,1,0.796962,"and restrict the profits businessmen could make]. As a result, [industry operated out of small, expensive, highly inefficient industrial units]. (2) Strangely, conventional wisdom inside the Beltway regards these transfer payments as “uncontrollable” or “nondiscretionary.” Implicit connectives are taken to occur between adjacent sentences not related by any explicit connective. They are annotated with whatever explicit connective the annotator feels could be inserted, with the original meaning retained. Assessment of inter-annotator agreement groups these annotations into five coarse classes (Miltsakaki et al., 2004). Currently, we are not annotating implicit connectives intra-sententially (such as between a main clause and a free adjunct) or across paragraphs. What counts as a legal argument? The simplest argument to a connective is what we take to be the minimum unit of discourse. Because we take discourse relations to hold between abstract objects, we require that an argument contain at least one clause-level predication (usually a verb – tensed or untensed), though it may span as much as a sequence of clauses or sentences. The two exceptions are nominal phrases that express an event or a state, and di"
W04-0212,J88-2003,0,0.332427,"Missing"
W04-0212,P95-1018,0,0.0380731,"inconsistencies in how the lexical items are analyzed. We believe that the PDTB annotation can contribute to a range of linguistic discovery and language modeling tasks, such as      providing empirical evidence for the DLTAG claim that discourse adverbials get one argument anaphorically, while structural connectives such as conjunctions establish relations between adjacent units of text (Creswell et al., 2002). acquiring common usage patterns of connectives and identifying their dependencies, in order to support “natural” choices in Natural Language Generation (di Eugenio et al., 1997; Moser and Moore, 1995; Williams and Reiter, 2003). developing decision procedures for resolving and interpreting discourse adverbials (Miltsakaki et al., 2003) which can be built on top of discourse parsing systems (Forbes et al., 2003). developing “word sense disambiguation” procedures for distinguishing among different senses of a connective and hence interpreting connectives correctly (e.g., distinguishing between temporal and explanatory since, between hypothetical and counterfactual if, between epistemic and semantic because, etc.) providing empirical evidence for theories of anaphoric phenomena such as verb"
W04-0212,P04-1011,1,0.743438,"Missing"
W04-0212,W98-0315,1,0.941827,"in the form of verb predicateargument annotation and eventually similar annotation of nominalizations. We have been developing yet another annotation layer above these both. The Penn Discourse TreeBank (PDTB) adds low-level discourse structure and semantics through the annotation of discourse connectives and their arguments, using connective-specific semantic role labels. With this added knowledge, the PDTB (together with the PTB and PropBank) should support more in-depth NLP research and more powerful applications. Work on the PDTB is grounded in a lexicalized approach to discourse – DLTAG (Webber and Joshi, 1998; Webber et al., 1999a; Webber et al., 2000; Webber et al., 2003). Here, low-level discourse structure and semantics are taken to result (in part) from composing elementary predicateargument relations whose predicates come mainly from discourse connectives1 and whose arguments 1 Despite this, we have deliberately adopted a policy of havcome from units of discourse – clausal, sentential or multi-sentential units. The PDTB therefore differs from the RST-annotated corpus (Carlson et al., 2003) which starts with (abstract) rhetorical relations (Mann and Thompson, 1988) and annotates a subset of th"
W04-0212,P99-1006,1,0.939969,"dicateargument annotation and eventually similar annotation of nominalizations. We have been developing yet another annotation layer above these both. The Penn Discourse TreeBank (PDTB) adds low-level discourse structure and semantics through the annotation of discourse connectives and their arguments, using connective-specific semantic role labels. With this added knowledge, the PDTB (together with the PTB and PropBank) should support more in-depth NLP research and more powerful applications. Work on the PDTB is grounded in a lexicalized approach to discourse – DLTAG (Webber and Joshi, 1998; Webber et al., 1999a; Webber et al., 2000; Webber et al., 2003). Here, low-level discourse structure and semantics are taken to result (in part) from composing elementary predicateargument relations whose predicates come mainly from discourse connectives1 and whose arguments 1 Despite this, we have deliberately adopted a policy of havcome from units of discourse – clausal, sentential or multi-sentential units. The PDTB therefore differs from the RST-annotated corpus (Carlson et al., 2003) which starts with (abstract) rhetorical relations (Mann and Thompson, 1988) and annotates a subset of the Penn WSJ corpus wit"
W04-0212,W01-1605,0,\N,Missing
W04-0212,J03-4002,1,\N,Missing
W04-2703,J93-2004,0,0.0274317,"being built directly on top of the Penn TreeBank and Propbank, thus supporting the extraction of useful syntactic and semantic features and providing a richer substrate for the development and evaluation of practical algorithms. We provide a detailed preliminary analysis of inter-annotator agreement – both the level of agreement and the types of inter-annotator variation. 1 Introduction Large scale annotated corpora have played a critical role in speech and natural language research. The Penn TreeBank (PTB) is an example of such a resource with worldwide impact on natural language processing (Marcus et al., 1993). However, the PTB deals with text only at the sentence level: with the demand for more powerful NLP applications comes a need for greater richness in annotation. At the sentence level, Penn Propbank is adding predicate-argument annotation to sentences in PTB (Kingsbury and Palmer, 2002). At the discourselevel are efforts to produce corpora annotated with rhetorical relations (Carlson et al., 2003). This paper describes a more basic discourse-level annotation project – the Penn Discourse TreeBank (PDTB) – that aims to produce a large-scale corpus in which discourse connectives are annotated, a"
W04-2703,W03-2608,1,0.843833,"reement between them and by correcting for chance expected agreement. However, the statistic requires the data tokens to be classified into discrete categories, and as a result, we could not apply it to our data since the PDTB annotation tokens cannot be classified as such. Rather, annotation in the PDTB constitutes either selection of a span of text for the arguments of connectives which can be of indeterminate length or providing explicit expressions for implicit connectives from an open-ended class of expressions. 8 For a preliminary corpus-based analysis of the arguments of ‘instead’, see Miltsakaki et al. (2003). Instead, we have assessed inter-annotator agreement in terms of agreement/disagreement on span or named expression identity for each token as a percentage of the pairs of spans or expressions that actually matched versus those that should have. For the argument annotations, we use a most conservative measure - the exact match criterion. In addition, we also used different diagnostics for the argument annotations for the explicit connectives, reporting percentage agreement on different classes of tokens, such as those in which the first argument (ARG1) annotations and second argument (ARG2) a"
W04-2703,P99-1006,1,0.897467,"nyi and van den Berg, 1996). In these approaches, the additional meaning the discourse contributes beyond the sentence derives from discourse relations. Specification of the discourse relations for a discourse thus constitutes a description of a certain level of discourse structure. Rather than starting from (abstract) discourse relations, we describe an approach to annotating a largescale corpus in terms of a more basic characterisation of discourse structure in terms of discourse connectives and their arguments. The motivation for such an approach stems from work by Webber and Joshi (1998), Webber et al. (1999a), Webber et al. (2000) which integrates sentence level structures with discourse level structure (using tree-adjoining grammars for both cases, LTAG and DLTAG, respectively). 1 This allows structural composition and its associated semantic composition at the sentence level to be smoothly carried over to the discourse level, a goal also shared by Gardent (1997), Schilder (1997) and Polanyi and van den Berg (1996), among others. 2 Discourse connectives and their arguments can be successfully annotated with high reliability (cf. Section 4). This is not surprising, given that the task resembles"
W04-2703,kingsbury-palmer-2002-treebank,0,0.269727,"annotator agreement – both the level of agreement and the types of inter-annotator variation. 1 Introduction Large scale annotated corpora have played a critical role in speech and natural language research. The Penn TreeBank (PTB) is an example of such a resource with worldwide impact on natural language processing (Marcus et al., 1993). However, the PTB deals with text only at the sentence level: with the demand for more powerful NLP applications comes a need for greater richness in annotation. At the sentence level, Penn Propbank is adding predicate-argument annotation to sentences in PTB (Kingsbury and Palmer, 2002). At the discourselevel are efforts to produce corpora annotated with rhetorical relations (Carlson et al., 2003). This paper describes a more basic discourse-level annotation project – the Penn Discourse TreeBank (PDTB) – that aims to produce a large-scale corpus in which discourse connectives are annotated, along with their arguments. There have been several approaches to describing discourse in terms of discourse relations (Mann and Thompson, 1988; Asher and Lascarides, 1998; Polanyi and van den Berg, 1996). In these approaches, the additional meaning the discourse contributes beyond the se"
W04-2703,W98-0315,1,0.725528,"nd Lascarides, 1998; Polanyi and van den Berg, 1996). In these approaches, the additional meaning the discourse contributes beyond the sentence derives from discourse relations. Specification of the discourse relations for a discourse thus constitutes a description of a certain level of discourse structure. Rather than starting from (abstract) discourse relations, we describe an approach to annotating a largescale corpus in terms of a more basic characterisation of discourse structure in terms of discourse connectives and their arguments. The motivation for such an approach stems from work by Webber and Joshi (1998), Webber et al. (1999a), Webber et al. (2000) which integrates sentence level structures with discourse level structure (using tree-adjoining grammars for both cases, LTAG and DLTAG, respectively). 1 This allows structural composition and its associated semantic composition at the sentence level to be smoothly carried over to the discourse level, a goal also shared by Gardent (1997), Schilder (1997) and Polanyi and van den Berg (1996), among others. 2 Discourse connectives and their arguments can be successfully annotated with high reliability (cf. Section 4). This is not surprising, given tha"
W04-2703,J03-4002,1,\N,Missing
W05-0305,P97-1003,0,0.0334303,"Missing"
W05-0305,J00-3005,0,0.0156263,"t. Consider example (12), where the PTB requires annotators to include the verb of attribution said and its subject Delmed in the complement of although. But although as a discourse connective denies the expectation that the supply of dialysis products will be discontinued when the distribution arrangement ends. It does not convey the expectation that Delmed will not say such things. On the other hand, in (13), the contrast established by while is between the opinions of two entities i.e., advocates and their opponents.4 4 This distinction is hard to capture in an RST-based parsing framework (Marcu, 2000). According to the RST-based annotation scheme (Carlson et al., 2003) ‘although Delmed said’ and ‘while opponents argued’ are elementary discourse units 32 (12) The current distribution arrangement ends in March 1990, although Delmed said it will continue to provide some supplies of the peritoneal dialysis products to National Medical, the spokeswoman said. (13) Advocates said the 90-cent-an-hour rise, to $4.25 an hour by April 1991, is too small for the working poor, while opponents argued that the increase will still hurt small business and cost many thousands of jobs. In Section 5, we will"
W05-0305,W04-2703,1,0.741875,"iscourse structure, in terms of the arguments of connectives, due in large part to attribution. We describe these differences, an algorithm for detecting them, and finally some experimental results. These results have implications for automating discourse annotation based on syntactic annotation. 1 Introduction The overall goal of the Penn Discourse Treebank (PDTB) is to annotate the million word WSJ corpus in the Penn TreeBank (Marcus et al., 1993) with a layer of discourse annotations. A preliminary report on this project was presented at the 2004 workshop on Frontiers in Corpus Annotation (Miltsakaki et al., 2004a), where we described our annotation of discourse connectives (both explicit and implicit) along with their (clausal) arguments. Further work done since then includes the annotation of attribution: that is, who has expressed each argument to a discourse connective (the writer or some other speaker or author) and who has expressed the discourse relation itself. These ascriptions need not be the same. Of particular interest is the fact that attribution may or may not play a role in the relation established by a connective. This may lead to a lack of congruence between arguments at the syntactic"
W05-0305,miltsakaki-etal-2004-penn,1,0.916062,"iscourse structure, in terms of the arguments of connectives, due in large part to attribution. We describe these differences, an algorithm for detecting them, and finally some experimental results. These results have implications for automating discourse annotation based on syntactic annotation. 1 Introduction The overall goal of the Penn Discourse Treebank (PDTB) is to annotate the million word WSJ corpus in the Penn TreeBank (Marcus et al., 1993) with a layer of discourse annotations. A preliminary report on this project was presented at the 2004 workshop on Frontiers in Corpus Annotation (Miltsakaki et al., 2004a), where we described our annotation of discourse connectives (both explicit and implicit) along with their (clausal) arguments. Further work done since then includes the annotation of attribution: that is, who has expressed each argument to a discourse connective (the writer or some other speaker or author) and who has expressed the discourse relation itself. These ascriptions need not be the same. Of particular interest is the fact that attribution may or may not play a role in the relation established by a connective. This may lead to a lack of congruence between arguments at the syntactic"
W05-0305,W04-0212,1,0.860298,"the level of sentence-bound annotation. 2 Overview of the PDTB The PDTB builds on the DLTAG approach to discourse structure (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) in which connectives are discourse-level predicates which project predicate-argument structure on a par with verbs at 29 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29–36, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics the sentence level. Initial work on the PDTB has been described in Miltsakaki et al. (2004a), Miltsakaki et al. (2004b), Prasad et al. (2004). The key contribution of the PDTB design framework is its bottom-up approach to discourse structure: Instead of appealing to an abstract (and arbitrary) set of discourse relations whose identification may confound multiple sources of discourse meaning, we start with the annotation of discourse connectives and their arguments, thus exposing a clearly defined level of discourse representation. The PDTB annotates as explicit discourse connectives all subordinating conjunctions, coordinating conjunctions and discourse adverbials. These predicates establish relations between two abstract objects s"
W05-0305,W03-1014,0,0.0063381,"re looking to raise in the year ending March 21 compares with only $2.7 billion raise on the capital market in the previous year. IMPLICIT - in contrast In fiscal 1984, before Mr. Gandhi came into power, only $810 million was raised. When complete, the PDTB will contain approximately 35K annotations: 15K annotations of the 100 explicit connectives identified in the corpus and 20K annotations of implicit connectives.3 3 Annotation of attribution Wiebe and her colleagues have pointed out the importance of ascribing beliefs and assertions expressed in text to the agent(s) holding or making them (Riloff and Wiebe, 2003; Wiebe et al., 2004; Wiebe et al., 2005). They have also gone a considerable way towards specifying how such subjective material should be annotated (Wiebe, 2002). Since we take discourse connectives to convey semantic predicate-argument relations between abstract objects, one can distinguish a variety of cases depending on the attribution of the discourse relation or its 3 The annotation guidelines for the PDTB are available at http://www.cis.upenn.edu/ pdtb.  arguments; that is, whether the relation or arguments are ascribed to the author of the text or someone other than the author. Case"
W05-0305,W98-0315,1,0.823932,"the attribution of the arguments of a connective and the relation it conveys. In Sections 4 and 5, we describe mismatches that arise between the discourse arguments of a connective and the syntactic annotation as provided by the Penn TreeBank (PTB), in the cases where all the arguments of the connective are in the same sentence. In Section 6, we will discuss some implications of these issues for the theory and practice of discourse annotation and their relevance even at the level of sentence-bound annotation. 2 Overview of the PDTB The PDTB builds on the DLTAG approach to discourse structure (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) in which connectives are discourse-level predicates which project predicate-argument structure on a par with verbs at 29 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29–36, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics the sentence level. Initial work on the PDTB has been described in Miltsakaki et al. (2004a), Miltsakaki et al. (2004b), Prasad et al. (2004). The key contribution of the PDTB design framework is its bottom-up approach to discourse structure: Instead of appealing to an"
W05-0305,P99-1006,1,0.857391,"rguments of a connective and the relation it conveys. In Sections 4 and 5, we describe mismatches that arise between the discourse arguments of a connective and the syntactic annotation as provided by the Penn TreeBank (PTB), in the cases where all the arguments of the connective are in the same sentence. In Section 6, we will discuss some implications of these issues for the theory and practice of discourse annotation and their relevance even at the level of sentence-bound annotation. 2 Overview of the PDTB The PDTB builds on the DLTAG approach to discourse structure (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) in which connectives are discourse-level predicates which project predicate-argument structure on a par with verbs at 29 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29–36, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics the sentence level. Initial work on the PDTB has been described in Miltsakaki et al. (2004a), Miltsakaki et al. (2004b), Prasad et al. (2004). The key contribution of the PDTB design framework is its bottom-up approach to discourse structure: Instead of appealing to an abstract (and arbitr"
W05-0305,J04-3002,0,0.0189811,"he year ending March 21 compares with only $2.7 billion raise on the capital market in the previous year. IMPLICIT - in contrast In fiscal 1984, before Mr. Gandhi came into power, only $810 million was raised. When complete, the PDTB will contain approximately 35K annotations: 15K annotations of the 100 explicit connectives identified in the corpus and 20K annotations of implicit connectives.3 3 Annotation of attribution Wiebe and her colleagues have pointed out the importance of ascribing beliefs and assertions expressed in text to the agent(s) holding or making them (Riloff and Wiebe, 2003; Wiebe et al., 2004; Wiebe et al., 2005). They have also gone a considerable way towards specifying how such subjective material should be annotated (Wiebe, 2002). Since we take discourse connectives to convey semantic predicate-argument relations between abstract objects, one can distinguish a variety of cases depending on the attribution of the discourse relation or its 3 The annotation guidelines for the PDTB are available at http://www.cis.upenn.edu/ pdtb.  arguments; that is, whether the relation or arguments are ascribed to the author of the text or someone other than the author. Case 1: The relation and"
W05-0305,J93-2004,0,\N,Missing
W05-0305,J03-4002,1,\N,Missing
W06-0305,J93-2004,0,\N,Missing
W06-0305,W04-2703,1,\N,Missing
W06-0305,W05-0308,0,\N,Missing
W06-0305,W04-0212,1,\N,Missing
W06-0305,H05-1116,0,\N,Missing
W06-0305,W05-0305,1,\N,Missing
W06-0305,W03-1017,0,\N,Missing
W06-0305,J03-4002,1,\N,Missing
W06-0305,J04-3002,0,\N,Missing
W06-0305,P02-1053,0,\N,Missing
W06-0305,W02-1011,0,\N,Missing
W08-0614,J93-2004,0,\N,Missing
W08-0614,W04-2703,1,\N,Missing
W08-0614,prasad-etal-2008-penn,1,\N,Missing
W09-3029,I08-2099,1,0.828553,"in the Penn Treebank Corpus. Recent interest in cross-linguistic studies of discourse relations has led to the initiation of similar discourse annotation projects in other languages as well, such as Chinese (Xue, 2005), Czech (Mladová et al., 2008), and Turkish (Deniz and Webber, 2008). In this paper, we describe our ongoing work on the creation of a Hindi Discourse Relation Bank (HDRB), broadly following the approach of the PDTB.1 The size of the HDRB corpus is 200K words and it is drawn from a 400K word corpus on which Hindi syntactic dependency annotation is being independently conducted (Begum et al., 2008). Source corpus texts are taken from the Hindi newspaper Amar Ujala, and comprise news articles from several domains, such as politics, sports, films, etc. We 1 An earlier study of Hindi discourse connectives towards the creation of HDRB is presented in Prasad et al. (2008). present our characterization of discourse connectives and their arguments in Hindi (Section 2), our proposals for modifying the sense classification scheme (Section 3), and present some crosslinguistics comparisons based on annotations done so far (Section 4). Section 5 concludes with a summary and future work. 2 Discourse"
W09-3029,mladova-etal-2008-sentence,0,0.309198,"carried out so far. 1 Introduction To enable NLP research and applications beyond the sentence-level, corpora annotated with discourse level information have been developed. The recently developed Penn Discourse Treebank (PDTB) (Prasad et al., 2008), for example, provides annotations of discourse relations (e.g., causal, contrastive, temporal, and elaboration relations) in the Penn Treebank Corpus. Recent interest in cross-linguistic studies of discourse relations has led to the initiation of similar discourse annotation projects in other languages as well, such as Chinese (Xue, 2005), Czech (Mladová et al., 2008), and Turkish (Deniz and Webber, 2008). In this paper, we describe our ongoing work on the creation of a Hindi Discourse Relation Bank (HDRB), broadly following the approach of the PDTB.1 The size of the HDRB corpus is 200K words and it is drawn from a 400K word corpus on which Hindi syntactic dependency annotation is being independently conducted (Begum et al., 2008). Source corpus texts are taken from the Hindi newspaper Amar Ujala, and comprise news articles from several domains, such as politics, sports, films, etc. We 1 An earlier study of Hindi discourse connectives towards the creation"
W09-3029,W05-0312,0,0.242009,"nitial annotations carried out so far. 1 Introduction To enable NLP research and applications beyond the sentence-level, corpora annotated with discourse level information have been developed. The recently developed Penn Discourse Treebank (PDTB) (Prasad et al., 2008), for example, provides annotations of discourse relations (e.g., causal, contrastive, temporal, and elaboration relations) in the Penn Treebank Corpus. Recent interest in cross-linguistic studies of discourse relations has led to the initiation of similar discourse annotation projects in other languages as well, such as Chinese (Xue, 2005), Czech (Mladová et al., 2008), and Turkish (Deniz and Webber, 2008). In this paper, we describe our ongoing work on the creation of a Hindi Discourse Relation Bank (HDRB), broadly following the approach of the PDTB.1 The size of the HDRB corpus is 200K words and it is drawn from a 400K word corpus on which Hindi syntactic dependency annotation is being independently conducted (Begum et al., 2008). Source corpus texts are taken from the Hindi newspaper Amar Ujala, and comprise news articles from several domains, such as politics, sports, films, etc. We 1 An earlier study of Hindi discourse con"
W09-3029,I08-7009,0,0.484908,"Missing"
W09-3029,prasad-etal-2008-penn,1,\N,Missing
W09-3029,I08-7010,1,\N,Missing
W10-4310,N06-2015,0,0.01815,"ations. We use the part of speech (POS) tag associated with the head of the noun phrase to assign one of the following categories: pronoun, nominal, name or expletive. When the head does not belong to the above classes, we simply record its POS tag. We also mark whether the noun phrase is a definite description using the presence of the article ‘the’. Ex 2. Rolls-Royce Motor Cars Inc. said it expects its U.S sales to remain steady at about 1,200 cars in 1990. The luxury auto maker last year sold 1,214 cars in the U.S. We use the coreference annotations from the Ontonotes corpus (version 2.9) (Hovy et al., 2006) to compute our gold-standard entity features. The WSJ portion of this corpus contains 590 articles. Here, nominalizations and temporal expressions are also annotated for coreference but we use the links between noun phrases only. We expect these features computed on the gold-standard annotations to represent an upper bound on the performance of entity features. Finally, the Penn Treebank corpus (Marcus et al., 1994) is used to obtain gold-standard parse and grammatical role information. Only adjacent sentences within the same paragraph are used in our experiments. 3 Modification. We expected"
W10-4310,D09-1036,0,0.373089,"discourse relations and the way in which references to entities are realized. In our work, we employ features related to entity realization to automatically identify discourse relations in text. We focus on implicit relations that hold between adjacent sentences in the absence of discourse connectives such as “because” or “but”. Previous studies on this task have zeroed in on lexical indicators of relation sense: dependencies between words (Marcu and Echihabi, 2001; BlairGoldensohn et al., 2007) and the semantic orientation of words (Pitler et al., 2009), or on general syntactic regularities (Lin et al., 2009). 2 Data We use 590 Wall Street Journal (WSJ) articles with overlapping annotations for discourse, coreference and syntax from three corpora. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is the largest available resource of discourse relation annotations. In the PDTB, implicit relations are annotated between adjacent sentences in the same paragraph. They are assigned senses from a hierarchy containing four top level categories–Comparison, Contingency, Temporal and Expansion. Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
W10-4310,P09-1077,1,0.332049,"expressions. We aim to investigate the association between discourse relations and the way in which references to entities are realized. In our work, we employ features related to entity realization to automatically identify discourse relations in text. We focus on implicit relations that hold between adjacent sentences in the absence of discourse connectives such as “because” or “but”. Previous studies on this task have zeroed in on lexical indicators of relation sense: dependencies between words (Marcu and Echihabi, 2001; BlairGoldensohn et al., 2007) and the semantic orientation of words (Pitler et al., 2009), or on general syntactic regularities (Lin et al., 2009). 2 Data We use 590 Wall Street Journal (WSJ) articles with overlapping annotations for discourse, coreference and syntax from three corpora. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is the largest available resource of discourse relation annotations. In the PDTB, implicit relations are annotated between adjacent sentences in the same paragraph. They are assigned senses from a hierarchy containing four top level categories–Comparison, Contingency, Temporal and Expansion. Proceedings of SIGDIAL 2010: the 11th Annual Meetin"
W10-4310,prasad-etal-2008-penn,1,0.192545,"ract The role of entities has also been hypothesized as important for this task and entity-related features have been used alongside others (CorstonOliver, 1998; Sporleder and Lascarides, 2008). Corpus studies and reading time experiments performed by Wolf and Gibson (2006) have in fact demonstrated that the type of discourse relation linking two clauses influences the resolution of pronouns in them. However, the predictive power of entity-related features has not been studied independently of other factors. Further motivation for studying this type of features comes from new corpus evidence (Prasad et al., 2008), that about a quarter of all adjacent sentences are linked purely by entity coherence, solely because they talk about the same entity. Entity-related features would be expected to better separate out such relations. We present the first comprehensive study of the connection between entity features and discourse relations. We show that there are notable differences in properties of referring expressions across the different relations. Sense prediction can be done with results better than random baseline using only entity realization information. Their performance, however, is lower than a know"
W10-4310,miltsakaki-etal-2004-penn,1,\N,Missing
W10-4310,N07-1054,0,\N,Missing
W10-4310,J93-2004,0,\N,Missing
W10-4310,P02-1047,0,\N,Missing
W15-0210,afantenos-etal-2012-empirical,0,0.326281,"SemAF-DRel specification, and the ISO SemAF-DRel metamodel. 2.2 Representation of discourse structure One difference between frameworks concerns the representation of structure. For example, the RST Bank (Carlson et al., 2003), based on Rhetorical Structure Theory (Mann and Thompson, 1988), assumes a tree representation to subsume the complete text of the discourse. The Discourse Graphbank (Wolf and Gibson, 2005), based on Hobbs’ theory of discourse (Hobbs, 1990), allows for general graphs that allow multiple parents and crossing, and the DISCOR corpus (Reese et al., 2007) and ANNODIS corpus (Afantenos et al., 2012), based on Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides, 2003), allow directed acyclic graphs that allow for multiple parents, but not for crossing. There are also frameworks that are pre-theoretical or theory-neutral with respect to discourse structure, including the PDTB (Prasad et al., 2008), based loosely on DLTAG (Webber et al., 2003), and DiscAn (Sanders and Scholman, 2012), based on (Sanders et al., 1992). In both of these, individual relations along with their arguments are annotated without being combined to form a structure that encompasses the entire text."
W15-0210,W15-0201,1,0.884294,"Missing"
W15-0210,W10-1840,1,0.712188,"te syntax defines a representation for every structure defined by the abstract syntax; unambiguity means that every expression defined by the concrete syntax represents one and only one structure defined by the abstract syntax. A representation format defined by a concrete syntax which has these two properties is called an ideal representation format. An important aspect of this approach is that any ideal representation format is convertible through a meaning-preserving mapping to any other ideal representation format (including the GrAF format defined by Ide and Suderman (2007), as shown in (Ide and Bunt, 2010)). In this section we present the metamodel that expresses the conceptual view underlying DRelML and outline its abstract and concrete syntax. The semantics of DRelML annotations, which is defined through a translation into discourse representation structures (DRSs), is outlined in the appendix. Note that annotators only have to deal with the concrete DRelML syntax; the underlying abstract syntax is relevant mainly for establishing possible mappings between DRelML and other annotation schemes; the semantics is relevant for the extraction of content from DRelML annotated resources. 3.2 Metamode"
W15-0210,W07-1501,0,0.0108893,"ax. Completeness means that the concrete syntax defines a representation for every structure defined by the abstract syntax; unambiguity means that every expression defined by the concrete syntax represents one and only one structure defined by the abstract syntax. A representation format defined by a concrete syntax which has these two properties is called an ideal representation format. An important aspect of this approach is that any ideal representation format is convertible through a meaning-preserving mapping to any other ideal representation format (including the GrAF format defined by Ide and Suderman (2007), as shown in (Ide and Bunt, 2010)). In this section we present the metamodel that expresses the conceptual view underlying DRelML and outline its abstract and concrete syntax. The semantics of DRelML annotations, which is defined through a translation into discourse representation structures (DRSs), is outlined in the appendix. Note that annotators only have to deal with the concrete DRelML syntax; the underlying abstract syntax is relevant mainly for establishing possible mappings between DRelML and other annotation schemes; the semantics is relevant for the extraction of content from DRelML"
W15-0210,W10-4327,0,0.019635,"ing that is key to an understanding of the discourse, beyond the meaning conveyed by individual clauses and sentences. The types of abstract semantic objects connected by discourse relations include events, states, conditions and dialogue acts, that are typically expressed as sentences, but they can also be smaller or larger units (clauses, paragraphs, dialogue segments), and they may also occur between abstract objects not explicitly realized but inferrable from semantic content. Discourse relations and discourse structure are key ingredients for NLP tasks such as summarization (Marcu, 2000; Louis et al., 2010), complex question answering (Verberne et al., 2007), and natural language generation (McKeown, 1985; Hovy, 1993; Prasad et al., 2005) and there are now several international and collaborative efforts to create annotated resources of discourse relations, across languages as well as across genres, to support the development of such applications. This paper describes some of the research conducted with the aim to develop a proposal for an ISO standard for the annotation of semantic relations in discourse. A range of theoretical approaches and annotation efforts were analysed for their commonalit"
W15-0210,mladova-etal-2008-sentence,0,0.072123,"Missing"
W15-0210,prasad-etal-2008-penn,1,0.909562,"frameworks for representing discourse relations differ along several lines. This section provides a comparison of the most important frameworks, focusing on those that have been used as the basis for annotating discourse relations in corpora, in particular, the theory of discourse coherence developed by Hobbs (Hobbs, 1990), Rhetorical Structure Theory (Mann and Thompson, 1988), the cognitive account of coherence relations by Sanders et al (Sanders et al., 1992), Segmented Discourse Representation Theory (Asher and Lascarides, 2003), and the annotation framework of the Penn Discourse Treebank (Prasad et al., 2008, 2014), which is loosely based on DLTAG (Webber et al., 2003). The comparison highlights and discusses the differences that are considered relevant for developing a pivot representation in ISO SemAF-DRel. For each issue, the discussion is followed by the position adopted in the ISO standard. The section ends with a summary of the key concepts used in the ISO SemAF-DRel specification, and the ISO SemAF-DRel metamodel. 2.2 Representation of discourse structure One difference between frameworks concerns the representation of structure. For example, the RST Bank (Carlson et al., 2003), based on R"
W15-0210,J14-4007,1,0.897147,"Missing"
W15-0210,J03-4002,0,0.200844,"several lines. This section provides a comparison of the most important frameworks, focusing on those that have been used as the basis for annotating discourse relations in corpora, in particular, the theory of discourse coherence developed by Hobbs (Hobbs, 1990), Rhetorical Structure Theory (Mann and Thompson, 1988), the cognitive account of coherence relations by Sanders et al (Sanders et al., 1992), Segmented Discourse Representation Theory (Asher and Lascarides, 2003), and the annotation framework of the Penn Discourse Treebank (Prasad et al., 2008, 2014), which is loosely based on DLTAG (Webber et al., 2003). The comparison highlights and discusses the differences that are considered relevant for developing a pivot representation in ISO SemAF-DRel. For each issue, the discussion is followed by the position adopted in the ISO standard. The section ends with a summary of the key concepts used in the ISO SemAF-DRel specification, and the ISO SemAF-DRel metamodel. 2.2 Representation of discourse structure One difference between frameworks concerns the representation of structure. For example, the RST Bank (Carlson et al., 2003), based on Rhetorical Structure Theory (Mann and Thompson, 1988), assumes"
W15-0210,J05-2005,0,0.114169,"eloping a pivot representation in ISO SemAF-DRel. For each issue, the discussion is followed by the position adopted in the ISO standard. The section ends with a summary of the key concepts used in the ISO SemAF-DRel specification, and the ISO SemAF-DRel metamodel. 2.2 Representation of discourse structure One difference between frameworks concerns the representation of structure. For example, the RST Bank (Carlson et al., 2003), based on Rhetorical Structure Theory (Mann and Thompson, 1988), assumes a tree representation to subsume the complete text of the discourse. The Discourse Graphbank (Wolf and Gibson, 2005), based on Hobbs’ theory of discourse (Hobbs, 1990), allows for general graphs that allow multiple parents and crossing, and the DISCOR corpus (Reese et al., 2007) and ANNODIS corpus (Afantenos et al., 2012), based on Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides, 2003), allow directed acyclic graphs that allow for multiple parents, but not for crossing. There are also frameworks that are pre-theoretical or theory-neutral with respect to discourse structure, including the PDTB (Prasad et al., 2008), based loosely on DLTAG (Webber et al., 2003), and DiscAn (Sanders and"
W15-0210,P12-1008,0,0.0305757,"Missing"
W15-0210,W01-1605,0,\N,Missing
W15-2707,W05-0305,1,0.799224,"Missing"
W15-2707,D12-1083,0,0.012738,"(3) Now, we regard this as a largely phony issue, but the “long term” is nonetheless a big salon topic all around the Beltway. (5) PropBank: Verb = suspend Arg0 = The federal government Arg1 = sales of U.S. savings bonds ARGM-CAU = because Congress hasn’t lifted the ceiling on government debt (4) The U.S. wants the removal of . . .barriers to investment; Japan denies there are real barriers. Researchers working on discourse parsing have commented that intra-sentential (intra-S) discourse relations are, in general, easier to recognize than ones whose arguments are found in separate sentences (Joty et al., 2012; Lin et al., (6) PDTB: Connective = because Arg1 = The federal government suspended sales of U.S. savings bonds Arg2 = Congress hasn’t lifted the ceiling on government debt Sense = Contingency.Cause.Reason 64 Proceedings of the EMNLP 2015 Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 64–69, c Lisboa, Portugal, 18 September 2015. 2015 Association for Computational Linguistics. ARGM - ADV (2235) ARGM - CAU (657) ARGM - TMP (2503) ARGM - PNC (66) ARGM - MNR (13) TOTAL (5475) TEMPORAL CONTINGENCY COMPARISON EXPANSION TOTAL 222 14 2258 0 0 2494 1067 650 523"
W15-2707,J93-2004,0,0.0504287,"Missing"
W15-2707,W04-2705,0,0.14572,"Missing"
W15-2707,J05-1004,0,0.340826,"mproving the quality of recognizers capable of determining what, if any, discourse relations hold between intra-S units. Taking abstract objects to be expressed (arguably) typically as clauses headed by verbs or other predicates, the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) includes annotations of intra-S discourse relations but, as noted by Prasad et al. (2014), they are significantly underannotated in the corpus. At the same time, Prasad et al. (2014) point to possible overlaps between intra-S discourse relations in the PDTB and a subset of verb-argument annotations in PropBank (Palmer et al., 2005). The PropBank annotations of particular interest here are those in which the arguments are clausal adjuncts, labeled ARGM, and further assigned a semantic role. For example, the PropBank annotation of the verb suspend in Ex. 1 is shown in (5), with the adjunct clause annotated as ARGM and assigned the role CAU (causal). The PDTB annotation for the same example, shown in (6), marks because as the connective, ‘Contingency.Cause.Reason’ as the sense, the adjunct clause as Arg2 (defined as the argument attached to the connective), and the matrix clause as Arg1 (defined as the non-Arg2 argument)."
W15-2707,N04-1030,1,0.764484,"Missing"
W15-2707,W13-3516,1,0.897991,"Missing"
W15-2707,prasad-etal-2008-penn,1,0.826615,"bber@ed.ac.uk 3 Institute for Research in Cognitive Science, University of Pennsylvania {aleewk,joshi}@seas.upenn.edu 4 Boulder Language Technologies pradhan@bltek.com Abstract 2012; Feng, 2014). They are also quite useful in Language Technology applications that exploit sentence-level relations. Thus, there is particular value in improving the quality of recognizers capable of determining what, if any, discourse relations hold between intra-S units. Taking abstract objects to be expressed (arguably) typically as clauses headed by verbs or other predicates, the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) includes annotations of intra-S discourse relations but, as noted by Prasad et al. (2014), they are significantly underannotated in the corpus. At the same time, Prasad et al. (2014) point to possible overlaps between intra-S discourse relations in the PDTB and a subset of verb-argument annotations in PropBank (Palmer et al., 2005). The PropBank annotations of particular interest here are those in which the arguments are clausal adjuncts, labeled ARGM, and further assigned a semantic role. For example, the PropBank annotation of the verb suspend in Ex. 1 is shown in (5), with the adjunct clau"
W15-2707,C10-2118,1,0.939227,"Missing"
W15-2707,J14-4007,1,0.857151,"leewk,joshi}@seas.upenn.edu 4 Boulder Language Technologies pradhan@bltek.com Abstract 2012; Feng, 2014). They are also quite useful in Language Technology applications that exploit sentence-level relations. Thus, there is particular value in improving the quality of recognizers capable of determining what, if any, discourse relations hold between intra-S units. Taking abstract objects to be expressed (arguably) typically as clauses headed by verbs or other predicates, the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) includes annotations of intra-S discourse relations but, as noted by Prasad et al. (2014), they are significantly underannotated in the corpus. At the same time, Prasad et al. (2014) point to possible overlaps between intra-S discourse relations in the PDTB and a subset of verb-argument annotations in PropBank (Palmer et al., 2005). The PropBank annotations of particular interest here are those in which the arguments are clausal adjuncts, labeled ARGM, and further assigned a semantic role. For example, the PropBank annotation of the verb suspend in Ex. 1 is shown in (5), with the adjunct clause annotated as ARGM and assigned the role CAU (causal). The PDTB annotation for the same"
W15-2707,N07-1069,0,0.060601,"Missing"
W16-1704,P14-1065,0,0.0139892,"Missing"
W16-1704,N09-1064,0,0.0255764,"Missing"
W16-1704,J15-3002,0,0.0272451,"Missing"
W16-1704,W12-3624,0,0.0492071,"Missing"
W16-1704,J93-2004,0,0.0590896,"wsj 1270] (13) Then take the expected return and subtract one standard deviation. [wsj 1564] (14) Be careful boys; use good judgment. [wsj 0596] 2.2 Discourse Adverbials (9) The NAM embraces efforts, which both the adminisAs can be seen from the presence of then in Ex. 9, conjoined VPs can themselves contain discourse adverbials. As with all discourse adverbials, ones that appear in Arg2 of a conjoined VP can link to material elsewhere in the text, as in Ex. 15 (15) Separately, the Federal Energy Regulatory CommisSince these were incorrectly analyzed according to the Penn TreeBank Guidelines (Marcus et al., 1993) and do not actually differ from the tokens already included in the corpus, we decided to include them. On the other hand, we decided to exclude tokens containing conjoined verbs that should possibly have been analyzed as conjoined VPs, such as exist and fight in While the discourse adverbial still shares its Arg2 with the conjoined VP, its Arg1 has been taken to be the FERC turning down its request for approval of its possible purchase of PS of New Hampshire, which appears in the previous sentence. Although such adverbials can link to material in previous sentences, the far more common situat"
W16-1704,prasad-etal-2008-penn,1,0.835795,"Missing"
W16-1704,C10-2118,1,0.597562,"equency with which each PDTB2 sense has been replaced by a specific PDTB3 sense. 4.2 Implicit=instead be part of the world mentality,” declares Charles M. Jordan, GM’s vice president for design . . . (Expansion.Conjunction, Expansion.Substitution.Arg2-as-subst) [wsj 0956] (38) . . . Exxon Corp. built the plant but Implicit=then closed it in 1985. (Comparison.Concession.Arg2-as-denier, Temporal.Asynchronous.Precedence) [wsj 1748] 3. If inserting an implicit connective was perceived as redundant, appropriate material in Arg2 could be annotated as AltLex (Ex. 39), as done elsewhere in the PDTB2 (Prasad et al., 2010). (39) His policies went beyond his control and resulted . . . in riots and disturbances. (Expansion.Conjunction, Contingency.Cause.Result) [wsj 0290] Sense labelling of conjoined VP tokens The second guideline above points to a new feature of our discourse annotation: While multiple relations were annotated in the PDTB2 as holding between identical or overlapping argument spans, all were associated with either multiple explicit connectives or multiple inferred relations. What is new in the annotation of conjoined VPs is the possibility of an explicit relation co-occurring with ones that are i"
W16-1704,J14-4007,1,\N,Missing
W16-1704,W01-1605,0,\N,Missing
W17-5502,C16-2026,1,0.843617,"on’t talk about my work,” he says. (NoRel) The president of CBS News, David W. Burke, didn’t return numerous telephone calls. To identify challenges and explore the feasibility of annotating cross-paragraph implicit relations on a large scale, texts from the PDTB corpus were selected to cover a range of sub-genres (Webber, 2009) and lengths. These texts contained 440 current paragraph first sentence (CPFS) tokens (excluding the first sentence in each text) not already related to the prior text by an inter-sentential explicit connective. These tokens were annotated in the PDTB Annotation Tool (Lee et al., 2016) over the three phases described below. In adjacent contexts not related by a connective, an inferred relation is annotated as either an implicit relation (Ex. 3) when it can be expressed by inserting a connective, or an AltLex (alternatively lexicalized) relation (Ex. 4) if insertion of a connective leads to a perception of relation redundancy, indicating the presence of some alternative lexico-syntactic marking of the relation. When a discourse relation is not inferred, the context is annotated as EntRel (Ex. 5) if an entity-based relation is perceived, and as NoRel (Ex. 6) otherwise. Sectio"
W17-5502,W10-4327,0,0.0606877,"Missing"
W17-5502,J93-2004,0,0.0586454,"Missing"
W17-5502,W04-2703,1,0.757531,"Missing"
W17-5502,K16-2002,0,0.0222643,"Missing"
W17-5502,C16-1246,1,0.901119,"Missing"
W17-5502,prasad-etal-2008-penn,1,0.892678,"Missing"
W17-5502,prasad-etal-2010-exploiting,1,0.888782,"Missing"
W17-5502,J14-4007,1,0.923757,"Missing"
W17-5502,C08-1101,0,0.0475251,"Missing"
W17-5502,K15-2002,0,0.0327623,"Missing"
W17-5502,P09-1076,0,0.013884,"and speech-acts (+/-ζ) that may or may not be associated with one of the defined arguments of the relation. 3 The Experiment (6) The executive producer of ”Saturday Night With Connie Chung,” Andrew Lack, declines to discuss recreations as a practice or his show, in particular. ”I don’t talk about my work,” he says. (NoRel) The president of CBS News, David W. Burke, didn’t return numerous telephone calls. To identify challenges and explore the feasibility of annotating cross-paragraph implicit relations on a large scale, texts from the PDTB corpus were selected to cover a range of sub-genres (Webber, 2009) and lengths. These texts contained 440 current paragraph first sentence (CPFS) tokens (excluding the first sentence in each text) not already related to the prior text by an inter-sentential explicit connective. These tokens were annotated in the PDTB Annotation Tool (Lee et al., 2016) over the three phases described below. In adjacent contexts not related by a connective, an inferred relation is annotated as either an implicit relation (Ex. 3) when it can be expressed by inserting a connective, or an AltLex (alternatively lexicalized) relation (Ex. 4) if insertion of a connective leads to a"
W17-5502,J03-4002,0,0.191029,"Missing"
W17-5502,W16-1704,1,0.855125,"ially volatile storage facilities and crossblending operations away from town. • We fall back to the PDTB strategy of fully annotating only adjacent implicit relations, while also employing an underspecified marking of non-adjacent ones. (5) The proposed petrochemical plant would use naphtha to manufacture the petrochemicals propylene and ethylene and their resin derivatives, polypropylene and polyethylene. (EntRel) These are the raw materials used in making plastic • We introduce new guidelines to (a) better represent the inter-dependency of relations in a 8 Figure 1: PDTB-3 Sense Hierarchy (Webber et al., 2016) Modified to Include Arg1/Arg2-as-instance and Hypophora. Only asymmetric relations are specified further at Level-3, to differentiate directionality of the arguments. Superscript symbols on Level-2 senses indicate features for implicit beliefs (+/-β) and speech-acts (+/-ζ) that may or may not be associated with one of the defined arguments of the relation. 3 The Experiment (6) The executive producer of ”Saturday Night With Connie Chung,” Andrew Lack, declines to discuss recreations as a practice or his show, in particular. ”I don’t talk about my work,” he says. (NoRel) The president of CBS Ne"
W17-5502,K15-2001,1,0.913236,"Missing"
W17-5502,K16-2001,0,0.260047,"Missing"
W17-5502,W01-1605,0,\N,Missing
W18-4710,afantenos-etal-2012-empirical,0,0.0528617,"Missing"
W18-4710,al-saif-markert-2010-leeds,0,0.026557,"otated relations, making it the largest such corpus available to date. Largely because the PDTB was based on the simple idea that discourse relations are grounded in an identifiable set of explicit words or phrases (discourse connectives) or simply in sentence adjacency, it has been taken up and used by many researchers in the NLP community and more recently, by researchers in psycholinguistics as well. It has also stimulated the development of similar resources in other languages (Chinese (Zhou and Xue, 2015), Czech (Pol´akov´a et al., 2013), Hindi (Oza et al., 2009), Modern Standard Arabic (Al-Saif and Markert, 2010), Turkish (Zeyrek and Webber, 2008) and French (Danlos et al., 2012)) and domains (biomedical texts (Prasad et al., 2011), conversational dialogues (Tonelli et al., 2010)), the organization of community-level shared tasks on shallow discourse parsing (Xue et al., 2015; Xue et al., 2016), and a cross-lingual discourse annotation of parallel texts, the TED-MDB corpus (Zeyrek et al., 2018), to support both linguistic understanding of coherence in different languages and improvements in machine translation of discourse connectives. Given only three years in which to develop guidelines, and annotat"
W18-4710,W17-7404,0,0.0608727,"j 0725] Table 2: New senses in PDTB-3 The response to the question can answer the information need explicitly, as in Exs. (17-18), or implicitly (Ex. 19). And the answer can also indicate that the information need cannot be fulfilled (Ex. 20). (19) So can a magazine survive by downright thumbing its nose at major advertisers? Garbage magazine, billed as ”The Practical Journal for the Environment,” is about to find out. [wsj 0062] (20) With all this, can stock prices hold their own? ”The question is unanswerable at this point” she says. [wsj 0681] Because these relations involve dialogue acts (Bunt et al., 2017), which we treat as distinct from discourse relations, and because they are uninstantiable as connectives, we have added a new coherence relation type for them — called HYPOPHORA. Of course, not all questions in a discourse are dialogue acts. H YPOPHORA does not apply when the subsequent text relates to a question in other ways – for example, with rhetorical questions that are posed for dramatic effect or to make an assertion, rather than to elicit an answer, as in Ex. (21), or if the subsequent text provides an explanation for why the question has been asked, as in Ex. (22). In such cases, an"
W18-4710,F12-2042,0,0.260077,"gely because the PDTB was based on the simple idea that discourse relations are grounded in an identifiable set of explicit words or phrases (discourse connectives) or simply in sentence adjacency, it has been taken up and used by many researchers in the NLP community and more recently, by researchers in psycholinguistics as well. It has also stimulated the development of similar resources in other languages (Chinese (Zhou and Xue, 2015), Czech (Pol´akov´a et al., 2013), Hindi (Oza et al., 2009), Modern Standard Arabic (Al-Saif and Markert, 2010), Turkish (Zeyrek and Webber, 2008) and French (Danlos et al., 2012)) and domains (biomedical texts (Prasad et al., 2011), conversational dialogues (Tonelli et al., 2010)), the organization of community-level shared tasks on shallow discourse parsing (Xue et al., 2015; Xue et al., 2016), and a cross-lingual discourse annotation of parallel texts, the TED-MDB corpus (Zeyrek et al., 2018), to support both linguistic understanding of coherence in different languages and improvements in machine translation of discourse connectives. Given only three years in which to develop guidelines, and annotate and release the PDTB, we knew that it would be incomplete (Prasad"
W18-4710,L16-1629,1,0.825458,"In the PDTB, we have as yet found no evidence for the reverse directionality for either of these senses. 6 Conclusion We have presented highlights from our work on enriching the PDTB with new relations, which has also led to modifications and extensions to the PDTB guidelines. Annotating a further ⇠13K discourse relations and reviewing existing PDTB-2 annotation to bring it in line with the new guidelines has highlighted the importance of assessing consistency across the corpus — that similar tokens are annotated in a similar way, no matter when they were annotated. Such semantic consistency (Hollenstein et al., 2016) is meant to facilitate improvement in all future applications of the PDTB-3. Consistency checks are described in the detailed annotation manual that will accompany the corpus in its LDC distribution, as well as being available at the PDTB website. Acknowledgements Missing from the list of authors is the name of our colleague and dear friend, Aravind K Joshi. Aravind conceived of the Penn Discourse TreeBank and was instrumental in its development from its beginnings in 2005 through the new PDTB-3. Aravind died peacefully on 31 December 2017, and we miss him more than we can say. An obituary wi"
W18-4710,J93-2004,0,0.0649142,"Missing"
W18-4710,W09-3029,1,0.76508,"ed in 2008, contains over 40K tokens of annotated relations, making it the largest such corpus available to date. Largely because the PDTB was based on the simple idea that discourse relations are grounded in an identifiable set of explicit words or phrases (discourse connectives) or simply in sentence adjacency, it has been taken up and used by many researchers in the NLP community and more recently, by researchers in psycholinguistics as well. It has also stimulated the development of similar resources in other languages (Chinese (Zhou and Xue, 2015), Czech (Pol´akov´a et al., 2013), Hindi (Oza et al., 2009), Modern Standard Arabic (Al-Saif and Markert, 2010), Turkish (Zeyrek and Webber, 2008) and French (Danlos et al., 2012)) and domains (biomedical texts (Prasad et al., 2011), conversational dialogues (Tonelli et al., 2010)), the organization of community-level shared tasks on shallow discourse parsing (Xue et al., 2015; Xue et al., 2016), and a cross-lingual discourse annotation of parallel texts, the TED-MDB corpus (Zeyrek et al., 2018), to support both linguistic understanding of coherence in different languages and improvements in machine translation of discourse connectives. Given only thr"
W18-4710,J05-1004,0,0.246178,"Missing"
W18-4710,I13-1011,0,0.0350587,"Missing"
W18-4710,prasad-etal-2008-penn,1,0.84406,"chy, and all changes have been propagated through the rest of the corpus. 1 Introduction The last decade has seen growing interest in enabling language technology and psycholinguistics to move beyond the sentence, to what can be derived from larger units of text. This has led to greater interest in the properties of discourse. One such property is the coherence between clauses and sentences arising from low-level discourse relations. This level of meaning has been made overt through manual annotation in the Penn Discourse TreeBank (PDTB), developed with NSF support.1 Version 2.0. of the PDTB (Prasad et al., 2008), released in 2008, contains over 40K tokens of annotated relations, making it the largest such corpus available to date. Largely because the PDTB was based on the simple idea that discourse relations are grounded in an identifiable set of explicit words or phrases (discourse connectives) or simply in sentence adjacency, it has been taken up and used by many researchers in the NLP community and more recently, by researchers in psycholinguistics as well. It has also stimulated the development of similar resources in other languages (Chinese (Zhou and Xue, 2015), Czech (Pol´akov´a et al., 2013),"
W18-4710,C10-2118,1,0.807988,"t, they were encouraged to identify the non-connective expression in Arg2 that they took as the source of the perceived redundancy as the AltLex, Annotating intra-sentential discourse relations in the PDTB-3 has led to modifying the above convention in two ways — what is annotated as AltLex and where AltLex can be annotated. With respect to what is annotated as AltLex, reliably identifiable AltLex expressions in the PDTB-2 included one part that conveyed the relation and one part that referred anaphorically or elliptically to Arg1, as in “after that” or “a likely reason for the disparity is” (Prasad et al., 2010). To allow for AltLex expressions in the context of intra-sentential discourse relations, we have allowed expressions of any form or syntactic class to be labeled as AltLex, including adjectives and adjective-modifiers such as additional, next, further, and earlier. While these expressions continue to suggest the relation, unlike AltLex expressions in PDTB-2, the reference to Arg1 may be implicit. That is, while next implies next to something, that something may be implicit. One consequence of this new convention is that words such as further and next, that can appear as discourse adverbials,"
W18-4710,J14-4007,1,0.837648,", 2012)) and domains (biomedical texts (Prasad et al., 2011), conversational dialogues (Tonelli et al., 2010)), the organization of community-level shared tasks on shallow discourse parsing (Xue et al., 2015; Xue et al., 2016), and a cross-lingual discourse annotation of parallel texts, the TED-MDB corpus (Zeyrek et al., 2018), to support both linguistic understanding of coherence in different languages and improvements in machine translation of discourse connectives. Given only three years in which to develop guidelines, and annotate and release the PDTB, we knew that it would be incomplete (Prasad et al., 2014). With additional support from the NSF, we have now addressed many of the gaps in the corpus, adding over 17K new discourse relations. Most of the new relations occur intra-sententially, but there are also ⇠300 inter-sentential implicit relations between adjacent sentences whose annotation is missing from the PDTB-2.2 This paper focuses on the new intrasentential relations annotated in the PDTB-3. We also discuss major modifications and extensions to the PDTB guidelines, including the sense hierarchy, which have resulted from our study of the new relations, and which have been propagated throu"
W18-4710,W15-2707,1,0.887923,"Missing"
W18-4710,W17-5502,1,0.813461,"tential implicit relations between adjacent sentences whose annotation is missing from the PDTB-2.2 This paper focuses on the new intrasentential relations annotated in the PDTB-3. We also discuss major modifications and extensions to the PDTB guidelines, including the sense hierarchy, which have resulted from our study of the new relations, and which have been propagated throughout the corpus. PDTB-3, which we plan to release to the community in Fall 2018, will contain over 53K tokens of discourse relations, and as with PDTB-2, will 1 http://www.seas.upenn.edu/˜pdtb Separate from the PDTB-3, Prasad et al. (2017) address the annotation of cross-paragraph implicit relations that are not annotated in either PDTB-2 or PDTB-3. These annotations are provided for 145 texts from Sections 01, 06, and 23 of the Wall Street Journal corpus, producing a full-text annotated sub-corpus merged with the PDTB-3 annotations for the same texts. However, because the annotation guidelines developed for the cross-paragraph annotation depart in some respects from the PDTB guidelines in ways not incorporated in PDTB-3, these annotations will be released to the community separately, via github (https://github.com/pdtb-upenn/f"
W18-4710,tonelli-etal-2010-annotation,1,0.78837,"fiable set of explicit words or phrases (discourse connectives) or simply in sentence adjacency, it has been taken up and used by many researchers in the NLP community and more recently, by researchers in psycholinguistics as well. It has also stimulated the development of similar resources in other languages (Chinese (Zhou and Xue, 2015), Czech (Pol´akov´a et al., 2013), Hindi (Oza et al., 2009), Modern Standard Arabic (Al-Saif and Markert, 2010), Turkish (Zeyrek and Webber, 2008) and French (Danlos et al., 2012)) and domains (biomedical texts (Prasad et al., 2011), conversational dialogues (Tonelli et al., 2010)), the organization of community-level shared tasks on shallow discourse parsing (Xue et al., 2015; Xue et al., 2016), and a cross-lingual discourse annotation of parallel texts, the TED-MDB corpus (Zeyrek et al., 2018), to support both linguistic understanding of coherence in different languages and improvements in machine translation of discourse connectives. Given only three years in which to develop guidelines, and annotate and release the PDTB, we knew that it would be incomplete (Prasad et al., 2014). With additional support from the NSF, we have now addressed many of the gaps in the cor"
W18-4710,W16-1704,1,0.918526,"flection, Mr. Oka says, he concluded that Nissan is being prudent in following its slow-startup strategy instead of simply copying Lexus. (EXPANSION . SUBSTITUTION . ARG 1- AS - SUBST ) [ WSJ 0286] Third, the restriction on arguments to clauses (with a small set of specific exceptions) precluded relations between conjoined verb phrases. The PDTB-2 exceptions to clausal realization did allow verb phrases to be valid arguments, but not of the VP conjunction itself. Thus, in Ex. (7), while because was annotated, the VP conjunction and was not. Conjoined VPs have now been annotated in the PDTB-3 (Webber et al., 2016), as in Ex. (8) and Ex. (9). (7) She became an abortionist accidentally, and continued because it enabled her to buy jam, cocoa and other warrationed goodies. (CONTINGENCY. CAUSE . REASON) [wsj 0039] (8) She became an abortionist accidentally, and continued because it enabled her to buy jam, cocoa and other warrationed goodies. (EXPANSION . CONJUNCTION) [wsj 0039] 88 Intra-S Context Free Adjuncts Free TO-infinitives Prep. Clausal Subordination Conjoined VPs S Conjunction Implicits Total ˜Num ˜2200 ˜1500 ˜1600 ˜5800 ˜1800 ˜13000 Table 1: Approximate distribution of new intra-sentential relation"
W18-4710,J05-2005,0,0.140369,"ONDITION) [wsj 0443] Since researchers may be interested in analyzing these constructional AltLex’s further, we have assigned them the relation type A LT L EX C, to indicate that they are a sub-type of Altlex. Tokens of this type have all the same fields as an AltLex. They are just marked for easy identification and review. 94 5 Mapping to ISO-DR-Core Existing annotation frameworks (which, apart from the PDTB, have led to the creation of several other corpora of coherence relations, including Afantenos et al. (2012), Carlson et al. (2003), Reese et al. (2007), Sanders and Scholman (2012), and Wolf and Gibson (2005)) exhibit some major differences in their underlying assumptions, but there are also strong compatibilities. ISO DR-Core (ISO 2476178: 2016) forms part of an effort to develop an international standard for the annotation of discourse relations.4 One of the outcomes of this effort (Bunt and Prasad, 2016) was to provide clear and mutually consistent definitions of a set of core discourse relations (senses) – ISO-DR-Core – many of which have similar definitions in different frameworks, and provide mappings from ISO-DR-Core relations to relations in different frameworks, including the PDTB. With t"
W18-4710,K15-2001,1,0.915269,"Missing"
W18-4710,K16-2001,1,0.85992,"used by many researchers in the NLP community and more recently, by researchers in psycholinguistics as well. It has also stimulated the development of similar resources in other languages (Chinese (Zhou and Xue, 2015), Czech (Pol´akov´a et al., 2013), Hindi (Oza et al., 2009), Modern Standard Arabic (Al-Saif and Markert, 2010), Turkish (Zeyrek and Webber, 2008) and French (Danlos et al., 2012)) and domains (biomedical texts (Prasad et al., 2011), conversational dialogues (Tonelli et al., 2010)), the organization of community-level shared tasks on shallow discourse parsing (Xue et al., 2015; Xue et al., 2016), and a cross-lingual discourse annotation of parallel texts, the TED-MDB corpus (Zeyrek et al., 2018), to support both linguistic understanding of coherence in different languages and improvements in machine translation of discourse connectives. Given only three years in which to develop guidelines, and annotate and release the PDTB, we knew that it would be incomplete (Prasad et al., 2014). With additional support from the NSF, we have now addressed many of the gaps in the corpus, adding over 17K new discourse relations. Most of the new relations occur intra-sententially, but there are also"
W18-4710,I08-7009,1,0.83098,"st such corpus available to date. Largely because the PDTB was based on the simple idea that discourse relations are grounded in an identifiable set of explicit words or phrases (discourse connectives) or simply in sentence adjacency, it has been taken up and used by many researchers in the NLP community and more recently, by researchers in psycholinguistics as well. It has also stimulated the development of similar resources in other languages (Chinese (Zhou and Xue, 2015), Czech (Pol´akov´a et al., 2013), Hindi (Oza et al., 2009), Modern Standard Arabic (Al-Saif and Markert, 2010), Turkish (Zeyrek and Webber, 2008) and French (Danlos et al., 2012)) and domains (biomedical texts (Prasad et al., 2011), conversational dialogues (Tonelli et al., 2010)), the organization of community-level shared tasks on shallow discourse parsing (Xue et al., 2015; Xue et al., 2016), and a cross-lingual discourse annotation of parallel texts, the TED-MDB corpus (Zeyrek et al., 2018), to support both linguistic understanding of coherence in different languages and improvements in machine translation of discourse connectives. Given only three years in which to develop guidelines, and annotate and release the PDTB, we knew tha"
W18-4710,L18-1301,0,0.0149818,"cs as well. It has also stimulated the development of similar resources in other languages (Chinese (Zhou and Xue, 2015), Czech (Pol´akov´a et al., 2013), Hindi (Oza et al., 2009), Modern Standard Arabic (Al-Saif and Markert, 2010), Turkish (Zeyrek and Webber, 2008) and French (Danlos et al., 2012)) and domains (biomedical texts (Prasad et al., 2011), conversational dialogues (Tonelli et al., 2010)), the organization of community-level shared tasks on shallow discourse parsing (Xue et al., 2015; Xue et al., 2016), and a cross-lingual discourse annotation of parallel texts, the TED-MDB corpus (Zeyrek et al., 2018), to support both linguistic understanding of coherence in different languages and improvements in machine translation of discourse connectives. Given only three years in which to develop guidelines, and annotate and release the PDTB, we knew that it would be incomplete (Prasad et al., 2014). With additional support from the NSF, we have now addressed many of the gaps in the corpus, adding over 17K new discourse relations. Most of the new relations occur intra-sententially, but there are also ⇠300 inter-sentential implicit relations between adjacent sentences whose annotation is missing from t"
W19-0411,L16-1629,1,0.892074,"Missing"
W19-0411,D14-1036,0,0.0194314,"Missing"
W19-0411,P09-2004,0,0.594105,"be subject to both usage and sense ambiguity, as has already been discussed in the literature. But discourse connectives are no different from other linguistic expressions in being subject to other types of ambiguity as well. Four are illustrated and discussed here. 1 Introduction Discourse connectives, like other linguistic expressions, are subject to ambiguity. Two types of ambiguity — usage ambiguity, whether or not a given token is serving as a discourse connective in its context, and sense ambiguity, what discourse relation(s) a given token is signalling — were the subject of a study by Pitler and Nenkova (2009), who showed how syntactic features could help resolve them both. But discourse connectives are no different from other linguistic expressions in being subject to other types of ambiguity as well. Four of them are discussed here, as a way of encouraging researchers to determine whether existing disambiguation methods suffice to handle them or whether the methods need to be extended. Ignoring the full range of ambiguity of discourse connectives can lead to discourse relations being mis-labelled both manually (during annotation) and automatically (during discourse parsing). As background to pres"
W19-0411,prasad-etal-2008-penn,1,0.687912,"ve been discussed in the context of other linguistic forms. Section 3 discusses part-of-speech ambiguity, which can affect how a given token functions as a discourse connective. Section 4 discusses multi-word ambiguity, where a sequence of tokens can be ambiguous between a sequence of separate elements and a single multi-word discourse connective. Section 5 discusses a scope ambiguity that affects the sense of discourse connectives. Finally, Section 6 discusses semantic role ambiguity involving the arguments of certain CONCESSION relations. 2 2.1 Background PDTB-2 The Penn Discourse Treebank (Prasad et al., 2008) was created as the largest public repository of annotated discourse relations (over 43K), including over 18.4K signalled by explicit discourse connectives (coordinating or subordinating conjunctions, or discourse adverbials). All relations in the corpus are labelled with either one or two senses from a three-level sense hierarchy, whose top level comprised four non-terminal senses: E XPANSION, C OMPARISON, C ONTINGENCY and T EMPORAL. Most discourse relations were labelled with terminal senses, except where annotators were unable to decide and backed off to a level-2 (or in some cases, a top-l"
W19-0411,J14-4007,1,0.825043,"implicit connectives that signalled the sense(s) they inferred to hold between the arguments. The approach in the PDTB-2 is agnostic about any higher-level discourse structure, and as such, made no attempt to build a tree or graph structure of relations over the text as a whole. The size and availability of the PDTB-2 spawned the field of shallow discourse parsing, as in the 2015 and 2016 CoNLL shared tasks (Xue et al., 2015, 2016), as well as the development of similar resources for other languages, including Chinese, Hindi, and Turkish. An in-depth discussion of the PDTB-2 can be found in (Prasad et al., 2014). 2.2 Pitler & Nenkova (2009) Pitler and Nenkova (2009) showed how syntactic features could be used in disambiguating both usage ambiguity and sense ambiguity. To understand these types of ambiguity, consider the word since. Ex. 1 illustrates its non-discourse usage, where since is simply a temporal preposition. Both Ex. 2 and Ex. 3 illustrate discourse usages and also the sense ambiguity of since, signalling a purely temporal relation in Ex. 2 and a purely causal relation in Ex. 3. (1) She has been up since 5am. (2) There have been over 100 mergers since the most recent wave of friendly takeo"
W19-0411,K15-2001,1,0.873247,"consisted of two arguments labelled Arg1 and Arg2, with each relation anchored by either an explicit discourse connective or adjacency. In the latter case, annotators inserted one or more implicit connectives that signalled the sense(s) they inferred to hold between the arguments. The approach in the PDTB-2 is agnostic about any higher-level discourse structure, and as such, made no attempt to build a tree or graph structure of relations over the text as a whole. The size and availability of the PDTB-2 spawned the field of shallow discourse parsing, as in the 2015 and 2016 CoNLL shared tasks (Xue et al., 2015, 2016), as well as the development of similar resources for other languages, including Chinese, Hindi, and Turkish. An in-depth discussion of the PDTB-2 can be found in (Prasad et al., 2014). 2.2 Pitler & Nenkova (2009) Pitler and Nenkova (2009) showed how syntactic features could be used in disambiguating both usage ambiguity and sense ambiguity. To understand these types of ambiguity, consider the word since. Ex. 1 illustrates its non-discourse usage, where since is simply a temporal preposition. Both Ex. 2 and Ex. 3 illustrate discourse usages and also the sense ambiguity of since, signall"
W19-0411,K16-2001,1,0.909654,"Missing"
