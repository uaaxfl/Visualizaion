2020.acl-main.631,D14-1179,0,0.0682267,"Missing"
2020.acl-main.631,N19-1423,0,0.640741,"Term Extraction Aspect term extraction (ATE) and sentiment classiﬁcation are two fundamental subtasks of aspectbased sentiment analysis. While the former aims to extract aspect terms in review sentences, the latter tries to determine their sentiment polarities. To deal with ATE, many traditional techniques like syntactic rules (Qiu et al., 2011), hidden Markov models (Jin et al., 2009), and conditional random ﬁelds (Li et al., 2010; Toh and Su, 2016) have been explored. Recently, neural network techniques such as LSTM (Liu et al., 2015), CNN (Xu et al., 2018), and attention (Li et al., 2018; Devlin et al., 2019) have been applied for ATE. Luo et al. (2019) and He et al. (2019) further proposed to predict aspect term and polarity jointly in a multi-task learning approach so as to take advantage of their relatedness. Generally, the above approaches treat ATE as a sequence labeling problem. In their pioneering work, Ma et al. (2019) formulated ATE as a sequence-to-sequence task. So far, one of the remaining challenges for ATE lies in the lack of annotated data, especially when today’s neural models are becoming increasingly large and complex. 2.2 Text Data Augmentation Generative adversarial network (GA"
2020.acl-main.631,D18-1045,0,0.0184292,"Generative adversarial network (GAN) (Goodfellow et al., 2014) and variational autoencoder (VAE) (Kingma and Welling, 2013) are two neural network based generative models that are capable of generating text conditioned on input text and can be applied for data augmentation of sentence-level sentiment analysis (Gupta, 2019; Hu et al., 2017). These methods encode an input text into latent variables and generate new texts by decoding the latent variables in continuous space. However, they can hardly ensure high-quality sentences in terms of readability and label compatibility. Back translation (Edunov et al., 2018; Sennrich et al., 2016) is another augmentation approach for text data, but is less controllable, although it is good at maintaining the global semantics of an original sentence. As a class of replacement approach, Zhang et al. (2015) and Wang and Yang (2015) proposed to substitute all replaceable words with corresponding synonyms from WordNet (Miller, 1995). Differently, Kobayashi (2018) and Wu et al. (2019) proposed to randomly replace words with those predicted by a pre-trained language model. Nevertheless, none of the above augmentation approaches is applicable for aspect term extraction"
2020.acl-main.631,P19-1048,0,0.174641,"ion are two fundamental subtasks of aspectbased sentiment analysis. While the former aims to extract aspect terms in review sentences, the latter tries to determine their sentiment polarities. To deal with ATE, many traditional techniques like syntactic rules (Qiu et al., 2011), hidden Markov models (Jin et al., 2009), and conditional random ﬁelds (Li et al., 2010; Toh and Su, 2016) have been explored. Recently, neural network techniques such as LSTM (Liu et al., 2015), CNN (Xu et al., 2018), and attention (Li et al., 2018; Devlin et al., 2019) have been applied for ATE. Luo et al. (2019) and He et al. (2019) further proposed to predict aspect term and polarity jointly in a multi-task learning approach so as to take advantage of their relatedness. Generally, the above approaches treat ATE as a sequence labeling problem. In their pioneering work, Ma et al. (2019) formulated ATE as a sequence-to-sequence task. So far, one of the remaining challenges for ATE lies in the lack of annotated data, especially when today’s neural models are becoming increasingly large and complex. 2.2 Text Data Augmentation Generative adversarial network (GAN) (Goodfellow et al., 2014) and variational autoencoder (VAE) (Ki"
2020.acl-main.631,P18-1031,0,0.0507968,"The above augmentation is modeled as a ﬁnegrained conditional language generation task implemented by a masked sequence-to-sequence generation model. As depicted in Figure 2, the model adopts Transformer (Vaswani et al., 2017) as its basic architecture, and consists of a 6-layer encoder and a 6-layer decoder with 12 attention heads in each layer. The embedding size and hidden size are both 768, and the feed-forward ﬁlter size is 3072. The generation model is initialized with the pre-trained weights of MASS. To further incorporate the domain knowledge, we perform in-domain pre-training as in (Howard and Ruder, 2018).1 3.2.1 Training The training process is illustrated in Algorithm 1. For each batch, we ﬁrst sample a few examples from the training set with replacement (Line 4) according to a probability p speciﬁed in Equation (1). The chosen examples are then masked using the Fragment Masking Strategy function (Line 6) to generate training examples for our model. We elaborate on Algorithm 1 in the following paragraphs. Fragment Masking Strategy 3.1 Problem Formulation Given a training set D of review texts, in which each sample includes a sequence of n words X = [x1 , x2 , ..., xn ] and a label sequence L"
2020.acl-main.631,N18-2072,0,0.0769429,"p, the real difﬁculty of data augmentation in ATE is generating a new sentence while aligning with the original label sequence and making the original aspect term remain an opinion target. Existing augmentation models such as GAN (Goodfellow et al., 2014) and VAE (Kingma and Welling, 2013) tend to change the opinion target unpredictably and thus are not applicable for this task. Another genre of augmentation strategy is based on word replacement. It generates a new sentence by replacing one or multiple words with their synonyms (Zhang et al., 2015) or with words predicted by a language model (Kobayashi, 2018). This approach seems to be able to address the above issue in ATE augmentation, yet it only brings very lim7056 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7056–7066 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ited changes to the original sentences and cannot produce diversiﬁed sentences. Intuitively, augmentation strategies are effective when they increase the diversity of training data seen by a model. We argue in this paper that the augmentation for aspect term extraction calls for a conditional approach, which is t"
2020.acl-main.631,C10-1074,0,0.118157,"ugmentation method works, and test its implementation with other language models to illustrate why this masked sequence-to-sequence framework is favored. 2 2.1 Related Work Aspect Term Extraction Aspect term extraction (ATE) and sentiment classiﬁcation are two fundamental subtasks of aspectbased sentiment analysis. While the former aims to extract aspect terms in review sentences, the latter tries to determine their sentiment polarities. To deal with ATE, many traditional techniques like syntactic rules (Qiu et al., 2011), hidden Markov models (Jin et al., 2009), and conditional random ﬁelds (Li et al., 2010; Toh and Su, 2016) have been explored. Recently, neural network techniques such as LSTM (Liu et al., 2015), CNN (Xu et al., 2018), and attention (Li et al., 2018; Devlin et al., 2019) have been applied for ATE. Luo et al. (2019) and He et al. (2019) further proposed to predict aspect term and polarity jointly in a multi-task learning approach so as to take advantage of their relatedness. Generally, the above approaches treat ATE as a sequence labeling problem. In their pioneering work, Ma et al. (2019) formulated ATE as a sequence-to-sequence task. So far, one of the remaining challenges for"
2020.acl-main.631,D15-1168,0,0.084018,"asked sequence-to-sequence framework is favored. 2 2.1 Related Work Aspect Term Extraction Aspect term extraction (ATE) and sentiment classiﬁcation are two fundamental subtasks of aspectbased sentiment analysis. While the former aims to extract aspect terms in review sentences, the latter tries to determine their sentiment polarities. To deal with ATE, many traditional techniques like syntactic rules (Qiu et al., 2011), hidden Markov models (Jin et al., 2009), and conditional random ﬁelds (Li et al., 2010; Toh and Su, 2016) have been explored. Recently, neural network techniques such as LSTM (Liu et al., 2015), CNN (Xu et al., 2018), and attention (Li et al., 2018; Devlin et al., 2019) have been applied for ATE. Luo et al. (2019) and He et al. (2019) further proposed to predict aspect term and polarity jointly in a multi-task learning approach so as to take advantage of their relatedness. Generally, the above approaches treat ATE as a sequence labeling problem. In their pioneering work, Ma et al. (2019) formulated ATE as a sequence-to-sequence task. So far, one of the remaining challenges for ATE lies in the lack of annotated data, especially when today’s neural models are becoming increasingly lar"
2020.acl-main.631,P19-1056,0,0.160379,"d sentiment classiﬁcation are two fundamental subtasks of aspectbased sentiment analysis. While the former aims to extract aspect terms in review sentences, the latter tries to determine their sentiment polarities. To deal with ATE, many traditional techniques like syntactic rules (Qiu et al., 2011), hidden Markov models (Jin et al., 2009), and conditional random ﬁelds (Li et al., 2010; Toh and Su, 2016) have been explored. Recently, neural network techniques such as LSTM (Liu et al., 2015), CNN (Xu et al., 2018), and attention (Li et al., 2018; Devlin et al., 2019) have been applied for ATE. Luo et al. (2019) and He et al. (2019) further proposed to predict aspect term and polarity jointly in a multi-task learning approach so as to take advantage of their relatedness. Generally, the above approaches treat ATE as a sequence labeling problem. In their pioneering work, Ma et al. (2019) formulated ATE as a sequence-to-sequence task. So far, one of the remaining challenges for ATE lies in the lack of annotated data, especially when today’s neural models are becoming increasingly large and complex. 2.2 Text Data Augmentation Generative adversarial network (GAN) (Goodfellow et al., 2014) and variational"
2020.acl-main.631,P19-1344,0,0.699694,"s (Qiu et al., 2011), hidden Markov models (Jin et al., 2009), and conditional random ﬁelds (Li et al., 2010; Toh and Su, 2016) have been explored. Recently, neural network techniques such as LSTM (Liu et al., 2015), CNN (Xu et al., 2018), and attention (Li et al., 2018; Devlin et al., 2019) have been applied for ATE. Luo et al. (2019) and He et al. (2019) further proposed to predict aspect term and polarity jointly in a multi-task learning approach so as to take advantage of their relatedness. Generally, the above approaches treat ATE as a sequence labeling problem. In their pioneering work, Ma et al. (2019) formulated ATE as a sequence-to-sequence task. So far, one of the remaining challenges for ATE lies in the lack of annotated data, especially when today’s neural models are becoming increasingly large and complex. 2.2 Text Data Augmentation Generative adversarial network (GAN) (Goodfellow et al., 2014) and variational autoencoder (VAE) (Kingma and Welling, 2013) are two neural network based generative models that are capable of generating text conditioned on input text and can be applied for data augmentation of sentence-level sentiment analysis (Gupta, 2019; Hu et al., 2017). These methods e"
2020.acl-main.631,D14-1162,0,0.0837727,"erate four augmented sentences according to Section 3.2.2 with the proportion r set to 0.5. The four new sentences are allocated to four different sets. This leads to four generated datasets. 4.2.2 ATE Models To examine our data augmentation method, we use the original training sets and the augmented training sets to train several ATE models. The details of these models are as follows. BiLSTM-CRF is a popular model for sequence labeling tasks. Its structure includes a BiLSTM followed by a CRF layer (Lafferty et al., 2001). The word embeddings for this model are initialized by GloVe-840B-300d (Pennington et al., 2014) and ﬁxed during training. The hidden size is set to 300, and we use Adam (Kingma and Ba, 2014) with a learning rate of 1e-4 and L2 weight decay of 1e-5 to optimize this model. Seq2Seq for ATE (Ma et al., 2019) is the ﬁrst effort to apply a sequence-to-sequence model for aspect term extraction. It adopts GRU (Cho et al., 2014) for both the encoder and the decoder. The encoder takes a source sentence as input, and the decoder generates a label sequence as the result. This approach is also equipped with a gated unit network and a position-aware attention network. BERT for token classiﬁcation (De"
2020.acl-main.631,S16-1002,0,0.131517,"Missing"
2020.acl-main.631,S14-2004,0,0.17422,"X, model tends not to generate a same segment as the old one when the masked segment is longer than 4. The above process can be run multiple times with different start positions, and generates multiple new examples from a source example. In this approach, each source example is augmented in turn. 7059 4 Experiment In this section, we ﬁrst introduce the experimental datasets and several popular ATE models. Then, we report the experimental results, which are obtained by averaging ﬁve runs with different initializations. 4.1 Datasets Two widely-used datasets, the Laptop from SemEval 2014 Task 4 (Pontiki et al., 2014) and the Restaurants from SemEval 2016 Task 5 (Pontiki et al., 2016), are used for our evaluations. The statistics of the two datasets are shown in Table 1, which tells clearly that there are only a limited number of samples in both datasets. Dataset Training #Sent Laptop 3045 Restaurant 2000 Testing #Aspect #Sent 2358 1743 800 676 #Aspect 654 622 Table 1: Statistics of our datasets. #Sent and #Aspect denote the count of sentence and aspect, respectively. 4.2 Experimental Setup 4.2.1 Dataset Augmentation For each of the two datasets, we hold out 150 examples from the original training set for"
2020.acl-main.631,J11-1002,0,0.130006,"han previous approaches. • We provide qualitative analysis and discussions as to why our augmentation method works, and test its implementation with other language models to illustrate why this masked sequence-to-sequence framework is favored. 2 2.1 Related Work Aspect Term Extraction Aspect term extraction (ATE) and sentiment classiﬁcation are two fundamental subtasks of aspectbased sentiment analysis. While the former aims to extract aspect terms in review sentences, the latter tries to determine their sentiment polarities. To deal with ATE, many traditional techniques like syntactic rules (Qiu et al., 2011), hidden Markov models (Jin et al., 2009), and conditional random ﬁelds (Li et al., 2010; Toh and Su, 2016) have been explored. Recently, neural network techniques such as LSTM (Liu et al., 2015), CNN (Xu et al., 2018), and attention (Li et al., 2018; Devlin et al., 2019) have been applied for ATE. Luo et al. (2019) and He et al. (2019) further proposed to predict aspect term and polarity jointly in a multi-task learning approach so as to take advantage of their relatedness. Generally, the above approaches treat ATE as a sequence labeling problem. In their pioneering work, Ma et al. (2019) for"
2020.acl-main.631,P18-2124,0,0.0427537,"Missing"
2020.acl-main.631,D16-1264,0,0.023476,". We refer to this model as BERT-FTC in the following paragraphs. DE-CNN (Xu et al., 2018) uses two types of word embeddings: general-purpose and domainspeciﬁc embeddings.3 While the former adopt GloVe-840B-300d, the latter are trained on a review corpus. They are concatenated and fed to a CNN model of 4 stacked convolutional layers. BERT-PT (Xu et al., 2019)4 utilizes the weights of pre-trained BERT for initialization. To adapt to both domain knowledge and task-speciﬁc knowledge, it is then post-trained on a large-scale unsupervised domain dataset and a machine reading comprehension dataset (Rajpurkar et al., 2016, 2018). So far, it is the state of the art for ATE. The above models are all open-sourced and their default settings are employed in our experiments. 4.3 4.3.1 Results and Analysis Effect of Double Augmentation We combine the original training set with each of the four generated datasets (refer to 4.2.1) and obtain four augmented training sets, each doubling the original training set in size. For each model, we train it on the four augmented training sets, respectively, and take their average F1-scores on the test set. By comparing this score with the model trained on the original training se"
2020.acl-main.631,P16-1009,0,0.0486454,"al network (GAN) (Goodfellow et al., 2014) and variational autoencoder (VAE) (Kingma and Welling, 2013) are two neural network based generative models that are capable of generating text conditioned on input text and can be applied for data augmentation of sentence-level sentiment analysis (Gupta, 2019; Hu et al., 2017). These methods encode an input text into latent variables and generate new texts by decoding the latent variables in continuous space. However, they can hardly ensure high-quality sentences in terms of readability and label compatibility. Back translation (Edunov et al., 2018; Sennrich et al., 2016) is another augmentation approach for text data, but is less controllable, although it is good at maintaining the global semantics of an original sentence. As a class of replacement approach, Zhang et al. (2015) and Wang and Yang (2015) proposed to substitute all replaceable words with corresponding synonyms from WordNet (Miller, 1995). Differently, Kobayashi (2018) and Wu et al. (2019) proposed to randomly replace words with those predicted by a pre-trained language model. Nevertheless, none of the above augmentation approaches is applicable for aspect term extraction task, as they are all ta"
2020.acl-main.631,D19-1322,0,0.0397957,"d not to extract these aspect terms in the test stage. As a result, the model makes many false-negative errors, leading to poor Recall scores. This indicates that label embeddings are helpful for generating qualiﬁed sentences for aspect term extraction. 5.3 Why Sequence-to-Sequence Generation? As mentioned before, we formulate the data augmentation for aspect term extraction as a conditional generation problem that is solved by masked sequence-to-sequence learning. One may argue that other pre-trained language models like BERT and GPT-2 are also competent for this task as in (Wu et al., 2019; Sudhakar et al., 2019; Keskar et al., 2019). Here we compare them and demonstrate the superiority of our approach in this task. Following some previous work (Wu et al., 2019; Sudhakar et al., 2019; Keskar et al., 2019), we modify the settings of BERT and GPT-2 to make them ﬁt this task. Readers are recommended to refer to Appendix for more details. Moreover, a widelyused replacement-based method is implemented for comparison, in which half of the tokens are randomly replaced by their synonyms from WordNet (Miller, 1995). We use ﬂuency6 and BLEU7 to evaluate the generated sentences. Note that these datasets do not"
2020.acl-main.631,S16-1045,0,0.153159,"d works, and test its implementation with other language models to illustrate why this masked sequence-to-sequence framework is favored. 2 2.1 Related Work Aspect Term Extraction Aspect term extraction (ATE) and sentiment classiﬁcation are two fundamental subtasks of aspectbased sentiment analysis. While the former aims to extract aspect terms in review sentences, the latter tries to determine their sentiment polarities. To deal with ATE, many traditional techniques like syntactic rules (Qiu et al., 2011), hidden Markov models (Jin et al., 2009), and conditional random ﬁelds (Li et al., 2010; Toh and Su, 2016) have been explored. Recently, neural network techniques such as LSTM (Liu et al., 2015), CNN (Xu et al., 2018), and attention (Li et al., 2018; Devlin et al., 2019) have been applied for ATE. Luo et al. (2019) and He et al. (2019) further proposed to predict aspect term and polarity jointly in a multi-task learning approach so as to take advantage of their relatedness. Generally, the above approaches treat ATE as a sequence labeling problem. In their pioneering work, Ma et al. (2019) formulated ATE as a sequence-to-sequence task. So far, one of the remaining challenges for ATE lies in the lac"
2020.acl-main.631,D15-1306,0,0.0561585,"ta augmentation of sentence-level sentiment analysis (Gupta, 2019; Hu et al., 2017). These methods encode an input text into latent variables and generate new texts by decoding the latent variables in continuous space. However, they can hardly ensure high-quality sentences in terms of readability and label compatibility. Back translation (Edunov et al., 2018; Sennrich et al., 2016) is another augmentation approach for text data, but is less controllable, although it is good at maintaining the global semantics of an original sentence. As a class of replacement approach, Zhang et al. (2015) and Wang and Yang (2015) proposed to substitute all replaceable words with corresponding synonyms from WordNet (Miller, 1995). Differently, Kobayashi (2018) and Wu et al. (2019) proposed to randomly replace words with those predicted by a pre-trained language model. Nevertheless, none of the above augmentation approaches is applicable for aspect term extraction task, as they are all targeted at sentence-level classiﬁcation and may change opinion targets and aspect labels unexpectedly during augmentation. 2.3 MASS Pre-training a large language model and ﬁne-tuning it on downstream tasks has become a new paradigm. MASS"
2020.acl-main.631,N19-1242,0,0.0635178,"work and a position-aware attention network. BERT for token classiﬁcation (Devlin et al., 2019) uses pre-trained BERT with a linear layer. We implement this model using open source2 and initialize its parameters with the pre-trained BERTBASE-UNCASED model. We refer to this model as BERT-FTC in the following paragraphs. DE-CNN (Xu et al., 2018) uses two types of word embeddings: general-purpose and domainspeciﬁc embeddings.3 While the former adopt GloVe-840B-300d, the latter are trained on a review corpus. They are concatenated and fed to a CNN model of 4 stacked convolutional layers. BERT-PT (Xu et al., 2019)4 utilizes the weights of pre-trained BERT for initialization. To adapt to both domain knowledge and task-speciﬁc knowledge, it is then post-trained on a large-scale unsupervised domain dataset and a machine reading comprehension dataset (Rajpurkar et al., 2016, 2018). So far, it is the state of the art for ATE. The above models are all open-sourced and their default settings are employed in our experiments. 4.3 4.3.1 Results and Analysis Effect of Double Augmentation We combine the original training set with each of the four generated datasets (refer to 4.2.1) and obtain four augmented traini"
2020.acl-main.631,P18-2094,0,0.708509,"1: Examples of ATE augmentation, where B, I and O denote that a word is the beginning, inside and outside of opinion target, respectively. Aspect term extraction (ATE), which aims to identify and extract the aspects on which users express their sentiments (Hu and Liu, 2004; Liu, 2012), is a fundamental task in aspect-level sentiment analysis. For example, in the sentence of “The screen is very large and crystal clear with amazing colors and resolution”, “screen”, “colors” and “resolution” are the aspect terms to extract in this task. ATE is typically formulated as a sequence labeling problem (Xu et al., 2018, 2019; Li et al., 2018), where each word is appended with a label indicating if it identiﬁes an aspect. Sentence and label sequence are both used to train a ATE model. One of the remaining challenges with this task is the shortage of annotated data. While data augmentation appears to be a solution to this problem, it Corresponding author. O is 漡match the original label sequence, and 瀡screen瀢 is the opinion target in this sentence Introduction ∗ I O faces two main obstacles here. First, the new sentences must adhere to their original label sequences strictly. As shown in Figure 1, the generati"
2020.acl-main.734,D15-1141,0,0.592706,"onally serves as the first step in Chinese language processing, especially for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling methodology, recent ap"
2020.acl-main.734,P17-1110,0,0.372844,"ese language processing, especially for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling methodology, recent approaches with neural networks are prove"
2020.acl-main.734,D17-1070,0,0.02872,"reating that n-gram as a segment: DL(D) = − Word Embedding Size Hidden State Size Hidden State Layers Key Embedding Size Value Embedding Size Dropout Rate Bi-LSTM Model Implementation Following previous studies (Sun and Xu, 2011; Chen et al., 2015, 2017; Ma et al., 2018; Qiu et al., 2019), we use four segmentation labels in our experiments, i.e., T = {B, I, E, S}. Among them, B, I, and E indicate a character is the beginning, inside, and the ending of a word and S denotes that the character is a single-character word. Since text representation plays an important role to facilitate many tasks (Conneau et al., 2017; Song et al., 2017, 2018; Sileo et al., 2019), we try two effective and well-known encoders, i.e., Bi-LSTM and BERT4 . In addition, we test WMS EG on a pretrained encoder for Chinese language, i.e., ZEN5 (Diao et al., 2019), which learns n-gram information in its pre-training from large raw corpora and outperforms BERT on many Chinese NLP tasks. Table 5 shows the hyperparameter settings for all the encoders: for the Bi-LSTM encoder, we follow the setting of Chen et al. (2015) and adopt their character embeddings for exi , and for BERT and ZEN encoders, we follow the default settings in their"
2020.acl-main.734,J09-4006,0,0.0296813,"guish important n-grams within a certain context and thus improves CWS performance accordingly. Figure 3: Heatmaps of weights learned for (a) keys and (b) values in the memory, and (c) the tags from the decoder, with respect to each character in an input sentence. Higher weights are visualized with darker colors. 5 Related Work As one of the most fundamental NLP tasks for Chinese language processing, CWS has been studied for decades, with two steams of methods, i.e., word-based and character-based ones (Xue and Shen, 2003; Peng et al., 2004; Levow, 2006; Zhao et al., 2006; Zhao and Kit, 2008; Li and Sun, 2009; Song et al., 2009a; Li, 2011; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model performance, which is also observed in many other NLP tasks (Song et al."
2020.acl-main.734,N19-1423,0,0.20461,"tistics of the five benchmark datasets, in terms of the number of character and word tokens and types in each training and test set. Out-of-vocabulary (OOV) rate is the percentage of unseen word tokens in the test set. where Wo is a trainable parameter and the output ai ∈ R|T |is a weight vector with its each dimension corresponding to a segmentation label. 2.3 Text Encoders and Decoders To ensure wordhood memory networks functionalize, one requires to generate hi for each xi by [h1 , h2 , ..., hi , ..., hl ] = Encoder(X ) (6) where the Encoder can be different models, e.g., Bi-LSTM and BERT (Devlin et al., 2019), to represent a sequence of Chinese characters into vectors. Once all ai are generated from the memory for each xi , a decoder takes them to predict a sequence of segmentation labels Yb = yb1 yb2 · · · ybl for X by Yb = Decoder(A) (7) where A = a1 a2 · · · ai · · · al is the sequence of output from Eq. 5. The Decoder can be implemented by different algorithms, such as softmax: exp(ati ) ybi = arg max P|T | t t=1 exp(ai ) (8) where ati is the value at dimension t in ai . Or one can use CRF for the Decoder: exp(Wc · ai + bc ) yi−1 yi exp(Wc · ai ) + bc ybi = arg max P yi ∈T (9) where Wc ∈ R|T |"
2020.acl-main.734,C18-2004,0,0.0148075,"ther experiments and analyses also demonstrate the robustness of our proposed framework with respect to different wordhood measures and the efficiency of wordhood information in cross-domain experiments.1 1 Introduction Unlike most written languages in the world, the Chinese writing system does not use explicit delimiters (e.g., white space) to separate words in written text. Therefore, Chinese word segmentation (CWS) conventionally serves as the first step in Chinese language processing, especially for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextu"
2020.acl-main.734,D18-1529,0,0.175184,"Missing"
2020.acl-main.734,I05-3017,0,0.495569,"e Decoder can be implemented by different algorithms, such as softmax: exp(ati ) ybi = arg max P|T | t t=1 exp(ai ) (8) where ati is the value at dimension t in ai . Or one can use CRF for the Decoder: exp(Wc · ai + bc ) yi−1 yi exp(Wc · ai ) + bc ybi = arg max P yi ∈T (9) where Wc ∈ R|T |×|T |and bc ∈ R|T |are trainable parameters to model the transition for yi−1 to yi . BC C HAR # W ORD # C HAR T YPE # W ORD T YPE # OOV R ATE 3.1 Experimental Settings Datasets We employ five benchmark datasets in our experiments: four of them, namely, MSR, PKU, AS, and C ITY U, are from SIGHAN 2005 Bakeoff (Emerson, 2005) and the fifth one is CTB6 (Xue et al., 2005). AS and C ITY U are in traditional Chinese characters whereas the other three use simplified MZ NW W EB 275K 483K 403K 443K 342K 184K 287K 258K 260K 210K 3K 3K 4K 3K 4K 12K 23K 26K 21K 21K 3.4 6.0 8.9 5.9 7.1 Table 3: Statistics of CTB7 with respect to five different genres. The OOV rate for each genre is computed based on the vocabulary from all the other four genres. ones. Following previous studies (Chen et al., 2015, 2017; Qiu et al., 2019), we convert traditional Chinese characters in AS and C ITY U into simplified ones.3 For MSR, AS, PKU, and"
2020.acl-main.734,J04-1004,0,0.239292,"quence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling methodology, recent approaches with neural networks are proved to be powerful in this task (Chen et al., 2015; Ma et al., 2018; Higashiyama et al., 2019). However, since neural networks (e.g., LSTM) is considered to be able to provide a good modeling of contextual dependencies, less attention is paid to the idea of explicitly leveraging wordhood information of n-grams in the context as what had previously been done in non-neural models. Although some studies sidestepped the idea by incorporating contextual n-grams (Pei"
2020.acl-main.734,N19-1276,0,0.411604,"for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling methodology, recent approaches with neural networks are proved to be powerful in this task (Chen et al.,"
2020.acl-main.734,W99-0701,0,0.099425,"Missing"
2020.acl-main.734,W06-0115,0,0.174747,"he proposed memory mechanism can identify and distinguish important n-grams within a certain context and thus improves CWS performance accordingly. Figure 3: Heatmaps of weights learned for (a) keys and (b) values in the memory, and (c) the tags from the decoder, with respect to each character in an input sentence. Higher weights are visualized with darker colors. 5 Related Work As one of the most fundamental NLP tasks for Chinese language processing, CWS has been studied for decades, with two steams of methods, i.e., word-based and character-based ones (Xue and Shen, 2003; Peng et al., 2004; Levow, 2006; Zhao et al., 2006; Zhao and Kit, 2008; Li and Sun, 2009; Song et al., 2009a; Li, 2011; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model performance, wh"
2020.acl-main.734,P11-1141,0,0.0261476,"in context and thus improves CWS performance accordingly. Figure 3: Heatmaps of weights learned for (a) keys and (b) values in the memory, and (c) the tags from the decoder, with respect to each character in an input sentence. Higher weights are visualized with darker colors. 5 Related Work As one of the most fundamental NLP tasks for Chinese language processing, CWS has been studied for decades, with two steams of methods, i.e., word-based and character-based ones (Xue and Shen, 2003; Peng et al., 2004; Levow, 2006; Zhao et al., 2006; Zhao and Kit, 2008; Li and Sun, 2009; Song et al., 2009a; Li, 2011; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model performance, which is also observed in many other NLP tasks (Song et al., 2009b; Xiong et al., 2011; S"
2020.acl-main.734,P15-1167,0,0.0196888,"ys and (b) values in the memory, and (c) the tags from the decoder, with respect to each character in an input sentence. Higher weights are visualized with darker colors. 5 Related Work As one of the most fundamental NLP tasks for Chinese language processing, CWS has been studied for decades, with two steams of methods, i.e., word-based and character-based ones (Xue and Shen, 2003; Peng et al., 2004; Levow, 2006; Zhao et al., 2006; Zhao and Kit, 2008; Li and Sun, 2009; Song et al., 2009a; Li, 2011; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model performance, which is also observed in many other NLP tasks (Song et al., 2009b; Xiong et al., 2011; Shrestha, 2014; Shi et al., 2016; Diao et al., 2019). Recently, CWS benefits from neural networks and further progress a"
2020.acl-main.734,I13-1181,0,0.0286822,"CWS performance accordingly. Figure 3: Heatmaps of weights learned for (a) keys and (b) values in the memory, and (c) the tags from the decoder, with respect to each character in an input sentence. Higher weights are visualized with darker colors. 5 Related Work As one of the most fundamental NLP tasks for Chinese language processing, CWS has been studied for decades, with two steams of methods, i.e., word-based and character-based ones (Xue and Shen, 2003; Peng et al., 2004; Levow, 2006; Zhao et al., 2006; Zhao and Kit, 2008; Li and Sun, 2009; Song et al., 2009a; Li, 2011; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model performance, which is also observed in many other NLP tasks (Song et al., 2009b; Xiong et al., 2011; Shrestha, 2014; Shi et al., 2016; Diao e"
2020.acl-main.734,D16-1147,0,0.433032,"EG. “N ” denotes a lexicon constructed by wordhood measures. N-grams (keys) appearing in the input sentence “部分居民生活水平” (some residents’ living standard) and the wordhood information (values) of those n-grams are extracted from the lexicon. Then, together with the output from the text encoder, n-grams (keys) and their wordhood information (values) are fed into the memory module, whose output passes through a decoder to get final predictions of segmentation labels for every character in the input sentence. CWS by leveraging wordhood information. In detail, we utilize key-value memory networks (Miller et al., 2016) to incorporate character n-grams with their wordhood measurements in a general sequence labeling paradigm, where the memory module can be incorporated with different prevailing encoders (e.g., BiLSTM and BERT) and decoders (e.g., softmax and CRF). For the memory, we map n-grams and their wordhood information to keys and values in it, respectively, and one can use different wordhood measures to generate such information. Then for each input character, the memory module addresses all the n-grams in the key list that contain the character and uses their corresponding values to generate an output"
2020.acl-main.734,P14-1028,0,0.202698,"ion (CWS) conventionally serves as the first step in Chinese language processing, especially for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling met"
2020.acl-main.734,C04-1081,0,0.4212,"some extent, that the proposed memory mechanism can identify and distinguish important n-grams within a certain context and thus improves CWS performance accordingly. Figure 3: Heatmaps of weights learned for (a) keys and (b) values in the memory, and (c) the tags from the decoder, with respect to each character in an input sentence. Higher weights are visualized with darker colors. 5 Related Work As one of the most fundamental NLP tasks for Chinese language processing, CWS has been studied for decades, with two steams of methods, i.e., word-based and character-based ones (Xue and Shen, 2003; Peng et al., 2004; Levow, 2006; Zhao et al., 2006; Zhao and Kit, 2008; Li and Sun, 2009; Song et al., 2009a; Li, 2011; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model pe"
2020.acl-main.734,N16-1176,0,0.0402135,"Missing"
2020.acl-main.734,W14-3916,0,0.0115495,"1; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model performance, which is also observed in many other NLP tasks (Song et al., 2009b; Xiong et al., 2011; Shrestha, 2014; Shi et al., 2016; Diao et al., 2019). Recently, CWS benefits from neural networks and further progress are made with embeddings (Pei et al., 2014; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017), recurrent neural models (Chen et al., 2015; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019) and even adversarial learning (Chen et al., 2017). To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus (Wang and Xu, 2017; Higashiyama et al., 2019), where"
2020.acl-main.734,N19-1351,0,0.0811794,"Missing"
2020.acl-main.734,W06-0137,1,0.813609,"xt. Therefore, Chinese word segmentation (CWS) conventionally serves as the first step in Chinese language processing, especially for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following th"
2020.acl-main.734,W09-3511,1,0.699376,"Missing"
2020.acl-main.734,C12-2116,1,0.870706,"75.34 70.7 77.33 76.39 95.55 97.2 96.22 96.91 81.40 87.5 73.58 86.91 96.0 95.8 95.95 96.2 96.7 96.4 - 85.4 - WMS EG (BERT-CRF) WMS EG (ZEN-CRF) 98.28 98.40 86.67 84.87 96.51 96.53 86.76 85.36 96.58 96.62 78.48 79.64 97.80 97.93 87.57 90.15 97.16 97.25 88.00 88.46 Table 7: Performance (F-score) comparison between WMS EG (BT-CRF and ZEN-CRF with woodhood memory networks) and previous state-of-the-art models on the test set of five benchmark datasets. 4.2 Cross-Domain Performance As domain variance is always an important factor affecting the performance of NLP systems especially word semgenters (Song et al., 2012; Song and Xia, 2013), in addition to the experiments on benchmark datasets, we also run WMS EG on CTB7 across domains (genres in this case) with and without the memory module. To test on each genre, we use the union of the data from the other four genres to train our segmenter and use AV to extract n-grams from the entire raw text from CTB7 in this experiment. Table 8 reports the results in Fscore and OOV recall, which show a similar trend as that in Table 6, where WMS EG outperforms baselines for all five genres. Particularly, for genres with large domain variance (e.g., the ones with high O"
2020.acl-main.734,P98-2206,0,0.795795,"https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling methodology, recent approaches with neural networks are proved to be powerful in this task (Chen et al., 2015; Ma et al., 2018; Higashiyama et al., 2019). However, since neural networks (e.g., LSTM) is considered to be able to provide a good modeling of contextual dependencies, less attention is paid to the idea of explicitly leveraging wordhood information of n-grams in the context as what had previously been done in non-neural models. Although some studies sidestepped the idea by"
2020.acl-main.734,D11-1090,0,0.705009,"ese word segmentation (CWS) conventionally serves as the first step in Chinese language processing, especially for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the seq"
2020.acl-main.734,2020.acl-main.735,1,0.413504,"els, there were studies leverage external information, such as vocabularies from auto-segmented external corpus (Wang and Xu, 2017; Higashiyama et al., 2019), where Higashiyama et al. (2019) introduced a word attention mechanism to learn from large granular texts during the CWS process. In addition, the studies from Chen et al. (2017) and Qiu et al. (2019) try to improve CWS by learning from data annotated through different segmentation criteria. Moreover, there is a study leveraging auto-analyzed syntactic knowledge obtained from off-the-shelf toolkits to help CWS and part-of-speech tagging (Tian et al., 2020). Compare to these studies, WMS EG offers an alternative solution to robustly enhancing neural CWS models without requiring external resources. 6 Conclusion In this paper, we propose WMS EG, a neural framework for CWS using wordhood memory networks, which maps n-grams and their wordhood information to keys and values in it and appropriately models the values according to the importance of keys in a specific context. The framework follows the sequence labeling paradigm, and the encoders and decoders in it can be implemented by various prevailing models. To the best of our knowledge, this is the"
2020.acl-main.734,I05-3027,0,0.406871,"words in written text. Therefore, Chinese word segmentation (CWS) conventionally serves as the first step in Chinese language processing, especially for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012)."
2020.acl-main.734,I17-1017,0,0.0284743,"coder, with respect to each character in an input sentence. Higher weights are visualized with darker colors. 5 Related Work As one of the most fundamental NLP tasks for Chinese language processing, CWS has been studied for decades, with two steams of methods, i.e., word-based and character-based ones (Xue and Shen, 2003; Peng et al., 2004; Levow, 2006; Zhao et al., 2006; Zhao and Kit, 2008; Li and Sun, 2009; Song et al., 2009a; Li, 2011; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model performance, which is also observed in many other NLP tasks (Song et al., 2009b; Xiong et al., 2011; Shrestha, 2014; Shi et al., 2016; Diao et al., 2019). Recently, CWS benefits from neural networks and further progress are made with embeddings (Pei et al., 2014; Ma and Hinrich"
2020.acl-main.734,K17-1016,1,0.448121,"a segment: DL(D) = − Word Embedding Size Hidden State Size Hidden State Layers Key Embedding Size Value Embedding Size Dropout Rate Bi-LSTM Model Implementation Following previous studies (Sun and Xu, 2011; Chen et al., 2015, 2017; Ma et al., 2018; Qiu et al., 2019), we use four segmentation labels in our experiments, i.e., T = {B, I, E, S}. Among them, B, I, and E indicate a character is the beginning, inside, and the ending of a word and S denotes that the character is a single-character word. Since text representation plays an important role to facilitate many tasks (Conneau et al., 2017; Song et al., 2017, 2018; Sileo et al., 2019), we try two effective and well-known encoders, i.e., Bi-LSTM and BERT4 . In addition, we test WMS EG on a pretrained encoder for Chinese language, i.e., ZEN5 (Diao et al., 2019), which learns n-gram information in its pre-training from large raw corpora and outperforms BERT on many Chinese NLP tasks. Table 5 shows the hyperparameter settings for all the encoders: for the Bi-LSTM encoder, we follow the setting of Chen et al. (2015) and adopt their character embeddings for exi , and for BERT and ZEN encoders, we follow the default settings in their papers (Devlin et a"
2020.acl-main.734,P11-1129,0,0.0371253,"al., 2009a; Li, 2011; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model performance, which is also observed in many other NLP tasks (Song et al., 2009b; Xiong et al., 2011; Shrestha, 2014; Shi et al., 2016; Diao et al., 2019). Recently, CWS benefits from neural networks and further progress are made with embeddings (Pei et al., 2014; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017), recurrent neural models (Chen et al., 2015; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019) and even adversarial learning (Chen et al., 2017). To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus (Wang and Xu, 2017; Higashiyama et a"
2020.acl-main.734,N18-2028,1,0.651549,"Missing"
2020.acl-main.734,P16-2092,0,0.0294413,"Missing"
2020.acl-main.734,song-xia-2012-using,1,0.821784,"k (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling methodology, recent approaches with neural networks are proved to be powerful in this task (Chen et al., 2015; Ma et al., 2018; Higashiyama et al., 2019). However, since neural networks (e.g., LSTM) is considered to be able to provide a good modeling of contextual dependencies, less attention is paid to the idea of explicitly leveraging wordhood information of n-grams in the context as what had previously been done in non-neural models. Although some studies sidestepped the idea by incorporating contextual n-grams (Pei et al., 2014; Zhou et"
2020.acl-main.734,I13-1071,1,0.72415,".39 95.55 97.2 96.22 96.91 81.40 87.5 73.58 86.91 96.0 95.8 95.95 96.2 96.7 96.4 - 85.4 - WMS EG (BERT-CRF) WMS EG (ZEN-CRF) 98.28 98.40 86.67 84.87 96.51 96.53 86.76 85.36 96.58 96.62 78.48 79.64 97.80 97.93 87.57 90.15 97.16 97.25 88.00 88.46 Table 7: Performance (F-score) comparison between WMS EG (BT-CRF and ZEN-CRF with woodhood memory networks) and previous state-of-the-art models on the test set of five benchmark datasets. 4.2 Cross-Domain Performance As domain variance is always an important factor affecting the performance of NLP systems especially word semgenters (Song et al., 2012; Song and Xia, 2013), in addition to the experiments on benchmark datasets, we also run WMS EG on CTB7 across domains (genres in this case) with and without the memory module. To test on each genre, we use the union of the data from the other four genres to train our segmenter and use AV to extract n-grams from the entire raw text from CTB7 in this experiment. Table 8 reports the results in Fscore and OOV recall, which show a similar trend as that in Table 6, where WMS EG outperforms baselines for all five genres. Particularly, for genres with large domain variance (e.g., the ones with high OOV rates such as MZ a"
2020.acl-main.734,P12-1083,0,0.0759507,"other three use simplified MZ NW W EB 275K 483K 403K 443K 342K 184K 287K 258K 260K 210K 3K 3K 4K 3K 4K 12K 23K 26K 21K 21K 3.4 6.0 8.9 5.9 7.1 Table 3: Statistics of CTB7 with respect to five different genres. The OOV rate for each genre is computed based on the vocabulary from all the other four genres. ones. Following previous studies (Chen et al., 2015, 2017; Qiu et al., 2019), we convert traditional Chinese characters in AS and C ITY U into simplified ones.3 For MSR, AS, PKU, and C ITY U, we follow their official training/test data split. For CTB6, we use the same split as that stated in Yang and Xue (2012); Chen et al. (2015); Higashiyama et al. (2019), and only use its test set for the final experiment. Table 2 show the statistics of all datasets in terms of the number of characters and words and the percentage of out-of-vocabulary (OOV) words in the dev/test sets with respect to the training set. In addition, we also use CTB7 (LDC2010T07) to perform our cross-domain experiments. There are five genres in CTB7, including broadcast conversation (BC), broadcast news (BN), magazine (MZ), newswire (NW), and weblog (W EB). The statistics of all the genres are reported in Table 3, where the OOV rate"
2020.acl-main.734,N18-1122,0,0.0848416,"Missing"
2020.acl-main.734,D18-1351,1,0.837204,"t performance on all those datasets. Further experiments and analyses also demonstrate the robustness of our proposed framework with respect to different wordhood measures and the efficiency of wordhood information in cross-domain experiments.1 1 Introduction Unlike most written languages in the world, the Chinese writing system does not use explicit delimiters (e.g., white space) to separate words in written text. Therefore, Chinese word segmentation (CWS) conventionally serves as the first step in Chinese language processing, especially for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were p"
2020.acl-main.734,D13-1031,0,0.404153,"ainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling methodology, recent approaches with neural networks are proved to be powerful in this task (Chen et al., 2015; Ma et al., 2018; Higashiyama et al., 2019). However, since neural networks (e.g., LSTM) is considered to be able to provide a good modeling of contextual dependencies, less atte"
2020.acl-main.734,P16-1040,0,0.0819743,"e first step in Chinese language processing, especially for many downstream tasks such as text classification (Zeng et al., 2018), question answering (Liu et al., 2018), machine translation (Yang et al., 2018), etc. In the past two decades, the mainstream methodology of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling methodology, recent approaches with neural"
2020.acl-main.734,W06-0127,0,0.0614675,"emory mechanism can identify and distinguish important n-grams within a certain context and thus improves CWS performance accordingly. Figure 3: Heatmaps of weights learned for (a) keys and (b) values in the memory, and (c) the tags from the decoder, with respect to each character in an input sentence. Higher weights are visualized with darker colors. 5 Related Work As one of the most fundamental NLP tasks for Chinese language processing, CWS has been studied for decades, with two steams of methods, i.e., word-based and character-based ones (Xue and Shen, 2003; Peng et al., 2004; Levow, 2006; Zhao et al., 2006; Zhao and Kit, 2008; Li and Sun, 2009; Song et al., 2009a; Li, 2011; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Liu et al., 2016; Zhang et al., 2016; Wang and Xu, 2017; Zhou et al., 2017; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Gong et al., 2019; Qiu et al., 2019). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence; n-grams are used in some of these studies to enhance model performance, which is also observe"
2020.acl-main.734,I08-1002,0,0.236306,"oadcast conversation (BC), broadcast news (BN), magazine (MZ), newswire (NW), and weblog (W EB). The statistics of all the genres are reported in Table 3, where the OOV rate for each genre is computed according to the union of all other genres. For example, the OOV rate for BC is computed with respect to the union of BN, MZ, NW, and W EB. 3.2 3 BN Wordhood Measures We experiment with three wordhood measures to construct N . The main experiment adopts the aforementioned AV as the measure to rank all ngrams, because AV was shown to be the most effective wordhood measure in previous CWS studies (Zhao and Kit, 2008). Since AV is sensitive to 3 The conversion scripts are from https://github. com/skydark/nstools/tree/master/zhtools AV PMI DLG MSR PKU AS C ITY U CTB6 49K 18K 32K 71K 16K 22K 105K 22K 32K 104K 21K 27K 50K 16K 16K Table 4: The size of lexicon N generated from different wordhood measures under our settings. corpus size, in our experiments we use different AV thresholds when building the lexicon for each dataset: the threshold is 2 for PKU, C ITY U, CTB6 and CTB7, and 5 for MSR and AS. To test the the robustness of WMS EG, we also try two other wordhood measures, i.e., PMI (Sun et al., 1998) and"
2020.acl-main.734,D17-1079,0,0.305042,"y of CWS treated CWS as a character-based ∗ Partially done as an intern at Sinovation Ventures. Corresponding author. 1 WMS EG (code and the best performing models) is released at https://github.com/SVAIGBA/WMSeg. † sequence labeling task (Tseng et al., 2005; Song et al., 2006; Sun and Xu, 2011; Pei et al., 2014; Chen et al., 2015; Zhang et al., 2016; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019), where various studies were proposed to effectively extract contextual features to help better predicting segmentation labels for each character (Zhang et al., 2013; Zhou et al., 2017; Higashiyama et al., 2019). Among all the contextual features, the ones measuring wordhood for n-grams illustrate their helpfulness in many nonneural CWS models (Sun et al., 1998; Xue and Shen, 2003; Feng et al., 2004; Song and Xia, 2012). Later, following the track of the sequence labeling methodology, recent approaches with neural networks are proved to be powerful in this task (Chen et al., 2015; Ma et al., 2018; Higashiyama et al., 2019). However, since neural networks (e.g., LSTM) is considered to be able to provide a good modeling of contextual dependencies, less attention is paid to th"
2020.acl-main.735,P06-2013,0,0.0821828,"eatures and their knowledge instances for X , we use a two-way attention design to have separate attention for S and K. Particularly, the two ways, namely, the feature way and the knowledge way, are identical in architecture, where each way has a feed-forward attention module (Raffel and Ellis, 2015). For each xi , its Si and Ki are firstly fed into the feature attention way and the knowledge attention way, respectively, then computed within each way, and their final attention vectors are combined to feedback to the backbone model. Take the feature way as an example, the attention 3 Following Chen et al. (2006), the list has 12 syntactic labels, namely, ADJP, ADVP, CLP, DNP, DP, DVP, LCP, LST, NP, PP, QP, and VP. weight for each context feature si,j is computed by s exp(h&gt; i · ei,j ) asi,j = Pmi &gt; s j=1 exp(hi · ei,j ) (1) where hi is the vector from a text encoder for xi and esi,j the embedding of si,j . Then we have the weighted embedding asi for all si,j in Si via asi = mi X asi,j esi,j (2) j=1 P where denotes a element-wise sum operation. For the knowledge way, the same process is applied to get aki by distinguishing and weighting each knowledge instance ki,j . Finally, the output of the two att"
2020.acl-main.735,N19-1423,0,0.138063,"n Si via asi = mi X asi,j esi,j (2) j=1 P where denotes a element-wise sum operation. For the knowledge way, the same process is applied to get aki by distinguishing and weighting each knowledge instance ki,j . Finally, the output of the two attention ways are obtained through an concatenation of the two vectors: ai = asi ⊕ aki . 2.3 Joint Tagging with Two-way Attentions To functionalize the joint tagging, the two-way attentions interact with the backbone model through the encoded vector hi and its output ai for each xi . For hi , one can apply many prevailing encoders, e.g., Bi-LSTM or BERT (Devlin et al., 2019), to get the vector list [h1 h2 · · · hi · · · hl ] for X . Once ai is obtained, we concatenate it with hi and send it through a fully connected layer to align the dimension of the output for final prediction: oi = W · (hi ⊕ ai ) + b (3) where W and b are trainable parameters. Afterwards, conditional random fields (CRF) is used to estimate the probability for yi over all possible joint CWS and POS tags under xi and yi−1 by exp(Wc · oi + bc ) yi−1 yi exp(Wc · oi + bc ) p(yi |xi ) = P (4) Here, Wc and bc are the weight matrix and the bias vector, respectively, and they are estimated using the (y"
2020.acl-main.735,L18-1550,0,0.0126971,"ntly released Chinese encoder pre-trained with n-gram information and outperforms BERT in many downstream tasks. For the Bi-LSTM encoder, we set its hidden state size to 200 and use the character embeddings released by Shao et al. (2017) to initialize its input representations. For BERT and ZEN, we follow their default settings, e.g., 12 layers of selfattentions with the dimension of 768. For the two-way attention module, we randomly initialize the embeddings for all context features and their corresponding knowledge instances, where one can also use pre-trained embeddings (Song et al., 2018; Grave et al., 2018; Zhang et al., 2019; Yamada et al., 2020) for them. For all the 7 We use its version 3.9.2 downloaded from https:// stanfordnlp.github.io/CoreNLP/. 8 We download the model from https://github. com/nikitakit/self-attentive-parser. 9 We use the Chinese base model from https://s3. amazonaws.com/models.huggingface.co/. 10 https://github.com/sinovation/ZEN 8290 CTB5 Seg Joint CTB6 Seg Joint CTB7 Seg Joint CTB9 Seg Joint Seg UD1 Joint Seg UD2 Joint SCT BNP 98.02 - 95.49 95.50 96.62 - 90.85 94.43 96.53 - 92.73 92.95 93.63 - 88.23 88.09 80.50* 0.00* 0.00* 80.50* 36.11* 37.16* Bi-LSTM + POS (SCT) + Sy"
2020.acl-main.735,N09-1032,0,0.0316382,"performs previous studies on the joint task and achieves new state-of-the-art performance on all datasets. While some of the previous studies use auto-analyzed knowledge (Wang et al., 2011; Zhang et al., 2018), they regard such knowledge as gold reference and consequently could suffer from errors in the auto-analyzed results. In contrast, our proposed model is able to selectively model the input information and to discriminate useful knowledge instances through the two-way attentions. 3.4 Cross-Domain Performance Domain variance is an important factor affecting the performance of NLP systems (Guo et al., 2009; McClosky et al., 2010; Song and Xia, 2013). To further demonstrate the effectiveness of T WASP, we conduct cross-domain experiments on the eight genres of CTB9 using BERT and ZEN as the baseline and their enhanced version with POS knowledge from SCT. In doing so, we test on each genre with the models trained on the data from all other genres. The results on both segmentation and the joint task are reported in Table 5, where the SCT results are also included as a reference. The comparison between the baselines and T WASP with POS knowledge clearly shows the consistency of performance improvem"
2020.acl-main.735,D19-1549,0,0.0628014,"oposed two-way attention module (as shown in the right part of Figure 2) takes the syntactic knowledge produced from the input sentence, analyzes it and then feeds it to the tagging process. In this section, we firstly introduce the auto-analyzed knowledge, then explain how the two-way attentions consume such knowledge, and finally describe how the joint CWS and POS tagging works with the resulted attentions. 2.1 Auto-analyzed Knowledge Auto-analyzed knowledge is demonstrated to be an effective type of resources to help NLP systems understand the texts (Song et al., 2017; Seyler et al., 2018; Huang and Carley, 2019). One challenge for leveraging external knowledge for the joint task is that gold-standard annotations are extremely rare for text in most domains, especially the syntactic annotations. An alternative solution is to use off-the-shelf NLP systems to produce such knowledge, which is proved to be useful in previous studies (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2018). Rather than processing an entire corpus and then extracting features or training embeddings from the resulted corpus as in previous studies, our model does not treat knowledge as gold references: i"
2020.acl-main.735,D07-1117,0,0.269741,"rface word order, they are much closer in the dependency structure (the subject depends on “报告 VV” and ”书 NN” depends on the the object). This example shows that syntactic structure provides useful cues for CWS and POS tagging. Syntactic knowledge can be obtained from manually constructed resources such as treebanks and grammars, but such resources require considerate efforts to create and might not be available for a particular language or a particular domain. A more practical alternative is to use syntactic structures automatically generated by off-the-shelf toolkits. Some previous studies (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2018) verified the idea for this task by learning from autoprocessed corpora. However, their studies treat auto-processed corpora as gold reference and thus are unable to distinguishingly use it according to its quality (the resulted knowledge is not accurate in most cases). Therefore, the way to effectively leverage such auto-generated knowledge for the joint CWS and POS tagging task is not fully explored. In this paper, we propose a neural model named T WASP with a two-way attention mechanism to improve joint CWS and POS tagging by learn"
2020.acl-main.735,P09-1059,0,0.329979,"ey are much closer in the dependency structure (the subject depends on “报告 VV” and ”书 NN” depends on the the object). This example shows that syntactic structure provides useful cues for CWS and POS tagging. Syntactic knowledge can be obtained from manually constructed resources such as treebanks and grammars, but such resources require considerate efforts to create and might not be available for a particular language or a particular domain. A more practical alternative is to use syntactic structures automatically generated by off-the-shelf toolkits. Some previous studies (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2018) verified the idea for this task by learning from autoprocessed corpora. However, their studies treat auto-processed corpora as gold reference and thus are unable to distinguishingly use it according to its quality (the resulted knowledge is not accurate in most cases). Therefore, the way to effectively leverage such auto-generated knowledge for the joint CWS and POS tagging task is not fully explored. In this paper, we propose a neural model named T WASP with a two-way attention mechanism to improve joint CWS and POS tagging by learning from auto-analyz"
2020.acl-main.735,P08-1102,0,0.201522,"Missing"
2020.acl-main.735,P18-1249,0,0.0252787,"c tagset (42 tags for Chinese). We refer to the corpus with the two tagsets as UD1 and UD2, respectively, and use the official splits of train/dev/test in our experiments. The statistics for the aforementioned datasets are in Table 1. 5 We use its version 2.4 downloaded from https:// universaldependencies.org/. 6 The conversation scripts are from https://github. com/skydark/nstools/tree/master/zhtools Implementation To obtain the aforementioned three types of knowledge, we use two off-the-shelf toolkits, Stanford CoreNLP Toolkit (SCT)7 (Manning et al., 2014) and Berkeley Neural Parser (BNP)8 (Kitaev and Klein, 2018): the former tokenizes and parses a Chinese sentence, producing POS tags, phrase structure and dependency structure of the sentence; the latter does POS tagging and syntactic parsing on a pre-tokenized sentence. Both toolkits were trained on CTB data and thus produced CTB POS tags. To extract knowledge, we firstly use SCT to automatically segment sentences and then run both SCT and BNP for POS tagging and parsing. Table 2 shows the size of S and K for all the datasets. We test the model with three encoders: two of them, namely Bi-LSTM and BERT9 (Devlin et al., 2019), are widely used; the third"
2020.acl-main.735,P09-1058,0,0.184232,"Missing"
2020.acl-main.735,N18-2041,0,0.0894766,"fore, S5 = [“结合”, “成”, “分子”] and K5 = [“结合 VP”, “成 VP”, “分子 VP”]. Dependency Relations Given a character xi , let wi be the word that contains xi . The context features Si include wi , wi ’s governor, and wi ’s dependents in the dependency structure; those words combined with their inbound dependency relation labels form Ki . For example, for x6 =“分”, w6 = “分子”, which depends on “结合” with a dependency label dobj. Therefore, S6 = [“分子”, “结 合”], and K6 = [“分子 obj”, “结合 root”]. 2.2 Two-Way Attentions Attention has been shown to be an effective method for incorporating knowledge into NLP systems (Kumar et al., 2018; Margatina et al., 2019) but it cannot be used directly for feature and knowledge in pair-wise forms. Previous studies on the joint task normally directly concatenate the embeddings from context features and knowledge instances into the embeddings of characters (Zhang et al., 2018), which could be problematic for incorporating auto-analyzed, error-prone syntactic knowledge obtained from off-the-shelf toolkits. For both features and their knowledge instances for X , we use a two-way attention design to have separate attention for S and K. Particularly, the two ways, namely, the feature way and"
2020.acl-main.735,P17-1111,0,0.391604,"biguous part (in green color) has dependencies from distant words (in yellow color). boundaries in a sentence and the latter, on the top of segmentation results, assigns a POS tag to each word to indicate its syntactical property in the sentence. To effectively perform CWS and POS tagging, combining them into a joint task is proved to have better performance than separately conducting the two tasks in a sequence (Ng and Low, 2004). Therefore, many studies were proposed in the past decade for joint CWS and POS tagging (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). These studies, regardless of whether they used conventional approaches (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013) or deep learning based approaches (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018), focused on incorporating contextual information into their joint tagger. Introduction Chinese word segmentation (CWS) and part-ofspeech (POS) tagging are two fundamental and crucial tasks in natural language processing (NLP) for Chinese. The former one aims to find word ∗ Partially done as an intern at Sinovation"
2020.acl-main.735,P14-5010,0,0.00535636,"amely the universal tagset (15 tags) and language-specific tagset (42 tags for Chinese). We refer to the corpus with the two tagsets as UD1 and UD2, respectively, and use the official splits of train/dev/test in our experiments. The statistics for the aforementioned datasets are in Table 1. 5 We use its version 2.4 downloaded from https:// universaldependencies.org/. 6 The conversation scripts are from https://github. com/skydark/nstools/tree/master/zhtools Implementation To obtain the aforementioned three types of knowledge, we use two off-the-shelf toolkits, Stanford CoreNLP Toolkit (SCT)7 (Manning et al., 2014) and Berkeley Neural Parser (BNP)8 (Kitaev and Klein, 2018): the former tokenizes and parses a Chinese sentence, producing POS tags, phrase structure and dependency structure of the sentence; the latter does POS tagging and syntactic parsing on a pre-tokenized sentence. Both toolkits were trained on CTB data and thus produced CTB POS tags. To extract knowledge, we firstly use SCT to automatically segment sentences and then run both SCT and BNP for POS tagging and parsing. Table 2 shows the size of S and K for all the datasets. We test the model with three encoders: two of them, namely Bi-LSTM"
2020.acl-main.735,P19-1385,0,0.0346658,"”, “分子”] and K5 = [“结合 VP”, “成 VP”, “分子 VP”]. Dependency Relations Given a character xi , let wi be the word that contains xi . The context features Si include wi , wi ’s governor, and wi ’s dependents in the dependency structure; those words combined with their inbound dependency relation labels form Ki . For example, for x6 =“分”, w6 = “分子”, which depends on “结合” with a dependency label dobj. Therefore, S6 = [“分子”, “结 合”], and K6 = [“分子 obj”, “结合 root”]. 2.2 Two-Way Attentions Attention has been shown to be an effective method for incorporating knowledge into NLP systems (Kumar et al., 2018; Margatina et al., 2019) but it cannot be used directly for feature and knowledge in pair-wise forms. Previous studies on the joint task normally directly concatenate the embeddings from context features and knowledge instances into the embeddings of characters (Zhang et al., 2018), which could be problematic for incorporating auto-analyzed, error-prone syntactic knowledge obtained from off-the-shelf toolkits. For both features and their knowledge instances for X , we use a two-way attention design to have separate attention for S and K. Particularly, the two ways, namely, the feature way and the knowledge way, are i"
2020.acl-main.735,D16-1147,0,0.312264,"posed attention module extracts the context features associated with the character and their corresponding knowledge instances according to the auto-analyzed results, then computes the attentions separately for features and knowledge in each attention way, and finally concatenates the attentions from two ways to guide the tagging process. In doing so, our model can distinguish the important auto-analyzed knowledge based on their contributions to the task and thus avoid being influenced by some inferior knowledge instances. Compared to another prevailing model, i.e., key-value memory networks (Miller et al., 2016), which can learn from pair-wisely organized information, the twoway attentions not only are able to do so, but also fully leverage features and their knowledge rather than using one to weight the other.2 We experiment with three types of knowledge, namely, POS labels, syntactic constituents, and dependency relations, in our experiments. The experimental results on five benchmark datasets illustrate the effectiveness of our model, where state-of-the-art performance for the joint task is achieved on all datasets. We also perform several analyses, which confirm the validity of using two-way atte"
2020.acl-main.735,W04-3236,0,0.643969,"r joint CWS and POS tagging, where state-of-the-art performance is achieved on five benchmark datasets.1 1 Figure 1: An example sentence with CWS and POS tagging results, where the ambiguous part (in green color) has dependencies from distant words (in yellow color). boundaries in a sentence and the latter, on the top of segmentation results, assigns a POS tag to each word to indicate its syntactical property in the sentence. To effectively perform CWS and POS tagging, combining them into a joint task is proved to have better performance than separately conducting the two tasks in a sequence (Ng and Low, 2004). Therefore, many studies were proposed in the past decade for joint CWS and POS tagging (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). These studies, regardless of whether they used conventional approaches (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013) or deep learning based approaches (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018), focused on incorporating contextual information into their joint tagger. Introduction Chinese word segmentation (CWS) and part-of"
2020.acl-main.735,L16-1262,0,0.0225018,"Missing"
2020.acl-main.735,D12-1046,0,0.104152,"Missing"
2020.acl-main.735,P18-2039,0,0.113768,"bone paradigm, the proposed two-way attention module (as shown in the right part of Figure 2) takes the syntactic knowledge produced from the input sentence, analyzes it and then feeds it to the tagging process. In this section, we firstly introduce the auto-analyzed knowledge, then explain how the two-way attentions consume such knowledge, and finally describe how the joint CWS and POS tagging works with the resulted attentions. 2.1 Auto-analyzed Knowledge Auto-analyzed knowledge is demonstrated to be an effective type of resources to help NLP systems understand the texts (Song et al., 2017; Seyler et al., 2018; Huang and Carley, 2019). One challenge for leveraging external knowledge for the joint task is that gold-standard annotations are extremely rare for text in most domains, especially the syntactic annotations. An alternative solution is to use off-the-shelf NLP systems to produce such knowledge, which is proved to be useful in previous studies (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2018). Rather than processing an entire corpus and then extracting features or training embeddings from the resulted corpus as in previous studies, our model does not treat knowle"
2020.acl-main.735,I17-1018,0,0.433143,"n color) has dependencies from distant words (in yellow color). boundaries in a sentence and the latter, on the top of segmentation results, assigns a POS tag to each word to indicate its syntactical property in the sentence. To effectively perform CWS and POS tagging, combining them into a joint task is proved to have better performance than separately conducting the two tasks in a sequence (Ng and Low, 2004). Therefore, many studies were proposed in the past decade for joint CWS and POS tagging (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). These studies, regardless of whether they used conventional approaches (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013) or deep learning based approaches (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018), focused on incorporating contextual information into their joint tagger. Introduction Chinese word segmentation (CWS) and part-ofspeech (POS) tagging are two fundamental and crucial tasks in natural language processing (NLP) for Chinese. The former one aims to find word ∗ Partially done as an intern at Sinovation Ventures. Correspo"
2020.acl-main.735,P14-2042,0,0.156438,"Missing"
2020.acl-main.735,K17-1016,1,0.380283,"To enhance the backbone paradigm, the proposed two-way attention module (as shown in the right part of Figure 2) takes the syntactic knowledge produced from the input sentence, analyzes it and then feeds it to the tagging process. In this section, we firstly introduce the auto-analyzed knowledge, then explain how the two-way attentions consume such knowledge, and finally describe how the joint CWS and POS tagging works with the resulted attentions. 2.1 Auto-analyzed Knowledge Auto-analyzed knowledge is demonstrated to be an effective type of resources to help NLP systems understand the texts (Song et al., 2017; Seyler et al., 2018; Huang and Carley, 2019). One challenge for leveraging external knowledge for the joint task is that gold-standard annotations are extremely rare for text in most domains, especially the syntactic annotations. An alternative solution is to use off-the-shelf NLP systems to produce such knowledge, which is proved to be useful in previous studies (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2018). Rather than processing an entire corpus and then extracting features or training embeddings from the resulted corpus as in previous studies, our model"
2020.acl-main.735,2020.acl-main.734,1,0.413504,"icates the validity of using two attention ways for features and knowledge, i.e., compared to (1), as well as the advantage of learning from both of them, i.e., compared to (2) and (3). Interestingly, in the three settings, (3) outperforms (1), which could be explained by that, with normal attention, mixed feature and knowledge instances in it may make it difficult to weight them for the joint task. There are other methods for using both context features and knowledge in a neural framework, such as key-value memory networks (kvMN) (Miller et al., 2016), which is demonstrated to improve CWS by Tian et al. (2020). Thus we compare our approach with kvMN, in which context features are mapped to keys and knowledge to values. We follow the standard protocol of the kvMN, e.g., addressing keys by Si and reading values from Ki through the corresponding knowledge for each key, computing weights from all key embeddings, and outputting the weighted embeddings from all values. The result from the kvMN is reported at the last row of Table 6, where its performance is not as good as the two-way attentions, and even 8292 Genre BC BN CS DF MZ NW SC WB Seg SCT Joint 96.27 96.98 89.83 91.34 95.69 97.41 84.87 95.99 BERT"
2020.acl-main.735,I11-1035,0,0.789335,"n the dependency structure (the subject depends on “报告 VV” and ”书 NN” depends on the the object). This example shows that syntactic structure provides useful cues for CWS and POS tagging. Syntactic knowledge can be obtained from manually constructed resources such as treebanks and grammars, but such resources require considerate efforts to create and might not be available for a particular language or a particular domain. A more practical alternative is to use syntactic structures automatically generated by off-the-shelf toolkits. Some previous studies (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2018) verified the idea for this task by learning from autoprocessed corpora. However, their studies treat auto-processed corpora as gold reference and thus are unable to distinguishingly use it according to its quality (the resulted knowledge is not accurate in most cases). Therefore, the way to effectively leverage such auto-generated knowledge for the joint CWS and POS tagging task is not fully explored. In this paper, we propose a neural model named T WASP with a two-way attention mechanism to improve joint CWS and POS tagging by learning from auto-analyzed syntactic knowle"
2020.acl-main.735,P13-1076,0,0.266162,"S and POS tagging results, where the ambiguous part (in green color) has dependencies from distant words (in yellow color). boundaries in a sentence and the latter, on the top of segmentation results, assigns a POS tag to each word to indicate its syntactical property in the sentence. To effectively perform CWS and POS tagging, combining them into a joint task is proved to have better performance than separately conducting the two tasks in a sequence (Ng and Low, 2004). Therefore, many studies were proposed in the past decade for joint CWS and POS tagging (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). These studies, regardless of whether they used conventional approaches (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013) or deep learning based approaches (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018), focused on incorporating contextual information into their joint tagger. Introduction Chinese word segmentation (CWS) and part-ofspeech (POS) tagging are two fundamental and crucial tasks in natural language processing (NLP) for Chinese. The former one aims to find word ∗"
2020.acl-main.735,D19-1528,1,0.81449,"e encoder pre-trained with n-gram information and outperforms BERT in many downstream tasks. For the Bi-LSTM encoder, we set its hidden state size to 200 and use the character embeddings released by Shao et al. (2017) to initialize its input representations. For BERT and ZEN, we follow their default settings, e.g., 12 layers of selfattentions with the dimension of 768. For the two-way attention module, we randomly initialize the embeddings for all context features and their corresponding knowledge instances, where one can also use pre-trained embeddings (Song et al., 2018; Grave et al., 2018; Zhang et al., 2019; Yamada et al., 2020) for them. For all the 7 We use its version 3.9.2 downloaded from https:// stanfordnlp.github.io/CoreNLP/. 8 We download the model from https://github. com/nikitakit/self-attentive-parser. 9 We use the Chinese base model from https://s3. amazonaws.com/models.huggingface.co/. 10 https://github.com/sinovation/ZEN 8290 CTB5 Seg Joint CTB6 Seg Joint CTB7 Seg Joint CTB9 Seg Joint Seg UD1 Joint Seg UD2 Joint SCT BNP 98.02 - 95.49 95.50 96.62 - 90.85 94.43 96.53 - 92.73 92.95 93.63 - 88.23 88.09 80.50* 0.00* 0.00* 80.50* 36.11* 37.16* Bi-LSTM + POS (SCT) + Syn. (SCT) + Dep. (SCT"
2020.acl-main.735,N18-2028,1,0.510158,"9), which is a recently released Chinese encoder pre-trained with n-gram information and outperforms BERT in many downstream tasks. For the Bi-LSTM encoder, we set its hidden state size to 200 and use the character embeddings released by Shao et al. (2017) to initialize its input representations. For BERT and ZEN, we follow their default settings, e.g., 12 layers of selfattentions with the dimension of 768. For the two-way attention module, we randomly initialize the embeddings for all context features and their corresponding knowledge instances, where one can also use pre-trained embeddings (Song et al., 2018; Grave et al., 2018; Zhang et al., 2019; Yamada et al., 2020) for them. For all the 7 We use its version 3.9.2 downloaded from https:// stanfordnlp.github.io/CoreNLP/. 8 We download the model from https://github. com/nikitakit/self-attentive-parser. 9 We use the Chinese base model from https://s3. amazonaws.com/models.huggingface.co/. 10 https://github.com/sinovation/ZEN 8290 CTB5 Seg Joint CTB6 Seg Joint CTB7 Seg Joint CTB9 Seg Joint Seg UD1 Joint Seg UD2 Joint SCT BNP 98.02 - 95.49 95.50 96.62 - 90.85 94.43 96.53 - 92.73 92.95 93.63 - 88.23 88.09 80.50* 0.00* 0.00* 80.50* 36.11* 37.16* Bi-L"
2020.acl-main.735,I13-1071,1,0.549957,"sk and achieves new state-of-the-art performance on all datasets. While some of the previous studies use auto-analyzed knowledge (Wang et al., 2011; Zhang et al., 2018), they regard such knowledge as gold reference and consequently could suffer from errors in the auto-analyzed results. In contrast, our proposed model is able to selectively model the input information and to discriminate useful knowledge instances through the two-way attentions. 3.4 Cross-Domain Performance Domain variance is an important factor affecting the performance of NLP systems (Guo et al., 2009; McClosky et al., 2010; Song and Xia, 2013). To further demonstrate the effectiveness of T WASP, we conduct cross-domain experiments on the eight genres of CTB9 using BERT and ZEN as the baseline and their enhanced version with POS knowledge from SCT. In doing so, we test on each genre with the models trained on the data from all other genres. The results on both segmentation and the joint task are reported in Table 5, where the SCT results are also included as a reference. The comparison between the baselines and T WASP with POS knowledge clearly shows the consistency of performance improvement with twoway attentions, where for both B"
2020.acl-main.735,P11-1139,0,0.186691,"nce with CWS and POS tagging results, where the ambiguous part (in green color) has dependencies from distant words (in yellow color). boundaries in a sentence and the latter, on the top of segmentation results, assigns a POS tag to each word to indicate its syntactical property in the sentence. To effectively perform CWS and POS tagging, combining them into a joint task is proved to have better performance than separately conducting the two tasks in a sequence (Ng and Low, 2004). Therefore, many studies were proposed in the past decade for joint CWS and POS tagging (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). These studies, regardless of whether they used conventional approaches (Jiang et al., 2008, 2009; Sun, 2011; Zeng et al., 2013) or deep learning based approaches (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018), focused on incorporating contextual information into their joint tagger. Introduction Chinese word segmentation (CWS) and part-ofspeech (POS) tagging are two fundamental and crucial tasks in natural language processing (NLP) for Chinese. The former one a"
2020.coling-main.187,D17-1070,0,0.0298199,"ur Model Len. (b) Results from ZEN Table 3: F scores for segmentation and joint tagging of M C AP OS T under different settings on the development set of five datasets, where the results of models with BERT encoder and ZEN encoder are reported in (a) and (b), respectively. “Freq.” and “Len.” refer to the n-gram categorization strategies based on n-gram frequency and n-gram length; “AV”, “DLG”, and “PMI” stands for different ways to construct the lexicon N ; “N/A” is the abbreviation for not applicable. 3.3 Implementations Since text representation plays an important role in model performance (Conneau et al., 2017; Song et al., 2017; Song et al., 2018), in our experiment, we try two well-known Chinese text encoders as the backbone model: Chinese version of pre-trained BERT9 (Devlin et al., 2019) and ZEN10 (Diao et al., 2019). For both BERT and ZEN, we follow their default settings in our experiments (i.e., for both BERT and ZEN, we use 12 layers of multi-head attentions on character encoding with the dimension of hidden vectors set to be 768; for ZEN, we use 6 layers of n-gram representations). For the models with the multi-channel attention module, we use two criteria to categorize the n-grams that ar"
2020.coling-main.187,N19-1423,0,0.221929,"use “大” is a component of s1 and s2 but is (4) 3 not a part of s3 . As a result, for each entire channel, its resulted weight is computed by (k) ai = mk X (k) (k) ai,j ej (3) j=1 Finally, the overall attention of different n-grams for xi is the concatenation of attentions from all channels: M (k) ai = δk ai (4) n with a trainable positive parameter δk to balance the contribution of each channel. 2.2 Joint Tagging with the Attentions To leverage the n-grams through the proposed attention module, we first obtain the hidden vector hi of each xi in the input sequence from the encoder (e.g., BERT (Devlin et al., 2019)) of the backbone model. Next, we feed the resulting hi to the attention module and obtain its output ai , which contains the weighted contextual information carried by the n-gram features. Then, we incorporate such weighted information into the backbone model by concatenating ai with hi and align the resulting vector to the output dimension by a trainable matrix Wd , which is represented by ui = Wd · (hi ⊕ ai ) (5) Afterwards, we pass ui to a conditional random field (CRF) decoder to estimate the joint label ybi for xi . 3 Experiment Settings 3.1 Datasets In our experiments, five Chinese benc"
2020.coling-main.187,J04-1004,0,0.0345089,"N CTB5 DLG PMI 34.5K 13.5K 10.6K AV CTB6 DLG PMI 29.7K 16.6K 13.6K AV CTB7 DLG PMI 27.9K 18.9K 16.6K AV CTB9 DLG PMI 38.1K 27.5K 28.9K AV UD DLG PMI 19.6K 5.3K 1.1K Table 2: The size of lexicon N constructed by AV, DLG, and PMI. 3.2 Lexicon Construction To enhance the joint CWS and POS tagging through the multi-channel attentions, we need to construct the lexicon N which is simply a list of n-grams.6 In this study, we do not want our approach to rely on existing n-gram resources. Therefore, we use three unsupervised methods to obtain n-grams from each datasets, namely, accessor variety (AV) (Feng et al., 2004), description length gain (DLG) (Kit and Wilks, 1999), and pointwise mutual information (PMI) (Sun et al., 1998). Accessor Variety Given a character n-gram s, let left access number Lav (s) be the number of distinct characters that precede s in the corpus. The right access number Rav (s) is defined similarly. The AV score of s is the minimal number of the left and right access numbers: AV (s) = min(Lav (s), Rav (s)) (6) In general, n-grams with higher AV scores are more likely to be words in Chinese. Since AV is sensitive to the size of dataset, in our experiments, we use different thresholds"
2020.coling-main.187,N19-1276,0,0.0243396,"features to enhance text representation for CWS and POS tagging in many studies (Song et al., 2009; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Shao et al., 2017; Zhang et al., 2018). However, for joint CWS and POS tagging, previous approaches to leveraging the n-gram features are limited to directly concatenating the n-gram embeddings with the input character embedding, where unimportant n-grams may mislead the model and result in incorrect predictions. Therefore, assigning appropriate weights to different n-grams regarding to their contexts is a potential effective solution (Higashiyama et al., 2019; Tian et al., 2020b) to the joint task and we propose to use multi-channel attention to tackle this mission. In detail, we first categorize n-grams by a specific metric, which in this study is either their frequencies or lengths and then model the grouped n-grams in separate channels of attentions. As a result, the contributions of the salient n-grams are highlighted and the attention weights are not dominated by frequent n-grams or the short ones that tend to appear in more sentences. Our model is thus able to leverage the highlighted n-grams accordingly and avoid being misled by the unimpor"
2020.coling-main.187,P08-1102,0,0.129073,"Missing"
2020.coling-main.187,P09-1059,0,0.0415727,"they may be infrequent in the dataset. 5 Related Work There are basically two approaches to CWS and POS tagging: to perform the two tasks it in a pipeline framework; or to treat them as a joint task where the two tasks are conducted simultaneously, which is known as joint CWS and POS tagging. Ng and Low (2004) provided a comprehensive study to compare the two approaches and found that the joint approach outperform the pipeline one. Therefore, in the past two decades, the majority of studies on CWS and POS tagging applied the joint approach to these tasks (Ng and Low, 2004; Jiang et al., 2008; Jiang et al., 2009; Wang et al., 2011; Sun, 2011; Zeng et al., 2013), where n-grams are widely used as features carrying contextual information to improve model performance. Recently, neural methods, especially the recurrent neural networks (e.g., bi-LSTM) have demonstrated their effectiveness to encode contextual information, and thus significantly improve the model performance in joint CWS and POS tagging. Even though, improvements can still be obtained when n-grams are incorporated into the neural taggers (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018; Tian et al., 2020a). Fo"
2020.coling-main.187,P09-1058,0,0.110448,"Missing"
2020.coling-main.187,P17-1111,0,0.191001,"state-of-the-art performance on all five datasets.1 1 Introduction Chinese word segmentation (CWS) and part-of-speech (POS) tagging are two fundamental tasks in Chinese natural language processing (NLP). Although they can be treated as two separate tasks in a sequential order, it has been demonstrated by previous studies that processing them jointly in a unified sequence labeling framework could be more effective, where CWS and POS tags are predicted in a single step (Ng and Low, 2004; Jiang et al., 2008; Wang et al., 2011; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Zhang et al., 2014; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In doing so, existing studies mainly focused on incorporating contextual information (e.g., n-grams) as features into their joint taggers, which had been widely used as an effective way to improve model performance especially before neural models were widely used. Although neural models are powerful in modeling long text sequences, external features from larger granular texts are still demonstrated to be useful in existing neural models (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In these models, contextual feature"
2020.coling-main.187,W04-3236,0,0.645681,"five benchmark datasets for CWS and POS tagging demonstrate that our approach outperforms strong baseline models and achieves state-of-the-art performance on all five datasets.1 1 Introduction Chinese word segmentation (CWS) and part-of-speech (POS) tagging are two fundamental tasks in Chinese natural language processing (NLP). Although they can be treated as two separate tasks in a sequential order, it has been demonstrated by previous studies that processing them jointly in a unified sequence labeling framework could be more effective, where CWS and POS tags are predicted in a single step (Ng and Low, 2004; Jiang et al., 2008; Wang et al., 2011; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Zhang et al., 2014; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In doing so, existing studies mainly focused on incorporating contextual information (e.g., n-grams) as features into their joint taggers, which had been widely used as an effective way to improve model performance especially before neural models were widely used. Although neural models are powerful in modeling long text sequences, external features from larger granular texts are still demonstrated to be useful in existing n"
2020.coling-main.187,L16-1262,0,0.0291257,"Missing"
2020.coling-main.187,D12-1046,0,0.0530485,"Missing"
2020.coling-main.187,I17-1018,0,0.326212,"ormance on all five datasets.1 1 Introduction Chinese word segmentation (CWS) and part-of-speech (POS) tagging are two fundamental tasks in Chinese natural language processing (NLP). Although they can be treated as two separate tasks in a sequential order, it has been demonstrated by previous studies that processing them jointly in a unified sequence labeling framework could be more effective, where CWS and POS tags are predicted in a single step (Ng and Low, 2004; Jiang et al., 2008; Wang et al., 2011; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Zhang et al., 2014; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In doing so, existing studies mainly focused on incorporating contextual information (e.g., n-grams) as features into their joint taggers, which had been widely used as an effective way to improve model performance especially before neural models were widely used. Although neural models are powerful in modeling long text sequences, external features from larger granular texts are still demonstrated to be useful in existing neural models (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In these models, contextual features are leveraged by"
2020.coling-main.187,P14-2042,0,0.042988,"Missing"
2020.coling-main.187,song-xia-2012-using,1,0.920217,"s sentence. 2074 Figure 2: The overall architecture of our character-based model for the joint CWS and POS tagging with an example input and output. On the left is the backbone model following the sequence labeling paradigm; on the right is the multi-channel attention module with n-grams categorized by their length. Different attention channels for n-grams associated with “大” (big) are highlighted with distinct colors. 2.1 The Multi-channel Attentions N-grams have been used as useful contextual features to enhance text representation for CWS and POS tagging in many studies (Song et al., 2009; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Shao et al., 2017; Zhang et al., 2018). However, for joint CWS and POS tagging, previous approaches to leveraging the n-gram features are limited to directly concatenating the n-gram embeddings with the input character embedding, where unimportant n-grams may mislead the model and result in incorrect predictions. Therefore, assigning appropriate weights to different n-grams regarding to their contexts is a potential effective solution (Higashiyama et al., 2019; Tian et al., 2020b) to the joint task and we propose to use multi-channel attention to tackle"
2020.coling-main.187,I13-1071,1,0.875801,"architecture of our character-based model for the joint CWS and POS tagging with an example input and output. On the left is the backbone model following the sequence labeling paradigm; on the right is the multi-channel attention module with n-grams categorized by their length. Different attention channels for n-grams associated with “大” (big) are highlighted with distinct colors. 2.1 The Multi-channel Attentions N-grams have been used as useful contextual features to enhance text representation for CWS and POS tagging in many studies (Song et al., 2009; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Shao et al., 2017; Zhang et al., 2018). However, for joint CWS and POS tagging, previous approaches to leveraging the n-gram features are limited to directly concatenating the n-gram embeddings with the input character embedding, where unimportant n-grams may mislead the model and result in incorrect predictions. Therefore, assigning appropriate weights to different n-grams regarding to their contexts is a potential effective solution (Higashiyama et al., 2019; Tian et al., 2020b) to the joint task and we propose to use multi-channel attention to tackle this mission. In detail, we first cate"
2020.coling-main.187,W09-3511,1,0.800858,"terpretation of this sentence. 2074 Figure 2: The overall architecture of our character-based model for the joint CWS and POS tagging with an example input and output. On the left is the backbone model following the sequence labeling paradigm; on the right is the multi-channel attention module with n-grams categorized by their length. Different attention channels for n-grams associated with “大” (big) are highlighted with distinct colors. 2.1 The Multi-channel Attentions N-grams have been used as useful contextual features to enhance text representation for CWS and POS tagging in many studies (Song et al., 2009; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Shao et al., 2017; Zhang et al., 2018). However, for joint CWS and POS tagging, previous approaches to leveraging the n-gram features are limited to directly concatenating the n-gram embeddings with the input character embedding, where unimportant n-grams may mislead the model and result in incorrect predictions. Therefore, assigning appropriate weights to different n-grams regarding to their contexts is a potential effective solution (Higashiyama et al., 2019; Tian et al., 2020b) to the joint task and we propose to use multi-channel"
2020.coling-main.187,C12-2116,1,0.718021,"ure 2: The overall architecture of our character-based model for the joint CWS and POS tagging with an example input and output. On the left is the backbone model following the sequence labeling paradigm; on the right is the multi-channel attention module with n-grams categorized by their length. Different attention channels for n-grams associated with “大” (big) are highlighted with distinct colors. 2.1 The Multi-channel Attentions N-grams have been used as useful contextual features to enhance text representation for CWS and POS tagging in many studies (Song et al., 2009; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Shao et al., 2017; Zhang et al., 2018). However, for joint CWS and POS tagging, previous approaches to leveraging the n-gram features are limited to directly concatenating the n-gram embeddings with the input character embedding, where unimportant n-grams may mislead the model and result in incorrect predictions. Therefore, assigning appropriate weights to different n-grams regarding to their contexts is a potential effective solution (Higashiyama et al., 2019; Tian et al., 2020b) to the joint task and we propose to use multi-channel attention to tackle this mission. In d"
2020.coling-main.187,K17-1016,1,0.664752,"lts from ZEN Table 3: F scores for segmentation and joint tagging of M C AP OS T under different settings on the development set of five datasets, where the results of models with BERT encoder and ZEN encoder are reported in (a) and (b), respectively. “Freq.” and “Len.” refer to the n-gram categorization strategies based on n-gram frequency and n-gram length; “AV”, “DLG”, and “PMI” stands for different ways to construct the lexicon N ; “N/A” is the abbreviation for not applicable. 3.3 Implementations Since text representation plays an important role in model performance (Conneau et al., 2017; Song et al., 2017; Song et al., 2018), in our experiment, we try two well-known Chinese text encoders as the backbone model: Chinese version of pre-trained BERT9 (Devlin et al., 2019) and ZEN10 (Diao et al., 2019). For both BERT and ZEN, we follow their default settings in our experiments (i.e., for both BERT and ZEN, we use 12 layers of multi-head attentions on character encoding with the dimension of hidden vectors set to be 768; for ZEN, we use 6 layers of n-gram representations). For the models with the multi-channel attention module, we use two criteria to categorize the n-grams that are used in different"
2020.coling-main.187,N18-2028,1,0.769763,"3: F scores for segmentation and joint tagging of M C AP OS T under different settings on the development set of five datasets, where the results of models with BERT encoder and ZEN encoder are reported in (a) and (b), respectively. “Freq.” and “Len.” refer to the n-gram categorization strategies based on n-gram frequency and n-gram length; “AV”, “DLG”, and “PMI” stands for different ways to construct the lexicon N ; “N/A” is the abbreviation for not applicable. 3.3 Implementations Since text representation plays an important role in model performance (Conneau et al., 2017; Song et al., 2017; Song et al., 2018), in our experiment, we try two well-known Chinese text encoders as the backbone model: Chinese version of pre-trained BERT9 (Devlin et al., 2019) and ZEN10 (Diao et al., 2019). For both BERT and ZEN, we follow their default settings in our experiments (i.e., for both BERT and ZEN, we use 12 layers of multi-head attentions on character encoding with the dimension of hidden vectors set to be 768; for ZEN, we use 6 layers of n-gram representations). For the models with the multi-channel attention module, we use two criteria to categorize the n-grams that are used in different channels. The first"
2020.coling-main.187,P98-2206,0,0.47617,"PMI 38.1K 27.5K 28.9K AV UD DLG PMI 19.6K 5.3K 1.1K Table 2: The size of lexicon N constructed by AV, DLG, and PMI. 3.2 Lexicon Construction To enhance the joint CWS and POS tagging through the multi-channel attentions, we need to construct the lexicon N which is simply a list of n-grams.6 In this study, we do not want our approach to rely on existing n-gram resources. Therefore, we use three unsupervised methods to obtain n-grams from each datasets, namely, accessor variety (AV) (Feng et al., 2004), description length gain (DLG) (Kit and Wilks, 1999), and pointwise mutual information (PMI) (Sun et al., 1998). Accessor Variety Given a character n-gram s, let left access number Lav (s) be the number of distinct characters that precede s in the corpus. The right access number Rav (s) is defined similarly. The AV score of s is the minimal number of the left and right access numbers: AV (s) = min(Lav (s), Rav (s)) (6) In general, n-grams with higher AV scores are more likely to be words in Chinese. Since AV is sensitive to the size of dataset, in our experiments, we use different thresholds for the five datasets: 2 for CTB5, 3 for CTB6, 4 for CTB7, 5 for CTB9, and 1 for UD. For each dataset, we collec"
2020.coling-main.187,P11-1139,0,0.1266,"ate that our approach outperforms strong baseline models and achieves state-of-the-art performance on all five datasets.1 1 Introduction Chinese word segmentation (CWS) and part-of-speech (POS) tagging are two fundamental tasks in Chinese natural language processing (NLP). Although they can be treated as two separate tasks in a sequential order, it has been demonstrated by previous studies that processing them jointly in a unified sequence labeling framework could be more effective, where CWS and POS tags are predicted in a single step (Ng and Low, 2004; Jiang et al., 2008; Wang et al., 2011; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Zhang et al., 2014; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In doing so, existing studies mainly focused on incorporating contextual information (e.g., n-grams) as features into their joint taggers, which had been widely used as an effective way to improve model performance especially before neural models were widely used. Although neural models are powerful in modeling long text sequences, external features from larger granular texts are still demonstrated to be useful in existing neural models (Zheng et al., 2013; Kurita et al., 2"
2020.coling-main.187,2020.acl-main.735,1,0.774344,"representation for CWS and POS tagging in many studies (Song et al., 2009; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Shao et al., 2017; Zhang et al., 2018). However, for joint CWS and POS tagging, previous approaches to leveraging the n-gram features are limited to directly concatenating the n-gram embeddings with the input character embedding, where unimportant n-grams may mislead the model and result in incorrect predictions. Therefore, assigning appropriate weights to different n-grams regarding to their contexts is a potential effective solution (Higashiyama et al., 2019; Tian et al., 2020b) to the joint task and we propose to use multi-channel attention to tackle this mission. In detail, we first categorize n-grams by a specific metric, which in this study is either their frequencies or lengths and then model the grouped n-grams in separate channels of attentions. As a result, the contributions of the salient n-grams are highlighted and the attention weights are not dominated by frequent n-grams or the short ones that tend to appear in more sentences. Our model is thus able to leverage the highlighted n-grams accordingly and avoid being misled by the unimportant ones. To train"
2020.coling-main.187,2020.acl-main.734,1,0.90683,"representation for CWS and POS tagging in many studies (Song et al., 2009; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Shao et al., 2017; Zhang et al., 2018). However, for joint CWS and POS tagging, previous approaches to leveraging the n-gram features are limited to directly concatenating the n-gram embeddings with the input character embedding, where unimportant n-grams may mislead the model and result in incorrect predictions. Therefore, assigning appropriate weights to different n-grams regarding to their contexts is a potential effective solution (Higashiyama et al., 2019; Tian et al., 2020b) to the joint task and we propose to use multi-channel attention to tackle this mission. In detail, we first categorize n-grams by a specific metric, which in this study is either their frequencies or lengths and then model the grouped n-grams in separate channels of attentions. As a result, the contributions of the salient n-grams are highlighted and the attention weights are not dominated by frequent n-grams or the short ones that tend to appear in more sentences. Our model is thus able to leverage the highlighted n-grams accordingly and avoid being misled by the unimportant ones. To train"
2020.coling-main.187,I11-1035,0,0.415347,"OS tagging demonstrate that our approach outperforms strong baseline models and achieves state-of-the-art performance on all five datasets.1 1 Introduction Chinese word segmentation (CWS) and part-of-speech (POS) tagging are two fundamental tasks in Chinese natural language processing (NLP). Although they can be treated as two separate tasks in a sequential order, it has been demonstrated by previous studies that processing them jointly in a unified sequence labeling framework could be more effective, where CWS and POS tags are predicted in a single step (Ng and Low, 2004; Jiang et al., 2008; Wang et al., 2011; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Zhang et al., 2014; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In doing so, existing studies mainly focused on incorporating contextual information (e.g., n-grams) as features into their joint taggers, which had been widely used as an effective way to improve model performance especially before neural models were widely used. Although neural models are powerful in modeling long text sequences, external features from larger granular texts are still demonstrated to be useful in existing neural models (Zheng et al., 2013; Kurit"
2020.coling-main.187,P13-1076,0,0.0939792,"r approach outperforms strong baseline models and achieves state-of-the-art performance on all five datasets.1 1 Introduction Chinese word segmentation (CWS) and part-of-speech (POS) tagging are two fundamental tasks in Chinese natural language processing (NLP). Although they can be treated as two separate tasks in a sequential order, it has been demonstrated by previous studies that processing them jointly in a unified sequence labeling framework could be more effective, where CWS and POS tags are predicted in a single step (Ng and Low, 2004; Jiang et al., 2008; Wang et al., 2011; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Zhang et al., 2014; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In doing so, existing studies mainly focused on incorporating contextual information (e.g., n-grams) as features into their joint taggers, which had been widely used as an effective way to improve model performance especially before neural models were widely used. Although neural models are powerful in modeling long text sequences, external features from larger granular texts are still demonstrated to be useful in existing neural models (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2"
2020.coling-main.187,D10-1082,0,0.034898,"ation and joint tagging F scores of the models (using BERT encoder) with normal and multi-channel attentions are illustrated in (a) and (c), where N is constructed by including n-grams whose frequency is in range [2i , +∞) (1≤i≤10) and whose length is in range [1, n] (1≤n≤10), respectively. The weights (i.e., δk in Eq. 4) assigned to n-gram groups categorized by frequency and length in the multi-channel attention module are shown in (b) and (d), respectively. resources, such as well-defined dictionaries (Wang et al., 2011; Zhang et al., 2014), syntactic features form manual crafted resources (Zhang and Clark, 2010), information of Chinese radicals (Shao et al., 2017), or large auto-processed data (Zhang et al., 2018) are used, our approach only leverages the resource from the datasets, which reduces the cost to train a joint CWS and POS tagger. Overall, the above results demonstrate that weighting n-grams separately is an appropriate approach to improve joint CWS and POS tagging without requiring extra knowledge. 4.2 Effect of N-gram Categorization Methods We analyze the effect of different categorization methods, i.e., n-gram frequency and n-gram length, to the joint task. For frequency-based methods,"
2020.coling-main.187,E14-1062,0,0.300566,"models and achieves state-of-the-art performance on all five datasets.1 1 Introduction Chinese word segmentation (CWS) and part-of-speech (POS) tagging are two fundamental tasks in Chinese natural language processing (NLP). Although they can be treated as two separate tasks in a sequential order, it has been demonstrated by previous studies that processing them jointly in a unified sequence labeling framework could be more effective, where CWS and POS tags are predicted in a single step (Ng and Low, 2004; Jiang et al., 2008; Wang et al., 2011; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Zhang et al., 2014; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In doing so, existing studies mainly focused on incorporating contextual information (e.g., n-grams) as features into their joint taggers, which had been widely used as an effective way to improve model performance especially before neural models were widely used. Although neural models are powerful in modeling long text sequences, external features from larger granular texts are still demonstrated to be useful in existing neural models (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In these model"
2020.coling-main.187,D13-1061,0,0.390194,"rms strong baseline models and achieves state-of-the-art performance on all five datasets.1 1 Introduction Chinese word segmentation (CWS) and part-of-speech (POS) tagging are two fundamental tasks in Chinese natural language processing (NLP). Although they can be treated as two separate tasks in a sequential order, it has been demonstrated by previous studies that processing them jointly in a unified sequence labeling framework could be more effective, where CWS and POS tags are predicted in a single step (Ng and Low, 2004; Jiang et al., 2008; Wang et al., 2011; Sun, 2011; Zeng et al., 2013; Zheng et al., 2013; Zhang et al., 2014; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2018). In doing so, existing studies mainly focused on incorporating contextual information (e.g., n-grams) as features into their joint taggers, which had been widely used as an effective way to improve model performance especially before neural models were widely used. Although neural models are powerful in modeling long text sequences, external features from larger granular texts are still demonstrated to be useful in existing neural models (Zheng et al., 2013; Kurita et al., 2017; Shao et al., 2017; Zhang et al., 2"
2020.coling-main.24,2020.acl-main.514,0,0.229001,"on the dependency tree of the input sentence and an attention matrix (shown at the upper right side of Figure 1) is applied to the edges in the graph to weight the contextual features associated with a specific word, i.e., “soup”. In the following text, we firstly introduce normal GCN, then elaborate our proposed D-GCN, and finally illustrate EASA labeling with D-GCN. 2.1 Graph Convolutional Networks The representation of an input sentence always plays an important role in achieving good model performance when it is fed to different natural language processing (NLP) tasks (Song et al., 2017; Babanejad et al., 2020). Contextual features, such as n-grams and syntactic information, have been demonstrated to be highly useful to enhance text representation and thus improve NLP model performance (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Bastings et al., 2017; Marcheggiani and Titov, 2017; Yoon et al., 2018; Seyler et al., 2018; Kumar et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Margatina et al., 2019; De Cao et al., 2019; Tian et al., 2020a; Tia"
2020.coling-main.24,D17-1209,0,0.0606046,"Missing"
2020.coling-main.24,N19-1240,0,0.047342,"Missing"
2020.coling-main.24,N19-1423,0,0.238783,"as that in Eq.(1), attention (through pi,j ) is applied to the edge between xi and xj to weight different contextual features. Specifically, pi,j is computed via (l 1) (l 1) (l 1) (l 1) ai,j · exp(hi · hj ) (l) pi,j = Pn (l 1) (l 1) · hj ) j=1 ai,j · exp(hi (3) where hi · hj computes the interaction between xi and xj through inner product. Note that we also apply ai,j from A to computing pi,j so that the attention for any two words can be easily ignored if there is not an edge between them (ai,j = 0). 2.3 Tagging with Directional Graph Convolutional Networks (0) In our approach, we use BERT (Devlin et al., 2019) to encode the input X and obtain the hidden vector hi for (0) (L) each xi . Then we feed hi to L layers of D-GCN and obtain the corresponding output hi . Afterwards, we use (L) (L) a trainable matrix W to align hi to the output space by oi = W · hi . Finally, we apply a softmax decoder to oi to predict the joint label yˆi for aspect extraction and sentiment analysis via exp(oti ) yˆi = arg max P|T | t t=1 exp(oi ) (4) where T denotes the label set and oti refers to the value at dimension t in oi . 3 Experiment 3.1 Settings In our experiments, we use three benchmark datasets, including restaur"
2020.coling-main.24,P14-2009,0,0.0562269,", and finally illustrate EASA labeling with D-GCN. 2.1 Graph Convolutional Networks The representation of an input sentence always plays an important role in achieving good model performance when it is fed to different natural language processing (NLP) tasks (Song et al., 2017; Babanejad et al., 2020). Contextual features, such as n-grams and syntactic information, have been demonstrated to be highly useful to enhance text representation and thus improve NLP model performance (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Bastings et al., 2017; Marcheggiani and Titov, 2017; Yoon et al., 2018; Seyler et al., 2018; Kumar et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Margatina et al., 2019; De Cao et al., 2019; Tian et al., 2020a; Tian et al., 2020c; Tian et al., 2020d). In addition, it is also proved that GCN could be a powerful model to capture context features suggested by the graph-alike signals, e.g., dependency tree, of an input sentence. 2 A joint label contains two parts: aspect boundary identifier (i.e., B, I, E, O) and the sentiment"
2020.coling-main.24,P19-1048,0,0.240256,"t phenomenal”, the aspect terms are “ambiance” and “food” and the sentiment polarities towards them are positive and negative, respectively. In general, there are mainly three types of approaches for this task, i.e., pipeline, multi-task, and joint-label approaches. Pipeline approaches (Mitchell et al., 2013; Zhang et al., 2015; Hu et al., 2019) perform aspect extraction and sentiment analysis in a sequence, which is not straightforward and suffers from error propagation among different steps; multi-task approaches (Mitchell et al., 2013; Zhang et al., 2015; Ma et al., 2018; Luo et al., 2019; He et al., 2019; Hu et al., 2019) apply an encoder to the input and use a separate decoding process to extract aspects and predict their sentiments, where there could be mismatches between the two decoding results. As a comparison, joint-label approaches (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Li et al., 2019a; Hu et al., 2019) extract aspect terms and predict their sentiments simultaneously through a unified labeling scheme, which not only provides an one-step solution to EASA, but also avoids the aforementioned problems in the other two approaches. In most cases, previous studies demon"
2020.coling-main.24,P19-1051,0,0.353692,"ion End-to-end aspect-based sentiment analysis (EASA) aims to extract aspect terms in the text and predict their sentiment polarities so as to understand targeted sentiment towards particular objects. For example, in the sentence “The ambiance is minimal but food is not phenomenal”, the aspect terms are “ambiance” and “food” and the sentiment polarities towards them are positive and negative, respectively. In general, there are mainly three types of approaches for this task, i.e., pipeline, multi-task, and joint-label approaches. Pipeline approaches (Mitchell et al., 2013; Zhang et al., 2015; Hu et al., 2019) perform aspect extraction and sentiment analysis in a sequence, which is not straightforward and suffers from error propagation among different steps; multi-task approaches (Mitchell et al., 2013; Zhang et al., 2015; Ma et al., 2018; Luo et al., 2019; He et al., 2019; Hu et al., 2019) apply an encoder to the input and use a separate decoding process to extract aspects and predict their sentiments, where there could be mismatches between the two decoding results. As a comparison, joint-label approaches (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Li et al., 2019a; Hu et al., 20"
2020.coling-main.24,D19-1549,0,0.134628,"ifferent natural language processing (NLP) tasks (Song et al., 2017; Babanejad et al., 2020). Contextual features, such as n-grams and syntactic information, have been demonstrated to be highly useful to enhance text representation and thus improve NLP model performance (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Bastings et al., 2017; Marcheggiani and Titov, 2017; Yoon et al., 2018; Seyler et al., 2018; Kumar et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Margatina et al., 2019; De Cao et al., 2019; Tian et al., 2020a; Tian et al., 2020c; Tian et al., 2020d). In addition, it is also proved that GCN could be a powerful model to capture context features suggested by the graph-alike signals, e.g., dependency tree, of an input sentence. 2 A joint label contains two parts: aspect boundary identifier (i.e., B, I, E, O) and the sentiment mark (i.e., POS, NEG, NEU). Normal GCN models usually have L layers, and its input graph can be built upon the dependency tree of the input sentence, where an edge is added to every two words, i.e., xi , and xj , if"
2020.coling-main.24,D07-1117,0,0.0592072,"specific word, i.e., “soup”. In the following text, we firstly introduce normal GCN, then elaborate our proposed D-GCN, and finally illustrate EASA labeling with D-GCN. 2.1 Graph Convolutional Networks The representation of an input sentence always plays an important role in achieving good model performance when it is fed to different natural language processing (NLP) tasks (Song et al., 2017; Babanejad et al., 2020). Contextual features, such as n-grams and syntactic information, have been demonstrated to be highly useful to enhance text representation and thus improve NLP model performance (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Bastings et al., 2017; Marcheggiani and Titov, 2017; Yoon et al., 2018; Seyler et al., 2018; Kumar et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Margatina et al., 2019; De Cao et al., 2019; Tian et al., 2020a; Tian et al., 2020c; Tian et al., 2020d). In addition, it is also proved that GCN could be a powerful model to capture context features suggested by the graph-alike signals, e.g., dependency tree, of an"
2020.coling-main.24,P09-1059,0,0.0285581,"“soup”. In the following text, we firstly introduce normal GCN, then elaborate our proposed D-GCN, and finally illustrate EASA labeling with D-GCN. 2.1 Graph Convolutional Networks The representation of an input sentence always plays an important role in achieving good model performance when it is fed to different natural language processing (NLP) tasks (Song et al., 2017; Babanejad et al., 2020). Contextual features, such as n-grams and syntactic information, have been demonstrated to be highly useful to enhance text representation and thus improve NLP model performance (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Bastings et al., 2017; Marcheggiani and Titov, 2017; Yoon et al., 2018; Seyler et al., 2018; Kumar et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Margatina et al., 2019; De Cao et al., 2019; Tian et al., 2020a; Tian et al., 2020c; Tian et al., 2020d). In addition, it is also proved that GCN could be a powerful model to capture context features suggested by the graph-alike signals, e.g., dependency tree, of an input sentence. 2 A"
2020.coling-main.24,N18-2041,0,0.0259554,"ays an important role in achieving good model performance when it is fed to different natural language processing (NLP) tasks (Song et al., 2017; Babanejad et al., 2020). Contextual features, such as n-grams and syntactic information, have been demonstrated to be highly useful to enhance text representation and thus improve NLP model performance (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Bastings et al., 2017; Marcheggiani and Titov, 2017; Yoon et al., 2018; Seyler et al., 2018; Kumar et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Margatina et al., 2019; De Cao et al., 2019; Tian et al., 2020a; Tian et al., 2020c; Tian et al., 2020d). In addition, it is also proved that GCN could be a powerful model to capture context features suggested by the graph-alike signals, e.g., dependency tree, of an input sentence. 2 A joint label contains two parts: aspect boundary identifier (i.e., B, I, E, O) and the sentiment mark (i.e., POS, NEG, NEU). Normal GCN models usually have L layers, and its input graph can be built upon the dependency tree of the"
2020.coling-main.24,D19-5505,0,0.162893,"et al., 2015; Hu et al., 2019) perform aspect extraction and sentiment analysis in a sequence, which is not straightforward and suffers from error propagation among different steps; multi-task approaches (Mitchell et al., 2013; Zhang et al., 2015; Ma et al., 2018; Luo et al., 2019; He et al., 2019; Hu et al., 2019) apply an encoder to the input and use a separate decoding process to extract aspects and predict their sentiments, where there could be mismatches between the two decoding results. As a comparison, joint-label approaches (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Li et al., 2019a; Hu et al., 2019) extract aspect terms and predict their sentiments simultaneously through a unified labeling scheme, which not only provides an one-step solution to EASA, but also avoids the aforementioned problems in the other two approaches. In most cases, previous studies demonstrate that a good modeling of contextual information is effective in improving EASA performance. However, these studies mainly rely on powerful encoders (e.g., Bi-LSTM, CNN, BERT) (Zhang et al., 2015; Ma et al., 2018; Schmitt et al., 2018; Li et al., 2019a; Li et al., 2019b; Luo et al., 2019; He et al., 2019; Hu e"
2020.coling-main.24,P19-1056,0,0.14653,"mal but food is not phenomenal”, the aspect terms are “ambiance” and “food” and the sentiment polarities towards them are positive and negative, respectively. In general, there are mainly three types of approaches for this task, i.e., pipeline, multi-task, and joint-label approaches. Pipeline approaches (Mitchell et al., 2013; Zhang et al., 2015; Hu et al., 2019) perform aspect extraction and sentiment analysis in a sequence, which is not straightforward and suffers from error propagation among different steps; multi-task approaches (Mitchell et al., 2013; Zhang et al., 2015; Ma et al., 2018; Luo et al., 2019; He et al., 2019; Hu et al., 2019) apply an encoder to the input and use a separate decoding process to extract aspects and predict their sentiments, where there could be mismatches between the two decoding results. As a comparison, joint-label approaches (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Li et al., 2019a; Hu et al., 2019) extract aspect terms and predict their sentiments simultaneously through a unified labeling scheme, which not only provides an one-step solution to EASA, but also avoids the aforementioned problems in the other two approaches. In most cases, previ"
2020.coling-main.24,D18-1504,0,0.0195945,"ambiance is minimal but food is not phenomenal”, the aspect terms are “ambiance” and “food” and the sentiment polarities towards them are positive and negative, respectively. In general, there are mainly three types of approaches for this task, i.e., pipeline, multi-task, and joint-label approaches. Pipeline approaches (Mitchell et al., 2013; Zhang et al., 2015; Hu et al., 2019) perform aspect extraction and sentiment analysis in a sequence, which is not straightforward and suffers from error propagation among different steps; multi-task approaches (Mitchell et al., 2013; Zhang et al., 2015; Ma et al., 2018; Luo et al., 2019; He et al., 2019; Hu et al., 2019) apply an encoder to the input and use a separate decoding process to extract aspects and predict their sentiments, where there could be mismatches between the two decoding results. As a comparison, joint-label approaches (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Li et al., 2019a; Hu et al., 2019) extract aspect terms and predict their sentiments simultaneously through a unified labeling scheme, which not only provides an one-step solution to EASA, but also avoids the aforementioned problems in the other two approaches. In"
2020.coling-main.24,D17-1159,0,0.121773,"., 2019; He et al., 2019; Hu et al., 2019) and pre-trained embedings (e.g., GloVe, word2vec, FastText) (Schmitt et al., 2018; Li et al., 2019a) to learn contextual information, with limited effort paid to leveraging advanced architectures and extra knowledge for this task. To extend such effort, graph convolutional networks (GCN) was proposed and shows its effectiveness in conventional sentiment analysis (Zhang et al., 2019; Sun et al., 2019), as well as other tasks, e.g., text classification (Kipf and Welling, 2016), neural machine translation (Bastings et al., 2017), semantic role labeling (Marcheggiani and Titov, 2017), etc. Moreover, consider that discriminatively modeling the contextual features of a given word according to their positional relations to the word is helpful in text representation learning * Equal contribution. Corresponding author. 1 The code and models are released at https://github.com/cuhksz-nlp/DGSA. This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/. † License details: http: Figure 1: The overall architecture of our approach, where the graph is built upon the dependency tree of an input sentence, with all edges"
2020.coling-main.24,P19-1385,0,0.0256148,"e processing (NLP) tasks (Song et al., 2017; Babanejad et al., 2020). Contextual features, such as n-grams and syntactic information, have been demonstrated to be highly useful to enhance text representation and thus improve NLP model performance (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Bastings et al., 2017; Marcheggiani and Titov, 2017; Yoon et al., 2018; Seyler et al., 2018; Kumar et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Margatina et al., 2019; De Cao et al., 2019; Tian et al., 2020a; Tian et al., 2020c; Tian et al., 2020d). In addition, it is also proved that GCN could be a powerful model to capture context features suggested by the graph-alike signals, e.g., dependency tree, of an input sentence. 2 A joint label contains two parts: aspect boundary identifier (i.e., B, I, E, O) and the sentiment mark (i.e., POS, NEG, NEU). Normal GCN models usually have L layers, and its input graph can be built upon the dependency tree of the input sentence, where an edge is added to every two words, i.e., xi , and xj , if there exists a dependen"
2020.coling-main.24,D13-1171,0,0.0620208,"Missing"
2020.coling-main.24,S14-2004,0,0.0606874,"or hi for (0) (L) each xi . Then we feed hi to L layers of D-GCN and obtain the corresponding output hi . Afterwards, we use (L) (L) a trainable matrix W to align hi to the output space by oi = W · hi . Finally, we apply a softmax decoder to oi to predict the joint label yˆi for aspect extraction and sentiment analysis via exp(oti ) yˆi = arg max P|T | t t=1 exp(oi ) (4) where T denotes the label set and oti refers to the value at dimension t in oi . 3 Experiment 3.1 Settings In our experiments, we use three benchmark datasets, including restaurant (REST) dataset from SemEval ABSA challenges (Pontiki et al., 2014; Pontiki et al., 2015; Pontiki et al., 2016), laptop (LPTP) dataset from Pontiki et al. (2014), and Twitter (TWTR) dataset from Mitchell et al. (2013). All these datasets contain the ground truth labels of target aspect and their sentiment polarities. Following (Li et al., 2019a; Li et al., 2019b; He et al., 2019; Hu et al., 2019), we only consider three sentiment polarities, i.e., positive, negative, and neutral, where all cases with conflict label in REST and LPTP dataset are ignored. We report the statistics (the number of sentences, aspects with respect to positive, neutral, and negative"
2020.coling-main.24,S15-2082,0,0.0710655,"Missing"
2020.coling-main.24,D18-1139,0,0.0233367,"int-label approaches (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Li et al., 2019a; Hu et al., 2019) extract aspect terms and predict their sentiments simultaneously through a unified labeling scheme, which not only provides an one-step solution to EASA, but also avoids the aforementioned problems in the other two approaches. In most cases, previous studies demonstrate that a good modeling of contextual information is effective in improving EASA performance. However, these studies mainly rely on powerful encoders (e.g., Bi-LSTM, CNN, BERT) (Zhang et al., 2015; Ma et al., 2018; Schmitt et al., 2018; Li et al., 2019a; Li et al., 2019b; Luo et al., 2019; He et al., 2019; Hu et al., 2019) and pre-trained embedings (e.g., GloVe, word2vec, FastText) (Schmitt et al., 2018; Li et al., 2019a) to learn contextual information, with limited effort paid to leveraging advanced architectures and extra knowledge for this task. To extend such effort, graph convolutional networks (GCN) was proposed and shows its effectiveness in conventional sentiment analysis (Zhang et al., 2019; Sun et al., 2019), as well as other tasks, e.g., text classification (Kipf and Welling, 2016), neural machine translation (B"
2020.coling-main.24,P18-2039,0,0.0629334,"ut sentence always plays an important role in achieving good model performance when it is fed to different natural language processing (NLP) tasks (Song et al., 2017; Babanejad et al., 2020). Contextual features, such as n-grams and syntactic information, have been demonstrated to be highly useful to enhance text representation and thus improve NLP model performance (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Bastings et al., 2017; Marcheggiani and Titov, 2017; Yoon et al., 2018; Seyler et al., 2018; Kumar et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Margatina et al., 2019; De Cao et al., 2019; Tian et al., 2020a; Tian et al., 2020c; Tian et al., 2020d). In addition, it is also proved that GCN could be a powerful model to capture context features suggested by the graph-alike signals, e.g., dependency tree, of an input sentence. 2 A joint label contains two parts: aspect boundary identifier (i.e., B, I, E, O) and the sentiment mark (i.e., POS, NEG, NEU). Normal GCN models usually have L layers, and its input graph can be built upon the dep"
2020.coling-main.24,N18-2074,0,0.0209903,"dels are released at https://github.com/cuhksz-nlp/DGSA. This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/. † License details: http: Figure 1: The overall architecture of our approach, where the graph is built upon the dependency tree of an input sentence, with all edges in the graph illustrated in the adjacency matrix. The red, blue, and orange colors illustrate our modeling for contextual features in left, right, and self positional relations with a specific word, respectively. (Zhang et al., 2017; Song et al., 2018; Shaw et al., 2018; Tian et al., 2020b), any encoder for EASA could also be beneficial from adding such treatment to model the input text. Therefore, it is expected to enhance conventional GCN with directional information for different parts of input, so that one can distinguish them and appropriately model the contextual information for EASA. In this paper, we propose directional graph convolutional networks (D-GCN) for EASA, which performs the task following the sequence labeling paradigm and models dependency relations among words in the input with an appropriate architecture. Specifically, for an input sent"
2020.coling-main.24,song-xia-2012-using,1,0.806252,"tly introduce normal GCN, then elaborate our proposed D-GCN, and finally illustrate EASA labeling with D-GCN. 2.1 Graph Convolutional Networks The representation of an input sentence always plays an important role in achieving good model performance when it is fed to different natural language processing (NLP) tasks (Song et al., 2017; Babanejad et al., 2020). Contextual features, such as n-grams and syntactic information, have been demonstrated to be highly useful to enhance text representation and thus improve NLP model performance (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Bastings et al., 2017; Marcheggiani and Titov, 2017; Yoon et al., 2018; Seyler et al., 2018; Kumar et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Margatina et al., 2019; De Cao et al., 2019; Tian et al., 2020a; Tian et al., 2020c; Tian et al., 2020d). In addition, it is also proved that GCN could be a powerful model to capture context features suggested by the graph-alike signals, e.g., dependency tree, of an input sentence. 2 A joint label contains two parts: aspect"
2020.coling-main.24,I13-1071,1,0.481837,"e our proposed D-GCN, and finally illustrate EASA labeling with D-GCN. 2.1 Graph Convolutional Networks The representation of an input sentence always plays an important role in achieving good model performance when it is fed to different natural language processing (NLP) tasks (Song et al., 2017; Babanejad et al., 2020). Contextual features, such as n-grams and syntactic information, have been demonstrated to be highly useful to enhance text representation and thus improve NLP model performance (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Bastings et al., 2017; Marcheggiani and Titov, 2017; Yoon et al., 2018; Seyler et al., 2018; Kumar et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Margatina et al., 2019; De Cao et al., 2019; Tian et al., 2020a; Tian et al., 2020c; Tian et al., 2020d). In addition, it is also proved that GCN could be a powerful model to capture context features suggested by the graph-alike signals, e.g., dependency tree, of an input sentence. 2 A joint label contains two parts: aspect boundary identifier (i.e., B, I, E, O)"
2020.coling-main.24,C12-2116,1,0.457484,"GCN, then elaborate our proposed D-GCN, and finally illustrate EASA labeling with D-GCN. 2.1 Graph Convolutional Networks The representation of an input sentence always plays an important role in achieving good model performance when it is fed to different natural language processing (NLP) tasks (Song et al., 2017; Babanejad et al., 2020). Contextual features, such as n-grams and syntactic information, have been demonstrated to be highly useful to enhance text representation and thus improve NLP model performance (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Bastings et al., 2017; Marcheggiani and Titov, 2017; Yoon et al., 2018; Seyler et al., 2018; Kumar et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Margatina et al., 2019; De Cao et al., 2019; Tian et al., 2020a; Tian et al., 2020c; Tian et al., 2020d). In addition, it is also proved that GCN could be a powerful model to capture context features suggested by the graph-alike signals, e.g., dependency tree, of an input sentence. 2 A joint label contains two parts: aspect boundary identifie"
2020.coling-main.24,K17-1016,1,0.631193,"the graph is built on the dependency tree of the input sentence and an attention matrix (shown at the upper right side of Figure 1) is applied to the edges in the graph to weight the contextual features associated with a specific word, i.e., “soup”. In the following text, we firstly introduce normal GCN, then elaborate our proposed D-GCN, and finally illustrate EASA labeling with D-GCN. 2.1 Graph Convolutional Networks The representation of an input sentence always plays an important role in achieving good model performance when it is fed to different natural language processing (NLP) tasks (Song et al., 2017; Babanejad et al., 2020). Contextual features, such as n-grams and syntactic information, have been demonstrated to be highly useful to enhance text representation and thus improve NLP model performance (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Bastings et al., 2017; Marcheggiani and Titov, 2017; Yoon et al., 2018; Seyler et al., 2018; Kumar et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Margatina et al., 2019; De Cao et al., 2019"
2020.coling-main.24,N18-2028,1,0.764322,". 1 The code and models are released at https://github.com/cuhksz-nlp/DGSA. This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/. † License details: http: Figure 1: The overall architecture of our approach, where the graph is built upon the dependency tree of an input sentence, with all edges in the graph illustrated in the adjacency matrix. The red, blue, and orange colors illustrate our modeling for contextual features in left, right, and self positional relations with a specific word, respectively. (Zhang et al., 2017; Song et al., 2018; Shaw et al., 2018; Tian et al., 2020b), any encoder for EASA could also be beneficial from adding such treatment to model the input text. Therefore, it is expected to enhance conventional GCN with directional information for different parts of input, so that one can distinguish them and appropriately model the contextual information for EASA. In this paper, we propose directional graph convolutional networks (D-GCN) for EASA, which performs the task following the sequence labeling paradigm and models dependency relations among words in the input with an appropriate architecture. Specifically"
2020.coling-main.24,D19-1569,0,0.233763,"udies mainly rely on powerful encoders (e.g., Bi-LSTM, CNN, BERT) (Zhang et al., 2015; Ma et al., 2018; Schmitt et al., 2018; Li et al., 2019a; Li et al., 2019b; Luo et al., 2019; He et al., 2019; Hu et al., 2019) and pre-trained embedings (e.g., GloVe, word2vec, FastText) (Schmitt et al., 2018; Li et al., 2019a) to learn contextual information, with limited effort paid to leveraging advanced architectures and extra knowledge for this task. To extend such effort, graph convolutional networks (GCN) was proposed and shows its effectiveness in conventional sentiment analysis (Zhang et al., 2019; Sun et al., 2019), as well as other tasks, e.g., text classification (Kipf and Welling, 2016), neural machine translation (Bastings et al., 2017), semantic role labeling (Marcheggiani and Titov, 2017), etc. Moreover, consider that discriminatively modeling the contextual features of a given word according to their positional relations to the word is helpful in text representation learning * Equal contribution. Corresponding author. 1 The code and models are released at https://github.com/cuhksz-nlp/DGSA. This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.org"
2020.coling-main.24,2020.acl-main.735,1,0.954476,"t https://github.com/cuhksz-nlp/DGSA. This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/. † License details: http: Figure 1: The overall architecture of our approach, where the graph is built upon the dependency tree of an input sentence, with all edges in the graph illustrated in the adjacency matrix. The red, blue, and orange colors illustrate our modeling for contextual features in left, right, and self positional relations with a specific word, respectively. (Zhang et al., 2017; Song et al., 2018; Shaw et al., 2018; Tian et al., 2020b), any encoder for EASA could also be beneficial from adding such treatment to model the input text. Therefore, it is expected to enhance conventional GCN with directional information for different parts of input, so that one can distinguish them and appropriately model the contextual information for EASA. In this paper, we propose directional graph convolutional networks (D-GCN) for EASA, which performs the task following the sequence labeling paradigm and models dependency relations among words in the input with an appropriate architecture. Specifically, for an input sentence, we firstly bu"
2020.coling-main.24,2020.emnlp-main.487,1,0.857995,"t https://github.com/cuhksz-nlp/DGSA. This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/. † License details: http: Figure 1: The overall architecture of our approach, where the graph is built upon the dependency tree of an input sentence, with all edges in the graph illustrated in the adjacency matrix. The red, blue, and orange colors illustrate our modeling for contextual features in left, right, and self positional relations with a specific word, respectively. (Zhang et al., 2017; Song et al., 2018; Shaw et al., 2018; Tian et al., 2020b), any encoder for EASA could also be beneficial from adding such treatment to model the input text. Therefore, it is expected to enhance conventional GCN with directional information for different parts of input, so that one can distinguish them and appropriately model the contextual information for EASA. In this paper, we propose directional graph convolutional networks (D-GCN) for EASA, which performs the task following the sequence labeling paradigm and models dependency relations among words in the input with an appropriate architecture. Specifically, for an input sentence, we firstly bu"
2020.coling-main.24,2020.findings-emnlp.153,1,0.904057,"t https://github.com/cuhksz-nlp/DGSA. This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/. † License details: http: Figure 1: The overall architecture of our approach, where the graph is built upon the dependency tree of an input sentence, with all edges in the graph illustrated in the adjacency matrix. The red, blue, and orange colors illustrate our modeling for contextual features in left, right, and self positional relations with a specific word, respectively. (Zhang et al., 2017; Song et al., 2018; Shaw et al., 2018; Tian et al., 2020b), any encoder for EASA could also be beneficial from adding such treatment to model the input text. Therefore, it is expected to enhance conventional GCN with directional information for different parts of input, so that one can distinguish them and appropriately model the contextual information for EASA. In this paper, we propose directional graph convolutional networks (D-GCN) for EASA, which performs the task following the sequence labeling paradigm and models dependency relations among words in the input with an appropriate architecture. Specifically, for an input sentence, we firstly bu"
2020.coling-main.24,2020.acl-main.734,1,0.926154,"t https://github.com/cuhksz-nlp/DGSA. This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/. † License details: http: Figure 1: The overall architecture of our approach, where the graph is built upon the dependency tree of an input sentence, with all edges in the graph illustrated in the adjacency matrix. The red, blue, and orange colors illustrate our modeling for contextual features in left, right, and self positional relations with a specific word, respectively. (Zhang et al., 2017; Song et al., 2018; Shaw et al., 2018; Tian et al., 2020b), any encoder for EASA could also be beneficial from adding such treatment to model the input text. Therefore, it is expected to enhance conventional GCN with directional information for different parts of input, so that one can distinguish them and appropriately model the contextual information for EASA. In this paper, we propose directional graph convolutional networks (D-GCN) for EASA, which performs the task following the sequence labeling paradigm and models dependency relations among words in the input with an appropriate architecture. Specifically, for an input sentence, we firstly bu"
2020.coling-main.24,I11-1035,0,0.0830395,"owing text, we firstly introduce normal GCN, then elaborate our proposed D-GCN, and finally illustrate EASA labeling with D-GCN. 2.1 Graph Convolutional Networks The representation of an input sentence always plays an important role in achieving good model performance when it is fed to different natural language processing (NLP) tasks (Song et al., 2017; Babanejad et al., 2020). Contextual features, such as n-grams and syntactic information, have been demonstrated to be highly useful to enhance text representation and thus improve NLP model performance (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Bastings et al., 2017; Marcheggiani and Titov, 2017; Yoon et al., 2018; Seyler et al., 2018; Kumar et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Margatina et al., 2019; De Cao et al., 2019; Tian et al., 2020a; Tian et al., 2020c; Tian et al., 2020d). In addition, it is also proved that GCN could be a powerful model to capture context features suggested by the graph-alike signals, e.g., dependency tree, of an input sentence. 2 A joint label contai"
2020.coling-main.24,N18-1142,0,0.0206193,"sentation of an input sentence always plays an important role in achieving good model performance when it is fed to different natural language processing (NLP) tasks (Song et al., 2017; Babanejad et al., 2020). Contextual features, such as n-grams and syntactic information, have been demonstrated to be highly useful to enhance text representation and thus improve NLP model performance (Huang et al., 2007; Jiang et al., 2009; Wang et al., 2011; Song and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Bastings et al., 2017; Marcheggiani and Titov, 2017; Yoon et al., 2018; Seyler et al., 2018; Kumar et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Margatina et al., 2019; De Cao et al., 2019; Tian et al., 2020a; Tian et al., 2020c; Tian et al., 2020d). In addition, it is also proved that GCN could be a powerful model to capture context features suggested by the graph-alike signals, e.g., dependency tree, of an input sentence. 2 A joint label contains two parts: aspect boundary identifier (i.e., B, I, E, O) and the sentiment mark (i.e., POS, NEG, NEU). Normal GCN models usually have L layers, and its input graph can"
2020.coling-main.24,D15-1073,0,0.137144,"tasets.1 1 Introduction End-to-end aspect-based sentiment analysis (EASA) aims to extract aspect terms in the text and predict their sentiment polarities so as to understand targeted sentiment towards particular objects. For example, in the sentence “The ambiance is minimal but food is not phenomenal”, the aspect terms are “ambiance” and “food” and the sentiment polarities towards them are positive and negative, respectively. In general, there are mainly three types of approaches for this task, i.e., pipeline, multi-task, and joint-label approaches. Pipeline approaches (Mitchell et al., 2013; Zhang et al., 2015; Hu et al., 2019) perform aspect extraction and sentiment analysis in a sequence, which is not straightforward and suffers from error propagation among different steps; multi-task approaches (Mitchell et al., 2013; Zhang et al., 2015; Ma et al., 2018; Luo et al., 2019; He et al., 2019; Hu et al., 2019) apply an encoder to the input and use a separate decoding process to extract aspects and predict their sentiments, where there could be mismatches between the two decoding results. As a comparison, joint-label approaches (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Li et al., 20"
2020.coling-main.24,D17-1004,0,0.0381213,"Corresponding author. 1 The code and models are released at https://github.com/cuhksz-nlp/DGSA. This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/. † License details: http: Figure 1: The overall architecture of our approach, where the graph is built upon the dependency tree of an input sentence, with all edges in the graph illustrated in the adjacency matrix. The red, blue, and orange colors illustrate our modeling for contextual features in left, right, and self positional relations with a specific word, respectively. (Zhang et al., 2017; Song et al., 2018; Shaw et al., 2018; Tian et al., 2020b), any encoder for EASA could also be beneficial from adding such treatment to model the input text. Therefore, it is expected to enhance conventional GCN with directional information for different parts of input, so that one can distinguish them and appropriately model the contextual information for EASA. In this paper, we propose directional graph convolutional networks (D-GCN) for EASA, which performs the task following the sequence labeling paradigm and models dependency relations among words in the input with an appropriate archite"
2020.coling-main.24,D19-1464,0,0.143251,"e. However, these studies mainly rely on powerful encoders (e.g., Bi-LSTM, CNN, BERT) (Zhang et al., 2015; Ma et al., 2018; Schmitt et al., 2018; Li et al., 2019a; Li et al., 2019b; Luo et al., 2019; He et al., 2019; Hu et al., 2019) and pre-trained embedings (e.g., GloVe, word2vec, FastText) (Schmitt et al., 2018; Li et al., 2019a) to learn contextual information, with limited effort paid to leveraging advanced architectures and extra knowledge for this task. To extend such effort, graph convolutional networks (GCN) was proposed and shows its effectiveness in conventional sentiment analysis (Zhang et al., 2019; Sun et al., 2019), as well as other tasks, e.g., text classification (Kipf and Welling, 2016), neural machine translation (Bastings et al., 2017), semantic role labeling (Marcheggiani and Titov, 2017), etc. Moreover, consider that discriminatively modeling the contextual features of a given word according to their positional relations to the word is helpful in text representation learning * Equal contribution. Corresponding author. 1 The code and models are released at https://github.com/cuhksz-nlp/DGSA. This work is licensed under a Creative Commons Attribution 4.0 International License. //"
2020.coling-main.63,P18-1063,0,0.0388768,"Missing"
2020.coling-main.63,E17-2110,0,0.0155177,"ed in this study, we show that high-quality summaries can be generated by extracting two types of utterances, namely, problem statements and treatment recommendations. Experimental results demonstrate that HET outperforms strong baselines and models from previous studies, and adding conversation-related features can further improve system performance.1 1 Introduction Applying natural language processing (NLP) techniques to the medical field is a prevailing trend nowadays and has great potential in many applications, such as key information extraction in medical literature (Kim ˇ et al., 2011; Dernoncourt et al., 2017; Seva et al., 2018), risk factor identification in electronic health records (Chang et al., 2015; Cormack et al., 2015; Cheng et al., 2016), and medical question answering (Pampari et al., 2018; Tian et al., 2019). As the demand for healthcare services increases greatly in the past decades,2 it is urgent to improve the quality and efficiency of healthcare, reduce workload and mental stress of health providers and increase patient satisfaction. Recently, Internet-based healthcare platforms such as online doctor systems and doctor-patient cyber communities have been increasingly used by patient"
2020.coling-main.63,N19-1423,0,0.0316364,"versation with the two types summaries: “SUM1” for problem statement and “SUM2” for treatment recommendations. SUM2 has two types, i.e., type A and B, which will be explained in the next section. Besides, we propose a hierarchical encoder-tagger (HET) model for extractive summarization to tag each utterance in a medical conversation with regard to whether an utterance is a problem statement or a treatment recommendation. We further enhance the model with end-to-end memory networks (Sukhbaatar et al., 2015) to incorporate the information in relevant utterances in the conversation. We use BERT (Devlin et al., 2019) as the token-level encoder and try several utterance-level encoders and taggers. Experimental results show that HET outperforms strong baselines as well as models from previous studies on this dataset. Analyses are also conducted to better understand our findings from the results. 2 A Corpus of Medical Conversations Medical conversation is a type of task-oriented conversation. Different from ordinary conversations in which topics are often fluid, in task-specific conversations, participants interact to accomplish a projected set of goals and sub-goals (Litman and Allen, 1987; Drew and Heritag"
2020.coling-main.63,N19-1276,0,0.042384,"h is illustrated in Figure 2. Also, it is worth noting that our method can generate the two types of summaries simultaneously, since they directly come from the predicted PD/DT/OT labels. In the following texts, we firstly introduce the memory module and then elaborate the whole hierarchical tagging process with the memories. 3.1 Utterance Memories As discussed above, we regard our summarization task as an utterance tagging process. Similar to other tagging tasks in which contextual information is highly helpful in determining the output tags (Song and Xia, 2012; Marcheggiani and Titov, 2017; Higashiyama et al., 2019; Tian et al., 2020a; Tian et al., 2020b), for each utterance ui in the conversation, relevant utterances in each conversation also provide useful information (Zhang et al., 2018) to determine whether a particular utterance is important. To exploit the information from relevant utterances, we adopt end-to-end memory networks (Sukhbaatar et al., 2015), which (as well as the variants) have been demonstrated to be useful in many tasks (Miller et al., 2016; Tian et al., 2020c), to learn from them to facilitate important utterance tagging. In doing so, we first map all utterances [u1 , · · · , uj ,"
2020.coling-main.63,W16-3648,0,0.0908501,"particular health problems, it is possible to perform the task by identifying important utterances in such conversations. In this study, important utterances refer to the utterances that contain key information for the medical problem or for the treatment. Therefore, our focus is different from existing studies on utterances in conversations, where they pay more attention to assessment of utterances with respect to the functionalities of utterances in the conversations, such as analyzing automatically generated utterances regarding their suitability within particular conversational contexts (Inaba and Takahashi, 2016; Lison and Bibauw, 2017), evaluation of human conversational performance on readability, sensibility, and social involvement (Dascalu et al., 2010), and identification of segments of utterance that are produced with more emphases for certain interactional purposes (Takeuchi et al., 2007). Little research has been done to identify important utterances that contribute to a specific outcome of a conversation, which in this study refers to the content about patient’s problem and recommendation treatment in the conversation. To conduct the medical conversation summarization task, in this paper, we"
2020.coling-main.63,W13-3214,0,0.0244349,"the label PT and DT to generate the summary of patient’s problem (SUM1) and doctor’s diagnoses (SUM2), respectively. 4 Experiments 4.1 Settings We experiment our HET model with and without the memory on our corpus. For model implementation, at the token-level encoder (TE), we use the Chinese version of BERT10 and ZEN (Diao et al., 2019)11 with their default settings, where for both BERT and ZEN, we use 12 layers of multi-head attentions with the dimension of hidden vectors set to 768; for the utterance level, we firstly run experiments with no encoder; then following previous studies such as (Kalchbrenner and Blunsom, 2013; Zhao et al., 2017; Kumar et al., 2018), we experiment with two recurrent neural network models (namely, LSTM and BiLSTM) to encode the utterance sequence for each conversation, where the dimension of hidden states is set to 300 for LSTM and 150 for BiLSTM encoder. In the memory module, the embedding matrix and BiLSTM encoder for obtaining the value vectors vj for uj are applied directly to the Chinese characters in the utterance. All parameters in the embedding matrix and the BiLSTM encoder in the memory module are initialized randomly, with the dimension of embedding and hidden states set t"
2020.coling-main.63,N16-1062,0,0.024137,"lead to unreadable summmaries in most cases. Therefore, to have a good performance in conversation summarization in the medical domain, task-specific designs of summarization model are expected. 5.2 Utterance Modeling in Conversations Studies on dialogue systems have drawn much attention recently, where many of them have been done on utterance modeling in human-human conversations (Wang et al., 2018a; Liu et al., 2019). In these studies, one stream of utterance modeling focuses on dialogue act classifications, which aims to attribute one of predefined acts to each utterance in conversations (Lee and Dernoncourt, 2016; Liu et al., 2017; Kumar et al., 2018; Wang et al., 2018b; Raheja and Tetreault, 2019). Another stream focuses on assessment of utterances in terms of their quality in various aspects, such as sentiment analysis (Inaba and Takahashi, 2016; Lison and Bibauw, 2017; Misra et al., 2019). Our study on extractive summarizations for conversation can be regarded as in the line of the latter stream in evaluating utterances for human-human conversations, where little research has been done for utterances based on their importance to the pragmatic outcomes (i.e., summaries for problem statement and trea"
2020.coling-main.63,W17-5546,0,0.154587,", it is possible to perform the task by identifying important utterances in such conversations. In this study, important utterances refer to the utterances that contain key information for the medical problem or for the treatment. Therefore, our focus is different from existing studies on utterances in conversations, where they pay more attention to assessment of utterances with respect to the functionalities of utterances in the conversations, such as analyzing automatically generated utterances regarding their suitability within particular conversational contexts (Inaba and Takahashi, 2016; Lison and Bibauw, 2017), evaluation of human conversational performance on readability, sensibility, and social involvement (Dascalu et al., 2010), and identification of segments of utterance that are produced with more emphases for certain interactional purposes (Takeuchi et al., 2007). Little research has been done to identify important utterances that contribute to a specific outcome of a conversation, which in this study refers to the content about patient’s problem and recommendation treatment in the conversation. To conduct the medical conversation summarization task, in this paper, we propose a new benchmark"
2020.coling-main.63,D17-1231,0,0.0180325,"ies in most cases. Therefore, to have a good performance in conversation summarization in the medical domain, task-specific designs of summarization model are expected. 5.2 Utterance Modeling in Conversations Studies on dialogue systems have drawn much attention recently, where many of them have been done on utterance modeling in human-human conversations (Wang et al., 2018a; Liu et al., 2019). In these studies, one stream of utterance modeling focuses on dialogue act classifications, which aims to attribute one of predefined acts to each utterance in conversations (Lee and Dernoncourt, 2016; Liu et al., 2017; Kumar et al., 2018; Wang et al., 2018b; Raheja and Tetreault, 2019). Another stream focuses on assessment of utterances in terms of their quality in various aspects, such as sentiment analysis (Inaba and Takahashi, 2016; Lison and Bibauw, 2017; Misra et al., 2019). Our study on extractive summarizations for conversation can be regarded as in the line of the latter stream in evaluating utterances for human-human conversations, where little research has been done for utterances based on their importance to the pragmatic outcomes (i.e., summaries for problem statement and treatment recommendati"
2020.coling-main.63,D19-1300,1,0.818553,"SUM1) and treatment recommendation (SUM2), respectively. Therefore, adding SR would help our model to focus more on the utterances for the patients and the doctors when it is predicting PD and DT labels for SUM1 and SUM2, respectively. 5 5.1 Related Work Extractive Summarization As a direct research line related to our work, extractive summarization aims to extract important sentences in the input and use them to form a summary. Most previous studies focused on document summarization (Nallapati et al., 2017; Narayan et al., 2018; Wang et al., 2019; Zhang et al., 2019; Xiao and Carenini, 2019; Luo et al., 2019) while some focused on summarization of meeting transcripts (Riedhammer et al., 2010; Singla et al., 2017), where their problem settings and data preparation are different from ours. Specifically, compared with summarization for documents, our task of conversation summarization is more challenging because utterances in the conversation are less formally written and there are speaker role changes during the entire conversation; compared with summarization for meeting transcripts, where the summary is similar to a short meeting-log, our task requires to generate more informative summaries to fac"
2020.coling-main.63,D17-1159,0,0.0187951,"utterance-level encoders, which is illustrated in Figure 2. Also, it is worth noting that our method can generate the two types of summaries simultaneously, since they directly come from the predicted PD/DT/OT labels. In the following texts, we firstly introduce the memory module and then elaborate the whole hierarchical tagging process with the memories. 3.1 Utterance Memories As discussed above, we regard our summarization task as an utterance tagging process. Similar to other tagging tasks in which contextual information is highly helpful in determining the output tags (Song and Xia, 2012; Marcheggiani and Titov, 2017; Higashiyama et al., 2019; Tian et al., 2020a; Tian et al., 2020b), for each utterance ui in the conversation, relevant utterances in each conversation also provide useful information (Zhang et al., 2018) to determine whether a particular utterance is important. To exploit the information from relevant utterances, we adopt end-to-end memory networks (Sukhbaatar et al., 2015), which (as well as the variants) have been demonstrated to be useful in many tasks (Miller et al., 2016; Tian et al., 2020c), to learn from them to facilitate important utterance tagging. In doing so, we first map all utt"
2020.coling-main.63,W19-1306,0,0.0262848,"recently, where many of them have been done on utterance modeling in human-human conversations (Wang et al., 2018a; Liu et al., 2019). In these studies, one stream of utterance modeling focuses on dialogue act classifications, which aims to attribute one of predefined acts to each utterance in conversations (Lee and Dernoncourt, 2016; Liu et al., 2017; Kumar et al., 2018; Wang et al., 2018b; Raheja and Tetreault, 2019). Another stream focuses on assessment of utterances in terms of their quality in various aspects, such as sentiment analysis (Inaba and Takahashi, 2016; Lison and Bibauw, 2017; Misra et al., 2019). Our study on extractive summarizations for conversation can be regarded as in the line of the latter stream in evaluating utterances for human-human conversations, where little research has been done for utterances based on their importance to the pragmatic outcomes (i.e., summaries for problem statement and treatment recommendations in our study) of the conversations. 6 Conclusion and Future Work In this paper, we proposed a new task of medical conversation summarization, which is performed by identifying important utterances in the conversation between patients and doctors. Based on the re"
2020.coling-main.63,N18-1158,0,0.0157194,"m the patient and the doctor could be more important in generating problem statement (SUM1) and treatment recommendation (SUM2), respectively. Therefore, adding SR would help our model to focus more on the utterances for the patients and the doctors when it is predicting PD and DT labels for SUM1 and SUM2, respectively. 5 5.1 Related Work Extractive Summarization As a direct research line related to our work, extractive summarization aims to extract important sentences in the input and use them to form a summary. Most previous studies focused on document summarization (Nallapati et al., 2017; Narayan et al., 2018; Wang et al., 2019; Zhang et al., 2019; Xiao and Carenini, 2019; Luo et al., 2019) while some focused on summarization of meeting transcripts (Riedhammer et al., 2010; Singla et al., 2017), where their problem settings and data preparation are different from ours. Specifically, compared with summarization for documents, our task of conversation summarization is more challenging because utterances in the conversation are less formally written and there are speaker role changes during the entire conversation; compared with summarization for meeting transcripts, where the summary is similar to a"
2020.coling-main.63,D18-1258,0,0.0453017,"te that HET outperforms strong baselines and models from previous studies, and adding conversation-related features can further improve system performance.1 1 Introduction Applying natural language processing (NLP) techniques to the medical field is a prevailing trend nowadays and has great potential in many applications, such as key information extraction in medical literature (Kim ˇ et al., 2011; Dernoncourt et al., 2017; Seva et al., 2018), risk factor identification in electronic health records (Chang et al., 2015; Cormack et al., 2015; Cheng et al., 2016), and medical question answering (Pampari et al., 2018; Tian et al., 2019). As the demand for healthcare services increases greatly in the past decades,2 it is urgent to improve the quality and efficiency of healthcare, reduce workload and mental stress of health providers and increase patient satisfaction. Recently, Internet-based healthcare platforms such as online doctor systems and doctor-patient cyber communities have been increasingly used by patients and health professionals with the hope that they would alleviate the ever-increasing demands for healthcare services and reduce the inaccessibility of services caused by geographical and socio"
2020.coling-main.63,N19-1373,0,0.019483,"n conversation summarization in the medical domain, task-specific designs of summarization model are expected. 5.2 Utterance Modeling in Conversations Studies on dialogue systems have drawn much attention recently, where many of them have been done on utterance modeling in human-human conversations (Wang et al., 2018a; Liu et al., 2019). In these studies, one stream of utterance modeling focuses on dialogue act classifications, which aims to attribute one of predefined acts to each utterance in conversations (Lee and Dernoncourt, 2016; Liu et al., 2017; Kumar et al., 2018; Wang et al., 2018b; Raheja and Tetreault, 2019). Another stream focuses on assessment of utterances in terms of their quality in various aspects, such as sentiment analysis (Inaba and Takahashi, 2016; Lison and Bibauw, 2017; Misra et al., 2019). Our study on extractive summarizations for conversation can be regarded as in the line of the latter stream in evaluating utterances for human-human conversations, where little research has been done for utterances based on their importance to the pragmatic outcomes (i.e., summaries for problem statement and treatment recommendations in our study) of the conversations. 6 Conclusion and Future Work"
2020.coling-main.63,W18-2305,0,0.0210964,"that high-quality summaries can be generated by extracting two types of utterances, namely, problem statements and treatment recommendations. Experimental results demonstrate that HET outperforms strong baselines and models from previous studies, and adding conversation-related features can further improve system performance.1 1 Introduction Applying natural language processing (NLP) techniques to the medical field is a prevailing trend nowadays and has great potential in many applications, such as key information extraction in medical literature (Kim ˇ et al., 2011; Dernoncourt et al., 2017; Seva et al., 2018), risk factor identification in electronic health records (Chang et al., 2015; Cormack et al., 2015; Cheng et al., 2016), and medical question answering (Pampari et al., 2018; Tian et al., 2019). As the demand for healthcare services increases greatly in the past decades,2 it is urgent to improve the quality and efficiency of healthcare, reduce workload and mental stress of health providers and increase patient satisfaction. Recently, Internet-based healthcare platforms such as online doctor systems and doctor-patient cyber communities have been increasingly used by patients and health profess"
2020.coling-main.63,W17-4506,0,0.0220319,"ocus more on the utterances for the patients and the doctors when it is predicting PD and DT labels for SUM1 and SUM2, respectively. 5 5.1 Related Work Extractive Summarization As a direct research line related to our work, extractive summarization aims to extract important sentences in the input and use them to form a summary. Most previous studies focused on document summarization (Nallapati et al., 2017; Narayan et al., 2018; Wang et al., 2019; Zhang et al., 2019; Xiao and Carenini, 2019; Luo et al., 2019) while some focused on summarization of meeting transcripts (Riedhammer et al., 2010; Singla et al., 2017), where their problem settings and data preparation are different from ours. Specifically, compared with summarization for documents, our task of conversation summarization is more challenging because utterances in the conversation are less formally written and there are speaker role changes during the entire conversation; compared with summarization for meeting transcripts, where the summary is similar to a short meeting-log, our task requires to generate more informative summaries to facilitate the needs of providing useful information to potential patients from the online platform. General"
2020.coling-main.63,song-xia-2012-using,1,0.87114,"the token-level and utterance-level encoders, which is illustrated in Figure 2. Also, it is worth noting that our method can generate the two types of summaries simultaneously, since they directly come from the predicted PD/DT/OT labels. In the following texts, we firstly introduce the memory module and then elaborate the whole hierarchical tagging process with the memories. 3.1 Utterance Memories As discussed above, we regard our summarization task as an utterance tagging process. Similar to other tagging tasks in which contextual information is highly helpful in determining the output tags (Song and Xia, 2012; Marcheggiani and Titov, 2017; Higashiyama et al., 2019; Tian et al., 2020a; Tian et al., 2020b), for each utterance ui in the conversation, relevant utterances in each conversation also provide useful information (Zhang et al., 2018) to determine whether a particular utterance is important. To exploit the information from relevant utterances, we adopt end-to-end memory networks (Sukhbaatar et al., 2015), which (as well as the variants) have been demonstrated to be useful in many tasks (Miller et al., 2016; Tian et al., 2020c), to learn from them to facilitate important utterance tagging. In"
2020.coling-main.63,N18-2028,1,0.798582,"Missing"
2020.coling-main.63,D07-1048,0,0.119092,"Missing"
2020.coling-main.63,W19-5027,1,0.740721,"s strong baselines and models from previous studies, and adding conversation-related features can further improve system performance.1 1 Introduction Applying natural language processing (NLP) techniques to the medical field is a prevailing trend nowadays and has great potential in many applications, such as key information extraction in medical literature (Kim ˇ et al., 2011; Dernoncourt et al., 2017; Seva et al., 2018), risk factor identification in electronic health records (Chang et al., 2015; Cormack et al., 2015; Cheng et al., 2016), and medical question answering (Pampari et al., 2018; Tian et al., 2019). As the demand for healthcare services increases greatly in the past decades,2 it is urgent to improve the quality and efficiency of healthcare, reduce workload and mental stress of health providers and increase patient satisfaction. Recently, Internet-based healthcare platforms such as online doctor systems and doctor-patient cyber communities have been increasingly used by patients and health professionals with the hope that they would alleviate the ever-increasing demands for healthcare services and reduce the inaccessibility of services caused by geographical and socio-economic barriers."
2020.coling-main.63,2020.acl-main.735,1,0.710374,"2. Also, it is worth noting that our method can generate the two types of summaries simultaneously, since they directly come from the predicted PD/DT/OT labels. In the following texts, we firstly introduce the memory module and then elaborate the whole hierarchical tagging process with the memories. 3.1 Utterance Memories As discussed above, we regard our summarization task as an utterance tagging process. Similar to other tagging tasks in which contextual information is highly helpful in determining the output tags (Song and Xia, 2012; Marcheggiani and Titov, 2017; Higashiyama et al., 2019; Tian et al., 2020a; Tian et al., 2020b), for each utterance ui in the conversation, relevant utterances in each conversation also provide useful information (Zhang et al., 2018) to determine whether a particular utterance is important. To exploit the information from relevant utterances, we adopt end-to-end memory networks (Sukhbaatar et al., 2015), which (as well as the variants) have been demonstrated to be useful in many tasks (Miller et al., 2016; Tian et al., 2020c), to learn from them to facilitate important utterance tagging. In doing so, we first map all utterances [u1 , · · · , uj , · · · , un ] in th"
2020.coling-main.63,2020.emnlp-main.487,1,0.559632,"2. Also, it is worth noting that our method can generate the two types of summaries simultaneously, since they directly come from the predicted PD/DT/OT labels. In the following texts, we firstly introduce the memory module and then elaborate the whole hierarchical tagging process with the memories. 3.1 Utterance Memories As discussed above, we regard our summarization task as an utterance tagging process. Similar to other tagging tasks in which contextual information is highly helpful in determining the output tags (Song and Xia, 2012; Marcheggiani and Titov, 2017; Higashiyama et al., 2019; Tian et al., 2020a; Tian et al., 2020b), for each utterance ui in the conversation, relevant utterances in each conversation also provide useful information (Zhang et al., 2018) to determine whether a particular utterance is important. To exploit the information from relevant utterances, we adopt end-to-end memory networks (Sukhbaatar et al., 2015), which (as well as the variants) have been demonstrated to be useful in many tasks (Miller et al., 2016; Tian et al., 2020c), to learn from them to facilitate important utterance tagging. In doing so, we first map all utterances [u1 , · · · , uj , · · · , un ] in th"
2020.coling-main.63,2020.acl-main.734,1,0.72857,"2. Also, it is worth noting that our method can generate the two types of summaries simultaneously, since they directly come from the predicted PD/DT/OT labels. In the following texts, we firstly introduce the memory module and then elaborate the whole hierarchical tagging process with the memories. 3.1 Utterance Memories As discussed above, we regard our summarization task as an utterance tagging process. Similar to other tagging tasks in which contextual information is highly helpful in determining the output tags (Song and Xia, 2012; Marcheggiani and Titov, 2017; Higashiyama et al., 2019; Tian et al., 2020a; Tian et al., 2020b), for each utterance ui in the conversation, relevant utterances in each conversation also provide useful information (Zhang et al., 2018) to determine whether a particular utterance is important. To exploit the information from relevant utterances, we adopt end-to-end memory networks (Sukhbaatar et al., 2015), which (as well as the variants) have been demonstrated to be useful in many tasks (Miller et al., 2016; Tian et al., 2020c), to learn from them to facilitate important utterance tagging. In doing so, we first map all utterances [u1 , · · · , uj , · · · , un ] in th"
2020.coling-main.63,L18-1464,1,0.833299,"ly. in our work, this challenge may not be an issue because the redundancy in the original input is limited and directly concatenating selected utterances with their same order in the original conversation does not lead to unreadable summmaries in most cases. Therefore, to have a good performance in conversation summarization in the medical domain, task-specific designs of summarization model are expected. 5.2 Utterance Modeling in Conversations Studies on dialogue systems have drawn much attention recently, where many of them have been done on utterance modeling in human-human conversations (Wang et al., 2018a; Liu et al., 2019). In these studies, one stream of utterance modeling focuses on dialogue act classifications, which aims to attribute one of predefined acts to each utterance in conversations (Lee and Dernoncourt, 2016; Liu et al., 2017; Kumar et al., 2018; Wang et al., 2018b; Raheja and Tetreault, 2019). Another stream focuses on assessment of utterances in terms of their quality in various aspects, such as sentiment analysis (Inaba and Takahashi, 2016; Lison and Bibauw, 2017; Misra et al., 2019). Our study on extractive summarizations for conversation can be regarded as in the line of th"
2020.coling-main.63,P19-1214,0,0.0221498,"Missing"
2020.coling-main.63,2020.nlpmc-1.3,1,0.62841,"tand our findings from the results. 2 A Corpus of Medical Conversations Medical conversation is a type of task-oriented conversation. Different from ordinary conversations in which topics are often fluid, in task-specific conversations, participants interact to accomplish a projected set of goals and sub-goals (Litman and Allen, 1987; Drew and Heritage, 1992). Specifically, for conversations in the medical domain from online medical platforms, the projected goal is for the doctor to diagnose and offer treatment recommendation for the patient’s problem (Drew and Heritage, 1992; Robinson, 2012; Wang et al., 2020). Particularly in China, many platforms make such medical conversations publicly available so that new patients with similar problem can search relevant conversations and find helpful information from them. Therefore, summarization of the patient’s problem and doctor’s recommendations in a conversation could be highly important because such summaries can help the new patients locate the key information, especially when a conversation is too long. To conduct such summarization, a straightforward solution is to identity the important utterances that contain key information for problem statements"
2020.coling-main.63,D19-1298,0,0.0185914,"ating problem statement (SUM1) and treatment recommendation (SUM2), respectively. Therefore, adding SR would help our model to focus more on the utterances for the patients and the doctors when it is predicting PD and DT labels for SUM1 and SUM2, respectively. 5 5.1 Related Work Extractive Summarization As a direct research line related to our work, extractive summarization aims to extract important sentences in the input and use them to form a summary. Most previous studies focused on document summarization (Nallapati et al., 2017; Narayan et al., 2018; Wang et al., 2019; Zhang et al., 2019; Xiao and Carenini, 2019; Luo et al., 2019) while some focused on summarization of meeting transcripts (Riedhammer et al., 2010; Singla et al., 2017), where their problem settings and data preparation are different from ours. Specifically, compared with summarization for documents, our task of conversation summarization is more challenging because utterances in the conversation are less formally written and there are speaker role changes during the entire conversation; compared with summarization for meeting transcripts, where the summary is similar to a short meeting-log, our task requires to generate more informati"
2020.coling-main.63,N18-1151,1,0.832548,"ls. In the following texts, we firstly introduce the memory module and then elaborate the whole hierarchical tagging process with the memories. 3.1 Utterance Memories As discussed above, we regard our summarization task as an utterance tagging process. Similar to other tagging tasks in which contextual information is highly helpful in determining the output tags (Song and Xia, 2012; Marcheggiani and Titov, 2017; Higashiyama et al., 2019; Tian et al., 2020a; Tian et al., 2020b), for each utterance ui in the conversation, relevant utterances in each conversation also provide useful information (Zhang et al., 2018) to determine whether a particular utterance is important. To exploit the information from relevant utterances, we adopt end-to-end memory networks (Sukhbaatar et al., 2015), which (as well as the variants) have been demonstrated to be useful in many tasks (Miller et al., 2016; Tian et al., 2020c), to learn from them to facilitate important utterance tagging. In doing so, we first map all utterances [u1 , · · · , uj , · · · , un ] in the conversation into their memory vectors and value vectors. The memory vectors (denoted by mj for uj ) are directly copied from the utterance representation obt"
2020.coling-main.63,P19-1499,0,0.0187593,"e important in generating problem statement (SUM1) and treatment recommendation (SUM2), respectively. Therefore, adding SR would help our model to focus more on the utterances for the patients and the doctors when it is predicting PD and DT labels for SUM1 and SUM2, respectively. 5 5.1 Related Work Extractive Summarization As a direct research line related to our work, extractive summarization aims to extract important sentences in the input and use them to form a summary. Most previous studies focused on document summarization (Nallapati et al., 2017; Narayan et al., 2018; Wang et al., 2019; Zhang et al., 2019; Xiao and Carenini, 2019; Luo et al., 2019) while some focused on summarization of meeting transcripts (Riedhammer et al., 2010; Singla et al., 2017), where their problem settings and data preparation are different from ours. Specifically, compared with summarization for documents, our task of conversation summarization is more challenging because utterances in the conversation are less formally written and there are speaker role changes during the entire conversation; compared with summarization for meeting transcripts, where the summary is similar to a short meeting-log, our task requires t"
2020.coling-main.63,W17-5505,0,0.0263391,"the summary of patient’s problem (SUM1) and doctor’s diagnoses (SUM2), respectively. 4 Experiments 4.1 Settings We experiment our HET model with and without the memory on our corpus. For model implementation, at the token-level encoder (TE), we use the Chinese version of BERT10 and ZEN (Diao et al., 2019)11 with their default settings, where for both BERT and ZEN, we use 12 layers of multi-head attentions with the dimension of hidden vectors set to 768; for the utterance level, we firstly run experiments with no encoder; then following previous studies such as (Kalchbrenner and Blunsom, 2013; Zhao et al., 2017; Kumar et al., 2018), we experiment with two recurrent neural network models (namely, LSTM and BiLSTM) to encode the utterance sequence for each conversation, where the dimension of hidden states is set to 300 for LSTM and 150 for BiLSTM encoder. In the memory module, the embedding matrix and BiLSTM encoder for obtaining the value vectors vj for uj are applied directly to the Chinese characters in the utterance. All parameters in the embedding matrix and the BiLSTM encoder in the memory module are initialized randomly, with the dimension of embedding and hidden states set to 768 and 384, resp"
2020.coling-main.97,W18-6402,0,0.0137156,"decoders. (Blackwood et al., 2018) devised a task-specific attention mechanism to improve the translation quality. In addition, (Sen et al., 2019) considered unsupervised multilingual NMT by utilizing a shared encoder and some language-specific decoders. As regard to many-to-one translation, RNN-based (Zoph and Knight, 2016) and Transformer-based multi-source translation (Junczys-Dowmunt and Grundkiewicz, 2018) are two multi-encoder methods. Multi-modal Machine Translation Multi-modal have been extensively studied due to multi-modal MT shared tasks (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Prior to the shared tasks, (Hitschler et al., 2016) proposed a phrase-based statistical MT model (PBSMT) to generate translation candidates and re-rank them with image features. Afterwards, RNN-based architectures have been adopted in multi-modal machine translation (Elliott et al., 2015). Based on such framework, (Libovick`y and Helcl, 2017) devised attention mechanism to improve the translation while (Caglayan et al., 2016) leveraged spatial visual features to a separate visual attention mechanism. Apart from that, (Calixto et al., 2019) incorporated image features through latent variables"
2020.coling-main.97,C18-1263,0,0.0177191,"Wang et al., 2018) have proposed three strategies to improve the performance. (Luong et al., 2015a) combined multiple encoders and decoders, one encoder for each source language and one decoder for each target language respectively, for many-to-many translation. Based on that, (Firat et al., 2016) devised a sharing attention mechanism while (Lu et al., 2018) incorporated an explicit neural interlingua into such multilingual encoder-decoder. 1123 Afterwards, (Ha et al., 2016) and (Johnson et al., 2017) proposed one universal encoder and decoder to take place of multiple encoders and decoders. (Blackwood et al., 2018) devised a task-specific attention mechanism to improve the translation quality. In addition, (Sen et al., 2019) considered unsupervised multilingual NMT by utilizing a shared encoder and some language-specific decoders. As regard to many-to-one translation, RNN-based (Zoph and Knight, 2016) and Transformer-based multi-source translation (Junczys-Dowmunt and Grundkiewicz, 2018) are two multi-encoder methods. Multi-modal Machine Translation Multi-modal have been extensively studied due to multi-modal MT shared tasks (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Prior to th"
2020.coling-main.97,P19-1642,0,0.0133444,"d tasks (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Prior to the shared tasks, (Hitschler et al., 2016) proposed a phrase-based statistical MT model (PBSMT) to generate translation candidates and re-rank them with image features. Afterwards, RNN-based architectures have been adopted in multi-modal machine translation (Elliott et al., 2015). Based on such framework, (Libovick`y and Helcl, 2017) devised attention mechanism to improve the translation while (Caglayan et al., 2016) leveraged spatial visual features to a separate visual attention mechanism. Apart from that, (Calixto et al., 2019) incorporated image features through latent variables. (Libovick`y et al., 2018) adopted multisource Transformer with different input combination strategies. To the best of our knowledge, previous methods did not consider the conflicts among various input sources. We are the first to consider this problem in multi-source translation and learn the invariance to improve the translation quality. 3 Background In this section, we introduce two general multi-encoder based multi-source NMT frameworks, namely RNN-based and Transformer-based approaches. They basically have the same architecture with mu"
2020.coling-main.97,D18-1327,0,0.0162771,"also captures implicit invariance between different sources. 1 Introduction Neural machine translation (NMT) systems in general translate one source language to a target language. Various one-to-one attentional encoder-decoder architectures (Bahdanau et al., 2014; Luong et al., 2015b; Vaswani et al., 2017) were designed to learn word and structure mappings including formations, grammatical correspondences between source and target languages. Recently, multi-source NMT (Zoph and Knight, 2016), which simultaneously takes multiple different languages (Dabre et al., 2017; Libovick`y et al., 2018; Currey and Heafield, 2018) as input when translating to another one, is emerging. Its intuitive idea is ambiguity between one source language and the target language could be reduced by another source language via the “triangulation” proposed by (Kay, 2000). For example, it is hard to tell who was with a telescope by the ambiguous English sentence in Figure 1 (a). But it could be more easily to be translated to Spanish if provided with the corresponding Chinese phrase “我通过望远镜”, which corresponds to “I see through a telescope” in English. Besides, the complementary source could be extended to non-language input such as"
2020.coling-main.97,W11-2107,0,0.0173767,"all the sentences are firstly tokenized by Moses tokenizers† , and then segmented into subword units with Byte Pair Encoding (BPE) (Sennrich et al., 2016) for later processing. 5.2 Baselines and Evaluation Protocol We compare our proposed SIN-RNN and SIN-Transformer with two corresponding baselines: RNNbased multi-source NMT (Zoph and Knight, 2016) (denoted as RNN in the following part), and Transformer-based multi-source NMT (Junczys-Dowmunt and Grundkiewicz, 2018) (denoted as Transformer in the following part). We measure translation quality using BLEU (Papineni et al., 2002) ‡ and METEOR (Denkowski and Lavie, 2011) § . The score is computed on tokenized text after merging the BPE-based sub-word symbols. We use single reference in our evaluation and they are case sensitive. 5.3 Implementation Details We use two-layer bidirectional-LSTM as encoder and four-layer LSTM as decoder in our RNN models with hidden size of 512. The Transformer-based methods have 6 layers in both encoder and decoder, while 16 heads in multi-head attention. In loss function (Eq. 4), α and β are set to 1.0 and 0.1 by grid search, individually. We adopt the optimizer Adam (Kingma and Ba, 2014) with β1 = 0.9, β2 = 0.98 and a weight de"
2020.coling-main.97,P15-1166,0,0.0451372,"prevail multi-encoder based multi-source NMT frameworks. • We verify the performance of our model on both multi-lingual and multi-modal NMT tasks. Extensive experimental results show that our SIN can provide large-margin improvements on both tasks and the invariant information between different sources are encouragingly learnt by our model. 2 Related work Multi-lingual Machine Translation Multilingual machine translation addresses the machine translation between multiple source and target languages, which contains one-to-many (one-source-to-manytarget), many-to-many, many-to-one approaches. (Dong et al., 2015) combines a single encoder with multiple attentional decoders for one-to-many translation, based on which (Wang et al., 2018) have proposed three strategies to improve the performance. (Luong et al., 2015a) combined multiple encoders and decoders, one encoder for each source language and one decoder for each target language respectively, for many-to-many translation. Based on that, (Firat et al., 2016) devised a sharing attention mechanism while (Lu et al., 2018) incorporated an explicit neural interlingua into such multilingual encoder-decoder. 1123 Afterwards, (Ha et al., 2016) and (Johnson"
2020.coling-main.97,W16-3210,0,0.0451663,"Missing"
2020.coling-main.97,W17-4718,0,0.0789997,"ive idea is ambiguity between one source language and the target language could be reduced by another source language via the “triangulation” proposed by (Kay, 2000). For example, it is hard to tell who was with a telescope by the ambiguous English sentence in Figure 1 (a). But it could be more easily to be translated to Spanish if provided with the corresponding Chinese phrase “我通过望远镜”, which corresponds to “I see through a telescope” in English. Besides, the complementary source could be extended to non-language input such as images, also known as multimodal NMT (Libovick`y and Helcl, 2017; Elliott et al., 2017). It takes an image with description in source languages, that is then translated into a target language. The images are expected to provide additional signals for better translations, which abides by similar assumptions in multi-source NMT with different languages. A specific instance is illustrated in Figure 1 (b). In this paper, we consider general multisource NMT which contains more than one source as input. Our proposed model could be naturally adapted to these two specific sub-tasks, namely multi-lingual translation and multi-modal translation. For simplicity and convenience to describe,"
2020.coling-main.97,N16-1101,0,0.062025,"l machine translation addresses the machine translation between multiple source and target languages, which contains one-to-many (one-source-to-manytarget), many-to-many, many-to-one approaches. (Dong et al., 2015) combines a single encoder with multiple attentional decoders for one-to-many translation, based on which (Wang et al., 2018) have proposed three strategies to improve the performance. (Luong et al., 2015a) combined multiple encoders and decoders, one encoder for each source language and one decoder for each target language respectively, for many-to-many translation. Based on that, (Firat et al., 2016) devised a sharing attention mechanism while (Lu et al., 2018) incorporated an explicit neural interlingua into such multilingual encoder-decoder. 1123 Afterwards, (Ha et al., 2016) and (Johnson et al., 2017) proposed one universal encoder and decoder to take place of multiple encoders and decoders. (Blackwood et al., 2018) devised a task-specific attention mechanism to improve the translation quality. In addition, (Sen et al., 2019) considered unsupervised multilingual NMT by utilizing a shared encoder and some language-specific decoders. As regard to many-to-one translation, RNN-based (Zoph"
2020.coling-main.97,P16-1227,0,0.0245545,"pecific attention mechanism to improve the translation quality. In addition, (Sen et al., 2019) considered unsupervised multilingual NMT by utilizing a shared encoder and some language-specific decoders. As regard to many-to-one translation, RNN-based (Zoph and Knight, 2016) and Transformer-based multi-source translation (Junczys-Dowmunt and Grundkiewicz, 2018) are two multi-encoder methods. Multi-modal Machine Translation Multi-modal have been extensively studied due to multi-modal MT shared tasks (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Prior to the shared tasks, (Hitschler et al., 2016) proposed a phrase-based statistical MT model (PBSMT) to generate translation candidates and re-rank them with image features. Afterwards, RNN-based architectures have been adopted in multi-modal machine translation (Elliott et al., 2015). Based on such framework, (Libovick`y and Helcl, 2017) devised attention mechanism to improve the translation while (Caglayan et al., 2016) leveraged spatial visual features to a separate visual attention mechanism. Apart from that, (Calixto et al., 2019) incorporated image features through latent variables. (Libovick`y et al., 2018) adopted multisource Trans"
2020.coling-main.97,W18-6467,0,0.271477,"orporated an explicit neural interlingua into such multilingual encoder-decoder. 1123 Afterwards, (Ha et al., 2016) and (Johnson et al., 2017) proposed one universal encoder and decoder to take place of multiple encoders and decoders. (Blackwood et al., 2018) devised a task-specific attention mechanism to improve the translation quality. In addition, (Sen et al., 2019) considered unsupervised multilingual NMT by utilizing a shared encoder and some language-specific decoders. As regard to many-to-one translation, RNN-based (Zoph and Knight, 2016) and Transformer-based multi-source translation (Junczys-Dowmunt and Grundkiewicz, 2018) are two multi-encoder methods. Multi-modal Machine Translation Multi-modal have been extensively studied due to multi-modal MT shared tasks (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Prior to the shared tasks, (Hitschler et al., 2016) proposed a phrase-based statistical MT model (PBSMT) to generate translation candidates and re-rank them with image features. Afterwards, RNN-based architectures have been adopted in multi-modal machine translation (Elliott et al., 2015). Based on such framework, (Libovick`y and Helcl, 2017) devised attention mechanism to improve the tra"
2020.coling-main.97,P17-2031,0,0.0306964,"Missing"
2020.coling-main.97,W18-6326,0,0.0328118,"Missing"
2020.coling-main.97,W18-6309,0,0.0154875,"ultiple source and target languages, which contains one-to-many (one-source-to-manytarget), many-to-many, many-to-one approaches. (Dong et al., 2015) combines a single encoder with multiple attentional decoders for one-to-many translation, based on which (Wang et al., 2018) have proposed three strategies to improve the performance. (Luong et al., 2015a) combined multiple encoders and decoders, one encoder for each source language and one decoder for each target language respectively, for many-to-many translation. Based on that, (Firat et al., 2016) devised a sharing attention mechanism while (Lu et al., 2018) incorporated an explicit neural interlingua into such multilingual encoder-decoder. 1123 Afterwards, (Ha et al., 2016) and (Johnson et al., 2017) proposed one universal encoder and decoder to take place of multiple encoders and decoders. (Blackwood et al., 2018) devised a task-specific attention mechanism to improve the translation quality. In addition, (Sen et al., 2019) considered unsupervised multilingual NMT by utilizing a shared encoder and some language-specific decoders. As regard to many-to-one translation, RNN-based (Zoph and Knight, 2016) and Transformer-based multi-source translati"
2020.coling-main.97,D15-1166,0,0.292792,"llel sources. Such network can be easily integrated with multi-encoder based multi-source NMT methods (e.g. multi-encoder RNN and transformer) to enhance the translation results. Extensive experiments on two multi-source translation tasks demonstrate that the proposed approach not only achieves clear gains in translation quality but also captures implicit invariance between different sources. 1 Introduction Neural machine translation (NMT) systems in general translate one source language to a target language. Various one-to-one attentional encoder-decoder architectures (Bahdanau et al., 2014; Luong et al., 2015b; Vaswani et al., 2017) were designed to learn word and structure mappings including formations, grammatical correspondences between source and target languages. Recently, multi-source NMT (Zoph and Knight, 2016), which simultaneously takes multiple different languages (Dabre et al., 2017; Libovick`y et al., 2018; Currey and Heafield, 2018) as input when translating to another one, is emerging. Its intuitive idea is ambiguity between one source language and the target language could be reduced by another source language via the “triangulation” proposed by (Kay, 2000). For example, it is hard"
2020.coling-main.97,P02-1040,0,0.107675,"riplets, respectively. In both tasks, all the sentences are firstly tokenized by Moses tokenizers† , and then segmented into subword units with Byte Pair Encoding (BPE) (Sennrich et al., 2016) for later processing. 5.2 Baselines and Evaluation Protocol We compare our proposed SIN-RNN and SIN-Transformer with two corresponding baselines: RNNbased multi-source NMT (Zoph and Knight, 2016) (denoted as RNN in the following part), and Transformer-based multi-source NMT (Junczys-Dowmunt and Grundkiewicz, 2018) (denoted as Transformer in the following part). We measure translation quality using BLEU (Papineni et al., 2002) ‡ and METEOR (Denkowski and Lavie, 2011) § . The score is computed on tokenized text after merging the BPE-based sub-word symbols. We use single reference in our evaluation and they are case sensitive. 5.3 Implementation Details We use two-layer bidirectional-LSTM as encoder and four-layer LSTM as decoder in our RNN models with hidden size of 512. The Transformer-based methods have 6 layers in both encoder and decoder, while 16 heads in multi-head attention. In loss function (Eq. 4), α and β are set to 1.0 and 0.1 by grid search, individually. We adopt the optimizer Adam (Kingma and Ba, 2014)"
2020.coling-main.97,P19-1297,0,0.0220888,"encoders and decoders, one encoder for each source language and one decoder for each target language respectively, for many-to-many translation. Based on that, (Firat et al., 2016) devised a sharing attention mechanism while (Lu et al., 2018) incorporated an explicit neural interlingua into such multilingual encoder-decoder. 1123 Afterwards, (Ha et al., 2016) and (Johnson et al., 2017) proposed one universal encoder and decoder to take place of multiple encoders and decoders. (Blackwood et al., 2018) devised a task-specific attention mechanism to improve the translation quality. In addition, (Sen et al., 2019) considered unsupervised multilingual NMT by utilizing a shared encoder and some language-specific decoders. As regard to many-to-one translation, RNN-based (Zoph and Knight, 2016) and Transformer-based multi-source translation (Junczys-Dowmunt and Grundkiewicz, 2018) are two multi-encoder methods. Multi-modal Machine Translation Multi-modal have been extensively studied due to multi-modal MT shared tasks (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Prior to the shared tasks, (Hitschler et al., 2016) proposed a phrase-based statistical MT model (PBSMT) to generate transl"
2020.coling-main.97,P16-1162,0,0.0432406,"pus. Based on this, we obtain eligible trilingual sentence triples from each language, and result in 188K triples in the training set. For multi-modal translation task, we evaluate our models in Multi30K dataset (Elliott et al., 2016). The dataset contains triplets of images, English captions and corresponding sentences in German, French and Czech (Cz). The training, validation and test set contain 29, 000, 1, 014 and 1, 000 triplets, respectively. In both tasks, all the sentences are firstly tokenized by Moses tokenizers† , and then segmented into subword units with Byte Pair Encoding (BPE) (Sennrich et al., 2016) for later processing. 5.2 Baselines and Evaluation Protocol We compare our proposed SIN-RNN and SIN-Transformer with two corresponding baselines: RNNbased multi-source NMT (Zoph and Knight, 2016) (denoted as RNN in the following part), and Transformer-based multi-source NMT (Junczys-Dowmunt and Grundkiewicz, 2018) (denoted as Transformer in the following part). We measure translation quality using BLEU (Papineni et al., 2002) ‡ and METEOR (Denkowski and Lavie, 2011) § . The score is computed on tokenized text after merging the BPE-based sub-word symbols. We use single reference in our evaluat"
2020.coling-main.97,W16-2346,0,0.0503543,"Missing"
2020.coling-main.97,D18-1326,0,0.0140284,"multi-modal NMT tasks. Extensive experimental results show that our SIN can provide large-margin improvements on both tasks and the invariant information between different sources are encouragingly learnt by our model. 2 Related work Multi-lingual Machine Translation Multilingual machine translation addresses the machine translation between multiple source and target languages, which contains one-to-many (one-source-to-manytarget), many-to-many, many-to-one approaches. (Dong et al., 2015) combines a single encoder with multiple attentional decoders for one-to-many translation, based on which (Wang et al., 2018) have proposed three strategies to improve the performance. (Luong et al., 2015a) combined multiple encoders and decoders, one encoder for each source language and one decoder for each target language respectively, for many-to-many translation. Based on that, (Firat et al., 2016) devised a sharing attention mechanism while (Lu et al., 2018) incorporated an explicit neural interlingua into such multilingual encoder-decoder. 1123 Afterwards, (Ha et al., 2016) and (Johnson et al., 2017) proposed one universal encoder and decoder to take place of multiple encoders and decoders. (Blackwood et al.,"
2020.coling-main.97,N16-1004,0,0.306928,"ulti-source translation tasks demonstrate that the proposed approach not only achieves clear gains in translation quality but also captures implicit invariance between different sources. 1 Introduction Neural machine translation (NMT) systems in general translate one source language to a target language. Various one-to-one attentional encoder-decoder architectures (Bahdanau et al., 2014; Luong et al., 2015b; Vaswani et al., 2017) were designed to learn word and structure mappings including formations, grammatical correspondences between source and target languages. Recently, multi-source NMT (Zoph and Knight, 2016), which simultaneously takes multiple different languages (Dabre et al., 2017; Libovick`y et al., 2018; Currey and Heafield, 2018) as input when translating to another one, is emerging. Its intuitive idea is ambiguity between one source language and the target language could be reduced by another source language via the “triangulation” proposed by (Kay, 2000). For example, it is hard to tell who was with a telescope by the ambiguous English sentence in Figure 1 (a). But it could be more easily to be translated to Spanish if provided with the corresponding Chinese phrase “我通过望远镜”, which corresp"
2020.emnlp-main.107,C18-1139,0,0.0304634,"0, 2020. 2020 Association for Computational Linguistics space for each token from pre-trained word embedding models, such as GloVe (Pennington et al., 2014) and Tencent Embedding (Song et al., 2018b), and encode semantic information through an attentive semantic augmentation module. Then we apply a gate module to weigh the contribution of the augmentation module and context encoding module in the NER process. To further improve NER performance, we also attempt multiple types of pre-trained word embeddings for feature extraction, which has been demonstrated to be effective in previous studies (Akbik et al., 2018; Jie and Lu, 2019; Kasai et al., 2019; Kim et al., 2019; Yan et al., 2019). To evaluate our approach, we conduct experiments on three benchmark datasets, where the results show that our model outperforms the stateof-the-arts with clear advantage across all datasets. 2 The Proposed Model The task of social media NER is conventionally regarded as sequence labeling task, where an input sequence X = x1 , x2 , · · · , xn with n tokens is annotated with its corresponding NE labels Yb = yb1 , yb2 , · · · , ybn in the same length. Following this paradigm, we propose a neural model with semantic augme"
2020.emnlp-main.107,N19-1075,0,0.352197,"the semantic information, we propose a gate module to aggregate such information to the backbone NER model. Particularly, we use a reset gate to control the information flow by g = σ(W1 · hi + W2 · vi + bg ), (4) Chinese (5) to balance the information from context encoder and the augmentation module, where ui is the derived output of the gate module; ◦ represents the element-wise multiplication operation and 1 is a 1-vector with its all elements equal to 1. 2.3 Tagging Procedure To provide hi to the augmentation module, we adopt a context encoding module (denoted as CE) proposed by Yan et al. (2019). Compared with vanilla Transformers, this encoder additionally models the direction and distance information of the input, which has been demonstrated to be useful for the NER task. Therefore, the encoding procedure of the input text can be denoted as H = CE(E), (6) where H = [h1 , h2 , · · · , hn ] and E = [e1 , e2 , . . . , en ] are lists of hidden vectors and embeddings of X , respectively. In addition, since pretrained word embeddings contain substantial con270 389 51.4 270 414 45.2 Table 1: The statistics of all benchmark datasets w.r.t. the number of sentences (# Sent.), named entities"
2020.emnlp-main.107,P19-2026,0,0.0279615,"Missing"
2020.emnlp-main.107,D11-1141,0,0.0553313,"LP) tasks (Pang et al., 2019; Martins et al., 2019). However, NER in social media remains a challenging task because (i) it suffers from the data spar* S-PER O O Figure 1: An example shows that an NE tagged with “PER” (Person) is suggested by its similar words. Introduction Equal contribution. Corresponding author. 1 The code and the best performing models are available at https://github.com/cuhksz-nlp/SANER. † O sity problem since entities usually represent a small part of proper names, which makes the task hard to be generalized; (ii) social media texts do not follow strict syntactic rules (Ritter et al., 2011). To tackle these challenges, previous studies tired to leverage domain information (e.g., existing gazetteer and embeddings trained on large social media text) and external features (e.g., part-of-speech tags) to help with social media NER (Peng and Dredze, 2015; Aguilar et al., 2017). However, these approaches rely on extra efforts to obtain such extra information and suffer from noise in the resulted information. For example, training embeddings for social media domain could bring a lot unusual expressions to the vocabulary. Inspired by studies using semantic augmentation (especially from l"
2020.emnlp-main.107,N19-1351,0,0.0389037,"Missing"
2020.emnlp-main.107,K17-1016,1,0.825052,"ht the semantic information carried by the extracted words. Afterwards, the weighted semantic information is leveraged to enhance the backbone model through a gate module. In the following text, we firstly introduce the encoding procedure for augmenting semantic information. Then, we present the gate module to incorporate augmented information into the backbone model. Finally, we elaborate the tagging procedure for NER with the aforementioned enhancement. 2.1 Attentive Semantic Augmentation The high quality of text representation is the key to obtain good model performance for many NLP tasks (Song et al., 2017; Sileo et al., 2019). However, obtaining such text representation is not easy in the social media domain because of data sparsity problem. Motivated by this fact, we propose seFigure 2: The overall architecture of our proposed model with semantic augmentation. An example sentence and its output NE labels are given, where the augmented semantic information for the word “Chris” are also illustrated with the processing through the augmentation module and the gate module. mantic augmentation mechanism for social media NER by enhancing the representation of each token in the input sentence with th"
2020.emnlp-main.107,N18-2028,1,0.937195,".g., existing gazetteer and embeddings trained on large social media text) and external features (e.g., part-of-speech tags) to help with social media NER (Peng and Dredze, 2015; Aguilar et al., 2017). However, these approaches rely on extra efforts to obtain such extra information and suffer from noise in the resulted information. For example, training embeddings for social media domain could bring a lot unusual expressions to the vocabulary. Inspired by studies using semantic augmentation (especially from lexical semantics) to improve model performance on many NLP tasks (Song and Xia, 2013; Song et al., 2018a; Kumar et al., 2019; Amjad et al., 2020), it is also a potential promising solution to solving social media NER. Figure 1 shows a typical case. “Chris”, supposedly tagged with “Person” in this example sentence, is tagged as other labels in most cases. Therefore, in the predicting process, it is difficult to label “Chris” correctly. A sound solution is to augment the semantic space of “Chris” through its similar words, such as “Jason” and “Mike”, which can be obtained by existing pre-trained word embeddings from the general domain. In this paper, we propose an effective approach to NER for so"
2020.emnlp-main.107,D15-1064,0,0.063766,"Introduction Equal contribution. Corresponding author. 1 The code and the best performing models are available at https://github.com/cuhksz-nlp/SANER. † O sity problem since entities usually represent a small part of proper names, which makes the task hard to be generalized; (ii) social media texts do not follow strict syntactic rules (Ritter et al., 2011). To tackle these challenges, previous studies tired to leverage domain information (e.g., existing gazetteer and embeddings trained on large social media text) and external features (e.g., part-of-speech tags) to help with social media NER (Peng and Dredze, 2015; Aguilar et al., 2017). However, these approaches rely on extra efforts to obtain such extra information and suffer from noise in the resulted information. For example, training embeddings for social media domain could bring a lot unusual expressions to the vocabulary. Inspired by studies using semantic augmentation (especially from lexical semantics) to improve model performance on many NLP tasks (Song and Xia, 2013; Song et al., 2018a; Kumar et al., 2019; Amjad et al., 2020), it is also a potential promising solution to solving social media NER. Figure 1 shows a typical case. “Chris”, suppo"
2020.emnlp-main.107,I13-1071,1,0.859918,"omain information (e.g., existing gazetteer and embeddings trained on large social media text) and external features (e.g., part-of-speech tags) to help with social media NER (Peng and Dredze, 2015; Aguilar et al., 2017). However, these approaches rely on extra efforts to obtain such extra information and suffer from noise in the resulted information. For example, training embeddings for social media domain could bring a lot unusual expressions to the vocabulary. Inspired by studies using semantic augmentation (especially from lexical semantics) to improve model performance on many NLP tasks (Song and Xia, 2013; Song et al., 2018a; Kumar et al., 2019; Amjad et al., 2020), it is also a potential promising solution to solving social media NER. Figure 1 shows a typical case. “Chris”, supposedly tagged with “Person” in this example sentence, is tagged as other labels in most cases. Therefore, in the predicting process, it is difficult to label “Chris” correctly. A sound solution is to augment the semantic space of “Chris” through its similar words, such as “Jason” and “Mike”, which can be obtained by existing pre-trained word embeddings from the general domain. In this paper, we propose an effective app"
2020.emnlp-main.107,D14-1162,0,0.082168,"Missing"
2020.emnlp-main.107,W16-3919,0,0.142404,"Missing"
2020.emnlp-main.107,N18-1202,0,0.0433159,".94 48.41 48.36 48.98 49.26 50.02 65.36 65.01 66.24 68.21 69.32 1 2 3 4 5 N DS DS AU AU N N Y N Y 52.98 53.11 54.02 54.29 55.01 48.82 48.71 49.56 49.81 50.36 66.02 65.78 67.52 68.46 69.80 (a) Development Set (b) Test Set Table 2: F 1 scores of the baseline model and ours enhanced with semantic augmentation (“SE”) and the gate module (“GA”) on the development (a) and test (b) sets. “DS” and “AU ” represent the direct summation and attentive augmentation module, respectively. Y and N denote the use and non-use of corresponding modules. for each language.2 Specifically, for English, we use ELMo (Peters et al., 2018) and BERT-cased large (Devlin et al., 2019); for Chinese, we use Tencent Embedding (Song et al., 2018b), and ZEN (Diao et al., 2019).3 In the context encoding module, we use a two-layer transformer-based encoder proposed by Yan et al. (2019) with 128 hidden units and 12 heads. To extract similar words carrying augmented semantic information, we use the pretrained word embeddings from GloVe for English and those embedding from Tencent Embeddings for Chinese to extract the most similar 10 words (i.e., m = 10) 4 . In the augmentation module, we randomly initialize the embeddings of the extracted"
2020.emnlp-main.107,D19-1396,0,0.0205416,"Missing"
2020.emnlp-main.107,2020.acl-main.735,1,0.761086,"osine similarities and denote them as Ci = {ci,1 , ci,2 , · · · , ci,j , · · · , ci,m } (1) Afterwards, we use another embedding matrix to map all extracted words ci,j to their corresponding embeddings ei,j . Since not all ci,j ∈ Ci are helpful for predicting the NE label of xi in the given context, it is important to distinguish the contributions of different words to the NER task in that context. Consider that the attention and weight based approaches are demonstrated to be effective choices to selectively leverage extra information in many tasks (Kumar et al., 2018; Margatina et al., 2019; Tian et al., 2020a,d,b,c), we propose an attentive semantic augmentation module (denoted as AU ) to weight the words according to their contributions to the task in different contexts. Specifically, for each token xi , the augmentation module assigns a weight to each word ci,j ∈ Ci by exp(hi · ei,j ) pi,j = Pm , j=i exp(hi · ei,j ) (2) where hi is the hidden vector for xi obtained from 1384 the context encoder with its dimension matching that of the embedding (i.e., ei,j ) of ci,j . Then, we apply the weight pi,j to the word ci,j to compute the final augmented semantic representation by m X vi = pi,j ei,j , (3"
2020.emnlp-main.107,2020.findings-emnlp.153,1,0.717809,"osine similarities and denote them as Ci = {ci,1 , ci,2 , · · · , ci,j , · · · , ci,m } (1) Afterwards, we use another embedding matrix to map all extracted words ci,j to their corresponding embeddings ei,j . Since not all ci,j ∈ Ci are helpful for predicting the NE label of xi in the given context, it is important to distinguish the contributions of different words to the NER task in that context. Consider that the attention and weight based approaches are demonstrated to be effective choices to selectively leverage extra information in many tasks (Kumar et al., 2018; Margatina et al., 2019; Tian et al., 2020a,d,b,c), we propose an attentive semantic augmentation module (denoted as AU ) to weight the words according to their contributions to the task in different contexts. Specifically, for each token xi , the augmentation module assigns a weight to each word ci,j ∈ Ci by exp(hi · ei,j ) pi,j = Pm , j=i exp(hi · ei,j ) (2) where hi is the hidden vector for xi obtained from 1384 the context encoder with its dimension matching that of the embedding (i.e., ei,j ) of ci,j . Then, we apply the weight pi,j to the word ci,j to compute the final augmented semantic representation by m X vi = pi,j ei,j , (3"
2020.emnlp-main.107,2020.acl-main.734,1,0.827164,"osine similarities and denote them as Ci = {ci,1 , ci,2 , · · · , ci,j , · · · , ci,m } (1) Afterwards, we use another embedding matrix to map all extracted words ci,j to their corresponding embeddings ei,j . Since not all ci,j ∈ Ci are helpful for predicting the NE label of xi in the given context, it is important to distinguish the contributions of different words to the NER task in that context. Consider that the attention and weight based approaches are demonstrated to be effective choices to selectively leverage extra information in many tasks (Kumar et al., 2018; Margatina et al., 2019; Tian et al., 2020a,d,b,c), we propose an attentive semantic augmentation module (denoted as AU ) to weight the words according to their contributions to the task in different contexts. Specifically, for each token xi , the augmentation module assigns a weight to each word ci,j ∈ Ci by exp(hi · ei,j ) pi,j = Pm , j=i exp(hi · ei,j ) (2) where hi is the hidden vector for xi obtained from 1384 the context encoder with its dimension matching that of the embedding (i.e., ei,j ) of ci,j . Then, we apply the weight pi,j to the word ci,j to compute the final augmented semantic representation by m X vi = pi,j ei,j , (3"
2020.emnlp-main.107,P18-1144,0,0.083248,"Missing"
2020.emnlp-main.107,P19-1336,0,0.0795523,"Missing"
2020.emnlp-main.107,N19-1342,0,0.0320308,"Missing"
2020.emnlp-main.112,W11-2107,0,0.0987532,"addition, we also compare our model with those in previous studies, including conventional image captioning models, e.g., ST (Vinyals et al., 2015), ATT 2 IN (Rennie et al., 2017), A DA ATT (Lu et al., 2017), T OPDOWN (Anderson et al., 2018), and the ones proposed for the medical domain, e.g., C OATT (Jing et al., 2018), H RGR (Li et al., 2018) and C MAS -R L (Jing et al., 2019). The performance of the aforementioned models is evaluated by conventional natural language generation (NLG) metrics and clinical efficacy (CE) metrics6 . The NLG metrics7 include BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and ROUGE-L (Lin, 2004). For clinical efficacy metrics, we use the CheXpert (Irvin et al., 2019)8 to label the generated reports and compare the results with ground truths in 14 different categories related to thoracic diseases and support devices. Precision, recall and F1 are used to evaluate model performance for these metrics. Baseline and Evaluation Metrics To compare with our proposed model, the following ones are used as the main baselines: 4 • BASE: this is the vanilla Transformer, with three layers, 8 heads and 512 hidden units without other extensions and modifications. • BASE + RM:"
2020.emnlp-main.112,N19-1423,0,0.0220659,"a hierarchical LSTM to generate reports. Li et al. (2018, 2019) proposed to use a manually extracted template database to help generation with bunches of special techniques to utilize templates. Liu et al. (2019) proposed an approach with reinforcement learning to maintain the clinical accuracy of generated reports. Compared to these studies, our model offers an alternative solution to this task with an effective and efficient enhancement of Transformer via memory. Extra knowledge (e.g., pre-trained embeddings (Song et al., 2017; Song and Shi, 2018; Zhang et al., 2019) and pretrained models (Devlin et al., 2019; Diao et al., 2019)) can provide useful information and thus enhance model performance for many NLP tasks (Tian et al., 2020a,b,c). Specifically, memory and memory-augmented neural networks (Zeng et al., 2018; Santoro et al., 2018; Diao et al., 2020; Tian et al., 2020d) are another line of related research, which can be traced back to Weston et al. (2015), which proposed memory networks to leverage extra information for question answering; then Sukhbaatar et al. (2015) improved it with an end-to-end design to ensure the model being trained with less supervision. Particularly for Transformer,"
2020.emnlp-main.112,P19-1657,0,0.496269,"R, show that our proposed approach outperforms previous models with respect to both language generation metrics and clinical evaluations. Particularly, this is the first work reporting the generation results on MIMIC-CXR to the best of our knowledge. Further analyses also demonstrate that our approach is able to generate long reports with necessary medical terms as well as meaningful image-text attention mappings.1 1 Figure 1: An example chest X-ray image and its report including findings and impression. in this area (Jing et al., 2018; Li et al., 2018; Johnson et al., 2019; Liu et al., 2019; Jing et al., 2019). Introduction Radiology report generation, which aims to automatically generate a free-text description for a clinical radiograph (e.g., chest X-ray), has emerged as a prominent attractive research direction in both artificial intelligence and clinical medicine. It can greatly expedite the automation of workflows and improve the quality and standardization of health care. Recently, there are many methods proposed † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/R2Gen. 1 Practically, a significant challenge of radiology report genera"
2020.emnlp-main.112,P18-1240,0,0.556578,"l results on two prevailing radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed approach outperforms previous models with respect to both language generation metrics and clinical evaluations. Particularly, this is the first work reporting the generation results on MIMIC-CXR to the best of our knowledge. Further analyses also demonstrate that our approach is able to generate long reports with necessary medical terms as well as meaningful image-text attention mappings.1 1 Figure 1: An example chest X-ray image and its report including findings and impression. in this area (Jing et al., 2018; Li et al., 2018; Johnson et al., 2019; Liu et al., 2019; Jing et al., 2019). Introduction Radiology report generation, which aims to automatically generate a free-text description for a clinical radiograph (e.g., chest X-ray), has emerged as a prominent attractive research direction in both artificial intelligence and clinical medicine. It can greatly expedite the automation of workflows and improve the quality and standardization of health care. Recently, there are many methods proposed † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz"
2020.emnlp-main.112,W04-1013,0,0.0175582,"those in previous studies, including conventional image captioning models, e.g., ST (Vinyals et al., 2015), ATT 2 IN (Rennie et al., 2017), A DA ATT (Lu et al., 2017), T OPDOWN (Anderson et al., 2018), and the ones proposed for the medical domain, e.g., C OATT (Jing et al., 2018), H RGR (Li et al., 2018) and C MAS -R L (Jing et al., 2019). The performance of the aforementioned models is evaluated by conventional natural language generation (NLG) metrics and clinical efficacy (CE) metrics6 . The NLG metrics7 include BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and ROUGE-L (Lin, 2004). For clinical efficacy metrics, we use the CheXpert (Irvin et al., 2019)8 to label the generated reports and compare the results with ground truths in 14 different categories related to thoracic diseases and support devices. Precision, recall and F1 are used to evaluate model performance for these metrics. Baseline and Evaluation Metrics To compare with our proposed model, the following ones are used as the main baselines: 4 • BASE: this is the vanilla Transformer, with three layers, 8 heads and 512 hidden units without other extensions and modifications. • BASE + RM: this is a simple alterna"
2020.emnlp-main.112,P02-1040,0,0.10639,"/content/ mimic-cxr/2.0.0/ 5 In addition, we also compare our model with those in previous studies, including conventional image captioning models, e.g., ST (Vinyals et al., 2015), ATT 2 IN (Rennie et al., 2017), A DA ATT (Lu et al., 2017), T OPDOWN (Anderson et al., 2018), and the ones proposed for the medical domain, e.g., C OATT (Jing et al., 2018), H RGR (Li et al., 2018) and C MAS -R L (Jing et al., 2019). The performance of the aforementioned models is evaluated by conventional natural language generation (NLG) metrics and clinical efficacy (CE) metrics6 . The NLG metrics7 include BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and ROUGE-L (Lin, 2004). For clinical efficacy metrics, we use the CheXpert (Irvin et al., 2019)8 to label the generated reports and compare the results with ground truths in 14 different categories related to thoracic diseases and support devices. Precision, recall and F1 are used to evaluate model performance for these metrics. Baseline and Evaluation Metrics To compare with our proposed model, the following ones are used as the main baselines: 4 • BASE: this is the vanilla Transformer, with three layers, 8 heads and 512 hidden units without other extensi"
2020.emnlp-main.112,K17-1016,1,0.783773,"s from blue to red represent the weights from low to high. mechanism and leveraged a hierarchical LSTM to generate reports. Li et al. (2018, 2019) proposed to use a manually extracted template database to help generation with bunches of special techniques to utilize templates. Liu et al. (2019) proposed an approach with reinforcement learning to maintain the clinical accuracy of generated reports. Compared to these studies, our model offers an alternative solution to this task with an effective and efficient enhancement of Transformer via memory. Extra knowledge (e.g., pre-trained embeddings (Song et al., 2017; Song and Shi, 2018; Zhang et al., 2019) and pretrained models (Devlin et al., 2019; Diao et al., 2019)) can provide useful information and thus enhance model performance for many NLP tasks (Tian et al., 2020a,b,c). Specifically, memory and memory-augmented neural networks (Zeng et al., 2018; Santoro et al., 2018; Diao et al., 2020; Tian et al., 2020d) are another line of related research, which can be traced back to Weston et al. (2015), which proposed memory networks to leverage extra information for question answering; then Sukhbaatar et al. (2015) improved it with an end-to-end design to"
2020.emnlp-main.112,2020.acl-main.735,1,0.810786,"generation with bunches of special techniques to utilize templates. Liu et al. (2019) proposed an approach with reinforcement learning to maintain the clinical accuracy of generated reports. Compared to these studies, our model offers an alternative solution to this task with an effective and efficient enhancement of Transformer via memory. Extra knowledge (e.g., pre-trained embeddings (Song et al., 2017; Song and Shi, 2018; Zhang et al., 2019) and pretrained models (Devlin et al., 2019; Diao et al., 2019)) can provide useful information and thus enhance model performance for many NLP tasks (Tian et al., 2020a,b,c). Specifically, memory and memory-augmented neural networks (Zeng et al., 2018; Santoro et al., 2018; Diao et al., 2020; Tian et al., 2020d) are another line of related research, which can be traced back to Weston et al. (2015), which proposed memory networks to leverage extra information for question answering; then Sukhbaatar et al. (2015) improved it with an end-to-end design to ensure the model being trained with less supervision. Particularly for Transformer, there are also memory-based methods proposed. For example, Lample et al. (2019) proposed to solve the under-fitting problem o"
2020.emnlp-main.112,D18-1351,1,0.837995,"9) proposed an approach with reinforcement learning to maintain the clinical accuracy of generated reports. Compared to these studies, our model offers an alternative solution to this task with an effective and efficient enhancement of Transformer via memory. Extra knowledge (e.g., pre-trained embeddings (Song et al., 2017; Song and Shi, 2018; Zhang et al., 2019) and pretrained models (Devlin et al., 2019; Diao et al., 2019)) can provide useful information and thus enhance model performance for many NLP tasks (Tian et al., 2020a,b,c). Specifically, memory and memory-augmented neural networks (Zeng et al., 2018; Santoro et al., 2018; Diao et al., 2020; Tian et al., 2020d) are another line of related research, which can be traced back to Weston et al. (2015), which proposed memory networks to leverage extra information for question answering; then Sukhbaatar et al. (2015) improved it with an end-to-end design to ensure the model being trained with less supervision. Particularly for Transformer, there are also memory-based methods proposed. For example, Lample et al. (2019) proposed to solve the under-fitting problem of Transformer by introducing a product-key layer that is similar to a memory module."
2020.emnlp-main.112,D19-1528,1,0.837741,"s from low to high. mechanism and leveraged a hierarchical LSTM to generate reports. Li et al. (2018, 2019) proposed to use a manually extracted template database to help generation with bunches of special techniques to utilize templates. Liu et al. (2019) proposed an approach with reinforcement learning to maintain the clinical accuracy of generated reports. Compared to these studies, our model offers an alternative solution to this task with an effective and efficient enhancement of Transformer via memory. Extra knowledge (e.g., pre-trained embeddings (Song et al., 2017; Song and Shi, 2018; Zhang et al., 2019) and pretrained models (Devlin et al., 2019; Diao et al., 2019)) can provide useful information and thus enhance model performance for many NLP tasks (Tian et al., 2020a,b,c). Specifically, memory and memory-augmented neural networks (Zeng et al., 2018; Santoro et al., 2018; Diao et al., 2020; Tian et al., 2020d) are another line of related research, which can be traced back to Weston et al. (2015), which proposed memory networks to leverage extra information for question answering; then Sukhbaatar et al. (2015) improved it with an end-to-end design to ensure the model being trained with less"
2020.emnlp-main.112,2020.emnlp-main.487,1,0.7566,"generation with bunches of special techniques to utilize templates. Liu et al. (2019) proposed an approach with reinforcement learning to maintain the clinical accuracy of generated reports. Compared to these studies, our model offers an alternative solution to this task with an effective and efficient enhancement of Transformer via memory. Extra knowledge (e.g., pre-trained embeddings (Song et al., 2017; Song and Shi, 2018; Zhang et al., 2019) and pretrained models (Devlin et al., 2019; Diao et al., 2019)) can provide useful information and thus enhance model performance for many NLP tasks (Tian et al., 2020a,b,c). Specifically, memory and memory-augmented neural networks (Zeng et al., 2018; Santoro et al., 2018; Diao et al., 2020; Tian et al., 2020d) are another line of related research, which can be traced back to Weston et al. (2015), which proposed memory networks to leverage extra information for question answering; then Sukhbaatar et al. (2015) improved it with an end-to-end design to ensure the model being trained with less supervision. Particularly for Transformer, there are also memory-based methods proposed. For example, Lample et al. (2019) proposed to solve the under-fitting problem o"
2020.emnlp-main.112,2020.findings-emnlp.153,1,0.768422,"generation with bunches of special techniques to utilize templates. Liu et al. (2019) proposed an approach with reinforcement learning to maintain the clinical accuracy of generated reports. Compared to these studies, our model offers an alternative solution to this task with an effective and efficient enhancement of Transformer via memory. Extra knowledge (e.g., pre-trained embeddings (Song et al., 2017; Song and Shi, 2018; Zhang et al., 2019) and pretrained models (Devlin et al., 2019; Diao et al., 2019)) can provide useful information and thus enhance model performance for many NLP tasks (Tian et al., 2020a,b,c). Specifically, memory and memory-augmented neural networks (Zeng et al., 2018; Santoro et al., 2018; Diao et al., 2020; Tian et al., 2020d) are another line of related research, which can be traced back to Weston et al. (2015), which proposed memory networks to leverage extra information for question answering; then Sukhbaatar et al. (2015) improved it with an end-to-end design to ensure the model being trained with less supervision. Particularly for Transformer, there are also memory-based methods proposed. For example, Lample et al. (2019) proposed to solve the under-fitting problem o"
2020.emnlp-main.112,2020.acl-main.734,1,0.820777,"generation with bunches of special techniques to utilize templates. Liu et al. (2019) proposed an approach with reinforcement learning to maintain the clinical accuracy of generated reports. Compared to these studies, our model offers an alternative solution to this task with an effective and efficient enhancement of Transformer via memory. Extra knowledge (e.g., pre-trained embeddings (Song et al., 2017; Song and Shi, 2018; Zhang et al., 2019) and pretrained models (Devlin et al., 2019; Diao et al., 2019)) can provide useful information and thus enhance model performance for many NLP tasks (Tian et al., 2020a,b,c). Specifically, memory and memory-augmented neural networks (Zeng et al., 2018; Santoro et al., 2018; Diao et al., 2020; Tian et al., 2020d) are another line of related research, which can be traced back to Weston et al. (2015), which proposed memory networks to leverage extra information for question answering; then Sukhbaatar et al. (2015) improved it with an end-to-end design to ensure the model being trained with less supervision. Particularly for Transformer, there are also memory-based methods proposed. For example, Lample et al. (2019) proposed to solve the under-fitting problem o"
2020.emnlp-main.487,J07-3004,0,0.229948,"llustrated in the red boxes. In this paper, we propose attentive GCN (AGCN) for CCG supertagging, where its input graph is built based on chunks (n-grams) extracted with unsupervised methods. In detail, two types of edges in the graph are introduced to model word relations within and across chunks and an attention mechanism is applied to GCN to weight those edges. In doing so, different contextual information are discriminatively learned to facilitate CCG supertagging without requiring any external resources. The validity of our approach is demonstrated by experimental results on the CCGbank (Hockenmaier and Steedman, 2007), where state-of-the-art performance is obtained for both tagging and parsing. 2 The Approach We treat CCG supertagging as a sequence labeling task, where the input is a sentence with n words X = x1 x2 · · · xi · · · xn , and the output is a sequence of supertags Yb = yb1 yb2 · · · ybi · · · ybn . Our approach uses attentive GCN (A-GCN) to incorporate information of word pairs through a graph; the graph is built based on n-grams in the input sentence that appear in a lexicon N . This lexicon GCN Normal GCN models with L layers learn from word pairs suggested by the dependency parsing results o"
2020.emnlp-main.487,D19-1549,0,0.0389459,"few rules afterwards. Building an accurate supertagger in a sequence labeling process requires a good modeling of contextual information. Recent neural approaches to supertagging mainly focused on leveraging powerful encoders with recurrent models (Lewis et al., 2016; Vaswani et al., 2016; Clark et al., 2018), with limited attention paid to modeling extra contextual features such as word pairs with strong relations. Graph convolutional networks (GCN) is demonstrated to be an effective approach to model such contextual information between words in many NLP tasks (Marcheggiani and Titov, 2017; Huang and Carley, 2019; De Cao et al., 2019; Huang et al., 2019); thus we want to determine whether this approach can also help CCG supertagging. However, we cannot directly apply conventional GCN models to CCG supertagging because in most of the previous studies the GCN models are built over the edges in the dependency tree of an input sentence. As high-quality dependency parsers are not always available, we do not want our CCG supertaggers to rely on the existence of dependency parsers. Thus, we need another way to extract useful word pairs to build GCN models. For that, we propose to obtain word pairs from frequ"
2020.emnlp-main.487,D19-1345,0,0.0285233,"upertagger in a sequence labeling process requires a good modeling of contextual information. Recent neural approaches to supertagging mainly focused on leveraging powerful encoders with recurrent models (Lewis et al., 2016; Vaswani et al., 2016; Clark et al., 2018), with limited attention paid to modeling extra contextual features such as word pairs with strong relations. Graph convolutional networks (GCN) is demonstrated to be an effective approach to model such contextual information between words in many NLP tasks (Marcheggiani and Titov, 2017; Huang and Carley, 2019; De Cao et al., 2019; Huang et al., 2019); thus we want to determine whether this approach can also help CCG supertagging. However, we cannot directly apply conventional GCN models to CCG supertagging because in most of the previous studies the GCN models are built over the edges in the dependency tree of an input sentence. As high-quality dependency parsers are not always available, we do not want our CCG supertaggers to rely on the existence of dependency parsers. Thus, we need another way to extract useful word pairs to build GCN models. For that, we propose to obtain word pairs from frequent chunks (n-grams) in the corpus, becaus"
2020.emnlp-main.487,P17-1174,0,0.0274195,"Therefore, in normal GCN, for each xi , all the xj that connect to xi are treated exactly the same. 2.2 Graph Construction based on Chunks Since CCG supertagging is also a parsing task, we do not want our approach to rely on the existence of a dependency parser. Without such a parser, we need an alternative for finding good word pairs to build the graph in A-GCN (which is equivalent to build the adjacency matrix A). Inspired by the studies that leverage chunks (n-grams) as effective features to carry contextual information and enhance model performance (Song et al., 2009; Song and Xia, 2012; Ishiwatari et al., 2017; Yoon et al., 2018; Zhang et al., 2019; Tian et al., 2020a,c,b), we propose to construct the graph based on the chunks (n-grams) extracted from a pre-constructed n-gram lexicon N . Specifically, the lexicon is constructed by computing the PMI of any two adjacent words s0 , s00 in the training set by P M I(s0 , s00 ) = log p(s0 s00 ) p(s0 )p(s00 ) (2) where p is the probability of an n-gram (i.e., s0 , s00 and s0 s00 ) in the training set; then a high PMI Figure 2: Examples of the two types of edges for building the graph in an input sentence, in which chunks (ngrams) extracted from the lexico"
2020.emnlp-main.487,P10-1036,0,0.0814397,"Missing"
2020.emnlp-main.487,N19-1020,0,0.079789,"Missing"
2020.emnlp-main.487,2020.acl-main.735,1,0.432409,"to xi are treated exactly the same. 2.2 Graph Construction based on Chunks Since CCG supertagging is also a parsing task, we do not want our approach to rely on the existence of a dependency parser. Without such a parser, we need an alternative for finding good word pairs to build the graph in A-GCN (which is equivalent to build the adjacency matrix A). Inspired by the studies that leverage chunks (n-grams) as effective features to carry contextual information and enhance model performance (Song et al., 2009; Song and Xia, 2012; Ishiwatari et al., 2017; Yoon et al., 2018; Zhang et al., 2019; Tian et al., 2020a,c,b), we propose to construct the graph based on the chunks (n-grams) extracted from a pre-constructed n-gram lexicon N . Specifically, the lexicon is constructed by computing the PMI of any two adjacent words s0 , s00 in the training set by P M I(s0 , s00 ) = log p(s0 s00 ) p(s0 )p(s00 ) (2) where p is the probability of an n-gram (i.e., s0 , s00 and s0 s00 ) in the training set; then a high PMI Figure 2: Examples of the two types of edges for building the graph in an input sentence, in which chunks (ngrams) extracted from the lexicon N are highlighted in green; example in-chunk and cross-c"
2020.emnlp-main.487,2020.findings-emnlp.153,1,0.442853,"to xi are treated exactly the same. 2.2 Graph Construction based on Chunks Since CCG supertagging is also a parsing task, we do not want our approach to rely on the existence of a dependency parser. Without such a parser, we need an alternative for finding good word pairs to build the graph in A-GCN (which is equivalent to build the adjacency matrix A). Inspired by the studies that leverage chunks (n-grams) as effective features to carry contextual information and enhance model performance (Song et al., 2009; Song and Xia, 2012; Ishiwatari et al., 2017; Yoon et al., 2018; Zhang et al., 2019; Tian et al., 2020a,c,b), we propose to construct the graph based on the chunks (n-grams) extracted from a pre-constructed n-gram lexicon N . Specifically, the lexicon is constructed by computing the PMI of any two adjacent words s0 , s00 in the training set by P M I(s0 , s00 ) = log p(s0 s00 ) p(s0 )p(s00 ) (2) where p is the probability of an n-gram (i.e., s0 , s00 and s0 s00 ) in the training set; then a high PMI Figure 2: Examples of the two types of edges for building the graph in an input sentence, in which chunks (ngrams) extracted from the lexicon N are highlighted in green; example in-chunk and cross-c"
2020.emnlp-main.487,N19-1093,1,0.810025,"Missing"
2020.emnlp-main.487,N16-1027,0,0.0365662,"t understanding. Therefore, CCG parse often provides useful information for many downstream natural language processing (NLP) tasks such as logical reasoning (Yoshikawa et al., 2018) and semantic parsing (Beschke, 2019). To perform CCG parsing in different languages, most studies conducted a supertagging-parsing pipline (Clark and Curran, 2007; Kummerfeld et al., † Corresponding author. Our code and models for CCG supertagging are released at https://github.com/cuhksz-nlp/NeST-CCG. 1 2010; Song et al., 2012; Lewis and Steedman, 2014b; Huang and Song, 2015; Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Yoshikawa et al., 2017), in which their main focus is the first step, and they generated the CCG parse trees directly from supertags with a few rules afterwards. Building an accurate supertagger in a sequence labeling process requires a good modeling of contextual information. Recent neural approaches to supertagging mainly focused on leveraging powerful encoders with recurrent models (Lewis et al., 2016; Vaswani et al., 2016; Clark et al., 2018), with limited attention paid to modeling extra contextual features such as word pairs with strong relations. Graph convolutional networks (GCN) is"
2020.emnlp-main.487,P15-2041,0,0.0379227,"tactic and semantic knowledge for text understanding. Therefore, CCG parse often provides useful information for many downstream natural language processing (NLP) tasks such as logical reasoning (Yoshikawa et al., 2018) and semantic parsing (Beschke, 2019). To perform CCG parsing in different languages, most studies conducted a supertagging-parsing pipline (Clark and Curran, 2007; Kummerfeld et al., † Corresponding author. Our code and models for CCG supertagging are released at https://github.com/cuhksz-nlp/NeST-CCG. 1 2010; Song et al., 2012; Lewis and Steedman, 2014b; Huang and Song, 2015; Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Yoshikawa et al., 2017), in which their main focus is the first step, and they generated the CCG parse trees directly from supertags with a few rules afterwards. Building an accurate supertagger in a sequence labeling process requires a good modeling of contextual information. Recent neural approaches to supertagging mainly focused on leveraging powerful encoders with recurrent models (Lewis et al., 2016; Vaswani et al., 2016; Clark et al., 2018), with limited attention paid to modeling extra contextual features such as word pairs with strong relatio"
2020.emnlp-main.487,N18-1142,0,0.122602,", for each xi , all the xj that connect to xi are treated exactly the same. 2.2 Graph Construction based on Chunks Since CCG supertagging is also a parsing task, we do not want our approach to rely on the existence of a dependency parser. Without such a parser, we need an alternative for finding good word pairs to build the graph in A-GCN (which is equivalent to build the adjacency matrix A). Inspired by the studies that leverage chunks (n-grams) as effective features to carry contextual information and enhance model performance (Song et al., 2009; Song and Xia, 2012; Ishiwatari et al., 2017; Yoon et al., 2018; Zhang et al., 2019; Tian et al., 2020a,c,b), we propose to construct the graph based on the chunks (n-grams) extracted from a pre-constructed n-gram lexicon N . Specifically, the lexicon is constructed by computing the PMI of any two adjacent words s0 , s00 in the training set by P M I(s0 , s00 ) = log p(s0 s00 ) p(s0 )p(s00 ) (2) where p is the probability of an n-gram (i.e., s0 , s00 and s0 s00 ) in the training set; then a high PMI Figure 2: Examples of the two types of edges for building the graph in an input sentence, in which chunks (ngrams) extracted from the lexicon N are highlighted"
2020.emnlp-main.487,N18-2065,0,0.0150593,"ous studies in terms of both supertagging and parsing. Further analyses illustrate the effectiveness of each component in our approach to discriminatively learn from word pairs to enhance CCG supertagging.1 1 Introduction Combinatory categorial grammar (CCG) is a lexicalized grammatical formalism, where the lexical categories (also known as supertags) of the words in a sentence provide informative syntactic and semantic knowledge for text understanding. Therefore, CCG parse often provides useful information for many downstream natural language processing (NLP) tasks such as logical reasoning (Yoshikawa et al., 2018) and semantic parsing (Beschke, 2019). To perform CCG parsing in different languages, most studies conducted a supertagging-parsing pipline (Clark and Curran, 2007; Kummerfeld et al., † Corresponding author. Our code and models for CCG supertagging are released at https://github.com/cuhksz-nlp/NeST-CCG. 1 2010; Song et al., 2012; Lewis and Steedman, 2014b; Huang and Song, 2015; Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Yoshikawa et al., 2017), in which their main focus is the first step, and they generated the CCG parse trees directly from supertags with a few rules afterwards"
2020.emnlp-main.487,P17-1026,0,0.206165,"fore, CCG parse often provides useful information for many downstream natural language processing (NLP) tasks such as logical reasoning (Yoshikawa et al., 2018) and semantic parsing (Beschke, 2019). To perform CCG parsing in different languages, most studies conducted a supertagging-parsing pipline (Clark and Curran, 2007; Kummerfeld et al., † Corresponding author. Our code and models for CCG supertagging are released at https://github.com/cuhksz-nlp/NeST-CCG. 1 2010; Song et al., 2012; Lewis and Steedman, 2014b; Huang and Song, 2015; Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Yoshikawa et al., 2017), in which their main focus is the first step, and they generated the CCG parse trees directly from supertags with a few rules afterwards. Building an accurate supertagger in a sequence labeling process requires a good modeling of contextual information. Recent neural approaches to supertagging mainly focused on leveraging powerful encoders with recurrent models (Lewis et al., 2016; Vaswani et al., 2016; Clark et al., 2018), with limited attention paid to modeling extra contextual features such as word pairs with strong relations. Graph convolutional networks (GCN) is demonstrated to be an eff"
2020.findings-emnlp.153,P17-1152,0,0.0346125,"r approach in parsing Arabic, Chinese, and English, where state-of-the-art performance is obtained by our approach on all of them.1 1 Figure 1: The treelet of an example of the form “V+NP+PP”, where the “PP” should attach to the “V” (in green) rather than the “NP” (in red). Introduction Constituency parsing, which aims to generate a structured syntactic parse tree for a given sentence, is one of the most fundamental tasks in natural language processing (NLP), and plays an important role in many downstream tasks such as relation extraction (Jiang and Diesner, 2019), natural language inference (Chen et al., 2017), and machine translation (Ma et al., 2018). Recently, † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is es"
2020.findings-emnlp.153,E06-1047,0,0.0497131,"h ability. DATASETS S ENT T OKEN ASL ATB T RAIN D EV T EST 16K 2K 2K 596K 70K 70K 31.4 30.5 29.9 CTB5 T RAIN D EV T EST 17K 350 348 478K 7K 8K 27.4 19.5 23.0 PTB T RAIN D EV T EST 40K 2K 2K 950K 40K 57K 23.9 23.6 23.5 24K 17K 458K 446K 19.0 26.2 B ROWN (F ULL ) G ENIA (F ULL ) Table 1: The statistics of all experimental datasets (with splits) in terms of sentence and token numbers, and average sentence length (ASL). Arabic Penn Treebank 2.0 (ATB) (Maamouri et al., 2004), the Chinese Penn Treebank 5 (CTB5) (Xue et al., 2005), and Penn Treebank 3 (PTB) (Marcus et al., 1993).4 For ATB, we follow Chiang et al. (2006) and Green and Manning (2010) to use their split5 to get the training/dev/test sets and convert the texts in the dataset from Buckwalter transliteration6 to modern standard Arabic. For CTB5 and PTB, we follow Shen et al. (2018) and Kamigaito et al. (2017) to split the datasets. Moreover, we use the Brown Corpus (Marcus et al., 1993) and Genia (Tateisi et al., 2005) for cross-domain experiments.7 For all datasets, we follow Suzuki et al. (2018) to clean up the raw data8 and report the statistics of each resulted dataset in Table 1. 3.2 N-gram Lexicon Construction For n-gram extraction, we compu"
2020.findings-emnlp.153,D16-1257,0,0.0238305,"the other is the chart-based approaches (Collins, 1997; Glaysher and Moldovan, 2006). Recently, neural methods start to play a dominant role in this task, where improvements mainly come from powerful encodings (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Stern et al., 2017; Gaddy et al., 2018; Kitaev and Klein, 2018; Kitaev et al., 2019; Fried et al., 2019). Moreover, there are studies that do not follow the aforementioned methodologies, which instead regard the task as a sequence-to-sequence generation task (Vinyals et al., 2015; Suzuki et al., 2018), a language modeling (Choe and Charniak, 2016) task or a sequence labeling task (G´omezRodr´ıguez and Vilares, 2018). To further improve the performance, some studies leverage extra resources (such as auto-parsed large corpus (Vinyals et al., 2015), pre-trained word embeddings (Kitaev and Klein, 2018)), HPSG information (Zhou and Zhao, 2019; Mrini et al., 2019), or use model ensembles (Kitaev et al., 2019). Compared to these studies, our approach offers an alternative way to enhance constituency parsing with effective leveraging of n-gram information. Moreover, the proposed span attention addresses the limitation of previous studies (Kita"
2020.findings-emnlp.153,P97-1003,0,0.728866,"one of the most fundamental tasks in natural language processing (NLP), and plays an important role in many downstream tasks such as relation extraction (Jiang and Diesner, 2019), natural language inference (Chen et al., 2017), and machine translation (Ma et al., 2018). Recently, † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is essential to modeling the entire sentence. Particularly, a significant boost on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerf"
2020.findings-emnlp.153,D16-1001,0,0.0245578,"ore, in deciding which component (i.e., “compete” or “customer”) the with-PP should attach to, n-grams (e.g., the uni-gram “companies”) may provide useful cues, since ”customers with companies” is less likely than “compete with companies”. 5 Related Work There are two main types of parsing methodologies. One is the transition-based approaches (Sagae and Lavie, 2005); the other is the chart-based approaches (Collins, 1997; Glaysher and Moldovan, 2006). Recently, neural methods start to play a dominant role in this task, where improvements mainly come from powerful encodings (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Stern et al., 2017; Gaddy et al., 2018; Kitaev and Klein, 2018; Kitaev et al., 2019; Fried et al., 2019). Moreover, there are studies that do not follow the aforementioned methodologies, which instead regard the task as a sequence-to-sequence generation task (Vinyals et al., 2015; Suzuki et al., 2018), a language modeling (Choe and Charniak, 2016) task or a sequence labeling task (G´omezRodr´ıguez and Vilares, 2018). To further improve the performance, some studies leverage extra resources (such as auto-parsed large corpus (Vinyals et al., 2015), pre-trained word embeddi"
2020.findings-emnlp.153,N19-1423,0,0.0385639,"rge uncased (LU) version of BERT. PARM reports the number of trainable parameters in each model. that the two words co-occur a lot in the dataset and are more likely to form an n-gram. We set the threshold to 0 to determine whether a delimiter should be inserted between the two adjacent words x0 and x00 . In other words, to build the lexicon N from a dataset, we use PMI as an unsupervised segmentation method to segment the dataset and collect all n-grams (n ≤ 5)9 appearing at least twice in the training and development sets combined.10 3.3 Model Implementation In our experiments, we use BERT (Devlin et al., 2019) as the basic encoder for all three languages and use ZEN (Diao et al., 2019) and XLNet-large (Yang et al., 2019) for Chinese and English, respectively.11 For BERT, ZEN, and XLNet, we use the default hyper-parameter settings. (e.g., 24 layers with 1024 dimensional hidden vector for the large models). In addition, following Kitaev et al. (2019), Zhou and Zhao (2019) and Mrini et al. (2019), we 9 We empirically set the max n-gram length to 5 as a unified threshold for all three languages. 10 We show the details of extracting the lexicon with example n-grams in the Appendix. 11 We download BERT m"
2020.findings-emnlp.153,N16-1024,0,0.172676,"V” (in green) rather than the “NP” (in red). Introduction Constituency parsing, which aims to generate a structured syntactic parse tree for a given sentence, is one of the most fundamental tasks in natural language processing (NLP), and plays an important role in many downstream tasks such as relation extraction (Jiang and Diesner, 2019), natural language inference (Chen et al., 2017), and machine translation (Ma et al., 2018). Recently, † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is essential to modeling the entire sentence. Particularly, a significant boost on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that emp"
2020.findings-emnlp.153,P19-1031,0,0.043252,"ags), and achieve stateof-the-art performance on all datasets. Compared with Zhou and Zhao (2019) and Mrini et al. (2019) which improve constituency parsing by leveraging the dependency information when training their head phrase structure grammar (HPSG) parser, our approach enhances the task from another direction by incorporating n-gram information through the span attentions as a way to address the limitation of using hidden vector subtraction to represent spans. 4.2 Cross-domain Experiments To further explore whether our approach can be generalized across domains, we follow the setting of Fried et al. (2019) to conduct cross-domain experiments on the Brown and Genia corpus using the models with SA and C AT SA, as well as their corresponding baseline. Note that, for fair comparison, we use BERT-large cased as the encoder without using the predicted POS tags. We follow Fried et al. (2019) to train models on the training set of PTB and evaluate them on the entire Brown corpus and the entire Genia corpus. To construct 14 We use the version of 3.9.2 obtained from https:// stanfordnlp.github.io/CoreNLP/. 15 We obtain their models from https://github.com/ nikitakit/self-attentive-parser. 16 For our mode"
2020.findings-emnlp.153,N18-1091,0,0.0406819,"Missing"
2020.findings-emnlp.153,P06-2038,0,0.292051,"atural language processing (NLP), and plays an important role in many downstream tasks such as relation extraction (Jiang and Diesner, 2019), natural language inference (Chen et al., 2017), and machine translation (Ma et al., 2018). Recently, † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is essential to modeling the entire sentence. Particularly, a significant boost on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-tr"
2020.findings-emnlp.153,D18-1162,0,0.026786,"Missing"
2020.findings-emnlp.153,C10-1045,0,0.0235998,"T OKEN ASL ATB T RAIN D EV T EST 16K 2K 2K 596K 70K 70K 31.4 30.5 29.9 CTB5 T RAIN D EV T EST 17K 350 348 478K 7K 8K 27.4 19.5 23.0 PTB T RAIN D EV T EST 40K 2K 2K 950K 40K 57K 23.9 23.6 23.5 24K 17K 458K 446K 19.0 26.2 B ROWN (F ULL ) G ENIA (F ULL ) Table 1: The statistics of all experimental datasets (with splits) in terms of sentence and token numbers, and average sentence length (ASL). Arabic Penn Treebank 2.0 (ATB) (Maamouri et al., 2004), the Chinese Penn Treebank 5 (CTB5) (Xue et al., 2005), and Penn Treebank 3 (PTB) (Marcus et al., 1993).4 For ATB, we follow Chiang et al. (2006) and Green and Manning (2010) to use their split5 to get the training/dev/test sets and convert the texts in the dataset from Buckwalter transliteration6 to modern standard Arabic. For CTB5 and PTB, we follow Shen et al. (2018) and Kamigaito et al. (2017) to split the datasets. Moreover, we use the Brown Corpus (Marcus et al., 1993) and Genia (Tateisi et al., 2005) for cross-domain experiments.7 For all datasets, we follow Suzuki et al. (2018) to clean up the raw data8 and report the statistics of each resulted dataset in Table 1. 3.2 N-gram Lexicon Construction For n-gram extraction, we compute the pointwise mutual infor"
2020.findings-emnlp.153,D19-5323,0,0.0173603,"benchmark datasets demonstrate the effectiveness of our approach in parsing Arabic, Chinese, and English, where state-of-the-art performance is obtained by our approach on all of them.1 1 Figure 1: The treelet of an example of the form “V+NP+PP”, where the “PP” should attach to the “V” (in green) rather than the “NP” (in red). Introduction Constituency parsing, which aims to generate a structured syntactic parse tree for a given sentence, is one of the most fundamental tasks in natural language processing (NLP), and plays an important role in many downstream tasks such as relation extraction (Jiang and Diesner, 2019), natural language inference (Chen et al., 2017), and machine translation (Ma et al., 2018). Recently, † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturin"
2020.findings-emnlp.153,P18-1110,0,0.035292,"Missing"
2020.findings-emnlp.153,I17-2002,0,0.0161686,"(F ULL ) G ENIA (F ULL ) Table 1: The statistics of all experimental datasets (with splits) in terms of sentence and token numbers, and average sentence length (ASL). Arabic Penn Treebank 2.0 (ATB) (Maamouri et al., 2004), the Chinese Penn Treebank 5 (CTB5) (Xue et al., 2005), and Penn Treebank 3 (PTB) (Marcus et al., 1993).4 For ATB, we follow Chiang et al. (2006) and Green and Manning (2010) to use their split5 to get the training/dev/test sets and convert the texts in the dataset from Buckwalter transliteration6 to modern standard Arabic. For CTB5 and PTB, we follow Shen et al. (2018) and Kamigaito et al. (2017) to split the datasets. Moreover, we use the Brown Corpus (Marcus et al., 1993) and Genia (Tateisi et al., 2005) for cross-domain experiments.7 For all datasets, we follow Suzuki et al. (2018) to clean up the raw data8 and report the statistics of each resulted dataset in Table 1. 3.2 N-gram Lexicon Construction For n-gram extraction, we compute the pointwise mutual information (PMI) of any two adjacent words x0 , x00 in the dataset by P M I(x0 , x00 ) = log p(x0 x00 ) p(x0 )p(x00 ) (10) where p is the probability of an n-gram (i.e., x0 , x00 and x0 x00 ) in a dataset. A high PMI score suggest"
2020.findings-emnlp.153,P19-1340,0,0.0640023,"red). Introduction Constituency parsing, which aims to generate a structured syntactic parse tree for a given sentence, is one of the most fundamental tasks in natural language processing (NLP), and plays an important role in many downstream tasks such as relation extraction (Jiang and Diesner, 2019), natural language inference (Chen et al., 2017), and machine translation (Ma et al., 2018). Recently, † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is essential to modeling the entire sentence. Particularly, a significant boost on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transfor"
2020.findings-emnlp.153,P18-1249,0,0.301291,"uhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is essential to modeling the entire sentence. Particularly, a significant boost on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-trained embeddings and extra syntactic information) can also provide useful contextual information and thus enhance model performance in many NLP tasks (Pennington et al., 2014; Song et al., 2018a; Zhang et al., 2019; Mrini et al., 2019; Tian et al., 2020a,b). As one type of the extra information, n-grams are used as a simple yet effectiv"
2020.findings-emnlp.153,Q17-1029,0,0.0146219,"component (i.e., “compete” or “customer”) the with-PP should attach to, n-grams (e.g., the uni-gram “companies”) may provide useful cues, since ”customers with companies” is less likely than “compete with companies”. 5 Related Work There are two main types of parsing methodologies. One is the transition-based approaches (Sagae and Lavie, 2005); the other is the chart-based approaches (Collins, 1997; Glaysher and Moldovan, 2006). Recently, neural methods start to play a dominant role in this task, where improvements mainly come from powerful encodings (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Stern et al., 2017; Gaddy et al., 2018; Kitaev and Klein, 2018; Kitaev et al., 2019; Fried et al., 2019). Moreover, there are studies that do not follow the aforementioned methodologies, which instead regard the task as a sequence-to-sequence generation task (Vinyals et al., 2015; Suzuki et al., 2018), a language modeling (Choe and Charniak, 2016) task or a sequence labeling task (G´omezRodr´ıguez and Vilares, 2018). To further improve the performance, some studies leverage extra resources (such as auto-parsed large corpus (Vinyals et al., 2015), pre-trained word embeddings (Kitaev and Klein"
2020.findings-emnlp.153,P18-1116,0,0.0167613,"glish, where state-of-the-art performance is obtained by our approach on all of them.1 1 Figure 1: The treelet of an example of the form “V+NP+PP”, where the “PP” should attach to the “V” (in green) rather than the “NP” (in red). Introduction Constituency parsing, which aims to generate a structured syntactic parse tree for a given sentence, is one of the most fundamental tasks in natural language processing (NLP), and plays an important role in many downstream tasks such as relation extraction (Jiang and Diesner, 2019), natural language inference (Chen et al., 2017), and machine translation (Ma et al., 2018). Recently, † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is essential to modeling the entire sentence. Pa"
2020.findings-emnlp.153,P14-5010,0,0.00337342,"RIED ET AL ., 2019) 93.10 87.54 BERT + SA + C AT SA 93.13 93.24 93.29 87.58 87.50 87.53 Table 4: Cross-domain experiment results (F1 scores) from previous studies and our models (based on BERTLC), on the entire Brown and Genia corpora when trained from the training set of PTB. POS tags as the additional input, which suggests that the predicted POS tags may have more conflict with ZEN compared with BERT. Moreover, we run our models on the test set of each dataset and compare the results with previous studies, as well as the ones from prevailing parsers, i.e., Stanford CoreNLP Toolkits (SCT)14 (Manning et al., 2014) and Berkeley Neural Parser (BNP)15 (Kitaev and Klein, 2018). The results are reported in Table 3, where the models using predicted POS tags are marked with “*”.16 Our models with C AT SA outperform previous best performing models from Zhou and Zhao (2019) and Mrini et al. (2019) under different settings (i.e., whether to use the predicted POS tags), and achieve stateof-the-art performance on all datasets. Compared with Zhou and Zhao (2019) and Mrini et al. (2019) which improve constituency parsing by leveraging the dependency information when training their head phrase structure grammar (HPSG"
2020.findings-emnlp.153,J93-2004,0,0.0782316,"so they are suitable to be grouped by such ability. DATASETS S ENT T OKEN ASL ATB T RAIN D EV T EST 16K 2K 2K 596K 70K 70K 31.4 30.5 29.9 CTB5 T RAIN D EV T EST 17K 350 348 478K 7K 8K 27.4 19.5 23.0 PTB T RAIN D EV T EST 40K 2K 2K 950K 40K 57K 23.9 23.6 23.5 24K 17K 458K 446K 19.0 26.2 B ROWN (F ULL ) G ENIA (F ULL ) Table 1: The statistics of all experimental datasets (with splits) in terms of sentence and token numbers, and average sentence length (ASL). Arabic Penn Treebank 2.0 (ATB) (Maamouri et al., 2004), the Chinese Penn Treebank 5 (CTB5) (Xue et al., 2005), and Penn Treebank 3 (PTB) (Marcus et al., 1993).4 For ATB, we follow Chiang et al. (2006) and Green and Manning (2010) to use their split5 to get the training/dev/test sets and convert the texts in the dataset from Buckwalter transliteration6 to modern standard Arabic. For CTB5 and PTB, we follow Shen et al. (2018) and Kamigaito et al. (2017) to split the datasets. Moreover, we use the Brown Corpus (Marcus et al., 1993) and Genia (Tateisi et al., 2005) for cross-domain experiments.7 For all datasets, we follow Suzuki et al. (2018) to clean up the raw data8 and report the statistics of each resulted dataset in Table 1. 3.2 N-gram Lexicon Co"
2020.findings-emnlp.153,D14-1162,0,0.0923621,"Missing"
2020.findings-emnlp.153,C10-1100,0,0.0130132,"ion. For instance, Figure 1 illustrates the treelet of an example in the form of “V+NP+PP”. As a classic example of PP-attachment ambiguity, a parser may wrongly attach the “PP” to the “NP” if it only focuses on the words at the boundaries of the text span “flag ... year” and in-between information is not represented properly. In this case, n-grams within that span (e.g., the uni-gram “telescope”) can provide useful cues indicating that the “PP” should be attached to the “V”. Although there are traditional non-neural parsers using n-grams as features to improve parsing (Sagae and Lavie, 2005; Pitler et al., 2010), they are limited in treating them euqally without learning their weights. Therefore, unimportant n-grams may deliver misleading information and lead to wrong predictions. To address this problem, in this paper, we propose a span attention module to enhance chartbased neural constituency parsing by incorporating appropriate n-grams into span representations. Specifically, for each text span we extract all its substrings that appear in an n-gram lexicon; the span attention uses the normal attention mechanism to weight them with respect to their contributions to predict the constituency label o"
2020.findings-emnlp.153,W05-1513,0,0.470284,"fundamental tasks in natural language processing (NLP), and plays an important role in many downstream tasks such as relation extraction (Jiang and Diesner, 2019), natural language inference (Chen et al., 2017), and machine translation (Ma et al., 2018). Recently, † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is essential to modeling the entire sentence. Particularly, a significant boost on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extr"
2020.findings-emnlp.153,P18-1108,0,0.0141978,"K 446K 19.0 26.2 B ROWN (F ULL ) G ENIA (F ULL ) Table 1: The statistics of all experimental datasets (with splits) in terms of sentence and token numbers, and average sentence length (ASL). Arabic Penn Treebank 2.0 (ATB) (Maamouri et al., 2004), the Chinese Penn Treebank 5 (CTB5) (Xue et al., 2005), and Penn Treebank 3 (PTB) (Marcus et al., 1993).4 For ATB, we follow Chiang et al. (2006) and Green and Manning (2010) to use their split5 to get the training/dev/test sets and convert the texts in the dataset from Buckwalter transliteration6 to modern standard Arabic. For CTB5 and PTB, we follow Shen et al. (2018) and Kamigaito et al. (2017) to split the datasets. Moreover, we use the Brown Corpus (Marcus et al., 1993) and Genia (Tateisi et al., 2005) for cross-domain experiments.7 For all datasets, we follow Suzuki et al. (2018) to clean up the raw data8 and report the statistics of each resulted dataset in Table 1. 3.2 N-gram Lexicon Construction For n-gram extraction, we compute the pointwise mutual information (PMI) of any two adjacent words x0 , x00 in the dataset by P M I(x0 , x00 ) = log p(x0 x00 ) p(x0 )p(x00 ) (10) where p is the probability of an n-gram (i.e., x0 , x00 and x0 x00 ) in a datas"
2020.findings-emnlp.153,W09-3511,1,0.530073,"employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-trained embeddings and extra syntactic information) can also provide useful contextual information and thus enhance model performance in many NLP tasks (Pennington et al., 2014; Song et al., 2018a; Zhang et al., 2019; Mrini et al., 2019; Tian et al., 2020a,b). As one type of the extra information, n-grams are used as a simple yet effective source of contextual feature in many studies (Song et al., 2009; Song and Xia, 2012; Yoon et al., 2018; Tian et al., 2020c) Therefore, they could be potentially beneficial for parsing as well. However, recent chart-based parers (Stern et al., 2017; Kitaev and Klein, 2018; Gaddy et al., 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) make rare effort to leverage such n-gram information. Another potential issue with current 1691 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1691–1703 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Figure 2: The architecture of the chart-based constituency parser with s"
2020.findings-emnlp.153,N18-2028,1,0.782206,"ling the entire sentence. Particularly, a significant boost on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-trained embeddings and extra syntactic information) can also provide useful contextual information and thus enhance model performance in many NLP tasks (Pennington et al., 2014; Song et al., 2018a; Zhang et al., 2019; Mrini et al., 2019; Tian et al., 2020a,b). As one type of the extra information, n-grams are used as a simple yet effective source of contextual feature in many studies (Song et al., 2009; Song and Xia, 2012; Yoon et al., 2018; Tian et al., 2020c) Therefore, they could be potentially beneficial for parsing as well. However, recent chart-based parers (Stern et al., 2017; Kitaev and Klein, 2018; Gaddy et al., 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) make rare effort to leverage such n-gram information. Another potential issue with current 1691 Findings of the Associ"
2020.findings-emnlp.153,song-xia-2012-using,1,0.831296,"xt encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-trained embeddings and extra syntactic information) can also provide useful contextual information and thus enhance model performance in many NLP tasks (Pennington et al., 2014; Song et al., 2018a; Zhang et al., 2019; Mrini et al., 2019; Tian et al., 2020a,b). As one type of the extra information, n-grams are used as a simple yet effective source of contextual feature in many studies (Song et al., 2009; Song and Xia, 2012; Yoon et al., 2018; Tian et al., 2020c) Therefore, they could be potentially beneficial for parsing as well. However, recent chart-based parers (Stern et al., 2017; Kitaev and Klein, 2018; Gaddy et al., 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) make rare effort to leverage such n-gram information. Another potential issue with current 1691 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1691–1703 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Figure 2: The architecture of the chart-based constituency parser with span attention, with"
2020.findings-emnlp.153,P17-1076,0,0.249882,"r than the “NP” (in red). Introduction Constituency parsing, which aims to generate a structured syntactic parse tree for a given sentence, is one of the most fundamental tasks in natural language processing (NLP), and plays an important role in many downstream tasks such as relation extraction (Jiang and Diesner, 2019), natural language inference (Chen et al., 2017), and machine translation (Ma et al., 2018). Recently, † Corresponding author. Our code and the best performing models are released at https://github.com/cuhksz-nlp/SAPar. 1 neural parsers (Vinyals et al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is essential to modeling the entire sentence. Particularly, a significant boost on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text en"
2020.findings-emnlp.153,P18-2097,0,0.203982,"TB) (Maamouri et al., 2004), the Chinese Penn Treebank 5 (CTB5) (Xue et al., 2005), and Penn Treebank 3 (PTB) (Marcus et al., 1993).4 For ATB, we follow Chiang et al. (2006) and Green and Manning (2010) to use their split5 to get the training/dev/test sets and convert the texts in the dataset from Buckwalter transliteration6 to modern standard Arabic. For CTB5 and PTB, we follow Shen et al. (2018) and Kamigaito et al. (2017) to split the datasets. Moreover, we use the Brown Corpus (Marcus et al., 1993) and Genia (Tateisi et al., 2005) for cross-domain experiments.7 For all datasets, we follow Suzuki et al. (2018) to clean up the raw data8 and report the statistics of each resulted dataset in Table 1. 3.2 N-gram Lexicon Construction For n-gram extraction, we compute the pointwise mutual information (PMI) of any two adjacent words x0 , x00 in the dataset by P M I(x0 , x00 ) = log p(x0 x00 ) p(x0 )p(x00 ) (10) where p is the probability of an n-gram (i.e., x0 , x00 and x0 x00 ) in a dataset. A high PMI score suggests 4 All the datasets are obtained from the official release of Linguistic Data Consortium. The catalog numbers for ATB part 1-3 are LDC2003T06, LDC2004T02, LDC2005T20, for CTB5 is LDC2005T01,"
2020.findings-emnlp.153,I05-2038,0,0.0420425,"and token numbers, and average sentence length (ASL). Arabic Penn Treebank 2.0 (ATB) (Maamouri et al., 2004), the Chinese Penn Treebank 5 (CTB5) (Xue et al., 2005), and Penn Treebank 3 (PTB) (Marcus et al., 1993).4 For ATB, we follow Chiang et al. (2006) and Green and Manning (2010) to use their split5 to get the training/dev/test sets and convert the texts in the dataset from Buckwalter transliteration6 to modern standard Arabic. For CTB5 and PTB, we follow Shen et al. (2018) and Kamigaito et al. (2017) to split the datasets. Moreover, we use the Brown Corpus (Marcus et al., 1993) and Genia (Tateisi et al., 2005) for cross-domain experiments.7 For all datasets, we follow Suzuki et al. (2018) to clean up the raw data8 and report the statistics of each resulted dataset in Table 1. 3.2 N-gram Lexicon Construction For n-gram extraction, we compute the pointwise mutual information (PMI) of any two adjacent words x0 , x00 in the dataset by P M I(x0 , x00 ) = log p(x0 x00 ) p(x0 )p(x00 ) (10) where p is the probability of an n-gram (i.e., x0 , x00 and x0 x00 ) in a dataset. A high PMI score suggests 4 All the datasets are obtained from the official release of Linguistic Data Consortium. The catalog numbers f"
2020.findings-emnlp.153,C18-1011,0,0.0400782,"Missing"
2020.findings-emnlp.153,2020.acl-main.735,1,0.527704,"on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-trained embeddings and extra syntactic information) can also provide useful contextual information and thus enhance model performance in many NLP tasks (Pennington et al., 2014; Song et al., 2018a; Zhang et al., 2019; Mrini et al., 2019; Tian et al., 2020a,b). As one type of the extra information, n-grams are used as a simple yet effective source of contextual feature in many studies (Song et al., 2009; Song and Xia, 2012; Yoon et al., 2018; Tian et al., 2020c) Therefore, they could be potentially beneficial for parsing as well. However, recent chart-based parers (Stern et al., 2017; Kitaev and Klein, 2018; Gaddy et al., 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) make rare effort to leverage such n-gram information. Another potential issue with current 1691 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1691–"
2020.findings-emnlp.153,2020.emnlp-main.487,1,0.442853,"on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-trained embeddings and extra syntactic information) can also provide useful contextual information and thus enhance model performance in many NLP tasks (Pennington et al., 2014; Song et al., 2018a; Zhang et al., 2019; Mrini et al., 2019; Tian et al., 2020a,b). As one type of the extra information, n-grams are used as a simple yet effective source of contextual feature in many studies (Song et al., 2009; Song and Xia, 2012; Yoon et al., 2018; Tian et al., 2020c) Therefore, they could be potentially beneficial for parsing as well. However, recent chart-based parers (Stern et al., 2017; Kitaev and Klein, 2018; Gaddy et al., 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) make rare effort to leverage such n-gram information. Another potential issue with current 1691 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1691–"
2020.findings-emnlp.153,2020.acl-main.734,1,0.563505,"on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-trained embeddings and extra syntactic information) can also provide useful contextual information and thus enhance model performance in many NLP tasks (Pennington et al., 2014; Song et al., 2018a; Zhang et al., 2019; Mrini et al., 2019; Tian et al., 2020a,b). As one type of the extra information, n-grams are used as a simple yet effective source of contextual feature in many studies (Song et al., 2009; Song and Xia, 2012; Yoon et al., 2018; Tian et al., 2020c) Therefore, they could be potentially beneficial for parsing as well. However, recent chart-based parers (Stern et al., 2017; Kitaev and Klein, 2018; Gaddy et al., 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) make rare effort to leverage such n-gram information. Another potential issue with current 1691 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1691–"
2020.findings-emnlp.153,N03-1033,0,0.287347,"/github.com/zihangdai/xlnet. add three additional token-level self-attention layers to the top of BERT, ZEN, and XLNet. For other settings, we randomly initialize all ngram embeddings used in our attention module12 with their dimension matching that of the hidden vectors obtained from the encoder (e.g., 1024 for BERT-large). Besides, we run our experiments with and without predicted part-of-speech (POS) tags. Following previous studies, for the experiments without POS tags, we take sentences as the only input; for the experiments with POS tags, we obtain the POS tags from Stanford POS Tagger (Toutanova et al., 2003) and incorporate the POS tags by directly concatenating their embeddings with the output of the BERT/ZEN/XLNet encoder. Following previous studies (Suzuki et al., 2018; Kitaev et al., 2019), we use hinge loss during the training process and evaluate different models by by precision, recall, F1 score, and complete match score via the standard evaluation toolkit E VALB13 . During the training process, we try three learning rates, i.e., 5e-5, 1e-5, 5e-6, with a fixed random seed, pick the model with the best F1 score on the development set, and evaluate it on the test set. 12 We also try initiali"
2020.findings-emnlp.153,N18-1142,0,0.151668,"ransformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-trained embeddings and extra syntactic information) can also provide useful contextual information and thus enhance model performance in many NLP tasks (Pennington et al., 2014; Song et al., 2018a; Zhang et al., 2019; Mrini et al., 2019; Tian et al., 2020a,b). As one type of the extra information, n-grams are used as a simple yet effective source of contextual feature in many studies (Song et al., 2009; Song and Xia, 2012; Yoon et al., 2018; Tian et al., 2020c) Therefore, they could be potentially beneficial for parsing as well. However, recent chart-based parers (Stern et al., 2017; Kitaev and Klein, 2018; Gaddy et al., 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) make rare effort to leverage such n-gram information. Another potential issue with current 1691 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1691–1703 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Figure 2: The architecture of the chart-based constituency parser with span attention, with an example partial"
2020.findings-emnlp.153,N19-1093,1,0.875724,"Missing"
2020.findings-emnlp.153,P19-1230,0,0.240512,"al., 2015; Dyer et al., 2016; Stern et al., 2017; Kitaev et al., 2019) without using any grammar rules significantly outperform conventional statistical grammar-based ones (Collins, 1997; Sagae and Lavie, 2005; Glaysher and Moldovan, 2006; Song and Kit, 2009), because neural networks, especially recurrent models (e.g, Bi-LSTM), are adept in capturing long range contextual information, which is essential to modeling the entire sentence. Particularly, a significant boost on the performance of chart-based parsers is observed from some recent studies (Kitaev and Klein, 2018; Kitaev et al., 2019; Zhou and Zhao, 2019) that employ advanced text encoders (i.e., Transformer, BERT, and XLNet), which further demonstrates the usefulness of contexts for parsing. In general, besides powerful encoders, other extra information (such as pre-trained embeddings and extra syntactic information) can also provide useful contextual information and thus enhance model performance in many NLP tasks (Pennington et al., 2014; Song et al., 2018a; Zhang et al., 2019; Mrini et al., 2019; Tian et al., 2020a,b). As one type of the extra information, n-grams are used as a simple yet effective source of contextual feature in many stud"
2020.findings-emnlp.378,N19-1078,0,0.0625682,"For the text input, we use three types of embeddings for each language by default. Specifically, for English, we use Glove (100dimension)6 (Pennington et al., 2014), ELMo (Peters et al., 2018), and the BERT-cased large7 (Devlin et al., 2019) (the derived embeddings for each word); for Chinese, we use pre-trained character and bi-gram embeddings8 released by Zhang and Yang (2018) (denoted as Giga), Tencent Embedding9 (Song et al., 2018b), and ZEN10 (Diao et al., 2019). For both BERT and ZEN, we follow their 5 Among these datasets, ON5e and ON4c are multi-lingual datasets. We follow Yan et al. (2019) by extracting the corresponding English and Chinese part from them. 6 We download the Glove.6B embedding from https: //nlp.stanford.edu/projects/glove/ 7 We obtain the pre-trained BERT from https:// github.com/google-research/bert. 8 We obtain the embeddings from https://github. com/jiesutd/LatticeLSTM. 9 We use the official release from https://ai. tencent.com/ailab/nlp/embedding.html. 10 We use the pre-trained ZEN-base downloaded from https://github.com/sinovation/ZEN. Note that we do not use the Chinese BERT since ZEN performs better across three Chinese datasets. For reference, we report"
2020.findings-emnlp.378,C18-1139,0,0.123995,"mation from different types, e.g., POS labels, syntactic constituents, or dependency relations; syntax attention is proposed to weight different types of such syntactic information, and the gate mechanism controls the contribution of the results from the context encoding and the syntax attention to the NER process. Through the attentive ensemble, important syntactic information is highlighted and emphasized during labeling NEs. In addition, to further improve NER performance, we also try different types of pre-trained word embeddings, which is demonstrated to be effective in previous studies (Akbik et al., 2018; Jie and Lu, 2019; Liu et al., 2019b; Yan et al., 2019). We experiment our approach on six widely used benchmark datasets from the general domain, where half of them are in English and the other half are in Chinese. Experimental results on all datasets suggest the effectiveness of our approach to enhance NER through syntactic information, where state-of-theart results are achieved on all datasets. 2 The Proposed Model NER is conventionally regarded as a typical sequence labeling task, where an input sequence X = x1 , x2 , · · · , xi , · · · , xn with n tokens is annotated with its correspondi"
2020.findings-emnlp.378,D19-1539,0,0.0771513,"arded as a sequence labeling task with models such as hidden Markov model (HMM) (Bikel * Equal contribution. Corresponding author. 1 The code and the best performing models are available at https://github.com/cuhksz-nlp/AESINER † et al., 1997) and conditional random field (CRF) (McCallum and Li, 2003) applied to it in previous studies. Recently, neural models play a dominate role in this task and illustrated promising results (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Strubell et al., 2017; Yadav and Bethard, 2018; Chen et al., 2019; Jie and Lu, 2019; Liu et al., 2019d; Baevski et al., 2019), because they are powerful in encoding contextual information and thus drive NER systems to better understand the text and recognize NEs in the input text. Although it is straightforward and effective to use neural models to help NER, it is expected to incorporate more useful features into an NER system. Among all such features, syntactic ones, such as part-of-speech (POS) labels, syntactic constituents, dependency relations, are of high importance to NER because they are effective in identifying the inherited structure in a piece of text and thus guide the system to find appropriate NEs acco"
2020.findings-emnlp.378,A97-1029,0,0.276273,"Missing"
2020.findings-emnlp.378,D19-1549,0,0.0348284,"cy relations (c) for “Salt” in the example sentence, where associated contextual features and the corresponding instances of syntactic information are highlighted in blue. 2.1 Syntactic Information Extraction A good representation of the input text is the key to obtain good model performance for many NLP tasks (Song et al., 2017; Sileo et al., 2019). Normally, a straightforward way to improve model performance is to enhance text representation by embeddings of extra features, which is demonstrated to be useful across tasks (Marcheggiani and Titov, 2017; Song et al., 2018a; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020c), including NER (Zhang and Yang, 2018; Seyler et al., 2018; Sui et al., 2019; Gui et al., 2019b,a; Liu et al., 2019b). Among different types of extra features, the syntactic one has been proved to be helpful in previous studies for NER, where the effectiveness of POS labels, syntactic constituents, and dependency relations, are demonstrated by McCallum (2003), Li et al. (2017), and Cetoli et al. (2018), respectively. In this paper, we also focus on these three types of syntactic information. In doing so, we obtain the POS labels, the syntax tree and the dependency parsing"
2020.findings-emnlp.378,D19-1399,0,0.424805,"odology for NER is conventionally regarded as a sequence labeling task with models such as hidden Markov model (HMM) (Bikel * Equal contribution. Corresponding author. 1 The code and the best performing models are available at https://github.com/cuhksz-nlp/AESINER † et al., 1997) and conditional random field (CRF) (McCallum and Li, 2003) applied to it in previous studies. Recently, neural models play a dominate role in this task and illustrated promising results (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Strubell et al., 2017; Yadav and Bethard, 2018; Chen et al., 2019; Jie and Lu, 2019; Liu et al., 2019d; Baevski et al., 2019), because they are powerful in encoding contextual information and thus drive NER systems to better understand the text and recognize NEs in the input text. Although it is straightforward and effective to use neural models to help NER, it is expected to incorporate more useful features into an NER system. Among all such features, syntactic ones, such as part-of-speech (POS) labels, syntactic constituents, dependency relations, are of high importance to NER because they are effective in identifying the inherited structure in a piece of text and thus gui"
2020.findings-emnlp.378,P08-1047,0,0.0603502,"information in the text without requiring to extract manually crafted features (Huang et al., 2015; Lample et al., 2016; Strubell et al., 2017; Zhang and Yang, 2018; Peters et al., 2018; Yadav and Bethard, 2018; Cetoli et al., 2018; Akbik et al., 2018, 2019; Chen et al., 2019; Devlin et al., 2019; Zhu and Wang, 2019; Liu et al., 2019b; Baevski et al., 2019; Yan et al., 2019; Xu et al., 2019a; Zhu et al., 2020; Luo et al.). However, to enhance NER, it is straightforward to incorporate more knowledge to it than only modeling from contexts. Therefore, additional resources such as knowledge base (Kazama and Torisawa, 2008; Tkachenko and Simanovsky, 2012; Seyler et al., 2018; Liu et al., 2019b,a; Gui et al., 2019b,a) and syntactic information (McCallum, 2003; Mohit and Hwa, 2005; Finkel and Manning, 2009; Li et al., 2017; Luo et al., 2018; Cetoli et al., 2018; Jie and Lu, 2019) are applied in previous studies. Particularly, Luo et al. (2018) and Dang et al. (2018) 19 “Bill” is part of the “PERSON” entity in most cases. Conclusion In this paper, we proposed a neural model following the sequence labeling paradigm to enhance NER through attentive ensemble of syntactic information. Particularly, the attentive ensem"
2020.findings-emnlp.378,N18-2041,0,0.0167887,"s and br the bias term, and use The Syntax Attention Upon encoding each type of syntactic information by KVMN, one can assemble different types of them with an overall representation. The most straightforward way of doing so is to concatenate the encoding from each type by si = ⊕ sci qic = σ(Wqc · (hi ⊕ sci ) + bcq ) (2) where hi is the hidden vector for xi obtained from the context encoder. Afterwards, we apply the weights pci,j to their corresponding syntactic inforc by mation vi,j mi X Motivated by studies that selectively leverage different features by assigning different weights to them (Kumar et al., 2018; Higashiyama et al., 2019; Tian et al., 2020a,b), we propose a syntax attention for the syntactic information ensemble. Particularly, for each syntactic type c, we firstly concatenate sci with hi and use the resulting vector to compute the weight qic for sci : (4) oi = [ri ◦ hi ] ⊕ [(1 − ri ) ◦ si ] (9) to control the contribution of them, where oi is the output of the gate mechanism corresponding to input xi , 1 is a 1-vector with its dimension matching hi and ◦ the element-wise multiplication operation. c∈C where si is the aggregated results of sci , the embedding for each syntactic type fr"
2020.findings-emnlp.378,N16-1030,0,0.169354,"ng (Dong and Lapata, 2018) and entity linking (Martins et al., 2019), etc. The main methodology for NER is conventionally regarded as a sequence labeling task with models such as hidden Markov model (HMM) (Bikel * Equal contribution. Corresponding author. 1 The code and the best performing models are available at https://github.com/cuhksz-nlp/AESINER † et al., 1997) and conditional random field (CRF) (McCallum and Li, 2003) applied to it in previous studies. Recently, neural models play a dominate role in this task and illustrated promising results (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Strubell et al., 2017; Yadav and Bethard, 2018; Chen et al., 2019; Jie and Lu, 2019; Liu et al., 2019d; Baevski et al., 2019), because they are powerful in encoding contextual information and thus drive NER systems to better understand the text and recognize NEs in the input text. Although it is straightforward and effective to use neural models to help NER, it is expected to incorporate more useful features into an NER system. Among all such features, syntactic ones, such as part-of-speech (POS) labels, syntactic constituents, dependency relations, are of high importance to NER because they"
2020.findings-emnlp.378,D17-1282,0,0.334101,"NER systems to better understand the text and recognize NEs in the input text. Although it is straightforward and effective to use neural models to help NER, it is expected to incorporate more useful features into an NER system. Among all such features, syntactic ones, such as part-of-speech (POS) labels, syntactic constituents, dependency relations, are of high importance to NER because they are effective in identifying the inherited structure in a piece of text and thus guide the system to find appropriate NEs accordingly, which is proved in a large body of previous studies (McCallum, 2003; Li et al., 2017; Luo et al., 2018; Dang et al., 2018; Jie and Lu, 2019). Although promising results are obtained, existing models are limited in regarding extra features as gold references and directly concatenate them with word embeddings. Therefore, such features are not distinguished and separately treated when they are used in those NER models, where the noise in the extra features (e.g., inaccurate POS tagging results) may hurt model performance. As a result, it is still a challenge to find an appropriate way to incorporate external information into neural models for NER. Moreover, in most cases, one wo"
2020.findings-emnlp.378,N19-1117,0,0.0377894,"Missing"
2020.findings-emnlp.378,P19-1524,0,0.122307,"conventionally regarded as a sequence labeling task with models such as hidden Markov model (HMM) (Bikel * Equal contribution. Corresponding author. 1 The code and the best performing models are available at https://github.com/cuhksz-nlp/AESINER † et al., 1997) and conditional random field (CRF) (McCallum and Li, 2003) applied to it in previous studies. Recently, neural models play a dominate role in this task and illustrated promising results (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Strubell et al., 2017; Yadav and Bethard, 2018; Chen et al., 2019; Jie and Lu, 2019; Liu et al., 2019d; Baevski et al., 2019), because they are powerful in encoding contextual information and thus drive NER systems to better understand the text and recognize NEs in the input text. Although it is straightforward and effective to use neural models to help NER, it is expected to incorporate more useful features into an NER system. Among all such features, syntactic ones, such as part-of-speech (POS) labels, syntactic constituents, dependency relations, are of high importance to NER because they are effective in identifying the inherited structure in a piece of text and thus guide the system to f"
2020.findings-emnlp.378,N19-1247,0,0.351569,"conventionally regarded as a sequence labeling task with models such as hidden Markov model (HMM) (Bikel * Equal contribution. Corresponding author. 1 The code and the best performing models are available at https://github.com/cuhksz-nlp/AESINER † et al., 1997) and conditional random field (CRF) (McCallum and Li, 2003) applied to it in previous studies. Recently, neural models play a dominate role in this task and illustrated promising results (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Strubell et al., 2017; Yadav and Bethard, 2018; Chen et al., 2019; Jie and Lu, 2019; Liu et al., 2019d; Baevski et al., 2019), because they are powerful in encoding contextual information and thus drive NER systems to better understand the text and recognize NEs in the input text. Although it is straightforward and effective to use neural models to help NER, it is expected to incorporate more useful features into an NER system. Among all such features, syntactic ones, such as part-of-speech (POS) labels, syntactic constituents, dependency relations, are of high importance to NER because they are effective in identifying the inherited structure in a piece of text and thus guide the system to f"
2020.findings-emnlp.378,P19-1233,0,0.26676,"conventionally regarded as a sequence labeling task with models such as hidden Markov model (HMM) (Bikel * Equal contribution. Corresponding author. 1 The code and the best performing models are available at https://github.com/cuhksz-nlp/AESINER † et al., 1997) and conditional random field (CRF) (McCallum and Li, 2003) applied to it in previous studies. Recently, neural models play a dominate role in this task and illustrated promising results (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Strubell et al., 2017; Yadav and Bethard, 2018; Chen et al., 2019; Jie and Lu, 2019; Liu et al., 2019d; Baevski et al., 2019), because they are powerful in encoding contextual information and thus drive NER systems to better understand the text and recognize NEs in the input text. Although it is straightforward and effective to use neural models to help NER, it is expected to incorporate more useful features into an NER system. Among all such features, syntactic ones, such as part-of-speech (POS) labels, syntactic constituents, dependency relations, are of high importance to NER because they are effective in identifying the inherited structure in a piece of text and thus guide the system to f"
2020.findings-emnlp.378,2020.acl-main.528,0,0.0616566,"Missing"
2020.findings-emnlp.378,P14-5010,0,0.00330855,"78 89.86 54.68 54.61 54.56 54.79 49.81 49.89 49.96 50.21 79.92 80.29 80.41 80.65 96.19 96.23 96.31 96.43 68.94 69.01 68.76 69.37 D EP. √ √ √ Table 3: F 1 scores of our models with different combinations of syntactic information. “T YPE” indicates how they are combined, where “DC” and “SA” refer to direct concatenation and syntax attention, respectively. default settings, i.e., 24 layers of self-attention with 1024 dimensional embeddings for BERT-large and 12 layers of self-attention with 768 dimensional embeddings for ZEN-base. For syntactic information, we use the Stanford CoreNLP Toolkit11 (Manning et al., 2014) to produce the aforementioned three types of syntactic information, i.e. POS labels, syntactic constituents, and dependency relations, for each input text. In the context encoding layer, we use a two-layer Adapted-Transformer encoder12 with 128 hidden units and 12 heads and set the dropout rate to 0.2. For the memory module, all key and value embeddings are initialized randomly. During the training process, we fix all pretrained embeddings and use Adam (Kingma and Ba, 2015) to optimize negative log-likelihood loss function with the learning rate set to η = 0.0001, β1 = 0.9 and β2 = 0.99. In a"
2020.findings-emnlp.378,D17-1159,0,0.0355709,"nformation in POS labels (a), syntactic constituents (b), and dependency relations (c) for “Salt” in the example sentence, where associated contextual features and the corresponding instances of syntactic information are highlighted in blue. 2.1 Syntactic Information Extraction A good representation of the input text is the key to obtain good model performance for many NLP tasks (Song et al., 2017; Sileo et al., 2019). Normally, a straightforward way to improve model performance is to enhance text representation by embeddings of extra features, which is demonstrated to be useful across tasks (Marcheggiani and Titov, 2017; Song et al., 2018a; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020c), including NER (Zhang and Yang, 2018; Seyler et al., 2018; Sui et al., 2019; Gui et al., 2019b,a; Liu et al., 2019b). Among different types of extra features, the syntactic one has been proved to be helpful in previous studies for NER, where the effectiveness of POS labels, syntactic constituents, and dependency relations, are demonstrated by McCallum (2003), Li et al. (2017), and Cetoli et al. (2018), respectively. In this paper, we also focus on these three types of syntactic information. In doing so, we ob"
2020.findings-emnlp.378,P19-2026,0,0.0456365,"Missing"
2020.findings-emnlp.378,W03-0430,0,0.0698727,"as locations, organizations, person names, etc., in running texts, and thus plays an important role in downstream NLP applications including question answering (Pang et al., 2019), semantic parsing (Dong and Lapata, 2018) and entity linking (Martins et al., 2019), etc. The main methodology for NER is conventionally regarded as a sequence labeling task with models such as hidden Markov model (HMM) (Bikel * Equal contribution. Corresponding author. 1 The code and the best performing models are available at https://github.com/cuhksz-nlp/AESINER † et al., 1997) and conditional random field (CRF) (McCallum and Li, 2003) applied to it in previous studies. Recently, neural models play a dominate role in this task and illustrated promising results (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Strubell et al., 2017; Yadav and Bethard, 2018; Chen et al., 2019; Jie and Lu, 2019; Liu et al., 2019d; Baevski et al., 2019), because they are powerful in encoding contextual information and thus drive NER systems to better understand the text and recognize NEs in the input text. Although it is straightforward and effective to use neural models to help NER, it is expected to incorporate more useful fea"
2020.findings-emnlp.378,D16-1147,0,0.250751,", ViD = [Salt_comp., City_NP] City_root] Figure 1: The overall architecture of the proposed NER model integrated with attentive ensemble of different syntactic information. An example input sentence and its output entity labels are given and the syntactic information for the word “Salt” are illustrated with their processing through KVMN, syntax attention and the gate mechanism. In this paper, we propose a sequence labeling based neural model to enhance NER by incorporating different types of syntactic information, which is conducted by attentive ensemble with key-value memory networks (KVMN) (Miller et al., 2016), syntax attention and the gate mechanism. Particularly, the KVMN is applied to encode the context features and their syntax information from different types, e.g., POS labels, syntactic constituents, or dependency relations; syntax attention is proposed to weight different types of such syntactic information, and the gate mechanism controls the contribution of the results from the context encoding and the syntax attention to the NER process. Through the attentive ensemble, important syntactic information is highlighted and emphasized during labeling NEs. In addition, to further improve NER pe"
2020.findings-emnlp.378,I17-2049,0,0.0871176,"rt model performance if it is not leveraged appropriately. Inspired by the studies that 2 There are 10 accepted constituent nodes, including NP, VP, PP, ADVP, SBAR, ADJP, PRT, INTJ, CONJP and LST, which are selected from the types used in the CoNLL-2003 shared task (Sang and Meulder, 2003). 3 Note that, in this case, we do not have context features selected from the dependents since “Salt” do not have any dependents according to the dependency parse result. 4233 use KVMN and its variants to weight and leverage extra features to enhance model performance in many NLP tasks (Miller et al., 2016; Mino et al., 2017; Xu et al., 2019b; Tian et al., 2020d), for each type of the syntactic information (denoted as c), we propose a KVMN module (Mc ) to model the pair-wisely organized context features and the syntactic information instances. Specifically, for each xi in the input, we firstly map its context features and the syntactic information to keys and values in the KVMN, which are dec , . . . , kc , . . . , kc c noted by Kic = [ki,1 i,j i,mi ] and Vi = c , . . . , vc , . . . , vc [vi,1 i,j i,mi ], respectively, with mi the number of context features for xi . Next, we use two matrices to map them to their"
2020.findings-emnlp.378,P05-3015,0,0.102119,"Peters et al., 2018; Yadav and Bethard, 2018; Cetoli et al., 2018; Akbik et al., 2018, 2019; Chen et al., 2019; Devlin et al., 2019; Zhu and Wang, 2019; Liu et al., 2019b; Baevski et al., 2019; Yan et al., 2019; Xu et al., 2019a; Zhu et al., 2020; Luo et al.). However, to enhance NER, it is straightforward to incorporate more knowledge to it than only modeling from contexts. Therefore, additional resources such as knowledge base (Kazama and Torisawa, 2008; Tkachenko and Simanovsky, 2012; Seyler et al., 2018; Liu et al., 2019b,a; Gui et al., 2019b,a) and syntactic information (McCallum, 2003; Mohit and Hwa, 2005; Finkel and Manning, 2009; Li et al., 2017; Luo et al., 2018; Cetoli et al., 2018; Jie and Lu, 2019) are applied in previous studies. Particularly, Luo et al. (2018) and Dang et al. (2018) 19 “Bill” is part of the “PERSON” entity in most cases. Conclusion In this paper, we proposed a neural model following the sequence labeling paradigm to enhance NER through attentive ensemble of syntactic information. Particularly, the attentive ensemble consists of three components in a sequence: each type of syntactic information is encoded by key-value memory networks, different information types are the"
2020.findings-emnlp.378,D15-1064,0,0.0541266,"Missing"
2020.findings-emnlp.378,D14-1162,0,0.0822318,"Missing"
2020.findings-emnlp.378,N18-1202,0,0.377352,"three Chinese datasets, i.e., OntoNotes 4.0 (ON4c) (Weischedel et al., 2011), Resume (RE) (Zhang and Yang, 2018), Weibo (WE) (Peng and Dredze, to be useful for NER comparing to the vanilla Transformer. Implementation To label NEs, we use the BIOES tagging scheme instead of the standard BIO scheme for the reason that previous studies have shown optimistic improvement with this scheme (Lample et al., 2016; Yan et al., 2019). For the text input, we use three types of embeddings for each language by default. Specifically, for English, we use Glove (100dimension)6 (Pennington et al., 2014), ELMo (Peters et al., 2018), and the BERT-cased large7 (Devlin et al., 2019) (the derived embeddings for each word); for Chinese, we use pre-trained character and bi-gram embeddings8 released by Zhang and Yang (2018) (denoted as Giga), Tencent Embedding9 (Song et al., 2018b), and ZEN10 (Diao et al., 2019). For both BERT and ZEN, we follow their 5 Among these datasets, ON5e and ON4c are multi-lingual datasets. We follow Yan et al. (2019) by extracting the corresponding English and Chinese part from them. 6 We download the Glove.6B embedding from https: //nlp.stanford.edu/projects/glove/ 7 We obtain the pre-trained BERT f"
2020.findings-emnlp.378,W13-3516,0,0.0269825,"Missing"
2020.findings-emnlp.378,P18-2039,0,0.229471,"eatures and the corresponding instances of syntactic information are highlighted in blue. 2.1 Syntactic Information Extraction A good representation of the input text is the key to obtain good model performance for many NLP tasks (Song et al., 2017; Sileo et al., 2019). Normally, a straightforward way to improve model performance is to enhance text representation by embeddings of extra features, which is demonstrated to be useful across tasks (Marcheggiani and Titov, 2017; Song et al., 2018a; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020c), including NER (Zhang and Yang, 2018; Seyler et al., 2018; Sui et al., 2019; Gui et al., 2019b,a; Liu et al., 2019b). Among different types of extra features, the syntactic one has been proved to be helpful in previous studies for NER, where the effectiveness of POS labels, syntactic constituents, and dependency relations, are demonstrated by McCallum (2003), Li et al. (2017), and Cetoli et al. (2018), respectively. In this paper, we also focus on these three types of syntactic information. In doing so, we obtain the POS labels, the syntax tree and the dependency parsing results from an off-the-shelf NLP toolkit (e.g., Stanford Parser) for each inpu"
2020.findings-emnlp.378,N19-1351,0,0.0568017,"Missing"
2020.findings-emnlp.378,K17-1016,1,0.580509,"ontext Features: [Salt, City] Syntactic Info.: [Salt_NNP, is_VBZ, Lake_NNP] Syntactic Info.: [Salt_NP, Lake_NP, City_NP] Syntactic Info.: [Salt_compound, City_root] (a) (b) City (c) Figure 2: The extracted syntactic information in POS labels (a), syntactic constituents (b), and dependency relations (c) for “Salt” in the example sentence, where associated contextual features and the corresponding instances of syntactic information are highlighted in blue. 2.1 Syntactic Information Extraction A good representation of the input text is the key to obtain good model performance for many NLP tasks (Song et al., 2017; Sileo et al., 2019). Normally, a straightforward way to improve model performance is to enhance text representation by embeddings of extra features, which is demonstrated to be useful across tasks (Marcheggiani and Titov, 2017; Song et al., 2018a; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020c), including NER (Zhang and Yang, 2018; Seyler et al., 2018; Sui et al., 2019; Gui et al., 2019b,a; Liu et al., 2019b). Among different types of extra features, the syntactic one has been proved to be helpful in previous studies for NER, where the effectiveness of POS labels, syntactic c"
2020.findings-emnlp.378,N18-2028,1,0.85075,"syntactic constituents (b), and dependency relations (c) for “Salt” in the example sentence, where associated contextual features and the corresponding instances of syntactic information are highlighted in blue. 2.1 Syntactic Information Extraction A good representation of the input text is the key to obtain good model performance for many NLP tasks (Song et al., 2017; Sileo et al., 2019). Normally, a straightforward way to improve model performance is to enhance text representation by embeddings of extra features, which is demonstrated to be useful across tasks (Marcheggiani and Titov, 2017; Song et al., 2018a; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020c), including NER (Zhang and Yang, 2018; Seyler et al., 2018; Sui et al., 2019; Gui et al., 2019b,a; Liu et al., 2019b). Among different types of extra features, the syntactic one has been proved to be helpful in previous studies for NER, where the effectiveness of POS labels, syntactic constituents, and dependency relations, are demonstrated by McCallum (2003), Li et al. (2017), and Cetoli et al. (2018), respectively. In this paper, we also focus on these three types of syntactic information. In doing so, we obtain the POS labels"
2020.findings-emnlp.378,D17-1283,0,0.101681,"2018) and entity linking (Martins et al., 2019), etc. The main methodology for NER is conventionally regarded as a sequence labeling task with models such as hidden Markov model (HMM) (Bikel * Equal contribution. Corresponding author. 1 The code and the best performing models are available at https://github.com/cuhksz-nlp/AESINER † et al., 1997) and conditional random field (CRF) (McCallum and Li, 2003) applied to it in previous studies. Recently, neural models play a dominate role in this task and illustrated promising results (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Strubell et al., 2017; Yadav and Bethard, 2018; Chen et al., 2019; Jie and Lu, 2019; Liu et al., 2019d; Baevski et al., 2019), because they are powerful in encoding contextual information and thus drive NER systems to better understand the text and recognize NEs in the input text. Although it is straightforward and effective to use neural models to help NER, it is expected to incorporate more useful features into an NER system. Among all such features, syntactic ones, such as part-of-speech (POS) labels, syntactic constituents, dependency relations, are of high importance to NER because they are effective in ident"
2020.findings-emnlp.378,D19-1396,0,0.0114208,"sponding instances of syntactic information are highlighted in blue. 2.1 Syntactic Information Extraction A good representation of the input text is the key to obtain good model performance for many NLP tasks (Song et al., 2017; Sileo et al., 2019). Normally, a straightforward way to improve model performance is to enhance text representation by embeddings of extra features, which is demonstrated to be useful across tasks (Marcheggiani and Titov, 2017; Song et al., 2018a; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020c), including NER (Zhang and Yang, 2018; Seyler et al., 2018; Sui et al., 2019; Gui et al., 2019b,a; Liu et al., 2019b). Among different types of extra features, the syntactic one has been proved to be helpful in previous studies for NER, where the effectiveness of POS labels, syntactic constituents, and dependency relations, are demonstrated by McCallum (2003), Li et al. (2017), and Cetoli et al. (2018), respectively. In this paper, we also focus on these three types of syntactic information. In doing so, we obtain the POS labels, the syntax tree and the dependency parsing results from an off-the-shelf NLP toolkit (e.g., Stanford Parser) for each input sequence X . The"
2020.findings-emnlp.378,2020.acl-main.735,1,0.917278,"lt” in the example sentence, where associated contextual features and the corresponding instances of syntactic information are highlighted in blue. 2.1 Syntactic Information Extraction A good representation of the input text is the key to obtain good model performance for many NLP tasks (Song et al., 2017; Sileo et al., 2019). Normally, a straightforward way to improve model performance is to enhance text representation by embeddings of extra features, which is demonstrated to be useful across tasks (Marcheggiani and Titov, 2017; Song et al., 2018a; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020c), including NER (Zhang and Yang, 2018; Seyler et al., 2018; Sui et al., 2019; Gui et al., 2019b,a; Liu et al., 2019b). Among different types of extra features, the syntactic one has been proved to be helpful in previous studies for NER, where the effectiveness of POS labels, syntactic constituents, and dependency relations, are demonstrated by McCallum (2003), Li et al. (2017), and Cetoli et al. (2018), respectively. In this paper, we also focus on these three types of syntactic information. In doing so, we obtain the POS labels, the syntax tree and the dependency parsing results from an off"
2020.findings-emnlp.378,2020.emnlp-main.487,1,0.856055,"lt” in the example sentence, where associated contextual features and the corresponding instances of syntactic information are highlighted in blue. 2.1 Syntactic Information Extraction A good representation of the input text is the key to obtain good model performance for many NLP tasks (Song et al., 2017; Sileo et al., 2019). Normally, a straightforward way to improve model performance is to enhance text representation by embeddings of extra features, which is demonstrated to be useful across tasks (Marcheggiani and Titov, 2017; Song et al., 2018a; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020c), including NER (Zhang and Yang, 2018; Seyler et al., 2018; Sui et al., 2019; Gui et al., 2019b,a; Liu et al., 2019b). Among different types of extra features, the syntactic one has been proved to be helpful in previous studies for NER, where the effectiveness of POS labels, syntactic constituents, and dependency relations, are demonstrated by McCallum (2003), Li et al. (2017), and Cetoli et al. (2018), respectively. In this paper, we also focus on these three types of syntactic information. In doing so, we obtain the POS labels, the syntax tree and the dependency parsing results from an off"
2020.findings-emnlp.378,2020.findings-emnlp.153,1,0.892955,"lt” in the example sentence, where associated contextual features and the corresponding instances of syntactic information are highlighted in blue. 2.1 Syntactic Information Extraction A good representation of the input text is the key to obtain good model performance for many NLP tasks (Song et al., 2017; Sileo et al., 2019). Normally, a straightforward way to improve model performance is to enhance text representation by embeddings of extra features, which is demonstrated to be useful across tasks (Marcheggiani and Titov, 2017; Song et al., 2018a; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020c), including NER (Zhang and Yang, 2018; Seyler et al., 2018; Sui et al., 2019; Gui et al., 2019b,a; Liu et al., 2019b). Among different types of extra features, the syntactic one has been proved to be helpful in previous studies for NER, where the effectiveness of POS labels, syntactic constituents, and dependency relations, are demonstrated by McCallum (2003), Li et al. (2017), and Cetoli et al. (2018), respectively. In this paper, we also focus on these three types of syntactic information. In doing so, we obtain the POS labels, the syntax tree and the dependency parsing results from an off"
2020.findings-emnlp.378,2020.acl-main.734,1,0.774228,"lt” in the example sentence, where associated contextual features and the corresponding instances of syntactic information are highlighted in blue. 2.1 Syntactic Information Extraction A good representation of the input text is the key to obtain good model performance for many NLP tasks (Song et al., 2017; Sileo et al., 2019). Normally, a straightforward way to improve model performance is to enhance text representation by embeddings of extra features, which is demonstrated to be useful across tasks (Marcheggiani and Titov, 2017; Song et al., 2018a; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020c), including NER (Zhang and Yang, 2018; Seyler et al., 2018; Sui et al., 2019; Gui et al., 2019b,a; Liu et al., 2019b). Among different types of extra features, the syntactic one has been proved to be helpful in previous studies for NER, where the effectiveness of POS labels, syntactic constituents, and dependency relations, are demonstrated by McCallum (2003), Li et al. (2017), and Cetoli et al. (2018), respectively. In this paper, we also focus on these three types of syntactic information. In doing so, we obtain the POS labels, the syntax tree and the dependency parsing results from an off"
2020.findings-emnlp.378,N19-1301,0,0.222611,"e if it is not leveraged appropriately. Inspired by the studies that 2 There are 10 accepted constituent nodes, including NP, VP, PP, ADVP, SBAR, ADJP, PRT, INTJ, CONJP and LST, which are selected from the types used in the CoNLL-2003 shared task (Sang and Meulder, 2003). 3 Note that, in this case, we do not have context features selected from the dependents since “Salt” do not have any dependents according to the dependency parse result. 4233 use KVMN and its variants to weight and leverage extra features to enhance model performance in many NLP tasks (Miller et al., 2016; Mino et al., 2017; Xu et al., 2019b; Tian et al., 2020d), for each type of the syntactic information (denoted as c), we propose a KVMN module (Mc ) to model the pair-wisely organized context features and the syntactic information instances. Specifically, for each xi in the input, we firstly map its context features and the syntactic information to keys and values in the KVMN, which are dec , . . . , kc , . . . , kc c noted by Kic = [ki,1 i,j i,mi ] and Vi = c , . . . , vc , . . . , vc [vi,1 i,j i,mi ], respectively, with mi the number of context features for xi . Next, we use two matrices to map them to their embeddings, c and"
2020.findings-emnlp.378,C18-1182,0,0.134615,"g (Martins et al., 2019), etc. The main methodology for NER is conventionally regarded as a sequence labeling task with models such as hidden Markov model (HMM) (Bikel * Equal contribution. Corresponding author. 1 The code and the best performing models are available at https://github.com/cuhksz-nlp/AESINER † et al., 1997) and conditional random field (CRF) (McCallum and Li, 2003) applied to it in previous studies. Recently, neural models play a dominate role in this task and illustrated promising results (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Strubell et al., 2017; Yadav and Bethard, 2018; Chen et al., 2019; Jie and Lu, 2019; Liu et al., 2019d; Baevski et al., 2019), because they are powerful in encoding contextual information and thus drive NER systems to better understand the text and recognize NEs in the input text. Although it is straightforward and effective to use neural models to help NER, it is expected to incorporate more useful features into an NER system. Among all such features, syntactic ones, such as part-of-speech (POS) labels, syntactic constituents, dependency relations, are of high importance to NER because they are effective in identifying the inherited stru"
2020.findings-emnlp.378,N19-1093,1,0.883892,"Missing"
2020.findings-emnlp.378,P19-1336,0,0.0266855,"Missing"
2020.findings-emnlp.378,N19-1342,0,0.0213628,"tic information with attentive ensemble to encode, weight and select them to help NER, which proves its effectiveness and has the potential to be applied in other similar tasks. 6 7 Related Work Recently, neural models play dominant roles in NER because of their effectiveness in capturing contextual information in the text without requiring to extract manually crafted features (Huang et al., 2015; Lample et al., 2016; Strubell et al., 2017; Zhang and Yang, 2018; Peters et al., 2018; Yadav and Bethard, 2018; Cetoli et al., 2018; Akbik et al., 2018, 2019; Chen et al., 2019; Devlin et al., 2019; Zhu and Wang, 2019; Liu et al., 2019b; Baevski et al., 2019; Yan et al., 2019; Xu et al., 2019a; Zhu et al., 2020; Luo et al.). However, to enhance NER, it is straightforward to incorporate more knowledge to it than only modeling from contexts. Therefore, additional resources such as knowledge base (Kazama and Torisawa, 2008; Tkachenko and Simanovsky, 2012; Seyler et al., 2018; Liu et al., 2019b,a; Gui et al., 2019b,a) and syntactic information (McCallum, 2003; Mohit and Hwa, 2005; Finkel and Manning, 2009; Li et al., 2017; Luo et al., 2018; Cetoli et al., 2018; Jie and Lu, 2019) are applied in previous studies"
2020.findings-emnlp.425,Q17-1010,0,0.221932,"asks. In this experiment, ZEN and BERT use two settings, i.e., training from (R): randomly initialized parameters and (P): pre-trained model, which is the Google released Chinese BERT base model. The results are reported in Table 2, with the performance on both development12 and test set for each task presented in the table. Overall, in both R and P settings, ZEN outperforms BERT in all seven tasks, which clearly indicates the advantage of introducing n-grams into the encoding of character sequences.13 This observation is similar to that from Dos Santos and Gatti (2014); Lample et al. (2016); Bojanowski et al. (2017); Liu et al. (2019a). In detail, when compare R and P settings, 12 Most of the previous studies show their performance on the development set of the aforementioned tasks and we follow them to do so in order to provide a reference and comparison. 13 There are other studies that demonstrate the effectiveness of ZEN on CWS (Tian et al., 2020c), POS tagging (Tian et al., 2020a), constituency parsing (Tian et al., 2020b), and NER (Nie et al., 2020a,b), in which their models equipped with ZEN encoder consistently outperform the ones with BERT. 4733 CWS T EST BERT (R) ZEN (R) 95.14 96.05 POS D EV T E"
2020.findings-emnlp.425,W19-4828,0,0.0175102,"represent all n-grams, whose information are thus encoded in different levels matching the correspondent layers in BERT. We adopt Transformer (Vaswani et al., 2017) as the encoder, which is a multi-layer encoder that can model the interactions among all n-grams through their representations in each layer. This modeling power is of high importance for ZEN because for certain context, salient n-grams are more useful than random others, and such salient n-grams are expected to be emphasized in pre-training. This effect can be achieved by multi-head self-attention (MhA) mechanism in Transformer (Clark et al., 2019). In detail, the transformer for n-grams is the same as its original version for sequence modeling, except that it does not encode n-gram positions because all n-grams are treated equally without a sequential order. Each n-gram extracted for the input is represented by an embedding from the n-gram embedding matrix. Therefore, for all extract n-grams, we obtain the j-th n-gram embedding ej as the input and denote (l) it in layer l of the n-gram encoder by µj , and formulate the encoding process across layers by MhA (l+1) µj (l) = M hA(Q = µj , K = V = U (l) ) (2) (l) where µj is used as the que"
2020.findings-emnlp.425,D18-1269,0,0.0193153,"onal Chinese language processing Bakeoff 20066 . • Document classification (DC): THUCNews (News) dataset (Sun et al., 2016) from Sina news with 10 evenly distributed classes. • Sentiment analysis (SA): The ChnSentiCorp7 (CSC) dataset with 12,000 documents from three domains, i.e., book, computer and hotel. • Sentence pair matching (SPM): The LCQMC (a large-scale Chinese question matching corpus) proposed by Liu et al. (2018), where each instance is a pair of two sentences with a label indicating whether their intent is matched. • Natural language inference (NLI): The Chinese part of the XNLI (Conneau et al., 2018). The statistics of these datasets with respect to their splits are reported in Table 1. For CWS, POS, we fine-tune and test according to their standard split of training and test sets. For the other tasks, we follow the settings of Cui et al. (2019) to process those datasets in our experiments. 3.2 Implementation N-grams to build the lexicon L are extracted8 from the same training corpus, i.e., Chinese Wikipedia https://dumps.wikimedia.org/zhwiki/ 4732 6 http://sighan.cs.uchicago.edu/bakeoff2006/ https://github.com/pengming617/bert classification 8 The extraction can be conducted by various m"
2020.findings-emnlp.425,C14-1008,0,0.0505423,"o their performance on the aforementioned NLP tasks. In this experiment, ZEN and BERT use two settings, i.e., training from (R): randomly initialized parameters and (P): pre-trained model, which is the Google released Chinese BERT base model. The results are reported in Table 2, with the performance on both development12 and test set for each task presented in the table. Overall, in both R and P settings, ZEN outperforms BERT in all seven tasks, which clearly indicates the advantage of introducing n-grams into the encoding of character sequences.13 This observation is similar to that from Dos Santos and Gatti (2014); Lample et al. (2016); Bojanowski et al. (2017); Liu et al. (2019a). In detail, when compare R and P settings, 12 Most of the previous studies show their performance on the development set of the aforementioned tasks and we follow them to do so in order to provide a reference and comparison. 13 There are other studies that demonstrate the effectiveness of ZEN on CWS (Tian et al., 2020c), POS tagging (Tian et al., 2020a), constituency parsing (Tian et al., 2020b), and NER (Nie et al., 2020a,b), in which their models equipped with ZEN encoder consistently outperform the ones with BERT. 4733 CWS"
2020.findings-emnlp.425,I05-3017,0,0.112193,"n different encoders including ZEN. To clean the base corpus, we remove useless symbols and translate all traditional characters into simplified ones, and 5 lowercase all English letters. The resulted corpus contains 474M tokens and 23K unique characters. For fine-tuning, we choose seven NLP tasks and their corresponding benchmark datasets in our experiments, many of them have been used in previous studies (Cui et al., 2019; Sun et al., 2019a,b). These tasks and datasets are described as follows. • Chinese word segmentation (CWS): MSR dataset from SIGHAN2005 Chinese word segmentation Bakeoff (Emerson, 2005). • Part-of-speech (POS) tagging: CTB5 (Xue et al., 2005) dataset with standard splits. • Named entity recognition (NER): MSRA dataset from international Chinese language processing Bakeoff 20066 . • Document classification (DC): THUCNews (News) dataset (Sun et al., 2016) from Sina news with 10 evenly distributed classes. • Sentiment analysis (SA): The ChnSentiCorp7 (CSC) dataset with 12,000 documents from three domains, i.e., book, computer and hotel. • Sentence pair matching (SPM): The LCQMC (a large-scale Chinese question matching corpus) proposed by Liu et al. (2018), where each instance i"
2020.findings-emnlp.425,N19-1276,0,0.20166,"s well explained in previous studies (Devlin et al., 2018; Yu and Jiang, 2019), in this paper, we focus on the details of ZEN, by explaining how n-grams are processed and incorporated into the character encoder. 2.1 N-gram Extraction A high quality of text representation plays an important role to obtain good performance for many NLP tasks (Song et al., 2017; Zhu et al., 2019; Liu and Lapata, 2019), where a powerful encoder is required to model more contextual information. Inspired by the studies (Song et al., 2009; Song and Xia, 2012; Ouyang et al., 2017; Kim et al., 2018; Peng et al., 2018; Higashiyama et al., 2019; Tian et al., 2020c; Li et al., 2020) that leverage the large granularity contextual information carried by n-grams to enhance text representation for Chinese, we propose ZEN to enhance character based text encoders (e.g., BERT) by leveraging ngrams. In doing so, we extract n-grams prior to pre-training ZEN through two different steps. The first step is to prepare an n-gram lexicon (denoted as L), from which one can use any unsupervised method to extract n-grams from large corpora for later processing. The second step of n-gram extraction is performed during pre-training, where some n-grams i"
2020.findings-emnlp.425,W18-6120,0,0.0197703,"er encoder. Since the basis of BERT is well explained in previous studies (Devlin et al., 2018; Yu and Jiang, 2019), in this paper, we focus on the details of ZEN, by explaining how n-grams are processed and incorporated into the character encoder. 2.1 N-gram Extraction A high quality of text representation plays an important role to obtain good performance for many NLP tasks (Song et al., 2017; Zhu et al., 2019; Liu and Lapata, 2019), where a powerful encoder is required to model more contextual information. Inspired by the studies (Song et al., 2009; Song and Xia, 2012; Ouyang et al., 2017; Kim et al., 2018; Peng et al., 2018; Higashiyama et al., 2019; Tian et al., 2020c; Li et al., 2020) that leverage the large granularity contextual information carried by n-grams to enhance text representation for Chinese, we propose ZEN to enhance character based text encoders (e.g., BERT) by leveraging ngrams. In doing so, we extract n-grams prior to pre-training ZEN through two different steps. The first step is to prepare an n-gram lexicon (denoted as L), from which one can use any unsupervised method to extract n-grams from large corpora for later processing. The second step of n-gram extraction is perfor"
2020.findings-emnlp.425,N16-1030,0,0.0475055,"e aforementioned NLP tasks. In this experiment, ZEN and BERT use two settings, i.e., training from (R): randomly initialized parameters and (P): pre-trained model, which is the Google released Chinese BERT base model. The results are reported in Table 2, with the performance on both development12 and test set for each task presented in the table. Overall, in both R and P settings, ZEN outperforms BERT in all seven tasks, which clearly indicates the advantage of introducing n-grams into the encoding of character sequences.13 This observation is similar to that from Dos Santos and Gatti (2014); Lample et al. (2016); Bojanowski et al. (2017); Liu et al. (2019a). In detail, when compare R and P settings, 12 Most of the previous studies show their performance on the development set of the aforementioned tasks and we follow them to do so in order to provide a reference and comparison. 13 There are other studies that demonstrate the effectiveness of ZEN on CWS (Tian et al., 2020c), POS tagging (Tian et al., 2020a), constituency parsing (Tian et al., 2020b), and NER (Nie et al., 2020a,b), in which their models equipped with ZEN encoder consistently outperform the ones with BERT. 4733 CWS T EST BERT (R) ZEN (R"
2020.findings-emnlp.425,2020.acl-main.611,0,0.0632082,"t al., 2018; Yu and Jiang, 2019), in this paper, we focus on the details of ZEN, by explaining how n-grams are processed and incorporated into the character encoder. 2.1 N-gram Extraction A high quality of text representation plays an important role to obtain good performance for many NLP tasks (Song et al., 2017; Zhu et al., 2019; Liu and Lapata, 2019), where a powerful encoder is required to model more contextual information. Inspired by the studies (Song et al., 2009; Song and Xia, 2012; Ouyang et al., 2017; Kim et al., 2018; Peng et al., 2018; Higashiyama et al., 2019; Tian et al., 2020c; Li et al., 2020) that leverage the large granularity contextual information carried by n-grams to enhance text representation for Chinese, we propose ZEN to enhance character based text encoders (e.g., BERT) by leveraging ngrams. In doing so, we extract n-grams prior to pre-training ZEN through two different steps. The first step is to prepare an n-gram lexicon (denoted as L), from which one can use any unsupervised method to extract n-grams from large corpora for later processing. The second step of n-gram extraction is performed during pre-training, where some n-grams in L are selected according to each tra"
2020.findings-emnlp.425,N15-1142,0,0.0260101,"est that ZEN provides a potential solution to some text analyzing tasks, e.g., chunking and keyphrase extraction. 6 Related Work ZEN thus provides an alternative solution that explicitly encodes n-grams into character-based encoding, rather than through weak supervision, i.e., masking, to incorporate word/phrase information. 7 Representation learning of text attracts much attention in recent years, with the rise of deep learning in NLP (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014). There are considerable interests in representing text with contextualized information (Ling et al., 2015; Melamud et al., 2016; Bojanowski et al., 2017; Song et al., 2018; Peters et al., 2018a). Following this paradigm, pre-trained models have been proposed and are proven useful in many NLP tasks (Devlin et al., 2018; Radford et al., 2018, 2019; Yang et al., 2019; Liu et al., 2019c). In detail, such models can be categorized into two types: autoregressive and autoencoding encoders. The former models behave like normal language models that predict the probability distributions of text units following observed texts. These models, such as GPT (Radford et al., 2018) and GPT2 (Radford et al., 2019),"
2020.findings-emnlp.425,N19-1247,0,0.173427,"ZEN and BERT use two settings, i.e., training from (R): randomly initialized parameters and (P): pre-trained model, which is the Google released Chinese BERT base model. The results are reported in Table 2, with the performance on both development12 and test set for each task presented in the table. Overall, in both R and P settings, ZEN outperforms BERT in all seven tasks, which clearly indicates the advantage of introducing n-grams into the encoding of character sequences.13 This observation is similar to that from Dos Santos and Gatti (2014); Lample et al. (2016); Bojanowski et al. (2017); Liu et al. (2019a). In detail, when compare R and P settings, 12 Most of the previous studies show their performance on the development set of the aforementioned tasks and we follow them to do so in order to provide a reference and comparison. 13 There are other studies that demonstrate the effectiveness of ZEN on CWS (Tian et al., 2020c), POS tagging (Tian et al., 2020a), constituency parsing (Tian et al., 2020b), and NER (Nie et al., 2020a,b), in which their models equipped with ZEN encoder consistently outperform the ones with BERT. 4733 CWS T EST BERT (R) ZEN (R) 95.14 96.05 POS D EV T EST 93.64 93.79 93."
2020.findings-emnlp.425,C18-1166,0,0.0244378,"Missing"
2020.findings-emnlp.425,D19-1387,0,0.0247169,"verall architecture of ZEN is shown in Figure 1, where the backbone model (character encoder) is BERT4 (Devlin et al., 2018), enhanced by n-gram information represented by a multi-layer encoder. Since the basis of BERT is well explained in previous studies (Devlin et al., 2018; Yu and Jiang, 2019), in this paper, we focus on the details of ZEN, by explaining how n-grams are processed and incorporated into the character encoder. 2.1 N-gram Extraction A high quality of text representation plays an important role to obtain good performance for many NLP tasks (Song et al., 2017; Zhu et al., 2019; Liu and Lapata, 2019), where a powerful encoder is required to model more contextual information. Inspired by the studies (Song et al., 2009; Song and Xia, 2012; Ouyang et al., 2017; Kim et al., 2018; Peng et al., 2018; Higashiyama et al., 2019; Tian et al., 2020c; Li et al., 2020) that leverage the large granularity contextual information carried by n-grams to enhance text representation for Chinese, we propose ZEN to enhance character based text encoders (e.g., BERT) by leveraging ngrams. In doing so, we extract n-grams prior to pre-training ZEN through two different steps. The first step is to prepare an n-gram"
2020.findings-emnlp.425,2021.ccl-1.108,0,0.0949615,"Missing"
2020.findings-emnlp.425,K16-1006,0,0.0136696,"es a potential solution to some text analyzing tasks, e.g., chunking and keyphrase extraction. 6 Related Work ZEN thus provides an alternative solution that explicitly encodes n-grams into character-based encoding, rather than through weak supervision, i.e., masking, to incorporate word/phrase information. 7 Representation learning of text attracts much attention in recent years, with the rise of deep learning in NLP (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014). There are considerable interests in representing text with contextualized information (Ling et al., 2015; Melamud et al., 2016; Bojanowski et al., 2017; Song et al., 2018; Peters et al., 2018a). Following this paradigm, pre-trained models have been proposed and are proven useful in many NLP tasks (Devlin et al., 2018; Radford et al., 2018, 2019; Yang et al., 2019; Liu et al., 2019c). In detail, such models can be categorized into two types: autoregressive and autoencoding encoders. The former models behave like normal language models that predict the probability distributions of text units following observed texts. These models, such as GPT (Radford et al., 2018) and GPT2 (Radford et al., 2019), are trained to encode"
2020.findings-emnlp.425,2020.findings-emnlp.378,1,0.826563,"g n-grams into the encoding of character sequences.13 This observation is similar to that from Dos Santos and Gatti (2014); Lample et al. (2016); Bojanowski et al. (2017); Liu et al. (2019a). In detail, when compare R and P settings, 12 Most of the previous studies show their performance on the development set of the aforementioned tasks and we follow them to do so in order to provide a reference and comparison. 13 There are other studies that demonstrate the effectiveness of ZEN on CWS (Tian et al., 2020c), POS tagging (Tian et al., 2020a), constituency parsing (Tian et al., 2020b), and NER (Nie et al., 2020a,b), in which their models equipped with ZEN encoder consistently outperform the ones with BERT. 4733 CWS T EST BERT (R) ZEN (R) 95.14 96.05 POS D EV T EST 93.64 93.79 93.23 93.37 NER T EST 87.11 88.39 DC D EV T EST 96.02 96.11 95.77 96.05 SA D EV T EST 93.41 93.92 92.33 93.51 SPM D EV T EST 85.62 86.12 85.53 85.78 NLI D EV T EST 72.12 72.66 71.44 72.31 Table 3: The performance of BERT and ZEN on seven NLP tasks when they are trained on a small corpus. the performance gap between ZEN (P) and BERT (P) is larger than that in their R setting, which illustrates that learning an encoder with relia"
2020.findings-emnlp.425,W09-3511,1,0.710411,"), enhanced by n-gram information represented by a multi-layer encoder. Since the basis of BERT is well explained in previous studies (Devlin et al., 2018; Yu and Jiang, 2019), in this paper, we focus on the details of ZEN, by explaining how n-grams are processed and incorporated into the character encoder. 2.1 N-gram Extraction A high quality of text representation plays an important role to obtain good performance for many NLP tasks (Song et al., 2017; Zhu et al., 2019; Liu and Lapata, 2019), where a powerful encoder is required to model more contextual information. Inspired by the studies (Song et al., 2009; Song and Xia, 2012; Ouyang et al., 2017; Kim et al., 2018; Peng et al., 2018; Higashiyama et al., 2019; Tian et al., 2020c; Li et al., 2020) that leverage the large granularity contextual information carried by n-grams to enhance text representation for Chinese, we propose ZEN to enhance character based text encoders (e.g., BERT) by leveraging ngrams. In doing so, we extract n-grams prior to pre-training ZEN through two different steps. The first step is to prepare an n-gram lexicon (denoted as L), from which one can use any unsupervised method to extract n-grams from large corpora for later"
2020.findings-emnlp.425,2020.emnlp-main.107,1,0.843238,"g n-grams into the encoding of character sequences.13 This observation is similar to that from Dos Santos and Gatti (2014); Lample et al. (2016); Bojanowski et al. (2017); Liu et al. (2019a). In detail, when compare R and P settings, 12 Most of the previous studies show their performance on the development set of the aforementioned tasks and we follow them to do so in order to provide a reference and comparison. 13 There are other studies that demonstrate the effectiveness of ZEN on CWS (Tian et al., 2020c), POS tagging (Tian et al., 2020a), constituency parsing (Tian et al., 2020b), and NER (Nie et al., 2020a,b), in which their models equipped with ZEN encoder consistently outperform the ones with BERT. 4733 CWS T EST BERT (R) ZEN (R) 95.14 96.05 POS D EV T EST 93.64 93.79 93.23 93.37 NER T EST 87.11 88.39 DC D EV T EST 96.02 96.11 95.77 96.05 SA D EV T EST 93.41 93.92 92.33 93.51 SPM D EV T EST 85.62 86.12 85.53 85.78 NLI D EV T EST 72.12 72.66 71.44 72.31 Table 3: The performance of BERT and ZEN on seven NLP tasks when they are trained on a small corpus. the performance gap between ZEN (P) and BERT (P) is larger than that in their R setting, which illustrates that learning an encoder with relia"
2020.findings-emnlp.425,D14-1162,0,0.098061,"m this case study not only illustrates the details of how n-grams enhance 4736 the pre-train model, but also suggest that ZEN provides a potential solution to some text analyzing tasks, e.g., chunking and keyphrase extraction. 6 Related Work ZEN thus provides an alternative solution that explicitly encodes n-grams into character-based encoding, rather than through weak supervision, i.e., masking, to incorporate word/phrase information. 7 Representation learning of text attracts much attention in recent years, with the rise of deep learning in NLP (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014). There are considerable interests in representing text with contextualized information (Ling et al., 2015; Melamud et al., 2016; Bojanowski et al., 2017; Song et al., 2018; Peters et al., 2018a). Following this paradigm, pre-trained models have been proposed and are proven useful in many NLP tasks (Devlin et al., 2018; Radford et al., 2018, 2019; Yang et al., 2019; Liu et al., 2019c). In detail, such models can be categorized into two types: autoregressive and autoencoding encoders. The former models behave like normal language models that predict the probability distributions of text units f"
2020.findings-emnlp.425,N18-1202,0,0.717045,"-tuned with the character encoder (BERT). Therefore ZEN incorporates the comprehensive information of both the character sequence and words or phrases it contains. Experimental results illustrated the effectiveness of ZEN on a series of Chinese NLP tasks, where state-of-the-art results is achieved on most tasks with requiring less resource than other published encoders. It is also shown that reasonable performance is obtained when ZEN is trained on a small corpus, which is important for applying pre-training techniques to scenarios with limited data.1 1 Introduction Pre-trained text encoders (Peters et al., 2018b; Devlin et al., 2018; Radford et al., 2018, 2019; Yang et al., 2019) have drawn much attention in natural language processing (NLP), because state-of-theart performance can be obtained for many NLP tasks using such encoders. In general, these encoders are implemented by training a deep neural * Work done during the internship at Sinovation Ventures. Corresponding author. 1 The code and pre-trained models of ZEN are available at https://github.com/sinovation/ZEN. † model on large unlabeled corpora. Although the use of big data brings success to these pre-trained encoders, it is still unclear"
2020.findings-emnlp.425,D19-1005,0,0.0219483,"ormation. We also compare ZEN (P) with existing pretrained encoders on the same NLP tasks, with their results listed in the middle part of Table 2.15 Such encoders include BERT-wwm (Cui et al., 2019), ERNIE 1.0 (Sun et al., 2019a), ERNIE 2.0 (B & L) (Sun et al., 2019b), NEZHA (B & L) (Wei et al., 2019) where B and L denote the base and large BERT backbone model, respectively. Note that although there are other pre-trained encoders with exploiting entity knowledge or multi-model signals, they are not compared in this paper because external information are required in their work (e.g. KnowBERT (Peters et al., 2019)). In fact, even though without using such external information, ZEN still achieves the state-of-the-art performance on many of the tasks experimented. In general, the results clearly indicate the effectiveness of ZEN. In detail, for the comparison between ZEN and BERT-wwm, it shows that, when starting from pre-trained BERT, ZEN outperforms BERT-wwm on all tasks that BERT-wwm has re14 Such as fixed expressions and common phrases, which may have less varied meanings than other ordinary combinations of characters and random character sequences. 15 We only report the results from their conducted"
2020.findings-emnlp.425,K17-1016,1,0.871108,"ed n-grams are explicitly 2 ZEN The overall architecture of ZEN is shown in Figure 1, where the backbone model (character encoder) is BERT4 (Devlin et al., 2018), enhanced by n-gram information represented by a multi-layer encoder. Since the basis of BERT is well explained in previous studies (Devlin et al., 2018; Yu and Jiang, 2019), in this paper, we focus on the details of ZEN, by explaining how n-grams are processed and incorporated into the character encoder. 2.1 N-gram Extraction A high quality of text representation plays an important role to obtain good performance for many NLP tasks (Song et al., 2017; Zhu et al., 2019; Liu and Lapata, 2019), where a powerful encoder is required to model more contextual information. Inspired by the studies (Song et al., 2009; Song and Xia, 2012; Ouyang et al., 2017; Kim et al., 2018; Peng et al., 2018; Higashiyama et al., 2019; Tian et al., 2020c; Li et al., 2020) that leverage the large granularity contextual information carried by n-grams to enhance text representation for Chinese, we propose ZEN to enhance character based text encoders (e.g., BERT) by leveraging ngrams. In doing so, we extract n-grams prior to pre-training ZEN through two different step"
2020.findings-emnlp.425,N18-2028,1,0.86907,"tasks, e.g., chunking and keyphrase extraction. 6 Related Work ZEN thus provides an alternative solution that explicitly encodes n-grams into character-based encoding, rather than through weak supervision, i.e., masking, to incorporate word/phrase information. 7 Representation learning of text attracts much attention in recent years, with the rise of deep learning in NLP (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014). There are considerable interests in representing text with contextualized information (Ling et al., 2015; Melamud et al., 2016; Bojanowski et al., 2017; Song et al., 2018; Peters et al., 2018a). Following this paradigm, pre-trained models have been proposed and are proven useful in many NLP tasks (Devlin et al., 2018; Radford et al., 2018, 2019; Yang et al., 2019; Liu et al., 2019c). In detail, such models can be categorized into two types: autoregressive and autoencoding encoders. The former models behave like normal language models that predict the probability distributions of text units following observed texts. These models, such as GPT (Radford et al., 2018) and GPT2 (Radford et al., 2019), are trained to encode a uni-directional context. Differently, the"
2020.findings-emnlp.425,song-xia-2012-using,1,0.787241,"am information represented by a multi-layer encoder. Since the basis of BERT is well explained in previous studies (Devlin et al., 2018; Yu and Jiang, 2019), in this paper, we focus on the details of ZEN, by explaining how n-grams are processed and incorporated into the character encoder. 2.1 N-gram Extraction A high quality of text representation plays an important role to obtain good performance for many NLP tasks (Song et al., 2017; Zhu et al., 2019; Liu and Lapata, 2019), where a powerful encoder is required to model more contextual information. Inspired by the studies (Song et al., 2009; Song and Xia, 2012; Ouyang et al., 2017; Kim et al., 2018; Peng et al., 2018; Higashiyama et al., 2019; Tian et al., 2020c; Li et al., 2020) that leverage the large granularity contextual information carried by n-grams to enhance text representation for Chinese, we propose ZEN to enhance character based text encoders (e.g., BERT) by leveraging ngrams. In doing so, we extract n-grams prior to pre-training ZEN through two different steps. The first step is to prepare an n-gram lexicon (denoted as L), from which one can use any unsupervised method to extract n-grams from large corpora for later processing. The sec"
2020.findings-emnlp.425,2020.acl-main.735,1,0.902743,"us studies (Devlin et al., 2018; Yu and Jiang, 2019), in this paper, we focus on the details of ZEN, by explaining how n-grams are processed and incorporated into the character encoder. 2.1 N-gram Extraction A high quality of text representation plays an important role to obtain good performance for many NLP tasks (Song et al., 2017; Zhu et al., 2019; Liu and Lapata, 2019), where a powerful encoder is required to model more contextual information. Inspired by the studies (Song et al., 2009; Song and Xia, 2012; Ouyang et al., 2017; Kim et al., 2018; Peng et al., 2018; Higashiyama et al., 2019; Tian et al., 2020c; Li et al., 2020) that leverage the large granularity contextual information carried by n-grams to enhance text representation for Chinese, we propose ZEN to enhance character based text encoders (e.g., BERT) by leveraging ngrams. In doing so, we extract n-grams prior to pre-training ZEN through two different steps. The first step is to prepare an n-gram lexicon (denoted as L), from which one can use any unsupervised method to extract n-grams from large corpora for later processing. The second step of n-gram extraction is performed during pre-training, where some n-grams in L are selected ac"
2020.findings-emnlp.425,2020.acl-main.734,1,0.925515,"us studies (Devlin et al., 2018; Yu and Jiang, 2019), in this paper, we focus on the details of ZEN, by explaining how n-grams are processed and incorporated into the character encoder. 2.1 N-gram Extraction A high quality of text representation plays an important role to obtain good performance for many NLP tasks (Song et al., 2017; Zhu et al., 2019; Liu and Lapata, 2019), where a powerful encoder is required to model more contextual information. Inspired by the studies (Song et al., 2009; Song and Xia, 2012; Ouyang et al., 2017; Kim et al., 2018; Peng et al., 2018; Higashiyama et al., 2019; Tian et al., 2020c; Li et al., 2020) that leverage the large granularity contextual information carried by n-grams to enhance text representation for Chinese, we propose ZEN to enhance character based text encoders (e.g., BERT) by leveraging ngrams. In doing so, we extract n-grams prior to pre-training ZEN through two different steps. The first step is to prepare an n-gram lexicon (denoted as L), from which one can use any unsupervised method to extract n-grams from large corpora for later processing. The second step of n-gram extraction is performed during pre-training, where some n-grams in L are selected ac"
2020.nlpmc-1.3,J00-3003,0,0.726861,"Missing"
2020.nlpmc-1.3,W18-2309,1,0.830408,"ok? 2 DAD: Okay. Thank you. 3 DOC: You’re welcome. In Ex 4, the physician initiates the closure of the medical conversation by making a future arrangement for the patient’s follow-up visit at line 1. The patient’s father accepts the proposal and the two parties immediately proceed to the terminal exchange (thank you-you’re welcome) at lines 2-3. Besides the closure initiation action (a) making 4.1 Annotating conversation structure Figure 1 illustrates how the hierarchical structure of medical conversation is annotated according to the COSTA scheme. Detail of the COSTA scheme can be found in (Wang et al., 2018). 15 it is not unusual that the physicians and patients go back and forth between these phases. By annotating which phase a turn belongs to, we are not only able to show where the boundaries are among the phases in medical conversation, but also how transitions are coordinated by the participants. Pair and turn dependency Unlike many other types of discourse, conversation is interactive in nature. Thus, turns in conversation cannot be understood alone. Instead, each turn should be understood regarding whether they set up an expectation for a next turn or fulfill the expectation set up by a pri"
2020.nlpmc-1.3,xia-etal-2000-developing,1,0.534917,"ts. Adopting the Conversation Analysis transcribing conventions (Jefferson, 2004), each conversation is segmented into turns at turn-taking positions. Besides capturing the verbatim of each turn, the transcription also captures a series of para-linguistic features (e.g., dysfluencies, intonations, overlaps of turns, noticeable silence in and between turns, non-verbal actions such as nodding, etc.), which are essential aspects of natural spoken language. In addition, the transcribed text is automatically segmented into words using an in-house CRF word segmenter trained on the Chinese Treebank (Xia et al., 2000), so as to provide the necessary basis for conducting related NLP tasks. 3.3 Our Analysis Sequence expansion and making treatment decisions A second aspect of our analysis focuses on sequences within some particular phases in medical conversation. For example, within treatment recommendation phase, we examined how treatment recommendations are delivered and received. Specifically, it is found that treatment decisions (e.g., antibiotic prescriptions) can be negotiated between physicians and patients by patients withholding acceptance of physicians’ recommendations. Thus, the minimal form of ‘re"
2021.acl-long.259,W19-1909,0,0.044606,"Missing"
2021.acl-long.259,N18-3011,0,0.0173227,"ot shown here. However, based on considerable empirical evidence, we conclude that the unreliable representations of domain-specific n-grams (words and phrases) might be one of the main causes for model degradation. 7 Related Work A large performance drop of pre-trained models caused by domain shift has been observed and many domain-specific BERT models (Beltagy et al., 2019; Alsentzer et al., 2019; Huang et al., 2019; Lee et al., 2020) have been introduced to bridge the domain gap. For example, SciBERT (Beltagy et al., 2019) is trained on 1.14M scientific papers from Semantic Scholar corpus (Ammar et al., 2018) for 7 days on TPU v3-8 machine and BioBERT (Lee et al., 2020) is trained on PubMed abstracts and PMC full text articles for 23 days on eight NVIDIA V100 GPUs. ClinicalBERT (Alsentzer et al., 2019) is trained on about 2 million notes in the MIMIC-III v1.4 database (Johnson et al., 2016) for 17-18 days on a single GeForce GTX TITAN X 12 GB GPU. However, they all incur a huge computational cost, which is not affordable for many university labs or institutions. This is precisely why we believe that our efficient adaptor is useful to the community. Although Gururangan et al. (2020) introduced task"
2021.acl-long.259,N19-1423,0,0.0391047,"ords in the new domain. Experimental results illustrate the effectiveness of T-DNA on eight lowresource downstream tasks from four domains. We show that T-DNA is able to achieve significant improvements compared to existing methods on most tasks using limited data with lower computational costs. Moreover, further analyses demonstrate the importance and effectiveness of both unseen words and the information of different granularities.1 1 Introduction Pre-trained language models have achieved great success and shown promise in various application scenarios across natural language understanding (Devlin et al., 2019; Liu et al., 2019; Tian et al., 2020a) and generation (Lewis et al., 2020; Zhang 1 Our code is available at https://github.com/ shizhediao/T-DNA. et al., 2020; Yang et al., 2020). Normally applying pre-trained language models to different applications follows a two-stage paradigm: pre-training on a large unlabeled corpus and then fine-tuning on a downstream task dataset. However, when there are domain gaps between pre-training and fine-tuning data, previous studies (Beltagy et al., 2019; Lee et al., 2020) have observed a performance drop caused by the incapability of generalization to new dom"
2021.acl-long.259,2020.acl-main.740,0,0.0356762,"Missing"
2021.acl-long.259,N19-1276,0,0.0279988,"extension module to adapt an augmenting embedding for the in-domain vocabulary but it still needs large continuous pre-training. Similar to our work, they highlight the importance of the domain-specific words but all of these work neither explore the understanding of performance drop during a domain shift nor examine the importance of multi-grained information. Large granularity contextual information carried by spans or n-grams has proven to be helpful to enhance text representation for Chinese (Song et al., 2009; Song and Xia, 2012; Ouyang et al., 2017; Kim et al., 2018; Peng et al., 2018; Higashiyama et al., 2019; Tian et al., 2020e,b; Li et al., 2020; Diao et al., 2020; Song et al., 2021) and English (Joshi et al., 2020; Xiao et al., 2020; Tian et al., 2020c,d). In addition to text encoders on pre-training, the kNN-LM (Khandelwal et al., 2019) proposes to augment the language model for effective domain adaptation, by varying the nearest neighbor datastore of similar contexts without further training. However, all of the previous studies focused on either general pre-training procedures or different tasks (e.g., language modeling), and did not explore the effectiveness of multigrained information for"
2021.acl-long.259,P18-1031,0,0.0607262,"Missing"
2021.acl-long.259,W18-6120,0,0.0239442,"ExBERT (Tai et al., 2020) applied an extension module to adapt an augmenting embedding for the in-domain vocabulary but it still needs large continuous pre-training. Similar to our work, they highlight the importance of the domain-specific words but all of these work neither explore the understanding of performance drop during a domain shift nor examine the importance of multi-grained information. Large granularity contextual information carried by spans or n-grams has proven to be helpful to enhance text representation for Chinese (Song et al., 2009; Song and Xia, 2012; Ouyang et al., 2017; Kim et al., 2018; Peng et al., 2018; Higashiyama et al., 2019; Tian et al., 2020e,b; Li et al., 2020; Diao et al., 2020; Song et al., 2021) and English (Joshi et al., 2020; Xiao et al., 2020; Tian et al., 2020c,d). In addition to text encoders on pre-training, the kNN-LM (Khandelwal et al., 2019) proposes to augment the language model for effective domain adaptation, by varying the nearest neighbor datastore of similar contexts without further training. However, all of the previous studies focused on either general pre-training procedures or different tasks (e.g., language modeling), and did not explore the e"
2021.acl-long.259,2020.acl-main.703,0,0.0247869,"f T-DNA on eight lowresource downstream tasks from four domains. We show that T-DNA is able to achieve significant improvements compared to existing methods on most tasks using limited data with lower computational costs. Moreover, further analyses demonstrate the importance and effectiveness of both unseen words and the information of different granularities.1 1 Introduction Pre-trained language models have achieved great success and shown promise in various application scenarios across natural language understanding (Devlin et al., 2019; Liu et al., 2019; Tian et al., 2020a) and generation (Lewis et al., 2020; Zhang 1 Our code is available at https://github.com/ shizhediao/T-DNA. et al., 2020; Yang et al., 2020). Normally applying pre-trained language models to different applications follows a two-stage paradigm: pre-training on a large unlabeled corpus and then fine-tuning on a downstream task dataset. However, when there are domain gaps between pre-training and fine-tuning data, previous studies (Beltagy et al., 2019; Lee et al., 2020) have observed a performance drop caused by the incapability of generalization to new domains. Towards filling the gaps, the main research stream (Beltagy et al.,"
2021.acl-long.259,2020.acl-main.611,0,0.0222363,"ng for the in-domain vocabulary but it still needs large continuous pre-training. Similar to our work, they highlight the importance of the domain-specific words but all of these work neither explore the understanding of performance drop during a domain shift nor examine the importance of multi-grained information. Large granularity contextual information carried by spans or n-grams has proven to be helpful to enhance text representation for Chinese (Song et al., 2009; Song and Xia, 2012; Ouyang et al., 2017; Kim et al., 2018; Peng et al., 2018; Higashiyama et al., 2019; Tian et al., 2020e,b; Li et al., 2020; Diao et al., 2020; Song et al., 2021) and English (Joshi et al., 2020; Xiao et al., 2020; Tian et al., 2020c,d). In addition to text encoders on pre-training, the kNN-LM (Khandelwal et al., 2019) proposes to augment the language model for effective domain adaptation, by varying the nearest neighbor datastore of similar contexts without further training. However, all of the previous studies focused on either general pre-training procedures or different tasks (e.g., language modeling), and did not explore the effectiveness of multigrained information for domain adaptation. We hence view them a"
2021.acl-long.259,2021.ccl-1.108,0,0.0790855,"Missing"
2021.acl-long.259,D18-1360,0,0.0176294,"periments on eight classification tasks from four domains including biomedical sciences, computer science, news and reviews. The datasets are described as follows. • C HEM P ROT (Kringelum et al., 2016), a manually annotated chemical–protein interaction dataset extracted from 5,031 abstracts for relation classification. • RCT (Dernoncourt and Lee, 2017), which contains approximately 200,000 abstracts from public medicine with the role of each sentence clearly identified. • C ITATION I NTENT (Jurgens et al., 2018), which contains around 2,000 citations annotated for their function. • S CI ERC (Luan et al., 2018), which consists of 500 scientific abstracts annotated for relation classification. • H YPER PARTISAN (Kiesel et al., 2019), which contains 645 articles from Hyperpartisan news with either extreme left-wing or right-wing standpoint used for partisanship classification. • AGN EWS (Zhang et al., 2015), consisting of 127,600 categorized articles from more than 2000 news source for topic classification. 7 We show some analyses and discussion of data size in Section 6.2. 3339 D OMAIN B IO M ED DATASET CP RCT CI SE HP AG AM IMDB 4.1K 895K 4.1K 895K 2.4K 547K 3.4K 773K 13 1.8K 267K 180K 27.4M 30K 4.6"
2021.acl-long.259,P11-1015,0,0.0964101,"nguistics 2 The Motivation As observed in Gururangan et al. (2020), the transfer gain of domain-specific pre-training becomes increasingly significant when the source and target domain are vastly dissimilar in terms of the vocabulary overlap. Motivated by this association between transfer gain and vocabulary distribution, we further investigate the shift of words and phrases across domains and attempt to alleviate the degradation of language models without large domainspecific corpora. In particular, we start with a RoBERTa-base model from the generic domain and then fine-tune it on the IMDB (Maas et al., 2011) dataset. We investigate the outputs predicted by the [CLS] embedding on the IMDB development set and divide them into two categories: correct predictions (true 100 90 Label correct false 80 Ratio ing, we propose a light-weight Transformer-based Domain-aware N-gram Adaptor (T-DNA) by incorporating n-gram representations to bridge the domain gap between source and target vocabulary. Specifically, the proposed model is able to explicitly learn and incorporate better representations of domain-specific words and phrases (in the form of n-grams) by the adaptor networks with only requiring small pie"
2021.acl-long.259,P16-1162,0,0.049462,"2.29 92.11 93.13 94.34 IMDB w. 92.91↑0.62 92.89↑0.78 93.32↑0.19 94.81↑0.47 Table 3: Performance gains of T-DNA w.r.t. different sampling ratios of RCT, AG, AM and IMDB datasets. w. and w.o indicate whether the model is equipped with T-DNA or not. The uparrow marks where a positive gain is obtained. of data size. In addition, we examine the attention mechanism to verify the effects of n-gram representations during the domain shift. The details are illustrated in this section. 6.1 Effects of Different Granularities The lexical unit in RoBERTa is a subword obtained from byte pair encoding (BPE) (Sennrich et al., 2016) tokenization, resulting in a smaller token space and more training data for each token. Our approach provides coarse-grained information carried by the larger lexical units, n-gram. To verify the contribution of larger granularity information, we compare the improvement brought by T-DNA with information of different granularities, for n from 0 to 3. Note that here n means that we extract and incorporate all n-grams with a length smaller or equal to n (within a certain granularity). For example, n = 3 means that we include all unigrams, bigrams and trigrams. Two consistent observations could b"
2021.acl-long.259,W09-3511,1,0.52533,"useless for domain-specific 3343 words by Tai et al. (2020). ExBERT (Tai et al., 2020) applied an extension module to adapt an augmenting embedding for the in-domain vocabulary but it still needs large continuous pre-training. Similar to our work, they highlight the importance of the domain-specific words but all of these work neither explore the understanding of performance drop during a domain shift nor examine the importance of multi-grained information. Large granularity contextual information carried by spans or n-grams has proven to be helpful to enhance text representation for Chinese (Song et al., 2009; Song and Xia, 2012; Ouyang et al., 2017; Kim et al., 2018; Peng et al., 2018; Higashiyama et al., 2019; Tian et al., 2020e,b; Li et al., 2020; Diao et al., 2020; Song et al., 2021) and English (Joshi et al., 2020; Xiao et al., 2020; Tian et al., 2020c,d). In addition to text encoders on pre-training, the kNN-LM (Khandelwal et al., 2019) proposes to augment the language model for effective domain adaptation, by varying the nearest neighbor datastore of similar contexts without further training. However, all of the previous studies focused on either general pre-training procedures or different"
2021.acl-long.259,song-xia-2012-using,1,0.728188,"specific 3343 words by Tai et al. (2020). ExBERT (Tai et al., 2020) applied an extension module to adapt an augmenting embedding for the in-domain vocabulary but it still needs large continuous pre-training. Similar to our work, they highlight the importance of the domain-specific words but all of these work neither explore the understanding of performance drop during a domain shift nor examine the importance of multi-grained information. Large granularity contextual information carried by spans or n-grams has proven to be helpful to enhance text representation for Chinese (Song et al., 2009; Song and Xia, 2012; Ouyang et al., 2017; Kim et al., 2018; Peng et al., 2018; Higashiyama et al., 2019; Tian et al., 2020e,b; Li et al., 2020; Diao et al., 2020; Song et al., 2021) and English (Joshi et al., 2020; Xiao et al., 2020; Tian et al., 2020c,d). In addition to text encoders on pre-training, the kNN-LM (Khandelwal et al., 2019) proposes to augment the language model for effective domain adaptation, by varying the nearest neighbor datastore of similar contexts without further training. However, all of the previous studies focused on either general pre-training procedures or different tasks (e.g., langua"
2021.acl-long.259,2020.findings-emnlp.129,0,0.0322442,"n words into the original vocabulary list and learn their representation by pretraining from scratch (Beltagy et al., 2019; Gu et al., 2020), which requires substantial resources and training data. Moreover, SciBERT (Beltagy et al., 2019) found that in-domain vocabulary is helpful but not significant while we attribute it to the inefficiency of implicit learning of in-domain vocabulary. To represent OOV words in multilingual settings, the mixture mapping method (Wang et al., 2019) utilized a mixture of English subwords embedding, but it has been shown useless for domain-specific 3343 words by Tai et al. (2020). ExBERT (Tai et al., 2020) applied an extension module to adapt an augmenting embedding for the in-domain vocabulary but it still needs large continuous pre-training. Similar to our work, they highlight the importance of the domain-specific words but all of these work neither explore the understanding of performance drop during a domain shift nor examine the importance of multi-grained information. Large granularity contextual information carried by spans or n-grams has proven to be helpful to enhance text representation for Chinese (Song et al., 2009; Song and Xia, 2012; Ouyang et al., 2017;"
2021.acl-long.259,2020.emnlp-demos.15,0,0.0689126,"Missing"
2021.acl-long.259,2020.acl-main.735,1,0.916296,"sults illustrate the effectiveness of T-DNA on eight lowresource downstream tasks from four domains. We show that T-DNA is able to achieve significant improvements compared to existing methods on most tasks using limited data with lower computational costs. Moreover, further analyses demonstrate the importance and effectiveness of both unseen words and the information of different granularities.1 1 Introduction Pre-trained language models have achieved great success and shown promise in various application scenarios across natural language understanding (Devlin et al., 2019; Liu et al., 2019; Tian et al., 2020a) and generation (Lewis et al., 2020; Zhang 1 Our code is available at https://github.com/ shizhediao/T-DNA. et al., 2020; Yang et al., 2020). Normally applying pre-trained language models to different applications follows a two-stage paradigm: pre-training on a large unlabeled corpus and then fine-tuning on a downstream task dataset. However, when there are domain gaps between pre-training and fine-tuning data, previous studies (Beltagy et al., 2019; Lee et al., 2020) have observed a performance drop caused by the incapability of generalization to new domains. Towards filling the gaps, the m"
2021.acl-long.259,2020.emnlp-main.487,1,0.87898,"sults illustrate the effectiveness of T-DNA on eight lowresource downstream tasks from four domains. We show that T-DNA is able to achieve significant improvements compared to existing methods on most tasks using limited data with lower computational costs. Moreover, further analyses demonstrate the importance and effectiveness of both unseen words and the information of different granularities.1 1 Introduction Pre-trained language models have achieved great success and shown promise in various application scenarios across natural language understanding (Devlin et al., 2019; Liu et al., 2019; Tian et al., 2020a) and generation (Lewis et al., 2020; Zhang 1 Our code is available at https://github.com/ shizhediao/T-DNA. et al., 2020; Yang et al., 2020). Normally applying pre-trained language models to different applications follows a two-stage paradigm: pre-training on a large unlabeled corpus and then fine-tuning on a downstream task dataset. However, when there are domain gaps between pre-training and fine-tuning data, previous studies (Beltagy et al., 2019; Lee et al., 2020) have observed a performance drop caused by the incapability of generalization to new domains. Towards filling the gaps, the m"
2021.acl-long.259,2020.findings-emnlp.153,1,0.903269,"sults illustrate the effectiveness of T-DNA on eight lowresource downstream tasks from four domains. We show that T-DNA is able to achieve significant improvements compared to existing methods on most tasks using limited data with lower computational costs. Moreover, further analyses demonstrate the importance and effectiveness of both unseen words and the information of different granularities.1 1 Introduction Pre-trained language models have achieved great success and shown promise in various application scenarios across natural language understanding (Devlin et al., 2019; Liu et al., 2019; Tian et al., 2020a) and generation (Lewis et al., 2020; Zhang 1 Our code is available at https://github.com/ shizhediao/T-DNA. et al., 2020; Yang et al., 2020). Normally applying pre-trained language models to different applications follows a two-stage paradigm: pre-training on a large unlabeled corpus and then fine-tuning on a downstream task dataset. However, when there are domain gaps between pre-training and fine-tuning data, previous studies (Beltagy et al., 2019; Lee et al., 2020) have observed a performance drop caused by the incapability of generalization to new domains. Towards filling the gaps, the m"
2021.acl-long.259,2020.acl-main.734,1,0.904489,"sults illustrate the effectiveness of T-DNA on eight lowresource downstream tasks from four domains. We show that T-DNA is able to achieve significant improvements compared to existing methods on most tasks using limited data with lower computational costs. Moreover, further analyses demonstrate the importance and effectiveness of both unseen words and the information of different granularities.1 1 Introduction Pre-trained language models have achieved great success and shown promise in various application scenarios across natural language understanding (Devlin et al., 2019; Liu et al., 2019; Tian et al., 2020a) and generation (Lewis et al., 2020; Zhang 1 Our code is available at https://github.com/ shizhediao/T-DNA. et al., 2020; Yang et al., 2020). Normally applying pre-trained language models to different applications follows a two-stage paradigm: pre-training on a large unlabeled corpus and then fine-tuning on a downstream task dataset. However, when there are domain gaps between pre-training and fine-tuning data, previous studies (Beltagy et al., 2019; Lee et al., 2020) have observed a performance drop caused by the incapability of generalization to new domains. Towards filling the gaps, the m"
2021.acl-long.259,K19-1030,0,0.0204954,"rs. One way to handle OOV words is to simply utilize and learn an “unknown” embedding during training. Another way is to add in-domain words into the original vocabulary list and learn their representation by pretraining from scratch (Beltagy et al., 2019; Gu et al., 2020), which requires substantial resources and training data. Moreover, SciBERT (Beltagy et al., 2019) found that in-domain vocabulary is helpful but not significant while we attribute it to the inefficiency of implicit learning of in-domain vocabulary. To represent OOV words in multilingual settings, the mixture mapping method (Wang et al., 2019) utilized a mixture of English subwords embedding, but it has been shown useless for domain-specific 3343 words by Tai et al. (2020). ExBERT (Tai et al., 2020) applied an extension module to adapt an augmenting embedding for the in-domain vocabulary but it still needs large continuous pre-training. Similar to our work, they highlight the importance of the domain-specific words but all of these work neither explore the understanding of performance drop during a domain shift nor examine the importance of multi-grained information. Large granularity contextual information carried by spans or n-gr"
2021.acl-long.259,2021.naacl-main.136,0,0.072486,"Missing"
2021.acl-long.259,2020.findings-emnlp.140,0,0.0504548,"Missing"
2021.acl-long.259,2020.acl-demos.30,0,0.0355806,"Missing"
2021.acl-long.344,2020.osact-1.2,0,0.0127916,"ion Following Soares et al. (2019), we insert four special tokens (i.e., “&lt;e1&gt;”, “&lt;/e1&gt;”, “&lt;e2&gt;”, and “&lt;/e2&gt;”) into the input sentence to mark the boundary11 of the two entities to be investigated, which allows the encoder to distinguish the position of entities during encoding and thus improves model performance. For the encoder, we try BERT (Devlin et al., 2019), because it is a powerful pre-trained language model which and whose variants have achieved state-of-the-art performance in many NLP tasks (Wu and He, 2019; Soares et al., 2019; Wu et al., 2019; Diao et al., 2020; Song et al., 2020; Antoun et al., 2020; Tian et al., 2020a,b,d, 2021b; Qin et al., 2021; Song et al., 2021). Specifically, we use the uncased version of BERT-base and 10 We do not distinguish the two groups of connections in A-GCN once they are represented by the adjacency matrix. 11 For example, “&lt;e1&gt;” and “&lt;/e1&gt;” are respectively inserted right before and after the entity E1 in the input X . BERT-large12 following the default settings (e.g., for BERT-base, we use 12 layers of multi-head attentions with 768-dimensional hidden vectors; for BERT-large, we use 24 layers of multi-head attentions with 1024-dimensional hidden vectors)."
2021.acl-long.344,Q17-1010,0,0.03741,"an off-the-shelf toolkit, R is the relation type set; p computes the probability of a particular relation r 2 R given the two entities and rb the output of A-GCN, which takes X and TX as the input. Following texts start with a brief introduction of the standard GCN model, then elaborate our proposed A-GCN equipped with dependency type information, and lastly illustrate the process of applying A-GCN to the classification paradigm for RE. 2.1 Standard Graph Convolutional Networks Generally, a good text representation is a prerequisite to achieve outstanding model performance (Song et al., 2017; Bojanowski et al., 2017; Song et al., 2018; Song and Shi, 2018; Hajdik et al., 2019). To enhance the text representation and thus obtain a good understanding of the running text, many studies (Song et al., 2009, 2012; Song and Xia, 2013; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2019; Mandya et al., 2020; Nie et al., 2020) tried to leverage contextual features, such as n-grams and syntactic information, through different model architectures. Among all these architecture choices, graph convolutional networks (GCN) is a widely used architecture to encode the information in a graph, where in each GCN layer,"
2021.acl-long.344,2020.coling-main.24,1,0.605217,"t al., 2020) tried to leverage contextual features, such as n-grams and syntactic information, through different model architectures. Among all these architecture choices, graph convolutional networks (GCN) is a widely used architecture to encode the information in a graph, where in each GCN layer, information in each node communicates to its neighbors through the connections between them. The effectiveness of GCN models to encode the contextual information over a graph of an input sentence has been demonstrated by many previous studies (Zhang et al., 2018; Guo et al., 2019; Sun et al., 2020; Chen et al., 2020; Yu et al., 2020; Mandya et al., 2020; Tian et al., 2020c, 2021a). Normally, the graph in the standard GCN model is built from word dependencies and is represented by an adjacency matrix A = (ai,j )n⇥n where ai,j = 1 if i = j or there is a dependency connection2 (arc) between two words xi and xj in the dependency tree TX and ai,j = 0 otherwise. Based on A, for 2 Normally the direction of the connection is ignored. Figure 2: The overall architecture of the proposed A-GCN for RE illustrated with an example input sentence (the two entities “defamation” and “bishop” are highlighted in blue and re"
2021.acl-long.344,2021.findings-acl.221,1,0.629498,"tems or manually constructed features. These methods are superior in capturing contextual information and thus enable RE systems to better understand the text and identify relations between entities in the given text. Adopting neural models to help RE is not only straightforward and effective, but is also expected to incorporate more diverse and informative knowledge into RE systems. Among all different knowledge sources, syntactic information, especially the dependency trees, have been demonstrated to be beneficial in many studies (Miwa and Bansal, 2016; Zhang et al., 2018; Sun et al., 2020; Chen et al., 2021) because they provide long-distance word connections between useful words and thus accordingly guide the system to better extract relations between entity pairs. However, intensively leveraging dependency information may not always lead to good RE performance, because the noise in the dependency tree can potentially introduce confusions to relation classification (Xu et al., 2015; Yu et al., 2020), especially when those trees are automatically generated. For example, Figure 1 shows an example sentence with its dependency tree, where the dependency connection between “pumpkin mixture” and “bowl"
2021.acl-long.344,P18-2014,0,0.125287,"rk datasets. where o is a |R|-dimensional vector with each of its value referring to a relation type in the relation type set R. Finally, we apply a softmax function of o to predict the relation rb between E1 and E2 by exp (ou ) rb = arg max P|R| u u=1 exp (o ) (10) with ou representing the value at dimension u in o. 3 Experimental Settings 3.1 Datasets In the experiments, we use two English benchmark datasets for RE, namely, ACE2005EN (ACE05)5 and SemEval 2010 Task 8 (SemEval)6 (Hendrickx et al., 2010). For ACE05, we use its English section and follow previous studies (Miwa and Bansal, 2016; Christopoulou et al., 2018; Ye et al., 2019) to pre-process it (two small subsets cts and un are removed) and split the documents into training, development, and test sets7 . For SemEval, we use its official train/test split8 . The numbers of unique relation types in ACE05 and SemEval are 7 and 19, respectively. We report the number of instances (i.e., entity pairs), for train/dev/test sets of ACE05 and SemEval benchmark datasets in Table 1. 3.2 Dependency Graph Construction To construct graphs for A-GCN, we use Standard CoreNLP Toolkits (SCT)9 to obtain the dependency tree TX for each input sentence X . Although our a"
2021.acl-long.344,2021.acl-short.68,1,0.323735,"pecial tokens (i.e., “&lt;e1&gt;”, “&lt;/e1&gt;”, “&lt;e2&gt;”, and “&lt;/e2&gt;”) into the input sentence to mark the boundary11 of the two entities to be investigated, which allows the encoder to distinguish the position of entities during encoding and thus improves model performance. For the encoder, we try BERT (Devlin et al., 2019), because it is a powerful pre-trained language model which and whose variants have achieved state-of-the-art performance in many NLP tasks (Wu and He, 2019; Soares et al., 2019; Wu et al., 2019; Diao et al., 2020; Song et al., 2020; Antoun et al., 2020; Tian et al., 2020a,b,d, 2021b; Qin et al., 2021; Song et al., 2021). Specifically, we use the uncased version of BERT-base and 10 We do not distinguish the two groups of connections in A-GCN once they are represented by the adjacency matrix. 11 For example, “&lt;e1&gt;” and “&lt;/e1&gt;” are respectively inserted right before and after the entity E1 in the input X . BERT-large12 following the default settings (e.g., for BERT-base, we use 12 layers of multi-head attentions with 768-dimensional hidden vectors; for BERT-large, we use 24 layers of multi-head attentions with 1024-dimensional hidden vectors). For A-GCN, we randomly initialize all trainable"
2021.acl-long.344,P15-1061,0,0.0544283,"d at https://github.com/cuhksz-nlp/RE-AGCN. † Figure 1: An illustration of noises in the dependency tree that can hurt relation extraction, where the word dependency connected in between “pumpkin mixture” and “bowl” (whose relation is content-container) may introduce confusion to the system when the object is to predict the relation between “milk” and “pumpkin mixture” (whose relation is entity-destination). et al., 2019), question answering (Xu et al., 2016a), and summarization (Wang and Cardie, 2012). Recently, neural RE methods (Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017) with powerful encoders (such as CNN, RNN, and Transformers) have significantly improved model performance for RE without requiring any elaborately designed systems or manually constructed features. These methods are superior in capturing contextual information and thus enable RE systems to better understand the text and identify relations between entities in the given text. Adopting neural models to help RE is not only straightforward and effective, but is also expected to incorporate more diverse and informative k"
2021.acl-long.344,C16-1238,0,0.0140511,"and types to improve RE, we conduct a case study with our A-GCN models with different dependency graphs (i.e., two layers of A-GCN (Full) and A-GCN (L + G) with BERTlarge encoder) on an example sentence “A central vacuum is a vacuum motor and filtration system built inside a canister.”. Figure 5 shows the sentence where both the two models correctly predict the relation between “motor” (E1 ) and “canister” 6 Related Work Recently, neural networks with integrating external knowledge or resources play important roles in RE because of their superiority in better capturing contextual information (Shen and Huang, 2016; Soares et al., 2019). Particularly, as one kind of such knowledge, dependency parses show their effectiveness in supporting RE for its ability dency connections so that less-informative dependencies are smartly pruned. Experimental results and analyses on two English benchmark datasets for relation extraction demonstrate the effectiveness of our approach, especially for entities with long word-sequence distances, where state-of-theart performance is obtained on both datasets. Acknowledgements Figure 5: Visualizations of weights assigned to different dependency connections of A-GCN (Full) and"
2021.acl-long.344,D17-1004,0,0.0440773,"es in the dependency tree that can hurt relation extraction, where the word dependency connected in between “pumpkin mixture” and “bowl” (whose relation is content-container) may introduce confusion to the system when the object is to predict the relation between “milk” and “pumpkin mixture” (whose relation is entity-destination). et al., 2019), question answering (Xu et al., 2016a), and summarization (Wang and Cardie, 2012). Recently, neural RE methods (Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017) with powerful encoders (such as CNN, RNN, and Transformers) have significantly improved model performance for RE without requiring any elaborately designed systems or manually constructed features. These methods are superior in capturing contextual information and thus enable RE systems to better understand the text and identify relations between entities in the given text. Adopting neural models to help RE is not only straightforward and effective, but is also expected to incorporate more diverse and informative knowledge into RE systems. Among all different knowledge sources, syntactic info"
2021.acl-long.344,P16-2034,0,0.0344622,"llustration of noises in the dependency tree that can hurt relation extraction, where the word dependency connected in between “pumpkin mixture” and “bowl” (whose relation is content-container) may introduce confusion to the system when the object is to predict the relation between “milk” and “pumpkin mixture” (whose relation is entity-destination). et al., 2019), question answering (Xu et al., 2016a), and summarization (Wang and Cardie, 2012). Recently, neural RE methods (Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017) with powerful encoders (such as CNN, RNN, and Transformers) have significantly improved model performance for RE without requiring any elaborately designed systems or manually constructed features. These methods are superior in capturing contextual information and thus enable RE systems to better understand the text and identify relations between entities in the given text. Adopting neural models to help RE is not only straightforward and effective, but is also expected to incorporate more diverse and informative knowledge into RE systems. Among all different knowledge so"
2021.acl-long.459,2020.emnlp-main.112,1,0.894004,"ned visual and textual features point to the same content. The reason for the restraint comes from both the limitation of annotated correspondences between image and text for supervised learning as well as the lack of good model design to learn the correspondences. Unfortunately, few studies2 are dedicated to solving the restraint. Therefore, it is expected to have a better solution to model the alignments across modalities and further improve the generation ability, although promising results are continuously acquired by other approaches (Li et al., 2018; Liu et al., 2019; Jing et al., 2019; Chen et al., 2020). 2 Along this research track, recently there is only Jing et al. (2018) studying on a multi-task learning framework with a coattention mechanism to explicitly explore information linking particular parts in a radiograph and its corresponding report. 5904 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5904–5914 August 1–6, 2021. ©2021 Association for Computational Linguistics ? … “heart” “is” Output Embedding y?&quot;? Softmax Linear … … x? querying ???&quot;? Transformer ×N Decoder"
2021.acl-long.459,W19-1909,0,0.0360313,"Missing"
2021.acl-long.459,2021.findings-acl.221,1,0.752442,"related task to ours is image captioning, a cross-modal task involving natural language processing and computer vision, which aims to describe images in sentences (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018; Wang et al., 2019; Cornia et al., 2019). Among these studies, the most related study from Cornia et al. (2019) also proposed to leverage memory matrices to learn a priori knowledge for visual features using memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Zeng et al., 2018; Santoro et al., 2018; Nie et al., 2020; Diao et al., 2020; Tian et al., 2020b, 2021; Chen et al., 2021), but such operation is only performed during the encoding process. Different from this work, the memory in our model is designed to align the visual and textual features, and the memory operations (i.e., querying and responding) are performed in both the encoding and decoding process. Recently, many advanced NLP techniques (e.g., pre-trained language models) have been applied to tasks in the medical domain (Pampari et al., 2018; Zhang et al., 2018; Wang et al., 2018; Alsentzer et al., 2019; Tian et al., 2019, 2020a; Wang et al., 2020; Lee et al., 2020; Song et al., 2020). Being one of the app"
2021.acl-long.459,W11-2107,0,0.0154445,"the “AVG . ∆” column. ing conventional image captioning models, e.g., ST (Vinyals et al., 2015), ATT 2 IN (Rennie et al., 2017), A DA ATT (Lu et al., 2017), T OPDOWN (Anderson et al., 2018), and the ones proposed for the medical domain, e.g., C OATT (Jing et al., 2018), H RGR (Li et al., 2018), C MAS -R L (Jing et al., 2019) and R2G EN (Chen et al., 2020). Following Chen et al. (2020), we evaluate the above models by two types of metrics, conventional natural language generation (NLG) metrics and clinical efficacy (CE) metrics8 . The NLG metrics9 include BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and ROUGE-L (Lin, 2004). For CE metrics, the CheXpert (Irvin et al., 2019)10 is applied to label the generated reports and compare the results with ground truths in 14 different categories related to thoracic diseases and support devices. We use precision, recall and F1 to evaluate model performance for CE metrics. 3.3 Implementation Details To ensure consistency with the experiment settings of previous work (Li et al., 2018; Chen et al., 2020), we use two images of a patient as input for report generation on IU X-R AY and one image for MIMIC-CXR. For visual extractor, we adopt the ResNet101"
2021.acl-long.459,P19-1657,0,0.0755524,"inical practice and normally requires considerable manual workload. Therefore, radiology report generation, which aims to automatically generate a free-text description based on a radiograph, is highly desired to ease the burden of † Corresponding author. 1 Our code and the best performing models are released at https://github.com/cuhksz-nlp/R2GenCMN. radiologists while maintaining the quality of health care. Recently, substantial progress has been made towards research on automated radiology report generation models (Jing et al., 2018; Li et al., 2018; Johnson et al., 2019; Liu et al., 2019; Jing et al., 2019). Most existing studies adopt a conventional encoder-decoder architecture, with convolutional neural networks (CNNs) as the encoder and recurrent (e.g., LSTM/GRU) or non-recurrent networks (e.g., Transformer) as the decoder following the image captioning paradigm (Vinyals et al., 2015; Anderson et al., 2018). Although these methods have achieved remarkable performance, they are still restrained in fully employing the information across radiology images and reports, such as the mappings demonstrated in Figure 1 that aligned visual and textual features point to the same content. The reason for t"
2021.acl-long.459,P18-1240,0,0.366226,", chest X-ray) and writing diagnostic reports are essential operations in clinical practice and normally requires considerable manual workload. Therefore, radiology report generation, which aims to automatically generate a free-text description based on a radiograph, is highly desired to ease the burden of † Corresponding author. 1 Our code and the best performing models are released at https://github.com/cuhksz-nlp/R2GenCMN. radiologists while maintaining the quality of health care. Recently, substantial progress has been made towards research on automated radiology report generation models (Jing et al., 2018; Li et al., 2018; Johnson et al., 2019; Liu et al., 2019; Jing et al., 2019). Most existing studies adopt a conventional encoder-decoder architecture, with convolutional neural networks (CNNs) as the encoder and recurrent (e.g., LSTM/GRU) or non-recurrent networks (e.g., Transformer) as the decoder following the image captioning paradigm (Vinyals et al., 2015; Anderson et al., 2018). Although these methods have achieved remarkable performance, they are still restrained in fully employing the information across radiology images and reports, such as the mappings demonstrated in Figure 1 that al"
2021.acl-long.459,D14-1162,0,0.0869911,"Missing"
2021.acl-long.459,N18-1202,0,0.0397931,"e transferred into {vt1 , vt2 , ..., vtj , ..., vtK } . Then, we obtain the memory responses for visual and textual features by weighting over the transferred memory vectors by rxs = ΣK i=1 wsi vsi (12) ryt = ΣK i=1 wti vti (13) where wsi and wti are the weights obtained from memory querying. Similar to memory querying, we apply memory responding to all the threads so as to obtain responses from different memory representation subspaces. 2.3 Encoder-Decoder Since the quality of input representation plays an important role in model performance (Pennington et al., 2014; Song et al., 2017, 2018; Peters et al., 2018; Song and Shi, 2018; Devlin et al., 2019; Song et al., 2021), the encoder-decoder in our model is built upon standard Transformer (which is a powerful architecture that achieved state-of-the-art in many tasks), where memory responses of visual and textual features are functionalized as the input of the encoder and decoder so as to enhance the generation process. In detail, as the first step, the memory responses {rx1 , rx2 , ..., rxS } for visual features are fed into the encoder through {z1 , z2 , ..., zS } = fe (rx1 , rx2 , ..., rxS ) (14) where fe (·) represents the encoder. Then the resul"
2021.acl-long.459,W04-1013,0,0.0150578,"image captioning models, e.g., ST (Vinyals et al., 2015), ATT 2 IN (Rennie et al., 2017), A DA ATT (Lu et al., 2017), T OPDOWN (Anderson et al., 2018), and the ones proposed for the medical domain, e.g., C OATT (Jing et al., 2018), H RGR (Li et al., 2018), C MAS -R L (Jing et al., 2019) and R2G EN (Chen et al., 2020). Following Chen et al. (2020), we evaluate the above models by two types of metrics, conventional natural language generation (NLG) metrics and clinical efficacy (CE) metrics8 . The NLG metrics9 include BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and ROUGE-L (Lin, 2004). For CE metrics, the CheXpert (Irvin et al., 2019)10 is applied to label the generated reports and compare the results with ground truths in 14 different categories related to thoracic diseases and support devices. We use precision, recall and F1 to evaluate model performance for CE metrics. 3.3 Implementation Details To ensure consistency with the experiment settings of previous work (Li et al., 2018; Chen et al., 2020), we use two images of a patient as input for report generation on IU X-R AY and one image for MIMIC-CXR. For visual extractor, we adopt the ResNet101 (He et al., 2016) pretra"
2021.acl-long.459,2020.findings-emnlp.378,1,0.767097,"and text features. 5 Related Work In general, the most popular related task to ours is image captioning, a cross-modal task involving natural language processing and computer vision, which aims to describe images in sentences (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018; Wang et al., 2019; Cornia et al., 2019). Among these studies, the most related study from Cornia et al. (2019) also proposed to leverage memory matrices to learn a priori knowledge for visual features using memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Zeng et al., 2018; Santoro et al., 2018; Nie et al., 2020; Diao et al., 2020; Tian et al., 2020b, 2021; Chen et al., 2021), but such operation is only performed during the encoding process. Different from this work, the memory in our model is designed to align the visual and textual features, and the memory operations (i.e., querying and responding) are performed in both the encoding and decoding process. Recently, many advanced NLP techniques (e.g., pre-trained language models) have been applied to tasks in the medical domain (Pampari et al., 2018; Zhang et al., 2018; Wang et al., 2018; Alsentzer et al., 2019; Tian et al., 2019, 2020a; Wang et al.,"
2021.acl-long.459,D18-1258,0,0.0212713,"ng memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Zeng et al., 2018; Santoro et al., 2018; Nie et al., 2020; Diao et al., 2020; Tian et al., 2020b, 2021; Chen et al., 2021), but such operation is only performed during the encoding process. Different from this work, the memory in our model is designed to align the visual and textual features, and the memory operations (i.e., querying and responding) are performed in both the encoding and decoding process. Recently, many advanced NLP techniques (e.g., pre-trained language models) have been applied to tasks in the medical domain (Pampari et al., 2018; Zhang et al., 2018; Wang et al., 2018; Alsentzer et al., 2019; Tian et al., 2019, 2020a; Wang et al., 2020; Lee et al., 2020; Song et al., 2020). Being one of the applications and extensions of image captioning to the medical domain, radiology report generation aims to depicting radiology images with professional reports. Existing methods were designed and proposed to better align images and texts or to exploit highly-patternized features of texts. For the former studies, Jing et al. (2018) proposed a co-attention mechanism to simultaneously explore visual and semantic information with a mul"
2021.acl-long.459,P02-1040,0,0.110826,"red to BASE is also presented in the “AVG . ∆” column. ing conventional image captioning models, e.g., ST (Vinyals et al., 2015), ATT 2 IN (Rennie et al., 2017), A DA ATT (Lu et al., 2017), T OPDOWN (Anderson et al., 2018), and the ones proposed for the medical domain, e.g., C OATT (Jing et al., 2018), H RGR (Li et al., 2018), C MAS -R L (Jing et al., 2019) and R2G EN (Chen et al., 2020). Following Chen et al. (2020), we evaluate the above models by two types of metrics, conventional natural language generation (NLG) metrics and clinical efficacy (CE) metrics8 . The NLG metrics9 include BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and ROUGE-L (Lin, 2004). For CE metrics, the CheXpert (Irvin et al., 2019)10 is applied to label the generated reports and compare the results with ground truths in 14 different categories related to thoracic diseases and support devices. We use precision, recall and F1 to evaluate model performance for CE metrics. 3.3 Implementation Details To ensure consistency with the experiment settings of previous work (Li et al., 2018; Chen et al., 2020), we use two images of a patient as input for report generation on IU X-R AY and one image for MIMIC-CXR. For visua"
2021.acl-long.459,K17-1016,1,0.828631,"..., vsj , ..., vsK } are transferred into {vt1 , vt2 , ..., vtj , ..., vtK } . Then, we obtain the memory responses for visual and textual features by weighting over the transferred memory vectors by rxs = ΣK i=1 wsi vsi (12) ryt = ΣK i=1 wti vti (13) where wsi and wti are the weights obtained from memory querying. Similar to memory querying, we apply memory responding to all the threads so as to obtain responses from different memory representation subspaces. 2.3 Encoder-Decoder Since the quality of input representation plays an important role in model performance (Pennington et al., 2014; Song et al., 2017, 2018; Peters et al., 2018; Song and Shi, 2018; Devlin et al., 2019; Song et al., 2021), the encoder-decoder in our model is built upon standard Transformer (which is a powerful architecture that achieved state-of-the-art in many tasks), where memory responses of visual and textual features are functionalized as the input of the encoder and decoder so as to enhance the generation process. In detail, as the first step, the memory responses {rx1 , rx2 , ..., rxS } for visual features are fed into the encoder through {z1 , z2 , ..., zS } = fe (rx1 , rx2 , ..., rxS ) (14) where fe (·) represents"
2021.acl-long.459,N18-2028,1,0.829287,"Missing"
2021.acl-long.459,2020.coling-main.63,1,0.783401,"et al., 2020b, 2021; Chen et al., 2021), but such operation is only performed during the encoding process. Different from this work, the memory in our model is designed to align the visual and textual features, and the memory operations (i.e., querying and responding) are performed in both the encoding and decoding process. Recently, many advanced NLP techniques (e.g., pre-trained language models) have been applied to tasks in the medical domain (Pampari et al., 2018; Zhang et al., 2018; Wang et al., 2018; Alsentzer et al., 2019; Tian et al., 2019, 2020a; Wang et al., 2020; Lee et al., 2020; Song et al., 2020). Being one of the applications and extensions of image captioning to the medical domain, radiology report generation aims to depicting radiology images with professional reports. Existing methods were designed and proposed to better align images and texts or to exploit highly-patternized features of texts. For the former studies, Jing et al. (2018) proposed a co-attention mechanism to simultaneously explore visual and semantic information with a multi-task learning framework. For the latter studies, Li et al. (2018) introduced a template database to incorporate patternized information and Che"
2021.acl-long.459,2021.eacl-main.326,1,0.840317,"Missing"
2021.acl-long.459,W18-5623,0,0.0216031,"ston et al., 2015; Sukhbaatar et al., 2015; Zeng et al., 2018; Santoro et al., 2018; Nie et al., 2020; Diao et al., 2020; Tian et al., 2020b, 2021; Chen et al., 2021), but such operation is only performed during the encoding process. Different from this work, the memory in our model is designed to align the visual and textual features, and the memory operations (i.e., querying and responding) are performed in both the encoding and decoding process. Recently, many advanced NLP techniques (e.g., pre-trained language models) have been applied to tasks in the medical domain (Pampari et al., 2018; Zhang et al., 2018; Wang et al., 2018; Alsentzer et al., 2019; Tian et al., 2019, 2020a; Wang et al., 2020; Lee et al., 2020; Song et al., 2020). Being one of the applications and extensions of image captioning to the medical domain, radiology report generation aims to depicting radiology images with professional reports. Existing methods were designed and proposed to better align images and texts or to exploit highly-patternized features of texts. For the former studies, Jing et al. (2018) proposed a co-attention mechanism to simultaneously explore visual and semantic information with a multi-task learning fra"
2021.acl-long.459,W19-5027,1,0.836663,"Santoro et al., 2018; Nie et al., 2020; Diao et al., 2020; Tian et al., 2020b, 2021; Chen et al., 2021), but such operation is only performed during the encoding process. Different from this work, the memory in our model is designed to align the visual and textual features, and the memory operations (i.e., querying and responding) are performed in both the encoding and decoding process. Recently, many advanced NLP techniques (e.g., pre-trained language models) have been applied to tasks in the medical domain (Pampari et al., 2018; Zhang et al., 2018; Wang et al., 2018; Alsentzer et al., 2019; Tian et al., 2019, 2020a; Wang et al., 2020; Lee et al., 2020; Song et al., 2020). Being one of the applications and extensions of image captioning to the medical domain, radiology report generation aims to depicting radiology images with professional reports. Existing methods were designed and proposed to better align images and texts or to exploit highly-patternized features of texts. For the former studies, Jing et al. (2018) proposed a co-attention mechanism to simultaneously explore visual and semantic information with a multi-task learning framework. For the latter studies, Li et al. (2018) introduced a"
2021.acl-long.459,2020.acl-main.734,1,0.764676,"general, the most popular related task to ours is image captioning, a cross-modal task involving natural language processing and computer vision, which aims to describe images in sentences (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018; Wang et al., 2019; Cornia et al., 2019). Among these studies, the most related study from Cornia et al. (2019) also proposed to leverage memory matrices to learn a priori knowledge for visual features using memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Zeng et al., 2018; Santoro et al., 2018; Nie et al., 2020; Diao et al., 2020; Tian et al., 2020b, 2021; Chen et al., 2021), but such operation is only performed during the encoding process. Different from this work, the memory in our model is designed to align the visual and textual features, and the memory operations (i.e., querying and responding) are performed in both the encoding and decoding process. Recently, many advanced NLP techniques (e.g., pre-trained language models) have been applied to tasks in the medical domain (Pampari et al., 2018; Zhang et al., 2018; Wang et al., 2018; Alsentzer et al., 2019; Tian et al., 2019, 2020a; Wang et al., 2020; Lee et al., 2020; Song et al.,"
2021.acl-long.459,W18-2309,1,0.835033,"ukhbaatar et al., 2015; Zeng et al., 2018; Santoro et al., 2018; Nie et al., 2020; Diao et al., 2020; Tian et al., 2020b, 2021; Chen et al., 2021), but such operation is only performed during the encoding process. Different from this work, the memory in our model is designed to align the visual and textual features, and the memory operations (i.e., querying and responding) are performed in both the encoding and decoding process. Recently, many advanced NLP techniques (e.g., pre-trained language models) have been applied to tasks in the medical domain (Pampari et al., 2018; Zhang et al., 2018; Wang et al., 2018; Alsentzer et al., 2019; Tian et al., 2019, 2020a; Wang et al., 2020; Lee et al., 2020; Song et al., 2020). Being one of the applications and extensions of image captioning to the medical domain, radiology report generation aims to depicting radiology images with professional reports. Existing methods were designed and proposed to better align images and texts or to exploit highly-patternized features of texts. For the former studies, Jing et al. (2018) proposed a co-attention mechanism to simultaneously explore visual and semantic information with a multi-task learning framework. For the lat"
2021.acl-long.459,2020.nlpmc-1.3,1,0.733692,"et al., 2020; Diao et al., 2020; Tian et al., 2020b, 2021; Chen et al., 2021), but such operation is only performed during the encoding process. Different from this work, the memory in our model is designed to align the visual and textual features, and the memory operations (i.e., querying and responding) are performed in both the encoding and decoding process. Recently, many advanced NLP techniques (e.g., pre-trained language models) have been applied to tasks in the medical domain (Pampari et al., 2018; Zhang et al., 2018; Wang et al., 2018; Alsentzer et al., 2019; Tian et al., 2019, 2020a; Wang et al., 2020; Lee et al., 2020; Song et al., 2020). Being one of the applications and extensions of image captioning to the medical domain, radiology report generation aims to depicting radiology images with professional reports. Existing methods were designed and proposed to better align images and texts or to exploit highly-patternized features of texts. For the former studies, Jing et al. (2018) proposed a co-attention mechanism to simultaneously explore visual and semantic information with a multi-task learning framework. For the latter studies, Li et al. (2018) introduced a template database to incor"
2021.acl-long.459,D18-1351,1,0.845766,"mediate medium to interact between image and text features. 5 Related Work In general, the most popular related task to ours is image captioning, a cross-modal task involving natural language processing and computer vision, which aims to describe images in sentences (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018; Wang et al., 2019; Cornia et al., 2019). Among these studies, the most related study from Cornia et al. (2019) also proposed to leverage memory matrices to learn a priori knowledge for visual features using memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Zeng et al., 2018; Santoro et al., 2018; Nie et al., 2020; Diao et al., 2020; Tian et al., 2020b, 2021; Chen et al., 2021), but such operation is only performed during the encoding process. Different from this work, the memory in our model is designed to align the visual and textual features, and the memory operations (i.e., querying and responding) are performed in both the encoding and decoding process. Recently, many advanced NLP techniques (e.g., pre-trained language models) have been applied to tasks in the medical domain (Pampari et al., 2018; Zhang et al., 2018; Wang et al., 2018; Alsentzer et al., 2019"
2021.acl-short.68,W17-1302,0,0.0176594,"itics and outperform all previous studies, on both datasets.1 1 Introduction Modern standard Arabic (MSA) is generally written without diacritics, which poses a challenge to text processing and understanding in downstream applications, such as text-to-speech generation (Drago et al., 2008) and reading comprehension (Hermena et al., 2015). Restoration of such diacritics, known as diacritization, becomes an important task for Arabic natural language processing (NLP). Among different diacritization methods (Pasha et al., 2014; Shahrour et al., 2015; Zitouni et al., 2006; Habash and Rambow, 2007; Darwish et al., 2017), the neural ones (Abandah et al., 2015a; Fadel et al., 2019a,b; Zalmout and Habash, 2019, 2020; Darwish et al., 2020) achieve the best performance due to their better capability in incorporating contextual features. To further improve diacritization, automatically generated knowledge * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/AD-RDAT. † from off-the-shelf toolkits, such as morphological features, parts-of-speech tags, and automatic diacritization results, have been extensively applied to this task (Zit"
2021.acl-short.68,N19-1423,0,0.0177165,"nimize LD and maximize LS synchronously. 3 Experiments 3.1 Settings In our experiments, We use two benchmark datasets, i.e., ATB (Arabic Treebank Part 1, 2, and 3) (Maamouri et al., 2004) and Tashkeela (Zerrouki and Balla, 2017), following the same settings in previous studies.4 For implementation, we run Farasa5 (Abdelali et al., 2016) on the two datasets and collect their diacritization results for regularized decoding. Since the quality of text representation normally dominates the model performance (Pennington et al., 2014; Song et al., 2017, 2018; Peters et al., 2018; Song and Shi, 2018; Devlin et al., 2019), in our experiments, we test two types of widely used and powerful encoders, i.e., BiLSTM and Transformer (Vaswani et al., 2017), for SE and PE. For the embeddings, we use AraBERT (Antoun et al., 2020) and the large version of ZEN 2.0 (Song et al., 2021) with their default settings (i.e. 12 layers of multi-head attentions with 768 dimensional hidden vectors for AraBERT and 24 layers of multi-head attentions with 1024 dimensional hidden vectors for ZEN 2.0) to perform the initialization (we use the output of the last layer).6 We train our model for 20 epochs in total, with the first 10 for the"
2021.acl-short.68,N16-3003,0,0.0138066,"of experimental results (i.e., DER, WER, and accuracy) between previous studies and our models with AraBERT and ZEN 2.0 embeddings on the test sets of the ATB and Tashkeela. where λ is a positive coefficient that controls the influence of LS in the adversarial training, so that to minimize LD and maximize LS synchronously. 3 Experiments 3.1 Settings In our experiments, We use two benchmark datasets, i.e., ATB (Arabic Treebank Part 1, 2, and 3) (Maamouri et al., 2004) and Tashkeela (Zerrouki and Balla, 2017), following the same settings in previous studies.4 For implementation, we run Farasa5 (Abdelali et al., 2016) on the two datasets and collect their diacritization results for regularized decoding. Since the quality of text representation normally dominates the model performance (Pennington et al., 2014; Song et al., 2017, 2018; Peters et al., 2018; Song and Shi, 2018; Devlin et al., 2019), in our experiments, we test two types of widely used and powerful encoders, i.e., BiLSTM and Transformer (Vaswani et al., 2017), for SE and PE. For the embeddings, we use AraBERT (Antoun et al., 2020) and the large version of ZEN 2.0 (Song et al., 2021) with their default settings (i.e. 12 layers of multi-head atte"
2021.acl-short.68,2020.findings-emnlp.378,1,0.714553,"R 2.48 2.08 2.03 2.66 1.96 1.87 7.33 6.02 5.86 7.65 5.62 5.54 2.19 1.83 1.77 2.33 1.67 1.56 4.79 3.91 3.75 4.99 3.37 3.64 93.27 94.50 94.69 92.95 94.86 94.94 (b) ZEN 2.0 Table 1: Experimental results (i.e., DER and WER with and without the case ending being considered and accuracy) of baselines and our models with RD and AT using AraBERT (a) and ZEN 2.0 (b) on the test sets of ATB and Tashkeela, “BiLSTM” and “Transformer” denote the encoders (i.e., SE and PE) used in the models. vent the noise in the auto-generated knowledge from significantly hurting the model performance (Tang et al., 2020; Nie et al., 2020; Chen et al., 2020; Mandya et al., 2020; Tian et al., 2020a,b, 2021a,b; Chen et al., 2021). To tackle this challenge, we propose to learn from a special decoding process, which is integrated into the main diacritization model, in order to reduce error propagation compared to directly using the knowledge instances or their features. As shown in Figure 1, the proposed regularized decoding is an extra output process separated from the main tagger and performed ∗ on another sequence of labels Y K , which are the auto-generated knowledge instances (diacritization labels) annotated by an existing t"
2021.acl-short.68,pasha-etal-2014-madamira,0,0.030068,"Missing"
2021.acl-short.68,D14-1162,0,0.0852519,"ve coefficient that controls the influence of LS in the adversarial training, so that to minimize LD and maximize LS synchronously. 3 Experiments 3.1 Settings In our experiments, We use two benchmark datasets, i.e., ATB (Arabic Treebank Part 1, 2, and 3) (Maamouri et al., 2004) and Tashkeela (Zerrouki and Balla, 2017), following the same settings in previous studies.4 For implementation, we run Farasa5 (Abdelali et al., 2016) on the two datasets and collect their diacritization results for regularized decoding. Since the quality of text representation normally dominates the model performance (Pennington et al., 2014; Song et al., 2017, 2018; Peters et al., 2018; Song and Shi, 2018; Devlin et al., 2019), in our experiments, we test two types of widely used and powerful encoders, i.e., BiLSTM and Transformer (Vaswani et al., 2017), for SE and PE. For the embeddings, we use AraBERT (Antoun et al., 2020) and the large version of ZEN 2.0 (Song et al., 2021) with their default settings (i.e. 12 layers of multi-head attentions with 768 dimensional hidden vectors for AraBERT and 24 layers of multi-head attentions with 1024 dimensional hidden vectors for ZEN 2.0) to perform the initialization (we use the output o"
2021.acl-short.68,N18-1202,0,0.00970672,"n the adversarial training, so that to minimize LD and maximize LS synchronously. 3 Experiments 3.1 Settings In our experiments, We use two benchmark datasets, i.e., ATB (Arabic Treebank Part 1, 2, and 3) (Maamouri et al., 2004) and Tashkeela (Zerrouki and Balla, 2017), following the same settings in previous studies.4 For implementation, we run Farasa5 (Abdelali et al., 2016) on the two datasets and collect their diacritization results for regularized decoding. Since the quality of text representation normally dominates the model performance (Pennington et al., 2014; Song et al., 2017, 2018; Peters et al., 2018; Song and Shi, 2018; Devlin et al., 2019), in our experiments, we test two types of widely used and powerful encoders, i.e., BiLSTM and Transformer (Vaswani et al., 2017), for SE and PE. For the embeddings, we use AraBERT (Antoun et al., 2020) and the large version of ZEN 2.0 (Song et al., 2021) with their default settings (i.e. 12 layers of multi-head attentions with 768 dimensional hidden vectors for AraBERT and 24 layers of multi-head attentions with 1024 dimensional hidden vectors for ZEN 2.0) to perform the initialization (we use the output of the last layer).6 We train our model for 20"
2021.acl-short.68,D15-1152,0,0.0138723,"wed auto-generated knowledge, our model can still learn adequate diacritics and outperform all previous studies, on both datasets.1 1 Introduction Modern standard Arabic (MSA) is generally written without diacritics, which poses a challenge to text processing and understanding in downstream applications, such as text-to-speech generation (Drago et al., 2008) and reading comprehension (Hermena et al., 2015). Restoration of such diacritics, known as diacritization, becomes an important task for Arabic natural language processing (NLP). Among different diacritization methods (Pasha et al., 2014; Shahrour et al., 2015; Zitouni et al., 2006; Habash and Rambow, 2007; Darwish et al., 2017), the neural ones (Abandah et al., 2015a; Fadel et al., 2019a,b; Zalmout and Habash, 2019, 2020; Darwish et al., 2020) achieve the best performance due to their better capability in incorporating contextual features. To further improve diacritization, automatically generated knowledge * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/AD-RDAT. † from off-the-shelf toolkits, such as morphological features, parts-of-speech tags, and automatic d"
2021.acl-short.68,2020.acl-main.588,0,0.212231,"Missing"
2021.acl-short.68,P06-1073,0,0.099307,"ledge, our model can still learn adequate diacritics and outperform all previous studies, on both datasets.1 1 Introduction Modern standard Arabic (MSA) is generally written without diacritics, which poses a challenge to text processing and understanding in downstream applications, such as text-to-speech generation (Drago et al., 2008) and reading comprehension (Hermena et al., 2015). Restoration of such diacritics, known as diacritization, becomes an important task for Arabic natural language processing (NLP). Among different diacritization methods (Pasha et al., 2014; Shahrour et al., 2015; Zitouni et al., 2006; Habash and Rambow, 2007; Darwish et al., 2017), the neural ones (Abandah et al., 2015a; Fadel et al., 2019a,b; Zalmout and Habash, 2019, 2020; Darwish et al., 2020) achieve the best performance due to their better capability in incorporating contextual features. To further improve diacritization, automatically generated knowledge * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/AD-RDAT. † from off-the-shelf toolkits, such as morphological features, parts-of-speech tags, and automatic diacritization results,"
2021.bionlp-1.23,2020.coling-main.24,1,0.905505,"ferential Diagnosis) of the patient, distinguish it from other diseases and exclude the possible diagnosis of other diseases 治疗计划 (Treatment Plan) Class Patient’s name, gender, age, reason for admission, time of admission Medical examinations to be done in the next step, and preliminary treatment plan Table 1: The major five parts of information contained in one First Course Record in Chinese EMRs. 2 Related Work NLP for medical text has draw many attentions in the recent years (Xue et al., 2012; Xu et al., 2015; Li et al., 2019; Tian et al., 2019, 2020a; Song et al., 2020; Wang et al., 2020; Chen et al., 2020b), especially for the EMR texts. Among different tasks to process Chinese EMR texts, CWS and medical concept recognition are two fundamental ones that draw much attentions from previous studies. Due to the dramatic performance drop when applying the model trained from open source corpus on the medical field, previous studies (Xu et al., 2014, 2015; Li et al., 2015; Zhang et al., 2016; He et al., 2017) always construct Chinese medical datasets themselves and test their models on the datasets. However, most constructed datasets used for CWS are relatively small, where there are only roughly 100"
2021.bionlp-1.23,D15-1141,0,0.0256855,"nd can be used to describe the body, abnormalities, etc. E.g., “胃 肠型感冒” (gastrointestinal cold) where “胃肠型” (gastrointestinal) are Qualitative medical concepts. 4 Methods A good text representation is highly important in achieving a promising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memor"
2021.bionlp-1.23,2020.emnlp-main.112,1,0.905694,"ferential Diagnosis) of the patient, distinguish it from other diseases and exclude the possible diagnosis of other diseases 治疗计划 (Treatment Plan) Class Patient’s name, gender, age, reason for admission, time of admission Medical examinations to be done in the next step, and preliminary treatment plan Table 1: The major five parts of information contained in one First Course Record in Chinese EMRs. 2 Related Work NLP for medical text has draw many attentions in the recent years (Xue et al., 2012; Xu et al., 2015; Li et al., 2019; Tian et al., 2019, 2020a; Song et al., 2020; Wang et al., 2020; Chen et al., 2020b), especially for the EMR texts. Among different tasks to process Chinese EMR texts, CWS and medical concept recognition are two fundamental ones that draw much attentions from previous studies. Due to the dramatic performance drop when applying the model trained from open source corpus on the medical field, previous studies (Xu et al., 2014, 2015; Li et al., 2015; Zhang et al., 2016; He et al., 2017) always construct Chinese medical datasets themselves and test their models on the datasets. However, most constructed datasets used for CWS are relatively small, where there are only roughly 100"
2021.bionlp-1.23,N19-1423,0,0.0194832,"text representation is highly important in achieving a promising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets"
2021.bionlp-1.23,2020.findings-emnlp.425,1,0.793346,"hly important in achieving a promising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Rec"
2021.bionlp-1.23,2020.wanlp-1.5,0,0.0410023,"ious CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medical concept recognition, we regard it as a character-based sequence labeling task and perform it in a similar way with named entity 215 If a Chinese word is translated into multiple English words, we use “*” in the English translation to mark its boundary in Table 4. E.g., “3天” is translated"
2021.bionlp-1.23,Q18-1005,1,0.837874,"otated medical sentence in ACEMR with the corresponding English translations. The abbreviations of tags are used for annotation. ment behaviors. E.g., “予” (given), “入院治疗” (admission to hospital for treatment). Qualitative emphasizes a qualitative description of something, rather than a direct measurement and can be used to describe the body, abnormalities, etc. E.g., “胃 肠型感冒” (gastrointestinal cold) where “胃肠型” (gastrointestinal) are Qualitative medical concepts. 4 Methods A good text representation is highly important in achieving a promising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre"
2021.bionlp-1.23,2020.coling-main.78,0,0.0400041,"EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medical concept recognition, we regard it as a character-based sequence labeling task and perform it in a similar way with named entity 215 If a Chinese word is translated into multiple English words, we use “*” in the English translation to mark its bounda"
2021.bionlp-1.23,P14-5010,0,0.00252111,"b-class “BP”, and thus the tags for the three characters are “B-BP”, “I-BP”, and “E-BP”, respectively. We try BiLSTM, BERT, ZEN, as well as TwASP (Tian et al., 2020b) with the CRF decoder for medical concept recognition. TwASP is a model that leverages the auto-generated syntactic information (e.g., the POS tags (POS), the dependency relations (Dep.), and the syntactic constituents (Syn.)) through a two-way attention mechanism to improve model performance for sequence labeling tasks. To obtain the syntactic information of the input sentence required by TwASP, we use Stanford CoreNLP Toolkits (Manning et al., 2014) to obtain the POS tags, the dependency tree, and the constituent syntax tree. Figure 1 shows an example sentence (with English translation) and the three types of the auto-generated syntactic information. 5 BiLSTM + CRF + Tencent Embedding ZEN + CRF + KVMN 99.01 98.99 99.03 Table 6: CWS performance for different composition of training data where +CRF, +KVMN, +Tencent Embedding represent the use of CRF layer, memory network (WMSeg) and Tencent Embedding respectively. character embeddings from Tencent Embedding4 (Song et al., 2018), with the training epoch, batch size, and learning rate set to"
2021.bionlp-1.23,D16-1147,0,0.0238089,"6; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medical concept recognition, we regard it as a character-based sequence labeling task and perform it in a similar way with named entity 215 If a Chinese word is translated into multiple English words, we use “*” in the English translation to mark its boundary in Table 4. E.g., “3天” is translated into “*3 days*”. Methods Prec. Recall F1 76.85 77.22 CTB Only WMSeg *ZEN is the base model 77.60 CTB+ACEMR Figure"
2021.bionlp-1.23,2020.emnlp-main.107,1,0.762718,"1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medical concept recognition, we regard it as a character-based sequence labeling task and perform it in a similar way with named entity 215 If a Chinese word is translated into multiple English words, we use “*” in the English trans"
2021.bionlp-1.23,C12-2116,1,0.75437,"ing, rather than a direct measurement and can be used to describe the body, abnormalities, etc. E.g., “胃 肠型感冒” (gastrointestinal cold) where “胃肠型” (gastrointestinal) are Qualitative medical concepts. 4 Methods A good text representation is highly important in achieving a promising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is C"
2021.bionlp-1.23,K17-1016,1,0.832577,": An example of annotated medical sentence in ACEMR with the corresponding English translations. The abbreviations of tags are used for annotation. ment behaviors. E.g., “予” (given), “入院治疗” (admission to hospital for treatment). Qualitative emphasizes a qualitative description of something, rather than a direct measurement and can be used to describe the body, abnormalities, etc. E.g., “胃 肠型感冒” (gastrointestinal cold) where “胃肠型” (gastrointestinal) are Qualitative medical concepts. 4 Methods A good text representation is highly important in achieving a promising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein"
2021.bionlp-1.23,N18-2028,1,0.842704,"ntence required by TwASP, we use Stanford CoreNLP Toolkits (Manning et al., 2014) to obtain the POS tags, the dependency tree, and the constituent syntax tree. Figure 1 shows an example sentence (with English translation) and the three types of the auto-generated syntactic information. 5 BiLSTM + CRF + Tencent Embedding ZEN + CRF + KVMN 99.01 98.99 99.03 Table 6: CWS performance for different composition of training data where +CRF, +KVMN, +Tencent Embedding represent the use of CRF layer, memory network (WMSeg) and Tencent Embedding respectively. character embeddings from Tencent Embedding4 (Song et al., 2018), with the training epoch, batch size, and learning rate set to 50, 32, and 0.001, respectively. For BERT, ZEN, and WMSeg, we use the official settings (e.g., 768 dimensional hidden vectors with 12 multi-head self-attentions for BERT), where the number of training epoch is 50, the batch size is 16, and the learning rate is 1e-5. The experimental results of CWS are presented in Table 6 with three different settings (i.e., CTB Only, CTB+ACEMR, and ACEMR Only). The CTB Only setting displays the results of WMSeg model (with ZEN encoder) when it is trained on CTB6 only and evaluated on the ACEMR te"
2021.bionlp-1.23,2020.coling-main.63,1,0.774205,"断 According to the main complaint (Differential Diagnosis) of the patient, distinguish it from other diseases and exclude the possible diagnosis of other diseases 治疗计划 (Treatment Plan) Class Patient’s name, gender, age, reason for admission, time of admission Medical examinations to be done in the next step, and preliminary treatment plan Table 1: The major five parts of information contained in one First Course Record in Chinese EMRs. 2 Related Work NLP for medical text has draw many attentions in the recent years (Xue et al., 2012; Xu et al., 2015; Li et al., 2019; Tian et al., 2019, 2020a; Song et al., 2020; Wang et al., 2020; Chen et al., 2020b), especially for the EMR texts. Among different tasks to process Chinese EMR texts, CWS and medical concept recognition are two fundamental ones that draw much attentions from previous studies. Due to the dramatic performance drop when applying the model trained from open source corpus on the medical field, previous studies (Xu et al., 2014, 2015; Li et al., 2015; Zhang et al., 2016; He et al., 2017) always construct Chinese medical datasets themselves and test their models on the datasets. However, most constructed datasets used for CWS are relatively s"
2021.bionlp-1.23,W19-5027,1,0.820897,"several disease names 鉴别诊断 According to the main complaint (Differential Diagnosis) of the patient, distinguish it from other diseases and exclude the possible diagnosis of other diseases 治疗计划 (Treatment Plan) Class Patient’s name, gender, age, reason for admission, time of admission Medical examinations to be done in the next step, and preliminary treatment plan Table 1: The major five parts of information contained in one First Course Record in Chinese EMRs. 2 Related Work NLP for medical text has draw many attentions in the recent years (Xue et al., 2012; Xu et al., 2015; Li et al., 2019; Tian et al., 2019, 2020a; Song et al., 2020; Wang et al., 2020; Chen et al., 2020b), especially for the EMR texts. Among different tasks to process Chinese EMR texts, CWS and medical concept recognition are two fundamental ones that draw much attentions from previous studies. Due to the dramatic performance drop when applying the model trained from open source corpus on the medical field, previous studies (Xu et al., 2014, 2015; Li et al., 2015; Zhang et al., 2016; He et al., 2017) always construct Chinese medical datasets themselves and test their models on the datasets. However, most constructed datasets use"
2021.bionlp-1.23,2020.findings-emnlp.378,1,0.816592,"omising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medica"
2021.bionlp-1.23,2020.acl-main.735,1,0.904769,"omising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medica"
2021.bionlp-1.23,2020.coling-main.187,1,0.904792,"omising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medica"
2021.bionlp-1.23,2020.acl-main.734,1,0.904701,"omising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medica"
2021.bionlp-1.23,I13-1071,1,0.786645,"direct measurement and can be used to describe the body, abnormalities, etc. E.g., “胃 肠型感冒” (gastrointestinal cold) where “胃肠型” (gastrointestinal) are Qualitative medical concepts. 4 Methods A good text representation is highly important in achieving a promising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that levera"
2021.bionlp-1.23,2020.nlpmc-1.3,1,0.735847,"main complaint (Differential Diagnosis) of the patient, distinguish it from other diseases and exclude the possible diagnosis of other diseases 治疗计划 (Treatment Plan) Class Patient’s name, gender, age, reason for admission, time of admission Medical examinations to be done in the next step, and preliminary treatment plan Table 1: The major five parts of information contained in one First Course Record in Chinese EMRs. 2 Related Work NLP for medical text has draw many attentions in the recent years (Xue et al., 2012; Xu et al., 2015; Li et al., 2019; Tian et al., 2019, 2020a; Song et al., 2020; Wang et al., 2020; Chen et al., 2020b), especially for the EMR texts. Among different tasks to process Chinese EMR texts, CWS and medical concept recognition are two fundamental ones that draw much attentions from previous studies. Due to the dramatic performance drop when applying the model trained from open source corpus on the medical field, previous studies (Xu et al., 2014, 2015; Li et al., 2015; Zhang et al., 2016; He et al., 2017) always construct Chinese medical datasets themselves and test their models on the datasets. However, most constructed datasets used for CWS are relatively small, where there a"
2021.bionlp-1.23,D11-1090,0,0.0428177,"cription of something, rather than a direct measurement and can be used to describe the body, abnormalities, etc. E.g., “胃 肠型感冒” (gastrointestinal cold) where “胃肠型” (gastrointestinal) are Qualitative medical concepts. 4 Methods A good text representation is highly important in achieving a promising performance in many NLP tasks (Song et al., 2017; Liu and Lapata, 2018; Song and Shi, 2018). Therefore, we select several well-known models for CWS and medical concept recognition tasks and test them on ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2"
2021.bionlp-1.23,2021.naacl-main.231,1,0.755948,"n and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medical concept recognition, we regard it as a character-based sequence labeling task and perform it in a similar way with named entity 215 If a Chinese word is translated into multiple English words, we use “*” in the English translation to mark its boundary in Table 4. E.g., “3天” is translated into “*3 days*”. Me"
2021.bionlp-1.23,2021.eacl-main.326,1,0.727122,"n and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medical concept recognition, we regard it as a character-based sequence labeling task and perform it in a similar way with named entity 215 If a Chinese word is translated into multiple English words, we use “*” in the English translation to mark its boundary in Table 4. E.g., “3天” is translated into “*3 days*”. Me"
2021.bionlp-1.23,C18-1307,0,0.0182227,"ersity of Hong Kong (Shenzhen), ♥ University of Washington ♣ PingHu Hospital of Shenzhen University 4 Shenzhen Hospital of Shanghai University of Traditional Chinese Medicine ♦ Shenzhen Research Institute of Big Data ♠ yangliu5@link.cuhk.edu.cn ♥ yhtian@uw.edu ♣ wusong@szu.edu.cn ♦ wanxiang@sribd.cn ♠ {changtsunghui,songyan}@cuhk.edu.cn Abstract Chinese word segmentation (CWS) and medical concept recognition are two important and related Chinese word segmentation (CWS) and medtasks for Chinese MLP, which received much attenical concept recognition are two fundamental tion in previous studies (Xing et al., 2018; Wang tasks to process Chinese electronic medical et al., 2019). The first task (i.e., CWS) aims to records (EMRs) and play important roles in segment Chinese text (i.e., character sequence) into downstream tasks for understanding Chinese EMRs. One challenge to these tasks is the words, which is a necessary step for MLP because lack of medical domain datasets with highthe meaning of many medical terms cannot be simquality annotations, especially medical-related ply inferred by its component characters. For examtags that reveal the characteristics of Chinese ple, it is hard to infer the meanin"
2021.bionlp-1.23,2020.acl-main.577,0,0.019368,"ACEMR corpus. 4.1 CWS for Chinese EMR For CWS, we follow the convention in previous CWS studies (Sun and Xu, 2011; Song et al., 2012; Song and Xia, 2013; Chen et al., 2015; Zhang et al., 2016; Qiu et al., 2019) to regard it as a sequence labeling task with the “BIES” scheme. We select four well-know models, namely, BiLSTM, BERT (Devlin et al., 2019), ZEN (Diao et al., 2020), and WMSeg (Tian et al., 2020d) with softmax and CRF decoder. Herein, BERT and ZEN are pre-trained language models that have achieved state-of-the-art performance in many NLP tasks (Liang et al., 2020; Tian et al., 2020c; Yu et al., 2020; Nie et al., 2020; Luoma and Pyysalo, 2020; Chen et al., 2020a; Helwe et al., 2020; Tian et al., 2021a,b). WMSeg is CWS model that leverages key-value memory networks (KVMN) (Miller et al., 2016) to incorporate wordhood information to improve model performance, which achieves state-of-the-art performance on many CWS benchmark datasets. 4.2 Medical Concept Recognition Similarly, for medical concept recognition, we regard it as a character-based sequence labeling task and perform it in a similar way with named entity 215 If a Chinese word is translated into multiple English words, we use “*” in"
2021.eacl-main.326,2020.acl-main.514,0,0.0197027,"he set of sentiment polarities for y and p computes the probability of predicting y 2 T given X and A. yˆ refers to the predicted sentiment polarity type for A in the context of X . In the rest of this section, we firstly describe KVMN for leveraging word dependencies, then explain how the resulted representations are integrated into the backbone sentiment classifier. 2.1 KVMN for Word Dependencies High quality text representations always play a crucial role to obtain good model performance for different NLP tasks (Song et al., 2017; Seyler et al., 2018; Song and Shi, 2018; Song et al., 2018; Babanejad et al., 2020), where contextual features, including n-grams and syntactic information, have been demonstrated to be effective in enhancing text representation and thus leads to improvements on different models (Song et al., 2006, 2009; Song 3727 and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Seyler et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020b,c,d,e; Chen et al., 2020). Among all these features, dependency ones have been widely used, especially for ASA. To incorporate word dependencies into AS"
2021.eacl-main.326,P19-1279,0,0.0302928,"VMN models word dependencies from X only, A only, and X + A, respectively. the collection thus stops here. Therefore, the resulted words (keys) in second-order dependencies and their corresponding dependency types (values) for “service” are K11 = {bar, poor, f antastic, although, mark}, and V11 = {bar compound, poor nsubj, f antastic advcl, although mark, is cop}, respectively. 3.3 Implementation Details We adopt BERT-base-uncased and BERT-largeuncased7 as the encoders in our approach, which are demonstrated to be the most effective encoders for many NLP tasks (Strakov´a et al., 2019; Baldini Soares et al., 2019; Xu et al., 2019). In our experiments, we use their default settings for the two BERT encoders (i.e., for BERT-base-uncased, we use 12 layers with 768 dimensional hidden vectors; and for BERT-large-uncased, we use 24 layers with 1024 dimensional hidden vectors). For all experiments, we use Adam optimizer (Kingma and Ba, 2014) and try different combinations of learning rates, dropout rates, and batch size.8 In addition, we apply Xavier initialization (Glorot and Bengio, 2010) on all trainable parameters including the embeddings for keys and values in the KVMN. Moreover, we use the cross-entrop"
2021.eacl-main.326,2020.coling-main.24,1,0.878698,"model performance for different NLP tasks (Song et al., 2017; Seyler et al., 2018; Song and Shi, 2018; Song et al., 2018; Babanejad et al., 2020), where contextual features, including n-grams and syntactic information, have been demonstrated to be effective in enhancing text representation and thus leads to improvements on different models (Song et al., 2006, 2009; Song 3727 and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Seyler et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020b,c,d,e; Chen et al., 2020). Among all these features, dependency ones have been widely used, especially for ASA. To incorporate word dependencies into ASA task, there are many options, including attention mechanism (He et al., 2018a) where the information of dependency types among word pairs are omitted, and GNN and Transformerbased methods (Sun et al., 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020) that require complicated architectures to model the entire dependency structure of an input text. Compared to these options, KVMN, whose variants have been demonstrated to be effective in incorporating cont"
2021.eacl-main.326,D17-1047,0,0.0987093,"y scenarios, e.g., product review analysis, social media tracking, etc., ASA attracts much attention in the natural language processing (NLP) community for years (Tang et al., 2016a,b; He et al., 2018a; Sun et al., 2019; Zhang et al., 2019; Song et al., 2019; Huang and Carley, 2019). * Equal contribution. Corresponding author. 1 The code and different models are released at https: //github.com/cuhksz-nlp/ASA-WD. † In recent studies, neural networks, especially recurrent models with attention mechanism, are widely applied in this task, where many of them (Wang et al., 2016; Tang et al., 2016a; Chen et al., 2017; Ma et al., 2017; Fan et al., 2018; Liang et al., 2019; Tang et al., 2020) model semantic relatedness between context and aspect words to facilitate sentiment analysis on aspects. There are other approaches using additional inputs such as word position (Gu et al., 2018), document information (He et al., 2018b; Li et al., 2018a), commonsense knowledge (Ma et al., 2018). Among all such inputs, dependency results of the input text are proved to be a kind of useful information (He et al., 2018a; Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020), b"
2021.eacl-main.326,P19-1052,0,0.0613631,"also effectively weights them through the memory mechanism according to their contributions to the ASA task. We evaluate the proposed approach on five benchmark datasets, where our approach outperforms the baselines on all datasets and achieves state-of-the-art on three of them. 2 The Approach The task of ASA aims to analyze the sentiment of a text towards a specific aspect, which is formalized as a classification task performing on sentence-aspect pairs (Tang et al., 2016b; Ma et al., 2017; Xue and Li, 2018; Hazarika et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Tang et al., 2019; Chen and Qian, 2019; Tan et al., 2019). In detail, each input sentence and the aspect terms in it are denoted by X = x1 , x2 , · · · , xn and A = a1 , a2 · · · , am , respectively, where A is the sub-string of X (A ⇢ X ), n and m refer to the word-based length of X and A. Following this paradigm, we design the architecture of our approach in Figure 1, with a BERT-based (Devlin et al., 2019) encoder illustrated on the left to compute the sentence-aspect pair representation r, and enhanced by the word dependency information obtained from the KVMN module on the right, then the result is fed into a softmax decoder t"
2021.eacl-main.326,N19-1423,0,0.0222577,"pect, which is formalized as a classification task performing on sentence-aspect pairs (Tang et al., 2016b; Ma et al., 2017; Xue and Li, 2018; Hazarika et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Tang et al., 2019; Chen and Qian, 2019; Tan et al., 2019). In detail, each input sentence and the aspect terms in it are denoted by X = x1 , x2 , · · · , xn and A = a1 , a2 · · · , am , respectively, where A is the sub-string of X (A ⇢ X ), n and m refer to the word-based length of X and A. Following this paradigm, we design the architecture of our approach in Figure 1, with a BERT-based (Devlin et al., 2019) encoder illustrated on the left to compute the sentence-aspect pair representation r, and enhanced by the word dependency information obtained from the KVMN module on the right, then the result is fed into a softmax decoder to predict the text sentiment towards the aspect. Therefore, ASA through our approach can be formalized as yˆ = arg max p(y|X , A, KVMN(X , A)) (1) y2T where T denotes the set of sentiment polarities for y and p computes the probability of predicting y 2 T given X and A. yˆ refers to the predicted sentiment polarity type for A in the context of X . In the rest of this sect"
2021.eacl-main.326,P14-2009,0,0.387134,"tions are integrated into the backbone sentiment classifier. 2.1 KVMN for Word Dependencies High quality text representations always play a crucial role to obtain good model performance for different NLP tasks (Song et al., 2017; Seyler et al., 2018; Song and Shi, 2018; Song et al., 2018; Babanejad et al., 2020), where contextual features, including n-grams and syntactic information, have been demonstrated to be effective in enhancing text representation and thus leads to improvements on different models (Song et al., 2006, 2009; Song 3727 and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Seyler et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020b,c,d,e; Chen et al., 2020). Among all these features, dependency ones have been widely used, especially for ASA. To incorporate word dependencies into ASA task, there are many options, including attention mechanism (He et al., 2018a) where the information of dependency types among word pairs are omitted, and GNN and Transformerbased methods (Sun et al., 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020) that require complicated architecture"
2021.eacl-main.326,D19-1551,0,0.011166,"classification with applying neural approaches (Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Xue and Li, 2018; Li et al., 2018b; Hu et al., 2019; Xu et al., 2019) such as recurrent models (e.g., bi-LSTM) and pretrained encoders (e.g., BERT) for effectively capturing contextual information. In addition to improving the input form, advanced models such as memory networks (Tang et al., 2016b; Chen et al., 2017; Wang et al., 2018; Zhu and Qian, 2018; Mao et al., 2019), attention mechanism (Wang et al., 2016; Ma et al., 2017; Hazarika et al., 2018), capsule networks (Du et al., 2019; Chen and Qian, 2019; Jiang et al., 2019), GNN (Huang and Carley, 2019; Sun et al., 2019; Zhang et al., 2019; Wang et al., 2020), and transformer (Tang et al., 2020) are applied to this task, with other studies leveraging external resources, including position information (Gu et al., 2018), document information (He et al., 2018b), commonsense knowledge (Ma et al., 2018), etc. Among all resources, syntactic information was proved to be the most effective one and successfully adopted in recent studies with GNN (Huang and Carley, 2019; Sun et al., 2019; Zhang et al., 2019). Compared with previou"
2021.eacl-main.326,D18-1380,0,0.0377375,"Missing"
2021.eacl-main.326,C18-1066,0,0.231461,"rley, 2019). * Equal contribution. Corresponding author. 1 The code and different models are released at https: //github.com/cuhksz-nlp/ASA-WD. † In recent studies, neural networks, especially recurrent models with attention mechanism, are widely applied in this task, where many of them (Wang et al., 2016; Tang et al., 2016a; Chen et al., 2017; Ma et al., 2017; Fan et al., 2018; Liang et al., 2019; Tang et al., 2020) model semantic relatedness between context and aspect words to facilitate sentiment analysis on aspects. There are other approaches using additional inputs such as word position (Gu et al., 2018), document information (He et al., 2018b; Li et al., 2018a), commonsense knowledge (Ma et al., 2018). Among all such inputs, dependency results of the input text are proved to be a kind of useful information (He et al., 2018a; Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020), because they can help the model locate important content that modifies the aspect words and thus further suggests the sentiment towards the aspect words. Previous approaches with attention mechanism (He et al., 2018a; Wang et al., 2020), graph neural networks (GNN) (Sun e"
2021.eacl-main.326,N19-1340,0,0.0261899,"ones have been widely used, especially for ASA. To incorporate word dependencies into ASA task, there are many options, including attention mechanism (He et al., 2018a) where the information of dependency types among word pairs are omitted, and GNN and Transformerbased methods (Sun et al., 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020) that require complicated architectures to model the entire dependency structure of an input text. Compared to these options, KVMN, whose variants have been demonstrated to be effective in incorporating contextual features (Miller et al., 2016; Guan et al., 2019; Song et al., 2020; Tian et al., 2020a,f; Nie et al., 2020), not only provides an appropriate way to leverage both word-word relations as well as their corresponding dependency types, but also weights different dependency information according to their contribution to the ASA task. In detail, to build the KVMN, we firstly collect all word-word relations extracted from the parse results of a corpus via an off-the-shelf toolkit and use them to form the key set, and map their corresponding dependency types to the value set. Then, two embedding matrices, K and V are applied to the key and value s"
2021.eacl-main.326,C18-1096,0,0.192978,"sentiment polarity of a given input text on the fine-grained level, where the sentiment towards a particular aspect in the text is predicted instead of the entire input. E.g., the sentiment of an aspect “bar service” in the sentence “Total environment is fantastic although bar service is poor.” is negative, although the text as a whole conveys a positive sentiment polarity. Due to its high practical value in many scenarios, e.g., product review analysis, social media tracking, etc., ASA attracts much attention in the natural language processing (NLP) community for years (Tang et al., 2016a,b; He et al., 2018a; Sun et al., 2019; Zhang et al., 2019; Song et al., 2019; Huang and Carley, 2019). * Equal contribution. Corresponding author. 1 The code and different models are released at https: //github.com/cuhksz-nlp/ASA-WD. † In recent studies, neural networks, especially recurrent models with attention mechanism, are widely applied in this task, where many of them (Wang et al., 2016; Tang et al., 2016a; Chen et al., 2017; Ma et al., 2017; Fan et al., 2018; Liang et al., 2019; Tang et al., 2020) model semantic relatedness between context and aspect words to facilitate sentiment analysis on aspects. Th"
2021.eacl-main.326,P18-2092,0,0.212857,"sentiment polarity of a given input text on the fine-grained level, where the sentiment towards a particular aspect in the text is predicted instead of the entire input. E.g., the sentiment of an aspect “bar service” in the sentence “Total environment is fantastic although bar service is poor.” is negative, although the text as a whole conveys a positive sentiment polarity. Due to its high practical value in many scenarios, e.g., product review analysis, social media tracking, etc., ASA attracts much attention in the natural language processing (NLP) community for years (Tang et al., 2016a,b; He et al., 2018a; Sun et al., 2019; Zhang et al., 2019; Song et al., 2019; Huang and Carley, 2019). * Equal contribution. Corresponding author. 1 The code and different models are released at https: //github.com/cuhksz-nlp/ASA-WD. † In recent studies, neural networks, especially recurrent models with attention mechanism, are widely applied in this task, where many of them (Wang et al., 2016; Tang et al., 2016a; Chen et al., 2017; Ma et al., 2017; Fan et al., 2018; Liang et al., 2019; Tang et al., 2020) model semantic relatedness between context and aspect words to facilitate sentiment analysis on aspects. Th"
2021.eacl-main.326,D19-1467,0,0.0725063,"“conj”). 6 Related Work Different from sentiment analysis for large granular texts, such as document and sentences, ASA focuses on processing sentiment polarities for a specific aspect (e.g., “pizza”) or category (e.g., “food”) in a piece of text. To address this task, early approaches (Jiang et al., 2011; Dong et al., 2014) followed the sentence classification paradigm and recent studies enhanced it as a mission of sentenceaspect pair classification with applying neural approaches (Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Xue and Li, 2018; Li et al., 2018b; Hu et al., 2019; Xu et al., 2019) such as recurrent models (e.g., bi-LSTM) and pretrained encoders (e.g., BERT) for effectively capturing contextual information. In addition to improving the input form, advanced models such as memory networks (Tang et al., 2016b; Chen et al., 2017; Wang et al., 2018; Zhu and Qian, 2018; Mao et al., 2019), attention mechanism (Wang et al., 2016; Ma et al., 2017; Hazarika et al., 2018), capsule networks (Du et al., 2019; Chen and Qian, 2019; Jiang et al., 2019), GNN (Huang and Carley, 2019; Sun et al., 2019; Zhang et al., 2019; Wang et al., 2020), and transformer (Tang et al.,"
2021.eacl-main.326,D18-1136,0,0.0749781,"d relations and their dependency types, but also effectively weights them through the memory mechanism according to their contributions to the ASA task. We evaluate the proposed approach on five benchmark datasets, where our approach outperforms the baselines on all datasets and achieves state-of-the-art on three of them. 2 The Approach The task of ASA aims to analyze the sentiment of a text towards a specific aspect, which is formalized as a classification task performing on sentence-aspect pairs (Tang et al., 2016b; Ma et al., 2017; Xue and Li, 2018; Hazarika et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Tang et al., 2019; Chen and Qian, 2019; Tan et al., 2019). In detail, each input sentence and the aspect terms in it are denoted by X = x1 , x2 , · · · , xn and A = a1 , a2 · · · , am , respectively, where A is the sub-string of X (A ⇢ X ), n and m refer to the word-based length of X and A. Following this paradigm, we design the architecture of our approach in Figure 1, with a BERT-based (Devlin et al., 2019) encoder illustrated on the left to compute the sentence-aspect pair representation r, and enhanced by the word dependency information obtained from the KVMN module on the right, then th"
2021.eacl-main.326,D19-1549,0,0.183124,"he sentiment towards a particular aspect in the text is predicted instead of the entire input. E.g., the sentiment of an aspect “bar service” in the sentence “Total environment is fantastic although bar service is poor.” is negative, although the text as a whole conveys a positive sentiment polarity. Due to its high practical value in many scenarios, e.g., product review analysis, social media tracking, etc., ASA attracts much attention in the natural language processing (NLP) community for years (Tang et al., 2016a,b; He et al., 2018a; Sun et al., 2019; Zhang et al., 2019; Song et al., 2019; Huang and Carley, 2019). * Equal contribution. Corresponding author. 1 The code and different models are released at https: //github.com/cuhksz-nlp/ASA-WD. † In recent studies, neural networks, especially recurrent models with attention mechanism, are widely applied in this task, where many of them (Wang et al., 2016; Tang et al., 2016a; Chen et al., 2017; Ma et al., 2017; Fan et al., 2018; Liang et al., 2019; Tang et al., 2020) model semantic relatedness between context and aspect words to facilitate sentiment analysis on aspects. There are other approaches using additional inputs such as word position (Gu et al.,"
2021.eacl-main.326,P11-1016,0,0.0633266,"rmation (oi ) comes from KVMN. Therefore, the different prediction results for the two aspects suggest that KVMN appropriately learns from salient dependency relations and types for each aspect, where different types have their own capabilities to enhance ASA accordingly (e.g., “nsubj” may contribute more than “conj”). 6 Related Work Different from sentiment analysis for large granular texts, such as document and sentences, ASA focuses on processing sentiment polarities for a specific aspect (e.g., “pizza”) or category (e.g., “food”) in a piece of text. To address this task, early approaches (Jiang et al., 2011; Dong et al., 2014) followed the sentence classification paradigm and recent studies enhanced it as a mission of sentenceaspect pair classification with applying neural approaches (Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Xue and Li, 2018; Li et al., 2018b; Hu et al., 2019; Xu et al., 2019) such as recurrent models (e.g., bi-LSTM) and pretrained encoders (e.g., BERT) for effectively capturing contextual information. In addition to improving the input form, advanced models such as memory networks (Tang et al., 2016b; Chen et al., 2017; Wang et al., 2018; Zhu a"
2021.eacl-main.326,D19-1654,0,0.0171453,"pproaches (Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Xue and Li, 2018; Li et al., 2018b; Hu et al., 2019; Xu et al., 2019) such as recurrent models (e.g., bi-LSTM) and pretrained encoders (e.g., BERT) for effectively capturing contextual information. In addition to improving the input form, advanced models such as memory networks (Tang et al., 2016b; Chen et al., 2017; Wang et al., 2018; Zhu and Qian, 2018; Mao et al., 2019), attention mechanism (Wang et al., 2016; Ma et al., 2017; Hazarika et al., 2018), capsule networks (Du et al., 2019; Chen and Qian, 2019; Jiang et al., 2019), GNN (Huang and Carley, 2019; Sun et al., 2019; Zhang et al., 2019; Wang et al., 2020), and transformer (Tang et al., 2020) are applied to this task, with other studies leveraging external resources, including position information (Gu et al., 2018), document information (He et al., 2018b), commonsense knowledge (Ma et al., 2018), etc. Among all resources, syntactic information was proved to be the most effective one and successfully adopted in recent studies with GNN (Huang and Carley, 2019; Sun et al., 2019; Zhang et al., 2019). Compared with previous studies, our approach offers an alternat"
2021.eacl-main.326,C18-1079,0,0.106317,"The code and different models are released at https: //github.com/cuhksz-nlp/ASA-WD. † In recent studies, neural networks, especially recurrent models with attention mechanism, are widely applied in this task, where many of them (Wang et al., 2016; Tang et al., 2016a; Chen et al., 2017; Ma et al., 2017; Fan et al., 2018; Liang et al., 2019; Tang et al., 2020) model semantic relatedness between context and aspect words to facilitate sentiment analysis on aspects. There are other approaches using additional inputs such as word position (Gu et al., 2018), document information (He et al., 2018b; Li et al., 2018a), commonsense knowledge (Ma et al., 2018). Among all such inputs, dependency results of the input text are proved to be a kind of useful information (He et al., 2018a; Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020), because they can help the model locate important content that modifies the aspect words and thus further suggests the sentiment towards the aspect words. Previous approaches with attention mechanism (He et al., 2018a; Wang et al., 2020), graph neural networks (GNN) (Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019;"
2021.eacl-main.326,P18-1087,0,0.0876779,"The code and different models are released at https: //github.com/cuhksz-nlp/ASA-WD. † In recent studies, neural networks, especially recurrent models with attention mechanism, are widely applied in this task, where many of them (Wang et al., 2016; Tang et al., 2016a; Chen et al., 2017; Ma et al., 2017; Fan et al., 2018; Liang et al., 2019; Tang et al., 2020) model semantic relatedness between context and aspect words to facilitate sentiment analysis on aspects. There are other approaches using additional inputs such as word position (Gu et al., 2018), document information (He et al., 2018b; Li et al., 2018a), commonsense knowledge (Ma et al., 2018). Among all such inputs, dependency results of the input text are proved to be a kind of useful information (He et al., 2018a; Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020), because they can help the model locate important content that modifies the aspect words and thus further suggests the sentiment towards the aspect words. Previous approaches with attention mechanism (He et al., 2018a; Wang et al., 2020), graph neural networks (GNN) (Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019;"
2021.eacl-main.326,P19-1462,0,0.0195841,"ia tracking, etc., ASA attracts much attention in the natural language processing (NLP) community for years (Tang et al., 2016a,b; He et al., 2018a; Sun et al., 2019; Zhang et al., 2019; Song et al., 2019; Huang and Carley, 2019). * Equal contribution. Corresponding author. 1 The code and different models are released at https: //github.com/cuhksz-nlp/ASA-WD. † In recent studies, neural networks, especially recurrent models with attention mechanism, are widely applied in this task, where many of them (Wang et al., 2016; Tang et al., 2016a; Chen et al., 2017; Ma et al., 2017; Fan et al., 2018; Liang et al., 2019; Tang et al., 2020) model semantic relatedness between context and aspect words to facilitate sentiment analysis on aspects. There are other approaches using additional inputs such as word position (Gu et al., 2018), document information (He et al., 2018b; Li et al., 2018a), commonsense knowledge (Ma et al., 2018). Among all such inputs, dependency results of the input text are proved to be a kind of useful information (He et al., 2018a; Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020), because they can help the model locate important content"
2021.eacl-main.326,J93-2004,0,0.0768025,"in some datasets, e.g., LAP14 and REST14, there are rather high percentages of sentences (e.g., the sentence in Figure 1) that contain different sentiments towards aspects, as shown in the D IFF . rows in Table 1, which indicates a bigger challenge on ASA comparing to sentiment analysis on an entire sentence. 3.2 Word Dependency Extraction Similar to previous studies (Wang et al., 2020; Tang et al., 2020) that also require dependency information, we employ the English version of SAPar5 (Tian et al., 2020e), which is the most effective constituency parser trained on English Pen TreeBank (PTB) (Marcus et al., 1993), to obtain the constituency trees of the input text and then convert because that many sentences have more than one aspect and such aspects usually have contrastive sentiment polarities. 4 The “conflict” label is used in LAP14, REST14/16 to identify aspects that have conflict sentiment polarities. 5 https://github.com/cuhksz-nlp/SAPar them into dependency trees by Stanford converter6 . Therefore, when a dependency tree is built on the entire input text, for each word in the text, one can find its dependent words and types according to the dependency paths on the tree. Consequently, the depend"
2021.eacl-main.326,D16-1147,0,0.464653,"formation mark … fantastic although = Total environment is fantastic although bar service is poor compound bar nsubj service cop is poor = bar service Figure 1: The overall architecture of the proposed model. The left part illustrates the backbone encoder (BERT) and decoder for ASA; the right part demonstrates the key-value memory networks (KVMN) for dependency information incorporation, where we use example word dependencies and their types (highlighted in yellow) of the aspect term “service” to show that how they are extracted, weighted and then fed into the left part for ASA. works (KVMN) (Miller et al., 2016). In detail, for each input text parsed by a dependency parser, we extract its dependency relations and feed them into the KVMN, in which word-word associations and their corresponding dependency types are mapped to keys and values, respectively. Then the KVMN learns and weights different dependency knowledge according to the contribution of their corresponding keys to the ASA task, and provides the resulted representations to a regular ASA model, i.e., a BERT-based classifier, for final aspect-level sentiment predictions. In doing so, the proposed approach not only comprehensively leverages b"
2021.eacl-main.326,2020.findings-emnlp.378,1,0.615861,"ate word dependencies into ASA task, there are many options, including attention mechanism (He et al., 2018a) where the information of dependency types among word pairs are omitted, and GNN and Transformerbased methods (Sun et al., 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020) that require complicated architectures to model the entire dependency structure of an input text. Compared to these options, KVMN, whose variants have been demonstrated to be effective in incorporating contextual features (Miller et al., 2016; Guan et al., 2019; Song et al., 2020; Tian et al., 2020a,f; Nie et al., 2020), not only provides an appropriate way to leverage both word-word relations as well as their corresponding dependency types, but also weights different dependency information according to their contribution to the ASA task. In detail, to build the KVMN, we firstly collect all word-word relations extracted from the parse results of a corpus via an off-the-shelf toolkit and use them to form the key set, and map their corresponding dependency types to the value set. Then, two embedding matrices, K and V are applied to the key and value sets with each vector representing a key or a value in the se"
2021.eacl-main.326,S15-2082,0,0.0815571,"Missing"
2021.eacl-main.326,S14-2004,0,0.255342,"Missing"
2021.eacl-main.326,P18-2039,0,0.0282568,"= arg max p(y|X , A, KVMN(X , A)) (1) y2T where T denotes the set of sentiment polarities for y and p computes the probability of predicting y 2 T given X and A. yˆ refers to the predicted sentiment polarity type for A in the context of X . In the rest of this section, we firstly describe KVMN for leveraging word dependencies, then explain how the resulted representations are integrated into the backbone sentiment classifier. 2.1 KVMN for Word Dependencies High quality text representations always play a crucial role to obtain good model performance for different NLP tasks (Song et al., 2017; Seyler et al., 2018; Song and Shi, 2018; Song et al., 2018; Babanejad et al., 2020), where contextual features, including n-grams and syntactic information, have been demonstrated to be effective in enhancing text representation and thus leads to improvements on different models (Song et al., 2006, 2009; Song 3727 and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Seyler et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020b,c,d,e; Chen et al., 2020). Among all these features, dependency ones have been widely us"
2021.eacl-main.326,W06-0137,1,0.454555,"describe KVMN for leveraging word dependencies, then explain how the resulted representations are integrated into the backbone sentiment classifier. 2.1 KVMN for Word Dependencies High quality text representations always play a crucial role to obtain good model performance for different NLP tasks (Song et al., 2017; Seyler et al., 2018; Song and Shi, 2018; Song et al., 2018; Babanejad et al., 2020), where contextual features, including n-grams and syntactic information, have been demonstrated to be effective in enhancing text representation and thus leads to improvements on different models (Song et al., 2006, 2009; Song 3727 and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Seyler et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020b,c,d,e; Chen et al., 2020). Among all these features, dependency ones have been widely used, especially for ASA. To incorporate word dependencies into ASA task, there are many options, including attention mechanism (He et al., 2018a) where the information of dependency types among word pairs are omitted, and GNN and Transformerbased methods (Sun et al., 2019; Zhang"
2021.eacl-main.326,W09-3511,1,0.725126,"Missing"
2021.eacl-main.326,C12-2116,1,0.717064,"hen explain how the resulted representations are integrated into the backbone sentiment classifier. 2.1 KVMN for Word Dependencies High quality text representations always play a crucial role to obtain good model performance for different NLP tasks (Song et al., 2017; Seyler et al., 2018; Song and Shi, 2018; Song et al., 2018; Babanejad et al., 2020), where contextual features, including n-grams and syntactic information, have been demonstrated to be effective in enhancing text representation and thus leads to improvements on different models (Song et al., 2006, 2009; Song 3727 and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Seyler et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020b,c,d,e; Chen et al., 2020). Among all these features, dependency ones have been widely used, especially for ASA. To incorporate word dependencies into ASA task, there are many options, including attention mechanism (He et al., 2018a) where the information of dependency types among word pairs are omitted, and GNN and Transformerbased methods (Sun et al., 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020"
2021.eacl-main.326,K17-1016,1,0.560636,"be formalized as yˆ = arg max p(y|X , A, KVMN(X , A)) (1) y2T where T denotes the set of sentiment polarities for y and p computes the probability of predicting y 2 T given X and A. yˆ refers to the predicted sentiment polarity type for A in the context of X . In the rest of this section, we firstly describe KVMN for leveraging word dependencies, then explain how the resulted representations are integrated into the backbone sentiment classifier. 2.1 KVMN for Word Dependencies High quality text representations always play a crucial role to obtain good model performance for different NLP tasks (Song et al., 2017; Seyler et al., 2018; Song and Shi, 2018; Song et al., 2018; Babanejad et al., 2020), where contextual features, including n-grams and syntactic information, have been demonstrated to be effective in enhancing text representation and thus leads to improvements on different models (Song et al., 2006, 2009; Song 3727 and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Seyler et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020b,c,d,e; Chen et al., 2020). Among all these features, dependency one"
2021.eacl-main.326,N18-2028,1,0.827858,"T where T denotes the set of sentiment polarities for y and p computes the probability of predicting y 2 T given X and A. yˆ refers to the predicted sentiment polarity type for A in the context of X . In the rest of this section, we firstly describe KVMN for leveraging word dependencies, then explain how the resulted representations are integrated into the backbone sentiment classifier. 2.1 KVMN for Word Dependencies High quality text representations always play a crucial role to obtain good model performance for different NLP tasks (Song et al., 2017; Seyler et al., 2018; Song and Shi, 2018; Song et al., 2018; Babanejad et al., 2020), where contextual features, including n-grams and syntactic information, have been demonstrated to be effective in enhancing text representation and thus leads to improvements on different models (Song et al., 2006, 2009; Song 3727 and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Seyler et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020b,c,d,e; Chen et al., 2020). Among all these features, dependency ones have been widely used, especially for ASA. To incorporate"
2021.eacl-main.326,2020.coling-main.63,1,0.527557,"ely used, especially for ASA. To incorporate word dependencies into ASA task, there are many options, including attention mechanism (He et al., 2018a) where the information of dependency types among word pairs are omitted, and GNN and Transformerbased methods (Sun et al., 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020) that require complicated architectures to model the entire dependency structure of an input text. Compared to these options, KVMN, whose variants have been demonstrated to be effective in incorporating contextual features (Miller et al., 2016; Guan et al., 2019; Song et al., 2020; Tian et al., 2020a,f; Nie et al., 2020), not only provides an appropriate way to leverage both word-word relations as well as their corresponding dependency types, but also weights different dependency information according to their contribution to the ASA task. In detail, to build the KVMN, we firstly collect all word-word relations extracted from the parse results of a corpus via an off-the-shelf toolkit and use them to form the key set, and map their corresponding dependency types to the value set. Then, two embedding matrices, K and V are applied to the key and value sets with each vecto"
2021.eacl-main.326,song-xia-2012-using,1,0.884977,"Missing"
2021.eacl-main.326,I13-1071,1,0.862302,"resulted representations are integrated into the backbone sentiment classifier. 2.1 KVMN for Word Dependencies High quality text representations always play a crucial role to obtain good model performance for different NLP tasks (Song et al., 2017; Seyler et al., 2018; Song and Shi, 2018; Song et al., 2018; Babanejad et al., 2020), where contextual features, including n-grams and syntactic information, have been demonstrated to be effective in enhancing text representation and thus leads to improvements on different models (Song et al., 2006, 2009; Song 3727 and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Seyler et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020b,c,d,e; Chen et al., 2020). Among all these features, dependency ones have been widely used, especially for ASA. To incorporate word dependencies into ASA task, there are many options, including attention mechanism (He et al., 2018a) where the information of dependency types among word pairs are omitted, and GNN and Transformerbased methods (Sun et al., 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020) that require compl"
2021.eacl-main.326,P19-1527,0,0.0632708,"Missing"
2021.eacl-main.326,D19-1569,0,0.463998,"of a given input text on the fine-grained level, where the sentiment towards a particular aspect in the text is predicted instead of the entire input. E.g., the sentiment of an aspect “bar service” in the sentence “Total environment is fantastic although bar service is poor.” is negative, although the text as a whole conveys a positive sentiment polarity. Due to its high practical value in many scenarios, e.g., product review analysis, social media tracking, etc., ASA attracts much attention in the natural language processing (NLP) community for years (Tang et al., 2016a,b; He et al., 2018a; Sun et al., 2019; Zhang et al., 2019; Song et al., 2019; Huang and Carley, 2019). * Equal contribution. Corresponding author. 1 The code and different models are released at https: //github.com/cuhksz-nlp/ASA-WD. † In recent studies, neural networks, especially recurrent models with attention mechanism, are widely applied in this task, where many of them (Wang et al., 2016; Tang et al., 2016a; Chen et al., 2017; Ma et al., 2017; Fan et al., 2018; Liang et al., 2019; Tang et al., 2020) model semantic relatedness between context and aspect words to facilitate sentiment analysis on aspects. There are other appro"
2021.eacl-main.326,D19-1342,0,0.238832,"ghts them through the memory mechanism according to their contributions to the ASA task. We evaluate the proposed approach on five benchmark datasets, where our approach outperforms the baselines on all datasets and achieves state-of-the-art on three of them. 2 The Approach The task of ASA aims to analyze the sentiment of a text towards a specific aspect, which is formalized as a classification task performing on sentence-aspect pairs (Tang et al., 2016b; Ma et al., 2017; Xue and Li, 2018; Hazarika et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Tang et al., 2019; Chen and Qian, 2019; Tan et al., 2019). In detail, each input sentence and the aspect terms in it are denoted by X = x1 , x2 , · · · , xn and A = a1 , a2 · · · , am , respectively, where A is the sub-string of X (A ⇢ X ), n and m refer to the word-based length of X and A. Following this paradigm, we design the architecture of our approach in Figure 1, with a BERT-based (Devlin et al., 2019) encoder illustrated on the left to compute the sentence-aspect pair representation r, and enhanced by the word dependency information obtained from the KVMN module on the right, then the result is fed into a softmax decoder to predict the text"
2021.eacl-main.326,C16-1311,0,0.595229,"(ASA) determines the sentiment polarity of a given input text on the fine-grained level, where the sentiment towards a particular aspect in the text is predicted instead of the entire input. E.g., the sentiment of an aspect “bar service” in the sentence “Total environment is fantastic although bar service is poor.” is negative, although the text as a whole conveys a positive sentiment polarity. Due to its high practical value in many scenarios, e.g., product review analysis, social media tracking, etc., ASA attracts much attention in the natural language processing (NLP) community for years (Tang et al., 2016a,b; He et al., 2018a; Sun et al., 2019; Zhang et al., 2019; Song et al., 2019; Huang and Carley, 2019). * Equal contribution. Corresponding author. 1 The code and different models are released at https: //github.com/cuhksz-nlp/ASA-WD. † In recent studies, neural networks, especially recurrent models with attention mechanism, are widely applied in this task, where many of them (Wang et al., 2016; Tang et al., 2016a; Chen et al., 2017; Ma et al., 2017; Fan et al., 2018; Liang et al., 2019; Tang et al., 2020) model semantic relatedness between context and aspect words to facilitate sentiment ana"
2021.eacl-main.326,D16-1021,0,0.536258,"(ASA) determines the sentiment polarity of a given input text on the fine-grained level, where the sentiment towards a particular aspect in the text is predicted instead of the entire input. E.g., the sentiment of an aspect “bar service” in the sentence “Total environment is fantastic although bar service is poor.” is negative, although the text as a whole conveys a positive sentiment polarity. Due to its high practical value in many scenarios, e.g., product review analysis, social media tracking, etc., ASA attracts much attention in the natural language processing (NLP) community for years (Tang et al., 2016a,b; He et al., 2018a; Sun et al., 2019; Zhang et al., 2019; Song et al., 2019; Huang and Carley, 2019). * Equal contribution. Corresponding author. 1 The code and different models are released at https: //github.com/cuhksz-nlp/ASA-WD. † In recent studies, neural networks, especially recurrent models with attention mechanism, are widely applied in this task, where many of them (Wang et al., 2016; Tang et al., 2016a; Chen et al., 2017; Ma et al., 2017; Fan et al., 2018; Liang et al., 2019; Tang et al., 2020) model semantic relatedness between context and aspect words to facilitate sentiment ana"
2021.eacl-main.326,2020.acl-main.588,0,0.591678,"SA attracts much attention in the natural language processing (NLP) community for years (Tang et al., 2016a,b; He et al., 2018a; Sun et al., 2019; Zhang et al., 2019; Song et al., 2019; Huang and Carley, 2019). * Equal contribution. Corresponding author. 1 The code and different models are released at https: //github.com/cuhksz-nlp/ASA-WD. † In recent studies, neural networks, especially recurrent models with attention mechanism, are widely applied in this task, where many of them (Wang et al., 2016; Tang et al., 2016a; Chen et al., 2017; Ma et al., 2017; Fan et al., 2018; Liang et al., 2019; Tang et al., 2020) model semantic relatedness between context and aspect words to facilitate sentiment analysis on aspects. There are other approaches using additional inputs such as word position (Gu et al., 2018), document information (He et al., 2018b; Li et al., 2018a), commonsense knowledge (Ma et al., 2018). Among all such inputs, dependency results of the input text are proved to be a kind of useful information (He et al., 2018a; Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020), because they can help the model locate important content that modifies the a"
2021.eacl-main.326,P19-1053,0,0.118438,"Missing"
2021.eacl-main.326,2020.acl-main.735,1,0.942296,"ucial role to obtain good model performance for different NLP tasks (Song et al., 2017; Seyler et al., 2018; Song and Shi, 2018; Song et al., 2018; Babanejad et al., 2020), where contextual features, including n-grams and syntactic information, have been demonstrated to be effective in enhancing text representation and thus leads to improvements on different models (Song et al., 2006, 2009; Song 3727 and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Seyler et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020b,c,d,e; Chen et al., 2020). Among all these features, dependency ones have been widely used, especially for ASA. To incorporate word dependencies into ASA task, there are many options, including attention mechanism (He et al., 2018a) where the information of dependency types among word pairs are omitted, and GNN and Transformerbased methods (Sun et al., 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020) that require complicated architectures to model the entire dependency structure of an input text. Compared to these options, KVMN, whose variants have been demonstrated to be effe"
2021.eacl-main.326,2020.coling-main.187,1,0.684316,"ucial role to obtain good model performance for different NLP tasks (Song et al., 2017; Seyler et al., 2018; Song and Shi, 2018; Song et al., 2018; Babanejad et al., 2020), where contextual features, including n-grams and syntactic information, have been demonstrated to be effective in enhancing text representation and thus leads to improvements on different models (Song et al., 2006, 2009; Song 3727 and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Seyler et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020b,c,d,e; Chen et al., 2020). Among all these features, dependency ones have been widely used, especially for ASA. To incorporate word dependencies into ASA task, there are many options, including attention mechanism (He et al., 2018a) where the information of dependency types among word pairs are omitted, and GNN and Transformerbased methods (Sun et al., 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020) that require complicated architectures to model the entire dependency structure of an input text. Compared to these options, KVMN, whose variants have been demonstrated to be effe"
2021.eacl-main.326,2020.emnlp-main.487,1,0.903247,"ucial role to obtain good model performance for different NLP tasks (Song et al., 2017; Seyler et al., 2018; Song and Shi, 2018; Song et al., 2018; Babanejad et al., 2020), where contextual features, including n-grams and syntactic information, have been demonstrated to be effective in enhancing text representation and thus leads to improvements on different models (Song et al., 2006, 2009; Song 3727 and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Seyler et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020b,c,d,e; Chen et al., 2020). Among all these features, dependency ones have been widely used, especially for ASA. To incorporate word dependencies into ASA task, there are many options, including attention mechanism (He et al., 2018a) where the information of dependency types among word pairs are omitted, and GNN and Transformerbased methods (Sun et al., 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020) that require complicated architectures to model the entire dependency structure of an input text. Compared to these options, KVMN, whose variants have been demonstrated to be effe"
2021.eacl-main.326,2020.findings-emnlp.153,1,0.943955,"ucial role to obtain good model performance for different NLP tasks (Song et al., 2017; Seyler et al., 2018; Song and Shi, 2018; Song et al., 2018; Babanejad et al., 2020), where contextual features, including n-grams and syntactic information, have been demonstrated to be effective in enhancing text representation and thus leads to improvements on different models (Song et al., 2006, 2009; Song 3727 and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Seyler et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020b,c,d,e; Chen et al., 2020). Among all these features, dependency ones have been widely used, especially for ASA. To incorporate word dependencies into ASA task, there are many options, including attention mechanism (He et al., 2018a) where the information of dependency types among word pairs are omitted, and GNN and Transformerbased methods (Sun et al., 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020) that require complicated architectures to model the entire dependency structure of an input text. Compared to these options, KVMN, whose variants have been demonstrated to be effe"
2021.eacl-main.326,2020.acl-main.734,1,0.926726,"ucial role to obtain good model performance for different NLP tasks (Song et al., 2017; Seyler et al., 2018; Song and Shi, 2018; Song et al., 2018; Babanejad et al., 2020), where contextual features, including n-grams and syntactic information, have been demonstrated to be effective in enhancing text representation and thus leads to improvements on different models (Song et al., 2006, 2009; Song 3727 and Xia, 2012; Song et al., 2012; Song and Xia, 2013; Dong et al., 2014; Miller et al., 2016; Seyler et al., 2018; Diao et al., 2019; Sun et al., 2019; Zhang et al., 2019; Huang and Carley, 2019; Tian et al., 2020b,c,d,e; Chen et al., 2020). Among all these features, dependency ones have been widely used, especially for ASA. To incorporate word dependencies into ASA task, there are many options, including attention mechanism (He et al., 2018a) where the information of dependency types among word pairs are omitted, and GNN and Transformerbased methods (Sun et al., 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020) that require complicated architectures to model the entire dependency structure of an input text. Compared to these options, KVMN, whose variants have been demonstrated to be effe"
2021.eacl-main.326,2020.acl-main.295,0,0.435375,"6; Tang et al., 2016a; Chen et al., 2017; Ma et al., 2017; Fan et al., 2018; Liang et al., 2019; Tang et al., 2020) model semantic relatedness between context and aspect words to facilitate sentiment analysis on aspects. There are other approaches using additional inputs such as word position (Gu et al., 2018), document information (He et al., 2018b; Li et al., 2018a), commonsense knowledge (Ma et al., 2018). Among all such inputs, dependency results of the input text are proved to be a kind of useful information (He et al., 2018a; Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019; Wang et al., 2020; Tang et al., 2020), because they can help the model locate important content that modifies the aspect words and thus further suggests the sentiment towards the aspect words. Previous approaches with attention mechanism (He et al., 2018a; Wang et al., 2020), graph neural networks (GNN) (Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019; Wang et al., 2020) and transformer (Tang et al., 2020) are applied in leveraging such information. However, most of them mainly focus on using the dependencies among words and omit to leverage other information such as relation types, which could pr"
2021.eacl-main.326,P18-1088,0,0.0185112,"ches (Jiang et al., 2011; Dong et al., 2014) followed the sentence classification paradigm and recent studies enhanced it as a mission of sentenceaspect pair classification with applying neural approaches (Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Xue and Li, 2018; Li et al., 2018b; Hu et al., 2019; Xu et al., 2019) such as recurrent models (e.g., bi-LSTM) and pretrained encoders (e.g., BERT) for effectively capturing contextual information. In addition to improving the input form, advanced models such as memory networks (Tang et al., 2016b; Chen et al., 2017; Wang et al., 2018; Zhu and Qian, 2018; Mao et al., 2019), attention mechanism (Wang et al., 2016; Ma et al., 2017; Hazarika et al., 2018), capsule networks (Du et al., 2019; Chen and Qian, 2019; Jiang et al., 2019), GNN (Huang and Carley, 2019; Sun et al., 2019; Zhang et al., 2019; Wang et al., 2020), and transformer (Tang et al., 2020) are applied to this task, with other studies leveraging external resources, including position information (Gu et al., 2018), document information (He et al., 2018b), commonsense knowledge (Ma et al., 2018), etc. Among all resources, syntactic information was proved to be the m"
2021.eacl-main.326,D16-1058,0,0.130774,"Missing"
2021.eacl-main.326,N19-1242,0,0.0916502,"Missing"
2021.eacl-main.326,P18-1234,0,0.34855,"oposed approach not only comprehensively leverages both word relations and their dependency types, but also effectively weights them through the memory mechanism according to their contributions to the ASA task. We evaluate the proposed approach on five benchmark datasets, where our approach outperforms the baselines on all datasets and achieves state-of-the-art on three of them. 2 The Approach The task of ASA aims to analyze the sentiment of a text towards a specific aspect, which is formalized as a classification task performing on sentence-aspect pairs (Tang et al., 2016b; Ma et al., 2017; Xue and Li, 2018; Hazarika et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Tang et al., 2019; Chen and Qian, 2019; Tan et al., 2019). In detail, each input sentence and the aspect terms in it are denoted by X = x1 , x2 , · · · , xn and A = a1 , a2 · · · , am , respectively, where A is the sub-string of X (A ⇢ X ), n and m refer to the word-based length of X and A. Following this paradigm, we design the architecture of our approach in Figure 1, with a BERT-based (Devlin et al., 2019) encoder illustrated on the left to compute the sentence-aspect pair representation r, and enhanced by the word dependenc"
2021.eacl-main.326,D19-1464,0,0.161492,"Missing"
2021.eacl-main.326,C18-1092,0,0.0166147,"2011; Dong et al., 2014) followed the sentence classification paradigm and recent studies enhanced it as a mission of sentenceaspect pair classification with applying neural approaches (Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Xue and Li, 2018; Li et al., 2018b; Hu et al., 2019; Xu et al., 2019) such as recurrent models (e.g., bi-LSTM) and pretrained encoders (e.g., BERT) for effectively capturing contextual information. In addition to improving the input form, advanced models such as memory networks (Tang et al., 2016b; Chen et al., 2017; Wang et al., 2018; Zhu and Qian, 2018; Mao et al., 2019), attention mechanism (Wang et al., 2016; Ma et al., 2017; Hazarika et al., 2018), capsule networks (Du et al., 2019; Chen and Qian, 2019; Jiang et al., 2019), GNN (Huang and Carley, 2019; Sun et al., 2019; Zhang et al., 2019; Wang et al., 2020), and transformer (Tang et al., 2020) are applied to this task, with other studies leveraging external resources, including position information (Gu et al., 2018), document information (He et al., 2018b), commonsense knowledge (Ma et al., 2018), etc. Among all resources, syntactic information was proved to be the most effective one an"
2021.emnlp-main.228,C16-1138,0,0.0205202,"ral models (Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Soares et al., 2019) with powerful encoders (e.g., Transformers) have achieved promising performance for relation extraction (RE), for the reason that the encoders are superior in capturing contextual information and thus allow RE systems to better understand the text and correctly identify the relations between entities in the given text. To further improve the ability of RE models to understand the context, many studies (Xu et al., 2016; Zhang et al., 2018; Guo et al., * Equal contribution. Corresponding author. 1 The code involved in this paper are released at https: //github.com/cuhksz-nlp/RE-NGCN. † 2019; Sun et al., 2020; Yu et al., 2020; Mandya et al., 2020; Tian et al., 2021d; Chen et al., 2021) leverage extra resources, such as auto-parsed word dependency, through graph-based approaches, e.g., graph convolutional networks (GCN). In doing so, such studies learn the long-distance connections among useful words from the dependency tree and extract relations between entity pairs accordingly. However, in doing so, dependen"
2021.emnlp-main.228,D15-1206,0,0.0290324,"istence of a dependency parser. Specifically, we construct the graph from n-grams extracted from a lexicon built from pointwise mutual information (PMI) and apply attention over the graph. Therefore, different word pairs from the contexts within and across n-grams are weighted in the model and facilitate RE accordingly. Experimental results with further analyses on two English benchmark datasets for RE demonstrate the effectiveness of our approach, where state-of-the-art performance is observed on both datasets.1 1 Introduction Recently, neural models (Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Soares et al., 2019) with powerful encoders (e.g., Transformers) have achieved promising performance for relation extraction (RE), for the reason that the encoders are superior in capturing contextual information and thus allow RE systems to better understand the text and correctly identify the relations between entities in the given text. To further improve the ability of RE models to understand the context, many studies (Xu et al., 2016; Zhang et al., 2018; Guo et al., * Equal contributio"
2021.emnlp-main.228,P19-1130,0,0.0185814,"hE2 ) (7) where o is a |R|-dimensional vector with each of its value referring to a relation type from the label set R. Finally, we apply a softmax function to o to predict the relation rb between E1 and E2 . 3 For example, if the edge type between xi and xj is (l) (l) (l) (l) CROSS, then Wui,j = Wcross and bui,j = bcross . 3 Experiments 3.1 Settings We run experiments on two English benchmark datasets for RE, namely, ACE2005EN (ACE05)4 and SemEval 2010 Task 8 (SemEval)5 (Hendrickx et al., 2010). For both datasets, we follow previous studies (Miwa and Bansal, 2016; Christopoulou et al., 2018; Ye et al., 2019) to pre-process them and split them into train/dev/test splits.6 To build the n-gram lexicon for graph construction, we perform PMI on the training set to extract n-grams whose lengths are within [1,5], with the threshold of the PMI score set to 0. For textual encoder, since the high-quality text representations are proved to be effective in enhancing the model performance (Mikolov et al., 2013; Song et al., 2018a,b; Song and Shi, 2018; Devlin et al., 2019; Yang et al., 2019; Song et al., 2021) and BERT is be able to provide high-quality text representations for natural language processing dow"
2021.emnlp-main.228,N18-1142,0,0.0136933,"ted in blue boxes. Two given entities (i.e., “Money” and “hedge funds”) are shown in red and blue colors, respectively. where GX is the graph built based on n-grams in X , R is the relation type set; p computes the probability of a particular relation type r ∈ R with the given input (i.e., X , GX , E1 , and E2 ), and rb is the prediction of our A-GCN model. In the following text, we firstly elaborate how we construct the graph based on n-grams, and then illustrate the architecture of the A-GCN model for RE. 2.1 Graph Construction from N-grams 2009; Song and Xia, 2012; Ishiwatari et al., 2017; Yoon et al., 2018; Tian et al., 2020a,b,c, 2021a), we propose to construct the graph for GCN-based models based on n-grams in X which are extracted from a pre-constructed n-gram lexicon N . N-gram Lexicon Construction Before we segment appropriate n-grams for each input sentence, an n-gram lexicon N is built over the entire corpus based on pointwise mutual information (PMI). Specifically, we firstly compute the PMI of any two adjacent words x0 , x00 for all data by Conventionally, the graph used in GCN-based models for natural language understanding tasks (inp(x0 x00 ) cluding RE) is constructed by the depende"
2021.emnlp-main.228,2020.emnlp-main.133,0,0.0521254,"Missing"
2021.emnlp-main.228,2020.coling-main.341,0,0.0288006,"ncoders (e.g., Transformers) have achieved promising performance for relation extraction (RE), for the reason that the encoders are superior in capturing contextual information and thus allow RE systems to better understand the text and correctly identify the relations between entities in the given text. To further improve the ability of RE models to understand the context, many studies (Xu et al., 2016; Zhang et al., 2018; Guo et al., * Equal contribution. Corresponding author. 1 The code involved in this paper are released at https: //github.com/cuhksz-nlp/RE-NGCN. † 2019; Sun et al., 2020; Yu et al., 2020; Mandya et al., 2020; Tian et al., 2021d; Chen et al., 2021) leverage extra resources, such as auto-parsed word dependency, through graph-based approaches, e.g., graph convolutional networks (GCN). In doing so, such studies learn the long-distance connections among useful words from the dependency tree and extract relations between entity pairs accordingly. However, in doing so, dependency parsers required by their approaches are not always available. In this dilemma, one needs another way to extract useful word connections to build the graph for GCN-based models, whereas limited attentions f"
2021.emnlp-main.228,P16-1123,0,0.0488864,"Missing"
2021.emnlp-main.228,C14-1220,0,0.0502407,"context graph, without relying on the existence of a dependency parser. Specifically, we construct the graph from n-grams extracted from a lexicon built from pointwise mutual information (PMI) and apply attention over the graph. Therefore, different word pairs from the contexts within and across n-grams are weighted in the model and facilitate RE accordingly. Experimental results with further analyses on two English benchmark datasets for RE demonstrate the effectiveness of our approach, where state-of-the-art performance is observed on both datasets.1 1 Introduction Recently, neural models (Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Soares et al., 2019) with powerful encoders (e.g., Transformers) have achieved promising performance for relation extraction (RE), for the reason that the encoders are superior in capturing contextual information and thus allow RE systems to better understand the text and correctly identify the relations between entities in the given text. To further improve the ability of RE models to understand the context, many studies (Xu et al., 2016; Zhang et al."
2021.emnlp-main.228,Y15-1009,0,0.0232422,"ly, we construct the graph from n-grams extracted from a lexicon built from pointwise mutual information (PMI) and apply attention over the graph. Therefore, different word pairs from the contexts within and across n-grams are weighted in the model and facilitate RE accordingly. Experimental results with further analyses on two English benchmark datasets for RE demonstrate the effectiveness of our approach, where state-of-the-art performance is observed on both datasets.1 1 Introduction Recently, neural models (Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Soares et al., 2019) with powerful encoders (e.g., Transformers) have achieved promising performance for relation extraction (RE), for the reason that the encoders are superior in capturing contextual information and thus allow RE systems to better understand the text and correctly identify the relations between entities in the given text. To further improve the ability of RE models to understand the context, many studies (Xu et al., 2016; Zhang et al., 2018; Guo et al., * Equal contribution. Corresponding author. 1 The code involved"
2021.emnlp-main.228,D18-1244,0,0.0168475,"et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Soares et al., 2019) with powerful encoders (e.g., Transformers) have achieved promising performance for relation extraction (RE), for the reason that the encoders are superior in capturing contextual information and thus allow RE systems to better understand the text and correctly identify the relations between entities in the given text. To further improve the ability of RE models to understand the context, many studies (Xu et al., 2016; Zhang et al., 2018; Guo et al., * Equal contribution. Corresponding author. 1 The code involved in this paper are released at https: //github.com/cuhksz-nlp/RE-NGCN. † 2019; Sun et al., 2020; Yu et al., 2020; Mandya et al., 2020; Tian et al., 2021d; Chen et al., 2021) leverage extra resources, such as auto-parsed word dependency, through graph-based approaches, e.g., graph convolutional networks (GCN). In doing so, such studies learn the long-distance connections among useful words from the dependency tree and extract relations between entity pairs accordingly. However, in doing so, dependency parsers required"
2021.emnlp-main.228,D17-1004,0,0.017845,"exicon built from pointwise mutual information (PMI) and apply attention over the graph. Therefore, different word pairs from the contexts within and across n-grams are weighted in the model and facilitate RE accordingly. Experimental results with further analyses on two English benchmark datasets for RE demonstrate the effectiveness of our approach, where state-of-the-art performance is observed on both datasets.1 1 Introduction Recently, neural models (Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Soares et al., 2019) with powerful encoders (e.g., Transformers) have achieved promising performance for relation extraction (RE), for the reason that the encoders are superior in capturing contextual information and thus allow RE systems to better understand the text and correctly identify the relations between entities in the given text. To further improve the ability of RE models to understand the context, many studies (Xu et al., 2016; Zhang et al., 2018; Guo et al., * Equal contribution. Corresponding author. 1 The code involved in this paper are released at https: //github.com/cuhksz-n"
2021.emnlp-main.228,P16-2034,0,0.0256345,"extracted from a lexicon built from pointwise mutual information (PMI) and apply attention over the graph. Therefore, different word pairs from the contexts within and across n-grams are weighted in the model and facilitate RE accordingly. Experimental results with further analyses on two English benchmark datasets for RE demonstrate the effectiveness of our approach, where state-of-the-art performance is observed on both datasets.1 1 Introduction Recently, neural models (Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Soares et al., 2019) with powerful encoders (e.g., Transformers) have achieved promising performance for relation extraction (RE), for the reason that the encoders are superior in capturing contextual information and thus allow RE systems to better understand the text and correctly identify the relations between entities in the given text. To further improve the ability of RE models to understand the context, many studies (Xu et al., 2016; Zhang et al., 2018; Guo et al., * Equal contribution. Corresponding author. 1 The code involved in this paper are released at https: /"
2021.emnlp-main.321,C18-1066,0,0.0202317,"f its local dataset to train the model locally. In the training process, the nodes compute the average gradient on their local datasets with the current global model. The server collects the gradients and aggregates them to update the global model. This process repeats until the global model converges. 2.2 Aspect-based Sentiment Analysis Aspect-based sentiment analysis (ABSA) is a longstanding NLP task of detecting a sentiment polarity towards a given aspect term in a sentence. Many recent studies applied neural network approaches to ABSA (Chen et al., 2017; Ma et al., 2017; Fan et al., 2018; Gu et al., 2018; He et al., 2018b; Huang and Carley, 2018; Li et al., 2018b; Chen and Qian, 2019; Hu et al., 2019; Du et al., 2019; Sun et al., 2 Related Work 2019; Zhang et al., 2019). Usually, external knowl2.1 Federated Learning edge is incorporated to obtain better understandFederated learning (FL) was first proposed by ings of contextual information so as to enhance the Google and then further developed by many studies model performance for natural language processover the past years (Shokri and Shmatikov, 2015; ing downstream tasks (including ABSA) (Li et al., Koneˇcn`y et al., 2016a,b; McMahan et al.,"
2021.emnlp-main.321,C18-1096,0,0.100203,"et to train the model locally. In the training process, the nodes compute the average gradient on their local datasets with the current global model. The server collects the gradients and aggregates them to update the global model. This process repeats until the global model converges. 2.2 Aspect-based Sentiment Analysis Aspect-based sentiment analysis (ABSA) is a longstanding NLP task of detecting a sentiment polarity towards a given aspect term in a sentence. Many recent studies applied neural network approaches to ABSA (Chen et al., 2017; Ma et al., 2017; Fan et al., 2018; Gu et al., 2018; He et al., 2018b; Huang and Carley, 2018; Li et al., 2018b; Chen and Qian, 2019; Hu et al., 2019; Du et al., 2019; Sun et al., 2 Related Work 2019; Zhang et al., 2019). Usually, external knowl2.1 Federated Learning edge is incorporated to obtain better understandFederated learning (FL) was first proposed by ings of contextual information so as to enhance the Google and then further developed by many studies model performance for natural language processover the past years (Shokri and Shmatikov, 2015; ing downstream tasks (including ABSA) (Li et al., Koneˇcn`y et al., 2016a,b; McMahan et al., 2017). 2018a; Ma"
2021.emnlp-main.321,P18-2092,0,0.0908934,"et to train the model locally. In the training process, the nodes compute the average gradient on their local datasets with the current global model. The server collects the gradients and aggregates them to update the global model. This process repeats until the global model converges. 2.2 Aspect-based Sentiment Analysis Aspect-based sentiment analysis (ABSA) is a longstanding NLP task of detecting a sentiment polarity towards a given aspect term in a sentence. Many recent studies applied neural network approaches to ABSA (Chen et al., 2017; Ma et al., 2017; Fan et al., 2018; Gu et al., 2018; He et al., 2018b; Huang and Carley, 2018; Li et al., 2018b; Chen and Qian, 2019; Hu et al., 2019; Du et al., 2019; Sun et al., 2 Related Work 2019; Zhang et al., 2019). Usually, external knowl2.1 Federated Learning edge is incorporated to obtain better understandFederated learning (FL) was first proposed by ings of contextual information so as to enhance the Google and then further developed by many studies model performance for natural language processover the past years (Shokri and Shmatikov, 2015; ing downstream tasks (including ABSA) (Li et al., Koneˇcn`y et al., 2016a,b; McMahan et al., 2017). 2018a; Ma"
2021.emnlp-main.321,D19-1467,0,0.020514,"rage gradient on their local datasets with the current global model. The server collects the gradients and aggregates them to update the global model. This process repeats until the global model converges. 2.2 Aspect-based Sentiment Analysis Aspect-based sentiment analysis (ABSA) is a longstanding NLP task of detecting a sentiment polarity towards a given aspect term in a sentence. Many recent studies applied neural network approaches to ABSA (Chen et al., 2017; Ma et al., 2017; Fan et al., 2018; Gu et al., 2018; He et al., 2018b; Huang and Carley, 2018; Li et al., 2018b; Chen and Qian, 2019; Hu et al., 2019; Du et al., 2019; Sun et al., 2 Related Work 2019; Zhang et al., 2019). Usually, external knowl2.1 Federated Learning edge is incorporated to obtain better understandFederated learning (FL) was first proposed by ings of contextual information so as to enhance the Google and then further developed by many studies model performance for natural language processover the past years (Shokri and Shmatikov, 2015; ing downstream tasks (including ABSA) (Li et al., Koneˇcn`y et al., 2016a,b; McMahan et al., 2017). 2018a; Ma et al., 2018; Chen et al., 2020b; Tang FL is to build machine-learning models ba"
2021.emnlp-main.321,D18-1136,0,0.0218131,"del locally. In the training process, the nodes compute the average gradient on their local datasets with the current global model. The server collects the gradients and aggregates them to update the global model. This process repeats until the global model converges. 2.2 Aspect-based Sentiment Analysis Aspect-based sentiment analysis (ABSA) is a longstanding NLP task of detecting a sentiment polarity towards a given aspect term in a sentence. Many recent studies applied neural network approaches to ABSA (Chen et al., 2017; Ma et al., 2017; Fan et al., 2018; Gu et al., 2018; He et al., 2018b; Huang and Carley, 2018; Li et al., 2018b; Chen and Qian, 2019; Hu et al., 2019; Du et al., 2019; Sun et al., 2 Related Work 2019; Zhang et al., 2019). Usually, external knowl2.1 Federated Learning edge is incorporated to obtain better understandFederated learning (FL) was first proposed by ings of contextual information so as to enhance the Google and then further developed by many studies model performance for natural language processover the past years (Shokri and Shmatikov, 2015; ing downstream tasks (including ABSA) (Li et al., Koneˇcn`y et al., 2016a,b; McMahan et al., 2017). 2018a; Ma et al., 2018; Chen et al"
2021.emnlp-main.321,D13-1171,0,0.0866447,"Missing"
2021.emnlp-main.321,2020.acl-main.293,0,0.0179339,"results for localized prediction. This is a critical barrier for ABSA task, where the data from different sources always contains heterogeneous vocabularies and expressing patterns. In this work, we propose to leverage TM to explore the topic information in the data and use it to guide the centralized model for making localized prediction. As for the input sentence, previous studies concatenate aspect term(s) directly to the end of an input sentence with a special token2 serving as the separator and feed the resulted sentence+aspect pair into an encoder (Song et al., 2019; Zeng et al., 2019; Phan and Ogunbona, 2020; Veyseh et al., If the encoder is BERT, the special token will be [SEP]. (7) where the k-th topic is represented by a onedimensional vector (esi,k ∈ R1 ) in exi ∈ Rdt . We feed esi,k into a multi-layer perceptron (MLP) to compute the source memories si,k by si,k = MLPs (esi,k ) (8) Afterward, we compute the attention weights pi,k for the k-th topic by exp (hXi · si,k ) pi,k = Pdt k=1 exp (hXi · si,j ) (9) Finally, pi,k is applied to target memories by ui = dt X pi,k · tk (10) k=1 3 2 (6) The details of the neural topic model are illustrated in Section 4.2 3945 Dataset Pos. # Neu. # Neg. # LAP"
2021.emnlp-main.321,S14-2004,0,0.124662,"Missing"
2021.emnlp-main.321,2020.findings-emnlp.128,0,0.0981788,"Missing"
2021.emnlp-main.321,2021.acl-short.68,1,0.691966,"ts are isolated to three nodes. Specifically, the first node holds LAP14; the second node holds REST14; the third node holds TWITTER. For encoder, considering that high-quality text representations from pre-trained embeddings or language models are able to effectively to enhance the model performance (Mikolov et al., 2013; Song et al., 2018a,b; Song and Shi, 2018; Devlin et al., 2019; Diao et al., 2020; Song et al., 2021) and BERT-based models have achieved great success in many NLP tasks (Mao et al., 2019; Xu et al., 2019a; Song et al., 2020; Tang et al., 2020; Tian et al., 2020a,c, 2021b,c; Qin et al., 2021a,b), we use the BERT-base-uncased and BERT-large-uncased13 (Devlin et al., 2019) to encode the encrypted input14 (i.e., Xei and Aei ) from Ni . For TM, we train our neural topic model using an unsupervised approach proposed by Miao et al. (2017) and then use the resulted topic-vocab matrix to initialize Wφ in TM-FL. In the training process of TM-FL, both BERT and Wφ are updated.15 Moreover, it is noted that for baselines (i.e., BT) on the single dataset and the union dataset, we choose the models based on their F1 scores with respect to the dev set of each dataset separately. For FL and TM-FL"
2021.emnlp-main.321,2021.emnlp-main.228,1,0.712256,"ts are isolated to three nodes. Specifically, the first node holds LAP14; the second node holds REST14; the third node holds TWITTER. For encoder, considering that high-quality text representations from pre-trained embeddings or language models are able to effectively to enhance the model performance (Mikolov et al., 2013; Song et al., 2018a,b; Song and Shi, 2018; Devlin et al., 2019; Diao et al., 2020; Song et al., 2021) and BERT-based models have achieved great success in many NLP tasks (Mao et al., 2019; Xu et al., 2019a; Song et al., 2020; Tang et al., 2020; Tian et al., 2020a,c, 2021b,c; Qin et al., 2021a,b), we use the BERT-base-uncased and BERT-large-uncased13 (Devlin et al., 2019) to encode the encrypted input14 (i.e., Xei and Aei ) from Ni . For TM, we train our neural topic model using an unsupervised approach proposed by Miao et al. (2017) and then use the resulted topic-vocab matrix to initialize Wφ in TM-FL. In the training process of TM-FL, both BERT and Wφ are updated.15 Moreover, it is noted that for baselines (i.e., BT) on the single dataset and the union dataset, we choose the models based on their F1 scores with respect to the dev set of each dataset separately. For FL and TM-FL"
2021.emnlp-main.321,2020.coling-main.63,1,0.744045,"e (BT-l) under different settings on three benchmark datasets. datasets are isolated to three nodes. Specifically, the first node holds LAP14; the second node holds REST14; the third node holds TWITTER. For encoder, considering that high-quality text representations from pre-trained embeddings or language models are able to effectively to enhance the model performance (Mikolov et al., 2013; Song et al., 2018a,b; Song and Shi, 2018; Devlin et al., 2019; Diao et al., 2020; Song et al., 2021) and BERT-based models have achieved great success in many NLP tasks (Mao et al., 2019; Xu et al., 2019a; Song et al., 2020; Tang et al., 2020; Tian et al., 2020a,c, 2021b,c; Qin et al., 2021a,b), we use the BERT-base-uncased and BERT-large-uncased13 (Devlin et al., 2019) to encode the encrypted input14 (i.e., Xei and Aei ) from Ni . For TM, we train our neural topic model using an unsupervised approach proposed by Miao et al. (2017) and then use the resulted topic-vocab matrix to initialize Wφ in TM-FL. In the training process of TM-FL, both BERT and Wφ are updated.15 Moreover, it is noted that for baselines (i.e., BT) on the single dataset and the union dataset, we choose the models based on their F1 scores with"
2021.emnlp-main.321,2020.emnlp-main.165,0,0.192244,"al results on a simulated environment for FL with three nodes demonRecently, FL has been applied to many downstrate the effectiveness of our approach, where stream natural language processing (NLP) applicaTM-FL outperforms different baselines includtions (Zhu et al., 2020) such as mobile keyboard ing some well-designed FL frameworks.1 prediction (Hard et al., 2018), language model training (Chen et al., 2019), representation learning 1 Introduction (Liu et al., 2019), spoken language understanding (Huang et al., 2020), medical relation extraction Aspect-based sentiment analysis (ABSA) is one (Sui et al., 2020), medical named entity recognition of the most popular natural language processing (Ge et al., 2020), and news recommendation (Qi * Equal contribution. et al., 2020). However, conventional FL techniques † Corresponding author. 1 are more suitable for nodes sharing homogeneous The code involved in this paper are released at https: //github.com/cuhksz-nlp/ASA-TM. data, which is seldom the case for NLP tasks be3942 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3942–3954 c November 7–11, 2021. 2021 Association for Computational Linguistics cause text"
2021.emnlp-main.321,D19-1569,0,0.0157547,"al. (2017) and then use the resulted topic-vocab matrix to initialize Wφ in TM-FL. In the training process of TM-FL, both BERT and Wφ are updated.15 Moreover, it is noted that for baselines (i.e., BT) on the single dataset and the union dataset, we choose the models based on their F1 scores with respect to the dev set of each dataset separately. For FL and TM-FL, we choose the models according to their average F1 score of the three F1 scores over the dev sets of the three datasets. For the evaluation metrics, we follow previous studies (Tang et al., 2016a; Chen et al., 2017; He et al., 2018a; Sun et al., 2019; Zhang et al., 2019) to evaluate all models via accuracy and macro-averaged F1 scores over all sentiment polarities, i.e., positive, neutral and negative. In the experiments, we run the baselines without federated learning (i.e. BT-b and BT-l) on the single dataset (i.e. LAP14, REST14, or TWITTER) and the combined dataset consisting of all the three datasets, denoted by the union dataset. However, it is rarely practical to have the model trained on the union dataset in real applications (since the data are isolated in different nodes). Therefore, the experimental results on the union dataset"
2021.emnlp-main.321,C16-1311,0,0.0328255,"model using an unsupervised approach proposed by Miao et al. (2017) and then use the resulted topic-vocab matrix to initialize Wφ in TM-FL. In the training process of TM-FL, both BERT and Wφ are updated.15 Moreover, it is noted that for baselines (i.e., BT) on the single dataset and the union dataset, we choose the models based on their F1 scores with respect to the dev set of each dataset separately. For FL and TM-FL, we choose the models according to their average F1 score of the three F1 scores over the dev sets of the three datasets. For the evaluation metrics, we follow previous studies (Tang et al., 2016a; Chen et al., 2017; He et al., 2018a; Sun et al., 2019; Zhang et al., 2019) to evaluate all models via accuracy and macro-averaged F1 scores over all sentiment polarities, i.e., positive, neutral and negative. In the experiments, we run the baselines without federated learning (i.e. BT-b and BT-l) on the single dataset (i.e. LAP14, REST14, or TWITTER) and the combined dataset consisting of all the three datasets, denoted by the union dataset. However, it is rarely practical to have the model trained on the union dataset in real applications (since the data are isolated in different nodes). T"
2021.emnlp-main.321,D16-1021,0,0.0223762,"model using an unsupervised approach proposed by Miao et al. (2017) and then use the resulted topic-vocab matrix to initialize Wφ in TM-FL. In the training process of TM-FL, both BERT and Wφ are updated.15 Moreover, it is noted that for baselines (i.e., BT) on the single dataset and the union dataset, we choose the models based on their F1 scores with respect to the dev set of each dataset separately. For FL and TM-FL, we choose the models according to their average F1 score of the three F1 scores over the dev sets of the three datasets. For the evaluation metrics, we follow previous studies (Tang et al., 2016a; Chen et al., 2017; He et al., 2018a; Sun et al., 2019; Zhang et al., 2019) to evaluate all models via accuracy and macro-averaged F1 scores over all sentiment polarities, i.e., positive, neutral and negative. In the experiments, we run the baselines without federated learning (i.e. BT-b and BT-l) on the single dataset (i.e. LAP14, REST14, or TWITTER) and the combined dataset consisting of all the three datasets, denoted by the union dataset. However, it is rarely practical to have the model trained on the union dataset in real applications (since the data are isolated in different nodes). T"
2021.emnlp-main.321,2020.acl-main.588,0,0.332504,"in a distributed environment, namely TM-FL, with a topic memory to enhance FL by providing categorical (topic) information for localized predictions, which can address the difficulty of identifying text sources caused by data inaccessibility. Specifically, the topic model serves as a server-side component to read different inputs from each node and respond with categorical weights to help the backbone ABSA classifier. Compared with previous ABSA studies that leverage extra features, e.g., document information (Li et al., 2018a), commonsense knowledge (Ma et al., 2018), and word dependencies (Tang et al., 2020), our approach offers an alternative to improve ABSA by leveraging extra labeled data through the FL framework enhanced by TM. Experimental results on a simulated environment with isolated data from laptop, restaurant reviews, and social media (i.e., Tweets), demonstrate the effectiveness of our approach, where TM-FL outperforms different baselines including the ones with well designed FL framework. each other node. Thus, the centralized model on the server-side cannot directly exploit the data to optimize its parameters. Instead, each node computes a local model update based on their data, an"
2021.emnlp-main.321,2021.findings-acl.376,1,0.859721,"earning edge is incorporated to obtain better understandFederated learning (FL) was first proposed by ings of contextual information so as to enhance the Google and then further developed by many studies model performance for natural language processover the past years (Shokri and Shmatikov, 2015; ing downstream tasks (including ABSA) (Li et al., Koneˇcn`y et al., 2016a,b; McMahan et al., 2017). 2018a; Ma et al., 2018; Chen et al., 2020b; Tang FL is to build machine-learning models based on et al., 2020; Tian et al., 2020b; Chen et al., 2021; datasets distributed across multiple devices while Tian et al., 2021b,c,d). However, most previous preventing data leakage. Generally, in federated studies assume an ideal environment where all the learning, the data is locally stored in different nodes data is accessible and visible to each other for the and never uploaded to the server or exchanged with experiments, which is rarely the case in real appli3943 Figure 1: An overview of the architecture for the centralized model at server of TM-FL, where the aspect term(s) (e.g., menu and dishes) in the input sentence are highlighted in green. cations. In this paper, we propose an alternative to handle the data"
2021.emnlp-main.321,N18-2028,1,0.807633,"3.31 72.38 77.02 76.15 7 8 FL (BT-l) TM-FL (BT-l) 81.35 78.21 82.29 79.25 85.71 78.28 86.07 79.00 74.28 73.46 74.57 73.63 Table 2: Accuracy and Macro-F1 scores of models using BERT-base (BT-b) and BERT-large (BT-l) under different settings on three benchmark datasets. datasets are isolated to three nodes. Specifically, the first node holds LAP14; the second node holds REST14; the third node holds TWITTER. For encoder, considering that high-quality text representations from pre-trained embeddings or language models are able to effectively to enhance the model performance (Mikolov et al., 2013; Song et al., 2018a,b; Song and Shi, 2018; Devlin et al., 2019; Diao et al., 2020; Song et al., 2021) and BERT-based models have achieved great success in many NLP tasks (Mao et al., 2019; Xu et al., 2019a; Song et al., 2020; Tang et al., 2020; Tian et al., 2020a,c, 2021b,c; Qin et al., 2021a,b), we use the BERT-base-uncased and BERT-large-uncased13 (Devlin et al., 2019) to encode the encrypted input14 (i.e., Xei and Aei ) from Ni . For TM, we train our neural topic model using an unsupervised approach proposed by Miao et al. (2017) and then use the resulted topic-vocab matrix to initialize Wφ in TM-FL. In the"
2021.emnlp-main.321,2021.eacl-main.326,1,0.830027,"earning edge is incorporated to obtain better understandFederated learning (FL) was first proposed by ings of contextual information so as to enhance the Google and then further developed by many studies model performance for natural language processover the past years (Shokri and Shmatikov, 2015; ing downstream tasks (including ABSA) (Li et al., Koneˇcn`y et al., 2016a,b; McMahan et al., 2017). 2018a; Ma et al., 2018; Chen et al., 2020b; Tang FL is to build machine-learning models based on et al., 2020; Tian et al., 2020b; Chen et al., 2021; datasets distributed across multiple devices while Tian et al., 2021b,c,d). However, most previous preventing data leakage. Generally, in federated studies assume an ideal environment where all the learning, the data is locally stored in different nodes data is accessible and visible to each other for the and never uploaded to the server or exchanged with experiments, which is rarely the case in real appli3943 Figure 1: An overview of the architecture for the centralized model at server of TM-FL, where the aspect term(s) (e.g., menu and dishes) in the input sentence are highlighted in green. cations. In this paper, we propose an alternative to handle the data"
2021.emnlp-main.321,2021.acl-long.344,1,0.886401,"earning edge is incorporated to obtain better understandFederated learning (FL) was first proposed by ings of contextual information so as to enhance the Google and then further developed by many studies model performance for natural language processover the past years (Shokri and Shmatikov, 2015; ing downstream tasks (including ABSA) (Li et al., Koneˇcn`y et al., 2016a,b; McMahan et al., 2017). 2018a; Ma et al., 2018; Chen et al., 2020b; Tang FL is to build machine-learning models based on et al., 2020; Tian et al., 2020b; Chen et al., 2021; datasets distributed across multiple devices while Tian et al., 2021b,c,d). However, most previous preventing data leakage. Generally, in federated studies assume an ideal environment where all the learning, the data is locally stored in different nodes data is accessible and visible to each other for the and never uploaded to the server or exchanged with experiments, which is rarely the case in real appli3943 Figure 1: An overview of the architecture for the centralized model at server of TM-FL, where the aspect term(s) (e.g., menu and dishes) in the input sentence are highlighted in green. cations. In this paper, we propose an alternative to handle the data"
2021.emnlp-main.321,2020.findings-emnlp.378,1,0.839612,"n et al., 2 Related Work 2019; Zhang et al., 2019). Usually, external knowl2.1 Federated Learning edge is incorporated to obtain better understandFederated learning (FL) was first proposed by ings of contextual information so as to enhance the Google and then further developed by many studies model performance for natural language processover the past years (Shokri and Shmatikov, 2015; ing downstream tasks (including ABSA) (Li et al., Koneˇcn`y et al., 2016a,b; McMahan et al., 2017). 2018a; Ma et al., 2018; Chen et al., 2020b; Tang FL is to build machine-learning models based on et al., 2020; Tian et al., 2020b; Chen et al., 2021; datasets distributed across multiple devices while Tian et al., 2021b,c,d). However, most previous preventing data leakage. Generally, in federated studies assume an ideal environment where all the learning, the data is locally stored in different nodes data is accessible and visible to each other for the and never uploaded to the server or exchanged with experiments, which is rarely the case in real appli3943 Figure 1: An overview of the architecture for the centralized model at server of TM-FL, where the aspect term(s) (e.g., menu and dishes) in the input sentence are h"
2021.emnlp-main.321,2020.coling-main.187,1,0.877664,"n et al., 2 Related Work 2019; Zhang et al., 2019). Usually, external knowl2.1 Federated Learning edge is incorporated to obtain better understandFederated learning (FL) was first proposed by ings of contextual information so as to enhance the Google and then further developed by many studies model performance for natural language processover the past years (Shokri and Shmatikov, 2015; ing downstream tasks (including ABSA) (Li et al., Koneˇcn`y et al., 2016a,b; McMahan et al., 2017). 2018a; Ma et al., 2018; Chen et al., 2020b; Tang FL is to build machine-learning models based on et al., 2020; Tian et al., 2020b; Chen et al., 2021; datasets distributed across multiple devices while Tian et al., 2021b,c,d). However, most previous preventing data leakage. Generally, in federated studies assume an ideal environment where all the learning, the data is locally stored in different nodes data is accessible and visible to each other for the and never uploaded to the server or exchanged with experiments, which is rarely the case in real appli3943 Figure 1: An overview of the architecture for the centralized model at server of TM-FL, where the aspect term(s) (e.g., menu and dishes) in the input sentence are h"
2021.emnlp-main.321,2020.findings-emnlp.153,1,0.895393,"n et al., 2 Related Work 2019; Zhang et al., 2019). Usually, external knowl2.1 Federated Learning edge is incorporated to obtain better understandFederated learning (FL) was first proposed by ings of contextual information so as to enhance the Google and then further developed by many studies model performance for natural language processover the past years (Shokri and Shmatikov, 2015; ing downstream tasks (including ABSA) (Li et al., Koneˇcn`y et al., 2016a,b; McMahan et al., 2017). 2018a; Ma et al., 2018; Chen et al., 2020b; Tang FL is to build machine-learning models based on et al., 2020; Tian et al., 2020b; Chen et al., 2021; datasets distributed across multiple devices while Tian et al., 2021b,c,d). However, most previous preventing data leakage. Generally, in federated studies assume an ideal environment where all the learning, the data is locally stored in different nodes data is accessible and visible to each other for the and never uploaded to the server or exchanged with experiments, which is rarely the case in real appli3943 Figure 1: An overview of the architecture for the centralized model at server of TM-FL, where the aspect term(s) (e.g., menu and dishes) in the input sentence are h"
2021.emnlp-main.321,2020.findings-emnlp.407,0,0.0601503,"Missing"
2021.emnlp-main.321,2020.acl-main.295,0,0.130523,"xploit the data to optimize its parameters. Instead, each node computes a local model update based on their data, and then the local updates in all nodes are aggregated by the centralized model to optimize parameters. Since such local model updates cannot be directly translated to the original data, the data privacy and security are significantly enhanced. However, there are some other approaches to apply FL, such as sending transformed or encrypted data which cannot be converted to the original data (Hard et al., 2018). FL has been applied to many areas (Yang et al., 2019b; Liu et al., 2020; Wang et al., 2020b; Zheng et al., 2020) and recently, many studies focus on optimizing the learning process (Koneˇcn`y et al., 2016b; Li et al., 2019; Zheng et al., 2020; Wang et al., 2020b). Particularly, the F EDERATEDAVERAGING algorithm, proposed by McMahan et al. (2017), is to combine node updates and produce a new global model. At the beginning of each training round, the global model is sent to a subset of nodes. Each of the selected nodes then randomly samples a subset of its local dataset to train the model locally. In the training process, the nodes compute the average gradient on their local datasets"
2021.emnlp-main.321,N19-1242,0,0.0508808,"Missing"
2021.emnlp-main.321,P18-1234,0,0.0170982,"edu.cn ♦ chenguimin@foxmail.com ♥ yhtian@uw.edu ♠ songyan@cuhk.edu.cn Abstract (NLP) tasks aiming to predict the sentiment polarity (i.e., “positive”, “negative”, and “neutral”) Aspect-based sentiment analysis (ABSA) prefor an aspect term in sentences. Currently, methdicts the sentiment polarity towards a parods based on deep learning have been widely utiticular aspect term in a sentence, which is lized for ABSA and demonstrated excellent potenan important task in real-world applications. tials (Chen et al., 2017; Zadeh et al., 2017; Zhang To perform ABSA, the trained model is reet al., 2018; Xue and Li, 2018; Zhao et al., 2018; quired to have a good understanding of the Chaturvedi et al., 2018; Xu et al., 2019b). However, contextual information, especially the particular patterns that suggest the sentiment polarthese methods still reach a bottleneck if there is no ity. However, these patterns typically vary in enough labeled training data. One feasible solution different sentences, especially when the senfor it is to leverage extra labeled data from other tences come from different sources (domains), sources or domains. However, in real applications, which makes ABSA still very challenging. Althe"
2021.emnlp-main.321,D19-1464,0,0.0293605,"Missing"
2021.emnlp-main.321,2020.findings-emnlp.55,0,0.506342,"ey are invisible to each other during aims to identify different isolated data sources the training stage (Hard et al., 2018). This property due to data inaccessibility by providing usemakes FL an essential technique for real applicaful categorical information for localized pretions with privacy and security requirements. dictions. Experimental results on a simulated environment for FL with three nodes demonRecently, FL has been applied to many downstrate the effectiveness of our approach, where stream natural language processing (NLP) applicaTM-FL outperforms different baselines includtions (Zhu et al., 2020) such as mobile keyboard ing some well-designed FL frameworks.1 prediction (Hard et al., 2018), language model training (Chen et al., 2019), representation learning 1 Introduction (Liu et al., 2019), spoken language understanding (Huang et al., 2020), medical relation extraction Aspect-based sentiment analysis (ABSA) is one (Sui et al., 2020), medical named entity recognition of the most popular natural language processing (Ge et al., 2020), and news recommendation (Qi * Equal contribution. et al., 2020). However, conventional FL techniques † Corresponding author. 1 are more suitable for nodes"
2021.findings-acl.221,P18-2014,0,0.208054,"Missing"
2021.findings-acl.221,P17-2057,0,0.0208081,"gested by the dependency tree of the input sentence has been demonstrated to be useful for relation extraction in many studies (Xu et al., 2015; Zhang et al., 2018; Guo et al., 2019). However, most models to leverage the dependency information are not naturally appropriate to model the dependency types among words. It is required to find an appropriate approach to leverage the dependency type information. Of all choices, key-value memory networks (KVMN) (Miller et al., 2016) is an effective solution in modeling pair-wisely organized information to improve many NLP tasks (Tapaswi et al., 2016; Das et al., 2017; Mino et al., 2017; Xu et al., 2019; Nie et al., 2020; Song et al., 2020; Tian et al., 2020a,d, 2021b). Specifically, KVMN maps the information instances into a list of memory slots si = (ki , vi ) (i is the index of the memory slot si ) with ki referring to the key and vi the value, respectively. The KVMN addresses the memory slot si by assigning a weight pi to the value vi by comparing the input (denoted by x) to the key ki : pi = softmax (AΦX (x) · AΦK (ki )) (1) where Φ· are functions that map the input features into their embeddings and A is a matrix that maps the embeddings into another"
2021.findings-acl.221,D16-1147,0,0.0243218,"is straightforward to consider integrating extra features to enhance contextual modeling. Of all such features, the syntactic information suggested by the dependency tree of the input sentence has been demonstrated to be useful for relation extraction in many studies (Xu et al., 2015; Zhang et al., 2018; Guo et al., 2019). However, most models to leverage the dependency information are not naturally appropriate to model the dependency types among words. It is required to find an appropriate approach to leverage the dependency type information. Of all choices, key-value memory networks (KVMN) (Miller et al., 2016) is an effective solution in modeling pair-wisely organized information to improve many NLP tasks (Tapaswi et al., 2016; Das et al., 2017; Mino et al., 2017; Xu et al., 2019; Nie et al., 2020; Song et al., 2020; Tian et al., 2020a,d, 2021b). Specifically, KVMN maps the information instances into a list of memory slots si = (ki , vi ) (i is the index of the memory slot si ) with ki referring to the key and vi the value, respectively. The KVMN addresses the memory slot si by assigning a weight pi to the value vi by comparing the input (denoted by x) to the key ki : pi = softmax (AΦX (x) · AΦK (k"
2021.findings-acl.221,I17-2049,0,0.0207829,"ndency tree of the input sentence has been demonstrated to be useful for relation extraction in many studies (Xu et al., 2015; Zhang et al., 2018; Guo et al., 2019). However, most models to leverage the dependency information are not naturally appropriate to model the dependency types among words. It is required to find an appropriate approach to leverage the dependency type information. Of all choices, key-value memory networks (KVMN) (Miller et al., 2016) is an effective solution in modeling pair-wisely organized information to improve many NLP tasks (Tapaswi et al., 2016; Das et al., 2017; Mino et al., 2017; Xu et al., 2019; Nie et al., 2020; Song et al., 2020; Tian et al., 2020a,d, 2021b). Specifically, KVMN maps the information instances into a list of memory slots si = (ki , vi ) (i is the index of the memory slot si ) with ki referring to the key and vi the value, respectively. The KVMN addresses the memory slot si by assigning a weight pi to the value vi by comparing the input (denoted by x) to the key ki : pi = softmax (AΦX (x) · AΦK (ki )) (1) where Φ· are functions that map the input features into their embeddings and A is a matrix that maps the embeddings into another vector space. Afte"
2021.findings-acl.221,2020.findings-emnlp.378,1,0.438172,"s been demonstrated to be useful for relation extraction in many studies (Xu et al., 2015; Zhang et al., 2018; Guo et al., 2019). However, most models to leverage the dependency information are not naturally appropriate to model the dependency types among words. It is required to find an appropriate approach to leverage the dependency type information. Of all choices, key-value memory networks (KVMN) (Miller et al., 2016) is an effective solution in modeling pair-wisely organized information to improve many NLP tasks (Tapaswi et al., 2016; Das et al., 2017; Mino et al., 2017; Xu et al., 2019; Nie et al., 2020; Song et al., 2020; Tian et al., 2020a,d, 2021b). Specifically, KVMN maps the information instances into a list of memory slots si = (ki , vi ) (i is the index of the memory slot si ) with ki referring to the key and vi the value, respectively. The KVMN addresses the memory slot si by assigning a weight pi to the value vi by comparing the input (denoted by x) to the key ki : pi = softmax (AΦX (x) · AΦK (ki )) (1) where Φ· are functions that map the input features into their embeddings and A is a matrix that maps the embeddings into another vector space. After addressing all memory slots, KVMN"
2021.findings-acl.221,D16-1040,0,0.0189517,"ng to its contribution to relation extraction. Our approach not only leverages dependency connections and types between words, but also distinguishes reliable dependency information from noisy ones and appropriately model them. The effectiveness of our approach is demonstrated by the experiments on two English benchmark datasets, where our approach achieves state-ofthe-art performance on both datasets.1 1 Figure 1: An illustration of an example sentence (including the entity terms “bone marrow” and “stem cells”) with its dependency parsing result. to downstream tasks such as schema induction (Nimishakavi et al., 2016), knowledge graph construction (Yu et al., 2017), and question answering (Xu et al., 2016). Normally, relation extraction aims to predict the relation between each pair of entities in a given sentence. For example, in the sentence “the [bone marrow]e1 produces [stem cells]e2 ” with the entity terms “bone marrow” and “stem cells”, the relation between the two entities is “Product-Producer“. Therefore, the ability of modeling the context from the input is of great importance to guarantee the performance of relation extraction. To this end, approaches based on neural networks have achieved promis"
2021.findings-acl.221,P15-1061,0,0.154113,"o predict the relation between each pair of entities in a given sentence. For example, in the sentence “the [bone marrow]e1 produces [stem cells]e2 ” with the entity terms “bone marrow” and “stem cells”, the relation between the two entities is “Product-Producer“. Therefore, the ability of modeling the context from the input is of great importance to guarantee the performance of relation extraction. To this end, approaches based on neural networks have achieved promising success for the task in the past decade (Socher et al., 2012; Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Wu and He, 2019; Soares et al., 2019; Fu et al., 2019; Aydar et al., 2020; Tian et al., 2021c) because of their effectiveness in capturing contextual information by powerful encoders. Introduction Relation extraction is an important natural language processing (NLP) task that facilitates information extraction, whose results is beneficial * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/RE-TaMM. † In addition, previous studies try"
2021.findings-acl.221,P19-1279,0,0.186883,"rrow]e1 produces [stem cells]e2 ” with the entity terms “bone marrow” and “stem cells”, the relation between the two entities is “Product-Producer“. Therefore, the ability of modeling the context from the input is of great importance to guarantee the performance of relation extraction. To this end, approaches based on neural networks have achieved promising success for the task in the past decade (Socher et al., 2012; Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Wu and He, 2019; Soares et al., 2019; Fu et al., 2019; Aydar et al., 2020; Tian et al., 2021c) because of their effectiveness in capturing contextual information by powerful encoders. Introduction Relation extraction is an important natural language processing (NLP) task that facilitates information extraction, whose results is beneficial * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/RE-TaMM. † In addition, previous studies try to improve relation extraction performance by incorporating extra knowledge into their models. Among all such knowl"
2021.findings-acl.221,D12-1110,0,0.0802923,"17), and question answering (Xu et al., 2016). Normally, relation extraction aims to predict the relation between each pair of entities in a given sentence. For example, in the sentence “the [bone marrow]e1 produces [stem cells]e2 ” with the entity terms “bone marrow” and “stem cells”, the relation between the two entities is “Product-Producer“. Therefore, the ability of modeling the context from the input is of great importance to guarantee the performance of relation extraction. To this end, approaches based on neural networks have achieved promising success for the task in the past decade (Socher et al., 2012; Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Wu and He, 2019; Soares et al., 2019; Fu et al., 2019; Aydar et al., 2020; Tian et al., 2021c) because of their effectiveness in capturing contextual information by powerful encoders. Introduction Relation extraction is an important natural language processing (NLP) task that facilitates information extraction, whose results is beneficial * Equal contribution. Corresponding author. 1 The code and models involved in this paper are rel"
2021.findings-acl.221,K17-1016,1,0.747767,", we follow previous studies (Hendrickx et al., 2010; Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Soares et al., 2019) to use its official train/test split. The statistics of the two datasets are summarized in Table 1. 4.2 Implementation In our experiments, we use Standard CoreNLP Toolkits (SCT)11 to obtain the dependency tree for each input sentence. Since the quality of text representation plays an important role in the performance of NLP models (Komninos and Manandhar, 2016; Song et al., 2017, 2018; Liu and Lapata, 2018; Song and Shi, 2018; Song et al., 2021), we use BERT12 (Devlin et al., 2019), which is a pre-trained language model that achieves stateof-the-art in many NLP tasks (Wu and He, 2019; Soares et al., 2019; Tian et al., 2020b,c, 2021a), as the encoder in our model. Specifically, we use the uncased version of BERT with its default settings (e.g., for BERT-base, we use 12 layers of multi-head attentions with 768 dimensional hidden vectors; for BERT-large, we use 24 layers of multihead attentions with 1024 dimensional hidden vectors) and fine-tune its all trainable parame"
2021.findings-acl.221,2020.coling-main.63,1,0.488536,"d to be useful for relation extraction in many studies (Xu et al., 2015; Zhang et al., 2018; Guo et al., 2019). However, most models to leverage the dependency information are not naturally appropriate to model the dependency types among words. It is required to find an appropriate approach to leverage the dependency type information. Of all choices, key-value memory networks (KVMN) (Miller et al., 2016) is an effective solution in modeling pair-wisely organized information to improve many NLP tasks (Tapaswi et al., 2016; Das et al., 2017; Mino et al., 2017; Xu et al., 2019; Nie et al., 2020; Song et al., 2020; Tian et al., 2020a,d, 2021b). Specifically, KVMN maps the information instances into a list of memory slots si = (ki , vi ) (i is the index of the memory slot si ) with ki referring to the key and vi the value, respectively. The KVMN addresses the memory slot si by assigning a weight pi to the value vi by comparing the input (denoted by x) to the key ki : pi = softmax (AΦX (x) · AΦK (ki )) (1) where Φ· are functions that map the input features into their embeddings and A is a matrix that maps the embeddings into another vector space. After addressing all memory slots, KVMN reads the values b"
2021.findings-acl.221,2021.naacl-main.231,1,0.596059,"one marrow” and “stem cells”, the relation between the two entities is “Product-Producer“. Therefore, the ability of modeling the context from the input is of great importance to guarantee the performance of relation extraction. To this end, approaches based on neural networks have achieved promising success for the task in the past decade (Socher et al., 2012; Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Wu and He, 2019; Soares et al., 2019; Fu et al., 2019; Aydar et al., 2020; Tian et al., 2021c) because of their effectiveness in capturing contextual information by powerful encoders. Introduction Relation extraction is an important natural language processing (NLP) task that facilitates information extraction, whose results is beneficial * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/RE-TaMM. † In addition, previous studies try to improve relation extraction performance by incorporating extra knowledge into their models. Among all such knowledge, syntactic information from the autogenerated depen"
2021.findings-acl.221,2021.eacl-main.326,1,0.821593,"one marrow” and “stem cells”, the relation between the two entities is “Product-Producer“. Therefore, the ability of modeling the context from the input is of great importance to guarantee the performance of relation extraction. To this end, approaches based on neural networks have achieved promising success for the task in the past decade (Socher et al., 2012; Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Wu and He, 2019; Soares et al., 2019; Fu et al., 2019; Aydar et al., 2020; Tian et al., 2021c) because of their effectiveness in capturing contextual information by powerful encoders. Introduction Relation extraction is an important natural language processing (NLP) task that facilitates information extraction, whose results is beneficial * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/RE-TaMM. † In addition, previous studies try to improve relation extraction performance by incorporating extra knowledge into their models. Among all such knowledge, syntactic information from the autogenerated depen"
2021.findings-acl.221,2021.acl-long.344,1,0.629498,"one marrow” and “stem cells”, the relation between the two entities is “Product-Producer“. Therefore, the ability of modeling the context from the input is of great importance to guarantee the performance of relation extraction. To this end, approaches based on neural networks have achieved promising success for the task in the past decade (Socher et al., 2012; Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Wu and He, 2019; Soares et al., 2019; Fu et al., 2019; Aydar et al., 2020; Tian et al., 2021c) because of their effectiveness in capturing contextual information by powerful encoders. Introduction Relation extraction is an important natural language processing (NLP) task that facilitates information extraction, whose results is beneficial * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/RE-TaMM. † In addition, previous studies try to improve relation extraction performance by incorporating extra knowledge into their models. Among all such knowledge, syntactic information from the autogenerated depen"
2021.findings-acl.221,2020.coling-main.187,1,0.897168,"relation extraction in many studies (Xu et al., 2015; Zhang et al., 2018; Guo et al., 2019). However, most models to leverage the dependency information are not naturally appropriate to model the dependency types among words. It is required to find an appropriate approach to leverage the dependency type information. Of all choices, key-value memory networks (KVMN) (Miller et al., 2016) is an effective solution in modeling pair-wisely organized information to improve many NLP tasks (Tapaswi et al., 2016; Das et al., 2017; Mino et al., 2017; Xu et al., 2019; Nie et al., 2020; Song et al., 2020; Tian et al., 2020a,d, 2021b). Specifically, KVMN maps the information instances into a list of memory slots si = (ki , vi ) (i is the index of the memory slot si ) with ki referring to the key and vi the value, respectively. The KVMN addresses the memory slot si by assigning a weight pi to the value vi by comparing the input (denoted by x) to the key ki : pi = softmax (AΦX (x) · AΦK (ki )) (1) where Φ· are functions that map the input features into their embeddings and A is a matrix that maps the embeddings into another vector space. After addressing all memory slots, KVMN reads the values by computing the wei"
2021.findings-acl.221,2020.emnlp-main.487,1,0.809938,"relation extraction in many studies (Xu et al., 2015; Zhang et al., 2018; Guo et al., 2019). However, most models to leverage the dependency information are not naturally appropriate to model the dependency types among words. It is required to find an appropriate approach to leverage the dependency type information. Of all choices, key-value memory networks (KVMN) (Miller et al., 2016) is an effective solution in modeling pair-wisely organized information to improve many NLP tasks (Tapaswi et al., 2016; Das et al., 2017; Mino et al., 2017; Xu et al., 2019; Nie et al., 2020; Song et al., 2020; Tian et al., 2020a,d, 2021b). Specifically, KVMN maps the information instances into a list of memory slots si = (ki , vi ) (i is the index of the memory slot si ) with ki referring to the key and vi the value, respectively. The KVMN addresses the memory slot si by assigning a weight pi to the value vi by comparing the input (denoted by x) to the key ki : pi = softmax (AΦX (x) · AΦK (ki )) (1) where Φ· are functions that map the input features into their embeddings and A is a matrix that maps the embeddings into another vector space. After addressing all memory slots, KVMN reads the values by computing the wei"
2021.findings-acl.221,2020.acl-main.734,1,0.84168,"relation extraction in many studies (Xu et al., 2015; Zhang et al., 2018; Guo et al., 2019). However, most models to leverage the dependency information are not naturally appropriate to model the dependency types among words. It is required to find an appropriate approach to leverage the dependency type information. Of all choices, key-value memory networks (KVMN) (Miller et al., 2016) is an effective solution in modeling pair-wisely organized information to improve many NLP tasks (Tapaswi et al., 2016; Das et al., 2017; Mino et al., 2017; Xu et al., 2019; Nie et al., 2020; Song et al., 2020; Tian et al., 2020a,d, 2021b). Specifically, KVMN maps the information instances into a list of memory slots si = (ki , vi ) (i is the index of the memory slot si ) with ki referring to the key and vi the value, respectively. The KVMN addresses the memory slot si by assigning a weight pi to the value vi by comparing the input (denoted by x) to the key ki : pi = softmax (AΦX (x) · AΦK (ki )) (1) where Φ· are functions that map the input features into their embeddings and A is a matrix that maps the embeddings into another vector space. After addressing all memory slots, KVMN reads the values by computing the wei"
2021.findings-acl.221,P19-1132,0,0.0145435,"dencies and their dependency types so as to predict the correct relation for the two entities: “Entity-Destination”. 6 Related Work Relation extraction is an important task in NLP, which significantly relies on a good modeling of the contextual information to achieve outstanding model performance. To improve the capability of context modeling for relation extraction, studies in the past decade leverage neural networks, such as using CNN (Zeng et al., 2014; Wang et al., 2016), RNN (Socher et al., 2012; Xu et al., 2015; Zhou et al., 2016) and BERT encoders (Wu and He, 2019; Soares et al., 2019; Wang et al., 2019). To further 2508 Figure 5: An example input fed into our model with TaMM (Both) and its correctly predicted relation between the two entities marked in red. Word dependencies are highlighted in different colors to visualize the total weights assigned to their corresponding in-entity and cross-entity memory slots, where darker color refers to higher weight. enhance the models for this task, incorporating extra knowledge into the models has been proved as an effective method, where normally three types of extra knowledge are used: lexical, syntactic and semantic knowledge, and syntactic knowled"
2021.findings-acl.221,P16-1123,0,0.0413461,"Missing"
2021.findings-acl.221,N19-1301,0,0.0237087,"input sentence has been demonstrated to be useful for relation extraction in many studies (Xu et al., 2015; Zhang et al., 2018; Guo et al., 2019). However, most models to leverage the dependency information are not naturally appropriate to model the dependency types among words. It is required to find an appropriate approach to leverage the dependency type information. Of all choices, key-value memory networks (KVMN) (Miller et al., 2016) is an effective solution in modeling pair-wisely organized information to improve many NLP tasks (Tapaswi et al., 2016; Das et al., 2017; Mino et al., 2017; Xu et al., 2019; Nie et al., 2020; Song et al., 2020; Tian et al., 2020a,d, 2021b). Specifically, KVMN maps the information instances into a list of memory slots si = (ki , vi ) (i is the index of the memory slot si ) with ki referring to the key and vi the value, respectively. The KVMN addresses the memory slot si by assigning a weight pi to the value vi by comparing the input (denoted by x) to the key ki : pi = softmax (AΦX (x) · AΦK (ki )) (1) where Φ· are functions that map the input features into their embeddings and A is a matrix that maps the embeddings into another vector space. After addressing all"
2021.findings-acl.221,P16-1220,0,0.0180806,"ns and types between words, but also distinguishes reliable dependency information from noisy ones and appropriately model them. The effectiveness of our approach is demonstrated by the experiments on two English benchmark datasets, where our approach achieves state-ofthe-art performance on both datasets.1 1 Figure 1: An illustration of an example sentence (including the entity terms “bone marrow” and “stem cells”) with its dependency parsing result. to downstream tasks such as schema induction (Nimishakavi et al., 2016), knowledge graph construction (Yu et al., 2017), and question answering (Xu et al., 2016). Normally, relation extraction aims to predict the relation between each pair of entities in a given sentence. For example, in the sentence “the [bone marrow]e1 produces [stem cells]e2 ” with the entity terms “bone marrow” and “stem cells”, the relation between the two entities is “Product-Producer“. Therefore, the ability of modeling the context from the input is of great importance to guarantee the performance of relation extraction. To this end, approaches based on neural networks have achieved promising success for the task in the past decade (Socher et al., 2012; Zeng et al., 2014; Zhang"
2021.findings-acl.221,D15-1206,0,0.224812,"ion extraction aims to predict the relation between each pair of entities in a given sentence. For example, in the sentence “the [bone marrow]e1 produces [stem cells]e2 ” with the entity terms “bone marrow” and “stem cells”, the relation between the two entities is “Product-Producer“. Therefore, the ability of modeling the context from the input is of great importance to guarantee the performance of relation extraction. To this end, approaches based on neural networks have achieved promising success for the task in the past decade (Socher et al., 2012; Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Wu and He, 2019; Soares et al., 2019; Fu et al., 2019; Aydar et al., 2020; Tian et al., 2021c) because of their effectiveness in capturing contextual information by powerful encoders. Introduction Relation extraction is an important natural language processing (NLP) task that facilitates information extraction, whose results is beneficial * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/RE-TaMM. † In addit"
2021.findings-acl.221,P19-1130,0,0.0640177,"ACE2005EN (ACE2005)8 and SemEval 2010 Task 8 (SemEval)9 (Hendrickx et al., 2010) are used in the experiments to evaluate our approach. For ACE2005, we follow the same preprocess as that in Christopoulou 8 https://catalog.ldc.upenn.edu/ LDC2006T06. 9 http://docs.google.com/View?docid= dfvxd49s_36c28v9pmw. 2505 Hyper-parameters Values Models Learning Rate Warmup Rate Dropout Rate Batch Size 5e − 6, 1e − 5, 2e − 5, 3e − 5 0.06, 0.1 0.1 16, 32, 64, 128 Table 2: The hyper-parameters tested in tuning our models. The best ones used in our final experiments are highlighted in boldface. et al. (2018); Ye et al. (2019), by removing the two small subsets: cts and un, and splitting the remaining 511 documents into three parts: 351 for training, 80 for development and the rest 80 for test10 . For SemEval, we follow previous studies (Hendrickx et al., 2010; Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Soares et al., 2019) to use its official train/test split. The statistics of the two datasets are summarized in Table 1. 4.2 Implementation In our experiments, we use Standard CoreNLP Toolkits (SCT)1"
2021.findings-acl.221,2020.coling-main.341,0,0.0216764,"or refers to higher weight. enhance the models for this task, incorporating extra knowledge into the models has been proved as an effective method, where normally three types of extra knowledge are used: lexical, syntactic and semantic knowledge, and syntactic knowledge has been proved to be useful for this task (Xu et al., 2015). With this finding, there are studies also using advanced neural architecture, such as graph convolutional networks, to incorporate syntactic knowledge from auto-generated dependency parse of the input sentence (Zhang et al., 2018; Guo et al., 2019; Sun et al., 2020; Yu et al., 2020; Mandya et al., 2020). Compared to the aforementioned studies, TaMM offers a simple yet effective nongraph-based approach to leverage dependencies for relation extraction. TaMM provides the ability not only incorporate both word dependencies and their types into the model to help improve relation extraction performance, but also discriminatively leverage the dependencies by assigning different weights to them, which can address the potential noise in the auto-generated dependencies and thus further improve model performance. 7 Conclusion In this paper, we proposed an effective method for rela"
2021.findings-acl.221,P17-1053,0,0.0164838,"ch not only leverages dependency connections and types between words, but also distinguishes reliable dependency information from noisy ones and appropriately model them. The effectiveness of our approach is demonstrated by the experiments on two English benchmark datasets, where our approach achieves state-ofthe-art performance on both datasets.1 1 Figure 1: An illustration of an example sentence (including the entity terms “bone marrow” and “stem cells”) with its dependency parsing result. to downstream tasks such as schema induction (Nimishakavi et al., 2016), knowledge graph construction (Yu et al., 2017), and question answering (Xu et al., 2016). Normally, relation extraction aims to predict the relation between each pair of entities in a given sentence. For example, in the sentence “the [bone marrow]e1 produces [stem cells]e2 ” with the entity terms “bone marrow” and “stem cells”, the relation between the two entities is “Product-Producer“. Therefore, the ability of modeling the context from the input is of great importance to guarantee the performance of relation extraction. To this end, approaches based on neural networks have achieved promising success for the task in the past decade (Soc"
2021.findings-acl.221,C14-1220,0,0.461338,"wering (Xu et al., 2016). Normally, relation extraction aims to predict the relation between each pair of entities in a given sentence. For example, in the sentence “the [bone marrow]e1 produces [stem cells]e2 ” with the entity terms “bone marrow” and “stem cells”, the relation between the two entities is “Product-Producer“. Therefore, the ability of modeling the context from the input is of great importance to guarantee the performance of relation extraction. To this end, approaches based on neural networks have achieved promising success for the task in the past decade (Socher et al., 2012; Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Wu and He, 2019; Soares et al., 2019; Fu et al., 2019; Aydar et al., 2020; Tian et al., 2021c) because of their effectiveness in capturing contextual information by powerful encoders. Introduction Relation extraction is an important natural language processing (NLP) task that facilitates information extraction, whose results is beneficial * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://gi"
2021.findings-acl.221,Y15-1009,0,0.0642802,"n between each pair of entities in a given sentence. For example, in the sentence “the [bone marrow]e1 produces [stem cells]e2 ” with the entity terms “bone marrow” and “stem cells”, the relation between the two entities is “Product-Producer“. Therefore, the ability of modeling the context from the input is of great importance to guarantee the performance of relation extraction. To this end, approaches based on neural networks have achieved promising success for the task in the past decade (Socher et al., 2012; Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Wu and He, 2019; Soares et al., 2019; Fu et al., 2019; Aydar et al., 2020; Tian et al., 2021c) because of their effectiveness in capturing contextual information by powerful encoders. Introduction Relation extraction is an important natural language processing (NLP) task that facilitates information extraction, whose results is beneficial * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/RE-TaMM. † In addition, previous studies try to improve relation"
2021.findings-acl.221,D18-1244,0,0.100591,"garded as a text classification task, where an input sentence X = x1 · · · xl has l words and two entities, i.e., E1 and E2 , in it are mapped to a particular relation class (denoted by yb).3 In most cases the contextual information is of great importance to make a correct prediction for relations. Therefore, it is straightforward to consider integrating extra features to enhance contextual modeling. Of all such features, the syntactic information suggested by the dependency tree of the input sentence has been demonstrated to be useful for relation extraction in many studies (Xu et al., 2015; Zhang et al., 2018; Guo et al., 2019). However, most models to leverage the dependency information are not naturally appropriate to model the dependency types among words. It is required to find an appropriate approach to leverage the dependency type information. Of all choices, key-value memory networks (KVMN) (Miller et al., 2016) is an effective solution in modeling pair-wisely organized information to improve many NLP tasks (Tapaswi et al., 2016; Das et al., 2017; Mino et al., 2017; Xu et al., 2019; Nie et al., 2020; Song et al., 2020; Tian et al., 2020a,d, 2021b). Specifically, KVMN maps the information in"
2021.findings-acl.221,D17-1004,0,0.103186,"xample, in the sentence “the [bone marrow]e1 produces [stem cells]e2 ” with the entity terms “bone marrow” and “stem cells”, the relation between the two entities is “Product-Producer“. Therefore, the ability of modeling the context from the input is of great importance to guarantee the performance of relation extraction. To this end, approaches based on neural networks have achieved promising success for the task in the past decade (Socher et al., 2012; Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Wu and He, 2019; Soares et al., 2019; Fu et al., 2019; Aydar et al., 2020; Tian et al., 2021c) because of their effectiveness in capturing contextual information by powerful encoders. Introduction Relation extraction is an important natural language processing (NLP) task that facilitates information extraction, whose results is beneficial * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/RE-TaMM. † In addition, previous studies try to improve relation extraction performance by incorporating extra knowledge i"
2021.findings-acl.221,P16-2034,0,0.430108,"ven sentence. For example, in the sentence “the [bone marrow]e1 produces [stem cells]e2 ” with the entity terms “bone marrow” and “stem cells”, the relation between the two entities is “Product-Producer“. Therefore, the ability of modeling the context from the input is of great importance to guarantee the performance of relation extraction. To this end, approaches based on neural networks have achieved promising success for the task in the past decade (Socher et al., 2012; Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Wu and He, 2019; Soares et al., 2019; Fu et al., 2019; Aydar et al., 2020; Tian et al., 2021c) because of their effectiveness in capturing contextual information by powerful encoders. Introduction Relation extraction is an important natural language processing (NLP) task that facilitates information extraction, whose results is beneficial * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/RE-TaMM. † In addition, previous studies try to improve relation extraction performance by incorporati"
2021.findings-acl.376,2020.coling-main.24,1,0.753372,"t localized prediction; on the contrary, models under the FL setting is stored on the server and shared by all nodes, so that optimizing the model on a particular node could significantly hurt the performance on others. Therefore, the setting of the Union reference model is the ideal situation which is hard to happen in real-applications and it thus provides a potential upper-boundary of model performance for FL-based approaches. because they are pre-trained language models that have been demonstrated to be effective in many NLP tasks (Nie et al., 2020; Huang et al., 2020a; Song et al., 2020; Chen et al., 2020; Fu et al., 2020; Tian et al., 2020a,b,c,d, 2021a,b; Chen et al., 2021; Qin et al., 2021). For both BERT and ZEN 2.0, we use the default settings (i.e., 12 layers of multi-head attentions with 768 dimensional hidden vectors for BERT and 24 layers of multi-head attentions with 1024 dimensional hidden vectors for ZEN 2.0). We use the vocabulary in Tencent Embedding6 (Song et al., 2018) to initialize our lexicon D and the n-gram embedding matrix, where n-grams whose character-based length higher than five are filtered out7 . During the training stage, we fix the n-gram embedding matrix and updat"
2021.findings-acl.376,2021.findings-acl.221,1,0.75154,"stored on the server and shared by all nodes, so that optimizing the model on a particular node could significantly hurt the performance on others. Therefore, the setting of the Union reference model is the ideal situation which is hard to happen in real-applications and it thus provides a potential upper-boundary of model performance for FL-based approaches. because they are pre-trained language models that have been demonstrated to be effective in many NLP tasks (Nie et al., 2020; Huang et al., 2020a; Song et al., 2020; Chen et al., 2020; Fu et al., 2020; Tian et al., 2020a,b,c,d, 2021a,b; Chen et al., 2021; Qin et al., 2021). For both BERT and ZEN 2.0, we use the default settings (i.e., 12 layers of multi-head attentions with 768 dimensional hidden vectors for BERT and 24 layers of multi-head attentions with 1024 dimensional hidden vectors for ZEN 2.0). We use the vocabulary in Tencent Embedding6 (Song et al., 2018) to initialize our lexicon D and the n-gram embedding matrix, where n-grams whose character-based length higher than five are filtered out7 . During the training stage, we fix the n-gram embedding matrix and update all other parameters (including BERT). For evaluation, we follow prev"
2021.findings-acl.376,P17-1110,0,0.246931,"simulated environment with five nodes confirm the effectiveness of our approach, where our approach outperforms different baselines including some well-designed FL frameworks.1 1 Introduction Chinese word segmentation (CWS) is a preliminary and vital task for natural language processing (NLP). This task aims to segment Chinese character sequence into words and thus is generally performed as a sequence labeling task (Tseng et al., 2005; Levow, 2006; Song et al., 2009a; Sun and Xu, 2011; Song and Xia, 2012, 2013; Mansur et al., 2013). Although recent neural-based CWS systems (Pei et al., 2014; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019; Ke et al., 2020; Huang et al., 2020a; Tian et al., 2020e) have achieved very good performance on benchmark datasets, it is still an unsolved task (Fu * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/GCASeg. † et al., 2020), because it is challenging to handle out-of-vocabulary words (OOV), especially in real applications where the test data may come from different sources. Although leveraging extra labeled data from other sources or domains could"
2021.findings-acl.376,N19-1423,0,0.00561963,"all combinations of them for each model on the development set, where models achieve highest F1 score on the development set is evaluated on the test set (the best hyper-parameter setting in our experiments is highlighted in boldface). 4 4.1 Results and Analysis Overall Results A good text representation is generally a prerequisite to achieve outstanding model performance (Pennington et al., 2014; Song and Shi, 2018; Peters et al., 2018). To obtain a high qulity of text representation, in our experiments, we try two types of encoder in the centralized model, i.e., the Chinese version of BERT (Devlin et al., 2019)4 and the large version of ZEN 2.0 (Song et al., 2021)5 , Table 4 illustrates the experimental results (i.e., F1 scores) of our GCA-FL models and all the aforementioned baselines (i.e., FL) and reference models (i.e., Single and Union) with BERT (a) and ZEN 2.0 (b) encoders on the test set of BC, BN, MZ, NW, and Web from CTB7. There are several observations from the test set results. First, models under the FL framework (i.e., FL and GCA-FL) outperform the reference model (Single) trained on the single node for both BERT and ZEN 2.0 encoder, which confirms that FL works well to leverage extra"
2021.findings-acl.376,2020.emnlp-main.457,0,0.0329918,"ion; on the contrary, models under the FL setting is stored on the server and shared by all nodes, so that optimizing the model on a particular node could significantly hurt the performance on others. Therefore, the setting of the Union reference model is the ideal situation which is hard to happen in real-applications and it thus provides a potential upper-boundary of model performance for FL-based approaches. because they are pre-trained language models that have been demonstrated to be effective in many NLP tasks (Nie et al., 2020; Huang et al., 2020a; Song et al., 2020; Chen et al., 2020; Fu et al., 2020; Tian et al., 2020a,b,c,d, 2021a,b; Chen et al., 2021; Qin et al., 2021). For both BERT and ZEN 2.0, we use the default settings (i.e., 12 layers of multi-head attentions with 768 dimensional hidden vectors for BERT and 24 layers of multi-head attentions with 1024 dimensional hidden vectors for ZEN 2.0). We use the vocabulary in Tencent Embedding6 (Song et al., 2018) to initialize our lexicon D and the n-gram embedding matrix, where n-grams whose character-based length higher than five are filtered out7 . During the training stage, we fix the n-gram embedding matrix and update all other param"
2021.findings-acl.376,N19-1276,0,0.1015,"es confirm the effectiveness of our approach, where our approach outperforms different baselines including some well-designed FL frameworks.1 1 Introduction Chinese word segmentation (CWS) is a preliminary and vital task for natural language processing (NLP). This task aims to segment Chinese character sequence into words and thus is generally performed as a sequence labeling task (Tseng et al., 2005; Levow, 2006; Song et al., 2009a; Sun and Xu, 2011; Song and Xia, 2012, 2013; Mansur et al., 2013). Although recent neural-based CWS systems (Pei et al., 2014; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019; Ke et al., 2020; Huang et al., 2020a; Tian et al., 2020e) have achieved very good performance on benchmark datasets, it is still an unsolved task (Fu * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/GCASeg. † et al., 2020), because it is challenging to handle out-of-vocabulary words (OOV), especially in real applications where the test data may come from different sources. Although leveraging extra labeled data from other sources or domains could alleviate this issue, in real applications,"
2021.findings-acl.376,2020.emnlp-main.318,0,0.307015,"ach outperforms different baselines including some well-designed FL frameworks.1 1 Introduction Chinese word segmentation (CWS) is a preliminary and vital task for natural language processing (NLP). This task aims to segment Chinese character sequence into words and thus is generally performed as a sequence labeling task (Tseng et al., 2005; Levow, 2006; Song et al., 2009a; Sun and Xu, 2011; Song and Xia, 2012, 2013; Mansur et al., 2013). Although recent neural-based CWS systems (Pei et al., 2014; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019; Ke et al., 2020; Huang et al., 2020a; Tian et al., 2020e) have achieved very good performance on benchmark datasets, it is still an unsolved task (Fu * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/GCASeg. † et al., 2020), because it is challenging to handle out-of-vocabulary words (OOV), especially in real applications where the test data may come from different sources. Although leveraging extra labeled data from other sources or domains could alleviate this issue, in real applications, such data are always located in different nodes and th"
2021.findings-acl.376,W06-0115,0,0.105629,"th data isolation, where a mechanism of global character associations is proposed to enhance FL to learn from different data sources. Experimental results on a simulated environment with five nodes confirm the effectiveness of our approach, where our approach outperforms different baselines including some well-designed FL frameworks.1 1 Introduction Chinese word segmentation (CWS) is a preliminary and vital task for natural language processing (NLP). This task aims to segment Chinese character sequence into words and thus is generally performed as a sequence labeling task (Tseng et al., 2005; Levow, 2006; Song et al., 2009a; Sun and Xu, 2011; Song and Xia, 2012, 2013; Mansur et al., 2013). Although recent neural-based CWS systems (Pei et al., 2014; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019; Ke et al., 2020; Huang et al., 2020a; Tian et al., 2020e) have achieved very good performance on benchmark datasets, it is still an unsolved task (Fu * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/GCASeg. † et al., 2020), because it is challenging to handle out-of-vocabulary words (O"
2021.findings-acl.376,W19-5030,0,0.0609647,"tain various expressing patterns. For example, in real applications such as Input Method Editors (IME, such as pinyin input environment), there are millions of individual users with their data stored in isolated nodes, where the different nodes could have diverse segmentation requirement due to the users’ preference. Therefore, the restricted data access of traditional FL approaches could result in inferior performance for CWS since they cannot update the model to facilitate localized prediction. Unfortunately, limited attentions have been paid to address this issue. Most existing approaches (Liu et al., 2019; Huang et al., 2020b; Sui et al., 2020) with FL on NLP (e.g., for language modeling 4306 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4306–4313 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: The server-node architecture of our approach. The encrypted information (i.e., encrypted data, word segmentation tags, and loss) communicates between a node and the server, where the locally stored data is inaccessible to other nodes during the training process. Figure 2: An overview of GCA-FL, where centralized model is illustrated on the"
2021.findings-acl.376,D18-1529,0,0.0217206,"Missing"
2021.findings-acl.376,I13-1181,0,0.0242951,"osed to enhance FL to learn from different data sources. Experimental results on a simulated environment with five nodes confirm the effectiveness of our approach, where our approach outperforms different baselines including some well-designed FL frameworks.1 1 Introduction Chinese word segmentation (CWS) is a preliminary and vital task for natural language processing (NLP). This task aims to segment Chinese character sequence into words and thus is generally performed as a sequence labeling task (Tseng et al., 2005; Levow, 2006; Song et al., 2009a; Sun and Xu, 2011; Song and Xia, 2012, 2013; Mansur et al., 2013). Although recent neural-based CWS systems (Pei et al., 2014; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019; Ke et al., 2020; Huang et al., 2020a; Tian et al., 2020e) have achieved very good performance on benchmark datasets, it is still an unsolved task (Fu * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/GCASeg. † et al., 2020), because it is challenging to handle out-of-vocabulary words (OOV), especially in real applications where the test data may come from different sourc"
2021.findings-acl.376,2020.findings-emnlp.378,1,0.614756,"be optimized on a particular local node to achieve the best localized prediction; on the contrary, models under the FL setting is stored on the server and shared by all nodes, so that optimizing the model on a particular node could significantly hurt the performance on others. Therefore, the setting of the Union reference model is the ideal situation which is hard to happen in real-applications and it thus provides a potential upper-boundary of model performance for FL-based approaches. because they are pre-trained language models that have been demonstrated to be effective in many NLP tasks (Nie et al., 2020; Huang et al., 2020a; Song et al., 2020; Chen et al., 2020; Fu et al., 2020; Tian et al., 2020a,b,c,d, 2021a,b; Chen et al., 2021; Qin et al., 2021). For both BERT and ZEN 2.0, we use the default settings (i.e., 12 layers of multi-head attentions with 768 dimensional hidden vectors for BERT and 24 layers of multi-head attentions with 1024 dimensional hidden vectors for ZEN 2.0). We use the vocabulary in Tencent Embedding6 (Song et al., 2018) to initialize our lexicon D and the n-gram embedding matrix, where n-grams whose character-based length higher than five are filtered out7 . During the t"
2021.findings-acl.376,P14-1028,0,0.167944,"ental results on a simulated environment with five nodes confirm the effectiveness of our approach, where our approach outperforms different baselines including some well-designed FL frameworks.1 1 Introduction Chinese word segmentation (CWS) is a preliminary and vital task for natural language processing (NLP). This task aims to segment Chinese character sequence into words and thus is generally performed as a sequence labeling task (Tseng et al., 2005; Levow, 2006; Song et al., 2009a; Sun and Xu, 2011; Song and Xia, 2012, 2013; Mansur et al., 2013). Although recent neural-based CWS systems (Pei et al., 2014; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019; Ke et al., 2020; Huang et al., 2020a; Tian et al., 2020e) have achieved very good performance on benchmark datasets, it is still an unsolved task (Fu * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/GCASeg. † et al., 2020), because it is challenging to handle out-of-vocabulary words (OOV), especially in real applications where the test data may come from different sources. Although leveraging extra labeled data from other source"
2021.findings-acl.376,D14-1162,0,0.0928901,"ther parameters (including BERT). For evaluation, we follow previous studies to use the F1 scores (Chen et al., 2017; Ma et al., 2018; Qiu et al., 2019). For other hyperparameter settings, we report them in Table 3. We test all combinations of them for each model on the development set, where models achieve highest F1 score on the development set is evaluated on the test set (the best hyper-parameter setting in our experiments is highlighted in boldface). 4 4.1 Results and Analysis Overall Results A good text representation is generally a prerequisite to achieve outstanding model performance (Pennington et al., 2014; Song and Shi, 2018; Peters et al., 2018). To obtain a high qulity of text representation, in our experiments, we try two types of encoder in the centralized model, i.e., the Chinese version of BERT (Devlin et al., 2019)4 and the large version of ZEN 2.0 (Song et al., 2021)5 , Table 4 illustrates the experimental results (i.e., F1 scores) of our GCA-FL models and all the aforementioned baselines (i.e., FL) and reference models (i.e., Single and Union) with BERT (a) and ZEN 2.0 (b) encoders on the test set of BC, BN, MZ, NW, and Web from CTB7. There are several observations from the test set r"
2021.findings-acl.376,N18-1202,0,0.0189939,"ion, we follow previous studies to use the F1 scores (Chen et al., 2017; Ma et al., 2018; Qiu et al., 2019). For other hyperparameter settings, we report them in Table 3. We test all combinations of them for each model on the development set, where models achieve highest F1 score on the development set is evaluated on the test set (the best hyper-parameter setting in our experiments is highlighted in boldface). 4 4.1 Results and Analysis Overall Results A good text representation is generally a prerequisite to achieve outstanding model performance (Pennington et al., 2014; Song and Shi, 2018; Peters et al., 2018). To obtain a high qulity of text representation, in our experiments, we try two types of encoder in the centralized model, i.e., the Chinese version of BERT (Devlin et al., 2019)4 and the large version of ZEN 2.0 (Song et al., 2021)5 , Table 4 illustrates the experimental results (i.e., F1 scores) of our GCA-FL models and all the aforementioned baselines (i.e., FL) and reference models (i.e., Single and Union) with BERT (a) and ZEN 2.0 (b) encoders on the test set of BC, BN, MZ, NW, and Web from CTB7. There are several observations from the test set results. First, models under the FL framewo"
2021.findings-acl.376,2021.acl-short.68,1,0.369797,"er and shared by all nodes, so that optimizing the model on a particular node could significantly hurt the performance on others. Therefore, the setting of the Union reference model is the ideal situation which is hard to happen in real-applications and it thus provides a potential upper-boundary of model performance for FL-based approaches. because they are pre-trained language models that have been demonstrated to be effective in many NLP tasks (Nie et al., 2020; Huang et al., 2020a; Song et al., 2020; Chen et al., 2020; Fu et al., 2020; Tian et al., 2020a,b,c,d, 2021a,b; Chen et al., 2021; Qin et al., 2021). For both BERT and ZEN 2.0, we use the default settings (i.e., 12 layers of multi-head attentions with 768 dimensional hidden vectors for BERT and 24 layers of multi-head attentions with 1024 dimensional hidden vectors for ZEN 2.0). We use the vocabulary in Tencent Embedding6 (Song et al., 2018) to initialize our lexicon D and the n-gram embedding matrix, where n-grams whose character-based length higher than five are filtered out7 . During the training stage, we fix the n-gram embedding matrix and update all other parameters (including BERT). For evaluation, we follow previous studies to use"
2021.findings-acl.376,song-xia-2012-using,1,0.7896,"acter associations is proposed to enhance FL to learn from different data sources. Experimental results on a simulated environment with five nodes confirm the effectiveness of our approach, where our approach outperforms different baselines including some well-designed FL frameworks.1 1 Introduction Chinese word segmentation (CWS) is a preliminary and vital task for natural language processing (NLP). This task aims to segment Chinese character sequence into words and thus is generally performed as a sequence labeling task (Tseng et al., 2005; Levow, 2006; Song et al., 2009a; Sun and Xu, 2011; Song and Xia, 2012, 2013; Mansur et al., 2013). Although recent neural-based CWS systems (Pei et al., 2014; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019; Ke et al., 2020; Huang et al., 2020a; Tian et al., 2020e) have achieved very good performance on benchmark datasets, it is still an unsolved task (Fu * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/GCASeg. † et al., 2020), because it is challenging to handle out-of-vocabulary words (OOV), especially in real applications where the test data m"
2021.findings-acl.376,I13-1071,1,0.625249,"Missing"
2021.findings-acl.376,2020.emnlp-main.165,0,0.219238,"xample, in real applications such as Input Method Editors (IME, such as pinyin input environment), there are millions of individual users with their data stored in isolated nodes, where the different nodes could have diverse segmentation requirement due to the users’ preference. Therefore, the restricted data access of traditional FL approaches could result in inferior performance for CWS since they cannot update the model to facilitate localized prediction. Unfortunately, limited attentions have been paid to address this issue. Most existing approaches (Liu et al., 2019; Huang et al., 2020b; Sui et al., 2020) with FL on NLP (e.g., for language modeling 4306 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4306–4313 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: The server-node architecture of our approach. The encrypted information (i.e., encrypted data, word segmentation tags, and loss) communicates between a node and the server, where the locally stored data is inaccessible to other nodes during the training process. Figure 2: An overview of GCA-FL, where centralized model is illustrated on the top and the example input sentence “南京市长"
2021.findings-acl.376,D11-1090,0,0.0394799,"ism of global character associations is proposed to enhance FL to learn from different data sources. Experimental results on a simulated environment with five nodes confirm the effectiveness of our approach, where our approach outperforms different baselines including some well-designed FL frameworks.1 1 Introduction Chinese word segmentation (CWS) is a preliminary and vital task for natural language processing (NLP). This task aims to segment Chinese character sequence into words and thus is generally performed as a sequence labeling task (Tseng et al., 2005; Levow, 2006; Song et al., 2009a; Sun and Xu, 2011; Song and Xia, 2012, 2013; Mansur et al., 2013). Although recent neural-based CWS systems (Pei et al., 2014; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019; Ke et al., 2020; Huang et al., 2020a; Tian et al., 2020e) have achieved very good performance on benchmark datasets, it is still an unsolved task (Fu * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/GCASeg. † et al., 2020), because it is challenging to handle out-of-vocabulary words (OOV), especially in real applications w"
2021.findings-acl.376,2021.naacl-main.231,1,0.781789,"Missing"
2021.findings-acl.376,W09-3511,1,0.644013,"Missing"
2021.findings-acl.376,2021.acl-long.344,1,0.364192,"Missing"
2021.findings-acl.376,N18-2028,1,0.847849,"y of model performance for FL-based approaches. because they are pre-trained language models that have been demonstrated to be effective in many NLP tasks (Nie et al., 2020; Huang et al., 2020a; Song et al., 2020; Chen et al., 2020; Fu et al., 2020; Tian et al., 2020a,b,c,d, 2021a,b; Chen et al., 2021; Qin et al., 2021). For both BERT and ZEN 2.0, we use the default settings (i.e., 12 layers of multi-head attentions with 768 dimensional hidden vectors for BERT and 24 layers of multi-head attentions with 1024 dimensional hidden vectors for ZEN 2.0). We use the vocabulary in Tencent Embedding6 (Song et al., 2018) to initialize our lexicon D and the n-gram embedding matrix, where n-grams whose character-based length higher than five are filtered out7 . During the training stage, we fix the n-gram embedding matrix and update all other parameters (including BERT). For evaluation, we follow previous studies to use the F1 scores (Chen et al., 2017; Ma et al., 2018; Qiu et al., 2019). For other hyperparameter settings, we report them in Table 3. We test all combinations of them for each model on the development set, where models achieve highest F1 score on the development set is evaluated on the test set (t"
2021.findings-acl.376,2020.coling-main.63,1,0.636414,"to achieve the best localized prediction; on the contrary, models under the FL setting is stored on the server and shared by all nodes, so that optimizing the model on a particular node could significantly hurt the performance on others. Therefore, the setting of the Union reference model is the ideal situation which is hard to happen in real-applications and it thus provides a potential upper-boundary of model performance for FL-based approaches. because they are pre-trained language models that have been demonstrated to be effective in many NLP tasks (Nie et al., 2020; Huang et al., 2020a; Song et al., 2020; Chen et al., 2020; Fu et al., 2020; Tian et al., 2020a,b,c,d, 2021a,b; Chen et al., 2021; Qin et al., 2021). For both BERT and ZEN 2.0, we use the default settings (i.e., 12 layers of multi-head attentions with 768 dimensional hidden vectors for BERT and 24 layers of multi-head attentions with 1024 dimensional hidden vectors for ZEN 2.0). We use the vocabulary in Tencent Embedding6 (Song et al., 2018) to initialize our lexicon D and the n-gram embedding matrix, where n-grams whose character-based length higher than five are filtered out7 . During the training stage, we fix the n-gram embeddi"
2021.findings-acl.376,2020.acl-main.735,1,0.893547,"rent baselines including some well-designed FL frameworks.1 1 Introduction Chinese word segmentation (CWS) is a preliminary and vital task for natural language processing (NLP). This task aims to segment Chinese character sequence into words and thus is generally performed as a sequence labeling task (Tseng et al., 2005; Levow, 2006; Song et al., 2009a; Sun and Xu, 2011; Song and Xia, 2012, 2013; Mansur et al., 2013). Although recent neural-based CWS systems (Pei et al., 2014; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019; Ke et al., 2020; Huang et al., 2020a; Tian et al., 2020e) have achieved very good performance on benchmark datasets, it is still an unsolved task (Fu * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/GCASeg. † et al., 2020), because it is challenging to handle out-of-vocabulary words (OOV), especially in real applications where the test data may come from different sources. Although leveraging extra labeled data from other sources or domains could alleviate this issue, in real applications, such data are always located in different nodes and thus are inaccessible"
2021.findings-acl.376,2020.emnlp-main.487,1,0.85376,"rent baselines including some well-designed FL frameworks.1 1 Introduction Chinese word segmentation (CWS) is a preliminary and vital task for natural language processing (NLP). This task aims to segment Chinese character sequence into words and thus is generally performed as a sequence labeling task (Tseng et al., 2005; Levow, 2006; Song et al., 2009a; Sun and Xu, 2011; Song and Xia, 2012, 2013; Mansur et al., 2013). Although recent neural-based CWS systems (Pei et al., 2014; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019; Ke et al., 2020; Huang et al., 2020a; Tian et al., 2020e) have achieved very good performance on benchmark datasets, it is still an unsolved task (Fu * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/GCASeg. † et al., 2020), because it is challenging to handle out-of-vocabulary words (OOV), especially in real applications where the test data may come from different sources. Although leveraging extra labeled data from other sources or domains could alleviate this issue, in real applications, such data are always located in different nodes and thus are inaccessible"
2021.findings-acl.376,2020.findings-emnlp.153,1,0.752458,"rent baselines including some well-designed FL frameworks.1 1 Introduction Chinese word segmentation (CWS) is a preliminary and vital task for natural language processing (NLP). This task aims to segment Chinese character sequence into words and thus is generally performed as a sequence labeling task (Tseng et al., 2005; Levow, 2006; Song et al., 2009a; Sun and Xu, 2011; Song and Xia, 2012, 2013; Mansur et al., 2013). Although recent neural-based CWS systems (Pei et al., 2014; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019; Ke et al., 2020; Huang et al., 2020a; Tian et al., 2020e) have achieved very good performance on benchmark datasets, it is still an unsolved task (Fu * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/GCASeg. † et al., 2020), because it is challenging to handle out-of-vocabulary words (OOV), especially in real applications where the test data may come from different sources. Although leveraging extra labeled data from other sources or domains could alleviate this issue, in real applications, such data are always located in different nodes and thus are inaccessible"
2021.findings-acl.376,2020.acl-main.734,1,0.845654,"rent baselines including some well-designed FL frameworks.1 1 Introduction Chinese word segmentation (CWS) is a preliminary and vital task for natural language processing (NLP). This task aims to segment Chinese character sequence into words and thus is generally performed as a sequence labeling task (Tseng et al., 2005; Levow, 2006; Song et al., 2009a; Sun and Xu, 2011; Song and Xia, 2012, 2013; Mansur et al., 2013). Although recent neural-based CWS systems (Pei et al., 2014; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019; Ke et al., 2020; Huang et al., 2020a; Tian et al., 2020e) have achieved very good performance on benchmark datasets, it is still an unsolved task (Fu * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/GCASeg. † et al., 2020), because it is challenging to handle out-of-vocabulary words (OOV), especially in real applications where the test data may come from different sources. Although leveraging extra labeled data from other sources or domains could alleviate this issue, in real applications, such data are always located in different nodes and thus are inaccessible"
2021.findings-acl.376,I05-3027,0,0.129538,"to help CWS deal with data isolation, where a mechanism of global character associations is proposed to enhance FL to learn from different data sources. Experimental results on a simulated environment with five nodes confirm the effectiveness of our approach, where our approach outperforms different baselines including some well-designed FL frameworks.1 1 Introduction Chinese word segmentation (CWS) is a preliminary and vital task for natural language processing (NLP). This task aims to segment Chinese character sequence into words and thus is generally performed as a sequence labeling task (Tseng et al., 2005; Levow, 2006; Song et al., 2009a; Sun and Xu, 2011; Song and Xia, 2012, 2013; Mansur et al., 2013). Although recent neural-based CWS systems (Pei et al., 2014; Chen et al., 2017; Ma et al., 2018; Higashiyama et al., 2019; Qiu et al., 2019; Ke et al., 2020; Huang et al., 2020a; Tian et al., 2020e) have achieved very good performance on benchmark datasets, it is still an unsolved task (Fu * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/GCASeg. † et al., 2020), because it is challenging to handle out-of-vocabu"
2021.findings-acl.376,I11-1035,0,0.0221073,"7)3 for CWS. Therefore, data from different genres are distributed to the five nodes without overlapping (i.e., the data sources in 3 https://catalog.ldc.upenn.edu/ LDC2010T07 4308 Hyper-parameters Values Learning Rate Warmup Rate Dropout Rate Batch Size 5e − 6, 1e-5, 3e − 5 0.1, 0.2 0.33 16, 32 Table 3: The hyper-parameters tested in tuning our models. The best ones used in our final experiments are highlighted in boldface. our simulation are heterogeneous), which is similar to the simulation setting of aforementioned previous studies. We split each genre into train/dev/test splits following Wang et al. (2011) and report the statistics (in terms of the number of sentences, word tokens, and OOV rate) in Table 2. 3.2 Baselines and Reference Models To show the effectiveness of our approach with GCA, we compare it with a baseline model that follows the FL framework without using it. In addition, we also run two reference models without both FL and GCA, where all training instances are not isolated and are accessible to each other. Specifically, the first reference model (denoted by Single) is trained and evaluated on the data from a single node (genre). The second (denoted by Union) is trained on the u"
2021.findings-acl.376,2020.findings-emnlp.55,0,0.214679,"e of our approach. The encrypted information (i.e., encrypted data, word segmentation tags, and loss) communicates between a node and the server, where the locally stored data is inaccessible to other nodes during the training process. Figure 2: An overview of GCA-FL, where centralized model is illustrated on the top and the example input sentence “南京市长江大桥” (Nanjing Yangtze River Bridge) from Ni is shown on the bottom. “/” in the output represents the delimiter for the word boundaries. (Hard et al., 2018; Chen et al., 2019), named entity recognition (Ge et al., 2020), and text classification (Zhu et al., 2020)) mainly focus on optimizing the learning process and ignore domain diversities. In this paper, we propose a FL-based neural model (GCA-FL) for CWS, which is enhanced by global character association (GCA) mechanism in a distributed environment. The GCA mechanism is designed to capture contextual information (patterns) in a particular input for localized predictions and to handle the difficulties in identifying text sources caused by data inaccessibility. Specifically, GCA is served as a server-side component to associate global character n-grams with different inputs from each node and respond"
2021.findings-acl.428,N19-1411,0,0.0282659,"odel discrete sequences and learns a parameterized prior by a generative model trained with WGAN. In contrast to our TILGAN whose Transformer-based encoder and decoder are both stochastic, ARAE uses RNN-based encoder and decoder which are both deterministic, as required in their theory, which reduces the model expressiveness and results in much poorer performance than ours as shown in Table 2. iVAE (Fang et al., 2019) proposes a VAE (Kingma and Welling, 2014) with an implicit posterior which is inferior to the implicit prior that we adopt according to the ablation study in Section 6.1. WAE-S (Bahuleyan et al., 2019) is a WAE(Tolstikhin et al., 2018) with a stochastic encoder trained using MMD with a distinct goal of improving the reconstruction ability. 8 Conclusion In this paper, we proposed Transformer-based Implicit Latent GAN (TILGAN), for text generation. It combines a Transformer autoencoder and a GAN through matching the distributions of multi-token sequences in the Transformer’s latent space based on KL divergence. To improve the local and global coherence, we introduced a multi-scale discriminator to utilize the semantic information on varying scales. To train the decoder reliably, we enhanced t"
2021.findings-acl.428,D19-1407,0,0.0321119,"Missing"
2021.findings-acl.428,D17-1230,0,0.0332986,"Missing"
2021.findings-acl.428,P19-1176,0,0.026604,"epresentations encoded by Transformer. Moreover, the decoder is enhanced by an additional KL loss to be consistent with the latent-generator. Experimental results on three benchmark datasets demonstrate the validity and effectiveness of our model, by obtaining significant improvements and a better quality-diversity trade-off in automatic and human evaluation for both unconditional and conditional generation tasks.1 1 Introduction In recent years, Transformer-based autoregressive (AR) models have made a dramatic impact in text generation tasks such as machine translation (Vaswani et al., 2017; Wang et al., 2019) and dialogue systems (Le et al., 2019; Ham et al., 2020), especially with the emergence of large pre-trained language models (Radford et al., 2019; Brown et al., 2020; Wu et al., 2020). However, AR models predict the next token conditioned on the ground truth * Equal Contribution. Our code is available at https://github.com/ shizhediao/TILGAN. 1 during training and on its own previously generated token during inference, which leads to a mismatch between training and generation stages, and this causes low quality of generated texts and bad generalization ability of models on unseen data (Wisem"
2021.findings-acl.428,N16-1098,0,0.023502,"al, based on the analysis of WAE (Tolstikhin et al., 2018). Let PG and Pr be the induced probability measures of pG (x) and pr (x) respectively. We have the Kantorovich’s formulation of the optimal transport (OT) problem with the L1 cost: W1 (Pr , PG ) = 2 inf Γ∈P(x∼Pr ,y∼PG ) Ex,y∼Γ [c(x, y)], We have considered sampling multiple different neighborhoods within a given sequence as well, whose empirical performance was shown to be comparable with our proposed scheme with one local neighborhood, so we only reported the latter since it is simpler to implement. 4847 TASK UG DATASET MSCOCO S TORY (Mostafazadeh et al., 2016). All of the preprocessing steps are the same as Chen et al. (2018) and Wang and Wan (2019). The statistics of the resulting datasets are reported in Table 1. CG WMTN EWS ROCS TORY VOCAB AVG L EN . 27842 10.4 5728 27.8 20000 10.0 T RAIN S# D EV S# T EST S# 120K 10K 278K 10K 390K 50K 50K 4.2 Table 1: The statistics of the datasets. Avg Len. means the average length of sentences. S# refers to number of sentences. UG and CG stand for unconditional generation and conditional generation, respectively. where c(x, y) = kx − yk1 is the cost function and P(x ∼ Pr , y ∼ PG ) is a set of all joint distri"
2021.findings-acl.428,D16-1137,0,0.0526182,"Missing"
2021.findings-acl.428,P02-1040,0,0.110581,"Missing"
2021.findings-acl.441,P18-1013,0,0.01893,"Work Our work focuses on summarizing the F INDINGS of radiology reports to generate the I MPRESSION, which is essentially an abstractive summarization task. For abstractive summarization, there exists a serious problem known as hallucination (Maynez et al., 2020), in which the generated summary contains fictional content. This problem also exists in AIG and would lead to misdiagnosis for the patient. To tackle this problem, in the general domain, many attempts have been made in terms of guiding information to control the generation process and output the high-quality summary (Li et al., 2018; Hsu et al., 2018; Pilault et al., 2020; Huang et al., 2020; Haonan et al., 2020). For the I MPRESSION generation task in the medical domain, there also exist several solutions. Zhang et al. (2018) encodes a section of the radiology report as the background information to guide the decoding process. MacAvaney et al. (2019) employs the entire ontological terms extracted from F INDINGS as the medical terms, and then enhances the summarizer by selecting the salient information. Gharebagh et al. (2020) further splits ontological terms into words and then incorporates these words into summarization by a separate en"
2021.findings-acl.441,2020.coling-main.24,1,0.723405,"uided decoder with their details and the training objective described below. F INDINGS Encoder Given a F INDINGS, denoted by X with N tokens, LSTM or the standard encoder from Transformer is applied to model the sequence and its output is the hidden state hx . The process is formulated as hx = ff e (x1 , ..., xi , ..., xN ) where ff e (·) refers to the F INDINGS encoder. 4981 (1) Graph Encoder For the node V and adjacency matrix A in the graph G constructed from the F INDINGS, we utilize graph neural networks2 (GNN) to encode them, because GNNs are powerful in encoding graph-like information (Chen et al., 2020; Zheng and Kordjamshidi, 2020; Tian et al., 2021a,b). In detail, two encoders are employed to extract features from G, where one is used to construct the background information and the other is used to generate the dynamic guiding information. The process is thus formalized as zb = fgb (V, A) l z = fgl (V, A) (2) (3) where fgb (·) and fgl (·) refer to two graph encoders, with zb and zl the intermeidate states used to generate the static background information and the dynamic guiding information, respectively. Graph Guided Decoder In our model, zb and zl from the graph encoders are integrated"
2021.findings-acl.441,2021.findings-acl.221,1,0.692737,"one single node in each F INDINGS even though they are not presented in the same entity. Besides modifying relation, some other relations are also important for I MPRESSION generation, such as relations between anatomy and observation. For example, in Figure 1, the relation between “pleural” (anatomy) and “effusion” (observation), which can be obtained for the detailed abnormality in the I M PRESSION . To capture these types of relations, we leverage dependency trees which have been widely used to model word-word relations in many studies (Tian et al., 2020a,b; Pouran Ben Veyseh et al., 2020; Chen et al., 2021). Thus, we define three types of edges for our word graph. Note that, each F INDINGS has its corresponding word graph: • Type I: this is the type using the natural order of words in an entity. In detail, we connect words if they are adjacent in the same entity. In Figure X P (yt |X, y<t )=pgen Pvocab (yt )+(1−pgen ) ati (5) 1, the pink dashed lines serve as the type I edge. i:xi=yt For example, “endotracheal tube” is an entity, so t that “endotracheal” is connected to “tube”. where ai is the distribution over source tokens at step t, which is obtained by performing the atten- • Type II: this i"
2021.findings-acl.441,2020.acl-main.457,0,0.0273182,"Missing"
2021.findings-acl.441,2020.tacl-1.5,0,0.0133753,"LOGYABS† 14.63 15.58 53.18 61.60 - 4.42 5.28 39.59 53.00 - 14.06 14.42 52.86 61.58 - 18.11 31.00 43.97 46.50 - 7.47 16.55 29.36 32.61 - 16.87 27.49 42.50 44.98 - 53.57 40.78 51.81 WGS UM (LSTM+GAT) WGS UM (T RANS +GAT) 64.32 61.63 55.48 50.98 63.97 61.73 47.48 48.37 33.03 33.34 45.43 46.68 54.97 56.38 43.64 44.75 53.81 55.32 plement our model based on Zhang et al. (2018)6 and Liu and Lapata (2019)7 . Since the quality of text representation plays an important role in model performance (Mikolov et al., 2013; Song et al., 2017, 2018; Peters et al., 2018; Song and Shi, 2018; Devlin et al., 2019; Joshi et al., 2020; Song et al., 2021), we try two powerful F INDINGS encoders, namely, LSTM and Transformer, which have achieved state-of-the-art results in many natural language processing tasks. For WGS UM (LSTM+GAT)), we employ 2-layer GAT with hidden size of 200 as our graph encoder, 2-layer BiLSTM encoder for findings sequence with hidden size of 100 for each direction and 1-layer LSTM for decoder with hidden size of 200. The dropout is set to 0.5 for embedding layer. We use Adam optimizer (Kingma and Ba, 2014) with the learning rate of 0.001. For WGS UM (T RANS +GAT), the graph encoder is a 2-layer GAT w"
2021.findings-acl.441,N18-2009,0,0.0117673,"ition. 5 Related Work Our work focuses on summarizing the F INDINGS of radiology reports to generate the I MPRESSION, which is essentially an abstractive summarization task. For abstractive summarization, there exists a serious problem known as hallucination (Maynez et al., 2020), in which the generated summary contains fictional content. This problem also exists in AIG and would lead to misdiagnosis for the patient. To tackle this problem, in the general domain, many attempts have been made in terms of guiding information to control the generation process and output the high-quality summary (Li et al., 2018; Hsu et al., 2018; Pilault et al., 2020; Huang et al., 2020; Haonan et al., 2020). For the I MPRESSION generation task in the medical domain, there also exist several solutions. Zhang et al. (2018) encodes a section of the radiology report as the background information to guide the decoding process. MacAvaney et al. (2019) employs the entire ontological terms extracted from F INDINGS as the medical terms, and then enhances the summarizer by selecting the salient information. Gharebagh et al. (2020) further splits ontological terms into words and then incorporates these words into summarizatio"
2021.findings-acl.441,2020.findings-emnlp.407,0,0.0260172,"words are treated as one single node in each F INDINGS even though they are not presented in the same entity. Besides modifying relation, some other relations are also important for I MPRESSION generation, such as relations between anatomy and observation. For example, in Figure 1, the relation between “pleural” (anatomy) and “effusion” (observation), which can be obtained for the detailed abnormality in the I M PRESSION . To capture these types of relations, we leverage dependency trees which have been widely used to model word-word relations in many studies (Tian et al., 2020a,b; Pouran Ben Veyseh et al., 2020; Chen et al., 2021). Thus, we define three types of edges for our word graph. Note that, each F INDINGS has its corresponding word graph: • Type I: this is the type using the natural order of words in an entity. In detail, we connect words if they are adjacent in the same entity. In Figure X P (yt |X, y<t )=pgen Pvocab (yt )+(1−pgen ) ati (5) 1, the pink dashed lines serve as the type I edge. i:xi=yt For example, “endotracheal tube” is an entity, so t that “endotracheal” is connected to “tube”. where ai is the distribution over source tokens at step t, which is obtained by performing the atte"
2021.findings-acl.441,P17-1099,0,0.352696,"uch words in the F INDINGS and describe their corresponding relations for AIG. In this paper, we propose to enhance AIG via a summarization model integrated with a word graph by leveraging salient words and their relations in the F INDINGS. In detail, the word graph is constructed by identifying the important words in the F INDINGS and building connections among them via different typed relations. To exploit the word graph, a Word Graph guided Summarization model (WGS UM) is designed to perform AIG, where the information from the word graph is integrated into the backbone decoder (e.g., LSTM (See et al., 2017) or Transformer (Liu and Lapata, 2019)) from two aspects, including enriching the decoder input as extra knowledge, as well as guiding the decoder to update its hidden states. Experimental results illustrate that WGS UM outperforms all baselines on two benchmark datasets, where the state-of-the-art performance is observed on all datasets in comparison with previous studies. Further analyses also investigate how different types of edges in the graph affects the performance of our proposed model. 2 The Proposed Method We follow the standard sequence-to-sequence paradigm for AIG. In doing so, we"
2021.findings-acl.441,P18-2027,0,0.0150773,"the following models as our baselines: • PG-LSTM (See et al., 2017): This is Pointer Generator Network (PGN) with copy mechanism where both encoder and decoder are vanilla LSTMs without graph information. • PG-T RANS (Liu and Lapata, 2019): This is also a PGN with both the encoder and decoder replaced with the transformer. Besides, we also compare our model with those in previous studies, including extractive summarization models, e.g., L EX R ANK (Erkan and Radev, 2004), T RANSFORMER EXT (Liu and Lapata, 2019), as well as abstractive summarization models, e.g., CAVC (Song et al., 2020), CGU (Lin et al., 2018) and O NTOLOGYABS (Gharebagh et al., 2020). In our experiments, we use ROUGE metrics (Lin, 2004) to evaluate the generated I MPRES SION s. We only report F 1 scores of ROUGE-1 (R1), ROUGE-2 (R-2) and ROUGE-L (R-L), where R-1, R-2 are unigram and bigram overlap measuring the informativeness and R-L is the longest common sub-sequence overlap aiming to assess fluency. In addition, to evaluate the factual consistency (FC), CheXbert (Smit et al., 2020)4 is utilized to detect 14 observations related to diseases in reference impressions and generated impressions. Then precision, recall and F1 score a"
2021.findings-acl.441,D19-1387,0,0.181569,"cribe their corresponding relations for AIG. In this paper, we propose to enhance AIG via a summarization model integrated with a word graph by leveraging salient words and their relations in the F INDINGS. In detail, the word graph is constructed by identifying the important words in the F INDINGS and building connections among them via different typed relations. To exploit the word graph, a Word Graph guided Summarization model (WGS UM) is designed to perform AIG, where the information from the word graph is integrated into the backbone decoder (e.g., LSTM (See et al., 2017) or Transformer (Liu and Lapata, 2019)) from two aspects, including enriching the decoder input as extra knowledge, as well as guiding the decoder to update its hidden states. Experimental results illustrate that WGS UM outperforms all baselines on two benchmark datasets, where the state-of-the-art performance is observed on all datasets in comparison with previous studies. Further analyses also investigate how different types of edges in the graph affects the performance of our proposed model. 2 The Proposed Method We follow the standard sequence-to-sequence paradigm for AIG. In doing so, we regard the F INDINGS as the source seq"
2021.findings-acl.441,2020.acl-main.173,0,0.0230449,"gists, our method covers almost all of the key information in the generated I MPRESSIONs. For example, the key information “moderate bilateral pleural effusions”,”mild pulmonary edema” and “small opacity in the right media ” in the three examples are not generated in PG-T RANS model, but they are necessary for describing the clinical condition. 5 Related Work Our work focuses on summarizing the F INDINGS of radiology reports to generate the I MPRESSION, which is essentially an abstractive summarization task. For abstractive summarization, there exists a serious problem known as hallucination (Maynez et al., 2020), in which the generated summary contains fictional content. This problem also exists in AIG and would lead to misdiagnosis for the patient. To tackle this problem, in the general domain, many attempts have been made in terms of guiding information to control the generation process and output the high-quality summary (Li et al., 2018; Hsu et al., 2018; Pilault et al., 2020; Huang et al., 2020; Haonan et al., 2020). For the I MPRESSION generation task in the medical domain, there also exist several solutions. Zhang et al. (2018) encodes a section of the radiology report as the background inform"
2021.findings-acl.441,2020.lrec-1.497,0,0.0353925,"Missing"
2021.findings-acl.441,N18-1202,0,0.00816503,"SPLIT R-1 R-2 R-L L EX R ANK T RANSFORMER E XT CAVC CGU O NTOLOGYABS† 14.63 15.58 53.18 61.60 - 4.42 5.28 39.59 53.00 - 14.06 14.42 52.86 61.58 - 18.11 31.00 43.97 46.50 - 7.47 16.55 29.36 32.61 - 16.87 27.49 42.50 44.98 - 53.57 40.78 51.81 WGS UM (LSTM+GAT) WGS UM (T RANS +GAT) 64.32 61.63 55.48 50.98 63.97 61.73 47.48 48.37 33.03 33.34 45.43 46.68 54.97 56.38 43.64 44.75 53.81 55.32 plement our model based on Zhang et al. (2018)6 and Liu and Lapata (2019)7 . Since the quality of text representation plays an important role in model performance (Mikolov et al., 2013; Song et al., 2017, 2018; Peters et al., 2018; Song and Shi, 2018; Devlin et al., 2019; Joshi et al., 2020; Song et al., 2021), we try two powerful F INDINGS encoders, namely, LSTM and Transformer, which have achieved state-of-the-art results in many natural language processing tasks. For WGS UM (LSTM+GAT)), we employ 2-layer GAT with hidden size of 200 as our graph encoder, 2-layer BiLSTM encoder for findings sequence with hidden size of 100 for each direction and 1-layer LSTM for decoder with hidden size of 200. The dropout is set to 0.5 for embedding layer. We use Adam optimizer (Kingma and Ba, 2014) with the learning rate of 0.001. F"
2021.findings-acl.441,2020.emnlp-main.748,0,0.0146847,"ses on summarizing the F INDINGS of radiology reports to generate the I MPRESSION, which is essentially an abstractive summarization task. For abstractive summarization, there exists a serious problem known as hallucination (Maynez et al., 2020), in which the generated summary contains fictional content. This problem also exists in AIG and would lead to misdiagnosis for the patient. To tackle this problem, in the general domain, many attempts have been made in terms of guiding information to control the generation process and output the high-quality summary (Li et al., 2018; Hsu et al., 2018; Pilault et al., 2020; Huang et al., 2020; Haonan et al., 2020). For the I MPRESSION generation task in the medical domain, there also exist several solutions. Zhang et al. (2018) encodes a section of the radiology report as the background information to guide the decoding process. MacAvaney et al. (2019) employs the entire ontological terms extracted from F INDINGS as the medical terms, and then enhances the summarizer by selecting the salient information. Gharebagh et al. (2020) further splits ontological terms into words and then incorporates these words into summarization by a separate encoder. Compared to the"
2021.findings-acl.441,2020.emnlp-main.117,0,0.0321807,"Missing"
2021.findings-acl.441,K17-1016,1,0.75002,"SPLIT R-1 R-2 R-L R ANDOM SPLIT R-1 R-2 R-L L EX R ANK T RANSFORMER E XT CAVC CGU O NTOLOGYABS† 14.63 15.58 53.18 61.60 - 4.42 5.28 39.59 53.00 - 14.06 14.42 52.86 61.58 - 18.11 31.00 43.97 46.50 - 7.47 16.55 29.36 32.61 - 16.87 27.49 42.50 44.98 - 53.57 40.78 51.81 WGS UM (LSTM+GAT) WGS UM (T RANS +GAT) 64.32 61.63 55.48 50.98 63.97 61.73 47.48 48.37 33.03 33.34 45.43 46.68 54.97 56.38 43.64 44.75 53.81 55.32 plement our model based on Zhang et al. (2018)6 and Liu and Lapata (2019)7 . Since the quality of text representation plays an important role in model performance (Mikolov et al., 2013; Song et al., 2017, 2018; Peters et al., 2018; Song and Shi, 2018; Devlin et al., 2019; Joshi et al., 2020; Song et al., 2021), we try two powerful F INDINGS encoders, namely, LSTM and Transformer, which have achieved state-of-the-art results in many natural language processing tasks. For WGS UM (LSTM+GAT)), we employ 2-layer GAT with hidden size of 200 as our graph encoder, 2-layer BiLSTM encoder for findings sequence with hidden size of 100 for each direction and 1-layer LSTM for decoder with hidden size of 200. The dropout is set to 0.5 for embedding layer. We use Adam optimizer (Kingma and Ba, 2014) with th"
2021.findings-acl.441,N18-2028,1,0.895899,"Missing"
2021.findings-acl.441,2021.naacl-main.231,1,0.715511,"objective described below. F INDINGS Encoder Given a F INDINGS, denoted by X with N tokens, LSTM or the standard encoder from Transformer is applied to model the sequence and its output is the hidden state hx . The process is formulated as hx = ff e (x1 , ..., xi , ..., xN ) where ff e (·) refers to the F INDINGS encoder. 4981 (1) Graph Encoder For the node V and adjacency matrix A in the graph G constructed from the F INDINGS, we utilize graph neural networks2 (GNN) to encode them, because GNNs are powerful in encoding graph-like information (Chen et al., 2020; Zheng and Kordjamshidi, 2020; Tian et al., 2021a,b). In detail, two encoders are employed to extract features from G, where one is used to construct the background information and the other is used to generate the dynamic guiding information. The process is thus formalized as zb = fgb (V, A) l z = fgl (V, A) (2) (3) where fgb (·) and fgl (·) refer to two graph encoders, with zb and zl the intermeidate states used to generate the static background information and the dynamic guiding information, respectively. Graph Guided Decoder In our model, zb and zl from the graph encoders are integrated into the backbone decoder (e.g., LSTM and Transfo"
2021.findings-acl.441,2021.acl-long.344,1,0.732288,"objective described below. F INDINGS Encoder Given a F INDINGS, denoted by X with N tokens, LSTM or the standard encoder from Transformer is applied to model the sequence and its output is the hidden state hx . The process is formulated as hx = ff e (x1 , ..., xi , ..., xN ) where ff e (·) refers to the F INDINGS encoder. 4981 (1) Graph Encoder For the node V and adjacency matrix A in the graph G constructed from the F INDINGS, we utilize graph neural networks2 (GNN) to encode them, because GNNs are powerful in encoding graph-like information (Chen et al., 2020; Zheng and Kordjamshidi, 2020; Tian et al., 2021a,b). In detail, two encoders are employed to extract features from G, where one is used to construct the background information and the other is used to generate the dynamic guiding information. The process is thus formalized as zb = fgb (V, A) l z = fgl (V, A) (2) (3) where fgb (·) and fgl (·) refer to two graph encoders, with zb and zl the intermeidate states used to generate the static background information and the dynamic guiding information, respectively. Graph Guided Decoder In our model, zb and zl from the graph encoders are integrated into the backbone decoder (e.g., LSTM and Transfo"
2021.findings-acl.441,2020.findings-emnlp.378,1,0.782674,"To avoid confusion, the repeated words are treated as one single node in each F INDINGS even though they are not presented in the same entity. Besides modifying relation, some other relations are also important for I MPRESSION generation, such as relations between anatomy and observation. For example, in Figure 1, the relation between “pleural” (anatomy) and “effusion” (observation), which can be obtained for the detailed abnormality in the I M PRESSION . To capture these types of relations, we leverage dependency trees which have been widely used to model word-word relations in many studies (Tian et al., 2020a,b; Pouran Ben Veyseh et al., 2020; Chen et al., 2021). Thus, we define three types of edges for our word graph. Note that, each F INDINGS has its corresponding word graph: • Type I: this is the type using the natural order of words in an entity. In detail, we connect words if they are adjacent in the same entity. In Figure X P (yt |X, y<t )=pgen Pvocab (yt )+(1−pgen ) ati (5) 1, the pink dashed lines serve as the type I edge. i:xi=yt For example, “endotracheal tube” is an entity, so t that “endotracheal” is connected to “tube”. where ai is the distribution over source tokens at step t, which"
2021.findings-acl.441,2020.emnlp-main.487,1,0.734941,"To avoid confusion, the repeated words are treated as one single node in each F INDINGS even though they are not presented in the same entity. Besides modifying relation, some other relations are also important for I MPRESSION generation, such as relations between anatomy and observation. For example, in Figure 1, the relation between “pleural” (anatomy) and “effusion” (observation), which can be obtained for the detailed abnormality in the I M PRESSION . To capture these types of relations, we leverage dependency trees which have been widely used to model word-word relations in many studies (Tian et al., 2020a,b; Pouran Ben Veyseh et al., 2020; Chen et al., 2021). Thus, we define three types of edges for our word graph. Note that, each F INDINGS has its corresponding word graph: • Type I: this is the type using the natural order of words in an entity. In detail, we connect words if they are adjacent in the same entity. In Figure X P (yt |X, y<t )=pgen Pvocab (yt )+(1−pgen ) ati (5) 1, the pink dashed lines serve as the type I edge. i:xi=yt For example, “endotracheal tube” is an entity, so t that “endotracheal” is connected to “tube”. where ai is the distribution over source tokens at step t, which"
2021.findings-acl.441,W18-5623,0,0.301544,"ical observations and an impression section (I MPRESSION) summarizing the most critical observations. In practice, the I MPRESSION is an essential part and 1 Our code and the best performing models are released at https://github.com/cuhksz-nlp/WGSum. plays an important role in delivering critical findings to clinicians. Therefore, summarizing F IND INGS helps to locate the most prominent observations so that the automatic process of doing so greatly eases the workload of radiologists. Recently, many methods are proposed for automatic impression generation (AIG) (Hassanpour and Langlotz, 2016; Zhang et al., 2018; Gharebagh et al., 2020), which are mainly based on the sequence-tosequence architecture with specific designs for the characteristics of this task. For example, MacAvaney et al. (2019) employed clinical terms within the F INDINGS as key information to enhance AIG. Based on this work, Gharebagh et al. (2020) further proposed to identify the importance of these clinical terms and selected the most salient ones to facilitate the recognition of significant content 4980 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4980–4990 August 1–6, 2021. ©2021 Association"
2021.findings-acl.441,2020.emnlp-main.714,0,0.0315401,"their details and the training objective described below. F INDINGS Encoder Given a F INDINGS, denoted by X with N tokens, LSTM or the standard encoder from Transformer is applied to model the sequence and its output is the hidden state hx . The process is formulated as hx = ff e (x1 , ..., xi , ..., xN ) where ff e (·) refers to the F INDINGS encoder. 4981 (1) Graph Encoder For the node V and adjacency matrix A in the graph G constructed from the F INDINGS, we utilize graph neural networks2 (GNN) to encode them, because GNNs are powerful in encoding graph-like information (Chen et al., 2020; Zheng and Kordjamshidi, 2020; Tian et al., 2021a,b). In detail, two encoders are employed to extract features from G, where one is used to construct the background information and the other is used to generate the dynamic guiding information. The process is thus formalized as zb = fgb (V, A) l z = fgl (V, A) (2) (3) where fgb (·) and fgl (·) refer to two graph encoders, with zb and zl the intermeidate states used to generate the static background information and the dynamic guiding information, respectively. Graph Guided Decoder In our model, zb and zl from the graph encoders are integrated into the backbone decoder (e.g"
2021.findings-acl.99,D19-1189,0,0.376618,"recommendation (CR) systems have attracted widespread attention for being a tool providing users potential items of interest through dialogue-based interactions. Though existing studies (Sun and Zhang, 2018; Zhang et al., 2018; Lei et al., 2020) proposed to integrate recommender and dialogue components for providing ( ) Corresponding Author Our code will release in https://github.com/ JD-AI-Research-NLP/RevCore. 1 user-specific suggestions through conversations, CR remains challengeable because (i) typical dialogues are short and lack sufficient item information for user preference capturing (Chen et al., 2019; Zhou et al., 2020), and (ii) difficulties exist in generating informative responses with item-related descriptions (Shao et al., 2017; Ghazvininejad et al., 2018; Wang et al., 2019b). Thus, recently, external information in the form of structured knowledge graphs (KG) is introduced to enhance item representations by using rich entity information in KG (Chen et al., 2019; Zhou et al., 2020). While KGbased methods improve CR to some extent, they are still limited in (i) worse versatility resulted from a high cost of KG construction; and (ii) inadequate integration of knowledge and response gen"
2021.findings-acl.99,P16-1014,0,0.036373,"he decoding stage takes them and the entity embedding E(C) as inputs of attention layers. These attention layers aim to fuse the external information from KG and reviews R into the context information, inspired by the work of Zhou et al. (2020). Given the decoding output of last time unit Pr3 (yi |Yi , R), (11) where Pr1 (·) is a generation probability function over the vocabulary, with Yi as the input. G and R represents the knowledge graph and reviews we use. Pr2 (·), Pr3 (·) are copy probability functions from KG entities and reviews, respectively, implemented by a standard copy mechanism (Gulcehre et al., 2016) (computing the distributions over the KG 1164 words or review words). Both probability functions are implemented with a softmax operation. To learn the response generation in the dialogue component, we set a cross-entropy loss: Lgen = − N  1 X log Pr(st |s1 , · · ·, st−1 ) , N (12) t=1 where N is the number of turns, st represents the tth utterance in the conversation. To train the whole model, it includes three steps: (i) pre-training the sentiment predictor in the review retrieval module; (ii) training the recommender component by minimizing Lrec ; (iii) training the dialogue component by"
2021.findings-acl.99,2020.tacl-1.5,0,0.0207558,"Missing"
2021.findings-acl.99,D19-1203,0,0.0270635,"g systems (Dodge et al., 2016; Yan et al., 2016; Benni et al., 2016; Bordes et al., 2017; Song et al., 2020) and structured knowledge-based info-seeking technics including question answering (Bao et al., 2014, 1168 2016; Yin et al., 2015; Yih et al., 2015; Shao et al., 2019) and question generation (Serban et al., 2016; Bao et al., 2018; Duˇsek et al., 2020) have encouraged the development of conversational recommendation systems, which dynamically obtain user preferences through interactive conversation with users. Multiple datasets have been constructed (Dodge et al., 2016; Li et al., 2018; Kang et al., 2019) to facilitate the study of this task. Li et al. (2018) collect a standard human-to-human multi-turn dialog dataset focusing on providing movie recommendations. Based on these datasets, various approaches are proposed to address different issues in CR systems. Specifically, external information is introduced to alleviate the coldstart problem, including knowledge bases (Wang et al., 2018), social networks (Daramola et al.), and knowledge graphs (Chen et al., 2019). Christakopoulou et al. (2016) use bandit-based exploreexploit strategy to minimize the number of user queries. Liu et al. (2020) c"
2021.findings-acl.99,N16-1014,0,0.0536031,"Dist-3 Dist-4 PPL Redial 2.4 3.1 3.9 4.2 6.1 14.0 15.0 18.3 22.7 23.6 32.0 33.6 37.8 43.3 45.4 Trans 0.148 0.225 0.263 0.289 0.373 0.424 0.151 0.236 0.368 0.434 0.527 0.558 0.137 0.228 0.423 0.519 0.615 0.612 17.0 28.1 17.9 9.8 10.7 10.2 KBRD KGSF RevCore (−KG) RevCore (+KG) Redial KBRD KGSF RevCore (−KG) Table 1: Results on the recommendation task. Best results are in bold. provided by human recommenders. Conversation evaluation comprises automatic and human evaluation. The metrics for automatic evaluation are perplexity (PPL) (Jelinek et al., 1977) and distinct n-gram (Dist-n, n = 2, 3, 4) (Li et al., 2016). Perplexity is a measurement for the fluency of natural language, where lower perplexity refers to higher fluency. Distinct n-gram is a measurement for the diversity of generated utterances. Specifically, we use distinct 3-gram and 4-gram at the sentence level to evaluate the diversity. The main purpose of our dialog component is a successful recommendation rather than imitating the ground truth responses. Therefore, we provide annotators to manually evaluate the results instead of using BLEU scores. The annotators evaluate the quality of generated dialogue responses from 3 aspects, i.e., coh"
2021.findings-acl.99,2020.acl-main.6,0,0.0429589,"al., 2020), and (ii) difficulties exist in generating informative responses with item-related descriptions (Shao et al., 2017; Ghazvininejad et al., 2018; Wang et al., 2019b). Thus, recently, external information in the form of structured knowledge graphs (KG) is introduced to enhance item representations by using rich entity information in KG (Chen et al., 2019; Zhou et al., 2020). While KGbased methods improve CR to some extent, they are still limited in (i) worse versatility resulted from a high cost of KG construction; and (ii) inadequate integration of knowledge and response generation (Lin et al., 2020). Given that, nowadays, users are greatly encouraged to share their consumption experience (e.g., restaurant, traveling, movie, etc.), reviews are easily accessed over the internet. Such reviews often provide rich and detailed user comments on different factors of interest, which are crucial in suggest1161 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1161–1173 August 1–6, 2021. ©2021 Association for Computational Linguistics Context KG entity emb Will Ferrell … S2: Yup I really liked it. Paul Feig Old School Bad Boys lookup table MLP classifier SA Wall-E …"
2021.findings-acl.99,2020.acl-main.98,0,0.157773,"Kang et al., 2019) to facilitate the study of this task. Li et al. (2018) collect a standard human-to-human multi-turn dialog dataset focusing on providing movie recommendations. Based on these datasets, various approaches are proposed to address different issues in CR systems. Specifically, external information is introduced to alleviate the coldstart problem, including knowledge bases (Wang et al., 2018), social networks (Daramola et al.), and knowledge graphs (Chen et al., 2019). Christakopoulou et al. (2016) use bandit-based exploreexploit strategy to minimize the number of user queries. Liu et al. (2020) conduct multi-goal planning to make a proactive conversational recommendation over multi-type dialogues. A multi-view method is proposed in Chen et al. (2020b) for the explainable conversational recommendation. The work of Pecune et al. (2020) builds a socially aware CR system engaging its users through a rapportbuilding dialogue to improve users’ perception. Different from all aforementioned previous work, we offer an alternative to AIG with an augmented conversational recommendation system by incorporating reviews that highly relevant to items. Particularly, our model is able to learn bette"
2021.findings-acl.99,P11-1015,0,0.04957,"Zhou et al. (2020) for fair comparison4 . Besides, the “review sentence” is selected according to the sentiment value and in a sentence-wise manner, and the token number of incorporated review sentences is set to 20, considering the balance between the original source and external source. We add the retrieved review sentences after the mentioned items in the dialogue component training to guide it to generate review-aware responses. The sentiment predictor for reviews is trained on the collected reviews. The sentiment predictor for dialog context is trained on the IMDb Movie Reviews Dataset (Maas et al., 2011) and then finetuned on the REDIAL dataset. 3.3 3 Experiment Settings 3.1 Dataset REDIAL (Li et al., 2018) is a widely-used dataset of real-world conversations around the theme of providing movie recommendations generated by the human in seeker-recommender pairs. REDIAL contains 10,021 conversations related to 64,362 movies, split into training, validation, and test sets using a ratio of 8:1:12 . To construct a review database, we crawled 30 reviews for each movie from IMDb3 website, which is one of the most popular and authoritative movie databases. Each review can be queried according to the"
2021.findings-acl.99,N18-1202,0,0.0196138,"a more precise recommendation. Review-augmented Response Generation Reviews can also augment the response generation in the conversation component. We build an encoder-decoder framework to handle the generation task. Retrieved reviews and context are encoded separately first, for the purpose of maintaining the dialog consistency. In the decoding stage, the review embedding is fused via an attention layer to generate informative responses. Considering that a good modeling of the input plays an important role to achieve an outstanding model performance (Mikolov et al., 2013; Song and Shi, 2018; Peters et al., 2018; Devlin et al., 2019; Song et al., 2021) and transformer-based approaches have achieved state-of-the-art in many NLP tasks (Vaswani et al., 2017; Chen et al., 2019; Zhou et al., 2020; Chen et al., 2020a; Joshi et al., 2020; Wang et al., 2020; Tian et al., 2020), we adopt two transformers as the encoders for context and reviews. Given a context C and the retrieved reviews R, the context embedding X(C) and review embedding R(C) are first obtained: X(C) = TransformerθX (C), R(C) = TransformerθR (R), Ai0 = MHA(Yi−1 , Yi−1 , Yi−1 ), Ai1 = MHA(Ai0 , X(C) , X(C) ), Ai2 = MHA(Ai1 , E(C) , E(C) ), (8)"
2021.findings-acl.99,D17-1235,0,0.0286127,"alogue-based interactions. Though existing studies (Sun and Zhang, 2018; Zhang et al., 2018; Lei et al., 2020) proposed to integrate recommender and dialogue components for providing ( ) Corresponding Author Our code will release in https://github.com/ JD-AI-Research-NLP/RevCore. 1 user-specific suggestions through conversations, CR remains challengeable because (i) typical dialogues are short and lack sufficient item information for user preference capturing (Chen et al., 2019; Zhou et al., 2020), and (ii) difficulties exist in generating informative responses with item-related descriptions (Shao et al., 2017; Ghazvininejad et al., 2018; Wang et al., 2019b). Thus, recently, external information in the form of structured knowledge graphs (KG) is introduced to enhance item representations by using rich entity information in KG (Chen et al., 2019; Zhou et al., 2020). While KGbased methods improve CR to some extent, they are still limited in (i) worse versatility resulted from a high cost of KG construction; and (ii) inadequate integration of knowledge and response generation (Lin et al., 2020). Given that, nowadays, users are greatly encouraged to share their consumption experience (e.g., restaurant,"
2021.findings-acl.99,2020.coling-main.63,1,0.657101,"users may have similar interests. Afterward, more sophisticated methods using neural networks are proposed and prove effective. For instance, neural factorization machines (He and Chua, 2017) and deep interest networks (Zhou et al., 2018) are used to estimate user preferences based on historical user-item interactions. Graphs are adopted in Wang et al. (2019b,a) to model complex relations among users, items, and attributes for a better representation of data. In recent years, major advances made in dialog systems (Dodge et al., 2016; Yan et al., 2016; Benni et al., 2016; Bordes et al., 2017; Song et al., 2020) and structured knowledge-based info-seeking technics including question answering (Bao et al., 2014, 1168 2016; Yin et al., 2015; Yih et al., 2015; Shao et al., 2019) and question generation (Serban et al., 2016; Bao et al., 2018; Duˇsek et al., 2020) have encouraged the development of conversational recommendation systems, which dynamically obtain user preferences through interactive conversation with users. Multiple datasets have been constructed (Dodge et al., 2016; Li et al., 2018; Kang et al., 2019) to facilitate the study of this task. Li et al. (2018) collect a standard human-to-human"
2021.findings-acl.99,2020.emnlp-main.487,1,0.657614,"rately first, for the purpose of maintaining the dialog consistency. In the decoding stage, the review embedding is fused via an attention layer to generate informative responses. Considering that a good modeling of the input plays an important role to achieve an outstanding model performance (Mikolov et al., 2013; Song and Shi, 2018; Peters et al., 2018; Devlin et al., 2019; Song et al., 2021) and transformer-based approaches have achieved state-of-the-art in many NLP tasks (Vaswani et al., 2017; Chen et al., 2019; Zhou et al., 2020; Chen et al., 2020a; Joshi et al., 2020; Wang et al., 2020; Tian et al., 2020), we adopt two transformers as the encoders for context and reviews. Given a context C and the retrieved reviews R, the context embedding X(C) and review embedding R(C) are first obtained: X(C) = TransformerθX (C), R(C) = TransformerθR (R), Ai0 = MHA(Yi−1 , Yi−1 , Yi−1 ), Ai1 = MHA(Ai0 , X(C) , X(C) ), Ai2 = MHA(Ai1 , E(C) , E(C) ), (8) Ai3 = MHA(Ai2 , R(C) , R(C) ), Yi = FFN(Ai3 ), E (C) = {EC , ER }, 2.3 Yi−1 , the current one Yi is generated by: where MHA(Q, K, V) represents the multi-head attention function (Vaswani et al., 2017), which takes a query, key, and value as input: MHA(Q, K, V)"
2021.findings-acl.99,2020.coling-main.510,1,0.729302,"xt are encoded separately first, for the purpose of maintaining the dialog consistency. In the decoding stage, the review embedding is fused via an attention layer to generate informative responses. Considering that a good modeling of the input plays an important role to achieve an outstanding model performance (Mikolov et al., 2013; Song and Shi, 2018; Peters et al., 2018; Devlin et al., 2019; Song et al., 2021) and transformer-based approaches have achieved state-of-the-art in many NLP tasks (Vaswani et al., 2017; Chen et al., 2019; Zhou et al., 2020; Chen et al., 2020a; Joshi et al., 2020; Wang et al., 2020; Tian et al., 2020), we adopt two transformers as the encoders for context and reviews. Given a context C and the retrieved reviews R, the context embedding X(C) and review embedding R(C) are first obtained: X(C) = TransformerθX (C), R(C) = TransformerθR (R), Ai0 = MHA(Yi−1 , Yi−1 , Yi−1 ), Ai1 = MHA(Ai0 , X(C) , X(C) ), Ai2 = MHA(Ai1 , E(C) , E(C) ), (8) Ai3 = MHA(Ai2 , R(C) , R(C) ), Yi = FFN(Ai3 ), E (C) = {EC , ER }, 2.3 Yi−1 , the current one Yi is generated by: where MHA(Q, K, V) represents the multi-head attention function (Vaswani et al., 2017), which takes a query, key, and value as"
2021.findings-acl.99,P16-1049,1,0.826327,"lsus, 2007; Wang et al., 2019b), which assumes that similar users may have similar interests. Afterward, more sophisticated methods using neural networks are proposed and prove effective. For instance, neural factorization machines (He and Chua, 2017) and deep interest networks (Zhou et al., 2018) are used to estimate user preferences based on historical user-item interactions. Graphs are adopted in Wang et al. (2019b,a) to model complex relations among users, items, and attributes for a better representation of data. In recent years, major advances made in dialog systems (Dodge et al., 2016; Yan et al., 2016; Benni et al., 2016; Bordes et al., 2017; Song et al., 2020) and structured knowledge-based info-seeking technics including question answering (Bao et al., 2014, 1168 2016; Yin et al., 2015; Yih et al., 2015; Shao et al., 2019) and question generation (Serban et al., 2016; Bao et al., 2018; Duˇsek et al., 2020) have encouraged the development of conversational recommendation systems, which dynamically obtain user preferences through interactive conversation with users. Multiple datasets have been constructed (Dodge et al., 2016; Li et al., 2018; Kang et al., 2019) to facilitate the study of t"
2021.findings-acl.99,P15-1128,1,0.674076,"factorization machines (He and Chua, 2017) and deep interest networks (Zhou et al., 2018) are used to estimate user preferences based on historical user-item interactions. Graphs are adopted in Wang et al. (2019b,a) to model complex relations among users, items, and attributes for a better representation of data. In recent years, major advances made in dialog systems (Dodge et al., 2016; Yan et al., 2016; Benni et al., 2016; Bordes et al., 2017; Song et al., 2020) and structured knowledge-based info-seeking technics including question answering (Bao et al., 2014, 1168 2016; Yin et al., 2015; Yih et al., 2015; Shao et al., 2019) and question generation (Serban et al., 2016; Bao et al., 2018; Duˇsek et al., 2020) have encouraged the development of conversational recommendation systems, which dynamically obtain user preferences through interactive conversation with users. Multiple datasets have been constructed (Dodge et al., 2016; Li et al., 2018; Kang et al., 2019) to facilitate the study of this task. Li et al. (2018) collect a standard human-to-human multi-turn dialog dataset focusing on providing movie recommendations. Based on these datasets, various approaches are proposed to address differen"
2021.naacl-main.231,P14-2009,0,0.0948588,"Missing"
2021.naacl-main.231,S14-2004,0,0.0482068,"that described in §2.2. For decoding, after we obtain o from ALE, we firstly map o to the label space by a fully connected layer, u = W · o + b, where W and b are the trainable matrix and the bias, respectively, and each dimension of u corresponds to a sentiment type. Thus, we employ a softmax function to u and predict the output sentiment yˆ for the aspect A in X by: exp(ut ) yˆ = arg max P|T | t t=1 exp(u ) (11) where ut is the value at dimension t in u. 3 3.1 Experimental Settings Datasets In the experiments, we employ five widely used English benchmark datasets: L AP 14 and R EST 14 from Pontiki et al. (2014), R EST 15 from Pontiki et al. (2015), R EST 16 from Pontiki et al. (2016), and T WITTER from Dong et al. (2014), with their official train/test splits. In addition, we try another recently released English dataset, named MAMS2 (Jiang et al., 2019), with the official train/dev/test splits for ABSA, which is much larger than the aforementioned five datasets. It is worth noting that, in addition to the positive, neutral, and negative sentiment labels, L AP 14, R EST 14, and R EST 16 2 We use the ATSA part of MAMS obtained from https: //github.com/siat-nlp/MAMS-for-ABSA. 2913 L AP 14 Acc F1 R EST"
2021.naacl-main.231,C12-2116,1,0.78501,"icting y 2 T given X and A through T-GCN and ALE. In the following texts, we firstly describe the construction of the graph with dependency types, then elaborate the details of our T-GCN model, and the ALE to incorporate contextual information from different T-GCN layers, and finally illustrate incorporating T-GCN to ABSA. 2.1 Type-aware Graph Construction Contextual features such as n-grams and syntactic information have been demonstrated to be useful to enhance text representation and thus improve model performance for many NLP tasks (Sun and Xu, 2011; Song and Xia, 2012; Gong et al., 2012; Song et al., 2012; Xu et al., 2015; Chen et al., 2017; Zhang et al., 2019b; Tang et al., 2020). In addition, it is demonstrated by many recent studies that GCN models are effective in capturing contextual features that are represented in graph-like signals, i.e., dependencies among words, of an input sentence (Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019a; Tian et al., 2020c; Chen et al., 2020). In the graph for conventional GCN models, each edge between any two words xi and xj in the input sentence is added to the graph if there is a 2911 (l) Figure 4: The illustration of how we compute hi for"
2021.naacl-main.231,K17-1016,1,0.703339,"Missing"
2021.naacl-main.231,N18-2028,1,0.878748,"Missing"
2021.naacl-main.231,song-xia-2012-using,1,0.879693,"and p computes the probability of predicting y 2 T given X and A through T-GCN and ALE. In the following texts, we firstly describe the construction of the graph with dependency types, then elaborate the details of our T-GCN model, and the ALE to incorporate contextual information from different T-GCN layers, and finally illustrate incorporating T-GCN to ABSA. 2.1 Type-aware Graph Construction Contextual features such as n-grams and syntactic information have been demonstrated to be useful to enhance text representation and thus improve model performance for many NLP tasks (Sun and Xu, 2011; Song and Xia, 2012; Gong et al., 2012; Song et al., 2012; Xu et al., 2015; Chen et al., 2017; Zhang et al., 2019b; Tang et al., 2020). In addition, it is demonstrated by many recent studies that GCN models are effective in capturing contextual features that are represented in graph-like signals, i.e., dependencies among words, of an input sentence (Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019a; Tian et al., 2020c; Chen et al., 2020). In the graph for conventional GCN models, each edge between any two words xi and xj in the input sentence is added to the graph if there is a 2911 (l) Figure 4: The"
2021.naacl-main.231,D19-1569,0,0.329195,"ing author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/ASA-TGCN. † ABSA system may predict wrong if it fails to capture the important contextual information for each aspects. Therefore, to model such contextual information, neural models (e.g., Bi-LSTM and Transformer (Vaswani et al., 2017)) have been widely used for ABSA and demonstrated to be useful for this task (Wang et al., 2016; Tang et al., 2016a; Chen et al., 2017; Ma et al., 2017; Fan et al., 2018). As a further enhancement of encoding contextual information for ABSA, there are studies (Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019a) using graph convolutional networks (GCN) to learn from a graph that is often built over the dependency parsing results of the input texts. As a result, the GCN models are able to learn from distant wordword relations that are more helpful to ABSA. However, GCN models used in these studies are limited by omitting the information carried in dependency types and treating all word-word relations in the graph equally, therefore unimportant relations may not be distinguished and mislead ABSA accordingly. For example, Figure 1 illustrates an example sent"
2021.naacl-main.231,D11-1090,0,0.244447,"ral, and negative) and p computes the probability of predicting y 2 T given X and A through T-GCN and ALE. In the following texts, we firstly describe the construction of the graph with dependency types, then elaborate the details of our T-GCN model, and the ALE to incorporate contextual information from different T-GCN layers, and finally illustrate incorporating T-GCN to ABSA. 2.1 Type-aware Graph Construction Contextual features such as n-grams and syntactic information have been demonstrated to be useful to enhance text representation and thus improve model performance for many NLP tasks (Sun and Xu, 2011; Song and Xia, 2012; Gong et al., 2012; Song et al., 2012; Xu et al., 2015; Chen et al., 2017; Zhang et al., 2019b; Tang et al., 2020). In addition, it is demonstrated by many recent studies that GCN models are effective in capturing contextual features that are represented in graph-like signals, i.e., dependencies among words, of an input sentence (Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019a; Tian et al., 2020c; Chen et al., 2020). In the graph for conventional GCN models, each edge between any two words xi and xj in the input sentence is added to the graph if there is a 29"
2021.naacl-main.231,D19-1342,0,0.0110713,"uate the proposed model, where the results illustrate its effectiveness and state-of-the-art performance is observed over previous studies on all datasets. We also perform further analysis to inThe Approach Given an input sentence X = x1 , x2 , · · · , xn and the aspect terms A ⇢ X (A is usually a sub-string of X ), the conventional ABSA approaches often take the sentence-aspect pair as the input and predicts A’s sentiment polarity yb (Tang et al., 2016b; Ma et al., 2017; Xue and Li, 2018; Hazarika et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Tang et al., 2019; Chen and Qian, 2019; Tan et al., 2019; Tang et al., 2020). We follow this paradigm and the overview of our approach is illustrated in Figure 2, with a contextual encoder (i.e., BERT), the proposed T-GCN and the attentive layer ensemble (ALE). The overall conceptual formalism of our approach can be written as yb = arg max p (y|ALE (T -GCN (X , A))) (1) y2T where T denotes the set of all sentiment labels for y (i.e., positive, neutral, and negative) and p computes the probability of predicting y 2 T given X and A through T-GCN and ALE. In the following texts, we firstly describe the construction of the graph with dependency types,"
2021.naacl-main.231,C16-1311,0,0.476213,"d but the wines are excellent.”, the sentiment polarity towards “drink menu” is negative while that towards “wines” is positive; an * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/ASA-TGCN. † ABSA system may predict wrong if it fails to capture the important contextual information for each aspects. Therefore, to model such contextual information, neural models (e.g., Bi-LSTM and Transformer (Vaswani et al., 2017)) have been widely used for ABSA and demonstrated to be useful for this task (Wang et al., 2016; Tang et al., 2016a; Chen et al., 2017; Ma et al., 2017; Fan et al., 2018). As a further enhancement of encoding contextual information for ABSA, there are studies (Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019a) using graph convolutional networks (GCN) to learn from a graph that is often built over the dependency parsing results of the input texts. As a result, the GCN models are able to learn from distant wordword relations that are more helpful to ABSA. However, GCN models used in these studies are limited by omitting the information carried in dependency types and treating all word-word relat"
2021.naacl-main.231,D16-1021,0,0.373443,"d but the wines are excellent.”, the sentiment polarity towards “drink menu” is negative while that towards “wines” is positive; an * Equal contribution. Corresponding author. 1 The code and models involved in this paper are released at https://github.com/cuhksz-nlp/ASA-TGCN. † ABSA system may predict wrong if it fails to capture the important contextual information for each aspects. Therefore, to model such contextual information, neural models (e.g., Bi-LSTM and Transformer (Vaswani et al., 2017)) have been widely used for ABSA and demonstrated to be useful for this task (Wang et al., 2016; Tang et al., 2016a; Chen et al., 2017; Ma et al., 2017; Fan et al., 2018). As a further enhancement of encoding contextual information for ABSA, there are studies (Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019a) using graph convolutional networks (GCN) to learn from a graph that is often built over the dependency parsing results of the input texts. As a result, the GCN models are able to learn from distant wordword relations that are more helpful to ABSA. However, GCN models used in these studies are limited by omitting the information carried in dependency types and treating all word-word relat"
2021.naacl-main.231,2020.acl-main.588,0,0.430634,"model, where the results illustrate its effectiveness and state-of-the-art performance is observed over previous studies on all datasets. We also perform further analysis to inThe Approach Given an input sentence X = x1 , x2 , · · · , xn and the aspect terms A ⇢ X (A is usually a sub-string of X ), the conventional ABSA approaches often take the sentence-aspect pair as the input and predicts A’s sentiment polarity yb (Tang et al., 2016b; Ma et al., 2017; Xue and Li, 2018; Hazarika et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Tang et al., 2019; Chen and Qian, 2019; Tan et al., 2019; Tang et al., 2020). We follow this paradigm and the overview of our approach is illustrated in Figure 2, with a contextual encoder (i.e., BERT), the proposed T-GCN and the attentive layer ensemble (ALE). The overall conceptual formalism of our approach can be written as yb = arg max p (y|ALE (T -GCN (X , A))) (1) y2T where T denotes the set of all sentiment labels for y (i.e., positive, neutral, and negative) and p computes the probability of predicting y 2 T given X and A through T-GCN and ALE. In the following texts, we firstly describe the construction of the graph with dependency types, then elaborate the d"
2021.naacl-main.231,P19-1053,0,0.0335247,"Missing"
2021.naacl-main.231,2021.eacl-main.326,1,0.800067,"ssify a sentence-aspect pair and most of studies try to explore the contextual information between aspect and the entire sentence to facilitate the analysis of sentiment (Dong et al., 2014; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Xue and Li, 2018; Li et al., 2018; Xu et al., 2019; Wang et al., 2020; Tang et al., 2020). To further enhancing the modeling of contextual information, dependency parses were leveraged by many studies, where adaptive recursive neural networks (Dong et al., 2014), attention mechanism (He et al., 2018a), and key-value memory networks (Tian et al., 2021) are used. Later, Huang and Carley (2019); Sun et al. (2019); Zhang et al. (2019a); Wang et al. (2020); Tang et al. (2020) leveraged graph neural models (e.g., GCN) for ABSA with their graph built upon the dependency tree obtained from off-the-self dependency parsers, and demonstrated promising results. The models in their studies normally focus on building the graph with the dependency structure without considering dependency types, meanwhile treating the edges in the graph equally. In addition, they usually use the output of the last layer to predict sentiment labels although their models co"
2021.naacl-main.231,2020.acl-main.735,1,0.905553,"res such as n-grams and syntactic information have been demonstrated to be useful to enhance text representation and thus improve model performance for many NLP tasks (Sun and Xu, 2011; Song and Xia, 2012; Gong et al., 2012; Song et al., 2012; Xu et al., 2015; Chen et al., 2017; Zhang et al., 2019b; Tang et al., 2020). In addition, it is demonstrated by many recent studies that GCN models are effective in capturing contextual features that are represented in graph-like signals, i.e., dependencies among words, of an input sentence (Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019a; Tian et al., 2020c; Chen et al., 2020). In the graph for conventional GCN models, each edge between any two words xi and xj in the input sentence is added to the graph if there is a 2911 (l) Figure 4: The illustration of how we compute hi for x3 =“menu” through a T-GCN layer. All words xj connected to “menu” with their dependency types (in embeddings eri,j ) are shown at the bottom part. we use a transition matrix to map all ri,j to their embeddings eri,j . 2.2 Figure 3: An illustration of how we build the typeaware graph from dependency parsing results and the detail of a T-GCN layer that consumes the graph."
2021.naacl-main.231,2020.coling-main.187,1,0.855528,"res such as n-grams and syntactic information have been demonstrated to be useful to enhance text representation and thus improve model performance for many NLP tasks (Sun and Xu, 2011; Song and Xia, 2012; Gong et al., 2012; Song et al., 2012; Xu et al., 2015; Chen et al., 2017; Zhang et al., 2019b; Tang et al., 2020). In addition, it is demonstrated by many recent studies that GCN models are effective in capturing contextual features that are represented in graph-like signals, i.e., dependencies among words, of an input sentence (Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019a; Tian et al., 2020c; Chen et al., 2020). In the graph for conventional GCN models, each edge between any two words xi and xj in the input sentence is added to the graph if there is a 2911 (l) Figure 4: The illustration of how we compute hi for x3 =“menu” through a T-GCN layer. All words xj connected to “menu” with their dependency types (in embeddings eri,j ) are shown at the bottom part. we use a transition matrix to map all ri,j to their embeddings eri,j . 2.2 Figure 3: An illustration of how we build the typeaware graph from dependency parsing results and the detail of a T-GCN layer that consumes the graph."
2021.naacl-main.231,2020.emnlp-main.487,1,0.748463,"res such as n-grams and syntactic information have been demonstrated to be useful to enhance text representation and thus improve model performance for many NLP tasks (Sun and Xu, 2011; Song and Xia, 2012; Gong et al., 2012; Song et al., 2012; Xu et al., 2015; Chen et al., 2017; Zhang et al., 2019b; Tang et al., 2020). In addition, it is demonstrated by many recent studies that GCN models are effective in capturing contextual features that are represented in graph-like signals, i.e., dependencies among words, of an input sentence (Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019a; Tian et al., 2020c; Chen et al., 2020). In the graph for conventional GCN models, each edge between any two words xi and xj in the input sentence is added to the graph if there is a 2911 (l) Figure 4: The illustration of how we compute hi for x3 =“menu” through a T-GCN layer. All words xj connected to “menu” with their dependency types (in embeddings eri,j ) are shown at the bottom part. we use a transition matrix to map all ri,j to their embeddings eri,j . 2.2 Figure 3: An illustration of how we build the typeaware graph from dependency parsing results and the detail of a T-GCN layer that consumes the graph."
2021.naacl-main.231,2020.acl-main.295,0,0.254282,"ver the dependency relations and types from the trees.5 Since high quality text representations can improve the performance of NLP models (Mikolov et al., 2013; Song et al., 2017; Bojanowski et al., 2017; Song and Shi, 2018; Song et al., 2018), we employ BERT (Devlin et al., 2019) as the context encoder, which and whose variants (Diao et al., 2020; Dai et al., 2019; Joshi et al., 2020) have demonstrated their effectiveness to encode context information and achieved stateof-the-art performance in many NLP tasks (Huang and Carley, 2019; Tian et al., 2020a,b; Tang et al., 2020; Nie et al., 2020; Wang et al., 2020). Specifically, we use the uncased BERT-base and BERTlarge6 with their default settings, i.e., 12 layers of self-attention with 768 dimensional hidden vectors for BERT-base and 24 layers of self-attention with 1024 dimensional hidden vectors for BERTlarge, and use three T-GCN layers. We try two ways to encode the input, where the first encodes the single sentence and the second encodes the sentence-aspect pair. For all models, we use the pre-trained parameters of BERT and initialize all other trainable parameters by Xavier (Glorot and Bengio, 2010). Moreover, we use the cross-entropy loss func"
2021.naacl-main.231,D16-1058,0,0.0714064,"Missing"
2021.naacl-main.231,N19-1242,0,0.0576932,"Missing"
2021.naacl-main.231,D15-1206,0,0.0574708,"X and A through T-GCN and ALE. In the following texts, we firstly describe the construction of the graph with dependency types, then elaborate the details of our T-GCN model, and the ALE to incorporate contextual information from different T-GCN layers, and finally illustrate incorporating T-GCN to ABSA. 2.1 Type-aware Graph Construction Contextual features such as n-grams and syntactic information have been demonstrated to be useful to enhance text representation and thus improve model performance for many NLP tasks (Sun and Xu, 2011; Song and Xia, 2012; Gong et al., 2012; Song et al., 2012; Xu et al., 2015; Chen et al., 2017; Zhang et al., 2019b; Tang et al., 2020). In addition, it is demonstrated by many recent studies that GCN models are effective in capturing contextual features that are represented in graph-like signals, i.e., dependencies among words, of an input sentence (Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019a; Tian et al., 2020c; Chen et al., 2020). In the graph for conventional GCN models, each edge between any two words xi and xj in the input sentence is added to the graph if there is a 2911 (l) Figure 4: The illustration of how we compute hi for x3 =“menu” throu"
2021.naacl-main.231,P18-1234,0,0.0725466,"extual information from such relations to enhance ABSA. Experiments on six English benchmark datasets are conducted to evaluate the proposed model, where the results illustrate its effectiveness and state-of-the-art performance is observed over previous studies on all datasets. We also perform further analysis to inThe Approach Given an input sentence X = x1 , x2 , · · · , xn and the aspect terms A ⇢ X (A is usually a sub-string of X ), the conventional ABSA approaches often take the sentence-aspect pair as the input and predicts A’s sentiment polarity yb (Tang et al., 2016b; Ma et al., 2017; Xue and Li, 2018; Hazarika et al., 2018; Fan et al., 2018; Huang and Carley, 2018; Tang et al., 2019; Chen and Qian, 2019; Tan et al., 2019; Tang et al., 2020). We follow this paradigm and the overview of our approach is illustrated in Figure 2, with a contextual encoder (i.e., BERT), the proposed T-GCN and the attentive layer ensemble (ALE). The overall conceptual formalism of our approach can be written as yb = arg max p (y|ALE (T -GCN (X , A))) (1) y2T where T denotes the set of all sentiment labels for y (i.e., positive, neutral, and negative) and p computes the probability of predicting y 2 T given X and"
2021.naacl-main.231,D19-1464,0,0.0341094,"Missing"
2021.naacl-main.231,N19-1093,1,0.904949,"in this paper are released at https://github.com/cuhksz-nlp/ASA-TGCN. † ABSA system may predict wrong if it fails to capture the important contextual information for each aspects. Therefore, to model such contextual information, neural models (e.g., Bi-LSTM and Transformer (Vaswani et al., 2017)) have been widely used for ABSA and demonstrated to be useful for this task (Wang et al., 2016; Tang et al., 2016a; Chen et al., 2017; Ma et al., 2017; Fan et al., 2018). As a further enhancement of encoding contextual information for ABSA, there are studies (Sun et al., 2019; Huang and Carley, 2019; Zhang et al., 2019a) using graph convolutional networks (GCN) to learn from a graph that is often built over the dependency parsing results of the input texts. As a result, the GCN models are able to learn from distant wordword relations that are more helpful to ABSA. However, GCN models used in these studies are limited by omitting the information carried in dependency types and treating all word-word relations in the graph equally, therefore unimportant relations may not be distinguished and mislead ABSA accordingly. For example, Figure 1 illustrates an example sentence with an aspect highlighted in red, wher"
C12-2116,D11-1033,0,0.0530956,"0; Plank and van Noord, 2011)), model combination (e.g., (McClosky et al., 2010)), feature copying (Daume III, 2007), semi-supervised learning (e.g., (McClosky et al., 2006)), and many more. The goal of training data selection is to choose a subset of training data that was similar to a given test data set. The challenge is to find a good measure for calculating the similarity between training sentences and the test data set. Moore and Lewis (2010) calculated the difference of the cross entropy values for a given sentence, based on language models from the source domain and the target domain. Axelrod et al. (2011) adopted the idea of cross entropy measurement for training data selection for machine translation, in three different ways: the first directly measured cross entropy for the source side of the text; the second is similar to (Moore and Lewis, 2010) and ranked the data using cross entropy difference; and the third, took into account the bilingual data on both the source and target side of translations. Both studies showed that the selected subset of training data worked better than the entire training corpus for machine translation. Plank and van Noord (2011) investigated several different trai"
C12-2116,P07-1033,0,0.133576,"Missing"
C12-2116,W99-0701,1,0.863971,"entropy gain (EG), is defined as in Eq 5, where q is a probability distribution estimated from C and q1 is one estimated from C + s, a new corpus formed by adding s to C . Intuitively, if s is similar to C , q1 will be very similar to q and EG(s, c) will be small. EG(s, C) = |H(C + s, q1) − H(C, q) | (5) The measures in Eq 3-5 can all be normalized by sentence length. For instance, Eq 6 shows the normalized entropy gain. We call it Average Entropy Gain (AEG). AEG(s, C) = 3.4 EG(s, C) leng th(s) (6) Descriptive Length Gain (DLG) Description length gain (DLG) is a goodness measure proposed by (Kit and Wilks, 1999) as an unsupervised learning approach to lexical acquisition (Kit, 2005; Kit and Zhao, 2007). Intuitively, the DLG of a string str w.r.t. a corpus C, DLG(str, C), indicates the reduction of description length of C when the characters in str are treated as a unit and all the occurrences of str in C are replaced by the index of the unit. Therefore, the more frequent str is in C and the longer str is, the higher DLG(str,C) is. The DLG calculation resorts to a re-implementation of the suffix array approach to counting n-grams (Kit and Wilks, 1998). Based on this property, we define a similarity me"
C12-2116,J93-2004,0,0.0460848,"ntropy of two probability distributions estimated from the training data and the test data. If that is the case, it implies that entropy-based measures could be effective for training data selection. We then propose several new entropy-based measures and test their effects on two NLP tasks: CWS and POS tagging. For evaluation, we use the Chinese Penn Treebank as described below. 2.1 Data The Chinese Penn Treebank (CTB) was developed in the late 1990s (Xia et al., 2000) and each sentence is word segmented, part-of-speech tagged, and bracketed with a scheme similar to the English Penn Treebank (Marcus et al., 1993). Its latest release is version 7.0 1 , which contains more than one million words from five genres: Broadcast Conversation (BC), Broadcast News (BN), Magazine (MZ), Newswire (NW), and Weblog (WB). Some statistics of CTB7 are given in Table 1. We have used CTB 7.0 for multiple experiments, some of them not directly related to this study. To prepare the data for all of our experiments, we divide the data in each genre into 1 Linguistic Data Consortium No. LDC2010T07 1192 Genre # of chars 275,289 Broadcast Conversation (BC) Broadcast News 482,667 (BN) # of words 184,161 # of files 86 287,442 1,1"
C12-2116,P06-1043,0,0.029327,"Adaptation, Training Data Selection, Entropy-based measures. Proceedings of COLING 2012: Posters, pages 1191–1200, COLING 2012, Mumbai, December 2012. 1191 1 Introduction The performance of Natural Language Processing (NLP) systems often degrades significantly when training and testing data come from different domains. There has been extensive research on methods for domain adaptation including training data selection (e.g.,(Moore and Lewis, 2010; Plank and van Noord, 2011)), model combination (e.g., (McClosky et al., 2010)), feature copying (Daume III, 2007), semi-supervised learning (e.g., (McClosky et al., 2006)), and many more. The goal of training data selection is to choose a subset of training data that was similar to a given test data set. The challenge is to find a good measure for calculating the similarity between training sentences and the test data set. Moore and Lewis (2010) calculated the difference of the cross entropy values for a given sentence, based on language models from the source domain and the target domain. Axelrod et al. (2011) adopted the idea of cross entropy measurement for training data selection for machine translation, in three different ways: the first directly measured"
C12-2116,N10-1004,0,0.0183548,"statistically significant improvement over random selection for both tasks. KEYWORDS: Domain Adaptation, Training Data Selection, Entropy-based measures. Proceedings of COLING 2012: Posters, pages 1191–1200, COLING 2012, Mumbai, December 2012. 1191 1 Introduction The performance of Natural Language Processing (NLP) systems often degrades significantly when training and testing data come from different domains. There has been extensive research on methods for domain adaptation including training data selection (e.g.,(Moore and Lewis, 2010; Plank and van Noord, 2011)), model combination (e.g., (McClosky et al., 2010)), feature copying (Daume III, 2007), semi-supervised learning (e.g., (McClosky et al., 2006)), and many more. The goal of training data selection is to choose a subset of training data that was similar to a given test data set. The challenge is to find a good measure for calculating the similarity between training sentences and the test data set. Moore and Lewis (2010) calculated the difference of the cross entropy values for a given sentence, based on language models from the source domain and the target domain. Axelrod et al. (2011) adopted the idea of cross entropy measurement for training"
C12-2116,P10-2041,0,0.0575601,"lts on the Chinese Penn Treebank indicate that some of the measures provide a statistically significant improvement over random selection for both tasks. KEYWORDS: Domain Adaptation, Training Data Selection, Entropy-based measures. Proceedings of COLING 2012: Posters, pages 1191–1200, COLING 2012, Mumbai, December 2012. 1191 1 Introduction The performance of Natural Language Processing (NLP) systems often degrades significantly when training and testing data come from different domains. There has been extensive research on methods for domain adaptation including training data selection (e.g.,(Moore and Lewis, 2010; Plank and van Noord, 2011)), model combination (e.g., (McClosky et al., 2010)), feature copying (Daume III, 2007), semi-supervised learning (e.g., (McClosky et al., 2006)), and many more. The goal of training data selection is to choose a subset of training data that was similar to a given test data set. The challenge is to find a good measure for calculating the similarity between training sentences and the test data set. Moore and Lewis (2010) calculated the difference of the cross entropy values for a given sentence, based on language models from the source domain and the target domain. A"
C12-2116,P11-1157,0,0.0869203,"Missing"
C12-2116,song-xia-2012-using,1,0.730643,"Missing"
C12-2116,N03-1033,0,0.0438304,"study, we chose to use the same data split for training and test to facilitate comparison with our other experiments. Training Development Test BC 211,795 30,678 32,816 BN 211,826 30,760 48,317 MZ 211,834 30,708 37,531 NW 211,853 30,726 44,543 WB 211,796 30,746 33,623 Table 2: CTB 7.0 Genre character counts for data splitting. 2.2 System performance and cross entropy In order to determine whether entropy-based measures are helpful in training data selection, we first check whether cross entropy correlates with system performance. For this, we first trained and tested the Stanford POS Tagger2 (Toutanova et al., 2003) on the CTB 7.0. The results are in Table 3, in which the training and testing genres are indicated by row labels and column labels, respectively. In the top part of the table, each cell (i, j) has two numbers, where i is the row and j is the column. The first number is the tagging accuracy, when the tagger is trained on the training data of the genre i , and tested on the test data of the genre j . The second number is cross entropy of the test data, estimated by a trigram language model built from the training data using the CMU-Cambridge LM Toolkit3 . The final row in the table lists the 2"
C12-2116,xia-etal-2000-developing,1,0.750312,"n (CWS) and POS tagging. 2 Methodology In this study, we first test whether there is a strong correlation between system performance and cross entropy of two probability distributions estimated from the training data and the test data. If that is the case, it implies that entropy-based measures could be effective for training data selection. We then propose several new entropy-based measures and test their effects on two NLP tasks: CWS and POS tagging. For evaluation, we use the Chinese Penn Treebank as described below. 2.1 Data The Chinese Penn Treebank (CTB) was developed in the late 1990s (Xia et al., 2000) and each sentence is word segmented, part-of-speech tagged, and bracketed with a scheme similar to the English Penn Treebank (Marcus et al., 1993). Its latest release is version 7.0 1 , which contains more than one million words from five genres: Broadcast Conversation (BC), Broadcast News (BN), Magazine (MZ), Newswire (NW), and Weblog (WB). Some statistics of CTB7 are given in Table 1. We have used CTB 7.0 for multiple experiments, some of them not directly related to this study. To prepare the data for all of our experiments, we divide the data in each genre into 1 Linguistic Data Consortiu"
D18-1273,W99-0411,0,0.210222,"1)是一个很聪明的男孩 他 (ta1) Table 1: Two examples of Chinese sentences containing spelling errors. Spelling errors are marked in red. The first sentence contains a visually similar spelling error, i.e., 已 (yi3) is misspelled as 己 (ji2). The second sentence contains a phonologically similar spelling error, i.e., 他 (ta1) is misspelled as 她 (ta1). Introduction Spelling check is a crucial task to detect and correct human spelling errors in running texts (Yu and Li, 2014). This task is vital for NLP applications such as search engine (Martins and Silva, 2004; Gao et al., 2010) and automatic essay scoring (Burstein and Chodorow, 1999; Lonsdale and Strong-Krause, 2003), for the reason that spelling errors not only affect reading but also sometimes completely alter the meaning delivered in a text fragment. Especially, in Chinese language processing, spelling errors can be more serious since they may affect fundamental tasks such as word segmentation (Xue, 2003; Song and Xia, 2012) and part-of-speech tagging (Chang et al., 1993; Jiang et al., 2008; Sun, 2011), etc. Of all causes lead to spelling errors, a major one comes from the misuse of Chinese input methods on daily texts, e.g., emails and social media posts. Table 1 ill"
D18-1273,W13-4418,0,0.0207753,"e wordlevel, 禁烟 (translation: forbid smoking) and 戒 烟 (translation: give up smoking) are two related common Chinese words; it needs to incorporate more context in order to improve the recall performance. Similar to our study on character-based corpus generation, one potential solution is to construct a word-level annotated corpus in order to better detect such spelling errors. 2524 4 Related Work In the line of research on spelling error detection and correction, most previous efforts focus on designing different models to improve the performance of CSC (Chang, 1995; Huang et al., 2007, 2008; Chang et al., 2013). Different from them, this work contributes to the generation of training datasets, which are important resources and can be used for improving many existing CSC models. Currently, the limited training datasets have set a high barrier for many data-driven approaches (Wang et al., 2013; Wang and Liao, 2015; Zheng et al., 2016). To the best of our knowledge, up to date, there is no large quantities of annotated data sets commonly available for CSC. Some previous work (Liu et al., 2009; Chang et al., 2013) pointed out that visually and phonologically similar characters are major contributing fac"
D18-1273,W13-4414,0,0.346087,"es of such Chinese spelling errors. The first incorrect sentence contains a misused character, 己 (ji2)1 , which has a similar shape to its corresponding correct character, i.e., 已 (yi3). In the second incorrect sentence, the boxed spelling error 她 (ta1) is phonetically identical to its corresponding correct one 他 (ta1). Owing to the limited number of available datasets, many state-of-the-art supervised models are seldom employed in this field, which hinders the development of CSC. Currently, some mainstream approaches still focus on using unsupervised methods, i.e., language model based ones (Chen et al., 2013; Yu et al., 2014; Tseng et al., 2015; Lee et al., 2016). As a result, the development of CSC techniques are restricted and thus CSC performance is not satisfied so far (Fung et al., 2017). To enhance CSC performance, the biggest challenge is the unavailability of large scale corpora with labeled spelling errors, which is of high value for training and applying supervised models. Such issue of data absence is mainly caused by the fact that annotating spelling errors is an expensive and challenging task. To address the data unavailability issue so that 1 ∗ This work was conducted during Dingmin"
D18-1273,C16-2041,0,0.123235,"s. Experimental results show that the BiLSTM models trained on our generated corpus yield better performance than their counterparts trained on the training dataset provided in the standard testing datasets. To further facilitating the CSC task, we construct confusion sets by collecting all incorrect variants for each character and their corresponding correct references. The effectiveness of the confusion set is confirmed in the error correction task, indicating that the constructed confusion sets are highly useful in many existing Chinese spelling check schemes (Chang, 1995; Wu et al., 2010; Dong et al., 2016). 2 Automatic Data Generation Spelling errors in Chinese are mainly caused by the misuse of visually or phonologically similar characters (Chang, 1995; Liu et al., 2011; Yu and Li, 2014). Errors of visually similar characters (henceforth V-style errors) are due to the prominence of character pairs visually similar to each other. The reason is that, Chinese, as a hieroglyph language, consists of more than sixty thousand characters2 . They are constructed by a limited number of radicals and components3 . As for errors caused by the misuse of phonologically similar characters (henceforth P-style"
D18-1273,C10-1041,0,0.254292,"n 我们应该认真对待这些 己 (ji2) 经发生的事 已 (yi3) 在我们班上， 她 (ta1)是一个很聪明的男孩 他 (ta1) Table 1: Two examples of Chinese sentences containing spelling errors. Spelling errors are marked in red. The first sentence contains a visually similar spelling error, i.e., 已 (yi3) is misspelled as 己 (ji2). The second sentence contains a phonologically similar spelling error, i.e., 他 (ta1) is misspelled as 她 (ta1). Introduction Spelling check is a crucial task to detect and correct human spelling errors in running texts (Yu and Li, 2014). This task is vital for NLP applications such as search engine (Martins and Silva, 2004; Gao et al., 2010) and automatic essay scoring (Burstein and Chodorow, 1999; Lonsdale and Strong-Krause, 2003), for the reason that spelling errors not only affect reading but also sometimes completely alter the meaning delivered in a text fragment. Especially, in Chinese language processing, spelling errors can be more serious since they may affect fundamental tasks such as word segmentation (Xue, 2003; Song and Xia, 2012) and part-of-speech tagging (Chang et al., 1993; Jiang et al., 2008; Sun, 2011), etc. Of all causes lead to spelling errors, a major one comes from the misuse of Chinese input methods on dail"
D18-1273,P08-1102,0,0.0970369,"Missing"
D18-1273,P98-1107,0,0.287098,"an that of visually similar spelling errors. Vision- and speech-related technologies are then adopted in our approach. As a technology to extract text information from images, optical character recognition recognizes the shapes and assigns characters. According to Nagy (1988); McBride-Chang et al. (2003), incorrectly recognized results are mainly due to the visual similarities among some different Chinese characters. On the other side, automatic speech recognition is an acoustics-based recognition process for handling audio stream, where phonologically similar characters are usually confused (Kaki et al., 1998; Voll et al., 2008; Braho et al., 2014). 5 Conclusion and Future Work In this paper, we proposed a hybrid approach to automatic generating Chinese corpus for spelling check with labeled spelling errors. Specifically, OCR- and ASR-based methods were used to generate labeled spelling errors by replacing visually and phonologically resembled characters. Human evaluation confirmed that our proposed method can produce common errors that are likely to be made by human and such errors can serve as effective annotated spelling errors for CSC. In our experiment, a neural tagging model was trained on t"
D18-1273,W16-4906,0,0.0676228,"Missing"
D18-1273,W09-3412,0,0.0352555,"ocus on designing different models to improve the performance of CSC (Chang, 1995; Huang et al., 2007, 2008; Chang et al., 2013). Different from them, this work contributes to the generation of training datasets, which are important resources and can be used for improving many existing CSC models. Currently, the limited training datasets have set a high barrier for many data-driven approaches (Wang et al., 2013; Wang and Liao, 2015; Zheng et al., 2016). To the best of our knowledge, up to date, there is no large quantities of annotated data sets commonly available for CSC. Some previous work (Liu et al., 2009; Chang et al., 2013) pointed out that visually and phonologically similar characters are major contributing factors for errors in Chinese texts, where the number of phonologically similar spelling errors is about two times than that of visually similar spelling errors. Vision- and speech-related technologies are then adopted in our approach. As a technology to extract text information from images, optical character recognition recognizes the shapes and assigns characters. According to Nagy (1988); McBride-Chang et al. (2003), incorrectly recognized results are mainly due to the visual similar"
D18-1273,W03-0209,0,0.29645,"Two examples of Chinese sentences containing spelling errors. Spelling errors are marked in red. The first sentence contains a visually similar spelling error, i.e., 已 (yi3) is misspelled as 己 (ji2). The second sentence contains a phonologically similar spelling error, i.e., 他 (ta1) is misspelled as 她 (ta1). Introduction Spelling check is a crucial task to detect and correct human spelling errors in running texts (Yu and Li, 2014). This task is vital for NLP applications such as search engine (Martins and Silva, 2004; Gao et al., 2010) and automatic essay scoring (Burstein and Chodorow, 1999; Lonsdale and Strong-Krause, 2003), for the reason that spelling errors not only affect reading but also sometimes completely alter the meaning delivered in a text fragment. Especially, in Chinese language processing, spelling errors can be more serious since they may affect fundamental tasks such as word segmentation (Xue, 2003; Song and Xia, 2012) and part-of-speech tagging (Chang et al., 1993; Jiang et al., 2008; Sun, 2011), etc. Of all causes lead to spelling errors, a major one comes from the misuse of Chinese input methods on daily texts, e.g., emails and social media posts. Table 1 illustrates two examples of such Chine"
D18-1273,song-xia-2012-using,1,0.842544,"check is a crucial task to detect and correct human spelling errors in running texts (Yu and Li, 2014). This task is vital for NLP applications such as search engine (Martins and Silva, 2004; Gao et al., 2010) and automatic essay scoring (Burstein and Chodorow, 1999; Lonsdale and Strong-Krause, 2003), for the reason that spelling errors not only affect reading but also sometimes completely alter the meaning delivered in a text fragment. Especially, in Chinese language processing, spelling errors can be more serious since they may affect fundamental tasks such as word segmentation (Xue, 2003; Song and Xia, 2012) and part-of-speech tagging (Chang et al., 1993; Jiang et al., 2008; Sun, 2011), etc. Of all causes lead to spelling errors, a major one comes from the misuse of Chinese input methods on daily texts, e.g., emails and social media posts. Table 1 illustrates two examples of such Chinese spelling errors. The first incorrect sentence contains a misused character, 己 (ji2)1 , which has a similar shape to its corresponding correct character, i.e., 已 (yi3). In the second incorrect sentence, the boxed spelling error 她 (ta1) is phonetically identical to its corresponding correct one 他 (ta1). Owing to th"
D18-1273,P11-1139,0,0.0193818,"u and Li, 2014). This task is vital for NLP applications such as search engine (Martins and Silva, 2004; Gao et al., 2010) and automatic essay scoring (Burstein and Chodorow, 1999; Lonsdale and Strong-Krause, 2003), for the reason that spelling errors not only affect reading but also sometimes completely alter the meaning delivered in a text fragment. Especially, in Chinese language processing, spelling errors can be more serious since they may affect fundamental tasks such as word segmentation (Xue, 2003; Song and Xia, 2012) and part-of-speech tagging (Chang et al., 1993; Jiang et al., 2008; Sun, 2011), etc. Of all causes lead to spelling errors, a major one comes from the misuse of Chinese input methods on daily texts, e.g., emails and social media posts. Table 1 illustrates two examples of such Chinese spelling errors. The first incorrect sentence contains a misused character, 己 (ji2)1 , which has a similar shape to its corresponding correct character, i.e., 已 (yi3). In the second incorrect sentence, the boxed spelling error 她 (ta1) is phonetically identical to its corresponding correct one 他 (ta1). Owing to the limited number of available datasets, many state-of-the-art supervised models"
D18-1273,yang-etal-2012-spell,0,0.0968314,"Liu et al., 2011; Yu and Li, 2014). Errors of visually similar characters (henceforth V-style errors) are due to the prominence of character pairs visually similar to each other. The reason is that, Chinese, as a hieroglyph language, consists of more than sixty thousand characters2 . They are constructed by a limited number of radicals and components3 . As for errors caused by the misuse of phonologically similar characters (henceforth P-style errors), we note that pronunciations of Chinese characters are usually defined by Pinyin, which consists of initials, finals, and tones4 . According to Yang et al. (2012), there are only 398 syllables for thousands of characters in modern Chinese. As a result, there are many Chinese characters sharing similar pronunciation, which further leads to the prominence of P-style errors. In the rest of this section, we describe how we generate these two types of errors in Section 2.1 and 2.2, respectively. 2.1 OCR-based Generation Inspired by the observation that optical character recognition (OCR) tools are likely to misidentify characters with those visually similar ones (Tong and Evans, 1996), we intentionally blur images with correct characters, and apply OCR tool"
D18-1273,W14-6835,0,0.204892,"s demonstrate the effectiveness of the corpus, therefore confirm the validity of our approach. 1 Correction 我们应该认真对待这些 己 (ji2) 经发生的事 已 (yi3) 在我们班上， 她 (ta1)是一个很聪明的男孩 他 (ta1) Table 1: Two examples of Chinese sentences containing spelling errors. Spelling errors are marked in red. The first sentence contains a visually similar spelling error, i.e., 已 (yi3) is misspelled as 己 (ji2). The second sentence contains a phonologically similar spelling error, i.e., 他 (ta1) is misspelled as 她 (ta1). Introduction Spelling check is a crucial task to detect and correct human spelling errors in running texts (Yu and Li, 2014). This task is vital for NLP applications such as search engine (Martins and Silva, 2004; Gao et al., 2010) and automatic essay scoring (Burstein and Chodorow, 1999; Lonsdale and Strong-Krause, 2003), for the reason that spelling errors not only affect reading but also sometimes completely alter the meaning delivered in a text fragment. Especially, in Chinese language processing, spelling errors can be more serious since they may affect fundamental tasks such as word segmentation (Xue, 2003; Song and Xia, 2012) and part-of-speech tagging (Chang et al., 1993; Jiang et al., 2008; Sun, 2011), etc"
D18-1273,W16-4907,0,0.0396323,"pus in order to better detect such spelling errors. 2524 4 Related Work In the line of research on spelling error detection and correction, most previous efforts focus on designing different models to improve the performance of CSC (Chang, 1995; Huang et al., 2007, 2008; Chang et al., 2013). Different from them, this work contributes to the generation of training datasets, which are important resources and can be used for improving many existing CSC models. Currently, the limited training datasets have set a high barrier for many data-driven approaches (Wang et al., 2013; Wang and Liao, 2015; Zheng et al., 2016). To the best of our knowledge, up to date, there is no large quantities of annotated data sets commonly available for CSC. Some previous work (Liu et al., 2009; Chang et al., 2013) pointed out that visually and phonologically similar characters are major contributing factors for errors in Chinese texts, where the number of phonologically similar spelling errors is about two times than that of visually similar spelling errors. Vision- and speech-related technologies are then adopted in our approach. As a technology to extract text information from images, optical character recognition recogniz"
D18-1273,W96-0108,0,0.528246,"ned by Pinyin, which consists of initials, finals, and tones4 . According to Yang et al. (2012), there are only 398 syllables for thousands of characters in modern Chinese. As a result, there are many Chinese characters sharing similar pronunciation, which further leads to the prominence of P-style errors. In the rest of this section, we describe how we generate these two types of errors in Section 2.1 and 2.2, respectively. 2.1 OCR-based Generation Inspired by the observation that optical character recognition (OCR) tools are likely to misidentify characters with those visually similar ones (Tong and Evans, 1996), we intentionally blur images with correct characters, and apply OCR tools on them to produce V-style spelling errors. In detail, we use Google Tesseract (Smith, 2007) as the OCR toolkit and the generation process is illustrated in Figure 1. Given a sentence, 2 http://www.hanzizidian.com. There are less than three hundred radicals in total. https://en.wikipedia.org/wiki/Radical_ (Chinese_characters) 4 https://en.wikipedia.org/wiki/Pinyin 2518 3 as the first step, we randomly select 1 ∼ 2 character(s) from it as our target characters to be detected by Tesseract, denoted as Ctargets . Specifica"
D18-1273,W15-3108,0,0.328433,"d-level annotated corpus in order to better detect such spelling errors. 2524 4 Related Work In the line of research on spelling error detection and correction, most previous efforts focus on designing different models to improve the performance of CSC (Chang, 1995; Huang et al., 2007, 2008; Chang et al., 2013). Different from them, this work contributes to the generation of training datasets, which are important resources and can be used for improving many existing CSC models. Currently, the limited training datasets have set a high barrier for many data-driven approaches (Wang et al., 2013; Wang and Liao, 2015; Zheng et al., 2016). To the best of our knowledge, up to date, there is no large quantities of annotated data sets commonly available for CSC. Some previous work (Liu et al., 2009; Chang et al., 2013) pointed out that visually and phonologically similar characters are major contributing factors for errors in Chinese texts, where the number of phonologically similar spelling errors is about two times than that of visually similar spelling errors. Vision- and speech-related technologies are then adopted in our approach. As a technology to extract text information from images, optical character"
D18-1273,W13-4412,0,0.030437,"to construct a word-level annotated corpus in order to better detect such spelling errors. 2524 4 Related Work In the line of research on spelling error detection and correction, most previous efforts focus on designing different models to improve the performance of CSC (Chang, 1995; Huang et al., 2007, 2008; Chang et al., 2013). Different from them, this work contributes to the generation of training datasets, which are important resources and can be used for improving many existing CSC models. Currently, the limited training datasets have set a high barrier for many data-driven approaches (Wang et al., 2013; Wang and Liao, 2015; Zheng et al., 2016). To the best of our knowledge, up to date, there is no large quantities of annotated data sets commonly available for CSC. Some previous work (Liu et al., 2009; Chang et al., 2013) pointed out that visually and phonologically similar characters are major contributing factors for errors in Chinese texts, where the number of phonologically similar spelling errors is about two times than that of visually similar spelling errors. Vision- and speech-related technologies are then adopted in our approach. As a technology to extract text information from imag"
D18-1273,W10-4107,0,0.189684,"d testing datasets. Experimental results show that the BiLSTM models trained on our generated corpus yield better performance than their counterparts trained on the training dataset provided in the standard testing datasets. To further facilitating the CSC task, we construct confusion sets by collecting all incorrect variants for each character and their corresponding correct references. The effectiveness of the confusion set is confirmed in the error correction task, indicating that the constructed confusion sets are highly useful in many existing Chinese spelling check schemes (Chang, 1995; Wu et al., 2010; Dong et al., 2016). 2 Automatic Data Generation Spelling errors in Chinese are mainly caused by the misuse of visually or phonologically similar characters (Chang, 1995; Liu et al., 2011; Yu and Li, 2014). Errors of visually similar characters (henceforth V-style errors) are due to the prominence of character pairs visually similar to each other. The reason is that, Chinese, as a hieroglyph language, consists of more than sixty thousand characters2 . They are constructed by a limited number of radicals and components3 . As for errors caused by the misuse of phonologically similar characters"
D18-1273,W13-4406,0,0.303421,"Missing"
D18-1273,O03-4002,0,0.212957,"on Spelling check is a crucial task to detect and correct human spelling errors in running texts (Yu and Li, 2014). This task is vital for NLP applications such as search engine (Martins and Silva, 2004; Gao et al., 2010) and automatic essay scoring (Burstein and Chodorow, 1999; Lonsdale and Strong-Krause, 2003), for the reason that spelling errors not only affect reading but also sometimes completely alter the meaning delivered in a text fragment. Especially, in Chinese language processing, spelling errors can be more serious since they may affect fundamental tasks such as word segmentation (Xue, 2003; Song and Xia, 2012) and part-of-speech tagging (Chang et al., 1993; Jiang et al., 2008; Sun, 2011), etc. Of all causes lead to spelling errors, a major one comes from the misuse of Chinese input methods on daily texts, e.g., emails and social media posts. Table 1 illustrates two examples of such Chinese spelling errors. The first incorrect sentence contains a misused character, 己 (ji2)1 , which has a similar shape to its corresponding correct character, i.e., 已 (yi3). In the second incorrect sentence, the boxed spelling error 她 (ta1) is phonetically identical to its corresponding correct one"
D18-1351,D17-1047,0,0.338482,"n on online platforms. A large body of dailygenerated contents, such as tweets, web search snippets, news feeds, and forum messages, have far outpaced the reading and understanding capacity of individuals. As a consequence, there is a pressing need for automatic language understanding techniques for processing and analyzing such texts (Zhang et al., 2018). Among those techniques, text classification is a critical and fundamental one proven to be useful in various downstream applications, such as text summarization (Hu et al., 2015), recommendation (Zhang et al., 2012), and sentiment analysis (Chen et al., 2017). Although many classification models like support vector machines (SVMs) (Wang and Manning, 2012) and neural networks (Kim, 2014; Xiao and Cho, 2016; Joulin et al., 2017) have demonstrated their success in processing formal and well-edited texts, such as news articles (Zhang ∗ This work was mainly conducted when Jichuan Zeng was an intern in Tencent AI Lab. † Jing Li is the corresponding author. et al., 2015b), their performance is inevitably compromised when directly applied to short and informal online texts. This inferior performance is attributed to the severe data sparsity nature of shor"
D18-1351,P15-1077,0,0.0493417,"ficient. Then, in target memory, via weighting target memory matrix T with ξ, we obtain the output representation R of the topic memory mechanism: R k = ξk T k (5) The concatenation of R and U (embedded xSeq ) further serves as the features for classification. In particular, similar to the memory networks in prior research (Sukhbaatar et al., 2015; Chen et al., 2017), our model can be extended to handle multiple computation layers (hops). As shown in Figure 2, each hop contains a source matrix and a target matrix, and different hops are stacked following the way presented in Sukhbaatar et al. (2015). 3122 # of labels 8 7 50 50 Dataset Snippets TagMyNews Twitter Weibo # of docs 12,332 32,567 15,056 21,944 Avg len per doc 17 8 5 6 Vocab size 7,334 9,433 6,962 10,121 Table 2: Statistics of the experimental datasets. Labels refers to class labels. Avg len per doc refers to the average count of words in each document instance. 2.3 Joint Learning The entire TMN model integrates the three modules in Figure 1, i.e., the neural topic model, the topic memory mechanism, and the classifier, which can be updated simultaneously in one framework. In doing so, we jointly tackle topic modeling and classi"
D18-1351,D17-1054,0,0.0277224,"two ReLU-actived neural perceptrons, both taking the topic-word weight matrix W φ ∈ RK×V as inputs. Recall that in NTM, we use fφ (·) to compute the word distributions given θ. W φ is the kernel weight matrix of fφ (·), where W φk,v represents the importance of the v-th word in reflecting the k-th topic. Assuming U as the embedded xSeq (word sequence form of x), in source memory, we compute the match between the k-th topic and the embedding of the l-th word in xSeq by P k,l = sigmoid(Ws [S k ; U l ] + bs ) (3) where [x; y] denotes the merge of x and y, and we use concatenation operation here (Dou, 2017; Chen et al., 2017). Ws and bs are parameters to be learned. To further combine the instance-topic mixture θ with P , we define the integrated memory weights as ξk = θk + γ X P k,l (4) l where γ is the pre-defined coefficient. Then, in target memory, via weighting target memory matrix T with ξ, we obtain the output representation R of the topic memory mechanism: R k = ξk T k (5) The concatenation of R and U (embedded xSeq ) further serves as the features for classification. In particular, similar to the memory networks in prior research (Sukhbaatar et al., 2015; Chen et al., 2017), our model"
D18-1351,N09-1041,0,0.0727458,"pre-trained topic mixtures are leveraged as part of features (Phan et al., 2008; Ren et al., 2016; Chen et al., 2017). Differently, our model encodes topic representations in a memory mechanism where topics are induced jointly with text classification in an end-to-end manner. 3127 Topic Models. Well-known topic models, e.g., probabilistic latent semantic analysis (pLSA) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003), have shown advantages in capturing effective semantic representations, and proven beneficial to varying downstream applications, such as summarization (Haghighi and Vanderwende, 2009) and recommendation (Zeng et al., 2018; Bai et al., 2018). For short text data, topic model variants have been proposed to reduce the effects of sparsity issues on topic modeling, such as biterm topic model (BTM) (Yan et al., 2013) and LeadLDA (Li et al., 2016b). Recently, owing to the popularity of variational auto-encoder (VAE) (Kingma and Welling, 2013), it is able to induce latent topics in neural networks, namely, neural topic models (NTM) (Miao et al., 2017; Srivastava and Sutton, 2017). Although the concept of NTM has been mentioned earlier in Cao et al. (2015), their model is based on"
D18-1351,P16-1199,1,0.931235,"nevitably compromised when directly applied to short and informal online texts. This inferior performance is attributed to the severe data sparsity nature of short texts, which results in the limited features available for classifiers (Phan et al., 2008). To alleviate the data sparsity problem, some approaches exploit knowledge from external resources like Wikipedia (Jin et al., 2011) and knowledge bases (Lucia and Ferrari, 2014; Wang et al., 2017a). These approaches, however, rely on a large volume of high-quality external data, which may be unavailable to some specific domains or languages (Li et al., 2016a). To illustrate the difficulties in classifying short texts, we take the tweet classification in Table 1 as an example. In the test instance S, only given the 11 words it contains, it is difficult to understand why its label is New.Music.Live. Without richer context, classifiers are likely to classify S into the same category as the training instance R1 , which happens to share many words with S, in spite of the different categories they belong to,1 rather than R2 , which only shares the word “wristbands” with S. Under this circumstance, how might we enrich the context of these short texts?"
D18-1351,D15-1229,0,0.0285805,"become an important form for individuals to voice opinions and share information on online platforms. A large body of dailygenerated contents, such as tweets, web search snippets, news feeds, and forum messages, have far outpaced the reading and understanding capacity of individuals. As a consequence, there is a pressing need for automatic language understanding techniques for processing and analyzing such texts (Zhang et al., 2018). Among those techniques, text classification is a critical and fundamental one proven to be useful in various downstream applications, such as text summarization (Hu et al., 2015), recommendation (Zhang et al., 2012), and sentiment analysis (Chen et al., 2017). Although many classification models like support vector machines (SVMs) (Wang and Manning, 2012) and neural networks (Kim, 2014; Xiao and Cho, 2016; Joulin et al., 2017) have demonstrated their success in processing formal and well-edited texts, such as news articles (Zhang ∗ This work was mainly conducted when Jichuan Zeng was an intern in Tencent AI Lab. † Jing Li is the corresponding author. et al., 2015b), their performance is inevitably compromised when directly applied to short and informal online texts. T"
D18-1351,P11-1016,0,0.042295,"s of prior work: short text classification and topic models. Short Text Classification. In the line of short text classification, most work focuses on alleviating the severe sparsity issues in short texts (Yan et al., 2013). Some previous efforts encode knowledge from external resource (Jin et al., 2011; Lucia and Ferrari, 2014; Wang et al., 2017a; Ma et al., 2018). Instead, our work learns effective representations only from internal data. For some specific classification tasks, such as sentiment analysis, manually-crafted features are designed to fit the target task (Pak and Paroubek, 2010; Jiang et al., 2011). Distinguished from them, we employ deep learning framework for representation learning, which requires no feature engineering process and thus ensures its general applicability to diverse classification scenarios. In comparison with the established classifiers applying deep learning methods (dos Santos and Gatti, 2014; Lee and Dernoncourt, 2016), our work differs from them in the leverage of corpus-level latent topic representations for alleviating data sparsity issues. In existing classification models using topic features, pre-trained topic mixtures are leveraged as part of features (Phan"
D18-1351,E17-2068,0,0.0340697,"derstanding capacity of individuals. As a consequence, there is a pressing need for automatic language understanding techniques for processing and analyzing such texts (Zhang et al., 2018). Among those techniques, text classification is a critical and fundamental one proven to be useful in various downstream applications, such as text summarization (Hu et al., 2015), recommendation (Zhang et al., 2012), and sentiment analysis (Chen et al., 2017). Although many classification models like support vector machines (SVMs) (Wang and Manning, 2012) and neural networks (Kim, 2014; Xiao and Cho, 2016; Joulin et al., 2017) have demonstrated their success in processing formal and well-edited texts, such as news articles (Zhang ∗ This work was mainly conducted when Jichuan Zeng was an intern in Tencent AI Lab. † Jing Li is the corresponding author. et al., 2015b), their performance is inevitably compromised when directly applied to short and informal online texts. This inferior performance is attributed to the severe data sparsity nature of short texts, which results in the limited features available for classifiers (Phan et al., 2008). To alleviate the data sparsity problem, some approaches exploit knowledge fro"
D18-1351,D14-1181,0,0.140904,"far outpaced the reading and understanding capacity of individuals. As a consequence, there is a pressing need for automatic language understanding techniques for processing and analyzing such texts (Zhang et al., 2018). Among those techniques, text classification is a critical and fundamental one proven to be useful in various downstream applications, such as text summarization (Hu et al., 2015), recommendation (Zhang et al., 2012), and sentiment analysis (Chen et al., 2017). Although many classification models like support vector machines (SVMs) (Wang and Manning, 2012) and neural networks (Kim, 2014; Xiao and Cho, 2016; Joulin et al., 2017) have demonstrated their success in processing formal and well-edited texts, such as news articles (Zhang ∗ This work was mainly conducted when Jichuan Zeng was an intern in Tencent AI Lab. † Jing Li is the corresponding author. et al., 2015b), their performance is inevitably compromised when directly applied to short and informal online texts. This inferior performance is attributed to the severe data sparsity nature of short texts, which results in the limited features available for classifiers (Phan et al., 2008). To alleviate the data sparsity prob"
D18-1351,N16-1062,0,0.0191593,", 2017a; Ma et al., 2018). Instead, our work learns effective representations only from internal data. For some specific classification tasks, such as sentiment analysis, manually-crafted features are designed to fit the target task (Pak and Paroubek, 2010; Jiang et al., 2011). Distinguished from them, we employ deep learning framework for representation learning, which requires no feature engineering process and thus ensures its general applicability to diverse classification scenarios. In comparison with the established classifiers applying deep learning methods (dos Santos and Gatti, 2014; Lee and Dernoncourt, 2016), our work differs from them in the leverage of corpus-level latent topic representations for alleviating data sparsity issues. In existing classification models using topic features, pre-trained topic mixtures are leveraged as part of features (Phan et al., 2008; Ren et al., 2016; Chen et al., 2017). Differently, our model encodes topic representations in a memory mechanism where topics are induced jointly with text classification in an end-to-end manner. 3127 Topic Models. Well-known topic models, e.g., probabilistic latent semantic analysis (pLSA) (Hofmann, 1999) and latent Dirichlet alloca"
D18-1351,D12-1020,0,0.0304616,"as sport because of the occurrence “super bowl”. In future work, we would exploit context-sensitive topical word embeddings (Witt et al., 2016), which is able to distinguish the meanings of the same word in different contexts. Another main error type comes from the failure to capture phrase-level semantics. Taking “On the merits of face time and living small” as an example, without understanding “face time” as a phrase, our model wrongly predicts its category as business instead of its correct label as sci tech. Such errors can be reduced by enhancing our NTM to phrase discovery topic models (Lindsey et al., 2012; He, 2016), which is worthy exploring in future work. 5 Related Work Our work mainly builds on two streams of prior work: short text classification and topic models. Short Text Classification. In the line of short text classification, most work focuses on alleviating the severe sparsity issues in short texts (Yan et al., 2013). Some previous efforts encode knowledge from external resource (Jin et al., 2011; Lucia and Ferrari, 2014; Wang et al., 2017a; Ma et al., 2018). Instead, our work learns effective representations only from internal data. For some specific classification tasks, such as s"
D18-1351,P17-1001,0,0.0513716,"Missing"
D18-1351,S10-1097,0,0.0173493,"nly builds on two streams of prior work: short text classification and topic models. Short Text Classification. In the line of short text classification, most work focuses on alleviating the severe sparsity issues in short texts (Yan et al., 2013). Some previous efforts encode knowledge from external resource (Jin et al., 2011; Lucia and Ferrari, 2014; Wang et al., 2017a; Ma et al., 2018). Instead, our work learns effective representations only from internal data. For some specific classification tasks, such as sentiment analysis, manually-crafted features are designed to fit the target task (Pak and Paroubek, 2010; Jiang et al., 2011). Distinguished from them, we employ deep learning framework for representation learning, which requires no feature engineering process and thus ensures its general applicability to diverse classification scenarios. In comparison with the established classifiers applying deep learning methods (dos Santos and Gatti, 2014; Lee and Dernoncourt, 2016), our work differs from them in the leverage of corpus-level latent topic representations for alleviating data sparsity issues. In existing classification models using topic features, pre-trained topic mixtures are leveraged as pa"
D18-1351,D14-1162,0,0.0845986,"43 0.846 0.844 0.073 0.070 0.159 0.232 0.261 0.375 0.381 0.385 0.382 0.010 0.009 0.111 0.164 0.177 0.348 0.362 0.368 0.365 0.102 0.116 0.192 0.331 0.379 0.547 0.553 0.537 0.556 0.019 0.039 0.147 0.277 0.348 0.547 0.550 0.532 0.556 0.961 0.964 0.961 0.964 0.848 0.851 0.847 0.851 0.394 0.397 0.386 0.375 0.568 0.591 0.569 0.589 Table 3: Comparisons of accuracy (Acc) and average F1 (Avg F1) on four benchmark datasets. Our TMN, either with separate or joint TM inference, performs significantly better than all the comparisons (p &lt; 0.05, paired t-test). datasets, we use pre-trained GloVe embeddings (Pennington et al., 2014)11 . For Twitter and Weibo datasets, we pre-train embeddings on largescale external data with 99M tweets and 467M Weibo messages, respectively. For the number of topics, we follow previous settings (Yan et al., 2013; Das et al., 2015; Dieng et al., 2016) to set K = 50. For all the other hyperparameters, we tune them on the development set by grid search. For our classifier, we employ CNN in experiment because of its better performance in short text classification than its counterparts such as RNN (Wang et al., 2017a). The hidden size of CNN is set as 500. The dimension of word embedding E = 20"
D18-1351,P13-4009,0,0.0603772,"Missing"
D18-1351,C14-1008,0,0.0808924,"Missing"
D18-1351,D17-1029,0,0.20897,"hang ∗ This work was mainly conducted when Jichuan Zeng was an intern in Tencent AI Lab. † Jing Li is the corresponding author. et al., 2015b), their performance is inevitably compromised when directly applied to short and informal online texts. This inferior performance is attributed to the severe data sparsity nature of short texts, which results in the limited features available for classifiers (Phan et al., 2008). To alleviate the data sparsity problem, some approaches exploit knowledge from external resources like Wikipedia (Jin et al., 2011) and knowledge bases (Lucia and Ferrari, 2014; Wang et al., 2017a). These approaches, however, rely on a large volume of high-quality external data, which may be unavailable to some specific domains or languages (Li et al., 2016a). To illustrate the difficulties in classifying short texts, we take the tweet classification in Table 1 as an example. In the test instance S, only given the 11 words it contains, it is difficult to understand why its label is New.Music.Live. Without richer context, classifiers are likely to classify S into the same category as the training instance R1 , which happens to share many words with S, in spite of the different categori"
D18-1351,P12-2018,0,0.718444,"ippets, news feeds, and forum messages, have far outpaced the reading and understanding capacity of individuals. As a consequence, there is a pressing need for automatic language understanding techniques for processing and analyzing such texts (Zhang et al., 2018). Among those techniques, text classification is a critical and fundamental one proven to be useful in various downstream applications, such as text summarization (Hu et al., 2015), recommendation (Zhang et al., 2012), and sentiment analysis (Chen et al., 2017). Although many classification models like support vector machines (SVMs) (Wang and Manning, 2012) and neural networks (Kim, 2014; Xiao and Cho, 2016; Joulin et al., 2017) have demonstrated their success in processing formal and well-edited texts, such as news articles (Zhang ∗ This work was mainly conducted when Jichuan Zeng was an intern in Tencent AI Lab. † Jing Li is the corresponding author. et al., 2015b), their performance is inevitably compromised when directly applied to short and informal online texts. This inferior performance is attributed to the severe data sparsity nature of short texts, which results in the limited features available for classifiers (Phan et al., 2008). To a"
D18-1351,N18-1035,1,0.643443,"features (Phan et al., 2008; Ren et al., 2016; Chen et al., 2017). Differently, our model encodes topic representations in a memory mechanism where topics are induced jointly with text classification in an end-to-end manner. 3127 Topic Models. Well-known topic models, e.g., probabilistic latent semantic analysis (pLSA) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003), have shown advantages in capturing effective semantic representations, and proven beneficial to varying downstream applications, such as summarization (Haghighi and Vanderwende, 2009) and recommendation (Zeng et al., 2018; Bai et al., 2018). For short text data, topic model variants have been proposed to reduce the effects of sparsity issues on topic modeling, such as biterm topic model (BTM) (Yan et al., 2013) and LeadLDA (Li et al., 2016b). Recently, owing to the popularity of variational auto-encoder (VAE) (Kingma and Welling, 2013), it is able to induce latent topics in neural networks, namely, neural topic models (NTM) (Miao et al., 2017; Srivastava and Sutton, 2017). Although the concept of NTM has been mentioned earlier in Cao et al. (2015), their model is based on matrix factorization. Differently, VAE"
D18-1351,Y15-1009,0,0.0219951,"o all test instances. We further compare with the widely-used baseline SVM+BOW, SVM with unigram features (Wang and Manning, 2012). We also consider other SVM-based baselines: SVM+LDA, SVM+BTM, SVM+NTM, whose features are topic distributions for instances learned by LDA (Blei et al., 2003), BTM (Yan et al., 2013), and NTM (Miao et al., 2017), respectively. In particular, BTM is one of the state-ofthe-art topic models for short texts. To compare with neural classifiers, we test bidirectional long 11 http://nlp.stanford.edu/data/glove. 6B.zip (200d) short-term memory with attention (AttBiLSTM) (Zhang et al., 2015a) and convolutional neural network (CNN) classifiers (Kim, 2014). No topic representation is encoded in these two classifiers. We also compare with the state-of-the-art shorttext classifier CNN+TEWE (Ren et al., 2016), i.e., CNN classifier with topic-enriched word embeddings (TEWE), where the word embeddings are enriched by pre-trained NTM-inferred topic models. Moreover, to investigate the effectiveness of our proposed topic memory mechanism, we compare with CNN+NTM, which concatenates the representations learned by CNN and topics induced by NTM as classification features. In addition, we co"
D18-1351,N18-1151,1,0.787289,"enotes the i-th training instance; S denotes a test instance. [class] is the ground-truth label. Bold words are indicative of an instance’s class label. Introduction Short texts have become an important form for individuals to voice opinions and share information on online platforms. A large body of dailygenerated contents, such as tweets, web search snippets, news feeds, and forum messages, have far outpaced the reading and understanding capacity of individuals. As a consequence, there is a pressing need for automatic language understanding techniques for processing and analyzing such texts (Zhang et al., 2018). Among those techniques, text classification is a critical and fundamental one proven to be useful in various downstream applications, such as text summarization (Hu et al., 2015), recommendation (Zhang et al., 2012), and sentiment analysis (Chen et al., 2017). Although many classification models like support vector machines (SVMs) (Wang and Manning, 2012) and neural networks (Kim, 2014; Xiao and Cho, 2016; Joulin et al., 2017) have demonstrated their success in processing formal and well-edited texts, such as news articles (Zhang ∗ This work was mainly conducted when Jichuan Zeng was an inte"
D18-1423,K16-1002,0,0.064903,"in a topic, especially when they are generated or extracted from an inventory (Wang et al., 2016c). On the other hand, Chinese poems are generally short in length, with every character carefully chosen to be concise and elegant. Yet, prior poem generation models with recurrent neural networks (RNN) are likely to generate highfrequency characters (Zhang et al., 2017a), and the resulted poems are trivial and boring. The reason is that RNN tends to be entrapped within local word co-occurrences, they normally fail to capture global characteristic such as topic or hierarchical semantic properties (Bowman et al., 2016). To address the aforementioned shortcomings, RNN is extended to autoencoder (Dai and Le, 2015) for improving sequence learning, which has *Corresponding author: Rui Yan (ruiyan@pku.edu.cn) †Work was partially done at Tencent AI Lab. 1 We use term and character interchangeably in this paper. 3890 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3890–3900 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics been proven to be appealing in explicitly modeling global properties such as syntactic, semantic, a"
D18-1423,D16-1126,0,0.031283,"s confirm the validity and effectiveness of our model, where its automatic and human evaluation scores outperform existing models. 1 Introduction In mastering concise, elegant wordings with aesthetic rhythms in fixed patterns, classical Chinese poem is a special cultural heritage to record personal emotions and political views, as well as document daily or historical events. Being a fascinating art, writing poems is an attractive task that researchers of artificial intelligence are interested in (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2012; Yan et al., 2013, 2016a; Ghazvininejad et al., 2016, 2017; Singh et al., 2017; Xu et al., 2018), partially for the reason that poem generation and its related research could benefit other constrained natural language generation tasks. Conventionally, rule-based models (Zhou et al., 2010) and statistical machine translation (SMT) models (He et al., 2012) are proposed for this task. Recently, deep neural models are employed to generate fluent and natural poems (Wang et al., 2016a; Yan, 2016; Zhang et al., 2017a). Although these models look promising, they are limited in many aspects, e.g., previous studies generally fail to keep thematic consist"
D18-1423,P17-4008,0,0.0532257,"Missing"
D18-1423,P17-1059,0,0.0269444,"e the reconstruction log-likelihood of the input x under the condition of c. Following the operation for VAE, we have the corresponding variational lower bound of pθ (x|c) formulated as L(θ, φ; x, c) = − KL(qφ (z|x, c) k pθ (z|c)) + Eqφ (z|x,c) [log pθ (x|z, c)] (2) which is similar to Eq.1 except that all items are introduced with c, such as qφ (z|x, c) and pθ (z|c), 3891 referring to the conditioned approximate posterior and the conditioned prior, respectively. 2.2 Problem Formulation Following the text-to-text generation paradigm (Ranzato et al., 2015; Kiddon et al., 2016; Hu et al., 2017; Ghosh et al., 2017), our task has a similar problem setting with conventional studies (Zhang and Lapata, 2014; Wang et al., 2016c), where a poem is generated in a line-by-line manner that each line serves as the input for the next one, as illustrated in Figure 1. To formulate this task, we separate its input and output with necessary notations as follows. The I NPUT of the entire model is a title, T =(e1 ,e2 ,. . . ,eN ), functionalized as the theme of the target poem2 , where ei refers to i-the character’s embedding and N is the length of the title. The first line L1 is generated only conditioned on the title T"
D18-1423,D10-1051,0,0.142401,"ion with (Hu et al., 2017; Yu et al., 2017; Lin et al., 2017; Zhang et al., 2017b; Guo et al., 2018). To the best of our knowledge, this work is the first one integrating CVAE and adversarial training with a discriminator for text generation, especially in a particular text genre, poetry. Automatic Poem Generation. According to methodology, previous approaches can be roughly classified into three categories: 1) rule and template based methods (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Zhou et al., 2010; Oliveira, 2012; Yan et al., 2013); 2) SMT approaches (Jiang and Zhou, 2008; Greene et al., 2010; He et al., 2012); 3) deep neural models (Zhang and Lapata, 2014; Wang et al., 2016b; Yan, 2016). Compared to rule-based and SMT models, neural models are able to learn more complicated representations and generate smooth poems. Most recent studies followed this paradigm. For example, Wang et al. (2016c) proposed a modified encoder-decoder model with keyword planning; Zhang et al. (2017a) adopted memory-augmented RNNs to dynamically choose each term from RNN output or a reserved inventory. To improve thematic consistency, Yang et al. (2017) combined CVAE and keywords planning. Compared to the"
D18-1423,P17-1016,0,0.0172345,"(Wang et al., 2016c; Yang et al., 2017) and improve term1 novelty (Zhang et al., 2017a), which are important characteristics of poems. In classical Chinese poem composing, thematic consistency and term novelty are usually mutually exclusive conditions to each other, i.e., consistent lines may bring duplicated terms while intriguing choices of characters could result in thematic diversities. On one hand, thematic consistency is essential for poems; it is preferred that all lines concentrate on the same theme throughout a poem. Previous work mainly focused on using keywords (Wang et al., 2016c; Hopkins and Kiela, 2017) to plan a poem so as to generate each line with a specific keyword. Such strategy is risky for the reason that the keywords are not guaranteed consistent in a topic, especially when they are generated or extracted from an inventory (Wang et al., 2016c). On the other hand, Chinese poems are generally short in length, with every character carefully chosen to be concise and elegant. Yet, prior poem generation models with recurrent neural networks (RNN) are likely to generate highfrequency characters (Zhang et al., 2017a), and the resulted poems are trivial and boring. The reason is that RNN tend"
D18-1423,C08-1048,0,0.0401591,"2017) and text generation with (Hu et al., 2017; Yu et al., 2017; Lin et al., 2017; Zhang et al., 2017b; Guo et al., 2018). To the best of our knowledge, this work is the first one integrating CVAE and adversarial training with a discriminator for text generation, especially in a particular text genre, poetry. Automatic Poem Generation. According to methodology, previous approaches can be roughly classified into three categories: 1) rule and template based methods (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Zhou et al., 2010; Oliveira, 2012; Yan et al., 2013); 2) SMT approaches (Jiang and Zhou, 2008; Greene et al., 2010; He et al., 2012); 3) deep neural models (Zhang and Lapata, 2014; Wang et al., 2016b; Yan, 2016). Compared to rule-based and SMT models, neural models are able to learn more complicated representations and generate smooth poems. Most recent studies followed this paradigm. For example, Wang et al. (2016c) proposed a modified encoder-decoder model with keyword planning; Zhang et al. (2017a) adopted memory-augmented RNNs to dynamically choose each term from RNN output or a reserved inventory. To improve thematic consistency, Yang et al. (2017) combined CVAE and keywords plan"
D18-1423,D16-1032,0,0.0321585,"e objective of CVAE is thus to maximize the reconstruction log-likelihood of the input x under the condition of c. Following the operation for VAE, we have the corresponding variational lower bound of pθ (x|c) formulated as L(θ, φ; x, c) = − KL(qφ (z|x, c) k pθ (z|c)) + Eqφ (z|x,c) [log pθ (x|z, c)] (2) which is similar to Eq.1 except that all items are introduced with c, such as qφ (z|x, c) and pθ (z|c), 3891 referring to the conditioned approximate posterior and the conditioned prior, respectively. 2.2 Problem Formulation Following the text-to-text generation paradigm (Ranzato et al., 2015; Kiddon et al., 2016; Hu et al., 2017; Ghosh et al., 2017), our task has a similar problem setting with conventional studies (Zhang and Lapata, 2014; Wang et al., 2016c), where a poem is generated in a line-by-line manner that each line serves as the input for the next one, as illustrated in Figure 1. To formulate this task, we separate its input and output with necessary notations as follows. The I NPUT of the entire model is a title, T =(e1 ,e2 ,. . . ,eN ), functionalized as the theme of the target poem2 , where ei refers to i-the character’s embedding and N is the length of the title. The first line L1 is gen"
D18-1423,N16-1014,0,0.0818547,"is paper. Similarity: For thematic consistency, it is challenging to automatically evaluate different models. We adopt the embedding average metric to score sentence-level similarity as that was applied in Wieting et al. (2015). In this paper, we accumulate the embeddings of all characters from the generated poems and that from the given title, and use cosine to compute the similarity between the two accumulated embeddings. Distinctness: As an important characteristic, poems use novel and unique characters to maintain their elegance and delicacy. Similar to that proposed for dialogue systems (Li et al., 2016), this evaluation is employed to measure character diversity by calculating the proportion of distinctive [1,4]-grams12 in the generated poems, where final distinctness values are normalized to [0,100]. Human Evaluation: Since writing poems is a complicated task, there always exist incoordinations between automatic metrics and human experiences. Hence, we conduct human evaluation to 11 We tried different values for λ, varying from 0.001 to 1, which result in similar performance of the CVAE-D. 12 Defined as the number of distinctive n-grams divided by the total number of n-grams, shown as Dist-"
D18-1423,P15-1107,0,0.0609514,"Missing"
D18-1423,D17-1230,0,0.0259758,"s may deliver different mood from others. Since our model does not explicitly control such attributes, thus one potential solution to address this issue is to introduce other features to model such information, which requires a special design to adjust the current model. We also notice there exists a few extraordinary bad cases where their basic characteristics, such as wording, fluency, etc., are unacceptable. This phenomenon is randomly observed with no patterns, which could be explained by the complexity of the model and the fragile natural of adversarial training (Goodfellow et al., 2014; Li et al., 2017). Careful parameter setting and considerate module assemble could mitigate this problem, thus lead to potential future work of designing more robust frameworks. 6 Related Work Deep Generative Models. This work can be seen as an extension of research on deep generative models (Salakhutdinov and Hinton, 2009; Bengio et al., 2014), where most of the previous work, including VAE and CVAE, focused on image generation (Sohn et al., 2015; Yan et al., 2016b). Since GAN (Goodfellow et al., 2014) is also a successful generative model, there are studies tried to integrate VAE and GAN (Larsen et al., 2016"
D18-1423,P02-1040,0,0.102408,"2.16 2.14 2.58 2.29 2.08 2.53 2.35 2.34 2.96 Ovr. 1.74 1.79 2.13 2.23 2.13 2.14 2.18 2.56 Table 3: Results of automatic and human evaluations. BLEU-1 and BLEU-2 are BLEU scores on unigrams and bigrams (p < 0.01); Sim refer to the similarity score; Dist-n corresponds to the distinctness of n-gram, with n = 1 to 4; Con., Flu., Mea., Poe., Ovr. represent consistency, fluency, meaning, poeticness, and overall, respectively. 2017) to set the balancing parameter λ to 0.1.11 4.4 Evaluation Metrics To comprehensively evaluate the generated poems, we employ the following metrics: BLEU: The BLEU score (Papineni et al., 2002) is an effective metric, widely used in machine translation, for measuring word overlapping between ground truth and generated sentences. In poem generation, BLEU is also utilized as a metric in previous studies (Zhang and Lapata, 2014; Wang et al., 2016a; Yan, 2016; Wang et al., 2016b). We follow their settings in this paper. Similarity: For thematic consistency, it is challenging to automatically evaluate different models. We adopt the embedding average metric to score sentence-level similarity as that was applied in Wieting et al. (2015). In this paper, we accumulate the embeddings of all c"
D18-1423,C16-1100,0,0.272591,"that researchers of artificial intelligence are interested in (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2012; Yan et al., 2013, 2016a; Ghazvininejad et al., 2016, 2017; Singh et al., 2017; Xu et al., 2018), partially for the reason that poem generation and its related research could benefit other constrained natural language generation tasks. Conventionally, rule-based models (Zhou et al., 2010) and statistical machine translation (SMT) models (He et al., 2012) are proposed for this task. Recently, deep neural models are employed to generate fluent and natural poems (Wang et al., 2016a; Yan, 2016; Zhang et al., 2017a). Although these models look promising, they are limited in many aspects, e.g., previous studies generally fail to keep thematic consistency (Wang et al., 2016c; Yang et al., 2017) and improve term1 novelty (Zhang et al., 2017a), which are important characteristics of poems. In classical Chinese poem composing, thematic consistency and term novelty are usually mutually exclusive conditions to each other, i.e., consistent lines may bring duplicated terms while intriguing choices of characters could result in thematic diversities. On one hand, thematic consisten"
D18-1423,P17-1046,0,0.0143534,"aintaining the advantages of VAE. It is verified in supervised dialogue generation (Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017) that CVAE can generate better responses with given dialogue contexts. Given the above background and to align it with our expectations for poem generation, it is worth trying to apply CVAE to create poems. In the meantime, consider that modeling thematic consistency with adversarial training is proven to be promising in controlled text generation (Hu et al., 2017), models for semantic matching can be potentially improved with an explicit discriminator (Wu et al., 2017), so does poem generation. In this paper, we propose a novel poem generation model (CVAE-D) using CVAE to generate novel terms and a discriminator (D) to explicitly control thematic consistency with adversarial training. To the best of our knowledge, this is the first work of generating poems with the combination of CVAE and adversarial training. Experiments on a large classical Chinese poetry corpus confirm that, through encoding inputs with latent variables and explicit measurement of thematic information, the proposed model outperforms existing ones in various evaluations. Quantitative and"
D18-1423,P17-1125,0,0.200231,"intelligence are interested in (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2012; Yan et al., 2013, 2016a; Ghazvininejad et al., 2016, 2017; Singh et al., 2017; Xu et al., 2018), partially for the reason that poem generation and its related research could benefit other constrained natural language generation tasks. Conventionally, rule-based models (Zhou et al., 2010) and statistical machine translation (SMT) models (He et al., 2012) are proposed for this task. Recently, deep neural models are employed to generate fluent and natural poems (Wang et al., 2016a; Yan, 2016; Zhang et al., 2017a). Although these models look promising, they are limited in many aspects, e.g., previous studies generally fail to keep thematic consistency (Wang et al., 2016c; Yang et al., 2017) and improve term1 novelty (Zhang et al., 2017a), which are important characteristics of poems. In classical Chinese poem composing, thematic consistency and term novelty are usually mutually exclusive conditions to each other, i.e., consistent lines may bring duplicated terms while intriguing choices of characters could result in thematic diversities. On one hand, thematic consistency is essential for poems; it is"
D18-1423,D14-1074,0,0.518039,"the operation for VAE, we have the corresponding variational lower bound of pθ (x|c) formulated as L(θ, φ; x, c) = − KL(qφ (z|x, c) k pθ (z|c)) + Eqφ (z|x,c) [log pθ (x|z, c)] (2) which is similar to Eq.1 except that all items are introduced with c, such as qφ (z|x, c) and pθ (z|c), 3891 referring to the conditioned approximate posterior and the conditioned prior, respectively. 2.2 Problem Formulation Following the text-to-text generation paradigm (Ranzato et al., 2015; Kiddon et al., 2016; Hu et al., 2017; Ghosh et al., 2017), our task has a similar problem setting with conventional studies (Zhang and Lapata, 2014; Wang et al., 2016c), where a poem is generated in a line-by-line manner that each line serves as the input for the next one, as illustrated in Figure 1. To formulate this task, we separate its input and output with necessary notations as follows. The I NPUT of the entire model is a title, T =(e1 ,e2 ,. . . ,eN ), functionalized as the theme of the target poem2 , where ei refers to i-the character’s embedding and N is the length of the title. The first line L1 is generated only conditioned on the title T , once this step is done, the model takes the input of the previous generated line as wel"
D18-1423,Q17-1036,0,0.166232,"intelligence are interested in (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2012; Yan et al., 2013, 2016a; Ghazvininejad et al., 2016, 2017; Singh et al., 2017; Xu et al., 2018), partially for the reason that poem generation and its related research could benefit other constrained natural language generation tasks. Conventionally, rule-based models (Zhou et al., 2010) and statistical machine translation (SMT) models (He et al., 2012) are proposed for this task. Recently, deep neural models are employed to generate fluent and natural poems (Wang et al., 2016a; Yan, 2016; Zhang et al., 2017a). Although these models look promising, they are limited in many aspects, e.g., previous studies generally fail to keep thematic consistency (Wang et al., 2016c; Yang et al., 2017) and improve term1 novelty (Zhang et al., 2017a), which are important characteristics of poems. In classical Chinese poem composing, thematic consistency and term novelty are usually mutually exclusive conditions to each other, i.e., consistent lines may bring duplicated terms while intriguing choices of characters could result in thematic diversities. On one hand, thematic consistency is essential for poems; it is"
D18-1423,P17-1061,0,0.472874,"obal properties such as syntactic, semantic, and discourse coherence (Li et al., 2015). Moreover, boosting autoencoder with variational inference (Kingma and Welling, 2014), known as variational autoencoder (VAE), can generate not only consistent but also novel and fluent term sequences (Bowman et al., 2016). To generalize VAE for versatile scenarios, conditional variational autoencoders (CVAE) are proposed to supervise a generation process with certain attributes while maintaining the advantages of VAE. It is verified in supervised dialogue generation (Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017) that CVAE can generate better responses with given dialogue contexts. Given the above background and to align it with our expectations for poem generation, it is worth trying to apply CVAE to create poems. In the meantime, consider that modeling thematic consistency with adversarial training is proven to be promising in controlled text generation (Hu et al., 2017), models for semantic matching can be potentially improved with an explicit discriminator (Wu et al., 2017), so does poem generation. In this paper, we propose a novel poem generation model (CVAE-D) using CVAE to generate novel terms"
D18-1423,P16-1222,1,0.843427,"with no patterns, which could be explained by the complexity of the model and the fragile natural of adversarial training (Goodfellow et al., 2014; Li et al., 2017). Careful parameter setting and considerate module assemble could mitigate this problem, thus lead to potential future work of designing more robust frameworks. 6 Related Work Deep Generative Models. This work can be seen as an extension of research on deep generative models (Salakhutdinov and Hinton, 2009; Bengio et al., 2014), where most of the previous work, including VAE and CVAE, focused on image generation (Sohn et al., 2015; Yan et al., 2016b). Since GAN (Goodfellow et al., 2014) is also a successful generative model, there are studies tried to integrate VAE and GAN (Larsen et al., 2016). In natural language processing, many recent deep generative models are applied to dialogue systems Serban et al. (2017); Shen et al. (2017); Zhao et al. (2017) and text generation with (Hu et al., 2017; Yu et al., 2017; Lin et al., 2017; Zhang et al., 2017b; Guo et al., 2018). To the best of our knowledge, this work is the first one integrating CVAE and adversarial training with a discriminator for text generation, especially in a particular tex"
D18-1442,N18-1158,0,0.411279,"the focus on abstractive summarization, extractive summarization remains an attractive method as it is capable of generating more grammatically and semantically correct summaries. This is the method we follow in this work. In extractive summarization, Cheng and Lapata (2016) propose a general framework for single-document text summarization using a hierarchical article encoder composed with an attention-based extractor. Following this, Nallapati et al. (2016a) propose a simple RNN-based sequence classifier which outperforms or matches the state-of-art models at the time. In another approach, Narayan et al. (2018) use a reinforcement learning method to optimize the Rouge evaluation metric for text summarization. The most recent work on this topic is (Wu and Hu, 2018), where the authors train a reinforced neural extractive summarization model called RNES that captures cross-sentence coherence patterns. Due to the fact that they use a different dataset and have not released their code, we are unable to compare our models with theirs. The idea of iteration has not been well explored for summarization. One related study is Xiong et al. (2016)’s work on dynamic memory networks, which designs neural networks"
D18-1442,D14-1162,0,0.0836015,"ed our model in Tensorflow (Abadi et al., 2016). The code for our models is available online1 . We mostly followed the settings in (Nallapati et al., 2016a) and trained the model using the Adam optimizer (Kingma and Ba, 2014) with initial learning rate 0.001 and anneals of 0.5 every 6 epochs until reaching 30 epochs. We selected three sentences with highest scores as summary. After preliminary exploration, we found that arranging them according to their scores consistently achieved the best performance. Experiments were performed with a batch size of 64 documents. We used 100-dimension GloVe (Pennington et al., 2014) embeddings trained on Wikipedia 2014 as our embedding initialization with a vocabulary size limited to 100k for speed purposes. We initialized out-of-vocabulary word embeddings over a uniform distribution within [0.2,0,2]. We also padded or cut sentences to contain exactly 70 words. Each GRU module had 1 layer with 200-dimensional hidden states and with either an initial state set up as described above or a random initial state. To prevent overfitting, we used dropout after each GRU network and embedding layer, and also applied L2 loss to all unbiased variables. The iteration number was set t"
D18-1442,radev-etal-2004-mead,0,0.215319,"icle, removing secondary or redundant concepts. Nowadays as there is a growing need for storing and digesting large amounts of textual data, automatic summarization systems have significant usage potential in society. Extractive summarization is a technique for generating summaries by directly choosing a subset of salient sentences from the original document to constitute the summary. Most efforts made towards extractive summarization either rely ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) on human-engineered features such as sentence length, word position, and frequency (Cohen, 2002; Radev et al., 2004; Woodsend and Lapata, 2010; Yan et al., 2011a,b, 2012) or use neural networks to automatically learn features for sentence selection (Cheng and Lapata, 2016; Nallapati et al., 2016a). Although existing extractive summarization methods have achieved great success, one limitation they share is that they generate the summary after only one pass through the document. However, in real-world human cognitive processes, people read a document multiple times in order to capture the main ideas. Browsing through the document only once often means the model cannot fully get at the document’s main ideas,"
D18-1442,W05-0620,0,0.154828,"Missing"
D18-1442,D15-1044,0,0.0610103,"an be classified into extractive summarization and abstractive summarization. Extractive summarization aims to generate a summary by integrating the most salient sentences in the document. Abstractive summarization aims to generate new content that concisely paraphrases the document from scratch. With the emergence of powerful neural network models for text processing, a vast majority of the literature on document summarization is dedicated to abstractive summarization. These models typically take the form of convolutional neural networks (CNN) or recurrent neural networks (RNN). For example, Rush et al. (2015) propose an encoder-decoder model which uses a local attention mechanism to generate summaries. Nallapati et al. (2016b) further develop this work by addressing problems that had not been adequately solved by the basic architecture, such as keyword modeling and capturing the hierarchy of sentenceto-word structures. In a follow-up work, Nallapati et al. (2017) propose a new summarization model which generates summaries by sampling a topic one sentence at a time, then producing words using an RNN decoder conditioned on the sentence topic. Another related work is by See et al. (2017), where the a"
D18-1442,P16-1046,0,0.554299,"mmarization systems have significant usage potential in society. Extractive summarization is a technique for generating summaries by directly choosing a subset of salient sentences from the original document to constitute the summary. Most efforts made towards extractive summarization either rely ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) on human-engineered features such as sentence length, word position, and frequency (Cohen, 2002; Radev et al., 2004; Woodsend and Lapata, 2010; Yan et al., 2011a,b, 2012) or use neural networks to automatically learn features for sentence selection (Cheng and Lapata, 2016; Nallapati et al., 2016a). Although existing extractive summarization methods have achieved great success, one limitation they share is that they generate the summary after only one pass through the document. However, in real-world human cognitive processes, people read a document multiple times in order to capture the main ideas. Browsing through the document only once often means the model cannot fully get at the document’s main ideas, leading to a subpar summarization. We share two examples of this. (1) Consider the situation where we almost finish reading a long article and forget some ma"
D18-1442,E17-2007,0,0.0838447,"Missing"
D18-1442,P17-1099,0,0.114979,"or example, Rush et al. (2015) propose an encoder-decoder model which uses a local attention mechanism to generate summaries. Nallapati et al. (2016b) further develop this work by addressing problems that had not been adequately solved by the basic architecture, such as keyword modeling and capturing the hierarchy of sentenceto-word structures. In a follow-up work, Nallapati et al. (2017) propose a new summarization model which generates summaries by sampling a topic one sentence at a time, then producing words using an RNN decoder conditioned on the sentence topic. Another related work is by See et al. (2017), where the authors use “pointing” and “coverage” techniques to generate more accurate summaries. Despite the focus on abstractive summarization, extractive summarization remains an attractive method as it is capable of generating more grammatically and semantically correct summaries. This is the method we follow in this work. In extractive summarization, Cheng and Lapata (2016) propose a general framework for single-document text summarization using a hierarchical article encoder composed with an attention-based extractor. Following this, Nallapati et al. (2016a) propose a simple RNN-based se"
D18-1442,P10-1058,0,0.110183,"dary or redundant concepts. Nowadays as there is a growing need for storing and digesting large amounts of textual data, automatic summarization systems have significant usage potential in society. Extractive summarization is a technique for generating summaries by directly choosing a subset of salient sentences from the original document to constitute the summary. Most efforts made towards extractive summarization either rely ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) on human-engineered features such as sentence length, word position, and frequency (Cohen, 2002; Radev et al., 2004; Woodsend and Lapata, 2010; Yan et al., 2011a,b, 2012) or use neural networks to automatically learn features for sentence selection (Cheng and Lapata, 2016; Nallapati et al., 2016a). Although existing extractive summarization methods have achieved great success, one limitation they share is that they generate the summary after only one pass through the document. However, in real-world human cognitive processes, people read a document multiple times in order to capture the main ideas. Browsing through the document only once often means the model cannot fully get at the document’s main ideas, leading to a subpar summari"
D18-1442,W04-3212,0,0.0184597,"tructed sentence representations. The iterative unit (also depicted above in Fig.1) is designed for this purpose. We use a GRUiter cell to generate the polished document representation, whose input is the final state of the selective reading network from the previous iteration, hns and whose initial state is set to the document representation of the previous iteration, Dk−1 . The updated document representation is computed by: Dk = GRUiter (hns , Dk−1 ) 4.3 (14) Decoder Next, we describe our decoders, which are depicted shaded in the right part of Fig.1. Following most sequence labeling task (Xue and Palmer, 2004; Carreras and M`arquez, 2005) where they learn a feature vector for each sentence, we use a bidirectional GRUdec network in each iteration to output features so as to calculate extracting probabilities. For k-th iteration, given the sentence rep→ resentation ← s as input and the document representation Dk as the initial state, our decoder encodes the features of all sentences in the hidden state hk = {hk0 , ..., hkns }: 4.4 → hki = GRUdec (← s , hki−1 ) (15) hk0 (16) = Dk Sentence Labeling Module Next, we use the feature of each sentence to generate corresponding extracting probability. Since"
D18-1442,P16-1222,1,0.822294,"ence patterns. Due to the fact that they use a different dataset and have not released their code, we are unable to compare our models with theirs. The idea of iteration has not been well explored for summarization. One related study is Xiong et al. (2016)’s work on dynamic memory networks, which designs neural networks with memory and attention mechanisms that exhibit certain reasoning capabilities required for question answering. Another related work is (Yan, 2016), where they generate poetry with iterative polishing sn chema. Similiar method can also be applied on couplet generation as in (Yan et al., 2016). We take some inspiration from their work but focus on document summarization. Another related work is (Singh et al., 2017), where the authors present a deep network called Hybrid MemNet for the single document summarization task, using a memory network as the document encoder. Compared to them, we do not borrow the memory network structure but propose a new iterative architecture. 4089 3 3.1 Methodology Problem Formulation In this work, we propose Iterative Text Summarization (ITS), an iteration-based supervised model for extractive text summarization. We treat the extractive summarization t"
D18-1442,D11-1124,1,0.758224,"Nowadays as there is a growing need for storing and digesting large amounts of textual data, automatic summarization systems have significant usage potential in society. Extractive summarization is a technique for generating summaries by directly choosing a subset of salient sentences from the original document to constitute the summary. Most efforts made towards extractive summarization either rely ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) on human-engineered features such as sentence length, word position, and frequency (Cohen, 2002; Radev et al., 2004; Woodsend and Lapata, 2010; Yan et al., 2011a,b, 2012) or use neural networks to automatically learn features for sentence selection (Cheng and Lapata, 2016; Nallapati et al., 2016a). Although existing extractive summarization methods have achieved great success, one limitation they share is that they generate the summary after only one pass through the document. However, in real-world human cognitive processes, people read a document multiple times in order to capture the main ideas. Browsing through the document only once often means the model cannot fully get at the document’s main ideas, leading to a subpar summarization. We share t"
D19-1300,P18-1063,0,0.0315907,"ssifier while Yasunaga et al. (2017) computes the salience of each sentence for selection with graph convolutional networks. In addition, reinforcement learning based methods (Wu and Hu, 2018; Narayan et al., 2018; Yao et al., 2018) have been proposed to directly optimize the evaluation metric ROUGE by combining cross-entropy loss with rewards from policy gradient reinforcement learning. Dong et al. (2018) considered extractive summarization as a contextual bandit and it performs well especially when good summary sentences appear late in the source document. Recently, Nallapati et al. (2017); Chen and Bansal (2018); Hsu et al. (2018) propose unified models and combine the advantages of both extractive and abstractive methods. Human Reading-inspired Strategy in NLP Recently, several pioneer researches began to study how to adapt human reading cognition process, usually including pre-reading, reading and post-reading (Avery and Graves, 1997; Saricoban, 3040 2002; Toprak and Almacıo˘glu, 2009; Pressley and Afflerbach, 2012), into various NLP-related applications. For example, Li et al. (2018) solved document-based question answering and by simulating human being’s reading strategy. Luo et al. (2018, 2019)"
D19-1300,P16-1046,0,0.70783,"aried length, and significantly outperform the state-of-the-art extractive methods in terms of ROUGE metrics. 1 Introduction Automatic text summarization has wide popularity in NLP applications such as producing digests, headlines and reports. Among the supervised methods, two main types are usually explored, namely abstractive and extractive summarizations (Nenkova et al., 2011). Compared with abstractive approaches, extractive methods are more practical and applicable as they are faster, simpler and more reliable on grammar as well as semantic information (Yao et al., 2018). Recent studies (Cheng and Lapata, 2016; Nallapati et al., 2017; Yasunaga et al., 2017; Feng et al., 2018) consider extractive summarization as a sequence labeling task, where each sentence is individually processed and determined ∗ Corresponding author. whether it should be extracted or not. Various neural networks are used to label each sentence and trained using cross-entropy loss to maximize the likelihood of the ground-truth labeled sequences, which may derive the mismatch between the cross-entropy objective function and the evaluation criterion. On the other hand, some reinforcement learning based methods (Wu and Hu, 2018; Na"
D19-1300,D18-1409,0,0.176228,"Missing"
D19-1300,P18-1013,0,0.0212968,"t al. (2017) computes the salience of each sentence for selection with graph convolutional networks. In addition, reinforcement learning based methods (Wu and Hu, 2018; Narayan et al., 2018; Yao et al., 2018) have been proposed to directly optimize the evaluation metric ROUGE by combining cross-entropy loss with rewards from policy gradient reinforcement learning. Dong et al. (2018) considered extractive summarization as a contextual bandit and it performs well especially when good summary sentences appear late in the source document. Recently, Nallapati et al. (2017); Chen and Bansal (2018); Hsu et al. (2018) propose unified models and combine the advantages of both extractive and abstractive methods. Human Reading-inspired Strategy in NLP Recently, several pioneer researches began to study how to adapt human reading cognition process, usually including pre-reading, reading and post-reading (Avery and Graves, 1997; Saricoban, 3040 2002; Toprak and Almacıo˘glu, 2009; Pressley and Afflerbach, 2012), into various NLP-related applications. For example, Li et al. (2018) solved document-based question answering and by simulating human being’s reading strategy. Luo et al. (2018, 2019) utilized the prior"
D19-1300,D14-1181,0,0.0035753,"h encodes the document into sentence embeddings as well as produces the feature set F including global and local features. Specifically, bidirectional LSTMs (BiLSTMs) on word- and sentence-level are first used to encode a document with N sentences into ds -dimentional sentence embeddings {S1 , S2 , . . . , SN }, Si ∈ Rds . Second, a global feature S¯ ∈ Rds is computed as an average of all the sentence vectors. Third, we use a convolutional neural network to refine gist of different paragraphs and generate multiple local features on the sentence level, which is different from previous methods (Kim, 2014; Narayan et al., 2017; Yao et al., 2018) processing on the word level. In detail, a stacking of N sentence vectors is represented as, S1:N = [S1 , S2 , · · · , SN ] ∈ RN ×ds , (1) We apply 1-D convolutional neural networks on S1:N followed with a max-over-time pooling so that a final document-level representation can be 3035 extracted. Specifically, we altogether used K convolutional nets with K different window sizes to summarize different gists of paragraphs. Finally, by stacking the outputs together, we get the final document-level representation for local features L1:K ∈ RK×ds . 2.3 Caref"
D19-1300,P19-1189,1,0.840629,"rmation provided by the rough reading and sentence affinities computed by the sentence decoder. It stops taking actions once the termination mechanism is triggered. After that, all the selected sentences are formed as a summary, and a final reward can be calculated by comparing to the labeled golden summary. As there is no intermediate reward before termination, the goal of the agent is to find a policy to maximize its expected long-term return. It is an intuitive choice to select sentences with the highest affinities as summary, which is similar to training data selection (Song et al., 2012; Liu et al., 2019). However, such an argmax policy is prone to only learn the easy patterns since it lacks exploration. We will show an example in Section 4.3. They should be explored to form the summary as well. Since the search space for summarization is extremely large, we must explicitly address the tradeoff between exploration and exploitation for fast learning, which is an active research area in reinforcement learning and applications (Pan et al., 2019a,b). In our work, we find the use of -greedy with stochastic policy works well enough to encourage exploration. Specifically, with a probability of 1 −"
D19-1300,W04-3252,0,0.361736,"hey are asked to rank the output of each system on three aspects, namely overall quality, coverage and non-redundancy. Notice that the best one will be marked rank 1 and so on, and two system would be ranked the same if their extracted summaries are identical. We report the average results in Table 4 and it shows that our HER is leading than BANDITSUM on overall quality and covRelated Work Extractive Text Summarization Researchers have developed many statistical methods for automatic extractive summarization. Traditional methods learn to score each sentence dependently (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Wong et al., 2008). Recently neural network based extractive methods (Cheng and Lapata, 2016; Nallapati et al., 2017; Feng et al., 2018; Shi et al., 2018) usually consider extractive summarization as sequence labeling tasks and aim to minimize the cross-entropy objective function. Narayan et al. (2017) utilizes side information to help sentence classifier while Yasunaga et al. (2017) computes the salience of each sentence for selection with graph convolutional networks. In addition, reinforcement learning based methods (Wu and Hu, 2018; Narayan et al., 2018; Yao et al., 2018) have been propo"
D19-1300,K16-1028,0,0.0291627,"; G). More concretely, agreedy = argmaxpθ (a|D) and this baseline satisfies that the probability of a sampled sequence would be increased when the summary it induces is better than what is obtained by greedy decoding. The procedure of HER is shown in Algorithm 1. 3 Experiment Settings In this section we present our experimental setup for evaluating the performance of the proposed HER, including the datasets, evaluation protocol, baselines and implementation details. Datasets: We evaluate our models on three datasets: the CNN, the DailyMail and the combined CNN/DailyMail (Hermann et al., 2015; Nallapati et al., 2016). We also use the standard splits of Hermann et al. (2015) for training, validation, and test (90, 266/1, 220/1, 093 documents for CNN and 196, 961/12, 148/10, 397 for DailyMail) with the same setting in See et al. (2017). Evaluation: We evaluate summarization quality using F1 ROUGE (Lin, 2004) including unigram and bigram overlap (ROUGE-1 and ROUGE-2) to assess informativeness and the longest common subsequence (ROUGE-L) to assess fluency with the reference summaries. We obtain ROUGE scores using a faster python im3037 plementation1 for training and evaluation, and the standard pyrouge packag"
D19-1300,N18-1158,0,0.465559,"16; Nallapati et al., 2017; Yasunaga et al., 2017; Feng et al., 2018) consider extractive summarization as a sequence labeling task, where each sentence is individually processed and determined ∗ Corresponding author. whether it should be extracted or not. Various neural networks are used to label each sentence and trained using cross-entropy loss to maximize the likelihood of the ground-truth labeled sequences, which may derive the mismatch between the cross-entropy objective function and the evaluation criterion. On the other hand, some reinforcement learning based methods (Wu and Hu, 2018; Narayan et al., 2018; Yao et al., 2018) directly optimize the evaluation metric by combining cross-entropy loss with rewards and train model with policy gradient reinforcement learning. Note that the rewards usually reflect the quality of extracted summary and measured by standard evaluation protocol. However, they still sequentially process text and tend to extract earlier sentences over later ones due to the sequential nature of selection (Dong et al., 2018). Although great efforts have been devoted to this field, most of the existing approaches neglect how human being reads and forms summaries. Human beings ar"
D19-1300,D14-1162,0,0.0824494,"inds of extractive methods: (1) Lead3 model simply selects the first three sentences. (2) NN-SE (Cheng and Lapata, 2016) and SummaRuNNer (Nallapati et al., 2017) are sequence labeling task and trained with cross-entropy loss. (3) Refresh (Narayan et al., 2018), DQN (Yao et al., 2018) and RNES (Wu and Hu, 2018) extract summary via reinforcement learning. (4) BANDITSUM (Dong et al., 2018) considers the extractive summarization as a contextual bandit but fails to simulate human reading recognition process. Implementation Details: We initialize word embeddings with 100-dimension Glove embeddings (Pennington et al., 2014). In rough reading, the encoder is hierarchical and each layer is a twostacked BiLSTM with a hidden size of 200. Therefore, sentence vectors and the document representation S¯ have a dimension of 400. For the variant CNN, we adopt filter windows H in {1, 2, 3} with 100 feature maps each and generate K = 3 local representations for each document. In careful reading, we set  = 0.1 for bandit policy. We also bound the minimum and maximum number of selected sentence to be 1 and 10 for termination mechanism. During training, we use the optimizer Adam (Kingma and Ba, 2014) with a learning rate of 1"
D19-1300,P17-1099,0,0.0986233,"dure of HER is shown in Algorithm 1. 3 Experiment Settings In this section we present our experimental setup for evaluating the performance of the proposed HER, including the datasets, evaluation protocol, baselines and implementation details. Datasets: We evaluate our models on three datasets: the CNN, the DailyMail and the combined CNN/DailyMail (Hermann et al., 2015; Nallapati et al., 2016). We also use the standard splits of Hermann et al. (2015) for training, validation, and test (90, 266/1, 220/1, 093 documents for CNN and 196, 961/12, 148/10, 397 for DailyMail) with the same setting in See et al. (2017). Evaluation: We evaluate summarization quality using F1 ROUGE (Lin, 2004) including unigram and bigram overlap (ROUGE-1 and ROUGE-2) to assess informativeness and the longest common subsequence (ROUGE-L) to assess fluency with the reference summaries. We obtain ROUGE scores using a faster python im3037 plementation1 for training and evaluation, and the standard pyrouge package2 for test following Dong et al. (2018). Baselines: We compare our proposed HER against four kinds of extractive methods: (1) Lead3 model simply selects the first three sentences. (2) NN-SE (Cheng and Lapata, 2016) and S"
D19-1300,C12-2116,1,0.802829,"the contextual information provided by the rough reading and sentence affinities computed by the sentence decoder. It stops taking actions once the termination mechanism is triggered. After that, all the selected sentences are formed as a summary, and a final reward can be calculated by comparing to the labeled golden summary. As there is no intermediate reward before termination, the goal of the agent is to find a policy to maximize its expected long-term return. It is an intuitive choice to select sentences with the highest affinities as summary, which is similar to training data selection (Song et al., 2012; Liu et al., 2019). However, such an argmax policy is prone to only learn the easy patterns since it lacks exploration. We will show an example in Section 4.3. They should be explored to form the summary as well. Since the search space for summarization is extremely large, we must explicitly address the tradeoff between exploration and exploitation for fast learning, which is an active research area in reinforcement learning and applications (Pan et al., 2019a,b). In our work, we find the use of -greedy with stochastic policy works well enough to encourage exploration. Specifically, with a p"
D19-1300,K17-1016,1,0.77908,"and abstractive methods. Human Reading-inspired Strategy in NLP Recently, several pioneer researches began to study how to adapt human reading cognition process, usually including pre-reading, reading and post-reading (Avery and Graves, 1997; Saricoban, 3040 2002; Toprak and Almacıo˘glu, 2009; Pressley and Afflerbach, 2012), into various NLP-related applications. For example, Li et al. (2018) solved document-based question answering and by simulating human being’s reading strategy. Luo et al. (2018, 2019) utilized the prior knowledge of human reading to solve sub-tasks in sentiment analysis. Song et al. (2017, 2018) enhanced word embeddings in a similar way. Yang et al. (2019) applied it for abstractive summarization, Zheng et al. (2019) simulated human behavior for reading comprehension, and Lei et al. (2019) utilized human-like semantic cognition for aspectlevel sentiment classification. In this paper, we attempt to perform extractive summarization under the inspiration of human reading recognition. 6 Conclusion Inspired by the reading cognition of human beings, we propose HER, a two-stage method, to mimic how people extract summaries. The whole learning process is formulated as a contextual ban"
D19-1300,C08-1124,0,0.0546748,"output of each system on three aspects, namely overall quality, coverage and non-redundancy. Notice that the best one will be marked rank 1 and so on, and two system would be ranked the same if their extracted summaries are identical. We report the average results in Table 4 and it shows that our HER is leading than BANDITSUM on overall quality and covRelated Work Extractive Text Summarization Researchers have developed many statistical methods for automatic extractive summarization. Traditional methods learn to score each sentence dependently (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Wong et al., 2008). Recently neural network based extractive methods (Cheng and Lapata, 2016; Nallapati et al., 2017; Feng et al., 2018; Shi et al., 2018) usually consider extractive summarization as sequence labeling tasks and aim to minimize the cross-entropy objective function. Narayan et al. (2017) utilizes side information to help sentence classifier while Yasunaga et al. (2017) computes the salience of each sentence for selection with graph convolutional networks. In addition, reinforcement learning based methods (Wu and Hu, 2018; Narayan et al., 2018; Yao et al., 2018) have been proposed to directly opti"
D19-1300,K17-1045,0,0.254053,"tate-of-the-art extractive methods in terms of ROUGE metrics. 1 Introduction Automatic text summarization has wide popularity in NLP applications such as producing digests, headlines and reports. Among the supervised methods, two main types are usually explored, namely abstractive and extractive summarizations (Nenkova et al., 2011). Compared with abstractive approaches, extractive methods are more practical and applicable as they are faster, simpler and more reliable on grammar as well as semantic information (Yao et al., 2018). Recent studies (Cheng and Lapata, 2016; Nallapati et al., 2017; Yasunaga et al., 2017; Feng et al., 2018) consider extractive summarization as a sequence labeling task, where each sentence is individually processed and determined ∗ Corresponding author. whether it should be extracted or not. Various neural networks are used to label each sentence and trained using cross-entropy loss to maximize the likelihood of the ground-truth labeled sequences, which may derive the mismatch between the cross-entropy objective function and the evaluation criterion. On the other hand, some reinforcement learning based methods (Wu and Hu, 2018; Narayan et al., 2018; Yao et al., 2018) directly"
D19-1516,D18-1016,0,0.0325657,"Missing"
D19-1516,P15-1136,0,0.0769668,"Missing"
D19-1516,D16-1245,0,0.0298109,"m/tensorflow/models/ tree/master/research/object_detection Since we are the first to proposed a visual-aware model for pronoun coreference resolution, we compare our results with existing models of general coreference resolution. • Deterministic model (Raghunathan et al., 2010) is a rule-based system that aggregates multiple functions for determining whether two mentions are coreferent based on hand-craft features. • Statistical model (Clark and Manning, 2015) learns upon human-designed entity-level features between clusters of mentions to produce accurate coreference chains. • Deep-RL model (Clark and Manning, 2016) applies reinforcement learning to mention-ranking models to form coreference clusters. • End-to-end model (Lee et al., 2018) is the stateof-the-art method of coreference resolution. It predicts coreference clusters via an end-to-end neural network that leverages pretrained word embeddings and contextual information. Last but not least, to demonstrate the effectiveness of the proposed model, we also present a variation of the End-to-end model, which can also use the visual information, as an extra baseline: • End-to-end+Visual first extracts features from images with ResNet-152 (He et al., 201"
D19-1516,P81-1019,0,0.331779,"le are discussing the view they can both see. Pronouns and noun phrases referring to the same entity are marked in same color. The first “it” in the dialogue labeled with blue color refers to the object “the big cake” and the second “it” labeled with green color refers to the statue in the image. more challenging than the general coreference resolution task. Introduction The question of how human beings resolve pronouns has long been an attractive research topic in both linguistics and natural language processing (NLP) communities, for the reason that pronoun itself has weak semantic meaning (Ehrlich, 1981) and the correct resolution of pronouns requires complex reasoning over various information. As a core task of natural language understanding, pronoun coreference resolution (PCR) (Hobbs, 1978) is the task of identifying the noun (phrase) that pronouns refer to. Compared with the general coreference resolution task, the stringmatching methods are no longer effective for pronouns (Stoyanov et al., 2009), which makes PCR ∗ Equal contribution. the cake Recently, great efforts have been devoted into the coreference resolution task (Raghunathan et al., 2010; Clark and Manning, 2015, 2016; Lee et al"
D19-1516,E12-3001,0,0.0336323,"On the contrary, as VisPro can effectively leverage the visual information and make the decision that “they” should refer to “2 zebras.” 7 Related Work In this section, we introduce the related work about pronoun coreference resolution and visualaware natural language processing problems. 7.1 Pronoun Coreference Resolution As one core task of natural language understanding, pronoun coreference resolution, the task of identifying mentions in text that the targeting pronoun refers to, plays a vital role in many downstream applications in natural language processing, such as machine translation (Guillou, 2012), summarization (Steinberger et al., 2007) and information extraction (Edens et al., 2003). Traditional studies focus on resolving pronouns in expert-annotated formal textual dataset such as ACE (NIST, 2003) or OntoNotes (Pradhan et al., 2012). However, models that perform well on these datasets might not perform as well in other scenarios such as dialogues due to the informal language and the lack of essential information (e.g., the shared view of two speakers). In this work, we thus focus on the PCR in dialogues and show that the information contained in the shared view can be crucial for un"
D19-1516,P03-1054,0,0.0188897,"et and invite annotators to annotate. In VisDial, each image is accompanied by a dialogue record discussing that image. One example is shown in Figure 1. In addition, VisDial also provides a caption for each image, which brings more information for us to create VisPro1 . In this section, we introduce the details about the dataset creation in terms of pre-processing, survey design, annotation, and post-processing. 2.1 Pre-processing To make the annotation task clear to annotators and help them provide accurate annotation, we first extract all the noun phrases and pronouns with Stanford Parser (Klein and Manning, 2003) and then provide the extracted noun phrases as candidate mentions to annotate on. To avoid the overlap of candidate noun phrases, we choose noun phrases with a height of two in parse trees. One example is shown in Figure 2. In the syntactic tree for the sentence “A man with a dog is walking on the grass,” we choose “A man,” “a dog” and “the grass” as candidates. If the height of noun phrases is not limited, then the noun phrase “A man with a dog” will cover “A man” and “a dog,” leading to confusion in the options. Following (Strube and M¨uller, 2003; Ng, 2005), we only select third-person per"
D19-1516,N18-2108,0,0.214955,"ch, 1981) and the correct resolution of pronouns requires complex reasoning over various information. As a core task of natural language understanding, pronoun coreference resolution (PCR) (Hobbs, 1978) is the task of identifying the noun (phrase) that pronouns refer to. Compared with the general coreference resolution task, the stringmatching methods are no longer effective for pronouns (Stoyanov et al., 2009), which makes PCR ∗ Equal contribution. the cake Recently, great efforts have been devoted into the coreference resolution task (Raghunathan et al., 2010; Clark and Manning, 2015, 2016; Lee et al., 2018) and good performance has been achieved on formal written text such as newspapers (Pradhan et al., 2012; Zhang et al., 2019a) and diagnose records (Zhang et al., 2019b). However, when it comes to dialogues, where more abundant information is needed, the performance of existing models becomes less satisfying. The reason behind is that, different from formal written language, correct understanding of spoken language often requires the support of other information sources. For example, when people chat with each other, if they intend to refer to some object that all speakers can see, they may dir"
D19-1516,D14-1162,0,0.0838539,"ed for models to detect plausible mentions outside the dialogue. The pool contains both mentions extracted from the corresponding caption and randomly selected negative mention samples from other captions. All models are evaluated based on the precision (P), recall (R), and F1 score. Last but not least, we split the test dataset by whether the correct antecedents of the pronoun appear in the dialogue or not. We denote these two groups as “Discussed” and “Not Discussed.” 5.2 Implementation Details Following previous work (Lee et al., 2018), we use the concatenation of the 300d GloVe embedding (Pennington et al., 2014) and the ELMo (Peters et al., 2018) embedding as the initial word representations. Out-of-vocabulary words are initialized with zero vectors. We adopt the “ssd resnet 50 fpn coco” model from Tensorflow detection model zoo5 as the object detection module. The size of hidden states in the LSTM module is set to 200, and the size of the projected embedding for computing similarity between text spans and object labels is 512. The feed-forward networks for contextual scoring and visual scoring have two 150-dimension hidden layers and one 100-dimension hidden layer, respectively. For model training,"
D19-1516,N18-1202,0,0.0153032,"ons outside the dialogue. The pool contains both mentions extracted from the corresponding caption and randomly selected negative mention samples from other captions. All models are evaluated based on the precision (P), recall (R), and F1 score. Last but not least, we split the test dataset by whether the correct antecedents of the pronoun appear in the dialogue or not. We denote these two groups as “Discussed” and “Not Discussed.” 5.2 Implementation Details Following previous work (Lee et al., 2018), we use the concatenation of the 300d GloVe embedding (Pennington et al., 2014) and the ELMo (Peters et al., 2018) embedding as the initial word representations. Out-of-vocabulary words are initialized with zero vectors. We adopt the “ssd resnet 50 fpn coco” model from Tensorflow detection model zoo5 as the object detection module. The size of hidden states in the LSTM module is set to 200, and the size of the projected embedding for computing similarity between text spans and object labels is 512. The feed-forward networks for contextual scoring and visual scoring have two 150-dimension hidden layers and one 100-dimension hidden layer, respectively. For model training, we use cross-entropy as the loss fu"
D19-1516,W12-4501,0,0.777325,". As a core task of natural language understanding, pronoun coreference resolution (PCR) (Hobbs, 1978) is the task of identifying the noun (phrase) that pronouns refer to. Compared with the general coreference resolution task, the stringmatching methods are no longer effective for pronouns (Stoyanov et al., 2009), which makes PCR ∗ Equal contribution. the cake Recently, great efforts have been devoted into the coreference resolution task (Raghunathan et al., 2010; Clark and Manning, 2015, 2016; Lee et al., 2018) and good performance has been achieved on formal written text such as newspapers (Pradhan et al., 2012; Zhang et al., 2019a) and diagnose records (Zhang et al., 2019b). However, when it comes to dialogues, where more abundant information is needed, the performance of existing models becomes less satisfying. The reason behind is that, different from formal written language, correct understanding of spoken language often requires the support of other information sources. For example, when people chat with each other, if they intend to refer to some object that all speakers can see, they may directly use pronouns such as “it” instead of describing or mentioning it in the first place. Sometimes, t"
D19-1516,D10-1048,0,0.587253,"ason that pronoun itself has weak semantic meaning (Ehrlich, 1981) and the correct resolution of pronouns requires complex reasoning over various information. As a core task of natural language understanding, pronoun coreference resolution (PCR) (Hobbs, 1978) is the task of identifying the noun (phrase) that pronouns refer to. Compared with the general coreference resolution task, the stringmatching methods are no longer effective for pronouns (Stoyanov et al., 2009), which makes PCR ∗ Equal contribution. the cake Recently, great efforts have been devoted into the coreference resolution task (Raghunathan et al., 2010; Clark and Manning, 2015, 2016; Lee et al., 2018) and good performance has been achieved on formal written text such as newspapers (Pradhan et al., 2012; Zhang et al., 2019a) and diagnose records (Zhang et al., 2019b). However, when it comes to dialogues, where more abundant information is needed, the performance of existing models becomes less satisfying. The reason behind is that, different from formal written language, correct understanding of spoken language often requires the support of other information sources. For example, when people chat with each other, if they intend to refer to s"
D19-1516,P09-1074,0,0.0182755,"an beings resolve pronouns has long been an attractive research topic in both linguistics and natural language processing (NLP) communities, for the reason that pronoun itself has weak semantic meaning (Ehrlich, 1981) and the correct resolution of pronouns requires complex reasoning over various information. As a core task of natural language understanding, pronoun coreference resolution (PCR) (Hobbs, 1978) is the task of identifying the noun (phrase) that pronouns refer to. Compared with the general coreference resolution task, the stringmatching methods are no longer effective for pronouns (Stoyanov et al., 2009), which makes PCR ∗ Equal contribution. the cake Recently, great efforts have been devoted into the coreference resolution task (Raghunathan et al., 2010; Clark and Manning, 2015, 2016; Lee et al., 2018) and good performance has been achieved on formal written text such as newspapers (Pradhan et al., 2012; Zhang et al., 2019a) and diagnose records (Zhang et al., 2019b). However, when it comes to dialogues, where more abundant information is needed, the performance of existing models becomes less satisfying. The reason behind is that, different from formal written language, correct understandin"
D19-1516,P03-1022,0,0.27428,"Missing"
D19-1516,N19-1093,1,0.83265,"tural language understanding, pronoun coreference resolution (PCR) (Hobbs, 1978) is the task of identifying the noun (phrase) that pronouns refer to. Compared with the general coreference resolution task, the stringmatching methods are no longer effective for pronouns (Stoyanov et al., 2009), which makes PCR ∗ Equal contribution. the cake Recently, great efforts have been devoted into the coreference resolution task (Raghunathan et al., 2010; Clark and Manning, 2015, 2016; Lee et al., 2018) and good performance has been achieved on formal written text such as newspapers (Pradhan et al., 2012; Zhang et al., 2019a) and diagnose records (Zhang et al., 2019b). However, when it comes to dialogues, where more abundant information is needed, the performance of existing models becomes less satisfying. The reason behind is that, different from formal written language, correct understanding of spoken language often requires the support of other information sources. For example, when people chat with each other, if they intend to refer to some object that all speakers can see, they may directly use pronouns such as “it” instead of describing or mentioning it in the first place. Sometimes, the object (name or t"
D19-1516,P19-1083,1,0.805317,"tural language understanding, pronoun coreference resolution (PCR) (Hobbs, 1978) is the task of identifying the noun (phrase) that pronouns refer to. Compared with the general coreference resolution task, the stringmatching methods are no longer effective for pronouns (Stoyanov et al., 2009), which makes PCR ∗ Equal contribution. the cake Recently, great efforts have been devoted into the coreference resolution task (Raghunathan et al., 2010; Clark and Manning, 2015, 2016; Lee et al., 2018) and good performance has been achieved on formal written text such as newspapers (Pradhan et al., 2012; Zhang et al., 2019a) and diagnose records (Zhang et al., 2019b). However, when it comes to dialogues, where more abundant information is needed, the performance of existing models becomes less satisfying. The reason behind is that, different from formal written language, correct understanding of spoken language often requires the support of other information sources. For example, when people chat with each other, if they intend to refer to some object that all speakers can see, they may directly use pronouns such as “it” instead of describing or mentioning it in the first place. Sometimes, the object (name or t"
D19-1528,J10-4007,0,0.091241,"Missing"
D19-1528,D14-1082,0,0.354488,"and word similarity demonstrate the effectiveness of the proposed model, and a further study of scalability also proves that our embeddings only need 1/20 of the original embedding size to achieve better performance. 1 Introduction Representing words as distributed representations is an important way for machines to process lexical semantics, which attracts much attention in natural language processing (NLP) in the past few years (Mikolov et al., 2013; Pennington et al., 2014; Song et al., 2017, 2018b; Song and Shi, 2018) with respect to its usefulness in many downstream tasks, e.g., parsing (Chen and Manning, 2014), machine translation (Zou ∗ Equal contribution. et al., 2013), coreference resolution (Lee et al., 2018), etc. Conventional word embeddings, e.g., word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), leverage the co-occurrence information among words to train a unified embedding for each word. Such models are popular and the resulting embeddings are widely used owing to their effectiveness and simplicity. However, these embeddings are not helpful for scenarios requiring words functionalizing separately under different situations, where selectional preference (SP) (Wilks, 1975)"
D19-1528,D14-1004,0,0.446568,"Missing"
D19-1528,N19-1423,0,0.0292815,"counting based method for the selectional preference acquisition task. • Distributional Similarity (DS) (Erk et al., 2010), a method that uses the similarity of the #W #P 571 2,500 360 6,000 Table 2: Statistics of Human-labeled SP Evaluation Sets. #W and #P indicate the numbers of words and pairs, respectively. As different datasets have different SP relations, we only report statistics about ‘nsubj’, ‘dobj’, and ‘amod’ (if available). • ELMo (Peters et al., 2018), a pretrained language model with contextual awareness. We use its static representations of words as the word embedding. • BERT (Devlin et al., 2019), a pretrained bi-directional contextualized word embedding model with state-of-the-art performance on many NLP tasks. SP Evaluation Set Keller (Keller and Lapata, 2003) SP-10K (Zhang et al., 2019a) embedding of the target argument and average embedding of observed golden arguments in the corpus to predict the preference strength. • Neural Network (NN) (de Cruys, 2014), an NN-based method for the SP acquisition task. This model achieves the state-of-the-art performance on the pseudo-disambiguation task. For word2Vec and GloVe, we use their released code. For D-embedding, we follow their origin"
D19-1528,J15-4004,0,0.0271936,"cing their overall semantics. We also compare MWE with pre-trained contextualized word embedding models in Table 4 for this task, with overall performance, embedding dimensions, and training times reported. It is observed that that MWE outperforms ELMo and achieves comparable results with BERT with smaller embedding dimension and much less training complexities. 3.4 Word Similarity Measurement In addition to SP acquisition, we also evaluate our embeddings on word similarity (WS) measurement to test whether the learned embedding can effectively capture the overall semantics. We use SimLex-999 (Hill et al., 2015) as the evaluation dataset for this task because it contains different word types, i.e., 666 noun pairs, 222 verb pairs, and 111 adjective pairs. We follow the conventional setting that uses the Spearman’s correlation to assess the correspondence between the similarity scores and human annotations on all word pairs. Evaluations are conducted on the final embeddings v for each relation and the center ones. Results are reported in Table 3 with several observations. First, our model achieves the best overall performance and significantly better on nouns, which can be explained by that nouns appea"
D19-1528,C16-1266,0,0.0324516,"Missing"
D19-1528,J03-3005,0,0.150689,"n different parameters in a sequential manner and applied in various areas. natively update c and u upon the convergence of c. As a result, we set λ to 1 in the first half of the training process and 0 afterwards. 3 Experiments Experiments are conducted to evaluate how our embeddings are performed on SP acquisition and word similarity measurement. 3.1 Implementation Details We use the English Wikipedia4 as the training corpus. The Stanford parser5 is used to obtain dependency relations among words. For the fair comparison, we follow existing work and set d = 300, s = 10, and a = 1. Following (Keller and Lapata, 2003) and (de Cruys, 2014), we select three dependency relations (nsubj, dobj, and amod) as follows: • nsubj: The preference of subject for a given verb. For example, it is plausible to say ‘dog barks’ rather than ‘stone barks’. The verb is viewed as the predicate (head) while the subject as the argument (tail). • dobj: The preference of object for a given verb. For example, it is plausible for ‘eat food’ rather than ‘eat house’. The verb is viewed as the predicate (head) while the object as the argument (tail). • amod: The preference of modifier for a given noun. For example, it is plausible to sa"
D19-1528,N18-2108,0,0.0513578,"o proves that our embeddings only need 1/20 of the original embedding size to achieve better performance. 1 Introduction Representing words as distributed representations is an important way for machines to process lexical semantics, which attracts much attention in natural language processing (NLP) in the past few years (Mikolov et al., 2013; Pennington et al., 2014; Song et al., 2017, 2018b; Song and Shi, 2018) with respect to its usefulness in many downstream tasks, e.g., parsing (Chen and Manning, 2014), machine translation (Zou ∗ Equal contribution. et al., 2013), coreference resolution (Lee et al., 2018), etc. Conventional word embeddings, e.g., word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), leverage the co-occurrence information among words to train a unified embedding for each word. Such models are popular and the resulting embeddings are widely used owing to their effectiveness and simplicity. However, these embeddings are not helpful for scenarios requiring words functionalizing separately under different situations, where selectional preference (SP) (Wilks, 1975) is a typical scenario. In general, SP refers to that, given a word (predicate) and a dependency relation"
D19-1528,P14-2050,0,0.500537,"ociation for Computational Linguistics Figure 1: Illustration of the multiplex embeddings for ‘sing’ and ‘song’. Black arrows present center embeddings for words’ overall semantics; blue and green arrows refer to words’ relational embeddings for relationdependent semantics. All relational embeddings for each word are designed to near its center embedding. nsubj and dobj relations are used as examples. treat ‘food’ and ‘eat’ as highly relevant words but never distinguish the function of ‘food’ to be a subject or an object to ‘eat’. To address this problem, the dependency-based embedding model (Levy and Goldberg, 2014) is proposed to treat a word through separate ones, e.g., ‘food@dobj’ and ‘food@nsubj’, under different syntactic relations, with the skip-gram (Mikolov et al., 2013) model being used to train the final embeddings. However, this method is limited in two aspects. First, sparseness is introduced because each word is treated as two irrelevant ones (e.g., ‘food@dobj’ and ‘food@nsubj’), so that the overall quality of learned embeddings is affected. Second, the resulting embedding size is too large1 , which is not appropriate either for storage or usage. Therefore, in this paper, we propose a multip"
D19-1528,D14-1162,0,0.123887,"small dimension for relational embeddings and our model is able to keep their effectiveness. Experiments on selectional preference acquisition and word similarity demonstrate the effectiveness of the proposed model, and a further study of scalability also proves that our embeddings only need 1/20 of the original embedding size to achieve better performance. 1 Introduction Representing words as distributed representations is an important way for machines to process lexical semantics, which attracts much attention in natural language processing (NLP) in the past few years (Mikolov et al., 2013; Pennington et al., 2014; Song et al., 2017, 2018b; Song and Shi, 2018) with respect to its usefulness in many downstream tasks, e.g., parsing (Chen and Manning, 2014), machine translation (Zou ∗ Equal contribution. et al., 2013), coreference resolution (Lee et al., 2018), etc. Conventional word embeddings, e.g., word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), leverage the co-occurrence information among words to train a unified embedding for each word. Such models are popular and the resulting embeddings are widely used owing to their effectiveness and simplicity. However, these embeddings are n"
D19-1528,N18-1202,0,0.0549045,"lear how to leverage these methods in downstream tasks, we label these methods as downstream unfriendly. • Posterior Probability (PP) (Resnik, 1997), a counting based method for the selectional preference acquisition task. • Distributional Similarity (DS) (Erk et al., 2010), a method that uses the similarity of the #W #P 571 2,500 360 6,000 Table 2: Statistics of Human-labeled SP Evaluation Sets. #W and #P indicate the numbers of words and pairs, respectively. As different datasets have different SP relations, we only report statistics about ‘nsubj’, ‘dobj’, and ‘amod’ (if available). • ELMo (Peters et al., 2018), a pretrained language model with contextual awareness. We use its static representations of words as the word embedding. • BERT (Devlin et al., 2019), a pretrained bi-directional contextualized word embedding model with state-of-the-art performance on many NLP tasks. SP Evaluation Set Keller (Keller and Lapata, 2003) SP-10K (Zhang et al., 2019a) embedding of the target argument and average embedding of observed golden arguments in the corpus to predict the preference strength. • Neural Network (NN) (de Cruys, 2014), an NN-based method for the SP acquisition task. This model achieves the stat"
D19-1528,P99-1014,0,0.0505662,"Missing"
D19-1528,D17-1068,0,0.0394857,"Missing"
D19-1528,P19-1071,1,0.908357,"and a dependency relation, human beings have certain preferences for the words (arguments) connecting to it. Such preferences are usually carried in dependency syntactic relations, for example, the verb ‘sing’ has plausible object words ‘song’ or ‘rhythm’ rather than other nouns such as ‘house’ or ‘potato’. With such characteristic, SP is proven to be important in natural language understanding for many cases and widely applied over a variety of NLP tasks, e.g., sense disambiguation (Resnik, 1997), semantic role classification (Zapirain et al., 2013), and coreference resolution (Hobbs, 1978; Zhang et al., 2019b,c), etc. Conventional SP acquisition methods are either based on counting (Resnik, 1997) or complex neural network (de Cruys, 2014), and the SP knowledge acquired in either way can not be directly leveraged into downstream tasks. On the other hand, the information captured by word embeddings can be seamlessly used in downstream tasks, which makes embedding a potential solution for the aforementioned problem. However, conventional word embeddings using one unified embedding for each word are not able to distinguish different relations types (such as various syntactic relations, which is cruci"
D19-1528,N19-1093,1,0.889896,"and a dependency relation, human beings have certain preferences for the words (arguments) connecting to it. Such preferences are usually carried in dependency syntactic relations, for example, the verb ‘sing’ has plausible object words ‘song’ or ‘rhythm’ rather than other nouns such as ‘house’ or ‘potato’. With such characteristic, SP is proven to be important in natural language understanding for many cases and widely applied over a variety of NLP tasks, e.g., sense disambiguation (Resnik, 1997), semantic role classification (Zapirain et al., 2013), and coreference resolution (Hobbs, 1978; Zhang et al., 2019b,c), etc. Conventional SP acquisition methods are either based on counting (Resnik, 1997) or complex neural network (de Cruys, 2014), and the SP knowledge acquired in either way can not be directly leveraged into downstream tasks. On the other hand, the information captured by word embeddings can be seamlessly used in downstream tasks, which makes embedding a potential solution for the aforementioned problem. However, conventional word embeddings using one unified embedding for each word are not able to distinguish different relations types (such as various syntactic relations, which is cruci"
D19-1528,P19-1083,1,0.86796,"and a dependency relation, human beings have certain preferences for the words (arguments) connecting to it. Such preferences are usually carried in dependency syntactic relations, for example, the verb ‘sing’ has plausible object words ‘song’ or ‘rhythm’ rather than other nouns such as ‘house’ or ‘potato’. With such characteristic, SP is proven to be important in natural language understanding for many cases and widely applied over a variety of NLP tasks, e.g., sense disambiguation (Resnik, 1997), semantic role classification (Zapirain et al., 2013), and coreference resolution (Hobbs, 1978; Zhang et al., 2019b,c), etc. Conventional SP acquisition methods are either based on counting (Resnik, 1997) or complex neural network (de Cruys, 2014), and the SP knowledge acquired in either way can not be directly leveraged into downstream tasks. On the other hand, the information captured by word embeddings can be seamlessly used in downstream tasks, which makes embedding a potential solution for the aforementioned problem. However, conventional word embeddings using one unified embedding for each word are not able to distinguish different relations types (such as various syntactic relations, which is cruci"
D19-1528,D13-1141,0,0.0335748,"s unfeasible to be used in downstream tasks. Effectively with the small dimension for our local relational embeddings, relation information can be preserved in a small-sized model, which shows a compatible space requirement with the conventional embeddings. 5 Related Work Learning word embeddings has become an important research topic in NLP (Bengio et al., 2003; Turney and Pantel, 2010; Collobert et al., 2011; Song and Shi, 2018), with the capability of embeddings demonstrated in different languages (Song et al., 2018a) and tasks such as parsing (Chen and Manning, 2014), machine translation (Zou et al., 2013), coreference resolution (Lee et al., 2018), etc. Conventional word embeddings (Mikolov et al., 2013; Pennington et al., 2014) often leverage 5254 word co-occurrence patterns, resulting in a major limitation that they coalesce different relationships between words into a single vector space. To address this limitation, dependency-based embedding model (Levy and Goldberg, 2014) was proposed to represent each word with several separate embeddings, and then suffers from its sparseness and the huge size of the resulting embeddings. Alternatively, our MWE model uses a set of (constrained and small)"
D19-1528,K17-1016,1,0.831006,"ional embeddings and our model is able to keep their effectiveness. Experiments on selectional preference acquisition and word similarity demonstrate the effectiveness of the proposed model, and a further study of scalability also proves that our embeddings only need 1/20 of the original embedding size to achieve better performance. 1 Introduction Representing words as distributed representations is an important way for machines to process lexical semantics, which attracts much attention in natural language processing (NLP) in the past few years (Mikolov et al., 2013; Pennington et al., 2014; Song et al., 2017, 2018b; Song and Shi, 2018) with respect to its usefulness in many downstream tasks, e.g., parsing (Chen and Manning, 2014), machine translation (Zou ∗ Equal contribution. et al., 2013), coreference resolution (Lee et al., 2018), etc. Conventional word embeddings, e.g., word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), leverage the co-occurrence information among words to train a unified embedding for each word. Such models are popular and the resulting embeddings are widely used owing to their effectiveness and simplicity. However, these embeddings are not helpful for scen"
D19-1528,N18-2028,1,0.859782,"t is easily computed for D-embeddings that 200,000 words will result in a 10GB model, which is unfeasible to be used in downstream tasks. Effectively with the small dimension for our local relational embeddings, relation information can be preserved in a small-sized model, which shows a compatible space requirement with the conventional embeddings. 5 Related Work Learning word embeddings has become an important research topic in NLP (Bengio et al., 2003; Turney and Pantel, 2010; Collobert et al., 2011; Song and Shi, 2018), with the capability of embeddings demonstrated in different languages (Song et al., 2018a) and tasks such as parsing (Chen and Manning, 2014), machine translation (Zou et al., 2013), coreference resolution (Lee et al., 2018), etc. Conventional word embeddings (Mikolov et al., 2013; Pennington et al., 2014) often leverage 5254 word co-occurrence patterns, resulting in a major limitation that they coalesce different relationships between words into a single vector space. To address this limitation, dependency-based embedding model (Levy and Goldberg, 2014) was proposed to represent each word with several separate embeddings, and then suffers from its sparseness and the huge size of"
D19-1528,C16-1203,0,0.0273576,"Missing"
E17-2116,P16-2073,0,0.0296669,"eddings cannot represent users absent in training data. To address this, one can always introduce a special token to present unknown users in the training stage, which is a commonly adopted technique in word embedding learning. User beck-s farmer-d kaminski-v lokay-m sanders-r williams-w3 Set1 #Folder #Msg 102 1795 28 3677 37 2691 12 2494 31 1184 20 2771 Set2 #Folder #Msg 78 1749 25 3672 32 2684 11 2493 29 1181 17 2766 Table 1: Email statistics for a selected set of users in Enron. Set1 removes non-topical folders while Set2 additionally disregards small folders. In a recent work proposed by (Yu et al., 2016), they obtained user embeddings through learning word embeddings from social texts. Their idea is similar to ours in terms of using a joint learning framework, but differs in two aspects. Their model relied on document vectors when trained directly or indirectly with word embeddings, while our framework does not require separate document embeddings in training. Furthermore, their user embeddings were averaged with word embeddings for next word prediction, which thus can be seen as a special type of word embeddings. In our approach user embeddings are concatenated with word embeddings in the pr"
I13-1071,D11-1033,0,0.512705,"com Abstract tasks (Daume III, 2007). However, a limitation of the method is that it requires labeled data in the target domain, a condition that is hard to meet when creating labeled data in the target domain is expensive and time-consuming. Training data selection addresses the differences between the source and target domains by choosing a subset of the training data in the source domain that is similar to the data in the target domain. When the amount of source training data is large, this method often provides better performance than using the entire training data (Moore and Lewis, 2010; Axelrod et al., 2011; Plank and van Noord, 2011; Song et al., 2012). However, when the amount of the training data is small, the selected subset is unlikely to outperform the entire training data because the trained model cannot benefit from unselected labeled data. To address the limitations of both methods, we propose to divide the whole source training data into two subsets via training data selection. We then treat the selected subset as coming from a pseudo target domain (i.e., a pseuodo domain that is similar to the target domain) and keep the unselected data in the source domain. Now we have labeled data f"
I13-1071,P07-1033,0,0.306268,"Missing"
I13-1071,2005.mtsummit-papers.30,0,0.0303374,"erformance of statistical machine translation systems. Axelrod et al. (2011) used cross entropy in three ways: the first one directly measured cross entropy for the source side of the text; the second one was similar to (Moore and Lewis, 2010) and ranked the data using cross entropy difference; the third one took into account the bilingual data on both the source and the target side of translations. Both studies showed that the selected subset of training data worked better than the entire training corpus for machine translation. In addition to these studies, there has been other work (e.g., (Eck et al., 2005; Munteanu and Marcu, 2005; Hildebrand et al., 2005; Lu et al., 2007)) that shows training data selection is an effective way to improve MT. Plank and van Noord (2011) experimented with several training data selection methods to improve the performance of dependency parsing and POS tagging. These methods fell into two categories: probabilistically-motivated and geometrically-motivated. Their experiments demonstrated that the proposed training data selection methods outperformed random selection. In our previous study (Song et al., 2012), we proposed several entropy-based measures for training"
I13-1071,2005.eamt-1.19,0,0.0414119,"n systems. Axelrod et al. (2011) used cross entropy in three ways: the first one directly measured cross entropy for the source side of the text; the second one was similar to (Moore and Lewis, 2010) and ranked the data using cross entropy difference; the third one took into account the bilingual data on both the source and the target side of translations. Both studies showed that the selected subset of training data worked better than the entire training corpus for machine translation. In addition to these studies, there has been other work (e.g., (Eck et al., 2005; Munteanu and Marcu, 2005; Hildebrand et al., 2005; Lu et al., 2007)) that shows training data selection is an effective way to improve MT. Plank and van Noord (2011) experimented with several training data selection methods to improve the performance of dependency parsing and POS tagging. These methods fell into two categories: probabilistically-motivated and geometrically-motivated. Their experiments demonstrated that the proposed training data selection methods outperformed random selection. In our previous study (Song et al., 2012), we proposed several entropy-based measures for training data selection, including averaged entropy gain (AE"
I13-1071,D07-1036,0,0.0225162,"(2011) used cross entropy in three ways: the first one directly measured cross entropy for the source side of the text; the second one was similar to (Moore and Lewis, 2010) and ranked the data using cross entropy difference; the third one took into account the bilingual data on both the source and the target side of translations. Both studies showed that the selected subset of training data worked better than the entire training corpus for machine translation. In addition to these studies, there has been other work (e.g., (Eck et al., 2005; Munteanu and Marcu, 2005; Hildebrand et al., 2005; Lu et al., 2007)) that shows training data selection is an effective way to improve MT. Plank and van Noord (2011) experimented with several training data selection methods to improve the performance of dependency parsing and POS tagging. These methods fell into two categories: probabilistically-motivated and geometrically-motivated. Their experiments demonstrated that the proposed training data selection methods outperformed random selection. In our previous study (Song et al., 2012), we proposed several entropy-based measures for training data selection, including averaged entropy gain (AEG), cross entropy,"
I13-1071,P10-2041,0,0.571163,"g, China clksong@gmail.com Abstract tasks (Daume III, 2007). However, a limitation of the method is that it requires labeled data in the target domain, a condition that is hard to meet when creating labeled data in the target domain is expensive and time-consuming. Training data selection addresses the differences between the source and target domains by choosing a subset of the training data in the source domain that is similar to the data in the target domain. When the amount of source training data is large, this method often provides better performance than using the entire training data (Moore and Lewis, 2010; Axelrod et al., 2011; Plank and van Noord, 2011; Song et al., 2012). However, when the amount of the training data is small, the selected subset is unlikely to outperform the entire training data because the trained model cannot benefit from unselected labeled data. To address the limitations of both methods, we propose to divide the whole source training data into two subsets via training data selection. We then treat the selected subset as coming from a pseudo target domain (i.e., a pseuodo domain that is similar to the target domain) and keep the unselected data in the source domain. Now"
I13-1071,J05-4003,0,0.0207348,"istical machine translation systems. Axelrod et al. (2011) used cross entropy in three ways: the first one directly measured cross entropy for the source side of the text; the second one was similar to (Moore and Lewis, 2010) and ranked the data using cross entropy difference; the third one took into account the bilingual data on both the source and the target side of translations. Both studies showed that the selected subset of training data worked better than the entire training corpus for machine translation. In addition to these studies, there has been other work (e.g., (Eck et al., 2005; Munteanu and Marcu, 2005; Hildebrand et al., 2005; Lu et al., 2007)) that shows training data selection is an effective way to improve MT. Plank and van Noord (2011) experimented with several training data selection methods to improve the performance of dependency parsing and POS tagging. These methods fell into two categories: probabilistically-motivated and geometrically-motivated. Their experiments demonstrated that the proposed training data selection methods outperformed random selection. In our previous study (Song et al., 2012), we proposed several entropy-based measures for training data selection, including"
I13-1071,P11-1157,0,0.0774577,"Missing"
I13-1071,song-xia-2012-using,1,0.810696,"AEG when a low percentage of data is selected, while its performance is comparable or slightly lower than AEG when a higher percentage of data is selected. To understand this behavior, we compare some statistics of the data sets, as in Table 5. Since OOV rate is important for CWS and POS tagging, we want to compare our coverage-based method and AEG for this factor, and the results are presented in Table 6. The table shows that when a small percentage 4.3 Chinese Word Segmentation To evaluate feature augmentation on CWS, we use a conditional random fields (CRF) word segmenter as described in (Song and Xia, 2012). A nice property of the segmenter is that it incorporates unsupervised learning to identify possible new words in the test data in order to enhance the segmenter’s performance on OOVs. To be more specific, the segmenter uses description length gain (DLG) (Kit and Wilks, 1999) for lexical acquisition as that was performed in (Kit, 2000; Kit, 2005). Then the decision of the unsupervised word segmentation is represented as features T0i , which indicates the tag of the current character C0 when it belongs to a word whose length i ranges from 1 to 5 charac6 Song et al. (2012) showed that AEG works"
I13-1071,C12-2116,1,0.831268,"limitation of the method is that it requires labeled data in the target domain, a condition that is hard to meet when creating labeled data in the target domain is expensive and time-consuming. Training data selection addresses the differences between the source and target domains by choosing a subset of the training data in the source domain that is similar to the data in the target domain. When the amount of source training data is large, this method often provides better performance than using the entire training data (Moore and Lewis, 2010; Axelrod et al., 2011; Plank and van Noord, 2011; Song et al., 2012). However, when the amount of the training data is small, the selected subset is unlikely to outperform the entire training data because the trained model cannot benefit from unselected labeled data. To address the limitations of both methods, we propose to divide the whole source training data into two subsets via training data selection. We then treat the selected subset as coming from a pseudo target domain (i.e., a pseuodo domain that is similar to the target domain) and keep the unselected data in the source domain. Now we have labeled data from both domains, we can apply feature augmenta"
I13-1071,N03-1033,0,0.0247425,"Missing"
I13-1071,xia-etal-2000-developing,1,0.611785,"n C, In this study, we ran several sets of experiments. the count( ) function is called recursively until a We compared our training data selection with shorter ngram inside the original ngram is found. other methods, and then evaluated our revised feaThe value of the count( ) function is zero only if ture augmentation method on the CWS and POS the token ti itself is an OOV. For the experiments tagging tasks. in this paper, we use trigram to count the ngram coverage. 4.1 Data 3.3 Feature Augmentation The Chinese Penn Treebank (CTB) version 7.04 As we mentioned before, a limitation of feature (Xia et al., 2000) is used in our experiments. It augmentation (Daume III, 2007) is that it requires contains about 1.2 million words from five genres: labeled data from the target domain, and very ofBroadcast Conversation (BC), Broadcast News ten such data is not available. To overcome this (BN), Magazine (MZ), Newswire (NW), and Welimitation, we use training data section on the blog (WB). The details of the five genres of CTB source domain data, treat the selected part of data 7.0 are shown in Table 1. as from a pseudo target domain, and leave the unWe divide the data in each genre into ten folds selected par"
I13-1071,W99-0701,0,\N,Missing
J18-4008,W05-0613,0,0.0178369,"auses, namely, elementary discourse units. Adjacent units are linked by rhetorical relations—condition, comparison, elaboration, and so forth. Based on RST, early work uses hand-coded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became popular. 2.2.2 Discourse Analysis on Conversations. Stolcke et al. (2000) provide one of the first studies focusing on this problem, and it provides a general schema of understanding conversations with discourse analysis. Because of the complex structure and informal language style, discourse parsing on conversations is still a challenging problem (Perret et al. 2016). Most research focuses o"
J18-4008,P06-1026,0,0.123548,"Missing"
J18-4008,D14-1226,0,0.0636155,"Missing"
J18-4008,W04-3240,0,0.130138,"Missing"
J18-4008,W09-3951,0,0.0825383,"Missing"
J18-4008,P15-1077,0,0.0674667,"Missing"
J18-4008,P06-1039,0,0.142338,"Missing"
J18-4008,P16-2044,0,0.0311334,"Another potential line is to combine our work with representation learning on social media. Although some previous studies have provided intriguing approaches to learning representations at the level of words (Mikolov et al. 2013; Mikolov, Yih, and Zweig 2013), sentences (Le and Mikolov 2014), and paragraphs (Kiros et al. 2015), they are limited in modeling social media content with colloquial relations. Following similar ideas in this work, where discourse and topics are jointly explored, we can conduct other types of representation learning, embeddings for words (Li et al. 2017b), messages (Dhingra et al. 2016), or users (Ding, Bickel, and Pan 2017), in the context of conversations, which should complement social media representation learning and vice versa. Appendix A In this section, we present the key steps for inferring our joint model of conversational discourse and latent topics. Its generation process has been described in Section 3. As described in Section 3, we use collapsed Gibbs sampling (Griffiths et al. 2004) for model inference. Before providing the formula of sampling steps, we first define the notations of all variables used in the formulations of Gibbs sampling, described in Table A"
J18-4008,D17-1241,0,0.0428534,"Missing"
J18-4008,C12-1047,0,0.0996001,"hy content and noncontent background (general information) (Daum´e and Marcu 2006; Haghighi and ¨ 2010), and (2) to cluster sentences Vanderwende 2009; C ¸ elikyilmaz and Hakkani-Tur 724 Li et al. A Joint Model of Discourse and Topics on Microblogs or documents into topics, with summaries then generated from each topic cluster for minimizing content redundancy (Salton et al. 1997; McKeown et al. 1999; Siddharthan, Nenkova, and McKeown 2004). Similar techniques have also been applied to summarize events or opinions on microblogs (Chakrabarti and Punera 2011; Long et al. 2011; Rosa et al. 2011; Duan et al. 2012; Shen et al. 2013; Meng et al. 2012). Our downstream application on microblog summarization lies in the research line of point (1), whereas we integrate the effects of discourse on key content identification, which has not been studied in any prior work. Also it is worth noting that, following point (2) to cluster messages before summarization is beyond the scope of this work because we are focusing on summarizing a single conversation tree, on which there are limited topics. We leave the potential of using our model to segment topics for multiconversation summarization to future work. 2.2 Di"
J18-4008,C10-1034,0,0.371052,"microblog conversations. 1. Introduction Over the past two decades, the Internet has been revolutionizing the way we communicate. Microblogging, a social networking channel over the Internet, further accelerates communication and information exchange. Popular microblog platforms, such as Twitter1 and Sina Weibo,2 have become important outlets for individuals to share information and voice opinions, which further benefit downstream applications such as instant detection of breaking events (Lin et al. 2010; Weng and Lee 2011; Peng et al. 2015), real-time and ad hoc search of microblog messages (Duan et al. 2010; Li et al. 2015b), public opinions and user behavior understanding on societal issues (Pak and Paroubek 2010; Popescu and Pennacchiotti 2010; Kouloumpis, Wilson, and Moore 2011), and so forth. However, the explosive growth of microblog data far outpaces human beings’ speed of reading and understanding. As a consequence, there is a pressing need for effective natural language processing (NLP) systems that can automatically identify gist information, and make sense of the unmanageable amount of user-generated social media content (Farzindar and Inkpen 2015). As one of the important and fundamen"
J18-4008,P14-1048,0,0.0169086,"nd so forth. Based on RST, early work uses hand-coded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became popular. 2.2.2 Discourse Analysis on Conversations. Stolcke et al. (2000) provide one of the first studies focusing on this problem, and it provides a general schema of understanding conversations with discourse analysis. Because of the complex structure and informal language style, discourse parsing on conversations is still a challenging problem (Perret et al. 2016). Most research focuses on the detection of dialogue acts (DAs),4 which are 4 Dialogue act can be used interchangeably with speech act (Stolcke et"
J18-4008,P07-1062,0,0.0392749,"urse units. Adjacent units are linked by rhetorical relations—condition, comparison, elaboration, and so forth. Based on RST, early work uses hand-coded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became popular. 2.2.2 Discourse Analysis on Conversations. Stolcke et al. (2000) provide one of the first studies focusing on this problem, and it provides a general schema of understanding conversations with discourse analysis. Because of the complex structure and informal language style, discourse parsing on conversations is still a challenging problem (Perret et al. 2016). Most research focuses on the detection of dial"
J18-4008,P11-2008,0,0.133607,"Missing"
J18-4008,N09-1041,0,0.248132,"as conversation starter. Topic Assignments. Messages on one conversation tree focus on related topics. To exploit such intuition in topic assignments, the topic of each message m on conversation tree c (i.e., zc,m ) is sampled from the topic mixture θc of conversation tree c. 3.2 Word-Level Modeling To distinguish varying types of word distributions to separately capture discourse, topic, and background representations, we follow the solutions from previous work to assign each word as a discrete and exact source that reflects one particular type of word representation (Daum´e and Marcu 2006; Haghighi and Vanderwende 2009; Ritter, Cherry, and Dolan 2010). To this end, for each word n in message m and tree c, a ternary variable xc,m,n ∈ {DISC, TOPIC, BACK} controls word n to fall into one of the three types: discourse, topic, and background word. In doing so, words in the given collection are explicitly separated into three types, based on which the word distributions representing discourse, topic, and background components are separated accordingly. Discourse words. (DISC) indicate the discourse role of a message; for example, in Figure 1, “How” and the question mark “?” reflect that [R1] should be assigned th"
J18-4008,P14-1002,0,0.0234826,"ded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became popular. 2.2.2 Discourse Analysis on Conversations. Stolcke et al. (2000) provide one of the first studies focusing on this problem, and it provides a general schema of understanding conversations with discourse analysis. Because of the complex structure and informal language style, discourse parsing on conversations is still a challenging problem (Perret et al. 2016). Most research focuses on the detection of dialogue acts (DAs),4 which are 4 Dialogue act can be used interchangeably with speech act (Stolcke et al. 2000). 725 Computational Linguistics Volume 44,"
J18-4008,D12-1083,0,0.0384174,"Missing"
J18-4008,P13-1160,0,0.0181528,"Missing"
J18-4008,D15-1259,1,0.704119,"tions. 1. Introduction Over the past two decades, the Internet has been revolutionizing the way we communicate. Microblogging, a social networking channel over the Internet, further accelerates communication and information exchange. Popular microblog platforms, such as Twitter1 and Sina Weibo,2 have become important outlets for individuals to share information and voice opinions, which further benefit downstream applications such as instant detection of breaking events (Lin et al. 2010; Weng and Lee 2011; Peng et al. 2015), real-time and ad hoc search of microblog messages (Duan et al. 2010; Li et al. 2015b), public opinions and user behavior understanding on societal issues (Pak and Paroubek 2010; Popescu and Pennacchiotti 2010; Kouloumpis, Wilson, and Moore 2011), and so forth. However, the explosive growth of microblog data far outpaces human beings’ speed of reading and understanding. As a consequence, there is a pressing need for effective natural language processing (NLP) systems that can automatically identify gist information, and make sense of the unmanageable amount of user-generated social media content (Farzindar and Inkpen 2015). As one of the important and fundamental text analyti"
J18-4008,P16-1199,1,0.313837,"t al. 2013). Quan et al. (2015) propose self-aggregation-based topic modeling (SATM) that aggregates texts jointly with topic inference. Another popular solution is to take into account word relations to alleviate document-level word sparseness. Biterm topic model (BTM) directly models the generation of word-pair co-occurrence patterns in each individual message (Yan et al. 2013; Cheng et al. 2014). More recently, word embeddings trained by large-scale external data are leveraged to capture word relations and improve topic models on short texts (Das, Zaheer, and Dyer 2015; Nguyen et al. 2015; Li et al. 2016a, 2017a; Shi et al. 2017; Xun et al. 2017). To date, most efforts focus on content in messages, but ignore the rich discourse structure embedded in ubiquitous user interactions on microblog platforms. On microblogs, which were originally built for user communication and interaction, conversations are freely formed on issues of interests by reposting messages and replying to others. When joining a conversation, users generally post topically related content, which naturally provide effective contextual information for topic discovery. AlvarezMelis and Saveski (2016) have shown that simply aggr"
J18-4008,D14-1220,0,0.0410723,"Missing"
J18-4008,W04-1013,0,0.0251576,"scourse and latent topics is fully unsupervised, therefore does not require any manual annotation. For evaluation, we conduct quantitative and qualitative analysis on large-scale Twitter and Sina Weibo corpora. Experimental results show that topics induced by our model are more coherent than existing models. Qualitative analysis on discourse further shows that our model can yield meaningful clusters of words related to manually crafted discourse categories. In addition, we present an empirical study on downstream application of microblog conversation summarization. Empirical results on ROUGE (Lin 2004) show that summaries produced based on our joint model contain more salient information than state-of-the-art summarization systems. Human evaluation also indicates that our output summaries are competitive with existing unsupervised summarization systems in the aspects of informativeness, conciseness, and readability. In summary, our contributions in this article are 3-fold: • 722 Microblog posts organized as conversation trees for topic modeling. We propose a novel concept of representing microblog posts as conversation trees by connecting microblog posts based on reposting and replying Li e"
J18-4008,D09-1036,0,0.0944317,"Missing"
J18-4008,W11-0709,0,0.0644897,"Missing"
J18-4008,C12-1104,0,0.0289953,"ev et al. 2004), TF-IDF (Inouye and Kalita 2011), integer linear programming (Liu, Liu, and Weng 2011; Takamura, Yokono, and Okumura 2011), graph learning (Sharifi, Hutton, and Kalita 2010), and so on. Later, researchers found that standard summarization models are not suitable for microblog posts because of the severe redundancy, noise, and sparsity problems exhibited in short and colloquial messages (Chang et al. 2013; Li et al. 2015a). To solve these problems, one common solution is to use social signals such as the user influence and retweet counts to help summarization (Duan et al. 2012; Liu et al. 2012; Chang et al. 2013). Different from the aforementioned studies, we do not include external features such 726 Li et al. A Joint Model of Discourse and Topics on Microblogs as the social network structure, which ensures the general applicability of our approach when applied to domains without such information. Discourse has been reported useful to microblog summarization. Zhang et al. (2013) and Li et al. (2015a) leverage dialogue acts to indicate summary-worthy messages. In the field of conversation summarization from other domains (e.g., meetings, forums, and e-mails), it is also popular to l"
J18-4008,J00-3005,0,0.250894,"re theory (RST) (Mann and Thompson 1988) is one of the most influential discourse theories. According to its assumption, a coherent document can be represented by text units at different levels (e.g., clauses, sentences, paragraphs) in a hierarchical tree structure. In particular, the minimal units in RST (i.e., leaves of the tree structure) are defined as sub-sentential clauses, namely, elementary discourse units. Adjacent units are linked by rhetorical relations—condition, comparison, elaboration, and so forth. Based on RST, early work uses hand-coded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became"
J18-4008,N13-1090,0,0.156114,"s regardless of the different discourse roles of messages. The work by Li et al. (2016b) serves as another prior effort to leverage conversation structure, captured by a supervised discourse tagger, on topic induction. Different from them, our model learns discourse structure for conversations in a fully unsupervised manner, which does not require annotated data. Another line of research tackles data sparseness by modeling word relations rather than word occurrences in documents. For example, recent research work has shown that distributional similarities of words captured by word embeddings (Mikolov et al. 2013; Mikolov, Yih, and Zweig 2013) are useful in recognizing interpretable topic word clusters from short texts (Das, Zaheer, and Dyer 2015; Nguyen et al. 2015; Li et al. 2016a, 2017a; Shi et al. 2017; Xun et al. 2017). These topic models heavily rely on meaningful word embeddings needed to be trained on a large-scale, high-quality external corpus, which should be both in the same domain and the same language as the data for topic modeling (Bollegala, Maehara, and Kawarabayashi 2015). However, such external resource is not always available. For example, to the best of our knowledge, there current"
J18-4008,D11-1024,0,0.0306012,"ocessing, which retains the same common settings to ensure comparable performance.18 Evaluation Metrics. Topic model evaluation is inherently difficult. Although in many previous studies perplexity is a popular metric to evaluate the predictive abilities of topic models given held-out data set with unseen words (Blei, Ng, and Jordan 2003), we do not consider perplexity here because high perplexity does not necessarily indicate semantically coherent topics in human perception (Chang et al. 2009). The quality of topics is commonly measured by UCI (Newman et al. 2010) and UMass coherence scores (Mimno et al. 2011), assuming that words representing a coherent topic are likely to co-occur within the same document. We only consider UMass coherence here as UMass and UCI generally agree with each other, according to Stevens et al. (2012). We also consider a newer evaluation metric, the CV coherence ¨ measure (Roder, Both, and Hinneburg 2015), as it has been proven to provide the scores closest to human evaluation compared with other widely used topic coherence metrics, including UCI and UMass scores.19 For the CV coherence measure, in brief, given a word list for topic representations (i.e., the top N words"
J18-4008,N06-1047,0,0.0509304,"we do not include external features such 726 Li et al. A Joint Model of Discourse and Topics on Microblogs as the social network structure, which ensures the general applicability of our approach when applied to domains without such information. Discourse has been reported useful to microblog summarization. Zhang et al. (2013) and Li et al. (2015a) leverage dialogue acts to indicate summary-worthy messages. In the field of conversation summarization from other domains (e.g., meetings, forums, and e-mails), it is also popular to leverage the pre-detected discourse structure for summarization (Murray et al. 2006; McKeown, Shrestha, and Rambow 2007; Wang and Cardie 2013; Bhatia, Biyani, and Mitra 2014; Bokaei, Sameti, and Liu 2016). Oya and Carenini (2014) and Qin, Wang, and Kim (2017) address discourse tagging together with salient content discovery on e-mails and meetings, and show the usefulness of their relations in summarization. For all the systems mentioned here, manually crafted tags and annotated data are required for discourse modeling. Instead, the discourse structure is discovered in a fully unsupervised manner in our model, which is represented by word distributions and can be different f"
J18-4008,N10-1012,0,0.234042,"(Phan, Nguyen, and Horiguchi 2008; Zeng et al. 2018a), and recommendation on microblogs (Zeng et al. 2018b). Conventionally, probabilistic topic models (e.g., probabilistic latent semantic analysis [Hofmann 1999] and latent Dirichlet allocation [Blei et al. 2003]) have achieved huge success over the past decade, owing to their fully unsupervised manner and ease of extension. The semantic structure discovered by these topic models have facilitated the progress of many research fields, for example, information retrieval (Boyd-Graber, Hu, and Mimno 2017), data mining (Lin et al. 2015), and NLP (Newman et al. 2010). Nevertheless, ascribing to their reliance on document-level word co-occurrence patterns, the progress is still limited to formal conventional documents such as news reports (Blei, Ng, and Jordan 2003) and scientific articles (Rosen-Zvi et al. 2004). The aforementioned models work poorly when directly applied to short and colloquial texts (e.g., microblog posts) owing to severe sparsity exhibited in such text genre (Wang and McCallum 2006; Hong and Davison 2010). Previous research has proposed several methods to deal with the sparsity issue in short texts. One common approach is to aggregate"
J18-4008,Q15-1022,0,0.575772,"ing 2010; Mehrotra et al. 2013). Quan et al. (2015) propose self-aggregation-based topic modeling (SATM) that aggregates texts jointly with topic inference. Another popular solution is to take into account word relations to alleviate document-level word sparseness. Biterm topic model (BTM) directly models the generation of word-pair co-occurrence patterns in each individual message (Yan et al. 2013; Cheng et al. 2014). More recently, word embeddings trained by large-scale external data are leveraged to capture word relations and improve topic models on short texts (Das, Zaheer, and Dyer 2015; Nguyen et al. 2015; Li et al. 2016a, 2017a; Shi et al. 2017; Xun et al. 2017). To date, most efforts focus on content in messages, but ignore the rich discourse structure embedded in ubiquitous user interactions on microblog platforms. On microblogs, which were originally built for user communication and interaction, conversations are freely formed on issues of interests by reposting messages and replying to others. When joining a conversation, users generally post topically related content, which naturally provide effective contextual information for topic discovery. AlvarezMelis and Saveski (2016) have shown"
J18-4008,N13-1039,0,0.0942168,"Missing"
J18-4008,W14-4318,0,0.0172156,"which ensures the general applicability of our approach when applied to domains without such information. Discourse has been reported useful to microblog summarization. Zhang et al. (2013) and Li et al. (2015a) leverage dialogue acts to indicate summary-worthy messages. In the field of conversation summarization from other domains (e.g., meetings, forums, and e-mails), it is also popular to leverage the pre-detected discourse structure for summarization (Murray et al. 2006; McKeown, Shrestha, and Rambow 2007; Wang and Cardie 2013; Bhatia, Biyani, and Mitra 2014; Bokaei, Sameti, and Liu 2016). Oya and Carenini (2014) and Qin, Wang, and Kim (2017) address discourse tagging together with salient content discovery on e-mails and meetings, and show the usefulness of their relations in summarization. For all the systems mentioned here, manually crafted tags and annotated data are required for discourse modeling. Instead, the discourse structure is discovered in a fully unsupervised manner in our model, which is represented by word distributions and can be different from any human designed discourse inventory. The effects of such discourse representations on salient content identification have not been explored"
J18-4008,pak-paroubek-2010-twitter,0,0.131623,"Missing"
J18-4008,N16-1013,0,0.0483436,"Missing"
J18-4008,prasad-etal-2008-penn,0,0.0237269,"n a hierarchical tree structure. In particular, the minimal units in RST (i.e., leaves of the tree structure) are defined as sub-sentential clauses, namely, elementary discourse units. Adjacent units are linked by rhetorical relations—condition, comparison, elaboration, and so forth. Based on RST, early work uses hand-coded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became popular. 2.2.2 Discourse Analysis on Conversations. Stolcke et al. (2000) provide one of the first studies focusing on this problem, and it provides a general schema of understanding conversations with discourse analysis. Because of the comple"
J18-4008,P17-1090,0,0.0965772,"Missing"
J18-4008,P13-4009,0,0.0372628,"Missing"
J18-4008,radev-etal-2004-mead,0,0.135384,"Missing"
J18-4008,J02-4001,0,0.172797,"Missing"
J18-4008,N10-1020,0,0.38114,"Missing"
J18-4008,N13-1135,0,0.0207025,"content background (general information) (Daum´e and Marcu 2006; Haghighi and ¨ 2010), and (2) to cluster sentences Vanderwende 2009; C ¸ elikyilmaz and Hakkani-Tur 724 Li et al. A Joint Model of Discourse and Topics on Microblogs or documents into topics, with summaries then generated from each topic cluster for minimizing content redundancy (Salton et al. 1997; McKeown et al. 1999; Siddharthan, Nenkova, and McKeown 2004). Similar techniques have also been applied to summarize events or opinions on microblogs (Chakrabarti and Punera 2011; Long et al. 2011; Rosa et al. 2011; Duan et al. 2012; Shen et al. 2013; Meng et al. 2012). Our downstream application on microblog summarization lies in the research line of point (1), whereas we integrate the effects of discourse on key content identification, which has not been studied in any prior work. Also it is worth noting that, following point (2) to cluster messages before summarization is beyond the scope of this work because we are focusing on summarizing a single conversation tree, on which there are limited topics. We leave the potential of using our model to segment topics for multiconversation summarization to future work. 2.2 Discourse Analysis D"
J18-4008,C04-1129,0,0.0301286,"Missing"
J18-4008,N03-1030,0,0.291035,"Missing"
J18-4008,D12-1087,0,0.0833286,"Missing"
J18-4008,J00-3003,0,0.760268,"Missing"
J18-4008,N09-1064,0,0.0691813,"Missing"
J18-4008,C04-1048,0,0.10585,"Missing"
J18-4008,P13-1137,0,0.0670602,"Missing"
J18-4008,J05-2005,0,0.082294,"rent levels (e.g., clauses, sentences, paragraphs) in a hierarchical tree structure. In particular, the minimal units in RST (i.e., leaves of the tree structure) are defined as sub-sentential clauses, namely, elementary discourse units. Adjacent units are linked by rhetorical relations—condition, comparison, elaboration, and so forth. Based on RST, early work uses hand-coded rules for automatic discourse analysis (Marcu 2000; Thanh, Abeysinghe, and Huyck 2004). Later, thanks to the development of large-scale discourse corpora—RST corpus (Carlson, Marcu, and Okurovsky 2001), Graph Bank corpus (Wolf and Gibson 2005), and Penn Discourse Treebank (Prasad et al. 2008)—data-driven and learning-based discourse parsers that exploit various manually designed features (Soricut and Marcu 2003; Baldridge and Lascarides 2005; Fisher and Roark 2007; Lin, Kan, and Ng 2009; Subba and Eugenio 2009; Joty, Carenini, and Ng 2012; Feng and Hirst 2014) and representative learning (Ji and Eisenstein 2014; Li, Li, and Hovy 2014) became popular. 2.2.2 Discourse Analysis on Conversations. Stolcke et al. (2000) provide one of the first studies focusing on this problem, and it provides a general schema of understanding conversati"
J18-4008,D18-1351,1,0.865754,") systems that can automatically identify gist information, and make sense of the unmanageable amount of user-generated social media content (Farzindar and Inkpen 2015). As one of the important and fundamental text analytic approaches, topic models extract key components embedded in microblog content by clustering words that describe similar semantic meanings to form latent “topics.” The derived intermediate topic representations have proven beneficial to many NLP applications for social media, such as summarization (Harabagiu and Hickl 2011), classification (Phan, Nguyen, and Horiguchi 2008; Zeng et al. 2018a), and recommendation on microblogs (Zeng et al. 2018b). Conventionally, probabilistic topic models (e.g., probabilistic latent semantic analysis [Hofmann 1999] and latent Dirichlet allocation [Blei et al. 2003]) have achieved huge success over the past decade, owing to their fully unsupervised manner and ease of extension. The semantic structure discovered by these topic models have facilitated the progress of many research fields, for example, information retrieval (Boyd-Graber, Hu, and Mimno 2017), data mining (Lin et al. 2015), and NLP (Newman et al. 2010). Nevertheless, ascribing to thei"
J18-4008,N18-1035,1,0.810545,") systems that can automatically identify gist information, and make sense of the unmanageable amount of user-generated social media content (Farzindar and Inkpen 2015). As one of the important and fundamental text analytic approaches, topic models extract key components embedded in microblog content by clustering words that describe similar semantic meanings to form latent “topics.” The derived intermediate topic representations have proven beneficial to many NLP applications for social media, such as summarization (Harabagiu and Hickl 2011), classification (Phan, Nguyen, and Horiguchi 2008; Zeng et al. 2018a), and recommendation on microblogs (Zeng et al. 2018b). Conventionally, probabilistic topic models (e.g., probabilistic latent semantic analysis [Hofmann 1999] and latent Dirichlet allocation [Blei et al. 2003]) have achieved huge success over the past decade, owing to their fully unsupervised manner and ease of extension. The semantic structure discovered by these topic models have facilitated the progress of many research fields, for example, information retrieval (Boyd-Graber, Hu, and Mimno 2017), data mining (Lin et al. 2015), and NLP (Newman et al. 2010). Nevertheless, ascribing to thei"
J18-4008,P15-1071,0,\N,Missing
J18-4008,P10-1084,0,\N,Missing
K17-1016,P12-1015,0,0.0187192,"two sentiments. For encoding this dictionary, we design a 18-dimension vector, in which the first 9 dimension represents the positive sentiment while the last 9 for negative sentiment. A word is thus encoded into a binary form where the corresponding dimension is set to 1 with others 0. For the aforementioned word “pretty”, its encoded vector will be “000001000 000000000”, in which the score 0.625 of positive activates the 6th dimension in the vector. In doing so, we form a 83K × 18 regularization matrix for the SWN dictionary. 4 4.1 Experiments Word Similarities Evaluation We use the MEN-3k (Bruni et al., 2012), SimLex999 (Hill et al., 2015) and WordSim-353 (Finkelstein et al., 2002) datasets to perform quantitative comparisons among different approaches to generating embeddings. The cosine scores are computed between the vectors of each pair of words in the datasets8 . The measures adopted are Pearson’s coefficient of product-moment correlation (γ) and Spearman’s rank correlation (ρ), which reflect how The resulting word embeddings based on joint learning as well as retrofitting are evaluated intrinsically and extrinsically. For intrinsic evaluation, we use word similarity benchmark to directly tes"
K17-1016,N15-1184,0,0.0465234,"Missing"
K17-1016,Q16-1002,0,0.0247951,"4; Faruqui et al., 2015; Liu et al., 2015a; Kiela et al., 2015; Wieting et al., 2015; Nguyen et al., 2016) or word distributional information (Maas et al., 2011; Liu et al., 2015b) has been proven as effective in enhancing word embeddings, especially for specific downstream tasks. Bian et al. (2014) proposed to improve embedding learning with different kinds of knowledge, such as morphological, syntactic and 150 semantic information. Wieting et al. (2015) improves embeddings by leveraging paraphrase pairs from the PPDB for learning phrase embeddings in the paraphrasing task. In a similar way, Hill et al. (2016) uses learned word embeddings as supervised knowledge for learning phrase embeddings. Although our approach is conceptually similar to previous work, it is different in several ways. For leveraging unlabeled data, the regularizer in this work is different from applying topic distributions as word vectors (Maas et al., 2011) or treating topics as conditional contexts (Liu et al., 2015b). For leveraging semantic knowledge, our regularizer does not require explicit word relations as used in previous studies (Yu and Dredze, 2014; Faruqui et al., 2015; Kiela et al., 2015), but takes encoded informa"
K17-1016,P16-2074,0,0.110859,"ources to obtain embeddings that are best suited for the target tasks, such as Maas et al. (2011) using a sentiment lexicon to enhance embeddings for sentiment classification. However, learning word embeddings with a particular target makes the approach less generic, also implying that customized adaptation has to be made whenever a new knowledge source is considered. Along the lines of improving embedding quality, semantic resources have been incorporated as guiding knowledge to refine objective functions in a joint learning framework (Bian et al., 2014; Xu et al., 2014; Yu and Dredze, 2014; Nguyen et al., 2016), or used for retrofitting based on word relations defined in the semantic lexicons (Faruqui et al., 2015; Kiela et al., 2015). These approaches, nonetheless, require explicit word relations defined in semantic resources, which is a difficult prerequisite for knowledge preparation. Given the above challenges, we propose a novel framework that extends typical context learning by integrating external knowledge sources for enhancing embedding learning. Compared to a well known work by Faruqui et al. (2015) that focused on tackling the task using a retrofitting1 framework on semantic lexicons, our"
K17-1016,J15-4004,0,0.0344352,"s dictionary, we design a 18-dimension vector, in which the first 9 dimension represents the positive sentiment while the last 9 for negative sentiment. A word is thus encoded into a binary form where the corresponding dimension is set to 1 with others 0. For the aforementioned word “pretty”, its encoded vector will be “000001000 000000000”, in which the score 0.625 of positive activates the 6th dimension in the vector. In doing so, we form a 83K × 18 regularization matrix for the SWN dictionary. 4 4.1 Experiments Word Similarities Evaluation We use the MEN-3k (Bruni et al., 2012), SimLex999 (Hill et al., 2015) and WordSim-353 (Finkelstein et al., 2002) datasets to perform quantitative comparisons among different approaches to generating embeddings. The cosine scores are computed between the vectors of each pair of words in the datasets8 . The measures adopted are Pearson’s coefficient of product-moment correlation (γ) and Spearman’s rank correlation (ρ), which reflect how The resulting word embeddings based on joint learning as well as retrofitting are evaluated intrinsically and extrinsically. For intrinsic evaluation, we use word similarity benchmark to directly test the quality of the learned em"
K17-1016,D14-1162,0,0.101715,"Missing"
K17-1016,D15-1242,0,0.0488127,"Missing"
K17-1016,P15-1145,0,0.0758369,"modeling (Mikolov et al., 2013c) or word co-occurrence factorization (Pennington et al., 1 In their study, joint learning was reported to be less effective than retrofitting. 143 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 143–152, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics Previous work has shown that many different sources can help learn better embeddings, such as semantic lexicons (Yu and Dredze, 2014; Faruqui et al., 2015; Kiela et al., 2015) or topic distributions (Maas et al., 2011; Liu et al., 2015b). To provide a more generic solution, we propose a unified framework that learns word embeddings from context (e.g., CBOW or SG) together with the flexibility of incorporating arbitrary external knowledge using the notion of a regularizer. Details are unfolded in following subsections. ternal knowledge has to be clustered beforehand according to their semantic relatedness (e.g., cold, icy, winter, frozen), and words of similar meanings are added as part of context for learning. This may set a high bar for preparing external knowledge since finding the precise word-word relations is required."
K17-1016,P11-1015,0,0.793183,"on (e.g., language modeling (Mikolov et al., 2013c) or word co-occurrence factorization (Pennington et al., 1 In their study, joint learning was reported to be less effective than retrofitting. 143 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 143–152, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics Previous work has shown that many different sources can help learn better embeddings, such as semantic lexicons (Yu and Dredze, 2014; Faruqui et al., 2015; Kiela et al., 2015) or topic distributions (Maas et al., 2011; Liu et al., 2015b). To provide a more generic solution, we propose a unified framework that learns word embeddings from context (e.g., CBOW or SG) together with the flexibility of incorporating arbitrary external knowledge using the notion of a regularizer. Details are unfolded in following subsections. ternal knowledge has to be clustered beforehand according to their semantic relatedness (e.g., cold, icy, winter, frozen), and words of similar meanings are added as part of context for learning. This may set a high bar for preparing external knowledge since finding the precise word-word rela"
K17-1016,P14-2089,0,0.225274,"leveraged relevant sources to obtain embeddings that are best suited for the target tasks, such as Maas et al. (2011) using a sentiment lexicon to enhance embeddings for sentiment classification. However, learning word embeddings with a particular target makes the approach less generic, also implying that customized adaptation has to be made whenever a new knowledge source is considered. Along the lines of improving embedding quality, semantic resources have been incorporated as guiding knowledge to refine objective functions in a joint learning framework (Bian et al., 2014; Xu et al., 2014; Yu and Dredze, 2014; Nguyen et al., 2016), or used for retrofitting based on word relations defined in the semantic lexicons (Faruqui et al., 2015; Kiela et al., 2015). These approaches, nonetheless, require explicit word relations defined in semantic resources, which is a difficult prerequisite for knowledge preparation. Given the above challenges, we propose a novel framework that extends typical context learning by integrating external knowledge sources for enhancing embedding learning. Compared to a well known work by Faruqui et al. (2015) that focused on tackling the task using a retrofitting1 framework on"
K17-1016,N13-1090,0,0.244419,"s paper. The resulting embeddings are evaluated by word similarity and sentiment classification. Experimental results show that our learning framework with regularization from prior knowledge improves embedding quality across multiple datasets, compared to a diverse collection of baseline methods. 1 Fei Xia University of Washington fxia@uw.edu Introduction Distributed representation of words (or word embedding) has been demonstrated to be effective in many natural language processing (NLP) tasks (Bengio et al., 2003; Collobert and Weston, 2008; Turney and Pantel, 2010; Collobert et al., 2011; Mikolov et al., 2013b,d; Weston et al., 2015). Conventional word embeddings are trained with a single objective function (e.g., language modeling (Mikolov et al., 2013c) or word co-occurrence factorization (Pennington et al., 1 In their study, joint learning was reported to be less effective than retrofitting. 143 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 143–152, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics Previous work has shown that many different sources can help learn better embeddings, such as semantic"
K17-1016,baccianella-etal-2010-sentiwordnet,0,\N,Missing
K17-1016,N13-1092,0,\N,Missing
L18-1464,J97-1002,0,0.615779,"Missing"
L18-1464,J86-3001,0,0.792435,"Missing"
L18-1464,J00-3003,0,0.451514,"tional structures at turn level or between a pair of turns (e.g., by distinguishing Forward Communicative Function and Backward Communicative Function (Core and Allen, 1997; Jurafsky et al., 1997)). Informed by the Conversation Analysis (CA) theory and approach, our annotation scheme allows us to capture hierarchical structures of conversation at several levels, including turn, adjacency pair, sequence, and overall organization. For conversational actions, most of the existing annotation schemes of dialog acts (DAs) were based on speech act theory (Core and Allen, 1997; Jurafsky et al., 1997; Stolcke et al., 2000; Hoxha et al., 2016) ; however, it was found that comprehension of indirect speech acts were very difficulty with the available schemes, primarily due to the fact that classifications of actions were based on the surface format of an utterance. Based on CA theory of actions in conversation, which considers the sequential position of a turn as critical for action recognition and ascription, our annotation scheme allows classifications of actions based on a turn’s structural position in conversation. It thus entails great flexibilities for annotating indirect conversational actions. 5.2. cal an"
L18-1464,xia-etal-2000-developing,1,0.613454,"Missing"
N18-1151,W17-5523,0,0.0243558,"helps to indicate keyphrase “president Duterte”. Such contextual information embedded in a conversation is nonetheless ignored for keyphrase extraction in existing approaches. In this paper, we present a neural keyphrase extraction framework that exploits conversation context, which is represented by neural encoders for capturing salient content to help in indicating keyphrases in target posts. Conversation context has been proven useful in many NLP tasks on social media, such as sentiment analysis (Ren et al., 2016), summarization (Chang et al., 2013; Li et al., 2015), and sarcasm detection (Ghosh et al., 2017). We use four context encoders in our model, namely, averaged embedding, RNN (Pearlmutter, 1989), attention (Bahdanau et al., 2014), and memory networks (Weston et al., 2015), which are proven useful in text representation (Cho et al., 2014; Weston et al., 2015; Huang et al., 2016; Nie et al., 2017). Particularly in this task, to the best of our knowledge, we are the first to encode conversations for detecting keyphrases in microblog posts. Experimental results on Twitter and Sina Weibo datasets demonstrate that, by effectively encoding context in conversations, our proposed approach outperfor"
N18-1151,P11-2008,0,0.0847263,"Missing"
N18-1151,N09-1041,0,0.236089,"Missing"
N18-1151,C10-2042,0,0.0169088,"dentifying good features from conversations. 5 Related Work Previous work on extracting keyphrases mainly focuses on formal texts like news reports (Wan and Xiao, 2008) and scientific articles (Nguyen and Kan, 2007). Existing keyphrase extraction models can be categorized as ranking-based models and tagging-based models. Ranking-based methods include models based on graph ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008), text clustering (Liu et al., 2009), TF-IDF (Jones, 2004; Zhang et al., 2007; Lee and Kim, 2008; Kireyev, 2009; Wu and Giles, 2013), etc. The empirical study provided by Hasan and Ng (2010) shows that TF-IDF has robust performance and can serve as a strong baseline. Tagging models focus on using manually-crafted features for binary classifiers to predict keyphrases (Frank et al., 1999; Tang et al., 2004; Medelyan and Witten, 2006). Our models are in the line of tagging approaches, and provide an alternative choice that incorporates additionally knowledge from conversations. Recently, keyphrase extraction methods have been extended to social media texts (Zhao et al., 2011; Bellaachia and Al-Dhelaan, 2012; Marujo Conclusion This work presents a keyphrase extraction framework for m"
N18-1151,P82-1020,0,0.77344,"Missing"
N18-1151,C16-1090,0,0.0271057,"h is represented by neural encoders for capturing salient content to help in indicating keyphrases in target posts. Conversation context has been proven useful in many NLP tasks on social media, such as sentiment analysis (Ren et al., 2016), summarization (Chang et al., 2013; Li et al., 2015), and sarcasm detection (Ghosh et al., 2017). We use four context encoders in our model, namely, averaged embedding, RNN (Pearlmutter, 1989), attention (Bahdanau et al., 2014), and memory networks (Weston et al., 2015), which are proven useful in text representation (Cho et al., 2014; Weston et al., 2015; Huang et al., 2016; Nie et al., 2017). Particularly in this task, to the best of our knowledge, we are the first to encode conversations for detecting keyphrases in microblog posts. Experimental results on Twitter and Sina Weibo datasets demonstrate that, by effectively encoding context in conversations, our proposed approach outperforms existing approaches by a large margin. Quantitative and qualitative analysis suggest that our framework performs robustly on keyphrases with various length. Some encoders such as memory networks can detect salient and topic-related content, whose occurrences are highly indicati"
N18-1151,N09-1060,0,0.0305661,"ontext improves both precision and recall, because supervision helps in identifying good features from conversations. 5 Related Work Previous work on extracting keyphrases mainly focuses on formal texts like news reports (Wan and Xiao, 2008) and scientific articles (Nguyen and Kan, 2007). Existing keyphrase extraction models can be categorized as ranking-based models and tagging-based models. Ranking-based methods include models based on graph ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008), text clustering (Liu et al., 2009), TF-IDF (Jones, 2004; Zhang et al., 2007; Lee and Kim, 2008; Kireyev, 2009; Wu and Giles, 2013), etc. The empirical study provided by Hasan and Ng (2010) shows that TF-IDF has robust performance and can serve as a strong baseline. Tagging models focus on using manually-crafted features for binary classifiers to predict keyphrases (Frank et al., 1999; Tang et al., 2004; Medelyan and Witten, 2006). Our models are in the line of tagging approaches, and provide an alternative choice that incorporates additionally knowledge from conversations. Recently, keyphrase extraction methods have been extended to social media texts (Zhao et al., 2011; Bellaachia and Al-Dhelaan, 20"
N18-1151,D15-1259,1,0.918633,"sentences, and then apply ranking-based models (Zhao et al., 2011; Bellaachia and AlDhelaan, 2012; Marujo et al., 2015) or sequence tagging models (Zhang et al., 2016) on them. It is arguable that these methods are suboptimal for recognizing salient content from short and informal messages due to the severe data sparsity problem. Considering that microblogs allow users to form conversations on issues of interests by reposting with comments2 and replying to messages for voicing opinions on previous discussed points, these conversations can enrich context for short messages (Chang et al., 2013; Li et al., 2015), and have been proven useful for identifying topicrelated content (Li et al., 2016). For example, Table 1 displays a target post with keyphrase “president Duterte” and its reposting and replying messages forming a conversation. Easily identified, critical words are mentioned multiple times in conversations. Such as in [R2], keyword “Duterte” re-occurs in the conversation. 2 On Twitter, reposting behavior is named as retweet. 1676 Proceedings of NAACL-HLT 2018, pages 1676–1686 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Also, topic-relevant conten"
N18-1151,P16-1199,1,0.852177,"Dhelaan, 2012; Marujo et al., 2015) or sequence tagging models (Zhang et al., 2016) on them. It is arguable that these methods are suboptimal for recognizing salient content from short and informal messages due to the severe data sparsity problem. Considering that microblogs allow users to form conversations on issues of interests by reposting with comments2 and replying to messages for voicing opinions on previous discussed points, these conversations can enrich context for short messages (Chang et al., 2013; Li et al., 2015), and have been proven useful for identifying topicrelated content (Li et al., 2016). For example, Table 1 displays a target post with keyphrase “president Duterte” and its reposting and replying messages forming a conversation. Easily identified, critical words are mentioned multiple times in conversations. Such as in [R2], keyword “Duterte” re-occurs in the conversation. 2 On Twitter, reposting behavior is named as retweet. 1676 Proceedings of NAACL-HLT 2018, pages 1676–1686 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Also, topic-relevant content, e.g., “head of state”, “another head of state”, “Obama”, helps to indicate keyphr"
N18-1151,D09-1027,0,0.0554021,"ecall by enriching more topic-related words. While for supervised method KEA, context improves both precision and recall, because supervision helps in identifying good features from conversations. 5 Related Work Previous work on extracting keyphrases mainly focuses on formal texts like news reports (Wan and Xiao, 2008) and scientific articles (Nguyen and Kan, 2007). Existing keyphrase extraction models can be categorized as ranking-based models and tagging-based models. Ranking-based methods include models based on graph ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008), text clustering (Liu et al., 2009), TF-IDF (Jones, 2004; Zhang et al., 2007; Lee and Kim, 2008; Kireyev, 2009; Wu and Giles, 2013), etc. The empirical study provided by Hasan and Ng (2010) shows that TF-IDF has robust performance and can serve as a strong baseline. Tagging models focus on using manually-crafted features for binary classifiers to predict keyphrases (Frank et al., 1999; Tang et al., 2004; Medelyan and Witten, 2006). Our models are in the line of tagging approaches, and provide an alternative choice that incorporates additionally knowledge from conversations. Recently, keyphrase extraction methods have been exten"
N18-1151,loza-etal-2014-building,0,0.0177095,"phrase for “DOUBLE STANDARD: Obama DOJ Prosecuted Others For Leaking FAR LESS Than Hillary Espionage URL” whose keyphrase should be “Espionage”. Because the corresponding conversation of this post is centered around “Hillary” instead of “Espionage”, such information is captured by the context encoder, which leads to incorrect keyphrase prediction. However, this type of error points out the potential of extending our framework to extracting keyphrases from conversations instead of a post, which would be beneficial to generating summary-worthy content for conversations (Fern´andez et al., 2008; Loza et al., 2014). 4.6 Ranking-based Models Table 8 reports the results of ranking models on Twitter and Weibo. We have the following observations. First, tagging-based models perform much better than ranking-based ones in keyphrase extraction. Comparing the results in Table 8 with that in Table 4 and Table 5, all neural taggers outperform non-neural ranking-based models by a large margin. This fact, again, confirms that keyphrase extraction is a challenging task on short microblog messages. Compared to ranking-based models, neural tagging models have the ability 1683 w/o context TF-IDF TextRank KEA w/ context"
N18-1151,P15-2105,0,0.29951,"Missing"
N18-1151,W04-3252,0,0.38298,"tokens and 4.6M words in the vocabulary. Weibo embeddings are trained on 467M Weibo messages with 1.7B words and 2.5M words in the vocabulary. In comparison, we employ neural taggers without encoding conversation context, which are based on RNN, GRU, LSTM, and BiLSTM. We also compare our models with the state-of-the-art joint-layer RNN (Zhang et al., 2016) and its GRU, LSTM, and BiLSTM variations. To further illustrate the effectiveness of leveraging conversation context for keyphrase extraction, we also evaluate some ranking-based models, namely, TF-IDF (Salton and Buckley, 1988), TextRank (Mihalcea and Tarau, 2004), and KEA implemented by KEA-3.011 (Witten et al., 1999). 1680 11 www.nzdl.org/Kea/Download/KEA-3.0.zip No encoder Context Encoder Avg Emb RNN GRU LSTM B I LSTM Att (LSTM) Att (BiLSTM) MemNN RNN 58.8±1.4 Single-layer Taggers GRU LSTM 66.2±0.8 67.3±1.6 BiLSTM 74.8±0.7 RNN 55.5±0.5 Joint-layer Taggers GRU LSTM 64.1±0.7 64.9±0.6 BiLSTM 76.8±0.5 63.3±0.9 58.2±1.6 56.5±0.8 59.4±2.0 60.8±1.7 62.4±1.8 59.6±1.4 61.1±0.4 68.2±0.7 64.9±0.6 67.0±0.6 67.6±0.8 68.6±1.0 67.6±1.1 68.6±0.6 69.3±0.5 76.6±0.9 73.1±0.1 73.8±0.7 75.5±0.2 75.9±0.7 75.8±1.2 76.5±0.8 79.1±1.1 61.1±1.2 60.9±0.5 58.4±1.1 61.1±1.9 61.6"
N18-1151,N13-1039,0,0.049243,"Missing"
N18-1151,N13-1026,0,0.0170393,"both precision and recall, because supervision helps in identifying good features from conversations. 5 Related Work Previous work on extracting keyphrases mainly focuses on formal texts like news reports (Wan and Xiao, 2008) and scientific articles (Nguyen and Kan, 2007). Existing keyphrase extraction models can be categorized as ranking-based models and tagging-based models. Ranking-based methods include models based on graph ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008), text clustering (Liu et al., 2009), TF-IDF (Jones, 2004; Zhang et al., 2007; Lee and Kim, 2008; Kireyev, 2009; Wu and Giles, 2013), etc. The empirical study provided by Hasan and Ng (2010) shows that TF-IDF has robust performance and can serve as a strong baseline. Tagging models focus on using manually-crafted features for binary classifiers to predict keyphrases (Frank et al., 1999; Tang et al., 2004; Medelyan and Witten, 2006). Our models are in the line of tagging approaches, and provide an alternative choice that incorporates additionally knowledge from conversations. Recently, keyphrase extraction methods have been extended to social media texts (Zhao et al., 2011; Bellaachia and Al-Dhelaan, 2012; Marujo Conclusion"
N18-1151,W03-1730,0,0.156428,"Missing"
N18-1151,D16-1080,0,0.772611,"tions such as information retrieval (Choi * Work was done during the internship at Tencent AI Lab. Our datasets are released at: http://ai.tencent. com/ailab/Encoding_Conversation_Context_ for_Neural_Keyphrase_Extraction_from_ Microblog_Posts.html 1 et al., 2012), text summarization (Zhao et al., 2011), event tracking (Ribeiro et al., 2017), etc. To date, most efforts on keyphrase extraction on microblogs treat messages as independent documents or sentences, and then apply ranking-based models (Zhao et al., 2011; Bellaachia and AlDhelaan, 2012; Marujo et al., 2015) or sequence tagging models (Zhang et al., 2016) on them. It is arguable that these methods are suboptimal for recognizing salient content from short and informal messages due to the severe data sparsity problem. Considering that microblogs allow users to form conversations on issues of interests by reposting with comments2 and replying to messages for voicing opinions on previous discussed points, these conversations can enrich context for short messages (Chang et al., 2013; Li et al., 2015), and have been proven useful for identifying topicrelated content (Li et al., 2016). For example, Table 1 displays a target post with keyphrase “presi"
N18-2028,P12-1015,0,0.0620188,"Missing"
N18-2028,N15-1184,0,0.0585757,"Missing"
N18-2028,P11-2008,0,0.0620614,"Missing"
N18-2028,J15-4004,0,0.0663978,"Missing"
N18-2028,D15-1242,0,0.0377637,"Missing"
N18-2028,P14-2131,0,0.0634583,"Missing"
N18-2028,N16-1030,0,0.0337005,"et al. (2015a), this task is performed in both news and social media data. For news data, we use Wall Street Journal (WSJ) proportion from the Penn Treebank (Marcus et al., 1993) and follow the standard split of 38,219/5,527/5,462 sentences for training, development, and test, respectively. The social media data is based on ARK dataset (Gimpel et al., 2011), which contains manual POS annotations on English tweets. The standard split of ARK contains 1,000/327/500 tweets as training/development/test set, respectively. POS prediction is conducted by a bidirectional LSTM-CRF (Huang et al., 2015; Lample et al., 2016) taking the produced embeddings as input. LSTM state size is setting to 200. For WSJ, we use the aforementioned embeddings trained from the Wiki corpus. For ARK, we prepare a Twitter corpus (TWT) to build embeddings. This data contains 100 million tweets collected through Twitter streaming API6 , followed by preprocessing using the toolkit described in Owoputi et al. (2013). The TWT embeddings are trained under the same procedure as the Wiki embeddings. Similar to word similarity task, we use CBOW, SG, CWin, SSG and SSSG as baselines in this task. Results are reported in Table 5. We observe th"
N18-2028,N15-1142,0,0.28912,"ional Skip-Gram: Explicitly Distinguishing Left and Right Context for Word Embeddings Yan Song, Shuming Shi, Jing Li, Haisong Zhang Tencent AI Lab {clksong,shumingshi,ameliajli,hansonzhang}@tencent.com Abstract resources, which are hard to obtain or annotate. To overcome such limitations, there are many approaches to further exploiting the characteristics of the running text, e.g., internal structure of the context. These approaches include enlarging the projection layer with consideration of word orders (Bansal et al., 2014; Ling et al., 2015a), learning context words with different weights (Ling et al., 2015b), etc. They are advantageous of learning word embeddings in an end-to-end unsupervised manner without requiring additional resources. Yet, they are also restricted in their implementation such as that they normally require a larger hidden layer or additional weights, which demand higher computation burden and could result in gradient explosion when embedding dimensions are enlarged. Another issue is that when considering word orders, they may suffer from data sparsity problem since n-gram coverage is much less than word, especially in the cold start scenario for a new domain where training d"
N18-2028,K17-1016,1,0.841233,"l with negative sampling (Mikolov et al., 2013a,c) is a popular choice for learning word embeddings and has had large impact in the community, for its efficient training and good performance in downstream applications. Although widely used for multiple tasks, SG model relies on word co-occurrence within local context in word prediction but ignores further detailed information such as word orders, positions. To improve original word embedding models, there are various studies leveraging external knowledge to update word embeddings with post processing (Faruqui et al., 2015; Kiela et al., 2015; Song et al., 2017) or supervised objectives (Yu and Dredze, 2014; Nguyen et al., 2016). However, these approaches are limited by reliable semantic 175 Proceedings of NAACL-HLT 2018, pages 175–180 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 2.1 Model SG SSG SSSG DSG Approach Skip-Gram Model The SG model (Mikolov et al., 2013b) is a popular choice to learn word embeddings by leveraging the relations between a word and its neighboring words. In detail, the SG model is to predict the context for each given word wt , and maximizes LSG |V | 1 X X log f (wt+i , wt ) = |"
N18-2028,D15-1161,0,0.310708,"ional Skip-Gram: Explicitly Distinguishing Left and Right Context for Word Embeddings Yan Song, Shuming Shi, Jing Li, Haisong Zhang Tencent AI Lab {clksong,shumingshi,ameliajli,hansonzhang}@tencent.com Abstract resources, which are hard to obtain or annotate. To overcome such limitations, there are many approaches to further exploiting the characteristics of the running text, e.g., internal structure of the context. These approaches include enlarging the projection layer with consideration of word orders (Bansal et al., 2014; Ling et al., 2015a), learning context words with different weights (Ling et al., 2015b), etc. They are advantageous of learning word embeddings in an end-to-end unsupervised manner without requiring additional resources. Yet, they are also restricted in their implementation such as that they normally require a larger hidden layer or additional weights, which demand higher computation burden and could result in gradient explosion when embedding dimensions are enlarged. Another issue is that when considering word orders, they may suffer from data sparsity problem since n-gram coverage is much less than word, especially in the cold start scenario for a new domain where training d"
N18-2028,P14-2089,0,0.0323907,"3a,c) is a popular choice for learning word embeddings and has had large impact in the community, for its efficient training and good performance in downstream applications. Although widely used for multiple tasks, SG model relies on word co-occurrence within local context in word prediction but ignores further detailed information such as word orders, positions. To improve original word embedding models, there are various studies leveraging external knowledge to update word embeddings with post processing (Faruqui et al., 2015; Kiela et al., 2015; Song et al., 2017) or supervised objectives (Yu and Dredze, 2014; Nguyen et al., 2016). However, these approaches are limited by reliable semantic 175 Proceedings of NAACL-HLT 2018, pages 175–180 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 2.1 Model SG SSG SSSG DSG Approach Skip-Gram Model The SG model (Mikolov et al., 2013b) is a popular choice to learn word embeddings by leveraging the relations between a word and its neighboring words. In detail, the SG model is to predict the context for each given word wt , and maximizes LSG |V | 1 X X log f (wt+i , wt ) = |V | Operations 2cC(n + 1)o 4c2 C(n + 1)o 4cC(n"
N18-2028,J93-2004,0,0.0613104,"n context, while not severely suffered from the data sparsity problem. Particularly among all SG models, DSG produces the best performance when trained on either the large or the small corpus. This fact further proves 3.3 Part-of-Speech Tagging Besides the intrinsic evaluation to test the embeddings semantically, we also evaluate different embeddings syntactically with an extrinsic evaluation: part-of-speech (POS) tagging. Following Ling et al. (2015a), this task is performed in both news and social media data. For news data, we use Wall Street Journal (WSJ) proportion from the Penn Treebank (Marcus et al., 1993) and follow the standard split of 38,219/5,527/5,462 sentences for training, development, and test, respectively. The social media data is based on ARK dataset (Gimpel et al., 2011), which contains manual POS annotations on English tweets. The standard split of ARK contains 1,000/327/500 tweets as training/development/test set, respectively. POS prediction is conducted by a bidirectional LSTM-CRF (Huang et al., 2015; Lample et al., 2016) taking the produced embeddings as input. LSTM state size is setting to 200. For WSJ, we use the aforementioned embeddings trained from the Wiki corpus. For AR"
N18-2028,N13-1090,0,0.705963,"s efficient as the original skip-gram model, when compared to other extensions of the skip-gram model. Experimental results show that our model outperforms others on different datasets in semantic (word similarity measurement) and syntactic (partof-speech tagging) evaluations, respectively. 1 Introduction Word embedding and its related techniques have shown to be vital for natural language processing (NLP) (Bengio et al., 2003; Collobert and Weston, 2008; Turney and Pantel, 2010; Collobert et al., 2011; Weston et al., 2015; Song and Lee, 2017). The skip-gram (SG) model with negative sampling (Mikolov et al., 2013a,c) is a popular choice for learning word embeddings and has had large impact in the community, for its efficient training and good performance in downstream applications. Although widely used for multiple tasks, SG model relies on word co-occurrence within local context in word prediction but ignores further detailed information such as word orders, positions. To improve original word embedding models, there are various studies leveraging external knowledge to update word embeddings with post processing (Faruqui et al., 2015; Kiela et al., 2015; Song et al., 2017) or supervised objectives (Y"
N18-2028,P16-2074,0,0.0174545,"oice for learning word embeddings and has had large impact in the community, for its efficient training and good performance in downstream applications. Although widely used for multiple tasks, SG model relies on word co-occurrence within local context in word prediction but ignores further detailed information such as word orders, positions. To improve original word embedding models, there are various studies leveraging external knowledge to update word embeddings with post processing (Faruqui et al., 2015; Kiela et al., 2015; Song et al., 2017) or supervised objectives (Yu and Dredze, 2014; Nguyen et al., 2016). However, these approaches are limited by reliable semantic 175 Proceedings of NAACL-HLT 2018, pages 175–180 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 2.1 Model SG SSG SSSG DSG Approach Skip-Gram Model The SG model (Mikolov et al., 2013b) is a popular choice to learn word embeddings by leveraging the relations between a word and its neighboring words. In detail, the SG model is to predict the context for each given word wt , and maximizes LSG |V | 1 X X log f (wt+i , wt ) = |V | Operations 2cC(n + 1)o 4c2 C(n + 1)o 4cC(n + 1)o 2cC(n + 2)o Tab"
N18-2028,N13-1039,0,0.0495604,"Missing"
N19-1093,E09-1018,0,0.0803473,"lution (Hobbs, 1978) was proposed. As an important yet vital sub-task of the general coreference resolution task, pronoun coreference resolution is to find the correct reference for a given pronominal anaphor in the context and has been shown to be crucial for a series of downstream tasks (Mitkov, 2014), including machine translation (Mitkov et al., 1995), summarization (Steinberger et al., 2007), information extraction (Edens et al., 2003), and dialog systems (Strube and M¨uller, 2003). Conventionally, people design rules (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998) or use features (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) to resolve the pronoun coreferences. These methods heavily rely on the coverage and quality of the manually defined rules and features. Until recently, end-to-end solution (Lee et al., 2017) was proposed towards solving the general coreference problem, where deep learning models were used to better capture contextual information. However, training such models on annotated corpora can be biased and normally does not consider external knowledge. Despite the great efforts made in this area in the past few decades (Hobbs, 1978; Mitkov, 1998; Ng, 2005; Rahman and Ng, 2009), prono"
N19-1093,P15-1136,0,0.158307,"Missing"
N19-1093,D16-1245,0,0.161268,"Missing"
N19-1093,P81-1019,0,0.646079,"e the validity and effectiveness of our model, where it outperforms state-of-the-art models by a large margin. 1 Figure 1: Pronoun coreference examples, where each example requires different knowledge for its resolution. Blue bold font refers to the target pronoun, where the correct noun reference and other candidates are marked by green underline and brackets, respectively. Introduction The question of how human beings resolve pronouns has long been of interest to both linguistics and natural language processing (NLP) communities, for the reason that pronoun itself has weak semantic meaning (Ehrlich, 1981) and brings challenges in natural language understanding. To explore solutions for that question, pronoun coreference resolution (Hobbs, 1978) was proposed. As an important yet vital sub-task of the general coreference resolution task, pronoun coreference resolution is to find the correct reference for a given pronominal anaphor in the context and has been shown to be crucial for a series of downstream tasks (Mitkov, 2014), including machine translation (Mitkov et al., 1995), summarization (Steinberger et al., 2007), information extraction (Edens et al., 2003), and dialog systems (Strube and M"
N19-1093,D17-1018,0,0.279033,"ontext and has been shown to be crucial for a series of downstream tasks (Mitkov, 2014), including machine translation (Mitkov et al., 1995), summarization (Steinberger et al., 2007), information extraction (Edens et al., 2003), and dialog systems (Strube and M¨uller, 2003). Conventionally, people design rules (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998) or use features (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) to resolve the pronoun coreferences. These methods heavily rely on the coverage and quality of the manually defined rules and features. Until recently, end-to-end solution (Lee et al., 2017) was proposed towards solving the general coreference problem, where deep learning models were used to better capture contextual information. However, training such models on annotated corpora can be biased and normally does not consider external knowledge. Despite the great efforts made in this area in the past few decades (Hobbs, 1978; Mitkov, 1998; Ng, 2005; Rahman and Ng, 2009), pronoun coreference resolution remains challenging. The reason behind is that the correct resolution of pronouns can be influenced by many factors (Ehrlich, 1981); many resolution decisions require reasoning upon d"
N19-1093,N18-2108,0,0.300199,"Missing"
N19-1093,N18-2028,1,0.81493,"Missing"
N19-1093,P11-1117,0,0.487864,"Missing"
N19-1093,P98-2143,0,0.817399,"r that question, pronoun coreference resolution (Hobbs, 1978) was proposed. As an important yet vital sub-task of the general coreference resolution task, pronoun coreference resolution is to find the correct reference for a given pronominal anaphor in the context and has been shown to be crucial for a series of downstream tasks (Mitkov, 2014), including machine translation (Mitkov et al., 1995), summarization (Steinberger et al., 2007), information extraction (Edens et al., 2003), and dialog systems (Strube and M¨uller, 2003). Conventionally, people design rules (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998) or use features (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) to resolve the pronoun coreferences. These methods heavily rely on the coverage and quality of the manually defined rules and features. Until recently, end-to-end solution (Lee et al., 2017) was proposed towards solving the general coreference problem, where deep learning models were used to better capture contextual information. However, training such models on annotated corpora can be biased and normally does not consider external knowledge. Despite the great efforts made in this area in the past few decades (Hobbs, 1978"
N19-1093,P03-1022,0,0.647751,"Missing"
N19-1093,1995.tmi-1.6,0,0.947111,"Missing"
N19-1093,C94-2189,0,0.580487,"ore solutions for that question, pronoun coreference resolution (Hobbs, 1978) was proposed. As an important yet vital sub-task of the general coreference resolution task, pronoun coreference resolution is to find the correct reference for a given pronominal anaphor in the context and has been shown to be crucial for a series of downstream tasks (Mitkov, 2014), including machine translation (Mitkov et al., 1995), summarization (Steinberger et al., 2007), information extraction (Edens et al., 2003), and dialog systems (Strube and M¨uller, 2003). Conventionally, people design rules (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998) or use features (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) to resolve the pronoun coreferences. These methods heavily rely on the coverage and quality of the manually defined rules and features. Until recently, end-to-end solution (Lee et al., 2017) was proposed towards solving the general coreference problem, where deep learning models were used to better capture contextual information. However, training such models on annotated corpora can be biased and normally does not consider external knowledge. Despite the great efforts made in this area in the past few decad"
N19-1093,N18-1151,1,0.831826,"ormation. However, training such models on annotated corpora can be biased and normally does not consider external knowledge. Despite the great efforts made in this area in the past few decades (Hobbs, 1978; Mitkov, 1998; Ng, 2005; Rahman and Ng, 2009), pronoun coreference resolution remains challenging. The reason behind is that the correct resolution of pronouns can be influenced by many factors (Ehrlich, 1981); many resolution decisions require reasoning upon different contextual and external knowledge (Rahman and Ng, 2011), which is also proved in other NLP tasks (Song et al., 2017, 2018; Zhang et al., 2018). Figure 1 demonstrates such requirement with three examples, where Example A depends on the plurality knowledge that ‘them’ refers to plural noun phrases; Example B illustrates the gender requirement of pronouns where ‘she’ can ∗ This work was done during the internship of the first author in Tencent AI Lab. 872 Proceedings of NAACL-HLT 2019, pages 872–881 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics only refer to a female person (girl); Example C requires a more general type of knowledge1 that ‘cats can climb trees but a dog normally does no"
N19-1093,D14-1162,0,0.0836574,"e final two-layer model outperforms the Feature Concatenation model. It proves that simply treating external knowledge as the feature, even though they are from the same sources, is not as effective as learning them in a joint framework. The reason • Feature Concatenation, a simplified version of the complete model that removes the second knowledge processing layer, but directly treats all external knowledge embeddings as features and concatenates them to span representations. 4.4 Implementation Following previous work (Lee et al., 2018), we use the concatenation of the 300d GloVe embeddings (Pennington et al., 2014) and the ELMo (Peters et al., 2018) embeddings as the initial word representations. Out-of-vocabulary words are initialized with zero vectors. Hyper-parameters are 12 13 Experimental Results https://stanfordnlp.github.io/CoreNLP/coref.html https://github.com/kentonl/e2e-coref 877 Figure 5: Effect of different thresholds on candidate numbers. Max and Average number of candidates after pruning are represented with solid lines in blue and orange, respectively. Two dashed lines indicate the max and the average number of candidates before pruning. F1 ∆F1 The Complete Model 81.0 - –Plurality knowled"
N19-1093,N18-1202,0,0.372661,"erformance of different models on the evaluation dataset. Precision (P), recall (R), and F1 score are reported, with the best one in each F1 column marked bold. features between clusters and mentions for coreference resolution. • Deep-RL model, proposed by Clark and Manning (2016), a reinforcement learning method to directly optimize the coreference matrix instead of the traditional loss function. • End2end is the current state-of-the-art coreference model (Lee et al., 2018), which performs in an end-to-end manner and leverages both the contextual information and a pre-trained language model (Peters et al., 2018). set as follows. The hidden state of the LSTM module is set to 200, and all the feed-forward networks in our model have two 150-dimension hidden layers. The default pruning threshold t for softmax pruning is set to 10−7 . All linguistic features (plurality and AG) and external knowledge (SP) are encoded as 20-dimension embeddings. For model training, we use cross-entropy as the loss function and Adam (Kingma and Ba, 2015) as the optimizer. All the aforementioned hyperparameters are initialized randomly, and we apply dropout rate 0.2 to all hidden layers in the model. Our model treats a candid"
N19-1093,W12-4501,0,0.131122,"., nsubj and dobj). The resulted frequency is grouped into the following buckets [1, 2, 3, 4, 5-7, 8-15, 16-31, 3263, 64+] and we use the bucket id as the final SP knowledge. Thus in the previous example: Table 1: Statistics of the evaluation dataset. Number of selected pronouns are reported. scores, the module allow more of them to proceed to the second layer. Compared with other conventional pruning methods (Lee et al., 2017, 2018) that generally keep a fixed number of candidates, our pruning strategy is more efficient and flexible. 4 Experiment Settings 4.1 Data The CoNLL-2012 shared task (Pradhan et al., 2012) corpus is used as the evaluation dataset, which is selected from the Ontonotes 5.07 . Following conventional approaches (Ng, 2005; Li et al., 2011), for each pronoun in the document, we consider candidate n from the previous two sentences and the current sentence. For pronouns, we consider two types of them following Ng (2005), i.e., third personal pronoun (she, her, he, him, them, they, it) and possessive pronoun (his, hers, its, their, theirs). Table 1 reports the number of the two type pronouns and the overall statistics for the experimental dataset. According to our selection range of can"
N19-1093,D10-1048,0,0.260818,"Missing"
N19-1093,D09-1101,0,0.0694923,"; Charniak and Elsner, 2009; Li et al., 2011) to resolve the pronoun coreferences. These methods heavily rely on the coverage and quality of the manually defined rules and features. Until recently, end-to-end solution (Lee et al., 2017) was proposed towards solving the general coreference problem, where deep learning models were used to better capture contextual information. However, training such models on annotated corpora can be biased and normally does not consider external knowledge. Despite the great efforts made in this area in the past few decades (Hobbs, 1978; Mitkov, 1998; Ng, 2005; Rahman and Ng, 2009), pronoun coreference resolution remains challenging. The reason behind is that the correct resolution of pronouns can be influenced by many factors (Ehrlich, 1981); many resolution decisions require reasoning upon different contextual and external knowledge (Rahman and Ng, 2011), which is also proved in other NLP tasks (Song et al., 2017, 2018; Zhang et al., 2018). Figure 1 demonstrates such requirement with three examples, where Example A depends on the plurality knowledge that ‘them’ refers to plural noun phrases; Example B illustrates the gender requirement of pronouns where ‘she’ can ∗ Th"
N19-1093,P11-1082,0,0.353145,"reference problem, where deep learning models were used to better capture contextual information. However, training such models on annotated corpora can be biased and normally does not consider external knowledge. Despite the great efforts made in this area in the past few decades (Hobbs, 1978; Mitkov, 1998; Ng, 2005; Rahman and Ng, 2009), pronoun coreference resolution remains challenging. The reason behind is that the correct resolution of pronouns can be influenced by many factors (Ehrlich, 1981); many resolution decisions require reasoning upon different contextual and external knowledge (Rahman and Ng, 2011), which is also proved in other NLP tasks (Song et al., 2017, 2018; Zhang et al., 2018). Figure 1 demonstrates such requirement with three examples, where Example A depends on the plurality knowledge that ‘them’ refers to plural noun phrases; Example B illustrates the gender requirement of pronouns where ‘she’ can ∗ This work was done during the internship of the first author in Tencent AI Lab. 872 Proceedings of NAACL-HLT 2019, pages 872–881 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics only refer to a female person (girl); Example C requires"
N19-1093,K17-1016,1,0.786915,"er capture contextual information. However, training such models on annotated corpora can be biased and normally does not consider external knowledge. Despite the great efforts made in this area in the past few decades (Hobbs, 1978; Mitkov, 1998; Ng, 2005; Rahman and Ng, 2009), pronoun coreference resolution remains challenging. The reason behind is that the correct resolution of pronouns can be influenced by many factors (Ehrlich, 1981); many resolution decisions require reasoning upon different contextual and external knowledge (Rahman and Ng, 2011), which is also proved in other NLP tasks (Song et al., 2017, 2018; Zhang et al., 2018). Figure 1 demonstrates such requirement with three examples, where Example A depends on the plurality knowledge that ‘them’ refers to plural noun phrases; Example B illustrates the gender requirement of pronouns where ‘she’ can ∗ This work was done during the internship of the first author in Tencent AI Lab. 872 Proceedings of NAACL-HLT 2019, pages 872–881 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics only refer to a female person (girl); Example C requires a more general type of knowledge1 that ‘cats can climb trees"
P09-1007,J90-2002,0,0.510764,"Missing"
P09-1007,D08-1092,0,0.0385535,"from CTL, City University of Hong Kong. 1 It is a tradition to call an annotated syntactic corpus as treebank in parsing community. 55 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 55–63, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP target language can effectively make use of them by only considering the most related information extracted from the translated text. The basic idea to support this work is to make use of the semantic connection between different languages. In this sense, it is related to the work of (Merlo et al., 2002) and (Burkett and Klein, 2008). The former showed that complementary information about English verbs can be extracted from their translations in a second language (Chinese) and the use of multilingual features improves classification performance of the English verbs. The latter iteratively trained a model to maximize the marginal likelihood of tree pairs, with alignments treated as latent variables, and then jointly parsing bilingual sentences in a translation pair. The proposed parser using features from monolingual and mutual constraints helped its log-linear model to achieve better performance for both monolingual parse"
P09-1007,I08-1012,0,0.573078,"n draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle golden-standard treeb"
P09-1007,D07-1098,0,0.0104809,"ting overlapped features as our work in (Zhao and Kit, 2008), especially, those character-level ones for Chinese parsing. Our implementation of maximum entropy adopts L-BFGS algorithm for parameter optimization as usual. 4.2 Parsing using a Beam Search Algorithm In Table 2, the feature preactn returns the previous parsing action type, and the subscript n stands for the action order before the current action. These are a group of Markovian features. Without this type of features, a shift-reduce parser may directly scan through an input sequence in linear time. Otherwise, following the work of (Duan et al., 2007) and (Zhao, 2009), the parsing algorithm is to search a parsing action sequence with the maximal probability. Sdi = argmax Y p(di |di−1 di−2 ...), i where Sdi is the object parsing action sequence, p(di |di−1 ...) is the conditional probability, and di 58 Figure 1: A comparison before and after translation is i-th parsing action. We use a beam search algorithm to find the object parsing action sequence. Table 2: Features for Parsing 5 Exploiting the Translated Treebank in .f orm, n = 0, 1 i.f orm + i1 .f orm in .char2 + in+1 .char2 , n = −1, 0 i.char−1 + i1 .char−1 in .char−2 n = 0, 3 i1 .char"
P09-1007,P06-1072,0,0.011487,"nsufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle golden-standard treebank, while all the previous works focus on the unlabeled data. Although cross-language technique has been used in other natural"
P09-1007,P08-1068,0,0.00863736,"ble. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle golden-standard treebank, while all the previous works focus on the unlabeled data. Although cross-language technique has been used in other natural language processing tasks, it is basically new"
P09-1007,E03-1008,0,0.0270136,"a resources to enhance an existing parser, it is related to domain adaption for parsing that has been draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are"
P09-1007,P06-1043,0,0.0262404,"an existing parser, it is related to domain adaption for parsing that has been draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our metho"
P09-1007,P08-1061,0,0.0118388,"scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle golden-standard treebank, while all the previous works focus on the unlabeled data. Although cross-language technique has been used in other natural language processing tasks,"
P09-1007,D07-1013,0,0.0336862,"om the source language to the target one. In detail, a word-based decoding is used, which adopts a loglinear framework as in (Och and Ney, 2002) with only two features, translation model and language model, P exp[ 2i=1 λi hi (c, e)] P (c|e) = P P2 c exp[ i=1 λi hi (c, e)] Where h1 (c, e) = log(pγ (c|e)) is the translation model, which is converted from the bilingual lexicon, and h2 (c, e) = log(pθ (c)) 4 Dependency Parsing: Baseline 4.1 Learning Model and Features is the language model, a word trigram model trained from the CTB. In our experiment, we set two weights λ1 = λ2 = 1. According to (McDonald and Nivre, 2007), all data-driven models for dependency parsing that have been proposed in recent years can be described as either graph-based or transition-based. 2 StarDict is an open source dictionary software, available at http://stardict.sourceforge.net/. 57 With notations defined in Table 1, a feature set as shown in Table 2 is adopted. Here, we explain some terms in Tables 1 and 2. We used a large scale feature selection approach as in (Zhao et al., 2009) to obtain the feature set in Table 2. Some feature notations in this paper are also borrowed from that work. The feature curroot returns the root of"
P09-1007,W05-1516,0,0.0299467,"Missing"
P09-1007,E06-1011,0,0.0426299,"Missing"
P09-1007,W03-3023,0,0.385266,"Missing"
P09-1007,P05-1012,0,0.211703,"d-by-word decoding, where not a parallel corpus but a bilingual lexicon is necessary, is adopted for the treebank translation. Using an ensemble method, the key information extracted from word pairs with dependency relations in the translated text is effectively integrated into the parser for the target language. The proposed method is evaluated in English and Chinese treebanks. It is shown that a translated English treebank helps a Chinese parser obtain a state-ofthe-art result. 1 Introduction Although supervised learning methods bring stateof-the-art outcome for dependency parser inferring (McDonald et al., 2005; Hall et al., 2007), a large enough data set is often required for specific parsing accuracy according to this type of methods. However, to annotate syntactic structure, either phrase- or dependency-based, is a costly job. Until now, the largest treebanks1 in various languages for syntax learning are with around one million words (or some other similar units). Limited data stand in the way of further performance enhancement. This is the case for each individual language at least. But, this is not the case as we observe all treebanks in different languages as a whole. For example, of ten treeb"
P09-1007,C08-1132,0,0.217068,"ted in (Wang et al., 2007). The experimental results in (McDonald and Nivre, 2007) show a negative impact on the parsing accuracy from too long dependency relation. For the proposed method, the improvement relative to dependency length is shown in Figure 2. From the figure, it is seen that our method gives observable better performance when dependency lengths are larger than 4. Although word order is changed, the results here show that the useful information from the translated treebank still help those long distance dependencies. 4 There is a slight exception: using the same data splitting, (Yu et al., 2008) reported UAS without p as 0.873 versus ours, 0.870. 61 chosen to generate some additional features to enhance the parser for the target language. The experimental results in English and Chinese treebanks show the proposed method is effective and helps the Chinese parser in this work achieve a state-of-the-art result. Note that our method is evaluated in two treebanks with a similar annotation style and it avoids using too many linguistic properties. Thus the method is in the hope of being used in other similarly annotated treebanks 5 . For an immediate example, we may adopt a translated Chine"
P09-1007,P02-1027,0,0.011287,"by a research fellowship from CTL, City University of Hong Kong. 1 It is a tradition to call an annotated syntactic corpus as treebank in parsing community. 55 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 55–63, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP target language can effectively make use of them by only considering the most related information extracted from the translated text. The basic idea to support this work is to make use of the semantic connection between different languages. In this sense, it is related to the work of (Merlo et al., 2002) and (Burkett and Klein, 2008). The former showed that complementary information about English verbs can be extracted from their translations in a second language (Chinese) and the use of multilingual features improves classification performance of the English verbs. The latter iteratively trained a model to maximize the marginal likelihood of tree pairs, with alignments treated as latent variables, and then jointly parsing bilingual sentences in a translation pair. The proposed parser using features from monolingual and mutual constraints helped its log-linear model to achieve better performa"
P09-1007,I08-3008,0,0.108101,"g bilingual sentences in a translation pair. The proposed parser using features from monolingual and mutual constraints helped its log-linear model to achieve better performance for both monolingual parsers and machine translation system. In this work, cross-language features will be also adopted as the latter work. However, although it is not essentially different, we only focus on dependency parsing itself, while the parsing scheme in (Burkett and Klein, 2008) based on a constituent representation. Among of existing works that we are aware of, we regard that the most similar one to ours is (Zeman and Resnik, 2008), who adapted a parser to a new language that is much poorer in linguistic resources than the source language. However, there are two main differences between their work and ours. The first is that they considered a pair of sufficiently related languages, Danish and Swedish, and made full use of the similar characteristics of two languages. Here we consider two quite different languages, English and Chinese. As fewer language properties are concerned, our approach holds the more possibility to be extended to other language pairs than theirs. The second is that a parallel corpus is required for"
P09-1007,W08-2127,1,0.799644,"g actions, a shift action and a reduce action are also defined to maintain the stack and the unprocessed sequence. In this work, we adopt a left-to-right arc-eager parsing model, that means that the parser scans the input sequence from left to right and right dependents are attached to their heads as soon as possible (Hall et al., 2007). While memory-based and margin-based learning approaches such as support vector machines are popularly applied to shift-reduce parsing, we apply maximum entropy model as the learning model for efficient training and adopting overlapped features as our work in (Zhao and Kit, 2008), especially, those character-level ones for Chinese parsing. Our implementation of maximum entropy adopts L-BFGS algorithm for parameter optimization as usual. 4.2 Parsing using a Beam Search Algorithm In Table 2, the feature preactn returns the previous parsing action type, and the subscript n stands for the action order before the current action. These are a group of Markovian features. Without this type of features, a shift-reduce parser may directly scan through an input sequence in linear time. Otherwise, following the work of (Duan et al., 2007) and (Zhao, 2009), the parsing algorithm i"
P09-1007,W03-3017,0,0.04529,"he first letter of POS tag of word coarse POS: the first two POS tags of word the left nearest verb The first character of a word The first two characters of a word The last character of a word The last two characters of a word ’s, i.e., ‘s.dprel’ means dependent label of character in the top of stack Feature combination, i.e., ‘s.char+i.char’ means both s.char and i.char work as a feature function. Although the former will be also used as comparison, the latter is chosen as the main parsing framework by this study for the sake of efficiency. In detail, a shift-reduce method is adopted as in (Nivre, 2003), where a classifier is used to make a parsing decision step by step. In each step, the classifier checks a word pair, namely, s, the top of a stack that consists of the processed words, and, i, the first word in the (input) unprocessed sequence, to determine if a dependent relation should be established between them. Besides two dependency arc building actions, a shift action and a reduce action are also defined to maintain the stack and the unprocessed sequence. In this work, we adopt a left-to-right arc-eager parsing model, that means that the parser scans the input sequence from left to ri"
P09-1007,W09-1208,1,0.28587,"nd Features is the language model, a word trigram model trained from the CTB. In our experiment, we set two weights λ1 = λ2 = 1. According to (McDonald and Nivre, 2007), all data-driven models for dependency parsing that have been proposed in recent years can be described as either graph-based or transition-based. 2 StarDict is an open source dictionary software, available at http://stardict.sourceforge.net/. 57 With notations defined in Table 1, a feature set as shown in Table 2 is adopted. Here, we explain some terms in Tables 1 and 2. We used a large scale feature selection approach as in (Zhao et al., 2009) to obtain the feature set in Table 2. Some feature notations in this paper are also borrowed from that work. The feature curroot returns the root of a partial parsing tree that includes a specified node. The feature charseq returns a character sequence whose members are collected from all identified children for a specified word. In Table 2, as for concatenating multiple substrings into a feature string, there are two ways, seq and bag. The former is to concatenate all substrings without do something special. The latter will remove all duplicated substrings, sort the rest and concatenate all"
P09-1007,P02-1038,0,0.0152292,"y reached in fact, as the following case is frequently encountered, multiple English words have to be translated into one Chinese word. To solve this problem, we use a policy that lets the output Chinese word only inherits the attached information of the highest syntactic head in the original multiple English words. 3.2 Translation A word-by-word statistical machine translation strategy is adopted to translate words attached with the respective dependency information from the source language to the target one. In detail, a word-based decoding is used, which adopts a loglinear framework as in (Och and Ney, 2002) with only two features, translation model and language model, P exp[ 2i=1 λi hi (c, e)] P (c|e) = P P2 c exp[ i=1 λi hi (c, e)] Where h1 (c, e) = log(pγ (c|e)) is the translation model, which is converted from the bilingual lexicon, and h2 (c, e) = log(pθ (c)) 4 Dependency Parsing: Baseline 4.1 Learning Model and Features is the language model, a word trigram model trained from the CTB. In our experiment, we set two weights λ1 = λ2 = 1. According to (McDonald and Nivre, 2007), all data-driven models for dependency parsing that have been proposed in recent years can be described as either grap"
P09-1007,P07-1078,0,0.0121418,"is related to domain adaption for parsing that has been draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation metho"
P09-1007,E09-1100,1,0.164926,"as our work in (Zhao and Kit, 2008), especially, those character-level ones for Chinese parsing. Our implementation of maximum entropy adopts L-BFGS algorithm for parameter optimization as usual. 4.2 Parsing using a Beam Search Algorithm In Table 2, the feature preactn returns the previous parsing action type, and the subscript n stands for the action order before the current action. These are a group of Markovian features. Without this type of features, a shift-reduce parser may directly scan through an input sequence in linear time. Otherwise, following the work of (Duan et al., 2007) and (Zhao, 2009), the parsing algorithm is to search a parsing action sequence with the maximal probability. Sdi = argmax Y p(di |di−1 di−2 ...), i where Sdi is the object parsing action sequence, p(di |di−1 ...) is the conditional probability, and di 58 Figure 1: A comparison before and after translation is i-th parsing action. We use a beam search algorithm to find the object parsing action sequence. Table 2: Features for Parsing 5 Exploiting the Translated Treebank in .f orm, n = 0, 1 i.f orm + i1 .f orm in .char2 + in+1 .char2 , n = −1, 0 i.char−1 + i1 .char−1 in .char−2 n = 0, 3 i1 .char−2 + i2 .char−2 +"
P09-1007,D07-1111,0,0.00895734,"for parsing that has been draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle g"
P09-1007,D07-1097,0,\N,Missing
P09-1007,D07-1096,0,\N,Missing
P13-1061,ma-2006-champollion,0,0.110518,"tckit,[yansong]}@[student.]cityu.edu.hk Abstract approaches resort to word correspondences in a bilingual lexicon to match bilingual sentences. A few sentence alignment methods and tools have also been explored to combine the two. Moore (2002) proposes a multi-pass search procedure using both sentence length and an automaticallyderived bilingual lexicon. Hunalign (Varga et al., 2005) is another sentence aligner that combines sentence length and a lexicon. Without a lexicon, it backs off to a length-based algorithm and then automatically derives a lexicon from the alignment result. Soon after, Ma (2006) develops the lexicon-based aligner Champollion, assuming that different words have different importance in aligning two sentences. Nevertheless, most existing approaches to sentence alignment follow the monotonicity assumption that coupled sentences in bitexts appear in a similar sequential order in two languages and crossings are not entertained in general (Langlais et al., 1998; Wu, 2010). Consequently the task of sentence alignment becomes handily solvable by means of such basic techniques as dynamic programming. In many scenarios, however, this prerequisite monotonicity cannot be guarante"
P13-1061,C10-2081,0,0.0176133,"of bilingual sentences. The general idea is that the closer two sentences are in length, the more likely they are to align. A notable difference of their methods is that the former uses sentence 628 Type 1-1 1-0 Micro P 0.827 0.359 0.809 Moore R F1 0.828 0.827 0.329 0.343 0.807 0.808 P 0.999 0.330 0.961 Hunalign R 0.972 0.457 0.951 F1 0.986 0.383 0.956 NonmoAlign P R F1 0.987 0.987 0.987 0.729 0.729 0.729 0.976 0.976 0.976 Table 3: Performance of monotonic alignment in comparison with the baseline methods. namic programming algorithm is applied to produce final alignment. Following this work, Li et al. (2010) propose a revised version of Champollion, attempting to improve its speed without performance loss. For this purpose, the input bitexts are first divided into smaller aligned fragments before applying Champollion to derive finer-grained sentence pairs. In another related work by Deng et al. (2007), a generative model is proposed, accompanied by two specific alignment strategies, i.e., dynamic programming and divisive clustering. Although a non-monotonic search process that tolerates two successive chunks in reverse order is involved, their work is essentially targeted at monotonic alignment."
P13-1061,moore-2002-fast,0,0.180659,"age of initial 1-1 alignment. Figure 4: Demonstration of monolingual consistency. The horizontal axis is the similarity of English sentence pairs and the vertical is the similarity of the corresponding pairs in Chinese. Type NonmoAlign initAlign 3.4 Non-Monotonic Alignment To test our aligner with non-monotonic sequences of sentences, we have them randomly scrambled in our experimental data. This undoubtedly increases the difficulty of sentence alignment, especially for the traditional approaches critically relying on monotonicity. The baseline methods used for comparison are Moore’s aligner (Moore, 2002) and Hunalign (Varga et al., 2005). Hunalign is configured with the option [-realign], which triggers a three-step procedure: after an initial alignment, Hunalign heuristically enriches its dictionary using word cooccurrences in identified sentence pairs; then, it re-runs the alignment process using the updated Impact of Initial Alignment The 1-1 initial alignment plays the role of labeled instances for the semisupervised learning. It is of critical importance to the learning performance. As shown in Table 1, our alignment function predicts 1451 1-1 pairings by virtue of anchor strings, among"
P13-1061,P91-1022,0,0.655052,"Missing"
P13-1061,J93-2003,0,0.215014,"Missing"
P13-1061,P93-1002,0,0.241488,"st divided into smaller aligned fragments before applying Champollion to derive finer-grained sentence pairs. In another related work by Deng et al. (2007), a generative model is proposed, accompanied by two specific alignment strategies, i.e., dynamic programming and divisive clustering. Although a non-monotonic search process that tolerates two successive chunks in reverse order is involved, their work is essentially targeted at monotonic alignment. length in number of characters while the latter in number of tokens. Both use dynamic programming to search for the best alignment. As shown in Chen (1993) and Wu (1994), however, sentencelength based methods suffer when the texts to be aligned contain small passages, or the languages involved share few cognates. The subsequent stage of sentence alignment research is accompanied by the advent of a handful of well-designed alignment tools. Moore (2002) proposes a three-pass procedure to find final alignment. Its bitext input is initially aligned based on sentence length. This step generates a set of strictly-selected sentence pairs for use to train an IBM translation model 1 (Brown et al., 1993). Its final step realigns the bitext using both sent"
P13-1061,H05-1010,0,0.0416629,"owing optimization A = arg max X s.t. m X i=1 Xij ≤ 1, m X n X Xij Fij created. Each posting includes a sentence identifier, the in-sentence frequency and positions of this term. The positions of terms are intersected to find common anchor strings. The anchor strings, once found, are used to calculate the initial affinity Fˆij of two sentences using Dice’s coefficient (7) i=1 j=1 n X j=1 Xij ≤ 1, Xij ∈ {0, 1} 2|C1i ∩ C2j | Fˆij = |C1i |+ |C2j | This turns sentence alignment into a problem to be resolved by binary linear programming (BIP), which has been successfully applied to word alignment (Taskar et al., 2005). Given a scoring matrix, it guarantees an optimal solution. 2.4 (8) where C1i and C2j are the anchor sets in si and tj , respectively, and |· |is the cardinality of a set. Apart from using anchor strings, other avenues for the initialization are studied in the evaluation section below, i.e., using another aligner and an existing lexicon. Alignment Initialization Once the above alignment function is available, the initial alignment matrix Aˆ can be derived from an initial relation matrix Fˆ obtained by an available alignment method. This work resorts to another approach to initializing the rel"
P13-1061,P91-1023,0,0.385877,"onmoAlign initialized with Hunalign (marked as NonmoAlign Hun) is also tested. The experimental results are presented in Figure 6. It shows that both Moore’s aligner and Hunalign work relatively well on bitexts with a low degree of nonmonotonicity, but their performance drops dramatically when the non-monotonicity is increased. Despite the improvement at low non-monotonicity by seeding our aligner with Hunalign, its performance decreases likewise when the degree of non-monotonicity increases, due to the quality de4 Related Work The research of sentence alignment originates in the early 1990s. Gale and Church (1991) and Brown (1991) report the early works using length statistics of bilingual sentences. The general idea is that the closer two sentences are in length, the more likely they are to align. A notable difference of their methods is that the former uses sentence 628 Type 1-1 1-0 Micro P 0.827 0.359 0.809 Moore R F1 0.828 0.827 0.329 0.343 0.807 0.808 P 0.999 0.330 0.961 Hunalign R 0.972 0.457 0.951 F1 0.986 0.383 0.956 NonmoAlign P R F1 0.987 0.987 0.987 0.729 0.729 0.729 0.976 0.976 0.976 Table 3: Performance of monotonic alignment in comparison with the baseline methods. namic programming algor"
P13-1061,P94-1012,0,0.200707,"ation (Brown et al., 1993), bilingual lexicography (Klavans et al., 1990), and cross-language information retrieval (Nie et al., 1999). Its objective is to identify correspondences between bilingual sentences in given bitexts. As summarized by Wu (2010), existing sentence alignment techniques rely mainly on sentence length and bilingual lexical resource. Approaches based on the former perform effectively on cognate languages but not on the others. For instance, the statistical correlation of sentence length between English and Chinese is not as high as that between two IndoEuropean languages (Wu, 1994). Lexicon-based 622 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 622–630, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 1. Interpretation of words and expressions 1.娆婆娆⎍䘬慳佑 "" British citizen"" ( 劙 ⚳ ℔ 㮹 ) means a person who has the status of a British citizen under the British Nationality Act 1981 (1981 c. 61 U.K.) Ⱦ⍿劙⚳ᾅ嬟Ṣ⢓ȿĩŃųŪŵŪŴũġűųŰŵŦŤŵŦťġűŦųŴŰůĪġ㊯㟡㒂˪ĲĺĹĲ⸜劙⚳⚳ 䯵㱽Ẍ˫ĩĲĺĹĲġŤįġķĲġŖįŌįĪ℟㚱⍿劙⚳ᾅ嬟Ṣ⢓幓↮䘬Ṣ ""British Dependent Territories citizen"" (劙⚳Ⱄ⛇℔㮹) means a person who has or had the status of a British Dependent"
P13-1061,I05-4010,1,0.867117,"Missing"
P13-1061,C90-3031,0,0.195217,"Missing"
P13-1061,P98-1117,0,0.139781,"Missing"
P13-1061,P10-1085,0,0.0166828,"liable information to link bilingual sentences into pairs, and thus can serve as useful cues for sentence alignment. In fact, they can be treated as a special type of highly reliable “bilexicon”. The anchor strings used in this work are derived by searching the bitexts using word-level inverted indexing, a basic technique widely used in information retrieval (Baeza-Yates and Ribeiro-Neto, 2011). For each index term, a list of postings is 2.5 Monolingual Affinity Although various kinds of information from a monolingual corpus have been exploited to boost statistical machine translation models (Liu et al., 2010; Su et al., 2012), we have not yet been exposed to any attempt to leverage monolingual sentence affinity for sentence alignment. In our framework, an attempt to this can be made through the computation of W and V . Let us take W as an example, where the entry Wij represents the affinity of sentence si and sentence sj , and it is set to 0 for i = j in order to avoid self-reinforcement during optimization (Zhou et al., 2004). When two sentences in S or T are not too short, or their content is not divergent in meaning, their semantic similarity can be estimated in terms of common words. Motivate"
P13-1061,J93-1004,0,\N,Missing
P13-1061,P12-1048,0,\N,Missing
P13-1061,J93-1006,0,\N,Missing
P13-1061,C98-1113,0,\N,Missing
P18-1222,councill-etal-2008-parscit,0,0.104443,"Micro Macro Micro Model DeepWalk - 66.57 76.56 66.57 76.56 w2v (I) w2v (I+O) ×/× ×/× 19.77 15.97 47.32 45.66 59.80 50.77 72.90 70.08 d2v-nc d2v-cac h-d2v (I) h-d2v (I+O) /  /  /  /  61.54 65.23 58.59 66.64 73.73 75.93 69.79 75.19 69.37 70.43 66.99 68.96 78.22 78.75 75.63 76.61 Table 5: F1 on DBLP when newcomers are discarded. Datasets and Experimental Settings We use three datasets from the academic paper domain, i.e., NIPS4 , ACL anthology5 and DBLP6 , as shown in Table 3. They all contain full text of papers, and are of small, medium, and large size, respectively. We apply ParsCit7 (Councill et al., 2008) to parse the citations and bibliography sections. Each identiﬁed citation string referring to a paper in the same dataset, e.g., [1] or (Author et al., 2018), is replaced by a global paper id. Consecutive citations like [1, 2] are regarded as multiple ground truths occupying one position. Following He et al. (2010), we take 50 words before and after a citation as the citation context. ˇ uˇrek and Sojka, 2010) is used to Gensim (Reh˚ implement all w2v and d2v baselines as well as h-d2v. We use cbow for w2v and pv-dbow for d2v, unless otherwise noted. For all three baselines, we set the (half)"
P18-1222,D07-1074,0,0.128488,"biquitous World Wide Web has boosted research interests on hypertext documents, e.g., personal webpages (Lu and Getoor, 2003), Wikipedia pages (Gabrilovich and Markovitch, 2007), as well as academic papers (Sugiyama and Kan, 2010). Unlike independent plain documents, a hypertext document (hyper-doc for short) links to another hyper-doc by a hyperlink or citation mark in its textual content. Given this essential distinction, hyperlinks or citations are worth speciﬁc modeling in many tasks such as link-based classiﬁcation (Lu and Getoor, 2003), web retrieval (Page et al., 1999), entity linking (Cucerzan, 2007), and citation recommendation (He et al., 2010). To model hypertext documents, various efforts (Cohn and Hofmann, 2000; Kataria et al., 2010; Perozzi et al., 2014; Zwicklbauer et al., 2016; Wang et al., 2016) have been made to depict networks of hyper-docs as well as their content. Among potential techniques, distributed representation (Mikolov et al., 2013; Le and Mikolov, 2014) tends to be promising since its validity and effectiveness are proven for plain documents on many natural language processing (NLP) tasks. Conventional attempts on utilizing embedding techniques in hyper-doc-related t"
P18-1222,K16-1026,0,0.019312,"task. Huang et al. (2015b) propose Neural Probabilistic Model (NPM) to tackle this problem with embeddings. Their model outperforms non-embedding ones (Kataria et al., 2010; Tang and Zhang, 2009; Huang et al., 2012). Ebesu and Fang (2017) also exploit neural networks for citation recommendation, but require author information as additional input. Compared with h-d2v, these models are limited in a task-speciﬁc setting. Embedding-based entity linking is another topic that exploits embeddings to model certain hyperdocs, i.e., Wikipedia (Huang et al., 2015a; Yamada et al., 2016; Sun et al., 2015; Fang et al., 2016; He et al., 2013; Zwicklbauer et al., 2016), for entity linking (Shen et al., 2015). It resembles citation recommendation in the sense that linked entities highly depend on the contexts. Meanwhile, it requires extra steps like candidate generation, and can beneﬁt from sophisticated techniques such as collective linking (Cucerzan, 2007). 3 Preliminaries We introduce notations and deﬁnitions, then formally deﬁne the embedding problem. We also propose four criteria for hyper-doc embedding models w.r.t their appropriateness and informativeness. 3.1 Notations and Deﬁnitions Let w ∈ W be a word fro"
P18-1222,N15-1184,0,0.0318277,"∈D exp(x d ) (3) To model contents’ impact on document vectors, we simply consider an additional objective function that is identical to pv-dm, i.e., enumerate words and contexts, and use the same input architecture as Figure 2 to predict the OUT vector of the current word. Such convenience owes to the fact that using two vectors makes the model parameters compatible with those of pv-dm. Note that combining the citation and content objectives leads to a joint learning framework. To facilitate easier and faster training, we adopt an alternative pre-training/ﬁne-tuning or retroﬁtting framework (Faruqui et al., 2015). We initialize with a predeﬁned number of pv-dm iterations, and then optimize Eq. 1 based on the initialization. Finally, similar to w2v (Mikolov et al., 2013) and d2v (Le and Mikolov, 2014), to make training efﬁcient, we adopt negative sampling: log σ(x dO t )+ n  Edi ∼PN (d) log σ(−x dO i ) i=1 (4) and use it to replace every log P (dt |ds , C). Following Huang et al. (2015b), we adopt a uniform distribution on D as the distribution PN (d). Unlike the other models in Table 1, h-d2v satisﬁes all four criteria. We refer to the example in Figure 2 to make the points clear. First, when optim"
P18-1222,P13-2006,0,0.0268736,"(2015b) propose Neural Probabilistic Model (NPM) to tackle this problem with embeddings. Their model outperforms non-embedding ones (Kataria et al., 2010; Tang and Zhang, 2009; Huang et al., 2012). Ebesu and Fang (2017) also exploit neural networks for citation recommendation, but require author information as additional input. Compared with h-d2v, these models are limited in a task-speciﬁc setting. Embedding-based entity linking is another topic that exploits embeddings to model certain hyperdocs, i.e., Wikipedia (Huang et al., 2015a; Yamada et al., 2016; Sun et al., 2015; Fang et al., 2016; He et al., 2013; Zwicklbauer et al., 2016), for entity linking (Shen et al., 2015). It resembles citation recommendation in the sense that linked entities highly depend on the contexts. Meanwhile, it requires extra steps like candidate generation, and can beneﬁt from sophisticated techniques such as collective linking (Cucerzan, 2007). 3 Preliminaries We introduce notations and deﬁnitions, then formally deﬁne the embedding problem. We also propose four criteria for hyper-doc embedding models w.r.t their appropriateness and informativeness. 3.1 Notations and Deﬁnitions Let w ∈ W be a word from a vocabulary W"
P18-1222,P07-2045,0,0.0194057,"ally deﬁne the embedding problem. We also propose four criteria for hyper-doc embedding models w.r.t their appropriateness and informativeness. 3.1 Notations and Deﬁnitions Let w ∈ W be a word from a vocabulary W , and d ∈ D be a document id (e.g., web page URLs and paper DOIs) from an id collection D. After ﬁltering out non-textual content, a hyper-document H is reorganized as a sequence of words and doc ids, 2385 Source doc ௦ (Zhao and Gildea, 2010) Context words ܥ … We also evaluate our model by computing the machine translation BLEU score (Papineni et al., 2002) using the Moses system (Koehn et al., 2007) … Target (Papineni et al., 2002) doc ௧ (Koehn et al., 2007) … … Original (a) Hyper-documents. (Zhao and Gildea, 2010) …We also evaluate our model by computing the machine translation BLEU score (Papineni et al., 2002) using the Moses system (Koehn et al., 2007)… … “Word” Vectors evaluate w2v BLEU … Citation as word (b) Citation as word. Word Vectors evaluate BLEU d2v … Doc Vectors (Zhao and (Koehn et al., 2007) Gildea, 2010) (Papineni et al., 2002) (Papineni et al., 2012) … …We also evaluate our model by computing the machine translation BLEU score using the Moses system … … machine translat"
P18-1222,P02-1040,0,0.119571,"e introduce notations and deﬁnitions, then formally deﬁne the embedding problem. We also propose four criteria for hyper-doc embedding models w.r.t their appropriateness and informativeness. 3.1 Notations and Deﬁnitions Let w ∈ W be a word from a vocabulary W , and d ∈ D be a document id (e.g., web page URLs and paper DOIs) from an id collection D. After ﬁltering out non-textual content, a hyper-document H is reorganized as a sequence of words and doc ids, 2385 Source doc ௦ (Zhao and Gildea, 2010) Context words ܥ … We also evaluate our model by computing the machine translation BLEU score (Papineni et al., 2002) using the Moses system (Koehn et al., 2007) … Target (Papineni et al., 2002) doc ௧ (Koehn et al., 2007) … … Original (a) Hyper-documents. (Zhao and Gildea, 2010) …We also evaluate our model by computing the machine translation BLEU score (Papineni et al., 2002) using the Moses system (Koehn et al., 2007)… … “Word” Vectors evaluate w2v BLEU … Citation as word (b) Citation as word. Word Vectors evaluate BLEU d2v … Doc Vectors (Zhao and (Koehn et al., 2007) Gildea, 2010) (Papineni et al., 2002) (Papineni et al., 2012) … …We also evaluate our model by computing the machine translation BLEU score"
P18-1222,K16-1025,0,0.0934266,") simply downcasts hyper-docs to plain documents and feeds them into word2vec (Mikolov et al., 2013) (w2v for short) or doc2vec (Le and Mikolov, 2014) (d2v for short). These approaches involve downgrading hyperlinks and inevitably omit certain information in hyper-docs. However, no previous work investigates the information loss, and how it affects the performance of such downcasting-based adaptations. The second type designs sophisticated embedding models to fulﬁll certain tasks, e.g., citation recommendation (Huang et al., 2015b), paper classiﬁcation (Wang et al., 2016), and entity linking (Yamada et al., 2016), etc. These models are limited to speciﬁc tasks, and it is yet unknown whether embeddings learned for those particular tasks can generalize to others. Based on the above facts, we are interested in two questions: • What information should hyper-doc embedding models preserve, and what nice property should they possess? • Is there a general approach to learning taskindependent embeddings of hyper-docs? To answer the two questions, we formalize the hyper-doc embedding task, and propose four criteria, i.e., content awareness, context awareness, newcomer friendliness, and context intent aware2384"
P18-1222,D10-1058,0,0.162956,"cilitate applications like hyper-doc classiﬁcation and citation recommendation. 3.3 are not referred to by any hyperlink in other hyper-docs. If such “newcomers” do not get embedded properly, downstream tasks involving them are infeasible or deteriorated. Criteria for Embedding Models A reasonable model should learn how contents and hyperlinks in hyper-docs impact both D and W. We propose the following criteria for models: • Content aware. Content words of a hyperdoc play the main role in describing it, so the document representation should depend on its own content. For example, the words in Zhao and Gildea (2010) should affect and contribute to its embedding. • Context aware. Hyperlink contexts usually provide a summary for the target document. Therefore, the target document’s vector should be impacted by words that others use to summarize it, e.g., paper Papineni et al. (2002) and the word “BLEU” in Figure 1(a). • Newcomer friendly. In a hyper-document network, it is inevitable that some documents We note that the ﬁrst three criteria are for hyperdocs, while the last one is desired for word vectors. 4 Representing Hypertext Documents In this section, we ﬁrst give the background of two prevailing tech"
P19-1083,E09-1018,0,0.0431558,"1995), dialog systems (Strube and M¨uller, 2003), information extraction (Edens et al., 2003), and summarization (Steinberger et al., 2007), etc. In general, to resolve pronoun coreferences, one needs intensive knowledge support. As shown in Table 1, answering the first question requires the knowledge on which object can be eaten (apple v.s. table), while the second question requires the knowledge that the CT scan is a test (not the hospital) and only tests can show something. Previously, rule-based (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998; Zhang et al., 2019a) and feature-based (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) supervised models were proposed to integrate knowledge to this task. However, while easy to incorporate external knowledge, these traditional methods faced the problem of no effective representation learning models can handle such complex knowledge. Later, end-toend solutions with neural models (Lee et al., 2017, 2018) achieved good performance on the general coreference resolution task. Although such algoIntroduction Being an important human language phenomenon, coreference brings simplicity for human languages while introducing a huge challenge for machines to process, esp"
P19-1083,P15-1136,0,0.0462386,"plurality feature denotes each s and p to be singular or plural. The animacy & gender (AG) feature denotes whether the n or p is a living object, and being male, female, or neutral if it is alive. For example, a mention ‘the girls’ is labeled as plural and female; we use triplets (‘the girls’, plurality, Plural) and (‘the girls’, AG, female) to represent them. As a result, we have 40,149 and 40,462 triplets for plurality and AG, respectively. • Deterministic model (Raghunathan et al., 2010), which is an unsupervised model and leverages manual rules to detect coreferences. • Statistical model (Clark and Manning, 2015), which is a supervised model and trained on manually crafted entity-level features between clusters and mentions. • Deep-RL model (Clark and Manning, 2016), which uses reinforcement learning to directly optimize the coreference matrix instead of the loss function of supervised learning. The above models are included in the Stanford CoreNLP toolkit9 . We also include a state-of-theart end-to-end neural model as one of our baselines: Selectional Preference (SP). Selectional preference (Hobbs, 1978) knowledge is employed as the last knowledge resource, which is the semantic constraint for word u"
P19-1083,D16-1245,0,0.0444309,"male, female, or neutral if it is alive. For example, a mention ‘the girls’ is labeled as plural and female; we use triplets (‘the girls’, plurality, Plural) and (‘the girls’, AG, female) to represent them. As a result, we have 40,149 and 40,462 triplets for plurality and AG, respectively. • Deterministic model (Raghunathan et al., 2010), which is an unsupervised model and leverages manual rules to detect coreferences. • Statistical model (Clark and Manning, 2015), which is a supervised model and trained on manually crafted entity-level features between clusters and mentions. • Deep-RL model (Clark and Manning, 2016), which uses reinforcement learning to directly optimize the coreference matrix instead of the loss function of supervised learning. The above models are included in the Stanford CoreNLP toolkit9 . We also include a state-of-theart end-to-end neural model as one of our baselines: Selectional Preference (SP). Selectional preference (Hobbs, 1978) knowledge is employed as the last knowledge resource, which is the semantic constraint for word usage. SP generally refers to that, given a predicate (e.g., verb), people have the preference for the argument (e.g., its object or subject) connected. To c"
P19-1083,P81-1019,0,0.771448,"r, while easy to incorporate external knowledge, these traditional methods faced the problem of no effective representation learning models can handle such complex knowledge. Later, end-toend solutions with neural models (Lee et al., 2017, 2018) achieved good performance on the general coreference resolution task. Although such algoIntroduction Being an important human language phenomenon, coreference brings simplicity for human languages while introducing a huge challenge for machines to process, especially for pronouns, which are hard to be interpreted owing to their weak semantic meanings (Ehrlich, 1981). As one challenging yet vital subtask of the general coreference resolution, pronoun coreference resolution (Hobbs, 1978) is to find the correct reference for a given pronominal anaphor in the context and has showed its importance in many natural language processing (NLP) ∗ This work was partially done during the internship of the first author in Tencent AI Lab. 867 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 867–876 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics rithms can effectively incorporate"
P19-1083,D10-1048,0,0.109095,"the noun phrases, so as to automatically generate linguistic knowledge (in the form of triplets) for our data. Specifically, the plurality feature denotes each s and p to be singular or plural. The animacy & gender (AG) feature denotes whether the n or p is a living object, and being male, female, or neutral if it is alive. For example, a mention ‘the girls’ is labeled as plural and female; we use triplets (‘the girls’, plurality, Plural) and (‘the girls’, AG, female) to represent them. As a result, we have 40,149 and 40,462 triplets for plurality and AG, respectively. • Deterministic model (Raghunathan et al., 2010), which is an unsupervised model and leverages manual rules to detect coreferences. • Statistical model (Clark and Manning, 2015), which is a supervised model and trained on manually crafted entity-level features between clusters and mentions. • Deep-RL model (Clark and Manning, 2016), which uses reinforcement learning to directly optimize the coreference matrix instead of the loss function of supervised learning. The above models are included in the Stanford CoreNLP toolkit9 . We also include a state-of-theart end-to-end neural model as one of our baselines: Selectional Preference (SP). Selec"
P19-1083,P11-1082,0,0.0351462,", for natural language understanding. Mention detection and coreference prediction are the two major focuses of the task as listed in Lee et al. (2017). Compared to general coreference problem, pronoun coreference resolution has its unique challenge since pronouns themselves have weak semantics meanings, which make it the most challenging sub-task in general coreference resolution. To address the unique difficulty brought by pronouns, we thus focus on resolving pronoun coreferences in this paper. Resolving pronoun coreference relations often requires the support of manually crafted knowledge (Rahman and Ng, 2011; Emami et al., 2018), 11 We omit the intermediate part of the long sentence in the table for a clear presentation. 874 better and more robust performance than state-ofthe-art models in the cross-domain scenario. especially for particular domains such as medicine (Uzuner et al., 2012) and biology (Cohen et al., 2017). Previous studies on pronoun coreference resolution incorporated external knowledge including human defined rules (Hobbs, 1978; Ng, 2005), e.g., number/gender requirement of different pronouns, domain-specific knowledge such as medical (Jindal and Roth, 2013) or biological (Trieu"
P19-1083,N18-2108,0,0.35483,"electional Preference (SP). Selectional preference (Hobbs, 1978) knowledge is employed as the last knowledge resource, which is the semantic constraint for word usage. SP generally refers to that, given a predicate (e.g., verb), people have the preference for the argument (e.g., its object or subject) connected. To collect SP knowledge, we first parse the English Wikipedia7 with the Stanford parser and extract all dependency edges in the format of (predicate, argument, relation, number), where predicate is the governor and argument the dependent in each dependency edge8 . Following • End2end (Lee et al., 2018), which is the current state-of-the-art model performing in an end-toend manner and leverages both contextual information and a pre-trained language model (Peters et al., 2018). We use their released code10 . In addition, to show the importance of incorporating knowledge, we also experiment with two variations of our model: • Without KG removes the KG component and keeps all other components in the same setting as that in our complete model. 6 am, is); the predicative is thus treated as the predicate for the subject (argument) in this paper. 9 https://stanfordnlp.github.io/CoreNLP/coref.html 1"
P19-1083,P11-1117,0,0.0529445,"Missing"
P19-1083,N18-2028,1,0.77866,"ing embeddings of all words in its tail. For example, if s is ‘the apple’ and the knowledge triplet (‘the apple’, IsA, ‘healthy food’) is found by searching the KG, we represent this relation from the averaged embeddings of ‘healthy’ and ‘food’. Consequently, for s and p, we denote their retrieved knowledge set as Ks and Kp respectively, where Ks contains ms related knowledge embeddings k1,s , k2,s , ..., kms ,s and Kp contains mp of them k1,p , k2,p , ..., kmp ,p . Contextual information is crucial to distinguish the semantics of a word or phrase, especially for text representation learning (Song et al., 2018; Song and Shi, 2018). In this work, a standard bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) model is used to encode each span with attentions (Bahdanau et al., 2014), which is similar to the one used in Lee et al. (2017). The structure is shown in Figure 2. Let initial word embeddings in a span si be denoted as x1 , ..., xT and their encoded representation be x∗1 , ..., x∗T . The weighted embeddings of each span xˆi is obtained by T X (3) Thus the span representation of s and p are marked as es and ep , respectively. Span Representation xˆi = , where αt is a standard feed-fo"
P19-1083,W18-2315,1,0.841852,"for Computational Linguistics, pages 867–876 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics rithms can effectively incorporate contextual information from large-scale external unlabeled data into the model, they are insufficient to incorporate existing complex knowledge into the representation for covering all the knowledge one needs to build a successful pronoun coreference system. In addition, overfitting is always observed on deep models, whose performance is thus limited in cross-domain scenarios and restricts their usage in real applications (Liu et al., 2018, 2019). Recently, a joint model (Zhang et al., 2019b) was proposed to connect the contextual information and human-designed features together for pronoun coreference resolution task (with gold mention support) and achieved the state-of-the-art performance. However, their model still requires the complex features designed by experts, which is expensive and difficult to acquire, and requires the support of the gold mentions. To address the limitations of the aforementioned models, in this paper, we propose a novel end-toend model that learns to resolve pronoun coreferences with general knowledg"
P19-1083,P19-1189,1,0.884331,"Missing"
P19-1083,P03-1022,0,0.665428,"Missing"
P19-1083,W18-2324,0,0.0308416,", 2011; Emami et al., 2018), 11 We omit the intermediate part of the long sentence in the table for a clear presentation. 874 better and more robust performance than state-ofthe-art models in the cross-domain scenario. especially for particular domains such as medicine (Uzuner et al., 2012) and biology (Cohen et al., 2017). Previous studies on pronoun coreference resolution incorporated external knowledge including human defined rules (Hobbs, 1978; Ng, 2005), e.g., number/gender requirement of different pronouns, domain-specific knowledge such as medical (Jindal and Roth, 2013) or biological (Trieu et al., 2018) ones, and world knowledge (Rahman and Ng, 2011), such as selectional preference (Wilks, 1975). Later, end-to-end solutions (Lee et al., 2017, 2018) were proposed to learn contextual information and solve coreferences synchronously with neural networks, e.g., LSTM. Their results proved that such knowledge is helpful when appropriately used for coreference resolution. However, external knowledge is often omitted in their models. Consider that context and external knowledge have their own advantages: the contextual information covering diverse text expressions that are difficult to be predefined"
P19-1083,P98-2143,0,0.505205,"respectively. tasks, such as machine translation (Mitkov et al., 1995), dialog systems (Strube and M¨uller, 2003), information extraction (Edens et al., 2003), and summarization (Steinberger et al., 2007), etc. In general, to resolve pronoun coreferences, one needs intensive knowledge support. As shown in Table 1, answering the first question requires the knowledge on which object can be eaten (apple v.s. table), while the second question requires the knowledge that the CT scan is a test (not the hospital) and only tests can show something. Previously, rule-based (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998; Zhang et al., 2019a) and feature-based (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) supervised models were proposed to integrate knowledge to this task. However, while easy to incorporate external knowledge, these traditional methods faced the problem of no effective representation learning models can handle such complex knowledge. Later, end-toend solutions with neural models (Lee et al., 2017, 2018) achieved good performance on the general coreference resolution task. Although such algoIntroduction Being an important human language phenomenon, coreference brings simplicity for hu"
P19-1083,1995.tmi-1.6,0,0.49878,"Missing"
P19-1083,C94-2189,0,0.612771,"ine blue fonts, respectively. tasks, such as machine translation (Mitkov et al., 1995), dialog systems (Strube and M¨uller, 2003), information extraction (Edens et al., 2003), and summarization (Steinberger et al., 2007), etc. In general, to resolve pronoun coreferences, one needs intensive knowledge support. As shown in Table 1, answering the first question requires the knowledge on which object can be eaten (apple v.s. table), while the second question requires the knowledge that the CT scan is a test (not the hospital) and only tests can show something. Previously, rule-based (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998; Zhang et al., 2019a) and feature-based (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) supervised models were proposed to integrate knowledge to this task. However, while easy to incorporate external knowledge, these traditional methods faced the problem of no effective representation learning models can handle such complex knowledge. Later, end-toend solutions with neural models (Lee et al., 2017, 2018) achieved good performance on the general coreference resolution task. Although such algoIntroduction Being an important human language phenomenon, coreference brings sim"
P19-1083,D14-1162,0,0.0891682,"efer to complex things and occur with low frequency. Moreover, there are significant gaps in the performance of different models, with the following observations. First, models with manually defined rules or features, which cannot cover rich contextual information, perform poorly. In contrast, deep learning models (e.g., End2end and our proposed models), which leverage text representations for context, outperform other approaches by a great margin, especially on the recall. SecImplementation Following the previous work (Lee et al., 2018), we use the concatenation of the 300d GloVe embeddings (Pennington et al., 2014) and the ELMo (Peters et al., 2018) embeddings as the initial word representations for computing span representations. For knowledge triplets, we use the GloVe embeddings to encode tail words in them. Outof-vocabulary words are initialized with zero vectors. The hidden state of the LSTM module is set to 200, and all the feed-forward networks have two 150-dimension hidden layers. The selection thresholds are set to 10−2 and 10−8 for the CoNLL and i2b2 dataset, respectively. For model training, we use cross-entropy as the loss function and Adam (Kingma and Ba, 2014) as the optimizer. All the afo"
P19-1083,N18-1202,0,0.357819,"nerally refers to that, given a predicate (e.g., verb), people have the preference for the argument (e.g., its object or subject) connected. To collect SP knowledge, we first parse the English Wikipedia7 with the Stanford parser and extract all dependency edges in the format of (predicate, argument, relation, number), where predicate is the governor and argument the dependent in each dependency edge8 . Following • End2end (Lee et al., 2018), which is the current state-of-the-art model performing in an end-toend manner and leverages both contextual information and a pre-trained language model (Peters et al., 2018). We use their released code10 . In addition, to show the importance of incorporating knowledge, we also experiment with two variations of our model: • Without KG removes the KG component and keeps all other components in the same setting as that in our complete model. 6 am, is); the predicative is thus treated as the predicate for the subject (argument) in this paper. 9 https://stanfordnlp.github.io/CoreNLP/coref.html 10 https://github.com/kentonl/e2e-coref https://stanfordnlp.github.io/CoreNLP/ https://dumps.wikimedia.org/enwiki/ 8 In the Stanford parser, an ‘nsubj’ edge is created between i"
P19-1083,N19-1093,1,0.547847,"tasks, such as machine translation (Mitkov et al., 1995), dialog systems (Strube and M¨uller, 2003), information extraction (Edens et al., 2003), and summarization (Steinberger et al., 2007), etc. In general, to resolve pronoun coreferences, one needs intensive knowledge support. As shown in Table 1, answering the first question requires the knowledge on which object can be eaten (apple v.s. table), while the second question requires the knowledge that the CT scan is a test (not the hospital) and only tests can show something. Previously, rule-based (Hobbs, 1978; Nasukawa, 1994; Mitkov, 1998; Zhang et al., 2019a) and feature-based (Ng, 2005; Charniak and Elsner, 2009; Li et al., 2011) supervised models were proposed to integrate knowledge to this task. However, while easy to incorporate external knowledge, these traditional methods faced the problem of no effective representation learning models can handle such complex knowledge. Later, end-toend solutions with neural models (Lee et al., 2017, 2018) achieved good performance on the general coreference resolution task. Although such algoIntroduction Being an important human language phenomenon, coreference brings simplicity for human languages while"
P19-1083,W12-4501,0,0.114752,"in certain contexts. To solve it, a knowledge attention module is proposed to select the appropriate knowledge. For each pair of (s, p), as shown in Figure 3, we first concatenate es and ep to get the overall (span, pronoun) representation es,p , which is used to select knowledge for both s and p. Taking that for s as example, we compute the weight of each ki ∈ Ks by wi = P βki e kj ∈Ks e βkj , Dataset X wi · k i . 7,749 1,007 1,037 2,229 222 321 31,806 3,747 4,078 i2b2 train test 2,024 1,244 685 367 270 166 2,979 1,777 30,334 10,845 3,208 44,387 Datasets • CoNLL: The CoNLL-2012 shared task (Pradhan et al., 2012) corpus, which is a widely used dataset selected from the Ontonotes 5.05 . • i2b2: The i2b2 shared task dataset (Uzuner et al., 2012), consisting of electronic medical records from two different organizations, namely, Partners HealthCare (Part) and Beth Israel Deaconess medical center (Beth). All records have been fully de-identified and manually annotated with coreferences. (6) We split the datasets into different proportions based on their original settings. Three types of pronouns are considered in this paper following Ng (2005), i.e., third personal pronoun (e.g., she, her, he, him, them,"
P19-1083,C98-2138,0,\N,Missing
P19-1189,D11-1033,0,0.0317964,"2015, 2017; Shu et al., 2018; Shankar et al., 2018). Their methodologies follow several mainstreams such as representation learning (Glorot et al., 2011; Chen et al., 2012; Baktashmotlagh et al., 2013; Song and Shi, 2018; Zhao et al., 2017), reweighing samples from the source domain (Borgwardt et al., 2006; Daum´e III, 2007; Song and Xia, 2013), and feature space transformation (Gopalan et al., 2011; Pan et al., 2011; Long et al., 2013), etc. Normally, the transferable knowledge across domains are derived from some certain data, while others contribute less and are costly to be learned from (Axelrod et al., 2011; Ruder and Plank, 2017). Thus, previous studies conduct domain adaptation through selecting relative and informative source data according to the nature of the target domain, via entropy-based methods (Song et al., 2012), Bayesian optimization (Ruder and Plank, 2017), etc. Particularly for NLP, TDS are proved to be effective in various tasks, such as in language modeling (Moore and Lewis, 2010), word segmentation (Song and Xia, 2012; Song et al., 2012), machine translation (Chen et al., 2016; van der Wees et al., Conclusion In this paper, we proposed a general TDS framework for domain adaptat"
P19-1189,C18-1070,0,0.0425056,"Missing"
P19-1189,P07-1056,0,0.897461,"is effective for learning across domains (Ruder and Plank, 2017) by preventing negative transfer from irrelevant samples and noisy labels (Rosenstein et al., 2005) while achieving equivalent performance with less computational efforts (Fan et al., 2017; Feng et al., 2018), especially when compared with learning-intensive domain adaptation methods such as sample reweighing (Borgwardt et al., 2006), feature distribution matching (Tzeng et al., 2014) and representation learning (Csurka, 2017). Although various TDS-based domain adaptation approaches were proposed for NLP tasks (Daum´e III, 2007; Blitzer et al., 2007a; Søgaard, 2011), most of them only consider scoring or ranking training data under a certain metric over the entire dataset, and then select the top n (or a proportion, which is a predefined hyper-parameter) items to learn. However, such pre-designed metrics are, always, neither able to cover effective characteristics for transferring domain knowledge nor can be applied in different data nature. Even though there exists a versatile metric, its hyper-parameter setting still demands further explorations. Moreover, conventional TDS is separate from model training, which requires more steps befo"
P19-1189,W06-1615,0,0.387644,"Missing"
P19-1189,P11-1014,0,0.103463,"P tasks. 1 Introduction Learning with massive data suffers from “Pyrrhic victory” where huge amounts of resource, e.g., computation, annotation, storage, etc., are consumed with many issues, one of which is that data quality considerably affects the performance of learned models. Especially in natural language ∗ This work was done during the internship of Miaofeng Liu and Hongbin Zou at Tencent AI Lab. † Corresponding authors. processing (NLP), such phenomenon is incredibly significant where noise and inaccurate annotations are demolishing models’ robustness when applying them across domains (Bollegala et al., 2011; Plank and Van Noord, 2011; Song and Xia, 2013; Ruder and Plank, 2018; Liu et al., 2018). Statistically, distribution mismatch is often observed between training and test data in such case. As a straightforward solution to reduce the impact of the mismatch, TDS is effective for learning across domains (Ruder and Plank, 2017) by preventing negative transfer from irrelevant samples and noisy labels (Rosenstein et al., 2005) while achieving equivalent performance with less computational efforts (Fan et al., 2017; Feng et al., 2018), especially when compared with learning-intensive domain adaptat"
P19-1189,2016.amta-researchers.8,0,0.0367906,"are derived from some certain data, while others contribute less and are costly to be learned from (Axelrod et al., 2011; Ruder and Plank, 2017). Thus, previous studies conduct domain adaptation through selecting relative and informative source data according to the nature of the target domain, via entropy-based methods (Song et al., 2012), Bayesian optimization (Ruder and Plank, 2017), etc. Particularly for NLP, TDS are proved to be effective in various tasks, such as in language modeling (Moore and Lewis, 2010), word segmentation (Song and Xia, 2012; Song et al., 2012), machine translation (Chen et al., 2016; van der Wees et al., Conclusion In this paper, we proposed a general TDS framework for domain adaptation via reinforcement learning, which matches the representations of the selected data from the source domain and the guidance set from the target domain and pass the similarity at different steps as rewards to guide a selection distribution generator. Through the generator, different instances from the source domain are selected to train a task-specific predictor. To this end, not only those data relevant to the target domain are selected, but also task- and domain-specific representations a"
P19-1189,P07-1033,0,0.376781,"Missing"
P19-1189,D14-1181,0,0.0105485,"Missing"
P19-1189,Q16-1023,0,0.0514922,"Missing"
P19-1189,P16-2067,0,0.0608347,"Missing"
P19-1189,W18-2315,1,0.611526,"mounts of resource, e.g., computation, annotation, storage, etc., are consumed with many issues, one of which is that data quality considerably affects the performance of learned models. Especially in natural language ∗ This work was done during the internship of Miaofeng Liu and Hongbin Zou at Tencent AI Lab. † Corresponding authors. processing (NLP), such phenomenon is incredibly significant where noise and inaccurate annotations are demolishing models’ robustness when applying them across domains (Bollegala et al., 2011; Plank and Van Noord, 2011; Song and Xia, 2013; Ruder and Plank, 2018; Liu et al., 2018). Statistically, distribution mismatch is often observed between training and test data in such case. As a straightforward solution to reduce the impact of the mismatch, TDS is effective for learning across domains (Ruder and Plank, 2017) by preventing negative transfer from irrelevant samples and noisy labels (Rosenstein et al., 2005) while achieving equivalent performance with less computational efforts (Fan et al., 2017; Feng et al., 2018), especially when compared with learning-intensive domain adaptation methods such as sample reweighing (Borgwardt et al., 2006), feature distribution matc"
P19-1189,P11-1157,0,0.0498213,"Missing"
P19-1189,P10-2041,0,0.0461813,"., 2011; Pan et al., 2011; Long et al., 2013), etc. Normally, the transferable knowledge across domains are derived from some certain data, while others contribute less and are costly to be learned from (Axelrod et al., 2011; Ruder and Plank, 2017). Thus, previous studies conduct domain adaptation through selecting relative and informative source data according to the nature of the target domain, via entropy-based methods (Song et al., 2012), Bayesian optimization (Ruder and Plank, 2017), etc. Particularly for NLP, TDS are proved to be effective in various tasks, such as in language modeling (Moore and Lewis, 2010), word segmentation (Song and Xia, 2012; Song et al., 2012), machine translation (Chen et al., 2016; van der Wees et al., Conclusion In this paper, we proposed a general TDS framework for domain adaptation via reinforcement learning, which matches the representations of the selected data from the source domain and the guidance set from the target domain and pass the similarity at different steps as rewards to guide a selection distribution generator. Through the generator, different instances from the source domain are selected to train a task-specific predictor. To this end, not only those da"
P19-1189,P18-2064,0,0.0175185,"n a highly similar distribution as that in the target domain (Figure 3(d)), with matched shape between the points in red and other colors. Such visualization confirms that our TDS framework not only selects the most appropriate instances (similar in the distribution shape), but also learns better representations (located at the similar positions of target domain instances) for them with respect to the target domain, which further illustrates the validity and effectiveness of joint selecting and learning from training instances for domain adaptation. 5 Related Work 2017), and multilingual NER (Murthy et al., 2018). Recently, RL and representation learning provided new possibilities for TDS. For example, Fan et al. (2017) proposed to allocate appropriate training data at different training stages, which helps achieving comparative accuracy with less computational efforts compared with the model trained on the entire data. Feng et al. (2018) used sequential one-step actions for each single instance where every action is decided based on the previous one. As a result, their selection becomes a consuming process where the complexity is determined by the amount of the source data. For representation learnin"
P19-1189,D17-1038,0,0.554188,"done during the internship of Miaofeng Liu and Hongbin Zou at Tencent AI Lab. † Corresponding authors. processing (NLP), such phenomenon is incredibly significant where noise and inaccurate annotations are demolishing models’ robustness when applying them across domains (Bollegala et al., 2011; Plank and Van Noord, 2011; Song and Xia, 2013; Ruder and Plank, 2018; Liu et al., 2018). Statistically, distribution mismatch is often observed between training and test data in such case. As a straightforward solution to reduce the impact of the mismatch, TDS is effective for learning across domains (Ruder and Plank, 2017) by preventing negative transfer from irrelevant samples and noisy labels (Rosenstein et al., 2005) while achieving equivalent performance with less computational efforts (Fan et al., 2017; Feng et al., 2018), especially when compared with learning-intensive domain adaptation methods such as sample reweighing (Borgwardt et al., 2006), feature distribution matching (Tzeng et al., 2014) and representation learning (Csurka, 2017). Although various TDS-based domain adaptation approaches were proposed for NLP tasks (Daum´e III, 2007; Blitzer et al., 2007a; Søgaard, 2011), most of them only consider"
P19-1189,P18-1096,0,0.0149383,"c victory” where huge amounts of resource, e.g., computation, annotation, storage, etc., are consumed with many issues, one of which is that data quality considerably affects the performance of learned models. Especially in natural language ∗ This work was done during the internship of Miaofeng Liu and Hongbin Zou at Tencent AI Lab. † Corresponding authors. processing (NLP), such phenomenon is incredibly significant where noise and inaccurate annotations are demolishing models’ robustness when applying them across domains (Bollegala et al., 2011; Plank and Van Noord, 2011; Song and Xia, 2013; Ruder and Plank, 2018; Liu et al., 2018). Statistically, distribution mismatch is often observed between training and test data in such case. As a straightforward solution to reduce the impact of the mismatch, TDS is effective for learning across domains (Ruder and Plank, 2017) by preventing negative transfer from irrelevant samples and noisy labels (Rosenstein et al., 2005) while achieving equivalent performance with less computational efforts (Fan et al., 2017; Feng et al., 2018), especially when compared with learning-intensive domain adaptation methods such as sample reweighing (Borgwardt et al., 2006), featur"
P19-1189,P11-2120,0,0.033912,"ing across domains (Ruder and Plank, 2017) by preventing negative transfer from irrelevant samples and noisy labels (Rosenstein et al., 2005) while achieving equivalent performance with less computational efforts (Fan et al., 2017; Feng et al., 2018), especially when compared with learning-intensive domain adaptation methods such as sample reweighing (Borgwardt et al., 2006), feature distribution matching (Tzeng et al., 2014) and representation learning (Csurka, 2017). Although various TDS-based domain adaptation approaches were proposed for NLP tasks (Daum´e III, 2007; Blitzer et al., 2007a; Søgaard, 2011), most of them only consider scoring or ranking training data under a certain metric over the entire dataset, and then select the top n (or a proportion, which is a predefined hyper-parameter) items to learn. However, such pre-designed metrics are, always, neither able to cover effective characteristics for transferring domain knowledge nor can be applied in different data nature. Even though there exists a versatile metric, its hyper-parameter setting still demands further explorations. Moreover, conventional TDS is separate from model training, which requires more steps before an adapted mod"
P19-1189,C12-2116,1,0.856665,"Zhao et al., 2017), reweighing samples from the source domain (Borgwardt et al., 2006; Daum´e III, 2007; Song and Xia, 2013), and feature space transformation (Gopalan et al., 2011; Pan et al., 2011; Long et al., 2013), etc. Normally, the transferable knowledge across domains are derived from some certain data, while others contribute less and are costly to be learned from (Axelrod et al., 2011; Ruder and Plank, 2017). Thus, previous studies conduct domain adaptation through selecting relative and informative source data according to the nature of the target domain, via entropy-based methods (Song et al., 2012), Bayesian optimization (Ruder and Plank, 2017), etc. Particularly for NLP, TDS are proved to be effective in various tasks, such as in language modeling (Moore and Lewis, 2010), word segmentation (Song and Xia, 2012; Song et al., 2012), machine translation (Chen et al., 2016; van der Wees et al., Conclusion In this paper, we proposed a general TDS framework for domain adaptation via reinforcement learning, which matches the representations of the selected data from the source domain and the guidance set from the target domain and pass the similarity at different steps as rewards to guide a se"
P19-1189,N18-2028,1,0.822969,"eatures (T-S+D) outperform other Bayesian optimization baselines. Our models are also shown to be superior than measurement-based as well as neural models significantly in most domains. However, different from POS tagging, in this task, the predictor trained on the entire source data still performs the best on some domains, which can be explained by the complexity of the task. To precisely predict structured parsing results, in spite of noise from different domains, large amount of data might be more helpful because various contextual information is beneficial in text representation learning (Song et al., 2018). In this case, selection based methods sacrifice accuracy for their efficiency with less data. B D E K JS-E JS-D T-S T O -S T+T O -S T-S+D T O -S+D 72.49 75.28 75.39 76.07 75.75 76.20 77.16 68.21 73.75 76.27 75.92 76.62 77.60 79.00 76.78 72.53 81.91 81.69 81.74 82.66 81.92 77.54 80.05 83.41 83.06 83.39 84.98 84.29 SCL SST DAM SDAMS-LS SDAMS-SVM 74.57 76.32 75.61 77.95 77.86 76.30 78.77 77.57 78.80 79.02 78.93 83.57 82.79 83.98 84.18 82.07 85.19 84.23 85.96 85.78 R ANDOM A LL 76.78 75.28 78.25 82.27 78.48 79.68 80.58 84.50 SDG (JS) SDG (MMD) SDG (R E´ NYI ) SDG (L OSS ) 79.37 79.57 80.07 79.57"
P19-1189,N18-1112,0,0.0321913,"Missing"
P19-1189,song-xia-2012-using,1,0.826372,"3), etc. Normally, the transferable knowledge across domains are derived from some certain data, while others contribute less and are costly to be learned from (Axelrod et al., 2011; Ruder and Plank, 2017). Thus, previous studies conduct domain adaptation through selecting relative and informative source data according to the nature of the target domain, via entropy-based methods (Song et al., 2012), Bayesian optimization (Ruder and Plank, 2017), etc. Particularly for NLP, TDS are proved to be effective in various tasks, such as in language modeling (Moore and Lewis, 2010), word segmentation (Song and Xia, 2012; Song et al., 2012), machine translation (Chen et al., 2016; van der Wees et al., Conclusion In this paper, we proposed a general TDS framework for domain adaptation via reinforcement learning, which matches the representations of the selected data from the source domain and the guidance set from the target domain and pass the similarity at different steps as rewards to guide a selection distribution generator. Through the generator, different instances from the source domain are selected to train a task-specific predictor. To this end, not only those data relevant to the target domain are se"
P19-1189,I13-1071,1,0.907179,"suffers from “Pyrrhic victory” where huge amounts of resource, e.g., computation, annotation, storage, etc., are consumed with many issues, one of which is that data quality considerably affects the performance of learned models. Especially in natural language ∗ This work was done during the internship of Miaofeng Liu and Hongbin Zou at Tencent AI Lab. † Corresponding authors. processing (NLP), such phenomenon is incredibly significant where noise and inaccurate annotations are demolishing models’ robustness when applying them across domains (Bollegala et al., 2011; Plank and Van Noord, 2011; Song and Xia, 2013; Ruder and Plank, 2018; Liu et al., 2018). Statistically, distribution mismatch is often observed between training and test data in such case. As a straightforward solution to reduce the impact of the mismatch, TDS is effective for learning across domains (Ruder and Plank, 2017) by preventing negative transfer from irrelevant samples and noisy labels (Rosenstein et al., 2005) while achieving equivalent performance with less computational efforts (Fan et al., 2017; Feng et al., 2018), especially when compared with learning-intensive domain adaptation methods such as sample reweighing (Borgward"
P19-1189,W10-2605,0,0.0346571,"Missing"
P19-1189,D17-1147,0,0.0416557,"Missing"
P19-1189,P16-1029,0,0.0468943,"Missing"
P19-1304,D18-1246,1,0.897995,"ic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to aggregate the incoming representations of v’s incoming neighbors k {hk−1 u` , ∀u ∈ N` (v)} into a single vector, hN` (v) , where k is the iteration index. This aggregator 1 Lebron James is transl"
P19-1304,D18-1032,0,0.0812748,"ultilingual knowledge graphs (KGs), such as DBpedia (Auer et al., 2007) and Yago (Suchanek et al., 2007), represent human knowledge in the structured format and have been successfully used in many natural language processing applications. These KGs encode rich monolingual knowledge but lack the cross-lingual links to bridge the language gap. Therefore, the cross-lingual KG alignment task, which automatically matches entities in a multilingual KG, is proposed to address this problem. Most recently, several entity matching based approaches (Hao et al., 2016; Chen et al., 2016; Sun et al., 2017; Wang et al., 2018) have been proposed for this task. Generally, these approaches first project entities of each KG into lowdimensional vector spaces by encoding monolingual KG facts, and then learn a similarity score function to match entities based on their vector representations. However, since some entities in different languages may have different KG To address these drawbacks, we propose a topic entity graph to represent the KG context information of an entity. Unlike previous methods that utilize entity embeddings to match entities, we formulate this task as a graph matching problem between the topic enti"
P19-1304,Q17-1010,0,0.0352098,"8 37.29 74.49 83.45 91.56 84.71 92.35 EN-FR @1 @10 14.61 37.25 21.26 50.60 32.97 65.91 36.77 73.06 81.03 90.79 84.15 91.76 66.91 67.93 67.92 64.01 65.28 65.21 72.63 73.97 73.52 69.76 71.29 70.18 87.62 89.38 88.96 87.65 88.18 88.01 77.52 78.48 78.36 78.12 79.64 79.48 85.09 87.15 86.87 83.48 84.63 84.29 94.19 95.24 94.28 93.66 94.75 94.37 Table 1: Evaluation results on the datasets. non-linearity function σ is ReLU (Glorot et al., 2011) and the parameters of aggregators are randomly initialized. Since KGs are represented in different languages, we first retrieve monolingual fastText embeddings (Bojanowski et al., 2017) for each language, and apply the method proposed in Conneau et al. (2017) to align these word embeddings into a same vector space, namely, crosslingual word embeddings. We use these embeddings to initialize word representations in the first layer of GCN1 . Results and Discussion. Following previous works, we used Hits@1 and Hits@10 to evaluate our model, where Hits@k measures the proportion of correctly aligned entities ranked in the top k. We implemented a baseline (referred as BASELINE in Table 1) that selects k closest G2 entities to a given G1 entity in the cross-lingual embedding space,"
P19-1304,D18-1223,1,0.789993,"Missing"
P19-1304,D18-1110,1,0.821566,"will discuss in §4. 3 Graph Matching Model Figure 2 gives an overview of our method for aligning Lebron James in the English and Chinese knowledge graph1 . Specifically, we fist retrieve topic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to ag"
P19-1304,D18-1112,1,0.597172,"will discuss in §4. 3 Graph Matching Model Figure 2 gives an overview of our method for aligning Lebron James in the English and Chinese knowledge graph1 . Specifically, we fist retrieve topic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to ag"
P19-1304,P18-1030,0,0.0133111,"raph1 . Specifically, we fist retrieve topic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to aggregate the incoming representations of v’s incoming neighbors k {hk−1 u` , ∀u ∈ N` (v)} into a single vector, hN` (v) , where k is the iteration index."
song-xia-2012-using,W99-0701,0,\N,Missing
song-xia-2012-using,D11-1033,0,\N,Missing
song-xia-2012-using,N03-1033,0,\N,Missing
song-xia-2012-using,C00-2117,0,\N,Missing
song-xia-2012-using,C02-1126,0,\N,Missing
song-xia-2012-using,P09-1058,0,\N,Missing
song-xia-2012-using,P10-2041,0,\N,Missing
song-xia-2012-using,I11-1035,0,\N,Missing
song-xia-2012-using,D11-1090,0,\N,Missing
song-xia-2012-using,D07-1036,0,\N,Missing
song-xia-2012-using,P06-1043,0,\N,Missing
song-xia-2012-using,P11-1157,0,\N,Missing
song-xia-2012-using,J04-1004,0,\N,Missing
song-xia-2012-using,P11-1139,0,\N,Missing
song-xia-2012-using,O03-4002,0,\N,Missing
song-xia-2012-using,W06-0115,0,\N,Missing
song-xia-2012-using,W06-0127,0,\N,Missing
song-xia-2012-using,I05-3025,0,\N,Missing
song-xia-2012-using,N09-1068,0,\N,Missing
song-xia-2012-using,P07-1033,0,\N,Missing
song-xia-2012-using,D10-1082,0,\N,Missing
song-xia-2014-modern,W99-0701,0,\N,Missing
song-xia-2014-modern,J93-2004,0,\N,Missing
song-xia-2014-modern,W03-1728,0,\N,Missing
song-xia-2014-modern,ogiso-etal-2012-unidic,0,\N,Missing
song-xia-2014-modern,W13-2302,0,\N,Missing
song-xia-2014-modern,rognvaldsson-etal-2012-icelandic,0,\N,Missing
song-xia-2014-modern,erjavec-2012-goo300k,0,\N,Missing
song-xia-2014-modern,song-xia-2012-using,1,\N,Missing
song-xia-2014-modern,I13-1071,1,\N,Missing
song-xia-2014-modern,scheible-etal-2012-gatetogermanc,0,\N,Missing
song-xia-2014-modern,P07-1033,0,\N,Missing
W06-0137,W03-1728,0,0.0801333,"Missing"
W06-0137,I05-3025,0,0.0327208,"Missing"
W09-3511,D08-1037,0,0.0396655,"set an intermediate layer to represent the source and target names by phonemes or phonetic tags (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Gao et al., 2004). Having been studied extensively though, the phonemes-based approaches cannot break its performance ceiling for two reasons (Li et al., 2004): (1) Languagedependent phoneme representation is not easy to obtain; (2) The phonemic representation to source and target names usually causes error spread. Several approaches have been proposed for direct use of parallel texts for performance enhancement (Li et al., 2004; Li et al., 2007; Goldwasser and Roth, 2008). There is no straightforward mean for grouping characters or letters in the source or target language into better transliteration units for a better correspondence. There is 57 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 57–60, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP 2.1 Training for the NEWS2009 task (Li et al., 2009) and present the experimented results. 2 For the purpose of modeling the training data, the characters from both the source and target name entities for training are split up for alignment, and then phrase extraction is conducted to fin"
W09-3511,P04-1021,0,0.498239,"Introduction To transliterate a foreign name into a target language, a direct instrument is to make use of existing rules for converting text to syllabus, or at least a phoneme base to support such transformation. Following this path, the well developed noisy channel model used for transliteration usually set an intermediate layer to represent the source and target names by phonemes or phonetic tags (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Gao et al., 2004). Having been studied extensively though, the phonemes-based approaches cannot break its performance ceiling for two reasons (Li et al., 2004): (1) Languagedependent phoneme representation is not easy to obtain; (2) The phonemic representation to source and target names usually causes error spread. Several approaches have been proposed for direct use of parallel texts for performance enhancement (Li et al., 2004; Li et al., 2007; Goldwasser and Roth, 2008). There is no straightforward mean for grouping characters or letters in the source or target language into better transliteration units for a better correspondence. There is 57 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 57–60, c Suntec, Singapore, 7 Au"
W09-3511,P07-1016,0,0.0181121,"teration usually set an intermediate layer to represent the source and target names by phonemes or phonetic tags (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Gao et al., 2004). Having been studied extensively though, the phonemes-based approaches cannot break its performance ceiling for two reasons (Li et al., 2004): (1) Languagedependent phoneme representation is not easy to obtain; (2) The phonemic representation to source and target names usually causes error spread. Several approaches have been proposed for direct use of parallel texts for performance enhancement (Li et al., 2004; Li et al., 2007; Goldwasser and Roth, 2008). There is no straightforward mean for grouping characters or letters in the source or target language into better transliteration units for a better correspondence. There is 57 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 57–60, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP 2.1 Training for the NEWS2009 task (Li et al., 2009) and present the experimented results. 2 For the purpose of modeling the training data, the characters from both the source and target name entities for training are split up for alignment, and then phrase ex"
W09-3511,W09-3501,0,0.0395223,"in; (2) The phonemic representation to source and target names usually causes error spread. Several approaches have been proposed for direct use of parallel texts for performance enhancement (Li et al., 2004; Li et al., 2007; Goldwasser and Roth, 2008). There is no straightforward mean for grouping characters or letters in the source or target language into better transliteration units for a better correspondence. There is 57 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 57–60, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP 2.1 Training for the NEWS2009 task (Li et al., 2009) and present the experimented results. 2 For the purpose of modeling the training data, the characters from both the source and target name entities for training are split up for alignment, and then phrase extraction is conducted to find the mapping pairs of character sequence. The alignment is performed by expectationmaximization (EM) iterations in the IBM model-4 SMT training using the GIZA++ toolkit1 . In some runs, however, e.g., English to Chinese and English to Korean transliteration, the character number of the source text is always more than that of the target text, the training conduc"
W09-3511,P02-1038,0,0.0501158,"ehn et al., 2003), with maximum length 10, which is tuned on development data, for both the sourceto-target and the target-to-source character alignment. Then two transliteration models, namely p(t|s) and p(s|t), are generated by such extraction for each transliteration run. Another component involved in the training is an n-gram language model. We set n = 3 and have it trained with the available data of the target language in question. Transliteration as SMT In order to transliterate effectively via a phrase based SMT process for our transliteration task, we opt for the log-linear framework (Och and Ney, 2002), a straight-forward architecture to have several feature models integrated together as P exp[ ni=1 λi hi (s, t)] Pn P (t|s) = P (1) t exp[ i=1 λi hi (s, t)] Then the transliteration task is to find the proper source and corresponding target chunks to maximize P (t|s) as t = argmax P (t|s) (2) t In (1), hi (s, t) is a feature model formulated as a probability functions on a pair of source and target texts in logarithmic form, and λi is a parameter to optimize its contribution. The two most important models in this framework are the translation model (i.e., the transliteration model in our case"
W09-3511,P03-1021,0,0.0113349,"ur task, we have tested these choices for p(s, t) on all our development data, arriving at a similar result. However, we opt to use both p(s|t) and p(t|s) if they give similar transliteration quality in some language pairs. Thus we take p(t|s) for our primary transliteration model for searching candidate corresponding character sequences, and p(s|t) as a supplement. In addition to the translation model feature, another feature for the language model can be described as hi (s, t) = log p(t) 2.2 Optimization Using the development sets for the NEWS2009 task, a minimum error rate training (MERT) (Och, 2003) is applied to tune the parameters for the corresponding feature models in (1). The training is performed with regard to the mean F-score, which is also called fuzziness in top-1, measuring on average how different the top transliteration candidate is from its closest reference. It is worth noting that a high mean F-score indicates a high accuracy of top candidates, thus a high mean reciprocal rank (MRR), which is used to quantify the overall performance of transliteration. (4) Usually the n-gram language model is used for its effectiveness and simplicity. 1 58 http://code.google.com/p/giza-pp"
W09-3511,W03-1508,0,0.106229,"rated in the decoding process. Our evaluated results indicate that this approach performs well in all standard runs in the NEWS2009 Machine Transliteration Shared Task. 1 Introduction To transliterate a foreign name into a target language, a direct instrument is to make use of existing rules for converting text to syllabus, or at least a phoneme base to support such transformation. Following this path, the well developed noisy channel model used for transliteration usually set an intermediate layer to represent the source and target names by phonemes or phonetic tags (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Gao et al., 2004). Having been studied extensively though, the phonemes-based approaches cannot break its performance ceiling for two reasons (Li et al., 2004): (1) Languagedependent phoneme representation is not easy to obtain; (2) The phonemic representation to source and target names usually causes error spread. Several approaches have been proposed for direct use of parallel texts for performance enhancement (Li et al., 2004; Li et al., 2007; Goldwasser and Roth, 2008). There is no straightforward mean for grouping characters or letters in the source or target language into better transl"
W09-3511,koen-2004-pharaoh,0,\N,Missing
W09-3511,J98-4003,0,\N,Missing
W10-1706,P02-1038,0,0.0295564,"achine Intelligence Lab of Shanghai Jiao Tong University (SJTU-BCMI Lab). The system is based on the state-of-the-art SMT toolkit MOSES (Koehn et al., 2007). We use it to translate German, French and Spanish into English. Though different development sets used for training parameter tuning will certainly lead to quite different performance, we empirically find that the more sets we combine together, the more stable the performance is, and a development set similar with test set will help the performance improvement. 2 System Description The basic model of the our system is a log-linear model (Och and Ney, 2002). For given source lan∗ This work was partially supported by the National Natural Science Foundation of China (Grant No. 60903119, Grant No. 60773090 and Grant No. 90820018), the National Basic Research Program of China (Grant No. 2009CB320901), and the National High-Tech Research Program of China (Grant No.2008AA02Z315). † corresponding author 67 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 67–71, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics de fr es small large small large small large sentences 1540549 1"
W10-1706,J03-1002,0,0.00462666,"English, two sets of bilingual corpora are provided by the shared task organizer. The first set is the new release (version 5) of Europarl corpus which is the smaller. The second is a combination of other available data sets which is the larger. In detail, two corpora, europarl-v5 and news-commentary10 are for German, europarl-v5 and news-commentary10 plus undoc for French and Spanish, respectively. Details of training data are in Table 1. Only sentences with length 1 to 40 are acceptable for our task. We used the larger set for our primary submission. We adopt word alignment toolkit GIZA++ (Och and Ney, 2003) to learn word-level alignment with its default setting and grow-diag-final-and parameters. Given a sentence pair and its corresponding word-level alignment, phrases will be extracted by using the approach in (Och and Ney, 2004). Phrase probability is estimated by its relative frequency in the training corpus. Lexical reordering is determined by using the default setting of MOSES with msd-bidirectional parameter. For training the only language model (English), the data sets are extracted from monolingual parts of both europarl-v5 and news-commentary10, Introduction We present a machine transla"
W10-1706,J04-4002,0,0.034962,"ich is the larger. In detail, two corpora, europarl-v5 and news-commentary10 are for German, europarl-v5 and news-commentary10 plus undoc for French and Spanish, respectively. Details of training data are in Table 1. Only sentences with length 1 to 40 are acceptable for our task. We used the larger set for our primary submission. We adopt word alignment toolkit GIZA++ (Och and Ney, 2003) to learn word-level alignment with its default setting and grow-diag-final-and parameters. Given a sentence pair and its corresponding word-level alignment, phrases will be extracted by using the approach in (Och and Ney, 2004). Phrase probability is estimated by its relative frequency in the training corpus. Lexical reordering is determined by using the default setting of MOSES with msd-bidirectional parameter. For training the only language model (English), the data sets are extracted from monolingual parts of both europarl-v5 and news-commentary10, Introduction We present a machine translation system that represents our participation for the WMT10 shared task from Brain-like Computing and Machine Intelligence Lab of Shanghai Jiao Tong University (SJTU-BCMI Lab). The system is based on the state-of-the-art SMT too"
W10-1706,P03-1021,0,0.0134358,"opment set is chosen. The solid lines represents the performances of 10 incremental batch sets on the two test sets, the batch processing still gives a poor performance at the beginning, but the results become better and more stable when the development sets are continuously enlarged. This sort of results suggest that a combined development set may produce reliable results in the worst case. Our primary submission used the combined development set and the results as Table 4. log pλM (ti |si )} 1 As usual, minimum error rate training (MERT) is adopted for log-linear model parameter estimation (Och, 2003). There are many improvements on MERT in existing work (Bertoldi et al., 2009; Foster and Kuhn, 2009), but there is no demonstration that the weights with better performance on the development set would lead to a better result on the unseen test set. In our experiments, we found that different development sets will cause significant BLEU score differences, even as high as one percent. Thus the remained problem will be how to effectively choose the development set to obtain a better and more stable performance. 68 09-dev 16.46 16.67 16.74 16.15 16.44 16.50 17.15 16.51 17.03 16.25 09-batch 16.46"
W10-1706,W09-0418,0,0.0270243,"dotted lines have the inverse trend with the dotted in Figure 1(because the addition of these two values is constant), and the solid lines have the same trend with the dotted, which means that the good performance is mutual between test set and development sets: if tuning using A set could make a good result over B set, then vice versa. 3.6 Related Work The special challenge of the WMT shared task is domain adaptation, which is a hot topic in recent years and more relative to our experiments. Many existing works are about this topic (Koehn and Schroeder, 2007; Nakov, 2008; Nakov and Ng, 2009; Paul et al., 2009; Haque et al., 2009). However, most of previous works focus on language 3.5 On the Similarity between Development Set and Test Set This experiment is motivated by (Utiyama et al., 2009), where they used BLEU score to measure the similarity of a sentences pair and then extracted sentences similar with those in test set to 69 model, translation phrase table, lexicons model and factored translation model, few of them pay attention to the domain adaptation on the development set. For future work we consider to use some machine learning approaches to select sentences in development sets more relev"
W10-1706,W09-0439,0,0.0184323,"sets on the two test sets, the batch processing still gives a poor performance at the beginning, but the results become better and more stable when the development sets are continuously enlarged. This sort of results suggest that a combined development set may produce reliable results in the worst case. Our primary submission used the combined development set and the results as Table 4. log pλM (ti |si )} 1 As usual, minimum error rate training (MERT) is adopted for log-linear model parameter estimation (Och, 2003). There are many improvements on MERT in existing work (Bertoldi et al., 2009; Foster and Kuhn, 2009), but there is no demonstration that the weights with better performance on the development set would lead to a better result on the unseen test set. In our experiments, we found that different development sets will cause significant BLEU score differences, even as high as one percent. Thus the remained problem will be how to effectively choose the development set to obtain a better and more stable performance. 68 09-dev 16.46 16.67 16.74 16.15 16.44 16.50 17.15 16.51 17.03 16.25 09-batch 16.46 16.25 16.20 16.83 16.73 16.97 17.03 17.00 16.97 16.99 08-dev 16.38 16.66 16.94 16.18 16.64 16.75 17."
W10-1706,2009.iwslt-evaluation.12,0,0.284928,"Missing"
W10-1706,W07-0733,0,0.0385952,"the test set under self tuning, newstest2009 is 17.91). The dotted lines have the inverse trend with the dotted in Figure 1(because the addition of these two values is constant), and the solid lines have the same trend with the dotted, which means that the good performance is mutual between test set and development sets: if tuning using A set could make a good result over B set, then vice versa. 3.6 Related Work The special challenge of the WMT shared task is domain adaptation, which is a hot topic in recent years and more relative to our experiments. Many existing works are about this topic (Koehn and Schroeder, 2007; Nakov, 2008; Nakov and Ng, 2009; Paul et al., 2009; Haque et al., 2009). However, most of previous works focus on language 3.5 On the Similarity between Development Set and Test Set This experiment is motivated by (Utiyama et al., 2009), where they used BLEU score to measure the similarity of a sentences pair and then extracted sentences similar with those in test set to 69 model, translation phrase table, lexicons model and factored translation model, few of them pay attention to the domain adaptation on the development set. For future work we consider to use some machine learning approache"
W10-1706,W09-0412,0,0.0112677,"2009 is 17.91). The dotted lines have the inverse trend with the dotted in Figure 1(because the addition of these two values is constant), and the solid lines have the same trend with the dotted, which means that the good performance is mutual between test set and development sets: if tuning using A set could make a good result over B set, then vice versa. 3.6 Related Work The special challenge of the WMT shared task is domain adaptation, which is a hot topic in recent years and more relative to our experiments. Many existing works are about this topic (Koehn and Schroeder, 2007; Nakov, 2008; Nakov and Ng, 2009; Paul et al., 2009; Haque et al., 2009). However, most of previous works focus on language 3.5 On the Similarity between Development Set and Test Set This experiment is motivated by (Utiyama et al., 2009), where they used BLEU score to measure the similarity of a sentences pair and then extracted sentences similar with those in test set to 69 model, translation phrase table, lexicons model and factored translation model, few of them pay attention to the domain adaptation on the development set. For future work we consider to use some machine learning approaches to select sentences in developm"
W10-1706,Y09-2027,0,\N,Missing
W10-1706,W08-0320,0,\N,Missing
W10-1706,P07-2045,0,\N,Missing
W10-2409,W02-1001,0,0.00994382,"ates are generated for each source name. 3 Algorithm 1 Averaged perceptron training Input: Candidate list with reference {LIST (xj , yj )nj=1 , yi∗ }N i=1 Output: Averaged parameters 1: ω ~ ← 0, ω ~ a ← 0, c ← 1 2: for t = 1 to T do 3: for i = 1 to N do 4: yˆi ← argmaxy∈LIST (xj ,yj ) ω ~ · Φ(xi , yi ) 5: if yˆi 6= yi∗ then 6: ω ~ ←ω ~ + Φ(x∗i , yi∗ ) − Φ(ˆ xi , yˆi ) 7: ω ~a ← ω ~ a + c · {Φ(x∗i , yi∗ ) − Φ(ˆ xi , yˆi )} 8: end if 9: c←c+1 10: end for 11: end for 12: return ω ~ −ω ~ a /c Reranking 3.1 Learning Framework For reranking training and prediction, we adopt the averaged perceptron (Collins, 2002) as our learning framework, which has a more stable performance than the non-averaged version. It is presented in Algorithm 1. Where ω ~ is the vector of parameters we want to optimize, x, y are the corresponding source (with different syllabification) and target graphemes in the candidate list, and Φ represents the feature vector in the pair of x and y. In this algorithm, reference yi∗ is the most appropriate output in the candidate list according to the true target named entity in the training data. We use the Mean-F score to identify which candidate can be the reference, by locating the one"
W10-2409,P04-1021,0,0.394968,"sult from the candidates. We only consider bi-grams when using this feature. Target grapheme chain feature, f (tii−2 ); This feature measures the appropriateness of the generated target graphemes on both character and syllables level. It performs in a similar way as the language model for SMT decoding. We use tri-gram syllables in this learning framework. 3.2 Multiple Features The following features are used in our reranking process: Paired source-to-target transition feature, f (< s, t >ii−1 ); Transliteration correspondence feature, f (si , ti ); This type of feature is firstly proposed in (Li et al., 2004), aiming at generating source and target graphemes simultaneously under a suitable constraint. We use this feature to restrict the synchronous transition of both source and target graphemes, measuring how well are those transitions, such as for “st”, This feature describes the mapping between source and target graphemes, similar to the transliteration options in the phrase table in our previous generation process, where s and 2 In this work, we use Pinyin as the phonetic representation for Chinese. 63 whether “s” transliterated by “斯” is followed by “t” transliterated by “特”. In order to deal"
W10-2409,P03-1021,0,0.012959,"62–65, c Uppsala, Sweden, 16 July 2010. 2010 Association for Computational Linguistics try in it. After that, we use a phoneme resource2 to refine the phrase table by filtering out the wrongly extracted phrases and cleaning up the noise in it. In the decoding process, a dynamic pruning is performed when generating the hypothesis in each step, in which the threshold is variable according to the current searching space, for we need to obtain a good candidate list as precise as possible for the next stage. The parameter for each feature function in log-linear model is optimized by MERT training (Och, 2003). Finally, a maximum number of 50 candidates are generated for each source name. 3 Algorithm 1 Averaged perceptron training Input: Candidate list with reference {LIST (xj , yj )nj=1 , yi∗ }N i=1 Output: Averaged parameters 1: ω ~ ← 0, ω ~ a ← 0, c ← 1 2: for t = 1 to T do 3: for i = 1 to N do 4: yˆi ← argmaxy∈LIST (xj ,yj ) ω ~ · Φ(xi , yi ) 5: if yˆi 6= yi∗ then 6: ω ~ ←ω ~ + Φ(x∗i , yi∗ ) − Φ(ˆ xi , yˆi ) 7: ω ~a ← ω ~ a + c · {Φ(x∗i , yi∗ ) − Φ(ˆ xi , yˆi )} 8: end if 9: c←c+1 10: end for 11: end for 12: return ω ~ −ω ~ a /c Reranking 3.1 Learning Framework For reranking training and predic"
W10-2409,W09-3506,0,0.0503016,"sult, it is found in (Song et al., 2009) that, in contrast to the ordinary top-1 accuracy (ACC) score, its recall rate, which is defined in terms of whether the correct answer is generated in the n-best output list, is rather high. This observation suggests that if we could rearrange those outputs into a better order, especially, push the correct one to the top, the overall performance could be enhanced significantly, without any further refinement of the original generation process. This reranking strategy is proved to be efficient in transliteration generation with a multi-engine approach (Oh et al., 2009). In this paper, we present our recent work on reranking the transliteration candidates via an online discriminative learning framework, namely, the averaged perceptron. Multiple features are incorporated into it for performance enhancement. The following sections will give the technical details of our method and present its results for NEWS2010 shared task for named entity transliteration. Effective transliteration of proper names via grapheme conversion needs to find transliteration patterns in training data, and then generate optimized candidates for testing samples accordingly. However, th"
W10-2409,W09-3511,1,0.92429,"it is still unreliable to rank the candidates simply by their statistical translation scores for the purpose of selecting the best one. In order to make a proper choice, the direct orthographic mapping requires a precise alignment and a better transliteration option selection. Thus, powerful algorithms for effective use of the parallel data is indispensable, especially when the available data is limited in volume. Interestingly, although an SMT based approach could not achieve a precise top-1 transliteration re2 Generation For the generation of transliteration candidates, we follow the work (Song et al., 2009), using a phrase-based SMT procedure with the log-linear model P exp[ ni=1 λi hi (s, t)] Pn P (t|s) = P t exp[ i=1 λi hi (s, t)] (1) for decoding. Originally we use two directional phrase1 tables, which are learned for both directions of source-to-target and target-to-source, containing different entries of transliteration options. In order to facilitate the decoding by exploiting all possible choices in a better way, we combine the forward and backward directed phrase tables together, and recalculate the probability for each en1 It herein refers to a character sequence as described in (Song e"
W18-2309,J97-1002,0,0.472435,"Missing"
W18-2309,J86-3001,0,0.728681,"ures and Actions with the COSTA Scheme in Medical Conversations † Nan Wang†‡] , Yan Song♠ , Fei Xia‡ University of California-Los Angeles, CA, USA ‡ University of Washington, WA, USA ] Hunan University, Hunan, China ♠ Tencent AI Lab nwang3@ucla.edu Abstract made possible?’(Heritage, 1984; Schegloff, 2007; Sacks et al., 1974). In artificial intelligence, researchers also explored various theories and practices in analyzing conversation structures, based on which intelligent dialog systems can be developed to assist human with various types of tasks (Core and Allen, 1997; Carletta et al., 1997; Grosz and Sidner, 1986; Jurafsky et al., 1997; Stolcke et al., 2000; Mayfield et al., 2014). In medicine, research shows that a thorough understanding of physician-patient communication structure is important for delivering quality health care and achieving optimal health outcomes (Heritage and Maynard, 2006; Zolnierek and Dimatteo, 2009; Stivers, 2007). This paper describes the COSTA scheme for coding structures and actions in conversation. Informed by Conversation Analysis, the scheme introduces an innovative method for marking multi-layer structural organization of conversation and a structure-informed taxonomy"
W18-2309,J00-3003,0,0.769566,"Missing"
W18-2315,D14-1162,0,0.0810462,"iction layer φy , for ground truth y (k) of the k-th phrase pair, the instance-level matching loss is Preliminaries Before going into details of DA-BiMPM, we start with introducing the BiMPM model (Wang et al., 2017), which is illustrated by components outside the dotted box in Figure 1. Its encoding, matching, and aggregation layers are described as follows. Phrase Encoder. Given a disease phrase P = (p1 , . . . , pn ) with n words, BiMPM encode it as follows. First, it transforms P in to a vector sequence P = (p1 , . . . , pn ). Each word is represented by concatenating a pre-trained GloVe (Pennington et al., 2014) vector and a characterBiLSTM-encoded vector. A BiLSTM is then applied on P to represent context in both directions: ← − ← − ← − ← − ←−−− H P = ( h P1 , h P2 , . . . , h Pn ) = LSTM(P) − → − → − → − →P −−−→ H = ( h P1 , h P2 , . . . , h Pn ) = LSTM(P) (3) Here ⊗ denotes the multi-perspective matching operation deﬁned in (Wang et al., 2017). We refer readers to this paper for details. Aggregation Layer. Given all matching vectors MP = (mP1 , . . . , mPn ) by comparing P to Q, and MQ vice versa, we apply another BiLSTM layer to aggregate both of them, respectively. Formally, l(k) (φf , φy ) = l("
W18-2315,I17-2072,0,0.0246969,"data. But since all above models assume in-domain annotations, the effect of source-domain annotations remains uncertain on the trained models. This paper takes a perspective that is orthogonal to works on designing sophisticated matching networks. We employ domain adaptation in disease phrase matching to effectively exploit source annotations. Based on Bilateral Multi-Perspective Matching (BiMPM) (Wang et al., 2017), we propose a Domain-Adaptive BiMPM (DA-BiMPM) model. Inspired by domain-adversarial training (Ganin et al., 2016) on text classiﬁcation (Liu et al., 2017), relation extraction (Fu et al., 2017), and paraphrase identiﬁcation (Yu et al., 2018), we introduce a domain discriminator in addition to the matching predictor in BiMPM. With such a discriminator, DA-BiMPM is encouraged to learn features predictive of the matching labels, while being least discriminative of which domain the data comes from. In doing so, it is expected that the learned models distill domain-insensitive knowledge from source annotations. On two medical datasets from different subﬁelds, we set up non-adaptive baselines fed with or without sourcedomain annotations, as well as an adaptive one. Experimental results sh"
W18-2315,N16-1170,0,0.0126328,"king (Traylor et al., 2017), and disease inference (Nie et al., 2015). As deep learning drew attentions on various tasks (Lecun et al., 2015), dedicated neural matching models are also designed in two types of structures. 1) Siamese-based networks (Neculoiu et al., 2016; Mueller and Thyagarajan, 2016): the input phrases are ﬁrst encoded by the same network; the encoded vectors are then used to compute similarities by metrics like Cosine. 2) Matching-aggregating networks: ﬁnegrained units of the two phrases are represented and matched in word-by-word (Rockt¨aschel et al., 2015), one-direction (Wang and Jiang, 2016), or bilateral-multi-perspective (Wang et al., 2017) manners to produce matching features; the features are aggregated into a vector, based on which the matching label is predicted. Despite encouraging results in other areas, neural matching models still face speciﬁc challenges on medical data. Different medical subﬁelds like physiology and urology may adopt diverse terminologies. Due to their professional nature, it is hard to obtain human annotations at scale for a single subﬁeld. This causes systems on a particIntroduction In recent years, hospitals depend more on information systems to sto"
W18-2315,Q16-1019,0,0.0333504,"Missing"
W18-2315,P17-1001,0,0.0161651,"or more source domains for more training data. But since all above models assume in-domain annotations, the effect of source-domain annotations remains uncertain on the trained models. This paper takes a perspective that is orthogonal to works on designing sophisticated matching networks. We employ domain adaptation in disease phrase matching to effectively exploit source annotations. Based on Bilateral Multi-Perspective Matching (BiMPM) (Wang et al., 2017), we propose a Domain-Adaptive BiMPM (DA-BiMPM) model. Inspired by domain-adversarial training (Ganin et al., 2016) on text classiﬁcation (Liu et al., 2017), relation extraction (Fu et al., 2017), and paraphrase identiﬁcation (Yu et al., 2018), we introduce a domain discriminator in addition to the matching predictor in BiMPM. With such a discriminator, DA-BiMPM is encouraged to learn features predictive of the matching labels, while being least discriminative of which domain the data comes from. In doing so, it is expected that the learned models distill domain-insensitive knowledge from source annotations. On two medical datasets from different subﬁelds, we set up non-adaptive baselines fed with or without sourcedomain annotations, as well as a"
W18-2315,N12-1019,0,0.0235736,"domain annotations. 1 Phrase 1 Phrase 2 Label Latent syphilis, speciﬁed as early or late Latent syphilis, speciﬁed as early or late Syphilis latent Yes Late syphilis, speciﬁed No Table 1: Examples of disease phrase matching. 1). In the ﬁrst one, the absent participial modiﬁer and the different word order do not prevent the two phrases from matching. The second one is, however, a false match, though it shares more words and similar syntactic structures with Phrase 1. Given the variability of human languages, supervised phrase or sentence matching is widely applied in information identiﬁcation (Madnani et al., 2012; Yin et al., 2016), textual entailment (Marelli et al., 2014), web search (Li et al., 2014), entity linking (Traylor et al., 2017), and disease inference (Nie et al., 2015). As deep learning drew attentions on various tasks (Lecun et al., 2015), dedicated neural matching models are also designed in two types of structures. 1) Siamese-based networks (Neculoiu et al., 2016; Mueller and Thyagarajan, 2016): the input phrases are ﬁrst encoded by the same network; the encoded vectors are then used to compute similarities by metrics like Cosine. 2) Matching-aggregating networks: ﬁnegrained units of"
W18-2315,S14-2001,0,0.013374,", speciﬁed as early or late Latent syphilis, speciﬁed as early or late Syphilis latent Yes Late syphilis, speciﬁed No Table 1: Examples of disease phrase matching. 1). In the ﬁrst one, the absent participial modiﬁer and the different word order do not prevent the two phrases from matching. The second one is, however, a false match, though it shares more words and similar syntactic structures with Phrase 1. Given the variability of human languages, supervised phrase or sentence matching is widely applied in information identiﬁcation (Madnani et al., 2012; Yin et al., 2016), textual entailment (Marelli et al., 2014), web search (Li et al., 2014), entity linking (Traylor et al., 2017), and disease inference (Nie et al., 2015). As deep learning drew attentions on various tasks (Lecun et al., 2015), dedicated neural matching models are also designed in two types of structures. 1) Siamese-based networks (Neculoiu et al., 2016; Mueller and Thyagarajan, 2016): the input phrases are ﬁrst encoded by the same network; the encoded vectors are then used to compute similarities by metrics like Cosine. 2) Matching-aggregating networks: ﬁnegrained units of the two phrases are represented and matched in word-by-word (R"
W18-2315,W16-1617,0,0.0127729,"wever, a false match, though it shares more words and similar syntactic structures with Phrase 1. Given the variability of human languages, supervised phrase or sentence matching is widely applied in information identiﬁcation (Madnani et al., 2012; Yin et al., 2016), textual entailment (Marelli et al., 2014), web search (Li et al., 2014), entity linking (Traylor et al., 2017), and disease inference (Nie et al., 2015). As deep learning drew attentions on various tasks (Lecun et al., 2015), dedicated neural matching models are also designed in two types of structures. 1) Siamese-based networks (Neculoiu et al., 2016; Mueller and Thyagarajan, 2016): the input phrases are ﬁrst encoded by the same network; the encoded vectors are then used to compute similarities by metrics like Cosine. 2) Matching-aggregating networks: ﬁnegrained units of the two phrases are represented and matched in word-by-word (Rockt¨aschel et al., 2015), one-direction (Wang and Jiang, 2016), or bilateral-multi-perspective (Wang et al., 2017) manners to produce matching features; the features are aggregated into a vector, based on which the matching label is predicted. Despite encouraging results in other areas, neural matching models"
W19-5027,D18-1258,0,0.128355,"Missing"
W19-5027,D16-1264,0,0.0597248,"Missing"
W19-5027,C12-2116,1,0.899036,"Q-As, because the systems require a question to be one of the input. The reason we apply the A-Only and A-A settings to the adoption prediction task is that it helps identify whether features from an answer itself will contribute to its adopted flag assignment without knowing its question. To compare the relevancy task and the adoption prediction task, we also apply these two settings to the former task although they are not common settings in previous studies (Lai et al., 2018). Word segmentation has always been a challenge in Chinese NLP especially when it is applied to a particular domain (Song et al., 2012; Song and Xia, 2012, 2013). Therefore, instead of word embeddings (Song et al., 2018), we use Chinesecharacter-based embeddings to avoid word segmentation errors. We set the embedding size to 150. We use 155 and 245 as the lengths of questions and answers respectively. Short texts are padded with blank characters. We use 32 filters 4 Experiments on Two Prediction Tasks In this section, we use ChiMed-QA1 and ChiMedQA2 (See Table 12) to build NLP systems for the adoption prediction task and the relevancy prediction task, respectively. Both tasks are binary classification tasks with the same typ"
W19-5027,K17-1016,1,0.708581,"he adoption prediction task. Several benchmark models are applied to the datasets, producing good results for both tasks. 1 Introduction In the big data era, it is often challenging to locate the most helpful information in many realworld applications, such as search engine, customer service, personal assistant, etc. A series of NLP tasks, such as text representation, text classification, summarization, keyphrase extraction, and answer ranking, are able to help QA systems in finding relevant information (Siddiqi and Sharan, 2015; Allahyari et al., 2017; Yang et al., 2016; Joulin et al., 2016; Song et al., 2017, 2018). Currently, most QA corpora are built for the general domain focusing on extracting/generating answers from articles, such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), Dureader (He et al., 2017), SearchQA (Dunn et al., 2017), CoQA (Reddy et al., 2018), etc., with few others from community QA forums, 1 https://www.medhelp.org The code for constructing the corpus and the datasets used in this study are available at https://github. com/yuanheTian/ChiMed. 2 250 Proceedings of the BioNLP 2019 workshop, pages 250–260 c Florence, Italy, August 1, 2019. 2019 Associ"
W19-5027,N18-2028,1,0.762664,"ply the A-Only and A-A settings to the adoption prediction task is that it helps identify whether features from an answer itself will contribute to its adopted flag assignment without knowing its question. To compare the relevancy task and the adoption prediction task, we also apply these two settings to the former task although they are not common settings in previous studies (Lai et al., 2018). Word segmentation has always been a challenge in Chinese NLP especially when it is applied to a particular domain (Song et al., 2012; Song and Xia, 2012, 2013). Therefore, instead of word embeddings (Song et al., 2018), we use Chinesecharacter-based embeddings to avoid word segmentation errors. We set the embedding size to 150. We use 155 and 245 as the lengths of questions and answers respectively. Short texts are padded with blank characters. We use 32 filters 4 Experiments on Two Prediction Tasks In this section, we use ChiMed-QA1 and ChiMedQA2 (See Table 12) to build NLP systems for the adoption prediction task and the relevancy prediction task, respectively. Both tasks are binary classification tasks with the same type of input; the only difference is the meaning of class labels (relevancy vs. adopted"
W19-5027,song-xia-2012-using,1,0.876273,"ystems require a question to be one of the input. The reason we apply the A-Only and A-A settings to the adoption prediction task is that it helps identify whether features from an answer itself will contribute to its adopted flag assignment without knowing its question. To compare the relevancy task and the adoption prediction task, we also apply these two settings to the former task although they are not common settings in previous studies (Lai et al., 2018). Word segmentation has always been a challenge in Chinese NLP especially when it is applied to a particular domain (Song et al., 2012; Song and Xia, 2012, 2013). Therefore, instead of word embeddings (Song et al., 2018), we use Chinesecharacter-based embeddings to avoid word segmentation errors. We set the embedding size to 150. We use 155 and 245 as the lengths of questions and answers respectively. Short texts are padded with blank characters. We use 32 filters 4 Experiments on Two Prediction Tasks In this section, we use ChiMed-QA1 and ChiMedQA2 (See Table 12) to build NLP systems for the adoption prediction task and the relevancy prediction task, respectively. Both tasks are binary classification tasks with the same type of input; the only"
W19-5027,C18-1181,0,0.021353,") Q-As where a question and both of its answers are input (See Figure 4). ARC-I, DUET, and DRMM are run under the settings of Q-A and Q-As, because the systems require a question to be one of the input. The reason we apply the A-Only and A-A settings to the adoption prediction task is that it helps identify whether features from an answer itself will contribute to its adopted flag assignment without knowing its question. To compare the relevancy task and the adoption prediction task, we also apply these two settings to the former task although they are not common settings in previous studies (Lai et al., 2018). Word segmentation has always been a challenge in Chinese NLP especially when it is applied to a particular domain (Song et al., 2012; Song and Xia, 2012, 2013). Therefore, instead of word embeddings (Song et al., 2018), we use Chinesecharacter-based embeddings to avoid word segmentation errors. We set the embedding size to 150. We use 155 and 245 as the lengths of questions and answers respectively. Short texts are padded with blank characters. We use 32 filters 4 Experiments on Two Prediction Tasks In this section, we use ChiMed-QA1 and ChiMedQA2 (See Table 12) to build NLP systems for the"
W19-5027,I13-1071,1,0.935533,"Missing"
W19-5027,N18-1140,0,0.0577105,"Missing"
W19-5027,S15-2047,0,0.0492481,"Missing"
W19-5027,D07-1003,0,0.017873,"Missing"
W19-5027,W18-2309,1,0.508459,"uistics University of Washington yhtian@uw.edu Weicheng Ma Computer Science Department New York University wm724@nyu.edu Fei Xia Department of Linguistics University of Washington fxia@uw.edu Yan Song Tencent AI Lab clksong@gmail.com Abstract such as TrecQA (Wang et al., 2007), WikiQA (Yang et al., 2015), and SemEval-2015 (Nakov et al., 2015). In the medical domain, most medial QA corpora consist of scientific articles, such as BioASQ (Tsatsaronis et al., 2012), emrQA (Pampari et al., ˇ 2018), and CliCR (Suster and Daelemans, 2018). Although some studies were done for conversational datasets (Wang et al., 2018a,b), corpora designed for community QA are extremely rare. Meanwhile, given that many online medical service forums have emerged (e.g. MedHelp1 ), there are increasing demands from users to search for answers for their medical concerns. One might be tempted to build QA corpora from such forums. However, in doing so, one must address a series of challenges such as how to ensure the quality of the derived corpus despite the noise in the original forum data. In this paper, we introduce our work on building a Chinese medical QA corpus named ChiMed by crawling data from a big Chinese medical forum"
W19-5027,L18-1464,1,0.682527,"uistics University of Washington yhtian@uw.edu Weicheng Ma Computer Science Department New York University wm724@nyu.edu Fei Xia Department of Linguistics University of Washington fxia@uw.edu Yan Song Tencent AI Lab clksong@gmail.com Abstract such as TrecQA (Wang et al., 2007), WikiQA (Yang et al., 2015), and SemEval-2015 (Nakov et al., 2015). In the medical domain, most medial QA corpora consist of scientific articles, such as BioASQ (Tsatsaronis et al., 2012), emrQA (Pampari et al., ˇ 2018), and CliCR (Suster and Daelemans, 2018). Although some studies were done for conversational datasets (Wang et al., 2018a,b), corpora designed for community QA are extremely rare. Meanwhile, given that many online medical service forums have emerged (e.g. MedHelp1 ), there are increasing demands from users to search for answers for their medical concerns. One might be tempted to build QA corpora from such forums. However, in doing so, one must address a series of challenges such as how to ensure the quality of the derived corpus despite the noise in the original forum data. In this paper, we introduce our work on building a Chinese medical QA corpus named ChiMed by crawling data from a big Chinese medical forum"
W19-5027,D15-1237,0,0.0571836,"Missing"
W19-5044,N18-1132,0,0.362285,"used to classify the final representation. 2018). BERT (Devlin et al., 2018) pre-trains the model with large unlabeled corpora which allows better text representations. MT-DNN (Liu et al., 2019c) leverages multi-task learning (Liu et al., 2015) to fine-tune the BERT weights using the GLUE datasets (Wang et al., 2018). The authors showed that resulting representations outperform BERT on many NLU tasks. On top of this sentence pair modeling scheme, previous studies have independently leveraged syntax (Chen et al., 2016), external knowledge (Chen et al., 2018; Lu et al., 2019), ensemble methods (Ghaeini et al., 2018b), and language model fine-tuning (Alsentzer et al., 2019) to improve the performance of NLI systems. Nonetheless, to our knowledge, there have been no empirical results on the effect of combining these additions simultaneously. Additionally, as recent studies have pointed out that pre-trained contextualThe recent Transformer-based models have been demonstrated to be a better encoder at NLI than CNN and LSTM by fully attending over the two sentences (Radford, 2018; Devlin et al., 416 with VT E (p, h) referring to the output of the text encoder, a vector representing p and h. Pre-training on l"
W19-5044,S17-2057,0,0.0664922,"Missing"
W19-5044,P03-1003,0,0.643026,"Missing"
W19-5044,W19-1909,0,0.0576562,"Missing"
W19-5044,W19-5039,0,0.0531377,"formation, and a feature encoder which injects some degree of domain knowledge into the model (see §3). We conduct unsupervised pre-training for the text encoder on biomedical corpora to compensate for the lack of domain-specific supervision (Lee et al., 2019). To enhance our model, we also use model ensemble and conflict resolution strategies, corresponding to the two top dashed boxes in Figure 1 and are explained in §4. The datasets and implementation detail are described in §5. The experimental results on the MedNLI dataset (Romanov and Shivade, 2018) and the MEDIQA 2019 shared task 1 (Ben Abacha et al., 2019) are reported in §6.1 Natural language inference (NLI) is challenging, especially when it is applied to technical domains such as biomedical settings. In this paper, we propose a hybrid approach to biomedical NLI where different types of information are exploited for this task. Our base model includes a pre-trained text encoder as the core component, and a syntax encoder and a feature encoder to capture syntactic and domain-specific information. Then we combine the output of different base models to form more powerful ensemble models. Finally, we design two conflict resolution strategies when"
W19-5044,N19-1419,0,0.0239519,"over the two sentences (Radford, 2018; Devlin et al., 416 with VT E (p, h) referring to the output of the text encoder, a vector representing p and h. Pre-training on large unlabeled corpora with a language modeling objective has facilitated many recent state-of-the-art advancements (Peters et al., 2018; Radford, 2018; Devlin et al., 2018; Lee et al., 2019; Radford et al., 2019). Inspired by these results, we enhance the MT-DNN representation by further fine-tuning on unlabeled biomedical data to mitigate the lack of in-domain supervision. ized representations contain rich linguistic signals (Hewitt and Manning, 2019; Liu et al., 2019b), it is reasonable to ask whether explicitly integrating knowledge will continue to augment such representations. Our work can be seen as an empirical study to examine the efficacy of applying multiple additions on top of Transformer-based models. 3 Base Model NLI is generally treated as a three-way classification task that models whether a given premise p entails, contradicts, or is neutral to a hypothesis h. A classifier f is learned taking p and h as input to predict the class probabilities >  (1) f (p, h) = Pe Pc Pn 3.2 Linguistic understandings, for example coreferen"
W19-5044,Q17-1010,0,0.0383203,"M for Table 2 and 3, and we call it Embedding I. Romanov and Shivade (2018) used embeddings trained on biomedical corpora and observed non-trivial accuracy gain over generaldomain embeddings. Thus, we also experimented with two domain-specific word embeddings that they used and released to initialize the TreeLSTM, and we will call them Embedding II and III. Here is a quick summary of the embeddings: I. GloVe embedding trained on Wikipedia 2014 + Gigaword 5; II. Embedding initialized with common crawl7 GloVe and fine-tuned on BioASQ and then MIMIC-III; III. Embedding initialized with fastText (Bojanowski et al., 2017) trained on Wikipedia and fine-tuned on MIMIC-III. 6.2 Model Enhancement Results We want a diverse set of member models to achieve better ensemble performance. We present ones that lead to better ensemble performance in Table 6. We also report the ensemble models and conflict resolution results in Table 6. Table 4 shows the effect of these embeddings. The first row is the best result from Table 3, which uses Embedding I, and the next two rows are the results when the embedding is changed. The table shows that using specific in-domain embeddings (the second and the third rows in Table 4) does n"
W19-5044,D15-1075,0,0.322514,"he output of different base models to form more powerful ensemble models. Finally, we design two conflict resolution strategies when the test data contain multiple (premise, hypothesis) pairs with the same premise. We train our models on the MedNLI dataset, yielding the best performance on the test set of the MEDIQA 2019 Task 1. 1 Sicong Huang Department of ECE University of Washington huangs33@uw.edu Introduction Natural language inference (NLI) (MacCartney and Manning, 2009), also known as textual entailment, is an important natural language processing (NLP) task that has long been studied (Bowman et al., 2015; Parikh et al., 2016; Chen et al., 2016; Conneau et al., 2017; Tay et al., 2018). It aims to capture the relationship between two sentences, identifying whether a given premise entails, contradicts, or is neutral to a given hypothesis. Success in NLI is crucial for achieving semantic comprehension of human language, which in turn is a prerequisite to accomplish natural language understanding (NLU). In general, accurate NLI systems facilitate many downstream tasks, such as commonsense reasoning (Zellers et al., 2018) and question answering (Abacha and Demner-Fushman, 2016, 2017). Most of exist"
W19-5044,P18-1224,0,0.0225028,"ate the local attended information. A softmax layer is used to classify the final representation. 2018). BERT (Devlin et al., 2018) pre-trains the model with large unlabeled corpora which allows better text representations. MT-DNN (Liu et al., 2019c) leverages multi-task learning (Liu et al., 2015) to fine-tune the BERT weights using the GLUE datasets (Wang et al., 2018). The authors showed that resulting representations outperform BERT on many NLU tasks. On top of this sentence pair modeling scheme, previous studies have independently leveraged syntax (Chen et al., 2016), external knowledge (Chen et al., 2018; Lu et al., 2019), ensemble methods (Ghaeini et al., 2018b), and language model fine-tuning (Alsentzer et al., 2019) to improve the performance of NLI systems. Nonetheless, to our knowledge, there have been no empirical results on the effect of combining these additions simultaneously. Additionally, as recent studies have pointed out that pre-trained contextualThe recent Transformer-based models have been demonstrated to be a better encoder at NLI than CNN and LSTM by fully attending over the two sentences (Radford, 2018; Devlin et al., 416 with VT E (p, h) referring to the output of the text"
W19-5044,D17-1070,0,0.211583,"mble models. Finally, we design two conflict resolution strategies when the test data contain multiple (premise, hypothesis) pairs with the same premise. We train our models on the MedNLI dataset, yielding the best performance on the test set of the MEDIQA 2019 Task 1. 1 Sicong Huang Department of ECE University of Washington huangs33@uw.edu Introduction Natural language inference (NLI) (MacCartney and Manning, 2009), also known as textual entailment, is an important natural language processing (NLP) task that has long been studied (Bowman et al., 2015; Parikh et al., 2016; Chen et al., 2016; Conneau et al., 2017; Tay et al., 2018). It aims to capture the relationship between two sentences, identifying whether a given premise entails, contradicts, or is neutral to a given hypothesis. Success in NLI is crucial for achieving semantic comprehension of human language, which in turn is a prerequisite to accomplish natural language understanding (NLU). In general, accurate NLI systems facilitate many downstream tasks, such as commonsense reasoning (Zellers et al., 2018) and question answering (Abacha and Demner-Fushman, 2016, 2017). Most of existing NLI studies are conducted in the general domain (Marelli e"
W19-5044,C18-1328,0,0.0355147,"Missing"
W19-5044,P17-2057,0,0.0658863,"Missing"
W19-5044,D14-1162,0,0.0829161,"es the constituency parses provided by the dataset to a vector representation via Tree-LSTM; an MT-DNN based text encoder; a feature encoder that encodes domain and generic string-based features through fully-connected layers; and a softmax classifier that takes in the concatenation (⊕) of the three encoders’ output and generates a prediction. The output of base models is sent to the ensemble and conflict resolution modules (the multimodal attention method is depicted here as an example) to make a final prediction. Xu, 2018). The premise and hypothesis are separately embedded (e.g. via GloVe (Pennington et al., 2014) or ELMo (Peters et al., 2018)) and encoded (e.g. via CNN or LSTM). Typically an interaction layer is employed to add information alignment between the premise and the hypothesis. For example, between the two baseline models used in the MedNLI dataset, InferSent (Conneau et al., 2017) computes the interaction vector via [p; h; |p − h|; p ∗ h] and ESIM (Chen et al., 2016) uses an attention matrix to softly align the two representations. ESIM also appends an inference composition layer to propagate the local attended information. A softmax layer is used to classify the final representation. 2018"
W19-5044,N18-1202,0,0.15143,"d by the dataset to a vector representation via Tree-LSTM; an MT-DNN based text encoder; a feature encoder that encodes domain and generic string-based features through fully-connected layers; and a softmax classifier that takes in the concatenation (⊕) of the three encoders’ output and generates a prediction. The output of base models is sent to the ensemble and conflict resolution modules (the multimodal attention method is depicted here as an example) to make a final prediction. Xu, 2018). The premise and hypothesis are separately embedded (e.g. via GloVe (Pennington et al., 2014) or ELMo (Peters et al., 2018)) and encoded (e.g. via CNN or LSTM). Typically an interaction layer is employed to add information alignment between the premise and the hypothesis. For example, between the two baseline models used in the MedNLI dataset, InferSent (Conneau et al., 2017) computes the interaction vector via [p; h; |p − h|; p ∗ h] and ESIM (Chen et al., 2016) uses an attention matrix to softly align the two representations. ESIM also appends an inference composition layer to propagate the local attended information. A softmax layer is used to classify the final representation. 2018). BERT (Devlin et al., 2018)"
W19-5044,W04-1013,0,0.0114582,"entifies and vectorizes biomedical named entities using pre-trained medical taggers and counts (1) the number of each entity type in p and h; and (2) the number of shared entities and shared entity types in a (p, h) pair. In addition to domain knowledge, inspired by Bowman et al. (2015) and Abacha and DemnerFushman (2016), we also extract generic string features and use them to capture the similarity between p and h and then convert the results into vectors. Such similarity information includes ngram overlap, Levenshtein distance (Levenshtein, 1966), Jaccard similarity (Jaccard, 1901), ROUGE (Lin, 2004) and BLEU (Papineni et al., 2001) scores, and absolute length difference.2 To encode the aforementioned features into vectors, each extracted feature is represented by a single scalar and then grouped with others into an array, denoted by v(d) and v(g) for domain and generic features, respectively. Later, they are converted into dense representations by linear transformations and a ReLU nonlinearty. For domain features, this process can be formulated by (d) VF E (p, h) = ReLU(W(d) v(d) + b(d) ) We enhance the base models discussed above with two techniques, namely model ensemble and conflict r"
W19-5044,P19-1189,1,0.90627,"edical domain such as biomedical question answering (Abacha and Demner-Fushman, 2019) and cohort selection (Glicksberg et al., 2018). Many biomedical NLP applications require automatic understanding of symptom descriptions and examination reports (Abacha and Demner-Fushman, 2016, 2017) and therefore can greatly benefit from accurate biomedical NLI systems. In this study, we propose a hybrid approach to biomedical NLI, which includes three main components, as illustrated in Figure 1. The main component is the base model (the largest box in the figure), which includes three encoders: an MT-DNN (Liu et al., 2019c) based text encoder, a syntax encoder that captures structural information, and a feature encoder which injects some degree of domain knowledge into the model (see §3). We conduct unsupervised pre-training for the text encoder on biomedical corpora to compensate for the lack of domain-specific supervision (Lee et al., 2019). To enhance our model, we also use model ensemble and conflict resolution strategies, corresponding to the two top dashed boxes in Figure 1 and are explained in §4. The datasets and implementation detail are described in §5. The experimental results on the MedNLI dataset"
W19-5044,N19-1112,0,0.370595,"edical domain such as biomedical question answering (Abacha and Demner-Fushman, 2019) and cohort selection (Glicksberg et al., 2018). Many biomedical NLP applications require automatic understanding of symptom descriptions and examination reports (Abacha and Demner-Fushman, 2016, 2017) and therefore can greatly benefit from accurate biomedical NLI systems. In this study, we propose a hybrid approach to biomedical NLI, which includes three main components, as illustrated in Figure 1. The main component is the base model (the largest box in the figure), which includes three encoders: an MT-DNN (Liu et al., 2019c) based text encoder, a syntax encoder that captures structural information, and a feature encoder which injects some degree of domain knowledge into the model (see §3). We conduct unsupervised pre-training for the text encoder on biomedical corpora to compensate for the lack of domain-specific supervision (Lee et al., 2019). To enhance our model, we also use model ensemble and conflict resolution strategies, corresponding to the two top dashed boxes in Figure 1 and are explained in §4. The datasets and implementation detail are described in §5. The experimental results on the MedNLI dataset"
W19-5044,D18-1187,0,0.111483,") based text encoder, a syntax encoder that captures structural information, and a feature encoder which injects some degree of domain knowledge into the model (see §3). We conduct unsupervised pre-training for the text encoder on biomedical corpora to compensate for the lack of domain-specific supervision (Lee et al., 2019). To enhance our model, we also use model ensemble and conflict resolution strategies, corresponding to the two top dashed boxes in Figure 1 and are explained in §4. The datasets and implementation detail are described in §5. The experimental results on the MedNLI dataset (Romanov and Shivade, 2018) and the MEDIQA 2019 shared task 1 (Ben Abacha et al., 2019) are reported in §6.1 Natural language inference (NLI) is challenging, especially when it is applied to technical domains such as biomedical settings. In this paper, we propose a hybrid approach to biomedical NLI where different types of information are exploited for this task. Our base model includes a pre-trained text encoder as the core component, and a syntax encoder and a feature encoder to capture syntactic and domain-specific information. Then we combine the output of different base models to form more powerful ensemble models."
W19-5044,N15-1092,0,0.0298709,"For example, between the two baseline models used in the MedNLI dataset, InferSent (Conneau et al., 2017) computes the interaction vector via [p; h; |p − h|; p ∗ h] and ESIM (Chen et al., 2016) uses an attention matrix to softly align the two representations. ESIM also appends an inference composition layer to propagate the local attended information. A softmax layer is used to classify the final representation. 2018). BERT (Devlin et al., 2018) pre-trains the model with large unlabeled corpora which allows better text representations. MT-DNN (Liu et al., 2019c) leverages multi-task learning (Liu et al., 2015) to fine-tune the BERT weights using the GLUE datasets (Wang et al., 2018). The authors showed that resulting representations outperform BERT on many NLU tasks. On top of this sentence pair modeling scheme, previous studies have independently leveraged syntax (Chen et al., 2016), external knowledge (Chen et al., 2018; Lu et al., 2019), ensemble methods (Ghaeini et al., 2018b), and language model fine-tuning (Alsentzer et al., 2019) to improve the performance of NLI systems. Nonetheless, to our knowledge, there have been no empirical results on the effect of combining these additions simultaneo"
W19-5044,K17-1016,1,0.850001,"max Pr (2) r∈{e,c,n} As illustrated in Figure 1, our base model contains three modules. The widely used pre-trained Transformer model (Devlin et al., 2018; Liu et al., 2019c) serves as the basic text encoder to represent p and h. A syntax encoder and a feature encoder are also utilized to augment the basic representation by extracting and encoding more information from the input. The details of these encoders and how they are combined for f are discussed in the following subsections. 3.1 VSE (p) = Tree-LSTM(Parse(p)) Text Encoder Text representation is crucial to facilitate downstream tasks (Song et al., 2017, 2018). As a part of recent advancements in NLP, pre-trained models provide strong baselines for sentence representations and allow great generalizability for the represented text. Therefore, to represent p and h, we adopt a pre-trained Transformer model, MTDNN (Liu et al., 2019c), as the text encoder in our base model. MT-DNN is based on BERT (Devlin et al., 2018) and additionally fine-tuned on GLUE (Wang et al., 2018), a set of NLU datasets including NLI subsets. Through its multi-task learning objective, MT-DNN allows a more general and powerful representation for natural language understa"
W19-5044,P19-1441,0,0.34083,"edical domain such as biomedical question answering (Abacha and Demner-Fushman, 2019) and cohort selection (Glicksberg et al., 2018). Many biomedical NLP applications require automatic understanding of symptom descriptions and examination reports (Abacha and Demner-Fushman, 2016, 2017) and therefore can greatly benefit from accurate biomedical NLI systems. In this study, we propose a hybrid approach to biomedical NLI, which includes three main components, as illustrated in Figure 1. The main component is the base model (the largest box in the figure), which includes three encoders: an MT-DNN (Liu et al., 2019c) based text encoder, a syntax encoder that captures structural information, and a feature encoder which injects some degree of domain knowledge into the model (see §3). We conduct unsupervised pre-training for the text encoder on biomedical corpora to compensate for the lack of domain-specific supervision (Lee et al., 2019). To enhance our model, we also use model ensemble and conflict resolution strategies, corresponding to the two top dashed boxes in Figure 1 and are explained in §4. The datasets and implementation detail are described in §5. The experimental results on the MedNLI dataset"
W19-5044,N18-2028,1,0.843036,"Missing"
W19-5044,W02-0109,0,0.465772,"Missing"
W19-5044,D18-1455,0,0.0280001,"ubsets. Through its multi-task learning objective, MT-DNN allows a more general and powerful representation for natural language understanding than BERT (Liu et al., 2019c). Formally, one can briefly describe the encoder as VT E (p, h) = MT-DNN(p, h) Syntax Encoder (4) where VSE (p) is the output vector. Once p and h are encoded, the final output of this encoder is the concatenation of the two output vectors VSE (p, h) = VSE (p) ⊕ VSE (h) 3.3 (5) Feature Encoder The explicit integration of entity-level external knowledge has been used to improve many NLP models’ performance (Das et al., 2017; Sun et al., 2018). Domain knowledge has also been demonstrated to be useful for in-domain tasks (Romanov and Shivade, 2018; Lu et al., 2019). Therefore, in addition to generic encoders such as MTDNN and Tree-LSTM, we further enhance the model with domain-specific knowledge through indirectly leveraging labeled biomedical data for (3) 417 4 other tasks. To do that, we propose a domain feature encoder that identifies and vectorizes biomedical named entities using pre-trained medical taggers and counts (1) the number of each entity type in p and h; and (2) the number of shared entities and shared entity types in"
W19-5044,P15-1150,0,0.0703295,"Missing"
W19-5044,D18-1185,0,0.0418285,"Missing"
W19-5044,marelli-etal-2014-sick,0,0.0250401,"al., 2017; Tay et al., 2018). It aims to capture the relationship between two sentences, identifying whether a given premise entails, contradicts, or is neutral to a given hypothesis. Success in NLI is crucial for achieving semantic comprehension of human language, which in turn is a prerequisite to accomplish natural language understanding (NLU). In general, accurate NLI systems facilitate many downstream tasks, such as commonsense reasoning (Zellers et al., 2018) and question answering (Abacha and Demner-Fushman, 2016, 2017). Most of existing NLI studies are conducted in the general domain (Marelli et al., 2014; Bowman 2 Related Work A common neural network approach to address the NLI task is sentence pair modeling (Lan and 1 Our code is publicly available at https://github. com/ZhaofengWu/MEDIQA_WTMED 415 Proceedings of the BioNLP 2019 workshop, pages 415–426 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Figure 1: Our overall system. Our base model consists of three encoders and a softmax classifier: a syntax encoder that encodes the constituency parses provided by the dataset to a vector representation via Tree-LSTM; an MT-DNN based text encoder; a feature encod"
W19-5044,W19-5034,0,0.031164,"n. hi = W(h) pi + b(h) Experiment Settings 5.3 (17) Implementation For MT-DNN, we use its own hyperparameters without modification. By default, we use 300-dimensional GloVe embeddings trained on Wikipedia and Gigawords (Pennington et al., The output probability distribution of i-th pair is f (M A) (p, hi ) = softmax(W(o) h0i + b(o) ) (18) 3 Finally, the prediction is computed by Eq. (2). 419 With the tool https://github.com/jtourille/mimic-tools Text Encoder 2014) to initialize the Tree-LSTM, which reduces each parse tree into a 100-dimensional vector. In the feature encoder, we use scispaCy (Neumann et al., 2019) to extract 38 domain features4 . We also extract 27 linguistic features from the 6 categories specified in §3.3. We project the 38 domain features into 38×20 = 760 dimensions and the 27 linguistic features into 27 × 20 = 540 dimensions with fully-connected layers (See Equation (6)). We fine-tune the text encoder with MIMIC-III discharge summaries using the same objectives as BERT, i.e. masked language model and next sentence prediction, for 8 epochs. For training, we use the AdaMax optimizer (Kingma and Ba, 2014) with learning rate 5 × 10−5 . We use a batch size of 16 and train each model for"
W19-5044,W18-5446,0,0.23131,"InferSent (Conneau et al., 2017) computes the interaction vector via [p; h; |p − h|; p ∗ h] and ESIM (Chen et al., 2016) uses an attention matrix to softly align the two representations. ESIM also appends an inference composition layer to propagate the local attended information. A softmax layer is used to classify the final representation. 2018). BERT (Devlin et al., 2018) pre-trains the model with large unlabeled corpora which allows better text representations. MT-DNN (Liu et al., 2019c) leverages multi-task learning (Liu et al., 2015) to fine-tune the BERT weights using the GLUE datasets (Wang et al., 2018). The authors showed that resulting representations outperform BERT on many NLU tasks. On top of this sentence pair modeling scheme, previous studies have independently leveraged syntax (Chen et al., 2016), external knowledge (Chen et al., 2018; Lu et al., 2019), ensemble methods (Ghaeini et al., 2018b), and language model fine-tuning (Alsentzer et al., 2019) to improve the performance of NLI systems. Nonetheless, to our knowledge, there have been no empirical results on the effect of combining these additions simultaneously. Additionally, as recent studies have pointed out that pre-trained co"
W19-5044,2001.mtsummit-papers.68,0,0.0105756,"es biomedical named entities using pre-trained medical taggers and counts (1) the number of each entity type in p and h; and (2) the number of shared entities and shared entity types in a (p, h) pair. In addition to domain knowledge, inspired by Bowman et al. (2015) and Abacha and DemnerFushman (2016), we also extract generic string features and use them to capture the similarity between p and h and then convert the results into vectors. Such similarity information includes ngram overlap, Levenshtein distance (Levenshtein, 1966), Jaccard similarity (Jaccard, 1901), ROUGE (Lin, 2004) and BLEU (Papineni et al., 2001) scores, and absolute length difference.2 To encode the aforementioned features into vectors, each extracted feature is represented by a single scalar and then grouped with others into an array, denoted by v(d) and v(g) for domain and generic features, respectively. Later, they are converted into dense representations by linear transformations and a ReLU nonlinearty. For domain features, this process can be formulated by (d) VF E (p, h) = ReLU(W(d) v(d) + b(d) ) We enhance the base models discussed above with two techniques, namely model ensemble and conflict resolution: ensemble models combin"
W19-5044,N18-1101,0,0.0780091,"Missing"
W19-5044,D18-1009,0,0.0157628,"an important natural language processing (NLP) task that has long been studied (Bowman et al., 2015; Parikh et al., 2016; Chen et al., 2016; Conneau et al., 2017; Tay et al., 2018). It aims to capture the relationship between two sentences, identifying whether a given premise entails, contradicts, or is neutral to a given hypothesis. Success in NLI is crucial for achieving semantic comprehension of human language, which in turn is a prerequisite to accomplish natural language understanding (NLU). In general, accurate NLI systems facilitate many downstream tasks, such as commonsense reasoning (Zellers et al., 2018) and question answering (Abacha and Demner-Fushman, 2016, 2017). Most of existing NLI studies are conducted in the general domain (Marelli et al., 2014; Bowman 2 Related Work A common neural network approach to address the NLI task is sentence pair modeling (Lan and 1 Our code is publicly available at https://github. com/ZhaofengWu/MEDIQA_WTMED 415 Proceedings of the BioNLP 2019 workshop, pages 415–426 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Figure 1: Our overall system. Our base model consists of three encoders and a softmax classifier: a syntax encod"
W19-5044,N19-1093,1,0.83268,"2019b), it is reasonable to ask whether explicitly integrating knowledge will continue to augment such representations. Our work can be seen as an empirical study to examine the efficacy of applying multiple additions on top of Transformer-based models. 3 Base Model NLI is generally treated as a three-way classification task that models whether a given premise p entails, contradicts, or is neutral to a hypothesis h. A classifier f is learned taking p and h as input to predict the class probabilities >  (1) f (p, h) = Pe Pc Pn 3.2 Linguistic understandings, for example coreference relations (Zhang et al., 2019a,b), could aid the interpretation of a sentence. Syntactic structures are often useful for deciding the entailment of a sentence pair (Chen et al., 2016). There exist numerous NLI examples where a hypothesis is merely the premise with adjunct phrases removed. The syntax encoder also mitigates the outof-vocabulary issue which is common in specific domains (Liu et al., 2019a) by capturing the structural information. Therefore, we include a syntax encoder in our base model. We use Tree-LSTM (Tai et al., 2015) to model constituency parse trees of p and h. For each sentence, we encode it according"
W19-5044,P19-1083,1,0.843284,"2019b), it is reasonable to ask whether explicitly integrating knowledge will continue to augment such representations. Our work can be seen as an empirical study to examine the efficacy of applying multiple additions on top of Transformer-based models. 3 Base Model NLI is generally treated as a three-way classification task that models whether a given premise p entails, contradicts, or is neutral to a hypothesis h. A classifier f is learned taking p and h as input to predict the class probabilities >  (1) f (p, h) = Pe Pc Pn 3.2 Linguistic understandings, for example coreference relations (Zhang et al., 2019a,b), could aid the interpretation of a sentence. Syntactic structures are often useful for deciding the entailment of a sentence pair (Chen et al., 2016). There exist numerous NLI examples where a hypothesis is merely the premise with adjunct phrases removed. The syntax encoder also mitigates the outof-vocabulary issue which is common in specific domains (Liu et al., 2019a) by capturing the structural information. Therefore, we include a syntax encoder in our base model. We use Tree-LSTM (Tai et al., 2015) to model constituency parse trees of p and h. For each sentence, we encode it according"
W19-5803,K16-1018,1,0.858083,"Missing"
W19-5803,P11-2008,0,0.174181,"Missing"
W19-5803,N13-1039,0,0.173991,"Missing"
W19-5803,petrov-etal-2012-universal,0,0.108421,"Missing"
W19-5803,P16-2067,0,0.0276283,"is an effective method on improving performance on a target task with few labeled training datasets. In our setting, the knowledge learned from Chinese and English datasets can be considered as a process of knowledge transfer, which jointly contribute to our task of tagging crosslingual texts. • An adversarial network is implemented to improve the share representation, aiming at achieving better tagging performance on cross-lingual texts. Recent advances suggest that recurrent neural networks are capable of learning useful representation information for modeling problems of sequential nature (Plank et al., 2016). In this section, we describe our social media POS tagger, which is based on bidirectional long short term memory (BiLSTM). Since there is lack of annotated social media data as training data, we consider using other out-of-domain labeled data and labeled from different languages, both of which are monolingual. Instead of the common monolingual embeddings, we use cross-lingual embeddings as a bridge between different languages. Our joint model is trained based on different labeled datasets from different domains and languages. Furthermore, we improve the proposed joint model with an adversari"
W19-5803,N06-1014,0,0.00865602,"adopt two approaches to train a bilingual embeddings. 3.2.1 Unsupervised Training We adopt the method proposed in (Zou et al., 2013) to achieve bilingual embeddings. First, by Figure 1: The general architecture of our proposed model. The green box and red box represents two feed-forward networks, which are used for the tagging task and the language identification task, respectively. Note that we only show the operation on the one hidden state of BiLSTM’s outputs, and other hidden states have the same operation. using the machine translation word alignments extracted with the Berkeley Aligner (Liang et al., 2006), two alignment matrices (Azh→en , Aen→zh ) are achieved. Next, two combined objectives are optimized during training: JCO−zh + λJT EO−en→zh (1) JCO−en + λJT EO−zh→en (2) Equation 1 and 2 are optimized for Chinese embeddings and English embeddings, respectively. For example, for Chinese embedding, JCO−zh is to keep the monolingual features of Chinese language itself, and JT EO−en→zh is to optimize the Translation Equivalence. The embeddings are learned through curriculum training on the Chinese Gigaword corpus. 3.2.2 Embedding Projection Instead of training embeddings joint for two languages,"
W19-5803,J93-2004,0,0.0829814,"s alteration in the semantics has a deleterious effect on all the subsequent steps in the NLP pipeline, e.g., Syntactic Parsing, Dependency Parsing, etc. Compared with formal texts, like newswire articles, the POS tagging performance in the social media texts is still far from satisfactory (Ritter et al., 2011; Gimpel et al., 2011). Most state-of-the-art POS tagging approaches are based on supervised methods, in which a large amount of annotated data is needed to train models. However, many datasets constructed for the POS tagging task are from carefully-edited newswire articles, such as PTB (Marcus et al., 1993) and CTB (Xia, 2000), which are greatly different from social media texts. The difference in domains between training data and testing data may heavily impact the performance of approaches based on supervised methods. Hence, most state-of-the-art POS taggers cannot achieve the same performance as reported on newswire domain when applied on social media texts (Owoputi et al., 2013). However, enormous quantities of user generated content on social media are giving increasing attention as well as valuable sources for a variety of applications, such as recommendation (Jiang and Yang, 2017), diseas"
W19-5803,D11-1141,0,0.112111,"Missing"
W19-5803,D13-1141,0,0.0113529,"word embeddings using continuous models for language-specific corpora. However, Chinese- and English- embeddings trained from their own language-specific corpora usually share a totally different semantic space since each language has its own vocabulary space. Therefore, although the Chinese word “政 府” shares the same knowledge semantics with its English translation “government”, their distance in the distributed word representation space is not close as we expect. Specially, we adopt two approaches to train a bilingual embeddings. 3.2.1 Unsupervised Training We adopt the method proposed in (Zou et al., 2013) to achieve bilingual embeddings. First, by Figure 1: The general architecture of our proposed model. The green box and red box represents two feed-forward networks, which are used for the tagging task and the language identification task, respectively. Note that we only show the operation on the one hidden state of BiLSTM’s outputs, and other hidden states have the same operation. using the machine translation word alignments extracted with the Berkeley Aligner (Liang et al., 2006), two alignment matrices (Azh→en , Aen→zh ) are achieved. Next, two combined objectives are optimized during trai"
Y09-1008,E87-1007,0,0.270944,"Missing"
Y09-1008,A00-2019,0,0.0881093,"Missing"
Y09-1008,P08-1021,0,0.433201,"Missing"
Y09-1008,P03-2026,0,\N,Missing
Y10-1067,W06-0127,0,0.0990011,"nd 7002190). Copyright 2010 by Jianguo Chen, John Smith, and Kinko Yamada 583 584 Student Award Papers Furthermore, as studies concerning novel terms are not so common, this paper considers how syntactic information integrated under a machine learning framework can be helpful in discovering novel terms. As early as in 1995, Justeson and Katz defined novel terminology as terms that are newly introduced and not yet widely established, or terms that are current only in more advanced or specialized literature than that with which the intended audience can be presumed to be familiar. Utsuro et al. (2006) specifically define novel terms to be technical terms that are not included in any of existing lexicons of technical terms of the domain. In MeSH1, there will be annual changes to its descriptors (terms). As quoted from their website, „In biomedicine and related areas, new concepts are constantly emerging, old concepts are in a state of flux and terminology and usage are modified accordingly.‟ And „in selecting the expressions to be used for a new MeSH descriptor, it is the usual practice to adopt the expression most commonly used by the authors writing in the English language.‟ Therefore, no"
zhao-etal-2010-large,C04-1081,0,\N,Missing
zhao-etal-2010-large,P01-1005,0,\N,Missing
zhao-etal-2010-large,O03-4002,0,\N,Missing
zhao-etal-2010-large,W06-0137,1,\N,Missing
zhao-etal-2010-large,W06-0115,0,\N,Missing
zhao-etal-2010-large,W06-0127,1,\N,Missing
zhao-etal-2010-large,I08-4017,1,\N,Missing
zhao-etal-2010-large,Y06-1012,1,\N,Missing
