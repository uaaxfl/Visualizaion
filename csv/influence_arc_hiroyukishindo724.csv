2020.coling-main.271,D19-1371,0,0.0127032,"shi et al., 2019). Their method trains bidirectional long short-term memories (BiLSTMs) (Hochreiter and Schmidhuber, 1997) on annotated data, and runs the CKY algorithm to find the globally optimal coordinate structures in a sentence. 4.2 Dataset We evaluate our method on GENIA treebank beta (Tateisi et al., 2005), which is a biomedical-domain corpus that consists of abstracts taken from the MEDLINE database, and contains syntactic annotations, 1 When the similarity takes a negative value, we multiply the square by -1. We used word embeddings pre-trained in bio-domain corpora, namely SciBERT (Beltagy et al., 2019) and Biowordvec (Yijia et al., 2019). For ELMo, we used the model trained on PubMed, available at https://allennlp.org/elmo. 3 When there is a word decomposed into subwords, we create the word vector from the mean of the subword vectors. 4 The span of the second conjunct is determined by the path of the edit graph; once the path reaches the right-most column, we stop the operation and regard the last vertex as the span of the second conjunct. 5 We tune our hyper-parameters, namely, the skip score and normalization value, on the extended Penn Treebank (Ficler and Goldberg, 2016a). 6 Note that t"
2020.coling-main.271,Q17-1010,0,0.00770191,"but not the RARE-mediated signal.” A diagonal edge represents the alignment between two words at the top and right of the edge. In this example, three pairs of words (“the–the,” “retinoid-induced–RARE-mediated,” and “program–signal”) are aligned. The vertical and horizontal edges represent a skipping operation, which indicates that the words are not aligned. To calculate the similarities of the words, we use the square of the cosine similarity of the word embeddings.1 We used three different word embedding methods, namely, BERT (Devlin et al., 2019), ELMo (Peters et al., 2018), and FastText2 (Bojanowski et al., 2017) to investigate the impacts of the embedding methods. For BERT and ELMo, we input a whole sentence containing a conjunction, and use the last layer of the hidden states as the contextualized word embeddings.3 The largest difference between the approach by Shimbo and Hara (2007) and our method is that they use coordination-annotated data to train the feature weights, whereas our method does not. This difference requires some modifications in their algorithm: Because we do not have access to the gold span of the conjuncts, we need to consider all possible candidates of conjuncts within the outer"
2020.coling-main.271,N19-1423,0,0.0435968,"articularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009), and our system extends and simplifies such methods by using neural embedding representations instead of the handcrafted features or heuristic rules used in their approaches. Our experiments show that, even without training, our method achieves good results in identifying nominal coordination bo"
2020.coling-main.271,P16-1079,0,0.0805813,"ges in named entity recognition (NER) tasks, and most of the current NER models (Ma and Hovy, 2016) can identify only non-elliptical conjuncts, e.g., “breast cancer cells,” or incorrectly extract the whole coordinate phrases as single complex entities. Therefore, identifying coordinated noun phrases is crucial to improving the model performance in NLP, particularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and"
2020.coling-main.271,D16-1003,0,0.102405,"ges in named entity recognition (NER) tasks, and most of the current NER models (Ma and Hovy, 2016) can identify only non-elliptical conjuncts, e.g., “breast cancer cells,” or incorrectly extract the whole coordinate phrases as single complex entities. Therefore, identifying coordinated noun phrases is crucial to improving the model performance in NLP, particularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and"
2020.coling-main.271,P09-1109,1,0.833289,"et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009), and our system extends and simplifies such methods by using neural embedding representations instead of the handcrafted features or heuristic rules used in their approaches. Our experiments show that, even without training, our method achieves good results in identifying nominal coordination boundaries in the GENIA corpus (Tateisi et al., 2005). When targeting only the coordination of noun phrases that do not contain clauses or prepositional phrases, our method is even comparable to a supervised baseline model trained on annotated data. This work is licensed under a Creative Commons Attribut"
2020.coling-main.271,J94-4001,0,0.813011,"Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009), and our system extends and simplifies such methods by using neural embedding representations instead of the handcrafted features or heuristic rules used in their approaches. Our experiments show that, even without training, our method achieves good results in identifying nominal coordination boundaries in the GENIA corpus (Tateisi et al., 2005). When targeting only the coordination of noun phrases that do not contain clauses or prepositional phrases, our method is even comparable to a supervised baseline model trained on annotated data. This work is"
2020.coling-main.271,P16-1101,0,0.017409,"scientific literature, coordination is a common syntactic structure and is frequently used to describe technical terminologies. These coordinate structures often involve ellipsis, a linguistic phenomenon in which certain redundant words inferable from the context are omitted. For instance, the phrase “prostate cancer and breast cancer cells” conjoins two cell names, “prostate cancer cell” and “breast cancer cell,” with the token “cell” eliminated from the first conjunct. This phenomenon raises significant challenges in named entity recognition (NER) tasks, and most of the current NER models (Ma and Hovy, 2016) can identify only non-elliptical conjuncts, e.g., “breast cancer cells,” or incorrectly extract the whole coordinate phrases as single complex entities. Therefore, identifying coordinated noun phrases is crucial to improving the model performance in NLP, particularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not"
2020.coling-main.271,N18-1202,0,0.118081,"the model performance in NLP, particularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009), and our system extends and simplifies such methods by using neural embedding representations instead of the handcrafted features or heuristic rules used in their approaches. Our experiments show that, even without training, our method achieves good results in iden"
2020.coling-main.271,D07-1064,0,0.387012,"et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009), and our system extends and simplifies such methods by using neural embedding representations instead of the handcrafted features or heuristic rules used in their approaches. Our experiments show that, even without training, our method achieves good results in identifying nominal coordination boundaries in the GENIA corpus (Tateisi et al., 2005). When targeting only the coordination of noun phrases that do not contain clauses or prepositional phrases, our method is even comparable to a supervised baseline model trained on annotated data. This work is licensed under a Creat"
2020.coling-main.271,I05-2038,0,0.297837,"f words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009), and our system extends and simplifies such methods by using neural embedding representations instead of the handcrafted features or heuristic rules used in their approaches. Our experiments show that, even without training, our method achieves good results in identifying nominal coordination boundaries in the GENIA corpus (Tateisi et al., 2005). When targeting only the coordination of noun phrases that do not contain clauses or prepositional phrases, our method is even comparable to a supervised baseline model trained on annotated data. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 3043 Proceedings of the 28th International Conference on Computational Linguistics, pages 3043–3049 Barcelona, Spain (Online), December 8-13, 2020 2 Related Studies The goal of our method is to identify the coordination boundaries of noun phrases, includ"
2020.coling-main.271,I17-1027,1,0.837084,"ion (NER) tasks, and most of the current NER models (Ma and Hovy, 2016) can identify only non-elliptical conjuncts, e.g., “breast cancer cells,” or incorrectly extract the whole coordinate phrases as single complex entities. Therefore, identifying coordinated noun phrases is crucial to improving the model performance in NLP, particularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and H"
2020.coling-main.271,N19-1343,1,0.805816,"t of the current NER models (Ma and Hovy, 2016) can identify only non-elliptical conjuncts, e.g., “breast cancer cells,” or incorrectly extract the whole coordinate phrases as single complex entities. Therefore, identifying coordinated noun phrases is crucial to improving the model performance in NLP, particularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2"
2020.emnlp-demos.4,D17-1277,0,0.18255,"ponent in various recent studies. We publicize the source code, demonstration, and the pretrained embeddings for 12 languages at https://wikipedia2vec.github.io. 1 Introduction Entity embeddings, i.e., vector representations of entities in knowledge base (KB), have played a vital role in many recent models in natural language processing (NLP). These embeddings provide rich information (or knowledge) regarding entities available in KB using fixed continuous vectors. They have been shown to be beneficial not only for tasks directly related to entities (e.g., entity linking (Yamada et al., 2016; Ganea and Hofmann, 2017)) but also for general NLP tasks (e.g., text classification (Yamada and Shindo, 2019), question answering (Poerner et al., 2019)). Notably, recent studies have also shown that these embeddings can be used to enhance the performance of state-of-the-art contextualized word embeddings (i.e., BERT (Devlin et al., 2019)) on downstream tasks (Zhang et al., 2019; Peters et al., 2019; Poerner et al., 2019). 23 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 23–30 c November 16-20, 2020. 2020 Association for Computational Linguistics using dimensionality reduction algorithms. The demonstr"
2020.emnlp-demos.4,Q17-1010,0,0.714014,"ly similar words and entities close to one another in the vector space. In particular, our tool implements the word-based skip-gram model (Mikolov et al., 2013a,b) to learn word embeddings, and its extensions proposed in Yamada et al. (2016) to learn entity embeddings. Wikipedia2Vec enables users to train embeddings by simply running a single command with a Wikipedia dump file as an input. We highly optimized our implementation, which makes our implementation of the skip-gram model faster than the well-established implementaˇ uˇrek and Sojka, 2010) tion available in gensim (Reh˚ and fastText (Bojanowski et al., 2017). Experimental results demonstrated that our tool achieved enhanced quality compared to the existing tools on several standard benchmarks. Notably, our tool achieved a state-of-the-art result on the entity relatedness task based on the KORE dataset. Due to its effectiveness and efficiency, our tool has been successfully used in various downstream NLP tasks, including entity linking (Yamada et al., 2016; Eshel et al., 2017; Chen et al., 2019), named entity recognition (Sato et al., 2017; Lara-Clares and Garcia-Serrano, 2019), question answering (Yamada et al., 2018b; Poerner et al., 2019), know"
2020.emnlp-demos.4,P17-1149,0,0.0306014,"e model. Note that we used the RDF2Vec and Wiki2Vec as baselines in our experiments, and achieved enhanced empirical performance over these tools on the KORE dataset. Additionally, there have been various relational embedding models proposed (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015) that aim to learn the entity representations that are particularly effective for knowledge graph completion tasks. Related Work Many studies have recently proposed methods to learn entity embeddings from a KB (Hu et al., 2015; Li et al., 2016; Tsai and Roth, 2016; Yamada et al., 2016, 2017, 2018a; Cao et al., 2017; Ganea and Hofmann, 2017). These embeddings are typically based on conventional word embedding models (e.g., skip-gram (Mikolov et al., 2013a)) trained with data retrieved from a KB. For example, Ristoski et al. (2018) proposed RDF2Vec, which learns entity embeddings using the skip-gram model with inputs generated by random walks over the large knowledge graphs such as Wikidata and DBpedia. Furthermore, a simple method that has been widely used in various studies (Yaghoobzadeh and Schutze, 2015; Yamada et al., 2017, 2018a; AlBadrashiny et al., 2017; Suzuki et al., 2018) trains entity embeddin"
2020.emnlp-demos.4,C16-1252,0,0.0214659,"d by internal hyperlinks of Wikipedia as additional contexts to train the model. Note that we used the RDF2Vec and Wiki2Vec as baselines in our experiments, and achieved enhanced empirical performance over these tools on the KORE dataset. Additionally, there have been various relational embedding models proposed (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015) that aim to learn the entity representations that are particularly effective for knowledge graph completion tasks. Related Work Many studies have recently proposed methods to learn entity embeddings from a KB (Hu et al., 2015; Li et al., 2016; Tsai and Roth, 2016; Yamada et al., 2016, 2017, 2018a; Cao et al., 2017; Ganea and Hofmann, 2017). These embeddings are typically based on conventional word embedding models (e.g., skip-gram (Mikolov et al., 2013a)) trained with data retrieved from a KB. For example, Ristoski et al. (2018) proposed RDF2Vec, which learns entity embeddings using the skip-gram model with inputs generated by random walks over the large knowledge graphs such as Wikidata and DBpedia. Furthermore, a simple method that has been widely used in various studies (Yaghoobzadeh and Schutze, 2015; Yamada et al., 2017, 2018"
2020.emnlp-demos.4,N19-1423,0,0.0231458,"anguage processing (NLP). These embeddings provide rich information (or knowledge) regarding entities available in KB using fixed continuous vectors. They have been shown to be beneficial not only for tasks directly related to entities (e.g., entity linking (Yamada et al., 2016; Ganea and Hofmann, 2017)) but also for general NLP tasks (e.g., text classification (Yamada and Shindo, 2019), question answering (Poerner et al., 2019)). Notably, recent studies have also shown that these embeddings can be used to enhance the performance of state-of-the-art contextualized word embeddings (i.e., BERT (Devlin et al., 2019)) on downstream tasks (Zhang et al., 2019; Peters et al., 2019; Poerner et al., 2019). 23 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 23–30 c November 16-20, 2020. 2020 Association for Computational Linguistics using dimensionality reduction algorithms. The demonstration also allows users to explore the embeddings by querying similar words and entities. The source code has been tested on Linux, Windows, and macOS, and released under the Apache License 2.0. We also release the pretrained embeddings for 12 languages (i.e., English, Arabic, Chinese, Dutch, French, German, Italia"
2020.emnlp-demos.4,K17-1008,1,0.782208,", which makes our implementation of the skip-gram model faster than the well-established implementaˇ uˇrek and Sojka, 2010) tion available in gensim (Reh˚ and fastText (Bojanowski et al., 2017). Experimental results demonstrated that our tool achieved enhanced quality compared to the existing tools on several standard benchmarks. Notably, our tool achieved a state-of-the-art result on the entity relatedness task based on the KORE dataset. Due to its effectiveness and efficiency, our tool has been successfully used in various downstream NLP tasks, including entity linking (Yamada et al., 2016; Eshel et al., 2017; Chen et al., 2019), named entity recognition (Sato et al., 2017; Lara-Clares and Garcia-Serrano, 2019), question answering (Yamada et al., 2018b; Poerner et al., 2019), knowledge graph completion (Shah et al., 2019), paraphrase detection (Duong et al., 2019), fake news detection (Singh et al., 2019), and text classification (Yamada and Shindo, 2019). We also introduce a web-based demonstration of our tool that visualizes the embeddings by plotting them onto a two- or three-dimensional space The embeddings of entities in a large knowledge base (e.g., Wikipedia) are highly beneficial for solvi"
2020.emnlp-demos.4,D19-1180,0,0.0378261,"Missing"
2020.emnlp-demos.4,D15-1083,0,0.0606389,"Missing"
2020.emnlp-demos.4,D19-1005,0,0.039818,"Missing"
2020.emnlp-demos.4,K19-1052,1,0.932139,"ol achieved a state-of-the-art result on the entity relatedness task based on the KORE dataset. Due to its effectiveness and efficiency, our tool has been successfully used in various downstream NLP tasks, including entity linking (Yamada et al., 2016; Eshel et al., 2017; Chen et al., 2019), named entity recognition (Sato et al., 2017; Lara-Clares and Garcia-Serrano, 2019), question answering (Yamada et al., 2018b; Poerner et al., 2019), knowledge graph completion (Shah et al., 2019), paraphrase detection (Duong et al., 2019), fake news detection (Singh et al., 2019), and text classification (Yamada and Shindo, 2019). We also introduce a web-based demonstration of our tool that visualizes the embeddings by plotting them onto a two- or three-dimensional space The embeddings of entities in a large knowledge base (e.g., Wikipedia) are highly beneficial for solving various natural language tasks that involve real world knowledge. In this paper, we present Wikipedia2Vec, a Pythonbased open-source tool for learning the embeddings of words and entities from Wikipedia. The proposed tool enables users to learn the embeddings efficiently by issuing a single command with a Wikipedia dump file as an argument. We also"
2020.emnlp-demos.4,K16-1025,1,0.91512,"kipedia2Vec, a Python-based open source tool for learning the embeddings of words and entities easily and efficiently from Wikipedia. Due to its scale, availability in a variety of languages, and constantly evolving nature, Wikipedia is commonly used as a KB to learn entity embeddings. Our proposed tool jointly learns the embeddings of words and entities, and places semantically similar words and entities close to one another in the vector space. In particular, our tool implements the word-based skip-gram model (Mikolov et al., 2013a,b) to learn word embeddings, and its extensions proposed in Yamada et al. (2016) to learn entity embeddings. Wikipedia2Vec enables users to train embeddings by simply running a single command with a Wikipedia dump file as an input. We highly optimized our implementation, which makes our implementation of the skip-gram model faster than the well-established implementaˇ uˇrek and Sojka, 2010) tion available in gensim (Reh˚ and fastText (Bojanowski et al., 2017). Experimental results demonstrated that our tool achieved enhanced quality compared to the existing tools on several standard benchmarks. Notably, our tool achieved a state-of-the-art result on the entity relatedness"
2020.emnlp-demos.4,C18-1016,1,0.8471,"nsim (Reh˚ and fastText (Bojanowski et al., 2017). Experimental results demonstrated that our tool achieved enhanced quality compared to the existing tools on several standard benchmarks. Notably, our tool achieved a state-of-the-art result on the entity relatedness task based on the KORE dataset. Due to its effectiveness and efficiency, our tool has been successfully used in various downstream NLP tasks, including entity linking (Yamada et al., 2016; Eshel et al., 2017; Chen et al., 2019), named entity recognition (Sato et al., 2017; Lara-Clares and Garcia-Serrano, 2019), question answering (Yamada et al., 2018b; Poerner et al., 2019), knowledge graph completion (Shah et al., 2019), paraphrase detection (Duong et al., 2019), fake news detection (Singh et al., 2019), and text classification (Yamada and Shindo, 2019). We also introduce a web-based demonstration of our tool that visualizes the embeddings by plotting them onto a two- or three-dimensional space The embeddings of entities in a large knowledge base (e.g., Wikipedia) are highly beneficial for solving various natural language tasks that involve real world knowledge. In this paper, we present Wikipedia2Vec, a Pythonbased open-source tool for"
2020.emnlp-demos.4,I17-2017,1,0.909757,"n the well-established implementaˇ uˇrek and Sojka, 2010) tion available in gensim (Reh˚ and fastText (Bojanowski et al., 2017). Experimental results demonstrated that our tool achieved enhanced quality compared to the existing tools on several standard benchmarks. Notably, our tool achieved a state-of-the-art result on the entity relatedness task based on the KORE dataset. Due to its effectiveness and efficiency, our tool has been successfully used in various downstream NLP tasks, including entity linking (Yamada et al., 2016; Eshel et al., 2017; Chen et al., 2019), named entity recognition (Sato et al., 2017; Lara-Clares and Garcia-Serrano, 2019), question answering (Yamada et al., 2018b; Poerner et al., 2019), knowledge graph completion (Shah et al., 2019), paraphrase detection (Duong et al., 2019), fake news detection (Singh et al., 2019), and text classification (Yamada and Shindo, 2019). We also introduce a web-based demonstration of our tool that visualizes the embeddings by plotting them onto a two- or three-dimensional space The embeddings of entities in a large knowledge base (e.g., Wikipedia) are highly beneficial for solving various natural language tasks that involve real world knowled"
2020.emnlp-demos.4,P19-1139,0,0.085606,"provide rich information (or knowledge) regarding entities available in KB using fixed continuous vectors. They have been shown to be beneficial not only for tasks directly related to entities (e.g., entity linking (Yamada et al., 2016; Ganea and Hofmann, 2017)) but also for general NLP tasks (e.g., text classification (Yamada and Shindo, 2019), question answering (Poerner et al., 2019)). Notably, recent studies have also shown that these embeddings can be used to enhance the performance of state-of-the-art contextualized word embeddings (i.e., BERT (Devlin et al., 2019)) on downstream tasks (Zhang et al., 2019; Peters et al., 2019; Poerner et al., 2019). 23 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 23–30 c November 16-20, 2020. 2020 Association for Computational Linguistics using dimensionality reduction algorithms. The demonstration also allows users to explore the embeddings by querying similar words and entities. The source code has been tested on Linux, Windows, and macOS, and released under the Apache License 2.0. We also release the pretrained embeddings for 12 languages (i.e., English, Arabic, Chinese, Dutch, French, German, Italian, Japanese, Polish, Portuguese, Russian,"
2020.emnlp-demos.4,N16-1072,0,0.0174405,"erlinks of Wikipedia as additional contexts to train the model. Note that we used the RDF2Vec and Wiki2Vec as baselines in our experiments, and achieved enhanced empirical performance over these tools on the KORE dataset. Additionally, there have been various relational embedding models proposed (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015) that aim to learn the entity representations that are particularly effective for knowledge graph completion tasks. Related Work Many studies have recently proposed methods to learn entity embeddings from a KB (Hu et al., 2015; Li et al., 2016; Tsai and Roth, 2016; Yamada et al., 2016, 2017, 2018a; Cao et al., 2017; Ganea and Hofmann, 2017). These embeddings are typically based on conventional word embedding models (e.g., skip-gram (Mikolov et al., 2013a)) trained with data retrieved from a KB. For example, Ristoski et al. (2018) proposed RDF2Vec, which learns entity embeddings using the skip-gram model with inputs generated by random walks over the large knowledge graphs such as Wikidata and DBpedia. Furthermore, a simple method that has been widely used in various studies (Yaghoobzadeh and Schutze, 2015; Yamada et al., 2017, 2018a; AlBadrashiny et al"
2020.emnlp-demos.4,D14-1167,0,0.0372418,"ntity(""Python ( programming language)""))[:3] [(<Word python>, 0.7265), (<Entity Ruby (programming language)>, 0.6856), (<Entity Perl>, 0.6794)] Figure 2: An example that uses the Wikipedia2Vec embeddings on a Python interactive shell. neighboring entities connected by internal hyperlinks of Wikipedia as additional contexts to train the model. Note that we used the RDF2Vec and Wiki2Vec as baselines in our experiments, and achieved enhanced empirical performance over these tools on the KORE dataset. Additionally, there have been various relational embedding models proposed (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015) that aim to learn the entity representations that are particularly effective for knowledge graph completion tasks. Related Work Many studies have recently proposed methods to learn entity embeddings from a KB (Hu et al., 2015; Li et al., 2016; Tsai and Roth, 2016; Yamada et al., 2016, 2017, 2018a; Cao et al., 2017; Ganea and Hofmann, 2017). These embeddings are typically based on conventional word embedding models (e.g., skip-gram (Mikolov et al., 2013a)) trained with data retrieved from a KB. For example, Ristoski et al. (2018) proposed RDF2Vec, which learns entity embeddi"
2020.emnlp-main.523,C18-1139,0,0.258466,"E achieves a new state of the art by outperforming K-Adapter by 0.7 F1 points. 6446 Name BERT (Zhang et al., 2019) C-GCN (Zhang et al., 2018b) ERNIE (Zhang et al., 2019) SpanBERT (Joshi et al., 2020) MTB (Baldini Soares et al., 2019) KnowBERT (Peters et al., 2019) KEPLER (Wang et al., 2019b) K-Adapter (Wang et al., 2020) RoBERTa (Wang et al., 2020) LUKE Prec. 67.2 69.9 70.0 70.8 71.6 70.4 68.9 70.2 70.4 Rec. 64.8 63.3 66.1 70.9 71.4 73.0 75.4 72.4 75.1 F1 66.0 66.4 68.0 70.8 71.5 71.5 71.7 72.0 71.3 72.7 Name LSTM-CRF (Lample et al., 2016) ELMo (Peters et al., 2018) BERT (Devlin et al., 2019) Akbik et al. (2018) Baevski et al. (2019) RoBERTa LUKE Table 3: Results of named entity recognition on the CoNLL-2003 dataset. Table 2: Results of relation classification on the TACRED dataset. 4.2 Relation Classification Relation classification determines the correct relation between head and tail entities in a sentence. We conduct experiments using TACRED dataset (Zhang et al., 2017), a large-scale relation classification dataset containing 106,264 sentences with 42 relation types. Following Wang et al. (2020), we report the micro-precision, recall, and F1, and use the micro-F1 as the primary metric. Model We"
2020.emnlp-main.523,D19-6007,0,0.0272885,"Missing"
2020.emnlp-main.523,N18-1202,0,0.23558,"of this line of work, when representing entities in text, are that (1) they need to resolve entities in the text to corresponding KB entries to represent the entities, and (2) they cannot represent entities that do not exist in the KB. Contextualized Word Representations Many recent studies have addressed entity-related tasks based on the contextualized representations of entities in text computed using the word representations of CWRs (Zhang et al., 2019; Baldini Soares et al., 2019; Peters et al., 2019; Joshi et al., 2020; Wang et al., 2019b, 2020). Representative examples of CWRs are ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), which are based on deep bidirectional long short-term memory (LSTM) and the transformer (Vaswani et al., 2017), respectively. BERT is trained using an MLM, a pretraining task that masks random words in the text and trains the model to predict the masked words. Most recent CWRs, such as RoBERTa (Liu et al., 2020), XLNet (Yang et al., 2019), SpanBERT (Joshi et al., 2020), ALBERT (Lan et al., 2020), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020), are based on transformer trained using a task equivalent to or similar to the MLM. Similar to our proposed pre"
2020.emnlp-main.523,D19-1005,0,0.0594225,"Missing"
2020.emnlp-main.523,D18-1309,0,0.060673,"Missing"
2020.emnlp-main.523,D16-1264,0,0.378255,"ed model by conducting extensive experiments on five standard entity-related tasks: entity typing, relation classification, NER, cloze-style QA, and extractive QA. Our model outperforms all baseline models, including RoBERTa, in all experiments, and obtains state-of-the-art results on five tasks: entity typing on the Open Entity dataset (Choi et al., 2018), relation classification on the TACRED dataset (Zhang et al., 2017), NER on the CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003), clozestyle QA on the ReCoRD dataset (Zhang et al., 2018a), and extractive QA on the SQuAD 1.1 dataset (Rajpurkar et al., 2016). We publicize our source code and pretrained representations at https://github.com/studio-ousia/luke. The main contributions of this paper are summarized as follows: • We propose LUKE, a new contextualized representations specifically designed to address entityrelated tasks. LUKE is trained to predict randomly masked words and entities using a large amount of entity-annotated corpus obtained from Wikipedia. • We introduce an entity-aware self-attention mechanism, an effective extension of the original mechanism of transformer. The proposed mechanism considers the type of the tokens (words or"
2020.emnlp-main.523,K16-1025,1,0.760269,"ractive question answering). Our source code and pretrained representations are available at https: //github.com/studio-ousia/luke. 1 Introduction Many natural language tasks involve entities, e.g., relation classification, entity typing, named entity recognition (NER), and question answering (QA). Key to solving such entity-related tasks is a model to learn the effective representations of entities. Conventional entity representations assign each entity a fixed embedding vector that stores information regarding the entity in a knowledge base (KB) (Bordes et al., 2013; Trouillon et al., 2016; Yamada et al., 2016, 2017). Although these models capture the rich information in the KB, they require entity linking to represent entities in a text, and cannot represent entities that do not exist in the KB. By contrast, contextualized word representations (CWRs) based on the transformer (Vaswani et al., 2017), such as BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2020), provide effective general-purpose word representations trained with unsupervised pretraining tasks based on language modeling. Many recent studies have solved entity-related tasks using the contextualized representations of entities com"
2020.emnlp-main.523,D18-1244,0,0.38931,"the token attended to. We validate the effectiveness of our proposed model by conducting extensive experiments on five standard entity-related tasks: entity typing, relation classification, NER, cloze-style QA, and extractive QA. Our model outperforms all baseline models, including RoBERTa, in all experiments, and obtains state-of-the-art results on five tasks: entity typing on the Open Entity dataset (Choi et al., 2018), relation classification on the TACRED dataset (Zhang et al., 2017), NER on the CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003), clozestyle QA on the ReCoRD dataset (Zhang et al., 2018a), and extractive QA on the SQuAD 1.1 dataset (Rajpurkar et al., 2016). We publicize our source code and pretrained representations at https://github.com/studio-ousia/luke. The main contributions of this paper are summarized as follows: • We propose LUKE, a new contextualized representations specifically designed to address entityrelated tasks. LUKE is trained to predict randomly masked words and entities using a large amount of entity-annotated corpus obtained from Wikipedia. • We introduce an entity-aware self-attention mechanism, an effective extension of the original mechanism of transfor"
2020.emnlp-main.523,D17-1004,0,0.289926,". To this end, we enhance the self-attention mechanism by adopting different query mechanisms based on the attending token and the token attended to. We validate the effectiveness of our proposed model by conducting extensive experiments on five standard entity-related tasks: entity typing, relation classification, NER, cloze-style QA, and extractive QA. Our model outperforms all baseline models, including RoBERTa, in all experiments, and obtains state-of-the-art results on five tasks: entity typing on the Open Entity dataset (Choi et al., 2018), relation classification on the TACRED dataset (Zhang et al., 2017), NER on the CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003), clozestyle QA on the ReCoRD dataset (Zhang et al., 2018a), and extractive QA on the SQuAD 1.1 dataset (Rajpurkar et al., 2016). We publicize our source code and pretrained representations at https://github.com/studio-ousia/luke. The main contributions of this paper are summarized as follows: • We propose LUKE, a new contextualized representations specifically designed to address entityrelated tasks. LUKE is trained to predict randomly masked words and entities using a large amount of entity-annotated corpus obtained from Wi"
2020.emnlp-main.523,P19-1139,0,0.450505,"se models capture the rich information in the KB, they require entity linking to represent entities in a text, and cannot represent entities that do not exist in the KB. By contrast, contextualized word representations (CWRs) based on the transformer (Vaswani et al., 2017), such as BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2020), provide effective general-purpose word representations trained with unsupervised pretraining tasks based on language modeling. Many recent studies have solved entity-related tasks using the contextualized representations of entities computed based on CWRs (Zhang et al., 2019; Peters et al., 2019; Joshi et al., 2020). However, the architecture of CWRs is not well suited to representing entities for the following two reasons: (1) Because CWRs do not output the span-level representations of entities, they typically need to learn how to compute such representations based on a downstream dataset that is typically small. (2) Many entity-related tasks, e.g., relation classification and QA, involve reasoning about the relationships between entities. Although the transformer can capture the complex relationships between words by relating them to each other multiple times"
2021.acl-long.275,C18-1139,0,0.0137256,"g learning rate ητ = η0 /(1 + γ · τ ), where τ is the index of the current epoch. For ACE2004, ACE2005, and GENIA, the initial learning rates η0 are 0.2, 0.2, and 0.1, and the decay rates γ are 0.01, 0.02, and 0.02 respectively. We set the weight decay rate, the momentum, the batch size, and the number of epochs to be 10−8 , 0.5, 32, and 100 respectively, especially we use batch size 64 on the GENIA dataset. We clip the gradient exceeding 5. Besides, we also conduct experiments to evaluate the performance of our model with contextual word representations. BERT (Devlin et al., 2019) and Flair (Akbik et al., 2018) are the most commonly used contextual word representations in previous work, and have also been proved that they can substantially improve the model performance. In these settings, contextual word representations are concatenated with word and character representations to form the token representations, i.e., xt = [wt , ct , et ], where et is the contextual word representation and it is not fine-tuned in any of our experiments. 1 https://github.com/cambridgeltl/ BioNLP-2016 3551 Methods P ACE2004 R F1 Ju et al. (2018) Wang et al. (2018) Wang and Lu (2018) Luo and Zhao (2020) Lin et al. (2019)"
2021.acl-long.275,N18-1079,0,0.0165581,"ll possible spans and utilize a maximum entropy tagger (Byrne, 2007) and neural networks (Xu et al., 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) for classification. Luan et al. (2019) additionally aims to consider the relationship among entities and proposed a novel method to jointly learn both entities and relations. Hypergraph-based Model Lu and Roth (2015) proposed a hyper-graph structure, in which edges are connected to multiple nodes to represents nested entities. Muis and Lu (2017) and Wang and Lu (2018) resolved spurious structures and ambiguous issue of hyper-graph structure. And Katiyar and Cardie (2018) proposed another kind of hyper-graph structure. Parsing-based Model Finkel and Manning (2009) indicated all these nested entities are located in some non-terminal nodes of the constituency parses of the original sentences, thus they proposed to use a CRF-based constituency parser to obtain them. However, the cubic time complexity limits its applicability. Wang et al. (2018) instead proposed to use a transition-based constituency parser to incrementally build constituency forest, its linear time complexity ensures it can handle longer sentences. 5 Conclusion In this paper, we proposed a simple"
2021.acl-long.275,W16-2922,0,0.0122812,"can be found in Table 1. Dataset Sentences Mentions |Y| m ACE2004 ACE2005 GENIA 6,198 / 742 / 809 7,285 / 968 / 1,058 15,022 / 1,669 / 1,855 22,195 / 2,514 / 3,034 24,700 / 3,218 / 3,029 47,006 / 4,461 / 5,596 29 29 21 6 6 4 Table 1: Sizes of the dataset shown in the train/dev/test split. |Y |is the size of the label set, m is the maximal depth of entity nesting. Hyper-parameters Settings For word embeddings initialization, we utilize 100dimensional pre-trained GloVe (Pennington et al., 2014) for the ACE2004 and the ACE2005 datasets, and use 200-dimensional biomedical domain word embeddings1 (Chiu et al., 2016) for the GENIA dataset. Moreover, we randomly initialize 30dimensional vectors for character embeddings. The hidden state dimension of character-level LSTM dc is 100, i.e., 50 in each direction, thus the dimension of token representation dx is 200. We apply dropout (Srivastava et al., 2014) on token representations before feeding it into the encoder. The hidden state dimension of the three-layered LSTM is 600 for ACE2004 and ACE2005, i.e., 300 in each direction, and 400 for GENIA. Choosing a different dimension is because the maximal depth of entity nesting m is different. We apply layer norma"
2021.acl-long.275,N19-1423,0,0.00801778,"nt descent (SGD), with a decaying learning rate ητ = η0 /(1 + γ · τ ), where τ is the index of the current epoch. For ACE2004, ACE2005, and GENIA, the initial learning rates η0 are 0.2, 0.2, and 0.1, and the decay rates γ are 0.01, 0.02, and 0.02 respectively. We set the weight decay rate, the momentum, the batch size, and the number of epochs to be 10−8 , 0.5, 32, and 100 respectively, especially we use batch size 64 on the GENIA dataset. We clip the gradient exceeding 5. Besides, we also conduct experiments to evaluate the performance of our model with contextual word representations. BERT (Devlin et al., 2019) and Flair (Akbik et al., 2018) are the most commonly used contextual word representations in previous work, and have also been proved that they can substantially improve the model performance. In these settings, contextual word representations are concatenated with word and character representations to form the token representations, i.e., xt = [wt , ct , et ], where et is the contextual word representation and it is not fine-tuned in any of our experiments. 1 https://github.com/cambridgeltl/ BioNLP-2016 3551 Methods P ACE2004 R F1 Ju et al. (2018) Wang et al. (2018) Wang and Lu (2018) Luo an"
2021.acl-long.275,N16-1030,0,0.0169756,"at each level. In addition, we demonstrate that recognizing innermost entities first results in better performance than the conventional outermost entities first scheme. We provide extensive experimental results on ACE2004, ACE2005, and GENIA datasets to show the effectiveness and efficiency of our proposed method. 1 PER ROLE ROLE PER Figure 1: An example of nested NER. Introduction Named entity recognition (NER), as a key technique in natural language processing, aims at detecting entities and assigning semantic category labels to them. Early research (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) proposed to employ deep learning methods and obtained significant performance improvements. However, most of them assume that the entities are not nested within other entities, so-called flat NER. Inherently, these methods do not work satisfactorily when nested entities exist. Figure 1 displays an example of the nested NER task. Recently, a large number of papers proposed novel methods (Fisher and Vlachos, 2019; Wang et al., 2020) for the nested NER task. Among them, layered methods solve this task through multi-level sequential labeling, in which entities are divided into several levels, whe"
2021.acl-long.275,doddington-etal-2004-automatic,0,0.037815,"ion function. We optimize our model by minimizing the sum of the negative log-likelihoods of all levels. L=− m X log p (y l |Hl ) (10) l=1 On the decoding stage, we iteratively apply the Viterbi algorithm (Forney, 1973) at each level to search the most probable label sequences. ˆ l = arg max p (y 0 |Hl ) y (11) y 0 ∈Y n The pseudocodes of the training and the decoding algorithms with max or logsumexp potential function can be found in Algorithms 1 and 2, respectively. 3 Experiments 3.1 Datasets We conduct experiments on three nested named entity recognition datasets in English, i.e., ACE2004 (Doddington et al., 2004), ACE2005 (Walker et al., 2006) and GENIA (Kim et al., 2003). We divide all these datasets into tran/dev/test split by following Shibuya and Hovy (2020) and Wang et al. (2020). The dataset statistics can be found in Table 1. Dataset Sentences Mentions |Y| m ACE2004 ACE2005 GENIA 6,198 / 742 / 809 7,285 / 968 / 1,058 15,022 / 1,669 / 1,855 22,195 / 2,514 / 3,034 24,700 / 3,218 / 3,029 47,006 / 4,461 / 5,596 29 29 21 6 6 4 Table 1: Sizes of the dataset shown in the train/dev/test split. |Y |is the size of the label set, m is the maximal depth of entity nesting. Hyper-parameters Settings For word"
2021.acl-long.275,D09-1015,0,0.145613,"Missing"
2021.acl-long.275,P19-1585,0,0.0578994,"(NER), as a key technique in natural language processing, aims at detecting entities and assigning semantic category labels to them. Early research (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) proposed to employ deep learning methods and obtained significant performance improvements. However, most of them assume that the entities are not nested within other entities, so-called flat NER. Inherently, these methods do not work satisfactorily when nested entities exist. Figure 1 displays an example of the nested NER task. Recently, a large number of papers proposed novel methods (Fisher and Vlachos, 2019; Wang et al., 2020) for the nested NER task. Among them, layered methods solve this task through multi-level sequential labeling, in which entities are divided into several levels, where the term level indicates the depth of entity nesting, and sequential labeling is performed repeatedly. As a special case of layered method, Shibuya and Hovy (2020) force the ∗ ROLE This work was done when the first author was at NAIST. next level entities to locate on the second-best path of the current level search space. Hence, their algorithm can repeatedly detect inner entities through applying a conventi"
2021.acl-long.275,N18-1131,0,0.16333,"h contextual word representations. BERT (Devlin et al., 2019) and Flair (Akbik et al., 2018) are the most commonly used contextual word representations in previous work, and have also been proved that they can substantially improve the model performance. In these settings, contextual word representations are concatenated with word and character representations to form the token representations, i.e., xt = [wt , ct , et ], where et is the contextual word representation and it is not fine-tuned in any of our experiments. 1 https://github.com/cambridgeltl/ BioNLP-2016 3551 Methods P ACE2004 R F1 Ju et al. (2018) Wang et al. (2018) Wang and Lu (2018) Luo and Zhao (2020) Lin et al. (2019) Strakov´a et al. (2019) Shibuya and Hovy (2020) Wang et al. (2020) Our Method (naive) Our Method (max) Our Method (logsumexp) 74.9 78.0 71.8 72.4 73.3 75.1 78.92 79.93 80.83 81.12 81.90 81.24 75.33 75.10 78.86 77.71 78.05 78.96 Strakov´a et al. (2019) [B] Shibuya and Hovy (2020) [B] Wang et al. (2020) [B] Our Method (naive)[B] Our Method (max)[B] Our Method (logsumexp)[B] 84.71 85.23 86.08 86.19 86.27 86.42 Strakov´a et al. (2019) [B+F] Shibuya and Hovy (2020) [B+F] Wang et al. (2020) [B+F] Our Method (naive)[B+F] Our"
2021.acl-long.275,P19-1511,0,0.0237053,"Missing"
2021.acl-long.275,D15-1102,0,0.024461,"ese l-gram spans. Region-based Model Lin et al. (2019) proposed an anchor-region network to recognize nested entities through detecting anchor words and entity boundaries first, and then classify each detected span. Exhaustive models simply enumerate all possible spans and utilize a maximum entropy tagger (Byrne, 2007) and neural networks (Xu et al., 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) for classification. Luan et al. (2019) additionally aims to consider the relationship among entities and proposed a novel method to jointly learn both entities and relations. Hypergraph-based Model Lu and Roth (2015) proposed a hyper-graph structure, in which edges are connected to multiple nodes to represents nested entities. Muis and Lu (2017) and Wang and Lu (2018) resolved spurious structures and ambiguous issue of hyper-graph structure. And Katiyar and Cardie (2018) proposed another kind of hyper-graph structure. Parsing-based Model Finkel and Manning (2009) indicated all these nested entities are located in some non-terminal nodes of the constituency parses of the original sentences, thus they proposed to use a CRF-based constituency parser to obtain them. However, the cubic time complexity limits i"
2021.acl-long.275,N19-1308,0,0.0155588,"m. Wang et al. (2020) proposed to learn the l-gram representations at layer l through applying a decoder component to reduce a sentence layer by layer and to directly classify these l-gram spans. Region-based Model Lin et al. (2019) proposed an anchor-region network to recognize nested entities through detecting anchor words and entity boundaries first, and then classify each detected span. Exhaustive models simply enumerate all possible spans and utilize a maximum entropy tagger (Byrne, 2007) and neural networks (Xu et al., 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) for classification. Luan et al. (2019) additionally aims to consider the relationship among entities and proposed a novel method to jointly learn both entities and relations. Hypergraph-based Model Lu and Roth (2015) proposed a hyper-graph structure, in which edges are connected to multiple nodes to represents nested entities. Muis and Lu (2017) and Wang and Lu (2018) resolved spurious structures and ambiguous issue of hyper-graph structure. And Katiyar and Cardie (2018) proposed another kind of hyper-graph structure. Parsing-based Model Finkel and Manning (2009) indicated all these nested entities are located in some non-terminal"
2021.acl-long.275,2020.acl-main.571,0,0.0565698,"2019) and Flair (Akbik et al., 2018) are the most commonly used contextual word representations in previous work, and have also been proved that they can substantially improve the model performance. In these settings, contextual word representations are concatenated with word and character representations to form the token representations, i.e., xt = [wt , ct , et ], where et is the contextual word representation and it is not fine-tuned in any of our experiments. 1 https://github.com/cambridgeltl/ BioNLP-2016 3551 Methods P ACE2004 R F1 Ju et al. (2018) Wang et al. (2018) Wang and Lu (2018) Luo and Zhao (2020) Lin et al. (2019) Strakov´a et al. (2019) Shibuya and Hovy (2020) Wang et al. (2020) Our Method (naive) Our Method (max) Our Method (logsumexp) 74.9 78.0 71.8 72.4 73.3 75.1 78.92 79.93 80.83 81.12 81.90 81.24 75.33 75.10 78.86 77.71 78.05 78.96 Strakov´a et al. (2019) [B] Shibuya and Hovy (2020) [B] Wang et al. (2020) [B] Our Method (naive)[B] Our Method (max)[B] Our Method (logsumexp)[B] 84.71 85.23 86.08 86.19 86.27 86.42 Strakov´a et al. (2019) [B+F] Shibuya and Hovy (2020) [B+F] Wang et al. (2020) [B+F] Our Method (naive)[B+F] Our Method (max)[B+F] Our Method (logsumexp)[B+F] 84.51 85.94"
2021.acl-long.275,P16-1101,0,0.0330506,"ion for recognition at each level. In addition, we demonstrate that recognizing innermost entities first results in better performance than the conventional outermost entities first scheme. We provide extensive experimental results on ACE2004, ACE2005, and GENIA datasets to show the effectiveness and efficiency of our proposed method. 1 PER ROLE ROLE PER Figure 1: An example of nested NER. Introduction Named entity recognition (NER), as a key technique in natural language processing, aims at detecting entities and assigning semantic category labels to them. Early research (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) proposed to employ deep learning methods and obtained significant performance improvements. However, most of them assume that the entities are not nested within other entities, so-called flat NER. Inherently, these methods do not work satisfactorily when nested entities exist. Figure 1 displays an example of the nested NER task. Recently, a large number of papers proposed novel methods (Fisher and Vlachos, 2019; Wang et al., 2020) for the nested NER task. Among them, layered methods solve this task through multi-level sequential labeling, in which entities are divided in"
2021.acl-long.275,P19-1527,0,0.0242444,"Missing"
2021.acl-long.275,D17-1276,0,0.0193825,"ting anchor words and entity boundaries first, and then classify each detected span. Exhaustive models simply enumerate all possible spans and utilize a maximum entropy tagger (Byrne, 2007) and neural networks (Xu et al., 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) for classification. Luan et al. (2019) additionally aims to consider the relationship among entities and proposed a novel method to jointly learn both entities and relations. Hypergraph-based Model Lu and Roth (2015) proposed a hyper-graph structure, in which edges are connected to multiple nodes to represents nested entities. Muis and Lu (2017) and Wang and Lu (2018) resolved spurious structures and ambiguous issue of hyper-graph structure. And Katiyar and Cardie (2018) proposed another kind of hyper-graph structure. Parsing-based Model Finkel and Manning (2009) indicated all these nested entities are located in some non-terminal nodes of the constituency parses of the original sentences, thus they proposed to use a CRF-based constituency parser to obtain them. However, the cubic time complexity limits its applicability. Wang et al. (2018) instead proposed to use a transition-based constituency parser to incrementally build constitu"
2021.acl-long.275,D14-1162,0,0.0864412,"ll these datasets into tran/dev/test split by following Shibuya and Hovy (2020) and Wang et al. (2020). The dataset statistics can be found in Table 1. Dataset Sentences Mentions |Y| m ACE2004 ACE2005 GENIA 6,198 / 742 / 809 7,285 / 968 / 1,058 15,022 / 1,669 / 1,855 22,195 / 2,514 / 3,034 24,700 / 3,218 / 3,029 47,006 / 4,461 / 5,596 29 29 21 6 6 4 Table 1: Sizes of the dataset shown in the train/dev/test split. |Y |is the size of the label set, m is the maximal depth of entity nesting. Hyper-parameters Settings For word embeddings initialization, we utilize 100dimensional pre-trained GloVe (Pennington et al., 2014) for the ACE2004 and the ACE2005 datasets, and use 200-dimensional biomedical domain word embeddings1 (Chiu et al., 2016) for the GENIA dataset. Moreover, we randomly initialize 30dimensional vectors for character embeddings. The hidden state dimension of character-level LSTM dc is 100, i.e., 50 in each direction, thus the dimension of token representation dx is 200. We apply dropout (Srivastava et al., 2014) on token representations before feeding it into the encoder. The hidden state dimension of the three-layered LSTM is 600 for ACE2004 and ACE2005, i.e., 300 in each direction, and 400 for"
2021.acl-long.275,W95-0107,0,0.136025,"o propose three different selection strategies for fully leveraging information among hidden states. Besides, Shibuya and Hovy (2020) proposed to recognize entities from outermost to inner. We empirically demonstrate that extracting the innermost entities first results in better performance. This may due to the fact that some long entities do not contain any inner entity, so using outermostfirst encoding mixes these entities with other short entities at the same levels, therefore leading encoder representations to be dislocated. In this paper, we convert entities to the IOBES encoding scheme (Ramshaw and Marcus, 1995), and solve nested NER through applying CRF level by level. Our contributions are considered as fourfold, (a) we design a novel nested NER algorithm to explicitly exclude the influence of the best path through using a different potential function at each level, (b) we propose three different selection strategies for fully utilizing information among hidden states, (c) we empirically demonstrate that recognizing entities from innermost to outer results in better performance, (d) and we provide extensive experimental results to demonstrate the effectiveness and efficiency of our proposed method"
2021.acl-long.275,2020.acl-demos.38,0,0.0134811,"al information, then selecting chunks in the original order is sufficient, thus our dynamic selecting mechanism can only slightly improve the model performance. 3.5 nested outermost entities at the same level would dislocate the encoding representation. Furthermore, even if we use the outermost-first encoding scheme, our method is superior to Shibuya and Hovy (2020), which further demonstrates the effectiveness of excluding the influence of the best path. 3.6 Time Complexity and Speed The time complexity of encoder is O (n), and because we employ the same tree reduction acceleration trick4 as Rush (2020), the time complexity of CRF is reduced to O (log n), therefore the overall time complexity is O (n + m · log n). Even our model outperforms slightly worse than Wang et al. (2020), the training and inference speed of our model is much faster than them, as shown in Table 4, since we do not need to stack the decoding component to 16 layers. Especially, when we increase the batch size to 64, the decoding speed is more than two times faster than their model. Method Batch Size Training Decoding Wang et al. (2020) 16 32 64 1,937.16 3,632.64 6,298.85 3,626.53 4,652.05 5,113.85 Our Method 16 32 64 4,1"
2021.acl-long.275,2020.tacl-1.39,0,0.0610903,"not nested within other entities, so-called flat NER. Inherently, these methods do not work satisfactorily when nested entities exist. Figure 1 displays an example of the nested NER task. Recently, a large number of papers proposed novel methods (Fisher and Vlachos, 2019; Wang et al., 2020) for the nested NER task. Among them, layered methods solve this task through multi-level sequential labeling, in which entities are divided into several levels, where the term level indicates the depth of entity nesting, and sequential labeling is performed repeatedly. As a special case of layered method, Shibuya and Hovy (2020) force the ∗ ROLE This work was done when the first author was at NAIST. next level entities to locate on the second-best path of the current level search space. Hence, their algorithm can repeatedly detect inner entities through applying a conventional conditional random field (CRF) (Lafferty et al., 2001) and then exclude the obtained best paths from the search space. To accelerate computation, they also designed an algorithm to efficiently compute the partition function with the best path excluded. Moreover, because they search the outermost entities first, performing the second-best path s"
2021.acl-long.275,D18-1309,0,0.0332534,"Missing"
2021.acl-long.275,D18-1019,0,0.0639049,"ERT (Devlin et al., 2019) and Flair (Akbik et al., 2018) are the most commonly used contextual word representations in previous work, and have also been proved that they can substantially improve the model performance. In these settings, contextual word representations are concatenated with word and character representations to form the token representations, i.e., xt = [wt , ct , et ], where et is the contextual word representation and it is not fine-tuned in any of our experiments. 1 https://github.com/cambridgeltl/ BioNLP-2016 3551 Methods P ACE2004 R F1 Ju et al. (2018) Wang et al. (2018) Wang and Lu (2018) Luo and Zhao (2020) Lin et al. (2019) Strakov´a et al. (2019) Shibuya and Hovy (2020) Wang et al. (2020) Our Method (naive) Our Method (max) Our Method (logsumexp) 74.9 78.0 71.8 72.4 73.3 75.1 78.92 79.93 80.83 81.12 81.90 81.24 75.33 75.10 78.86 77.71 78.05 78.96 Strakov´a et al. (2019) [B] Shibuya and Hovy (2020) [B] Wang et al. (2020) [B] Our Method (naive)[B] Our Method (max)[B] Our Method (logsumexp)[B] 84.71 85.23 86.08 86.19 86.27 86.42 Strakov´a et al. (2019) [B+F] Shibuya and Hovy (2020) [B+F] Wang et al. (2020) [B+F] Our Method (naive)[B+F] Our Method (max)[B+F] Our Method (logsume"
2021.acl-long.275,D18-1124,0,0.0249544,"Missing"
2021.acl-long.275,2020.acl-main.525,0,0.174513,"in natural language processing, aims at detecting entities and assigning semantic category labels to them. Early research (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) proposed to employ deep learning methods and obtained significant performance improvements. However, most of them assume that the entities are not nested within other entities, so-called flat NER. Inherently, these methods do not work satisfactorily when nested entities exist. Figure 1 displays an example of the nested NER task. Recently, a large number of papers proposed novel methods (Fisher and Vlachos, 2019; Wang et al., 2020) for the nested NER task. Among them, layered methods solve this task through multi-level sequential labeling, in which entities are divided into several levels, where the term level indicates the depth of entity nesting, and sequential labeling is performed repeatedly. As a special case of layered method, Shibuya and Hovy (2020) force the ∗ ROLE This work was done when the first author was at NAIST. next level entities to locate on the second-best path of the current level search space. Hence, their algorithm can repeatedly detect inner entities through applying a conventional conditional ran"
2021.acl-long.275,P17-1114,0,0.0249752,"e other is the tokens in recognized entities, to model the interaction among them. Wang et al. (2020) proposed to learn the l-gram representations at layer l through applying a decoder component to reduce a sentence layer by layer and to directly classify these l-gram spans. Region-based Model Lin et al. (2019) proposed an anchor-region network to recognize nested entities through detecting anchor words and entity boundaries first, and then classify each detected span. Exhaustive models simply enumerate all possible spans and utilize a maximum entropy tagger (Byrne, 2007) and neural networks (Xu et al., 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) for classification. Luan et al. (2019) additionally aims to consider the relationship among entities and proposed a novel method to jointly learn both entities and relations. Hypergraph-based Model Lu and Roth (2015) proposed a hyper-graph structure, in which edges are connected to multiple nodes to represents nested entities. Muis and Lu (2017) and Wang and Lu (2018) resolved spurious structures and ambiguous issue of hyper-graph structure. And Katiyar and Cardie (2018) proposed another kind of hyper-graph structure. Parsing-based Model Finkel and"
2021.acl-long.275,P18-1030,0,0.0116537,"s naive. These observations further demonstrate our dynamic chunk selection strategies are capable of learning more meaningful representations. 4 Related Work Existing NER algorithms commonly employ various neural networks to leverage more morphological and contextual information to improve performance. For example, to handle the out-ofvocabulary issue through introducing morphological features, Huang et al. (2015) proposed to employ manual spelling feature, while Ma and Hovy (2016) and Lample et al. (2016) suggested introducing CNN and LSTM to build word representations from character-level. Zhang et al. (2018) and Chen et al. (2019) introduced global representation to enhance encoder capability of encoding contextual information. Layered Model As a layered model, Ju et al. (2018) dynamically update span-level representations for next layer recognition according to recognized inner entities. Fisher and Vlachos (2019) proposed a merge and label method to enhance this idea further. Recently, Shibuya and Hovy (2020) designed a novel algorithm to efficiently learn and decode the second-best path on the span of detected entities. Luo and Zhao (2020) build two different graphs, one is the original token s"
2021.acl-long.275,D19-1034,0,0.0118739,"ties, to model the interaction among them. Wang et al. (2020) proposed to learn the l-gram representations at layer l through applying a decoder component to reduce a sentence layer by layer and to directly classify these l-gram spans. Region-based Model Lin et al. (2019) proposed an anchor-region network to recognize nested entities through detecting anchor words and entity boundaries first, and then classify each detected span. Exhaustive models simply enumerate all possible spans and utilize a maximum entropy tagger (Byrne, 2007) and neural networks (Xu et al., 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) for classification. Luan et al. (2019) additionally aims to consider the relationship among entities and proposed a novel method to jointly learn both entities and relations. Hypergraph-based Model Lu and Roth (2015) proposed a hyper-graph structure, in which edges are connected to multiple nodes to represents nested entities. Muis and Lu (2017) and Wang and Lu (2018) resolved spurious structures and ambiguous issue of hyper-graph structure. And Katiyar and Cardie (2018) proposed another kind of hyper-graph structure. Parsing-based Model Finkel and Manning (2009) indicated all these nested en"
2021.findings-acl.164,K18-2005,0,0.0214902,"number of epochs is 100, and gradients exceed 5 will be clipped. In addition, since the pre-trained contextualized word embeddings technique is widely accepted as a new fundamental utility of natural language processing, we also conduct experiments with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). In these settings, tokens are represented as xt = [wt , ct , et ], where et is the contextual word representation. ELMo vectors are obtained by averaging output vectors over all layers of ELMo. For English experiments, we use the original checkpoint, and use the checkpoints provided by Che et al. (2018) for Chinese experiments. BERT representations are the averages all BERT subword embeddings in the last four layers. Following Li et al. (2020b) and Li et al. (2020a), we utilize bert-large-cased and hfl/chinese-bert-wwm checkpoints for English and Chinese experiments respectively. 4.3 Evaluation NER experiments are evaluated by using F1 scores, and POS tagging experiments are evaluated with accuracy scores. All of our experiments were run 4 times with different random seeds, and the averaged scores are reported in the following tables. Our models3 are implemented with deep learning framework"
2021.findings-acl.164,Q16-1026,0,0.580398,"ly than Cui and Zhang (2019). Notably, the model of Jie and Lu (2019) relies on external dependency annotations, whereas our model requires no external knowledge4 . In the case of employing ELMo, our model outperforms Jie and Lu (2019) by 0.11 F1 score. On the CoNLL 2003 English dataset, our model performs worse than these baseline models, but, with ELMo, it outperforms Jie and Lu (2019) and 3 https://github.com/speedcell4/refiner In this paper, we use “external knowledge” to denote any additional resources other than word embeddings and contextual word representations. 1877 4 Model EK P R F1 Chiu and Nichols (2016) Strubell et al. (2017) Li et al. (2017) Ghaddar and Langlais (2018) Fisher and Vlachos (2019) Cui and Zhang (2019) Yan et al. (2019) Jie and Lu (2019) Our Method X X X - 86.04 88.00 88.53 88.71 86.53 86.50 88.50 88.60 86.28 86.84 87.21 87.95 87.59 88.16 88.43 88.52 88.65 Yan et al. (2019) [E] Jie and Lu (2019) [E] Our Method [E] X - 89.59 89.51 90.17 90.48 89.78 89.88 89.99 Devlin et al. (2019) [B] Fisher and Vlachos (2019) [B] Li et al. (2020b) [B] Yu et al. (2020)[B] Our Method [B] - 90.01 92.98 91.1 90.00 88.35 89.95 91.5 91.17 89.16 89.71 91.11 91.3 90.93 our model significantly outperfor"
2021.findings-acl.164,D19-1422,0,0.0595106,"ploited subword-level features. Moreover, introducing long-term dependency features is also found to be beneficial for sequential labeling. Jie and Lu (2019) attempted to explicitly exploit dependency relations with additional annotations, while Zhang et al. (2018) and Chen et al. (2019) endeavored to learn these relations implicitly with more complex encoders. ∗ O This work was done when the first author was at NAIST. However, as Tishby and Zaslavsky (2015) pointed out, features are not created equal, only the target-relevant features are profitable for improving model performance. Recently, Cui and Zhang (2019) proposed a hierarchically-refined label attention network (LAN), which explicitly leverages label embeddings and captures long-term label dependency relations through multiple refinements layers. Individually picking up the most likely label at each time step is undoubtedly critical, however, considering the entire historical progress is also indispensable. We find that the locally normalized attention, which Cui and Zhang (2019) used to leverage information from label embeddings, can eventually hurt performance. Since it only considers the current time step but ignores labels at other time s"
2021.findings-acl.164,N19-1423,0,0.180674,"g rate ητ = η0 /(1 + 0.075 · τ ), where τ is the index of the current epoch, and the initial learning rate η0 for Chinese experiments without contextual word representations is 0.05, and for all the other experiments we use 0.1. The weight decay rate is 10−8 , the momentum is 0.15, the batch size is 10, the number of epochs is 100, and gradients exceed 5 will be clipped. In addition, since the pre-trained contextualized word embeddings technique is widely accepted as a new fundamental utility of natural language processing, we also conduct experiments with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). In these settings, tokens are represented as xt = [wt , ct , et ], where et is the contextual word representation. ELMo vectors are obtained by averaging output vectors over all layers of ELMo. For English experiments, we use the original checkpoint, and use the checkpoints provided by Che et al. (2018) for Chinese experiments. BERT representations are the averages all BERT subword embeddings in the last four layers. Following Li et al. (2020b) and Li et al. (2020a), we utilize bert-large-cased and hfl/chinese-bert-wwm checkpoints for English and Chinese experiments respectively. 4.3 Evaluat"
2021.findings-acl.164,C18-1161,0,0.0258358,"Missing"
2021.findings-acl.164,D19-1096,0,0.0263898,"Missing"
2021.findings-acl.164,P19-1027,0,0.0111793,", each tag is a part-of-speech category. For instance, NN represents a singular noun and VBN is the past participle of a verb. Introduction Sequential labeling tasks, e.g., named entity recognition (NER) and part-of-speech (POS) tagging, play an important role in natural language processing. Figure 1 shows two examples of sequential labeling tasks. Early studies focused on introducing rich features to improve performance. For example, to handle out-of-vocabulary words by introducing morphological features, Lample et al. (2016) and Ma and Hovy (2016) leveraged character-level features, whereas Heinzerling and Strube (2019) exploited subword-level features. Moreover, introducing long-term dependency features is also found to be beneficial for sequential labeling. Jie and Lu (2019) attempted to explicitly exploit dependency relations with additional annotations, while Zhang et al. (2018) and Chen et al. (2019) endeavored to learn these relations implicitly with more complex encoders. ∗ O This work was done when the first author was at NAIST. However, as Tishby and Zaslavsky (2015) pointed out, features are not created equal, only the target-relevant features are profitable for improving model performance. Recentl"
2021.findings-acl.164,D19-1399,0,0.169071,"named entity recognition (NER) and part-of-speech (POS) tagging, play an important role in natural language processing. Figure 1 shows two examples of sequential labeling tasks. Early studies focused on introducing rich features to improve performance. For example, to handle out-of-vocabulary words by introducing morphological features, Lample et al. (2016) and Ma and Hovy (2016) leveraged character-level features, whereas Heinzerling and Strube (2019) exploited subword-level features. Moreover, introducing long-term dependency features is also found to be beneficial for sequential labeling. Jie and Lu (2019) attempted to explicitly exploit dependency relations with additional annotations, while Zhang et al. (2018) and Chen et al. (2019) endeavored to learn these relations implicitly with more complex encoders. ∗ O This work was done when the first author was at NAIST. However, as Tishby and Zaslavsky (2015) pointed out, features are not created equal, only the target-relevant features are profitable for improving model performance. Recently, Cui and Zhang (2019) proposed a hierarchically-refined label attention network (LAN), which explicitly leverages label embeddings and captures long-term labe"
2021.findings-acl.164,N16-1030,0,0.747727,"ity, while O signifies this word is outside any named entity. In the case of POS tagging, each tag is a part-of-speech category. For instance, NN represents a singular noun and VBN is the past participle of a verb. Introduction Sequential labeling tasks, e.g., named entity recognition (NER) and part-of-speech (POS) tagging, play an important role in natural language processing. Figure 1 shows two examples of sequential labeling tasks. Early studies focused on introducing rich features to improve performance. For example, to handle out-of-vocabulary words by introducing morphological features, Lample et al. (2016) and Ma and Hovy (2016) leveraged character-level features, whereas Heinzerling and Strube (2019) exploited subword-level features. Moreover, introducing long-term dependency features is also found to be beneficial for sequential labeling. Jie and Lu (2019) attempted to explicitly exploit dependency relations with additional annotations, while Zhang et al. (2018) and Chen et al. (2019) endeavored to learn these relations implicitly with more complex encoders. ∗ O This work was done when the first author was at NAIST. However, as Tishby and Zaslavsky (2015) pointed out, features are not created"
2021.findings-acl.164,D19-1099,0,0.0456713,"Missing"
2021.findings-acl.164,D18-1149,0,0.0286425,"he widespread use of contextual word representations, e.g., ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), and BERT (Devlin et al., 2019), greatly improves the performance of NER models and they are accepted as new fundamental techniques of natural language processing. Intuitively speaking, the refinement mechanism provides the models with additional chances to revise previous decisions. In existing work, this method was successfully applied to various tasks, e.g., text classification (Yu et al., 2017), sequential labeling (Cui and Zhang, 2019; Lyu et al., 2019), machine translation (Lee et al., 2018), and question answering (Nema et al., 2019). Our work is not the first attempt of introducing refinement mechanism to sequential labeling tasks. Cui and Zhang (2019) relied on locally normalized attention to softly refine hidden representations layer by layer, while Liu et al. (2019a) chose to discretely filter out target-irrelevant semantic aspects and thus could be considered as a hard refinement mechanism. 6 Conclusion Motivated by the structured attention, we enhanced the previous refinement mechanism by replacing the locally normalized attention with our globally normalized attention. Ex"
2021.findings-acl.164,P16-1101,0,0.630049,"is word is outside any named entity. In the case of POS tagging, each tag is a part-of-speech category. For instance, NN represents a singular noun and VBN is the past participle of a verb. Introduction Sequential labeling tasks, e.g., named entity recognition (NER) and part-of-speech (POS) tagging, play an important role in natural language processing. Figure 1 shows two examples of sequential labeling tasks. Early studies focused on introducing rich features to improve performance. For example, to handle out-of-vocabulary words by introducing morphological features, Lample et al. (2016) and Ma and Hovy (2016) leveraged character-level features, whereas Heinzerling and Strube (2019) exploited subword-level features. Moreover, introducing long-term dependency features is also found to be beneficial for sequential labeling. Jie and Lu (2019) attempted to explicitly exploit dependency relations with additional annotations, while Zhang et al. (2018) and Chen et al. (2019) endeavored to learn these relations implicitly with more complex encoders. ∗ O This work was done when the first author was at NAIST. However, as Tishby and Zaslavsky (2015) pointed out, features are not created equal, only the target"
2021.findings-acl.164,D17-1282,0,0.103833,"l of Jie and Lu (2019) relies on external dependency annotations, whereas our model requires no external knowledge4 . In the case of employing ELMo, our model outperforms Jie and Lu (2019) by 0.11 F1 score. On the CoNLL 2003 English dataset, our model performs worse than these baseline models, but, with ELMo, it outperforms Jie and Lu (2019) and 3 https://github.com/speedcell4/refiner In this paper, we use “external knowledge” to denote any additional resources other than word embeddings and contextual word representations. 1877 4 Model EK P R F1 Chiu and Nichols (2016) Strubell et al. (2017) Li et al. (2017) Ghaddar and Langlais (2018) Fisher and Vlachos (2019) Cui and Zhang (2019) Yan et al. (2019) Jie and Lu (2019) Our Method X X X - 86.04 88.00 88.53 88.71 86.53 86.50 88.50 88.60 86.28 86.84 87.21 87.95 87.59 88.16 88.43 88.52 88.65 Yan et al. (2019) [E] Jie and Lu (2019) [E] Our Method [E] X - 89.59 89.51 90.17 90.48 89.78 89.88 89.99 Devlin et al. (2019) [B] Fisher and Vlachos (2019) [B] Li et al. (2020b) [B] Yu et al. (2020)[B] Our Method [B] - 90.01 92.98 91.1 90.00 88.35 89.95 91.5 91.17 89.16 89.71 91.11 91.3 90.93 our model significantly outperforms them by 1.41 F1 score with BERT. More"
2021.findings-acl.164,2020.acl-main.611,0,0.138687,"We release our CRF implementation with these two tricks as an independent library1 for future study and use. 3.5 Character Embeddings Initialization We describe a trick for Chinese character embeddings initialization. The most striking difference between Chinese and English is that the minimal semantic units, i.e., sememes, of Chinese are characters instead of words or subwords. The character vocabulary size of Chinese, e.g., around 2,000 on the OntoNote 5.0 dataset, is markedly larger than English, e.g., around 100 on the OntoNotes 5.0 English dataset. Existing models (Zhang and Yang, 2018; Li et al., 2020a) generally focused on introducing additional pre-trained character embeddings on the top of lexicon embeddings, and attempted to selectively leverage information from both of them according to the different word segmentation schemes. However, we notice that most of these characters already exist in the word vocabulary as single-character words, thus we employ a randomly initialized orthogonal matrix2 to project the pre-trained word embeddings into the same dimension as the character embeddings, and use these projected embeddings for initialization. 4 Experiments 4.1 Datasets We conduct exper"
2021.findings-acl.164,2020.acl-main.519,0,0.207212,"We release our CRF implementation with these two tricks as an independent library1 for future study and use. 3.5 Character Embeddings Initialization We describe a trick for Chinese character embeddings initialization. The most striking difference between Chinese and English is that the minimal semantic units, i.e., sememes, of Chinese are characters instead of words or subwords. The character vocabulary size of Chinese, e.g., around 2,000 on the OntoNote 5.0 dataset, is markedly larger than English, e.g., around 100 on the OntoNotes 5.0 English dataset. Existing models (Zhang and Yang, 2018; Li et al., 2020a) generally focused on introducing additional pre-trained character embeddings on the top of lexicon embeddings, and attempted to selectively leverage information from both of them according to the different word segmentation schemes. However, we notice that most of these characters already exist in the word vocabulary as single-character words, thus we employ a randomly initialized orthogonal matrix2 to project the pre-trained word embeddings into the same dimension as the character embeddings, and use these projected embeddings for initialization. 4 Experiments 4.1 Datasets We conduct exper"
2021.findings-acl.164,P19-1532,0,0.334171,"word representation already provides rich enough morphological information, thus careful character embeddings initialization can only bring little benefit. On the OntoNotes 5.0 ChiTable 2: Experimental results on the OntoNotes 5.0 English dataset. Checkmark X in the “EK” column indicates that external knowledge is utilized in that model. [E] and [B] stands for ELMo and BERT respectively. Bold and underlined numbers indicate the best and the second-best results respectively. Model EK P R F1 Huang et al. (2015) Lample et al. (2016) Ma and Hovy (2016) Zhang et al. (2018) Chiu and Nichols (2016) Liu et al. (2019a) Yan et al. (2019) Liu et al. (2019b) Our Method X X X - 90.70 90.81 88.83 90.94 91.21 91.57 91.62 91.80 91.33 91.96 90.76 Jie and Lu (2019) [E] Yan et al. (2019)[E] Our Method [E] X - 92.60 93.19 92.40 92.62 92.89 Devlin et al. (2019) [B] Li et al. (2020b) [B] Yu et al. (2020) [B] Our Method [B] - 92.33 93.7 92.66 94.61 93.3 92.98 92.8 93.04 93.5 93.23 Model EK P R F1 Zhang and Yang (2018) Mengge et al. (2019) Gui et al. (2019a) Gui et al. (2019b) Yan et al. (2019) Li et al. (2020a) Our Method Our Method (init) X X X X X X - 76.35 76.78 76.40 76.13 75.28 75.49 71.56 72.54 72.60 73.68 72.39"
2021.findings-acl.164,P19-1233,0,0.180243,"word representation already provides rich enough morphological information, thus careful character embeddings initialization can only bring little benefit. On the OntoNotes 5.0 ChiTable 2: Experimental results on the OntoNotes 5.0 English dataset. Checkmark X in the “EK” column indicates that external knowledge is utilized in that model. [E] and [B] stands for ELMo and BERT respectively. Bold and underlined numbers indicate the best and the second-best results respectively. Model EK P R F1 Huang et al. (2015) Lample et al. (2016) Ma and Hovy (2016) Zhang et al. (2018) Chiu and Nichols (2016) Liu et al. (2019a) Yan et al. (2019) Liu et al. (2019b) Our Method X X X - 90.70 90.81 88.83 90.94 91.21 91.57 91.62 91.80 91.33 91.96 90.76 Jie and Lu (2019) [E] Yan et al. (2019)[E] Our Method [E] X - 92.60 93.19 92.40 92.62 92.89 Devlin et al. (2019) [B] Li et al. (2020b) [B] Yu et al. (2020) [B] Our Method [B] - 92.33 93.7 92.66 94.61 93.3 92.98 92.8 93.04 93.5 93.23 Model EK P R F1 Zhang and Yang (2018) Mengge et al. (2019) Gui et al. (2019a) Gui et al. (2019b) Yan et al. (2019) Li et al. (2020a) Our Method Our Method (init) X X X X X X - 76.35 76.78 76.40 76.13 75.28 75.49 71.56 72.54 72.60 73.68 72.39"
2021.findings-acl.164,L18-1008,0,0.0245265,"(Marcus et al., 1993) and the Universal Dependencies (UD) v2.2 English dataset for POS tagging experiments. The only data pre-processing that we have performed is replacing digital tokens with a special token. And we convert labels to the IOBES labeling scheme (Ramshaw and Marcus, 1995; Ratinov and Roth, 2009) on NER datasets. The dataset statistics are provided in Table 1. 4.2 Hyper-parameter Settings Following Cui and Zhang (2019) and Jie and Lu (2019), 100-dimensional Glove (Pennington et al., 2014) word embeddings are utilized for all the English experiments, and 300-dimensional FastText (Mikolov et al., 2018) word embeddings are employed for Chinese experiments. The dimension of character embeddings is 30, and the hidden states dimension dc of the character bidirectional LSTM is 100, i.e., 50 in each direction. We apply dropout (Srivastava et al., 2014) on token representations with a rate of 0.5. For encoding and refinement layers, the dimension of the hidden state dh of bidirectional LSTMs is 600, i.e., 300 in each direction. We apply dropout (l) on hidden states ht with a rate of 0.5 before feeding into refinement layers. The number of refinement layers L is just 1. We optimize our model by app"
2021.findings-acl.164,D19-1326,0,0.056138,"Missing"
2021.findings-acl.164,D14-1162,0,0.0940471,"of target label types. For NER datasets, we count types with the IOBES labeling scheme. Street Journal (WSJ) dataset (Marcus et al., 1993) and the Universal Dependencies (UD) v2.2 English dataset for POS tagging experiments. The only data pre-processing that we have performed is replacing digital tokens with a special token. And we convert labels to the IOBES labeling scheme (Ramshaw and Marcus, 1995; Ratinov and Roth, 2009) on NER datasets. The dataset statistics are provided in Table 1. 4.2 Hyper-parameter Settings Following Cui and Zhang (2019) and Jie and Lu (2019), 100-dimensional Glove (Pennington et al., 2014) word embeddings are utilized for all the English experiments, and 300-dimensional FastText (Mikolov et al., 2018) word embeddings are employed for Chinese experiments. The dimension of character embeddings is 30, and the hidden states dimension dc of the character bidirectional LSTM is 100, i.e., 50 in each direction. We apply dropout (Srivastava et al., 2014) on token representations with a rate of 0.5. For encoding and refinement layers, the dimension of the hidden state dh of bidirectional LSTMs is 600, i.e., 300 in each direction. We apply dropout (l) on hidden states ht with a rate of 0."
2021.findings-acl.164,N18-1202,0,0.241169,"ent (SGD) with decaying learning rate ητ = η0 /(1 + 0.075 · τ ), where τ is the index of the current epoch, and the initial learning rate η0 for Chinese experiments without contextual word representations is 0.05, and for all the other experiments we use 0.1. The weight decay rate is 10−8 , the momentum is 0.15, the batch size is 10, the number of epochs is 100, and gradients exceed 5 will be clipped. In addition, since the pre-trained contextualized word embeddings technique is widely accepted as a new fundamental utility of natural language processing, we also conduct experiments with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). In these settings, tokens are represented as xt = [wt , ct , et ], where et is the contextual word representation. ELMo vectors are obtained by averaging output vectors over all layers of ELMo. For English experiments, we use the original checkpoint, and use the checkpoints provided by Che et al. (2018) for Chinese experiments. BERT representations are the averages all BERT subword embeddings in the last four layers. Following Li et al. (2020b) and Li et al. (2020a), we utilize bert-large-cased and hfl/chinese-bert-wwm checkpoints for English and Chinese experi"
2021.findings-acl.164,W13-3516,0,0.0423898,"Missing"
2021.findings-acl.164,W95-0107,0,0.625899,"lish 38,219 / 5,527 / 5,462 12,544 / 2,003 / 2,078 45 50 Table 1: Dataset statistics, where the “Sentences” column displays the number of sentences in train/dev/test split respectively, the |Y |column displays the number of target label types. For NER datasets, we count types with the IOBES labeling scheme. Street Journal (WSJ) dataset (Marcus et al., 1993) and the Universal Dependencies (UD) v2.2 English dataset for POS tagging experiments. The only data pre-processing that we have performed is replacing digital tokens with a special token. And we convert labels to the IOBES labeling scheme (Ramshaw and Marcus, 1995; Ratinov and Roth, 2009) on NER datasets. The dataset statistics are provided in Table 1. 4.2 Hyper-parameter Settings Following Cui and Zhang (2019) and Jie and Lu (2019), 100-dimensional Glove (Pennington et al., 2014) word embeddings are utilized for all the English experiments, and 300-dimensional FastText (Mikolov et al., 2018) word embeddings are employed for Chinese experiments. The dimension of character embeddings is 30, and the hidden states dimension dc of the character bidirectional LSTM is 100, i.e., 50 in each direction. We apply dropout (Srivastava et al., 2014) on token repres"
2021.findings-acl.164,W09-1119,0,0.111709,"2 12,544 / 2,003 / 2,078 45 50 Table 1: Dataset statistics, where the “Sentences” column displays the number of sentences in train/dev/test split respectively, the |Y |column displays the number of target label types. For NER datasets, we count types with the IOBES labeling scheme. Street Journal (WSJ) dataset (Marcus et al., 1993) and the Universal Dependencies (UD) v2.2 English dataset for POS tagging experiments. The only data pre-processing that we have performed is replacing digital tokens with a special token. And we convert labels to the IOBES labeling scheme (Ramshaw and Marcus, 1995; Ratinov and Roth, 2009) on NER datasets. The dataset statistics are provided in Table 1. 4.2 Hyper-parameter Settings Following Cui and Zhang (2019) and Jie and Lu (2019), 100-dimensional Glove (Pennington et al., 2014) word embeddings are utilized for all the English experiments, and 300-dimensional FastText (Mikolov et al., 2018) word embeddings are employed for Chinese experiments. The dimension of character embeddings is 30, and the hidden states dimension dc of the character bidirectional LSTM is 100, i.e., 50 in each direction. We apply dropout (Srivastava et al., 2014) on token representations with a rate of"
2021.findings-acl.164,2020.acl-demos.38,0,0.0284906,"We apply the Viterbi algorithm (Forney, 1973) to efficiently search for the most probable label sequences on the decoding stage. ˆ = arg max p (y 0 |h(L+1) ) y (14) y 0 ∈Y n 3.4 Complexity and Implementation Tricks One concern regarding our proposed method is its computational complexity, as it requires to compute not only the partition function but also the marginal probability. Calculating the partition function, as in Equation 8, is the well-known bottleneck of CRF computation. And this is commonly achieved through reducing potential matrices by applying matrix multiplications. Similar to Rush (2020), we make use of the associative property of matrix multiplication to accelerate computation. The product of multiplying matrices A, B, C, and D is equivalent to the product of AB and CD. Leveraging the power of GPU to compute AB and CD in parallel, and recursively applying this trick, we can reduce the time complexity of obtainP|B| ing the partition function from O ( i=1 |x|i ) to P|B| O ( i=1 log |x|i ), where |x|i is the length of i-th sentence in batch B. Moreover, instead of padding the sequence length |xi |out to the nearest power of two as Rush (2020) does, we pre-compile argument indic"
2021.findings-acl.164,D17-1283,0,0.114274,"019). Notably, the model of Jie and Lu (2019) relies on external dependency annotations, whereas our model requires no external knowledge4 . In the case of employing ELMo, our model outperforms Jie and Lu (2019) by 0.11 F1 score. On the CoNLL 2003 English dataset, our model performs worse than these baseline models, but, with ELMo, it outperforms Jie and Lu (2019) and 3 https://github.com/speedcell4/refiner In this paper, we use “external knowledge” to denote any additional resources other than word embeddings and contextual word representations. 1877 4 Model EK P R F1 Chiu and Nichols (2016) Strubell et al. (2017) Li et al. (2017) Ghaddar and Langlais (2018) Fisher and Vlachos (2019) Cui and Zhang (2019) Yan et al. (2019) Jie and Lu (2019) Our Method X X X - 86.04 88.00 88.53 88.71 86.53 86.50 88.50 88.60 86.28 86.84 87.21 87.95 87.59 88.16 88.43 88.52 88.65 Yan et al. (2019) [E] Jie and Lu (2019) [E] Our Method [E] X - 89.59 89.51 90.17 90.48 89.78 89.88 89.99 Devlin et al. (2019) [B] Fisher and Vlachos (2019) [B] Li et al. (2020b) [B] Yu et al. (2020)[B] Our Method [B] - 90.01 92.98 91.1 90.00 88.35 89.95 91.5 91.17 89.16 89.71 91.11 91.3 90.93 our model significantly outperforms them by 1.41 F1 scor"
2021.findings-acl.164,D18-1279,0,0.0300428,"Missing"
2021.findings-acl.164,N18-1089,0,0.0439197,"Missing"
2021.findings-acl.164,2020.acl-main.577,0,0.129141,"to denote any additional resources other than word embeddings and contextual word representations. 1877 4 Model EK P R F1 Chiu and Nichols (2016) Strubell et al. (2017) Li et al. (2017) Ghaddar and Langlais (2018) Fisher and Vlachos (2019) Cui and Zhang (2019) Yan et al. (2019) Jie and Lu (2019) Our Method X X X - 86.04 88.00 88.53 88.71 86.53 86.50 88.50 88.60 86.28 86.84 87.21 87.95 87.59 88.16 88.43 88.52 88.65 Yan et al. (2019) [E] Jie and Lu (2019) [E] Our Method [E] X - 89.59 89.51 90.17 90.48 89.78 89.88 89.99 Devlin et al. (2019) [B] Fisher and Vlachos (2019) [B] Li et al. (2020b) [B] Yu et al. (2020)[B] Our Method [B] - 90.01 92.98 91.1 90.00 88.35 89.95 91.5 91.17 89.16 89.71 91.11 91.3 90.93 our model significantly outperforms them by 1.41 F1 score with BERT. Moreover, on the OntoNotes 5.0 Chinese dataset, our model constantly outperforms the best previous work (Jie and Lu, 2019) by 0.65 F1 score without utilizing external knowledge. Besides, we can notice initializing character embeddings with our trick remarkably improves model performance by 0.76 F1 score on the OntoNotes 4.0 Chinese dataset, even this improvement reduces to only 0.00 and 0.20 F1 scores on ELMo and BERT experiments."
2021.findings-acl.164,D17-1056,0,0.0258509,"ll possible spans and to utilize a biaffine classifier to assign category labels to them. Besides, the widespread use of contextual word representations, e.g., ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), and BERT (Devlin et al., 2019), greatly improves the performance of NER models and they are accepted as new fundamental techniques of natural language processing. Intuitively speaking, the refinement mechanism provides the models with additional chances to revise previous decisions. In existing work, this method was successfully applied to various tasks, e.g., text classification (Yu et al., 2017), sequential labeling (Cui and Zhang, 2019; Lyu et al., 2019), machine translation (Lee et al., 2018), and question answering (Nema et al., 2019). Our work is not the first attempt of introducing refinement mechanism to sequential labeling tasks. Cui and Zhang (2019) relied on locally normalized attention to softly refine hidden representations layer by layer, while Liu et al. (2019a) chose to discretely filter out target-irrelevant semantic aspects and thus could be considered as a hard refinement mechanism. 6 Conclusion Motivated by the structured attention, we enhanced the previous refineme"
2021.findings-acl.164,P18-1030,0,0.266715,"ge processing. Figure 1 shows two examples of sequential labeling tasks. Early studies focused on introducing rich features to improve performance. For example, to handle out-of-vocabulary words by introducing morphological features, Lample et al. (2016) and Ma and Hovy (2016) leveraged character-level features, whereas Heinzerling and Strube (2019) exploited subword-level features. Moreover, introducing long-term dependency features is also found to be beneficial for sequential labeling. Jie and Lu (2019) attempted to explicitly exploit dependency relations with additional annotations, while Zhang et al. (2018) and Chen et al. (2019) endeavored to learn these relations implicitly with more complex encoders. ∗ O This work was done when the first author was at NAIST. However, as Tishby and Zaslavsky (2015) pointed out, features are not created equal, only the target-relevant features are profitable for improving model performance. Recently, Cui and Zhang (2019) proposed a hierarchically-refined label attention network (LAN), which explicitly leverages label embeddings and captures long-term label dependency relations through multiple refinements layers. Individually picking up the most likely label at"
2021.findings-acl.164,P18-1144,0,0.01654,"to O (maxi log |x|i ). We release our CRF implementation with these two tricks as an independent library1 for future study and use. 3.5 Character Embeddings Initialization We describe a trick for Chinese character embeddings initialization. The most striking difference between Chinese and English is that the minimal semantic units, i.e., sememes, of Chinese are characters instead of words or subwords. The character vocabulary size of Chinese, e.g., around 2,000 on the OntoNote 5.0 dataset, is markedly larger than English, e.g., around 100 on the OntoNotes 5.0 English dataset. Existing models (Zhang and Yang, 2018; Li et al., 2020a) generally focused on introducing additional pre-trained character embeddings on the top of lexicon embeddings, and attempted to selectively leverage information from both of them according to the different word segmentation schemes. However, we notice that most of these characters already exist in the word vocabulary as single-character words, thus we employ a randomly initialized orthogonal matrix2 to project the pre-trained word embeddings into the same dimension as the character embeddings, and use these projected embeddings for initialization. 4 Experiments 4.1 Datasets"
C18-1016,N13-1092,0,0.0509364,"Missing"
C18-1016,D13-1029,0,0.0747455,"Missing"
C18-1016,P15-1125,0,0.0421276,"Missing"
C18-1016,P15-1162,0,0.122857,"Missing"
C18-1016,C16-1252,0,0.215859,"Missing"
C18-1016,Q15-1023,0,0.0628516,"Missing"
C18-1016,K17-1012,0,0.0608319,"Missing"
C18-1016,N15-1054,0,0.0485688,"Missing"
C18-1016,D14-1167,0,0.183748,"Missing"
C18-1016,D15-1083,0,0.129063,"Missing"
C18-1016,E17-1055,0,0.0377722,"Missing"
C18-1016,K16-1025,1,0.914735,"Missing"
D16-1109,D15-1085,0,0.0260782,"ence from the stack. Using this approach, they achieved high performance in terms of both dependency parsing and disfluency detection on the Switchboard corpus. However, the authors assume that the input texts to parse are transcribed by human annotators, which, in practice, is unrealistic. In real-world applications, in addition to disfluencies, the input texts contain ASR errors; these issues might degrade the parsing performance. For example, proper nouns that are not contained in the ASR system vocabulary may break up into smaller pieces, yielding a difficult problem for the parsing unit (Cheng et al., 2015): Joint dependency parsing with disfluency detection is an important task in speech language processing. Recent methods show high performance for this task, although most authors make the unrealistic assumption that input texts are transcribed by human annotators. In real-world applications, the input text is typically the output of an automatic speech recognition (ASR) system, which implies that the text contains not only disfluency noises but also recognition errors from the ASR system. In this work, we propose a parsing method that handles both disfluency and ASR errors using an incremental"
D16-1109,E14-4009,0,0.0228192,"for transferring the gold dependency annotation to the ASR output texts to construct training data for our parser. We conducted an experiment on the Switchboard corpus and show that our method outperforms conventional methods in terms of dependency parsing and disfluency detection. 1 Introduction Spontaneous speech is different from written text in many ways, one of which is that it contains disfluencies, that is, parts of the utterance that are corrected by the speaker during the utterance. NLP system performance is reported to deteriorate when there are disfluencies, for example, with SMT (Cho et al., 2014). Therefore, it is desirable to preprocess the speech before passing it to other NLP tasks. REF: what can we get at Litanfeeth HYP: what can we get it leaks on feet In this work, we propose a method for joint dependency parsing and disfluency detection that can robustly parse ASR output texts. Our parser handles both disfluencies and ASR errors using an incremental shift-reduce algorithm, with novel features that consider recognition errors of the ASR system. Furthermore, to evaluate dependency parsing per1036 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processi"
D16-1109,de-marneffe-etal-2006-generating,0,0.178034,"Missing"
D16-1109,N15-1029,0,0.350391,"Missing"
D16-1109,Q14-1011,0,0.421667,"tArcError, and RightArcError, to handle disfluencies and ASR errors. Edit action removes a disfluent token when it is the first element of the stack. This is different from Honnibal (2014)’s Edit action: theirs accumulates consecutive disfluent tokens on the top of the stack and removes them all at once, whereas our method removes this kind of token one-by-one. Use of this Edit action guarantees that the length of the action sequence is always 2n − 1. This property is advantageous because the parser can use the standard beam search and does not require normalization, such as those adopted in (Honnibal and Johnson, 2014) and (Zhu et al., 2013). LeftArcError and RightArcError act in the same way as LeftArc and RightArc, except that these act only on ASR error tokens, whereas the original LeftArc and RightArc are reserved for non ASR error tokens. Using two different kinds of Arc actions for the two types of tokens (ASR error or not) allows for the weights not to be shared between them, and is expected to yield improved performance. In the experiment below, we train all of the models using structured perceptron with max violation (Huang et al., 2012). The feature set is mainly based on (Honnibal and Johnson, 20"
D16-1109,D14-1009,0,0.0135883,"d Disfluency Detection for Automatic Speech Recognition Texts Masashi Yoshikawa and Hiroyuki Shindo and Yuji Matsumoto Graduate School of Information and Science Nara Institute of Science and Technology 8916-5, Takayama, Ikoma, Nara, 630-0192, Japan { masashi.yoshikawa.yh8, shindo, matsu }@is.naist.jp Abstract There are a number of studies that address the problem of detecting disfluencies. Some of these studies include dependency parsing (Honnibal and Johnson, 2014; Wu et al., 2015; Rasooli and Tetreault, 2014), whereas others are dedicated systems (Qian and Liu, 2013; Ferguson et al., 2015; Hough and Purver, 2014; Hough and Schlangen, 2015; Liu et al., 2003). Among these studies, Honnibal (2014) and Wu (2015) address this problem by adding a new action to transition-based dependency parsing that removes the disfluent parts of the input sentence from the stack. Using this approach, they achieved high performance in terms of both dependency parsing and disfluency detection on the Switchboard corpus. However, the authors assume that the input texts to parse are transcribed by human annotators, which, in practice, is unrealistic. In real-world applications, in addition to disfluencies, the input texts con"
D16-1109,N12-1015,0,0.060834,"Missing"
D16-1109,N13-1102,0,0.118923,"oint Transition-based Dependency Parsing and Disfluency Detection for Automatic Speech Recognition Texts Masashi Yoshikawa and Hiroyuki Shindo and Yuji Matsumoto Graduate School of Information and Science Nara Institute of Science and Technology 8916-5, Takayama, Ikoma, Nara, 630-0192, Japan { masashi.yoshikawa.yh8, shindo, matsu }@is.naist.jp Abstract There are a number of studies that address the problem of detecting disfluencies. Some of these studies include dependency parsing (Honnibal and Johnson, 2014; Wu et al., 2015; Rasooli and Tetreault, 2014), whereas others are dedicated systems (Qian and Liu, 2013; Ferguson et al., 2015; Hough and Purver, 2014; Hough and Schlangen, 2015; Liu et al., 2003). Among these studies, Honnibal (2014) and Wu (2015) address this problem by adding a new action to transition-based dependency parsing that removes the disfluent parts of the input sentence from the stack. Using this approach, they achieved high performance in terms of both dependency parsing and disfluency detection on the Switchboard corpus. However, the authors assume that the input texts to parse are transcribed by human annotators, which, in practice, is unrealistic. In real-world applications, i"
D16-1109,E14-4010,0,0.144007,"Missing"
D16-1109,N03-1033,0,0.0728066,"Missing"
D16-1109,P15-1048,0,0.481042,"Missing"
D16-1109,P11-2033,0,0.0477469,"gned tokens do not match exactly on the character level, the mismatch is regarded as an instance of a substitution type of ASR error. Therefore, we encode this fact in the label of the arc from the token to its head. In Figure 1(a), the words “made” and “slipped” in the ASR hypothesis do not match the gold transcription tokens, “may” and “flip”, respectively. Therefore, we automatically re-label the arc from each token to its head as “error”. 3 Transition-based Dependency Parsing To parse texts that contain disfluencies and ASR errors, we extend the ArcEager shift-reduce dependency parser of (Zhang and Nivre, 2011). Our proposed parser adopts the same Shift, Reduce, LeftArc, and RightArc actions as ArcEager. To this parser we add three new actions, i.e., Edit, LeftArcError, and RightArcError, to handle disfluencies and ASR errors. Edit action removes a disfluent token when it is the first element of the stack. This is different from Honnibal (2014)’s Edit action: theirs accumulates consecutive disfluent tokens on the top of the stack and removes them all at once, whereas our method removes this kind of token one-by-one. Use of this Edit action guarantees that the length of the action sequence is always"
D16-1109,P13-1043,0,0.028161,"handle disfluencies and ASR errors. Edit action removes a disfluent token when it is the first element of the stack. This is different from Honnibal (2014)’s Edit action: theirs accumulates consecutive disfluent tokens on the top of the stack and removes them all at once, whereas our method removes this kind of token one-by-one. Use of this Edit action guarantees that the length of the action sequence is always 2n − 1. This property is advantageous because the parser can use the standard beam search and does not require normalization, such as those adopted in (Honnibal and Johnson, 2014) and (Zhu et al., 2013). LeftArcError and RightArcError act in the same way as LeftArc and RightArc, except that these act only on ASR error tokens, whereas the original LeftArc and RightArc are reserved for non ASR error tokens. Using two different kinds of Arc actions for the two types of tokens (ASR error or not) allows for the weights not to be shared between them, and is expected to yield improved performance. In the experiment below, we train all of the models using structured perceptron with max violation (Huang et al., 2012). The feature set is mainly based on (Honnibal and Johnson, 2014), such as the disflu"
D18-1191,W04-2412,0,0.0923687,"sentations far away from each other (Wen et al., 2016; Luo et al., 2017). 6 Related Work 6.1 Semantic Role Labeling Tasks Automatic SRL has been widely studied (Gildea and Jurafsky, 2002). There have been two main styles of SRL. Figure 4 illustrates an example of span-based and dependency-based SRL. In dependency-based SRL (at the upper part of Figure 4), the correct A2 argument for the predicate “hit” is the word “with”. On one hand, in span-based SRL (at the lower part of Figure 4), the correct A2 argument is the span “with the bat”. For span-based SRL, the CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) provided the task settings and datasets. In the task settings, various SRL models, from traditional pipeline models to recent neural ones, have been proposed and competed with each other (Pradhan et al., 2005; He et al., 2017; Tan et al., 2018). For dependency-based SRL, the CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) provided the task settings and datasets. As in span-based SRL, recent neural models achieved high-performance in dependency-based SRL (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017; He et al., 2018b; Cai e"
D18-1191,P18-1192,0,0.353053,"reras and Marquez, 2004; Carreras and M`arquez, 2005) provided the task settings and datasets. In the task settings, various SRL models, from traditional pipeline models to recent neural ones, have been proposed and competed with each other (Pradhan et al., 2005; He et al., 2017; Tan et al., 2018). For dependency-based SRL, the CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) provided the task settings and datasets. As in span-based SRL, recent neural models achieved high-performance in dependency-based SRL (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017; He et al., 2018b; Cai et al., 2018). This paper focuses on span-based SRL. 6.2 BIO-based SRL Models • FrameNet-style SRL (Baker et al., 1998) • PropBank-style SRL (Palmer et al., 2005) In this paper, we have tackled PropBank-style SRL.11 In PropBank-style SRL, there have been two main task settings. • Span-based SRL: CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) • Dependency-based SRL: CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) 11 A1 A2 A1 1.0 A1 Detailed descriptions on FrameNet-style and PropBankstyle SRL can be found in Baker"
D18-1191,W05-0620,0,0.873706,"Missing"
D18-1191,kingsbury-palmer-2002-treebank,0,0.280375,"ocuses on span-based SRL. 6.2 BIO-based SRL Models • FrameNet-style SRL (Baker et al., 1998) • PropBank-style SRL (Palmer et al., 2005) In this paper, we have tackled PropBank-style SRL.11 In PropBank-style SRL, there have been two main task settings. • Span-based SRL: CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) • Dependency-based SRL: CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) 11 A1 A2 A1 1.0 A1 Detailed descriptions on FrameNet-style and PropBankstyle SRL can be found in Baker et al. (1998); Das et al. (2014); Kingsbury and Palmer (2002); Palmer et al. (2005). Span-based SRL can be solved as BIO sequential tagging (Hacioglu et al., 2004; Pradhan et al., 2005; M`arquez et al., 2005). Neural models State-of-the-art SRL models use neural networks based on the BIO tagging approach. The pioneering neural SRL model was proposed by Collobert et al. (2011). They use convolutional neural networks (CNNs) and CRFs. Instead of CNNs, Zhou and Xu (2015) and He et al. (2017) used stacked BiLSTMs and achieved strong performance without syntactic inputs. Tan et al. (2018) replaced stacked BiLSTMs with self-attention architectures. Strubell et"
D18-1191,D15-1112,0,0.160051,"Missing"
D18-1191,J02-3001,0,0.363318,"model. On Label Embeddings We analyze the label embeddings in the labeling function (Eq. 8). Figure 3 shows the distribution of the learned label embeddings. The adjunct labels are close to each other, which are likely to be less discriminative. Also, the core label A2 is close to the adjunct label DIR, which are often confused by the model. To enhance the discriminative power, it is promising to apply techniques that keep label representations far away from each other (Wen et al., 2016; Luo et al., 2017). 6 Related Work 6.1 Semantic Role Labeling Tasks Automatic SRL has been widely studied (Gildea and Jurafsky, 2002). There have been two main styles of SRL. Figure 4 illustrates an example of span-based and dependency-based SRL. In dependency-based SRL (at the upper part of Figure 4), the correct A2 argument for the predicate “hit” is the word “with”. On one hand, in span-based SRL (at the lower part of Figure 4), the correct A2 argument is the span “with the bat”. For span-based SRL, the CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) provided the task settings and datasets. In the task settings, various SRL models, from traditional pipeline models to recent neur"
D18-1191,P18-1249,0,0.0437631,"d to the performance improvement (Collobert et al., 2011; Zhou and Xu, 2015; He et al., 2017). Recently, Peters et al. (2018) integrated contextualized word representation, ELMo, into the model of He et al. (2017) and improved the performance by 3.2 F1 score. Strubell and McCallum (2018) also integrated ELMo into the model of Strubell et al. (2018) and reported the performance improvement. 6.3 Span-based SRL Models 6.4 Span-based Models in Other NLP Tasks In syntactic parsing, Wang and Chang (2016) proposed an LSTM-based sentence segment embedding method named LSTM-Minus. Stern et al. (2017); Kitaev and Klein (2018) incorporated the LSTM Minus into their parsing model and achieved the best results in constituency parsing. In coreference resolution, Lee et al. (2017, 2018) presented an end-to-end coreference resolution model, which considers all spans in a document as potential mentions and learn distributions over possible antecedents for each. Our model can be regarded as an extension of their model. 7 Conclusion and Future Work Another line of approaches to SRL is labeled span modeling (Xue and Palmer, 2004; Koomen et al., 2005; Toutanova et al., 2005). Typical models Typically, in this approach, model"
D18-1191,W05-0625,0,0.0637979,"entence segment embedding method named LSTM-Minus. Stern et al. (2017); Kitaev and Klein (2018) incorporated the LSTM Minus into their parsing model and achieved the best results in constituency parsing. In coreference resolution, Lee et al. (2017, 2018) presented an end-to-end coreference resolution model, which considers all spans in a document as potential mentions and learn distributions over possible antecedents for each. Our model can be regarded as an extension of their model. 7 Conclusion and Future Work Another line of approaches to SRL is labeled span modeling (Xue and Palmer, 2004; Koomen et al., 2005; Toutanova et al., 2005). Typical models Typically, in this approach, models firstly identify candidate argument spans (argument identification) and then classify each span into one of the semantic role labels (argument classification). For inference, several effective methods have been proposed, such as structural constraint inference by using integer linear programming (Punyakanok et al., 2008) or dynamic programming (T¨ackstr¨om et al., 2015; FitzGerald et al., 2015). Recent span-based model A very recent work, He et al. (2018a), proposed a span-based SRL model similar to our model. They a"
D18-1191,D17-1018,0,0.134271,"ectly predicting the spans. Another approach is based on labeled span prediction (T¨ackstr¨om et al., 2015; FitzGerald et al., 2015). This approach scores each span with its label. One advantage of this approach is to allow us to design and use span-level features, that are difficult to use in BIO tagging approaches. However, the performance has lagged behind that of the state-of-the-art BIO-based neural models. To fill this gap, this paper presents a simple and accurate span-based model. Inspired by recent span-based models in syntactic parsing and coreference resolution (Stern et al., 2017; Lee et al., 2017), our model directly scores all possible labeled spans based on span representations induced from neural networks. At decoding time, we greedily select higher scoring labeled spans. The model parameters are learned by optimizing loglikelihood of correct labeled spans. We evaluate the performance of our span-based model on the CoNLL-2005 and 2012 datasets (Carreras and M`arquez, 2005; Pradhan et al., 2012). Experimental results show that the spanbased model outperforms the BiLSTM-CRF model. In addition, by using contextualized word representations, ELMo (Peters et al., 2018), our ensemble model"
D18-1191,N18-2108,0,0.0400001,"Missing"
D18-1191,K17-1041,0,0.172056,"span-based SRL, the CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) provided the task settings and datasets. In the task settings, various SRL models, from traditional pipeline models to recent neural ones, have been proposed and competed with each other (Pradhan et al., 2005; He et al., 2017; Tan et al., 2018). For dependency-based SRL, the CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) provided the task settings and datasets. As in span-based SRL, recent neural models achieved high-performance in dependency-based SRL (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017; He et al., 2018b; Cai et al., 2018). This paper focuses on span-based SRL. 6.2 BIO-based SRL Models • FrameNet-style SRL (Baker et al., 1998) • PropBank-style SRL (Palmer et al., 2005) In this paper, we have tackled PropBank-style SRL.11 In PropBank-style SRL, there have been two main task settings. • Span-based SRL: CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) • Dependency-based SRL: CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) 11 A1 A2 A1 1.0 A1 Detailed descriptions on FrameNet-sty"
D18-1191,W04-2416,0,0.0441822,"le SRL (Palmer et al., 2005) In this paper, we have tackled PropBank-style SRL.11 In PropBank-style SRL, there have been two main task settings. • Span-based SRL: CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) • Dependency-based SRL: CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) 11 A1 A2 A1 1.0 A1 Detailed descriptions on FrameNet-style and PropBankstyle SRL can be found in Baker et al. (1998); Das et al. (2014); Kingsbury and Palmer (2002); Palmer et al. (2005). Span-based SRL can be solved as BIO sequential tagging (Hacioglu et al., 2004; Pradhan et al., 2005; M`arquez et al., 2005). Neural models State-of-the-art SRL models use neural networks based on the BIO tagging approach. The pioneering neural SRL model was proposed by Collobert et al. (2011). They use convolutional neural networks (CNNs) and CRFs. Instead of CNNs, Zhou and Xu (2015) and He et al. (2017) used stacked BiLSTMs and achieved strong performance without syntactic inputs. Tan et al. (2018) replaced stacked BiLSTMs with self-attention architectures. Strubell et al. (2018) improved the self-attention SRL model by incorporating syntactic information. 1637 Word r"
D18-1191,D17-1159,0,0.072181,"004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) provided the task settings and datasets. In the task settings, various SRL models, from traditional pipeline models to recent neural ones, have been proposed and competed with each other (Pradhan et al., 2005; He et al., 2017; Tan et al., 2018). For dependency-based SRL, the CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) provided the task settings and datasets. As in span-based SRL, recent neural models achieved high-performance in dependency-based SRL (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017; He et al., 2018b; Cai et al., 2018). This paper focuses on span-based SRL. 6.2 BIO-based SRL Models • FrameNet-style SRL (Baker et al., 1998) • PropBank-style SRL (Palmer et al., 2005) In this paper, we have tackled PropBank-style SRL.11 In PropBank-style SRL, there have been two main task settings. • Span-based SRL: CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) • Dependency-based SRL: CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) 11 A1 A2 A1 1.0 A1 Detailed descriptions on FrameNet-style and PropBankstyle SRL can b"
D18-1191,W05-0628,0,0.118562,"Missing"
D18-1191,J05-1004,0,0.643444,"odels to recent neural ones, have been proposed and competed with each other (Pradhan et al., 2005; He et al., 2017; Tan et al., 2018). For dependency-based SRL, the CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) provided the task settings and datasets. As in span-based SRL, recent neural models achieved high-performance in dependency-based SRL (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017; He et al., 2018b; Cai et al., 2018). This paper focuses on span-based SRL. 6.2 BIO-based SRL Models • FrameNet-style SRL (Baker et al., 1998) • PropBank-style SRL (Palmer et al., 2005) In this paper, we have tackled PropBank-style SRL.11 In PropBank-style SRL, there have been two main task settings. • Span-based SRL: CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) • Dependency-based SRL: CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) 11 A1 A2 A1 1.0 A1 Detailed descriptions on FrameNet-style and PropBankstyle SRL can be found in Baker et al. (1998); Das et al. (2014); Kingsbury and Palmer (2002); Palmer et al. (2005). Span-based SRL can be solved as BIO sequential tagging (Hacioglu et al., 2004; Pradh"
D18-1191,D14-1162,0,0.0829841,"d in our BiLSTM-span model. 4.3 Model Setup As the base function fbase , we use 4 BiLSTM layers with 300 dimensional hidden units. To optimize the model parameters, we use Adam (Kingma and Ba, 2014). Other hyperparameters are described in Appendix C in detail. As the objective function, we use the crossentropy Lθ in Eq. 3 with L2 weight decay, Lθ = ∑ (X,Y )∈D ℓθ (X, Y ) + λ ||θ||2 , 2 (11) where the hyperparameter λ is the coefficient governing the L2 weight decay. 6 http://ronan.collobert.com/senna/ http://allennlp.org/elmo 8 In our preliminary experiments, we also used the GloVe embeddings (Pennington et al., 2014), but the performance was worse than SENNA. 1634 7 4.4 Results We report averaged scores across five different runs of the model training. Tables 1 and 2 show the experimental results on the CoNLL-2005 and 2012 datasets. Overall, our span-based ensemble model using ELMo achieved the best F1 scores, 87.4 F1 and 87.0 F1 on the CoNLL-2005 and CoNLL-2012 datasets, respectively. In comparison with the CRF-based single model, our span-based single model consistently yielded better F1 scores regardless of the word embeddings, S ENNA and ELM O. Although the performance difference was small between the"
D18-1191,D18-1548,0,0.236057,"Missing"
D18-1191,N18-1202,0,0.368834,"n (Stern et al., 2017; Lee et al., 2017), our model directly scores all possible labeled spans based on span representations induced from neural networks. At decoding time, we greedily select higher scoring labeled spans. The model parameters are learned by optimizing loglikelihood of correct labeled spans. We evaluate the performance of our span-based model on the CoNLL-2005 and 2012 datasets (Carreras and M`arquez, 2005; Pradhan et al., 2012). Experimental results show that the spanbased model outperforms the BiLSTM-CRF model. In addition, by using contextualized word representations, ELMo (Peters et al., 2018), our ensemble model achieves the state-of-the-art results, 87.4 F1 and 87.0 F1 on the CoNLL-2005 and 2012 datasets, respectively. Empirical analysis on these results shows that the label prediction ability of our span-based model is better than that of the CRF-based model. Another finding is that ELMo improves the model performance for span boundary identification. In summary, our main contributions include: • A simple span-based model that achieves the state-of-the-art results. • Quantitative and qualitative analysis on strengths and weaknesses of the span-based model. • Empirical analysis o"
D18-1191,W08-2121,0,0.214884,"Missing"
D18-1191,W05-0634,0,0.0598564,"ates an example of span-based and dependency-based SRL. In dependency-based SRL (at the upper part of Figure 4), the correct A2 argument for the predicate “hit” is the word “with”. On one hand, in span-based SRL (at the lower part of Figure 4), the correct A2 argument is the span “with the bat”. For span-based SRL, the CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) provided the task settings and datasets. In the task settings, various SRL models, from traditional pipeline models to recent neural ones, have been proposed and competed with each other (Pradhan et al., 2005; He et al., 2017; Tan et al., 2018). For dependency-based SRL, the CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) provided the task settings and datasets. As in span-based SRL, recent neural models achieved high-performance in dependency-based SRL (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017; He et al., 2018b; Cai et al., 2018). This paper focuses on span-based SRL. 6.2 BIO-based SRL Models • FrameNet-style SRL (Baker et al., 1998) • PropBank-style SRL (Palmer et al., 2005) In this paper, we have tackled PropBank-style SRL.11 In PropBank-style SRL, t"
D18-1191,W12-4501,0,0.061457,"ral models. To fill this gap, this paper presents a simple and accurate span-based model. Inspired by recent span-based models in syntactic parsing and coreference resolution (Stern et al., 2017; Lee et al., 2017), our model directly scores all possible labeled spans based on span representations induced from neural networks. At decoding time, we greedily select higher scoring labeled spans. The model parameters are learned by optimizing loglikelihood of correct labeled spans. We evaluate the performance of our span-based model on the CoNLL-2005 and 2012 datasets (Carreras and M`arquez, 2005; Pradhan et al., 2012). Experimental results show that the spanbased model outperforms the BiLSTM-CRF model. In addition, by using contextualized word representations, ELMo (Peters et al., 2018), our ensemble model achieves the state-of-the-art results, 87.4 F1 and 87.0 F1 on the CoNLL-2005 and 2012 datasets, respectively. Empirical analysis on these results shows that the label prediction ability of our span-based model is better than that of the CRF-based model. Another finding is that ELMo improves the model performance for span boundary identification. In summary, our main contributions include: • A simple span"
D18-1191,J08-2005,0,0.288347,"over possible antecedents for each. Our model can be regarded as an extension of their model. 7 Conclusion and Future Work Another line of approaches to SRL is labeled span modeling (Xue and Palmer, 2004; Koomen et al., 2005; Toutanova et al., 2005). Typical models Typically, in this approach, models firstly identify candidate argument spans (argument identification) and then classify each span into one of the semantic role labels (argument classification). For inference, several effective methods have been proposed, such as structural constraint inference by using integer linear programming (Punyakanok et al., 2008) or dynamic programming (T¨ackstr¨om et al., 2015; FitzGerald et al., 2015). Recent span-based model A very recent work, He et al. (2018a), proposed a span-based SRL model similar to our model. They also used BiLSTMs to induce span representations in an endto-end fashion. A main difference is that while they model P(r|i, j), we model P(i, j|r). In other words, while their model seeks to select an appropriate label for each span (label selection), our model seeks to select appropriate spans for each label (span selection). This point distinguishes between their model and ours. FrameNet span-bas"
D18-1191,Q15-1003,0,0.0745551,"Missing"
D18-1191,I17-1027,1,0.839811,"concatenated and used as the feature for a span s = (i, j). The resulting vector hs is a 2dhidden dimensional vector. The middle part of Figure 1 shows an example of this process. For the span (3, 5), the span feature function fspan receives the 3rd and 5th features (h3 and h5 ). Then, these two vectors are added, and the 5th vector is subtracted from the 3rd vector. The resulting vectors are concatenated and given to the labeling function flabel . Our design of the span features is inspired by the span (or segment) features used in syntactic parsing (Wang and Chang, 2016; Stern et al., 2017; Teranishi et al., 2017). While these neural span features cannot be used in BIO-based SRL models, they can easily be incorporated into spanbased models. flabel (hs , r) = W[r] · hs , row vector. As the result of the inner product of W[r] and hs , we obtain the score for a span (i, j) with a label r. The upper part of Figure 1 shows an example of this process. The span representation hs for the span s = (3, 5) is created from addition and subtraction of h3 and h5 . Then, we calculate the inner product of hs and W[r]. The score for the label A0 is 2.1, and the score for the label A1 is 3.7. In the same manner, by calc"
D18-1191,P05-1073,0,0.0993099,"ding method named LSTM-Minus. Stern et al. (2017); Kitaev and Klein (2018) incorporated the LSTM Minus into their parsing model and achieved the best results in constituency parsing. In coreference resolution, Lee et al. (2017, 2018) presented an end-to-end coreference resolution model, which considers all spans in a document as potential mentions and learn distributions over possible antecedents for each. Our model can be regarded as an extension of their model. 7 Conclusion and Future Work Another line of approaches to SRL is labeled span modeling (Xue and Palmer, 2004; Koomen et al., 2005; Toutanova et al., 2005). Typical models Typically, in this approach, models firstly identify candidate argument spans (argument identification) and then classify each span into one of the semantic role labels (argument classification). For inference, several effective methods have been proposed, such as structural constraint inference by using integer linear programming (Punyakanok et al., 2008) or dynamic programming (T¨ackstr¨om et al., 2015; FitzGerald et al., 2015). Recent span-based model A very recent work, He et al. (2018a), proposed a span-based SRL model similar to our model. They also used BiLSTMs to induc"
D18-1191,P16-1218,0,0.2806,"es of the i-th and j-th hidden states are concatenated and used as the feature for a span s = (i, j). The resulting vector hs is a 2dhidden dimensional vector. The middle part of Figure 1 shows an example of this process. For the span (3, 5), the span feature function fspan receives the 3rd and 5th features (h3 and h5 ). Then, these two vectors are added, and the 5th vector is subtracted from the 3rd vector. The resulting vectors are concatenated and given to the labeling function flabel . Our design of the span features is inspired by the span (or segment) features used in syntactic parsing (Wang and Chang, 2016; Stern et al., 2017; Teranishi et al., 2017). While these neural span features cannot be used in BIO-based SRL models, they can easily be incorporated into spanbased models. flabel (hs , r) = W[r] · hs , row vector. As the result of the inner product of W[r] and hs , we obtain the score for a span (i, j) with a label r. The upper part of Figure 1 shows an example of this process. The span representation hs for the span s = (3, 5) is created from addition and subtraction of h3 and h5 . Then, we calculate the inner product of hs and W[r]. The score for the label A0 is 2.1, and the score for the"
D18-1191,W04-3212,0,0.141723,"oposed an LSTM-based sentence segment embedding method named LSTM-Minus. Stern et al. (2017); Kitaev and Klein (2018) incorporated the LSTM Minus into their parsing model and achieved the best results in constituency parsing. In coreference resolution, Lee et al. (2017, 2018) presented an end-to-end coreference resolution model, which considers all spans in a document as potential mentions and learn distributions over possible antecedents for each. Our model can be regarded as an extension of their model. 7 Conclusion and Future Work Another line of approaches to SRL is labeled span modeling (Xue and Palmer, 2004; Koomen et al., 2005; Toutanova et al., 2005). Typical models Typically, in this approach, models firstly identify candidate argument spans (argument identification) and then classify each span into one of the semantic role labels (argument classification). For inference, several effective methods have been proposed, such as structural constraint inference by using integer linear programming (Punyakanok et al., 2008) or dynamic programming (T¨ackstr¨om et al., 2015; FitzGerald et al., 2015). Recent span-based model A very recent work, He et al. (2018a), proposed a span-based SRL model similar"
D18-1191,P15-1109,0,0.531738,"ults, 87.4 F1 and 87.0 F1 on the CoNLL-2005 and 2012 datasets, respectively. 1 Introduction Semantic Role Labeling (SRL) is a shallow semantic parsing task whose goal is to recognize the predicate-argument structure of each predicate. Given a sentence and a target predicate, SRL systems have to predict semantic arguments of the predicate. Each argument is a span, a unit that consists of one or more words. A key to the argument span prediction is how to represent and model spans. One popular approach to it is based on BIO tagging schemes. State-of-the-art neural SRL models adopt this approach (Zhou and Xu, 2015; He et al., 2017; Tan et al., 2018). Using features induced by neural networks, they predict a BIO tag for each word. Words at the beginning and inside of argument spans have the “B” and “I” tags, and words outside argument spans have the tag “O.” While yielding high accuracies, this approach reconstructs argument spans from the predicted BIO tags instead of directly predicting the spans. Another approach is based on labeled span prediction (T¨ackstr¨om et al., 2015; FitzGerald et al., 2015). This approach scores each span with its label. One advantage of this approach is to allow us to desig"
D18-1191,P17-1076,0,0.294602,"tags instead of directly predicting the spans. Another approach is based on labeled span prediction (T¨ackstr¨om et al., 2015; FitzGerald et al., 2015). This approach scores each span with its label. One advantage of this approach is to allow us to design and use span-level features, that are difficult to use in BIO tagging approaches. However, the performance has lagged behind that of the state-of-the-art BIO-based neural models. To fill this gap, this paper presents a simple and accurate span-based model. Inspired by recent span-based models in syntactic parsing and coreference resolution (Stern et al., 2017; Lee et al., 2017), our model directly scores all possible labeled spans based on span representations induced from neural networks. At decoding time, we greedily select higher scoring labeled spans. The model parameters are learned by optimizing loglikelihood of correct labeled spans. We evaluate the performance of our span-based model on the CoNLL-2005 and 2012 datasets (Carreras and M`arquez, 2005; Pradhan et al., 2012). Experimental results show that the spanbased model outperforms the BiLSTM-CRF model. In addition, by using contextualized word representations, ELMo (Peters et al., 2018),"
D18-1191,W18-2904,0,0.0163241,"Tan et al. (2018) replaced stacked BiLSTMs with self-attention architectures. Strubell et al. (2018) improved the self-attention SRL model by incorporating syntactic information. 1637 Word representations Typical word representations, such as SENNA (Collobert et al., 2011) and GloVe (Pennington et al., 2014), have been used and contributed to the performance improvement (Collobert et al., 2011; Zhou and Xu, 2015; He et al., 2017). Recently, Peters et al. (2018) integrated contextualized word representation, ELMo, into the model of He et al. (2017) and improved the performance by 3.2 F1 score. Strubell and McCallum (2018) also integrated ELMo into the model of Strubell et al. (2018) and reported the performance improvement. 6.3 Span-based SRL Models 6.4 Span-based Models in Other NLP Tasks In syntactic parsing, Wang and Chang (2016) proposed an LSTM-based sentence segment embedding method named LSTM-Minus. Stern et al. (2017); Kitaev and Klein (2018) incorporated the LSTM Minus into their parsing model and achieved the best results in constituency parsing. In coreference resolution, Lee et al. (2017, 2018) presented an end-to-end coreference resolution model, which considers all spans in a document as potentia"
D18-1191,P98-1013,0,\N,Missing
D18-1191,C98-1013,0,\N,Missing
D18-1191,P17-1044,0,\N,Missing
I17-1027,J94-4001,0,0.843456,"Missing"
I17-1027,P06-1055,0,0.0336394,"the similarity score. Hanamoto (2012) used dual decomposition to combine an HPSG parser with the model of Hara et al. (2009). The method of use of the replaceability property has recently been adopted by Ficler and Goldberg (2016). They incorporated the replaceability property between conjuncts into the feature representations, as well as the similarity property. They made use of these properties to assign scores to candidate pairs of conjuncts. Their method consists of three components: a binary classifier to detect the presence of coordination, the parser extended from the Berkeley Parser (Petrov et al., 2006) to generate candidate pairs, and a discriminative neural network to identify conjuncts. As similarity features, they compute the Euclidean distance between the two representations of con6 Conclusions We propose a neural network model to disambiguate coordinate structure boundaries. Our method relies on two properties: (i) conjuncts tend to have a similar structure in syntax or semantics and (ii) conjuncts can be replaced with each other, maintaining sentence consistency. On the basis of these observations, we compute two feature vectors from a sequence of vectors produced by bidirectional RNN"
I17-1027,P16-1079,0,0.387324,"NE) = w · hk + b s = [Score(N ONE); Score(1, k + 1); . . . ; Replaceability feature vector We define a feature vector based on the conjunct replaceability as follows. frepl (h1:N , i, j, k) =  |hi−1 hi − hi−1 hk+1 |;  |hj hj+1 − hk−1 hj+1 | To cope with the absence of coordination against a coordinator, we also calculate the score for a candidate of N ONE. The score N ONE is simply computed as the product of a weight vector and the sentence-level representation of the coordinator from the RNN layer. 4 Experiments We evaluate our proposed model using the coordination annotated Penn Treebank (Ficler, 2016) and the Genia treebank beta (Kim et al., 2003). We present the number of occurrences of coordinator words and the number of sentences with coordination in Table 13 . 3 We consider “and,” “or,” “but,” “nor,” and “and/or” in the PTB and “and,” “or,” and “but” in the Genia as coordinator words following Ficler and Goldberg (2016) and Hara et al. (2009). (7) 268 Penn Treebank Training Development Testing Genia # Coordinators 27903 (24450) 22670 (17893) 953 (848) 1282 (1099) 3598 (3598) # Sentences 21314 (19095) 17282 (13932) 742 (673) 985 (873) 2508 (2508) Table 1: The number of coordinators in t"
I17-1027,D07-1064,0,0.886581,"s, or external resources such as thesauri. To overcome these problems, Ficler and Goldberg (2016) proposed a neural network model with the replaceability feature as well as the similarity feature. Their model produces candidate pairs of conjuncts using probabilities assigned by the Berkeley Parser. All candidate pairs are scored on the basis of the similarity, replaceability and parser-derived features, and then the best scored pair is picked. Their approach outperforms existing constituent parsers for the Penn Treebank and similarity-based coordination disambiguation methods such as those by Shimbo and Hara (2007) and Hara et al. (2009) for the Genia treebank. Although Ficler and Goldberg’s (2016) method improves performance significantly, it heavily depends on the syntactic parser. They use the outputs from the parser not only for candidates generation and the feature for scoring, but also for computation of the similarities. The problems of propagated errors from the parser and dependencies on external resources still remain in their work. In this work, we propose a neural network model for coordination disambiguation that does not require any external syntactic parser. Our model exploits both the si"
I17-1027,D16-1003,0,0.494479,"tudies of coordination disambiguation rely only on the similarities between conjuncts, despite the fact that similarities are not always helpful (Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto, 2012). For example, the sentence “[at least two commercial versions have been put on the U.S. market], and [an estimated 500 have been sold].” does not have sim264 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 264–272, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (whose conjuncts make sense individually) and outperforms the methods by Ficler and Goldberg (2016) and Hara et al. (2009) in Section 4. The contributions of our work include the following: (i) Our model can capture dissimilar conjuncts as well as similar ones using the similarity and replaceability features. (ii) Our model performs better than others without any thesauri, feature engineering, or syntactic parsers to extract conjunct features. 2 Figure 1: The coordination identification task and our subtask. Coordinate Structure Analysis 2.1 Task Description Coordination is a frequently occurring syntactic structure along with several phrases, known as conjuncts. The task of coordination di"
I17-1027,E12-1044,0,0.267796,"g coordination still remains one of the difficult problems that state-of-the-art parsers cannot cope with. Given a coordinator word, how can we find conjuncts? Coordinate structures are characterized by two properties: (1) similar structures often appear in conjuncts, and (2) one conjunct can be replaced with another conjunct without losing sentence consistency in syntax or semantics. However, many previous studies of coordination disambiguation rely only on the similarities between conjuncts, despite the fact that similarities are not always helpful (Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto, 2012). For example, the sentence “[at least two commercial versions have been put on the U.S. market], and [an estimated 500 have been sold].” does not have sim264 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 264–272, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (whose conjuncts make sense individually) and outperforms the methods by Ficler and Goldberg (2016) and Hara et al. (2009) in Section 4. The contributions of our work include the following: (i) Our model can capture dissimilar conjuncts as well as similar ones using the simil"
I17-1027,P09-1109,1,0.154005,"ch as thesauri. To overcome these problems, Ficler and Goldberg (2016) proposed a neural network model with the replaceability feature as well as the similarity feature. Their model produces candidate pairs of conjuncts using probabilities assigned by the Berkeley Parser. All candidate pairs are scored on the basis of the similarity, replaceability and parser-derived features, and then the best scored pair is picked. Their approach outperforms existing constituent parsers for the Penn Treebank and similarity-based coordination disambiguation methods such as those by Shimbo and Hara (2007) and Hara et al. (2009) for the Genia treebank. Although Ficler and Goldberg’s (2016) method improves performance significantly, it heavily depends on the syntactic parser. They use the outputs from the parser not only for candidates generation and the feature for scoring, but also for computation of the similarities. The problems of propagated errors from the parser and dependencies on external resources still remain in their work. In this work, we propose a neural network model for coordination disambiguation that does not require any external syntactic parser. Our model exploits both the similarity and replaceabi"
I17-1027,P15-1150,0,0.0289335,"jpost = g(hk+1:j ) (k + 1 ≤ j ≤ N ) 267 (4) Then vipre and vjpost are fed into the following two feature extraction functions. Similarity feature vector In order to capture the similarity between the preconjunct and the post-conjunct, the feature vector is computed as follows:   fsim (vipre , vjpost ) = |vipre − vjpost |; vipre vjpost (5) where |vipre − vjpost |is the absolute value of element-wise subtraction, and vipre vjpost is element-wise multiplication. These subtraction and multiplication operations are intended to model the semantic distance and relatedness (Ji and Eisenstein, 2013; Tai et al., 2015; Hashimoto et al., 2016). yˆ = arg max pˆθ (y|x)  y (9) (6) 3.5 Learning The loss function is the negative log-likelihood of the true pair of conjuncts y (k) : J(θ) = − D X log pˆθ (y (d) |x(d) ) + d=1 λ kθk2 2 (10) where D is the number of occurrences of coordinator words in a training dataset, θ is a set of model parameters, and the hyperparameter λ adjusts the regularization strength. The model parameters are optimized by minimizing the loss using the stochastic gradient descent (SGD). This layer computes the scores of pairs of conjuncts based on the similarity feature vectors and the rep"
I17-1027,N03-1033,0,0.0866573,"he inner, outer, and exact metrics, we simply divide the preconjuncts into subconjuncts using the character “,” as the divider. Evaluation Using the Penn Treebank Experimental Setup We use the coordination annotated Penn Treebank and divide it into wsj 2-21 as the training set, wsj 22 as the development set, and wsj 23 as the testing set. We use pretrained 200-dimensional word embeddings from the New York Times section in English Gigaword (fifth edition) (Parker et al., 2011) using Word2Vec4 with its default parameter. For the POS tags, we use 10-way jackknifing using the Stanford POS Tagger (Toutanova et al., 2003) and initialize the 50-dimensional embeddings with the uniform distribution within [−1, 1]. We use three-layer bidirectional LSTMs as an RNN layer. The dimensionality of the LSTM hidden vectors in each direction is selected from {400, 600}. Our MLP consists of one hidden layer with ReLU activation, and an output layer. The number of the hidden layer units is selected from {1200, 2400}. The model parameters are optimized by the minibatched SGD with a batch size of 20. The learning rate is automatically tuned by Adam (Kingma and Ba, 2014). When training, we apply dropout (Srivastava et al., 2014"
I17-1027,D13-1090,0,0.0275831,"i:k−1 ) (1 ≤ i ≤ k − 1) vjpost = g(hk+1:j ) (k + 1 ≤ j ≤ N ) 267 (4) Then vipre and vjpost are fed into the following two feature extraction functions. Similarity feature vector In order to capture the similarity between the preconjunct and the post-conjunct, the feature vector is computed as follows:   fsim (vipre , vjpost ) = |vipre − vjpost |; vipre vjpost (5) where |vipre − vjpost |is the absolute value of element-wise subtraction, and vipre vjpost is element-wise multiplication. These subtraction and multiplication operations are intended to model the semantic distance and relatedness (Ji and Eisenstein, 2013; Tai et al., 2015; Hashimoto et al., 2016). yˆ = arg max pˆθ (y|x)  y (9) (6) 3.5 Learning The loss function is the negative log-likelihood of the true pair of conjuncts y (k) : J(θ) = − D X log pˆθ (y (d) |x(d) ) + d=1 λ kθk2 2 (10) where D is the number of occurrences of coordinator words in a training dataset, θ is a set of model parameters, and the hyperparameter λ adjusts the regularization strength. The model parameters are optimized by minimizing the loss using the stochastic gradient descent (SGD). This layer computes the scores of pairs of conjuncts based on the similarity feature v"
I17-1027,C08-1054,0,0.295405,"Missing"
I17-1027,W15-2208,1,0.882138,"Missing"
I17-2017,P16-1101,0,0.325294,"nd syntactic chunking are segment-level sequence modeling tasks, which require to recognize a segment from a sequence of words. A segment means a sequence of words that may compose an expression as shown in Figure 1. Current high performance NER systems use the word-level linear chain Conditional Random Fields (CRF) (Lafferty et al., 2001) with neural networks. Especially, it has been shown that the combination of LSTMs (Hochreiter and Schmidhuber, 1997; Gers et al., 2000), convolutional neural networks (CNNs) (LeCun et al., 1989), and word-level CRF achieves the state-of-the-art performance (Ma and Hovy, 2016). Figure 1 shows an overview of the word-level CRF for NER. However, the word-level neural CRF has two main limitations: (1) it captures only first-order word label dependencies thus it cannot capture 97 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 97–102, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 2 Word-level Neural CRF score for jumping from tag yi−1 to yi , and Y indicates all possible paths. At test time, the predicted sequence is obtained by finding the highest score in a all possible paths using Viterbi algorithm as fol"
I17-2017,D13-1032,0,0.0788849,"Missing"
I17-2017,Q16-1026,0,0.072638,"Missing"
I17-2017,D14-1162,0,0.077618,"v 99.71 99.96 99.98 Test Prec. Recall F1 BLSTM-CNN 90.85 91.92 91.38 BLSTM-CNN-CRF 94.67 94.43 94.55 Our method 94.55 95.12 94.84 Table 3: Result of CoNLL 2000 Chunking. Test 99.27 99.71 99.83 Table 1: Threshold T and Oracle score on NER. Test Prec. Recall F1 BLSTM-CNN 89.04 90.40 89.72 BLSTM-CNN-CRF3 90.82 91.11 90.96 Our method 91.07 91.50 91.28 + Binary Dict 91.05 91.69 91.37 + WikiEmb Dict 91.29 91.58 91.44 + Binary + WikiEmb 91.47 91.62 91.55 Ma and Hovy (2016) 91.35 91.06 91.21 Table 2: Result of CoNLL 2003 English NER. level CNN, and 100 dimentional pre-trained word embedding of GloVe (Pennington et al., 2014). At input layer and output layer, we apply dropout (Srivastava et al., 2014) with rate at 0.5. In our model, we set 400 filters with window size 3 in CNN for segment vector. To optimize our model, we use AdaDelta (Zeiler, 2012) with batch size 10 and gradient clipping 5. We use early stopping (Caruana et al., 2001) based on performance on development sets. improves the F1 score from 91.28 to 91.44. Eventually, we achieve the F1 score 91.55 with two dictionary features. The results of CoNLL 2000 Chunking is shown in Table 3. Similar to NER task, by adding a CRF layer to BLSTM-CNN, it improves"
I17-2017,P16-2038,0,0.023615,"is the probability of a possible NE type. Finally, we apply a linear chain CRF to find the highest score path in the segment lattice as we describe in Section 2. 4.1 Datasets We evaluate our method on two segment-level sequence tagging tasks: NER and text chunking3 . For NER, we use CoNLL 2003 English NER shared task (Tjong Kim Sang and De Meulder, 2003). Following previous work (Ma and Hovy, 2016), we use BIOES tagging scheme in the wordlevel tagging model. For text chunking, we use the CoNLL 2000 English text chunking shared task (Tjong Kim Sang and Buchholz, 2000). Following previous work (Søgaard and Goldberg, 2016), the section 19 of WSJ corpus is used as the development set. We use BIOES tagging scheme in the word-level tagging model and measure performance using F1 score in all experiments. 3.3 Dictionary Features for NER In this subsection, we describe the use of two additional dictionary features for NER. Since an entry of named entity dictionary and the segment in our model are in one-to-one correspondence, it is easy to directly incorporate the dictionary features into our model. We use following two dictionary features on NER task. 4.2 Model Settings To generate a segment lattice, we train word-l"
I17-2017,W00-0726,0,0.623325,"Missing"
I17-2017,N16-1030,0,0.244544,"Missing"
I17-2017,W03-0419,0,0.160611,"Missing"
I17-2017,K16-1025,1,0.901799,"Missing"
I17-2017,P16-1134,0,0.0802545,"Missing"
K16-1025,E06-1002,0,0.0353246,"Missing"
K16-1025,Q15-1011,0,0.595915,"). We adopted WLM as baseline. Table 1 shows the results. The score for WLM was obtained from Huang et al. (Huang et al., 2015). Our method clearly outperformed WLM. The results show that our method accurately captures pairwise entity relatedness. 4.3 4.3.1 TAC 2010 The TAC 2010 dataset is another popular NED dataset constructed for the Text Analysis Conference (TAC)5 (Ji et al., 2010). The dataset is based on news articles from various agencies and Web log data, and consists of a training and a test set containing 1,043 and 1,013 documents, respectively. Following past work (He et al., 2013; Chisholm and Hachey, 2015), we used mentions only with a valid entry in the KB, and reported the micro-accuracy score of the top-ranked candidate entities. We trained our model using the training set and assessed its performance using the test set. Consequently, we evaluated our model on 1,020 mentions contained in the test set. For candidate generation, we used a dictionary that was directly built from the Wikipedia dump mentioned previously. Similar to past work, we retrieved possible mention surfaces of an entity from (1) the title of the entity, (2) the title of another entity redirecting to the entity, and (3) the"
K16-1025,D07-1074,0,0.139516,"involved entities with no inbound KB anchor. These errors might be addressed using KB data other than KB anchors, such as the description of the entities and the KB categories in order to avoid dependence on the KB anchors. This remains part of our future work. 5 Related Work Early NED methods addressed the problem as a well-studied word sense disambiguation problem (Mihalcea and Csomai, 2007). These methods primarily focused on modeling the similarity of textual (local) context. Most recent stateof-the-art methods focus on modeling coherence among disambiguated entities in the same document (Cucerzan, 2007; Milne and Witten, 2008b; Hoffart et al., 2011; Ratinov et al., 2011). These approaches have also been called collective or global approaches in the literature. Learning the representations of entities for NED has been addressed in past literature. Guo and Barbosa (Guo and Barbosa, 2014) used random walks on KB graphs to construct vector representations of entities and documents to address NED. Blanco et al. (Blanco et al., 2015) proposed a method to map entities into the word embedding (i.e., Word2vec (Mikolov et al., 2013b)) space using entity descriptions in the KB and applied it for NED."
K16-1025,Q15-1023,0,0.378274,"Missing"
K16-1025,P13-2006,0,0.823135,"ning et al., 2008). We adopted WLM as baseline. Table 1 shows the results. The score for WLM was obtained from Huang et al. (Huang et al., 2015). Our method clearly outperformed WLM. The results show that our method accurately captures pairwise entity relatedness. 4.3 4.3.1 TAC 2010 The TAC 2010 dataset is another popular NED dataset constructed for the Text Analysis Conference (TAC)5 (Ji et al., 2010). The dataset is based on news articles from various agencies and Web log data, and consists of a training and a test set containing 1,043 and 1,013 documents, respectively. Following past work (He et al., 2013; Chisholm and Hachey, 2015), we used mentions only with a valid entry in the KB, and reported the micro-accuracy score of the top-ranked candidate entities. We trained our model using the training set and assessed its performance using the test set. Consequently, we evaluated our model on 1,020 mentions contained in the test set. For candidate generation, we used a dictionary that was directly built from the Wikipedia dump mentioned previously. Similar to past work, we retrieved possible mention surfaces of an entity from (1) the title of the entity, (2) the title of another entity redirectin"
K16-1025,D11-1072,0,0.73953,"Missing"
K16-1025,D14-1162,0,0.128559,"Missing"
K16-1025,P11-1138,0,0.911582,"ment d as context words.1 Moreover, we ignore a context word if the surface of mention m contains it. We then measure the similarity between candidate entity and the derived textual context by using cosine similarity between v~cw and the vector of entity v~e . 3.1.2 Modeling Coherence It has been revealed that effectively modeling coherence in the assignment of entities to mentions is important for NED. However, this is a chickenand-egg problem because the assignment of entities to mentions, which is required to measure coherence, is not possible prior to performing NED. Similar to past work (Ratinov et al., 2011), we address this problem by employing a simple twostep approach: we first train the machine learning model using the coherence score among unambiguous mentions2 , in addition to other features, and then retrain the model using the coherence score among the predicted entity assignments instead. To estimate coherence, we first calculate the vector representation of the context entities and measure the similarity between the vector of the context entities and that of the target entity e. Note that context entities are unambiguous entities in the first step, and predicted entities are used instea"
K16-1025,D14-1167,0,0.169436,"Missing"
K16-1025,D15-1083,0,0.0513715,"Missing"
K16-1025,P15-1125,0,\N,Missing
K16-1025,N15-1026,0,\N,Missing
K17-1042,E17-2026,0,0.115622,"ward directional LSTM that reads the sequence from beginning to end with the output vector of the backward directional LSTM that reads the sequence in the reverse direction. cas:n cas:na OUT(cas) OUT(cas) 3.3 LSTM rt Our baseline model does not share any information between morphosyntactic prediction tasks, as it is trained separately. However, it is beneficial to utilize information from other morphosyntactic categories when predicting a label for one category. In order to do this, we adopt a multi-task learning approach (Collobert et al., 2011; Yang et al., 2016; Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017). Specifically, we use parameter sharing in the hidden layers of our bi-LSTM model so that we can generate a unified model that can carry information beneficial to each task. LSTM LSTM LSTM wt rt+1 ct Hb (“love”) wt+1 ct+1 fy (“in”) ct Concat LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM Character Lookup Table <w&gt; H b Joint Prediction Model gen:m pos:noun cas:n </w&gt; ••• Hb (“love”) ••• OUT(pos) ••• OUT(cas) ••• OUT(gen) Figure 1: Top: Our baseline model for the category “cas”. We have one model for each category, resulting in 14 models in total. Bottom: How to crea"
K17-1042,N04-4038,0,0.376178,"POS tagging, Mueller et al. (2013) presented an approximated higher-order CRF for morphosyntactic tagging across six languages, assuming gold clitic segmentation. Pasha et al. (2014) used an analyze-anddisambiguate approach, in which they ranked the possible analyses provided by a morphological analyzer for each space-delimited word. The stateof-the-art tagger (Shahrour et al., 2015) extended their model by adjusting the outputs of Pasha et al.’s tagger by utilizing case-state classifiers that incorporate additional syntactic information provided by a dependency parser and hand-written rules. Diab et al. (2004) proposed a segmentationbased approach, in which they tag each cliticsegmented token using SVMs. Mohamed and K¨ubler (2010) proposed a word-based approach which takes space-delimited words as inputs and uses memory-based learning. Their experiment showed that the word-based approach performed better than the segmentation-based approach, avoiding segmentation error propagation. Zhang et al. (2015) proposed joint modeling of segmentation, POS tagging, and dependency parsing using a randomized greedy algorithm. The aforementioned studies were focused on tagging 428 Compared to their approaches, o"
K17-1042,E17-1005,0,0.089899,"sequence from beginning to end with the output vector of the backward directional LSTM that reads the sequence in the reverse direction. cas:n cas:na OUT(cas) OUT(cas) 3.3 LSTM rt Our baseline model does not share any information between morphosyntactic prediction tasks, as it is trained separately. However, it is beneficial to utilize information from other morphosyntactic categories when predicting a label for one category. In order to do this, we adopt a multi-task learning approach (Collobert et al., 2011; Yang et al., 2016; Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017). Specifically, we use parameter sharing in the hidden layers of our bi-LSTM model so that we can generate a unified model that can carry information beneficial to each task. LSTM LSTM LSTM wt rt+1 ct Hb (“love”) wt+1 ct+1 fy (“in”) ct Concat LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM Character Lookup Table <w&gt; H b Joint Prediction Model gen:m pos:noun cas:n </w&gt; ••• Hb (“love”) ••• OUT(pos) ••• OUT(cas) ••• OUT(gen) Figure 1: Top: Our baseline model for the category “cas”. We have one model for each category, resulting in 14 models in total. Bottom: How to create character-level embeddings. <w&gt;"
K17-1042,P05-1071,0,0.425079,"logy 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan {inoue.go.ib4, shindo, matsu}@is.naist.jp Abstract Part-of-speech (POS) tagging is a fundamental task in natural language processing. The granularity of the POS tag set that reflects languagespecific information varies from language to language. In morphologically simple languages such as English, the size of the tag set is typically less than a hundred. On the other hand, in morphologically rich languages such as Arabic, the number of theoretically possible tags can be up to 333,000, of which only 2,200 tags might appear in an actual corpus (Habash and Rambow, 2005). One reason for this is that in the tagging scheme for such languages, a complete POS tag is formed by combining tags from multiple tag sets defined for each morphosyntactic category. For example, a complete POS tag for the word Hb (“love”)2 can be defined as the combination of a noun from the coarse POS category, a nominative (n) from the case category, “not applicable” (na) from the mood category, and so on. The enormous number of resulting tags causes fine-grained POS tagging for Arabic to be more challenging. In order to perform this task, it is beneficial to utilize information from othe"
K17-1042,N10-1105,0,0.0500973,"Missing"
K17-1042,L16-1681,0,0.0365912,"Missing"
K17-1042,D13-1032,0,0.102368,"Missing"
K17-1042,C16-2047,0,0.171353,"Missing"
K17-1042,P16-2038,0,0.246909,"the output vector of the forward directional LSTM that reads the sequence from beginning to end with the output vector of the backward directional LSTM that reads the sequence in the reverse direction. cas:n cas:na OUT(cas) OUT(cas) 3.3 LSTM rt Our baseline model does not share any information between morphosyntactic prediction tasks, as it is trained separately. However, it is beneficial to utilize information from other morphosyntactic categories when predicting a label for one category. In order to do this, we adopt a multi-task learning approach (Collobert et al., 2011; Yang et al., 2016; Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017). Specifically, we use parameter sharing in the hidden layers of our bi-LSTM model so that we can generate a unified model that can carry information beneficial to each task. LSTM LSTM LSTM wt rt+1 ct Hb (“love”) wt+1 ct+1 fy (“in”) ct Concat LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM Character Lookup Table <w&gt; H b Joint Prediction Model gen:m pos:noun cas:n </w&gt; ••• Hb (“love”) ••• OUT(pos) ••• OUT(cas) ••• OUT(gen) Figure 1: Top: Our baseline model for the category “cas”. We have one model for each category, resulting in 14 models in"
K17-1042,N15-1005,0,0.0263409,"eir model by adjusting the outputs of Pasha et al.’s tagger by utilizing case-state classifiers that incorporate additional syntactic information provided by a dependency parser and hand-written rules. Diab et al. (2004) proposed a segmentationbased approach, in which they tag each cliticsegmented token using SVMs. Mohamed and K¨ubler (2010) proposed a word-based approach which takes space-delimited words as inputs and uses memory-based learning. Their experiment showed that the word-based approach performed better than the segmentation-based approach, avoiding segmentation error propagation. Zhang et al. (2015) proposed joint modeling of segmentation, POS tagging, and dependency parsing using a randomized greedy algorithm. The aforementioned studies were focused on tagging 428 Compared to their approaches, our model is simple but powerful: It does not assume gold clitic segmentation, since segmentation is also modeled as part of the morphosyntactic categories, nor does it require the additional pipeline process of syntactic parsing. Nonetheless, it is more accurate than the current state-of-the-art. Another related line of work tackles sequential labeling problems using multi-task learning with deep"
K17-1042,pasha-etal-2014-madamira,0,0.366943,"Missing"
K17-1042,P16-2067,0,0.0337,"ory “cas”. We have one model for each category, resulting in 14 models in total. Bottom: How to create character-level embeddings. <w&gt; and </w&gt; indicates the beginning and the end of a word. cas:na ••• LSTM rt Independent Prediction Model gen:na ••• OUT(pos) ••• OUT(cas) ••• OUT(gen) LSTM LSTM LSTM !t ct Hb (“love”) 3.2 pos:prep rt+1 !t+1 ct+1 fy (“in”) Figure 2: Multi-task bi-directional LSTM model for fine-grained Arabic POS tagging. For our baseline method, we use a model that independently predicts each morphosyntactic category using bi-LSTMs. Our baseline is similar to the basic model in Plank et al. (2016). The top part of Figure 1 illustrates an overview of our baseline model. Given a sequence of n words x1:n , we encode each word xt into a vector represenFigure 2 shows an overview of our joint model. The output vectors of the bi-LSTMs are fed into multiple output layers, each performing a corresponding morphosyntactic prediction task. Our model trains to minimize the cross-entropy loss 423 L(ˆ y f ine , y f ine ) = ••• 1 X L(ˆ ym , ym ) |M |m∈M (cas) ; ... ; dt (gen) ; ... ; dt cas:na ••• OUT(pos) ••• OUT(cas) ••• OUT(gen) LSTM LSTM z ,t rt gen:na ••• LSTM ct rt+1 dt Hb (“love”) One of our co"
K17-1042,D15-1152,0,0.222371,"ing for Arabic to be more challenging. In order to perform this task, it is beneficial to utilize information from other morphosyntactic categories when predicting a label for one category. For example, if a word is a noun, it should take one of three tags from the case category: nominative (n), accusative (a), or genitive (g), while it should take “not applicable” (na) from the mood category since mood is not defined for nominals. However, most of the previous approaches in Arabic did not utilize this information, applying one model for each task (Habash and Rambow, 2005; Pasha et al., 2014; Shahrour et al., 2015). To make use of this information, we propose an approach that jointly models multiple morphosyntactic prediction tasks using a multi-task learning scheme. Specifically, we adopt parameter sharing in our bi-directional LSTM model in the hope that the shared parameters will store information beneficial to multiple tasks. To further boost the performance, we propose a method of incorporating tag dictionary information into our neural models by combining word representations with representations of the sets of possible tags. Our experiments showed that the joint model 1 Our code is available at h"
K19-1052,D17-1277,0,0.0435953,"failed in difficult cases such as predicting Tokugawa shogunate instead of Tokugawa Ieyasu. 6 Additionally, our work is also related to studies on entity linking. Entity linking models can be roughly classified into two groups: local models, which resolve entity names independently using the contextual relevance of the entity given a document, and global models, in which all the entity names in a document are resolved simultaneously to select a topically coherent set of results (Ratinov et al., 2011). Recent state-of-the-art models typically combine both of these models (Yamada et al., 2016; Ganea and Hofmann, 2017; Cao et al., 2018; Kolitsas et al., 2018). However, several studies also showed that the local model alone can achieve results competitive to those of the global and combined models (Eshel et al., 2017; Ganea and Hofmann, 2017; Yamada et al., 2017; Cao et al., 2018; Kolitsas et al., 2018). In this study, we adopt a simple but effective local model, which uses cosine similarity between the embedding of the target entity and the word-based representation of the document to capture the relevance of an entity given a document. Related Work KB entities have been conventionally used to model the se"
K19-1052,C18-1057,0,0.0240531,"s such as predicting Tokugawa shogunate instead of Tokugawa Ieyasu. 6 Additionally, our work is also related to studies on entity linking. Entity linking models can be roughly classified into two groups: local models, which resolve entity names independently using the contextual relevance of the entity given a document, and global models, in which all the entity names in a document are resolved simultaneously to select a topically coherent set of results (Ratinov et al., 2011). Recent state-of-the-art models typically combine both of these models (Yamada et al., 2016; Ganea and Hofmann, 2017; Cao et al., 2018; Kolitsas et al., 2018). However, several studies also showed that the local model alone can achieve results competitive to those of the global and combined models (Eshel et al., 2017; Ganea and Hofmann, 2017; Yamada et al., 2017; Cao et al., 2018; Kolitsas et al., 2018). In this study, we adopt a simple but effective local model, which uses cosine similarity between the embedding of the target entity and the word-based representation of the document to capture the relevance of an entity given a document. Related Work KB entities have been conventionally used to model the semantics in texts."
K19-1052,N13-1122,0,0.0206461,"referent entities (e.g., Washington, D.C. and George Washington). In particular, we first take all words and phrases in a document, treat them as entity names if they exist in the dictionary, and detect all possible referent entities for each detected entity name. Following past work (Hasibi et al., 2016; Xiong et al., 2016), the boundary overlaps of the names are resolved by detecting only those that are the earliest and the longest. We use Wikipedia as the target KB, and the entity dictionary is built by using the names and their referent entities of all internal anchor links in Wikipedia (Guo et al., 2013). We also collect two statistics from Wikipedia, namely link probability and commonness (Mihalcea and Csomai, 2007; Milne and Witten, 2008). The former is the probability of a name being used as an anchor link in Wikipedia, whereas the latter is the probability of a name referring to an entity in Wikipedia. We generate a list of entities by concatenating all possible referent entities contained in the dictionary for each detected entity name, and feed it zword = N 1 X vwi , N (1) i=1 where vw ∈ Rd is the embedding of word w. We then derive the entity-based representation of D as a weighted ave"
K19-1052,D13-1184,0,0.0343827,"Missing"
K19-1052,D11-1072,0,0.103175,"Missing"
K19-1052,E14-4040,0,0.357976,"they are uniquely identified in a KB. One key issue here is to determine the way in which to associate a document with its relevant entities. An existing straightforward approach (Peng et al., 2016; Xiong et al., 2016) involves creating a set of relevant entities using an entity linking system to detect and disambiguate the names of entities in a document. However, this approach is problematic because (1) entity linking systems produce disambiguation errors (Cornolti et al., 2013), and (2) entities appearing in a document are not necessarily relevant to the given document (Gamon et al., 2013; Dunietz and Gillick, 2014). This study proposes the Neural Attentive Bagof-Entities (NABoE) model, which is a neural network model that addresses the text classification problem by modeling the semantics in the target documents using entities in the KB. For each entity name in a document (e.g., “Apple”), our model first detects entities that may be referred to by this name (e.g., Apple Inc., Apple (food)), and then represents the document using the weighted average of the embeddings of these entities. The weights are computed using a novel neural attention mechanism that enables the model to focus on a small subset of"
K19-1052,D14-1070,0,0.0920848,"Missing"
K19-1052,K17-1008,1,0.841394,"fied into two groups: local models, which resolve entity names independently using the contextual relevance of the entity given a document, and global models, in which all the entity names in a document are resolved simultaneously to select a topically coherent set of results (Ratinov et al., 2011). Recent state-of-the-art models typically combine both of these models (Yamada et al., 2016; Ganea and Hofmann, 2017; Cao et al., 2018; Kolitsas et al., 2018). However, several studies also showed that the local model alone can achieve results competitive to those of the global and combined models (Eshel et al., 2017; Ganea and Hofmann, 2017; Yamada et al., 2017; Cao et al., 2018; Kolitsas et al., 2018). In this study, we adopt a simple but effective local model, which uses cosine similarity between the embedding of the target entity and the word-based representation of the document to capture the relevance of an entity given a document. Related Work KB entities have been conventionally used to model the semantics in texts. A representative example is Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2006, 2007), which represents a document using a bag of entities, namely a sparse vector of wh"
K19-1052,P15-1162,0,0.0501179,"Missing"
K19-1052,E17-2068,0,0.0226973,"s a result, our model achieved state-of-the-art results on all datasets. The source code of the proposed model is available online at https://github.com/ wikipedia2vec/wikipedia2vec. 1 Introduction Text classification is an important task, and its applications span a wide range of activities such as topic classification, spam detection, and sentiment classification. Recent studies showed that models based on neural networks can outperform conventional models (e.g., na¨ıve Bayes) on text classification tasks (Kim, 2014; Iyyer et al., 2015; Tang et al., 2015; Dai and Le, 2015; Jin et al., 2016; Joulin et al., 2017; Shen et al., 2018). Typical neural network-based text classification models are based on words. They typically use words in the target documents as inputs, map words into continuous vectors (embeddings), and capture the semantics in documents by using compositional functions over word embeddings such as averaging or summation of word embeddings, convolutional neural networks (CNN), and recurrent neural networks (RNN). 563 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 563–573 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Ling"
K19-1052,D14-1181,0,0.0361213,"asets) and a popular factoid question answering dataset based on a trivia quiz game. As a result, our model achieved state-of-the-art results on all datasets. The source code of the proposed model is available online at https://github.com/ wikipedia2vec/wikipedia2vec. 1 Introduction Text classification is an important task, and its applications span a wide range of activities such as topic classification, spam detection, and sentiment classification. Recent studies showed that models based on neural networks can outperform conventional models (e.g., na¨ıve Bayes) on text classification tasks (Kim, 2014; Iyyer et al., 2015; Tang et al., 2015; Dai and Le, 2015; Jin et al., 2016; Joulin et al., 2017; Shen et al., 2018). Typical neural network-based text classification models are based on words. They typically use words in the target documents as inputs, map words into continuous vectors (embeddings), and capture the semantics in documents by using compositional functions over word embeddings such as averaging or summation of word embeddings, convolutional neural networks (CNN), and recurrent neural networks (RNN). 563 Proceedings of the 23rd Conference on Computational Natural Language Learnin"
K19-1052,K18-1050,0,0.0199606,"ng Tokugawa shogunate instead of Tokugawa Ieyasu. 6 Additionally, our work is also related to studies on entity linking. Entity linking models can be roughly classified into two groups: local models, which resolve entity names independently using the contextual relevance of the entity given a document, and global models, in which all the entity names in a document are resolved simultaneously to select a topically coherent set of results (Ratinov et al., 2011). Recent state-of-the-art models typically combine both of these models (Yamada et al., 2016; Ganea and Hofmann, 2017; Cao et al., 2018; Kolitsas et al., 2018). However, several studies also showed that the local model alone can achieve results competitive to those of the global and combined models (Eshel et al., 2017; Ganea and Hofmann, 2017; Yamada et al., 2017; Cao et al., 2018; Kolitsas et al., 2018). In this study, we adopt a simple but effective local model, which uses cosine similarity between the embedding of the target entity and the word-based representation of the document to capture the relevance of an entity given a document. Related Work KB entities have been conventionally used to model the semantics in texts. A representative example"
K19-1052,P17-1170,0,0.0230146,"ation (Gabrilovich and Markovitch, 2006; Gupta and Ratinov, 2008; Negi and Rosner, 2013) and information retrieval (Egozi et al., 2011; Xiong et al., 2016), Several neural network models that use KB entities to capture the semantics in texts have been proposed. These models typically depend on an additional preprocessing step that extracts the relevant entities from the target texts. For example, Wang et al. (2017) used the Probase conceptualization API for short text classification by retrieving the Probase entities that were relevant to the target text and used them in a model based on CNN. Pilehvar et al. (2017) also extracted entities using a graph-based linking algorithm and used these entities in a neural network model. A similar approach was adopted in Yamada et al. (2018b,c); they extracted entities from the target text using an entity linking system and simply used the detected entities in a neural network model. However, un7 Conclusions This study proposed NABoE, which is a neural network model that performs text classification using entities in Wikipedia. We combined simple dictionary-based entity detection with a neural attention mechanism to enable the model to focus on a small number of un"
K19-1052,P11-1138,0,0.10161,"ance and the unambiguity of entity e in document D using the attention function. Thus, the problem is related to the tasks of entity salience detection (Gamon et al., 2013; Dunietz and Gillick, 2014), which aims to detect entities relevant (or salient) to the document, and entity linking, which aims to resolve the ambiguity of entities. The key assumption relating to these two tasks in the literature is that if an entity is semantically related to the given document, it is relevant to the document (Dunietz and Gillick, 2014), and it is likely to appear in the document (Milne and Witten, 2008; Ratinov et al., 2011). With this in mind and following past work (Yamada et al., 2016), we use the cosine similarity between ve and zword as a feature. Further, as in past entity linking studies, we also use the commonness of the name referring to the entity. Moreover, we derive a representation based both on entities and words by simply adding zentity and zword 1 : zf ull = zentity + zword . 3.2 We initialized the embeddings of words (vw ) and entities (ve ) using pretrained embeddings trained on KB. To learn embeddings from the KB, we used the method adopted in the open source Wikipedia2Vec tool (Yamada et al.,"
K19-1052,P18-1041,0,0.278424,"achieved state-of-the-art results on all datasets. The source code of the proposed model is available online at https://github.com/ wikipedia2vec/wikipedia2vec. 1 Introduction Text classification is an important task, and its applications span a wide range of activities such as topic classification, spam detection, and sentiment classification. Recent studies showed that models based on neural networks can outperform conventional models (e.g., na¨ıve Bayes) on text classification tasks (Kim, 2014; Iyyer et al., 2015; Tang et al., 2015; Dai and Le, 2015; Jin et al., 2016; Joulin et al., 2017; Shen et al., 2018). Typical neural network-based text classification models are based on words. They typically use words in the target documents as inputs, map words into continuous vectors (embeddings), and capture the semantics in documents by using compositional functions over word embeddings such as averaging or summation of word embeddings, convolutional neural networks (CNN), and recurrent neural networks (RNN). 563 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 563–573 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics mon et al.,"
K19-1052,D15-1167,0,0.0451954,"estion answering dataset based on a trivia quiz game. As a result, our model achieved state-of-the-art results on all datasets. The source code of the proposed model is available online at https://github.com/ wikipedia2vec/wikipedia2vec. 1 Introduction Text classification is an important task, and its applications span a wide range of activities such as topic classification, spam detection, and sentiment classification. Recent studies showed that models based on neural networks can outperform conventional models (e.g., na¨ıve Bayes) on text classification tasks (Kim, 2014; Iyyer et al., 2015; Tang et al., 2015; Dai and Le, 2015; Jin et al., 2016; Joulin et al., 2017; Shen et al., 2018). Typical neural network-based text classification models are based on words. They typically use words in the target documents as inputs, map words into continuous vectors (embeddings), and capture the semantics in documents by using compositional functions over word embeddings such as averaging or summation of word embeddings, convolutional neural networks (CNN), and recurrent neural networks (RNN). 563 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 563–573 c Hong Kong, China, No"
K19-1052,S13-2089,0,0.0205716,"ed representation of the document to capture the relevance of an entity given a document. Related Work KB entities have been conventionally used to model the semantics in texts. A representative example is Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2006, 2007), which represents a document using a bag of entities, namely a sparse vector of which each dimension corresponds to the relevance score of the text to each entity. This simple method is shown to be effective for various NLP tasks including text classification (Gabrilovich and Markovitch, 2006; Gupta and Ratinov, 2008; Negi and Rosner, 2013) and information retrieval (Egozi et al., 2011; Xiong et al., 2016), Several neural network models that use KB entities to capture the semantics in texts have been proposed. These models typically depend on an additional preprocessing step that extracts the relevant entities from the target texts. For example, Wang et al. (2017) used the Probase conceptualization API for short text classification by retrieving the Probase entities that were relevant to the target text and used them in a model based on CNN. Pilehvar et al. (2017) also extracted entities using a graph-based linking algorithm and"
K19-1052,K16-1025,1,0.919656,"tion function. Thus, the problem is related to the tasks of entity salience detection (Gamon et al., 2013; Dunietz and Gillick, 2014), which aims to detect entities relevant (or salient) to the document, and entity linking, which aims to resolve the ambiguity of entities. The key assumption relating to these two tasks in the literature is that if an entity is semantically related to the given document, it is relevant to the document (Dunietz and Gillick, 2014), and it is likely to appear in the document (Milne and Witten, 2008; Ratinov et al., 2011). With this in mind and following past work (Yamada et al., 2016), we use the cosine similarity between ve and zword as a feature. Further, as in past entity linking studies, we also use the commonness of the name referring to the entity. Moreover, we derive a representation based both on entities and words by simply adding zentity and zword 1 : zf ull = zentity + zword . 3.2 We initialized the embeddings of words (vw ) and entities (ve ) using pretrained embeddings trained on KB. To learn embeddings from the KB, we used the method adopted in the open source Wikipedia2Vec tool (Yamada et al., 2016, 2018a). In particular, we generated an entity-annotated cor"
K19-1052,P16-1037,0,0.0261058,"dies attempted to use entities in a knowledge base (KB) (e.g., Wikipedia) to capture the semantics in documents. These models typically represent a document by using a set of entities (or bag of entities) relevant to the document (Gabrilovich and Markovitch, 2006, 2007; Xiong et al., 2016). The main benefit of using entities instead of words is that unlike words, entities provide unambiguous semantic signals because they are uniquely identified in a KB. One key issue here is to determine the way in which to associate a document with its relevant entities. An existing straightforward approach (Peng et al., 2016; Xiong et al., 2016) involves creating a set of relevant entities using an entity linking system to detect and disambiguate the names of entities in a document. However, this approach is problematic because (1) entity linking systems produce disambiguation errors (Cornolti et al., 2013), and (2) entities appearing in a document are not necessarily relevant to the given document (Gamon et al., 2013; Dunietz and Gillick, 2014). This study proposes the Neural Attentive Bagof-Entities (NABoE) model, which is a neural network model that addresses the text classification problem by modeling the sem"
K19-1052,C18-1016,1,0.650173,"dataset and ignored the other words and entities. The size of the embeddings of words and entities was set to d = 300. We used early stopping based on the accuracy of the development set of each dataset to avoid overfitting of the model. 4.2 • SWEM-concat (Shen et al., 2018): This model is based on a neural network model with simple pooling operations (i.e., average and max pooling) over pretrained word embeddings.6 Despite its simplicity, it outperformed many neural network-based models such as the word-based CNN model (Kim, 2014) and RNN model with LSTM units (Shen et al., 2018). • TextEnt (Yamada et al., 2018b): This model learns entity-aware document embeddings from Wikipedia, and uses a neural network model with the learned embeddings as pretrained parameters to address text classification. As described in Section 2.1, we also tested the variants of our NABoE-entity and NABoEfull models for which Wikifier and TAGME were used as the entity detection methods. 4.3 Results Table 1 shows the results of our models and those of our baselines. Here, w/o att. and w/o emb. signify the model without the neural attention mechanism (all attention weights ae are set to K1 , where K is the number of entities i"
K19-1052,Q17-1028,1,\N,Missing
L16-1263,P14-1070,0,0.394582,"single token improves accuracy for various NLP tasks, such as dependency parsing (Nivre and Nilsson, 2004), and constituency parsing (Arun and Keller, 2005). For syntactic parsing that takes in MWEs, it is preferable that the information of an MWE (e.g., part of speech and span of tokens) be integrated into a corpus, such as a phrase or dependency treebank, because an MWE should be a syntactic unit. Actually, an MWE is grouped under “subtree” in French Treebank (Abeill´e et al., 2003), which is often used in research focusing on both MWE recognition and syntactic parsing (Green et al., 2011; Candito and Constant, 2014). However, MWEs are not annotated in Penn Treebank, the standard corpus of English syntactic parsing. In dependency structure that takes MWEs into consideration (i.e., MWE-aware dependency), each MWE becomes a single node. Therefore, to convert word-based dependency to MWE-aware dependency directly, one could combine nodes in an MWE into a single node. Nevertheless, this method often leads to the following problem: A node derived from an MWE could have multiple heads and the whole dependency structure including MWE might be cyclic (Figure 1). This is mainly because Penn Treebank style annotati"
L16-1263,W08-1301,0,0.20658,"Missing"
L16-1263,N09-1037,0,0.0258983,"structure tree. For example, “even though” in Figure 3a is annotated as an MWE in Shigeto et al. (2013). We convert it as in Figure 3b. If we can convert the span of an MWE into a single subtree without influencing the structures of other subtrees, we call this instance “Simple”. Otherwise the instance is “Complex”. When grouping MWE, we focus on the LCA-tree, which is the subtree rooted in the Least Common Ancestor (LCA) of the components of the MWE. In Figure 3a, the tree rooted in the LCA of “even” and “though” (LCA here being SBAR) is the LCA-tree. The method we described above relates to Finkel and Manning (2009). For joint parsing and named entity recognition, they classified named entities which do not correspond to a phrase in the constituency tree to the following two categories. A named entity belonging to the first category is contiguous multiple children of some nonterminal. This category corresponds to the above “Simple” case. On the other hand, A span of each named entity belonging to the second category crosses brackets in the parse tree. It corresponds to the above “Complex” case. 3.1. Simple Case In the “Simple” case, we insert a new internal node under the LCA (Figure 3a → Figure 3b). Thi"
L16-1263,D11-1067,0,0.200124,"bining an MWE into a single token improves accuracy for various NLP tasks, such as dependency parsing (Nivre and Nilsson, 2004), and constituency parsing (Arun and Keller, 2005). For syntactic parsing that takes in MWEs, it is preferable that the information of an MWE (e.g., part of speech and span of tokens) be integrated into a corpus, such as a phrase or dependency treebank, because an MWE should be a syntactic unit. Actually, an MWE is grouped under “subtree” in French Treebank (Abeill´e et al., 2003), which is often used in research focusing on both MWE recognition and syntactic parsing (Green et al., 2011; Candito and Constant, 2014). However, MWEs are not annotated in Penn Treebank, the standard corpus of English syntactic parsing. In dependency structure that takes MWEs into consideration (i.e., MWE-aware dependency), each MWE becomes a single node. Therefore, to convert word-based dependency to MWE-aware dependency directly, one could combine nodes in an MWE into a single node. Nevertheless, this method often leads to the following problem: A node derived from an MWE could have multiple heads and the whole dependency structure including MWE might be cyclic (Figure 1). This is mainly because"
L16-1263,H05-1066,0,0.11424,"Missing"
L16-1263,P13-2017,0,0.158907,"Missing"
L16-1263,W13-1021,1,\N,Missing
L18-1175,chaimongkol-etal-2014-corpus,0,0.0152538,"eneral-purpose linguistic annotation tools such as BRAT (Stenetorp et al., 2012) and WebAnno (Yimam et al., 2013) only support text documents. Some commercial software packages provide annotation functions for PDF, however, they lack a function of relation annotation suitable for dependency relation and coreference chain. Since PDF has become widespread standard for many publications, a linguistic annotation tool for PDF is strongly desired for knowledge extraction from PDF documents. For example, previous work has developed an annotated corpus for coreference resolution on scientific papers (Panot et al., 2014; Schafer et al., 2012; Steven et al., 2008). In their work, PDF articles are converted to plain-text format using OCR software, then import them to a text annotation tool. As pointed out in the literature, OCR errors are present in the data and they need to clean up the text by viewing the associated PDF file. This motivates us to develop a new annotation tool that can directly annotate on PDF. There are two types of annotation processes for creating an annotated text from a PDF file as shown in Figure 1. One is to convert the PDF into plain text or HTML format, then annotate it using a text"
L18-1175,C12-2103,0,0.170153,"istic annotation tools such as BRAT (Stenetorp et al., 2012) and WebAnno (Yimam et al., 2013) only support text documents. Some commercial software packages provide annotation functions for PDF, however, they lack a function of relation annotation suitable for dependency relation and coreference chain. Since PDF has become widespread standard for many publications, a linguistic annotation tool for PDF is strongly desired for knowledge extraction from PDF documents. For example, previous work has developed an annotated corpus for coreference resolution on scientific papers (Panot et al., 2014; Schafer et al., 2012; Steven et al., 2008). In their work, PDF articles are converted to plain-text format using OCR software, then import them to a text annotation tool. As pointed out in the literature, OCR errors are present in the data and they need to clean up the text by viewing the associated PDF file. This motivates us to develop a new annotation tool that can directly annotate on PDF. There are two types of annotation processes for creating an annotated text from a PDF file as shown in Figure 1. One is to convert the PDF into plain text or HTML format, then annotate it using a text annotation tool, as in"
L18-1175,E12-2021,0,0.35351,"ation conflicts. PDFAnno is freely available under open-source license at https://github.com/paperai/pdfanno. Keywords: text annotation, annotation tool, pdf 1. Introduction Gold standard annotations for texts are a prerequisite for training and evaluation of statistical models in Natural Language Processing (NLP). Since human annotation is known as one of the most costly and time-consuming tasks in NLP, an easy-to-use and easy-to-manage annotation tool is highly required for cost effective development of gold standard data. Currently, general-purpose linguistic annotation tools such as BRAT (Stenetorp et al., 2012) and WebAnno (Yimam et al., 2013) only support text documents. Some commercial software packages provide annotation functions for PDF, however, they lack a function of relation annotation suitable for dependency relation and coreference chain. Since PDF has become widespread standard for many publications, a linguistic annotation tool for PDF is strongly desired for knowledge extraction from PDF documents. For example, previous work has developed an annotated corpus for coreference resolution on scientific papers (Panot et al., 2014; Schafer et al., 2012; Steven et al., 2008). In their work, P"
L18-1175,bird-etal-2008-acl,0,0.265542,"such as BRAT (Stenetorp et al., 2012) and WebAnno (Yimam et al., 2013) only support text documents. Some commercial software packages provide annotation functions for PDF, however, they lack a function of relation annotation suitable for dependency relation and coreference chain. Since PDF has become widespread standard for many publications, a linguistic annotation tool for PDF is strongly desired for knowledge extraction from PDF documents. For example, previous work has developed an annotated corpus for coreference resolution on scientific papers (Panot et al., 2014; Schafer et al., 2012; Steven et al., 2008). In their work, PDF articles are converted to plain-text format using OCR software, then import them to a text annotation tool. As pointed out in the literature, OCR errors are present in the data and they need to clean up the text by viewing the associated PDF file. This motivates us to develop a new annotation tool that can directly annotate on PDF. There are two types of annotation processes for creating an annotated text from a PDF file as shown in Figure 1. One is to convert the PDF into plain text or HTML format, then annotate it using a text annotation tool, as in the previous work. An"
L18-1175,P13-4001,0,0.108004,"Missing"
L18-1356,P09-1113,0,0.0692397,"Missing"
L18-1396,J09-2001,0,0.0583446,"Missing"
L18-1396,P14-1070,0,0.127933,"ompound nouns and compound function words. An accurate recognition of VMWEs is challenging because VMWEs could be discontinuous (e.g., take .. off). We show the main categories of VMWEs in Table 1. While dependency parsing and MWE recognition could be solved independently, dependency structures in that each MWE is a syntactic unit are preferable to word-based dependency structures for downstream NLP tasks, such as semantic parsing. Because MWE recognition could help syntactic parsing (Nivre and Nilsson, 2004; Eryi˘git et al., 2011), several works tackle MWE-aware dependency parsing in French (Candito and Constant, 2014; Nasr et al., 2015). They use French Treebank (Abeill´e et al., 2003) because of its explicit MWE annotations. Regarding English MWEs, Schneider et al. (2014) constructs an MWE-annotated corpus based on English Web Treebank (Bies et al., 2012). However, the number of VMWE occurrences (1,444) and types (1,155) in their corpus is relatively small-scale. In this work, we conduct full-scale VMWE annotations on the Wall Street Journal (WSJ) portion of English Ontonotes (Pradhan et al., 2007), which results in 7,833 VMWE occurrences and 1,608 types. Concretely, we construct a VMWE dictionary based"
L18-1396,W11-3806,0,0.0772059,"Missing"
L18-1396,L16-1263,1,0.854487,"s of “take the reins” and “take over”. Also, we resolve pseudo overlaps originating from false annotations. As a result, we reduce the number of overlaps to 11 instances, which correspond to essential overlaps, such as “look back” and “look .. on .. as” in the following sentence: “He may be able to look back on this election as the Finally, we check inclusions and overlaps between annotations by us and those by (Komai et al., 2015), which results in 159 inclusions and 40 overlaps. Regarding inclusions, 2497 5 6 http://dictionary.cambridge.org http://idioms.thefreedictionary.com literal usage. Kato et al. (2016) and Kato et al. (2017) integrates annotations of these functional MWEs and named entities (NEs) 8 into phrase structures by establishing MWEs as subtrees. They exploit this dataset for experiments on English MWE-aware dependency parsing. (a) A positive instance (non-literal usage) 4. (b) A negative instance (literal usage) Figure 3: Positive and negative instances of a VMWE (get up). In this work, we conduct large-scale annotations of English VMWEs in the Wall Street Journal portion of Ontonotes. Based on a VMWE dictionary extracted from English Wiktionary, we collect possible VMWE occurrence"
L18-1396,P17-2068,1,0.803739,"nd “take over”. Also, we resolve pseudo overlaps originating from false annotations. As a result, we reduce the number of overlaps to 11 instances, which correspond to essential overlaps, such as “look back” and “look .. on .. as” in the following sentence: “He may be able to look back on this election as the Finally, we check inclusions and overlaps between annotations by us and those by (Komai et al., 2015), which results in 159 inclusions and 40 overlaps. Regarding inclusions, 2497 5 6 http://dictionary.cambridge.org http://idioms.thefreedictionary.com literal usage. Kato et al. (2016) and Kato et al. (2017) integrates annotations of these functional MWEs and named entities (NEs) 8 into phrase structures by establishing MWEs as subtrees. They exploit this dataset for experiments on English MWE-aware dependency parsing. (a) A positive instance (non-literal usage) 4. (b) A negative instance (literal usage) Figure 3: Positive and negative instances of a VMWE (get up). In this work, we conduct large-scale annotations of English VMWEs in the Wall Street Journal portion of Ontonotes. Based on a VMWE dictionary extracted from English Wiktionary, we collect possible VMWE occurrences in Ontonotes, and fil"
L18-1396,Y15-2015,1,0.931392,", a function-head scheme is preferable to a content-head scheme. 1,235 270 80 ≥5 31 Total 7,833 23 1,608 Table 2: Corpus statistics. We show VMWE instances and types by the number of constituent word tokens. # of gaps VMWE instances 0 6,855 1 968 2 10 Table 3: VMWE instances by the number of gaps. VPCs, we regard a candidate as a positive VMWE occurrence iff the dependency label is “prt”. For prepositional verbs, if the dependency label is “prep”, and there is no gap between the verb and the particle, we regard this candidate as a positive VMWE occurrence. This is subject to rules proposed by Komai et al. (2015). Otherwise, we conduct crowdsourced annotations. 2.2. Large-scale Annotations of VMWEs by Crowdsourcing Figure 2: A screenshot of a web interface for VMWE annotations on CrowdFlower. oneself). We exclude candidates that do not include any verbs by using gold part-of-speech information. Also, we filter out candidates that have other verbs or punctuation marks within the gaps. Because most of the VMWEs are syntactically regular, we filter a VMWE whose components form a subtree in a Stanford basic dependency tree (Marneffe and Manning, 2008), which is converted from a phrase structure tree given"
L18-1396,P13-2017,0,0.0472047,"Missing"
L18-1396,P15-1108,0,0.0536278,"Missing"
L18-1396,W17-1704,0,0.070234,"s a dataset for French MWE-aware dependency parsing (Candito and Constant, 2014) because of its explicit MWE annotations. It consists of phrase structure trees, augmented with morphological information and functional annotations of verbal dependents. Second, Vincze (2012) provides an English-Hungarian parallel corpus annotated for LVCs, which belong to VMWEs. Their corpus contains 703 LVCs in Hungarian and 727 in English based on 14,261 sentence alignment units, taken from economiclegal texts and literature. Recently, PARSEME organized a shared task on automatic identification of verbal MWEs (Savary et al., 2017). They provide annotation guidelines and annotated corpora of 5.5 million tokens and 60,000 VMWE annotations for 18 languages. Note that their corpora do not support English in edition 1.0. Regarding English MWEs, Shigeto et al. (2013) first constructs an MWE dictionary by extracting functional MWEs 7 from the English-language Wiktionary, and classifies their occurrences in Ontonotes into either MWE or Conclusion 2. We get VMWE occurrences in Ontonotes for only 1,608 out of 8,369 types in our VMWE dictionary. Therefore, we plan to explore VMWE occurrences on a larger corpus, such as the Annota"
L18-1396,schneider-etal-2014-comprehensive,0,0.173587,"Missing"
L18-1396,vincze-2012-light,0,0.254981,"er of discontinuous instances. Our corpus annotations are represented as token indices of components of VMWEs. By using them, we can classify potential VMWEs in our corpus as positive and negative instances (Figure 3). 3. Related Work We introduce several MWE-annotated corpora. First, French Treebank (Abeill´e et al., 2003) is often used as a dataset for French MWE-aware dependency parsing (Candito and Constant, 2014) because of its explicit MWE annotations. It consists of phrase structure trees, augmented with morphological information and functional annotations of verbal dependents. Second, Vincze (2012) provides an English-Hungarian parallel corpus annotated for LVCs, which belong to VMWEs. Their corpus contains 703 LVCs in Hungarian and 727 in English based on 14,261 sentence alignment units, taken from economiclegal texts and literature. Recently, PARSEME organized a shared task on automatic identification of verbal MWEs (Savary et al., 2017). They provide annotation guidelines and annotated corpora of 5.5 million tokens and 60,000 VMWE annotations for 18 languages. Note that their corpora do not support English in edition 1.0. Regarding English MWEs, Shigeto et al. (2013) first constructs"
N19-1286,S10-1057,0,0.0237209,"ll relational information. Our proposed model achieves new state-of-the-art results on SemEval-2010 Task 8, compared with other complex models. 2 Related Work RC plays a significant role in many NLP applications. Recent work usually present the task from a supervised perspective. 2793 Proceedings of NAACL-HLT 2019, pages 2793–2798 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Traditional supervised approaches can be divided into feature-based methods and kernel methods. Feature-based methods focus on extracting and combining relevant features. Rink and Harabagiu (2010) leveraged useful features to achieve the best performance on SemEval-2010 Task 8. Meanwhile, kernel methods measure the structural similarity between two data samples, based on carefully designed kernels. Wang (2008) combined convolutional kernel and syntactic features to gain benefits for relation extraction. Nowadays, deep neural networks are widely utilized in RC. Zeng et al. (2014) exploited a CNN to extract lexical and sentence features. Qin et al. (2016) used ETF to specify target entities in input sentences and fed them to a CNN. Vu et al. (2016) combined CNN and RNN to improve perform"
N19-1286,N16-1065,0,0.0341265,"Missing"
N19-1286,P16-1123,0,0.0491196,"Missing"
N19-1286,D14-1181,0,0.0107207,"Missing"
N19-1286,I08-2119,0,0.0389526,"y present the task from a supervised perspective. 2793 Proceedings of NAACL-HLT 2019, pages 2793–2798 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Traditional supervised approaches can be divided into feature-based methods and kernel methods. Feature-based methods focus on extracting and combining relevant features. Rink and Harabagiu (2010) leveraged useful features to achieve the best performance on SemEval-2010 Task 8. Meanwhile, kernel methods measure the structural similarity between two data samples, based on carefully designed kernels. Wang (2008) combined convolutional kernel and syntactic features to gain benefits for relation extraction. Nowadays, deep neural networks are widely utilized in RC. Zeng et al. (2014) exploited a CNN to extract lexical and sentence features. Qin et al. (2016) used ETF to specify target entities in input sentences and fed them to a CNN. Vu et al. (2016) combined CNN and RNN to improve performance. Some recent work leveraged SDP for RC. Yang et al. (2016) proposed a position encoding CNN based on dependency parse trees, while Wen (2017) presented a model that learns representations from SDP, using both CNN"
N19-1286,N16-1175,0,0.0580871,"Missing"
N19-1286,D18-1250,0,0.084662,"man, 2015). Traditional approaches (Kambhatla, 2004; Zhang et al., 2006) usually rely heavily on hand-crafted features and lexical resources, or elaborately designed kernels, which are time-consuming and challenging to adapt to novel domains. Recently, neural network (NN) models have dominated the work on RC since they can effectively learn meaningful hidden features without human intervention. However, most previous NN models only exploit one of the following structures to represent relation instances: raw word sequences (Zhou et al., 2016; Wang et al., 2016) and dependency trees (Wen, 2017; Le et al., 2018). While raw sequences can provide all the information of relation instances, they also add noise to the models from redundant information. While dependency tree structures help the models focus on the concise information captured by the shortest dependency path (SDP) between two entities, they lose some supplementary context in the raw sequence. It is clear that the raw sequence and SDP highly complement each other. We, therefore, combine them to be more effective in determining the relation without losing any information. While CNNs are able to learn short patterns (local features) (LeCun et"
N19-1286,D15-1062,0,0.0832671,"Missing"
N19-1286,D16-1007,0,0.238505,"plement each other. We, therefore, combine them to be more effective in determining the relation without losing any information. While CNNs are able to learn short patterns (local features) (LeCun et al., 1995), RNNs have been effective in learning word sequence information (long-distance features) (Chung et al., 2014). In this paper, we present a new model combining both CNNs and RNNs, exploiting the information from both the raw sequence and the SDP. Our contributions are summarized as follows: (a) We combine Entity Tag Feature (ETF) (Qin et al., 2016) and Tree-based Position Feature (TPF) (Yang et al., 2016) to improve the semantic information between the marked entities in the raw input sentences. (b) We propose Segment-Level Attention-based Convolutional Neural Networks (SACNNs) which automatically pay special attention to the important text segments from the raw sentence for RC. (c) We build Dependency-based Recurrent Neural Networks (DepRNNs) on the SDP to gain longdistance features. Then, we combine the SACNN and the DepRNN to preserve the full relational information. Our proposed model achieves new state-of-the-art results on SemEval-2010 Task 8, compared with other complex models. 2 Relate"
N19-1286,C14-1220,0,0.0708909,"on for Computational Linguistics Traditional supervised approaches can be divided into feature-based methods and kernel methods. Feature-based methods focus on extracting and combining relevant features. Rink and Harabagiu (2010) leveraged useful features to achieve the best performance on SemEval-2010 Task 8. Meanwhile, kernel methods measure the structural similarity between two data samples, based on carefully designed kernels. Wang (2008) combined convolutional kernel and syntactic features to gain benefits for relation extraction. Nowadays, deep neural networks are widely utilized in RC. Zeng et al. (2014) exploited a CNN to extract lexical and sentence features. Qin et al. (2016) used ETF to specify target entities in input sentences and fed them to a CNN. Vu et al. (2016) combined CNN and RNN to improve performance. Some recent work leveraged SDP for RC. Yang et al. (2016) proposed a position encoding CNN based on dependency parse trees, while Wen (2017) presented a model that learns representations from SDP, using both CNN and RNN. 3 Our Method Given a sentence S with an annotated pair of entities (e1, e2), we aim to identify the semantic relation between them. Since the set of target relati"
N19-1286,P06-1104,0,0.0744021,"path of the related entities. Experiments on the SemEval-2010 Task 8 dataset show that our model is comparable to the stateof-the-art without using any external lexical features. 1 Introduction Relation classification (RC) is a fundamental task in Natural Language Processing (NLP) that aims to identify semantic relations between pairs of marked entities in given sentences (instances). It has attracted much research effort as it plays a vital role in many NLP applications such as Information Extraction and Question Answering (Nguyen and Grishman, 2015). Traditional approaches (Kambhatla, 2004; Zhang et al., 2006) usually rely heavily on hand-crafted features and lexical resources, or elaborately designed kernels, which are time-consuming and challenging to adapt to novel domains. Recently, neural network (NN) models have dominated the work on RC since they can effectively learn meaningful hidden features without human intervention. However, most previous NN models only exploit one of the following structures to represent relation instances: raw word sequences (Zhou et al., 2016; Wang et al., 2016) and dependency trees (Wen, 2017; Le et al., 2018). While raw sequences can provide all the information of"
N19-1286,P16-2034,0,0.144372,"such as Information Extraction and Question Answering (Nguyen and Grishman, 2015). Traditional approaches (Kambhatla, 2004; Zhang et al., 2006) usually rely heavily on hand-crafted features and lexical resources, or elaborately designed kernels, which are time-consuming and challenging to adapt to novel domains. Recently, neural network (NN) models have dominated the work on RC since they can effectively learn meaningful hidden features without human intervention. However, most previous NN models only exploit one of the following structures to represent relation instances: raw word sequences (Zhou et al., 2016; Wang et al., 2016) and dependency trees (Wen, 2017; Le et al., 2018). While raw sequences can provide all the information of relation instances, they also add noise to the models from redundant information. While dependency tree structures help the models focus on the concise information captured by the shortest dependency path (SDP) between two entities, they lose some supplementary context in the raw sequence. It is clear that the raw sequence and SDP highly complement each other. We, therefore, combine them to be more effective in determining the relation without losing any information. W"
N19-1343,P16-1079,0,0.547122,"000, which is in the same weight class but is much slower and has less memory, and the T-1600, which also uses a 286 microprocessor, but which weighs almost twice as much and is three times the size,” we cannot find correct conjuncts for each coordinator at a glance. The presence of coordination makes a sentence more ambiguous and longer, resulting in errors in syntactic parsing. To identify the conjuncts of a given coordinator, previous studies have explored two properties of coordinate structures: (1) similarity – conjuncts tend to be similar; (2) replaceability – conjuncts can be replaced. Ficler and Goldberg (2016b) combine the syntactic parser and neural networks to compute the similarity and replaceability features of conjuncts. Teranishi et al. (2017) also exploit the two properties without deploying any syntactic parser, and achieve state-of-the-art results. Although both approaches outperform the similarity-based approaches (Shimbo and Hara, 2007; Hara et al., 2009), they cannot handle more than two conjuncts in a coordination, and multiple coordinations in a sentence at one time. Hence, their systems may produce coordinations that conflict with each other. In contrast, Hara et al. (2009) define p"
N19-1343,D16-1003,0,0.62932,"000, which is in the same weight class but is much slower and has less memory, and the T-1600, which also uses a 286 microprocessor, but which weighs almost twice as much and is three times the size,” we cannot find correct conjuncts for each coordinator at a glance. The presence of coordination makes a sentence more ambiguous and longer, resulting in errors in syntactic parsing. To identify the conjuncts of a given coordinator, previous studies have explored two properties of coordinate structures: (1) similarity – conjuncts tend to be similar; (2) replaceability – conjuncts can be replaced. Ficler and Goldberg (2016b) combine the syntactic parser and neural networks to compute the similarity and replaceability features of conjuncts. Teranishi et al. (2017) also exploit the two properties without deploying any syntactic parser, and achieve state-of-the-art results. Although both approaches outperform the similarity-based approaches (Shimbo and Hara, 2007; Hara et al., 2009), they cannot handle more than two conjuncts in a coordination, and multiple coordinations in a sentence at one time. Hence, their systems may produce coordinations that conflict with each other. In contrast, Hara et al. (2009) define p"
N19-1343,E12-1044,0,0.0208099,"computes scores based on the syntactic and morphological features assigned to edges and nodes in a sequence alignment. While their method focused on non-nested coordinations, Hara et al. (2009) extended their work to accommodate nested coordinations using CFG rules. A consistent global structure of coordinations is produced using discriminative functions based on the similarity of conjuncts with dynamic programming. Our concept of the CKY parsing is borrowed from their work; however, a key difference of our approach lies in how it computes the score of conjuncts and trains the score function. Hanamoto et al. (2012) used dual decomposition to combine HPSG parsing with the discriminative model developed by Hara et al. (2009). 5.2 Non Similarity-based Approaches Kawahara and Kurohashi (2008) focused on resolving the ambiguities of coordinate structures without the use of any similarities. Their method relied on the dependency relations surrounding the conjuncts and the generative probabilities of phrases. Yoshimoto et al. (2015) extended the Eis3401 ner algorithm by adding new rules to accommodate coordinations during dependency parsing. 5.3 Coordination Boundary Identification using Neural Networks Ficler"
N19-1343,P09-1109,1,0.934359,"ic parsing. To identify the conjuncts of a given coordinator, previous studies have explored two properties of coordinate structures: (1) similarity – conjuncts tend to be similar; (2) replaceability – conjuncts can be replaced. Ficler and Goldberg (2016b) combine the syntactic parser and neural networks to compute the similarity and replaceability features of conjuncts. Teranishi et al. (2017) also exploit the two properties without deploying any syntactic parser, and achieve state-of-the-art results. Although both approaches outperform the similarity-based approaches (Shimbo and Hara, 2007; Hara et al., 2009), they cannot handle more than two conjuncts in a coordination, and multiple coordinations in a sentence at one time. Hence, their systems may produce coordinations that conflict with each other. In contrast, Hara et al. (2009) define production rules for coordination in order to output consistent coordinate structures. Here, we propose a new framework for coordination boundary identification. We generalize a scoring function that takes a pair of spans with a coordinator and returns a higher score when the two spans appear to be coordinated. Using this function in the CKY parsing with producti"
N19-1343,P82-1020,0,0.8199,"Missing"
N19-1343,P07-1086,0,0.0441636,"hological information, and not contextual word senses, are clues for shorter and similar coordinations such as NP coordinations. For the feature extraction function of the outer-boundary scoring model, the concat function that performs the same function as the inner-boundary scoring model does not achieve competitive advantage. The feature function described as Eq. 12 is designed to capRelated Work Similarity-based Approaches For the coordination identification task in Japanese, Kurohashi and Nagao (1994) used a chart to find the highest similarity pair of conjuncts using dynamic programming. Hogan (2007) developed a generative parsing model for coordinated noun phrases, incorporating symmetry in conjunct structures and head words. Shimbo and Hara (2007) proposed a discriminative model that computes scores based on the syntactic and morphological features assigned to edges and nodes in a sequence alignment. While their method focused on non-nested coordinations, Hara et al. (2009) extended their work to accommodate nested coordinations using CFG rules. A consistent global structure of coordinations is produced using discriminative functions based on the similarity of conjuncts with dynamic pro"
N19-1343,C08-1054,0,0.0310004,"ons, Hara et al. (2009) extended their work to accommodate nested coordinations using CFG rules. A consistent global structure of coordinations is produced using discriminative functions based on the similarity of conjuncts with dynamic programming. Our concept of the CKY parsing is borrowed from their work; however, a key difference of our approach lies in how it computes the score of conjuncts and trains the score function. Hanamoto et al. (2012) used dual decomposition to combine HPSG parsing with the discriminative model developed by Hara et al. (2009). 5.2 Non Similarity-based Approaches Kawahara and Kurohashi (2008) focused on resolving the ambiguities of coordinate structures without the use of any similarities. Their method relied on the dependency relations surrounding the conjuncts and the generative probabilities of phrases. Yoshimoto et al. (2015) extended the Eis3401 ner algorithm by adding new rules to accommodate coordinations during dependency parsing. 5.3 Coordination Boundary Identification using Neural Networks Ficler and Goldberg (2016b) used neural networks for the coordination boundary identification task. They incorporated the replaceability property between conjuncts, in addition to the"
N19-1343,J94-4001,0,0.477605,"he other hand, the use of contextual embedding, ELMo, does not improve performance. We deduce that POS tags and morphological information, and not contextual word senses, are clues for shorter and similar coordinations such as NP coordinations. For the feature extraction function of the outer-boundary scoring model, the concat function that performs the same function as the inner-boundary scoring model does not achieve competitive advantage. The feature function described as Eq. 12 is designed to capRelated Work Similarity-based Approaches For the coordination identification task in Japanese, Kurohashi and Nagao (1994) used a chart to find the highest similarity pair of conjuncts using dynamic programming. Hogan (2007) developed a generative parsing model for coordinated noun phrases, incorporating symmetry in conjunct structures and head words. Shimbo and Hara (2007) proposed a discriminative model that computes scores based on the syntactic and morphological features assigned to edges and nodes in a sequence alignment. While their method focused on non-nested coordinations, Hara et al. (2009) extended their work to accommodate nested coordinations using CFG rules. A consistent global structure of coordina"
N19-1343,P16-1101,0,0.0285531,"nk beta (Kim et al., 2003) (GENIA). Unlike the evaluation by Teranishi et al. (2017) and Ficler and Goldberg (2016b), we strip the PTB of all quotation marks (“) and (”) to normalize irregular coordinations such as h. . . “Daybreak,” “Daywatch,” “Newsday,” and “Newsnight,” . . . i. We follow the standard train/development/test split on the PTB. For the GENIA, we do not apply the preprocessing described above. We evaluate the model through a five-fold cross-validation, as in Hara et al. (2009). 4.1.2 Model We use pretrained word vectors, POS tags, and character vectors produced by the CharCNN (Ma and Hovy, 2016), regarded as the default. We also investigate the performance of the model, using three different word representations for the encoder: (1) pretrained word embeddings; GloVe (Pennington et al., 2014) for the PTB, BioASQ (Tsatsaronis et al., 2012) for the GENIA, (2) contextualized sentence embeddings; ELMo, (3) randomly initialized word vectors. For the PTB, POS tags are obtained using the Stanford POS Tagger (Toutanova et al., 2003) with 10-way jackknifing. For the GENIA, we use the gold POS tags, as in Hara et al. (2009). To optimize the model parameters, we use Adam (Kingma and Ba, 2015). O"
N19-1343,D14-1162,0,0.087859,"oordinations such as h. . . “Daybreak,” “Daywatch,” “Newsday,” and “Newsnight,” . . . i. We follow the standard train/development/test split on the PTB. For the GENIA, we do not apply the preprocessing described above. We evaluate the model through a five-fold cross-validation, as in Hara et al. (2009). 4.1.2 Model We use pretrained word vectors, POS tags, and character vectors produced by the CharCNN (Ma and Hovy, 2016), regarded as the default. We also investigate the performance of the model, using three different word representations for the encoder: (1) pretrained word embeddings; GloVe (Pennington et al., 2014) for the PTB, BioASQ (Tsatsaronis et al., 2012) for the GENIA, (2) contextualized sentence embeddings; ELMo, (3) randomly initialized word vectors. For the PTB, POS tags are obtained using the Stanford POS Tagger (Toutanova et al., 2003) with 10-way jackknifing. For the GENIA, we use the gold POS tags, as in Hara et al. (2009). To optimize the model parameters, we use Adam (Kingma and Ba, 2015). Other hyperparameters are described in Appendix A. 4.1.3 Baseline Model We adopt our implementation of Teranishi et al. (2017) as the baseline. The original model of Teranishi et al. (2017) predicts th"
N19-1343,N18-1202,0,0.0237265,"es the vector to a MLP. ff eature (bl , er , wt , h1:N ) =   hbl − ht+1 ; her − ht−1 Coordinator Classifier We use a linear transformation of the sentencelevel representation of a coordinator key for fckey . fckey (wt ) = Wckey ht + bckey (10) (12) fouter (bl , er , wt ) = out w2out ReLU(W1out r) + bout 1 ) + b2 l (13) r r = ff eature (b , e , wt , h1:N ) (9) The dimensionality of each resulting vector ht is 2dhidden . For the BiLSTMs inputs, we use finput to map words and POS tags onto their representations. We can use different word representations including a pretrained word model, ELMo (Peters et al., 2018), BERT (Devlin et al., 2018) or character-level LSTMs/convolutional neural networks (CharCNNs). We demonstrate the differences between the different choices in Section 4. The entire network consisting of finput and BiLSTMs is referred to as the encoder; it is shared by the three neural networks in the higher layer. in Outer-Boundary Scoring Model Encoder To get sentence-level representations for a sequence of words and POS tags, we use bidirectional long short-term memories (BiLSTMs) (Hochreiter and Schmidhuber, 1997). h1:N = BiLSTMs(finput (w1:N , p1:N )) hidden d , win ∈ where W1in ∈ Rd ×4d"
N19-1343,C18-1011,0,0.0248453,"function in the CKY parsing with production rules for coordination, our system produces globally consistent coordinations in a given sentence. To obtain such a function, we decompose the task into three independent subtasks – finding a coordinator, identifying the inner boundaries of a pair of conjuncts and delineating its outer boundaries. We use three different neural networks for the tasks, and the networks are trained on the basis of their local decisions. Our method is inspired by recent successes with locally-trained models for structured inference problems such as constituency parsing (Teng and Zhang, 2018) and dependency parsing (Dozat and Manning, 2017) without globally-optimized training. Experimental results reveal that our model outperforms existing systems and our strong baseline, an extension of Teranishi et al. (2017), and ensures that the global structure of the coordinations is consistent. In summary, our contributions include the following: • We propose a simple framework that trains a generalized scoring function of a pair of conjuncts and uses it for inference. • We decompose the task and use three local models that interoperate for the CKY parsing. 3394 Proceedings of NAACL-HLT 201"
N19-1343,I17-1027,1,0.524882,"ighs almost twice as much and is three times the size,” we cannot find correct conjuncts for each coordinator at a glance. The presence of coordination makes a sentence more ambiguous and longer, resulting in errors in syntactic parsing. To identify the conjuncts of a given coordinator, previous studies have explored two properties of coordinate structures: (1) similarity – conjuncts tend to be similar; (2) replaceability – conjuncts can be replaced. Ficler and Goldberg (2016b) combine the syntactic parser and neural networks to compute the similarity and replaceability features of conjuncts. Teranishi et al. (2017) also exploit the two properties without deploying any syntactic parser, and achieve state-of-the-art results. Although both approaches outperform the similarity-based approaches (Shimbo and Hara, 2007; Hara et al., 2009), they cannot handle more than two conjuncts in a coordination, and multiple coordinations in a sentence at one time. Hence, their systems may produce coordinations that conflict with each other. In contrast, Hara et al. (2009) define production rules for coordination in order to output consistent coordinate structures. Here, we propose a new framework for coordination boundar"
N19-1343,N03-1033,0,0.0469421,"el through a five-fold cross-validation, as in Hara et al. (2009). 4.1.2 Model We use pretrained word vectors, POS tags, and character vectors produced by the CharCNN (Ma and Hovy, 2016), regarded as the default. We also investigate the performance of the model, using three different word representations for the encoder: (1) pretrained word embeddings; GloVe (Pennington et al., 2014) for the PTB, BioASQ (Tsatsaronis et al., 2012) for the GENIA, (2) contextualized sentence embeddings; ELMo, (3) randomly initialized word vectors. For the PTB, POS tags are obtained using the Stanford POS Tagger (Toutanova et al., 2003) with 10-way jackknifing. For the GENIA, we use the gold POS tags, as in Hara et al. (2009). To optimize the model parameters, we use Adam (Kingma and Ba, 2015). Other hyperparameters are described in Appendix A. 4.1.3 Baseline Model We adopt our implementation of Teranishi et al. (2017) as the baseline. The original model of Teranishi et al. (2017) predicts the beginning and the end of a coordinate structure, and then splits it into conjuncts by commas. Their model decides the boundary of a coordinate structure individually, which may cause conflicts with that of other coordinate structure(s)"
N19-1343,W15-2208,1,0.840907,". Our concept of the CKY parsing is borrowed from their work; however, a key difference of our approach lies in how it computes the score of conjuncts and trains the score function. Hanamoto et al. (2012) used dual decomposition to combine HPSG parsing with the discriminative model developed by Hara et al. (2009). 5.2 Non Similarity-based Approaches Kawahara and Kurohashi (2008) focused on resolving the ambiguities of coordinate structures without the use of any similarities. Their method relied on the dependency relations surrounding the conjuncts and the generative probabilities of phrases. Yoshimoto et al. (2015) extended the Eis3401 ner algorithm by adding new rules to accommodate coordinations during dependency parsing. 5.3 Coordination Boundary Identification using Neural Networks Ficler and Goldberg (2016b) used neural networks for the coordination boundary identification task. They incorporated the replaceability property between conjuncts, in addition to the similarity property, in the computation of a score for a pair of conjuncts. They first used a binary classifier for coordinating words; then, they extracted probable candidate pairs of conjuncts using the Berkeley Parser (Petrov et al., 2006"
N19-1343,P06-1055,0,0.0499212,"imoto et al. (2015) extended the Eis3401 ner algorithm by adding new rules to accommodate coordinations during dependency parsing. 5.3 Coordination Boundary Identification using Neural Networks Ficler and Goldberg (2016b) used neural networks for the coordination boundary identification task. They incorporated the replaceability property between conjuncts, in addition to the similarity property, in the computation of a score for a pair of conjuncts. They first used a binary classifier for coordinating words; then, they extracted probable candidate pairs of conjuncts using the Berkeley Parser (Petrov et al., 2006); afterward, they assigned scores to the pairs using neural networks. However, the shortcoming of their work is that it is highly dependent on the external parser. The work of Teranishi et al. (2017) developed an end-to-end model, as opposed to the pipeline approach of Ficler and Goldberg (2016b). They also used similarity and replaceability feature representations without information from a syntactic parser. While Ficler and Goldberg (2016b) cut off improbable pairs of conjuncts ahead of training, Teranishi et al. (2017) calculated scores for all possible pairs of the beginning and the end of"
N19-1343,D07-1064,0,0.489229,"ng in errors in syntactic parsing. To identify the conjuncts of a given coordinator, previous studies have explored two properties of coordinate structures: (1) similarity – conjuncts tend to be similar; (2) replaceability – conjuncts can be replaced. Ficler and Goldberg (2016b) combine the syntactic parser and neural networks to compute the similarity and replaceability features of conjuncts. Teranishi et al. (2017) also exploit the two properties without deploying any syntactic parser, and achieve state-of-the-art results. Although both approaches outperform the similarity-based approaches (Shimbo and Hara, 2007; Hara et al., 2009), they cannot handle more than two conjuncts in a coordination, and multiple coordinations in a sentence at one time. Hence, their systems may produce coordinations that conflict with each other. In contrast, Hara et al. (2009) define production rules for coordination in order to output consistent coordinate structures. Here, we propose a new framework for coordination boundary identification. We generalize a scoring function that takes a pair of spans with a coordinator and returns a higher score when the two spans appear to be coordinated. Using this function in the CKY p"
P10-2025,P07-1001,0,0.0172614,"and translation probability, which leads to a significant improvement in SMT performance. Many word alignment approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner (Vogel et al., 1996; Och and Ney, 2003; Fraser and Marcu, 2007). One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus. This monolingual knowledge makes it easier to determine corresponding words correctly. For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency of words in each language can help the alignment process (Ma et al., 2008). It has been shown that such grammatical 137 Proceedings of the ACL 2010 Conference Short Papers, pages 137–141, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics translation probability from {e to } f under the kth topic: p (f |e, z = k ). T = Ti,i′ is a state transition probability of a first order Markov process. Fig. 1 shows a graphical model of HM-BiTAM. The total likelihood of bilingual sentence pairs {E, F } can be obtained by marginalizing out laten"
P10-2025,D07-1006,0,0.0229483,"roves word alignment quality. 1 Introduction Word alignment is an essential step in most phrase and syntax based statistical machine translation (SMT). It is an inference problem of word correspondences between different languages given parallel sentence pairs. Accurate word alignment can induce high quality phrase detection and translation probability, which leads to a significant improvement in SMT performance. Many word alignment approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner (Vogel et al., 1996; Och and Ney, 2003; Fraser and Marcu, 2007). One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus. This monolingual knowledge makes it easier to determine corresponding words correctly. For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency of words in each language can help the alignment process (Ma et al., 2008). It has been shown that such grammatical 137 Proceedings of the ACL 2010 Conference Short Papers, pages 137–141, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for C"
P10-2025,W08-0409,0,0.0194292,"t approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner (Vogel et al., 1996; Och and Ney, 2003; Fraser and Marcu, 2007). One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus. This monolingual knowledge makes it easier to determine corresponding words correctly. For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency of words in each language can help the alignment process (Ma et al., 2008). It has been shown that such grammatical 137 Proceedings of the ACL 2010 Conference Short Papers, pages 137–141, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics translation probability from {e to } f under the kth topic: p (f |e, z = k ). T = Ti,i′ is a state transition probability of a first order Markov process. Fig. 1 shows a graphical model of HM-BiTAM. The total likelihood of bilingual sentence pairs {E, F } can be obtained by marginalizing out latent variables z, a and θ, p (F, E; Ψ) = ∑∑ z p (F, E, z, a, θ; Ψ) dθ, where Ψ = {α, β, T, B} is a paramete"
P10-2025,P00-1056,0,0.0606791,"GIZA++ is an implementation of IBM-model 4 and HMM, and HM-BiTAM corresponds to ζ = 0 in eq. 7. We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006). We trained the word alignment in two directions: English to French, and French to English. The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006). We evaluated these results for precision, recall, Fmeasure and alignment error rate (AER), which are standard metrics for word alignment accuracy (Och and Ney, 2000). Figure 2: Graphical model of synonym pair generative process correspond to the same word in a different language, thus they make it easy to infer accurate word alignment. HM-BiTAM and the synonym model share parameters in order to incorporate monolingual synonym information into the bilingual word alignment model. This can be achieved e in eq. 3 as, via reparameterizing Ψ ( ) e p f e, k; Ψ ( ) e p e, k; Ψ ≡ p (f |e, k; B ) , (4) ≡ p (e |k; β ) p (k; α) . (5) Overall, we re-define the synonym pair model with the HM-BiTAM parameter set Ψ, { } p( f, f ′ ; Ψ) ∏ ∑ 1 αk βk,e Bf,e,k Bf ′ ,e,k . (6)"
P10-2025,J03-1002,0,0.0178838,"d significantly improves word alignment quality. 1 Introduction Word alignment is an essential step in most phrase and syntax based statistical machine translation (SMT). It is an inference problem of word correspondences between different languages given parallel sentence pairs. Accurate word alignment can induce high quality phrase detection and translation probability, which leads to a significant improvement in SMT performance. Many word alignment approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner (Vogel et al., 1996; Och and Ney, 2003; Fraser and Marcu, 2007). One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus. This monolingual knowledge makes it easier to determine corresponding words correctly. For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency of words in each language can help the alignment process (Ma et al., 2008). It has been shown that such grammatical 137 Proceedings of the ACL 2010 Conference Short Papers, pages 137–141, c Uppsala, Sweden, 11-16 July 201"
P10-2025,C96-2141,0,0.849951,"t our proposed method significantly improves word alignment quality. 1 Introduction Word alignment is an essential step in most phrase and syntax based statistical machine translation (SMT). It is an inference problem of word correspondences between different languages given parallel sentence pairs. Accurate word alignment can induce high quality phrase detection and translation probability, which leads to a significant improvement in SMT performance. Many word alignment approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner (Vogel et al., 1996; Och and Ney, 2003; Fraser and Marcu, 2007). One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus. This monolingual knowledge makes it easier to determine corresponding words correctly. For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency of words in each language can help the alignment process (Ma et al., 2008). It has been shown that such grammatical 137 Proceedings of the ACL 2010 Conference Short Papers, pages 137–141, c Uppsala, Swe"
P10-2025,P05-1074,0,0.0405879,"t topic zn ∈ {1, . . . , k, . . . , K}, where K is the number of latent topics. Let N be the number of sentence pairs, and In and Jn be the lengths of En and Fn , respectively. In this framework, all of the bilingual sentence pairs {E, F } = {(En , Fn )}N n=1 are generated as follows. ( ) ∑ ( ) p f, f ′ ∝ p (f |s ) p f ′ |s p (s) . (2) s We define a pair (e, k) as a representation of the sense s, where e and k are a word in a different language E and a latent topic, respectively. It has been shown that a word e in a different language is an appropriate representation of s in synonym modeling (Bannard and Callison-Burch, 2005). We assume that adding a latent topic k for the sense is very useful for disambiguating word meaning, and thus that (e, k) gives us a good approximation of s. Under this assumption, the synonym pair generative model can be defined as follows. 1. θ ∼ Dirichlet (α): sample topic-weight vector 2. For each sentence pair (En , Fn ) (a) zn ∼ M ultinomial (θ): sample the topic (b) en,i:In |zn ∼ p (En |zn ; β ): sample English (c) (1) a words from a monolingual unigram model given topic zn For each position jn = 1, . . . , Jn i. ajn ∼ p (ajn |ajn −1 ; T ): sample an alignment link ajn from a first or"
P10-2025,P06-2124,0,0.0185093,"rom WordNet 2.1 (Miller, 1995) and WOLF 0.1.4 (Sagot and Fiser, 2008), respectively. WOLF is a semantic resource constructed from the Princeton WordNet and various multilingual resources. We selected synonym pairs where both words were included in the bilingual training set. We compared the word alignment performance of our model with that of GIZA++ 1.03 1 (Vogel et al., 1996; Och and Ney, 2003), and HMBiTAM (Zhao and Xing, 2008) implemented by us. GIZA++ is an implementation of IBM-model 4 and HMM, and HM-BiTAM corresponds to ζ = 0 in eq. 7. We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006). We trained the word alignment in two directions: English to French, and French to English. The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006). We evaluated these results for precision, recall, Fmeasure and alignment error rate (AER), which are standard metrics for word alignment accuracy (Och and Ney, 2000). Figure 2: Graphical model of synonym pair generative process correspond to the same word in a different language, thus they make it easy to infer a"
P10-2025,W03-0301,0,\N,Missing
P11-2036,P10-2042,0,0.211575,"r. The base distribution over initial trees is defined as P0 (e |X ), and the base distribution over simple auxiliary trees is defined as P00 (e |X ). An initial tree ei replaces a frontier node with probability p (ei |e−i , X, dX , θX ). On the other hand, a simple auxiliary tree e0 i inserts an internal node  0 , where with probability aX ×p0 e0i e0−i , X, d0X , θX aX is an insertion probability defined for each X. The stopping probabilities are common to both initial and auxiliary trees. 3.2 Grammar Decomposition We develop a grammar decomposition technique, which is an extension of work (Cohn and Blunsom, 2010) on BTSG model, to deal with an insertion operator. The motivation behind grammar decomposition is that it is hard to consider all possible 208 ins (N girl) probability ins (N girl) → the (N girl) → Nins (N (JJ pretty) N*) (1 − aDT ) × aN 1 0 α(N (JJ pretty) N*),N (N girl) Nins (N (JJ pretty) N*) → JJ(JJ pretty) N(N girl) JJ(JJ pretty) → pretty 1 N(N girl) → girl 1 (1 − aJJ ) × 1 Table 1: The rules and probabilities of grammar decomposition for Fig. 2. derivations explicitly since the base distribution assigns non-zero probability to an infinite number of initial and auxiliary trees. Alternati"
P11-2036,N09-1036,0,0.0317833,"0 (ei , |X ) , (1) where αei ,X θX +dX ·t·,X . θX +n−i ·,X = n−i e ,X −dX ·tei ,X i and βX θX +n−i ·,X = e−i = e1 , . . . , ei−1 are previously generated initial trees, and n−i ei ,X is the number of times ei has been used in e−i . tei ,X is the number of taP bles labeled with ei . n−i = e n−i ·,X e,X and t·,X = P e te,X are the total counts of initial trees and tables, respectively. The PYP prior produces “rich get richer” statistics: a few initial trees are often used for derivation while many are rarely used, and this is shown empirically to be well-suited for natural language (Teh, 2006b; Johnson and Goldwater, 2009). The base probability of an initial tree, P0 (e |X ), is given as follows. (a) (b) P0 (e |X ) = Y PMLE (r) × r∈CFG(e) × Y Y sA Figure 1: Example of (a) substitution and (b) insertion (dotted line). A∈LEAF(e) (1 − sB ) , (2) B∈INTER(e) where CFG (e) is a set of decomposed CFG productions of e, PMLE (r) is a maximum likelihood estimate (MLE) of r. LEAF (e) and INTER (e) are sets of leaf and internal symbols of e, respectively. sX is a stopping probability defined for each X. 3 3.1 Insertion Operator for BTSG Tree Insertion Model We propose a model that incorporates an insertion operator in BTSG"
P11-2036,P05-1010,0,0.0343211,"n a top-down manner. 3. Accept or reject the derivation sample by using the MH test. The MH algorithm is described in detail in (Cohn and Blunsom, 2010). The hyperparameters of our model are updated with the auxiliary variable technique (Teh, 2006a). 4 method CFG BTSG BTSG + insertion CFG BTSG BTSG + insertion Experiments We ran experiments on the British National Corpus (BNC) Treebank 3 and the WSJ English Penn Treebank. We did not use a development set since our model automatically updates the hyperparameters for every iteration. The treebank data was binarized using the CENTER-HEAD method (Matsuzaki et al., 2005). We replaced lexical words with counts ≤ 1 in the training set with one of three unknown 1 Results from (Cohn and Blunsom, 2010). Results for length ≤ 40. 3 http://nclt.computing.dcu.ie/~jfoster/resources/ 2 (Cohn and Blunsom, 2010) Table 3: Full Penn Treebank dataset experiments words using lexical features. We trained our model using a training set, and then sampled 10k derivations for each sentence in a test set. Parsing results were obtained with the MER algorithm (Cohn et al., 2011) using the 10k derivation samples. We show the bracketing F1 score of predicted parse trees evaluated by EV"
P11-2036,P06-1055,0,0.264362,"Missing"
P11-2036,P09-2012,0,0.0811452,"d JJ, respectively. Note that the probability of a derivation according to Table 1 is the same as the probability of a derivation obtained from the distribution over the initial and auxiliary trees (i.e. eq. 1 and eq. 3). In Table 1, we assume that the auxiliary tree “(N (JJ pretty) (N*))” is sampled from the first term of eq. 3. When it is sampled from the second term, we alternatively assign the probability 0 β(N (JJ pretty) N*), N . 3.3 Table 2: Small dataset experiments CFG BTSG BTSG + insertion Training # rules (# aux. trees) 35374 (-) 80026 (0) 65099 (25) F1 71.0 85.0 85.3 - 82.62 85.3 (Post and Gildea, 2009) We use a blocked Metropolis-Hastings (MH) algorithm (Cohn and Blunsom, 2010) to train our model. The MH algorithm learns BTSG model parameters efficiently, and it can be applied to our insertion model. The MH algorithm consists of the following three steps. For each sentence, 1. Calculate the inside probability (Lari and Young, 1991) in a bottom-up manner using the grammar decomposition. 2. Sample a derivation tree in a top-down manner. 3. Accept or reject the derivation sample by using the MH test. The MH algorithm is described in detail in (Cohn and Blunsom, 2010). The hyperparameters of ou"
P11-2036,J95-4002,0,0.690892,"that our model outperforms a standard PCFG and BTSG for a small dataset. For a large dataset, our model obtains comparable results to BTSG, making the number of grammar rules much smaller than with BTSG. 2 1 Introduction Tree substitution grammar (TSG) is a promising formalism for modeling language data. TSG generalizes context free grammars (CFG) by allowing nonterminal nodes to be replaced with subtrees of arbitrary size. A natural extension of TSG involves adding an insertion operator for combining subtrees as in tree adjoining grammars (TAG) (Joshi, 1985) or tree insertion grammars (TIG) (Schabes and Waters, 1995). An insertion operator is helpful for expressing various syntax patterns with fewer grammar rules, thus we expect that adding an insertion operator will improve parsing accuracy and realize a compact grammar size. One of the challenges of adding an insertion operator is that the computational cost of grammar induction is high since tree insertion significantly increases the number of possible subtrees. Previous work on TAG and TIG induction (Xia, 1999; Chiang, 2003; Chen et al., 2006) has addressed the problem using language-specific heuristics and a maxiOverview of BTSG Model We briefly revi"
P11-2036,P06-1124,0,0.304811,"ei ,X + βX P0 (ei , |X ) , (1) where αei ,X θX +dX ·t·,X . θX +n−i ·,X = n−i e ,X −dX ·tei ,X i and βX θX +n−i ·,X = e−i = e1 , . . . , ei−1 are previously generated initial trees, and n−i ei ,X is the number of times ei has been used in e−i . tei ,X is the number of taP bles labeled with ei . n−i = e n−i ·,X e,X and t·,X = P e te,X are the total counts of initial trees and tables, respectively. The PYP prior produces “rich get richer” statistics: a few initial trees are often used for derivation while many are rarely used, and this is shown empirically to be well-suited for natural language (Teh, 2006b; Johnson and Goldwater, 2009). The base probability of an initial tree, P0 (e |X ), is given as follows. (a) (b) P0 (e |X ) = Y PMLE (r) × r∈CFG(e) × Y Y sA Figure 1: Example of (a) substitution and (b) insertion (dotted line). A∈LEAF(e) (1 − sB ) , (2) B∈INTER(e) where CFG (e) is a set of decomposed CFG productions of e, PMLE (r) is a maximum likelihood estimate (MLE) of r. LEAF (e) and INTER (e) are sets of leaf and internal symbols of e, respectively. sX is a stopping probability defined for each X. 3 3.1 Insertion Operator for BTSG Tree Insertion Model We propose a model that incorporate"
P11-2036,P00-1058,0,\N,Missing
P12-1046,P10-1112,0,0.121454,"as been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al., 2010). TSG is a natural extension of CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, but is still inferior to state-of-the-art parsers (e.g., the Berkeley parser (Petrov et al., 2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CF"
P12-1046,D10-1117,0,0.113633,"nparametric Bayesian model. The PYP produces power-law distributions, which have been shown to be well-suited for such uses as language modeling (Teh, 2006b), and TSG induction (Cohn et al., 2010). One major issue as regards modeling an SR-TSG is that the space of the grammar rules will be very sparse since SR-TSG allows for arbitrarily large tree fragments and also an arbitrarily large set of symbol subcategories. To address the sparseness problem, we employ a hierarchical PYP to encode a backoff scheme from the SRTSG rules to simpler CFG rules, inspired by recent work on dependency parsing (Blunsom and Cohn, 2010). Our model consists of a three-level hierarchy. Table 1 shows an example of the SR-TSG rule and its backoff tree fragments as an illustration of this threelevel hierarchy. The topmost level of our model is a distribution over the SR-TSG rules as follows. e |xk Gxk ∼ Gxk  ∼ PYP dxk , θxk , P sr-tsg (· |xk ) , where xk is a refined root symbol of an elementary tree e, while x is a raw nonterminal symbol in the corpus and k = 0, 1, . . . is an index of the symbol subcategory. Suppose x is NP and its symbol subcategory is 0, then xk is NP0 . The PYP has three parameters: (dxk , θxk , P sr-tsg )."
P12-1046,P05-1022,0,0.838813,"ten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, but is still inferior to state-of-the-art parsers (e.g., the Berkeley parser (Petrov et al., 2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement"
P12-1046,N10-1081,0,0.163802,"However, an adaptor grammar differs from ours in that all its rules are complete: all leaf nodes must be terminal symbols, while our model permits nonterminal symbols as leaf nodes. Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as (a) (b) (c) Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of (a). The refinement annotation is hyphenated with a nonterminal symbol. morphology analysis, word segmentation (Johnson and Goldwater, 2009), and dependency grammar induction (Cohen et al., 2010), rather than constituent syntax parsing. An all-fragments grammar (Bansal and Klein, 2010) is another variant of TSG that aims to utilize all possible subtrees as rules. It maps a TSG to an implicit representation to make the grammar tractable and practical for large-scale parsing. The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task. As mentioned in the introduction, our model focuses on the automatic learning of a TSG and symbol refinement without heuristics. 3 Symbol-Refined Tr"
P12-1046,J03-4003,0,0.043631,"2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments. As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010), large 440 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tree fragments and symbol refinement work complementarily for syntactic parsing. F"
P12-1046,D09-1076,0,0.042041,"Missing"
P12-1046,N09-2064,0,0.139748,"Missing"
P12-1046,N04-1035,0,0.0974858,"Missing"
P12-1046,P08-1067,0,0.305949,"Missing"
P12-1046,N09-1036,0,0.151665,"ol refinement, and is thus closely related to our SR-TSG model. However, an adaptor grammar differs from ours in that all its rules are complete: all leaf nodes must be terminal symbols, while our model permits nonterminal symbols as leaf nodes. Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as (a) (b) (c) Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of (a). The refinement annotation is hyphenated with a nonterminal symbol. morphology analysis, word segmentation (Johnson and Goldwater, 2009), and dependency grammar induction (Cohen et al., 2010), rather than constituent syntax parsing. An all-fragments grammar (Bansal and Klein, 2010) is another variant of TSG that aims to utilize all possible subtrees as rules. It maps a TSG to an implicit representation to make the grammar tractable and practical for large-scale parsing. The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task. As mentioned in the introduction, our model focuses on the automatic learning of a TSG and sy"
P12-1046,N07-1018,0,0.296046,"a process of forming a parse tree. It starts with a root symbol and rewrites (substi441 p (e |t ) ∝ p (t |e ) p (e) . where p (t |e ) is either equal to 1 (when t and e are consistent) or 0 (otherwise). Therefore, the task of TSG induction from parse trees turns out to consist of modeling the prior distribution p (e). Recent work on TSG induction defines p (e) as a nonparametric Bayesian model such as the Dirichlet Process (Ferguson, 1973) or the Pitman-Yor Process to encourage sparse and compact grammars. Several studies have combined TSG induction and symbol refinement. An adaptor grammar (Johnson et al., 2007a) is a sort of nonparametric Bayesian TSG model with symbol refinement, and is thus closely related to our SR-TSG model. However, an adaptor grammar differs from ours in that all its rules are complete: all leaf nodes must be terminal symbols, while our model permits nonterminal symbols as leaf nodes. Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as (a) (b) (c) Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of (a). The refinement annotation is hyphenated with a no"
P12-1046,J98-4004,0,0.640761,"Petrov et al., 2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments. As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010), large 440 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tree fragments and symbol refinement work complementarily for synta"
P12-1046,P03-1054,0,0.254122,"ion Syntactic parsing has played a central role in natural language processing. The resulting syntactic analysis can be used for various applications such as machine translation (Galley et al., 2004; DeNeefe and Knight, 2009), sentence compression (Cohn and Lapata, 2009; Yamangil and Shieber, 2010), and question answering (Wang et al., 2007). Probabilistic context-free grammar (PCFG) underlies many statistical parsers, however, it is well known that the PCFG rules extracted from treebank data via maximum likelihood estimation do not perform well due to unrealistic context freedom assumptions (Klein and Manning, 2003). In recent years, there has been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al., 2010). TSG is a natural extension of CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 201"
P12-1046,J93-2004,0,0.058271,"ation probabilities according to our model. This heuristics is helpful for finding large tree fragments and learning compact grammars. 4.3 Hyperparameter Estimation We treat hyperparameters {d, θ} as random variables and update their values for every MCMC iteration. We place a prior on the hyperparameters as follows: d ∼ Beta (1, 1), θ ∼ Gamma (1, 1). The values of d and θ are optimized with the auxiliary variable technique (Teh, 2006a). 5 Experiment Model 5.1 Settings CFG 5.1.1 Data Preparation We ran experiments on the Wall Street Journal (WSJ) portion of the English Penn Treebank data set (Marcus et al., 1993), using a standard data split (sections 2–21 for training, 22 for development and 23 for testing). We also used section 2 as a small training set for evaluating the performance of our model under low-resource conditions. Henceforth, we distinguish the small training set (section 2) from the full training set (sections 2-21). The treebank data is right-binarized (Matsuzaki et al., 2005) to construct grammars with only unary and binary productions. We replace lexical words with count ≤ 5 in the training data with one of 50 unknown words using lexical features, following (Petrov et al., 2006). We"
P12-1046,P05-1010,1,0.934163,"harniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments. As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010), large 440 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tree fragments and symbol refinement work complementarily for syntactic parsing. For example, Bansal and Kl"
P12-1046,P06-1055,0,0.730889,"CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, but is still inferior to state-of-the-art parsers (e.g., the Berkeley parser (Petrov et al., 2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Colli"
P12-1046,N10-1003,0,0.14792,"Missing"
P12-1046,P09-2012,0,0.2292,"anslation (Galley et al., 2004; DeNeefe and Knight, 2009), sentence compression (Cohn and Lapata, 2009; Yamangil and Shieber, 2010), and question answering (Wang et al., 2007). Probabilistic context-free grammar (PCFG) underlies many statistical parsers, however, it is well known that the PCFG rules extracted from treebank data via maximum likelihood estimation do not perform well due to unrealistic context freedom assumptions (Klein and Manning, 2003). In recent years, there has been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al., 2010). TSG is a natural extension of CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, bu"
P12-1046,P06-1124,0,0.186641,"R-TSG derivations from a corpus of parse trees in an unsupervised fashion. That is, we wish to infer the symbol subcategories of every node and substitution site (i.e., nodes where substitution occurs) from parse trees. Extracted rules and their probabilities can be used to parse new raw sentences. 442 3.1 Probabilistic Model We define a probabilistic model of an SR-TSG based on the Pitman-Yor Process (PYP) (Pitman and Yor, 1997), namely a sort of nonparametric Bayesian model. The PYP produces power-law distributions, which have been shown to be well-suited for such uses as language modeling (Teh, 2006b), and TSG induction (Cohn et al., 2010). One major issue as regards modeling an SR-TSG is that the space of the grammar rules will be very sparse since SR-TSG allows for arbitrarily large tree fragments and also an arbitrarily large set of symbol subcategories. To address the sparseness problem, we employ a hierarchical PYP to encode a backoff scheme from the SRTSG rules to simpler CFG rules, inspired by recent work on dependency parsing (Blunsom and Cohn, 2010). Our model consists of a three-level hierarchy. Table 1 shows an example of the SR-TSG rule and its backoff tree fragments as an il"
P12-1046,D07-1003,0,0.0199049,"Missing"
P12-1046,P10-1096,0,0.0399252,"Missing"
P12-1046,D09-1161,0,0.157163,"Missing"
P12-1046,D07-1058,0,0.0141628,"urthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments. As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010), large 440 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tree fragments and symbol refinement work complementarily for syntactic parsing. For example, Bansal and Klein (2010) have reported that deterministic symbol refinement with heuristics helps improve the accuracy of a TSG parser. In this paper, we propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. SR-TSG is an extension of the conventio"
P15-1093,W02-1001,0,0.0740395,"Missing"
P15-1093,P10-1160,0,0.100864,"Missing"
P15-1093,I11-1023,1,0.841898,"Missing"
P15-1093,P08-1067,0,0.0786277,"Missing"
P15-1093,W07-1522,1,0.769102,"Missing"
P15-1093,P09-2022,0,0.517951,"Missing"
P15-1093,P13-1116,0,0.0315645,"Missing"
P15-1093,E06-1011,0,0.0563756,"Missing"
P15-1093,I11-1085,0,0.409695,"Missing"
P15-1093,D08-1055,0,0.319631,"Missing"
P15-1093,D14-1041,0,0.0607383,"Missing"
P15-1093,D14-1109,0,0.0485623,"Missing"
P15-2140,P13-2131,0,0.0581399,"Missing"
P15-2140,P14-1134,0,0.297952,"of two nouns (e.g., assigning a relation 1 851 http://amr.isi.edu/ Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 851–856, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Train 3504 Dev 463 Test 398 person! &quot;! retire-01! plant! Table 1: Statistics of the extracted NP data work-01! a two-step approach: first identifying distinct concepts (nodes) in the AMR graph, then defining the dependency relations between those concepts (Flanigan et al., 2014). In the concept identification step, unlike POS tagging, one word is sometimes assigned with more than one concept, and the number of possible concepts is far more than the number of possible parts-of-speech. As the concept identification accuracy remains low, such a pipeline method suffers from error propagation, thus resulting in a suboptimal AMR parsing performance. To solve this problem, we extend a transitionbased dependency parsing algorithm, and propose a novel algorithm which jointly identifies the concepts and the relations in AMR trees. Compared to the baseline, our method improves"
P15-2140,N03-1013,0,0.0103458,"T((run-01)) and S HIFT((sleep-01)) are different actions, they share the features “S”, “S”◦“DICTPRED ” because they share the generation rule DICTPRED . Action a S HIFT(c) φaction ((σ, [wi |β], R), a) {“S”, “S” ◦ rule(wi , c), “S” ◦ rule(wi , c) ◦ c} L EFT-R EDUCE(r, n) {“L-R”, “L-R” ◦ r, “L-R” ◦ r ◦ n} R IGHT-R EDUCE(r, n) {“R-R”, “R-R”◦r, “R-R”◦r ◦n} E MPTY-R EDUCE {“E-R”} Table 5: Feature sets for the action word. Finally, we add the rules DICTPRED and DICTNOUN . These two rules need conversion from nouns and adjectives to their verb and noun forms, For this conversion, we use CatVar v2.1 (Habash and Dorr, 2003), which lists categorial variations of words (such as verb run for noun runner). We also use definitions of the predicates from PropBank (Palmer et al., 2005), which AMR tries to reuse as much as possible, and impose constraints that the defined predicates can only have semantic relations consistent with the definition. During the training, we use the max-violation perceptron (Huang et al., 2012) with beam size 8 and average the parameters. During the testing, we also perform beam search with beam size 8. Table 6 shows the overall performance on NP semantic structure analysis. We evaluate the"
P15-2140,I11-1136,0,0.0124877,") retire-01 ARG0-of ARG2 work-01 person 7 L EFT-R EDUCE(ARG0-of, nroot ) ARG0-of work-01 ARG2 Figure 3: Derivation of an AMR tree for a retired plant worker (σ0 and σ1 denote the top and the second top of the stack, respectively.) Action S HIFT(c(wi )) L EFT-R EDUCE(r, n) R IGHT-R EDUCE(r, n) E MPTY-R EDUCE Current state (σ, [wi |β], R) ([σ|ci |cj ], β, R) ([σ|ci |cj ], β, R) ([σ|φ], β, R) Next state ([σ|c(wi )], β, R) r ([σ|cj ], β, R ∪ {nroot (ci ) ← − n(cj )}) r ([σ|ci ], β, R ∪ {n(ci ) − → nroot (cj )}) (σ, β, R) Table 2: Definitions of the actions relations. Our algorithm is similar to (Hatori et al., 2011), in which they perform POS tagging and dependency parsing jointly by assigning a POS tag to a word when performing S HIFT, but differs in that, unlike POS tagging, one word is sometimes assigned with more than one concept. In our algorithm, the input words are stored in the buffer and the identified concepts are stored in the stack. S HIFT identifies a concept subtree associated with the top word in the buffer. R EDUCE identifies the dependency relation between the top two concept subtrees in the stack. Figure 3 illustrates the process of deriving an AMR tree for a retired plant worker, and F"
P15-2140,N12-1015,0,0.0367744,"Missing"
P15-2140,P03-1054,0,0.00434346,"m spanning tree (MST) algorithm used for dependency parsing (McDonald et al., 2005). They report that using gold concepts yields much better performance, implying that joint identification of concepts and relations can be helpful. Abstract Meaning Representation 2.1 Extraction of NPs We extract substructures (subtrees) corresponding to NPs from the AMR Bank (LDC2014T12). In the AMR Bank, there is no alignment between the words and the concepts (nodes) in the AMR graphs. We obtain this alignment by using the rule-based alignment tool by Flanigan et al. (2014). Then, we use the Stanford Parser (Klein and Manning, 2003) to obtain constituency trees, and extract NPs that contain more than one noun and are not included by another NP. We exclude NPs that contain named entities, because they would require various kinds of manually crafted rules for each type of named entity. We also exclude NPs that contain possessive pronouns or conjunctions, which prove problematic for the alignment tool. Table 1 shows the statistics of the extracted NP data. 3 Proposed Method In this paper, we propose a novel approach for mapping the word sequence in an NP to an AMR tree, where the concepts (nodes) corresponding to the words"
P15-2140,J02-3004,0,0.0634192,"e, given a sequence of words in an NP. The previous method for AMR parsing takes a Introduction Semantic structure analysis of noun phrases (NPs) is an important research topic, which is beneficial for various NLP tasks, such as machine translation and question answering (Nakov and Hearst, 2013; Nakov, 2013). Among the previous works on NP analysis are internal NP structure analysis (Vadas and Curran, 2007; Vadas and Curran, 2008), noun-noun relation analysis of noun compounds (Girju et al., 2005; Tratz and Hovy, 2010; Kim and Baldwin, 2013), and predicate-argument analysis of noun compounds (Lapata, 2002). The goal of internal NP structure analysis is to assign bracket information inside an NP (e.g., (lung cancer) deaths indicates that the phrase lung cancer modifies the head deaths). In noun-noun relation analysis, the goal is to assign one of the predefined semantic relations to a noun compound consisting of two nouns (e.g., assigning a relation 1 851 http://amr.isi.edu/ Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 851–856, c Beijing, China, July 26-31, 2"
P15-2140,H05-1066,0,0.16785,"Missing"
P15-2140,W04-0308,0,0.026101,"P. We exclude NPs that contain named entities, because they would require various kinds of manually crafted rules for each type of named entity. We also exclude NPs that contain possessive pronouns or conjunctions, which prove problematic for the alignment tool. Table 1 shows the statistics of the extracted NP data. 3 Proposed Method In this paper, we propose a novel approach for mapping the word sequence in an NP to an AMR tree, where the concepts (nodes) corresponding to the words and the dependency relations between those concepts must be identified. We extend the arc-standard algorithm by Nivre (2004) for AMR parsing, and propose a transition-based algorithm which jointly identifies concepts and dependency ARG0-of! retire-01! plant! ARG2! 2.2 Previous Method for AMR Analysis a We adopt the method proposed by Flanigan et al. (2014) as our baseline, which is a two-step pipeline method of concept identification step and retired! plant! person! ARG0-of! work-01! worker! Figure 4: a retired plant worker in AMR 852 Previous action 0 (initial state) 1 S HIFT(EMPTY(a)) 2 E MPTY-R EDUCE σ1 ∅ 3 S HIFT(DICTPRED (retired)) 4 S HIFT(LEMMA(plant)) σ0 retire-01 retire-01 plant β [a retired plant worker]"
P15-2140,J05-1004,0,0.0126885,"a S HIFT(c) φaction ((σ, [wi |β], R), a) {“S”, “S” ◦ rule(wi , c), “S” ◦ rule(wi , c) ◦ c} L EFT-R EDUCE(r, n) {“L-R”, “L-R” ◦ r, “L-R” ◦ r ◦ n} R IGHT-R EDUCE(r, n) {“R-R”, “R-R”◦r, “R-R”◦r ◦n} E MPTY-R EDUCE {“E-R”} Table 5: Feature sets for the action word. Finally, we add the rules DICTPRED and DICTNOUN . These two rules need conversion from nouns and adjectives to their verb and noun forms, For this conversion, we use CatVar v2.1 (Habash and Dorr, 2003), which lists categorial variations of words (such as verb run for noun runner). We also use definitions of the predicates from PropBank (Palmer et al., 2005), which AMR tries to reuse as much as possible, and impose constraints that the defined predicates can only have semantic relations consistent with the definition. During the training, we use the max-violation perceptron (Huang et al., 2012) with beam size 8 and average the parameters. During the testing, we also perform beam search with beam size 8. Table 6 shows the overall performance on NP semantic structure analysis. We evaluate the performance using the Smatch score (Cai and Knight, 4 Experiments We conduct an experiment using our NP data set (Table 1). We use the implementation 2 of (Fl"
P15-2140,P10-1070,0,0.0245489,"ures corresponding to NPs are trees. Thus, we define our task as predicting the AMR tree structure, given a sequence of words in an NP. The previous method for AMR parsing takes a Introduction Semantic structure analysis of noun phrases (NPs) is an important research topic, which is beneficial for various NLP tasks, such as machine translation and question answering (Nakov and Hearst, 2013; Nakov, 2013). Among the previous works on NP analysis are internal NP structure analysis (Vadas and Curran, 2007; Vadas and Curran, 2008), noun-noun relation analysis of noun compounds (Girju et al., 2005; Tratz and Hovy, 2010; Kim and Baldwin, 2013), and predicate-argument analysis of noun compounds (Lapata, 2002). The goal of internal NP structure analysis is to assign bracket information inside an NP (e.g., (lung cancer) deaths indicates that the phrase lung cancer modifies the head deaths). In noun-noun relation analysis, the goal is to assign one of the predefined semantic relations to a noun compound consisting of two nouns (e.g., assigning a relation 1 851 http://amr.isi.edu/ Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on"
P15-2140,P07-1031,0,0.0378677,", since we found out that NPs mostly form trees rather than graphs in the AMR Bank, we can assume that AMR substructures corresponding to NPs are trees. Thus, we define our task as predicting the AMR tree structure, given a sequence of words in an NP. The previous method for AMR parsing takes a Introduction Semantic structure analysis of noun phrases (NPs) is an important research topic, which is beneficial for various NLP tasks, such as machine translation and question answering (Nakov and Hearst, 2013; Nakov, 2013). Among the previous works on NP analysis are internal NP structure analysis (Vadas and Curran, 2007; Vadas and Curran, 2008), noun-noun relation analysis of noun compounds (Girju et al., 2005; Tratz and Hovy, 2010; Kim and Baldwin, 2013), and predicate-argument analysis of noun compounds (Lapata, 2002). The goal of internal NP structure analysis is to assign bracket information inside an NP (e.g., (lung cancer) deaths indicates that the phrase lung cancer modifies the head deaths). In noun-noun relation analysis, the goal is to assign one of the predefined semantic relations to a noun compound consisting of two nouns (e.g., assigning a relation 1 851 http://amr.isi.edu/ Proceedings of the 5"
P15-2140,P08-1039,0,0.0235787,"t NPs mostly form trees rather than graphs in the AMR Bank, we can assume that AMR substructures corresponding to NPs are trees. Thus, we define our task as predicting the AMR tree structure, given a sequence of words in an NP. The previous method for AMR parsing takes a Introduction Semantic structure analysis of noun phrases (NPs) is an important research topic, which is beneficial for various NLP tasks, such as machine translation and question answering (Nakov and Hearst, 2013; Nakov, 2013). Among the previous works on NP analysis are internal NP structure analysis (Vadas and Curran, 2007; Vadas and Curran, 2008), noun-noun relation analysis of noun compounds (Girju et al., 2005; Tratz and Hovy, 2010; Kim and Baldwin, 2013), and predicate-argument analysis of noun compounds (Lapata, 2002). The goal of internal NP structure analysis is to assign bracket information inside an NP (e.g., (lung cancer) deaths indicates that the phrase lung cancer modifies the head deaths). In noun-noun relation analysis, the goal is to assign one of the predefined semantic relations to a noun compound consisting of two nouns (e.g., assigning a relation 1 851 http://amr.isi.edu/ Proceedings of the 53rd Annual Meeting of the"
P15-2140,2003.mtsummit-systems.9,0,\N,Missing
P17-1146,W05-0620,0,0.14695,"Missing"
P17-1146,P09-2022,0,0.676518,"Linguistics https://doi.org/10.18653/v1/P17-1146 Figure 2: Overview of neural models: (i) single-sequence and (ii) multi-sequence models. In this paper, we first introduce a basic model that uses RNNs. This model independently estimates the arguments of each predicate without considering multi-predicate interactions (Sec. 3). Then, extending this model, we propose a neural model that uses Grid-RNNs (Sec. 4). Performing experiments on the NAIST Text Corpus (Iida et al., 2007), we demonstrate that even without syntactic information, our neural models outperform previous syntax-dependent models (Imamura et al., 2009; Ouchi et al., 2015). In particular, the neural model using Grid-RNNs achieved the best result. This suggests that the proposed grid-type neural architecture effectively captures multi-predicate interactions and contributes to performance improvements. 1 2 Japanese Predicate Argument Structure Analysis 2.1 Task Description In Japanese PAS analysis, arguments are identified that each fulfills one of the three major case roles, nominative (NOM), accusative (ACC) and dative (DAT) cases, for each predicate. Arguments can be divided into the following three categories according to the positions re"
P17-1146,P15-1093,1,0.276039,"mantic analysis task, in which systems are required to identify the semantic units of a sentence, such as who did what to whom. In prodrop languages such as Japanese, Chinese and Italian, arguments are often omitted in text, and such argument omission is regarded as one of the most problematic issues facing PAS analysis (Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Hangyo et al., 2013). In response to the argument omission problem, in Japanese PAS analysis, a joint model of the interactions between multiple predicates has been gaining popularity and achieved the state-ofthe-art results (Ouchi et al., 2015; Shibata et al., 2016). This approach is based on the linguistic intuition that the predicates in a sentence are semantically related to each other, and capturing this relation can be useful for PAS analysis. In the example sentence in Figure 1, the word “男 i (mani )” is the accusative argument of the predicate “逮捕し た (arrested)” and is shared by the other predicate “逃走した (escaped)” as its nominative argument. Considering the semantic relation between “逮捕 した (arrested)” and “逃走した (escaped)”, we intuitively know that the person arrested by someone is likely to be the escaper. That is, informat"
P17-1146,D13-1095,0,0.403526,"rguments. “NOM” and “ACC” denote the nominative and accusative arguments, respectively. “ϕi ” is a zero pronoun, referring to the antecedent “男 i (mani )”. Introduction Predicate argument structure (PAS) analysis is a basic semantic analysis task, in which systems are required to identify the semantic units of a sentence, such as who did what to whom. In prodrop languages such as Japanese, Chinese and Italian, arguments are often omitted in text, and such argument omission is regarded as one of the most problematic issues facing PAS analysis (Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Hangyo et al., 2013). In response to the argument omission problem, in Japanese PAS analysis, a joint model of the interactions between multiple predicates has been gaining popularity and achieved the state-ofthe-art results (Ouchi et al., 2015; Shibata et al., 2016). This approach is based on the linguistic intuition that the predicates in a sentence are semantically related to each other, and capturing this relation can be useful for PAS analysis. In the example sentence in Figure 1, the word “男 i (mani )” is the accusative argument of the predicate “逮捕し た (arrested)” and is shared by the other predicate “逃走した"
P17-1146,I11-1023,1,0.962427,"rticular, the neural model using Grid-RNNs achieved the best result. This suggests that the proposed grid-type neural architecture effectively captures multi-predicate interactions and contributes to performance improvements. 1 2 Japanese Predicate Argument Structure Analysis 2.1 Task Description In Japanese PAS analysis, arguments are identified that each fulfills one of the three major case roles, nominative (NOM), accusative (ACC) and dative (DAT) cases, for each predicate. Arguments can be divided into the following three categories according to the positions relative to their predicates (Hayashibe et al., 2011; Ouchi et al., 2015): Dep: Arguments that have direct syntactic dependency on the predicate. Zero: Arguments referred to by zero pronouns within the same sentence that have no direct syntactic dependency on the predicate. Inter-Zero: Arguments referred to by zero pronouns outside of the same sentence. 1 Our source code is publicly available https://github.com/hiroki13/neural-pasa-system at For example, in Figure 1, the nominative argument “警察 (police)” for the predicate “逮捕した (arrested)” is regarded as a Dep argument, because the argument has a direct syntactic dependency on the predicate. By"
P17-1146,I11-1085,0,0.566161,"he lower edges denote case arguments. “NOM” and “ACC” denote the nominative and accusative arguments, respectively. “ϕi ” is a zero pronoun, referring to the antecedent “男 i (mani )”. Introduction Predicate argument structure (PAS) analysis is a basic semantic analysis task, in which systems are required to identify the semantic units of a sentence, such as who did what to whom. In prodrop languages such as Japanese, Chinese and Italian, arguments are often omitted in text, and such argument omission is regarded as one of the most problematic issues facing PAS analysis (Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Hangyo et al., 2013). In response to the argument omission problem, in Japanese PAS analysis, a joint model of the interactions between multiple predicates has been gaining popularity and achieved the state-ofthe-art results (Ouchi et al., 2015; Shibata et al., 2016). This approach is based on the linguistic intuition that the predicates in a sentence are semantically related to each other, and capturing this relation can be useful for PAS analysis. In the example sentence in Figure 1, the word “男 i (mani )” is the accusative argument of the predicate “逮捕し た (arrested)” and is shared by the"
P17-1146,P16-1117,0,0.604448,", in which systems are required to identify the semantic units of a sentence, such as who did what to whom. In prodrop languages such as Japanese, Chinese and Italian, arguments are often omitted in text, and such argument omission is regarded as one of the most problematic issues facing PAS analysis (Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Hangyo et al., 2013). In response to the argument omission problem, in Japanese PAS analysis, a joint model of the interactions between multiple predicates has been gaining popularity and achieved the state-ofthe-art results (Ouchi et al., 2015; Shibata et al., 2016). This approach is based on the linguistic intuition that the predicates in a sentence are semantically related to each other, and capturing this relation can be useful for PAS analysis. In the example sentence in Figure 1, the word “男 i (mani )” is the accusative argument of the predicate “逮捕し た (arrested)” and is shared by the other predicate “逃走した (escaped)” as its nominative argument. Considering the semantic relation between “逮捕 した (arrested)” and “逃走した (escaped)”, we intuitively know that the person arrested by someone is likely to be the escaper. That is, information about one predicate"
P17-1146,D08-1055,0,0.573656,"quence model, we use the softmax function to calculate the probability of the case labels of each word wt for each predicate pm : (L) ym,t = softmax(Wy hm,t ) (L) where hm,t is a hidden state vector calculated in the last grid layer. 5 Related Work 5.1 Japanese PAS Analysis Approaches Existing approaches to Japanese PAS analysis are divided into two categories: (i) the pointwise approach and (ii) the joint approach. The pointwise approach involves estimating the score of each argument candidate for one predicate, and then selecting the argument candidate with the maximum score as an argument (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida et al., 2016). The joint approach involves scoring all the predicateargument combinations in one sentence, and then selecting the combination with the highest score (Yoshikawa et al., 2011; Sasano and Kurohashi, 1595 2011; Ouchi et al., 2015; Shibata et al., 2016). Compared with the pointwise approach, the joint approach achieves better results. 5.2 Multi-Predicate Interactions Ouchi et al. (2015) reported that it is beneficial to Japanese PAS analysis to capture the interactions between all predicates in a sentence. This is based on the lin"
P17-1146,W07-1522,1,0.842901,"sociation for Computational Linguistics, pages 1591–1600 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1146 Figure 2: Overview of neural models: (i) single-sequence and (ii) multi-sequence models. In this paper, we first introduce a basic model that uses RNNs. This model independently estimates the arguments of each predicate without considering multi-predicate interactions (Sec. 3). Then, extending this model, we propose a neural model that uses Grid-RNNs (Sec. 4). Performing experiments on the NAIST Text Corpus (Iida et al., 2007), we demonstrate that even without syntactic information, our neural models outperform previous syntax-dependent models (Imamura et al., 2009; Ouchi et al., 2015). In particular, the neural model using Grid-RNNs achieved the best result. This suggests that the proposed grid-type neural architecture effectively captures multi-predicate interactions and contributes to performance improvements. 1 2 Japanese Predicate Argument Structure Analysis 2.1 Task Description In Japanese PAS analysis, arguments are identified that each fulfills one of the three major case roles, nominative (NOM), accusative"
P17-1146,1983.tc-1.13,0,0.102912,"Missing"
P17-1146,P11-1081,0,0.209368,"ndency relations, and the lower edges denote case arguments. “NOM” and “ACC” denote the nominative and accusative arguments, respectively. “ϕi ” is a zero pronoun, referring to the antecedent “男 i (mani )”. Introduction Predicate argument structure (PAS) analysis is a basic semantic analysis task, in which systems are required to identify the semantic units of a sentence, such as who did what to whom. In prodrop languages such as Japanese, Chinese and Italian, arguments are often omitted in text, and such argument omission is regarded as one of the most problematic issues facing PAS analysis (Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Hangyo et al., 2013). In response to the argument omission problem, in Japanese PAS analysis, a joint model of the interactions between multiple predicates has been gaining popularity and achieved the state-ofthe-art results (Ouchi et al., 2015; Shibata et al., 2016). This approach is based on the linguistic intuition that the predicates in a sentence are semantically related to each other, and capturing this relation can be useful for PAS analysis. In the example sentence in Figure 1, the word “男 i (mani )” is the accusative argument of the predicate “逮捕し た (arre"
P17-1146,D14-1041,0,0.0358225,"wa et al., 2011; Sasano and Kurohashi, 1595 2011; Ouchi et al., 2015; Shibata et al., 2016). Compared with the pointwise approach, the joint approach achieves better results. 5.2 Multi-Predicate Interactions Ouchi et al. (2015) reported that it is beneficial to Japanese PAS analysis to capture the interactions between all predicates in a sentence. This is based on the linguistic intuition that the predicates in a sentence are semantically related to each other, and that the information regarding this semantic relation can be useful for PAS analysis. Similarly, in semantic role labeling (SRL), Yang and Zong (2014) reported that their reranking model, which captures the multi-predicate interactions, is effective for the English constituentbased SRL task (Carreras and M`arquez, 2005). Taking this a step further, we propose a neural architecture that effectively models the multipredicate interactions. 5.3 Neural Approaches Japanese PAS In recent years, several attempts have been made to apply neural networks to Japanese PAS analysis (Shibata et al., 2016; Iida et al., 2016)4 . In Shibata et al. (2016), a feed-forward neural network is used for the score calculation part of the joint model proposed by Ouch"
P17-1146,D15-1260,0,0.527956,"Missing"
P17-1146,I11-1126,1,0.931219,"Missing"
P17-1146,D16-1132,0,0.564313,"lity of the case labels of each word wt for each predicate pm : (L) ym,t = softmax(Wy hm,t ) (L) where hm,t is a hidden state vector calculated in the last grid layer. 5 Related Work 5.1 Japanese PAS Analysis Approaches Existing approaches to Japanese PAS analysis are divided into two categories: (i) the pointwise approach and (ii) the joint approach. The pointwise approach involves estimating the score of each argument candidate for one predicate, and then selecting the argument candidate with the maximum score as an argument (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida et al., 2016). The joint approach involves scoring all the predicateargument combinations in one sentence, and then selecting the combination with the highest score (Yoshikawa et al., 2011; Sasano and Kurohashi, 1595 2011; Ouchi et al., 2015; Shibata et al., 2016). Compared with the pointwise approach, the joint approach achieves better results. 5.2 Multi-Predicate Interactions Ouchi et al. (2015) reported that it is beneficial to Japanese PAS analysis to capture the interactions between all predicates in a sentence. This is based on the linguistic intuition that the predicates in a sentence are semantical"
P17-1146,P15-1109,0,0.39401,"ds of information: (i) the context of the entire sentence, and (ii) multi-predicate interactions. For the former, we introduce single-sequence model that induces context-sensitive representations from a sequence of argument candidates of a predicate. For the latter, we introduce multisequence model that induces predicate-sensitive representations from multiple sequences of argument candidates of all predicates in a sentence (shown in Figure 2). 3 Single-Sequence Model The single-sequence model exploits stacked bidirectional RNNs (Bi-RNN) (Schuster and Paliwal, 1997; Graves et al., 2005, 2013; Zhou and Xu, 2015). Figure 3 shows the overall architecture, which consists of the following three components: Input Layer: Map each word to a feature vector representation. Figure 4: Example of feature extraction. The underlined word is the target predicate. From the sentence “彼女はパンを食べた。(She ate bread.)”, three types of features are extracted for the target predicate “食べた (ate)”. Figure 5: Example of the process of creating a feature vector. The extracted features are mapped to each vector, and all the vectors are concatenated into one feature vector. In the following subsections, we describe each of these thr"
P17-2068,N09-1037,0,0.252376,"e consistent with MWEs, by extending Kato et al. (2016)’s corpus 2 . As is the case with their corpus, each MME is a syntactic unit in an MWE-aware dependency structure from our corpus (Figure 1b). Moreover, our corpus includes not only functional MWEs but also NEs. Because NEs are highly productive and occur more frequently than functional MWEs, they are difficult to cover in a dictionary. To solve complex Natural Language Processing (NLP) tasks that require deep syntactic analysis, various levels of annotation such as parse trees and named entities (NEs) must be consistent with one another (Finkel and Manning, 2009). Otherwise, it is usually impossible to combine these pieces of information effectively. However, the standard syntactic corpus of English, Penn Treebank, is not concerned with consistency between syntactic trees and spans of multiword expressions (MWEs). In Penn Treebank, that is, an MWE-span does not always correspond to a span dominated by a single non-terminal node. Therefore, word-based dependency structures converted from Penn Treebank are generally inconsistent with MWE-spans (Figure 1a). To mitigate this inconsistency, Kato et al. (2016) estabConsistency between NE-spans and phrase st"
P17-2068,P13-2017,0,0.0725619,"Missing"
P17-2068,P15-1108,0,0.16997,"20-way jackknifing for the training split. The test split was automatically tagged by the sequential labeler trained on the training split. 12 When calculating UAS/LAS, we removed punctuation. 13 FUM only focuses on MWE-spans, whereas FTM focuses on both MWE-spans and MWE POS tags. 430 respect to functional MWEs. investigate a CFG-based model and a model based on tree-substitution grammars. Second, Candito and Constant (2014) compares several architectures for graph-based dependency parsing and MWE recognition, in which MWE recognition is conducted before, during, and after parsing. Finally, Nasr et al. (2015) explores a joint model of MWE recognition and dependency parsing. They focus on complex function words. In terms of data representation, they adopt one similar to ours, insofar as the components of an MWE are linked by dependency edges whose labels are MWEspecific. By adding MWE-specific features to the joint model, however, we observe at least a 2.52 / 3.00 point improvement in terms of UAS / LAS regarding the first tokens of MWEs, and a 2.90 / 2.99 point improvement regarding FUM / FTM. As a result, we obtain a 1.35 / 1.28 point improvement with joint(+pred span) compared with the pipeline"
P17-2068,J13-1009,0,0.0598946,"Missing"
P17-2068,L16-1263,1,0.904354,"ts over a pipeline model. 1 (a) a word-based dependency structure (b) an MWE-aware dependency structure Figure 1: A word-based and an MWE-aware dependency structure. In the former, a span of an MWE (“a number of”) does not correspond to any subtree. The MWE is represented as a single node in the latter structure. lishes each span of functional MWEs 1 as a subtree of a phrase structure in the Wall Street Journal portion of Ontonotes (Pradhan et al., 2007). Introduction To pursue this direction further, we construct a corpus such that dependency structures are consistent with MWEs, by extending Kato et al. (2016)’s corpus 2 . As is the case with their corpus, each MME is a syntactic unit in an MWE-aware dependency structure from our corpus (Figure 1b). Moreover, our corpus includes not only functional MWEs but also NEs. Because NEs are highly productive and occur more frequently than functional MWEs, they are difficult to cover in a dictionary. To solve complex Natural Language Processing (NLP) tasks that require deep syntactic analysis, various levels of annotation such as parse trees and named entities (NEs) must be consistent with one another (Finkel and Manning, 2009). Otherwise, it is usually imp"
P17-2068,schneider-etal-2014-comprehensive,0,0.0376648,"hing. 6 Acknowledgments Related Work This work was partially supported by JST CREST Grant Number JPMJCR1513 and JSPS KAKENHI Grant Number 15K16053. We are grateful to members of the Computational Linguistics Laboratory at NAIST, and to the anonymous reviewers for their valuable feedback. Regarding the preparation of a title list from English-language Wikipedia articles, we are particularly grateful for the assistance given by Motoki Sato. Whereas French Treebank is available for French MWEs (Abeill´e et al., 2003), there have been only limited corpora for English MWE-aware dependency parsing. Schneider et al. (2014) constructs an MWE-annotated corpus on English Web Treebank (Bies et al., 2012). However, this corpus is relatively small as training data for a parser, and its MWE annotations are not consistent with syntactic trees. By contrast, our corpus covers the whole of the WSJ portion of Ontonotes and ensures consistency between MWE annotations and parse trees. References Anne Abeill´e, Lionel Cl´ement, and Franc¸ois Toussenel. 2003. Building a Treebank for French, Springer Netherlands, Dordrecht, pages 165–187. Korkontzelos and Manandhar (2010) reports an improvement in base-phrase chunking by pregro"
P17-2068,N10-1089,0,0.0347379,"r English MWE-aware dependency parsing. Schneider et al. (2014) constructs an MWE-annotated corpus on English Web Treebank (Bies et al., 2012). However, this corpus is relatively small as training data for a parser, and its MWE annotations are not consistent with syntactic trees. By contrast, our corpus covers the whole of the WSJ portion of Ontonotes and ensures consistency between MWE annotations and parse trees. References Anne Abeill´e, Lionel Cl´ement, and Franc¸ois Toussenel. 2003. Building a Treebank for French, Springer Netherlands, Dordrecht, pages 165–187. Korkontzelos and Manandhar (2010) reports an improvement in base-phrase chunking by pregrouping MWEs as words-with-spaces. They focus on compound nouns, adjective-noun constructions, and named entities. However, they use gold MWE-spans, and this is not a realistic setting. By contrast, we use predicted MWE-spans. Ann Bies, Justin Mott, Colin Warner, and Seth Kulick. 2012. English web treebank. Technical Report LDC2012T13, Linguistic Data Consortium, Philadelphia, Pennsylvania, USA. . Marie Candito and Matthieu Constant. 2014. Strategies for contiguous multiword expression analysis and dependency parsing. In Proceedings of the"
P17-2068,N03-1033,0,0.102424,"red span) Dependency Parsing (First tokens of MWEs) Functional MWEs NEs UAS LAS UAS 78.89 64.01 85.58 71.28 65.05 85.07 79.93 73.70 85.79 81.31 74.74 85.89 MWE Recognition LAS 82.41 81.49 82.82 83.23 Functional MWEs FUM FTM 96.76 96.42 91.01 89.93 97.94 97.25 97.59 96.91 NEs FUM 89.81 88.47 90.16 91.32 Table 4: Breakdown of experimental results by type of MWE. Note that UAS / LAS are calculated regarding first tokens of MWEs. For NEs, the FTM is the same as the FUM because each NE always takes NNP as an MWE-level POS tag, and is not repeated. the POS tags predicted by the Stanford POS tagger (Toutanova et al., 2003) 10 . For the pipeline model and joint(+pred span), we used MWEspans and MWE POS tags predicted by CRF 11 . For dependency parsing, we used Redshift (Honnibal et al., 2013) for all models, with a beam size of 16 for decoding. For training, we removed non-projective dependency trees. For testing, we parsed all sentences. To evaluate parsing, we used unlabeled and labeled attachment scores (UAS/LAS) 12 . For the pipeline model, we converted each concatenated token corresponding to an MWE into a head-initial structure and compared this with the gold tree. For the joint model, we directly compared"
P17-2068,W08-1300,0,0.0796598,"Missing"
P17-2068,P11-2033,0,0.254436,"ONEY, QUANTITY, ORDINAL, and CARDINAL. Note that we only focus on multiword NEs. 5 https://catalog.ldc.upenn.edu/LDC2017T01 6 https://en.wiktionary.org 7 We do not require manual annotations for Case (A). 8 NEs have NNP as an MWE-level POS tag. Table 2: Histogram tabling the consistency between MWE-spans and phrase structures. 3 Although Kato et al. (2016) conducts experiments regarding MWE-aware dependency parsing, they use gold MWE-spans. This is not a realistic scenario. By contrast, our parsing models do not use gold MWE-spans. 428 baseline features and rich non-local features proposed by Zhang and Nivre (2011). 3.2 Joint Model In the proposed joint model, MWE-spans and MWE POS tags are encoded as dependency labels, and conventional word-based dependency parsing is performed by an arc-eager transitionbased parser. We use the same parsing features used in the pipeline model. We convert MWEs in MWE-aware dependency structures (Figure 1b) to head-initial structures (Figure 3) that encode MWE-spans and MWE POS tags. Note that this representation is similar to Universal Dependency (McDonald et al., 2013). When parsing, we use constraints based on a history of transitions and the dictionary of functional"
P18-4010,W16-4901,1,0.882811,"Missing"
P18-4010,Y13-2007,0,0.0184071,"gence Project (AIP) {liu.jun.lc3, shindo, matsu}@is.naist.jp Abstract ない (have to)”, “ことができる (be able to)”. Due to various meanings and usages of Japanese functional expressions, it is fairly difficult for JSL learners to learn them. In recent years, certain online Japanese learning systems are developed to support JSL learners, such as Reading Tutor2, Asunaro3, Rikai4, and WWWJDIC5. Some of these systems are particularly designed to enable JSL learners to read and write Japanese texts by offering the word information with their corresponding difficulty information or translation information (Ohno et al., 2013; Toyoda 2016). However, learners’ native language background has not been taken into account in these systems. Moreover, these systems provide learners with limited information about the various types of Japanese functional expressions, which learners actually intend to learn as a part of the procedure for learning Japanese. Therefore, developing a learning system that can assist JSL learners to learn Japanese functional expressions is crucial in Japanese education. In this paper, we present Jastudy, a computerassisted learning system, aiming at helping Chinese-speaking JSL learners with thei"
P19-1158,P17-2096,0,0.0603862,"thod ensures that the tokenization is consistent between training and evaluation, particularly for a sentence containing a low frequency phrase. 3.5 Embedding for Unfixed Vocabulary Since our model does not limit the vocabulary, there are many ways to tokenize a single sentence. To use token-level representations, we typically employ a lookup embedding mechanism, which requires a fixed vocabulary. In our model, however, the vocabulary changes as the language model is updated. We, therefore, introduce word embeddings with continuous cache inspired by (Grave et al., 2016; Kawakami et al., 2017; Cai et al., 2017). This method enables the proposed model to assign token-level representations to recently sampled tokens. Although embeddings of older tokens are discarded from the cache memory, we assume that meaningful tokens to solve the task appear frequently, and they remain in the cache during training if the size of the cache is large enough. By updating representations in the cache, the model can use token-level information adequately. In our embedding mechanism with a cache component, the model has a list Q that stores |Q |elements of recent tokenization history. The model also keeps a lookup table"
P19-1158,P06-1085,0,0.23266,"at our new tokenization strategy is effective on some classification tasks. 8 F1-score 80.31 80.41 78.95 81.71 80.46 https://www.rondhuit.com/download. html#ldcc 6 Related Work Our work is related to word segmentation for a neural network encoder. To tokenize a sentence into subwords without dictionary-based segmentation, BPE is commonly used in neural machine translation (NMT) (Sennrich et al., 2016). BPE forces a merger of tokens without any exceptions, and tokenization does not become natural. The problem associate with BPE has been addressed using a language model to tokenize a sentence. (Goldwater et al., 2006, 2009) proposed unsupervised word segmentation by sampling tokenization and updating a language model with Gibbs sampling. The language model for unsupervised word segmentation is smoothed with base probabilities of words to give a probability for all possible words in a text. (Mochihashi et al., 2009) extended this to the use of blocked Gibbs sampling, which samples tokenization by a sentence. The authors introduced a nested Bayesian language model that calculates a probability of a word by hierarchical language models. Recently, (Kudo and Richardson, 2018) proposed a subword generator for N"
P19-1158,P15-1162,0,0.0539002,"Missing"
P19-1158,E17-2068,0,0.0402014,"ow that our method achieves better performance than previous methods. 1 Introduction Tokenization is a fundamental problem in text classification such as sentiment analysis (Tang et al., 2014; Kim, 2014; dos Santos and Gatti, 2014), topic detection (Lai et al., 2015; Zhang et al., 2015), and spam detection (Liu and Jia, 2012; Liu et al., 2016). In text classification with neural networks, sentence representation is calculated based on tokens that compose the sentence. Specifically, a sentence is first tokenized into meaningful units such as characters, words, and subwords (Zhang et al., 2015; Joulin et al., 2017). Then, the token embeddings are looked up and fed into a neural network encoder such as a feed-forward neural network (Iyyer et al., 2015), a convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014), or a long short-term memory (LSTM) network (Wang et al., 2016a,b). For English and other languages that use the Latin alphabet, the whitespace is a good indicator of word segmentation. However, tokenization is a non-trivial problem in unsegmented languages such as Chinese and Japanese since they have no explicit word boundaries. For these languages, tokenizers based on supervised"
P19-1158,P14-1062,0,0.0175508,"ntos and Gatti, 2014), topic detection (Lai et al., 2015; Zhang et al., 2015), and spam detection (Liu and Jia, 2012; Liu et al., 2016). In text classification with neural networks, sentence representation is calculated based on tokens that compose the sentence. Specifically, a sentence is first tokenized into meaningful units such as characters, words, and subwords (Zhang et al., 2015; Joulin et al., 2017). Then, the token embeddings are looked up and fed into a neural network encoder such as a feed-forward neural network (Iyyer et al., 2015), a convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014), or a long short-term memory (LSTM) network (Wang et al., 2016a,b). For English and other languages that use the Latin alphabet, the whitespace is a good indicator of word segmentation. However, tokenization is a non-trivial problem in unsegmented languages such as Chinese and Japanese since they have no explicit word boundaries. For these languages, tokenizers based on supervised machine learning with a dictionary (Zhang et al., 2003; Kudo, 2006) have been used to segment a sentence into units (Lai et al., 2015). In addition, we use a neural network-based word segmenter to tokenize a raw cor"
P19-1158,P17-1137,0,0.235461,"phase. This updating method ensures that the tokenization is consistent between training and evaluation, particularly for a sentence containing a low frequency phrase. 3.5 Embedding for Unfixed Vocabulary Since our model does not limit the vocabulary, there are many ways to tokenize a single sentence. To use token-level representations, we typically employ a lookup embedding mechanism, which requires a fixed vocabulary. In our model, however, the vocabulary changes as the language model is updated. We, therefore, introduce word embeddings with continuous cache inspired by (Grave et al., 2016; Kawakami et al., 2017; Cai et al., 2017). This method enables the proposed model to assign token-level representations to recently sampled tokens. Although embeddings of older tokens are discarded from the cache memory, we assume that meaningful tokens to solve the task appear frequently, and they remain in the cache during training if the size of the cache is large enough. By updating representations in the cache, the model can use token-level information adequately. In our embedding mechanism with a cache component, the model has a list Q that stores |Q |elements of recent tokenization history. The model also ke"
P19-1158,D14-1181,0,0.00650062,"r model incorporates a language model for unsupervised tokenization into a text classifier and then trains both models simultaneously. To make the model robust against infrequent tokens, we sampled segmentation for each sentence stochastically during training, which resulted in improved performance of text classification. We conducted experiments on sentiment analysis as a text classification task and show that our method achieves better performance than previous methods. 1 Introduction Tokenization is a fundamental problem in text classification such as sentiment analysis (Tang et al., 2014; Kim, 2014; dos Santos and Gatti, 2014), topic detection (Lai et al., 2015; Zhang et al., 2015), and spam detection (Liu and Jia, 2012; Liu et al., 2016). In text classification with neural networks, sentence representation is calculated based on tokens that compose the sentence. Specifically, a sentence is first tokenized into meaningful units such as characters, words, and subwords (Zhang et al., 2015; Joulin et al., 2017). Then, the token embeddings are looked up and fed into a neural network encoder such as a feed-forward neural network (Iyyer et al., 2015), a convolutional neural network (CNN) (Kim"
P19-1158,P18-1007,0,0.401623,"re frequently when sampling. We use a dictionary-based morphological analyzer or unsupervised word segmentation to tokenize a corpus initially, and the language model is initialized with the tokenized corpus. 3.3 Nested Unigram Language Model pbase (t : c1 ...cM ) = puni (c1 ) is given as Sampling Tokenization With the nested unigram language model introduced above, the tokenization of a sentence is sampled from the distribution P (t|s) where t is possible tokenization for the sentence. A probability of tokenization is obtained Q by a nested language model (4) as p(t|s) = t∈t p(t). Following (Kudo, 2018) and (Mochihashi et al., 2009), we employ a dynamic programming (DP) technique called forward filtering backward sampling (FFBS) (Scott, 2002) to sample tokens stochastically. With FFBS, we can sample tokens in a sentence from a distribution considering all possible tokenizations within the limit of the maximum token length l. In the forward calculation of FFBS, a DP Table D is calculated as follows: pbi (cm |cm−1 ) min(i−j,l) m=2 (5) To deal with a token that includes an unknown character, both puni (cm ) and pbi (cm |cm−1 ) are also calculated by a smoothed language model. A smoothed charact"
P19-1158,D18-2012,0,0.029369,"anguage model to tokenize a sentence. (Goldwater et al., 2006, 2009) proposed unsupervised word segmentation by sampling tokenization and updating a language model with Gibbs sampling. The language model for unsupervised word segmentation is smoothed with base probabilities of words to give a probability for all possible words in a text. (Mochihashi et al., 2009) extended this to the use of blocked Gibbs sampling, which samples tokenization by a sentence. The authors introduced a nested Bayesian language model that calculates a probability of a word by hierarchical language models. Recently, (Kudo and Richardson, 2018) proposed a subword generator for NMT, which tokenizes a sentence stochastically with a subwordlevel language model while (Kudo, 2018) reports improvement in performance of NMT by the idea 1627 of sampling tokenization. Considering multiple subwords makes an NMT model robust against noise and segmentation errors. This differs from BPE in that it does not merge tokens uniquely by its frequency and differs from unsupervised word segmentation with a language model in that it limits subword vocabulary. Our work is similar to this line of research, but we focus on NLP tasks that do not require deco"
P19-1158,P09-1012,0,0.904122,"n sampling. We use a dictionary-based morphological analyzer or unsupervised word segmentation to tokenize a corpus initially, and the language model is initialized with the tokenized corpus. 3.3 Nested Unigram Language Model pbase (t : c1 ...cM ) = puni (c1 ) is given as Sampling Tokenization With the nested unigram language model introduced above, the tokenization of a sentence is sampled from the distribution P (t|s) where t is possible tokenization for the sentence. A probability of tokenization is obtained Q by a nested language model (4) as p(t|s) = t∈t p(t). Following (Kudo, 2018) and (Mochihashi et al., 2009), we employ a dynamic programming (DP) technique called forward filtering backward sampling (FFBS) (Scott, 2002) to sample tokens stochastically. With FFBS, we can sample tokens in a sentence from a distribution considering all possible tokenizations within the limit of the maximum token length l. In the forward calculation of FFBS, a DP Table D is calculated as follows: pbi (cm |cm−1 ) min(i−j,l) m=2 (5) To deal with a token that includes an unknown character, both puni (cm ) and pbi (cm |cm−1 ) are also calculated by a smoothed language model. A smoothed character unigram probability puni (c"
P19-1158,C14-1008,0,0.0476734,"rates a language model for unsupervised tokenization into a text classifier and then trains both models simultaneously. To make the model robust against infrequent tokens, we sampled segmentation for each sentence stochastically during training, which resulted in improved performance of text classification. We conducted experiments on sentiment analysis as a text classification task and show that our method achieves better performance than previous methods. 1 Introduction Tokenization is a fundamental problem in text classification such as sentiment analysis (Tang et al., 2014; Kim, 2014; dos Santos and Gatti, 2014), topic detection (Lai et al., 2015; Zhang et al., 2015), and spam detection (Liu and Jia, 2012; Liu et al., 2016). In text classification with neural networks, sentence representation is calculated based on tokens that compose the sentence. Specifically, a sentence is first tokenized into meaningful units such as characters, words, and subwords (Zhang et al., 2015; Joulin et al., 2017). Then, the token embeddings are looked up and fed into a neural network encoder such as a feed-forward neural network (Iyyer et al., 2015), a convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al.,"
P19-1158,P16-1162,0,0.458161,"trivial problem in unsegmented languages such as Chinese and Japanese since they have no explicit word boundaries. For these languages, tokenizers based on supervised machine learning with a dictionary (Zhang et al., 2003; Kudo, 2006) have been used to segment a sentence into units (Lai et al., 2015). In addition, we use a neural network-based word segmenter to tokenize a raw corpus in Chinese text classification (Zhou et al., 2016; Zhang and Yang, 2018). In machine translation, subword tokenization with byte pair encoding (BPE) addresses the problem of unknown words and improves performance (Sennrich et al., 2016). However, segmentation is potentially ambiguous, and it is unclear whether preset tokenization offers the best performance for target tasks. To address this problem, in this paper, we propose a new tokenization strategy that segments a sentence stochastically and trains a classification model with various segmentations. During training, our model first segments sentences into tokens stochastically with the language model and then feeds the tokenized sentences into a neural text classifier. The text classifier is trained to decrease the cross-entropy loss for true labels, and the language mode"
P19-1158,P14-1146,0,0.0408455,"these problems. Our model incorporates a language model for unsupervised tokenization into a text classifier and then trains both models simultaneously. To make the model robust against infrequent tokens, we sampled segmentation for each sentence stochastically during training, which resulted in improved performance of text classification. We conducted experiments on sentiment analysis as a text classification task and show that our method achieves better performance than previous methods. 1 Introduction Tokenization is a fundamental problem in text classification such as sentiment analysis (Tang et al., 2014; Kim, 2014; dos Santos and Gatti, 2014), topic detection (Lai et al., 2015; Zhang et al., 2015), and spam detection (Liu and Jia, 2012; Liu et al., 2016). In text classification with neural networks, sentence representation is calculated based on tokens that compose the sentence. Specifically, a sentence is first tokenized into meaningful units such as characters, words, and subwords (Zhang et al., 2015; Joulin et al., 2017). Then, the token embeddings are looked up and fed into a neural network encoder such as a feed-forward neural network (Iyyer et al., 2015), a convolutional neural network"
P19-1158,P16-2037,0,0.0131871,"2015), and spam detection (Liu and Jia, 2012; Liu et al., 2016). In text classification with neural networks, sentence representation is calculated based on tokens that compose the sentence. Specifically, a sentence is first tokenized into meaningful units such as characters, words, and subwords (Zhang et al., 2015; Joulin et al., 2017). Then, the token embeddings are looked up and fed into a neural network encoder such as a feed-forward neural network (Iyyer et al., 2015), a convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014), or a long short-term memory (LSTM) network (Wang et al., 2016a,b). For English and other languages that use the Latin alphabet, the whitespace is a good indicator of word segmentation. However, tokenization is a non-trivial problem in unsegmented languages such as Chinese and Japanese since they have no explicit word boundaries. For these languages, tokenizers based on supervised machine learning with a dictionary (Zhang et al., 2003; Kudo, 2006) have been used to segment a sentence into units (Lai et al., 2015). In addition, we use a neural network-based word segmenter to tokenize a raw corpus in Chinese text classification (Zhou et al., 2016; Zhang an"
P19-1158,D16-1058,0,0.0976317,"Missing"
P19-1158,W03-1730,0,0.0873188,"ed up and fed into a neural network encoder such as a feed-forward neural network (Iyyer et al., 2015), a convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014), or a long short-term memory (LSTM) network (Wang et al., 2016a,b). For English and other languages that use the Latin alphabet, the whitespace is a good indicator of word segmentation. However, tokenization is a non-trivial problem in unsegmented languages such as Chinese and Japanese since they have no explicit word boundaries. For these languages, tokenizers based on supervised machine learning with a dictionary (Zhang et al., 2003; Kudo, 2006) have been used to segment a sentence into units (Lai et al., 2015). In addition, we use a neural network-based word segmenter to tokenize a raw corpus in Chinese text classification (Zhou et al., 2016; Zhang and Yang, 2018). In machine translation, subword tokenization with byte pair encoding (BPE) addresses the problem of unknown words and improves performance (Sennrich et al., 2016). However, segmentation is potentially ambiguous, and it is unclear whether preset tokenization offers the best performance for target tasks. To address this problem, in this paper, we propose a new"
P19-1158,P18-1144,0,0.213352,"l., 2016a,b). For English and other languages that use the Latin alphabet, the whitespace is a good indicator of word segmentation. However, tokenization is a non-trivial problem in unsegmented languages such as Chinese and Japanese since they have no explicit word boundaries. For these languages, tokenizers based on supervised machine learning with a dictionary (Zhang et al., 2003; Kudo, 2006) have been used to segment a sentence into units (Lai et al., 2015). In addition, we use a neural network-based word segmenter to tokenize a raw corpus in Chinese text classification (Zhou et al., 2016; Zhang and Yang, 2018). In machine translation, subword tokenization with byte pair encoding (BPE) addresses the problem of unknown words and improves performance (Sennrich et al., 2016). However, segmentation is potentially ambiguous, and it is unclear whether preset tokenization offers the best performance for target tasks. To address this problem, in this paper, we propose a new tokenization strategy that segments a sentence stochastically and trains a classification model with various segmentations. During training, our model first segments sentences into tokens stochastically with the language model and then f"
Q17-1028,L16-1532,0,0.048984,"Missing"
Q17-1028,Q15-1011,0,0.56112,"in various NLP tasks such as information extraction and semantic search. The task is challenging because of the ambiguity in the meaning of entity mentions (e.g., “Washington” can refer to the state, the capital of the US, the first US president George Washington, and so forth). The key to improve the performance of EL is to accurately model the semantic context of entity mentions. Because our model learns the likelihood of an entity appearance in a given text, it can naturally be used for modeling the context of EL. 3.2.1 Setup Our experimental setup follows the setup described in past work (Chisholm and Hachey, 2015; He et al., 2013; Yamada et al., 2016). We use two standard datasets: the CoNLL dataset and the TAC 2010 dataset. The CoNLL dataset, which was proposed in Hoffart et al. (2011), includes training, development, and test sets consisting of 946, 216, and 231 documents, respectively. We use the training set to train our EL method, and the test set for measuring the performance of our method. We report the standard micro- (aggregates over all mentions) and macro- (aggregates over all documents) accuracies of the top-ranked candidate entities. The TAC 2010 dataset is another dataset constructed for"
Q17-1028,D07-1074,0,0.0472079,"Missing"
Q17-1028,K16-1026,0,0.0126085,"uestion answering (QA). In both tasks, we adopt a simple multi-layer perceptron (MLP) classifier with the learned representations as features. We tested our method using two standard datasets (i.e., CoNLL 2003 and TAC 2010) for the EL task and a popular factoid QA dataset based on the quiz bowl quiz game for the factoid QA task. As a result, our method outperformed recent state-of-the-art methods on both the EL and the factoid QA tasks. Additionally, there have also been proposed methods that map words and entities into the same continuous vector space (Wang et al., 2014; Yamada et al., 2016; Fang et al., 2016). Our work differs from these works because we aim to map texts (i.e., sentences and paragraphs) and entities into the same vector space. Our contributions are summarized as follows: • We propose a neural network model that jointly learns vector representations of texts and KB entities. We train the model using a large amount of entity annotations extracted directly from Wikipedia. • We demonstrate that our proposed representations are surprisingly effective for various NLP tasks. In particular, we apply the proposed model to three different NLP tasks, namely semantic textual similarity, entit"
Q17-1028,N13-1092,0,0.0206937,"Missing"
Q17-1028,P16-1059,0,0.314972,"the number of units in the hidden layers, and dropout is the dropout probability. • He (He et al., 2013) proposed a method for learning the representations of mention contexts and entities from KB using the stacked denoising auto-encoders. These representations were then used to address EL. • Chisholm (Chisholm and Hachey, 2015) used a support vector machine (SVM) with various features derived from KB and a Wikilinks dataset (Singh et al., 2012). • Pershina (Pershina et al., 2015) improved EL by modeling coherence using the personalized page rank algorithm. NTEE NTEE (w/o strsim) • Globerson (Globerson et al., 2016) improved the coherence model for EL by introducing an attention mechanism in order to focus only on strong relations of entities. Fixed NTEE SG-proj SG-proj-dbp Hoffart (2011) He (2013) • Yamada (Yamada et al., 2016) proposed a model for learning the joint distributed representations of words and KB entities from KB, and addressed EL using context models based on the representations. 3.2.4 Results Table 4 compares the results of our method with those obtained with the state-of-the-art methods. Our method achieved strong results on both the CoNLL and the TAC 2010 datasets. In particular, the N"
Q17-1028,D13-1029,0,0.0227696,"from the word-based text representation to the entity-based text representation. We consider that the reason why the fixed NTEE model performed well among datasets is that the entity-based text representations are more semantic (less syntactic) and contain less noise than the word-based text representations, thus are much more suitable for addressing this task. 7 We augment the corpus simply by appending the texts in DBpedia abstract corpus to the Wikipedia corpus. 401 Entity Linking Entity Linking (EL) (Cucerzan, 2007; Mihalcea and Csomai, 2007; Milne and Witten, 2008; Ratinov et al., 2011; Hajishirzi et al., 2013; Ling et al., 2015) is the task of resolving ambiguous mentions of entities to their referent entities in KB. EL has recently received considerable attention because of its effectiveness in various NLP tasks such as information extraction and semantic search. The task is challenging because of the ambiguity in the meaning of entity mentions (e.g., “Washington” can refer to the state, the capital of the US, the first US president George Washington, and so forth). The key to improve the performance of EL is to accurately model the semantic context of entity mentions. Because our model learns th"
Q17-1028,P13-2006,0,0.522088,"s information extraction and semantic search. The task is challenging because of the ambiguity in the meaning of entity mentions (e.g., “Washington” can refer to the state, the capital of the US, the first US president George Washington, and so forth). The key to improve the performance of EL is to accurately model the semantic context of entity mentions. Because our model learns the likelihood of an entity appearance in a given text, it can naturally be used for modeling the context of EL. 3.2.1 Setup Our experimental setup follows the setup described in past work (Chisholm and Hachey, 2015; He et al., 2013; Yamada et al., 2016). We use two standard datasets: the CoNLL dataset and the TAC 2010 dataset. The CoNLL dataset, which was proposed in Hoffart et al. (2011), includes training, development, and test sets consisting of 946, 216, and 231 documents, respectively. We use the training set to train our EL method, and the test set for measuring the performance of our method. We report the standard micro- (aggregates over all mentions) and macro- (aggregates over all documents) accuracies of the top-ranked candidate entities. The TAC 2010 dataset is another dataset constructed for the Text Analysi"
Q17-1028,N16-1162,0,0.0201566,"Missing"
Q17-1028,D11-1072,0,0.749426,"Missing"
Q17-1028,P15-1125,0,0.0781973,"ng et al., 2016; Hill et al., 2016b; Kenter et al., 2016). These methods aim to learn generic representations that are useful across domains similar to word embedding methods such as Word2vec (Mikolov et al., 2013b) and GloVe (Pennington et al., 2014). Another interesting approach is learning distributed representations of entities in a knowledge 1 https://github.com/studio-ousia/ntee base (KB) such as Wikipedia and Freebase. These methods encode information of entities in the KB into a continuous vector space. They are shown to be effective for various KB-related tasks such as entity search (Hu et al., 2015), entity linking (Hu et al., 2015; Yamada et al., 2016), and link prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015). In this paper, we describe a novel method to bridge these two different approaches. In particular, we propose Neural Text-Entity Encoder (NTEE), a neural network model to jointly learn distributed representations of texts (i.e., sentences and paragraphs) and KB entities. For every text in the KB, our model aims to predict its relevant entities, and places the text and the relevant entities close to each other in a continuous vector space. We use humanedited e"
Q17-1028,D14-1070,0,0.0360543,"Missing"
Q17-1028,P15-1162,0,0.0152441,"Missing"
Q17-1028,P16-1089,0,0.04729,"Missing"
Q17-1028,P14-2050,0,0.01562,"ructured linguistic resources. For instance, Wieting et al. (2016) trained their model using a large number of noisy phrase pairs retrieved from the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Hill et al. (2016b) use several public dictionaries to train the model by mapping definition texts in a dictionary to representations of the words explained by these texts. To our knowledge, our work is the first work to learn generic text representations with the supervision of entity annotations. Several methods have also been proposed for extending the word embedding methods. For example, Levy and Goldberg (2014) proposed a method to train word embedding with dependency-based conWord her dry tennis spanish moon Our model she (0.65) to (0.41) and (0.40) his (0.40) in (0.39) wet (0.48) arid (0.46) moisture (0.44) grows (0.44) dried (0.43) doubles (0.86) atp (0.79) wimbledon (0.78) wta (0.75) slam (0.74) spain (0.76) madrid (0.70) andalusia (0.64) valencia (0.61) seville (0.60) lunar (0.78) crater (0.66) rim (0.66) craters (0.65) midpoint (0.59) Skip-gram she (0.86) his (0.77) herself (0.71) him (0.66) mother (0.64) wet (0.81) moist (0.73) drier (0.72) drying (0.70) moister (0.69) badminton (0.75) hardco"
Q17-1028,P15-1107,0,0.0179797,"sks (i.e., sentence textual similarity, entity linking, and factoid question answering) involving both unsupervised and supervised settings. As a result, we achieved state-of-the-art results on all three of these tasks. Our code and trained models are publicly available for further academic research.1 1 Introduction Methods capable of learning distributed representations of arbitrary-length texts (i.e., fixed-length continuous vectors that encode the semantics of texts), such as sentences and paragraphs, have recently attracted considerable attention (Le and Mikolov, 2014; Kiros et al., 2015; Li et al., 2015; Wieting et al., 2016; Hill et al., 2016b; Kenter et al., 2016). These methods aim to learn generic representations that are useful across domains similar to word embedding methods such as Word2vec (Mikolov et al., 2013b) and GloVe (Pennington et al., 2014). Another interesting approach is learning distributed representations of entities in a knowledge 1 https://github.com/studio-ousia/ntee base (KB) such as Wikipedia and Freebase. These methods encode information of entities in the KB into a continuous vector space. They are shown to be effective for various KB-related tasks such as entity s"
Q17-1028,C16-1252,0,0.0161271,"tasks in this paper. Moreover, we also showed that the joint embedding of texts and entities can be applied not only to EL but also for wider applications such as semantic textual similarity and factoid QA. 5 Table 6: Examples of top five similar words with their cosine similarities in our learned word representations compared with those of the skip-gram model. texts, and Luan et al. (2016) used semantic role labeling for generating contexts to train word embedding. Moreover, a few recent studies on learning entity embedding based on word embedding methods have been reported (Hu et al., 2015; Li et al., 2016). These models are typically based on the skip-gram model and directly model the semantic relatedness between KB entities. Our work differs from these studies because we aim to learn representations of arbitrary-length texts in addition to entities. Another related approach is the relational embedding (or knowledge embedding) (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015), which encodes entities as continuous vectors and relations as some operations on the vector space, such as vector addition. These models typically learn representations from large KB graphs consisting of entities"
Q17-1028,Q15-1023,0,0.125699,"representation to the entity-based text representation. We consider that the reason why the fixed NTEE model performed well among datasets is that the entity-based text representations are more semantic (less syntactic) and contain less noise than the word-based text representations, thus are much more suitable for addressing this task. 7 We augment the corpus simply by appending the texts in DBpedia abstract corpus to the Wikipedia corpus. 401 Entity Linking Entity Linking (EL) (Cucerzan, 2007; Mihalcea and Csomai, 2007; Milne and Witten, 2008; Ratinov et al., 2011; Hajishirzi et al., 2013; Ling et al., 2015) is the task of resolving ambiguous mentions of entities to their referent entities in KB. EL has recently received considerable attention because of its effectiveness in various NLP tasks such as information extraction and semantic search. The task is challenging because of the ambiguity in the meaning of entity mentions (e.g., “Washington” can refer to the state, the capital of the US, the first US president George Washington, and so forth). The key to improve the performance of EL is to accurately model the semantic context of entity mentions. Because our model learns the likelihood of an e"
Q17-1028,P16-2020,0,0.0163299,"p-gram model and applied it to EL. Our method differs from their method in that their method does not directly model arbitrary-length texts (i.e., paragraphs and sentences), which we proved to be highly effective for various tasks in this paper. Moreover, we also showed that the joint embedding of texts and entities can be applied not only to EL but also for wider applications such as semantic textual similarity and factoid QA. 5 Table 6: Examples of top five similar words with their cosine similarities in our learned word representations compared with those of the skip-gram model. texts, and Luan et al. (2016) used semantic role labeling for generating contexts to train word embedding. Moreover, a few recent studies on learning entity embedding based on word embedding methods have been reported (Hu et al., 2015; Li et al., 2016). These models are typically based on the skip-gram model and directly model the semantic relatedness between KB entities. Our work differs from these studies because we aim to learn representations of arbitrary-length texts in addition to entities. Another related approach is the relational embedding (or knowledge embedding) (Bordes et al., 2013; Wang et al., 2014; Lin et a"
Q17-1028,marelli-etal-2014-sick,0,0.0168365,"human judgments of the semantic similarity between two sentence pairs. The task has been used as a standard method to evaluate the quality of distributed representations of sentences in past work (Kiros et al., 2015; Hill et al., 2016a; Kenter et al., 2016). 3.1.1 Setup Our experimental setup follows that of a previously published experiment (Hill et al., 2016a). We use two standard datasets: (1) the STS 2014 dataset (Agirre et al., 2014) consisting of 3,750 sentence pairs and human ratings from six different sources (e.g., newswire, web forums, dictionary glosses), and (2) the SICK dataset (Marelli et al., 2014) consisting of 10,000 pairs of sentences and human ratings. In both datasets, the ratings take values between 1 and 5, where a rating of 1 indicates that the sentence pair is not related, and a rating of 5 means that they are highly related. All sentence pairs except the 500 SICK trial pairs were used for our experiments. We train our model by experimenting with both paragraphs and sentences. Further, we introduce another training setting (denoted by fixed NTEE), where the parameters in the word representations and the entity representations are fixed throughout the training. We compute the co"
Q17-1028,P08-1028,0,0.0496219,"ectors of the two sentences in each sentence pair (de400 rived using Eq. (2)) and measure the Pearson’s r and Spearman’s p correlations between these distances and the gold-standard human ratings. Additionally, we use Pearson’s r as our primal score. 3.1.2 Baselines For baselines for this experiment, we selected the following four recent state-of-the-art models. Brief descriptions of these models are as follows: • Word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b) is a popular word embedding model. We compute a sentence representation by element-wise addition of the vectors of its words (Mitchell and Lapata, 2008). We add its skip-gram and CBOW models to our baselines. We train the model with the hyper-parameters and the Wikipedia corpus explained in Section 2.2. Thus, the skip-gram model is equivalent to the pre-trained representations used in our model. Furthermore, in order to conduct a fair comparison between the skip-gram model and our model, we also add skip-gram (plain), which is a skip-gram model trained using a different corpus. In particular, the corpus is augmented using the texts in DBpedia abstract corpus7 , and its entity annotations are treated as regular text phrases (not replaced to th"
Q17-1028,D14-1162,0,0.115375,"re publicly available for further academic research.1 1 Introduction Methods capable of learning distributed representations of arbitrary-length texts (i.e., fixed-length continuous vectors that encode the semantics of texts), such as sentences and paragraphs, have recently attracted considerable attention (Le and Mikolov, 2014; Kiros et al., 2015; Li et al., 2015; Wieting et al., 2016; Hill et al., 2016b; Kenter et al., 2016). These methods aim to learn generic representations that are useful across domains similar to word embedding methods such as Word2vec (Mikolov et al., 2013b) and GloVe (Pennington et al., 2014). Another interesting approach is learning distributed representations of entities in a knowledge 1 https://github.com/studio-ousia/ntee base (KB) such as Wikipedia and Freebase. These methods encode information of entities in the KB into a continuous vector space. They are shown to be effective for various KB-related tasks such as entity search (Hu et al., 2015), entity linking (Hu et al., 2015; Yamada et al., 2016), and link prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015). In this paper, we describe a novel method to bridge these two different approaches. In particular,"
Q17-1028,P11-1138,0,0.258618,"affine transformation from the word-based text representation to the entity-based text representation. We consider that the reason why the fixed NTEE model performed well among datasets is that the entity-based text representations are more semantic (less syntactic) and contain less noise than the word-based text representations, thus are much more suitable for addressing this task. 7 We augment the corpus simply by appending the texts in DBpedia abstract corpus to the Wikipedia corpus. 401 Entity Linking Entity Linking (EL) (Cucerzan, 2007; Mihalcea and Csomai, 2007; Milne and Witten, 2008; Ratinov et al., 2011; Hajishirzi et al., 2013; Ling et al., 2015) is the task of resolving ambiguous mentions of entities to their referent entities in KB. EL has recently received considerable attention because of its effectiveness in various NLP tasks such as information extraction and semantic search. The task is challenging because of the ambiguity in the meaning of entity mentions (e.g., “Washington” can refer to the state, the capital of the US, the first US president George Washington, and so forth). The key to improve the performance of EL is to accurately model the semantic context of entity mentions. Be"
Q17-1028,N13-1008,0,0.0352396,"am model and directly model the semantic relatedness between KB entities. Our work differs from these studies because we aim to learn representations of arbitrary-length texts in addition to entities. Another related approach is the relational embedding (or knowledge embedding) (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015), which encodes entities as continuous vectors and relations as some operations on the vector space, such as vector addition. These models typically learn representations from large KB graphs consisting of entities and relations. Similarly, the universal schema (Riedel et al., 2013; Toutanova et al., 2015; Verga et al., 2016) 407 Conclusions In this paper, we presented a novel model capable of jointly learning distributed representations of texts and entities from a large number of entity annotations in Wikipedia. Our aim was to construct the proposed general-purpose model such that it enables practitioners to address various NLP tasks with ease. We achieved state-of-the-art results on three important NLP tasks (i.e., semantic textual similarity, entity linking, and factoid question answering), which clearly demonstrated the effectiveness of our model. Furthermore, the"
Q17-1028,D15-1174,0,0.0405,"Missing"
Q17-1028,N16-1103,0,0.0190976,"tedness between KB entities. Our work differs from these studies because we aim to learn representations of arbitrary-length texts in addition to entities. Another related approach is the relational embedding (or knowledge embedding) (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015), which encodes entities as continuous vectors and relations as some operations on the vector space, such as vector addition. These models typically learn representations from large KB graphs consisting of entities and relations. Similarly, the universal schema (Riedel et al., 2013; Toutanova et al., 2015; Verga et al., 2016) 407 Conclusions In this paper, we presented a novel model capable of jointly learning distributed representations of texts and entities from a large number of entity annotations in Wikipedia. Our aim was to construct the proposed general-purpose model such that it enables practitioners to address various NLP tasks with ease. We achieved state-of-the-art results on three important NLP tasks (i.e., semantic textual similarity, entity linking, and factoid question answering), which clearly demonstrated the effectiveness of our model. Furthermore, the qualitative analysis showed that the characte"
Q17-1028,D14-1167,0,0.583219,"are useful across domains similar to word embedding methods such as Word2vec (Mikolov et al., 2013b) and GloVe (Pennington et al., 2014). Another interesting approach is learning distributed representations of entities in a knowledge 1 https://github.com/studio-ousia/ntee base (KB) such as Wikipedia and Freebase. These methods encode information of entities in the KB into a continuous vector space. They are shown to be effective for various KB-related tasks such as entity search (Hu et al., 2015), entity linking (Hu et al., 2015; Yamada et al., 2016), and link prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015). In this paper, we describe a novel method to bridge these two different approaches. In particular, we propose Neural Text-Entity Encoder (NTEE), a neural network model to jointly learn distributed representations of texts (i.e., sentences and paragraphs) and KB entities. For every text in the KB, our model aims to predict its relevant entities, and places the text and the relevant entities close to each other in a continuous vector space. We use humanedited entity annotations obtained from Wikipedia (see Table 1) as supervised data of relevant entities to the texts contain"
Q17-1028,K16-1025,1,0.410039,"2016). These methods aim to learn generic representations that are useful across domains similar to word embedding methods such as Word2vec (Mikolov et al., 2013b) and GloVe (Pennington et al., 2014). Another interesting approach is learning distributed representations of entities in a knowledge 1 https://github.com/studio-ousia/ntee base (KB) such as Wikipedia and Freebase. These methods encode information of entities in the KB into a continuous vector space. They are shown to be effective for various KB-related tasks such as entity search (Hu et al., 2015), entity linking (Hu et al., 2015; Yamada et al., 2016), and link prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015). In this paper, we describe a novel method to bridge these two different approaches. In particular, we propose Neural Text-Entity Encoder (NTEE), a neural network model to jointly learn distributed representations of texts (i.e., sentences and paragraphs) and KB entities. For every text in the KB, our model aims to predict its relevant entities, and places the text and the relevant entities close to each other in a continuous vector space. We use humanedited entity annotations obtained from Wikipedia (see Table 1)"
Q17-1028,Q16-1002,0,\N,Missing
S10-1086,D07-1050,1,0.257026,"D) task (Task 16 (Okumura et al., 2010)), which has two new characteristics: (1) Both training and test data across 3 or 4 domains. The training data include books or magazines (called PB), newspaper articles (PN), and white papers (OW). The test data also include documents from a Q&A site on the WWW (OC); (2) Test data include new senses (called X) that are not defined in dictionary. There is much previous research on WSD. In the case of Japanese, unsupervised approaches such as extended Lesk have performed well (Baldwin et al., 2010), although they are outperformed by supervised approaches (Tanaka et al., 2007; Murata et al., 2003). Therefore, we selected a supervised approach and constructed Support Vector Machines (SVM) and Maximum Entropy (MEM) classifiers using common features and topic features. We performed extensive experiments to investigate the best combinations of domains for training. We describe the data in Section 2, and our system in Section 3. Then in Section 4, we show the results and provide some discussion. 2.2 Data Pre-processing We performed two preliminary pre-processing steps. First, we restored the base forms because the given training and test data have no information about"
S10-1086,W10-4001,0,0.0904494,"data. We participated in the SemEval-2010 Japanese Word Sense Disambiguation (WSD) task (Task 16 (Okumura et al., 2010)), which has two new characteristics: (1) Both training and test data across 3 or 4 domains. The training data include books or magazines (called PB), newspaper articles (PN), and white papers (OW). The test data also include documents from a Q&A site on the WWW (OC); (2) Test data include new senses (called X) that are not defined in dictionary. There is much previous research on WSD. In the case of Japanese, unsupervised approaches such as extended Lesk have performed well (Baldwin et al., 2010), although they are outperformed by supervised approaches (Tanaka et al., 2007; Murata et al., 2003). Therefore, we selected a supervised approach and constructed Support Vector Machines (SVM) and Maximum Entropy (MEM) classifiers using common features and topic features. We performed extensive experiments to investigate the best combinations of domains for training. We describe the data in Section 2, and our system in Section 3. Then in Section 4, we show the results and provide some discussion. 2.2 Data Pre-processing We performed two preliminary pre-processing steps. First, we restored the"
S10-1086,D07-1108,0,\N,Missing
S10-1086,S07-1053,0,\N,Missing
S10-1086,S10-1012,0,\N,Missing
W16-3813,P14-1070,0,0.0630653,"Missing"
W16-3813,P16-1016,0,0.0195966,"Missing"
W16-3813,W08-1301,0,0.142876,"Missing"
W16-3813,L16-1263,1,0.877702,"Missing"
W16-3813,Y15-2015,1,0.927435,"ithin them. MWEs have specific grammatical functionalities and can be regarded as an important part of an extended lexicon. The objective of our work is to construct a wide coverage English syntactically flexible MWE lexicon, to describe their structures in dependency structures with possible modifiers within them, and to annotate their occurrences in the Wall Street Journal portion of OntoNotes corpus (Pradhan et al., 2007). There have been some attempts for constructing English MWE lexicon. An English fixed MWE lexicon and a list of phrasal verbs are presented in (Shigeto et al., 2015) and (Komai et al., 2015). They This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/ 102 Proceedings of the Workshop on Grammar and Lexicon: Interactions and Interfaces, pages 102–109, Osaka, Japan, December 11 2016. Licence details: also annotated the occurrences of those expressions in Penn Treebank. While most of English dictionaries for human use include a large list of multi-word expressions and idioms, to the best of our knowledge, there has been no comprehensive lexicon of English flexible MWEs constructed that is usable for NLP tasks."
W16-3813,L16-1368,0,0.0519487,"Missing"
W16-3813,schneider-etal-2014-comprehensive,0,0.0559871,"Missing"
W16-3813,silveira-etal-2014-gold,0,0.0671405,"Missing"
W16-3918,D15-1166,0,0.452325,"In this work, we adopt the character-level encoder-decoder model as a method of Japanese text normalization. Since the encoder-decoder model was proposed in the field of machine translation, it has This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 1 http://taku910.github.io/mecab/ 129 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 129–137, Osaka, Japan, December 11 2016. License details: http:// achieved good results in morphological inflection generation and error correction (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2015; Cho et al., 2014; Manaal et al., 2016; Barret et al., 2016). As these models are capable of capturing sequential patterns, it is possible to apply the encoder-decoder model to Japanese text normalization. As mentioned previously, Japanese has no explicit boundaries between words. Thus, Japanese text normalization is naturally posed to a character-level sequence to sequence learning. Although the encoder-decoder model has been shown its effectiveness in large datasets, it is much less effective for small datasets such as low-resource languages (Barret et al., 2016). Un"
W16-3918,D14-1011,0,0.241885,"4 describes how to construct a synthesized corpus. Section 5 discusses experiments that we have performed and our corresponding analyses of the experimental results. Section 6 concludes the paper with a brief summary and a mention of future work. 2 Related Work In this section, we mainly describe text normalization for Japanese. Furthermore, we explain some related works with regard to the encoder-decoder model in the field of machine translation. 2.1 Text Normalization Several studies for joint modeling of morphological analysis and text normalization have been conducted (Saito et al., 2014; Kaji and Kitsuregawa, 2014; Sasano et al., 2013). Saito et al. (2014) extracted nonstandard tokens from Twitter and blog texts, and manually annotated their standard forms. They automatically generated derivational patterns based on the character-level alignment between non-standard tokens and their standard forms, then included these patterns to a morphological lattice. Kaji et al. (2014) similarly proposed a joint modeling for morphological analysis and text normalization via their construction of a normalization dictionary using handcrafted rules that were similar to those used in (Sasano et al., 2013) to derive non"
W16-3918,C14-1167,0,0.0686058,"s research. Section 4 describes how to construct a synthesized corpus. Section 5 discusses experiments that we have performed and our corresponding analyses of the experimental results. Section 6 concludes the paper with a brief summary and a mention of future work. 2 Related Work In this section, we mainly describe text normalization for Japanese. Furthermore, we explain some related works with regard to the encoder-decoder model in the field of machine translation. 2.1 Text Normalization Several studies for joint modeling of morphological analysis and text normalization have been conducted (Saito et al., 2014; Kaji and Kitsuregawa, 2014; Sasano et al., 2013). Saito et al. (2014) extracted nonstandard tokens from Twitter and blog texts, and manually annotated their standard forms. They automatically generated derivational patterns based on the character-level alignment between non-standard tokens and their standard forms, then included these patterns to a morphological lattice. Kaji et al. (2014) similarly proposed a joint modeling for morphological analysis and text normalization via their construction of a normalization dictionary using handcrafted rules that were similar to those used in (Sasano"
W16-3918,I13-1019,0,0.48894,"a synthesized corpus. Section 5 discusses experiments that we have performed and our corresponding analyses of the experimental results. Section 6 concludes the paper with a brief summary and a mention of future work. 2 Related Work In this section, we mainly describe text normalization for Japanese. Furthermore, we explain some related works with regard to the encoder-decoder model in the field of machine translation. 2.1 Text Normalization Several studies for joint modeling of morphological analysis and text normalization have been conducted (Saito et al., 2014; Kaji and Kitsuregawa, 2014; Sasano et al., 2013). Saito et al. (2014) extracted nonstandard tokens from Twitter and blog texts, and manually annotated their standard forms. They automatically generated derivational patterns based on the character-level alignment between non-standard tokens and their standard forms, then included these patterns to a morphological lattice. Kaji et al. (2014) similarly proposed a joint modeling for morphological analysis and text normalization via their construction of a normalization dictionary using handcrafted rules that were similar to those used in (Sasano et al., 2013) to derive non-standard forms from t"
W16-3918,N16-1077,0,0.0623367,"del as a method of Japanese text normalization. Since the encoder-decoder model was proposed in the field of machine translation, it has This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 1 http://taku910.github.io/mecab/ 129 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 129–137, Osaka, Japan, December 11 2016. License details: http:// achieved good results in morphological inflection generation and error correction (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2015; Cho et al., 2014; Manaal et al., 2016; Barret et al., 2016). As these models are capable of capturing sequential patterns, it is possible to apply the encoder-decoder model to Japanese text normalization. As mentioned previously, Japanese has no explicit boundaries between words. Thus, Japanese text normalization is naturally posed to a character-level sequence to sequence learning. Although the encoder-decoder model has been shown its effectiveness in large datasets, it is much less effective for small datasets such as low-resource languages (Barret et al., 2016). Unfortunately, contrary to machine translation, Japanese non-stan"
W16-3918,D16-1163,0,0.14613,"panese text normalization. Since the encoder-decoder model was proposed in the field of machine translation, it has This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 1 http://taku910.github.io/mecab/ 129 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 129–137, Osaka, Japan, December 11 2016. License details: http:// achieved good results in morphological inflection generation and error correction (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2015; Cho et al., 2014; Manaal et al., 2016; Barret et al., 2016). As these models are capable of capturing sequential patterns, it is possible to apply the encoder-decoder model to Japanese text normalization. As mentioned previously, Japanese has no explicit boundaries between words. Thus, Japanese text normalization is naturally posed to a character-level sequence to sequence learning. Although the encoder-decoder model has been shown its effectiveness in large datasets, it is much less effective for small datasets such as low-resource languages (Barret et al., 2016). Unfortunately, contrary to machine translation, Japanese non-standard canonical pairs a"
W16-3918,P14-2111,0,0.0506947,"Missing"
W16-3918,P11-1038,0,0.195629,"Missing"
W16-3918,W04-3230,1,0.699446,"Missing"
W16-3918,P11-2093,0,0.0530837,"Missing"
W16-3918,C12-1093,0,0.0197183,"s issue, we propose a method of data augmentation to increase data size by converting existing resources into synthesized non-standard forms using handcrafted rules. We conducted an experiment to demonstrate that the synthesized corpus contributes to stably train an encoder-decoder model and improve the performance of Japanese text normalization. 1 Introduction With the rapid spread of the social media, many texts have been uploaded to the internet. As such, social media texts are considered important language resources owing to an increasing demand for information extraction and text mining (Lau et al., 2012; Aramaki et al., 2011). However, these texts include lexical variants such as insertions, phonetic substitutions and internet slangs. These non-standard forms adversely affect language analysis tools that are trained on a clean corpus. Since Japanese has no explicit boundaries between words, word segmentation and part-of-speech (POS) tagging are extremely important in Japanese language processing. For example, the output obtained using the Japanese morphological analyzer: MeCab 1 for a non-standard sentence: “このあぷりすげえええ！” is as follows: Input:このあぷりすげえええ！ (kono apuri sugeee; This app is greeea"
W16-3918,D11-1145,0,0.0158666,"e a method of data augmentation to increase data size by converting existing resources into synthesized non-standard forms using handcrafted rules. We conducted an experiment to demonstrate that the synthesized corpus contributes to stably train an encoder-decoder model and improve the performance of Japanese text normalization. 1 Introduction With the rapid spread of the social media, many texts have been uploaded to the internet. As such, social media texts are considered important language resources owing to an increasing demand for information extraction and text mining (Lau et al., 2012; Aramaki et al., 2011). However, these texts include lexical variants such as insertions, phonetic substitutions and internet slangs. These non-standard forms adversely affect language analysis tools that are trained on a clean corpus. Since Japanese has no explicit boundaries between words, word segmentation and part-of-speech (POS) tagging are extremely important in Japanese language processing. For example, the output obtained using the Japanese morphological analyzer: MeCab 1 for a non-standard sentence: “このあぷりすげえええ！” is as follows: Input:このあぷりすげえええ！ (kono apuri sugeee; This app is greeeat!) Output:この (This, Pr"
W16-3918,P16-1154,0,0.0838798,"Missing"
W18-4922,de-marneffe-etal-2014-universal,0,0.0290054,"Missing"
W18-4922,J93-2004,0,0.0615426,"Missing"
W18-4922,L16-1262,0,0.0285721,"Missing"
W18-4922,schneider-etal-2014-comprehensive,0,0.06236,"Missing"
W19-2609,H89-1033,0,0.0738321,"on and solving. Future work will focus on designing learning agents to solve the games, as well as improving text generation capabilities. We hope that the proposed approach will lead to developing useful systems for action graph extraction as well as other language understanding tasks. Discussion Our approach faces tough challenges. However, we are encouraged by the significant recent advances towards these challenges in related areas, and plan to leverage this progress for our framework. Programming semantics and rewards for instruction-following agents is known to be notoriously difficult (Winograd, 1972) as language and environments grow increasingly complex. Research on learned instruction-conditional reward models (Bahdanau et al., 2018) is a promising approach towards reducing the amount of “environment engineering” required. 66 References Zhengdong Lu, Haotian Cui, Xianggen Liu, Yukun Yan, and Daqi Zheng. 2018. Object-oriented neural programming (oonp) for document understanding. In ACL. The materials science procedural text corpus. https://github.com/olivettigroup/ annotated-materials-syntheses. Accessed: 5/4/2019. Li Lucy and Jon Gauthier. 2017. Are distributional representations ready"
W19-2609,K17-1034,0,0.0305074,"nguage. The ability to automatically parse these texts into a structured form could allow for datadriven synthesis planning, a key enabler in the design and discovery of novel materials (Kim et al., 2018; Mysore et al., 2017). A particularly useful parsing is action graph extraction, which maps a passage describing a procedure to a symbolic action-graph representation of the core entities, operations and their accompanying arguments, as they unfold throughout the text (Fig. 1). Procedural text understanding is a highly challenging task for today’s learning algorithms (Lucy and Gauthier, 2017; Levy et al., 2017). Synthesis procedures are especially challenging, as they are written in difficult and highly technical language assuming prior knowledge. Some texts are long, ∗ Work was begun while author was an intern at RIKEN and continued at the Hebrew University. 62 Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications, pages 62–71 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics Figure 1: Sample surface text (left) and possible corresponding action-graph (right) for typical partial material synthesis procedure. Operation numbers in p"
Y15-2015,D09-1049,0,0.0297569,"der and Smith, 2015). In MWE identification tasks, previous work integrated MWE recognition into POS tagging (Constant and Sigogne, 2011). An MWE identification method using Conditional Random Fields was also presented together with the data set (Shigeto et al., 2013). A joint model of MWE identification and constituency parsing was proposed (Constant et al., 2012). They allocated IOB2 tags to MWEs and used MWEs as special features when reranking the parse tree. However, it is difficult for these methods to detect discontinuous MWEs. In contrast, as for methods that can handle separable MWEs, Boukobza and Rappoport (2009) tackled MWE detection on specific MWE types with a binary classification method. In a framework of a sequential labeling method for MWE detection, a new IOB tag scheme, which is augmented to capture discontinuous MWEs and distinguish strong MWEs from week MWEs, was presented (Schneider et al., 2014a). Here strong MWEs indicate the expresJJ 17 ad hoc PRP 14 anything else 126 Other 6 no way sion which has strong idiomaticity, and week one indicate the expression which is to more likely to be a compositional phrase or collocation. Additionally, words between components of MWEs are called gaps, a"
Y15-2015,W11-0809,0,0.0137604,"ork MWEs can be roughly divided into two categories, separable and non-separable (or fixed) MWEs. Previous work annotated fixed MWEs on Penn Treebank, where they used syntactic trees of Penn Treebank and an MWE dictionary that is extracted from Wiktionary (Shigeto et al., 2013). In Schneider et al. (2014b), they annotated all types of MWEs on English Web Treebank completely by hand. Afterward, they added to supersenses, which mean coarsegrained semantic classes of lexical units (Schneider and Smith, 2015). In MWE identification tasks, previous work integrated MWE recognition into POS tagging (Constant and Sigogne, 2011). An MWE identification method using Conditional Random Fields was also presented together with the data set (Shigeto et al., 2013). A joint model of MWE identification and constituency parsing was proposed (Constant et al., 2012). They allocated IOB2 tags to MWEs and used MWEs as special features when reranking the parse tree. However, it is difficult for these methods to detect discontinuous MWEs. In contrast, as for methods that can handle separable MWEs, Boukobza and Rappoport (2009) tackled MWE detection on specific MWE types with a binary classification method. In a framework of a sequen"
Y15-2015,P12-1022,0,0.0128479,"cted from Wiktionary (Shigeto et al., 2013). In Schneider et al. (2014b), they annotated all types of MWEs on English Web Treebank completely by hand. Afterward, they added to supersenses, which mean coarsegrained semantic classes of lexical units (Schneider and Smith, 2015). In MWE identification tasks, previous work integrated MWE recognition into POS tagging (Constant and Sigogne, 2011). An MWE identification method using Conditional Random Fields was also presented together with the data set (Shigeto et al., 2013). A joint model of MWE identification and constituency parsing was proposed (Constant et al., 2012). They allocated IOB2 tags to MWEs and used MWEs as special features when reranking the parse tree. However, it is difficult for these methods to detect discontinuous MWEs. In contrast, as for methods that can handle separable MWEs, Boukobza and Rappoport (2009) tackled MWE detection on specific MWE types with a binary classification method. In a framework of a sequential labeling method for MWE detection, a new IOB tag scheme, which is augmented to capture discontinuous MWEs and distinguish strong MWEs from week MWEs, was presented (Schneider et al., 2014a). Here strong MWEs indicate the expr"
Y15-2015,W08-1301,0,0.0708201,"Missing"
Y15-2015,I13-1168,0,0.0200682,"dentification performance than rule-based and sequence-labeling methods. 1 Introduction Multiword Expressions (MWEs) are roughly defined as those that have “idiosyncratic interpretations that cross word boundaries (or spaces)” (Sag et al., 2001). Vocabulary sizes of single words and MWEs have roughly the same size, thus MWE identification is a crucial issue for deep analysis of natural language text. Indeed, it has been shown in the literature that MWE identification helps various NLP applications, such as information retrieval, machine translation, and syntactic parsing (Newman et al., 2012; Ghoneim and Diab, 2013; Nivre and Nilsson, 2004). Since huge cost is necessary for annotation, there are few corpora that are sufficiently annotated for English MWEs. Schneider et al. (2014b) constructed an MWE-annotated corpus on English Web Treebank, and proposed a sequen(a) We bring our computers up. (b) She goes over the question. (c) Someone goes over there. Figure 1: a positive instance (a) of a separable expression “bring up”, a positive instance (b) and a negative instance (c) of an inseparable expression “go over”. tial labeling method for MWE identification. However, they tried to manually cover the types"
Y15-2015,C12-1127,0,0.0134824,"achieves better MWE identification performance than rule-based and sequence-labeling methods. 1 Introduction Multiword Expressions (MWEs) are roughly defined as those that have “idiosyncratic interpretations that cross word boundaries (or spaces)” (Sag et al., 2001). Vocabulary sizes of single words and MWEs have roughly the same size, thus MWE identification is a crucial issue for deep analysis of natural language text. Indeed, it has been shown in the literature that MWE identification helps various NLP applications, such as information retrieval, machine translation, and syntactic parsing (Newman et al., 2012; Ghoneim and Diab, 2013; Nivre and Nilsson, 2004). Since huge cost is necessary for annotation, there are few corpora that are sufficiently annotated for English MWEs. Schneider et al. (2014b) constructed an MWE-annotated corpus on English Web Treebank, and proposed a sequen(a) We bring our computers up. (b) She goes over the question. (c) Someone goes over there. Figure 1: a positive instance (a) of a separable expression “bring up”, a positive instance (b) and a negative instance (c) of an inseparable expression “go over”. tial labeling method for MWE identification. However, they tried to"
Y15-2015,N15-1177,0,0.0127,"fficient for accurate MWE identification. We also investigate effective features for MWE identification. 2 Related Work MWEs can be roughly divided into two categories, separable and non-separable (or fixed) MWEs. Previous work annotated fixed MWEs on Penn Treebank, where they used syntactic trees of Penn Treebank and an MWE dictionary that is extracted from Wiktionary (Shigeto et al., 2013). In Schneider et al. (2014b), they annotated all types of MWEs on English Web Treebank completely by hand. Afterward, they added to supersenses, which mean coarsegrained semantic classes of lexical units (Schneider and Smith, 2015). In MWE identification tasks, previous work integrated MWE recognition into POS tagging (Constant and Sigogne, 2011). An MWE identification method using Conditional Random Fields was also presented together with the data set (Shigeto et al., 2013). A joint model of MWE identification and constituency parsing was proposed (Constant et al., 2012). They allocated IOB2 tags to MWEs and used MWEs as special features when reranking the parse tree. However, it is difficult for these methods to detect discontinuous MWEs. In contrast, as for methods that can handle separable MWEs, Boukobza and Rappopo"
Y15-2015,Q14-1016,0,0.119319,"tic interpretations that cross word boundaries (or spaces)” (Sag et al., 2001). Vocabulary sizes of single words and MWEs have roughly the same size, thus MWE identification is a crucial issue for deep analysis of natural language text. Indeed, it has been shown in the literature that MWE identification helps various NLP applications, such as information retrieval, machine translation, and syntactic parsing (Newman et al., 2012; Ghoneim and Diab, 2013; Nivre and Nilsson, 2004). Since huge cost is necessary for annotation, there are few corpora that are sufficiently annotated for English MWEs. Schneider et al. (2014b) constructed an MWE-annotated corpus on English Web Treebank, and proposed a sequen(a) We bring our computers up. (b) She goes over the question. (c) Someone goes over there. Figure 1: a positive instance (a) of a separable expression “bring up”, a positive instance (b) and a negative instance (c) of an inseparable expression “go over”. tial labeling method for MWE identification. However, they tried to manually cover the types of comprehensive MWEs, and the number of instances for each MWE was very limited. In this paper, we propose an efficient annotation method for separable MWEs appearin"
Y15-2015,schneider-etal-2014-comprehensive,0,0.513354,"tic interpretations that cross word boundaries (or spaces)” (Sag et al., 2001). Vocabulary sizes of single words and MWEs have roughly the same size, thus MWE identification is a crucial issue for deep analysis of natural language text. Indeed, it has been shown in the literature that MWE identification helps various NLP applications, such as information retrieval, machine translation, and syntactic parsing (Newman et al., 2012; Ghoneim and Diab, 2013; Nivre and Nilsson, 2004). Since huge cost is necessary for annotation, there are few corpora that are sufficiently annotated for English MWEs. Schneider et al. (2014b) constructed an MWE-annotated corpus on English Web Treebank, and proposed a sequen(a) We bring our computers up. (b) She goes over the question. (c) Someone goes over there. Figure 1: a positive instance (a) of a separable expression “bring up”, a positive instance (b) and a negative instance (c) of an inseparable expression “go over”. tial labeling method for MWE identification. However, they tried to manually cover the types of comprehensive MWEs, and the number of instances for each MWE was very limited. In this paper, we propose an efficient annotation method for separable MWEs appearin"
Y16-2006,W08-1209,0,0.0821262,"Missing"
Y16-2006,W11-0609,0,0.0603692,"Missing"
Y16-2006,P13-1095,0,0.0279677,"complicating mixture of emotions but also the intensity of each of them. In this paper, we propose a three steps method for the detection of emotions in conversation: 1)Building Emotion Lexicon from Wordnet (Miller, 1995). 2) Using simple Neural Network to adapt the lexicon to the training data. 3) Using Deep Network with features extracted from adapted lexicon and classify the multi-label corpus. Related Work Most of the work in the ﬁeld tried to deﬁne a small set of emotions (D’Mello et al., 2006; Yang et al., 2007) which involved only 3 and 4 emotional states respectively. Another work by Hasegawa et al. (2013) performed a multi-class classiﬁcation on dialog data from Twitter in Japanese. They automatically labeled the obtained dialogs by using emotional expression clues, which is similar to our collocation list explained in sub-section 3.3. We propose a more comprehensive approach by exploiting Plutchik’s notion which covers the full spectrum of human emotions to work on challenging multi-label conversation corpus. Having the same notion, Buitinck et al. (2015) proposed a simple Bag of Words approach and tuned RAKEL for multi-label classiﬁcation for movie reviews. We go further and work on conversa"
Y16-2006,P15-1101,0,0.106256,"ther. Generally, the expression of Emotion in general depends on the words being used. However, it also quite depends on the grammar structure and syntactic variables such as: negations, embedded sentence, and the type of sentence question, exclamation, command or statement (Collier, 2014). Therefore, similar to the detection of emotions of sentences in a paragraph, the context information of the whole conversation and what is said in the previous utterance should be taken into consideration. The extraction of context features will be further explained in sub-section 4.3.1 Unlike other works (Li et al., 2015; Wang et al., 2015) where small sets of basic emotions are used, we annotated the dataset using the notion of Plutchik’s basic emotion and dyads (1980). This eases the annotators’ task since it offers annotators with wider range of emotion labels (8 basic and 23 combinations) to choose from. Previous research often relied on a list of 6 basic emotions (Ekman et al., 1987) with some variants. However, this notion fails to show conﬂict side of some emotions. For example, people should not 30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea,"
Y16-2006,N12-1071,0,0.0321204,"Missing"
Y16-2006,S07-1013,0,0.0601109,"eate another emotional state dyads (Plutchik, 1980) The survey by Dave and Diwanji (2015) predicted the need for Emotion Detection in streaming data and the study of emotion ﬂow during chatting. In this paper, we tackle the simpliﬁed version of this task by detecting the emotions in conversation. The corpus we used is made of conversations among movie characters, who take turns in the conversation. Those turns are called utterances, which are then manually annotated in a multi-label manner. Emotion detection in conversation is essentially different from identifying emotions in news headlines (Strapparava and Mihalcea, 2007) or Tweets (Bollen et al., 2011) where each instance is independent of each other. Generally, the expression of Emotion in general depends on the words being used. However, it also quite depends on the grammar structure and syntactic variables such as: negations, embedded sentence, and the type of sentence question, exclamation, command or statement (Collier, 2014). Therefore, similar to the detection of emotions of sentences in a paragraph, the context information of the whole conversation and what is said in the previous utterance should be taken into consideration. The extraction of context"
Y16-2006,P15-2125,0,0.020663,"the expression of Emotion in general depends on the words being used. However, it also quite depends on the grammar structure and syntactic variables such as: negations, embedded sentence, and the type of sentence question, exclamation, command or statement (Collier, 2014). Therefore, similar to the detection of emotions of sentences in a paragraph, the context information of the whole conversation and what is said in the previous utterance should be taken into consideration. The extraction of context features will be further explained in sub-section 4.3.1 Unlike other works (Li et al., 2015; Wang et al., 2015) where small sets of basic emotions are used, we annotated the dataset using the notion of Plutchik’s basic emotion and dyads (1980). This eases the annotators’ task since it offers annotators with wider range of emotion labels (8 basic and 23 combinations) to choose from. Previous research often relied on a list of 6 basic emotions (Ekman et al., 1987) with some variants. However, this notion fails to show conﬂict side of some emotions. For example, people should not 30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016"
Y16-2006,P15-2129,0,0.0194134,"ve. Section 5 evaluates the lexicon, the effectiveness of the adapted lexicon and the proposed method in general. Section 6 gives the conclusion and discusses future work. 2 Figure 1: Plutchik’s basic emotion and dyads - image taken from http://twinklet8.blogspot.jp feel happiness and sadness from the same incident altogether. Furthermore, Ekman’s basic emotions are the result of observation made on human facial expressions so applying such notion in text classiﬁcation task seems irrelevant. Newer works relies on dimensional representation using valence-arousal space (Calvo and Mac Kim, 2013; Yu et al., 2015) Plutchik (1980) suggested 4 axes of bipolar basic emotions: Joy - Sadness, Fear - Anger, Trust Disgust, Surprise - Anticipatation. These primary emotions may blend to form the full spectrum of human emotional experience. The new complex emotions formed by them are called dyads (Figure 1). Plutchik’s notion reasonably explains the connection between emotions. Some emotions will not occur at the same time since they are on the opposite side of the axis. Complex emotions can also be viewed as combinations of primary ones. The idea enables us to approach emotion detection in a more comprehensive"
Y18-1046,P06-1032,0,0.0944715,"ee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et al., 2018). On Japanese GEC, much work has been done on particle error correction for JSL learners (Oyama, 2010; Ohki et al., 2011; Mizumoto et al., 2011; Imamura et al., 2014). Collecting large-scale annotated error data written by second language learners is not so easy. To cope with grammatical error data scarcity, several studies proposed effective approaches for generating artificial error data (Irmawati et al., 2017; Rei et al., 2017). Several approaches of using Statistical machine translation (SMT) for GEC have been proposed (Brockett et al., 2006; Mizumoto et al., 2011; Mizumoto et al., 2015). Recently, neural networks have shown success in many NLP tasks, such as machine translation (MT) (Eriguchi et al., 2016; Gehring et al., 2017), named en395 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 tity recognition (NER) (Kuru et al., 2016; Misawa et al., 2017) and etc. For GEC, several studies have applied neural machine translation (NMT) approach (Chollampatt et al., 2016; Yuan and Briscoe, 2016). NMT is applied in the GEC task as it may be possibl"
Y18-1046,W13-4414,0,0.0190012,"ating artificial error data. Section 4 describes how BiLSTM-CRF model is used to detect Japanese functional expressions and Section 5 explains the method for generating artificial error data. In Section 6, we conduct the experiments using neural machine translation and analyze the results. Section 7 concludes with a summation of this work and describes our future work. 2 Related Work Spelling correction is an automatic algorithm for detecting and correcting human spelling errors in every written language, which has been an active research in Natural Language Processing (NLP) (Sun et al. 2010; Chen et al. 2013; Liu et al. 2013; Liu et al. 2015;). Grammatical error correction (GEC) is a task of detecting and correcting grammatical errors in text written by native language writers or nonnative foreign language writers. over the past few decades, GEC in English has been widely researched, such as Helping Our Own (Dale and Kigarriff, 2011; Dale et al., 2012), CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). Many shared tasks on GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et"
Y18-1046,D14-1179,0,0.0421249,"Missing"
Y18-1046,W11-2838,0,0.061622,"Missing"
Y18-1046,W12-2006,0,0.0154424,"ribes our future work. 2 Related Work Spelling correction is an automatic algorithm for detecting and correcting human spelling errors in every written language, which has been an active research in Natural Language Processing (NLP) (Sun et al. 2010; Chen et al. 2013; Liu et al. 2013; Liu et al. 2015;). Grammatical error correction (GEC) is a task of detecting and correcting grammatical errors in text written by native language writers or nonnative foreign language writers. over the past few decades, GEC in English has been widely researched, such as Helping Our Own (Dale and Kigarriff, 2011; Dale et al., 2012), CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). Many shared tasks on GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et al., 2018). On Japanese GEC, much work has been done on particle error correction for JSL learners (Oyama, 2010; Ohki et al., 2011; Mizumoto et al., 2011; Imamura et al., 2014). Collecting large-scale annotated error data written by second language learners is not so easy. To cope with grammatical error data scarcity, several studies proposed effec"
Y18-1046,Y18-1000,0,0.22362,"Missing"
Y18-1046,C16-1087,0,0.0546775,"Missing"
Y18-1046,W15-4401,0,0.0170559,"Language Processing (NLP) (Sun et al. 2010; Chen et al. 2013; Liu et al. 2013; Liu et al. 2015;). Grammatical error correction (GEC) is a task of detecting and correcting grammatical errors in text written by native language writers or nonnative foreign language writers. over the past few decades, GEC in English has been widely researched, such as Helping Our Own (Dale and Kigarriff, 2011; Dale et al., 2012), CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). Many shared tasks on GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et al., 2018). On Japanese GEC, much work has been done on particle error correction for JSL learners (Oyama, 2010; Ohki et al., 2011; Mizumoto et al., 2011; Imamura et al., 2014). Collecting large-scale annotated error data written by second language learners is not so easy. To cope with grammatical error data scarcity, several studies proposed effective approaches for generating artificial error data (Irmawati et al., 2017; Rei et al., 2017). Several approaches of using Statistical machine translation (SMT) for GEC have been proposed (Brockett et al."
Y18-1046,W13-4409,1,0.770669,"rror data. Section 4 describes how BiLSTM-CRF model is used to detect Japanese functional expressions and Section 5 explains the method for generating artificial error data. In Section 6, we conduct the experiments using neural machine translation and analyze the results. Section 7 concludes with a summation of this work and describes our future work. 2 Related Work Spelling correction is an automatic algorithm for detecting and correcting human spelling errors in every written language, which has been an active research in Natural Language Processing (NLP) (Sun et al. 2010; Chen et al. 2013; Liu et al. 2013; Liu et al. 2015;). Grammatical error correction (GEC) is a task of detecting and correcting grammatical errors in text written by native language writers or nonnative foreign language writers. over the past few decades, GEC in English has been widely researched, such as Helping Our Own (Dale and Kigarriff, 2011; Dale et al., 2012), CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). Many shared tasks on GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et al., 2018). On J"
Y18-1046,P15-1002,0,0.0347606,"sks, such as machine translation (MT) (Eriguchi et al., 2016; Gehring et al., 2017), named en395 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 tity recognition (NER) (Kuru et al., 2016; Misawa et al., 2017) and etc. For GEC, several studies have applied neural machine translation (NMT) approach (Chollampatt et al., 2016; Yuan and Briscoe, 2016). NMT is applied in the GEC task as it may be possible to correct erroneous phrases and sentences that have not been seen in the training data more effectively (Luong et al., 2015). NMT-based systems thus may help ameliorate the shortage of large error-annotated learner corpora for GEC. As previous research mentioned above, few studies have aimed at spelling and grammatical error corrections on Japanese functional expressions. Therefore, our paper is an attempt to do this work using neural machine translation. 3 Language resources We use the following corpora for training the BiLSTM-CRF model to detect Japanese functional expressions. We use Lang-8 Learner, Tatoeba, HiraganaTimes corpora for generating artificial error data, because these three corpora are particularly"
Y18-1046,I11-1017,1,0.84508,"text written by native language writers or nonnative foreign language writers. over the past few decades, GEC in English has been widely researched, such as Helping Our Own (Dale and Kigarriff, 2011; Dale et al., 2012), CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). Many shared tasks on GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et al., 2018). On Japanese GEC, much work has been done on particle error correction for JSL learners (Oyama, 2010; Ohki et al., 2011; Mizumoto et al., 2011; Imamura et al., 2014). Collecting large-scale annotated error data written by second language learners is not so easy. To cope with grammatical error data scarcity, several studies proposed effective approaches for generating artificial error data (Irmawati et al., 2017; Rei et al., 2017). Several approaches of using Statistical machine translation (SMT) for GEC have been proposed (Brockett et al., 2006; Mizumoto et al., 2011; Mizumoto et al., 2015). Recently, neural networks have shown success in many NLP tasks, such as machine translation (MT) (Eriguchi et al., 2016; Gehring et al., 2017),"
Y18-1046,W15-4412,1,0.849798,"2017; Rao et al., 2018). On Japanese GEC, much work has been done on particle error correction for JSL learners (Oyama, 2010; Ohki et al., 2011; Mizumoto et al., 2011; Imamura et al., 2014). Collecting large-scale annotated error data written by second language learners is not so easy. To cope with grammatical error data scarcity, several studies proposed effective approaches for generating artificial error data (Irmawati et al., 2017; Rei et al., 2017). Several approaches of using Statistical machine translation (SMT) for GEC have been proposed (Brockett et al., 2006; Mizumoto et al., 2011; Mizumoto et al., 2015). Recently, neural networks have shown success in many NLP tasks, such as machine translation (MT) (Eriguchi et al., 2016; Gehring et al., 2017), named en395 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 tity recognition (NER) (Kuru et al., 2016; Misawa et al., 2017) and etc. For GEC, several studies have applied neural machine translation (NMT) approach (Chollampatt et al., 2016; Yuan and Briscoe, 2016). NMT is applied in the GEC task as it may be possible to correct erroneous phrases and sentences th"
Y18-1046,W14-1701,0,0.0155506,"n is an automatic algorithm for detecting and correcting human spelling errors in every written language, which has been an active research in Natural Language Processing (NLP) (Sun et al. 2010; Chen et al. 2013; Liu et al. 2013; Liu et al. 2015;). Grammatical error correction (GEC) is a task of detecting and correcting grammatical errors in text written by native language writers or nonnative foreign language writers. over the past few decades, GEC in English has been widely researched, such as Helping Our Own (Dale and Kigarriff, 2011; Dale et al., 2012), CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). Many shared tasks on GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et al., 2018). On Japanese GEC, much work has been done on particle error correction for JSL learners (Oyama, 2010; Ohki et al., 2011; Mizumoto et al., 2011; Imamura et al., 2014). Collecting large-scale annotated error data written by second language learners is not so easy. To cope with grammatical error data scarcity, several studies proposed effective approaches for generating artificial error data ("
Y18-1046,W13-3601,0,0.0259977,"pelling correction is an automatic algorithm for detecting and correcting human spelling errors in every written language, which has been an active research in Natural Language Processing (NLP) (Sun et al. 2010; Chen et al. 2013; Liu et al. 2013; Liu et al. 2015;). Grammatical error correction (GEC) is a task of detecting and correcting grammatical errors in text written by native language writers or nonnative foreign language writers. over the past few decades, GEC in English has been widely researched, such as Helping Our Own (Dale and Kigarriff, 2011; Dale et al., 2012), CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). Many shared tasks on GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et al., 2018). On Japanese GEC, much work has been done on particle error correction for JSL learners (Oyama, 2010; Ohki et al., 2011; Mizumoto et al., 2011; Imamura et al., 2014). Collecting large-scale annotated error data written by second language learners is not so easy. To cope with grammatical error data scarcity, several studies proposed effective approaches for generating artif"
Y18-1046,W06-2404,0,0.119078,"Missing"
Y18-1046,W18-3706,0,0.0128624,". 2013; Liu et al. 2013; Liu et al. 2015;). Grammatical error correction (GEC) is a task of detecting and correcting grammatical errors in text written by native language writers or nonnative foreign language writers. over the past few decades, GEC in English has been widely researched, such as Helping Our Own (Dale and Kigarriff, 2011; Dale et al., 2012), CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). Many shared tasks on GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et al., 2018). On Japanese GEC, much work has been done on particle error correction for JSL learners (Oyama, 2010; Ohki et al., 2011; Mizumoto et al., 2011; Imamura et al., 2014). Collecting large-scale annotated error data written by second language learners is not so easy. To cope with grammatical error data scarcity, several studies proposed effective approaches for generating artificial error data (Irmawati et al., 2017; Rei et al., 2017). Several approaches of using Statistical machine translation (SMT) for GEC have been proposed (Brockett et al., 2006; Mizumoto et al., 2011; Mizumoto et al., 2015)."
Y18-1046,W17-5032,0,0.0152929,"GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et al., 2018). On Japanese GEC, much work has been done on particle error correction for JSL learners (Oyama, 2010; Ohki et al., 2011; Mizumoto et al., 2011; Imamura et al., 2014). Collecting large-scale annotated error data written by second language learners is not so easy. To cope with grammatical error data scarcity, several studies proposed effective approaches for generating artificial error data (Irmawati et al., 2017; Rei et al., 2017). Several approaches of using Statistical machine translation (SMT) for GEC have been proposed (Brockett et al., 2006; Mizumoto et al., 2011; Mizumoto et al., 2015). Recently, neural networks have shown success in many NLP tasks, such as machine translation (MT) (Eriguchi et al., 2016; Gehring et al., 2017), named en395 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 tity recognition (NER) (Kuru et al., 2016; Misawa et al., 2017) and etc. For GEC, several studies have applied neural machine translation ("
Y18-1046,N16-1042,0,0.0345489,"machine translation (SMT) for GEC have been proposed (Brockett et al., 2006; Mizumoto et al., 2011; Mizumoto et al., 2015). Recently, neural networks have shown success in many NLP tasks, such as machine translation (MT) (Eriguchi et al., 2016; Gehring et al., 2017), named en395 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 tity recognition (NER) (Kuru et al., 2016; Misawa et al., 2017) and etc. For GEC, several studies have applied neural machine translation (NMT) approach (Chollampatt et al., 2016; Yuan and Briscoe, 2016). NMT is applied in the GEC task as it may be possible to correct erroneous phrases and sentences that have not been seen in the training data more effectively (Luong et al., 2015). NMT-based systems thus may help ameliorate the shortage of large error-annotated learner corpora for GEC. As previous research mentioned above, few studies have aimed at spelling and grammatical error corrections on Japanese functional expressions. Therefore, our paper is an attempt to do this work using neural machine translation. 3 Language resources We use the following corpora for training the BiLSTM-CRF model"
Y18-1046,P10-1028,0,0.0827212,"F model and generating artificial error data. Section 4 describes how BiLSTM-CRF model is used to detect Japanese functional expressions and Section 5 explains the method for generating artificial error data. In Section 6, we conduct the experiments using neural machine translation and analyze the results. Section 7 concludes with a summation of this work and describes our future work. 2 Related Work Spelling correction is an automatic algorithm for detecting and correcting human spelling errors in every written language, which has been an active research in Natural Language Processing (NLP) (Sun et al. 2010; Chen et al. 2013; Liu et al. 2013; Liu et al. 2015;). Grammatical error correction (GEC) is a task of detecting and correcting grammatical errors in text written by native language writers or nonnative foreign language writers. over the past few decades, GEC in English has been widely researched, such as Helping Our Own (Dale and Kigarriff, 2011; Dale et al., 2012), CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). Many shared tasks on GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et"
