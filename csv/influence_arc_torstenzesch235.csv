2020.aacl-main.37,P00-1031,0,0.324131,"Missing"
2020.aacl-main.37,P14-5011,1,0.760041,"der to investigate different options for segmentation of Chinese text. Table 9 gives an example for the different segmentation options, which will also be detailed in Section 5.2. Additionally, we add a pre-processing step to remove all auxiliary words in the data in order to simulate the effect of lemmatization in English content scoring. 5.1 General Experimental Setup For all our experiments, we use the ESCRITO (Zesch and Horbach, 2018) toolkit and extended it with readers and tokenization for Chinese text. ESCRITO is a publicly available general-purpose scoring framework based on DKPro TC (Daxenberger et al., 2014), which uses an SVM classifier (Cortes and Vapnik, 1995) using the SMO algorithm as provided by WEKA (Witten et al., 1999). For all kinds of features, we use the top 10000 most frequent 352 Data Set CESA ASAP-ZH ASAP-ZHM T ID 5 10 10 Score Example 2 The machine summarizes a large amount of data and finds the pattern from it 机器总结大量数据，从中找到规律 1 Machines can learn things by themselves 机器能自己学习东西 0 Let the machine learn human thinking ability 让机器学习人的思想能力 2 White: make the indoor temperature not too high, 白色使室内气温不太高 experiments show that white has the lowest light energy absorption rate 实验表明白色对光的能量吸收"
2020.aacl-main.37,S13-2045,0,0.0698385,"Missing"
2020.aacl-main.37,S13-2046,0,0.0552402,"Missing"
2020.aacl-main.37,W17-5908,1,0.807317,"Missing"
2020.aacl-main.37,C96-2184,0,0.497778,"Missing"
2020.aacl-main.37,P18-2023,0,0.0222462,"methods like long-short term memory recurrent neural networks (LSTM) (Yang et al., 2017; Huang et al., 2018) or deep autoencoders (Yang et al., 2018). Also for neural models using word embeddings, word-level tokenization is necessary. Wu and Yeh (2019) train 300-dimensional word2vec word embeddings on sentences from their data set along with Chinese Wikipedia articles and classify student answers with a convolution neural network (CNN). Li et al. (2019) use a Bidirectional Long ShortTerm Memory (Bi-LSTM) network for semantic feature extraction from pre-trained 300dimensional word embeddings (Li et al., 2018) and score student answers based on their similarity to the reference answer using a mutual attention mechanism. For segmentation, most prior work uses the jieba tokenizer 2 for pre-processing. However, 2 the performance of the tokenization is rarely discussed. We also notice that no related work uses segmentation on character or component level. Yang et al. (2018) perform stop word removal, but they do not mention if it included some kind of removal of grammatical markers. https://github.com/fxsjy/jieba 350 In this section, we review existing Chinese content scoring data sets. They are not pu"
2020.aacl-main.37,E09-1065,0,0.128424,"Missing"
2020.aacl-main.37,W17-5017,1,0.903033,"Missing"
2020.aacl-main.37,N15-1111,0,0.0362775,"Missing"
2020.aacl-main.37,W14-3505,1,0.892748,"Missing"
2020.aacl-main.37,W18-3718,0,0.0175169,"e question: What do panda bears eat? in English, especially lemmatization, have not yet been transferred to Chinese. Thus, we will explore in our experiments both n-gram features on different levels and the removal of auxiliary words. 3 Prior Work on Chinese Content Scoring 4 Chinese Scoring Data Sets As shown in Table 3, all prior work on Chinese content scoring uses lexical features on the word level, such as word n-grams and sentence length in tokens. They are not only used in shallow learning methods like support vector machines (SVM) or support vector regression (SVR) (Wang et al., 2008; Wu and Shih, 2018), but also applied to deep learning methods like long-short term memory recurrent neural networks (LSTM) (Yang et al., 2017; Huang et al., 2018) or deep autoencoders (Yang et al., 2018). Also for neural models using word embeddings, word-level tokenization is necessary. Wu and Yeh (2019) train 300-dimensional word2vec word embeddings on sentences from their data set along with Chinese Wikipedia articles and classify student answers with a convolution neural network (CNN). Li et al. (2019) use a Bidirectional Long ShortTerm Memory (Bi-LSTM) network for semantic feature extraction from pre-train"
2020.aacl-main.37,yang-etal-2012-spell,0,0.0690577,"Missing"
2020.aacl-main.37,D16-1100,0,0.0281258,"re less-frequently seen varisets, CESA and ASAP-ZH, and release them for research in the future. While previous work has been limited to word-level features, we conducted a comparison of features on different segmentation levels. Although the difference between feature sets was in general small, we found that some answers with unusual expressions have a tendency to be better scored with models trained on lower level features, such as character ngrams. In the future, we will extend our comparison of segmentation levels also to a deep learning setting, using embeddings of different granularity (Yin et al., 2016). human ants of 人类 , all of which are indicators of a correct answer. This supports the assumption that, like in English, character-level features can capture variance in learner answers, in this case by handling variance in lexical choice. The usage of Pinyin did not bring the expected benefit, possibly because the amount of spelling errors is not substantial enough in the data. Similarly, removing auxiliary words appears to have little influence on scoring performance. 7 References Hsuan-Chih Chen. 1992. Reading comprehension in chinese: Implications from character reading times. Language pr"
2020.aacl-main.37,W15-0615,1,0.829577,"Natural Language Processing, pages 347–357 c December 4 - 7, 2020. 2020 Association for Computational Linguistics machine learning pipeline for automatic scoring with state-of-art NLP tools for Chinese. We investigate the extraction of n-gram features on all possible segmentation levels. In addition, we use features based on the Pinyin transcription of Chinese texts and experiment with the removal of auxiliary words as an equivalent to lemmatization in English. We evaluate these features on our new data sets as well as, for comparison, an English data set translated into Chinese. nani (2013); Zesch et al. (2015)). In the light of the tokenziation challenges mentioned above, it is surprising that although most prior work on Chinese also applies word-level features (see Section 3), the performance of their tokenizers are barely discussed and character-level features are neglected altogether. Apart from words and characters, there are more possibilities of segmentation in Chinese as discussed above. Consider, for example, a 2 Chinese bi-morphemic word such as 熊猫 . It can additionally be segmented on the stroke, component and radical level as shown in Table 1. It has been argued that the morphological in"
2020.aacl-main.37,L18-1365,1,0.834535,"etup In this section, we adapt a state-of-the-art content scoring system to Chinese. We evaluate it in six settings with different feature sets on the data sets described above in order to investigate different options for segmentation of Chinese text. Table 9 gives an example for the different segmentation options, which will also be detailed in Section 5.2. Additionally, we add a pre-processing step to remove all auxiliary words in the data in order to simulate the effect of lemmatization in English content scoring. 5.1 General Experimental Setup For all our experiments, we use the ESCRITO (Zesch and Horbach, 2018) toolkit and extended it with readers and tokenization for Chinese text. ESCRITO is a publicly available general-purpose scoring framework based on DKPro TC (Daxenberger et al., 2014), which uses an SVM classifier (Cortes and Vapnik, 1995) using the SMO algorithm as provided by WEKA (Witten et al., 1999). For all kinds of features, we use the top 10000 most frequent 352 Data Set CESA ASAP-ZH ASAP-ZHM T ID 5 10 10 Score Example 2 The machine summarizes a large amount of data and finds the pattern from it 机器总结大量数据，从中找到规律 1 Machines can learn things by themselves 机器能自己学习东西 0 Let the machine learn"
2020.aacl-main.37,W12-2022,0,0.0183728,"in several settings on these data sets. Results show that features on lower segmentation levels such as character n-grams tend to have better performance than features on token level. 1 Introduction Short answer questions are a type of educational assessment that requires respondents to give natural language answers in response to a question or some reading material (Rademakers et al., 2005). The applications used to automatically score such questions are usually thought of as content scoring systems, because content (and not linguistic form) is taken into consideration for automatic scoring (Ziai et al., 2012). While there is a large research body for English content scoring, there is less research for Chinese.1 The largest obstacle for more research on Chinese is the lack of publicly available data sets of Chinese short answer questions. Language Level Unigrams English word characters panda p, a, n, d, a Chinese word characters 熊猫 熊，猫 components radicals 灬，犭 strokes ... Table 1: Comparison of segmentation possibilities in English and Chinese 1 In this work, we use the term ‘Chinese’ as abbreviation for Mandarin Chinese, which includes simplified and traditional written Chinese. Cantonese, Wu, Min"
2020.coling-main.76,S13-2046,0,0.0643111,"Missing"
2020.coling-main.76,W17-5908,1,0.884272,"Missing"
2020.coling-main.76,N18-1170,0,0.025134,"(Ebrahimi et al., 2017) generates textual adversarial examples by swapping one character with another by gradient computation, and Textbugger (Li et al., 2018) by inserting spaces or swapping random letters. These methods are quite likely to leave the semantics of the underlying text unchanged, which is much harder to achieve when texts are manipulated on the token level. A common strategy is to replace single words (usually nouns) with near-synonyms chosen by humans (Kuleshov et al., 2018) or as nearest neighbors in an embedding space (Pennington et al., 2014). At the sentence level, SCPNs (Iyyer et al., 2018) produce a paraphrase of a given sentence without changing the original meaning. In the educational domain, this is comparable to unusual, atypical or creative correct answers that run the risk of being classified as incorrect (Yoon et al., 2018). However, none of the generation methods above is suitable to our research. If we use a real, correct answer as starting point for transformation into an adversarial one and leave the semantic meaning unchanged, we generate a correct answer. But we need answers which are definitely wrong in content as adversarial answers, in order to test the system’s"
2020.coling-main.76,D14-1162,0,0.0833981,"owing the similar principle in NLP applications, HotFlip (Ebrahimi et al., 2017) generates textual adversarial examples by swapping one character with another by gradient computation, and Textbugger (Li et al., 2018) by inserting spaces or swapping random letters. These methods are quite likely to leave the semantics of the underlying text unchanged, which is much harder to achieve when texts are manipulated on the token level. A common strategy is to replace single words (usually nouns) with near-synonyms chosen by humans (Kuleshov et al., 2018) or as nearest neighbors in an embedding space (Pennington et al., 2014). At the sentence level, SCPNs (Iyyer et al., 2018) produce a paraphrase of a given sentence without changing the original meaning. In the educational domain, this is comparable to unusual, atypical or creative correct answers that run the risk of being classified as incorrect (Yoon et al., 2018). However, none of the generation methods above is suitable to our research. If we use a real, correct answer as starting point for transformation into an adversarial one and leave the semantic meaning unchanged, we generate a correct answer. But we need answers which are definitely wrong in content as"
2020.coling-main.76,W19-4411,1,0.822592,"ems that represent what is usually applied in practice – so we can find typical problems – instead of highly optimized systems whose vulnerabilities to adversarial input might be highly idiosyncratic. As a representative state-of-the-art shallow system, we selected the ESCRITO scoring toolkit (Zesch and Horbach, 2018) with an SVM classifier (Cortes and Vapnik, 1995), as implemented in Weka using the default PolyKernel. As features we used the top 10000 character 2-5 grams, the top 10000 word 1-5 grams, and answer length. As a deep learning system, we employed the RNN-based system described in Riordan et al. (2019). It uses pretrained word embeddings encoded by a single layer 250-dimensional bidirectional GRU. The hidden states of the GRU are aggregated by a max pooling mechanism. The output of the encoder is aggregated in a fully-connected feedforward layer with sigmoid activation that computes a scalar output for the predicted score. Characters are encoded with a sequence of 25-dimensional character embeddings (randomly initialized) followed by a convolutional neural network (100 filters and filter sizes of (3,4,5)). The character embeddings are concatenated with the word embeddings prior to the word-"
2020.coling-main.76,N18-3008,1,0.834324,"e the semantics of the underlying text unchanged, which is much harder to achieve when texts are manipulated on the token level. A common strategy is to replace single words (usually nouns) with near-synonyms chosen by humans (Kuleshov et al., 2018) or as nearest neighbors in an embedding space (Pennington et al., 2014). At the sentence level, SCPNs (Iyyer et al., 2018) produce a paraphrase of a given sentence without changing the original meaning. In the educational domain, this is comparable to unusual, atypical or creative correct answers that run the risk of being classified as incorrect (Yoon et al., 2018). However, none of the generation methods above is suitable to our research. If we use a real, correct answer as starting point for transformation into an adversarial one and leave the semantic meaning unchanged, we generate a correct answer. But we need answers which are definitely wrong in content as adversarial answers, in order to test the system’s ability of rejecting cheating behaviour. We start with basic methods like uniform selection of characters and words from a generic corpus, but also use methods that are specific to the educational task like shuffling words in correct answers. Th"
2020.coling-main.76,L18-1365,1,0.847326,"ersarial examples for this investigation. 3 https://github.com/graykode/gpt-2-Pytorch 885 as described in the previous section. 3.1 Scoring Systems Automatic scoring systems can be categorized into shallow and deep learning systems (Collobert and Weston, 2008). For our study, we focused on ‘typical’ systems that represent what is usually applied in practice – so we can find typical problems – instead of highly optimized systems whose vulnerabilities to adversarial input might be highly idiosyncratic. As a representative state-of-the-art shallow system, we selected the ESCRITO scoring toolkit (Zesch and Horbach, 2018) with an SVM classifier (Cortes and Vapnik, 1995), as implemented in Weka using the default PolyKernel. As features we used the top 10000 character 2-5 grams, the top 10000 word 1-5 grams, and answer length. As a deep learning system, we employed the RNN-based system described in Riordan et al. (2019). It uses pretrained word embeddings encoded by a single layer 250-dimensional bidirectional GRU. The hidden states of the GRU are aggregated by a max pooling mechanism. The output of the encoder is aggregated in a fully-connected feedforward layer with sigmoid activation that computes a scalar ou"
2020.coling-main.76,W15-0615,1,0.890875,"Missing"
2020.coling-main.76,W12-2022,0,0.0509054,"Missing"
2020.lrec-1.709,S12-1051,0,0.0340425,"n which the information of the Premise (2a) entails the information of the Hypothesis (2b). Contradiction is a symmetrical relation between two texts that cannot be true at the same time (3a and 3b)1 . Specificity is a directional relation between two texts in which one text is more precise (4a) and the other is more vague (4b). 1 a) Education is equal for all children. b) All children get the same education. 2 a) All children get the same education. b) Education exists. Natural Language Processing (NLP). Multiple datasets exist for each of these tasks (Dolan et al., 2004; Dagan et al., 2006; Agirre et al., 2012; Ganitkevitch et al., 2013; Bowman et al., 2015; Iyer et al., 2017; Lan et al., 2017; Kovatchev et al., 2018a). These tasks are also related to the more general problem of Natural Language Understanding (NLU) and are part of the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018). Recently, several researchers have argued that a single label such as “paraphrasing”, “textual entailment”, or “similarity” is not enough to characterize and understand the meaning relation (Sammons et al., 2010; Bhagat and Hovy, 2013; Vila et al., 2014; Cabrio and Magnini, 2013; LopezGazp"
2020.lrec-1.709,C18-1226,0,0.017357,"nsidering only one meaning relation (paraphrasing, textual entailment, textual similarity). This follows the traditional approach in the research on meaning relations: each relation is studied in isolation, with its own theoretical concepts, datasets, and practical tasks. In recent years, the ”single relation” approach has been questioned by several authors. Androutsopoulos and Malakasiotis (2010) analyze the relations between paraphrasing and textual entailment. Marelli et al. (2014) present SICK: a corpus that studies entailment, contradiction, and semantic similarity. Lan and Xu (2018) and Aldarmaki and Diab (2018) explore the transfer learning capabilities between paraphrasing and textual entailment. Gold et al. (2019) present a corpus that is annotated for paraphrasing, textual entailment, contradiction, specificity, and textual similarity. These works demonstrate that the different meaning relations can be studied together and can benefit from one another. However, to date, the joint research of meaning relations is limited only to the binary textual labels. There has been no work on comparing the different typologies and the way different relations can be decomposed. None of the existing typologies"
2020.lrec-1.709,benikova-zesch-2017-different,1,0.927698,"al., 2013; Bowman et al., 2015; Iyer et al., 2017; Lan et al., 2017; Kovatchev et al., 2018a). These tasks are also related to the more general problem of Natural Language Understanding (NLU) and are part of the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018). Recently, several researchers have argued that a single label such as “paraphrasing”, “textual entailment”, or “similarity” is not enough to characterize and understand the meaning relation (Sammons et al., 2010; Bhagat and Hovy, 2013; Vila et al., 2014; Cabrio and Magnini, 2013; LopezGazpio et al., 2016; Benikova and Zesch, 2017; Kovatchev et al., 2018a). These authors demonstrate that the different instances of meaning relations require different capabilities and linguistic knowledge. For example, the pairs 5 and 6 are both examples of a “paraphrasing” relation. However determining the relation in 5a–5b only requires lexical knowledge, while syntactic knowledge is also needed for correctly predicting the relation in 6a–6b. This distinction cannot be captured by a single “paraphrasing” label. The lack of distinction between such examples can be a problem in error analysis and in downstream applications. 5 a) Educatio"
2020.lrec-1.709,D15-1075,0,0.27558,"ils the information of the Hypothesis (2b). Contradiction is a symmetrical relation between two texts that cannot be true at the same time (3a and 3b)1 . Specificity is a directional relation between two texts in which one text is more precise (4a) and the other is more vague (4b). 1 a) Education is equal for all children. b) All children get the same education. 2 a) All children get the same education. b) Education exists. Natural Language Processing (NLP). Multiple datasets exist for each of these tasks (Dolan et al., 2004; Dagan et al., 2006; Agirre et al., 2012; Ganitkevitch et al., 2013; Bowman et al., 2015; Iyer et al., 2017; Lan et al., 2017; Kovatchev et al., 2018a). These tasks are also related to the more general problem of Natural Language Understanding (NLU) and are part of the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018). Recently, several researchers have argued that a single label such as “paraphrasing”, “textual entailment”, or “similarity” is not enough to characterize and understand the meaning relation (Sammons et al., 2010; Bhagat and Hovy, 2013; Vila et al., 2014; Cabrio and Magnini, 2013; LopezGazpio et al., 2016; Benikova and Zesch, 2017; Kovat"
2020.lrec-1.709,C04-1051,0,0.258013,"directional relation between two texts in which the information of the Premise (2a) entails the information of the Hypothesis (2b). Contradiction is a symmetrical relation between two texts that cannot be true at the same time (3a and 3b)1 . Specificity is a directional relation between two texts in which one text is more precise (4a) and the other is more vague (4b). 1 a) Education is equal for all children. b) All children get the same education. 2 a) All children get the same education. b) Education exists. Natural Language Processing (NLP). Multiple datasets exist for each of these tasks (Dolan et al., 2004; Dagan et al., 2006; Agirre et al., 2012; Ganitkevitch et al., 2013; Bowman et al., 2015; Iyer et al., 2017; Lan et al., 2017; Kovatchev et al., 2018a). These tasks are also related to the more general problem of Natural Language Understanding (NLU) and are part of the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018). Recently, several researchers have argued that a single label such as “paraphrasing”, “textual entailment”, or “similarity” is not enough to characterize and understand the meaning relation (Sammons et al., 2010; Bhagat and Hovy, 2013; Vila et al.,"
2020.lrec-1.709,N13-1092,0,0.0414273,"on of the Premise (2a) entails the information of the Hypothesis (2b). Contradiction is a symmetrical relation between two texts that cannot be true at the same time (3a and 3b)1 . Specificity is a directional relation between two texts in which one text is more precise (4a) and the other is more vague (4b). 1 a) Education is equal for all children. b) All children get the same education. 2 a) All children get the same education. b) Education exists. Natural Language Processing (NLP). Multiple datasets exist for each of these tasks (Dolan et al., 2004; Dagan et al., 2006; Agirre et al., 2012; Ganitkevitch et al., 2013; Bowman et al., 2015; Iyer et al., 2017; Lan et al., 2017; Kovatchev et al., 2018a). These tasks are also related to the more general problem of Natural Language Understanding (NLU) and are part of the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018). Recently, several researchers have argued that a single label such as “paraphrasing”, “textual entailment”, or “similarity” is not enough to characterize and understand the meaning relation (Sammons et al., 2010; Bhagat and Hovy, 2013; Vila et al., 2014; Cabrio and Magnini, 2013; LopezGazpio et al., 2016; Benikova a"
2020.lrec-1.709,P18-2103,0,0.014068,"that more frequent and relatively simple types like “lexical substitution”, “punctuation changes” and “modal verb changes” are easier across multiple automated PI systems, while other types like “negation switching”, “ellipsis” and “named entity reasoning” are much more challenging. Similar observations have been made in the field of Textual Entailment. Gururangan et al. (2018) discovered the presence of annotation artifacts that enable models that take into account only one of the texts (the hypothesis) to achieve performance substantially higher than the majority baselines in SNLI and MNLI. Glockner et al. (2018) showed that models trained with SNLI fail to resolve new pairs that require simple lexical substitution. Naik et al. (2018) create label-preserving adversarial examples and conclude that automated NLI models are not robust. Wallace et al. (2019) introduce universal triggers, that is, sequences of tokens that fool models when concatenated to any input. All these authors identify different problems and biases in the datasets and the systems trained on them. However they focus on a single phenomenon and/or a specific linguistic construction. A typology-based approach can evaluate the performance"
2020.lrec-1.709,W19-4004,1,0.77104,"(typology, annotation guidelines, and annotated corpus) to the community. Keywords: Paraphrasing, Textual Entailment, Specificity 1. Introduction This paper proposes a new approach for the decomposition of textual meaning relations. Instead of focusing on a single meaning relation we demonstrate that Paraphrasing, Textual Entailment, Contradiction, and Specificity can all be decomposed to a set of simpler and easier-to-define linguistic and reason-based phenomena. The set of “atomic” phenomena is shared across all relations. In this paper, we adopt the definitions of meaning relations used by Gold et al. (2019). Paraphrasing is a symmetrical relation between two differently worded texts with approximately the same content (1a and 1b). Textual Entailment is a directional relation between two texts in which the information of the Premise (2a) entails the information of the Hypothesis (2b). Contradiction is a symmetrical relation between two texts that cannot be true at the same time (3a and 3b)1 . Specificity is a directional relation between two texts in which one text is more precise (4a) and the other is more vague (4b). 1 a) Education is equal for all children. b) All children get the same educati"
2020.lrec-1.709,N18-2017,0,0.021508,"involved in that pair. That is, they showed that state-of-the-art automatic PI systems process “atomic paraphrases” in a different manner and with a statistically significant difference in quantitative performance (Accuracy and F1). They show that more frequent and relatively simple types like “lexical substitution”, “punctuation changes” and “modal verb changes” are easier across multiple automated PI systems, while other types like “negation switching”, “ellipsis” and “named entity reasoning” are much more challenging. Similar observations have been made in the field of Textual Entailment. Gururangan et al. (2018) discovered the presence of annotation artifacts that enable models that take into account only one of the texts (the hypothesis) to achieve performance substantially higher than the majority baselines in SNLI and MNLI. Glockner et al. (2018) showed that models trained with SNLI fail to resolve new pairs that require simple lexical substitution. Naik et al. (2018) create label-preserving adversarial examples and conclude that automated NLI models are not robust. Wallace et al. (2019) introduce universal triggers, that is, sequences of tokens that fool models when concatenated to any input. All"
2020.lrec-1.709,L18-1221,1,0.882192,"Missing"
2020.lrec-1.709,C18-2029,1,0.895494,"Missing"
2020.lrec-1.709,C18-1328,0,0.0127663,"typology is created considering only one meaning relation (paraphrasing, textual entailment, textual similarity). This follows the traditional approach in the research on meaning relations: each relation is studied in isolation, with its own theoretical concepts, datasets, and practical tasks. In recent years, the ”single relation” approach has been questioned by several authors. Androutsopoulos and Malakasiotis (2010) analyze the relations between paraphrasing and textual entailment. Marelli et al. (2014) present SICK: a corpus that studies entailment, contradiction, and semantic similarity. Lan and Xu (2018) and Aldarmaki and Diab (2018) explore the transfer learning capabilities between paraphrasing and textual entailment. Gold et al. (2019) present a corpus that is annotated for paraphrasing, textual entailment, contradiction, specificity, and textual similarity. These works demonstrate that the different meaning relations can be studied together and can benefit from one another. However, to date, the joint research of meaning relations is limited only to the binary textual labels. There has been no work on comparing the different typologies and the way different relations can be decomposed. No"
2020.lrec-1.709,D17-1126,0,0.015361,"b). Contradiction is a symmetrical relation between two texts that cannot be true at the same time (3a and 3b)1 . Specificity is a directional relation between two texts in which one text is more precise (4a) and the other is more vague (4b). 1 a) Education is equal for all children. b) All children get the same education. 2 a) All children get the same education. b) Education exists. Natural Language Processing (NLP). Multiple datasets exist for each of these tasks (Dolan et al., 2004; Dagan et al., 2006; Agirre et al., 2012; Ganitkevitch et al., 2013; Bowman et al., 2015; Iyer et al., 2017; Lan et al., 2017; Kovatchev et al., 2018a). These tasks are also related to the more general problem of Natural Language Understanding (NLU) and are part of the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018). Recently, several researchers have argued that a single label such as “paraphrasing”, “textual entailment”, or “similarity” is not enough to characterize and understand the meaning relation (Sammons et al., 2010; Bhagat and Hovy, 2013; Vila et al., 2014; Cabrio and Magnini, 2013; LopezGazpio et al., 2016; Benikova and Zesch, 2017; Kovatchev et al., 2018a). These authors de"
2020.lrec-1.709,P11-2057,0,0.375566,"escribes the annotation process - the corpus, the annotation guidelines, and the annotation interface. Section 5. shows the results of the annotation. Section 6. discusses the implications of the findings and the way our results relate to our objectives and research questions. Finally, Section 7. concludes the paper and addresses the future work. 2. Related Work The last several years have seen an increasing interest towards the decomposition of paraphrasing (Bhagat and Hovy, 2013; Vila et al., 2014; Benikova and Zesch, 2017; Kovatchev et al., 2018a), textual entailment (Sammons et al., 2010; LoBue and Yates, 2011; Cabrio and Magnini, 2013), and textual similarity (Lopez-Gazpio et al., 2016). Sammons et al. (2010) argue that in order to process a complex meaning relation such as textual entailment a competent speaker has to take several “inference steps”. This means that a meta-relation such as paraphrasing, textual entailment, or semantic similarity can be “decomposed” or broken down into such “inference steps”. These “inference steps”, traditionally called “types” can be either linguistic or reason-based in their nature. The linguistic types require certain linguistic capabilities from the speaker, w"
2020.lrec-1.709,marelli-etal-2014-sick,0,0.032807,"limitation of the different decompositional approaches is that there exist many different typologies and each typology is created considering only one meaning relation (paraphrasing, textual entailment, textual similarity). This follows the traditional approach in the research on meaning relations: each relation is studied in isolation, with its own theoretical concepts, datasets, and practical tasks. In recent years, the ”single relation” approach has been questioned by several authors. Androutsopoulos and Malakasiotis (2010) analyze the relations between paraphrasing and textual entailment. Marelli et al. (2014) present SICK: a corpus that studies entailment, contradiction, and semantic similarity. Lan and Xu (2018) and Aldarmaki and Diab (2018) explore the transfer learning capabilities between paraphrasing and textual entailment. Gold et al. (2019) present a corpus that is annotated for paraphrasing, textual entailment, contradiction, specificity, and textual similarity. These works demonstrate that the different meaning relations can be studied together and can benefit from one another. However, to date, the joint research of meaning relations is limited only to the binary textual labels. There ha"
2020.lrec-1.709,C18-1198,0,0.016336,"asier across multiple automated PI systems, while other types like “negation switching”, “ellipsis” and “named entity reasoning” are much more challenging. Similar observations have been made in the field of Textual Entailment. Gururangan et al. (2018) discovered the presence of annotation artifacts that enable models that take into account only one of the texts (the hypothesis) to achieve performance substantially higher than the majority baselines in SNLI and MNLI. Glockner et al. (2018) showed that models trained with SNLI fail to resolve new pairs that require simple lexical substitution. Naik et al. (2018) create label-preserving adversarial examples and conclude that automated NLI models are not robust. Wallace et al. (2019) introduce universal triggers, that is, sequences of tokens that fool models when concatenated to any input. All these authors identify different problems and biases in the datasets and the systems trained on them. However they focus on a single phenomenon and/or a specific linguistic construction. A typology-based approach can evaluate the performance and robustness of automated systems on a large variety of tasks. One limitation of the different decompositional approaches"
2020.lrec-1.709,P10-1122,0,0.0999767,"Missing"
2020.lrec-1.709,D19-1221,0,0.0119239,"easoning” are much more challenging. Similar observations have been made in the field of Textual Entailment. Gururangan et al. (2018) discovered the presence of annotation artifacts that enable models that take into account only one of the texts (the hypothesis) to achieve performance substantially higher than the majority baselines in SNLI and MNLI. Glockner et al. (2018) showed that models trained with SNLI fail to resolve new pairs that require simple lexical substitution. Naik et al. (2018) create label-preserving adversarial examples and conclude that automated NLI models are not robust. Wallace et al. (2019) introduce universal triggers, that is, sequences of tokens that fool models when concatenated to any input. All these authors identify different problems and biases in the datasets and the systems trained on them. However they focus on a single phenomenon and/or a specific linguistic construction. A typology-based approach can evaluate the performance and robustness of automated systems on a large variety of tasks. One limitation of the different decompositional approaches is that there exist many different typologies and each typology is created considering only one meaning relation (paraphr"
2020.lrec-1.709,W18-5446,0,0.0232102,"r is more vague (4b). 1 a) Education is equal for all children. b) All children get the same education. 2 a) All children get the same education. b) Education exists. Natural Language Processing (NLP). Multiple datasets exist for each of these tasks (Dolan et al., 2004; Dagan et al., 2006; Agirre et al., 2012; Ganitkevitch et al., 2013; Bowman et al., 2015; Iyer et al., 2017; Lan et al., 2017; Kovatchev et al., 2018a). These tasks are also related to the more general problem of Natural Language Understanding (NLU) and are part of the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018). Recently, several researchers have argued that a single label such as “paraphrasing”, “textual entailment”, or “similarity” is not enough to characterize and understand the meaning relation (Sammons et al., 2010; Bhagat and Hovy, 2013; Vila et al., 2014; Cabrio and Magnini, 2013; LopezGazpio et al., 2016; Benikova and Zesch, 2017; Kovatchev et al., 2018a). These authors demonstrate that the different instances of meaning relations require different capabilities and linguistic knowledge. For example, the pairs 5 and 6 are both examples of a “paraphrasing” relation. However determining the rel"
2021.bea-1.19,Q14-1040,1,0.60349,"fast to take and to evaluate. Thus, they are even used in commercial systems such as onDaF (Grotjahn, 2010). Despite these advantages, the creation of a wellworking c-test is a time-consuming task even for language experts. One reason is that the gap scheme is influenced e.g. by Named Entities or other words that would be hard to guess for learners. Improved tool support for curating c-tests has mitigated this problem to some extent (Zesch et al., 2018). Even with technological support, it is still hard (even for experts) to judge the difficulty of a given test without running a pilot study (Beinborn et al., 2014). Thus, there has been research on predicting (Beinborn et al., 2014) or manipulating (Lee et al., 2019) the difficulty of c-tests. These approaches heavily rely on the availability of training data, i.e. c-tests which have been taken by enough learners 2 User Perspective Figure 1 illustrates the interaction steps of a user with the application. Self Assessment To allow for an easy start with the tool and to avoid data security issues, we do not create user profiles, but ask users only for the language they want to learn and a self-assessment into one out of six proficiency levels, roughly mea"
2021.bea-1.19,P19-1035,0,0.0195974,"Despite these advantages, the creation of a wellworking c-test is a time-consuming task even for language experts. One reason is that the gap scheme is influenced e.g. by Named Entities or other words that would be hard to guess for learners. Improved tool support for curating c-tests has mitigated this problem to some extent (Zesch et al., 2018). Even with technological support, it is still hard (even for experts) to judge the difficulty of a given test without running a pilot study (Beinborn et al., 2014). Thus, there has been research on predicting (Beinborn et al., 2014) or manipulating (Lee et al., 2019) the difficulty of c-tests. These approaches heavily rely on the availability of training data, i.e. c-tests which have been taken by enough learners 2 User Perspective Figure 1 illustrates the interaction steps of a user with the application. Self Assessment To allow for an easy start with the tool and to avoid data security issues, we do not create user profiles, but ask users only for the language they want to learn and a self-assessment into one out of six proficiency levels, roughly meant to correspond to the six CEFR levels (Council of Europe, 2009). Optionally, a user can provide age an"
2021.konvens-1.18,N19-1009,0,0.0476472,"Missing"
2021.konvens-1.18,W17-1319,0,0.0409447,"Missing"
2021.konvens-1.18,W17-1606,0,0.0684039,"Missing"
2021.konvens-1.19,2020.lrec-1.520,0,0.0199476,"imple shapes in the context of Dataset Hours Speakers Pre-training English &gt;6,500 ? Transfer German Swiss German 315 70 4,823 191 Table 1: Overview of datasets image processing or simple sounds in the context of ASR, these layers can be frozen completely during fine-tuning. 3 Experimental Setup In our experiments, we transfer an English pretrained version of DeepSpeech to German and to Swiss German data and observe the impact of freezing fewer or more layers during training. 3.1 Datasets We trained the models for (standard) German on the German part of the Mozilla Common Voice speech dataset (Ardila et al., 2020). The utterances are typically between 3 and 5 seconds long and are collected from and reviewed by volunteers. This collection method entails a rather high number of speakers and quite some noise. The Swiss German models were trained on the data provided by Pl¨uss et al. (2020). This speech data was collected from speeches at the Bernese parliament. The English pre-trained model was trained by Mozilla on a combination of English speech datasets, including LibriSpeech and Common Voice English.2 The datasets for all three languages are described in Table 1. For inference and testing we used the"
2021.konvens-1.19,W11-2123,0,0.0124176,"ypically between 3 and 5 seconds long and are collected from and reviewed by volunteers. This collection method entails a rather high number of speakers and quite some noise. The Swiss German models were trained on the data provided by Pl¨uss et al. (2020). This speech data was collected from speeches at the Bernese parliament. The English pre-trained model was trained by Mozilla on a combination of English speech datasets, including LibriSpeech and Common Voice English.2 The datasets for all three languages are described in Table 1. For inference and testing we used the language model KenLM (Heafield, 2011), trained on the corpus described by Radeck-Arneth et al. (2015, Section 3.2). This corpus consists of a mixture of texts from the sources Wikipedia and Europarl as well as crawled sentences. The whole corpus was preprocessed with MaryTTS (Schr¨oder and Trouvain, 2003). 3.2 ASR Architecture We use Mozilla’s DeepSpeech version 0.7 for our experiments. The implementation differs in many ways from the original model presented by Hannun et al. (2014). The architecture is described in detail in the official documentation3 and is depicted in Figure 1. From the raw speech data, MelFrequency Cepstral"
2021.konvens-1.19,2020.iwclul-1.5,0,0.0161817,"terpretation as transforming the LSTM hidden state into character-level information. This stage should be equivalent across languages, as long as the LSTM hidden state is learned accordingly, which is ensured by not freezing the LSTM. For all models, we reinitialize the last layer, because of the different alphabet sizes of German / Swiss German and https://github.com/mozilla/DeepSpeech/releases/tag/v0.7.0 https://deepspeech.readthedocs.io/en/latest/DeepSpeech.html 4 https://github.com/mozilla/DeepSpeech/releases English (¨a, o¨ , u¨ ), but don’t reinitialize any other layers (as done e.g. by Hjortnaes et al. (2020)). The complete training script, as well as the modified versions of DeepSpeech that utilize layer freezing are available online5 . The weights were frozen by adding trainable=False at the appropriate places in the TensorFlow code, though some other custom modifications were necessary and are described online5 . For Swiss German, we do not train the network on the German dataset first and transfer from German to Swiss German, as this has been shown to lead to worse results (Agarwal and Zesch, 2020). 3.4 Results & Discussion Results of our baselines are very close to the values reported for Ger"
2021.konvens-1.19,W17-2620,0,0.0289945,"computational resources. To remedy this, it is often advantageous to employ transfer learning: Instead of initializing the parameters of the network randomly, the optimized parameters of a network trained on a similar task are reused. 1 https://github.com/mozilla/DeepSpeech Those parameters can then be fine-tuned to the specific task at hand, using less data and fewer computational resources. In the fine-tuning process many parameters of the original model may be “frozen”, i.e. held constant during training. This can speed up training and improve results when less training data is available (Kunze et al., 2017). The idea of taking deep neural networks trained on large datasets and fine-tuning them on tasks with less available training data has been popular in computer vision for years (Huh et al., 2016). More recently, with the emergence of end-to-end deep neural networks for automatic speech recognition (like DeepSpeech), it has also been used in this area (Kunze et al., 2017; Li et al., 2019). Deep neural networks learn representations of the input data in a hierarchical manner. The input is transformed into simplistic features in the first layers of a neural network and into more complex features"
2021.teachingnlp-1.6,2020.acl-tutorials.2,0,0.0300224,"is crucial for NLP researchers, developers, and deciders to be aware of the social implications of deploying a particular piece of language technology. They should be able to analyze the degree to which a setup conforms with ethical principles, which guide what is considered ‘right’ or ‘wrong’ (Deigh, 2010), and be aware of the potentially harmful side even of components developed with the aim of supporting humans. As a consequence, it is crucial to start early and embed ethics into the NLP curriculum to be taught along with the technological and linguistic material in an interactive manner (Bender et al., 2020). 1 https://gscl.org/en/resources/ethics-crash-course 49 Proceedings of the Fifth Workshop on Teaching NLP, pages 49–51 June 10–11, 2021. ©2021 Association for Computational Linguistics text of the provided comments such that other lecturers can easily work with it, and contains many linked exercises. The Markkula Center for Applied Ethics at Santa Clara University offers a slide set with a crash course on ethics along with materials for discussions focusing of ethics in the general area of technology.2 Our intention is very similar, but the focus is on NLP-specific issues. We found most exist"
2021.teachingnlp-1.6,D19-2004,0,0.0266506,"aterials for discussions focusing of ethics in the general area of technology.2 Our intention is very similar, but the focus is on NLP-specific issues. We found most existing freely-available courses on ethics in NLP to go into depth and usually span an entire semester.3 In addition, we are aware of several recent tutorials at ACL venues. Most similar to our materials is the tutorial on socially responsible NLP by Tsvetkov et al. (2018), which is a condensed version of a full-semester class.4 Our course differs from this tutorial in length and by mostly concentrating on NLP-specific examples. Chang et al. (2019) focus on sub-topics of ethical NLP such as fairness and mitigating bias.5 Bender et al. (2020) have started a collection of pointers to existing materials, as well as general pointers for teaching ethics for NLP.6 This collection has been a very valuable starting point for our own research. Ethical Considerations. Admittedly, a potential issue is that teachers could just “check off” the topic by using our material without deeper engagement, individual reading or reasoning. However, we believe that having a good starting point will actually lead to both teachers and students doing more researc"
2021.teachingnlp-1.6,P16-2096,0,0.0294809,"tended to be integrated into introductory NLP or computational linguistics courses. By making this material freely available, we aim at lowering the threshold to adding ethics to the curriculum. We hope that increased awareness will enable students to identify potentially unethical behavior. 1 Motivation and Overview The recent sharp rise in the capabilities of natural language processing (NLP) methods has led to a wide application of language technology, influencing many aspects of our daily lives. As a result, NLP technology can have considerable real-world consequences (Vitak et al., 2016; Hovy and Spruit, 2016). While language technology has the aim of supporting humans, mis-use of data or abuse of subjects, mis-representation, or direct harm are some of numerous potentially critical problems. Hence, it is crucial for NLP researchers, developers, and deciders to be aware of the social implications of deploying a particular piece of language technology. They should be able to analyze the degree to which a setup conforms with ethical principles, which guide what is considered ‘right’ or ‘wrong’ (Deigh, 2010), and be aware of the potentially harmful side even of components developed with the aim of sup"
2021.teachingnlp-1.6,W19-3637,0,0.0164934,"ions. Admittedly, a potential issue is that teachers could just “check off” the topic by using our material without deeper engagement, individual reading or reasoning. However, we believe that having a good starting point will actually lead to both teachers and students doing more research on this subject, and our companion material emphasizes the benefit of digging deeper. 2 of the relevant terminology and concepts. They should understand that there are different ethical theories at interplay with the field of NLP which are currently developing best practices for ethical conduct and systems (Prabhumoye et al., 2019). As a result, they should be able to critically reflect the on-going discourse in the community and, last but not least, have acquired the basis for behaving ethically in their own work. It is not our goal to provide ‘ultimate’ definitions of the concepts and we strongly advise lecturers not to pose exam questions aiming at memorizing definitions. Instead, the learning goals could be tested in the form of group presentations or written essays. The predominant principle behind the design of our crash course is activation. Besides giving clear descriptions of relevant concepts and terminology,"
2021.teachingnlp-1.6,N18-6005,0,0.0281931,"asily work with it, and contains many linked exercises. The Markkula Center for Applied Ethics at Santa Clara University offers a slide set with a crash course on ethics along with materials for discussions focusing of ethics in the general area of technology.2 Our intention is very similar, but the focus is on NLP-specific issues. We found most existing freely-available courses on ethics in NLP to go into depth and usually span an entire semester.3 In addition, we are aware of several recent tutorials at ACL venues. Most similar to our materials is the tutorial on socially responsible NLP by Tsvetkov et al. (2018), which is a condensed version of a full-semester class.4 Our course differs from this tutorial in length and by mostly concentrating on NLP-specific examples. Chang et al. (2019) focus on sub-topics of ethical NLP such as fairness and mitigating bias.5 Bender et al. (2020) have started a collection of pointers to existing materials, as well as general pointers for teaching ethics for NLP.6 This collection has been a very valuable starting point for our own research. Ethical Considerations. Admittedly, a potential issue is that teachers could just “check off” the topic by using our material wi"
2021.unimplicit-1.2,Q19-1030,0,0.0217603,"4). They report only a minor effect on automatic scoring performance. In this paper, we analyse which implicit phenomena occur in short answer scoring datasets. We then analyze the impact of implicit language on automatic scoring performance. 2 – Pandas are highly specialized. Koalas are, too. – Pandas are highly specialized. Koalas are highly specialized, too. Example 3: Ellipsis Numeric Terms In numeric expressions, the head word, i.e. usually the measurement unit, can often be left out. In cases with parallelism to a previous sentence this is a sub-type of an ellipsis, in others it is not (Elazar and Goldberg, 2019). Example 4 shows an instance of the latter case, where the implication is that this sentence talks about age, indicated by the use of turn in front of 30. Instead of saying that pandas turn 30 years old, this is shortened to saying that they turn 30. Implicit Language in Learner Answers There are a number of linguistic phenomena that pertain to the implicitness of language and are especially relevant for learner answers. In the following, we describe the ones we considered as candidates for our analysis. Coreference Coreference describes the phenomenon that the same entity is referred to seve"
2021.unimplicit-1.2,W08-0913,0,0.0422868,"ers and entities in the question and similarly target ellipses resolution, where part of the question is implied in the learner answer, both by aligning concepts from the learner answer to the question. They Introduction Automatic short answer scoring is an application area of natural language processing where short free-form answers written by students in an educational context are automatically scored based on the correctness of their content. They occur for example in science education (Nielsen et al., 2008; Dzikovska et al., 2010), but also in foreign language learning to measure reading (Bailey and Meurers, 2008; Meurers et al., 2011) or listening comprehension (Horbach et al., 2014). In such a scoring task, answers are graded based on their content alone - in comparison to essay scoring (Attali and Burstein, 2006) where also linguistic form is taken into consideration. Thus, judging whether an answer is correct or not may require the resolution of a number of implicit language phenomena as a form of normalization. Figure 1 11 Proceedings of the 1st Workshop on Understanding Implicit and Underspecified Language (UnImplicit 2021), pages 11–19 Bangkok, Thailand (online), August 5, 2021. ©2021 Associati"
2021.unimplicit-1.2,Q13-1032,0,0.0118745,"xtracting explicit versions of the answers regarding the different phenomena from the implicit versions. Pandas no Example 6: Presupposition Restrictive vs. Non-restrictive Remarks Any appositional adjective and any relative clause (Fabb, 1990) can either be restrictive, i.e. necessary for selecting the right entity out of a set of alternatives or non-restrictive. In the question Explain how pandas in China are similar 3.1 For our annotations we needed publicly available short-answer data in English where answers are full sentences and not only single phrases like in the Powergrading dataset (Basu et al., 2013). Ideally, there should be a larger amount of answers for a single prompt so that prompt-specific models can be trained later in Section 4. (For an overview of publicly available shortanswer datasets, see Horbach and Zesch (2019).) We consider two short answer datasets in our analysis. The first one is the Student Response Analysis Corpus (SRA) of the 2013 SemEval task 7 (Dzikovska et al., 2013). It consists of data from two different sources. The Beetle subset has 3k student answers to 56 questions about electricity and electronics. The SciEntsBank subset contains 10k student answers to 197 q"
2021.unimplicit-1.2,horbach-etal-2014-finding,1,0.820347,"here part of the question is implied in the learner answer, both by aligning concepts from the learner answer to the question. They Introduction Automatic short answer scoring is an application area of natural language processing where short free-form answers written by students in an educational context are automatically scored based on the correctness of their content. They occur for example in science education (Nielsen et al., 2008; Dzikovska et al., 2010), but also in foreign language learning to measure reading (Bailey and Meurers, 2008; Meurers et al., 2011) or listening comprehension (Horbach et al., 2014). In such a scoring task, answers are graded based on their content alone - in comparison to essay scoring (Attali and Burstein, 2006) where also linguistic form is taken into consideration. Thus, judging whether an answer is correct or not may require the resolution of a number of implicit language phenomena as a form of normalization. Figure 1 11 Proceedings of the 1st Workshop on Understanding Implicit and Underspecified Language (UnImplicit 2021), pages 11–19 Bangkok, Thailand (online), August 5, 2021. ©2021 Association for Computational Linguistics report a positive influence on overall s"
2021.unimplicit-1.2,P10-4003,0,0.00902298,"(2015) perform implicit resolution of coreferences between entities in learner answers and entities in the question and similarly target ellipses resolution, where part of the question is implied in the learner answer, both by aligning concepts from the learner answer to the question. They Introduction Automatic short answer scoring is an application area of natural language processing where short free-form answers written by students in an educational context are automatically scored based on the correctness of their content. They occur for example in science education (Nielsen et al., 2008; Dzikovska et al., 2010), but also in foreign language learning to measure reading (Bailey and Meurers, 2008; Meurers et al., 2011) or listening comprehension (Horbach et al., 2014). In such a scoring task, answers are graded based on their content alone - in comparison to essay scoring (Attali and Burstein, 2006) where also linguistic form is taken into consideration. Thus, judging whether an answer is correct or not may require the resolution of a number of implicit language phenomena as a form of normalization. Figure 1 11 Proceedings of the 1st Workshop on Understanding Implicit and Underspecified Language (UnImp"
2021.unimplicit-1.2,S13-2045,0,0.0236168,"w pandas in China are similar 3.1 For our annotations we needed publicly available short-answer data in English where answers are full sentences and not only single phrases like in the Powergrading dataset (Basu et al., 2013). Ideally, there should be a larger amount of answers for a single prompt so that prompt-specific models can be trained later in Section 4. (For an overview of publicly available shortanswer datasets, see Horbach and Zesch (2019).) We consider two short answer datasets in our analysis. The first one is the Student Response Analysis Corpus (SRA) of the 2013 SemEval task 7 (Dzikovska et al., 2013). It consists of data from two different sources. The Beetle subset has 3k student answers to 56 questions about electricity and electronics. The SciEntsBank subset contains 10k student answers to 197 questions about different science domains. All questions have a reference answer and (among others) 5-way labels judging the appropriateness of the student answers. The second dataset we consider is that of the 2012 Automated Student Assessment Prize (ASAP).1 It consists of about 2,200 student answers to each of ten science-related prompts. The answers to four of the prompts were rated on a four-"
2021.unimplicit-1.2,W11-2401,0,0.029002,"estion and similarly target ellipses resolution, where part of the question is implied in the learner answer, both by aligning concepts from the learner answer to the question. They Introduction Automatic short answer scoring is an application area of natural language processing where short free-form answers written by students in an educational context are automatically scored based on the correctness of their content. They occur for example in science education (Nielsen et al., 2008; Dzikovska et al., 2010), but also in foreign language learning to measure reading (Bailey and Meurers, 2008; Meurers et al., 2011) or listening comprehension (Horbach et al., 2014). In such a scoring task, answers are graded based on their content alone - in comparison to essay scoring (Attali and Burstein, 2006) where also linguistic form is taken into consideration. Thus, judging whether an answer is correct or not may require the resolution of a number of implicit language phenomena as a form of normalization. Figure 1 11 Proceedings of the 1st Workshop on Understanding Implicit and Underspecified Language (UnImplicit 2021), pages 11–19 Bangkok, Thailand (online), August 5, 2021. ©2021 Association for Computational Li"
2021.unimplicit-1.2,nielsen-etal-2008-annotating,0,0.0668891,"onent. Banjade et al. (2015) perform implicit resolution of coreferences between entities in learner answers and entities in the question and similarly target ellipses resolution, where part of the question is implied in the learner answer, both by aligning concepts from the learner answer to the question. They Introduction Automatic short answer scoring is an application area of natural language processing where short free-form answers written by students in an educational context are automatically scored based on the correctness of their content. They occur for example in science education (Nielsen et al., 2008; Dzikovska et al., 2010), but also in foreign language learning to measure reading (Bailey and Meurers, 2008; Meurers et al., 2011) or listening comprehension (Horbach et al., 2014). In such a scoring task, answers are graded based on their content alone - in comparison to essay scoring (Attali and Burstein, 2006) where also linguistic form is taken into consideration. Thus, judging whether an answer is correct or not may require the resolution of a number of implicit language phenomena as a form of normalization. Figure 1 11 Proceedings of the 1st Workshop on Understanding Implicit and Under"
2021.unimplicit-1.2,W14-4922,0,0.04351,"Missing"
2021.unimplicit-1.2,N18-1011,0,0.012714,"30 in the wild. – Pandas turn 30 years in the wild. Example 4: Numeric Terms Information Structure Another specific subcase of ellipses that is particularly important in a question and answer scenario is information structure (Krifka and Musan, 2012), i.e. the distinction whether the answer repeats given information from the question. Given the question that is shown in Example 5, bamboo is the focus of the answer, that actually answers the question. Focus has been automatically annotated for short answer data, although focus-based feature made only a minor difference in scoring performance (Ziai and Meurers, 2018). – Pandas live in China. They eat bamboo. – Pandas live in China. Pandas eat bamboo. Example 1: Coreference Bridging Anaphora The relationship between an anaphor and its antecedent may be indirect, constituting the special case of bridging anaphora (Clark, 1975). Take for example the statement shown in Example 2. While this can be understood from the context of the first sentence, it is left implicit that the second sentence refers to the fur of the panda. – What do pandas eat? Bamboo. – What do pandas eat? Pandas eat bamboo. Example 5: Information Structure – The panda is ill. The fur is dul"
2021.woah-1.22,S19-2121,1,0.846142,"available objects in images. Smith (2007) and EasyOCR5 can optically recognize the text embedded in an image. Related Work Multi-modal hateful meme detection is the task of identifying hate in the combination of textual and visual information. Textual Information In most previous works, hate speech detection has been performed solely in textual form. Despite many challenges (Vidgen et al., 2019), there have been several automatic detection systems developed to filter hateful statements (Waseem et al., 2017; Benikova et al., 2017; Wiegand et al., 2018; Kumar et al., 2018; Nobata et al., 2016; Aggarwal et al., 2019). One stateof-the-art model is BERT (Devlin et al., 2019). BERT is a contextualized transformer (Vaswani et al., 2017) based on a pre-trained language model which can be further fine-tuned for downstream applications such as hate speech classification. Visual Information For hateful meme classification, the Facebook challenge team4 proposed a unimodal training where a ResNet (He et al., 2015) encoder is used for image feature extraction. Apart from this, there has been a plenitude of work on extracting information from images, which is potentially useful for hateful meme detection. Image 3 htt"
2021.woah-1.22,S19-2007,0,0.0289844,"38 36 52 20 16 4 2 10 3 4 2 1 1 0.8 0.6 0.7 0.4 0.4 0.6 0.2 0.2 Other 56 3 0.5 Total 8,500 640 100 R ELIGION R ACE S EX NATIONALITY D ISABILITY Introduction In this work, we present our submission to the Shared Task on Hateful Memes at WOAH 2021: Workshop on Online Abuse and Harms.1 Detecting hateful memes that combine visual and textual elements is a relatively new task (Kiela et al., 2020). However, research can build on earlier work on the classification of hateful, abusive, or offending textual statements targeting individuals or groups based on gender, nationality, or sexual orientation (Basile et al., 2019; Burnap and Williams, 2014). Shared Task Description We only tackle Task A, which is predicting fine-grained labels for protected categories that are attacked in the memes, namely R ACE, D ISABILITY, R ELIGION, NATION ALITY , and S EX . The memes are provided in a multi-label setting. Table 1 shows the label distribution of the provided data set.2 Our System Our system is built on top of the winning system (Zhu, 2020) of the Hateful Memes Challenge (Kiela et al., 2020), which was a binary ∗ Labels Equal contribution of the first two authors 1 https://www.workshopononlineabuse.com/cfp/ shared-"
2021.woah-1.22,N19-1423,0,0.193358,"optically recognize the text embedded in an image. Related Work Multi-modal hateful meme detection is the task of identifying hate in the combination of textual and visual information. Textual Information In most previous works, hate speech detection has been performed solely in textual form. Despite many challenges (Vidgen et al., 2019), there have been several automatic detection systems developed to filter hateful statements (Waseem et al., 2017; Benikova et al., 2017; Wiegand et al., 2018; Kumar et al., 2018; Nobata et al., 2016; Aggarwal et al., 2019). One stateof-the-art model is BERT (Devlin et al., 2019). BERT is a contextualized transformer (Vaswani et al., 2017) based on a pre-trained language model which can be further fine-tuned for downstream applications such as hate speech classification. Visual Information For hateful meme classification, the Facebook challenge team4 proposed a unimodal training where a ResNet (He et al., 2015) encoder is used for image feature extraction. Apart from this, there has been a plenitude of work on extracting information from images, which is potentially useful for hateful meme detection. Image 3 https://github.com/aggarwalpiush/HateMemeDetection Visual-li"
2021.woah-1.22,W18-4401,0,0.0578373,"Missing"
2021.woah-1.22,P18-1238,0,0.0711715,"Missing"
2021.woah-1.22,W19-3509,0,0.0114286,"ags exhibits the best performance for Task A. We make our source code publicly available.3 2 processing systems such as Faster R-CNN or Inception V3 models (Ren et al., 2016; Szegedy et al., 2015) are useful for detecting available objects in images. Smith (2007) and EasyOCR5 can optically recognize the text embedded in an image. Related Work Multi-modal hateful meme detection is the task of identifying hate in the combination of textual and visual information. Textual Information In most previous works, hate speech detection has been performed solely in textual form. Despite many challenges (Vidgen et al., 2019), there have been several automatic detection systems developed to filter hateful statements (Waseem et al., 2017; Benikova et al., 2017; Wiegand et al., 2018; Kumar et al., 2018; Nobata et al., 2016; Aggarwal et al., 2019). One stateof-the-art model is BERT (Devlin et al., 2019). BERT is a contextualized transformer (Vaswani et al., 2017) based on a pre-trained language model which can be further fine-tuned for downstream applications such as hate speech classification. Visual Information For hateful meme classification, the Facebook challenge team4 proposed a unimodal training where a ResNet"
2021.woah-1.22,W17-3012,0,0.0215586,"ch as Faster R-CNN or Inception V3 models (Ren et al., 2016; Szegedy et al., 2015) are useful for detecting available objects in images. Smith (2007) and EasyOCR5 can optically recognize the text embedded in an image. Related Work Multi-modal hateful meme detection is the task of identifying hate in the combination of textual and visual information. Textual Information In most previous works, hate speech detection has been performed solely in textual form. Despite many challenges (Vidgen et al., 2019), there have been several automatic detection systems developed to filter hateful statements (Waseem et al., 2017; Benikova et al., 2017; Wiegand et al., 2018; Kumar et al., 2018; Nobata et al., 2016; Aggarwal et al., 2019). One stateof-the-art model is BERT (Devlin et al., 2019). BERT is a contextualized transformer (Vaswani et al., 2017) based on a pre-trained language model which can be further fine-tuned for downstream applications such as hate speech classification. Visual Information For hateful meme classification, the Facebook challenge team4 proposed a unimodal training where a ResNet (He et al., 2015) encoder is used for image feature extraction. Apart from this, there has been a plenitude of w"
C12-1011,J08-4004,0,0.0735347,"mentation of the system by Clough and Stevenson (2011) achieves F¯1 = .788. Our system again outperforms all other systems with F¯1 = .859. In our envisioned semi-supervised application scenario, potentially reused texts are presented to users in an informative manner. Here, fine-grained distinctions are not necessary, and we decided to go even one step further and fold all potential cases of text reuse. This variant of the dataset results in a binary classification of plagiarized/non-plagiarized texts. We present 10 11 12 An exhaustive discussion of inter-rater agreement measures is given by Artstein and Poesio (2008). http://www.ukp.tu-darmstadt.de/data/text-similarity/text-reuse-annotations Strength of agreement for κ values according to Landis and Koch (1977) We report the results for our re-implementation of the system by Clough and Stevenson (2011). In their original work, they did not evaluate on this dataset. 13 175 System Acc. F¯1 Majority Class Baseline Ferret Baseline .715 .684 .417 .535 Clough and Stevenson (2011)13 Sánchez-Vega et al. (2010) .692 .783 .680 .705 Our Approach .802 .768 exp. class. reuse no reuse reuse 151 30 no reuse 20 52 Table 6: Results and confusion matrix for the best classi"
C12-1011,R11-1071,1,0.849433,"ihalcea et al., 2006; Landauer et al., 1998; Gabrilovich and Markovitch, 2007). However, existing similarity measures typically exhibit a major limitation: They compute similarity only on features which can be derived from the content of the given texts. By following this approach, they inherently imply that the similarity computation process does not need to take any other text characteristics into account. In contrast, we propose that text reuse detection indeed benefits from also assessing similarity along other text characteristics (dimensions, henceforth). We follow empirical evidence by Bär et al. (2011) and focus on three characteristic similarity dimensions inherent to texts: content, structure, and style. Figure 1 shows an example of text reuse taken from the Wikipedia Rewrite Corpus (see Section 3.1) where parts of a given source text have been reused either verbatim or by using similar words or phrases. As the example illustrates, the process of creating reused text includes a revision step in which the editor has a certain degree of freedom on how to reuse the source text. This kind of similarity is detectable by content-centric text similarity measures. However, the editor has further"
C12-1011,C10-1005,0,0.0135997,"as occurred. After compiling two sets of n-grams, we compared them using the Jaccard coefficient, following Lyon et al. (2001), as well as using the containment measure (Broder, 1997). We tested n-gram sizes for n = 1, 2, . . . , 15, and will use the original system name Ferret (Lyon et al., 2004) to refer to the variant with n = 3 using the Jaccard coefficient, henceforth. Following the idea of comparing lexical patterns, we also used a measure which has not yet been considered for assessing content similarity: character n-gram profiles (Keselj et al., 2003).3 We follow the implementation by Barrón-Cedeño et al. (2010) and discard all characters (case insensitive) which are not in the alphabet Σ = {a, . . . , z, 0, . . . , 9}, then generate all n-grams on character level, weight them by a tfidf scheme, and finally compare the feature vectors of both the rewritten and the source text using the cosine measure. While in the original implementation only n = 3 was used, we generalize the measure to n = 2, 3, . . . , 15. In cases where the editor replaced content words by synonyms, string measures typically fail due to the vocabulary gap. We thus used similarity measures which are capable of measuring semantic si"
C12-1011,P04-2006,0,0.037944,"Missing"
C12-1011,W10-0701,0,0.0183427,"Missing"
C12-1011,J96-2004,0,0.0493822,"Missing"
C12-1011,P02-1020,0,0.0325053,"ve knowledge of what content is already present in the wiki, and what is not. As wikis are traditionally growing fast, this is hardly feasible, though. To remedy this issue, we aim at supporting authors of collaborative text collections by means of automatic text reuse detection. We envision a semi-supervised system that informs a content author of potentially pre-existing instances of text reuse, and then lets the author decide how to proceed, e.g. to merge both texts. Detecting text reuse has been studied in a variety of tasks and applications, e.g. the detection of journalistic text reuse (Clough et al., 2002), the identification of rewrite sources for ancient literary texts (Lee, 2007), or the analysis of text reuse in blogs and web pages (Abdel-Hamid et al., 2009). Another common instance of text reuse is plagiarism, with the additional constraint that the reuse needs to be unacknowledged. Near-duplicate detection is also a broad field of related work where the detection of text reuse is crucial, e.g. in the context of web search and crawling (Hoad and Zobel, 2003; Henzinger, 2006; Manku et al., 2007). Prior work, however, mainly utilizes fingerprinting and hashing techniques (Charikar, 2002) for"
C12-1011,W99-0625,0,0.0219255,"ing only stopwords. All n-grams of both texts are then compared using the containment measure (Broder, 1997). We tested n-gram sizes for n = 2, 3, . . . , 15. For the same reason, we also included part-of-speech n-grams in our feature set. Disregarding the actual words that appear in two given texts, computing n-grams along part-of-speech tags allows to detect syntactic similarities between these texts. Again, we tested n-gram sizes for n = 2, 3, . . . , 15, and compared the two sets using the containment measure (Broder, 1997). We also employed two similarity measures between pairs of words (Hatzivassiloglou et al., 1999). The word pair order measure assumes that a similar syntactical structure in reused texts may cause two words to occur in the same order in both texts (with any number of words in between). The complementary word pair distance measure counts the number of words which lie between those of a given pair. For each measure, we computed feature vectors for both texts along all shared word pairs and compared the vectors using Pearson’s correlation. 2.3 Stylistic Similarity Measures of stylistic similarity adopt ideas from authorship attribution (Mosteller and Wallace, 1964) or use statistical proper"
C12-1011,O97-1002,0,0.0394863,". , 9}, then generate all n-grams on character level, weight them by a tfidf scheme, and finally compare the feature vectors of both the rewritten and the source text using the cosine measure. While in the original implementation only n = 3 was used, we generalize the measure to n = 2, 3, . . . , 15. In cases where the editor replaced content words by synonyms, string measures typically fail due to the vocabulary gap. We thus used similarity measures which are capable of measuring semantic similarity between words. We used the following word similarity measures with WordNet (Fellbaum, 1998): Jiang and Conrath (1997), Lin (1998), and Resnik (1995). In order to scale these pairwise word similarity scores to the document level, we follow the aggregation strategy by Mihalcea et al. (2006): First, a directional similarity score simd (Ti , T j ) is computed from a text Ti to a second text T j (Eq. 1). Therefore, for each word w i in Ti , its best-matching counterpart in T j is sought (ma xSim(w i , T j )). The similarity scores of all these matches are summed up and weighted according to their inverse document frequency idf (Spärck Jones, 1972), then normalized. The final document-level similarity figure is th"
C12-1011,2005.mtsummit-papers.11,0,0.0165224,"ores of all these matches are summed up and weighted according to their inverse document frequency idf (Spärck Jones, 1972), then normalized. The final document-level similarity figure is the average of applying this strategy in both directions, from Ti to T j and vice-versa (Eq. 2). P simd (Ti , T j ) = wi maxSim(w i , T j ) · id f (w i ) P wi id f (w i ) (1) sim(Ti , T j ) =  1 simd (Ti , T j ) + simd (T j , Ti ) (2) 2 We also tested text expansion mechanisms with the semantic word similarity measures described above: We used the Moses SMT system (Koehn et al., 2007), trained on Europarl (Koehn, 2005), to translate the original English texts via a bridge language (Dutch) back to English. Thereby, the idea was that in the translation process additional lexemes are introduced which alleviate potential lexical gaps. We computed pairwise word similarity with the measures described above and aggregated according to Mihalcea et al. (2006). Furthermore, we used the statistical technique Latent Semantic Analysis (LSA) (Landauer et al., 3 Traditionally, character n-gram profiles have rather been shown successful for authorship attribution. However, the similarity scores of word n-grams and those of"
C12-1011,P07-2045,0,0.012772,"t (ma xSim(w i , T j )). The similarity scores of all these matches are summed up and weighted according to their inverse document frequency idf (Spärck Jones, 1972), then normalized. The final document-level similarity figure is the average of applying this strategy in both directions, from Ti to T j and vice-versa (Eq. 2). P simd (Ti , T j ) = wi maxSim(w i , T j ) · id f (w i ) P wi id f (w i ) (1) sim(Ti , T j ) =  1 simd (Ti , T j ) + simd (T j , Ti ) (2) 2 We also tested text expansion mechanisms with the semantic word similarity measures described above: We used the Moses SMT system (Koehn et al., 2007), trained on Europarl (Koehn, 2005), to translate the original English texts via a bridge language (Dutch) back to English. Thereby, the idea was that in the translation process additional lexemes are introduced which alleviate potential lexical gaps. We computed pairwise word similarity with the measures described above and aggregated according to Mihalcea et al. (2006). Furthermore, we used the statistical technique Latent Semantic Analysis (LSA) (Landauer et al., 3 Traditionally, character n-gram profiles have rather been shown successful for authorship attribution. However, the similarity"
C12-1011,P07-1060,0,0.0987682,"re traditionally growing fast, this is hardly feasible, though. To remedy this issue, we aim at supporting authors of collaborative text collections by means of automatic text reuse detection. We envision a semi-supervised system that informs a content author of potentially pre-existing instances of text reuse, and then lets the author decide how to proceed, e.g. to merge both texts. Detecting text reuse has been studied in a variety of tasks and applications, e.g. the detection of journalistic text reuse (Clough et al., 2002), the identification of rewrite sources for ancient literary texts (Lee, 2007), or the analysis of text reuse in blogs and web pages (Abdel-Hamid et al., 2009). Another common instance of text reuse is plagiarism, with the additional constraint that the reuse needs to be unacknowledged. Near-duplicate detection is also a broad field of related work where the detection of text reuse is crucial, e.g. in the context of web search and crawling (Hoad and Zobel, 2003; Henzinger, 2006; Manku et al., 2007). Prior work, however, mainly utilizes fingerprinting and hashing techniques (Charikar, 2002) for text comparison rather than methods from natural language processing. A commo"
C12-1011,W01-0515,0,0.129313,"discussed measures in order to stimulate the development of novel measures: http://code.google.com/p/dkpro-similarity-asl 169 distance according to a given metric. We used the following measures in our experiments: Jaro (1989), Jaro-Winkler (Winkler, 1990), Monge and Elkan (1997), and Levenshtein (1966). Starting from the observation that not all words in a document are of equal importance, we further employed a similarity measure which weights all words by a tfidf scheme (Salton and McGill, 1983) and computes text similarity as the cosine between two document vectors. Comparing word n-grams (Lyon et al., 2001) is a popular means for comparing lexical patterns between two texts. The more similar the patterns, the more likely is it that text reuse has occurred. After compiling two sets of n-grams, we compared them using the Jaccard coefficient, following Lyon et al. (2001), as well as using the containment measure (Broder, 1997). We tested n-gram sizes for n = 1, 2, . . . , 15, and will use the original system name Ferret (Lyon et al., 2004) to refer to the variant with n = 3 using the Jaccard coefficient, henceforth. Following the idea of comparing lexical patterns, we also used a measure which has"
C12-1109,W06-3814,0,0.0243746,"Missing"
C12-1109,S07-1070,0,0.0660507,"Missing"
C12-1109,S12-1059,1,0.0274292,"cal gap is reflected in all those recent advances, be it in knowledge-based or supervised WSD scenarios. In this paper, we employ a source of semantic similarity whose application to automatic WSD has never before been explored: using a distributional thesaurus, or DT (Lin, 1998), we expand the lexical representations of the context and sense definition with additional terms. On this expanded representation, we are able to apply the well-known overlap-based methods to text similarity without any modification. Lexical expansion has already proven useful in semantic text similarity evaluations (Bär et al., 2012), which is a task related to matching sense definitions to contexts. The intuition behind our approach is depicted in Figure 1: say we wish to disambiguate the word interest in the sentence, “The loan interest is paid monthly.” The correct sense definition from our MRD (“a fixed charge for borrowing money”) has no words in common with the context, and thus would not be selected by an overlap-based WSD algorithm. But with the addition of ten lexical expansions per content word (shown in smaller text), we increase the number of overlapping word pairs (shown in boldface) to seven. Observe also th"
C12-1109,D07-1108,0,0.022479,"algorithm was found to be a great improvement over the original algorithm. Subsequent researchers (e.g., Ponzetto and Navigli (2010)) have combined the “simplified” and “extended” approaches into a “simplified extended” algorithm, in which augmented definitions are compared not with each other, but with the target word context. Many successful approaches to automatic WSD in recent years rely on distributional information to model the “topicality” of the context and the sense definition.1 They include using vectorspace dimensionality reduction techniques like LSA (Gliozzo et al., 2005) or LDA (Cai et al., 2007; Li et al., 2010), additionally collected text material per sense as in topic signatures (Martínez et al., 2008), and clustering for word sense induction as features (Agirre et al., 2006; Biemann, 2012); the importance of bridging the lexical gap is reflected in all those recent advances, be it in knowledge-based or supervised WSD scenarios. In this paper, we employ a source of semantic similarity whose application to automatic WSD has never before been explored: using a distributional thesaurus, or DT (Lin, 1998), we expand the lexical representations of the context and sense definition with"
C12-1109,de-marneffe-etal-2006-generating,0,0.00424528,"Missing"
C12-1109,J93-1003,0,0.37538,"orpus from the Leipzig Corpora Collection3 (Biemann et al., 2007) with the Stanford parser (de Marneffe et al., 2006) and used collapsed dependencies to extract features for words: each dependency triple (w1 , r, w2 ) denoting a directed dependency of type r between words w1 and w2 results in a feature (r, w2 ) characterizing w1 , and a feature (w1 , r) characterizing w2 . Words are thereby represented by the concatenation of the surface form and the POS as assigned by the parser. After counting the frequency of each feature for each word, we apply a significance measure (log-likelihood test (Dunning, 1993)), rank features per word according to their significance, and prune the data, keeping only the 300 most salient features per word. The similarity of two words is given by the number of their common features (which we will shortly illustrate with an example). The pruning operation greatly reduces run time at thesaurus construction, rendering memory reduction techniques like Goyal et al. (2012) unnecessary. Despite its simplicity and the basic count of feature overlap, we found this setting to be equal to or better than more complex weighting schemes in word similarity evaluations. Across all p"
C12-1109,P05-1050,0,0.0337179,"onyms); their “extended” Lesk algorithm was found to be a great improvement over the original algorithm. Subsequent researchers (e.g., Ponzetto and Navigli (2010)) have combined the “simplified” and “extended” approaches into a “simplified extended” algorithm, in which augmented definitions are compared not with each other, but with the target word context. Many successful approaches to automatic WSD in recent years rely on distributional information to model the “topicality” of the context and the sense definition.1 They include using vectorspace dimensionality reduction techniques like LSA (Gliozzo et al., 2005) or LDA (Cai et al., 2007; Li et al., 2010), additionally collected text material per sense as in topic signatures (Martínez et al., 2008), and clustering for word sense induction as features (Agirre et al., 2006; Biemann, 2012); the importance of bridging the lexical gap is reflected in all those recent advances, be it in knowledge-based or supervised WSD scenarios. In this paper, we employ a source of semantic similarity whose application to automatic WSD has never before been explored: using a distributional thesaurus, or DT (Lin, 1998), we expand the lexical representations of the context"
C12-1109,D12-1100,0,0.0137786,"by represented by the concatenation of the surface form and the POS as assigned by the parser. After counting the frequency of each feature for each word, we apply a significance measure (log-likelihood test (Dunning, 1993)), rank features per word according to their significance, and prune the data, keeping only the 300 most salient features per word. The similarity of two words is given by the number of their common features (which we will shortly illustrate with an example). The pruning operation greatly reduces run time at thesaurus construction, rendering memory reduction techniques like Goyal et al. (2012) unnecessary. Despite its simplicity and the basic count of feature overlap, we found this setting to be equal to or better than more complex weighting schemes in word similarity evaluations. Across all parts of speech, the DT contains five or more similar terms for a vocabulary of over 150 000 words. To illustrate the DT, Table 1 shows the top three most similar words to the noun paper, together with the features which determine the similarities. Amongst their 300 most salient features as determined by the significance measure, newspaper and paper share 45, book and paper share 33, and articl"
C12-1109,E12-1059,1,0.0654393,"tion— but they might well result in assigning incorrect senses. A straightforward improvement would alter the lexical expansion mechanism as to be sensitive to the context—something that is captured, for example, by LDA sampling (Blei et al., 2003). A further extension would be to have the number of lexical expansions depend on the DT similarity score (be it static or 1793 contextualized) instead of the fixed number we used here. In the future, we would like to examine the interplay of lexical expansion methods in WSD systems with richer knowledge resources (e.g., Navigli and Ponzetto (2010); Gurevych et al. (2012)) and apply our approach to other languages with fewer lexical resources. Also, it seems promising to apply lexical expansion techniques to text similarity, text segmentation, machine translation, and semantic indexing. Acknowledgments We thank Richard Steuer for computing and providing us access to the distributional thesaurus. This work has been supported by the Hessian research excellence program Landes-Offensive zur Entwicklung Wissenschaftlich-ökonomischer Exzellenz (LOEWE) as part of the research center Digital Humanities, and also by the Volkswagen Foundation as part of the Lichtenberg"
C12-1109,E12-1039,0,0.0310549,"ted from the gloss, synonyms, and example sentences provided by WordNet, plus the 1784 same information for all senses in a direct semantic relation. This setup specifically targets situations where such a resource serves as the sense inventory but no large sense-annotated corpus is available for supervised WSD (thus precluding use of the most frequent sense backoff). This is the case for many languages, where wordnets but not manually tagged corpora are available, and also for domain-specific WSD using the English WordNet. Whereas other approaches in this setting (Ponzetto and Navigli, 2010; Henrich et al., 2012) aim at improving WSD accuracy through the combination of several lexical resources, we restrict ourselves to WordNet and bridge the lexical gap with non-supervised, data-driven methods. How one computes the overlap between two strings was left unspecified by Lesk; we therefore adopt the simple approach of removing occurrences of the target word, treating both strings as bags of case-insensitive word tokens, and taking the cardinality of their intersection. We do not preprocess the texts by lemmatization or stop word filtering, since the terms in the distributional thesaurus are likewise unpro"
C12-1109,S01-1004,0,0.0130406,"g error that is inevitable when representing a large vocabulary with a small fixed number of dimensions or topics. On the other hand, while vector-space models do a good job at ranking candidates according to their similarity,2 they fail to efficiently generate a top-ranked list of possible expansions: due to its size, it is infeasible to rank the full vocabulary every time. Lexical expansion methods based on distributional similarity, however, generate a short list of highly similar candidates. 1 Distributional information was also used in a much older, semi-automatic approach by Tugwell and Kilgarriff (2001). In their technique, “word sketches” consisting of common patterns of usage of a word were extracted from a large POS-tagged corpus and presented to a human operator for manual sense annotation. The pattern–sense associations were then used as input to bootstrapping WSD algorithm. 2 See Rapp (2004) for an early success of vector-space models on a semantic task. 1783 The interest : loan mortgage loans debt financing mortgages credit lease bond grant funding a fixed interest solved hefty resolved monthly additional existing reduced done current substantial is charge charges counts charging cost"
C12-1109,P10-1116,0,0.026512,"d to be a great improvement over the original algorithm. Subsequent researchers (e.g., Ponzetto and Navigli (2010)) have combined the “simplified” and “extended” approaches into a “simplified extended” algorithm, in which augmented definitions are compared not with each other, but with the target word context. Many successful approaches to automatic WSD in recent years rely on distributional information to model the “topicality” of the context and the sense definition.1 They include using vectorspace dimensionality reduction techniques like LSA (Gliozzo et al., 2005) or LDA (Cai et al., 2007; Li et al., 2010), additionally collected text material per sense as in topic signatures (Martínez et al., 2008), and clustering for word sense induction as features (Agirre et al., 2006; Biemann, 2012); the importance of bridging the lexical gap is reflected in all those recent advances, be it in knowledge-based or supervised WSD scenarios. In this paper, we employ a source of semantic similarity whose application to automatic WSD has never before been explored: using a distributional thesaurus, or DT (Lin, 1998), we expand the lexical representations of the context and sense definition with additional terms."
C12-1109,P98-2127,0,0.555994,"mensionality reduction techniques like LSA (Gliozzo et al., 2005) or LDA (Cai et al., 2007; Li et al., 2010), additionally collected text material per sense as in topic signatures (Martínez et al., 2008), and clustering for word sense induction as features (Agirre et al., 2006; Biemann, 2012); the importance of bridging the lexical gap is reflected in all those recent advances, be it in knowledge-based or supervised WSD scenarios. In this paper, we employ a source of semantic similarity whose application to automatic WSD has never before been explored: using a distributional thesaurus, or DT (Lin, 1998), we expand the lexical representations of the context and sense definition with additional terms. On this expanded representation, we are able to apply the well-known overlap-based methods to text similarity without any modification. Lexical expansion has already proven useful in semantic text similarity evaluations (Bär et al., 2012), which is a task related to matching sense definitions to contexts. The intuition behind our approach is depicted in Figure 1: say we wish to disambiguate the word interest in the sentence, “The loan interest is paid monthly.” The correct sense definition from o"
C12-1109,W03-2408,0,0.0264845,"h problem in computational linguistics. Approaches to WSD can be classified according to what lexical resources are used: knowledgebased techniques rely only on machine-readable dictionaries (MRDs), lexical semantic resources (LSRs), and untagged corpora, whereas supervised approaches instead or additionally use manually annotated training examples. Though supervised systems generally perform better, their use is restricted to scenarios where a sufficient amount of hand-crafted training data is available. Estimates for the amount of time required to produce such training data are pessimistic (Mihalcea and Chklovski, 2003); this knowledge acquisition bottleneck is the principal motivation behind research into semi-supervised and knowledge-based WSD. The latter have the advantage that, unlike manually annotated corpora, MRDs and LSRs do exist for many languages and domains. In the past, however, knowledge-based approaches have suffered from a variant of the lexical gap problem: when matching a sense description to a given context of a disambiguation target, it is often the case that the description and context do not have much vocabulary in common. We propose a new method to bridge this lexical gap which is base"
C12-1109,S07-1006,0,0.211195,"Missing"
C12-1109,P10-1023,0,0.0215006,"ith the correct sense description— but they might well result in assigning incorrect senses. A straightforward improvement would alter the lexical expansion mechanism as to be sensitive to the context—something that is captured, for example, by LDA sampling (Blei et al., 2003). A further extension would be to have the number of lexical expansions depend on the DT similarity score (be it static or 1793 contextualized) instead of the fixed number we used here. In the future, we would like to examine the interplay of lexical expansion methods in WSD systems with richer knowledge resources (e.g., Navigli and Ponzetto (2010); Gurevych et al. (2012)) and apply our approach to other languages with fewer lexical resources. Also, it seems promising to apply lexical expansion techniques to text similarity, text segmentation, machine translation, and semantic indexing. Acknowledgments We thank Richard Steuer for computing and providing us access to the distributional thesaurus. This work has been supported by the Hessian research excellence program Landes-Offensive zur Entwicklung Wissenschaftlich-ökonomischer Exzellenz (LOEWE) as part of the research center Digital Humanities, and also by the Volkswagen Foundation as"
C12-1109,S01-1005,0,0.140023,"Missing"
C12-1109,P10-1154,0,0.556073,"entences provided by some dictionaries; Kilgarriff and Rosenzweig (2000) found that including them (for simplified Lesk) led to significantly better performance than using the definitions alone. Banerjee and Pedersen (2002) observed that, where there exists a lexical resource like WordNet (Fellbaum, 1998) which also provides semantic relations between senses, 1782 these can be used to augment definitions with those from related senses (such as hypernyms and hyponyms); their “extended” Lesk algorithm was found to be a great improvement over the original algorithm. Subsequent researchers (e.g., Ponzetto and Navigli (2010)) have combined the “simplified” and “extended” approaches into a “simplified extended” algorithm, in which augmented definitions are compared not with each other, but with the target word context. Many successful approaches to automatic WSD in recent years rely on distributional information to model the “topicality” of the context and the sense definition.1 They include using vectorspace dimensionality reduction techniques like LSA (Gliozzo et al., 2005) or LDA (Cai et al., 2007; Li et al., 2010), additionally collected text material per sense as in topic signatures (Martínez et al., 2008), a"
C12-1109,rapp-2004-freely,0,0.0251874,"e to its size, it is infeasible to rank the full vocabulary every time. Lexical expansion methods based on distributional similarity, however, generate a short list of highly similar candidates. 1 Distributional information was also used in a much older, semi-automatic approach by Tugwell and Kilgarriff (2001). In their technique, “word sketches” consisting of common patterns of usage of a word were extracted from a large POS-tagged corpus and presented to a human operator for manual sense annotation. The pattern–sense associations were then used as input to bootstrapping WSD algorithm. 2 See Rapp (2004) for an early success of vector-space models on a semantic task. 1783 The interest : loan mortgage loans debt financing mortgages credit lease bond grant funding a fixed interest solved hefty resolved monthly additional existing reduced done current substantial is charge charges counts charging cost conviction allegation pay suspicion count part paid paying pay pays owed generated invested spent collected raised reimbursed for monthly. annual weekly yearly quarterly hefty daily regular additional substantial recent borrowing spending borrow lending borrowed debt investment raising inflows inve"
C12-1109,W04-0811,0,0.111045,"Missing"
C12-1109,S01-1037,0,0.158399,"d of sampling error that is inevitable when representing a large vocabulary with a small fixed number of dimensions or topics. On the other hand, while vector-space models do a good job at ranking candidates according to their similarity,2 they fail to efficiently generate a top-ranked list of possible expansions: due to its size, it is infeasible to rank the full vocabulary every time. Lexical expansion methods based on distributional similarity, however, generate a short list of highly similar candidates. 1 Distributional information was also used in a much older, semi-automatic approach by Tugwell and Kilgarriff (2001). In their technique, “word sketches” consisting of common patterns of usage of a word were extracted from a large POS-tagged corpus and presented to a human operator for manual sense annotation. The pattern–sense associations were then used as input to bootstrapping WSD algorithm. 2 See Rapp (2004) for an early success of vector-space models on a semantic task. 1783 The interest : loan mortgage loans debt financing mortgages credit lease bond grant funding a fixed interest solved hefty resolved monthly additional existing reduced done current substantial is charge charges counts charging cost"
C12-1109,vasilescu-etal-2004-evaluating,0,0.521787,"hich the word is found. This variant avoids the combinatorial explosion of word sense combinations the original version suffers from when trying to disambiguate multiple words in a text. Both the original and simplified versions of the Lesk algorithm suffer from low coverage due to the lexical gap problem: because the context and definitions are usually quite short, it is often the case that there are no overlapping content words at all. Various solutions to the problem have been proposed, with varying degrees of success. Lesk himself proposed increasing the size of the context window, though Vasilescu et al. (2004) found that performance was generally better for smaller contexts. Lesk also proposed augmenting the definitions with example sentences provided by some dictionaries; Kilgarriff and Rosenzweig (2000) found that including them (for simplified Lesk) led to significantly better performance than using the definitions alone. Banerjee and Pedersen (2002) observed that, where there exists a lexical resource like WordNet (Fellbaum, 1998) which also provides semantic relations between senses, 1782 these can be used to augment definitions with those from related senses (such as hypernyms and hyponyms);"
C12-1109,S07-1053,0,\N,Missing
C12-1109,C98-2122,0,\N,Missing
C16-1032,L16-1656,0,0.0166591,"how well coarse tagging can actually be done, and also focus on whether coarse-grained models transfer better between different kinds of texts, as e.g. Ritter et al. (2011) shows that a finegrained tagger trained on newswire data doesn’t work well on social media. Creating robust and accurate coarse-grained taggers is a worthwhile task on its own, as many NLP applications actually do not require fine-grained distinctions. For example, the popular TextRank algorithm (Mihalcea and Tarau, 2004) for keyphrase extraction uses coarse grained PoS tags to build the underlying co-occurrence graph, or Benikova and Biemann (2016) use coarse-grained tags to annotate semantic relations between nominals. Another advantage of our approach is that the second tagging step can be easily customized for specific needs, e.g. if a scholar wants to analyze the usage of a specific PoS tag. In this case, we can use fine-grained models with additional features that are informative for this sub-problem, but might not be helpful for the overall tagging task. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: creativecommons.org/licenses/by/4.0/ 328 Proceedings of COLING 2016, the 26t"
C16-1032,A00-1031,0,0.650041,"Missing"
C16-1032,H92-1022,0,0.329959,"is work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: creativecommons.org/licenses/by/4.0/ 328 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 328–336, Osaka, Japan, December 11-17 2016. http:// Figure 1: PoS tagging in two steps: The first step assigns a coarse tag (main word class) of a word. The second step determines the fine PoS tag based on the coarse PoS tag of the first step. Related Work We are not aware of prior work using a similar approach. Some rule-based approaches (Brill, 1992; Hepple, 2000) assign the most probable tag to each token and then use transformation rules to correct the initial assignment. However, both steps use the same granularity and the initial assignment only reflects the prior probability but is not usable in itself. There is also relatively little research on coarse-grained tagging. Gimpel et al. (2011) developed a tagger for Twitter data that uses a specialized coarse-grained tagset which is equivalent to our first step, but they do not aim at refining these assignments further as we do in the second step. Most approaches do fine-grained taggin"
C16-1032,P14-5011,1,0.831105,"Missing"
C16-1032,W14-5201,0,0.0274904,"Missing"
C16-1032,P11-2008,0,0.075809,"Missing"
C16-1032,P00-1036,0,0.126424,"cenced under a Creative Commons Attribution 4.0 International Licence. Licence details: creativecommons.org/licenses/by/4.0/ 328 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 328–336, Osaka, Japan, December 11-17 2016. http:// Figure 1: PoS tagging in two steps: The first step assigns a coarse tag (main word class) of a word. The second step determines the fine PoS tag based on the coarse PoS tag of the first step. Related Work We are not aware of prior work using a similar approach. Some rule-based approaches (Brill, 1992; Hepple, 2000) assign the most probable tag to each token and then use transformation rules to correct the initial assignment. However, both steps use the same granularity and the initial assignment only reflects the prior probability but is not usable in itself. There is also relatively little research on coarse-grained tagging. Gimpel et al. (2011) developed a tagger for Twitter data that uses a specialized coarse-grained tagset which is equivalent to our first step, but they do not aim at refining these assignments further as we do in the second step. Most approaches do fine-grained tagging first, and th"
C16-1032,J93-2004,0,0.0592504,"t the PoS tagging task into two steps. A first high precision step, where we only assign a coarse-grained tag which can be reliably learned also with rather limited training data, and which benefits from additional unlabeled data in the form of clusters or embeddings. And a second step, where we apply specialized tagging models that only have to choose from a much smaller set of possible labels. Figure 1 gives an overview of our approach using, in the first step, a coarse-grained tagset (similar to the universal tagset (Petrov et al., 2012)) and in the second step the fine-grained PTB tagset (Marcus et al., 1993). In the figure, we can see how knowing the coarse-grained tag informs the second step. For example, if we already know that beautiful is an adjective, we only have to choose between three possible tags (JJ, JJR, or JJS) instead of 45 tags for the full PTB tagset. Our approach requires that the coarse tagging in the first step is more accurate than using fine-grained tagging itself, as we will loose some accuracy through error propagation between the steps. We will thus first analyze how well coarse tagging can actually be done, and also focus on whether coarse-grained models transfer better b"
C16-1032,N13-1039,0,0.0384524,"Missing"
C16-1032,petrov-etal-2012-universal,0,0.0289183,"subtle differences between a large set of labels. We thus propose to split the PoS tagging task into two steps. A first high precision step, where we only assign a coarse-grained tag which can be reliably learned also with rather limited training data, and which benefits from additional unlabeled data in the form of clusters or embeddings. And a second step, where we apply specialized tagging models that only have to choose from a much smaller set of possible labels. Figure 1 gives an overview of our approach using, in the first step, a coarse-grained tagset (similar to the universal tagset (Petrov et al., 2012)) and in the second step the fine-grained PTB tagset (Marcus et al., 1993). In the figure, we can see how knowing the coarse-grained tag informs the second step. For example, if we already know that beautiful is an adjective, we only have to choose between three possible tags (JJ, JJR, or JJS) instead of 45 tags for the full PTB tagset. Our approach requires that the coarse tagging in the first step is more accurate than using fine-grained tagging itself, as we will loose some accuracy through error propagation between the steps. We will thus first analyze how well coarse tagging can actually"
C16-1032,D11-1141,0,0.0122101,"he coarse-grained tag informs the second step. For example, if we already know that beautiful is an adjective, we only have to choose between three possible tags (JJ, JJR, or JJS) instead of 45 tags for the full PTB tagset. Our approach requires that the coarse tagging in the first step is more accurate than using fine-grained tagging itself, as we will loose some accuracy through error propagation between the steps. We will thus first analyze how well coarse tagging can actually be done, and also focus on whether coarse-grained models transfer better between different kinds of texts, as e.g. Ritter et al. (2011) shows that a finegrained tagger trained on newswire data doesn’t work well on social media. Creating robust and accurate coarse-grained taggers is a worthwhile task on its own, as many NLP applications actually do not require fine-grained distinctions. For example, the popular TextRank algorithm (Mihalcea and Tarau, 2004) for keyphrase extraction uses coarse grained PoS tags to build the underlying co-occurrence graph, or Benikova and Biemann (2016) use coarse-grained tags to annotate semantic relations between nominals. Another advantage of our approach is that the second tagging step can be"
C16-1032,L16-1675,1,0.729462,"Missing"
C16-1032,W04-3252,0,\N,Missing
C16-1032,J92-4003,0,\N,Missing
C16-1198,borin-etal-2012-korp,0,0.107574,"-free texts written by experts for 2103 CEFR Levels B1 B2 A2 C1 Total Learner Output Texts Tokens 83 18,349 75 29,814 74 32,691 88 60,095 320 140,949 Expert Input Texts Tokens 157 37,168 258 79,124 288 101,297 115 71,723 818 289,312 Table 1: Overview of CEFR-level annotated Swedish datasets. L2 learners primarily intended as reading material. Both types of data are manually labeled for CEFR levels and automatically annotated across different linguistic dimensions including lemmatization, partof-speech (POS) tagging, and dependency parsing using the Sparv (previously known as ‘Korp’) pipeline (Borin et al., 2012). 3.1 L2 Output Texts Our source of output texts is SweLL (Volodina et al., 2016), a corpus consisting of L2 Swedish learner essays on a variety of topics, manually linked to CEFR levels. The essays also contain meta-information on learners’ mother tongue(s), age, gender, education level, the exam setting, and, in certain cases, topic and genre. The distribution of essays per level is given in Table 1. The corpus includes some essays at A1 and C2 levels, but these classes were too under-represented to be included in our experiments. As for A1 level, this may depend on learners’ limited ability"
C16-1198,P07-1033,0,0.234168,"Missing"
C16-1198,D12-1043,0,0.30718,"Missing"
C16-1198,P07-1034,0,0.07472,"Missing"
C16-1198,L16-1511,0,0.0633636,"Missing"
C16-1198,D15-1049,0,0.0519857,"-D OMAIN. Our experiment results in Table 4 show, in fact, that correcting learner errors does not have any statistically significant effect in the I N -D OMAIN setup, but it does improve performance to a great extent for most domain-adapted cases. This latter would support the hypothesis that correcting spelling and grammatical errors increases the similarity between the target and the source domain. The gain is especially large (+.182 F1 ) in the case of the S OURCE O NLY setup, which does not rely on annotated essays. E ASYA DAPT, which has been successfully used in an AES task previously (Phandi et al., 2015), is outperformed by most other domain adaptation methods in our case, independently from error normalization. In terms of F1 , +F EATURE using the predictions of a classifier trained on the L2 input texts performs best (.802 F1 ), however, the degree of misclassifications indicated by κ2 is smallest with W EIGHTED (.915), as in the case of the essays without error normalization. After error correction, W EIGHTED I NST S EL achieves approximately the same quality of performance for all measures as the aforementioned two best performing models W EIGHTED and +F EATURE. These all improve over the"
C16-1198,W14-3509,0,0.350768,"Missing"
C16-1198,W12-2019,0,0.060738,"LY log freq Subordinate IS Relative clause IS PP complement IS S-V to V ADJ IS ADJ variation ADV IS ADV variation N IS N variation Present PC to V Past V to V Present V to V Supine V to V Relative structure IS Nominal ratio N to V Modal V to V Particle IS 3SG pronoun IS Punctuation IS Subjunction IS PR to N PR to PP Verb IS V variation Function W IS Lex tkn to non-lex tkn Lex tkn to Nr tkn Neuter N IS CJ + SJ IS Table 2: Feature set. tokens longer than six characters (Bj¨ornsson, 1968). Rather than a simple type-token ratio (TTR), we use a bi-logarithmic and a square root equivalent following Vajjala and Meurers (2012). Lexical features incorporate information from the K ELLY list (Volodina and Kokkinakis, 2012), a frequency-based word list compiled using a corpus of web texts (thus completely independent of our datasets), which also provides a suggested CEFR level per each lemma based on frequency bands. For some feature values, incidence scores (IS) are computed, in other words, instead of absolute counts, normalized values per 1000 tokens are considered to reduce the influence of sentence length. Lexical complexity is modeled with a set of weakly lexicalized features, i.e. we do not use word forms or lem"
C16-1198,volodina-kokkinakis-2012-introducing,1,0.711547,"n ADV IS ADV variation N IS N variation Present PC to V Past V to V Present V to V Supine V to V Relative structure IS Nominal ratio N to V Modal V to V Particle IS 3SG pronoun IS Punctuation IS Subjunction IS PR to N PR to PP Verb IS V variation Function W IS Lex tkn to non-lex tkn Lex tkn to Nr tkn Neuter N IS CJ + SJ IS Table 2: Feature set. tokens longer than six characters (Bj¨ornsson, 1968). Rather than a simple type-token ratio (TTR), we use a bi-logarithmic and a square root equivalent following Vajjala and Meurers (2012). Lexical features incorporate information from the K ELLY list (Volodina and Kokkinakis, 2012), a frequency-based word list compiled using a corpus of web texts (thus completely independent of our datasets), which also provides a suggested CEFR level per each lemma based on frequency bands. For some feature values, incidence scores (IS) are computed, in other words, instead of absolute counts, normalized values per 1000 tokens are considered to reduce the influence of sentence length. Lexical complexity is modeled with a set of weakly lexicalized features, i.e. we do not use word forms or lemmas themselves as features, but the IS of their corresponding CEFR levels instead. This aspect"
C16-1198,W14-3510,1,0.907883,"Missing"
C16-1198,L16-1031,1,0.812365,"Missing"
C16-1198,W16-0502,0,0.0733742,"a news article (Zesch et al., 2015; Phandi et al., 2015). Zesch et al. (2015) explore which features are transferable from one essay grading task to another task based on a different prompt. They find that by excluding some highly domain-specific features, the transfer loss can be reduced significantly without noticeable differences in overall performance. A popular domain adaptation approach is E ASYA DAPT (Daum´e III, 2007) that augments the original feature space with source- and target-specific versions. Phandi et al. (2015) successfully applied E ASYA DAPT for automatic essay scoring and Xia et al. (2016) for the CEFR-level classification of L2 input texts with native language texts as source domain. 3 Datasets For our experiments, we use L2 Swedish data including learners’ output, i.e. error-prone essays written by learners, as well as L2 input data for learners, i.e. relatively error-free texts written by experts for 2103 CEFR Levels B1 B2 A2 C1 Total Learner Output Texts Tokens 83 18,349 75 29,814 74 32,691 88 60,095 320 140,949 Expert Input Texts Tokens 157 37,168 258 79,124 288 101,297 115 71,723 818 289,312 Table 1: Overview of CEFR-level annotated Swedish datasets. L2 learners primarily"
C16-1198,P11-1019,0,0.1513,"Missing"
C16-1198,W15-0626,1,0.907292,"Missing"
C16-1198,W13-1705,0,0.111122,"Missing"
D07-1060,W02-2006,0,0.0324426,"Missing"
D07-1060,P06-1046,0,0.0283785,"ombining written text with a published thesaurus to measure distance between concepts (or word senses) using distributional measures, thereby eliminating sense-conflation and achieving results better than the simple word-distance measures and indeed also most of the WordNet-based semantic measures. We called these measures distributional measures of concept-distance. Concept-distance 2 LSA is especially expensive as singular value decomposition, a key component for dimensionality reduction, requires computationally intensive matrix operations; making it less scalable to large amounts of text (Gorman and Curran, 2006). 572 measures can be used to measure distance between a word pair by choosing the distance between their closest senses. Thus, even though ‘children’s recreation’ is the predominant sense of play, the ‘drama’ sense is much closer to actor and so their distance will be chosen. These distributional conceptdistance approaches need to create only V × C cooccurrence and C × C distance matrices, where C is the number of categories or senses (usually about 1000). It should also be noted that unlike the best WordNet-based measures, distributional measures (both word- and concept-distance ones) can be"
D07-1060,I05-1067,1,0.955571,"ke resources that these methods require do not exist for most of the 3000–6000 languages in existence today and they are costly to create. In this paper, we introduce cross-lingual distributional measures of concept-distance, or simply cross-lingual measures, that determine the distance between a word pair belonging to a resource-poor language using a knowledge source in a resourcerich language and a bilingual lexicon3 . We will use the cross-lingual measures to calculate distances between German words using an English thesaurus and a German corpus. Although German is not resourcepoor per se, Gurevych (2005) has observed that the German wordnet GermaNet (Kunze, 2004) (about 60,000 synsets) is less developed than the English WordNet (Fellbaum, 1998) (about 117,000 synsets) with respect to the coverage of lexical items and lexical semantic relations represented therein. On the other hand, substantial raw corpora are available for the German language. Crucially for our evaluation, the existence of GermaNet allows comparison of our cross-lingual approach with monolingual ones. 2 Monolingual Distributional Measures In order to set the context for cross-lingual conceptdistance measures (Section 3), we"
D07-1060,O97-1002,0,0.740037,"guistic resources. In this paper, we propose a new method that allows us to compute semantic distance in a possibly resource-poor language by seamlessly combining its text with a knowledge source in a different, preferably resource-rich, language. We demonstrate the approach by combining German text with an English thesaurus to create English– German distributional profiles of concepts, which in turn will be used to measure the semantic distance between German words. Two classes of methods have been used in determining semantic distance. Semantic measures of concept-distance, such as those of Jiang and Conrath (1997) and Resnik (1995), rely on the structure of a knowledge source, such as WordNet, to determine the distance between two concepts defined in it (see Budanitsky and Hirst (2006) for a survey). Distributional measures of word-distance1 , such as cosine and α-skew divergence (Lee, 2001), deem 1 Many distributional approaches represent the sets of contexts of the target words as points in multidimensional cooccurrence space or as co-occurrence distributions. A measure, such as cosine, that captures vector distance or a measure, such as α-skew divergence, that captures distance between distributions"
D07-1060,S07-1004,0,0.0788107,"Missing"
D07-1060,W04-2607,1,0.78437,"e synset and in synsets close to it in the network. 576 (2005) and Zesch et al. (2007) asked native German speakers to mark two different sets of German word pairs with distance values. Set 1 (Gur65) consists of a German translation of the English Rubenstein and Goodenough (1965) dataset. It has 65 noun– noun word pairs. Set 2 (Gur350) is a larger dataset containing 350 word pairs made up of nouns, verbs, and adjectives. The semantically close word pairs in Gur65 are mostly synonyms or hypernyms (hyponyms) of each other, whereas those in Gur350 have both classical and non-classical relations (Morris and Hirst, 2004) with each other. Details of these semantic distance benchmarks13 are summarized in Table 2. Inter-subject correlations are indicative of the degree of ease in annotating the datasets. 4.1.2 Results and Discussion Word-pair distances determined using different distance measures are compared in two ways with the two human-created benchmarks. The rank ordering of the pairs from closest to most distant is evaluated with Spearman’s rank order correlation ρ; the distance judgments themselves are evaluated with Pearson’s correlation coefficient r. The higher the correlation, the more accurate the me"
D07-1060,2003.mtsummit-papers.42,0,0.0345828,"n raw text and possibly some shallow syntactic processing. They do not require any other manually-created resource, and tend to have a higher coverage. However, by themselves they perform poorly when compared to semantic measures (Mohammad and Hirst, 2006b) because when given a target word pair we usually need the distance between their closest senses, but distributional measures of word-distance tend to conflate the distances between all possible sense pairs. Latent semantic analysis (LSA) (Landauer et al., 1998) has also been used to measure distributional distance with encouraging results (Rapp, 2003). However, it too measures the distance between words and not senses. Further, the dimensionality reduction inherent to LSA has the effect of making the predominant sense more dominant while de-emphasizing the other senses. Therefore, an LSA-based approach will also conflate information from the different senses, and even more emphasis will be placed on the predominant senses. Given the semantically close target nouns play and actor, for example, a distributional measure will give a score that is some sort of a dominance-based average of the distances between their senses. The noun play has th"
D07-1060,P98-2127,0,0.234235,"Missing"
D07-1060,P04-1036,0,0.211979,"cooccurs with any word. A statistic such as PMI can then give the strength of association between w and c. with each of its senses is summed. The sense that has the highest cumulative association is chosen as the intended sense. A new bootstrapped WCCM is created such that each cell mi j , corresponding to en word wen i and concept c j , is populated with the en number of times wi co-occurs with any word used in sense cen j . Mohammad and Hirst (2006a) used the DPCs created from the bootstrapped WCCM to attain nearupper-bound results in the task of determining word sense dominance. Unlike the McCarthy et al. (2004) dominance system, our approach can be applied to much smaller target texts (a few hundred sentences) without the need for a large similarly-sensedistributed text5 . In Mohammad and Hirst (2006a), the DPC-based monolingual distributional measures of concept-distance were used to rank word pairs by their semantic similarity and to correct realword spelling errors, attaining markedly better results than monolingual distributional measures of word-distance. In the spelling correction task, the distributional concept-distance measures performed better than all WordNet-based measures as well, excep"
D07-1060,P06-1040,0,0.0290057,". Nachbildung (replica) b. Doppelkinn (double chin) d. Zweitschrift (copy) Our approach to evaluating distance measures fol14 In Table 3, all values are statistically significant at the 0.01 level (2-tailed), except for the one in italic (0.212), which is significant at the 0.05 level (2-tailed). 15 English translations are in parentheses. 577 lows that of Jarmasz and Szpakowicz (2003), who evaluated semantic similarity measures through their ability to solve synonym problems (80 TOEFL (Landauer and Dumais, 1997), 50 ESL (Turney, 2001), and 300 (English) Reader’s Digest Word Power questions). Turney (2006) used a similar approach to evaluate the identification of semantic relations, with 374 college-level multiple-choice word analogy questions. The Reader’s Digest Word Power (RDWP) benchmark for German consists of 1072 of these word-choice problems collected from the January 2001 to December 2005 issues of the Germanlanguage edition (Wallace and Wallace, 2005). We discarded 44 problems that had more than one correct answer, and 20 problems that used a phrase instead of a single term as the target. The remaining 1008 problems form our evaluation dataset, which is significantly larger than any of"
D07-1060,E06-1016,1,0.581452,"all performance is again better than the best monolingual measures. 1 Introduction Accurately estimating the semantic distance between concepts or between words in context has pervasive applications in computational linguistics, including machine translation, information retrieval, speech recognition, spelling correction, and text categorization (see Budanitsky and Hirst (2006) for discussion), and it is becoming clear that basing such measures on a combination of corpus statistics with a knowledge source, such as a dictionary, published thesaurus, or WordNet, can result in higher accuracies (Mohammad and Hirst, 2006b). This is because such knowledge sources capture semantic information about concepts and, to some extent, world knowledge. They also act as sense inventories for the words in a language. However, applying algorithms for semantic distance to most languages is hindered by the lack of linguistic resources. In this paper, we propose a new method that allows us to compute semantic distance in a possibly resource-poor language by seamlessly combining its text with a knowledge source in a different, preferably resource-rich, language. We demonstrate the approach by combining German text with an Eng"
D07-1060,W06-1605,1,0.605764,"all performance is again better than the best monolingual measures. 1 Introduction Accurately estimating the semantic distance between concepts or between words in context has pervasive applications in computational linguistics, including machine translation, information retrieval, speech recognition, spelling correction, and text categorization (see Budanitsky and Hirst (2006) for discussion), and it is becoming clear that basing such measures on a combination of corpus statistics with a knowledge source, such as a dictionary, published thesaurus, or WordNet, can result in higher accuracies (Mohammad and Hirst, 2006b). This is because such knowledge sources capture semantic information about concepts and, to some extent, world knowledge. They also act as sense inventories for the words in a language. However, applying algorithms for semantic distance to most languages is hindered by the lack of linguistic resources. In this paper, we propose a new method that allows us to compute semantic distance in a possibly resource-poor language by seamlessly combining its text with a knowledge source in a different, preferably resource-rich, language. We demonstrate the approach by combining German text with an Eng"
D07-1060,N07-2052,1,0.781373,"Missing"
D07-1060,C92-2070,0,\N,Missing
D07-1060,J06-1003,1,\N,Missing
D17-1076,attardi-etal-2010-resource,0,0.061148,"Missing"
D17-1076,L16-1107,0,0.0341532,"Missing"
D17-1076,bosco-etal-2012-parallel,0,0.0564066,"Missing"
D17-1076,agic-ljubesic-2014-setimes,0,0.062809,"Missing"
D17-1076,A00-1031,0,0.582935,"Missing"
D17-1076,P07-2053,0,0.0335342,"al., 2014) (Einarsson, 1976) (Ejerhed and Källgren, 1997) (Aluísio et al., 2003) (Paroubek, 2000) (Candito et al., 2014) (Bosco et al., 2012) (Marimon et al., 2014) (Željko Agi´c and Ljubeši´c, 2014) (Ljubeši´c et al., 2016) (Bejˇcek et al., 2013) (Przepiórkowski et al., 2008) (Bocharov et al., 2013) (Erjavec, 2010) (Erjavec, 2002) (Krek et al., 2013) (Augustinus et al., 2016) (Voutilainen, 2011) (Itai and Wintner, 2008) (Csendes et al., 2005) Table 1: Corpora used in our experiments To provide a reference value to a well-known PoS tagger, we will compare all results to the HMM-based HunPos (Halácsy et al., 2007) tagger, which is a freely available re-implementation of the TNT tagger (Brants, 2000). HunPos has been used before for training models of various languages and tagsets (Seraji, 2011; Attardi et al., 2010; Hládek et al., 2012) which is why we consider this tagger to be a suitable baseline. tagger unsuited for learning models for other languages or tagsets. We will, thus, experiment with features and their configurations, and investigate how well they perform in combination for learning fine-grained tagsets of various languages. We implement those experiments using CRF which are frequently use"
D17-1076,W10-1817,0,0.0406307,"Missing"
D17-1076,W16-2615,1,0.840947,"erman-1 rm -2 a NoIslann-3 r d Sw wegi ic Swedishan edi -1 sh B-P -2 Freortug French- . nc 1 I h-2 Sptalian ani sh Cro Croatian atia -1 Cz n-2 P ech Ru olish ssi SloSlovan Sloveneak ven -1 e-2 Af rik Finaans n HuHebrish nga ew ria n 10 GERMANIC ROMANIC SLAVIC 4 CRF Experiments OTHER We reviewed the recent literature to determine the most commonly used features for training PoS taggers. As re-occurring features, we found word ngrams, fixed character sequences focusing on either pre-, in-, or suffixes of words and word distributional knowledge for PoS taggers of various languages (Brants, 2000; Horsmann and Zesch, 2016; Ljubeši´c et al., 2016). Word- and characterngrams have been used with various parametrizations depending on the language and there is no agreement which parameters are most advisable. We will, hence, run a series of parameter-search experiments over the word- and character-ngram parametrization to determine a configuration applicable to all languages. For this, we evaluate all permutations of the subsequently introduced feature configurations with 10fold cross-validation. The objective is to find a configuration that works well on all corpora, languages, and tagsets. Figure 1: Coarse-graine"
D17-1076,R15-1016,0,0.0162444,"a learning rate of 0.1 and adding Gaussian noise of 0.2 to the embedding layer. We train word embeddings on the data we already used for the semantic feature in the CRF experiments by using fastText (Bojanowski et al., 2016) . The the character-level embeddings are trained on-the-fly. 100 Figure 2: Variance of CRF taggers (10fold CV) all kinds of character-ngrams. The cluster feature also performs better than the word-ngrams. Considering that we had to limit the amount of data for creating the clusters for comparability, this feature assumedly has more potential when using larger data sizes (Derczynski et al., 2015). The combination of all features in the column Best CRF shows that the features address quite different information and add up well, so unsurprisingly, this configuration reaches the overall best accuracies. The difference to HunPos is, with often less than one percent point difference, only small. Off-the-shelf taggers do, hence, not necessary have a disadvantage over constructing an own tagger. In the remainder of this work, we will use the Best CRF configuration when discussing CRF tagger results. 5 LSTM Experiments When using neural networks, the details of how word and character informat"
D17-1076,erjavec-2010-multext,0,0.0708613,"Missing"
D17-1076,L16-1676,0,0.040502,"Missing"
D17-1076,foth-etal-2014-size,0,0.0617338,"Missing"
D17-1076,marimon-etal-2014-iula,0,0.0276327,"Missing"
D17-1076,przepiorkowski-etal-2008-towards,0,0.0863472,"Missing"
D17-1076,quasthoff-etal-2006-corpus,0,0.0517486,"9.2 72.5 70.9 31.9 39.4 39.0 84.9 83.4 83.1 95.5 95.5 83.5 86.4 83.0 65.4 63.9 62.9 91.5 87.5 63.8 62.5 59.4 84.7 82.6 81.7 93.6 94.6 82.9 82.6 86.2 66.7 63.9 60.9 85.4 83.6 61.6 59.6 59.5 Other Afrikaans Finnish Hebrew Hungarian 95.7 62.6 82.3 72.7 75.0 10.0 41.7 13.9 95.3 77.1 81.3 86.7 80.3 48.5 60.9 63.3 95.8 67.8 76.3 72.0 81.9 33.8 53.3 31.7 97.8 82.3 90.5 89.9 89.6 56.7 68.5 69.6 97.3 81.3 90.3 89.4 85.5 55.8 60.1 69.5 Lang. Group Clusters All OOV Best CRF All OOV HunPos All OOV Table 2: Accuracy of CRF taggers (10fold CV) unlabelled text is obtained from the Leipzig Corpus Collection (Quasthoff et al., 2006), which provides large text quantities crawled from the web for many languages. We use 15 · 106 tokens to create the clusters from the same amount of text for all languages. We provide the cluster ids in substrings of varying length to the classifier (Owoputi et al., 2013). quite large which is caused by the lower number of character-ngrams in those configurations. For corpora such as Slovene-1, we see that more accurate configurations exist than Best CRF but more importantly, the selected configuration is always among the best working ones. We show the results of Best CRF and the performance"
D17-1076,W16-2613,0,0.0520193,"Missing"
D17-1076,W11-4654,0,0.0227025,"c, 2014) (Ljubeši´c et al., 2016) (Bejˇcek et al., 2013) (Przepiórkowski et al., 2008) (Bocharov et al., 2013) (Erjavec, 2010) (Erjavec, 2002) (Krek et al., 2013) (Augustinus et al., 2016) (Voutilainen, 2011) (Itai and Wintner, 2008) (Csendes et al., 2005) Table 1: Corpora used in our experiments To provide a reference value to a well-known PoS tagger, we will compare all results to the HMM-based HunPos (Halácsy et al., 2007) tagger, which is a freely available re-implementation of the TNT tagger (Brants, 2000). HunPos has been used before for training models of various languages and tagsets (Seraji, 2011; Attardi et al., 2010; Hládek et al., 2012) which is why we consider this tagger to be a suitable baseline. tagger unsuited for learning models for other languages or tagsets. We will, thus, experiment with features and their configurations, and investigate how well they perform in combination for learning fine-grained tagsets of various languages. We implement those experiments using CRF which are frequently used for PoS tagging (Remus et al., 2016; Ljubeši´c et al., 2016). The second paradigm is Architecture Engineering, which relies on methods to learn the input representation by themselve"
D17-1076,solberg-etal-2014-norwegian,0,0.062566,"Missing"
D17-1076,E09-1087,0,0.0812068,"Missing"
D17-1076,telljohann-etal-2004-tuba,0,0.147939,"Missing"
D17-1076,N13-1039,0,0.0320896,"77.1 81.3 86.7 80.3 48.5 60.9 63.3 95.8 67.8 76.3 72.0 81.9 33.8 53.3 31.7 97.8 82.3 90.5 89.9 89.6 56.7 68.5 69.6 97.3 81.3 90.3 89.4 85.5 55.8 60.1 69.5 Lang. Group Clusters All OOV Best CRF All OOV HunPos All OOV Table 2: Accuracy of CRF taggers (10fold CV) unlabelled text is obtained from the Leipzig Corpus Collection (Quasthoff et al., 2006), which provides large text quantities crawled from the web for many languages. We use 15 · 106 tokens to create the clusters from the same amount of text for all languages. We provide the cluster ids in substrings of varying length to the classifier (Owoputi et al., 2013). quite large which is caused by the lower number of character-ngrams in those configurations. For corpora such as Slovene-1, we see that more accurate configurations exist than Best CRF but more importantly, the selected configuration is always among the best working ones. We show the results of Best CRF and the performance of the individual features for each language in Table 2, and compare the results to HunPos, the highest accuracies are highlighted in grey. When evaluating the features separately, the character-ngrams reach the highest accuracy on OOV words. Especially on the Slavic langu"
D17-1076,paroubek-2000-language,0,0.0363128,"Missing"
D17-1076,D15-1176,0,\N,Missing
D17-1076,J92-4003,0,\N,Missing
E12-1054,N04-1038,0,0.0368053,"the precision of knowledge-based approaches has been under-estimated. Additionally, we show that knowledge-based approaches can be improved by using semantic relatedness measures that make use of knowledge beyond classical taxonomic relations. Finally, we show that statistical and knowledgebased methods can be combined for increased performance. 1 Introduction Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and D´esilets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004), or malapropism detection (Bolshakov and Gelbukh, 2003). The main idea is always to test what fits better into the current context: the actual term or a possible replacement that is phonetically, structurally, or semantically similar. We are going to focus on malapropism detection as it allows evaluating measures of contextual fitness in a more direct way than evaluating in a complex application which always entails influence from other components, e.g. the quality of the optical character recognition module (Walker et al., 2010). A malapropism or real-word spelling error occurs when a word i"
E12-1054,J06-1003,0,0.782501,"ch we consider to be perfectly valid candidates. Second, they also applied a filter using a list of known multi-words, as the probability for words to accidentally form multi-words is low. It is unclear which list was used. We could use multi-words from WordNet, but coverage would be rather limited. We decided not to use both filters in order to better assess the influence of the underlying semantic relatedness measure on the overall performance. The knowledge based approach uses semantic relatedness measures to determine the cohesion between a candidate and its context. In the experiments by Budanitsky and Hirst (2006), the measure by (Jiang and Conrath, 1997) yields the best results. However, a wide range of other measures have been proposed, cf. (Zesch and Gurevych, 2010). Some measures using a wider definition of semantic relatedness (Gabrilovich and Markovitch, 2007; Zesch et al., 2008b) instead of only using taxonomic relations in a knowledge source. As semantic relatedness measures usually return a numeric value, we need to determine a threshold θ in order to come up with a binary related/unrelated decision. Budanitsky and Hirst (2006) used a characteristic gap in the standard evaluation dataset by Ru"
E12-1054,P11-4017,1,0.787364,"re then corrected in subsequent revisions of the same article. The challenge lies in discriminating real-word spelling errors from all sorts of other changes, including non-word spelling errors, reformulations, or the correction of wrong facts. For that purpose, we apply a set of precisionoriented heuristics narrowing down the number of possible error candidates. Such an approach is feasible, as the high number of revisions in Wikipedia allows to be extremely selective. 2.1 Accessing the Revision Data We access the Wikipedia revision data using the freely available Wikipedia Revision Toolkit (Ferschke et al., 2011) together with the JWPL Wikipedia API (Zesch et al., 2008a).3 The API outputs plain text converted from Wiki-Markup, but the text still contains a small portion of leftover markup and other artifacts. Thus, we perform additional cleaning steps removing (i) tokens with more than 30 characters (often URLs), (ii) sentences with less than 5 or more than 200 tokens, and (iii) sentences containing a high fraction of special characters like ‘:’ usually indicating Wikipedia-specific artifacts like lists of language links. The remaining sentences are part-ofspeech tagged and lemmatized using TreeTagger"
E12-1054,P05-1045,0,0.0215103,"ed on the Google Web1T n-gram counts (Brants and Franz, 2006). We filter all sentences where the replaced token has a very low unigram count. We experimented with different values and found 25,000 for English and 10,000 for German to yield good results. Same Lemma The original token and the replaced token may not have the same lemma, e.g. ‘car’ and ‘cars’ would not pass this filter. Stopwords The replaced token should not be in a short list of stopwords (mostly function words). Named Entity The replaced token should not be part of a named entity. For this purpose, we applied the Stanford NER (Finkel et al., 2005). Normal Spelling Error We apply the Jazzy spelling detector4 and rule out all cases in which it is able to detect the error. Semantic Relation If the original token and the replaced token are in a close lexical-semantic rela4 http://jazzy.sourceforge.net/ tions, the change is likely to be semantically motivated, e.g. if “house” was replaced with “hut”. Thus, we do not consider cases, where we detect a direct semantic relation between the original and the replaced term. For this purpose, we use WordNet (Fellbaum, 1998) for English and GermaNet (Lemnitzer and Kunze, 2002) for German. 3 Resultin"
E12-1054,P96-1010,0,0.135744,"SJ corpus. This is consistent with our findings on the artificial errors based on the Brown corpus, but - as we have seen in the previous section - evaluation on the naturally occurring errors shows a different picture. They also tried to improve the model by permitting multiple corrections and using fixed-length context windows instead of sentences, but obtained discouraging results. All previously discussed methods are unsupervised in a way that they do not rely on any training data with annotated errors. However, real-word spelling correction has also been tackled by supervised approaches (Golding and Schabes, 1996; Jones and Martin, 1997; Carlson et al., 2001). Those methods rely on predefined confusion-sets, i.e. sets of words that are often confounded e.g. {peace, piece} or {weather, whether}. For each set, the methods learn a model of the context in which one or the other alternative is more probable. This yields very high precision, but only for the limited number of previously defined confusion sets. Our framework for extracting natural errors could be used to increase the number of known confusion sets. 7 Conclusions and Future Work In this paper, we evaluated two main approaches for measuring th"
E12-1054,H05-1007,0,0.06366,"Missing"
E12-1054,D09-1129,0,0.185513,"ance enormously increases the search space resulting in increased run-time and possibly decreased detection precision due to more false positives. 3.2 corpus that is known to be free of spelling errors, sentences are randomly sampled. For each sentence, a random word is selected and all strings with edit distance smaller than a given threshold (2 in our case) are generated. If one of those generated strings is a known word from the vocabulary, it is picked as the artificial error. Previous work on evaluating real-word spelling correction (Hirst and Budanitsky, 2005; WilcoxOHearn et al., 2008; Islam and Inkpen, 2009) used a dataset sampled from the Wall Street Journal corpus which is not freely available. Thus, we created a comparable English dataset of 1,000 artificial errors based on the easily available Brown corpus (Francis W. Nelson and Kuc¸era, 1964).8 Additionally, we created a German dataset with 1,000 artificial errors based on the TIGER corpus.9 4 Measuring Contextual Fitness There are two main approaches for measuring the contextual fitness of a word in its context: the statistical (Mays et al., 1991) and the knowledgebased approach (Hirst and Budanitsky, 2005). 4.1 Mays et al. (1991) introduce"
E12-1054,O97-1002,0,0.171386,"es. Second, they also applied a filter using a list of known multi-words, as the probability for words to accidentally form multi-words is low. It is unclear which list was used. We could use multi-words from WordNet, but coverage would be rather limited. We decided not to use both filters in order to better assess the influence of the underlying semantic relatedness measure on the overall performance. The knowledge based approach uses semantic relatedness measures to determine the cohesion between a candidate and its context. In the experiments by Budanitsky and Hirst (2006), the measure by (Jiang and Conrath, 1997) yields the best results. However, a wide range of other measures have been proposed, cf. (Zesch and Gurevych, 2010). Some measures using a wider definition of semantic relatedness (Gabrilovich and Markovitch, 2007; Zesch et al., 2008b) instead of only using taxonomic relations in a knowledge source. As semantic relatedness measures usually return a numeric value, we need to determine a threshold θ in order to come up with a binary related/unrelated decision. Budanitsky and Hirst (2006) used a characteristic gap in the standard evaluation dataset by Rubenstein and Goodenough (1965) that separa"
E12-1054,A97-1025,0,0.103743,"nt with our findings on the artificial errors based on the Brown corpus, but - as we have seen in the previous section - evaluation on the naturally occurring errors shows a different picture. They also tried to improve the model by permitting multiple corrections and using fixed-length context windows instead of sentences, but obtained discouraging results. All previously discussed methods are unsupervised in a way that they do not rely on any training data with annotated errors. However, real-word spelling correction has also been tackled by supervised approaches (Golding and Schabes, 1996; Jones and Martin, 1997; Carlson et al., 2001). Those methods rely on predefined confusion-sets, i.e. sets of words that are often confounded e.g. {peace, piece} or {weather, whether}. For each set, the methods learn a model of the context in which one or the other alternative is more probable. This yields very high precision, but only for the limited number of previously defined confusion sets. Our framework for extracting natural errors could be used to increase the number of known confusion sets. 7 Conclusions and Future Work In this paper, we evaluated two main approaches for measuring the contextual fitness of"
E12-1054,C90-2036,0,0.671071,"approach can be improved by using semantic relatedness measures that incorporate knowledge beyond the taxonomic relations in a classical lexical-semantic resource like WordNet. Finally, by combining both approaches, significant increases in precision or recall can be achieved. In future work, we want to evaluate a wider range of contextual fitness measures, and learn how to combine them using more sophisticated combination strategies. Both - the statistical as well as the knowledge-based approach - will benefit from a better model of the typist, as not all edit operations are equally likely (Kernighan et al., 1990). On the side of the error extraction, we are going to further improve the extraction process by incorporating more knowledge about the revisions. For example, vandalism is often reverted very quickly, which can be detected when looking at the full set of revisions of an article. We hope that making the experimental framework publicly available will foster future research in this field, as our results on the natural errors show that the problem is still quite challenging. Acknowledgments This work has been supported by the Volkswagen Foundation as part of the LichtenbergProfessorship Program u"
E12-1054,max-wisniewski-2010-mining,0,0.205929,"that precision can be significantly improved using the ‘Union’ strategy, while recall is only moderately improved using the ‘Intersect’ strategy. This means that (i) a large subset of errors is detected by both approaches that due to their different sources of knowledge mutually reinforce the detection leading to increased precision, and (ii) a small but otherwise undetectable subset of errors requires considering detections made by one approach only. 6 Related Work To our knowledge, we are the first to create a dataset of naturally occurring errors based on the revision history of Wikipedia. Max and Wisniewski (2010) used similar techniques to create a dataset of errors from the French Wikipedia. However, they target a wider class of errors including non-word spelling errors, and their class of real-word errors conflates malapropisms as well as other types of changes like reformulations. Thus, their dataset cannot be easily used for our purposes and is only available in French, while our framework allows creating datasets for all major languages with minimal manual effort. Another possible source of real-word spelling errors are learner corpora (Granger, 2002), e.g. the Cambridge Learner Corpus (Nicholls,"
E12-1054,W10-1004,0,0.0232143,"French Wikipedia. However, they target a wider class of errors including non-word spelling errors, and their class of real-word errors conflates malapropisms as well as other types of changes like reformulations. Thus, their dataset cannot be easily used for our purposes and is only available in French, while our framework allows creating datasets for all major languages with minimal manual effort. Another possible source of real-word spelling errors are learner corpora (Granger, 2002), e.g. the Cambridge Learner Corpus (Nicholls, 1999). However, annotation of errors is difficult and costly (Rozovskaya and Roth, 2010), only a small fraction of observed errors will be real-word spelling errors, and learners are likely to make dif535 ferent mistakes than proficient language users. Islam and Inkpen (2009) presented another statistical approach using the Google Web1T data (Brants and Franz, 2006) to create the n-gram model. It slightly outperformed the approach by Mays et al. (1991) when evaluated on a corpus of artificial errors based on the WSJ corpus. However, the results are not directly comparable, as Mays et al. (1991) used a much smaller n-gram model and our results in Section 5.1 show that the size of"
E12-1054,C04-1024,0,0.0117721,"ogether with the JWPL Wikipedia API (Zesch et al., 2008a).3 The API outputs plain text converted from Wiki-Markup, but the text still contains a small portion of leftover markup and other artifacts. Thus, we perform additional cleaning steps removing (i) tokens with more than 30 characters (often URLs), (ii) sentences with less than 5 or more than 200 tokens, and (iii) sentences containing a high fraction of special characters like ‘:’ usually indicating Wikipedia-specific artifacts like lists of language links. The remaining sentences are part-ofspeech tagged and lemmatized using TreeTagger (Schmid, 2004). Using these cleaned and annotated articles, we form pairs of adjacent article revisions (ri and ri+1 ). 2.2 Sentence Alignment Fully aligning all sentences of the adjacent revisions is a quite costly operation, as sentences can be split, joined, replaced, or moved in the article. However, we are only looking for sentence pairs which are almost identical except for the real-word spelling error and its correction. Thus, we form all sentence pairs and then apply an aggressive but cheap filter that rules out all sentences which (i) are equal, or (ii) whose lengths differ more than a small number"
E12-1054,D10-1024,0,0.0264674,"ter recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004), or malapropism detection (Bolshakov and Gelbukh, 2003). The main idea is always to test what fits better into the current context: the actual term or a possible replacement that is phonetically, structurally, or semantically similar. We are going to focus on malapropism detection as it allows evaluating measures of contextual fitness in a more direct way than evaluating in a complex application which always entails influence from other components, e.g. the quality of the optical character recognition module (Walker et al., 2010). A malapropism or real-word spelling error occurs when a word is replaced with another correctly spelled word which does not suit the context, e.g. “People with lots of honey usually live in big houses.”, where ‘money’ was replaced with ‘honey’. Besides typing mistakes, a major source of such errors is the failed attempt of automatic spelling correctors to correct a misspelled word (Hirst and Budanitsky, 2005). A real-word spelling error is hard to detect, as the erroneous word is not misspelled and fits syntactically into the sentence. Thus, measures of contextual fitness are required to det"
E12-1054,N10-1056,0,0.173724,"ollow-up experiments. The framework contains (i) methods to extract natural errors from Wikipedia, (ii) reference implementations of the knowledge-based and the statistical methods, and (iii) the evaluation datasets described in this paper. 2 Mining Errors from Wikipedia Measures of contextual fitness have previously been evaluated using artificially created datasets, as there are very few sources of sentences with naturally occurring errors and their corrections. Recently, the revision history of Wikipedia has been introduced as a valuable knowledge source for NLP (Nelken and Yamangil, 2008; Yatskar et al., 2010). It is also a possible source of natural errors, as it is likely that Wikipedia editors make 1 The same artificial data as described in Section 3.2. 2 http://code.google.com/p/dkpro-spelling-asl/ real-word spelling errors at some point, which are then corrected in subsequent revisions of the same article. The challenge lies in discriminating real-word spelling errors from all sorts of other changes, including non-word spelling errors, reformulations, or the correction of wrong facts. For that purpose, we apply a set of precisionoriented heuristics narrowing down the number of possible error c"
E12-1054,zesch-etal-2008-extracting,1,0.85059,"Missing"
E12-1054,kunze-lemnitzer-2002-germanet,0,\N,Missing
I13-1112,P12-2059,0,0.188451,"uction. Previous experiments with character-based MT have been performed for different purposes. Pennell and Liu (2011) expand text message abbreviations into proper English. In Stymne (2011), character-based MT is used for the identification of common spelling errors. Several other approaches also apply MT algorithms for transliteration of named entities to increase the vocabulary coverage (Rama and Gali, 2009; Finch and Sumita, 2008). For transliteration, characters from one alphabet are mapped onto corresponding letters in another alphabet. Cognates follow more complex production patterns. Nakov and Tiedemann (2012) aim at improving MT quality using cognates detected by character-based alignment. They focus on the language pair MacedonianBulgarian and use English as a bridge language. As they use cognate identification only as an intermediary step and do not provide evaluation results, we cannot directly compare with their work. To the best of our knowledge, we are the first to use statistical character-based MT for the goal of directly producing cognates. Character-Based Machine Translation Our approach relies on statistical phrase-based machine translation (MT). As we are not interested in the translat"
I13-1112,I08-8003,0,0.0278785,"r MT (Tiedemann, 2009; Koehn and Knight, 2002; Ribeiro et al., 2001). As explained above, we approach the phenomenon from the opposite direction and use statistical MT for cognate production. Previous experiments with character-based MT have been performed for different purposes. Pennell and Liu (2011) expand text message abbreviations into proper English. In Stymne (2011), character-based MT is used for the identification of common spelling errors. Several other approaches also apply MT algorithms for transliteration of named entities to increase the vocabulary coverage (Rama and Gali, 2009; Finch and Sumita, 2008). For transliteration, characters from one alphabet are mapped onto corresponding letters in another alphabet. Cognates follow more complex production patterns. Nakov and Tiedemann (2012) aim at improving MT quality using cognates detected by character-based alignment. They focus on the language pair MacedonianBulgarian and use English as a bridge language. As they use cognate identification only as an intermediary step and do not provide evaluation results, we cannot directly compare with their work. To the best of our knowledge, we are the first to use statistical character-based MT for the"
I13-1112,J03-1002,0,0.00657185,"ge in the 5 best productions and the MRR for each parameter. N-gram Size We start with the n-gram size parameter that determines the tokenization of the input, the respective format for unigrams, bigrams, and trigrams for the word banc looks as follows: # b a n c $ / #b ba an nc c$ / #ba ban anc nc$ Higher order n-grams in general increase the vocabulary and thus lead to better alignment. However, they also require a larger amount of training data, otherwise the number of unseen instances is Moses Parameters Finally, we tune the Moses parameter weights by applying minimum error rate training (Och and Ney, 2003) using the development set, but it makes almost no difference in this setting. Tuning optimizes the model with respect to the BLEU score. For our data, the BLEU score is quite high for all produced cognate candidates, but it is not indicative of the usefulness of the transformation. A word containing one wrong character is not necessarily better than a word con3 The cognates have been retrieved from several web resources and merged with the set used by Montalvo et al. (2012). All test cognate list can be found at: http://www.ukp.tu-darmstadt.de/data 886 0.7 Training Size 0.6 Cognates 0.5 Trans"
I13-1112,P02-1040,0,0.0867573,"Missing"
I13-1112,E12-1059,1,0.738926,"rve in Figure 3 shows the results. As expected, both coverage and MRR improve with increasing size of the training data, but we do not see much improvement after about 1,000 training instances. Thus, COP is able to learn stable patterns from relatively few training instances. However, even a list of 1,000 cognates is a hard constraint for some language pairs. Thus, we test if we can also produce satisfactory results with lower quality sets of training pairs that might be easier to obtain than a list of cognates. We use word pairs extracted from the freely available multilingual resources UBY (Gurevych et al., 2012) and Universal WordNet (UWN) (de Melo and Weikum, 2009). UBY combines several lexical-semantic resources, we use translations which were extracted from Wiktionary. UWN is based on WordNet and Wikipedia and provides automatically extracted translations for over 200 languages that are a bit noisier compared to UBY translations. Additionally, we queried the Microsoft Bing translation API using all words from 4.4 Comparison to Previous Work Previous work (Kondrak and Dorr, 2004; Inkpen et al., 2005; Sep´ulveda Torres and Aluisio, 2011; 4 http://www.bing.com/translator We only use every 5th word in"
I13-1112,I11-1109,0,0.0192185,"tly produce a cognate in the target language from an input word in another language. Consequently, in the remainder of the paper, we refer to our method as COP (COgnate Production). Exploiting the orthographic similarity of cognates to improve the alignment of words has already been analyzed as a useful preparation for MT (Tiedemann, 2009; Koehn and Knight, 2002; Ribeiro et al., 2001). As explained above, we approach the phenomenon from the opposite direction and use statistical MT for cognate production. Previous experiments with character-based MT have been performed for different purposes. Pennell and Liu (2011) expand text message abbreviations into proper English. In Stymne (2011), character-based MT is used for the identification of common spelling errors. Several other approaches also apply MT algorithms for transliteration of named entities to increase the vocabulary coverage (Rama and Gali, 2009; Finch and Sumita, 2008). For transliteration, characters from one alphabet are mapped onto corresponding letters in another alphabet. Cognates follow more complex production patterns. Nakov and Tiedemann (2012) aim at improving MT quality using cognates detected by character-based alignment. They focus"
I13-1112,W09-3528,0,0.0207182,"useful preparation for MT (Tiedemann, 2009; Koehn and Knight, 2002; Ribeiro et al., 2001). As explained above, we approach the phenomenon from the opposite direction and use statistical MT for cognate production. Previous experiments with character-based MT have been performed for different purposes. Pennell and Liu (2011) expand text message abbreviations into proper English. In Stymne (2011), character-based MT is used for the identification of common spelling errors. Several other approaches also apply MT algorithms for transliteration of named entities to increase the vocabulary coverage (Rama and Gali, 2009; Finch and Sumita, 2008). For transliteration, characters from one alphabet are mapped onto corresponding letters in another alphabet. Cognates follow more complex production patterns. Nakov and Tiedemann (2012) aim at improving MT quality using cognates detected by character-based alignment. They focus on the language pair MacedonianBulgarian and use English as a bridge language. As they use cognate identification only as an intermediary step and do not provide evaluation results, we cannot directly compare with their work. To the best of our knowledge, we are the first to use statistical ch"
I13-1112,W02-0902,0,0.12847,"ne translation rithm selects the best combination of sequences. The transformation is thus not performed on isolated characters, it also considers the surrounding sequences and can account for context-dependent phenomena. The goal of the approach is to directly produce a cognate in the target language from an input word in another language. Consequently, in the remainder of the paper, we refer to our method as COP (COgnate Production). Exploiting the orthographic similarity of cognates to improve the alignment of words has already been analyzed as a useful preparation for MT (Tiedemann, 2009; Koehn and Knight, 2002; Ribeiro et al., 2001). As explained above, we approach the phenomenon from the opposite direction and use statistical MT for cognate production. Previous experiments with character-based MT have been performed for different purposes. Pennell and Liu (2011) expand text message abbreviations into proper English. In Stymne (2011), character-based MT is used for the identification of common spelling errors. Several other approaches also apply MT algorithms for transliteration of named entities to increase the vocabulary coverage (Rama and Gali, 2009; Finch and Sumita, 2008). For transliteration,"
I13-1112,C04-1117,0,0.0333755,"et language. If we already have candidate pairs, string similarity measures can be used to distinguish cognates and unrelated pairs (Montalvo et al., 2012; Sep´ulveda Torres and Aluisio, 2011; Inkpen et al., 2005; Kondrak and Dorr, 2004). However, these measures do not take the regular production processes into account that can be found for most cognates, e.g. the English suffix ~tion becomes ~ci´on in Spanish like in nation-naci´on or addition-adici´on. Thus, an alternative approach is to manually extract or learn production rules that reflect the regularities (Gomes and Pereira Lopes, 2011; Schulz et al., 2004). All these methods are based on string alignment and thus cannot be directly applied to language pairs with different alphabets. A possible workaround would be to first transliterate foreign alphabets into Latin, but unambiguous transliteration is only possible for some languages. Methods that rely on the phonetic similarity of words (Kondrak, 2000) require a phonetic transcription that is not always available. Thus, we propose a novel production approach using statistical characterbased machine translation in order to directly produce cognates. We argue that this has the following advantages"
I13-1112,C04-1137,0,0.277757,"t of the sentence with the help of associated words like Konferenz-conference or FebruarFebruary. Such pairs of associated words are called cognates. 883 International Joint Conference on Natural Language Processing, pages 883–891, Nagoya, Japan, 14-18 October 2013. In order to construct such cognate lists, we need to decide whether a word in a source language has a cognate in a target language. If we already have candidate pairs, string similarity measures can be used to distinguish cognates and unrelated pairs (Montalvo et al., 2012; Sep´ulveda Torres and Aluisio, 2011; Inkpen et al., 2005; Kondrak and Dorr, 2004). However, these measures do not take the regular production processes into account that can be found for most cognates, e.g. the English suffix ~tion becomes ~ci´on in Spanish like in nation-naci´on or addition-adici´on. Thus, an alternative approach is to manually extract or learn production rules that reflect the regularities (Gomes and Pereira Lopes, 2011; Schulz et al., 2004). All these methods are based on string alignment and thus cannot be directly applied to language pairs with different alphabets. A possible workaround would be to first transliterate foreign alphabets into Latin, but"
I13-1112,W11-4508,0,0.0175818,"Everybody who knows English might grasp the gist of the sentence with the help of associated words like Konferenz-conference or FebruarFebruary. Such pairs of associated words are called cognates. 883 International Joint Conference on Natural Language Processing, pages 883–891, Nagoya, Japan, 14-18 October 2013. In order to construct such cognate lists, we need to decide whether a word in a source language has a cognate in a target language. If we already have candidate pairs, string similarity measures can be used to distinguish cognates and unrelated pairs (Montalvo et al., 2012; Sep´ulveda Torres and Aluisio, 2011; Inkpen et al., 2005; Kondrak and Dorr, 2004). However, these measures do not take the regular production processes into account that can be found for most cognates, e.g. the English suffix ~tion becomes ~ci´on in Spanish like in nation-naci´on or addition-adici´on. Thus, an alternative approach is to manually extract or learn production rules that reflect the regularities (Gomes and Pereira Lopes, 2011; Schulz et al., 2004). All these methods are based on string alignment and thus cannot be directly applied to language pairs with different alphabets. A possible workaround would be to first t"
I13-1112,A00-2038,0,0.520697,"e.g. the English suffix ~tion becomes ~ci´on in Spanish like in nation-naci´on or addition-adici´on. Thus, an alternative approach is to manually extract or learn production rules that reflect the regularities (Gomes and Pereira Lopes, 2011; Schulz et al., 2004). All these methods are based on string alignment and thus cannot be directly applied to language pairs with different alphabets. A possible workaround would be to first transliterate foreign alphabets into Latin, but unambiguous transliteration is only possible for some languages. Methods that rely on the phonetic similarity of words (Kondrak, 2000) require a phonetic transcription that is not always available. Thus, we propose a novel production approach using statistical characterbased machine translation in order to directly produce cognates. We argue that this has the following advantages: (i) it captures complex patterns in the same way machine translation captures complex rephrasing of sentences, (ii) it performs better than similarity measures from previous work on cognates, and (iii) it also works for language pairs with different alphabets. 2 Figure 1: Character-based machine translation rithm selects the best combination of seq"
I13-1112,J99-1003,0,0.07861,"candidate pairs, we get many pairs with tied ranks, which is problematic for computing coverage and MRR. Thus, we randomize pairs within one rank and report averaged results over 10 randomization runs.8 We compare COP to three frequently used string similarity measures (LCSR, DICE, and XDICE), which performed well in (Inkpen et al., 2005; Montalvo et al., 2012), and to SpSim which is based on learning production rules. The longest common subsequence ratio (LCSR) calculates the ratio of the length of the longest (not necessarily contiguous) common subsequence and the length of the longer word (Melamed, 1999). DICE (Adamson and Boreham, 1974) measures the shared character bigrams, while the variant XDICE (Brew and McKelvie, 1996) uses extended bigrams, i.e. trigrams without the middle letter. SpSim (Gomes and Pereira Lopes, 2011) is based on string alignment of identical characters for the extraction and generalization of the most frequent cognate patterns. Word pairs that follow these extracted cognate patterns are considered equally similar as pairs with identical spelling. Table 3 shows the results. The differences between the individual similarity measures are very small, string similarity per"
I13-1112,W11-2159,0,0.0160037,"ge. Consequently, in the remainder of the paper, we refer to our method as COP (COgnate Production). Exploiting the orthographic similarity of cognates to improve the alignment of words has already been analyzed as a useful preparation for MT (Tiedemann, 2009; Koehn and Knight, 2002; Ribeiro et al., 2001). As explained above, we approach the phenomenon from the opposite direction and use statistical MT for cognate production. Previous experiments with character-based MT have been performed for different purposes. Pennell and Liu (2011) expand text message abbreviations into proper English. In Stymne (2011), character-based MT is used for the identification of common spelling errors. Several other approaches also apply MT algorithms for transliteration of named entities to increase the vocabulary coverage (Rama and Gali, 2009; Finch and Sumita, 2008). For transliteration, characters from one alphabet are mapped onto corresponding letters in another alphabet. Cognates follow more complex production patterns. Nakov and Tiedemann (2012) aim at improving MT quality using cognates detected by character-based alignment. They focus on the language pair MacedonianBulgarian and use English as a bridge la"
I13-1112,2009.eamt-1.3,0,0.0319562,"acter-based machine translation rithm selects the best combination of sequences. The transformation is thus not performed on isolated characters, it also considers the surrounding sequences and can account for context-dependent phenomena. The goal of the approach is to directly produce a cognate in the target language from an input word in another language. Consequently, in the remainder of the paper, we refer to our method as COP (COgnate Production). Exploiting the orthographic similarity of cognates to improve the alignment of words has already been analyzed as a useful preparation for MT (Tiedemann, 2009; Koehn and Knight, 2002; Ribeiro et al., 2001). As explained above, we approach the phenomenon from the opposite direction and use statistical MT for cognate production. Previous experiments with character-based MT have been performed for different purposes. Pennell and Liu (2011) expand text message abbreviations into proper English. In Stymne (2011), character-based MT is used for the identification of common spelling errors. Several other approaches also apply MT algorithms for transliteration of named entities to increase the vocabulary coverage (Rama and Gali, 2009; Finch and Sumita, 200"
I13-1112,P07-2045,0,\N,Missing
L16-1675,A00-1031,0,0.791444,"Missing"
L16-1675,P12-2071,0,0.0147035,"e common Wall-Street-Journal split using a default feature set. Furthermore, we train a social media model fitted to the Twitter domain that extends the default feature set by adding domain-specific features and incorporating knowledge from unsupervised data resources. Keywords: PoS tagging, domain adaptation, feature engineering 1. Motivation Part-of-speech tagging is an important preprocessing step in natural language processing. Consequently, there are many implementations available such as the Stanford tagger (Toutanova et al., 2003), the TreeTagger (Schmid, 1994), or the ClearNLP tagger (Choi and Palmer, 2012). In order to adapt the behaviour of the tagger to the needs of the user, most taggers provide configurable models which are targeted towards different languages or domains. A user might e.g. switch between a standard English model and a German social media model depending on the task at hand. As there rarely is a perfect match between the available tagging models and the documents to be processed, most taggers allow users to train their own models. FlexTag goes beyond the usual re-training possibilities of established taggers by giving the user flexible control over the feature extraction and"
L16-1675,P14-5011,1,0.866422,"Tag As FlexTag is implemented in Java, it does not need to be installed but runs wherever a JVM is available. The user only needs to download the library, while existing FlexTag models are automatically downloaded upon first usage without any additional user intervention. For using a model, users need to specify a target language and the model type, e.g. trained on newswire vs. social media or slow but accurate vs. fast classifier. In order to make this simplicity possible, FlexTag relies on DKPro Core1 (Eckart de Castilho and Gurevych, 2014) for preprocessing and model loading, and DKPro TC (Daxenberger et al., 2014) for feature extraction and classification. FlexTag can be used standalone or as an Apache UIMA component (Ferrucci and Lally, 2004) within a more complex processing pipeline. Boolean Features (∀ characters) Training FlexTag Most other trainable taggers only support one input format and users are supposed to transform their data in the required format. In contrast, FlexTag makes no assumptions about the input format and relies on the UIMA reader concept supporting all readers that are compatible with the DKPro type system. For most common data formats, DKPro Core already provides the necessary"
L16-1675,W14-5201,0,0.1315,"Missing"
L16-1675,P00-1036,0,0.726012,"ight directly implement an optimized machine learning classifier or a domain-specific feature set. Figure 1a shows how taggers with a fixed model look from the user perspective. A fixed-model tagger is basically a big black box that accepts raw text as input and outputs tagged text. Replaceable Model The next step on the flexibility continuum are taggers with replaceable models (see Figure 1b). Here, the user can change the behavior of the tagger by choosing from a set of provided models, but the tagger itself provides no means for creating a model. An example is the rule-based Hepple tagger (Hepple, 2000), where a rule set for English is provided. Rulesets for other languages can be specified, but there is no method provided for creating new models from training data. Trainable Model A major step towards really custommade taggers is to let users train their own models as shown in Figure 1c. While the tagger is still a black box, it provides an additional interface to turn PoS annotated training data into a custom-made model. Once the model is trained, the tagger works exactly like in the replaceable model case. Examples for trainable taggers are Stanford (Toutanova et al., 2003) or TreeTagger"
L16-1675,J93-2004,0,0.0655774,"Missing"
L16-1675,N13-1039,0,0.21535,"Missing"
L16-1675,D11-1141,0,0.275909,"Missing"
L16-1675,N03-1033,0,0.59802,"Tag by first training a PoS tagger model with state-of-the-art performance on the common Wall-Street-Journal split using a default feature set. Furthermore, we train a social media model fitted to the Twitter domain that extends the default feature set by adding domain-specific features and incorporating knowledge from unsupervised data resources. Keywords: PoS tagging, domain adaptation, feature engineering 1. Motivation Part-of-speech tagging is an important preprocessing step in natural language processing. Consequently, there are many implementations available such as the Stanford tagger (Toutanova et al., 2003), the TreeTagger (Schmid, 1994), or the ClearNLP tagger (Choi and Palmer, 2012). In order to adapt the behaviour of the tagger to the needs of the user, most taggers provide configurable models which are targeted towards different languages or domains. A user might e.g. switch between a standard English model and a German social media model depending on the task at hand. As there rarely is a perfect match between the available tagging models and the documents to be processed, most taggers allow users to train their own models. FlexTag goes beyond the usual re-training possibilities of establis"
L16-1675,gimenez-marquez-2004-svmtool,0,\N,Missing
L18-1224,S12-1051,0,0.0609539,"w similarity between assertions (why people tend to agree with one assertion and disagree with the other) include: the two assertions are contradictory or contrasting (e.g., ‘It is safe to use vaccines.’ and ‘Vaccines cause autism.’), and underlying socio-cultural and political factors cause people to vote dissimilarly on two (sometimes seemingly unrelated) assertions (e.g., ‘Congress should immediately fund Trump’s wall.’ and ‘All immigrants should have the right to vote in the American elections.’). To further explore the relation between judgment similarity and semantic textual similarity (Agirre et al., 2012), we compute the textual overlap between assertions using the Jaccard index (Lyon et al., 2001) and examine the agreement scores of textually similar assertions. We observe that assertions with high text similarity often have very similar 1410 (a) cosine > 0.0 (b) cosine > 0.1 (c) cosine > 0.2 (d) cosine > 0.3 (e) cosine > 0.4 Figure 4: Similarity of participants visualized in an undirected graph for the issue Black Lives Matter. In the sub figures, we draw edges between two persons if their voting similarity is above a certain threshold. agreement scores. An example for this case are the foll"
L18-1224,W16-2802,0,0.023675,"proaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McDonald, 2008; Ganesan et al., 2010; Gerani et al., 2014) and on the clustering of argumentative elements (Trabelsi and Zaıane, 2014; ˇ Misra et al., 2015; Boltuˇzi´c and Snajder, 2015; Barker and Gaizauskas, 2016); however, these studies mainly explore the grouping of utterances into positive and negative clusters, or extracting opinion utterances and claims to create a summary. Here, we first establish a comprehensive representation of assertions for a controversial issue, and then propose discovering elements of this representation from free-text utterances in social media. 3. Understanding Controversial Issues Quantifiable and useful insights on a controversial issue can be obtained by having a large number of people vote on a large number of relevant assertions. As the manual creation of assertions"
L18-1224,W15-0514,0,0.0477572,"Missing"
L18-1224,P15-2072,0,0.0212045,"we focus on creating a language resource containing crowdsourced judgments on a large number of user generated assertions. Our second goal of understanding and summarizing public opinion from posts on social media is related to work on detecting sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016; Mohammad et al., 2018) and stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), argumentation mining (Kwon et al., 2007; Walker et al., 2012; Rosenthal and McKeown, 2012; Stab and Gurevych, 2014; Peldszus and Stede, 2016; Habernal and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McDonald, 2008; Ganesan et al., 2010; Gerani et al., 2014) and on the clustering of argumentative element"
L18-1224,L16-1591,0,0.014112,"ce containing crowdsourced judgments on a large number of user generated assertions. Our second goal of understanding and summarizing public opinion from posts on social media is related to work on detecting sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016; Mohammad et al., 2018) and stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), argumentation mining (Kwon et al., 2007; Walker et al., 2012; Rosenthal and McKeown, 2012; Stab and Gurevych, 2014; Peldszus and Stede, 2016; Habernal and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McDonald, 2008; Ganesan et al., 2010; Gerani et al., 2014) and on the clustering of argumentative elements (Trabelsi and Zaıane, 2014; ˇ Misra et"
L18-1224,C10-1039,0,0.0311501,"d Stede, 2016; Habernal and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McDonald, 2008; Ganesan et al., 2010; Gerani et al., 2014) and on the clustering of argumentative elements (Trabelsi and Zaıane, 2014; ˇ Misra et al., 2015; Boltuˇzi´c and Snajder, 2015; Barker and Gaizauskas, 2016); however, these studies mainly explore the grouping of utterances into positive and negative clusters, or extracting opinion utterances and claims to create a summary. Here, we first establish a comprehensive representation of assertions for a controversial issue, and then propose discovering elements of this representation from free-text utterances in social media. 3. Understanding Controversial Issues Quantifiable"
L18-1224,D14-1168,0,0.0133673,"l and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McDonald, 2008; Ganesan et al., 2010; Gerani et al., 2014) and on the clustering of argumentative elements (Trabelsi and Zaıane, 2014; ˇ Misra et al., 2015; Boltuˇzi´c and Snajder, 2015; Barker and Gaizauskas, 2016); however, these studies mainly explore the grouping of utterances into positive and negative clusters, or extracting opinion utterances and claims to create a summary. Here, we first establish a comprehensive representation of assertions for a controversial issue, and then propose discovering elements of this representation from free-text utterances in social media. 3. Understanding Controversial Issues Quantifiable and useful insights on"
L18-1224,W01-0515,0,0.487407,"e other) include: the two assertions are contradictory or contrasting (e.g., ‘It is safe to use vaccines.’ and ‘Vaccines cause autism.’), and underlying socio-cultural and political factors cause people to vote dissimilarly on two (sometimes seemingly unrelated) assertions (e.g., ‘Congress should immediately fund Trump’s wall.’ and ‘All immigrants should have the right to vote in the American elections.’). To further explore the relation between judgment similarity and semantic textual similarity (Agirre et al., 2012), we compute the textual overlap between assertions using the Jaccard index (Lyon et al., 2001) and examine the agreement scores of textually similar assertions. We observe that assertions with high text similarity often have very similar 1410 (a) cosine > 0.0 (b) cosine > 0.1 (c) cosine > 0.2 (d) cosine > 0.3 (e) cosine > 0.4 Figure 4: Similarity of participants visualized in an undirected graph for the issue Black Lives Matter. In the sub figures, we draw edges between two persons if their voting similarity is above a certain threshold. agreement scores. An example for this case are the following assertions: ‘Women should have the same rights than men.’ (ags = 0.84) ‘Women should have"
L18-1224,N15-1046,0,0.0265583,"al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McDonald, 2008; Ganesan et al., 2010; Gerani et al., 2014) and on the clustering of argumentative elements (Trabelsi and Zaıane, 2014; ˇ Misra et al., 2015; Boltuˇzi´c and Snajder, 2015; Barker and Gaizauskas, 2016); however, these studies mainly explore the grouping of utterances into positive and negative clusters, or extracting opinion utterances and claims to create a summary. Here, we first establish a comprehensive representation of assertions for a controversial issue, and then propose discovering elements of this representation from free-text utterances in social media. 3. Understanding Controversial Issues Quantifiable and useful insights on a controversial issue can be obtained by having a large number of people vote on a large number"
L18-1224,S16-1003,1,0.841664,"ned questions such as: whether these applications have an effect on voting behavior (Ladner and Pianzola, 2010), what characteristics their users have (Wall et al., 2009), or how their design affects their outcome (Louwerse and Rosema, 2014). In contrast, here we focus on creating a language resource containing crowdsourced judgments on a large number of user generated assertions. Our second goal of understanding and summarizing public opinion from posts on social media is related to work on detecting sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016; Mohammad et al., 2018) and stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), argumentation mining (Kwon et al., 2007; Walker et al., 2012; Rosenthal and McKeown, 2012; Stab and Gurevych, 2014; Peldszus and Stede, 2016; Habernal and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall u"
L18-1224,S18-1001,1,0.808268,"ience, where researchers have examined questions such as: whether these applications have an effect on voting behavior (Ladner and Pianzola, 2010), what characteristics their users have (Wall et al., 2009), or how their design affects their outcome (Louwerse and Rosema, 2014). In contrast, here we focus on creating a language resource containing crowdsourced judgments on a large number of user generated assertions. Our second goal of understanding and summarizing public opinion from posts on social media is related to work on detecting sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016; Mohammad et al., 2018) and stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), argumentation mining (Kwon et al., 2007; Walker et al., 2012; Rosenthal and McKeown, 2012; Stab and Gurevych, 2014; Peldszus and Stede, 2016; Habernal and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utteran"
L18-1224,D14-1006,0,0.0309952,"all et al., 2009), or how their design affects their outcome (Louwerse and Rosema, 2014). In contrast, here we focus on creating a language resource containing crowdsourced judgments on a large number of user generated assertions. Our second goal of understanding and summarizing public opinion from posts on social media is related to work on detecting sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016; Mohammad et al., 2018) and stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), argumentation mining (Kwon et al., 2007; Walker et al., 2012; Rosenthal and McKeown, 2012; Stab and Gurevych, 2014; Peldszus and Stede, 2016; Habernal and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McD"
L18-1224,P08-1036,0,0.0507931,"McKeown, 2012; Stab and Gurevych, 2014; Peldszus and Stede, 2016; Habernal and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McDonald, 2008; Ganesan et al., 2010; Gerani et al., 2014) and on the clustering of argumentative elements (Trabelsi and Zaıane, 2014; ˇ Misra et al., 2015; Boltuˇzi´c and Snajder, 2015; Barker and Gaizauskas, 2016); however, these studies mainly explore the grouping of utterances into positive and negative clusters, or extracting opinion utterances and claims to create a summary. Here, we first establish a comprehensive representation of assertions for a controversial issue, and then propose discovering elements of this representation from free-text utterances in social media. 3."
L18-1224,W14-1305,0,0.0300253,"sur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McDonald, 2008; Ganesan et al., 2010; Gerani et al., 2014) and on the clustering of argumentative elements (Trabelsi and Zaıane, 2014; ˇ Misra et al., 2015; Boltuˇzi´c and Snajder, 2015; Barker and Gaizauskas, 2016); however, these studies mainly explore the grouping of utterances into positive and negative clusters, or extracting opinion utterances and claims to create a summary. Here, we first establish a comprehensive representation of assertions for a controversial issue, and then propose discovering elements of this representation from free-text utterances in social media. 3. Understanding Controversial Issues Quantifiable and useful insights on a controversial issue can be obtained by having a large number of people v"
L18-1224,P15-1157,0,0.0174511,"g a language resource containing crowdsourced judgments on a large number of user generated assertions. Our second goal of understanding and summarizing public opinion from posts on social media is related to work on detecting sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016; Mohammad et al., 2018) and stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), argumentation mining (Kwon et al., 2007; Walker et al., 2012; Rosenthal and McKeown, 2012; Stab and Gurevych, 2014; Peldszus and Stede, 2016; Habernal and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang et al., 2006; Titov and McDonald, 2008; Titov and McDonald, 2008; Ganesan et al., 2010; Gerani et al., 2014) and on the clustering of argumentative elements (Trabelsi and Zaı"
L18-1224,walker-etal-2012-corpus,0,0.0366734,"a, 2010), what characteristics their users have (Wall et al., 2009), or how their design affects their outcome (Louwerse and Rosema, 2014). In contrast, here we focus on creating a language resource containing crowdsourced judgments on a large number of user generated assertions. Our second goal of understanding and summarizing public opinion from posts on social media is related to work on detecting sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016; Mohammad et al., 2018) and stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), argumentation mining (Kwon et al., 2007; Walker et al., 2012; Rosenthal and McKeown, 2012; Stab and Gurevych, 2014; Peldszus and Stede, 2016; Habernal and Gurevych, 2016), and framing (Entman, 1993; Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Johnson and Goldwasser, 2016). These approaches focus on identifying sentiment, stance, claims, premises, reasons, arguments, sentiment, etc. from individual utterances. In contrast, here we suggest quantifying information from a large number of social media utterances in order to gain the overall understanding of a complex issue. There exists work on opinion summarization (Hu and Liu, 2004; Zhuang"
L18-1365,P13-4021,1,0.893912,"Missing"
L18-1365,Q13-1032,0,0.514214,"ay to make life even better. Popular sites like @CAPS1, @CAPS2, @CAPS3, and so on make a good way to keep in touch with friends from your past, or even make new ones. But the most that I think thats the best in my opinion is going to school online. Once your done with colloge and you are a nuse, for an example you can get a higher degree like a registered nurse then being a @ORGANIZATION2. I think computers has a positive effect on people. Table 1: Examples for free-text tasks asking for answers of very different complexity. SAS2 for short-answer scoring, the Powergrading shortanswer dataset (Basu et al., 2013), the dataset by Mohler and Mihalcea (2009), the Student Response Analysis (SRA) dataset (Dzikovska et al., 2013), and the German CREG corpus (Meurers et al., 2011) to facilitate this task. Furthermore, we also support generic educational datasets, consisting of at least a set of learner answers, each with an ID and a score. We provide generic readers for such datasets in line-based formats (such as CSV) which novice users may use to integrate their own data without having to write their own reader. These readers support different properties of the data, such as categorical and numerical scori"
L18-1365,P14-5011,1,0.800561,"ir value can either be based on content alone (content-scoring) or on content and form (typically in holistic scores for essay scoring). Typically, the prompt in response to which answers are given is available. Predefined reference answers, i.e. sample solutions, are available in some cases. These additional materials can also be leveraged in automatic scoring; for example, to compare whether an answer to be scored is similar to the reference answer or to identify domain-specific vocabulary useful for spell-checking. In order to model this complex setup, we build E SCRITO on top of DKPro TC (Daxenberger et al., 2014), an UIMA-based open-source framework that provides easy access to various algorithms for supervised text classification and extensive parameter documentation enabling reproducible research. E SCRITO extends DKPro TC with respect to the specific needs of educational scoring applications: it offers easy access to existing educational datasets, various preprocessing options, state-of-the-art scoring features, evaluation and visualizations for common scoring scenarios, as well as options to integrate new data and to customize existing or to add new preprocessing components and feature extractors."
L18-1365,W14-5201,0,0.0399795,"Missing"
L18-1365,W16-0535,1,0.830587,"onceptions in the student answers (Basu et al., 2013). Second, a teacher might want to label some, but not all their data, train a classifier and then re-label the complete dataset (or only the so-far unlabeled data). In this scenario, we provide methods to select the items to be labeled in an informed way, for example by selecting items so that they cover as much of the feature space as possible. As an alternative, we provide methods from active learning, where items are dynamically selected in a way that a machine learner profits most from them, such that human annotation effort is reduced (Horbach and Palmer, 2016). 2.5. Evaluation and Visualization We report frequently used evaluation metrics depending on the type of labels used in a dataset: accuracy, quadratically and linearly weighted kappa, precision, recall, F-measure as well as correlation scores such as Pearson(Pearson, 1895) and Spearman(Spearman, 1904). To facilitate error analysis, incorrectly classified items (i.e. false positives and false negatives for a class) are written to separate files and can easily be inspected. Besides plain text result files, E SCRITO also writes tables and figures in LaTeX formatting or images. Additionally, we p"
L18-1365,W17-5908,1,0.560209,"owing best-practices in the field. Research scientists will find a low-level API, which allows them to access, customize, and extend all relevant aspects of automatic scoring including preprocessing, feature extraction, and machine learning setup. We ensure reproducibility of results through detailed automated documentation of experimental setups. We also have designed E SCRITO to be as language-independent as possible. It has been successfully applied to data in various languages. All parts of E SCRITO have already been used in research projects concerning essay scoring (Zesch et al., 2015b; Horbach et al., 2017c), spellchecking on learner data (Horbach et al., 2017a), clustering (Zesch et al., 2015a), and neural short-answer scoring (Riordan et al., 2017). This shows the wide applicability of the framework and that state-of-the-art approaches can be easily modeled within the framework. Related Work To the best of our knowledge, there are no other publicly available general-purpose scoring frameworks addressing either programmers or practitioners. Proprietary systems such as e-rater (Attali and Burstein, 2004) can only be used commercially and as a sort of black-box. While a number of scoring impleme"
L18-1365,W17-5040,1,0.916279,"owing best-practices in the field. Research scientists will find a low-level API, which allows them to access, customize, and extend all relevant aspects of automatic scoring including preprocessing, feature extraction, and machine learning setup. We ensure reproducibility of results through detailed automated documentation of experimental setups. We also have designed E SCRITO to be as language-independent as possible. It has been successfully applied to data in various languages. All parts of E SCRITO have already been used in research projects concerning essay scoring (Zesch et al., 2015b; Horbach et al., 2017c), spellchecking on learner data (Horbach et al., 2017a), clustering (Zesch et al., 2015a), and neural short-answer scoring (Riordan et al., 2017). This shows the wide applicability of the framework and that state-of-the-art approaches can be easily modeled within the framework. Related Work To the best of our knowledge, there are no other publicly available general-purpose scoring frameworks addressing either programmers or practitioners. Proprietary systems such as e-rater (Attali and Burstein, 2004) can only be used commercially and as a sort of black-box. While a number of scoring impleme"
L18-1365,L18-1403,1,0.837774,"et answers. These differences can, for example, be important when handling both similarity scores as well as distances. New features can easily be implemented and integrated by using interfaces for features either based on the learner answer text alone or its comparison with a prompt text or reference answer. Integration of Deep Learning Deep learning methods became widely used in various NLP areas including educational free-text scoring (Taghipour and Ng, 2016; Riordan et al., 2017) and are often a very powerful alternative to traditional shallow learning methods. DKPro TC has been extended (Horsmann and Zesch, 2018) to also provide interfaces to widely used deep learning frameworks including Keras (Chollet and others, 2015), DeepLearning4J 5 , and Dynet (Neubig et al., 2017), while ensuring reproducibility and easy preprocessing through DKPro TC. We integrate this extension to make sure deep learning methods can be used in E SCRITO. 2.4. We specify commonly used experimental setups that allow for both supervised and unsupervised machine learning scenarios according to the needs of the two user groups. From an NLP researcher’s perspective, the supervised case with labeled train and test data is certainly"
L18-1365,W11-2401,0,0.0245427,"ones. But the most that I think thats the best in my opinion is going to school online. Once your done with colloge and you are a nuse, for an example you can get a higher degree like a registered nurse then being a @ORGANIZATION2. I think computers has a positive effect on people. Table 1: Examples for free-text tasks asking for answers of very different complexity. SAS2 for short-answer scoring, the Powergrading shortanswer dataset (Basu et al., 2013), the dataset by Mohler and Mihalcea (2009), the Student Response Analysis (SRA) dataset (Dzikovska et al., 2013), and the German CREG corpus (Meurers et al., 2011) to facilitate this task. Furthermore, we also support generic educational datasets, consisting of at least a set of learner answers, each with an ID and a score. We provide generic readers for such datasets in line-based formats (such as CSV) which novice users may use to integrate their own data without having to write their own reader. These readers support different properties of the data, such as categorical and numerical scoring labels. They also support datasets where a prompt text is available and both datasets without reference an2 https://www.kaggle.com/c/asap-sas swers as well as da"
L18-1365,E09-1065,0,0.252608,"r sites like @CAPS1, @CAPS2, @CAPS3, and so on make a good way to keep in touch with friends from your past, or even make new ones. But the most that I think thats the best in my opinion is going to school online. Once your done with colloge and you are a nuse, for an example you can get a higher degree like a registered nurse then being a @ORGANIZATION2. I think computers has a positive effect on people. Table 1: Examples for free-text tasks asking for answers of very different complexity. SAS2 for short-answer scoring, the Powergrading shortanswer dataset (Basu et al., 2013), the dataset by Mohler and Mihalcea (2009), the Student Response Analysis (SRA) dataset (Dzikovska et al., 2013), and the German CREG corpus (Meurers et al., 2011) to facilitate this task. Furthermore, we also support generic educational datasets, consisting of at least a set of learner answers, each with an ID and a score. We provide generic readers for such datasets in line-based formats (such as CSV) which novice users may use to integrate their own data without having to write their own reader. These readers support different properties of the data, such as categorical and numerical scoring labels. They also support datasets where"
L18-1365,W13-1705,0,0.0260609,"Missing"
L18-1365,W15-1905,0,0.253694,"black-box. While a number of scoring implementations are publicly available, such as Neural Essay Assessor (Taghipour and ¨ Ng, 2016), an essay scoring system for Swedish (Ostling et al., 2013), or clustering-based scoring Zesch et al. (2015a), these implementations are typically centered around a specific dataset and method, and not straight-forward to extend or apply to new data. Equally, there are approaches for supporting teachers with free-text answers in MOOCs. An example is a plugin for the learning management system Moodle which sorts answers by their similarity to a reference answer (Pado and Kiefer, 2015). 2. Educational Scoring Toolkit Educational free-text scoring is often tackled as a classical supervised learning task with the goal to assign a label to some piece of text written by a learner in response 2310 Learner Writings • • • • DATA READER ASAP-AES ASAP-SAS Response Label Reference response Prompt material Powergrading PREPROCESSING POS Tagging • Stanford, • TreeTagger • … FEATURE EXTRACTION Basic • Length • Ngrams • … SemEval SRA CREG Parsing • Mate, • Stanford • … NER • OpenNLP, • Stanford • … Language • Correctness • Variability • … Informed Selection of Training data Semantics • P"
L18-1365,W17-5017,1,0.931047,"aspects of automatic scoring including preprocessing, feature extraction, and machine learning setup. We ensure reproducibility of results through detailed automated documentation of experimental setups. We also have designed E SCRITO to be as language-independent as possible. It has been successfully applied to data in various languages. All parts of E SCRITO have already been used in research projects concerning essay scoring (Zesch et al., 2015b; Horbach et al., 2017c), spellchecking on learner data (Horbach et al., 2017a), clustering (Zesch et al., 2015a), and neural short-answer scoring (Riordan et al., 2017). This shows the wide applicability of the framework and that state-of-the-art approaches can be easily modeled within the framework. Related Work To the best of our knowledge, there are no other publicly available general-purpose scoring frameworks addressing either programmers or practitioners. Proprietary systems such as e-rater (Attali and Burstein, 2004) can only be used commercially and as a sort of black-box. While a number of scoring implementations are publicly available, such as Neural Essay Assessor (Taghipour and ¨ Ng, 2016), an essay scoring system for Swedish (Ostling et al., 201"
L18-1365,D16-1193,0,0.0134276,"these reference answers by either taking the maximum, minimum, average, or all feature values produced when comparing a learner answer to the individual target answers. These differences can, for example, be important when handling both similarity scores as well as distances. New features can easily be implemented and integrated by using interfaces for features either based on the learner answer text alone or its comparison with a prompt text or reference answer. Integration of Deep Learning Deep learning methods became widely used in various NLP areas including educational free-text scoring (Taghipour and Ng, 2016; Riordan et al., 2017) and are often a very powerful alternative to traditional shallow learning methods. DKPro TC has been extended (Horsmann and Zesch, 2018) to also provide interfaces to widely used deep learning frameworks including Keras (Chollet and others, 2015), DeepLearning4J 5 , and Dynet (Neubig et al., 2017), while ensuring reproducibility and easy preprocessing through DKPro TC. We integrate this extension to make sure deep learning methods can be used in E SCRITO. 2.4. We specify commonly used experimental setups that allow for both supervised and unsupervised machine learning s"
L18-1365,W15-0615,1,0.769475,"n their own data following best-practices in the field. Research scientists will find a low-level API, which allows them to access, customize, and extend all relevant aspects of automatic scoring including preprocessing, feature extraction, and machine learning setup. We ensure reproducibility of results through detailed automated documentation of experimental setups. We also have designed E SCRITO to be as language-independent as possible. It has been successfully applied to data in various languages. All parts of E SCRITO have already been used in research projects concerning essay scoring (Zesch et al., 2015b; Horbach et al., 2017c), spellchecking on learner data (Horbach et al., 2017a), clustering (Zesch et al., 2015a), and neural short-answer scoring (Riordan et al., 2017). This shows the wide applicability of the framework and that state-of-the-art approaches can be easily modeled within the framework. Related Work To the best of our knowledge, there are no other publicly available general-purpose scoring frameworks addressing either programmers or practitioners. Proprietary systems such as e-rater (Attali and Burstein, 2004) can only be used commercially and as a sort of black-box. While a nu"
L18-1365,W15-0626,1,0.816455,"n their own data following best-practices in the field. Research scientists will find a low-level API, which allows them to access, customize, and extend all relevant aspects of automatic scoring including preprocessing, feature extraction, and machine learning setup. We ensure reproducibility of results through detailed automated documentation of experimental setups. We also have designed E SCRITO to be as language-independent as possible. It has been successfully applied to data in various languages. All parts of E SCRITO have already been used in research projects concerning essay scoring (Zesch et al., 2015b; Horbach et al., 2017c), spellchecking on learner data (Horbach et al., 2017a), clustering (Zesch et al., 2015a), and neural short-answer scoring (Riordan et al., 2017). This shows the wide applicability of the framework and that state-of-the-art approaches can be easily modeled within the framework. Related Work To the best of our knowledge, there are no other publicly available general-purpose scoring frameworks addressing either programmers or practitioners. Proprietary systems such as e-rater (Attali and Burstein, 2004) can only be used commercially and as a sort of black-box. While a nu"
L18-1403,W13-3520,0,0.0561101,"Missing"
L18-1403,A00-1031,0,0.0649803,"Missing"
L18-1403,J92-4003,0,0.489114,"Missing"
L18-1403,N16-1031,0,0.0281728,"Missing"
L18-1403,W02-1001,0,0.639077,"Missing"
L18-1403,P14-5011,1,0.713524,"architecture, but also of a potentially large number of processing steps to prepare the data. Furthermore, countless network parameters exist, which can greatly affect a network’s performance. Reproduction attempts, thus, lead to a high amount of time spent with constructing comparable processing setups. Even if the deep learning code is released, code that applies all preprocessing steps is often missing. Additional effort is often necessary to install and configure required third-party tools. A potential solution to these reproducibility challenges is DKPro Text Classification (DKPro TC)1 (Daxenberger et al., 2014). DKPro TC ensures that the same preprocessing is automatically applied to any (new) dataset and provides convenience services such as an automatic installation of third-party tools. DKPro TC experiments are end-to-end shareable, enabling a quick and easy execution of experiments by other researchers. However, until now, DKPro TC only supports shallow learning frameworks. In this work, we present a deep learning extension to DKPro TC called DeepTC. In addition to improved reproducibility, DeepTC also eases architecture analysis by moving boilerplate code for pruning word embeddings and vectori"
L18-1403,W14-5201,0,0.116594,"Missing"
L18-1403,N16-1082,0,0.0289904,"hainer (Tokui et al., 2015) to name just a few. Software such as Keras, Lasagne (Dieleman et al., 2015) or Fuel&Blocks (van Merri¨enboer et al., 2015) provide a simplified, building-block like interface to an underlying, low-level deep learning framework such as Theano. Data loading capabilities are included to some extent for instance for the well-known MNIST (Lecun et al., 1998) dataset with hand-written digits for image processing tasks. Furthermore, there are approaches to analyze what a neural network actually learns when applied to image and text processing tasks (Yosinski et al., 2015; Li et al., 2016). The deep learning software, thus, provides means to build prototypes quickly, but provides no means to ensure replicability by a third-party researcher. This means that all processing components must be manually provided and configured by the researcher who wants to run a certain prototype. Hence, the DeepTC extension fills a gap in the software landscape, which will improve reproducibility of deep learning experiments. 7. Conclusion We presented DeepTC, a deep learning extension of the NLP experiment framework DKPro TC. We discussed the current state of DKPro TC, which is limited to shallow"
L18-1403,J93-2004,0,0.06372,"searchers can always implement their own UIMA processing components. Applicability DKPro TC supports all common machine learning setups related to text classification tasks, i.e. single outcome (e.g. sentiment analysis), multi outcome or sequence classification (e.g. part-of-speech tagging), and regression (e.g. assessment of text reading difficulty). 2.2. Shallow Architecture Figure 1 shows a conceptual overview of DKPro TC. Reader The corpus data is read into DKPro TC by a reader component. Via DKPro Core dozens of common NLP formats are supported, for instance CoNLL, TEI, or Penn Treebank (Marcus et al., 1993). Preprocessing In this step, an optional pre-processing can be applied, which might entail tasks such as tokenization or part-of-speech tagging. Feature Extraction The feature extractors are applied to the data with access to information created during the preprocessing step. The extracted information is temporarily stored in an intermediate data format. Interface to Shallow Learning Frameworks The feature information is transformed into the data format of the selected machine learning framework. Evaluation If test data is provided, the trained model is applied to this dataset (after running"
N07-2052,W04-2607,0,0.100419,"rman datasets. We show that the performance of measures strongly depends on the underlying knowledge 2 Datasets Several German datasets for evaluation of SS or SR have been created so far (see Table 1). Gurevych (2005) conducted experiments with a German translation of an English dataset (Rubenstein and Goodenough, 1965), but argued that the dataset (Gur65) is too small (it contains only 65 noun pairs), and does not model SR. Thus, she created a German dataset containing 350 word pairs (Gur350) containing nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004). However, the dataset is biased towards strong classical relations, as word pairs were manually selected. Thus, Zesch and Gurevych (2006) semi-automatically created word pairs from domain-specific corpora. The resulting ZG222 dataset contains 222 word pairs that are connected by all kinds of lexical semantic relations. Hence, it is particularly suited for analyzing the capability of a measure to estimate SR. 205 Proceedings of NAACL HLT 2007, Companion Volume, pages 205–208, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics DATASET Gur65 Gur350 ZG222 Y EAR 2005 2006"
N07-2052,W06-1104,1,0.685605,"for evaluation of SS or SR have been created so far (see Table 1). Gurevych (2005) conducted experiments with a German translation of an English dataset (Rubenstein and Goodenough, 1965), but argued that the dataset (Gur65) is too small (it contains only 65 noun pairs), and does not model SR. Thus, she created a German dataset containing 350 word pairs (Gur350) containing nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004). However, the dataset is biased towards strong classical relations, as word pairs were manually selected. Thus, Zesch and Gurevych (2006) semi-automatically created word pairs from domain-specific corpora. The resulting ZG222 dataset contains 222 word pairs that are connected by all kinds of lexical semantic relations. Hence, it is particularly suited for analyzing the capability of a measure to estimate SR. 205 Proceedings of NAACL HLT 2007, Companion Volume, pages 205–208, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics DATASET Gur65 Gur350 ZG222 Y EAR 2005 2006 2006 L ANGUAGE German German German # PAIRS 65 350 222 POS N N, V, A N, V, A T YPE SS SR SR S CORES discrete {0,1,2,3,4} discrete {0,1,2,3"
N07-2052,W07-0201,1,0.678532,"based measures, we find that the knowledge source has a major influence on performance. When evaluated on Gur65, that contains pairs connected by SS, GermaNet based measures perform near the upper bound and outperform Wikipedia based measures by a wide margin. On Gur350 containing a mix of SS and SR pairs, most measures perform comparably. Finally, on ZG222, that contains pairs connected by SR, the best Wikipedia based measure outperforms all GermaNet based measures. The impressive performance of P L on the SR datasets cannot be explained with the structural properties of the category graph (Zesch and Gurevych, 2007). Semantically related terms, that would not be closely related in a taxonomic wordnet structure, are very likely to be categorized under the same Wikipedia category, resulting in short path lengths leading to high SR. These findings are contrary to that of (Strube and Ponzetto, 2006), where LC outperformed path length. They limited the search depth using a manually defined threshold, and did not compute SR between all candidate article pairs. Our results show that judgments on the performance of a measure must always be made with respect to the task at hand: computing SS or SR. Depending on t"
N07-2052,I05-1067,1,\N,Missing
N07-2052,J06-1003,0,\N,Missing
N19-1135,W17-3007,0,0.0605676,"Missing"
N19-1135,L18-1550,0,0.025332,"lve the task at hand. Laymen are readily available, for instance via crowdsourcing but also as student assistants who can be more cheaply employed than legal experts for annotating data. Table 2: Averaged 10-fold CV results for each decision on 1,100 Tweets, using an LSTM proach infeasible. Instead, we investigate how well each of the binary decisions shown in Figure 2 can be learned independently, which has a less skewed distribution. We use a bi-directional LSTM (Hochreiter and Schmidhuber, 1997) for classification.30 We use the 300-dimensional German pre-trained word embeddings provided by Grave et al. (2018), which are trained on the German common crawl. Table 2 shows averaged 10-fold CV results for each decision point. We observe that the accuracy is close to the underlying distribution of the two classes. The classification of the defamatory object has a mediocre performance. In particular, an insufficient coverage of group names and names of individuals in the dataset seem to be the main cause as the no classes usually perform considerably better than the yes classes. The classification of the decisions under defamatory conduct follows a similar trend. The few positive instances: factual claim"
N19-1135,N18-1036,0,0.0310892,"paper. Furthermore, we investigate automated detection of postings protected by the freedom of expression in order to assist social media moderators. We focus in particular on the process of inexpensive and scalable data annotation, as access to legal expertise is a major bottleneck for providing a sufficient amount of data for classifier training. 2 Related Work An automated detection of Internet discourse in which individuals or groups are verbally attacked has been intensively investigated under a variety of names, for instance: abusive language (Waseem et al., 2017), ad hominem arguments (Habernal et al., 2018), aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Macbeth et al., 2013), hate speech (Warner and Hirschberg, 2012; Ross et al., 2016; Del Vigna et al., 2017), offensive language usage (Razavi et al., 2010), profanity (Schmidt and Wiegand, 2017), threats (Oostdijk and van Halteren, 2013) or socially unacceptable discourse (Fišer et al., 2017). The majority of the work focuses on the English language with few exceptions for instance for German (Ross et al., 2016), Dutch (Oostdijk and van Halteren, 2013), Italian (Del Vigna et al., 2017) or Slovene (Fišer et al., 2017). The datas"
N19-1135,W18-4401,0,0.0306662,"Missing"
N19-1135,W17-1101,0,0.0147907,"tise is a major bottleneck for providing a sufficient amount of data for classifier training. 2 Related Work An automated detection of Internet discourse in which individuals or groups are verbally attacked has been intensively investigated under a variety of names, for instance: abusive language (Waseem et al., 2017), ad hominem arguments (Habernal et al., 2018), aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Macbeth et al., 2013), hate speech (Warner and Hirschberg, 2012; Ross et al., 2016; Del Vigna et al., 2017), offensive language usage (Razavi et al., 2010), profanity (Schmidt and Wiegand, 2017), threats (Oostdijk and van Halteren, 2013) or socially unacceptable discourse (Fišer et al., 2017). The majority of the work focuses on the English language with few exceptions for instance for German (Ross et al., 2016), Dutch (Oostdijk and van Halteren, 2013), Italian (Del Vigna et al., 2017) or Slovene (Fišer et al., 2017). The dataset annotated in Fišer et al. (2017) is the only one that includes a coarse-grained binary annotation category indicating if an utterance violates Slovene law. To the best of our knowledge, automatic determination as to whether the (textual) content of a posting"
N19-1135,W12-2103,0,0.0238902,"social media moderators. We focus in particular on the process of inexpensive and scalable data annotation, as access to legal expertise is a major bottleneck for providing a sufficient amount of data for classifier training. 2 Related Work An automated detection of Internet discourse in which individuals or groups are verbally attacked has been intensively investigated under a variety of names, for instance: abusive language (Waseem et al., 2017), ad hominem arguments (Habernal et al., 2018), aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Macbeth et al., 2013), hate speech (Warner and Hirschberg, 2012; Ross et al., 2016; Del Vigna et al., 2017), offensive language usage (Razavi et al., 2010), profanity (Schmidt and Wiegand, 2017), threats (Oostdijk and van Halteren, 2013) or socially unacceptable discourse (Fišer et al., 2017). The majority of the work focuses on the English language with few exceptions for instance for German (Ross et al., 2016), Dutch (Oostdijk and van Halteren, 2013), Italian (Del Vigna et al., 2017) or Slovene (Fišer et al., 2017). The dataset annotated in Fišer et al. (2017) is the only one that includes a coarse-grained binary annotation category indicating if an utt"
N19-1135,W17-3012,0,0.0123247,"as an offense against public peace in this paper. Furthermore, we investigate automated detection of postings protected by the freedom of expression in order to assist social media moderators. We focus in particular on the process of inexpensive and scalable data annotation, as access to legal expertise is a major bottleneck for providing a sufficient amount of data for classifier training. 2 Related Work An automated detection of Internet discourse in which individuals or groups are verbally attacked has been intensively investigated under a variety of names, for instance: abusive language (Waseem et al., 2017), ad hominem arguments (Habernal et al., 2018), aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Macbeth et al., 2013), hate speech (Warner and Hirschberg, 2012; Ross et al., 2016; Del Vigna et al., 2017), offensive language usage (Razavi et al., 2010), profanity (Schmidt and Wiegand, 2017), threats (Oostdijk and van Halteren, 2013) or socially unacceptable discourse (Fišer et al., 2017). The majority of the work focuses on the English language with few exceptions for instance for German (Ross et al., 2016), Dutch (Oostdijk and van Halteren, 2013), Italian (Del Vigna et al., 20"
N19-1135,N12-1084,0,0.0307114,"cted by the freedom of expression in order to assist social media moderators. We focus in particular on the process of inexpensive and scalable data annotation, as access to legal expertise is a major bottleneck for providing a sufficient amount of data for classifier training. 2 Related Work An automated detection of Internet discourse in which individuals or groups are verbally attacked has been intensively investigated under a variety of names, for instance: abusive language (Waseem et al., 2017), ad hominem arguments (Habernal et al., 2018), aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Macbeth et al., 2013), hate speech (Warner and Hirschberg, 2012; Ross et al., 2016; Del Vigna et al., 2017), offensive language usage (Razavi et al., 2010), profanity (Schmidt and Wiegand, 2017), threats (Oostdijk and van Halteren, 2013) or socially unacceptable discourse (Fišer et al., 2017). The majority of the work focuses on the English language with few exceptions for instance for German (Ross et al., 2016), Dutch (Oostdijk and van Halteren, 2013), Italian (Del Vigna et al., 2017) or Slovene (Fišer et al., 2017). The dataset annotated in Fišer et al. (2017) is the only one that includes"
N19-1135,S19-2010,0,0.0335828,"ch as slut, fat-ass or scumbag when directed at a private individual, not at a person of public interest. Furthermore, punishable statements addressing a specific group use more frequently offending comparisons or descriptions but no typical single or two-word insults. However, it is important to recall that the dataset has a focus on political debates. Accordingly, most statements tackle a topic of public interest, and are thus considered usually not punishable granting a high degree of protection under the freedom of expression. This analysis also shows that shared tasks such as OffensEval (Zampieri et al., 2019) tackle essentially only one step in the legal assessment, namely whether a statement is disparaging. Thus, they fall short of valuing the freedom of expression, which is in particular a problem for public discourse such as political debates, where opinions are often accompanied by ‘bad’ language. 4.2 Automated Detection For an automated detection, it would seem straight forward to distinguish between punishable and not punishable postings. This approach requires an extremely large amount of data for each of the two classes, which we do not have. The data distribution is skewed with the punish"
P07-1130,E06-1002,0,0.0209052,".3 A large body of research exists on using wordnets in NLP applications and in particular in IR (Moldovan and Mihalcea, 2000). The knowledge in wordnets has been typically utilized by expanding queries with related terms (Vorhees, 1994; Smeaton et al., 1994), concept indexing (Gonzalo et al., 1998), or similarity measures as ranking functions (Smeaton et al., 1994; M¨uller and Gurevych, 2006). Recently, Wikipedia has been discovered as a promising lexical semantic resource and successfully used in such different NLP tasks as question answering (Ahn et al., 2004), named entity disambiguation (Bunescu and Pasca, 2006), and information retrieval (Katz et al., 2005). Further research (Zesch et al., 2007b) indicates that German wordnet and Wikipedia show different performance depending on the task at hand. Departing from this, we first compare two semantic relatedness (SR) measures based on the information either in the German wordnet (Lin, 1998) called LIN, or in Wikipedia (Gabrilovich and Markovitch, 2007) called Explicit Semantic Analysis, or ESA. We evaluate their performance intrinsically on the tasks of (T-1) computing semantic relatedness, and (T-2) solving Reader’s Digest Word Power (RDWP) questions a"
P07-1130,W98-0705,0,0.0279276,"ubstantiated below. 2 System Architecture Integrating lexical semantic knowledge in ECG requires the existence of knowledge bases encoding domain and lexical knowledge. In this paper, we investigate the utility of two knowledge bases: (i) a German wordnet, GermaNet (Kunze, 2004), and (ii) the German portion of Wikipedia.3 A large body of research exists on using wordnets in NLP applications and in particular in IR (Moldovan and Mihalcea, 2000). The knowledge in wordnets has been typically utilized by expanding queries with related terms (Vorhees, 1994; Smeaton et al., 1994), concept indexing (Gonzalo et al., 1998), or similarity measures as ranking functions (Smeaton et al., 1994; M¨uller and Gurevych, 2006). Recently, Wikipedia has been discovered as a promising lexical semantic resource and successfully used in such different NLP tasks as question answering (Ahn et al., 2004), named entity disambiguation (Bunescu and Pasca, 2006), and information retrieval (Katz et al., 2005). Further research (Zesch et al., 2007b) indicates that German wordnet and Wikipedia show different performance depending on the task at hand. Departing from this, we first compare two semantic relatedness (SR) measures based on"
P07-1130,I05-7005,1,0.824845,"Net displays some structural differences and content oriented modifications. Its designers relied mainly on linguistic evidence, such as corpus frequency, rather than psycholinguistic motivations. Also, GermaNet employs artificial, i.e. non-lexicalized concepts, and adjectives are structured hierarchically as opposed to WordNet. Currently, GermaNet includes about 40000 synsets with more than 60000 word senses modelling nouns, verbs and adjectives. We use the semantic relatedness measure by Lin (1998) (referred to as LIN), as it consistently is among the best performing wordnet based measures (Gurevych and Niederlich, 2005; Budanitsky and Hirst, 2006). Lin defined semantic similarity using a formula derived from information theory. This measure is sometimes called a universal semantic similarity measure as it is supposed to be application, domain, and resource independent. Lin is computed as: 2 × log p(LCS(c1 , c2 )) simc1 ,c2 = log p(c1 ) + log p(c2 ) where c1 and c2 are concepts (word senses) corresponding to w1 and w2 , log p(c) is the information content, and LCS(c1 , c2 ) is the lowest common subsumer of the two concepts. The probability p is computed as the relative frequency of words (representing that c"
P07-1130,I05-1067,1,0.259128,"f their corresponding concept vectors. If we want to measure the semantic relatedness of texts instead of terms, we can also use ESA concept vectors. A text is represented as the average concept vector of its terms’ concept vectors. Then, the relatedness of two texts is computed as the cosine of their average concept vectors. As ESA uses all textual information in Wikipedia, the measure shows excellent coverage. Therefore, we select it as the second measure for integration into our IR system. 3.2 Datasets Semantic relatedness datasets for German employed in our study are presented in Table 1. Gurevych (2005) conducted experiments with two datasets: i) a German translation of the English dataset by Rubenstein and Goodenough (1965) (Gur65), and ii) a larger dataset containing 350 word pairs (Gur350). Zesch and Gurevych (2006) created a third dataset from domain-specific corpora using a semi-automatic process (ZG222). Gur65 is rather small and contains only noun-noun pairs connected by either synonymy or hypernymy. Gur350 contains nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004). However, word pairs for this dataset are biased towards s"
P07-1130,W04-2607,0,0.0104968,"atasets for German employed in our study are presented in Table 1. Gurevych (2005) conducted experiments with two datasets: i) a German translation of the English dataset by Rubenstein and Goodenough (1965) (Gur65), and ii) a larger dataset containing 350 word pairs (Gur350). Zesch and Gurevych (2006) created a third dataset from domain-specific corpora using a semi-automatic process (ZG222). Gur65 is rather small and contains only noun-noun pairs connected by either synonymy or hypernymy. Gur350 contains nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004). However, word pairs for this dataset are biased towards strong classical relations, as they were manually selected from a corpus. DATASET Gur65 Gur350 ZG222 Y EAR 2005 2006 2006 L ANGUAGE German German German # PAIRS 65 350 222 POS N N, V, A N, V, A S CORES discrete {0,1,2,3,4} discrete {0,1,2,3,4} discrete {0,1,2,3,4} # S UBJECTS 24 8 21 C ORRELATION r I NTER I NTRA .810 .690 .490 .647 Table 1: Comparison of datasets used for evaluating semantic relatedness in German. ZG222 does not have this bias. Following the work by Jarmasz and Szpakowicz (2003) and Turney (2006), we created a second da"
P07-1130,J06-3003,0,0.0142156,"al relations (Morris and Hirst, 2004). However, word pairs for this dataset are biased towards strong classical relations, as they were manually selected from a corpus. DATASET Gur65 Gur350 ZG222 Y EAR 2005 2006 2006 L ANGUAGE German German German # PAIRS 65 350 222 POS N N, V, A N, V, A S CORES discrete {0,1,2,3,4} discrete {0,1,2,3,4} discrete {0,1,2,3,4} # S UBJECTS 24 8 21 C ORRELATION r I NTER I NTRA .810 .690 .490 .647 Table 1: Comparison of datasets used for evaluating semantic relatedness in German. ZG222 does not have this bias. Following the work by Jarmasz and Szpakowicz (2003) and Turney (2006), we created a second dataset containing multiple choice questions. We collected 1072 multiple-choice word analogy questions from the German Reader’s Digest Word Power Game (RDWP) from January 2001 to December 2005 (Wallace and Wallace, 2005). We discarded 44 questions that had more than one correct answer, and 20 questions that used a phrase instead of a single term as query. The resulting 1008 questions form our evaluation dataset. An example question is given below: Muffin (muffin) a) Kleingeb¨ack (small cake) b) Spenglerwerkzeug (plumbing tool) c) Miesepeter (killjoy) d) Wildschaf (moufflo"
P07-1130,W06-1104,1,0.512394,"erms’ concept vectors. Then, the relatedness of two texts is computed as the cosine of their average concept vectors. As ESA uses all textual information in Wikipedia, the measure shows excellent coverage. Therefore, we select it as the second measure for integration into our IR system. 3.2 Datasets Semantic relatedness datasets for German employed in our study are presented in Table 1. Gurevych (2005) conducted experiments with two datasets: i) a German translation of the English dataset by Rubenstein and Goodenough (1965) (Gur65), and ii) a larger dataset containing 350 word pairs (Gur350). Zesch and Gurevych (2006) created a third dataset from domain-specific corpora using a semi-automatic process (ZG222). Gur65 is rather small and contains only noun-noun pairs connected by either synonymy or hypernymy. Gur350 contains nouns, verbs and adjectives that are connected by classical and non-classical relations (Morris and Hirst, 2004). However, word pairs for this dataset are biased towards strong classical relations, as they were manually selected from a corpus. DATASET Gur65 Gur350 ZG222 Y EAR 2005 2006 2006 L ANGUAGE German German German # PAIRS 65 350 222 POS N N, V, A N, V, A S CORES discrete {0,1,2,3,4"
P07-1130,N07-2052,1,0.675807,"Missing"
P07-1130,J06-1003,0,\N,Missing
P11-4013,A00-2004,0,0.0429566,"in page. Wikulu therefore supports users by analyzing long pages through employing text segmentation algorithms which detect topically coherent segments of text. It then suggests segment boundaries which the user may or may not accept for inserting a subheading which makes pages easier to read and better to navigate. As shown in Figure 3, users are also encouraged to set a title for each segment.8 When accepting one or more of these suggested boundaries, Wikulu stores them persistently in the wiki. Wikulu currently integrates text segmentation methods such as TextTiling (Hearst, 1997) or C99 (Choi, 2000). Summarizing Pages Similarly to segmenting pages, Wikulu makes long wiki pages more accessible by generating an extractive summary. While generative summaries generate a summary in own words, extractive summaries analyze the original wiki text sentence-by-sentence, rank each sentence, and return a list of the most important ones (see Figure 4). Wikulu integrates extractive text summarization methods such as LexRank (Erkan and Radev, 2004). Highlighting Keyphrases Another approach to assist users in better grasping the idea of a wiki page at a glance is to highlight important keyphrases (see F"
P11-4013,P07-1130,1,0.898161,"Missing"
P11-4013,J97-1003,0,0.156092,"is present on a certain page. Wikulu therefore supports users by analyzing long pages through employing text segmentation algorithms which detect topically coherent segments of text. It then suggests segment boundaries which the user may or may not accept for inserting a subheading which makes pages easier to read and better to navigate. As shown in Figure 3, users are also encouraged to set a title for each segment.8 When accepting one or more of these suggested boundaries, Wikulu stores them persistently in the wiki. Wikulu currently integrates text segmentation methods such as TextTiling (Hearst, 1997) or C99 (Choi, 2000). Summarizing Pages Similarly to segmenting pages, Wikulu makes long wiki pages more accessible by generating an extractive summary. While generative summaries generate a summary in own words, extractive summaries analyze the original wiki text sentence-by-sentence, rank each sentence, and return a list of the most important ones (see Figure 4). Wikulu integrates extractive text summarization methods such as LexRank (Erkan and Radev, 2004). Highlighting Keyphrases Another approach to assist users in better grasping the idea of a wiki page at a glance is to highlight importa"
P11-4013,W04-3252,0,\N,Missing
P11-4017,J93-1001,0,0.114449,"Missing"
P11-4017,P08-2035,0,0.0698863,"toolkit that solves both issues by reconstructing a certain past state of Wikipedia from its edit history, which is offered by the Wikimedia Foundation in form of a database dump. Section 3 gives a more detailed overview of the reconstruction process. Besides reconstructing past states of Wikipedia, the revision history data also constitutes a novel knowledge source for NLP algorithms. The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al., 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al., 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al., 2006). 1 http://download.wikimedia.org/ 97 Proceedings of the ACL-HLT 2011 System Demonstrations, pages 97–102, c Portland, Oregon, USA, 21 June 2011. 2011 Association for Computational Linguistics However, efficient access to this new resource has been limited by the immense size of the data. The revisions for all articles in the current English Wikipedia sum up to over 5"
P11-4017,N10-1056,0,0.0095054,"edia from its edit history, which is offered by the Wikimedia Foundation in form of a database dump. Section 3 gives a more detailed overview of the reconstruction process. Besides reconstructing past states of Wikipedia, the revision history data also constitutes a novel knowledge source for NLP algorithms. The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al., 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al., 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al., 2006). 1 http://download.wikimedia.org/ 97 Proceedings of the ACL-HLT 2011 System Demonstrations, pages 97–102, c Portland, Oregon, USA, 21 June 2011. 2011 Association for Computational Linguistics However, efficient access to this new resource has been limited by the immense size of the data. The revisions for all articles in the current English Wikipedia sum up to over 5 terabytes of text. Consequently, most of the above mentioned previous work"
P11-4017,zesch-gurevych-2010-better,1,0.824501,"2009). The majority of Wikipedia-based NLP algorithms works on single snapshots of Wikipedia, which are published by the Wikimedia Foundation as XML dumps at irregular intervals.1 Such a snapshot only represents the state of Wikipedia at a certain fixed point in time, while Wikipedia actually is a dynamic resource that is constantly changed by its millions of editors. This rapid change is bound to have an influence on the performance of NLP algorithms using Wikipedia data. However, the exact consequences are largely unknown, as only very few papers have systematically analyzed this influence (Zesch and Gurevych, 2010). This is mainly due to older snapshots becoming unavailable, as there is no official backup server. As a consequence, older experimental results cannot be reproduced anymore. In this paper, we present a toolkit that solves both issues by reconstructing a certain past state of Wikipedia from its edit history, which is offered by the Wikimedia Foundation in form of a database dump. Section 3 gives a more detailed overview of the reconstruction process. Besides reconstructing past states of Wikipedia, the revision history data also constitutes a novel knowledge source for NLP algorithms. The seq"
P11-4017,zesch-etal-2008-extracting,1,0.5734,"kit which provides access to Wikipedia with the help of a preprocessed database. It represents articles, categories and redirects as Java classes and provides access to the article content either as MediaWiki markup or as plain text. The toolkit mainly focuses on Wikipedia’s structure, the contained concepts, and semantic relations, but it makes little use of the textual content within the articles. Even though it was developed to work language independently, it focuses mainly on the English Wikipedia. Another open source API for accessing Wikipedia data from a preprocessed database is JWPL4 (Zesch et al., 2008). Like Wikipedia Miner, it also represents the content and structure of Wikipedia as Java objects. In addition to that, JWPL contains a MediaWiki markup parser to further analyze the article contents to make available fine-grained information like e.g. article sections, info-boxes, or first paragraphs. Furthermore, it was explicitly designed to work with all language versions of Wikipedia. We have chosen to extend JWPL with our revision toolkit, as it has better support for accessing article contents, natively supports multiple languages, and seems to have a larger and more active developer co"
P11-4017,W10-3504,0,\N,Missing
P13-2080,N12-1021,0,0.0900373,"Missing"
P13-2080,S13-2045,1,0.930106,"t is called partial textual entailment, because we are only interested in recognizing whether a single element of the hypothesis is entailed. To differentiate the two tasks, we will refer to the original textual entailment task as complete textual entailment. Partial textual entailment was first introduced by Nielsen et al. (2009), who presented a machine learning approach and showed significant improvement over baseline methods. Recently, a public benchmark has become available through the Joint Student Response Analysis and 8th Recognizing Textual Entailment (RTE) Challenge in SemEval 2013 (Dzikovska et al., 2013), on which we focus in this paper. Our goal in this paper is to investigate the idea of partial textual entailment, and assess whether Textual entailment is an asymmetric relation between two text fragments that describes whether one fragment can be inferred from the other. It thus cannot capture the notion that the target fragment is “almost entailed” by the given text. The recently suggested idea of partial textual entailment may remedy this problem. We investigate partial entailment under the faceted entailment model and the possibility of adapting existing textual entailment methods to thi"
P13-2080,R11-1063,1,0.82929,"rate movement in the body. H: The main job of muscles is to move bones. Lexical Inference This feature checks whether both facet words, or semantically related words, appear in T . We use WordNet (Fellbaum, 1998) with the Resnik similarity measure (Resnik, 1995) and count a facet term wi as matched if the similarity score exceeds a certain threshold (0.9, empirically determined on the training set). Both w1 and w2 must match for this module’s entailment decision to be positive. Syntactic Inference This module builds upon the open source1 Bar-Ilan University Textual Entailment Engine (BIUTEE) (Stern and Dagan, 2011). BIUTEE operates on dependency trees by applying a sequence of knowledge-based transformations that converts T into H. It determines entailment according to the “cost” of generating the hypothesis from the text. The cost model can be automatically tuned with a relatively small training set. BIUTEE has shown state-of-the-art performance on previous recognizing textual entailment challenges (Stern and Dagan, 2012). Since BIUTEE processes dependency trees, both T and the facet must be parsed. We therefore extract a path in H’s dependency tree that represents the facet. This is done by first pars"
P13-2080,P11-2098,1,0.802976,"mainly on lexical inference and syntax. We examined three representative modules that reflect these levels: Exact Match, Lexical Inference, and Syntactic Inference. Exact Match We represent T as a bag-of-words containing all tokens and lemmas appearing in the text. We then check whether both facet lemmas w1 , w2 appear in the text’s bag-of-words. Exact matching was used as a baseline in previous recognizing textual entailment challenges (Bentivogli et al., 2011), and similar methods of lemmamatching were used as a component in recognizing textual entailment systems (Clark and Harrison, 2010; Shnarch et al., 2011). Task Definition In order to tackle partial entailment, we need to find a way to decompose a hypothesis. Nielsen et al. (2009) defined a model of facets, where each such facet is a pair of words in the hypothesis and the direct semantic relation connecting those two words. We assume the simplified model that was used in RTE-8, where the relation between the words is not explicitly stated. Instead, it remains unstated, but its interpreted meaning should correspond to the manner in which the words are related in the hypothesis. For example, in the sentence “the main job of muscles is to move bo"
P13-2080,S12-1051,0,0.0646707,"Missing"
P13-2080,P12-3013,1,0.842323,"2 must match for this module’s entailment decision to be positive. Syntactic Inference This module builds upon the open source1 Bar-Ilan University Textual Entailment Engine (BIUTEE) (Stern and Dagan, 2011). BIUTEE operates on dependency trees by applying a sequence of knowledge-based transformations that converts T into H. It determines entailment according to the “cost” of generating the hypothesis from the text. The cost model can be automatically tuned with a relatively small training set. BIUTEE has shown state-of-the-art performance on previous recognizing textual entailment challenges (Stern and Dagan, 2012). Since BIUTEE processes dependency trees, both T and the facet must be parsed. We therefore extract a path in H’s dependency tree that represents the facet. This is done by first parsing H, and then locating the two nodes whose words compose the facet. We then find their lowest common ancestor (LCA), and extract the path P from w1 to The facet (muscles, move) refers to the agent role in H, and is expressed by T . However, the facet (move, bones), which refers to a theme or direct object relation in H, is unaddressed by T . 3 Entailment Modules Recognizing Faceted Entailment Our goal is to inv"
P13-4007,E09-1005,0,0.392799,"r et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. Furthermore, where the algorithm depends on linguistic annotations such as part-of-speech tags, the users are expected to supply these themselves, or else must use the annotators built into the system (which may not always be appropriate for the corpus language or domain). One alternative to coding WSD algorithms from scratch is to use general-purpose NLP toolkits such as NLTK (Bird, 2006) or DKPro (Gurevych 3 DKPro WSD"
P13-4007,P05-3014,0,0.0338443,"wn formats and structures: examples of sense-annotated corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. Furthermore, where the algorithm depends on linguistic annotations such as part-of-speech tags, the users are expected to supply these themselves, or else must use the annotators built into the system (which may not always be appropriate for the corpus language or domain). One alternative to coding WSD algorithms from scratch is to use gener"
P13-4007,H94-1046,0,0.257302,"he Senseval (and later SemEval) series of competitions, the first of which was held in 1998. Each competition defined a number of tasks with prescribed evaluation metrics, sense inventories, corpus file formats, and human-annotated test sets. For each task it was therefore possible to compare algorithms against each other. However, sense inventories and file formats still vary across tasks and competitions. There are also a number of increasingly popular resources used outside Senseval and SemEval, each with their own formats and structures: examples of sense-annotated corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soro"
P13-4007,P06-4018,0,0.0694924,"et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. Furthermore, where the algorithm depends on linguistic annotations such as part-of-speech tags, the users are expected to supply these themselves, or else must use the annotators built into the system (which may not always be appropriate for the corpus language or domain). One alternative to coding WSD algorithms from scratch is to use general-purpose NLP toolkits such as NLTK (Bird, 2006) or DKPro (Gurevych 3 DKPro WSD Our system, DKPro WSD, is implemented as a framework of UIMA components (type systems, collection readers, annotators, CAS consumers, resources) which the user combines into a data processing pipeline. We can best illustrate this with an example: Figure 1 shows a pipeline for running two disambiguation algorithms on the Estonian all-words task from Senseval-2. UIMA components are the solid, rounded boxes in the lower half of the diagram, and the data and algorithms they encapsulate are the light grey shapes in the upper half. The first component of the pipeline"
P13-4007,C12-1109,1,0.833346,"we would pass an English instead of Estonian language model to TreeTagger, and we would substitute the sense inventory resource exposing the Estonian EuroWordNet with one for WordNet 1.7.1. Crucially, none of the WSD algorithms need to be changed. Algorithms. As with sense inventories, WSD algorithms have a type and interface hierarchy according to what knowledge sources they require. Algorithms and baselines already implemented include the analytically calculated random sense baseline; the most frequent sense baseline; the original, simplified, extended, and lexically expanded Lesk variants (Miller et al., 2012); various The most important features of our system are 39 graph connectivity approaches from Navigli and Lapata (2010); Personalized PageRank (Agirre and Soroa, 2009); the supervised TWSI system (Biemann, 2013); and IMS (Zhong and Ng, 2010). Our open API permits users to program support for further knowledge-based and supervised algorithms. Linguistic annotators. Many WSD algorithms require linguistic annotations from segmenters, lemmatizers, POS taggers, parsers, etc. Off-theshelf UIMA components for producing such annotations, such as those provided by DKPro Core (Gurevych et al., 2007), ca"
P13-4007,E12-1059,1,0.820008,"entity linking tasks (McNamee and Dang, 2009), and the aforementioned MASC, SemCor, and WebCAGe corpora. Our prepackaged corpus analysis modules can compute statistics on monosemous terms, average polysemy, terms absent from the sense inventory, etc. Sense inventories. Sense inventories are abstracted into a system of types and interfaces according to the sort of lexical-semantic information they provide. There is currently support for WordNet (Fellbaum, 1998), WordNet++ (Ponzetto and Navigli, 2010), EuroWordNet (Vossen, 1998), the Turk Bootstrap Word Sense Inventory (Biemann, 2013), and UBY (Gurevych et al., 2012), which provides access to WordNet, Wikipedia, Wiktionary, GermaNet, VerbNet, FrameNet, OmegaWiki, and various alignments between them. The system can automatically convert between various versions of WordNet using the UPC mappings (Daud´e et al., 2003). A pipeline of this sort can be written with just a few lines of code: one or two to declare each component and if necessary bind it to the appropriate resources, and a final one to string the components together into a pipeline. Moreover, once such a pipeline is written it is simple to substitute functionally equivalent components. For example"
P13-4007,P05-3019,0,0.0243504,"d corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. Furthermore, where the algorithm depends on linguistic annotations such as part-of-speech tags, the users are expected to supply these themselves, or else must use the annotators built into the system (which may not always be appropriate for the corpus language or domain). One alternative to coding WSD algorithms from scratch is to use general-purpose NLP toolkits such as NLTK (Bird, 2006) o"
P13-4007,E12-1039,0,0.0296995,"first of which was held in 1998. Each competition defined a number of tasks with prescribed evaluation metrics, sense inventories, corpus file formats, and human-annotated test sets. For each task it was therefore possible to compare algorithms against each other. However, sense inventories and file formats still vary across tasks and competitions. There are also a number of increasingly popular resources used outside Senseval and SemEval, each with their own formats and structures: examples of sense-annotated corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a par"
P13-4007,pazienza-etal-2008-jmwnl,0,0.0256454,"ian language model TreeTagger results and statistics JMWNL simplified Lesk degree centrality sense inventory corpus reader answer key annotator WSD annotator linguistic annotator WSD annotator evaluator Figure 1: A sample DKPro WSD pipeline for the Estonian all-words data set from Senseval-2. as follows: Then come the two disambiguation algorithms, also modelled as UIMA annotators wrapping nonUIMA-aware algorithms. Each WSD annotator iterates over the instances in the CAS and annotates them with sense IDs from EuroWordNet. (EuroWordNet itself is accessed via a UIMA resource which wraps JMWNL (Pazienza et al., 2008) and which is bound to the two WSD annotators.) Finally, control passes to a CAS consumer which compares the WSD algorithms’ sense annotations against the gold-standard annotations produced by the answer key annotator, and outputs these sense annotations along with various evaluation metrics (precision, recall, etc.). Corpora and data sets. DKPro WSD currently has collection readers for all Senseval and SemEval all-words and lexical sample tasks, the AIDA CoNLL-YAGO data set (Hoffart et al., 2011), the TAC KBP entity linking tasks (McNamee and Dang, 2009), and the aforementioned MASC, SemCor,"
P13-4007,D11-1072,0,0.165694,"Missing"
P13-4007,P10-1154,0,0.22145,"Missing"
P13-4007,P10-2013,0,0.0210432,"al) series of competitions, the first of which was held in 1998. Each competition defined a number of tasks with prescribed evaluation metrics, sense inventories, corpus file formats, and human-annotated test sets. For each task it was therefore possible to compare algorithms against each other. However, sense inventories and file formats still vary across tasks and competitions. There are also a number of increasingly popular resources used outside Senseval and SemEval, each with their own formats and structures: examples of sense-annotated corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong"
P13-4007,C12-3031,0,0.0158374,"Linguistics, pages 37–42, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics et al., 2007). Such toolkits provide individual components potentially useful for WSD, such as WordNet-based measures of sense similarity and readers for the odd corpus format. However, these toolkits are not specifically geared towards development and evaluation of WSD systems; there is no unified type system or architecture which allows WSD-specific components to be combined or substituted orthogonally. The only general-purpose dedicated WSD system we are aware of is I Can Sense It (Joshi et al., 2012), a Web-based interface for running and evaluating various WSD algorithms. It includes I/O support for several corpus formats and implementations of a number of baseline and state-of-theart disambiguation algorithms. However, as with previous single-algorithm systems, it is not possible to select the sense inventory, and the user is responsible for pre-annotating the input text with POS tags. The usability and extensibility of the system are greatly restricted by the fact that it is a proprietary, closed-source application fully hosted by the developers. our system and further explain its capa"
P13-4007,N10-2006,0,0.01964,"s, corpus file formats, and human-annotated test sets. For each task it was therefore possible to compare algorithms against each other. However, sense inventories and file formats still vary across tasks and competitions. There are also a number of increasingly popular resources used outside Senseval and SemEval, each with their own formats and structures: examples of sense-annotated corpora include SemCor (Miller et al., 1994), MASC (Ide et al., 2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. F"
P13-4007,P10-4014,0,0.369651,"2010), and WebCAGe (Henrich et al., 2012), and sense inventories include VerbNet (Kipper et al., 2008), FrameNet (Ruppenhofer et al., 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. So despite attempts at standardization, the canon of WSD resources remains quite fragmented. The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al., 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. Furthermore, where the algorithm depends on linguistic annotations such as part-of-speech tags, the users are expected to supply these themselves, or else must use the annotators built into the system (which may not always be appropriate for the corpus language or domain). One alternative to coding WSD algorithms from scratch is to use general-purpose NLP toolkits such as NLTK (Bird, 2006) or DKPro (Gurevych 3 DKPro WSD Our system, DKPro WSD, is impl"
P13-4021,O97-1002,0,0.836865,"rs of words (Hatzivassiloglou et al., 1999). Pairwise Word Similarity These measures are based on pairwise word similarity computations which are then aggregated for the complete texts. The measures typically operate on a graph-based representation of words and the semantic relations among them within a lexical-semantic resource. DKPro Similarity therefore contains adapters for WordNet, Wiktionary5 , and Wikipedia, while the framework can easily be extended to other data sources that conform to a common interface (Garoufi et al., 2008). Pairwise similarity measures in DKPro Similarity include Jiang and Conrath (1997) or Resnik (1995). The aggregation for the complete texts can for example be done using the strategy by Mihalcea et al. (2006). Stylistic Similarity DKPro Similarity includes, for example, a measure which compares function word frequencies (Dinu and Popescu, 2009) between two texts. The framework also includes a set of measures which capture statistical properties of texts such as the type-token ratio (TTR) and the sequential TTR (McCarthy and Jarvis, 2010). Phonetic Similarity DKPro Similarity also allows to compute text similarity based on pairwise phonetic comparisons of words. It therefore"
P13-4021,P10-4006,0,0.0407944,"distance metrics. In DKPro Similarity, some string-based measures (see Section 3.1) are based on implementations from this library. SecondString Toolkit The freely available library by Cohen et al. (2003)12 is similar to SimMetrics, and also implemented in Java. It also contains several well-known text similarity measures on string sequences, and includes many of the measures which are also part of the SimMetrics Library. Some string-based measures in DKPro Similarity are based on the SecondString Toolkit. S-Space Package Even though no designated text similarity library, the S-Space Package (Jurgens and Stevens, 2010)8 contains some text similarity measures such as Latent Semantic Analysis (LSA) and Explicit Semantic Analysis (see Section 3.2). However, it is primarily focused on word space models which operate on word distributions in text. Besides such algorithms, it offers a variety of interfaces, data structures, evaluation datasets and metrics, and global operation utilities e.g. for dimension reduction using Singular Value Decomposition or randomized projections, which are particularly useful with such distributional word space models. DKPro Similarity integrates LSA based on the S-Space Package. 7 C"
P13-4021,S12-1059,1,0.847823,"Missing"
P13-4021,C12-1011,1,0.76946,"Missing"
P13-4021,P02-1020,0,0.0475005,"he-box without further configuration.6 DKPro Similarity contains two major types of experimental setups: (i) those for an intrinsic evaluation allow to evaluate the system performance in an isolated setting by comparing the system results with a human gold standard, and (ii) those for an extrinsic evaluation allow to evaluate the system with respect to a particular task at hand, where text similarity is a means for solving a concrete problem, e.g. recognizing textual entailment. readers come pre-packaged include, among others, the SemEval-2012 STS data (Agirre et al., 2012), the METER corpus (Clough et al., 2002), or the RTE 1–5 data (Dagan et al., 2006). As far as license terms allow redistribution, the datasets themselves are integrated into the framework. Similarity Scorer The Similarity Scorer allows to integrate any text similarity measure (which is decoupled from UIMA by default) into a UIMAbased pipeline. It builds upon the standardized text similarity interfaces and thus allows to easily exchange the text similarity measure as well as to specify the data types the measure should operate on, e.g. tokens or lemmas. Intrinsic Evaluation DKPro Similarity contains the setup (B¨ar et al., 2012a) whi"
P13-4021,N04-3012,0,0.128581,"mework. For that, DKPro Similarity also comes with an example module for getting started, which guides first-time users through both the stand-alone and the UIMA-coupled modes. Semantic Vectors The Semantic Vectors package is a package for distributional semantics (Widdows and Cohen, 2010)9 that contains measures such as LSA and allows for comparing documents within a given vector space. The main focus lies on word space models with a number of dimension reduction techniques, and applications on word spaces such as automatic thesaurus generation. WordNet::Similarity The open source package by Pedersen et al. (2004)10 is a popular Perl library for the similarity computation on WordNet. It comprises six word similarity measures that operate on WordNet, e.g. Jiang and Conrath (1997) or Resnik (1995). Unfortunately, no strategies have been added to the package yet which aggregate the word similarity scores for complete texts in a similar manner as described in Section 3.2. 8 code.google.com/p/airhead-research code.google.com/p/semanticvectors 10 sourceforge.net/projects/wn-similarity 9 125 Acknowledgements This work has been supported by the Volkswagen Foundation as part of the Lichtenberg Professorship Pro"
P13-4021,W99-0625,0,0.0170643,"ural, stylistic, and phonetic similarity. Semantic Similarity Measures DKPro Similarity also contains several measures which go beyond simple character sequences and compute text similarity on a semantic level. Structural Similarity Structural similarity between texts can be computed, for example, by comparing sets of stopword n-grams (Stamatatos, 2011). The idea here is that similar texts may preserve syntactic similarity while exchanging only content words. Other measures in DKPro Similarity allow to compare texts by part-of-speech ngrams, and order and distance features for pairs of words (Hatzivassiloglou et al., 1999). Pairwise Word Similarity These measures are based on pairwise word similarity computations which are then aggregated for the complete texts. The measures typically operate on a graph-based representation of words and the semantic relations among them within a lexical-semantic resource. DKPro Similarity therefore contains adapters for WordNet, Wiktionary5 , and Wikipedia, while the framework can easily be extended to other data sources that conform to a common interface (Garoufi et al., 2008). Pairwise similarity measures in DKPro Similarity include Jiang and Conrath (1997) or Resnik (1995)."
P13-4021,S12-1051,0,\N,Missing
P14-5006,S10-1004,0,0.0762358,"ases further. Experiments where noun chunks are selected as keyphrases perform best for this example. Named entities are too restrictive, but applicable for identifying relevant entities in a text. This is useful for tasks that are targeted towards entities, e.g. for finding experts (D¨orner et al., 2007) in a collection of domaindependent texts. The selection of a linguistic type is not limited as preprocessing components might introduce further types. 2.3 Ranking 2.5 Evaluation DKPro Keyphrases ships with all the metrics that have been traditionally used for evaluating keyphrase extraction. Kim et al. (2010) use precision and recall for a different number of keyphrases (5, 10 and 15 keyphrases). These metrics are widely used for evaluation in information retrieval. Precision @5 is the ratio of true positives in the set of extracted keyphrases when 5 keyphrases are extracted. Recall @5 is the ratio of true positives in the set of gold keyphrases when 5 keyphrases are extracted. Moreover, DKPro Keyphrases evaluates with MAP and R-precision. MAP is the mean average precision of extracted keyphrases from the highest scored keyphrase to the total number of extracted keyphrases. For each position in th"
P14-5006,D09-1137,0,0.0187621,"API, which offers automatic keyphrase extraction from texts. They provide a supervised approach for keyphrase extraction. For each keyphrase, KEA computes frequency, position, and semantic relatedness as features. Thus, for using KEA, the user needs to provide annotated training data. KEA generates keyphrases from n-grams with length from 1 to 3 tokens. A controlled vocabulary can be used to filter keyphrases. The configuration for keyphrase selection and filtering is limited compared to DKPro Keyphrases, which offers capabilities for changing the entire preprocessing or adding filters. Maui (Medelyan et al., 2009) enhances KEA by allowing the computation of semantic relatedness of keyphrases. It uses Wikipedia as a thesaurus and computes the keyphraseness of each keyphrase, which is the number of times a candidate was used as keyphrase in the training data (Medelyan et al., 2009). Although Maui provides training data along with their software, this training data is highly domain-specific. A shortcoming of KEA and Maui is the lack of any evaluation capabilities or the possibility to run parameter sweeping experiments. DKPro Keyphrases provides evaluation tools for automatic testing of many parameter set"
P14-5006,R09-1086,1,0.833455,"n average precision of extracted keyphrases from the highest scored keyphrase to the total number of extracted keyphrases. For each position in the rank, the precision at that position will be computed. Summing up the precision at each recall point and then taking its average will return the average precision for the text being evaluated. The mean average precision will be the mean from the sum of each text’s average precision from the dataset. R-precision is the ratio of true positives in the set of extracted keyphrases, when the set is limited to the same size as the set of gold keyphrases (Zesch and Gurevych, 2009). Filtering Filtering can be used together with overgenerating selection approaches like taking all ngrams to decrease the number of keyphrases before ranking. One possible approach is based on POS patterns. For example, using the POS patterns, Adjective-Noun, Adjective, and Noun limits the set of possible keyphrases to “dog”, “old cat”, “cat”, and “garden” in the previous example. This step can also been performed as part of the selection step, however, keeping it separated enables researchers to apply filters to keyphrases of any linguistic type. DKPro Keyphrases provides the possibility to"
P14-5006,W00-0405,0,0.242278,"Missing"
P14-5006,W04-3252,0,\N,Missing
P14-5011,P13-1166,0,0.0421748,"Missing"
P14-5011,P13-4020,0,0.0156067,"ws TextClassificationException { int nrOfEmoticons = JCasUtil.select(annoDb, EMO.class).size(); int nrOfTokens = JCasUtil.select(annoDb, Token.class).size(); double ratio = (double) nrOfEmoticons / nrOfTokens; return new Feature(&quot;EmoticonRatio&quot;, ratio).asList(); } } Listing 2: A DKPro TC document mode feature extractor measuring the ratio of emoticons to tokens. customizable research environment for quick experiments and does not provide predefined text classification setups. Furthermore, it does not support parameter sweeping and has no explicit support for creating experiment reports. Argo (Rak et al., 2013) is a web-based workbench with support for manual annotation and automatic analysis of mainly bio-medical data. Like DKPro TC, Argo is based on UIMA, but focuses on sequence tagging, and it lacks DKPro TC’s parameter sweeping capabilities. NLTK (Bird et al., 2009) is a general-purpose NLP toolkit written in Python. It offers components for a wide range of preprocessing tasks and also supports feature extraction and machine learning for supervised text classification. Like DKPro TC, it can be used to quickly setup baseline experiments. As opposed to DKPro TC, NLTK lacks a modular structure with"
P14-5011,D12-1042,0,0.0252752,"· Text Categorization · Keyphrase Assignment · Text Readability Unit/Sequence Mode · Named Entity Recognition · Part-of-Speech Tagging · Dialogue Act Tagging · Word Difficulty Pair Mode · Paraphrase Identification · Textual Entailment · Relation Extraction · Text Similarity Table 1: Supervised learning scenarios and feature modes supported in DKPro TC, with example NLP applications. • The pair mode is intended for problems which require a pair of texts as input, e.g. a pair of sentences to be classified as paraphrase or non-paraphrase. It represents a special case of multi-instance learning (Surdeanu et al., 2012), in which a document contains exactly two instances. Flexibility Users of a system for supervised learning on textual data should be able to choose between different machine learning approaches depending on the task at hand. In supervised machine learning, we have to distinguish between approaches based on classification and approaches based on regression. In classification, given a document d ∈ D and a set of labels C = {c1 , c2 , ..., cn }, we want to label each document d with L ⊂ C, where L is the set of relevant or true labels. In single-label classification, each document d is labeled w"
P14-5011,P11-2008,0,\N,Missing
Q14-1040,W11-1407,0,0.0588452,"nly recognition. However, Jakschik et al. (2010) transform the C-test 518 into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates. C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test beca"
Q14-1040,I13-1112,1,0.841584,"e words might not be part of the students active vocabulary and are only guessed because they occur as cognates in the students L1. This is supported by the fact that many of the cognate answers resemble orthographic principles from other languages, e.g. for skeletons we find *skellets, *skelleton(s), *skelets, *skelletts, *skeletton(s), *skeltons, *skeletes, and *skelette(s).11 In order to account for this phenomenon, we estimate the cognateness of words by gathering data from four different lists. We retrieve cognates from UBY using string similarity and from a cognate production algorithm (Beinborn et al., 2013). In addition, we consult the COCA list of academic words12 and a list of words with latin roots.13 Inflection Many errors are caused by wrong morphological inflection as in this example: And in har times like these, ... [harder] The base form hard (72) is provided more often than the correct comparative harder (48), although it is too short. Other inflection errors are caused by singular/plural and adjective/adverb confusion. In order to account for this phenomenon, we test whether the solution is in lemma form or carries any inflection markers using a lemmatizer. We also check whether the wo"
Q14-1040,H05-1103,0,0.258217,"ltiple choice options and find that this variant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates. C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text- and word-specific factors. The search for the factors that de"
Q14-1040,P14-5011,1,0.800424,"Missing"
Q14-1040,W14-5201,1,0.76658,"Missing"
Q14-1040,E12-1059,1,0.751574,"ompound (e.g. coastline) because the prefix only provides information about the first part of the word. In our approach, compounds are detected using a word splitting algorithm with an English dictionary.8 Another issue are polysemous words, as learners might know one sense of a word but not be aware of the existence of a second sense. Polysemy interferes with frequency, e.g. the word well has a high frequency, but it occurs only rarely in its sense fountain. In order to account for polysemy, we count the number of represented word senses for the solution in the lexical-semantic resource UBY (Gurevych et al., 2012). The two senses of well also differ in their word class. The word class has been studied as a difficulty indicator by several researchers but with mixed results. Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners. Sigott (1995) could not confirm any effect of the word class on C-test difficulty. 6 In all examples, we only highlight a single gap to illustrate a certain phenomenon. 7 http://www.grsampson.net/RSue.html 8 http://www.danielnaber.de/jwordsplitter/index en.html 521 The word class is determined"
Q14-1040,N07-1058,0,0.0251899,"n test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates. C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text- and word-specific factors. The search fo"
Q14-1040,W12-2016,0,0.135917,"ing ability examining only recognition. However, Jakschik et al. (2010) transform the C-test 518 into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates. C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the d"
Q14-1040,P11-1027,0,0.0135175,"e student. This feature is comparable to the semantic cache used by Brown (1989). Phonetic complexity Wrong answer variants for C-test gaps are often rooted in phonetic problems. The spelling of a word is more difficult, if it contains a rare sequence of characters. The word appropriate, for example, triggers 69 different answers, 40 of them were provided only once. In addition, a spelling error is more likely to occur, in words with rare grapheme-phoneme mapping as in Wednesday. We build a character-based language model that indicates the probability of a character sequence using BerkeleyLM (Pauls and Klein, 2011). In addition, we build a phonetic model using phonetisaurus, a statistical alignment algorithm that maps characters onto phonemes.14 Both models are trained only on words from the Basic English list in order to reflect the knowledge of a language learner.15 Based on this scarce data, the phonetic model only learns the most frequent character-tophoneme mappings and assigns higher phonetic scores to less general letter sequences. We use this score as a feature and additionally calculate the string similarity between the output and the correct pronunciation in the CMU dictionary.16 Another sourc"
Q14-1040,W12-2017,0,0.102172,"riant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates. C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text- and word-specific factors. The search for the factors that determine the difficulty of C-tests is tightly connected"
Q14-1040,P13-2043,0,0.0718477,"ty: the application of relaxed scoring schemes and the use of distractors. In relaxed scoring, teachers accept all tolerable candidates for a gap and not only the intended solution as in exact scoring. Unfortunately, this scoring method turned out to be quite subjective and time-consuming as it is not possible to anticipate all tolerable solutions (Raatz and Klein-Braley, 2002). The use of distractors circumvents this open solution space by providing a closed set of candidates from which the solution needs to be picked. Several approaches have been proposed for automatic distractor selection (Sakaguchi et al., 2013; Zesch and Melamud, 2014) to make sure that the distractors are not too hard nor too easy and are not a valid solution themselves. However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results. In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative. Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language"
Q14-1040,W10-1007,0,0.159893,"518 into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates. C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text- and word-specific"
Q14-1040,W14-1817,1,0.850075,"elaxed scoring schemes and the use of distractors. In relaxed scoring, teachers accept all tolerable candidates for a gap and not only the intended solution as in exact scoring. Unfortunately, this scoring method turned out to be quite subjective and time-consuming as it is not possible to anticipate all tolerable solutions (Raatz and Klein-Braley, 2002). The use of distractors circumvents this open solution space by providing a closed set of candidates from which the solution needs to be picked. Several approaches have been proposed for automatic distractor selection (Sakaguchi et al., 2013; Zesch and Melamud, 2014) to make sure that the distractors are not too hard nor too easy and are not a valid solution themselves. However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results. In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative. Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary,"
R09-1086,P05-1045,0,0.00307033,"mework for the comprehensive analysis of keyphrase extraction as shown in Figure 1. It was designed to be as language-independent as possible using either no language dependent information at all, or components that are already available for most languages (like tokenizers or chunkers). The preprocessing pipeline is based on the DKPro UIMA component repository [7]. Pre-Processing and Candidate Selection For preprocessing, we tokenize the documents, and split them into sentences. We integrated the TreeTagger for lemmatization, POS-tagging, and NP chunking [20], as well as the Stanford NER tool [5] for named entity recognition. From this pool of preprocessed data, we select as candidates Tokens, Lemmas, N-grams, Noun Phrases, and Named Entities. Following [15], we additionally use the restricted set of tokens Tokens (N,A) and lemmas Lemmas (N,A). Candidate Ranking The unsupervised graphbased methods (e.g. TextRank ) build a co-occurence graph using the candidates. The final candidate ranking is determined by computing the centrality scores of the graph nodes using PageRank. For tf.idf ranking, 3 It is formally defined as: Approx(kgold , kext ) = Exact ∨ Morph ∨ Includes. KEA TextRank tf"
R09-1086,N04-4005,0,0.141934,"se a corpus of training data to learn a keyphrase extraction model that is able to classify candidates as keyphrases. A well known supervised system is Kea [6] that uses all n-grams of a certain length as candidates, and ranks them using the probability of being a keyphrase. Kea is based on a Na¨ıve Bayes classifier using tf.idf and position as its main features. Extractor [22] is another supervised system that uses stems and stemmed n-grams as candidates. Its features are tuned using a genetic algorithm. Kea and Extractor are known to achieve roughly the same level of performance [23]. Hulth [10] uses a combination of lexical and syntactic features adding more linguistic knowledge which outperforms Kea. Medelyan and Witten [13] present the improved Kea++ that selects candidates with reference to a controlled vocabulary from a thesaurus or Wikipedia [14]. Turney [23] augments Kea with a feature set based on statistical word association to ensure that the returned keyphrase set is coherent. However, this assumption might not hold if a document covers different topics. Nguyen and Kan [16] augment Kea with features tailored towards scientific publications such as section information and c"
R09-1086,W08-1404,0,0.00452995,"on the properties of the evaluation dataset. Keywords keyphrase extraction; approximate matching 1 Introduction Keyphrases are small sets of expressions representing a document’s content. Keyphrase extraction is the task of automatically extracting such keyphrases from a document. The extracted phrases have to be present in the document itself, in contrast to keyphrase assignment (a multi-class text classification problem) where a fixed set of keyphrases is used which are not necessarily contained in the document. Keyphrase extraction has important applications in NLP including summarization [4, 11], clustering [9], as well as indexing and browsing [8], highlighting [22] and searching [2]. Despite the importance of the task, the evaluation of keyphrase extraction has not received much research attention in the past. In this paper, we address three core problems with the evaluation of keyphrase extraction: (i) the evaluation metric, (ii) the evaluation datasets, and (iii) the evaluation framework. The performance of most keyphrase extraction algorithms is evaluated by comparing whether the extracted keyphrases exactly match the human assigned gold standard keyphrases. However, this is kno"
R09-1086,C02-1142,0,0.0610057,"is section, we give an overview of (i) existing approaches to keyphrase extraction, (ii) the different ways to evaluate keyphrase extraction, and (iii) the datasets that have been used for evaluation. Keyphrase Extraction Approaches Existing methods for keyphrase extraction can be categorized into supervised and unsupervised approaches.1 1 Note that unsupervised approaches might use tools like NP chunkers relying on supervised approaches. However, as such 484 International Conference RANLP 2009 - Borovets, Bulgaria, pages 484–489 Closely related to keyphrase extraction are glossary extraction [17] and back-of-the-book indexing [3]. Unsupervised approaches usually select quite general sets of candidates (e.g. all tokens in a document), and use a subsequent ranking step to limit the selection to the most important candidates. For example, Barker and Cornacchia [1] restrict candidates to noun phrases, and rank them using heuristics based on length, term frequency, and head noun frequency. Bracewell et al. [2] also restrict candidates to noun phrases, and cluster them if they share a term. The clusters are ranked according to the noun phrase and token frequencies in the document. Finally,"
R09-1086,C08-2021,0,0.0295365,"anked using PageRank, and longer keyphrases can be reconstructed in a post-processing step merging adjacent keywords. The method was found to yield competitive results with state-of-theart supervised systems [15]. Wan and Xiao [24] expand TextRank by augmenting the graph with highly similar documents, which improves results compared with standard TextRank and a tf.idf baseline. Another branch of unsupervised approaches is based on statistical analysis. Tomokiyo and Hurst [21] use pointwise KL-divergence between language models derived from the documents and a reference corpus. Paukkeri et al. [18] use a similar method based on likelihood ratios. Matsuo and Ishizuka [12] present a statistical keyphrase extraction approach that does not make use of a reference corpus, but is based on co-occurrences of terms in a single document. Supervised approaches use a corpus of training data to learn a keyphrase extraction model that is able to classify candidates as keyphrases. A well known supervised system is Kea [6] that uses all n-grams of a certain length as candidates, and ranks them using the probability of being a keyphrase. Kea is based on a Na¨ıve Bayes classifier using tf.idf and positio"
R09-1086,W03-1805,0,0.0466009,"here the graph nodes are tokens and the edges reflect cooccurrence relations between tokens in the document. The nodes are ranked using PageRank, and longer keyphrases can be reconstructed in a post-processing step merging adjacent keywords. The method was found to yield competitive results with state-of-theart supervised systems [15]. Wan and Xiao [24] expand TextRank by augmenting the graph with highly similar documents, which improves results compared with standard TextRank and a tf.idf baseline. Another branch of unsupervised approaches is based on statistical analysis. Tomokiyo and Hurst [21] use pointwise KL-divergence between language models derived from the documents and a reference corpus. Paukkeri et al. [18] use a similar method based on likelihood ratios. Matsuo and Ishizuka [12] present a statistical keyphrase extraction approach that does not make use of a reference corpus, but is based on co-occurrences of terms in a single document. Supervised approaches use a corpus of training data to learn a keyphrase extraction model that is able to classify candidates as keyphrases. A well known supervised system is Kea [6] that uses all n-grams of a certain length as candidates, a"
R09-1086,W04-3252,0,\N,Missing
R11-1071,E09-1065,0,0.320234,"udgments across the whole similarity range. We then asked three annotators: “How similar are the given texts?” We then computed the Spearman correlation of each annotator’s ratings with the gold standard: ρA1 = 0.83, ρA2 = 0.65, and ρA3 = 0.85. The much lower correlation of 2 Articles written in Simple English use a limited vocabulary and easier grammar than the standard Wikipedia. 3 The last step requires all measures to be normalized. 516 Dataset Text Type / Domain Length in Terms () # Pairs 30 Sentence Pairs (Li et al., 2006) 50 Short Texts (Lee et al., 2005) Computer Science Assignments (Mohler and Mihalcea, 2009) Microsoft Paraphrase Corpus (Dolan et al., 2004) Concept Definitions News (Politics) 5–33 (11) 45–126 (80) 30 1,225 0–4 1–5 32 8–12 1–173 (18) 630 0–5 2 5–31 (19) 5,801 Computer Science News Rating Scale binary # Judges per Pair 2–3 Aggr. Similarity Table 2: Statistics for text similarity evaluation datasets 2 {ABC} {ABC} 1.5 3.1 Content Style +{D} Li et al. (2006) introduced 65 sentence pairs which are based on the noun pairs by Rubenstein and Goodenough (1965). Each noun was replaced by its definition from Collins Cobuild English Dictionary (Sinclair, 2001). The dataset contains judgments f"
R11-1071,W06-1603,0,0.0238547,"Missing"
R11-1071,W09-3204,0,0.0336359,"Missing"
R11-1071,U06-1019,0,0.037165,"Missing"
R11-1071,W09-3206,0,0.064296,"Missing"
R11-1071,U05-1023,0,0.0607695,"Missing"
R11-1071,C04-1051,0,0.0195908,"Missing"
R11-1071,I05-5003,0,0.0678646,"Missing"
R11-1071,P08-1048,0,\N,Missing
R13-1033,P07-1069,0,0.319718,"even wrong titles. Figure 2: TOC of this paper Supervised approaches learn a model of which document segments usually have a certain title. They are highly precise, but require training data and are limited to an a priori determined set of titles for which the model is trained. preprocessing including named entity recognition (Finkel et al., 2005), keyphrase extraction (Mihalcea and Tarau, 2004), and chunking (Schmid, 1994) which are then used as features for machine learning. To foster future research, we present two new datasets and compare results on these datasets and the one presented by Branavan et al. (2007). Our research contribution is to develop new algorithms for segment hierarchy identification, to present new evaluation datasets for all subtasks, and to compare our newly developed methods with the state of the art. We also provide a comprehensive analysis of the benefits and shortcomings of the applied methods. Figure 2 gives an overview of the paper’s organization (and at the same time highlights the usefulness of a TOC for the reader). Thus, we may safely skip the enumeration of paper sections and their content that usually concludes the introduction. 2 In the following, we organize the f"
R13-1033,H01-1011,0,0.0388201,"ear periods. A flexible system for generating segment titles enables the user to decide on which titles are more interesting and thus increasing the user’s benefit. each of the most frequent titles in each dataset. In Wikipedia, most articles have sections like See also, References, or External links, while books usually start with a chapter Preface. We restrict the list of title candidates to those appearing at least twice in the training data. We use a statistical model for predicting the title of a segment In contrast to previous approaches (Branavan et al., 2007; Nguyen and Shimazu, 2009; Jin and Hauptmann, 2001), we do not train on parts of the same document for which we want to predict titles, but rather on full documents of the same type (Wikipedia articles and books). This is an important difference, as in our usage scenario we need to generate full TOCs for previously unseen documents. On the Cormen dataset we cannot perform a trainings phase as it consists of one book. Evaluation Metrics We evaluated all approaches using two evaluation metrics. We propose accuracy as evaluation metric. A generated title is counted as correct only if it exactly matches the correct title. Hence, methods that gener"
R13-1033,R11-1106,0,0.0555659,"Missing"
R13-1033,W04-3252,0,0.00697675,"owing classes: Text-based approaches make use of only the text in the corresponding segment. Therefore, titles are limited to words appearing in the text. They can be applied in all situations, but will often create trivial or even wrong titles. Figure 2: TOC of this paper Supervised approaches learn a model of which document segments usually have a certain title. They are highly precise, but require training data and are limited to an a priori determined set of titles for which the model is trained. preprocessing including named entity recognition (Finkel et al., 2005), keyphrase extraction (Mihalcea and Tarau, 2004), and chunking (Schmid, 1994) which are then used as features for machine learning. To foster future research, we present two new datasets and compare results on these datasets and the one presented by Branavan et al. (2007). Our research contribution is to develop new algorithms for segment hierarchy identification, to present new evaluation datasets for all subtasks, and to compare our newly developed methods with the state of the art. We also provide a comprehensive analysis of the benefits and shortcomings of the applied methods. Figure 2 gives an overview of the paper’s organization (and"
R13-1033,D11-1071,0,0.0214634,"s. Furthermore, we want to continue develop better features for the task of hierarchy identification, and want to create methods for postprocessing a TOC in order to generate a coherent table-of-contents. We made the newly created evaluation datasets and our experimental framework publicly available in order to foster future research in table-ofcontents generation.8 the performance on some datasets. As these approaches typically use an independent set of title candidates, they can potentially achieve a higher performance. Commonly used combination strategies like voting or complex strategies (Chen, 2011) can only be applied within approaches from the same class, as different classes will output different titles. Besides, it is desirable to create a diversity of candidates without ignoring titles generated by only one approach. Results in Table 7 reveals that a combination of approaches provides the highest accuracy of all approaches. We cannot compare a list of generated titles to a gold title with Rouge, thus not presenting any numbers (n/a). We utilize the benefit of accuracy allowing to compare a set of generated titles to a gold title. In a real-world setting, a user selects the best titl"
R13-1033,P04-1015,0,0.22484,"Missing"
R13-1033,R09-1057,0,0.0897158,"of the dates of the fouryear periods. A flexible system for generating segment titles enables the user to decide on which titles are more interesting and thus increasing the user’s benefit. each of the most frequent titles in each dataset. In Wikipedia, most articles have sections like See also, References, or External links, while books usually start with a chapter Preface. We restrict the list of title candidates to those appearing at least twice in the training data. We use a statistical model for predicting the title of a segment In contrast to previous approaches (Branavan et al., 2007; Nguyen and Shimazu, 2009; Jin and Hauptmann, 2001), we do not train on parts of the same document for which we want to predict titles, but rather on full documents of the same type (Wikipedia articles and books). This is an important difference, as in our usage scenario we need to generate full TOCs for previously unseen documents. On the Cormen dataset we cannot perform a trainings phase as it consists of one book. Evaluation Metrics We evaluated all approaches using two evaluation metrics. We propose accuracy as evaluation metric. A generated title is counted as correct only if it exactly matches the correct title."
R13-1033,P05-1045,0,0.19965,"eration we divide related work into the following classes: Text-based approaches make use of only the text in the corresponding segment. Therefore, titles are limited to words appearing in the text. They can be applied in all situations, but will often create trivial or even wrong titles. Figure 2: TOC of this paper Supervised approaches learn a model of which document segments usually have a certain title. They are highly precise, but require training data and are limited to an a priori determined set of titles for which the model is trained. preprocessing including named entity recognition (Finkel et al., 2005), keyphrase extraction (Mihalcea and Tarau, 2004), and chunking (Schmid, 1994) which are then used as features for machine learning. To foster future research, we present two new datasets and compare results on these datasets and the one presented by Branavan et al. (2007). Our research contribution is to develop new algorithms for segment hierarchy identification, to present new evaluation datasets for all subtasks, and to compare our newly developed methods with the state of the art. We also provide a comprehensive analysis of the benefits and shortcomings of the applied methods. Figure 2 gi"
R19-1047,P15-1034,0,0.027341,"all and two kinds of precision in the evaluation in order to account for the feature of minimality. To explain this in more detail does not lie within the scope of this paper. Gashteovski et al. (2017) evaluates OLLIE (Mausam et al., 2012), ClausIE (Del Corro and 2 ReVerb Stanford In pragmatics, hedging is a textual construction that lessens the impact of an utterance. It is often expressed through modal verbs, adjectives, or adverbs. Table 1: Output of Proposition Extraction Systems and Our Two Baselines for the Sentence The waitress smiled at her friend now Gemulla, 2013), and Stanford OIE (Angeli et al., 2015) against their own system. Stanovsky et al. (2018) evaluates ClausIE, PropS (Stanovsky et al., 2016), and Open IE-4 against their new system, that we will call Allen (Stanovsky et al., 2018) herein, using precisionrecall, area under the curve, and F1-score. They compare the individual proposition elements. For a proposition to be judged as correct, the predicate and the syntactic heads of the arguments need to be the same as the gold standard. Saha et al. (2018) evaluate ClausIE, OpenIE-4, and CALMIE (a part of OpenIE) using precision. With the findings of this comparison, they introduce a new"
R19-1047,D16-1003,0,0.0234933,"c heads of the arguments need to be the same as the gold standard. Saha et al. (2018) evaluate ClausIE, OpenIE-4, and CALMIE (a part of OpenIE) using precision. With the findings of this comparison, they introduce a new version of their system, OpenIE-53 , In all described comparisons, the system of the respective authors is the best, which makes sense as it addresses the issue shown by the authors. 2.2 Propositions from Simple Sentences According to Saha et al. (2018) conjunctive sentences are one of the issues in proposition extraction, as conjunctions are a challenge to dependency parsers (Ficler and Goldberg, 2016) which proposition extraction systems are mostly built upon. Hence, Saha et al. (2018) built a system that automatically creates simple sentences from sentences with several conjunctions that are used for proposition extraction. For the proposition extraction of the simple sentences they used ClausIE and OpenIE. They evaluated their data using three different proposition datasets. The correctness of the extracted proposition from the original sentence were evaluated manually. In their study, simple sentences were sentences without conjunctions. 3 400 http://knowitall.github.io/openie/ Quirk (1"
R19-1047,D12-1048,0,0.0518814,"ollowingly no gold standard defining a valid extraction. Systems Table 1 shows the outputs from different systems, our baselines, and our gold standard. In their study, Gashteovski et al. (2017) aim at finding a system with minimal attributes, meaning that hedging2 and attributes expressed e.g. through relative clauses or adjectives, can be optionally removed. Thus, they use recall and two kinds of precision in the evaluation in order to account for the feature of minimality. To explain this in more detail does not lie within the scope of this paper. Gashteovski et al. (2017) evaluates OLLIE (Mausam et al., 2012), ClausIE (Del Corro and 2 ReVerb Stanford In pragmatics, hedging is a textual construction that lessens the impact of an utterance. It is often expressed through modal verbs, adjectives, or adverbs. Table 1: Output of Proposition Extraction Systems and Our Two Baselines for the Sentence The waitress smiled at her friend now Gemulla, 2013), and Stanford OIE (Angeli et al., 2015) against their own system. Stanovsky et al. (2018) evaluates ClausIE, PropS (Stanovsky et al., 2016), and Open IE-4 against their new system, that we will call Allen (Stanovsky et al., 2018) herein, using precisionrecal"
R19-1047,N18-2089,0,0.0747317,"ss of the extracted proposition from the original sentence were evaluated manually. In their study, simple sentences were sentences without conjunctions. 3 400 http://knowitall.github.io/openie/ Quirk (1985) defines a simple sentence as a sentence consisting of exactly one independent clause that does not contain any further clause as one of its elements. Hence, a complex sentence consists of more than one clause. This is also the definition that we use in our study. 2.3 Crowdsourcing Gold Standard Propositions Recent work used crowdsourcing for creating and evaluating proposition extraction (Michael et al., 2018; FitzGerald et al., 2018) in the setting of question answering. In short, they asked their crowdworkers to produce questions and answers in a way that resulted in the extraction of their predicates and arguments, without directly asking for predicate-argument structures. 3 Corpus Creation We create a corpus to evaluate the performance of proposition extraction systems entangled with and disentangled from the task of clause splitting. Our source corpus is the portion of the Aspect Based Sentiment Analysis (ABSA) task (Pontiki et al., 2014) concerned with restaurant reviews within one aspect –"
R19-1047,P18-1191,0,0.0223821,"oposition from the original sentence were evaluated manually. In their study, simple sentences were sentences without conjunctions. 3 400 http://knowitall.github.io/openie/ Quirk (1985) defines a simple sentence as a sentence consisting of exactly one independent clause that does not contain any further clause as one of its elements. Hence, a complex sentence consists of more than one clause. This is also the definition that we use in our study. 2.3 Crowdsourcing Gold Standard Propositions Recent work used crowdsourcing for creating and evaluating proposition extraction (Michael et al., 2018; FitzGerald et al., 2018) in the setting of question answering. In short, they asked their crowdworkers to produce questions and answers in a way that resulted in the extraction of their predicates and arguments, without directly asking for predicate-argument structures. 3 Corpus Creation We create a corpus to evaluate the performance of proposition extraction systems entangled with and disentangled from the task of clause splitting. Our source corpus is the portion of the Aspect Based Sentiment Analysis (ABSA) task (Pontiki et al., 2014) concerned with restaurant reviews within one aspect – service. We use all 423 se"
R19-1047,C18-1326,0,0.0864906,"e waitress The waitress smiled smiled smiled has now smiled at smiled at now smiled at now smiled at smiled at her friend |now at her friend now now friend her friend her friend her friend her friend now |at her friend BL1 BL2 The The waitress waitress smiled smiled at her friend now at her friend now 2.1 Us The waitress smiled at her friend |now Comparison of Proposition Systems Although there have been comparative studies of proposition extraction systems, there has been no extensive study on the impact of sentence complexity on proposition extraction system performance. Comparative Studies Niklaus et al. (2018) presented an overview of proposition extraction systems and classified them into the classic categories of learning-based, rule-based, and clause-based approaches, as well as approaches capturing interpropositional relationships. They described the specific problems each system tackles as well as gaps on the overall evolution of proposition extraction systems. Schneider et al. (2017) present a benchmark for analyzing errors in proposition extraction systems. Their classes are wrong boundaries, redundant extraction, wrong extraction, uninformative extraction, missing extraction, and out of sco"
R19-1047,D17-1278,0,0.0618783,"sses are wrong boundaries, redundant extraction, wrong extraction, uninformative extraction, missing extraction, and out of scope. Their pre-defined classes do not map directly to sentence complexity, although wrong boundaries and out of scope would also be of some interest in an even more detailed error analysis. Furthermore, according to Stanovsky and Dagan (2016) and Niklaus et al. (2018) there are no common guidelines and followingly no gold standard defining a valid extraction. Systems Table 1 shows the outputs from different systems, our baselines, and our gold standard. In their study, Gashteovski et al. (2017) aim at finding a system with minimal attributes, meaning that hedging2 and attributes expressed e.g. through relative clauses or adjectives, can be optionally removed. Thus, they use recall and two kinds of precision in the evaluation in order to account for the feature of minimality. To explain this in more detail does not lie within the scope of this paper. Gashteovski et al. (2017) evaluates OLLIE (Mausam et al., 2012), ClausIE (Del Corro and 2 ReVerb Stanford In pragmatics, hedging is a textual construction that lessens the impact of an utterance. It is often expressed through modal verbs"
R19-1047,D15-1204,0,0.0247854,"s pose difficulties to most systems. 1 Introduction Propositions are predicate-centered tuples consisting of the verb, the subject, and other arguments such as objects and modifiers. For example in Figure 1, “smiled” is the predicate and the other elements are arguments. The first argument is Figure 1: Example Sentence and Extracted Proposition reserved for the role of the subject, in this case “The waitress”, while “at her friend” and “now” are arguments, without further sub-specification. Propositions are used in language understanding tasks such as relation extraction (Riedel et al., 2013; Petroni et al., 2015), information retrieval (L¨oser et al., 2011; Giri et al., 2017), question answering (Khot et al., 2017), word analogy detection (Stanovsky et al., 2015), knowledge base construction (Dong et al., 2014; Stanovsky and Dagan, 2016), summarization (Melli et al., 2006), or other tasks that need comparative operations, such as equality, entailment, or contradiction, on phrases or sentences. The main goal of this paper is to empirically measure the influence of sentence complexity on the performance of proposition extraction systems. Complexity worsens the extraction of dependencies, on which propos"
R19-1047,S14-2004,0,0.112552,"Missing"
R19-1047,P17-2049,0,0.0257338,"f the verb, the subject, and other arguments such as objects and modifiers. For example in Figure 1, “smiled” is the predicate and the other elements are arguments. The first argument is Figure 1: Example Sentence and Extracted Proposition reserved for the role of the subject, in this case “The waitress”, while “at her friend” and “now” are arguments, without further sub-specification. Propositions are used in language understanding tasks such as relation extraction (Riedel et al., 2013; Petroni et al., 2015), information retrieval (L¨oser et al., 2011; Giri et al., 2017), question answering (Khot et al., 2017), word analogy detection (Stanovsky et al., 2015), knowledge base construction (Dong et al., 2014; Stanovsky and Dagan, 2016), summarization (Melli et al., 2006), or other tasks that need comparative operations, such as equality, entailment, or contradiction, on phrases or sentences. The main goal of this paper is to empirically measure the influence of sentence complexity on the performance of proposition extraction systems. Complexity worsens the extraction of dependencies, on which propositions are built. Hence, proposition extraction performance should decrease with increasing sentence com"
R19-1047,W17-6307,0,0.0286691,"tructions that would need long guidelines to create sentences with exactly one proposition. However, our guidelines insured that sentences were reduced in comparison to the original version, if possible. In this way, we are able to create a sufficiently big set of both simple and more complex sentences, as shown in Table 2. Crowdsourcing We used Amazon Turk for crowdsourcing our data. Michael et al. (2018) crowdsourced gold data for evaluating propositions. The sentence reduction performed here and also in Saha et al. (2018) is very similar to syntactic sentence simplification as performed by Lee and Don (2017). We paid 0.04 $ per HIT and 0.01 $ for each further reduced sentence. Each sentence was reduced by 3 workers. In this process, 2181 unique reduced sentences, which are all used in the following corpus creation process, were created from 423 original sentences. Evaluation of Reduced Sentences To measure the quality of the crowdsourced reduced sentences, we chose 100 random reduced sentences together with their original sentence and evalu5 However, this step turned out to be more difficult than expected, as some sentences contained several factors that could be reduced. However, this did not in"
R19-1047,N13-1008,0,0.013408,"of subordinate clauses pose difficulties to most systems. 1 Introduction Propositions are predicate-centered tuples consisting of the verb, the subject, and other arguments such as objects and modifiers. For example in Figure 1, “smiled” is the predicate and the other elements are arguments. The first argument is Figure 1: Example Sentence and Extracted Proposition reserved for the role of the subject, in this case “The waitress”, while “at her friend” and “now” are arguments, without further sub-specification. Propositions are used in language understanding tasks such as relation extraction (Riedel et al., 2013; Petroni et al., 2015), information retrieval (L¨oser et al., 2011; Giri et al., 2017), question answering (Khot et al., 2017), word analogy detection (Stanovsky et al., 2015), knowledge base construction (Dong et al., 2014; Stanovsky and Dagan, 2016), summarization (Melli et al., 2006), or other tasks that need comparative operations, such as equality, entailment, or contradiction, on phrases or sentences. The main goal of this paper is to empirically measure the influence of sentence complexity on the performance of proposition extraction systems. Complexity worsens the extraction of depend"
R19-1047,C18-1194,0,0.02774,"Missing"
R19-1047,H94-1020,0,0.16474,"ms can be improved. If different systems perform well on simple or complex sentences, the complexity distinction could help to identify the complexity of a sentence. The complexity of a sentence would then give a direction towards which system would be better to use. 2 Related Work Proposition are relational tupels extracted from sentences in the form of predicate-argument struc1 https://github.com/MeDarina/review_ propositions 399 Proceedings of Recent Advances in Natural Language Processing, pages 399–408, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_047 tures (Marcus et al., 1994). There are proposition models that further distinguish between the type of arguments. They do not only identify the subject, but more complex roles such as temporal and locational objects or causal clauses. Besides the theory and formalization of proposition, proposition extraction systems have performance issues on real data. Sentence Systems The waitress smiled at her friend now Subject Predicate Other Elements Allen ClausIE OLLIE OpenIE The waitress The waitress The waitress her The waitress waitress waitress The waitress The waitress smiled smiled smiled has now smiled at smiled at now sm"
R19-1047,D16-1252,0,0.102095,"predicate and the other elements are arguments. The first argument is Figure 1: Example Sentence and Extracted Proposition reserved for the role of the subject, in this case “The waitress”, while “at her friend” and “now” are arguments, without further sub-specification. Propositions are used in language understanding tasks such as relation extraction (Riedel et al., 2013; Petroni et al., 2015), information retrieval (L¨oser et al., 2011; Giri et al., 2017), question answering (Khot et al., 2017), word analogy detection (Stanovsky et al., 2015), knowledge base construction (Dong et al., 2014; Stanovsky and Dagan, 2016), summarization (Melli et al., 2006), or other tasks that need comparative operations, such as equality, entailment, or contradiction, on phrases or sentences. The main goal of this paper is to empirically measure the influence of sentence complexity on the performance of proposition extraction systems. Complexity worsens the extraction of dependencies, on which propositions are built. Hence, proposition extraction performance should decrease with increasing sentence complexity. The contribution of this work is threefold a) a gold standard corpus for propositions1 , b) an analysis of propositi"
R19-1047,P15-2050,0,0.0627846,"Missing"
R19-1047,N18-1081,0,0.0135688,"n in order to account for the feature of minimality. To explain this in more detail does not lie within the scope of this paper. Gashteovski et al. (2017) evaluates OLLIE (Mausam et al., 2012), ClausIE (Del Corro and 2 ReVerb Stanford In pragmatics, hedging is a textual construction that lessens the impact of an utterance. It is often expressed through modal verbs, adjectives, or adverbs. Table 1: Output of Proposition Extraction Systems and Our Two Baselines for the Sentence The waitress smiled at her friend now Gemulla, 2013), and Stanford OIE (Angeli et al., 2015) against their own system. Stanovsky et al. (2018) evaluates ClausIE, PropS (Stanovsky et al., 2016), and Open IE-4 against their new system, that we will call Allen (Stanovsky et al., 2018) herein, using precisionrecall, area under the curve, and F1-score. They compare the individual proposition elements. For a proposition to be judged as correct, the predicate and the syntactic heads of the arguments need to be the same as the gold standard. Saha et al. (2018) evaluate ClausIE, OpenIE-4, and CALMIE (a part of OpenIE) using precision. With the findings of this comparison, they introduce a new version of their system, OpenIE-53 , In all descr"
S12-1059,S12-1051,0,0.406442,": Official results on the test data for the top 5 participating runs out of 89 which were achieved on the known datasets MSRpar, MSRvid, and SMTeuroparl, as well as on the surprise datasets OnWN and SMTnews. We report the ranks (#1 : ALL, #2 : ALLnrm, #3 : Mean) and the corresponding Pearson correlation r according to the three offical evaluation metrics (see Sec. 6). The provided baseline is shown at the bottom of this table. metrics ALL (r = .823)4 and Mean (r = .677), and #2 for ALLnrm (r = .857). An exhaustive overview of all participating systems can be found in the STS task description (Agirre et al., 2012). 7 Conclusions and Future Work In this paper, we presented the UKP system, which performed best across the three official evaluation metrics in the pilot Semantic Textual Similarity (STS) task at SemEval-2012. While we did not reach the highest scores on any of the single datasets, our system was most robust across different data. In future work, it would be interesting to inspect the performance of a system that combines the output of all participating systems in a single linear model. We also propose that two major issues with the datasets are tackled in future work: (a) It is unclear how t"
S12-1059,C10-1005,0,0.0431732,"Missing"
S12-1059,P02-1020,0,0.0272336,"antic resources. Further, we employ a lexical substitution system and statistical machine translation to add additional lexemes, which alleviates lexical gaps. Our final models, one per dataset, consist of a log-linear combination of about 20 features, out of the possible 300+ features implemented. 1 Introduction The goal of the pilot Semantic Textual Similarity (STS) task at SemEval-2012 is to measure the degree of semantic equivalence between pairs of sentences. STS is fundamental to a variety of tasks and applications such as question answering (Lin and Pantel, 2001), text reuse detection (Clough et al., 2002) or automatic essay grading (Attali and Burstein, 2006). STS is also closely related to textual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (Dolan et al., 2004). It differs from both tasks, though, insofar as those operate on binary similarity decisions while STS is defined as a graded notion of similarity. STS further requires a bidirectional similarity relationship to hold between a pair of sentences rather than a unidirectional entailment relation as for the TE task. A multitude of measures for computing similarity between texts have been proposed in the past based on su"
S12-1059,C04-1051,0,0.0935903,"per dataset, consist of a log-linear combination of about 20 features, out of the possible 300+ features implemented. 1 Introduction The goal of the pilot Semantic Textual Similarity (STS) task at SemEval-2012 is to measure the degree of semantic equivalence between pairs of sentences. STS is fundamental to a variety of tasks and applications such as question answering (Lin and Pantel, 2001), text reuse detection (Clough et al., 2002) or automatic essay grading (Attali and Burstein, 2006). STS is also closely related to textual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (Dolan et al., 2004). It differs from both tasks, though, insofar as those operate on binary similarity decisions while STS is defined as a graded notion of similarity. STS further requires a bidirectional similarity relationship to hold between a pair of sentences rather than a unidirectional entailment relation as for the TE task. A multitude of measures for computing similarity between texts have been proposed in the past based on surface-level and/or semantic content features (Mihalcea et al., 2006; Landauer et al., 1998; Gabrilovich and Markovitch, 2007). The existing measures exhibit two major limitations,"
S12-1059,W99-0625,0,0.381092,"ifier. Nonetheless, we briefly list them for completeness. Structural similarity between texts can be detected by computing stopword n-grams (Stamatatos, 2011). Thereby, all content-bearing words are removed while stopwords are preserved. Stopword n-grams of both texts are compared using the containment measure (Broder, 1997). In our experiments, we tested n-gram sizes for n = 2, 3, . . . , 10. We also compute part-of-speech n-grams for various POS tags which we then compare using the containment measure and the Jaccard coefficient. We also used two similarity measures between pairs of words (Hatzivassiloglou et al., 1999): Word pair order tells whether two words occur in the same order in both texts (with any number of words in between), word pair distance counts the number of words which lie between those of a given pair. To compare texts along the stylistic dimension, we further use a function word frequencies measure (Dinu and Popescu, 2009) which operates on a set of 70 function words identified by Mosteller and Wallace (1964). Function word frequency vectors are computed and compared by Pearson correlation. We also include a number of measures which capture statistical properties of texts, such as typetok"
S12-1059,O97-1002,0,0.48473,"the original trigram variant to n = 2, 3, . . . , 15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate on a graph-based representation of words and the semantic relations among them within a lexicalsemantic resource. For this system, we used the algorithms by Jiang and Conrath (1997), Lin (1998a), and Resnik (1995) on WordNet (Fellbaum, 1998). In order to scale the resulting pairwise word similarities to the text level, we applied the aggregation strategy by Mihalcea et al. (2006): The sum of the idf -weighted similarity scores of each word with the best-matching counterpart in the other text is computed in both directions, then averaged. In our experiments, the measure by Resnik (1995) proved to be superior to the other measures and was used in all word similarity settings throughout this paper. Explicit Semantic Analysis We also used the vector space model Explicit Sema"
S12-1059,P07-2045,0,0.00189328,"n cardinal numbers (CD) was selected in the final models. 2.3 Text Expansion Mechanisms Lexical Substitution System We used the lexical substitution system based on supervised word sense disambiguation (Biemann, 2012). This system automatically provides substitutions for a set of about 1,000 frequent English nouns with high precision. For each covered noun, we added the substitutions to the text and computed the pairwise word similarity for the texts as described above. This feature alleviates the lexical gap for a subset of words. Statistical Machine Translation We used the Moses SMT system (Koehn et al., 2007) to translate the original English texts via three bridge languages (Dutch, German, Spanish) back to English. Thereby, the idea was that in the translation process additional lexemes are introduced which alleviate potential lexical gaps. The system was trained on Europarl made available by Koehn (2005), using the following configuration which was not optimized for this task: WMT112 baseline without tuning, with MGIZA alignment. The largest improvement was reached for computing pairwise word similarity (as described above) on the concatenation of the original text and the three back-translation"
S12-1059,2005.mtsummit-papers.11,0,0.00230974,"nouns with high precision. For each covered noun, we added the substitutions to the text and computed the pairwise word similarity for the texts as described above. This feature alleviates the lexical gap for a subset of words. Statistical Machine Translation We used the Moses SMT system (Koehn et al., 2007) to translate the original English texts via three bridge languages (Dutch, German, Spanish) back to English. Thereby, the idea was that in the translation process additional lexemes are introduced which alleviate potential lexical gaps. The system was trained on Europarl made available by Koehn (2005), using the following configuration which was not optimized for this task: WMT112 baseline without tuning, with MGIZA alignment. The largest improvement was reached for computing pairwise word similarity (as described above) on the concatenation of the original text and the three back-translations. 2.4 Measures Related to Structure and Style In our system, we also used measures which go beyond content and capture similarity along the structure and style dimensions inherent to texts. However, as we report later on, for this content1 www.wiktionary.org 0-5-grams, grow-diag-final-and alignment, m"
S12-1059,P98-2127,0,0.0293197,"nt to n = 2, 3, . . . , 15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate on a graph-based representation of words and the semantic relations among them within a lexicalsemantic resource. For this system, we used the algorithms by Jiang and Conrath (1997), Lin (1998a), and Resnik (1995) on WordNet (Fellbaum, 1998). In order to scale the resulting pairwise word similarities to the text level, we applied the aggregation strategy by Mihalcea et al. (2006): The sum of the idf -weighted similarity scores of each word with the best-matching counterpart in the other text is computed in both directions, then averaged. In our experiments, the measure by Resnik (1995) proved to be superior to the other measures and was used in all word similarity settings throughout this paper. Explicit Semantic Analysis We also used the vector space model Explicit Semantic Analys"
S12-1059,W01-0515,0,0.305307,"nes a set of shared contiguous substrings, whereby each substring is a match of maximal length. We further used the following measures, which, however, did not make it into the final models, since they were subsumed by the other measures: Jaro (1989), Jaro-Winkler (Winkler, 1990), Monge and Elkan (1997), and Levenshtein (1966). Character/word n-grams We compare character n-grams following the implementation by Barr´onCede˜no et al. (2010), thereby generalizing the original trigram variant to n = 2, 3, . . . , 15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate on a graph-based representation of words and the semantic relations among them within a lexicalsemantic resource. For this system, we used the algorithms by Jiang and Conrath (1997), Lin (1998a), and Resnik (1995) on WordNet (Fellbaum, 1998). In order to scale the resulting pairwise word similarities to the t"
S12-1059,R11-1063,0,0.0123301,"counterpart in the other text is computed in both directions, then averaged. In our experiments, the measure by Resnik (1995) proved to be superior to the other measures and was used in all word similarity settings throughout this paper. Explicit Semantic Analysis We also used the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007). Besides WordNet, we used two additional lexical-semantic resources for the construction of the ESA vector space: Wikipedia and Wiktionary1 . 436 Textual Entailment We experimented with using the BIUTEE textual entailment system (Stern and Dagan, 2011) for generating entailment scores to serve as features for the classifier. However, these features were not selected by the classifier. Distributional Thesaurus We used similarities from a Distributional Thesaurus (similar to Lin (1998b)) computed on 10M dependency-parsed sentences of English newswire as a source for pairwise word similarity, one additional feature per POS tag. However, only the feature based on cardinal numbers (CD) was selected in the final models. 2.3 Text Expansion Mechanisms Lexical Substitution System We used the lexical substitution system based on supervised word sense"
S12-1059,W07-1401,0,\N,Missing
S12-1059,C98-2122,0,\N,Missing
S13-2007,D10-1115,0,0.0186256,"ations on shorter sequences. 3.1 1. the act or state of touching; a touching or meeting, as of two things or people. Background and Description This subtask is based on the assumption that we first need a basic set of functions to compose the meaning of two words, in order to construct more complex models that compositionally determine the meaning of sentences, as a second step. For compositional distributional semantics, the need for these basic functions is discussed in Mitchell and Lapata (2008). Since then, many models have been proposed for addressing the task (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Guevara, 2010), but still comparative analysis is in general based on comparing sequences that consist of two words. As in Zanzotto et al. (2010), this subtask proposes to compare the similarity of a 2-word sequence and a single word. This is important as it is the basic step to analyse models that can compare any word sequences of different length. The development and testing set for this subtask were built based on the idea described in Zanzotto et al. (2010). Dictionaries were used as sources of 41 Figure 1: The definition of contact in a sample dictionary positive training examples. Dict"
S13-2007,S13-2018,0,0.0375986,"Missing"
S13-2007,S13-2016,0,0.0556694,"Missing"
S13-2007,W10-2805,0,0.0217452,"3.1 1. the act or state of touching; a touching or meeting, as of two things or people. Background and Description This subtask is based on the assumption that we first need a basic set of functions to compose the meaning of two words, in order to construct more complex models that compositionally determine the meaning of sentences, as a second step. For compositional distributional semantics, the need for these basic functions is discussed in Mitchell and Lapata (2008). Since then, many models have been proposed for addressing the task (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Guevara, 2010), but still comparative analysis is in general based on comparing sequences that consist of two words. As in Zanzotto et al. (2010), this subtask proposes to compare the similarity of a 2-word sequence and a single word. This is important as it is the basic step to analyse models that can compare any word sequences of different length. The development and testing set for this subtask were built based on the idea described in Zanzotto et al. (2010). Dictionaries were used as sources of 41 Figure 1: The definition of contact in a sample dictionary positive training examples. Dictionaries are nat"
S13-2007,S13-2020,0,0.0615235,"Missing"
S13-2007,P08-1028,0,0.0683415,"s a core problem, since satisfactory performance in computing the similarity of full sentences depends on similarity computations on shorter sequences. 3.1 1. the act or state of touching; a touching or meeting, as of two things or people. Background and Description This subtask is based on the assumption that we first need a basic set of functions to compose the meaning of two words, in order to construct more complex models that compositionally determine the meaning of sentences, as a second step. For compositional distributional semantics, the need for these basic functions is discussed in Mitchell and Lapata (2008). Since then, many models have been proposed for addressing the task (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Guevara, 2010), but still comparative analysis is in general based on comparing sequences that consist of two words. As in Zanzotto et al. (2010), this subtask proposes to compare the similarity of a 2-word sequence and a single word. This is important as it is the basic step to analyse models that can compare any word sequences of different length. The development and testing set for this subtask were built based on the idea described in Zanzotto et al. (2010). Diction"
S13-2007,P03-1048,0,0.0187643,"nts were allowed to use or ignore the training data, i.e. the systems could be supervised or unsupervised. Unsupervised systems were allowed to use the training data for development and parameter tuning. Since this is a core task, participating systems were not be able to use dictionaries or other prefabricated lists. Instead, they were allowed to use distributional similarity models, selectional preferences, measures of semantic similarity etc. Participating system responses were scored in terms of standard information retrieval measures: accuracy (A), precision (P), recall (R) and F1 score (Radev et al., 2003). Systems were encouraged to submit at most 3 solutions for each language, but submissions for fewer languages were accepted. Five research teams participated. Ten system runs were submitted for English, one for German (on data set: German - no names) and one for Italian. Table 2 illustrates the results of the evaluation process. The teams of (HsH) (Wartena, 2013), CLaC (Siblini and Kosseim, 2013), UMCC DLSI-(EPS) (D´avila et al., 2013), and ITNLP, the Harbin Institute of Technology, approached the task in a supervised way, while MELODI (Van de Cruys et al., 2013) participated with two unsuper"
S13-2007,S13-2019,0,0.0932092,"tional preferences, measures of semantic similarity etc. Participating system responses were scored in terms of standard information retrieval measures: accuracy (A), precision (P), recall (R) and F1 score (Radev et al., 2003). Systems were encouraged to submit at most 3 solutions for each language, but submissions for fewer languages were accepted. Five research teams participated. Ten system runs were submitted for English, one for German (on data set: German - no names) and one for Italian. Table 2 illustrates the results of the evaluation process. The teams of (HsH) (Wartena, 2013), CLaC (Siblini and Kosseim, 2013), UMCC DLSI-(EPS) (D´avila et al., 2013), and ITNLP, the Harbin Institute of Technology, approached the task in a supervised way, while MELODI (Van de Cruys et al., 2013) participated with two unsupervised approaches. Interestingly, Language Rank Participant Id 1 3 2 4 5 6 7 8 9 10 HsH CLaC CLaC CLaC MELODI UMCC DLSI-(EPS) ITNLP MELODI ITNLP ITNLP German 1 Italian 1 English run Id A R P rej. R rej. P F1 1 3 2 1 lvw 1 3 dm 1 2 .803 .794 .794 .788 .748 .724 .703 .689 .663 .659 .752 .707 .695 .638 .614 .613 .501 .481 .392 .427 .837 .856 .867 .910 .838 .787 .840 .825 .857 .797 .854 .881 .893 .937"
S13-2007,D11-1094,0,0.0586444,"Missing"
S13-2007,S13-2017,0,0.0241557,"Missing"
S13-2007,S13-2008,0,0.0216165,"milarity models, selectional preferences, measures of semantic similarity etc. Participating system responses were scored in terms of standard information retrieval measures: accuracy (A), precision (P), recall (R) and F1 score (Radev et al., 2003). Systems were encouraged to submit at most 3 solutions for each language, but submissions for fewer languages were accepted. Five research teams participated. Ten system runs were submitted for English, one for German (on data set: German - no names) and one for Italian. Table 2 illustrates the results of the evaluation process. The teams of (HsH) (Wartena, 2013), CLaC (Siblini and Kosseim, 2013), UMCC DLSI-(EPS) (D´avila et al., 2013), and ITNLP, the Harbin Institute of Technology, approached the task in a supervised way, while MELODI (Van de Cruys et al., 2013) participated with two unsupervised approaches. Interestingly, Language Rank Participant Id 1 3 2 4 5 6 7 8 9 10 HsH CLaC CLaC CLaC MELODI UMCC DLSI-(EPS) ITNLP MELODI ITNLP ITNLP German 1 Italian 1 English run Id A R P rej. R rej. P F1 1 3 2 1 lvw 1 3 dm 1 2 .803 .794 .794 .788 .748 .724 .703 .689 .663 .659 .752 .707 .695 .638 .614 .613 .501 .481 .392 .427 .837 .856 .867 .910 .838 .787 .840 ."
S13-2007,C10-1142,1,0.346352,"task is based on the assumption that we first need a basic set of functions to compose the meaning of two words, in order to construct more complex models that compositionally determine the meaning of sentences, as a second step. For compositional distributional semantics, the need for these basic functions is discussed in Mitchell and Lapata (2008). Since then, many models have been proposed for addressing the task (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Guevara, 2010), but still comparative analysis is in general based on comparing sequences that consist of two words. As in Zanzotto et al. (2010), this subtask proposes to compare the similarity of a 2-word sequence and a single word. This is important as it is the basic step to analyse models that can compare any word sequences of different length. The development and testing set for this subtask were built based on the idea described in Zanzotto et al. (2010). Dictionaries were used as sources of 41 Figure 1: The definition of contact in a sample dictionary positive training examples. Dictionaries are natural repositories of equivalences between words under definition and sequences of words used for defining them. Figure 1 presents t"
S13-2007,zesch-etal-2008-extracting,1,0.80164,"Missing"
S13-2048,S12-1059,1,0.886472,"Missing"
S13-2048,C12-1011,1,0.819518,"Missing"
S13-2048,S13-2045,1,0.847683,"tment Technische Universit¨at Darmstadt § Natural Language Processing Lab Computer Science Department Bar-Ilan University Abstract Our system combines text similarity measures with a textual entailment system. In the main task, we focused on the influence of lexicalized versus unlexicalized features, and how they affect performance on unseen questions and domains. We also participated in the pilot partial entailment task, where our system significantly outperforms a strong baseline. 1 Ido Dagan§ Introduction The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge (Dzikovska et al., 2013) brings together two important dimensions of Natural Language Processing: real-world applications and semantic inference technologies. The challenge focuses on the domain of middleschool quizzes, and attempts to emulate the meticulous marking process that teachers do on a daily basis. Given a question, a reference answer, and a student’s answer, the task is to determine whether the student answered correctly. While this is not a new task in itself, the challenge focuses on employing textual entailment technologies as the backbone of this educational application. As a consequence, we formalize"
S13-2048,W01-0515,0,0.0300847,"use semantic similarity measures in order to bridge a possible vocabulary gap between the student and reference answer. We use the ESA measure (Gabrilovich 3 code.google.com/p/dkpro-core-asl/ DKPro Core v1.4.0, TreeTagger models v20130204.0, Stanford parser PCFG model v20120709.0. 5 Using the 750 most frequent n-grams gave good results on the training set, so we also used this number for the test runs. 6 As basic similarity measures, we use greedy string tiling (Wise, 1996) with n = 3, longest common subsequence and longest common substring (Allison and Dix, 1986), and word n-gram containment(Lyon et al., 2001) with n = 2. 4 and Markovitch, 2007) based on concept vectors build from WordNet, Wiktionary, and Wikipedia. Spelling Features As spelling errors might be indicative of the answer quality, we use the number of spelling errors normalized by the text length as an additional feature. Entailment Features We run BIUTEE (Stern and Dagan, 2011) on the test instance (as T ) with each reference answer (as H), which results in an array of numerical entailment confidence values. If there is more than one reference answer, we compute all pairwise confidence scores and add the minimum, maximum, average, an"
S13-2048,R11-1063,1,0.940093,"provide a robust architecture for student response analysis, that can generalize and perform well in multiple domains. Moreover, we are interested in evaluating how well general-purpose technologies will perform in this setting. We therefore approach the challenge by combining two such technologies: DKPro Similarity –an extensive suite of text similarity measures– that has been successfully applied in other settings like the SemEval 2012 task on semantic textual similarity (B¨ar et al., 2012a) or reuse detection (B¨ar et al., 2012b). BIUTEE, the Bar-Ilan University Textual Entailment Engine (Stern and Dagan, 2011), which has shown state-of-the-art performance on recognizing textual entailment challenges. Our systems use both technologies to extract features, and combine them in a supervised model. Indeed, this approach works relatively well (with respect to other entries in the challenge), especially in unseen domains. 2 2.1 Background Text Similarity Text similarity is a bidirectional, continuous function which operates on pairs of texts of any length and returns a numeric score of how similar one text is to the other. In previous work (Mihalcea et al., 285 Second Joint Conference on Lexical and Compu"
S14-1004,R11-1071,1,0.8995,"Missing"
S14-1004,J06-1003,0,0.0796174,"ned by the term frequency in the corresponding dimension, i.e. in a certain Wikipedia article. The similarity of two words is then computed as the inner product (usually the cosine) of the two word vectors. We now show how ESA can be adapted successfully to work on the sense-level, too. Jaguar (animal) .0000 .0000 .0341 Zoo Figure 2: Similarity between senses. sure (Milne, 2007) and Lin (Lin, 1998) as examples of sense-level similarity measures2 and ESA as the prototypical word-level measure.3 The Lin measure is a widely used graph-based similarity measure from a family of similar approaches (Budanitsky and Hirst, 2006; Seco et al., 2004; Banerjee and Pedersen, 2002; Resnik, 1999; Jiang and Conrath, 1997; Grefenstette, 1992). It computes the similarity between two senses based on the information content (IC) of the lowest common subsumer (lcs) and both senses (see Formula 1). simlin = 2 IC(lcs) IC(sense1) + IC(sense2) 2.1 In the standard definintion, ESA computes the term frequency based on the number of times a term—usually a word—appears in a document. In order to make it work on the sense level, we will need a large sense-disambiguated corpus. Such a corpus could be obtained by performing word sense disa"
S14-1004,P92-1052,0,0.0407679,"o words is then computed as the inner product (usually the cosine) of the two word vectors. We now show how ESA can be adapted successfully to work on the sense-level, too. Jaguar (animal) .0000 .0000 .0341 Zoo Figure 2: Similarity between senses. sure (Milne, 2007) and Lin (Lin, 1998) as examples of sense-level similarity measures2 and ESA as the prototypical word-level measure.3 The Lin measure is a widely used graph-based similarity measure from a family of similar approaches (Budanitsky and Hirst, 2006; Seco et al., 2004; Banerjee and Pedersen, 2002; Resnik, 1999; Jiang and Conrath, 1997; Grefenstette, 1992). It computes the similarity between two senses based on the information content (IC) of the lowest common subsumer (lcs) and both senses (see Formula 1). simlin = 2 IC(lcs) IC(sense1) + IC(sense2) 2.1 In the standard definintion, ESA computes the term frequency based on the number of times a term—usually a word—appears in a document. In order to make it work on the sense level, we will need a large sense-disambiguated corpus. Such a corpus could be obtained by performing word sense disambiguating (Agirre and Edmonds, 2006; Navigli, 2009) on all words. However, as this is an error-prone task a"
S14-1004,O97-1002,0,0.190707,"cle. The similarity of two words is then computed as the inner product (usually the cosine) of the two word vectors. We now show how ESA can be adapted successfully to work on the sense-level, too. Jaguar (animal) .0000 .0000 .0341 Zoo Figure 2: Similarity between senses. sure (Milne, 2007) and Lin (Lin, 1998) as examples of sense-level similarity measures2 and ESA as the prototypical word-level measure.3 The Lin measure is a widely used graph-based similarity measure from a family of similar approaches (Budanitsky and Hirst, 2006; Seco et al., 2004; Banerjee and Pedersen, 2002; Resnik, 1999; Jiang and Conrath, 1997; Grefenstette, 1992). It computes the similarity between two senses based on the information content (IC) of the lowest common subsumer (lcs) and both senses (see Formula 1). simlin = 2 IC(lcs) IC(sense1) + IC(sense2) 2.1 In the standard definintion, ESA computes the term frequency based on the number of times a term—usually a word—appears in a document. In order to make it work on the sense level, we will need a large sense-disambiguated corpus. Such a corpus could be obtained by performing word sense disambiguating (Agirre and Edmonds, 2006; Navigli, 2009) on all words. However, as this is"
S14-1004,C12-1108,1,0.926356,"orpus. Such a corpus could be obtained by performing word sense disambiguating (Agirre and Edmonds, 2006; Navigli, 2009) on all words. However, as this is an error-prone task and we are more interested to showcase the overall principle, we rely on Wikipedia as an already manually disambiguated corpus. Wikipedia is a highly linked resource and articles can be considered as senses.4 We extract all links from all articles, with the link target as the term. This approach is not restricted to Wikipedia, but can be applied to any resource containing connections between articles, such as Wiktionary (Meyer and Gurevych, 2012b). Another reason to select Wikipedia as a corpus is that it will allow us to directly compare similarity values with the Wikipedia Link Measure as described above. After this more high-level introduction, we now focus on the mathematical foundation of ESA and disambiguated ESA (called ESA on senses). ESA and ESA on senses count the frequency of each term (or sense) in each document. Table 1 shows the corresponding term-document matrix for the example in Figure 1. The term Jaguar appears in all shown documents, but the term Zoo appears in the articles Dublin Zoo and Wildlife Park.5 A manual a"
S16-1069,W11-1701,0,0.630234,"tection (SemEval 2016 Task 6). We consider the task as a multidimensional classification problem and thus use a sequence of stacked classifiers. For subtask A, we utilize a rich feature set that does not rely on external information such as additional tweets or knowledge bases. For subtask B, we rely on the similarity of tweets in this task with tweets from subtask A in order to transfer the models learnt in subtask A. 2 1 Introduction Stance-taking is an essential and frequently observed part of online debates and other related forms of social media interaction (Somasundaran and Wiebe, 2009; Anand et al., 2011). In the SemEval 2016 Task 6: Detecting Stance in Tweets (Mohammad et al., 2016), stance is defined relative to a given target like a politician or a controversial topic. A text can then either be in favor of the given target (FAVOR), or against it (AGAINST). As the dataset also contains texts without a stance, we additionally have to deal with the the class NONE. Being able to automatically detect and classify stance in social media is important for a deeper understanding of debates and would thus be a great tool for information seekers such as researchers, journalists, customers, users, comp"
S16-1069,P13-4021,1,0.891964,"Missing"
S16-1069,W14-2107,0,0.208721,"Missing"
S16-1069,P14-5011,1,0.856084,"here are about 400-600 manually labeled tweets that can be used for training. As the targets are quite different, we train a separate classifier for each of them. Additionally, we split the three-way classification into a stacked classification, in which we first classify whether the tweet contains any stance (classes FAVOR and AGAINST) or no stance at all (class N ONE). In a second step, we classify the tweets labeled as containing a stance as FAVOR or AGAINST. This sequence of classifications is visualized in Figure 1. All shown classifications are implemented using the DKPro TC framework1 (Daxenberger et al., 2014) and utilize the integrated Weka SVM classifier. 1 version 0.8.0-SNAPSHOT 428 Proceedings of SemEval-2016, pages 428–433, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics Figure 1: Overview on the sequence of stacked classifications that is used for the supervised setting (subtask A) 2.1 Preprocessing framework2 We use the DKPro Core (Eckart de Castilho and Gurevych, 2014) for preprocessing. We apply the twitter-specific tokenizer Twokenizer3 (Gimpel et al., 2011), the DKPro default sentence splitter, and the Arktweet Pos tagger4 (Gimpel et al., 2011)."
S16-1069,W14-5201,0,0.0628447,"Missing"
S16-1069,P11-2008,0,0.146038,"Missing"
S16-1069,S16-1003,0,0.0931009,"ssification problem and thus use a sequence of stacked classifiers. For subtask A, we utilize a rich feature set that does not rely on external information such as additional tweets or knowledge bases. For subtask B, we rely on the similarity of tweets in this task with tweets from subtask A in order to transfer the models learnt in subtask A. 2 1 Introduction Stance-taking is an essential and frequently observed part of online debates and other related forms of social media interaction (Somasundaran and Wiebe, 2009; Anand et al., 2011). In the SemEval 2016 Task 6: Detecting Stance in Tweets (Mohammad et al., 2016), stance is defined relative to a given target like a politician or a controversial topic. A text can then either be in favor of the given target (FAVOR), or against it (AGAINST). As the dataset also contains texts without a stance, we additionally have to deal with the the class NONE. Being able to automatically detect and classify stance in social media is important for a deeper understanding of debates and would thus be a great tool for information seekers such as researchers, journalists, customers, users, companies, or governments. In addition, such analysis could help to create summaries"
S16-1069,P09-1026,0,0.0358971,"ed task on automated stance detection (SemEval 2016 Task 6). We consider the task as a multidimensional classification problem and thus use a sequence of stacked classifiers. For subtask A, we utilize a rich feature set that does not rely on external information such as additional tweets or knowledge bases. For subtask B, we rely on the similarity of tweets in this task with tweets from subtask A in order to transfer the models learnt in subtask A. 2 1 Introduction Stance-taking is an essential and frequently observed part of online debates and other related forms of social media interaction (Somasundaran and Wiebe, 2009; Anand et al., 2011). In the SemEval 2016 Task 6: Detecting Stance in Tweets (Mohammad et al., 2016), stance is defined relative to a given target like a politician or a controversial topic. A text can then either be in favor of the given target (FAVOR), or against it (AGAINST). As the dataset also contains texts without a stance, we additionally have to deal with the the class NONE. Being able to automatically detect and classify stance in social media is important for a deeper understanding of debates and would thus be a great tool for information seekers such as researchers, journalists, c"
S16-1069,W10-0214,0,0.154846,"stance taking behavior. Hence, we use the number of sentences starting with if and the occurrence of modal verbs as a feature. As stance-taking behavior may be indicated by the usage of exclamation- and question marks (Anand et al., 2011), we use as a feature the overall counts as well as the count of over-usage like ??? or !?!. Finally, we use as a feature the number of negation markers in a tweet. Stance-lexicon Features For each target, we create a unigram stance lexicon by computing their statistical association with one of the outcome val3 429 ues. This feature is inspired by the work of Somasundaran and Wiebe (2010) who use a subjectivity lexicon that was created by using the statistical association with negative and positive argumentation. We compute the association measure gmean (Evert, 2004) of every unigram towards the two poles (FAVOR/AGAINST or S TANCE/N ONE) and then use the difference between both values as the stance score s. The gmean association of a word x with a polarity class + is computed as: gmean+ (x) = p c+ (x) c+ (x) · c− (x) (1) where c+ (x) is the count of x in + and c− (x) is the count of x in −. Based on the computed stance lexicon, we calculate the final polarity of as the normali"
S16-1069,W14-2715,0,0.144489,"Missing"
S18-2026,P14-5011,1,0.84317,"lculate overlap between the surface forms of assertions. We use the following methods as implemented by DKPro Similarity (B¨ar et al., 2013)3 : (i) unigram overlap expressed by the Jaccard coefficient (Lyon et al., 2001), (ii) greedy string tiling (Wise, 1996), (iii) longest common sub string (Gusfield, 1997). Additionally, we use averaged word embeddings (Bojanowski et al., 2017). Beyond the baselines, we apply two machine learning approaches: a conventional SVM-based classifier and a neural network. The SVM classifier is implemented using LibSVM (Chang and Lin, 2011) as provided by DKProTC (Daxenberger et al., 2014).4 We use a combination of various ngram features, sentiment features (derived from the system by Kiritchenko et al. (2014)5 ), embedding features (averaged embeddings by Bojanowski et al. (2017)) and negation features. We used a linear kernel with C=100 and the nu-SVR 0 Number of Assertions Measuring Judgment Similarity Between Assertions 150 # of Judgments 100 Black Lives Matter Climate Change Creationism in School Foreign Aid Gender Equality Gun Rights Marijuana Mandatory Vaccination Media Bias Obama Care Same-sex Marriage US Electoral System US in the Middle East US Immigration Vegetarian"
S18-2026,S12-1051,0,0.0161181,"ational Semantics (*SEM), pages 214–224 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics are not easily available – e.g. if we want to predict a judgment on a new, unseen assertion. To overcome this limitation, we propose to use methods that consider the texts of the assertions to mimic judgment similarity and have thus the ability to generalize from existing data collections. 3 Figure 1: Overview on the two prediction tasks. Measuring the judgment similarity of two assertions is related to several NLP tasks such as the detection of semantic text similarity (STS) (Agirre et al., 2012), paraphrase recognition (Bhagat and Hovy, 2013), and textual entailment (Dagan et al., 2009). Unlike semantic text similarity, we do not use a notation of similarity based on the intuition of humans, but one that derives from the context of judgments. Hence, we define that the judgment similarity of two assertions is 1 if two assertions are consistently judged the same and are thus interchangeable in the context of our task. There are several reasons why assertions are judged similarly: their text may convey similar semantics such as in the assertions ‘Marijuana alleviates the suffering of ch"
S18-2026,P13-4021,1,0.885096,"Missing"
S18-2026,P16-1150,0,0.071505,"Missing"
S18-2026,E17-1024,0,0.0592228,"Missing"
S18-2026,L18-1403,1,0.831316,"t merges these branches. SNNs have been successfully used to predict text similarity (Mueller and Thyagarajan, 2016; Neculoiu et al., 2016) and match pairs of sentences (e.g. a tweet to reply) (Hu et al., 2014). In our SNN, a branch consists of a layer that translates the assertions into sequences of word embeddings, which is followed by a convolution layer with a filter size of two, max pooling over time layer, and a dense layer. To merge the branches, we calculate the cosine similarity of the extracted vector representations. The SNN was implemented using the deep learning framework deepTC (Horsmann and Zesch, 2018) in conjunction with Keras6 and Tensorflow (Abadi et al., 2016). In order to ensure full reproducibility of our results, the source code for both approaches is publicly available.7 We evaluate all approaches using 10-fold cross validation and calculate Pearson correlation between the prediction and the gold similarity. 5.2 Method SNN SVM Embedding distance Jaccard Greedy string tiling Longest common sub string 7 .61 .58 .07 .07 .06 .05 Table 2: Pearson correlation (averaged over all issues) of text-based approaches for approximating similarity of assertion judgments.5 Issue SVM SNN Climate Cha"
S18-2026,Q17-1010,0,0.0593545,"er self-pairing. 5.1 Total 200 Table 1: Issues and number of crowdsourced assertions and judgments. 50 −1.0 −0.5 0.0 0.5 Experimental Setup As baselines for this task, we utilize wellestablished semantic text similarity (STS) methods that calculate overlap between the surface forms of assertions. We use the following methods as implemented by DKPro Similarity (B¨ar et al., 2013)3 : (i) unigram overlap expressed by the Jaccard coefficient (Lyon et al., 2001), (ii) greedy string tiling (Wise, 1996), (iii) longest common sub string (Gusfield, 1997). Additionally, we use averaged word embeddings (Bojanowski et al., 2017). Beyond the baselines, we apply two machine learning approaches: a conventional SVM-based classifier and a neural network. The SVM classifier is implemented using LibSVM (Chang and Lin, 2011) as provided by DKProTC (Daxenberger et al., 2014).4 We use a combination of various ngram features, sentiment features (derived from the system by Kiritchenko et al. (2014)5 ), embedding features (averaged embeddings by Bojanowski et al. (2017)) and negation features. We used a linear kernel with C=100 and the nu-SVR 0 Number of Assertions Measuring Judgment Similarity Between Assertions 150 # of Judgmen"
S18-2026,W16-2815,0,0.025114,"there are effects such as the author’s followers affecting the visibility of posts and thereby the likelihood of a like or a retweet (Suh et al., 2010). In addition, we relate to works that aim at predicting whether two texts (Menini and Tonelli, 2016) or sequences of utterances (Wang and Cardie, 2014; Celli et al., 2016) express agreement or disagreement with each other. More broadly, we also relate to works that analyze stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016), or arguments (Habernal and ˇ Gurevych, 2016; Boltuzic and Snajder, 2016; BarHaim et al., 2017) that are expressed via text. In contrast to these works, we do not examine what judgment, sentiment, or claim is expressed by a text, but whether we can infer agreement or disagreement based on judgments which were made on other assertions. Finally, we relate to work on analyzing and predicting outcomes of congressional roll-call voting. These works constantly find that votes of politicians can be explained by a low number of underlying, ideological dimensions such as being left or right (Heckman and Snyder, 1996; Poole and Rosenthal, 1997, 2001). Our work is different"
S18-2026,W16-4312,0,0.0214649,"iffers significantly from our task: even if one agrees with a text, one might decide not to retweet or like it for any number of reasons. There are also cases in which one may retweet a post with which one disagrees in order to flag someone or something from the opposing community. Furthermore, there are effects such as the author’s followers affecting the visibility of posts and thereby the likelihood of a like or a retweet (Suh et al., 2010). In addition, we relate to works that aim at predicting whether two texts (Menini and Tonelli, 2016) or sequences of utterances (Wang and Cardie, 2014; Celli et al., 2016) express agreement or disagreement with each other. More broadly, we also relate to works that analyze stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016), or arguments (Habernal and ˇ Gurevych, 2016; Boltuzic and Snajder, 2016; BarHaim et al., 2017) that are expressed via text. In contrast to these works, we do not examine what judgment, sentiment, or claim is expressed by a text, but whether we can infer agreement or disagreement based on judgments which were made on other assertions. Finally, we relate to work on an"
S18-2026,W01-0515,0,0.174909,"two assertions. We calculate the gold similarity between all unique pairs (e.g. we do not use both a1 with a2 and a2 with a1 ) in our data and do not consider self-pairing. 5.1 Total 200 Table 1: Issues and number of crowdsourced assertions and judgments. 50 −1.0 −0.5 0.0 0.5 Experimental Setup As baselines for this task, we utilize wellestablished semantic text similarity (STS) methods that calculate overlap between the surface forms of assertions. We use the following methods as implemented by DKPro Similarity (B¨ar et al., 2013)3 : (i) unigram overlap expressed by the Jaccard coefficient (Lyon et al., 2001), (ii) greedy string tiling (Wise, 1996), (iii) longest common sub string (Gusfield, 1997). Additionally, we use averaged word embeddings (Bojanowski et al., 2017). Beyond the baselines, we apply two machine learning approaches: a conventional SVM-based classifier and a neural network. The SVM classifier is implemented using LibSVM (Chang and Lin, 2011) as provided by DKProTC (Daxenberger et al., 2014).4 We use a combination of various ngram features, sentiment features (derived from the system by Kiritchenko et al. (2014)5 ), embedding features (averaged embeddings by Bojanowski et al. (2017)"
S18-2026,S16-1003,1,0.884562,"Missing"
S18-2026,P14-1017,0,0.0275364,"s low semantic text similarity. In addition, two assertions can also have a strong judgment similarity because of underlying socio-cultural, political, or personal factors. For instance, the assertions ‘Consuming Marijuana has no impact on your success at work’ and ‘Marijuana is not addictive’ describe different arguments for legalizing marijuana, but judgments made on these assertions are often correlated. Our work also relates to other attempts on predicting reactions to text, such as predicting the number of retweets (Suh et al., 2010; Petrovic et al., 2011), the number of likes on tweets (Tan et al., 2014), the number of karma points of reddit posts (Wei et al., 2016), or sales from product descriptions (Pryzant et al., 2017). What those dicted using a Siamese neural network, which outperforms all other approaches by a wide margin. 2 Related Work Predicting Judgments In order to predict if someone will agree with an assertion, we need knowledge about that person. Ideally, we would have access to a large set of other assertions which the person has already judged. We could then measure the similarity between previous assertions and the new assertion and hypothesize that the judgment on the new a"
S18-2026,W16-1617,0,0.0334474,"0 version 1.0 5 The NRC-Canada system ranked first in the SemEval 2013 (Nakov et al., 2013) and 2014 (Rosenthal et al., 2014) tasks on sentiment analysis. 4 217 regression model. Iterative experiments showed that this configuration gave the most stable results across the issues. For the neural approach, we adapt Siamese neural networks (SNN), which consist of two identical branches or sub-networks that try to extract useful representations of the assertions and a final layer that merges these branches. SNNs have been successfully used to predict text similarity (Mueller and Thyagarajan, 2016; Neculoiu et al., 2016) and match pairs of sentences (e.g. a tweet to reply) (Hu et al., 2014). In our SNN, a branch consists of a layer that translates the assertions into sequences of word embeddings, which is followed by a convolution layer with a filter size of two, max pooling over time layer, and a dense layer. To merge the branches, we calculate the cosine similarity of the extracted vector representations. The SNN was implemented using the deep learning framework deepTC (Horsmann and Zesch, 2018) in conjunction with Keras6 and Tensorflow (Abadi et al., 2016). In order to ensure full reproducibility of our re"
S18-2026,W14-2617,0,0.0249449,"of popularity, which differs significantly from our task: even if one agrees with a text, one might decide not to retweet or like it for any number of reasons. There are also cases in which one may retweet a post with which one disagrees in order to flag someone or something from the opposing community. Furthermore, there are effects such as the author’s followers affecting the visibility of posts and thereby the likelihood of a like or a retweet (Suh et al., 2010). In addition, we relate to works that aim at predicting whether two texts (Menini and Tonelli, 2016) or sequences of utterances (Wang and Cardie, 2014; Celli et al., 2016) express agreement or disagreement with each other. More broadly, we also relate to works that analyze stance (Mohammad et al., 2016; Xu et al., 2016; Taul´e et al., 2017), sentiment (Pang and Lee, 2008; Liu, 2012; Mohammad, 2016), or arguments (Habernal and ˇ Gurevych, 2016; Boltuzic and Snajder, 2016; BarHaim et al., 2017) that are expressed via text. In contrast to these works, we do not examine what judgment, sentiment, or claim is expressed by a text, but whether we can infer agreement or disagreement based on judgments which were made on other assertions. Finally, we"
S18-2026,P16-2032,0,0.0248283,"also have a strong judgment similarity because of underlying socio-cultural, political, or personal factors. For instance, the assertions ‘Consuming Marijuana has no impact on your success at work’ and ‘Marijuana is not addictive’ describe different arguments for legalizing marijuana, but judgments made on these assertions are often correlated. Our work also relates to other attempts on predicting reactions to text, such as predicting the number of retweets (Suh et al., 2010; Petrovic et al., 2011), the number of likes on tweets (Tan et al., 2014), the number of karma points of reddit posts (Wei et al., 2016), or sales from product descriptions (Pryzant et al., 2017). What those dicted using a Siamese neural network, which outperforms all other approaches by a wide margin. 2 Related Work Predicting Judgments In order to predict if someone will agree with an assertion, we need knowledge about that person. Ideally, we would have access to a large set of other assertions which the person has already judged. We could then measure the similarity between previous assertions and the new assertion and hypothesize that the judgment on the new assertion should be the same as for a highly similar one. In Fig"
S18-2026,L18-1224,1,0.842854,"they wanted. On average each assertion is judged by about 45 persons and each participant judged over 400 assertions. For each person, agreement is encoded with 1, disagreement with −1, and missing values with 0 (as not all subjects judged all assertions). Additionally, we can also compute the aggregated agreement score for each assertion by simply subtracting the percentage of participants that disagreed with the assertion from the Data Collection For exploring how well the two tasks can be solved automatically, we use the dataset Nuanced Assertions on Controversial Issues (NAoCI) created by Wojatzki et al. (2018). The dataset contains assertions judged on a wide range of controversial issues.2 The NAoCI dataset mimics a common situation in many social media sites, where people e.g. up- or downvote social media posts. However, it does not have the experimental problems 2 The dataset is accessible from https://sites. google.com/view/you-on-issues/ 216 Issue 135 142 129 150 130 145 138 134 133 154 148 175 138 130 128 134 6 154 6 473 5 747 6 866 5 969 6 423 6 200 5 962 5 877 6 940 6 899 7 695 6 280 5 950 5 806 5 892 2 243 101 133 5 As mentioned above, we want to predict judgments on a previously unseen as"
S18-2026,S14-2009,0,0.028532,"icates that the participants more often agree with the assertions than they disagree. Consequently, baselines accounting for this imbalance perform strongly in predicting judgments on assertions. However, the distribution corresponds to what we observe in many social network sites, where e.g. the ratio of likes to dislikes is also clearly skewed towards likes. All data, the used questionnaires along with the directions and examples are publicly available on the project website.2 3 version 2.2.0 version 1.0 5 The NRC-Canada system ranked first in the SemEval 2013 (Nakov et al., 2013) and 2014 (Rosenthal et al., 2014) tasks on sentiment analysis. 4 217 regression model. Iterative experiments showed that this configuration gave the most stable results across the issues. For the neural approach, we adapt Siamese neural networks (SNN), which consist of two identical branches or sub-networks that try to extract useful representations of the assertions and a final layer that merges these branches. SNNs have been successfully used to predict text similarity (Mueller and Thyagarajan, 2016; Neculoiu et al., 2016) and match pairs of sentences (e.g. a tweet to reply) (Hu et al., 2014). In our SNN, a branch consists"
S19-2078,L18-1008,0,0.0415718,"is composed can be an indication for hate speech. For instance, we assume that tweets that have a strong positive sentiment are rarely hate speech. To measure the overall sentiment of tweets, we use the tool by Socher et al. (2013) to compute a sentiment score for each tweet. The computed sentiment score uses a five-degree scale from very positive to very negative. Word embeddings We use pre-trained word embeddings to enhance our tweet representation with a semantic component. For computing semantic features, we first average the 300dimensional (Spanish or English) word embeddings provided by Mikolov et al. (2018) of all words of a tweet. Next, we use every dimension of the averaged vector as a feature. Feature Engineering Approaches We now report on those approaches that are based on traditional machine learning algorithms and that represent the train and test instances using manually crafted and engineered features. The explored machine learning algorithms are: SVM (LibSVM by Chang and Lin (2011), XGBoost (Chen and Guestrin, 2016), RandomForest (Witten et al., 2016) and Vowpal Wabbit.1 We implement the classifiers using the text classification framework DKPro TC (Daxenberger et al., 2014) which inclu"
S19-2078,N13-1039,0,0.0656716,"Missing"
S19-2078,W17-1101,0,0.0131559,"s to the Spanish data. We now first briefly describe the provided data and then discuss the prediction approaches in more detail. Introduction Hateful, abusive, or offending statements which target individuals or groups on the basis of characteristics such as gender, nationality, or sexual orientation are called hate speech (Basile et al., 2019). Social media is particularly affected by hate speech, as it is known to poison the communication climate, build up negative sentiment towards groups of people, or even lead to reallife consequences (Warner and Hirschberg, 2012; Waseem and Hovy, 2016; Schmidt and Wiegand, 2017; Wojatzki et al., 2018; Benikova et al., 2017; Ross et al., 2017). In this work, we present our submission to the SemEval 2019 Task 5: Multilingual Detection of Hate (Subtask A) for English and Spanish. The objective in Subtask A was to build a system which is able to predict whether given tweets in English or in Spanish are hateful or not hateful towards women or immigrants. We develop a hate speech detection system by experimenting with a range of classifiers which are either based on engineered features or on neural network architectures. We systematically compare the performance of these"
S19-2078,D13-1170,0,0.0028034,"ve or normalize social media specific phenomena such as @-mentions, #-hashtags, URLs, and emojis as we hypothesize that these phenomena may provide useful signals for classification. For example, it is conceivable that a reference to the twitter-handle of Donald Trump (@realDonaldTrump) may indicate hatred towards immigrants. 2.1 Sentiment We also suspect that the tone in which a tweet is composed can be an indication for hate speech. For instance, we assume that tweets that have a strong positive sentiment are rarely hate speech. To measure the overall sentiment of tweets, we use the tool by Socher et al. (2013) to compute a sentiment score for each tweet. The computed sentiment score uses a five-degree scale from very positive to very negative. Word embeddings We use pre-trained word embeddings to enhance our tweet representation with a semantic component. For computing semantic features, we first average the 300dimensional (Spanish or English) word embeddings provided by Mikolov et al. (2018) of all words of a tweet. Next, we use every dimension of the averaged vector as a feature. Feature Engineering Approaches We now report on those approaches that are based on traditional machine learning algori"
S19-2078,W12-2103,0,0.0533314,"dataset and applied the best performing system as-is to the Spanish data. We now first briefly describe the provided data and then discuss the prediction approaches in more detail. Introduction Hateful, abusive, or offending statements which target individuals or groups on the basis of characteristics such as gender, nationality, or sexual orientation are called hate speech (Basile et al., 2019). Social media is particularly affected by hate speech, as it is known to poison the communication climate, build up negative sentiment towards groups of people, or even lead to reallife consequences (Warner and Hirschberg, 2012; Waseem and Hovy, 2016; Schmidt and Wiegand, 2017; Wojatzki et al., 2018; Benikova et al., 2017; Ross et al., 2017). In this work, we present our submission to the SemEval 2019 Task 5: Multilingual Detection of Hate (Subtask A) for English and Spanish. The objective in Subtask A was to build a system which is able to predict whether given tweets in English or in Spanish are hateful or not hateful towards women or immigrants. We develop a hate speech detection system by experimenting with a range of classifiers which are either based on engineered features or on neural network architectures. W"
S19-2078,P14-5011,1,0.834115,"ngs provided by Mikolov et al. (2018) of all words of a tweet. Next, we use every dimension of the averaged vector as a feature. Feature Engineering Approaches We now report on those approaches that are based on traditional machine learning algorithms and that represent the train and test instances using manually crafted and engineered features. The explored machine learning algorithms are: SVM (LibSVM by Chang and Lin (2011), XGBoost (Chen and Guestrin, 2016), RandomForest (Witten et al., 2016) and Vowpal Wabbit.1 We implement the classifiers using the text classification framework DKPro TC (Daxenberger et al., 2014) which includes all of the abovementioned classifiers. We use the following features to represent the tweets: 2.2 Besides traditional machine learning approach, we also experiment with neural network architectures: multilayer perceptrons (MLP), convolutional neural networks (CNN), bi-directional LSTMs and a combination of LSTMs and CNNs (LSTM + CNN). We initialize all setups with the 300-dimensional word embeddings provided by Mikolov et al. (2018), which were trained on the common crawl corpus. Furthermore, in all setups, we use a dropout of 0.25 after the embedding layer and update network w"
S19-2078,N16-2013,0,0.0163939,"performing system as-is to the Spanish data. We now first briefly describe the provided data and then discuss the prediction approaches in more detail. Introduction Hateful, abusive, or offending statements which target individuals or groups on the basis of characteristics such as gender, nationality, or sexual orientation are called hate speech (Basile et al., 2019). Social media is particularly affected by hate speech, as it is known to poison the communication climate, build up negative sentiment towards groups of people, or even lead to reallife consequences (Warner and Hirschberg, 2012; Waseem and Hovy, 2016; Schmidt and Wiegand, 2017; Wojatzki et al., 2018; Benikova et al., 2017; Ross et al., 2017). In this work, we present our submission to the SemEval 2019 Task 5: Multilingual Detection of Hate (Subtask A) for English and Spanish. The objective in Subtask A was to build a system which is able to predict whether given tweets in English or in Spanish are hateful or not hateful towards women or immigrants. We develop a hate speech detection system by experimenting with a range of classifiers which are either based on engineered features or on neural network architectures. We systematically compar"
S19-2078,N18-1095,0,0.0156553,"nt the tweets using word and character n-grams. We experiment with n-gram sizes in the range from 1-3 for word n-grams and 2-5 for character n-grams. To reduce the feature space, we only use the ngrams that are most common in the (English and Spanish) training data. We experiment with the frequency cut-off values of 200, 500 and 1,000. Hateword lists We hypothesize that the presence of specific hate or insult words gives an indication of whether a tweet constitutes hate speech. Hence, we check if the words in the tweets occur in lists of hate or insult words. We use the word lists provided by Wiegand et al. (2018), which contain a basic word list and a extended word list. 1 Neural Network Approaches MLP Besides the final softmax layer, our MLP has a total of 6 densely connected layers. Starting from the input, the layers have 256, 128, 64, 32, 16 and 8 nodes. We use relu as activation function in all layers. https://github.com/VowpalWabbit/vowpal wabbit 442 CNN Our CNN uses three stacked convolutional layers that use a filter size of two. The first layer has 128 nodes, the second 64 and the third 32. Subsequently, we apply max pooling, a dense layer with ten nodes and the final softmax classification l"
S19-2078,L18-1403,1,0.85444,"n layer. macro-F1 LSTM At the core of our LSTM is a bidirectional LSTM layer with 128 nodes. This layer is followed by two dense layers (40 and 10 nodes) and the softmax layer. LibSVM 0.780 wn=1 / topk=1000 cn=2-4 / topk=200 Random Forest 0.771 wn=1-2 / topk=500 cn=2-4 / topk=1000 XGBoost 0.764 wn=1-2 / topk=1000 cn=2-5 / topk=1000 Vowpal Wabbit 0.742 wn=1-3 / topk=1000 cn=2 / topk=200 Table 1: Results for Fine-tuned n-gram Features. LSTM + CNN For the combination of LSTM and CNN, we put our CNN model on top of LSTM model. All of the above-described architectures are implemented using deepTC (Horsmann and Zesch, 2018) with the Keras (Chollet et al., 2015) and Tensorflow (Abadi et al., 2015) backend. BERT We also experiment with Bidirectional Encoder Representations from Transformers (BERT), which recently excelled in a number of NLP tasks (Devlin et al., 2018). For our experiments, we use the provided pre-trained multilingual-cased BERT-Base model,2 a maximum sequence-length of 128 and batches of 32 instances. In the described configuration, BERT yields an accuracy of 0.66 after fine-tuning for the second time. As we observe that the performance of BERT begins to decrease from the third fine-tuning, we do"
S19-2121,W12-2103,0,0.124876,"urtful online postings is investigated under a variety of names. Waseem et al. (2017) focuses on abusive language, Kumar et al. (2018) tackles the problem as aggression while Macbeth et al. (2013) approaches this problem as cyberbullying to mention just a few. Furthermore, the field of hate speech detection is strongly related, which aims at detecting a similar kind of online statements (Waseem and Hovy, 2016; Wojatzki et al., 2018). Common approaches to detecting such socially unacceptable statements utilize rich feature sets consisting of word ngrams, surface forms and syntactical features (Warner and Hirschberg, 2012; Nobata et al., 2016). Human-knowledge is provided by word lists containing offenses as key words or phrases (Bassignana et al., 2018; Wiegand et al., 2018b). Xiang et al. (2012) approaches the task as topic modelling problem using Latent Dirichlet Allocation (Blei et al., 2003). These tasks are tackled with feature engineering-based approaches such as SVM or regression models but also with convolutional neural networks (Wiegand et al., 2018b). The Internet is frequently used for online debates and discussions, where individuals or groups are increasingly often verbally attacked. Online platf"
S19-2121,W17-3012,0,0.0956796,"ial test dataset. This paper describes LTL-UDE’s systems for the SemEval 2019 Shared Task 6. We present results for Subtask A and C. In Subtask A, we experiment with an embedding representation of postings and use a Multi-Layer Perceptron and BERT to categorize postings. Our best result reaches the 10th place (out of 103) using BERT. In Subtask C, we applied a two-vote classification approach with minority fallback, which is placed on the 19th rank (out of 65). 1 2 Introduction Related Work Detection of offensive or potentially hurtful online postings is investigated under a variety of names. Waseem et al. (2017) focuses on abusive language, Kumar et al. (2018) tackles the problem as aggression while Macbeth et al. (2013) approaches this problem as cyberbullying to mention just a few. Furthermore, the field of hate speech detection is strongly related, which aims at detecting a similar kind of online statements (Waseem and Hovy, 2016; Wojatzki et al., 2018). Common approaches to detecting such socially unacceptable statements utilize rich feature sets consisting of word ngrams, surface forms and syntactical features (Warner and Hirschberg, 2012; Nobata et al., 2016). Human-knowledge is provided by wor"
S19-2121,N16-2013,0,0.0358542,"ing BERT. In Subtask C, we applied a two-vote classification approach with minority fallback, which is placed on the 19th rank (out of 65). 1 2 Introduction Related Work Detection of offensive or potentially hurtful online postings is investigated under a variety of names. Waseem et al. (2017) focuses on abusive language, Kumar et al. (2018) tackles the problem as aggression while Macbeth et al. (2013) approaches this problem as cyberbullying to mention just a few. Furthermore, the field of hate speech detection is strongly related, which aims at detecting a similar kind of online statements (Waseem and Hovy, 2016; Wojatzki et al., 2018). Common approaches to detecting such socially unacceptable statements utilize rich feature sets consisting of word ngrams, surface forms and syntactical features (Warner and Hirschberg, 2012; Nobata et al., 2016). Human-knowledge is provided by word lists containing offenses as key words or phrases (Bassignana et al., 2018; Wiegand et al., 2018b). Xiang et al. (2012) approaches the task as topic modelling problem using Latent Dirichlet Allocation (Blei et al., 2003). These tasks are tackled with feature engineering-based approaches such as SVM or regression models but"
S19-2121,P11-2008,0,0.245838,"Missing"
S19-2121,N18-1095,0,0.0668777,"ssion while Macbeth et al. (2013) approaches this problem as cyberbullying to mention just a few. Furthermore, the field of hate speech detection is strongly related, which aims at detecting a similar kind of online statements (Waseem and Hovy, 2016; Wojatzki et al., 2018). Common approaches to detecting such socially unacceptable statements utilize rich feature sets consisting of word ngrams, surface forms and syntactical features (Warner and Hirschberg, 2012; Nobata et al., 2016). Human-knowledge is provided by word lists containing offenses as key words or phrases (Bassignana et al., 2018; Wiegand et al., 2018b). Xiang et al. (2012) approaches the task as topic modelling problem using Latent Dirichlet Allocation (Blei et al., 2003). These tasks are tackled with feature engineering-based approaches such as SVM or regression models but also with convolutional neural networks (Wiegand et al., 2018b). The Internet is frequently used for online debates and discussions, where individuals or groups are increasingly often verbally attacked. Online platform providers aim to remove such attacking posts or ideally, prevent them from being published. Manual verification of each posting by a human moderator is"
S19-2121,W18-4401,0,0.0668234,"Missing"
S19-2121,L18-1008,0,0.0132333,"aining more than 1,300 English tokens, (ii) UdS Lexicon of Abusive Words2 having 1,651 entries (Wiegand et al., 2018a), and (iii) Multilingual Lexicon of Words to Hurt from HurtLex (Bassignana et al., 2018) with 9,313 terms.3 A posting is classified as offensive if it contains any words in the before mentioned lists. Posting Embeddings We represent each posting by a dense embedding, which we create from word embeddings by summing up the vector values of the word representations. The resulting posting vector is re-scaled into the range zero to one. We use the pre-trained embeddings provided by Mikolov et al. (2018), which are trained on the common crawl corpus. Table 1: Subtask A: Results in term of macro F1 on a held-back development dataset containing 1,048 offensive postings and 2,192 not offensive (NOT) ones. which we expect to improve model performance. We translated the data into Russian, Chinese, and Arabic and back to English using Google’s translation service. We repeated the fine-tuning with this enriched dataset. Classifiers We apply the following classifiers: SVM (Chang and Lin, 2011), Logistic Regression (Fan et al., 2008), Random Forest (Breiman, 2001) and a Decision Tree (Breiman et al.,"
S19-2121,N19-1144,0,0.0292502,"d for online debates and discussions, where individuals or groups are increasingly often verbally attacked. Online platform providers aim to remove such attacking posts or ideally, prevent them from being published. Manual verification of each posting by a human moderator is infeasible due to the high amount of postings created every day. Consequently, automated detection of such attacking postings is the only feasible way to counter this kind of hostility. In this work, we present our results for the SemEval 2019 Shared Task 6: Identifying and Categorizing Offensive Language in Social Media (Zampieri et al., 2019b) on the OLID dataset (Zampieri et al., 2019a). Subtask A focuses on the binary distinction if a post is offensive or not, while Subtask C determines if the target is an individual, group, or other entity. Our submission for Subtask A ranks 10th, for Subtask C ranks 19th. For Subtask A, we experiment with word listbased classification, using classifiers such as SVM or logistic regression based on sentence embeddings, and neural network-based models such as a Multi-layer Perceptron (MLP) and Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018). We find that the"
S19-2121,S19-2010,0,0.0266217,"d for online debates and discussions, where individuals or groups are increasingly often verbally attacked. Online platform providers aim to remove such attacking posts or ideally, prevent them from being published. Manual verification of each posting by a human moderator is infeasible due to the high amount of postings created every day. Consequently, automated detection of such attacking postings is the only feasible way to counter this kind of hostility. In this work, we present our results for the SemEval 2019 Shared Task 6: Identifying and Categorizing Offensive Language in Social Media (Zampieri et al., 2019b) on the OLID dataset (Zampieri et al., 2019a). Subtask A focuses on the binary distinction if a post is offensive or not, while Subtask C determines if the target is an individual, group, or other entity. Our submission for Subtask A ranks 10th, for Subtask C ranks 19th. For Subtask A, we experiment with word listbased classification, using classifiers such as SVM or logistic regression based on sentence embeddings, and neural network-based models such as a Multi-layer Perceptron (MLP) and Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018). We find that the"
W06-1104,I05-1067,1,0.724261,", word sense disambiguation, text summarization or spelling correction. It is defined on different kinds of textual units, e.g. documents, parts of a document (e.g. words and their surrounding context), words or concepts (Lebart and Rajman, 2000).2 Linguistic distance between words is inverse to their semantic similarity or relatedness. Semantic similarity is typically defined via the lexical relations of synonymy (automobile – car) and hypernymy (vehicle – car), while semantic relatedness (SR) is defined to cover any kind of lexical or functional association that may exist between two words (Gurevych, 2005).3 Dissimilar words can be semantically related, e.g. via functional relationships (night – dark) or when they are antonyms (high – low). Many NLP applications require knowledge about semantic relatedness rather than just similarity (Budanitsky and Hirst, 2006). A number of competing approaches for computing semantic relatedness of words have been developed (see Section 2). A commonly accepted method for evaluating these approaches is to compare their results with a gold standard based on human judgments on word pairs. For that purpose, relatedness scores for each word pair have to be determin"
W06-1104,O97-1002,0,0.0826444,"edness in mind. Word pairs consisted only of nounnoun combinations and only general terms were included. Polysemous and homonymous words were not disambiguated to concepts, i.e. humans annotated semantic relatedness of words rather than concepts. is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether"
W06-1104,W04-2607,0,0.0254114,"to cope with domain-specific or technical terms. This is an important property if semantic relatedness is used in information retrieval where users tend to use specific search terms (Porsche) rather than general ones (car). Furthermore, manually selected word pairs are often biased towards highly related pairs (Gurevych, 2006), because human annotators tend to select only highly related pairs connected by relations they are aware of. Automatic corpus-based selection of word pairs is more objective, leading to a balanced dataset with pairs connected by all kinds of lexical-semantic relations. Morris and Hirst (2004) pointed out that many relations between words in a text are non-classical (i.e. other than typical taxonomic relations like synonymy or hypernymy) and therefore not covered by semantic similarity. Previous studies only considered semantic relatedness (or similarity) of words rather than concepts. However, polysemous or homonymous words should be annotated on the level of concepts. If we assume that bank has two meanings (“financial institution” vs. “river bank”)5 and it is paired with money, the result is two sense quali5 4 4.1 Experiment System architecture Figure 1 gives an overview of our"
W06-1104,H05-1077,0,0.03395,"odenough with the original 65 word pairs translated into German. She used an adapted experimental setup where test subjects had to assign discrete values {0,1,2,3,4} and word pairs were presented in isolation. This setup is also scalable to a higher number of word pairs (350) as was shown in Gurevych (2006). Finkelstein et al. (2002) annotated a larger set of word pairs (353), too. They used a 0-10 range of relatedness scores, but did not give further details about their experimental setup. In psycholinguistics, relatedness of words can also be determined through association tests (Schulte im Walde and Melinger, 2005). Results of such experiments are hard to quantify and cannot easily serve as the basis for evaluating SR measures. Rubenstein and Goodenough selected word pairs analytically to cover the whole spectrum of 4 That means, whether it fulfills some mathematical criteria: d(x, y) ≥ 0; d(x, y) = 0 ⇔ x = y; d(x, y) = d(y, x); d(x, z) ≤ d(x, y) + d(y, z). 17 PAPER R/G (1965) M/C (1991) Res (1995) Fin (2002) Gur (2005) Gur (2006) Z/G (2006) L ANGUAGE English English English English German German German PAIRS 65 30 30 353 65 350 328 POS N N N N, V, A N N, V, A N, V, A R EL - TYPE sim sim sim relat sim r"
W06-1104,J05-4002,0,0.0115492,"nounnoun combinations and only general terms were included. Polysemous and homonymous words were not disambiguated to concepts, i.e. humans annotated semantic relatedness of words rather than concepts. is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematical analysis cannot tell us whether a measure closely resembles human judgments or whether it performs best when used in a certain"
W06-1104,P94-1019,0,0.0168389,"mber of word pairs was manually selected, with semantic similarity instead of relatedness in mind. Word pairs consisted only of nounnoun combinations and only general terms were included. Polysemous and homonymous words were not disambiguated to concepts, i.e. humans annotated semantic relatedness of words rather than concepts. is presented. Section 5 discusses the results, and finally we draw some conclusions in Section 6. 2 Evaluating SR measures Various approaches for computing semantic relatedness of words or concepts have been proposed, e.g. dictionary-based (Lesk, 1986), ontology-based (Wu and Palmer, 1994; Leacock and Chodorow, 1998), information-based (Resnik, 1995; Jiang and Conrath, 1997) or distributional (Weeds and Weir, 2005). The knowledge sources used for computing relatedness can be as different as dictionaries, ontologies or large corpora. According to Budanitsky and Hirst (2006), there are three prevalent approaches for evaluating SR measures: mathematical analysis, applicationspecific evaluation and comparison with human judgments. Mathematical analysis can assess a measure with respect to some formal properties, e.g. whether a measure is a metric (Lin, 1998).4 However, mathematica"
W06-1104,J06-1003,0,\N,Missing
W07-0201,I05-1067,1,0.716974,"d. More problematic is that Fin353 consists of two subsets, which have been annotated by a different number of annotators. We performed further analysis of their dataset and found that the inter-annotator agreement7 differs considerably. These results suggest that further evaluation based on this data should actually regard it as two independent datasets. As Wikipedia is a multi-lingual resource, we are not bound to English datasets. Several German datasets are available that are larger than the existing English datasets and do not share the problems of the Finkelstein datasets (see Table 2). Gurevych (2005) conducted experiments with a German translation of an English dataset (Rubenstein and Goodenough, 1965), but argued that the dataset is too small and only contains noun-noun pairs connected 6 Note that we do not use multiple-choice synonym question datasets (Jarmasz and Szpakowicz, 2003), as this is a different task, which is not addressed in this paper. 7 We computed the correlation for all annotators pairwise and summarized the values using a Fisher Z-value transformation. DATASET RG65 MC30 Res30 Fin353 Y EAR 1965 1991 1995 2002 L ANGUAGE English English English English Gur65 Gur350 ZG222 2"
W07-0201,P94-1019,0,0.0154613,"e, we cannot use the WCG directly to compute SR. Additionally, the WCG would not provide sufficient coverage, as it is relatively small. Thus, transferring SR measures to the WCG requires some modifications. The task of estimating SR between terms is casted to the task of SR between Wikipedia articles devoted to these terms. SR between articles is measured via the categories assigned to these articles. Leacock and Chodorow (1998, LC) normalize the path-length with the depth of the graph, simLC (n1 , n2 ) = − log l(n1 , n2 ) 2 × depth where depth is the length of the longest path in the graph. Wu and Palmer (1994, WP) introduce a measure that uses the notion of a lowest common subsumer of 4 3.2 Adapting SR Measures to Wikipedia X simply delete one of the links running on the same level. This strategy never disconnects any nodes from a connected component. X X+1 X+1 X+1 4 Semantic Relatedness Experiments X X+1 X X OR X+1 X+1 A commonly accepted method for evaluating SR measures is to compare their results with a gold standard dataset based on human judgments on word pairs.6 X+1 X+1 4.1 Figure 3: Breaking cycles in the WCG. We define C1 and C2 as the set of categories assigned to article ai and aj , res"
W07-0201,O97-1002,0,0.0560794,"ber of competing approaches for computing semantic relatedness between words using a graph structure, and then discuss the changes that are necessary to adapt semantic relatedness algorithms to work on the WCG. 3.1 two nodes lcs(n1 , n2 ). In a directed graph, a lcs is the parent of both child nodes with the largest depth in the graph. simW P = 2 depth(lcs) l(n1 , lcs) + l(n2 , lcs) + 2 depth(lcs) Resnik (1995, Res), defines semantic similarity between two nodes as the information content (IC) value of their lcs. He used the relative corpus frequency to estimate the information content value. Jiang and Conrath (1997, JC) additionally use the IC of the nodes. distJC (n1 , n2 ) = IC(n1 ) + IC(n2 ) − 2IC(lcs) Note that JC returns a distance value instead of a similarity value. Lin (1998, Lin) defined semantic similarity using a formula derived from information theory. simLin (n1 , n2 ) = 2 × IC(lcs) IC(n1 ) + IC(n2 ) Because polysemous words may have more than one corresponding node in a semantic wordnet, the resulting semantic relatedness between two words w1 and w2 can be calculated as   min dist(n1 , n2 ) path  n1 ∈s(w1 ),n2 ∈s(w2 ) SR =  max sim(n1 , n2 ) IC  n1 ∈s(w1 ),n2 ∈s(w2 ) Wordnet Based Mea"
W07-0201,W06-1104,1,0.380253,",1,2,3,4} discrete {0,1,2,3,4} discrete {0,1,2,3,4} # S UBJECTS 51 38 10 13/16 13 16 24 8 21 C ORRELATION r I NTER I NTRA .850 .903 .731 .549 .810 .690 .490 .647 Table 2: Comparison of German datasets used for evaluating semantic relatedness. by either synonymy or hyponymy. Thus, she created a larger German dataset containing 350 word pairs (Gur350). It contains nouns, verbs and adjectives that are connected by classical and nonclassical relations (Morris and Hirst, 2004). However, word pairs for this dataset are biased towards strong classical relations, as they were manually selected. Thus, Zesch and Gurevych (2006) used a semi-automatic process to create word pairs from domain-specific corpora. The resulting ZG222 dataset contains 222 word pairs that are connected by all kinds of lexical semantic relations. Hence, it is particularly suited for analyzing the capability of a measure to estimate SR. 0.80 Gur65 0.60 0.40 0.20 0.00 GN Res JC Lin PL WP LC Correlation r 0.75 0.50 0.42 0.51 0.34 0.45 0.35 0.80 Gur350 0.60 4.2 Results and Discussion Figure 4 gives an overview of our experimental results of evaluating SR measures based on the WCG on three German datasets. We use Pearson’s product moment correlati"
W07-0201,N07-2052,1,0.779025,"al., 2006) have not treated them separately. However, the WCG should be treated separately, as it differs from the article graph. Article links are established because of any kind of relation between 3 Wikipedia can be downloaded //download.wikimedia.org/ from http: 2 articles, while links between categories are typically established because of hyponymy or meronymy relations. Holloway et al. (2005) create and visualize a category map based on co-occurrence of categories. Voss (2006) pointed out that the WCG is a kind of thesaurus that combines collaborative tagging and hierarchical indexing. Zesch et al. (2007a) identified the WCG as a valueable source of lexical semantic knowledge, but did not analytically analyze its properties. However, even if the WCG seems to be very similar to other semantic wordnets, a graph-theoretic analysis of the WCG is necessary to substantiate this claim. It is carried out in the next section. 2 Graph-theoretic Analysis of the WCG A graph-theoretic analysis of the WCG is required to estimate, whether graph based semantic relatedness measures developed for semantic wordnets can be transferred to the WCG. This is substantiated in a case study on computing semantic relate"
W07-0201,W04-2607,0,\N,Missing
W07-0201,J06-1003,0,\N,Missing
W11-2842,bird-etal-2008-acl,0,0.0477304,"ach (MDM1991) is based on the noisy-channel model assuming that the correct sentence s is transmitted through a noisy channel adding ‘noise’ which results in a word w being replaced by an error e leading the wrong sentence s0 which we observe. Hence, the probability of the correct word w, given the error e is observed, can be computed using a n-gram language model and a model of how likely the typist is to make a certain error. We use two language models: (i) based on the Google Web1T n-gram data (Brants and Franz, 2006), and (ii) based on all the papers in the ACL Anthology Reference Corpus (Bird et al., 2008). 2.1 Combined Approaches Our framework allows to easily combine spell checkers. In all the combination experiments, we used the MDM1991 with the Google n-gram model. JoinRWSE Only the two approaches targeted towards RWSEs (i.e. BH2005 and MDM1991) are combined. JoinAll All three spell checkers (Jazzy, BH2005, and MDM1991) are run in parallel and detections are joined as if only a single spell checker would have been used. IntersectAll All three spell checkers (Jazzy, BH2005, and MDM1991) are run in parallel, but only errors that are detected by each of the spell checkers are retained. 3 Preli"
W11-2842,O97-1002,0,0.00754464,"d towards RWSEs, we use it for reasons of comparison with other approaches. Detecting RWSEs We re-implemented two stateof-the-art approaches: the knowledge-based approach by Hirst and Budanitsky (2005) (BH2005) and the statistical approach by Mays et al. (1991) (MDM1991). Both approaches test the lexical cohesion of a word with its context. For that purpose, BH2005 computes the semantic relatedness of a target word with all other words in a certain context window to test whether the target word fits its context. Following Hirst and Budanitsky (2005), we use the semantic relatedness measure by Jiang and Conrath (1997) and WordNet (Fellbaum, 1998) as a knowledge source. If a target word does not fit its context, it is flagged as a possible error. Then, the set of valid words with low edit distance to the target word is computed. Each of the words in this set, that better fits into the given context than the target word, is selected as a possible correction. 1 2 http://code.google.com/p/dkpro-core-asl/ http://jazzy.sourceforge.net/ Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 260–262, c Nancy, France, September 2011. 2011 Association for Computational Linguistics P D"
W12-2036,W12-2006,0,0.0634803,"Missing"
W12-2036,P96-1010,0,0.113875,"the state of the art identified by the shared task. 1 2 Introduction UKP Lab already participated in the previous HOO Shared Task in 2011. Our knowledge-based system (Zesch, 2011) was targeted towards detecting realword spelling errors, but performed also well on a number of other error classes.1 However, it was not competitive for article and preposition errors where supervised systems based on confusion sets constitute the state of the art. Thus, we tailor the HOO 2011 system towards correcting article and preposition errors, but also implement a supervised approach based on confusion sets (Golding and Schabes, 1996; Jones and Martin, 1997; Carlson et al., 2001). We decided to implement a basic system that should be as flexible as possible and might serve as a basis for experiments in future rounds of the shared task. We also plan to model the most successful 1 http://clt.mq.edu.au/research/projects/hoo/hoo2011/reports/ hoo2011-UDposter.pdf Supervised Error Detection We implement a generic framework for article and preposition error detection based on the open-source DKPro framework.3 DKPro is a collection of software components for natural language processing based on the Apache UIMA framework (Ferrucci"
W12-2036,A97-1025,0,0.0941855,"fied by the shared task. 1 2 Introduction UKP Lab already participated in the previous HOO Shared Task in 2011. Our knowledge-based system (Zesch, 2011) was targeted towards detecting realword spelling errors, but performed also well on a number of other error classes.1 However, it was not competitive for article and preposition errors where supervised systems based on confusion sets constitute the state of the art. Thus, we tailor the HOO 2011 system towards correcting article and preposition errors, but also implement a supervised approach based on confusion sets (Golding and Schabes, 1996; Jones and Martin, 1997; Carlson et al., 2001). We decided to implement a basic system that should be as flexible as possible and might serve as a basis for experiments in future rounds of the shared task. We also plan to model the most successful 1 http://clt.mq.edu.au/research/projects/hoo/hoo2011/reports/ hoo2011-UDposter.pdf Supervised Error Detection We implement a generic framework for article and preposition error detection based on the open-source DKPro framework.3 DKPro is a collection of software components for natural language processing based on the Apache UIMA framework (Ferrucci and Lally, 2004). It co"
W12-2036,E06-1015,0,0.0394028,"chunk. As the framework allows to easily add new feature extractors, we are going to integrate the most successful features from the shared task. Due to the modular architecture of ClearTK, the implemented feature extractors could even be re-used for other classification tasks unrelated to spelling correction. 2.3 Classification ClearTK provides a wide range of adapters to well known machine learning frameworks and classification tools. As of April 2012, the following adapters are supported: • LIBSVM6 • MALLET7 (McCallum, 2002) • OpenNLP Maxent8 • SVMlight 9 (Joachims, 1999) • SVMlight -TK10 (Moschitti, 2006) • Weka11 (Hall et al., 2009) As we can easily switch the classifier, we tried a wide range of classifiers, but SVM worked generally best. For the official runs, we used SVM as implemented in the Weka toolkit with the parameter “BuildLogisticModels” which allows to base a detection decision on the confidence of the classifier in order to improve precision. 3 Knowledge-based Error Detection Besides the supervised system described above, we also apply our knowledge-based system from the HOO 2011 Pilot Round (Zesch, 2011). We reimplemented two state-of-the-art approaches: the 5 code.google.com/p/"
W12-2036,C04-1024,0,0.0168483,"s), (iii) generating performance reports, and (iv) storing all experimental results in a convenient manner. 2 We invite other participating teams to help with this effort. 3 http://code.google.com/p/dkpro-core-asl/ 302 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 302–306, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics 2.1 Linguistic Preprocessing For our basic implementation, we only use a few preprocessing steps. We tokenize and sentence split the data with the default DKPro segmenter, and then use TreeTagger (Schmid, 2004) to POS-tag and chunk the sentences. However, the framework allows the effortless addition of other preprocessing components, e.g. parsing or named-entity recognition. 2.2 Feature Extraction We implement a generic feature extraction process based on the ClearTK project (Ogren et al., 2008). ClearTK provides a set of highly flexible feature extractors that access the annotations (e.g. POS tags, chunks, etc.) created by the linguistic preprocessing. One important decision during training is to decide which instances should be used for feature extraction. In the simplest setting, each token is us"
W12-2036,W11-2842,1,0.891282,"ed by the HOO shared task. In this paper, we describe the UKP Lab system participating in the HOO 2012 Shared Task on preposition and determiner error correction. Our focus was to implement a highly flexible and modular system which can be easily augmented by other researchers. The system might be used to provide a level playground for subsequent shared tasks and enable further progress in this important research field on top of the state of the art identified by the shared task. 1 2 Introduction UKP Lab already participated in the previous HOO Shared Task in 2011. Our knowledge-based system (Zesch, 2011) was targeted towards detecting realword spelling errors, but performed also well on a number of other error classes.1 However, it was not competitive for article and preposition errors where supervised systems based on confusion sets constitute the state of the art. Thus, we tailor the HOO 2011 system towards correcting article and preposition errors, but also implement a supervised approach based on confusion sets (Golding and Schabes, 1996; Jones and Martin, 1997; Carlson et al., 2001). We decided to implement a basic system that should be as flexible as possible and might serve as a basis"
W14-1817,W11-1407,0,0.362988,"es with word difficulty). Pino and Eskenazi (2009) use distractors that are morphologically, orthographically, or phonetically similar (e.g. bread – beard). Another approach used in previous works to make distractors more challenging is utilizing thesauri (Sumita et al., 2005; Smith and Avinesh, 2010) or taxonomies (Hoshino and Nakagawa, 2007; Mitkov et al., 2009) to select words that are semantically similar to the target word. In addition to the target word, some approaches also consider the semantic relatedness of distractors with the whole carrier sentence or paragraph (Pino et al., 2008; Agarwal and Mannem, 2011; Mostow and Jang, 2012), i.e. they pick distractors that are from the same domain as the target word. Generally, selecting more challenging distractors usually means making them more similar to the target word. As this increases the probability that a distractor might actually be another correct answer, we need a more sophisticated approach for 1 Reliability Checking 3 Automatic Generation of Challenging Distractors Our goal is to automatically generate distractors that are as ‘close’ to the target word as possible, yet do not fit the carrier sentence context. To accomplish this, our strategy"
W14-1817,D10-1113,0,0.0327844,"s is that they are context-insensitive, i.e. they have a single score, which is not assessed with respect to the concrete context T under which they are applied. However, the appropriateness of an inference rule may in fact depend on this context. For example, ‘Microsoft acquire Skype → Microsoft purchase Skype’, is an appropriate application of the rule ‘acquire → purchase’, while ‘Children acquire skills → Children purchase skills’ is not. To address this issue, additional models were introduced that compute a different context-sensitive score per each context T , under which it is applied (Dinu and Lapata, 2010; Melamud et al., 2013). In this work, we use the resource provided by Melamud et al. (2013), which includes both context-sensitive and context-insensitive rules for over 2,000 frequent verbs.2 We use these rules to generate challenging distractors as we show next. 3.2 Figure 2: Filtering context-insensitive substitutions with context-sensitive ones in order to get challenging distractors. verbs, make it difficult for learners to understand how to properly use these words. Acquiring such fine-grained sense distinction skills is a prerequisite for really competent language usage. These skills c"
W14-1817,P13-2043,0,0.218248,"ound on the web. If such a sentence is found, the distractor is discarded. We note that the applicability of this approach is limited, as finding exact matches for such artificial sentences can be unlikely due to sparseness of natural languages. Therefore not finding an exact match does not necessarily rule out the possibility of an invalid distractor. Candidates Selection In some settings the set of possible distractors is known in advance, e.g. the set of English prepositions in preposition exercises (Lee and Seneff, 2007) or a confusion set with previously known errors like {two, too, to}. Sakaguchi et al. (2013) use data from the Lang-8 platform (a corpus of manually annotated errors1 ) in order to determine typical learner errors and use them as distractors. However, in the common setting only the target word is known and the set of distractors needs to be automatically generated. Randomly selecting distractors is a valid strategy (Mostow and Jang, 2012), but it is only suitable for the most beginner learners. More advanced learners can easily rule out distractors that do not fit grammatically or are too unrelated semantically. Thus, more advanced approaches usually employ basic strategies, such as"
W14-1817,W05-0210,0,0.656423,"ne way to perform reliability checking is by considering collocations involving the target word (Pino et al., 2008; Smith and Avinesh, 2010). For example, if the target word is strong, we can find the collocation strong tea. Then we can use powerful as a distractor because it is semantically similar to strong, yet *powerful tea is not a valid collocation. This approach is effective, but requires strong collocations to discriminate between valid and invalid distractors. Therefore it cannot be used with carrier sentences that do not contain strong collocations, such as the sentence in Figure 1. Sumita et al. (2005) apply a simple web search approach to judge the reliability of an item. They check whether the carrier sentence with the target word replaced by the distractor can be found on the web. If such a sentence is found, the distractor is discarded. We note that the applicability of this approach is limited, as finding exact matches for such artificial sentences can be unlikely due to sparseness of natural languages. Therefore not finding an exact match does not necessarily rule out the possibility of an invalid distractor. Candidates Selection In some settings the set of possible distractors is kno"
W14-1817,P13-1131,1,0.837692,"ext-insensitive, i.e. they have a single score, which is not assessed with respect to the concrete context T under which they are applied. However, the appropriateness of an inference rule may in fact depend on this context. For example, ‘Microsoft acquire Skype → Microsoft purchase Skype’, is an appropriate application of the rule ‘acquire → purchase’, while ‘Children acquire skills → Children purchase skills’ is not. To address this issue, additional models were introduced that compute a different context-sensitive score per each context T , under which it is applied (Dinu and Lapata, 2010; Melamud et al., 2013). In this work, we use the resource provided by Melamud et al. (2013), which includes both context-sensitive and context-insensitive rules for over 2,000 frequent verbs.2 We use these rules to generate challenging distractors as we show next. 3.2 Figure 2: Filtering context-insensitive substitutions with context-sensitive ones in order to get challenging distractors. verbs, make it difficult for learners to understand how to properly use these words. Acquiring such fine-grained sense distinction skills is a prerequisite for really competent language usage. These skills can be trained and teste"
W14-1817,W09-0207,0,0.206742,"trategies, such as choosing distractors with the same part-of-speech tag as the target word, or distractors with a corpus frequency comparable to the target word (Hoshino and Nakagawa, 2007) (based on the assumption that corpus frequency roughly correlates with word difficulty). Pino and Eskenazi (2009) use distractors that are morphologically, orthographically, or phonetically similar (e.g. bread – beard). Another approach used in previous works to make distractors more challenging is utilizing thesauri (Sumita et al., 2005; Smith and Avinesh, 2010) or taxonomies (Hoshino and Nakagawa, 2007; Mitkov et al., 2009) to select words that are semantically similar to the target word. In addition to the target word, some approaches also consider the semantic relatedness of distractors with the whole carrier sentence or paragraph (Pino et al., 2008; Agarwal and Mannem, 2011; Mostow and Jang, 2012), i.e. they pick distractors that are from the same domain as the target word. Generally, selecting more challenging distractors usually means making them more similar to the target word. As this increases the probability that a distractor might actually be another correct answer, we need a more sophisticated approac"
W14-1817,W12-2016,0,0.535486,"distractors involves two steps: Candidate Selection controls the difficulty of the items, while Reliability Checking ensures that the items remain solvable, i.e. it ensures 143 Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 143–148, c Baltimore, Maryland USA, June 26, 2014. 2014 Association for Computational Linguistics that there is only one correct answer. We note that this work is focused on single-word distractors rather than phrases (Gates et al., 2011), and only on target isolated carrier sentences rather than longer texts as in (Mostow and Jang, 2012). 2.1 checking the reliability of the distractor set. 2.2 In order to make sure that there is only one correct answer to a gap-fill item, there needs to be a way to decide for each distractor whether it fits into the context of the carrier sentence or not. In those cases, where we have a limited list of potential target words and distractors, e.g. in preposition exercises (Lee and Seneff, 2007), a supervised classifier can be trained to do this job. Given enough training data, this approach yields very high precision, but it cannot be easily applied to open word classes like nouns or verbs, wh"
W14-1817,O05-4001,0,\N,Missing
W14-3503,J90-1003,0,0.48123,"the right and repeat until either a match occurs or the target word became the most left-hand word in the window. If no match occurs, the window size is decreased and the procedure repeated. An example sentence that demonstrates the detection of a three word collocation is depicted in 35 Figure 2: Sliding Window: Collocation detection with window size of three Figure 2. The target word is forest. The window has to be shifted two times until it is over the phrase forest fire prevention in order to match. In order to quantify word association strength, we use pointwise mutual information (PMI) (Church and Hanks, 1990) and obtain the required word frequencies from the Google Web1T corpus (Brants and Franz, 2006). As PMI is only defined for bigrams, sequences of length n ≥ 3 have to be split into two units that are pseudo-bigrams.1 For example, calculating the PMI for forest fire prevention, would either lead to a split such as [forest fire] [prevention] or [forest] [fire prevention]. Following Korkontzelos et al. (2008), we use the so called “pessimistic split” to compute PMI by using the split with the highest likelihood of all possible splits: P(w ,...,w ) 1 n PMI pess (w1 , ..., w n ) = lo g P(w1 ,...,wi"
W14-3503,S13-1035,0,0.013666,"s We found that this is a strong indicator. 2.4 Distributional Thesaurus A sentence may contain many words that are semantically related to the target word, but that do not appear in a collocation: The compressor is necessitated by ____ which injects fuel into the cylinder → air In the example, we show semantically related words in bold-face. They help to guide the reader towards the right choice for the blank. We detect semantically related words using the distributional thesaurus from Biemann and Riedl (2013) that was computed over the top 1 million words of the English Google Books Corpus (Goldberg and Orwant, 2013). We retrieve the fifty highest ranked words that are associated with our target word in the thesaurus. These words are compared with all words occurring in the sentence. If two or more associated words are found in the sentence the detector matches. Unfortunately, the detector fires quite often, even in cases of a rather weak relationship between two words. We thus assign a low reliability. 2.5 Hearst Patterns Hearst patterns (Hearst, 1992) are lexico-syntactic pattern like “X such as Y, and Z” that are frequently used to detect hierarchical relationships between words. We argue that wherever"
W14-3503,C92-2082,0,0.127023,"the distributional thesaurus from Biemann and Riedl (2013) that was computed over the top 1 million words of the English Google Books Corpus (Goldberg and Orwant, 2013). We retrieve the fifty highest ranked words that are associated with our target word in the thesaurus. These words are compared with all words occurring in the sentence. If two or more associated words are found in the sentence the detector matches. Unfortunately, the detector fires quite often, even in cases of a rather weak relationship between two words. We thus assign a low reliability. 2.5 Hearst Patterns Hearst patterns (Hearst, 1992) are lexico-syntactic pattern like “X such as Y, and Z” that are frequently used to detect hierarchical relationships between words. We argue that wherever such a pattern can be found in a sentence, the ambiguity should be reduced as the reader can be guided by the explicitly stated relationships. Already small amounts of ____ such as beer or wine can affect your ability to drive. → alcohol 37 This structure allows the reader to abstract to the more general, deleted word. In the case of beer and wine, the reader may conclude that the deleted word is alcohol. Thus, if the more general word is d"
W14-3503,P13-2043,0,0.086948,"know for sure one correct answer namely the one that was used in the original sentence. Students might get frustrated if they provide a valid solution that is not recognized by the system. The same problem affects an alternative solution to the problem: providing a list of alternative answer options - called distractors (Sumita et al., 2005). Determining whether a distractor is actually another valid solution is equivalent to the problem described above. Thus, finding good distractors is still an unsolved problem that attracts a lot of research (Lee and Seneff, 2007; Smith and Avinesh, 2010; Sakaguchi et al., 2013; Zesch and Melamud, 2014). Furthermore, providing distractors for cloze items also considerably changes the nature of the task, as distractors are recognition stimuli, i.e. the student recognizes the correct answer rather than having to actively produce it (Gonzáles, 1996). Now consider example sentence (2): (2) I went to the ____ today and now I have sand in my shoes. Most people would come up with beach or maybe desert, while other solutions are highly unlikely, which means that the blank is less ambiguous than example (1). This leads to our research question, whether it is possible to find"
W14-3503,W05-0210,0,0.0401932,"ontext. Such ambiguity is not only a problem for language learners, but even native speakers frequently fail when facing such a task (Klein-Braley and Raatz, 1982). If cloze items should be automatically generated and scored, ambiguous items pose a serious problem, as we only know for sure one correct answer namely the one that was used in the original sentence. Students might get frustrated if they provide a valid solution that is not recognized by the system. The same problem affects an alternative solution to the problem: providing a list of alternative answer options - called distractors (Sumita et al., 2005). Determining whether a distractor is actually another valid solution is equivalent to the problem described above. Thus, finding good distractors is still an unsolved problem that attracts a lot of research (Lee and Seneff, 2007; Smith and Avinesh, 2010; Sakaguchi et al., 2013; Zesch and Melamud, 2014). Furthermore, providing distractors for cloze items also considerably changes the nature of the task, as distractors are recognition stimuli, i.e. the student recognizes the correct answer rather than having to actively produce it (Gonzáles, 1996). Now consider example sentence (2): (2) I went"
W14-3503,P08-3011,0,0.012238,"abeled training data is available. In the remainder of this section, we describe our detectors in more detail. The detectors are not mutually exclusive and a word might be detected by several detectors. 2.1 Collocation-based Detectors Subsequently, we will introduce three detectors that are variations of testing a word for being collocated with one or more nearby words. These detectors are motivated by the research field of word prediction where an algorithm tries to determine the next word a user might want to type on a keyboard based on the already entered word sequence (Li and Hirst, 2005; Trnka, 2008; Aliprandi et al., 2007). We hypothesize that if a word is easy to predict it should also be less ambiguous in that context. We further expect that longer collocated sequences are easier to predict than shorter ones. Another way to look at this problem is its relation to language model perplexity (Chen et al., 1998). Classically, perplexity makes a statement about how well a model predicts test data. In our task, we try to do the exact opposite. We want to determine a test set on which we would achieve a low perplexity on our (static) language model. 3-5 Gram Sliding Window This detector test"
W14-3503,W14-1817,1,0.743517,"ct answer namely the one that was used in the original sentence. Students might get frustrated if they provide a valid solution that is not recognized by the system. The same problem affects an alternative solution to the problem: providing a list of alternative answer options - called distractors (Sumita et al., 2005). Determining whether a distractor is actually another valid solution is equivalent to the problem described above. Thus, finding good distractors is still an unsolved problem that attracts a lot of research (Lee and Seneff, 2007; Smith and Avinesh, 2010; Sakaguchi et al., 2013; Zesch and Melamud, 2014). Furthermore, providing distractors for cloze items also considerably changes the nature of the task, as distractors are recognition stimuli, i.e. the student recognizes the correct answer rather than having to actively produce it (Gonzáles, 1996). Now consider example sentence (2): (2) I went to the ____ today and now I have sand in my shoes. Most people would come up with beach or maybe desert, while other solutions are highly unlikely, which means that the blank is less ambiguous than example (1). This leads to our research question, whether it is possible to find contexts that are specifi"
W15-0601,W11-1407,0,0.0160074,"distinguished into open and closed answer formats. In open formats, the learner has to actually produce the solution, while it can be selected from a small fixed set of multiple choice options in closed formats. This technique provides full control over the candidate space, but the selection of good answer options (distractors), that are not a proper solution, is a difficult task. Most previous works in the field of educational natural language processing focus on the generation of distractors to manipulate the difficulty, i.e. for cloze tests (Zesch and Melamud, 2014; Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). In addition to the answer format, the test types can be distinguished by the gap type and the deletion rate. On the local level, the gap type determines which portion of the word is deleted. On the global test level, the deletion rate determines the distribution of gaps in the text. A higher number of gaps per sentence results in a higher redundancy reduction. This increases the dependency between gaps as the mutilated context"
W15-0601,Q14-1040,1,0.76714,"ting institutions and conducted a learner study to obtain error rates for an additional test type.1 For a better understanding of the differences between test types, we first calculate the candidate space of potential answers and compare it to learner answers. We assume that higher answer ambiguity leads to higher difficulty. As all datasets allow binary scoring (correct/wrong), the difficulty of an item is interpreted as the proportion of wrong answers, also referred to as the error rate. We then build a generalized difficulty prediction framework based on an earlier approach we presented in Beinborn et al. (2014a) which was limited to English and to one specific test type. We evaluate the prediction for different test types and languages and obtain remarkable results for French and German. Many language tests are designed as multiple choice questions. The generalized prediction approach lacks predictive power for this format because the evaluation strategy for the answer candidates is solely based on word frequency. We develop two strategies for more sophisticated candidate ranking that are inspired by automatic solving methods based on language models and semantic relatedness. We show that the candi"
W15-0601,H05-1025,0,0.036304,"ambiguity. 5 Candidate evaluation strategies The main challenge for solving a reduced redundancy test consists in identifying the most suitable candidate in the candidate space. The context fitness of a candidate can be evaluated based on language model probabilities and on semantic relatedness between the candidate and the context. LM-based approach A probabilistic language model (LM) calculates the probability of a phrase based on the frequencies of lower order n-grams extracted from training data (Stolcke, 1994). This can be used to predict the fitness of a word for the sentential context. Bickel et al. (2005), for example, evaluate the use of probabilistic language models to support auto-completion of sentences in writing editors. In the completion scenario, only the left context is available, while the learner can also consider the right context in language tests. Zweig et al. (2012) thus model the problem of solving cloze tests by applying methods from lexical substitution to evaluate and rank the candidates. The part to be substituted is a gap and the set of “substitution candidates” is already provided by the answer options. Unfortunately, we cannot rely on static sentences for the open test f"
W15-0601,H05-1103,0,0.097309,"while it can be selected from a small fixed set of multiple choice options in closed formats. This technique provides full control over the candidate space, but the selection of good answer options (distractors), that are not a proper solution, is a difficult task. Most previous works in the field of educational natural language processing focus on the generation of distractors to manipulate the difficulty, i.e. for cloze tests (Zesch and Melamud, 2014; Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). In addition to the answer format, the test types can be distinguished by the gap type and the deletion rate. On the local level, the gap type determines which portion of the word is deleted. On the global test level, the deletion rate determines the distribution of gaps in the text. A higher number of gaps per sentence results in a higher redundancy reduction. This increases the dependency between gaps as the mutilated context of a single gap can only be recreated by solving the surrounding gaps. 2.1 Cloze test Cloze tests have been intr"
W15-0601,P14-5011,1,0.788352,"word class, inflection, compound structure, spelling difficulty, etc.)4 , the readability of the text (typetoken-ratio, number of clauses, average word and sentence length, etc.) and the test parameters (number of candidates, position of gap, etc). We also adapt the pipelines to include proper German and French pre-processing using DkPro (de Castilho and Gurevych, 2014) and adapt the candidate calculation to the different test types. In order to assure comparability to previous work, we also use support vector machines for regression in Weka (Hall et al., 2009) through the DKPro TC framework (Daxenberger et al., 2014). 4.3 Table 2 provides an overview of the test datasets used in this paper. It consists of four open formats and one closed format with multiple choice options. The number of participants is averaged over the texts because each participant worked with 5 texts in the open formats.5 The error rates should not be compared across test types because the participants had different levels of language proficiency. The high standard deviations indicate that each test contains gaps that are rather easy and others that are extremely difficult. In Beinborn et al. (2014a) we have shown that the error rate"
W15-0601,W14-5201,1,0.595942,"Missing"
W15-0601,N07-1058,0,0.0240565,"produce the solution, while it can be selected from a small fixed set of multiple choice options in closed formats. This technique provides full control over the candidate space, but the selection of good answer options (distractors), that are not a proper solution, is a difficult task. Most previous works in the field of educational natural language processing focus on the generation of distractors to manipulate the difficulty, i.e. for cloze tests (Zesch and Melamud, 2014; Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). In addition to the answer format, the test types can be distinguished by the gap type and the deletion rate. On the local level, the gap type determines which portion of the word is deleted. On the global test level, the deletion rate determines the distribution of gaps in the text. A higher number of gaps per sentence results in a higher redundancy reduction. This increases the dependency between gaps as the mutilated context of a single gap can only be recreated by solving the surrounding gaps. 2.1 Cloze test Cloze"
W15-0601,W14-3503,1,0.853128,"context of a single gap can only be recreated by solving the surrounding gaps. 2.1 Cloze test Cloze tests have been introduced by Taylor (1953) and have become the most popular form of reduced redundancy testing. In cloze tests, full words are deleted from a text. This strategy requires compre2 Figure 1: Example for a cloze question, the solution is observance. hensive context, so the deletion rate is usually every 7th word or higher (Brown, 1989). The main problem with cloze tests is that the gaps are usually highly ambiguous and the set of potential solutions cannot be exactly anticipated (Horsmann and Zesch, 2014). Therefore, most cloze tests are designed as closed formats, so that the correct solution can be selected from a set of distractors (see Figure 1 for an example). 2.2 C-test Although the cloze test is widely used, the setup contains several weaknesses such as the small number of gaps and the ambiguity of the solution. The C-test is an alternative of the cloze test that has been developed by Klein-Braley and Raatz (1982). The C-test construction principle enables a higher number of gaps on less text, every second word of a short paragraph is transformed into a gap. As this high deletion rate w"
W15-0601,W12-2016,0,0.0241095,"redundancy tests can be distinguished into open and closed answer formats. In open formats, the learner has to actually produce the solution, while it can be selected from a small fixed set of multiple choice options in closed formats. This technique provides full control over the candidate space, but the selection of good answer options (distractors), that are not a proper solution, is a difficult task. Most previous works in the field of educational natural language processing focus on the generation of distractors to manipulate the difficulty, i.e. for cloze tests (Zesch and Melamud, 2014; Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). In addition to the answer format, the test types can be distinguished by the gap type and the deletion rate. On the local level, the gap type determines which portion of the word is deleted. On the global test level, the deletion rate determines the distribution of gaps in the text. A higher number of gaps per sentence results in a higher redundancy reduction. This increases the dependency between gap"
W15-0601,W12-2017,0,0.0227427,"d set of multiple choice options in closed formats. This technique provides full control over the candidate space, but the selection of good answer options (distractors), that are not a proper solution, is a difficult task. Most previous works in the field of educational natural language processing focus on the generation of distractors to manipulate the difficulty, i.e. for cloze tests (Zesch and Melamud, 2014; Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). In addition to the answer format, the test types can be distinguished by the gap type and the deletion rate. On the local level, the gap type determines which portion of the word is deleted. On the global test level, the deletion rate determines the distribution of gaps in the text. A higher number of gaps per sentence results in a higher redundancy reduction. This increases the dependency between gaps as the mutilated context of a single gap can only be recreated by solving the surrounding gaps. 2.1 Cloze test Cloze tests have been introduced by Taylor (1953) and have become the most popula"
W15-0601,quasthoff-etal-2006-corpus,0,0.025272,"ap that scores the generated sub-sentences using a language model and only keeps the n best. For the closed cloze test, the number of generated sentences is of course limited to the number of candidates (5) because each sentence contains only one gap. We use 5-gram language models that are trained on monolingual news corpora using berkeleylm with Kneser-Ney smoothing.8 Zweig et al. (2012) trained their models explicitly on training data only from Sherlock Holmes novels. In order to better simulate learner knowledge, we use rather small and controlled training data from the Leipzig collection (Quasthoff et al., 2006) consisting of one million sentences for each language. For solving the test, we then select the generated sentence with the highest log-probability in the language model and count how many gaps are solved correctly. If several sentences obtain the same probability, we pick one at random. We run this strategy ten times and average the results. For comparison, we implement a baseline that always selects the most frequent candidate without considering the context. Semantic relatedness approach Language models cannot capture relations between distant words in the sentence. To account for this con"
W15-0601,W10-1007,0,0.163459,"he learner has to actually produce the solution, while it can be selected from a small fixed set of multiple choice options in closed formats. This technique provides full control over the candidate space, but the selection of good answer options (distractors), that are not a proper solution, is a difficult task. Most previous works in the field of educational natural language processing focus on the generation of distractors to manipulate the difficulty, i.e. for cloze tests (Zesch and Melamud, 2014; Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). In addition to the answer format, the test types can be distinguished by the gap type and the deletion rate. On the local level, the gap type determines which portion of the word is deleted. On the global test level, the deletion rate determines the distribution of gaps in the text. A higher number of gaps per sentence results in a higher redundancy reduction. This increases the dependency between gaps as the mutilated context of a single gap can only be recreated by solving the surrounding gaps"
W15-0601,W14-1817,1,0.853652,"e missing words. Reduced redundancy tests can be distinguished into open and closed answer formats. In open formats, the learner has to actually produce the solution, while it can be selected from a small fixed set of multiple choice options in closed formats. This technique provides full control over the candidate space, but the selection of good answer options (distractors), that are not a proper solution, is a difficult task. Most previous works in the field of educational natural language processing focus on the generation of distractors to manipulate the difficulty, i.e. for cloze tests (Zesch and Melamud, 2014; Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). In addition to the answer format, the test types can be distinguished by the gap type and the deletion rate. On the local level, the gap type determines which portion of the word is deleted. On the global test level, the deletion rate determines the distribution of gaps in the text. A higher number of gaps per sentence results in a higher redundancy reduction. This increases the"
W15-0601,W12-2704,0,0.0471167,"Missing"
W15-0601,P12-1063,0,0.394108,"c relatedness between the candidate and the context. LM-based approach A probabilistic language model (LM) calculates the probability of a phrase based on the frequencies of lower order n-grams extracted from training data (Stolcke, 1994). This can be used to predict the fitness of a word for the sentential context. Bickel et al. (2005), for example, evaluate the use of probabilistic language models to support auto-completion of sentences in writing editors. In the completion scenario, only the left context is available, while the learner can also consider the right context in language tests. Zweig et al. (2012) thus model the problem of solving cloze tests by applying methods from lexical substitution to evaluate and rank the candidates. The part to be substituted is a gap and the set of “substitution candidates” is already provided by the answer options. Unfortunately, we cannot rely on static sentences for the open test formats as the context needs to be determined by solving the surrounding gaps. For each gap, we take all candidates into account and generate all possible sentences resulting from the combinations with the candidates of subsequent gaps. This can lead to strong dependencies between"
W15-0615,Q13-1032,0,0.124574,"d on the assumption that the classifier is best informed by responses that are as different as possible (i.e. in the words used). In the second approach, we simulate letting annotators score whole clusters with a label that is used for all instances in this cluster. The main advantage of this method is that it yields multiple training instances with just one decision from the annotator. At the same time, judging whole clusters – especially if they are large – is more difficult than judging a single response, so we need to take this into consideration when comparing the results. 2 Related Work Basu et al. (2013) describe a related study on Powergrading, an approach for computer-assisted scoring of short-answer questions. They carry out experiments using crowd-sourced responses to questions from the US citizenship test. The end goal of that 124 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 124–132, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics Figure 1: Comparison of the classical supervised approach with clustering approach where a subset of instances is selected for manual annotation. work is the clust"
W15-0615,P14-5011,1,0.871661,"Missing"
W15-0615,horbach-etal-2014-finding,0,0.375529,"ain combination of tokens appears in the document, but also makes sure they are in the same dependency relation. Table 2: List of features tem on a certain item led to decreased performance on the others. For our experiments consistency is more important than especially good baseline results, and so we choose to run the same system on all ten items rather than developing ten separate systems that require individual tuning. Impact of Training Data Size The main question that we are exploring in this paper is whether some answers are more valuable for training than others (Lewis and Gale, 1994; Horbach et al., 2014). By carefully selecting the training instances, we should be able to train a model with performance comparable to the full model that uses less training data and thus is cheaper to create. In order to assess the potential of this approach, it will be useful to compare against the upper and lower bound in performance. For this purpose, we need to find the best and worst subset of graded answers. As the number of possible combinations of k instances from n answers is much too high to search in its entirety, we test 1,000 random samples while making sure that all outcome classes are found in the"
W15-0626,W99-0411,0,0.042171,"t of manually rated essays is required for training. Hence, it seems desirable to develop systems that work without this initial Many kinds of features have been proposed for essay grading (Valenti et al., 2003; Dikli, 2006). They differ in the degree of dependency to the task at hand. There are features that are strongly dependent on a task, e.g. when they detect important words or topics (Chen and He, 2013). Other features are less dependent, e.g. when they capture general characteristics of essays like the number of words in the essay (Östling, 2013; Lei et al., 2014), usage of connectors (Burstein and Chodorow, 1999; Lei et al., 2014), etc. We assume that a system which considers only task-independent features should perform well no matter what task it is trained on. However, it is unclear how much explanatory power the model might lose in this step. In this paper, we test this hypothesis by performing experiments with a state-of-theart essay grading system on English and German datasets. We categorize features into task-dependent and task-independent ones and evaluate the difference in grading accuracy between the corresponding models. We find that the task-independent models show a better performance f"
W15-0626,P98-1032,0,0.551526,"d Burstein, 2002). Therefore, we measure the text length by counting all tokens and sentences in an essay. The degree of task-dependence of this feature is directly connected to the time limit. The average sentence length in words and word length in characters can be an indicator for the degree of complexity a writer can master (Attali and Burstein, 2006; Mahana et al., 2012; Chen and He, 2013; Östling, 2013). As this is not particularly tied to a specific task, these features are weakly taskdependent. Variation in the syntactic structures used in an essay may indicate proficiency in writing (Burstein et al., 1998). Following Chen and He (2013), we operationalize this by measuring the ratio of distinct parse trees to all the trees and the average depths of the trees to compute syntactic variation features. Further, the parsing trees are used to measure the proportion of subordinate, causal and temporal clauses. Causal and temporal clauses are detected by causal or temporal conjunctions that could be found in subordinate-clauses. For example, a subordinate clause beginning with when is considered as temporal. The detection of causal- and temporal clauses is used to enrich the syntactic variability by a d"
W15-0626,D13-1180,0,0.203288,"xtracted and ratings are assigned according to the manifestations of the features (Attali and Burstein, 2006). In order to automatically learn the association between feature values and ratings a high amount of manually rated essays is required for training. Hence, it seems desirable to develop systems that work without this initial Many kinds of features have been proposed for essay grading (Valenti et al., 2003; Dikli, 2006). They differ in the degree of dependency to the task at hand. There are features that are strongly dependent on a task, e.g. when they detect important words or topics (Chen and He, 2013). Other features are less dependent, e.g. when they capture general characteristics of essays like the number of words in the essay (Östling, 2013; Lei et al., 2014), usage of connectors (Burstein and Chodorow, 1999; Lei et al., 2014), etc. We assume that a system which considers only task-independent features should perform well no matter what task it is trained on. However, it is unclear how much explanatory power the model might lose in this step. In this paper, we test this hypothesis by performing experiments with a state-of-theart essay grading system on English and German datasets. We c"
W15-0626,P14-5011,1,0.298074,"ew of the two feature groups with Oi,j as the number of times one annotator used in our experiments. The full feature set uses graded j and the other i, with Ei,j as the expected both strongly and weakly task-dependent features, grades given a random distribution and with while the reduced set only uses the weakly taskdependent ones. (i − j)2 wi,j = (N − 1)2 3.3 Essay Grading System In order to ensure a fair comparison, we re- as the weight of the grades. The metric produces implemented a state-of-the-art essay grading system a value for the agreement between the human gold based on DKPro TC (Daxenberger et al., 2014)3 standard and the machine grading. Group Feature which ensures easy reproducibility and replicability. Our system takes a set of graded essays and 3 version: 0.7 228 4 The preprocessing was realized with the DKPro Core 1.7.0 components used within DKPro TC: BreakIterator, TreeTagger, SnowballStemmer and StanfordParser. 5 Support Vector Machine provided by DKPro TC full 1 opinion 0.8 0.6 κ 0.6 κ 0.4 0.4 0.2 0.2 0 reduced 1 sourcebased 0.8 full reduced 0 3 4 5 6 1 Tasks 2 7 T1 T2 Figure 2: German dataset: Comparison of the full and reduced model 8 Figure 1: ASAP dataset: Comparison of the full"
W15-0626,krestel-etal-2008-minding,0,0.0131228,"guistic phenomena such as commas, quotations, or exclamation marks can serve as valuable features in a grade prediction. These features focus more on the structuring of an essay and are thus weakly task-dependent. For tasks that are source-based (i.e. a source text is provided on which the task is based), we augment this approach by also counting formal references like citations and line references. Source-based features are obviously strongly task-dependent. Using third party sources to support an argument can be a valuable hint for evidence (Bergler, 2006). Therefore, we use the approach of Krestel et al. (2008) to detect direct, indirect, and reported speech in essays. The approach relies on set of reporting verbs and rules to identify and distinguish these forms. 225 Style Features Another important aspect of essay quality is an appropriate style. Following Östling (2013), we use the relative ratio of POS-tags to detect style preferences of writers. We complemented this by a feature that measures the formality F of an essay (Heylighen and Dewaele, 2002) defined as: F = P c(i) P c(j) − + 100 i∈A n j∈B n 2 where A = {N, ADJ, PP, DET}, B = {PR, V, ADV, UH}, and n is the number of tokens in the text. T"
W15-0626,J93-2004,0,0.0607611,"ng are both indicators for bad essays. Thus, we categorize the features as weakly task-dependent. 2.9 Task-Similarity Features For source-based essays, we can determine the task similarity of an essay by computing the similarity between essay and the task specific source (Östling, 2013). There should be a certain degree of similarity between the source and the essay, but if the similarity is too high the essay might be plagiarized. We use Kullback–Leibler divergence between source and essay. A variant of this feature computes the corpus similarity to a neutral background corpus (Brown corpus (Marcus et al., 1993) in our case) in order to determine whether the essay was written specific enough. While the corpus similarity should be weakly task-dependent, the task similarity is of course strongly dependent on the task. 2.10 Set-Dependent Features So far, all features have only used the characteristics of a single essay, but it is also useful to take the whole set of essays into account. Instead of detecting characteristics of an individual essay the differences between essays in the set is examined. Setbased features can be based on topics (Burstein et al., 1998) or n-grams (Chen and He, 2013). We use w"
W15-0626,W13-1705,0,0.478193,"ciation between feature values and ratings a high amount of manually rated essays is required for training. Hence, it seems desirable to develop systems that work without this initial Many kinds of features have been proposed for essay grading (Valenti et al., 2003; Dikli, 2006). They differ in the degree of dependency to the task at hand. There are features that are strongly dependent on a task, e.g. when they detect important words or topics (Chen and He, 2013). Other features are less dependent, e.g. when they capture general characteristics of essays like the number of words in the essay (Östling, 2013; Lei et al., 2014), usage of connectors (Burstein and Chodorow, 1999; Lei et al., 2014), etc. We assume that a system which considers only task-independent features should perform well no matter what task it is trained on. However, it is unclear how much explanatory power the model might lose in this step. In this paper, we test this hypothesis by performing experiments with a state-of-theart essay grading system on English and German datasets. We categorize features into task-dependent and task-independent ones and evaluate the difference in grading accuracy between the corresponding models."
W15-0626,C98-1032,0,\N,Missing
W15-3603,P14-1119,0,0.0245233,"ng architecture and analyze the performance of four state-of-the-art algorithms. We then perform experiments on three German datasets, of which two have been created particularly for these experiments, in order to analyze the impact of decompounding on standard keyphrase extraction approaches. Decompounding has previously been successfully used in other applications, e.g. in machine translation (Koehn and Knight, 2003), information retrieval (Hollink et al., 2004; Alfonseca et al., 2008b; Alfonseca et al., 2008a), speech recognition (Ordelman, 2003), and word prediction (Baroni et al., 2002). Hasan and Ng (2014) have shown that infrequency errors are a major cause for lower keyphrase extraction results . To the best of our knowledge, we are the first to examine the influence of decompounding on keyphrase extraction. Introduction Most approaches for automatic extraction of keyphrases are based on the assumption that the more frequent a term or phrase is mentioned, the more important it is. Consequently, most extraction algorithms apply some kind of normalization, e.g. lemmatization or noun chunking (Hulth, 2003; Mihalcea and Tarau, 2004), in order to arrive with accurate counts. However, especially in"
W15-3603,P08-2064,0,0.141717,"er consists of the parts Deutsch (Engl.: German) and Lehrer (Engl.: teacher). In this paper, we propose a comprehensive decompounding architecture and analyze the performance of four state-of-the-art algorithms. We then perform experiments on three German datasets, of which two have been created particularly for these experiments, in order to analyze the impact of decompounding on standard keyphrase extraction approaches. Decompounding has previously been successfully used in other applications, e.g. in machine translation (Koehn and Knight, 2003), information retrieval (Hollink et al., 2004; Alfonseca et al., 2008b; Alfonseca et al., 2008a), speech recognition (Ordelman, 2003), and word prediction (Baroni et al., 2002). Hasan and Ng (2014) have shown that infrequency errors are a major cause for lower keyphrase extraction results . To the best of our knowledge, we are the first to examine the influence of decompounding on keyphrase extraction. Introduction Most approaches for automatic extraction of keyphrases are based on the assumption that the more frequent a term or phrase is mentioned, the more important it is. Consequently, most extraction algorithms apply some kind of normalization, e.g. lemmati"
W15-3603,W03-1028,0,0.104841,"a), speech recognition (Ordelman, 2003), and word prediction (Baroni et al., 2002). Hasan and Ng (2014) have shown that infrequency errors are a major cause for lower keyphrase extraction results . To the best of our knowledge, we are the first to examine the influence of decompounding on keyphrase extraction. Introduction Most approaches for automatic extraction of keyphrases are based on the assumption that the more frequent a term or phrase is mentioned, the more important it is. Consequently, most extraction algorithms apply some kind of normalization, e.g. lemmatization or noun chunking (Hulth, 2003; Mihalcea and Tarau, 2004), in order to arrive with accurate counts. However, especially in Germanic languages the frequent use of noun compounds has an adverse effect on the reliability of frequency counts. Consider for example a German document that talks about Lehrer (Engl.: teacher) without ever mentioning the word “Lehrer” at all, because it is always part of compounds like Deutschlehrer (Engl.: German teacher) or Gymnasiallehrer (Engl.: grammar school teacher). Thus, we argue that the problem can be solved by splitting noun compounds in meaningful parts, i.e. by performing decompounding"
W15-3603,S10-1040,0,0.0504271,"Missing"
W15-3603,E03-1076,0,0.161055,"schlehrer (Engl.: German teacher). in German. The compound Deutschlehrer consists of the parts Deutsch (Engl.: German) and Lehrer (Engl.: teacher). In this paper, we propose a comprehensive decompounding architecture and analyze the performance of four state-of-the-art algorithms. We then perform experiments on three German datasets, of which two have been created particularly for these experiments, in order to analyze the impact of decompounding on standard keyphrase extraction approaches. Decompounding has previously been successfully used in other applications, e.g. in machine translation (Koehn and Knight, 2003), information retrieval (Hollink et al., 2004; Alfonseca et al., 2008b; Alfonseca et al., 2008a), speech recognition (Ordelman, 2003), and word prediction (Baroni et al., 2002). Hasan and Ng (2014) have shown that infrequency errors are a major cause for lower keyphrase extraction results . To the best of our knowledge, we are the first to examine the influence of decompounding on keyphrase extraction. Introduction Most approaches for automatic extraction of keyphrases are based on the assumption that the more frequent a term or phrase is mentioned, the more important it is. Consequently, most"
W15-3603,biemann-etal-2008-asv,0,0.0281646,"ight), it creates a split and stops. Banana Splitter3 searches for the word from the right to the left, and if there is more than one possibility, the one with the longest split on the right side is taken as candidate. Data Driven counts the number of words in a dictionary, which contain a split at this position as prefix or suffix for every position in the input. A split is made at the position with the largest difference between prefix and suffix counts (Larson et al., 2000). ASV Toolbox4 uses a trained Compact Patricia Tree to recursively split parts from the beginning and end of the word (Biemann et al., 2008). Unlike the other algorithms, it generates only a single split candidate at each recursive step. For that reason, it does not need a ranker. It is also the only supervised (using lists of existing compounds) approach tested. 2.2 Freq. M.I. .64 .26 .58 .08 .71 .33 JWord Splitter Freq. M.I. .67 .59 .63 .20 .79 .73 Banana Splitter Freq. M.I. .70 .66 .40 .16 .83 .81 Data Driven Freq. M.I. .49 .40 .18 .04 .70 .58 .80 .75 .87 In these equations, N is the number of fragments the candidate has, w is the fragment itself, f (w) is the relative unigram frequency for that fragment w, bigr(wi , wj ) is th"
W15-3603,W14-5201,1,0.887716,"Missing"
W15-3603,R09-1086,1,0.898436,"perform stopword removal and decompounding as described in Section 2. It should be noted that in most preprocessing pipelines, decompounding should be the last step, as it heavily influences POS-tagging. We extract all lemmas in the document as keyphrase candidates and rank them according to basic ranking approaches based on frequency counts and the position in the document. We do not use more sophisticated extraction approaches, as we want to examine the influence of decompounding as directly as possible. However, it has been shown that frequency-based heuristics are a very strong baseline (Zesch and Gurevych, 2009), and even supervised keyphrase extraction methods such as KEA (Witten et al., 1999) use term frequency and position as the most important features and will be 7 www.medizin-forum.de/ www.dipf.de/en/research/projects/ pythagoras 8 9 nlp.stanford.edu/software/segmenter. shtml 12 heavily influenced by decompounding. We evaluate the following ranking methods: tfidfconstant ranks candidates according to their term frequency f (t, d) in the document. tf-idf decreases the impact of words that occur in most documents. The term frequency count is normalized with the inverse document frequency in the t"
W15-3603,W04-3252,0,\N,Missing
W16-0508,I13-1112,1,0.787621,"ateness because the Spanish spelling would be ingeniero. The spelling of marmalade with an e seems to be idiosyncratic to German learners. The above analyses are only performed on an anecdotal basis and need to be backed up with more thorough experimental studies. The examples support the intuitive assumption that cognates are particularly prone to spelling errors due to the diﬀerent orthographic and phonetic patterns in the L1 of the learner. The cognateness of words can be determined automatically using string similarity measures (Inkpen et al., 2005) or character-based machine translation (Beinborn et al., 2013). The learners in the EFC corpus also diﬀer in proﬁciency (e.g. German learners seem to be more advanced than Brazilian learners) which might also have an inﬂuence on the spelling error probability of words. However, it is complicated to disentangle the inﬂuence of the L1 and Correct Brazilian Mexican Chinese Russian German attention atention(27) attencion (10) atencion (3) atention (13) attencion(1) attentio (1) attaention (1) atttention (1) - attantion (5) atantion (1) atention (1) - departmental departament (10) departamente (1) departaments (1) department (1) - deparment (2) deparmental (1"
W16-0508,N09-3006,0,0.0220173,"discussed psycholinguistic analyses of spelling diﬃculty. In natural language processing, related work in the ﬁeld of spelling has focused on error correction (Ng et al., 2013; Ng et al., 2014). For ﬁnding the right correction, Deorowicz and Ciura (2005) analyze probable causes for spelling errors. They identify three types of causes (mistyping, misspelling and vocabulary incompetence) and model them using substitution rules. Toutanova and Moore (2002) use the similarity of pronunciations to pick the best correction for an error resulting in an improvement over state-of-the-art spellcheckers. Boyd (2009) build on their work but model the pronunciation of non-native speakers, leading to slight improvements in the pronunciationbased model. Modeling the spelling diﬃculty of words could also have a positive eﬀect on spelling correction because spelling errors would be easier to anticipate. Another important line of research is the development of spelling exercises. A popular recent example is the game Phontasia (Berkling et al., 2015a). It has been developed for L1 learners but could probably also be used for L2 learners. In this case, the ﬁndings on cross-lingual transfer could be integrated to"
W16-0508,P14-5011,1,0.842772,"Missing"
W16-0508,C12-1049,0,0.105468,"of the error frequency over all occurrences of the word (including the erroneous occurrences). perr (w) = EFC Table 2: Examples for high and low spelling error probsi ∈ D SD Error Probability high low (1) = (e, c) X Corpus ferr (w) f (w) (4) 77 The words are then ranked by their error probability to quantify spelling diﬃculty.10 This is only a rough approximation that ignores other factors such as repetition errors and learner ability because detailed learner data was not available for all corpora. In future work, more elaborate measures of spelling diﬃculty could be analyzed (see for example Ehara et al. (2012)). 3.3 Training and Test Data An inspection of the ranked probabilities indicates that the spelling diﬃculty of a word is a continuous variable which points to a regression problem. However, the number of spelling errors is too small to distinguish between a spelling error probability of 0.2 and 0.3, for example. Instead, we only focus on the extremes of the scale. 10 In the case of tied error probability, the word with the higher error frequency is ranked higher. In the case of an error frequency of zero for both words, the word with the lower correct frequency is ranked higher. Individual Fe"
W16-0508,W13-1718,0,0.030946,"et al. (2014), we analyzed that words with high spelling error probability lead to more diﬃcult exercises. This indicates, that spelling diﬃculty should also be considered in exercise generation. In text simpliﬁcation tasks (Specia et al., 2012), a quantiﬁcation of spelling diﬃculty could lead to more focused, learner-oriented lexical simpliﬁcation. Spelling problems are often inﬂuenced by cross-lingual transfer because learners apply patterns from their native language (Ringbom and Jarvis, 2009). Spelling errors can therefore be a good predictor for automatic natural language identiﬁcation (Nicolai et al., 2013). Language teachers are not always aware of these processes because they are often not familiar with the native language of their learners. Automatic prediction methods for L1-speciﬁc spelling diﬃculties can lead to a better understanding of cross-lingual transfer and support the development of individualized exercises. In this paper, we take an empirical approach and approximate spelling diﬃculty based on error frequencies in learner corpora. We extract more than 140,000 spelling errors by more than 85,000 learners from three learner corpora. Two corpora cover essays by learners of English an"
W16-0508,P11-1027,0,0.0122416,"wels to consonants. Both extremes—words with high density (e.g. aerie) and very low density (e.g. strength)—are likely to cause spelling problems. 3 4 grapheme length: 9, phoneme length: 5 http://www.speech.cs.cmu.edu/cgi-bin/cmudict 75 Character Sequence Probability We assume, that the grapheme–phoneme correspondence of a word is less intuitive, if the word contains a rare sequence of characters (e.g. gardener vs guarantee). To approximate this, we build a language model of character trigrams that indicates the probability of a character sequence using the framework Berkeleylm version 1.1.2 (Pauls and Klein, 2011). The quality of a language model is usually measured as the perplexity, i.e. the ability of the model to deal with unseen data. The perplexity can often be improved by using more training data. However, in this scenario, the model is supposed to perform worse on unseen data because it should model human learners. In order to reﬂect the sparse knowledge of a language learner, the model is trained only on the 800–1000 most frequent words from each language. We refer to these words as the Basic Vocabulary.5 Pronunciation Diﬃculty Furthermore, we try to capture the assumption that a spelling erro"
W16-0508,W11-1409,0,0.0298869,"L1 adults, L2 adults) are due to ambiguous sound–letter correspondences. Berkling et al. (2015b) study the interplay between graphemes and phonotactics in German in detail and developed a game to teach orthographic patterns to children. Peereman et al. (2007) provide a very good overview of factors inﬂuencing word diﬃculty and also highlight the importance of consistent grapheme–phoneme correspondence. It thus seems justiﬁed to focus on the phonetic problems. The features described below try to approximate the relationship between graphemes and phonemes from various angles. Orthographic Depth Rosa and Eskenazi (2011) analyze the inﬂuence of word complexity features on the vocabulary acquisition of L2 learners and show that words which follow a simple one-to-one mapping of graphemes to phonemes are considered to be easier than one-to-many or many-to-one mappings as in knowledge.3 The orthographic depth can be expressed as the grapheme-to-phoneme ratio (the word length in characters divided by the number of phonemes). For English, we calculate the number of phonemes based on the phonetic representation in the Carnegie Mellon University Pronouncing Dictionary.4 For Italian and German, a comparable pronunciat"
W16-0508,S12-1046,0,0.0287817,"ew sounds and their mapping to graphemes. English is a well-known example for a particularly inconsistent grapheme-to-phoneme mapping. For example, the sequence ough can be pronounced in six diﬀerent ways as in though, through, rough, cough, thought and bough.1 In many language learning scenarios, it is important to be aware of the spelling diﬃculty of a word. In Beinborn et al. (2014), we analyzed that words with high spelling error probability lead to more diﬃcult exercises. This indicates, that spelling diﬃculty should also be considered in exercise generation. In text simpliﬁcation tasks (Specia et al., 2012), a quantiﬁcation of spelling diﬃculty could lead to more focused, learner-oriented lexical simpliﬁcation. Spelling problems are often inﬂuenced by cross-lingual transfer because learners apply patterns from their native language (Ringbom and Jarvis, 2009). Spelling errors can therefore be a good predictor for automatic natural language identiﬁcation (Nicolai et al., 2013). Language teachers are not always aware of these processes because they are often not familiar with the native language of their learners. Automatic prediction methods for L1-speciﬁc spelling diﬃculties can lead to a better"
W16-0508,P02-1019,0,0.0318911,"of the L2 proﬁciency based on the current data and we leave this analysis to future work. phenomena occurring with L2 learners. 6 7 Related work In section 2, we already discussed psycholinguistic analyses of spelling diﬃculty. In natural language processing, related work in the ﬁeld of spelling has focused on error correction (Ng et al., 2013; Ng et al., 2014). For ﬁnding the right correction, Deorowicz and Ciura (2005) analyze probable causes for spelling errors. They identify three types of causes (mistyping, misspelling and vocabulary incompetence) and model them using substitution rules. Toutanova and Moore (2002) use the similarity of pronunciations to pick the best correction for an error resulting in an improvement over state-of-the-art spellcheckers. Boyd (2009) build on their work but model the pronunciation of non-native speakers, leading to slight improvements in the pronunciationbased model. Modeling the spelling diﬃculty of words could also have a positive eﬀect on spelling correction because spelling errors would be easier to anticipate. Another important line of research is the development of spelling exercises. A popular recent example is the game Phontasia (Berkling et al., 2015a). It has"
W16-0508,P11-1019,0,0.0376541,"corrections provided by teachers. We extract 167,713 annotations with the tag SP for spelling error.8 To our knowledge, this is by far the biggest available corpus with spelling errors from language learners. FCE The second corpus is part of the Cambridge Learner Corpus and consists of learner 7 It also contains essays by Czech learners, but this subset is signiﬁcantly smaller than the ones for the other two languages and is therefore not used here. 8 Some corrections have two diﬀerent tags; we only extract those with a single SP tag. 76 answers for the First Certiﬁcate in English (FCE) exam (Yannakoudakis et al., 2011). It contains 2,488 essays by 1,244 learners (each learner had to answer two tasks) from 16 nationalities. The essays have been corrected by oﬃcial examiners. We extract 4,074 annotations with the tag S for spelling error. Merlin The third corpus has been developed within the EU-project MERLIN (Boyd et al., 2014) and contains learner essays graded according to the Common European Reference Framework. The 813 Italian and the 1,033 German samples have been obtained as part of a test for the European language certiﬁcate (TELC). 752 of the German essays and 754 of the Italian essays were annotated"
W16-0519,P13-2121,0,0.0217936,"rmalize the probability distributions as for our purposes we are only interested in probability ratios. tences that are more suitable for learners. In this section we describe some considerations related to these choice, as well as the settings used for generating the exercises in our experiments. 3.1 Language Model Melamud et al. (2015) used gap-filler distributions (also known as substitute vectors), which are based on a language model, achieving state-of-the-art performance in lexical substitution tasks. Following their settings, we trained a 5-gram language model using the KenLM toolkit3 (Heafield et al., 2013) with modified Kneser-Ney smoothing on the two billion word ukWaC English web corpus (Baroni et al., 2009). We then used this language model with the FASTSUBS tool4 (Yuret, 2012) to generate the probability distribution of gap-filler words, pruned to the top 1000 most probable gap fillers. Using a large web corpus, such as ukWaC, entails good coverage of diverse language styles for our language model, at some acceptable cost of noisy low quality texts. 3.2 Gap Bundle Corpus While our algorithm can generate bundles given any set of input sentences, the quality of the exercises might vary strong"
W16-0519,W14-3503,1,0.829598,"age give add want make room information language 15 people final full best new 0 20 5 0 e av er ag gi ve d ad ak e w an t m fu ll fin al pe op le la ng ua in ge fo rm at io n ro om be st −5 ne w Disambiguation Measure D(b) 0.2 Figure 4: Success rate and disambiguation measure per item. no good gap bundle can be generated). 5 Related Work To the best of our knowledge, bundled gap filling is introduced in this work for the first time. Therefore, there is no directly related previous work. However, in a broader sense, our work continues research on automated handling of ambiguity in cloze tests. Horsmann and Zesch (2014) control for ambiguity of cloze tests by selecting low ambiguity sentences based on a series of (dis-)ambiguity indicators. However, they fail to reduce the ambiguity to a sufficient degree and limit the practical relevance, as a user is limited to a narrow set of gaps that fit their approach. As mentioned before, probably the most popular method for resolving ambiguity is to use a multiplechoice format, providing a set of distractors that may be generated automatically. These distractors may 179 be generated from common confusions (Lee and Seneff, 2007), typical learner errors (Sakaguchi et a"
W16-0519,N15-1050,1,0.829439,"words T , and (4) the number of gaps (or sentences) in a bundle. Note that C and G can be the same, but C would preferably be a very large corpus in order to derive a high quality language model, and G would be a possibly smaller, higher quality corpus containing sen2 We do not normalize the probability distributions as for our purposes we are only interested in probability ratios. tences that are more suitable for learners. In this section we describe some considerations related to these choice, as well as the settings used for generating the exercises in our experiments. 3.1 Language Model Melamud et al. (2015) used gap-filler distributions (also known as substitute vectors), which are based on a language model, achieving state-of-the-art performance in lexical substitution tasks. Following their settings, we trained a 5-gram language model using the KenLM toolkit3 (Heafield et al., 2013) with modified Kneser-Ney smoothing on the two billion word ukWaC English web corpus (Baroni et al., 2009). We then used this language model with the FASTSUBS tool4 (Yuret, 2012) to generate the probability distribution of gap-filler words, pruned to the top 1000 most probable gap fillers. Using a large web corpus,"
W16-0519,P13-2043,0,0.0310669,"d Zesch (2014) control for ambiguity of cloze tests by selecting low ambiguity sentences based on a series of (dis-)ambiguity indicators. However, they fail to reduce the ambiguity to a sufficient degree and limit the practical relevance, as a user is limited to a narrow set of gaps that fit their approach. As mentioned before, probably the most popular method for resolving ambiguity is to use a multiplechoice format, providing a set of distractors that may be generated automatically. These distractors may 179 be generated from common confusions (Lee and Seneff, 2007), typical learner errors (Sakaguchi et al., 2013) or by selecting words with the same wordclass or frequency in a reference corpus (Hoshino and Nakagawa, 2007). A major problem of gap-fill exercises with distractors is that they are often too easy – especially for advanced learners –, as the generated distractors do not make sense in the context of the gap. Therefore, some approaches go one step further considering distractor candidates that are highly compatible with the context of the gaps. Then they need to automatically judge that such distractors are not in fact correct answers themselves, e.g. by considering collocations of targets and"
W16-0519,W14-1817,1,0.752064,"010; Sumita et al., ). For example, Sumita et al. () check whether replacing the target with the distractor results in a sentence that exists on the web. If so, they conclude that the 1 Success Rate 0.8 0.6 0.4 Single Gap Bundle2 Bundle3 Bundle4 0.2 0 −2 0 2 4 6 8 10 12 14 Disambiguation Measure D(b) 16 18 Figure 5: Relationship between our disambiguation measure and success rate. Each point represents a single item in the user study. distractor is invalid. However, their approach seems to be limited, as it relies on finding exact matches of sentences which even on the web is rather unlikely. Zesch and Melamud (2014) generate distractors that are semantically similar to the target word in some sense, but not in the particular sense induced by the gap-fill context. While their approach points in a promising direction it fails to model a sufficiently large difficulty continuum, especially when targeting a group with high level of language proficiency. Ultimately, a disadvantage of gap-fill tests with distractors is that the test is a recognition task rather than a production task and therefore considerably easier. In contrast, our bundled gap-filling approach has the advantage that there is no recognition s"
W16-2615,P14-5011,1,0.847052,"okenization results Expected Actual (1) (2) (3) Doz. Doz . im Real Life imRealLife a. d. gestrigen a.d.gestrigen Table 3: Tokenization errors Empiri STTS PoS tags EMOASC PTKMA PTKIFG AKW HST ADR PTKMWL EMOIMG URL VVPPER VAPPER DM VMPPER ADVART KOUSPPER ONO PPERPPER EML Freq. 115 103 99 49 46 35 28 22 18 7 4 3 1 1 1 1 1 0 Standard STTS-PoS tags PTKANT PWAV KOKOM XY PDAT VAINF PWS VVIMP TRUNC KOUI PWAT VVIZU PIDAT PTKA APZR VMINF VAPP VMPP 3.2 Freq. Implementation We train a CRF classifier (Lafferty et al., 2001) using the FlexTag tagger (Zesch and Horsmann, 2016) which is based on the DKProTC (Daxenberger et al., 2014) machine learning framework. Our feature set uses a context window of ±2 tokens, the five-hundred most-frequent character ngrams over all bi, tri and four-grams and boolean features if a token is capitalized, a number, etc. 42 39 28 28 28 26 23 18 12 10 8 7 7 5 5 3 3 1 General Domain Adaptation As the provided training data will not be sufficient to train a competitive model, we decided to apply a domain adaption strategy that has been proposed as an effective method for improving tagging accuracy on social media texts (Ritter et al., 2011; Rehbein, 2013). We closely follow the process outline"
W16-2615,W14-5201,0,0.0368091,"Missing"
W16-2615,W16-2606,0,0.122936,"Missing"
W16-2615,N13-1037,0,0.0734751,"increase accuracy. 1 2 Tokenization While tokenization usually comprises of two subtasks (sentence boundary detection and token boundary detection), in the EmpiriST shared task, the sentence boundaries are already given and only the token boundaries should be detected. Introduction Tokenization and part-of-speech (PoS) tagging are two fundamental NLP tasks. Tokenization aims at detecting word and sentence boundaries in text while PoS tagging uses the recognized words and assigns each word its syntactical category. Both tasks are especially challenging when applied on noisy social media texts (Eisenstein, 2013). The main challenge when tokenizing social media text is the ambiguity of punctuation characters which occurs more frequently than in other domains. A major source of ambiguity are emoticons that show a surprising degree of complexity ranging from two-character emoticons such as :) to ncharacter emoticons such as (*.*#). Additionally challenges are introduced by missing whitespace characters and the use of non-standard abbreviations such as in [...] aus meiner (Doz.)Sicht.:) [...]. For PoS tagging, the main source of error are the frequently occurring unknown word forms that 2.1 Task Analysi"
W16-2615,foth-etal-2014-size,0,0.13876,"Missing"
W16-2615,P11-2008,0,0.203991,"Missing"
W16-2615,N13-1039,0,0.0741982,"s. Spelling variations of the same word form tend to be placed into the same cluster (Ritter et al., 2011), e.g. the unknown word i-wann occurs in the same 122 cluster as the correctly spelled and known word form irgendwann. This enables the classifier to learn that i-wann and irgendwann are distributional similar which provides a bias to assign i-wann the same PoS tag as irgendwann. We use 1000 clusters and consider words which occur at least 40 times as suggested by Ritter et al. (2011) we provide the resulting bit string in various length as feature to the classifier i.e. 2, 4, 6, ..., 16 (Owoputi et al., 2013) to inform the classifier about (partial) similarity between words. PoS tags, too. Due to unreliable upper- and lowercase usage in social media, we use case-insensitive matching. A main drawback of adding data from a foreign text domain such as the Tiger corpus is a different annotation scheme and its dominating size that decreases the weight of the EmpiriST training data. This causes a bias for choosing the tags from the bigger Tiger corpus. We attempt to adjust for this bias by adding boolean features if a word can occur with a PoS tag for one of the sparse new word classes to assign a highe"
W16-2615,D11-1141,0,0.872439,"amework (Eckart de Castilho and Gurevych, 2014), and a specialized social media tokenizer from the ArkTools suite (Gimpel et al., 2011). Whitespace tokenization and BreakIter are expected to perform poorly as neither tool is designed for processing social media text. The ArkTools tokenizer is tailored to English Twitter messages which are quite similar to the EmpiriST dataset, but will obviously not capture phenomena that are specific for German. Part-of-Speech Tagging Tagging social media text with off-the-shelve PoS taggers leads to a huge drop in accuracy compared to tagging newswire text (Ritter et al., 2011; Horsmann et al., 2015). The main cause for this drop is the high rate of out-of-vocabulary words, which are mainly caused by orthographical variations of known words (Eisenstein, 2013). 3.1 Shared Task Data The EmpiriST training dataset contains about 10k tokens of PoS annotated German social media text (the test data contains about 13k tokens). The dataset is annotated with an extended version of the STTS tagset which adds 18 new PoS tags to account for German social media phenomena 121 P CMC R F1 P Web R F1 ∅ F1 81.7 99.4 98.7 99.7 80.7 97.9 97.5 98.2 99.9 90.2 98.7 99.7 99.8 90.3 98.4 99."
W16-2615,telljohann-etal-2004-tuba,0,0.363585,"Missing"
W16-2615,L16-1675,1,0.820597,"LTL-UDE Whitespace BreakIter ArkTools LTL-UDE Table 2: Tokenization results Expected Actual (1) (2) (3) Doz. Doz . im Real Life imRealLife a. d. gestrigen a.d.gestrigen Table 3: Tokenization errors Empiri STTS PoS tags EMOASC PTKMA PTKIFG AKW HST ADR PTKMWL EMOIMG URL VVPPER VAPPER DM VMPPER ADVART KOUSPPER ONO PPERPPER EML Freq. 115 103 99 49 46 35 28 22 18 7 4 3 1 1 1 1 1 0 Standard STTS-PoS tags PTKANT PWAV KOKOM XY PDAT VAINF PWS VVIMP TRUNC KOUI PWAT VVIZU PIDAT PTKA APZR VMINF VAPP VMPP 3.2 Freq. Implementation We train a CRF classifier (Lafferty et al., 2001) using the FlexTag tagger (Zesch and Horsmann, 2016) which is based on the DKProTC (Daxenberger et al., 2014) machine learning framework. Our feature set uses a context window of ±2 tokens, the five-hundred most-frequent character ngrams over all bi, tri and four-grams and boolean features if a token is capitalized, a number, etc. 42 39 28 28 28 26 23 18 12 10 8 7 7 5 5 3 3 1 General Domain Adaptation As the provided training data will not be sufficient to train a competitive model, we decided to apply a domain adaption strategy that has been proposed as an effective method for improving tagging accuracy on social media texts (Ritter et al., 20"
W16-6002,W13-2322,0,0.0146081,"suited to perform the discussed operations by using Social Media domain specific clusters and topic labeling methods for the frame-labeling. We intend to evaluate the validity of our representation and approach extrinsically and application-based. 2 Types of representations We distinguish between representations on two levels: (i) argument-level, which can be robustly implemented more easily, and (ii) frame-level, which is highly expressive. Argument-level As shown in Figure 1, most argument representations consist of an event trigger, which is mostly a verb, and its corresponding arguments (Banarescu et al., 2013; Kingsbury and Palmer, 2003). Argument-level representations based on Social Media posts are used in applications such as e.g. creating event calendars for concerts and festivals (Becker et al., 2012) or creating overviews of important events on Twitter (Ritter et al., 2012). Frame-level On this level, events are represented as frame structures such as proposed by Fillmore (1976) that built upon the argument-level, i.e. the arguments are labeled with semantic roles. A well-known frame semantic tagger is S EMAFOR (Das, 2014). 6 Proceedings of EMNLP 2016 Workshop on Uphill Battles in Language P"
W16-6002,W14-3007,0,0.0697314,"which is mostly a verb, and its corresponding arguments (Banarescu et al., 2013; Kingsbury and Palmer, 2003). Argument-level representations based on Social Media posts are used in applications such as e.g. creating event calendars for concerts and festivals (Becker et al., 2012) or creating overviews of important events on Twitter (Ritter et al., 2012). Frame-level On this level, events are represented as frame structures such as proposed by Fillmore (1976) that built upon the argument-level, i.e. the arguments are labeled with semantic roles. A well-known frame semantic tagger is S EMAFOR (Das, 2014). 6 Proceedings of EMNLP 2016 Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods, pages 6–10, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics Figure 1: Representations of an exemplary event on argument and frame-level 3 Challenges The main challenge is to bridge the gap between argument and frame-level representation. 3.1 Performance of operations Our goal is to develop a representation that is both computable even on noisy Social Media text and expressive enough to support all required operations like equivalence. A s"
W16-6002,P06-1117,0,0.0403698,"non-standard text that contains spelling variations and neologisms that need to be dealt with. In our opinion, the lack of undefined units is an especially problematic issue in Social Media texts. Furthermore, it may contain innovative, informal or incomplete use of frames, due to space restrictions such as presented by Twitter. Also by cause of space restrictions, which lead to a lack of context, and considering the variety of topics that is addressed in So7 cial Media, it is more challenging to find a fitting frame out of an existing frame repository (Ritter et al., 2012; Li and Ji, 2016). Giuglea and Moschitti (2006) and M`ujdriczaMayd et al. (2016) tried to bridge the gap by combing repositories on frame and argument level and representing them based on Intersective Levin Classes (ILC) (Kipper et al., 2006). ILC, which are used in VerbNet (Kipper et al., 2006), are more fine-grained than classic Levin verb classes, formed according to alternations of the grammatical expression of their arguments (Levin, 1993). Classic Levin verb classes were used for measuring semantic evidence between verbs (Baker and Ruppenhofer, 2002). However, these approaches also have to deal with coverage problems due to their rel"
W16-6002,kipper-etal-2006-extending,0,0.0157527,"ermore, it may contain innovative, informal or incomplete use of frames, due to space restrictions such as presented by Twitter. Also by cause of space restrictions, which lead to a lack of context, and considering the variety of topics that is addressed in So7 cial Media, it is more challenging to find a fitting frame out of an existing frame repository (Ritter et al., 2012; Li and Ji, 2016). Giuglea and Moschitti (2006) and M`ujdriczaMayd et al. (2016) tried to bridge the gap by combing repositories on frame and argument level and representing them based on Intersective Levin Classes (ILC) (Kipper et al., 2006). ILC, which are used in VerbNet (Kipper et al., 2006), are more fine-grained than classic Levin verb classes, formed according to alternations of the grammatical expression of their arguments (Levin, 1993). Classic Levin verb classes were used for measuring semantic evidence between verbs (Baker and Ruppenhofer, 2002). However, these approaches also have to deal with coverage problems due to their reliance on manually crafted frame repositories. 4 Approach According to Modi et al. (2012) frame semantic parsing conceptually consists of 4 stages: 1. 2. 3. 4. Identification of frame-evoking elem"
W16-6002,N16-1137,0,0.294861,"when dealing with non-standard text that contains spelling variations and neologisms that need to be dealt with. In our opinion, the lack of undefined units is an especially problematic issue in Social Media texts. Furthermore, it may contain innovative, informal or incomplete use of frames, due to space restrictions such as presented by Twitter. Also by cause of space restrictions, which lead to a lack of context, and considering the variety of topics that is addressed in So7 cial Media, it is more challenging to find a fitting frame out of an existing frame repository (Ritter et al., 2012; Li and Ji, 2016). Giuglea and Moschitti (2006) and M`ujdriczaMayd et al. (2016) tried to bridge the gap by combing repositories on frame and argument level and representing them based on Intersective Levin Classes (ILC) (Kipper et al., 2006). ILC, which are used in VerbNet (Kipper et al., 2006), are more fine-grained than classic Levin verb classes, formed according to alternations of the grammatical expression of their arguments (Levin, 1993). Classic Levin verb classes were used for measuring semantic evidence between verbs (Baker and Ruppenhofer, 2002). However, these approaches also have to deal with cove"
W16-6002,Y10-1027,0,0.0263041,", we focus on the last two tasks, which we regard as tasks of the frame-level. We observe this task under the aspect of fitting the realization of operation tasks as discussed earlier. As we only regard predicate frames and their arguments for the role labeling, we will use predicate as a term for the unlabeled form of frame and argument as the unlabeled form of role. Pre-defined frame labels There have been attempts to bridge the gap on Social Media texts by projecting ontological information in the form of computed event types on the event trigger on the argument-level (Ritter et al., 2012; Li et al., 2010; Li and Ji, 2016) in order to solve the task of frame labeling. However, according to Ritter et al. (2012) the automatic mapping of pre-defined event types is insufficient for providing semantically meaningful information on the event. We aim to augment those approaches by inducing frame-like structures based on distributional semantics. Moreover, we want to use similarity clusters for the labeling of arguments in frames. We seek to compute the argument labels by the use of supersense tagging, similarly to the approach presented by Coppola et al. (2009). They successfully used the WordNet sup"
W16-6002,W12-1901,0,0.300905,"epositories on frame and argument level and representing them based on Intersective Levin Classes (ILC) (Kipper et al., 2006). ILC, which are used in VerbNet (Kipper et al., 2006), are more fine-grained than classic Levin verb classes, formed according to alternations of the grammatical expression of their arguments (Levin, 1993). Classic Levin verb classes were used for measuring semantic evidence between verbs (Baker and Ruppenhofer, 2002). However, these approaches also have to deal with coverage problems due to their reliance on manually crafted frame repositories. 4 Approach According to Modi et al. (2012) frame semantic parsing conceptually consists of 4 stages: 1. 2. 3. 4. Identification of frame-evoking elements Identification of their arguments Labeling of frames Labeling of roles We summarize these tasks in groups of two, namely identification and labeling, and discuss our approach towards them in the following subsections. 4.1 Identification of frame-evoking elements and their arguments We regard the first two tasks as tasks of the argument-level, which we plan to solve with part-ofspeech tagging and dependency parsing, by extracting all main verbs to solve the first task and considering"
W16-6002,L16-1484,0,0.036947,"Missing"
W16-6002,C10-2107,0,0.0302265,"ocial Media text and expressive enough to support all required operations like equivalence. A semantically equivalent sentence for our examplary sentence “The witch gave an apple to Snow White” would be “Snow White received an apple from the witch.”, as receive is the antonym of give and the roles of Giver and Receiver are inverted. On the argument-level, it remains a hard problem to establish the equivalence between the two sentences, while that would be easy on the frame-level. However, getting to the frame-level is an equally hard problem and frame representations suffer from low coverage (Palmer and Sporleder, 2010). 3.2 Coverage Palmer and Sporleder (2010) categorized and evaluated the coverage gaps in FrameNet (Baker et al., 2003). Coverage, whether of undefined units, lemmas, or senses, is of special importance when dealing with non-standard text that contains spelling variations and neologisms that need to be dealt with. In our opinion, the lack of undefined units is an especially problematic issue in Social Media texts. Furthermore, it may contain innovative, informal or incomplete use of frames, due to space restrictions such as presented by Twitter. Also by cause of space restrictions, which lead"
W16-6002,S12-1030,0,0.0131869,"ing by using an LDA extension, in which a combination of context and language features is used, as described by Riedl (2016). 5 Evaluation plan We plan to evaluate our approach in an extrinsic, application-based way on a manual gold standard containing event paraphrases. In order to test how well our approach performs in comparison to stateof-the-art approaches of both argument and frame representations, such as Das (2014) or Li and Ji (2016) in the task of equivalence computation, we will compare the results of all approaches. For this purpose, we plan to develop a dataset that is similar to Roth and Frank (2012), but tailored to the Social Media domain. They produced a corpus of alignments between semantically similar predicates and their arguments from news texts on the same event. 9 6 Summary In this paper we present our vision on a new event representation that enables the use of operations such as equivalence. We plan to use pre-processing to get the predicates and their arguments. The main focus of the work will be using sense clustering methods on domain-specific text and to apply these clusters on text. We plan to evaluate this application in an extrinsic, application-based way. Further on, we"
W16-6002,P15-4018,0,0.0189733,"this might seem less intuitive, we believe that due to the difficulties with Social Media data, the structures of full frames are less repetitive and are more difficult to cluster. Thus, by dividing the two tasks of predicate and argument clustering, we hope to achieve better results in our setting. Furthermore, in order to deal with the issues of the previously discussed peculiarities of the Social Media domain, we plan to train clusters on large amounts of Tweets. An example of our envisioned representation is shown in Figure 2, which was produced using the Twitter Bigram model of JoBimViz (Ruppert et al., 2015). Figure 2 shows the clustering for finding the correct sense in the labeling task, for both the predicate and its arguments. As the example shows, this representation has some flaws that need to be dealt with. It should be mentioned that the model used for this computation is pruned due to performance reasons, which is a cause for some of the flaws. For example, Snow White is not recognized as a Named Entity or a multi-word expression. To deal with the issue of the false Named Entity representation of Snow White presented in the exemplary representation, we plan to experiment with multi-word"
W17-5017,Q13-1032,0,0.169019,"stigate how several basic neural approaches similar to those used for automated essay scoring perform on short answer scoring. We show that neural architectures can outperform a strong nonneural baseline, but performance and optimal parameter settings vary across the more diverse types of prompts typical of short answer scoring. 1 1. Response length. Responses in SAS tasks are typically shorter. For example, while the ASAP-AES data contains essays that average between about 100 and 600 tokens (Shermis, 2014), short answer scoring datasets may have average answer lengths of just several words (Basu et al., 2013) to almost 60 words (Shermis, 2015). 2. Rubrics focus on content only in SAS vs. broader writing quality in AES. 3. Purpose and genre. AES tasks cover persuasive, narrative, and source-dependent reading comprehension and English Language Arts (ELA), while SAS tasks tend to be from science, math, and ELA reading comprehension. Introduction Deep neural network approaches have recently been successfully developed for several educational applications, including automated essay assessment. In several cases, neural network approaches exceeded the previous state of the art on essay scoring (Taghipour"
W17-5017,P14-5011,1,0.788102,".4) 522 140 449 (9.4) 540 (4.0) 48.4 3.9 9.8 12.5 0/1/2(/3) 0/1 2 or 5-way 2 or 5-way Table 1: Overview of the datasets used in this work. Since we train prompt-specific models for ASAPSAS and PG, we report the mean number of responses per set per prompt. For SRA, we train one model per label set across prompts and report the overall number of prompts per set as well as the mean number of responses per prompt per set (in parentheses). 3 Experiments 3.1 3.2 Method Baseline As a baseline system, we use a supervised learner based on a hand-crafted feature set. This baseline is based on DkPro TC (Daxenberger et al., 2014) and relies on support vector classification using Weka (Hall et al., 2009). We preprocess the data using the ClearNlp Segmenter 2 via DKPro Core (Eckart de Castilho and Gurevych, 2014). The features used in the baseline system comprise a commonly used and effective feature set for the SAS task. We use both binary word and character uni- to trigram occurrence features, using the top 10,000 most frequent ngrams in the training data, as well as answer length, measured by the number of tokens in a response. We carried out a series of experiments across datasets to discern the effect of specific p"
W17-5017,P16-1068,0,0.0802884,"ammar and spelling (Burstein et al., 2013). Short answer scoring, by contrast, typically focuses only on the accuracy Given these differences, the feature sets for AES and SAS systems are often different, with AES incorporating a larger set of features to capture writing quality (Shermis and Hamner, 2013). Nevertheless, deep learning approaches to AES have thus far demonstrated strong performance with minimal inputs consisting of unigrams and word embeddings. For example, Taghipour and Ng (2016) explore simple LSTM and CNN-based architectures with regression and evaluate on the ASAP-AES data. Alikaniotis et al. (2016) train score-specific word embeddings with several LSTM architectures. Dong and Zhang (2016) demonstrate that a hierarchical CNN architecture produces 159 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 159–168 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics strong results on the ASAP-AES data. Recently, Zhao et al. (2017) show state-of-the-art performance on the ASAP-AES dataset with a memory network architecture. In this work, we investigate whether deep neural network approaches with similarly mi"
W17-5017,D16-1115,0,0.0157986,"only on the accuracy Given these differences, the feature sets for AES and SAS systems are often different, with AES incorporating a larger set of features to capture writing quality (Shermis and Hamner, 2013). Nevertheless, deep learning approaches to AES have thus far demonstrated strong performance with minimal inputs consisting of unigrams and word embeddings. For example, Taghipour and Ng (2016) explore simple LSTM and CNN-based architectures with regression and evaluate on the ASAP-AES data. Alikaniotis et al. (2016) train score-specific word embeddings with several LSTM architectures. Dong and Zhang (2016) demonstrate that a hierarchical CNN architecture produces 159 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 159–168 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics strong results on the ASAP-AES data. Recently, Zhao et al. (2017) show state-of-the-art performance on the ASAP-AES dataset with a memory network architecture. In this work, we investigate whether deep neural network approaches with similarly minimal feature sets can produce good performance on the SAS task, including whether they can"
W17-5017,C16-1206,0,0.319922,"Missing"
W17-5017,N12-1021,0,0.0173533,"), so that there are no state-ofthe-art scoring results available for this dataset. For simplicity, we use the first out of three binary human-annotated correctness scores. 2.3 The three datasets we use cover different kinds of prompts and vary considerably in the length of the answers as well as their well-formedness. Table 1 shows basic statistics for each dataset. Figures 1, 2 and 3 show examples for each of the datasets. ASAP-SAS The Automated Student Assessment Prize Short Answer Scoring (ASAP-SAS) dataset1 contains 10 individual prompts, covering science, biology, 1 SRA The SRA dataset (Dzikovska et al., 2012) became widely known as the dataset used in SemEval2013 Shared Task 7 “The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge” (Dzikovska et al., 2013). It consists of two subsets: Beetle, with student responses from interacting with a tutorial dialogue system, and SciEntsBank (SEB) with science assessment questions. We use two label sets from the shared task: the 2-way labels classify responses as correct or incorrect, while the 5-way labels provide a more fine-grained classification of responses into the categories non domain, correct, partially correct incomple"
W17-5017,D14-1162,0,0.0971468,"lopment sets and evaluated on the test set. We report prompt-level results for this model in Section 3.6. For evaluation, we use quadratic weighted kappa (QWK) for the ASAP-SAS and Powergrading datasets. Because the class labels in the SRA dataset are unordered, we report the weighted F1 score, which was the preferred metric in the Semeval shared task (Dzikovska et al., 2016). 2 https://github.com/clir/clearnlp The networks are implemented in Keras and use the Theano backend. 3 162 ters for our experiments. For pretrained embeddings, in preliminary experiments the GloVe 100 dimension vectors (Pennington et al., 2014) performed slightly better than a selection of other offthe-shelf embeddings, and hence we use these for all conditions that involve pretrained embeddings. Embeddings for word tokens that are not found in the embeddings are randomly initialized from a uniform distribution. The convolutional layer uses a window length of 3 or 5 and 50 filters. We use a mean squared error loss for regression models and a cross-entropy loss for classification models. To train the network, we use RMSProp with ρ set to 0.9 and learning rate of 0.001. We clip the norm of the gradient to 10. The fully connected layer"
W17-5017,W15-0612,0,0.0298341,"character- and n-gram based system can learn somewhat more efficiently than the neural systems. • SRA: Because of the decreased performance of the combined best individual parameters on the development data, we use a 300dimensional unidirectional LSTM with attention mechanism. These models are “T&N tuned” in Table 4, which appear along with the non-neural baseline system. On ASAP-SAS, the “T&N tuned” parameter configuration outperformed the baseline system and the “T&N best” parameters. The tuned system does not reach the state-of-the-art Fisher-transformed mean score on the ASAPSAS dataset (Ramachandran et al., 2015)7 , which, like the winner of the ASAP-SAS competition (Tandalla, 2012), employed prompt-specific regular expressions. Other top performing systems used prompt-specific preprocessing and ensemble-based approaches over rich feature spaces (Higgins et al., 2014). On the SRA datasets, the “T&N tuned” model outperformed the baseline and the “T&N best” settings on average across prompts, by a larger margin than the other datasets. On the SRA data, as on the ASAP-SAS data, a gap remains between the tuned model’s performance and the state of the art. On SRA, this may be partly due to the use of “ques"
W17-5017,W14-5201,0,0.0715088,"Missing"
W17-5017,S13-2046,0,0.132328,"Tandalla, 2012), employed prompt-specific regular expressions. Other top performing systems used prompt-specific preprocessing and ensemble-based approaches over rich feature spaces (Higgins et al., 2014). On the SRA datasets, the “T&N tuned” model outperformed the baseline and the “T&N best” settings on average across prompts, by a larger margin than the other datasets. On the SRA data, as on the ASAP-SAS data, a gap remains between the tuned model’s performance and the state of the art. On SRA, this may be partly due to the use of “question indicator” features by the top performing systems (Heilman and Madnani, 2013; Ott et al., 2013). The performance improvement over the baseline system was larger on the development sets than on the test sets. Part of the reason for this is that the test set evaluation procedure likely did not choose the best-performing epoch for the neural models. 7 Ramachandran et al. (2015) state that their mean QWK is 0.0053 higher than the Tandalla result, so in Table 4 we report that score truncated to 3 decimal places rather than the rounded result reported in Ramachandran et al. (2015). 165 SRA Beetle 2-way 5-way SRA SEB 2-way 5-way Mean wF1 Mean wF1 Mean wF1 Mean wF1 Experiment"
W17-5017,D16-1193,0,0.229407,"l., 2013) to almost 60 words (Shermis, 2015). 2. Rubrics focus on content only in SAS vs. broader writing quality in AES. 3. Purpose and genre. AES tasks cover persuasive, narrative, and source-dependent reading comprehension and English Language Arts (ELA), while SAS tasks tend to be from science, math, and ELA reading comprehension. Introduction Deep neural network approaches have recently been successfully developed for several educational applications, including automated essay assessment. In several cases, neural network approaches exceeded the previous state of the art on essay scoring (Taghipour and Ng, 2016). The task of automated essay scoring (AES) is generally different from the task of automated short answer scoring (SAS). Essay scoring generally focuses on writing quality, a multidimensional construct that includes ideas and elaboration, organization, style, and writing conventions such as grammar and spelling (Burstein et al., 2013). Short answer scoring, by contrast, typically focuses only on the accuracy Given these differences, the feature sets for AES and SAS systems are often different, with AES incorporating a larger set of features to capture writing quality (Shermis and Hamner, 2013"
W17-5017,W16-0535,1,0.79015,"Missing"
W17-5017,S13-2045,0,\N,Missing
W17-5017,S13-2102,0,\N,Missing
W17-5040,W13-1739,0,0.0171634,"fferent prompts with up to 3000 responses per prompt. Since its release in 2012, the dataset has been widely used in a number of approaches (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Cummins et al., 2016). Another dataset, the CLC-FCE corpus (Yannakoudakis et al., 2011) contains essays written by ESOL test takers, but relatively little data per individual prompt (1,244 essays across 10 prompts), making it not the first choice for promptspecific approaches. Because of its extensive error annotations, it has also been used for the task of grammatical error detection and correction (e.g. Cahill et al. (2013) and Seo et al. (2012)). In Swedish, a corpus of high school essays has ¨ been released by Ostling (2013) with an overall 1 3 A more Challenging Essay Dataset As described above, most datasets are either small or target a wide range of proficiency levels, so that relatively shallow features are sufficient to achieve quite good performance. To overcome this problem, we have created a new dataset from essays written in German by prospective university students, mostly native speakers. The essays are one part of a large-scale assessment project at the University of Duisburg-Essen, SkaLa (Bremeric"
W17-5040,D13-1180,0,0.0547354,"Missing"
W17-5040,P16-1075,0,0.0148778,"e in length on the borderline between short answers and essays and are interesting because they, as well as our corpus, target writings by generally language-proficient population. The dominating publicly available dataset for essay scoring in recent year has been the data of the ASAP essay scoring challenge.1 It contains both source-based and opinion tasks targeting US students from grade 7 to 10 for 8 different prompts with up to 3000 responses per prompt. Since its release in 2012, the dataset has been widely used in a number of approaches (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Cummins et al., 2016). Another dataset, the CLC-FCE corpus (Yannakoudakis et al., 2011) contains essays written by ESOL test takers, but relatively little data per individual prompt (1,244 essays across 10 prompts), making it not the first choice for promptspecific approaches. Because of its extensive error annotations, it has also been used for the task of grammatical error detection and correction (e.g. Cahill et al. (2013) and Seo et al. (2012)). In Swedish, a corpus of high school essays has ¨ been released by Ostling (2013) with an overall 1 3 A more Challenging Essay Dataset As described above, most datasets"
W17-5040,P14-5011,1,0.835935,"es, and an SVM classifier, and an LSTM neural model based on embeddings. 4.1 LSTM architecture using a mean-over-time layer for aggregation in its reported best configuration exchanging the English word embeddings for German polyglot embeddings (Al-Rfou et al., 2013) and using 50 LSTM units for run-time efficiency. We also perform 10-fold cross-validation using 8 folds for training and 1 fold as development set to determine which of 50 epochs to use per run. SVM Classifier We use Weka’s (Hall et al., 2009) Support Vector classifier (SMO) in standard configuration as provided through DKPro TC (Daxenberger et al., 2014). We utilize a number of state-of-the-art features: As the essays in our dataset were written within a certain time limit, the length of an essay is an indicator of its quality. We measure length by the number of sentences, tokens and characters per essay. Additionally, we measure average sentence length in tokens and average token length in characters. N-gram features model words and phrases or constructions – in the case of POS n-grams – in an essay. We use boolean occurrence features for token and POS uni- to trigrams and token skip bi- to 5-grams. We count the occurrence of linguistic feat"
W17-5040,W13-3520,0,0.0144411,"erbal skills – aggregated score 1/2/3/4/5/6 64 Both G8 Overall Impression, aggregated from G2 to G8 1/2/3/4/5/6 60 Both Distribution Table 1: Scoring categories in our corpus. Note that a lower score corresponds to a better essay. 360 For our experiments, we rely on two state-ofthe-art systems: A classical supervised system based on hand-crafted features, and an SVM classifier, and an LSTM neural model based on embeddings. 4.1 LSTM architecture using a mean-over-time layer for aggregation in its reported best configuration exchanging the English word embeddings for German polyglot embeddings (Al-Rfou et al., 2013) and using 50 LSTM units for run-time efficiency. We also perform 10-fold cross-validation using 8 folds for training and 1 fold as development set to determine which of 50 epochs to use per run. SVM Classifier We use Weka’s (Hall et al., 2009) Support Vector classifier (SMO) in standard configuration as provided through DKPro TC (Daxenberger et al., 2014). We utilize a number of state-of-the-art features: As the essays in our dataset were written within a certain time limit, the length of an essay is an indicator of its quality. We measure length by the number of sentences, tokens and charact"
W17-5040,W14-5201,0,0.0612148,"Missing"
W17-5040,P16-1068,0,0.0193085,"i et al., 2016). Responses in this last dataset are in length on the borderline between short answers and essays and are interesting because they, as well as our corpus, target writings by generally language-proficient population. The dominating publicly available dataset for essay scoring in recent year has been the data of the ASAP essay scoring challenge.1 It contains both source-based and opinion tasks targeting US students from grade 7 to 10 for 8 different prompts with up to 3000 responses per prompt. Since its release in 2012, the dataset has been widely used in a number of approaches (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Cummins et al., 2016). Another dataset, the CLC-FCE corpus (Yannakoudakis et al., 2011) contains essays written by ESOL test takers, but relatively little data per individual prompt (1,244 essays across 10 prompts), making it not the first choice for promptspecific approaches. Because of its extensive error annotations, it has also been used for the task of grammatical error detection and correction (e.g. Cahill et al. (2013) and Seo et al. (2012)). In Swedish, a corpus of high school essays has ¨ been released by Ostling (2013) with an overall 1 3 A more Challenging"
W17-5040,C10-3009,0,0.026289,"Missing"
W17-5040,D16-1193,0,0.480554,"s a machine learning task (Dikli, 2006; Valenti et al., 2003). A wide range of features representing different aspects contributing to a good essay have been proposed such, as n-grams (Chen and He, 2013) or LSA (Foltz et al., 1999), ¨ length (Mahana et al., 2012; Ostling, 2013), linguistic correctness in terms of spelling and ¨ grammar (Mahana et al., 2012; Ostling, 2013), or cohesion and coherence of a text through identifying overlap between sentences and usage of connective devices (Lei et al., 2014). Recently, also neural methods have been proposed and successfully used for essay scoring (Taghipour and Ng, 2016). Most essay scoring approaches in recent years have been evaluated either on proprietary datasets or on a few publicly available ones. Not publicly available data include datasets used by Klebanov et al. (2016) with large amounts of college level exam data, or data from music teacher proficiency test (Madnani et al., 2016). Responses in this last dataset are in length on the borderline between short answers and essays and are interesting because they, as well as our corpus, target writings by generally language-proficient population. The dominating publicly available dataset for essay scoring"
W17-5040,W16-0507,0,0.138315,"es with a language proficiency level of its writer, therefore allowing to use them for language proficiency assessment. That means their labels do not necessarily reflect the proficiency of the current essay, but rather the general language proficiency of the writer. For example, the ETS corpus of non-native written English (Blanchard et al., 2013) contains 12,100 TOEFL test essays and has originally been published for the task of native-language identification (Tetreault et al., 2013), but also comes with coarse proficiency levels and has been used for the task of proficiency classification (Klebanov et al., 2016; Vajjala, 2017). Similarly the ICNALE corpus (Ishikawa, 2011) contains English essays from Asian writers where each essay has an assigned proficiency level. Beyond the English language, proficiency classification has been performed on the Swedish SweLL corpus (Volodina et al., 2016; Pil´an et al., 2016), and for Estonian (Vajjala and L˙eo, 2014). Related Work Automatic essay scoring is almost always tackled as a machine learning task (Dikli, 2006; Valenti et al., 2003). A wide range of features representing different aspects contributing to a good essay have been proposed such, as n-grams (Ch"
W17-5040,W13-1706,0,0.0253051,"a higher degree prompt-specific). Some other corpora were originally not designed for the task of essay scoring, but each sample comes with a language proficiency level of its writer, therefore allowing to use them for language proficiency assessment. That means their labels do not necessarily reflect the proficiency of the current essay, but rather the general language proficiency of the writer. For example, the ETS corpus of non-native written English (Blanchard et al., 2013) contains 12,100 TOEFL test essays and has originally been published for the task of native-language identification (Tetreault et al., 2013), but also comes with coarse proficiency levels and has been used for the task of proficiency classification (Klebanov et al., 2016; Vajjala, 2017). Similarly the ICNALE corpus (Ishikawa, 2011) contains English essays from Asian writers where each essay has an assigned proficiency level. Beyond the English language, proficiency classification has been performed on the Swedish SweLL corpus (Volodina et al., 2016; Pil´an et al., 2016), and for Estonian (Vajjala and L˙eo, 2014). Related Work Automatic essay scoring is almost always tackled as a machine learning task (Dikli, 2006; Valenti et al.,"
W17-5040,W16-0524,0,0.11054,"99), ¨ length (Mahana et al., 2012; Ostling, 2013), linguistic correctness in terms of spelling and ¨ grammar (Mahana et al., 2012; Ostling, 2013), or cohesion and coherence of a text through identifying overlap between sentences and usage of connective devices (Lei et al., 2014). Recently, also neural methods have been proposed and successfully used for essay scoring (Taghipour and Ng, 2016). Most essay scoring approaches in recent years have been evaluated either on proprietary datasets or on a few publicly available ones. Not publicly available data include datasets used by Klebanov et al. (2016) with large amounts of college level exam data, or data from music teacher proficiency test (Madnani et al., 2016). Responses in this last dataset are in length on the borderline between short answers and essays and are interesting because they, as well as our corpus, target writings by generally language-proficient population. The dominating publicly available dataset for essay scoring in recent year has been the data of the ASAP essay scoring challenge.1 It contains both source-based and opinion tasks targeting US students from grade 7 to 10 for 8 different prompts with up to 3000 responses"
W17-5040,W13-1705,0,0.258007,"Missing"
W17-5040,W14-3509,0,0.0275512,"Missing"
W17-5040,C16-1198,1,0.891253,"Missing"
W17-5040,W08-1006,0,0.0755649,"Missing"
W17-5040,P12-2064,0,0.0253389,"o 3000 responses per prompt. Since its release in 2012, the dataset has been widely used in a number of approaches (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Cummins et al., 2016). Another dataset, the CLC-FCE corpus (Yannakoudakis et al., 2011) contains essays written by ESOL test takers, but relatively little data per individual prompt (1,244 essays across 10 prompts), making it not the first choice for promptspecific approaches. Because of its extensive error annotations, it has also been used for the task of grammatical error detection and correction (e.g. Cahill et al. (2013) and Seo et al. (2012)). In Swedish, a corpus of high school essays has ¨ been released by Ostling (2013) with an overall 1 3 A more Challenging Essay Dataset As described above, most datasets are either small or target a wide range of proficiency levels, so that relatively shallow features are sufficient to achieve quite good performance. To overcome this problem, we have created a new dataset from essays written in German by prospective university students, mostly native speakers. The essays are one part of a large-scale assessment project at the University of Duisburg-Essen, SkaLa (Bremerich-Vos and Scholten-Ako"
W17-5040,P11-1019,0,0.431284,"ys and are interesting because they, as well as our corpus, target writings by generally language-proficient population. The dominating publicly available dataset for essay scoring in recent year has been the data of the ASAP essay scoring challenge.1 It contains both source-based and opinion tasks targeting US students from grade 7 to 10 for 8 different prompts with up to 3000 responses per prompt. Since its release in 2012, the dataset has been widely used in a number of approaches (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Cummins et al., 2016). Another dataset, the CLC-FCE corpus (Yannakoudakis et al., 2011) contains essays written by ESOL test takers, but relatively little data per individual prompt (1,244 essays across 10 prompts), making it not the first choice for promptspecific approaches. Because of its extensive error annotations, it has also been used for the task of grammatical error detection and correction (e.g. Cahill et al. (2013) and Seo et al. (2012)). In Swedish, a corpus of high school essays has ¨ been released by Ostling (2013) with an overall 1 3 A more Challenging Essay Dataset As described above, most datasets are either small or target a wide range of proficiency levels, so"
W17-5908,S13-2048,1,0.842096,"ignore spelling mistakes as far as possible, automatic scoring methods must include a spell-checking component to normalize an occurrence of vineager to vinegar. Thus spell-checking components are also a part of some content scoring systems, such as the top two performing systems in the ASAP challenge (Tandalla, 2012; Zbontar, 2012). However, it is unclear what impact spelling errors really have on the performance of content scoring systems. Many systems in the ASAP challenge, as well as some participating systems in the SemEval 2013 Student Response Analysis Task (Heilman and Madnani, 2013; Levy et al., 2013), used shallow features such as token n-grams (Zbontar, 2012; Conort, 2012). If a token in the test data is misspelled, then there is no way of knowing that it has the same meaning as the correct spelling of the word in the training data. At the same time, individual spelling error instances are often not occurring uniquely in a dataset: Depending on factors such as the learner group (for example native speakers or language learners with a certain native language) or the data collection method (handwriting vs. typing) some spelling errors will occur frequently while others will be rare. The mi"
W17-5908,P17-2086,0,0.0622231,"Missing"
W17-5908,C16-1198,1,0.809285,"that these words can be more important in content scoring than small function words. Therefore, those realistic errors might do more harm than random errors. In both conditions, Experiment 2 – Simulating Increased Error Frequencies As we have seen in the above experimental study, there is little difference between the scoring qual51 0.70 mance. In general, spell checking is often used as a preprocessing step in educational applications, especially those dealing with input written by learners, either non-natives or natives. In some works the influence of spell-checking is explicitly addressed. Pilan et al. (2016) predict the proficiency level of language learners using textbook material as training data and find that spell-checking improves classification performance. Keiper et al. (2016) show that normalizing reading comprehension answers written by language learners is beneficial for POS tagging accuracy. In some areas however, spelling errors can also be a useful source of information: In the related domain of native language identification, a discipline also dealing with learner texts, Chen et al. (2017) found that spelling errors provide valuable information when determining the native language o"
W17-5908,P14-5011,1,0.759826,"n one classification model for each of the ten ASAP prompts, using the published data split into training data and “public leaderbord” data for testing. We preprocess the data using the ClearNLP segmenter and POS tagger provided through DKPro Core (Eckart de Castilho and Gurevych, 2014). We use a standard feature set often used in content scoring (Higgins et al., 2014) and extract token 1–3 grams and character 2–4 grams using the top 10,000 most frequent n-grams in each feature group. We then train a SVM classifier (Hall et al., 2009) with default parameter settings provided through DKPro TC (Daxenberger et al., 2014). We evaluate using both accuracy and quadratically weighted kappa (QWK, Cohen (1968)), as proposed in the Kaggle competition for this dataset and present results averaged across all 10 prompts. One important property of this feature setup is that the character n-gram features could be able to cover useful information from misspelled words. If the word experiment is, for example, misspelled as expirment, there are n-grams shared between these two versions, such as the character trigrams exp, men or ent. Therefore, we also use a reduced feature set, where we only work with token n-gram features"
W17-5908,P13-4001,0,0.021811,"Missing"
W17-5908,W14-5201,0,0.0605644,"Missing"
W17-5908,S13-2046,0,0.239419,"r answers can simply try to ignore spelling mistakes as far as possible, automatic scoring methods must include a spell-checking component to normalize an occurrence of vineager to vinegar. Thus spell-checking components are also a part of some content scoring systems, such as the top two performing systems in the ASAP challenge (Tandalla, 2012; Zbontar, 2012). However, it is unclear what impact spelling errors really have on the performance of content scoring systems. Many systems in the ASAP challenge, as well as some participating systems in the SemEval 2013 Student Response Analysis Task (Heilman and Madnani, 2013; Levy et al., 2013), used shallow features such as token n-grams (Zbontar, 2012; Conort, 2012). If a token in the test data is misspelled, then there is no way of knowing that it has the same meaning as the correct spelling of the word in the training data. At the same time, individual spelling error instances are often not occurring uniquely in a dataset: Depending on factors such as the learner group (for example native speakers or language learners with a certain native language) or the data collection method (handwriting vs. typing) some spelling errors will occur frequently while others"
W18-0550,C12-1089,0,0.0304837,"6) for cross-lingual information retrieval) or translate features in a learned model ((Shi et al., 2010)), many approaches rely on having similar training data in both languages, often by means of parallel or comparable corpora (Gliozzo and Strapparava, 2006). If such corpora are not available, as is the case for our scenario, leveraging machine translation to create training data for handling a new language or to transfer test data into a language for which training data exists has been explored for example by Fortuna and ShaweTaylor, while other approaches use cross-lingual word embeddings (Klementiev et al., 2012). (A) Plastic type B was the superior in both trial 1 and trial 2. (B) Record the weight that was put on to show how much effected each plastic. Also conducting more trials (. . . ) After translating the answer automatically to German and back to English it looks like this: Type B plastic was the supervisor in both Trial 1 and Trial 2. (B) Write down the weight that was put on to show how much each one has made plastic. Also do more experiments (. . . ) 6 Conclusion In this paper we showed the general feasibility of cross-lingual short-answer scoring. We also identified a number of challenges:"
W18-0550,P07-1123,0,0.0604861,"translate train translate test Test 1 2 10 ∅ T EN E NT DE D E2T .49 .49 .08 .07 .46 .46 .34 .34 D ET D ET EN E N2T .41 .43 .39 .36 .39 .44 .40 .41 EN E N2T D ET D ET .35 .55 .08 .03 .43 .46 .29 .35 DE D E2T E NT E NT .26 .41 .35 .38 .33 .32 .31 .37 Table 7: Double translation in cross-lingual setting 5 Related Work To the best of our knowledge, there are no previous approaches to cross-lingual scoring in the educational domain. However, cross-lingual NLP approaches have been successfully used for a variety of tasks, including information retrieval (Oard and Diekema, 1998), sentiment analysis (Mihalcea et al., 2007) and textual similarity (Mohammad et al., 2007; Potthast et al., 2008). While in some of these approaches, dictionaries are used as the bridge the gap between languages to translate search queries (e.g. by Ballesteros and Croft (1996) for cross-lingual information retrieval) or translate features in a learned model ((Shi et al., 2010)), many approaches rely on having similar training data in both languages, often by means of parallel or comparable corpora (Gliozzo and Strapparava, 2006). If such corpora are not available, as is the case for our scenario, leveraging machine translation to creat"
W18-0550,D07-1060,1,0.720422,"EN E NT DE D E2T .49 .49 .08 .07 .46 .46 .34 .34 D ET D ET EN E N2T .41 .43 .39 .36 .39 .44 .40 .41 EN E N2T D ET D ET .35 .55 .08 .03 .43 .46 .29 .35 DE D E2T E NT E NT .26 .41 .35 .38 .33 .32 .31 .37 Table 7: Double translation in cross-lingual setting 5 Related Work To the best of our knowledge, there are no previous approaches to cross-lingual scoring in the educational domain. However, cross-lingual NLP approaches have been successfully used for a variety of tasks, including information retrieval (Oard and Diekema, 1998), sentiment analysis (Mihalcea et al., 2007) and textual similarity (Mohammad et al., 2007; Potthast et al., 2008). While in some of these approaches, dictionaries are used as the bridge the gap between languages to translate search queries (e.g. by Ballesteros and Croft (1996) for cross-lingual information retrieval) or translate features in a learned model ((Shi et al., 2010)), many approaches rely on having similar training data in both languages, often by means of parallel or comparable corpora (Gliozzo and Strapparava, 2006). If such corpora are not available, as is the case for our scenario, leveraging machine translation to create training data for handling a new language or"
W18-0550,E09-1065,0,0.0372517,"of datasets is available in English, so this is a realistic option for the source language. We use German as the target language due to familiarity with the language, as we need to be able to manually score the new dataset. Also, the expected translation quality between English and German is rather high providing a good test case for the feasibility of the approach in general. There is a set of publicly available English datasets that we could base our experiments on: The ASAP-2 short answer scoring dataset 4 , the Powergrading dataset by (Basu et al., 2013), the computer science dataset by (Mohler and Mihalcea, 2009), and the SemEval2013 dataset (Dzikovska et al., 2013). When deciding for a dataset, we took the following criteria into account: First, all necessary prompt material has to be completely available, including reading texts or connected images. This requirement rules out the SemEval2013 data, where the prompt contains pictures and graphs (such a drawing of a electrical circuit) that are necessary to answer the questions but that are not included in the dataset. Table 2: Monolingual scoring seperate instead of separate, which are both correctly translated to the German getrennt. Second, the less"
W18-0550,Q13-1032,0,0.0503483,"a are already available in one language. The majority of datasets is available in English, so this is a realistic option for the source language. We use German as the target language due to familiarity with the language, as we need to be able to manually score the new dataset. Also, the expected translation quality between English and German is rather high providing a good test case for the feasibility of the approach in general. There is a set of publicly available English datasets that we could base our experiments on: The ASAP-2 short answer scoring dataset 4 , the Powergrading dataset by (Basu et al., 2013), the computer science dataset by (Mohler and Mihalcea, 2009), and the SemEval2013 dataset (Dzikovska et al., 2013). When deciding for a dataset, we took the following criteria into account: First, all necessary prompt material has to be completely available, including reading texts or connected images. This requirement rules out the SemEval2013 data, where the prompt contains pictures and graphs (such a drawing of a electrical circuit) that are necessary to answer the questions but that are not included in the dataset. Table 2: Monolingual scoring seperate instead of separate, which are both"
W18-0550,D10-1103,0,0.0552834,"Missing"
W18-0550,P06-1070,0,0.0402405,"cessfully used for a variety of tasks, including information retrieval (Oard and Diekema, 1998), sentiment analysis (Mihalcea et al., 2007) and textual similarity (Mohammad et al., 2007; Potthast et al., 2008). While in some of these approaches, dictionaries are used as the bridge the gap between languages to translate search queries (e.g. by Ballesteros and Croft (1996) for cross-lingual information retrieval) or translate features in a learned model ((Shi et al., 2010)), many approaches rely on having similar training data in both languages, often by means of parallel or comparable corpora (Gliozzo and Strapparava, 2006). If such corpora are not available, as is the case for our scenario, leveraging machine translation to create training data for handling a new language or to transfer test data into a language for which training data exists has been explored for example by Fortuna and ShaweTaylor, while other approaches use cross-lingual word embeddings (Klementiev et al., 2012). (A) Plastic type B was the superior in both trial 1 and trial 2. (B) Record the weight that was put on to show how much effected each plastic. Also conducting more trials (. . . ) After translating the answer automatically to German"
W18-0550,L18-1365,1,0.833903,"and translating data from German to English reduces TTR while translating from English to German increases it. Some part of the difference, however, cannot be explained by the different languages and must come from the learner population, which is more homogeneous in the English version (highschool students) as compared to the German version (crowd-workers). 4.1 Experimental setup For our scoring experiments, we use a standard supervised machine learning setup with Weka’s SVM classifier in standard configuration as classification backbone, implemented using free-text scoring toolkit ESCRITO (Zesch and Horbach, 2018). We use token uni-, bi- and trigrams as well as character bi- to five-grams as features and evaluate our results using accuracy and quadratically weighted Kappa (Cohen, 1968). The English ASAP dataset comes with an established split into train and test data, which we reuse. The German dataset is very small in direct comparison, so that we cannot use a fixed split into training and test data. Therefore, we use 10-fold cross-validation for the German dataset. Vocabulary overlap Here, we compute the overlap between the vocabulary used in the English data and the vocabulary of the German dataset."
W18-0550,W12-2022,0,0.0423728,"Missing"
W18-7103,darwish-etal-2014-using,0,0.0602218,"Missing"
W18-7103,L16-1170,0,0.62727,"Missing"
W18-7103,W02-2025,0,0.231294,"Missing"
W18-7103,pasha-etal-2014-madamira,0,0.22371,"Missing"
W19-4004,S17-2001,0,0.0280257,", and natural language generation (Agirre et al., 2013). We use it in this paper to quantify the strength of relationship on a continuous scale. Given two linguistic expressions, semantic text similarity measures the degree of semantic equivalence (Agirre et al., 2013). For example, (a) and (b) have a semantic similarity score of 5 (on a scale from 0-5 as used in the SemEval STS task) (Agirre et al., 2013, 2014). Interaction between entailment and specificity Specificity was involved in rules for the recognition of textual entailment (Bobrow et al., 2007). Interaction with semantic similarity Cer et al. (2017) argue that to find paraphrases or entailment, some level of semantic similarity must be given. Furthermore, Cer et al. (2017) state that although semantic similarity includes both entailment and paraphrasing, it is different, as it has a gradation and not a binary measure of the semantic overlap. Based on their corpus, Marelli et al. (2014) state that paraphrases, entailment, and contradiction have a high similarity score; paraphrases having the highest and contradiction the lowest of them. There also was practical work using the interaction between semantic similarity and entailment: Yokote"
W19-4004,S14-2010,0,0.0177881,"theoretical settings (Bhagat and Hovy, 2013). The standard approach in most of the existing paraphrasing and entailment datasets is to use a more 4 29 Louis and Nenkova (2012) labelled individual sentences the task only in one direction. If the originally first sentence is more specific, it is forward specificity (FSpec), whereas if the originally second sentence is more specific than the first, it is backward specificity (BSpec). Semantic Similarity For semantic similarity (Sim), we do not only ask whether the pair is related, but rate the similarity on a scale 0-5. Unlike previous studies (Agirre et al., 2014), we decided not to provide explicit definitions for every point on the scale. Annotation Quality To ensure the quality of the annotations, we include 10 control pairs, which are hand-picked and slightly modified pairs from the original corpus, in each task.5 We discard workers who perform bad on the control pairs. 6 3.4 ing the approach used with semantic similarity, we also calculated Cohen’s kappa between each annotator and the majority vote for their pairs. We report the average kappa for each task.7 Table 2 shows the overall inter-annotator agreement for the binary tasks. We report: 1) th"
W19-4004,S13-1004,0,0.0214981,"ine translation (Pad´o et al., 1 Mostly, contradiction is regarded as one of the relations within an entailment annotation. 26 Proceedings of the 13th Linguistic Annotation Workshop, pages 26–36 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics intuition. Kovatchev et al. (2018) emphasize the similarity between linguistic phenomena underlying paraphrasing and entailment. There has been practical work on using paraphrasing to solve entailment (Bosma and Callison-Burch, 2006). 2009), question answering (Harabagiu and Hickl, 2006), and natural language generation (Agirre et al., 2013). We use it in this paper to quantify the strength of relationship on a continuous scale. Given two linguistic expressions, semantic text similarity measures the degree of semantic equivalence (Agirre et al., 2013). For example, (a) and (b) have a semantic similarity score of 5 (on a scale from 0-5 as used in the SemEval STS task) (Agirre et al., 2013, 2014). Interaction between entailment and specificity Specificity was involved in rules for the recognition of textual entailment (Bobrow et al., 2007). Interaction with semantic similarity Cer et al. (2017) argue that to find paraphrases or ent"
W19-4004,C04-1051,0,0.719247,"Missing"
W19-4004,C12-1011,1,0.803591,"ly to have the annotations as independent as possible. This differs from the SICK corpus annotation setting, where entailment, contradiction, and semantic similarity were annotated together. The complex nature of the meaning relations makes it difficult to come up with a precise and widely accepted definition and annotation instructions for each of them. This problem has already been emphasized in previous annotation tasks and theoretical settings (Bhagat and Hovy, 2013). The standard approach in most of the existing paraphrasing and entailment datasets is to use a more 4 29 Louis and Nenkova (2012) labelled individual sentences the task only in one direction. If the originally first sentence is more specific, it is forward specificity (FSpec), whereas if the originally second sentence is more specific than the first, it is backward specificity (BSpec). Semantic Similarity For semantic similarity (Sim), we do not only ask whether the pair is related, but rate the similarity on a scale 0-5. Unlike previous studies (Agirre et al., 2014), we decided not to provide explicit definitions for every point on the scale. Annotation Quality To ensure the quality of the annotations, we include 10 co"
W19-4004,P06-1114,0,0.0807764,"etection (Alzahrani and Salim, 2010; B¨ar et al., 2012), machine translation (Pad´o et al., 1 Mostly, contradiction is regarded as one of the relations within an entailment annotation. 26 Proceedings of the 13th Linguistic Annotation Workshop, pages 26–36 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics intuition. Kovatchev et al. (2018) emphasize the similarity between linguistic phenomena underlying paraphrasing and entailment. There has been practical work on using paraphrasing to solve entailment (Bosma and Callison-Burch, 2006). 2009), question answering (Harabagiu and Hickl, 2006), and natural language generation (Agirre et al., 2013). We use it in this paper to quantify the strength of relationship on a continuous scale. Given two linguistic expressions, semantic text similarity measures the degree of semantic equivalence (Agirre et al., 2013). For example, (a) and (b) have a semantic similarity score of 5 (on a scale from 0-5 as used in the SemEval STS task) (Agirre et al., 2013, 2014). Interaction between entailment and specificity Specificity was involved in rules for the recognition of textual entailment (Bobrow et al., 2007). Interaction with semantic similarity"
W19-4004,W07-1403,0,0.0967813,"Missing"
W19-4004,L18-1221,1,0.516021,"Missing"
W19-4004,D15-1075,0,0.0553724,"lexically differing sentences in the next step. Then, a group of 15 people, further on called sentence generators, is asked to generate true and false sentences that vary lexically from the source sentence.3 Overall, 780 sentences are generated. The 13 source sentences are not considered in the further procedure. For creating the true sentences, we ask each sentence generator to create two sentences that are true and for the false sentences, two sentences that are false given one source sentence. This way of generating a sentence pool is similar to that of the textual entailment SNLI corpus (Bowman et al., 2015), where the generators were asked to create true and false captions for given images. The following are exemplary true and false sentences created from one source sentence. Source: Getting a high educational degree is important for finding a good job, especially in big cities. In many countries, girls are less likely to get a good school education. Going to school socializes kids through constant interaction with others. One important part of modern education is technology, if not the most important. Modern assistants such Cortana, Alexa, or Siri make our everyday life easier by giving quicker"
W19-4004,louis-nenkova-2012-corpus,0,0.0697476,"zation (Harabagiu and Lacatusu, 2010). The complex nature of the meaning relations makes it difficult to come up with a precise and widely accepted definition for each of them. Also, there is a difference between theoretical definitions and definitions adopted in practical tasks. In this paper, we follow the approach taken in preSpecificity is a relation between phrases in which one phrase is more precise and the other more vague. Specificity is mostly regarded between noun phrases (Cruse, 1977; Enc¸, 1991; Farkas, 2002). However, there has also been work on specificity on the sentence level (Louis and Nenkova, 2012). In the following example, (c) is more specific than (d) as it gives information on who does not get good education: (c) Girls do not get good education. (d) Some children do not get good education. Semantic Similarity between texts is not a meaning relation in itself, but rather a gradation of meaning similarity. It has often been used as a proxy for the other relations in applications such as summarization (Lloret et al., 2008), plagiarism detection (Alzahrani and Salim, 2010; B¨ar et al., 2012), machine translation (Pad´o et al., 1 Mostly, contradiction is regarded as one of the relations"
W19-4004,J10-3003,0,0.0751263,"semantic similarity and entailment: Yokote et al. (2011) and Castillo and Cardenas (2010) used semantic similarity to solve entailment. Interaction between Relations Despite the interactions and close connection of these meaning relations, to our knowledge, there exists neither an empirical analysis of the connection between them nor a corpus enabling it. We bridge this gap by creating and analyzing a corpus of sentence pairs annotated with all discussed meaning relations. Our analysis finds that previously made assumptions on some relations (e.g. paraphrasing being bi-directional entailment (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010; Sukhareva et al., 2016)) are not necessarily right in a practical setting. Furthermore, we explore the interactions of the meaning relation of specificity, which has not been extensively studied from an empirical point of view. We find that it can be found in pairs on all levels of semantic relatedness and does not correlate with entailment. 2 2.2 There are several works describing the creation, annotation, and subsequent analysis of corpora with multiple parallel phenomena. MASC The annotation of corpora with multiple phenomena in parallel has been mo"
W19-4004,J93-2004,0,0.0665745,"tudied from an empirical point of view. We find that it can be found in pairs on all levels of semantic relatedness and does not correlate with entailment. 2 2.2 There are several works describing the creation, annotation, and subsequent analysis of corpora with multiple parallel phenomena. MASC The annotation of corpora with multiple phenomena in parallel has been most notably explored within the Manually Annotated Sub-Corpus (MASC) project2 — It is a largescale, multi-genre corpus manually annotated with multiple semantic layers, including WordNet senses(Miller, 1998), Penn Treebank Syntax (Marcus et al., 1993), and opinions. The multiple layers enable analyses between several phenomena. SICK is a corpus of around 10,000 sentence pairs that were annotated with semantic similarity and entailment in parallel (Marelli et al., 2014). As it is the corpus that is the most similar to our work, we will compare some of our annotation decisions and results with theirs. Sukhareva et al. (2016) annotated subclasses of entailment, including paraphrase, forward, revert, and null on propositions extracted from docRelated Work To our knowledge, there is no other work where the discussed meaning relations have been"
W19-4004,marelli-etal-2014-sick,0,0.595543,"scale from 0-5 as used in the SemEval STS task) (Agirre et al., 2013, 2014). Interaction between entailment and specificity Specificity was involved in rules for the recognition of textual entailment (Bobrow et al., 2007). Interaction with semantic similarity Cer et al. (2017) argue that to find paraphrases or entailment, some level of semantic similarity must be given. Furthermore, Cer et al. (2017) state that although semantic similarity includes both entailment and paraphrasing, it is different, as it has a gradation and not a binary measure of the semantic overlap. Based on their corpus, Marelli et al. (2014) state that paraphrases, entailment, and contradiction have a high similarity score; paraphrases having the highest and contradiction the lowest of them. There also was practical work using the interaction between semantic similarity and entailment: Yokote et al. (2011) and Castillo and Cardenas (2010) used semantic similarity to solve entailment. Interaction between Relations Despite the interactions and close connection of these meaning relations, to our knowledge, there exists neither an empirical analysis of the connection between them nor a corpus enabling it. We bridge this gap by creati"
W19-4004,W09-0404,0,0.0866955,"Missing"
W19-4004,L16-1338,0,0.030854,"tillo and Cardenas (2010) used semantic similarity to solve entailment. Interaction between Relations Despite the interactions and close connection of these meaning relations, to our knowledge, there exists neither an empirical analysis of the connection between them nor a corpus enabling it. We bridge this gap by creating and analyzing a corpus of sentence pairs annotated with all discussed meaning relations. Our analysis finds that previously made assumptions on some relations (e.g. paraphrasing being bi-directional entailment (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010; Sukhareva et al., 2016)) are not necessarily right in a practical setting. Furthermore, we explore the interactions of the meaning relation of specificity, which has not been extensively studied from an empirical point of view. We find that it can be found in pairs on all levels of semantic relatedness and does not correlate with entailment. 2 2.2 There are several works describing the creation, annotation, and subsequent analysis of corpora with multiple parallel phenomena. MASC The annotation of corpora with multiple phenomena in parallel has been most notably explored within the Manually Annotated Sub-Corpus (MAS"
zesch-etal-2008-extracting,E06-1002,0,\N,Missing
zesch-etal-2008-extracting,W07-0201,1,\N,Missing
zesch-etal-2008-extracting,N07-2052,1,\N,Missing
zesch-etal-2008-extracting,P07-1130,1,\N,Missing
zesch-etal-2008-extracting,D07-1093,0,\N,Missing
zesch-gurevych-2010-better,W06-2501,0,\N,Missing
zesch-gurevych-2010-better,W04-2607,0,\N,Missing
zesch-gurevych-2010-better,P06-1040,0,\N,Missing
zesch-gurevych-2010-better,P99-1020,0,\N,Missing
zesch-gurevych-2010-better,N07-2052,1,\N,Missing
zesch-gurevych-2010-better,D07-1090,0,\N,Missing
zesch-gurevych-2010-better,I05-1067,1,\N,Missing
zesch-gurevych-2010-better,J06-1003,0,\N,Missing
zesch-gurevych-2010-better,zesch-etal-2008-extracting,1,\N,Missing
