1994.amta-1.10,H93-1038,1,0.792898,"Missing"
1994.amta-1.10,1993.tmi-1.4,1,0.733597,"Missing"
1996.amta-1.10,H92-1030,0,0.0203237,"Missing"
1996.amta-1.10,J93-1002,0,0.0148156,"available for that language, in particular the hand-corrected parsed text of the Penn Tree Bank (Marcus et al., 1993). See Carroll and Charniak (1992) and Lari and Young (1990) for examples of this approach. In our experiments, a small, core grammar for the languages under development (in our case, Russian and Serbo-Croatian) is used instead of the information provided by the parsed Penn Tree Bank corpus. This 98 grammar will &quot;seed&apos;&apos; the development of a robust, large-scale grammar for these languages. This approach has been discussed, for example, in the work of Pereira and Schabes (1992) or Briscoe and Carroll (1993). We plan to combine this non-automatic top-down approach with any number of bottom-up organizing approaches, for instance, N-gram analyses of corpus which extend the methods applied by Finch and Chater (1991). 4 Rapid Lexical Acquisition MT-related lexicons at CRL include both large bilingual terminological glossaries and deep-coverage computational-semantic lexicons to support various MT engines. For rapid development, acquisition of all these resources must be enhanced. 4.1 Corpus-Based Bilingual Glossary Acquisition Bilingual glossaries support the glossary-based translation engine in the"
1996.amta-1.10,1994.amta-1.8,0,0.0103157,"work continues to advance the framework of the Pangloss MT project (e.g., Nirenburg (ed.), 1994), in which the word-for-word, glossary-based, transfer-based, knowledge-based and example-based translation engines have been used in a variety of configurations. 2.1 Reuse of Off-The-Shelf Linguistic Components In order to develop working prototypes of lower-end modules for the MT engines in a short time, offthe-shelf linguistic components were used whenever possible, in the spirit of, for example, work at Cm in Montreal (e.g., Isabelle, 1992). For example, the SPOST Spanish part-of-speech tagger (Farwell et al., 1994), the Juman Japanese morphological analyzer (Matsumoto et al., 1993) and the Penman English morphological generator (Penman, 1988) were integrated. Often, off-the-shelf resources were used as input for a semi-automatic process of knowledge acquisition. Thus, in our experiments Spanish, Japanese and Arabic bilingual dictionaries were derived from corresponding machine-readable versions of paper dictionaries using the LexBase approach (Guthrie et al., 1993). In reusing off-the-shelf components, a major problem has been the lack of compatibility between linguistic representations used by the vari"
1996.amta-1.10,A94-1016,1,0.838025,"oduces generation lexicons from analysis lexicons. 6. Automatic testing methods enable rapid validation of automatically acquired resources. 2 The Core Environment The Computing Research Laboratory (CRL) has developed a multi-engine architecture designed for rapid development of machine translation functionalities in a translator&apos;s workstation. This architecture is a descendant of the approach to multi-engine human-aided machine translation (HAMT) developed by Sergei Nirenburg and his associates at CMU CMT (Nirenburg et al., 1992; 1994; Frederking et al., 1993; Nirenburg and Frederking, 1994; Frederking and Nirenburg, 1994). At present, it serves as the environment for HAMT from Japanese, Arabic, Russian and Spanish to English. The system includes a general multilingual document handling environment supporting the translator&apos;s workstation and a basic multilingual machine translation architecture. Emphasis is made on reusing off-the-shelf linguistic components and on empirical, corpus-based acquisition of bilingual glossaries. The translator&apos;s workstation is designed to be used concurrently with information extraction and retrieval tools. To this end, the Temple Translator&apos;s Workstation, which provides an integra"
1996.amta-1.10,H93-1038,1,0.771772,"ion of manually acquired lexicons. 5. Lexicon reversal produces generation lexicons from analysis lexicons. 6. Automatic testing methods enable rapid validation of automatically acquired resources. 2 The Core Environment The Computing Research Laboratory (CRL) has developed a multi-engine architecture designed for rapid development of machine translation functionalities in a translator&apos;s workstation. This architecture is a descendant of the approach to multi-engine human-aided machine translation (HAMT) developed by Sergei Nirenburg and his associates at CMU CMT (Nirenburg et al., 1992; 1994; Frederking et al., 1993; Nirenburg and Frederking, 1994; Frederking and Nirenburg, 1994). At present, it serves as the environment for HAMT from Japanese, Arabic, Russian and Spanish to English. The system includes a general multilingual document handling environment supporting the translator&apos;s workstation and a basic multilingual machine translation architecture. Emphasis is made on reusing off-the-shelf linguistic components and on empirical, corpus-based acquisition of bilingual glossaries. The translator&apos;s workstation is designed to be used concurrently with information extraction and retrieval tools. To this en"
1996.amta-1.10,P91-1025,0,0.0146306,"destroy will have the semantics of an EVENT, as will the noun destruction (naturally, with a different linking in the syntax-semantics interface). Similarly, destroyer (as a person) would be represented using the same event with the addition of a HUMAN as a filler of the agent case-role. This built-in transcategoriality strongly facilitates applications such as interlingual MT, as it renders vacuous many problems connected with category mismatches and misalignments that plague those paradigms in MT that do not rely on extracting language-neutral text meaning representations (e.g., Dorr 1995; Kameyama, Ochitani, and Peters, 1991). The following phenomena seem to be appropriate for treatment with LRs: • Inflected Forms - Specifically, those inflectional phenomena which accompany changes in subcategorization frame (passivization, dative alternation, etc.). • Word Formation - The production of derived forms by LR is illustrated in a case study below, and includes formation of deverbal nominals (destruction, running), agentive nouns (catcher). Typically involving a shift in syntactic category, these LRs are often less productive than inflection-oriented ones. Consequently, derivational LRs are even more prone to overgene"
1996.amta-1.10,J93-2004,0,0.0303024,"sers for any language (Brill, 1994; Brill and Marcus, 1994), though they do not yet provide a mature enough technology for general use. CRL is working on semi-automatic development of robust grammars for languages with limited on-line resources. Limiting the resources is vital since the languages for which rapid MT development is most important are frequently those where electronic resources are scarce. Results reported to date refer to developing grammars for English using the extensive resources available for that language, in particular the hand-corrected parsed text of the Penn Tree Bank (Marcus et al., 1993). See Carroll and Charniak (1992) and Lari and Young (1990) for examples of this approach. In our experiments, a small, core grammar for the languages under development (in our case, Russian and Serbo-Croatian) is used instead of the information provided by the parsed Penn Tree Bank corpus. This 98 grammar will &quot;seed&apos;&apos; the development of a robust, large-scale grammar for these languages. This approach has been discussed, for example, in the work of Pereira and Schabes (1992) or Briscoe and Carroll (1993). We plan to combine this non-automatic top-down approach with any number of bottom-up orga"
1996.amta-1.10,C94-1101,0,0.0552497,"Missing"
1996.amta-1.10,C92-3149,0,0.0929712,"entations) enable rapid expansion of manually acquired lexicons. 5. Lexicon reversal produces generation lexicons from analysis lexicons. 6. Automatic testing methods enable rapid validation of automatically acquired resources. 2 The Core Environment The Computing Research Laboratory (CRL) has developed a multi-engine architecture designed for rapid development of machine translation functionalities in a translator&apos;s workstation. This architecture is a descendant of the approach to multi-engine human-aided machine translation (HAMT) developed by Sergei Nirenburg and his associates at CMU CMT (Nirenburg et al., 1992; 1994; Frederking et al., 1993; Nirenburg and Frederking, 1994; Frederking and Nirenburg, 1994). At present, it serves as the environment for HAMT from Japanese, Arabic, Russian and Spanish to English. The system includes a general multilingual document handling environment supporting the translator&apos;s workstation and a basic multilingual machine translation architecture. Emphasis is made on reusing off-the-shelf linguistic components and on empirical, corpus-based acquisition of bilingual glossaries. The translator&apos;s workstation is designed to be used concurrently with information extraction"
1996.amta-1.10,H94-1026,1,0.817895,"lexicons. 5. Lexicon reversal produces generation lexicons from analysis lexicons. 6. Automatic testing methods enable rapid validation of automatically acquired resources. 2 The Core Environment The Computing Research Laboratory (CRL) has developed a multi-engine architecture designed for rapid development of machine translation functionalities in a translator&apos;s workstation. This architecture is a descendant of the approach to multi-engine human-aided machine translation (HAMT) developed by Sergei Nirenburg and his associates at CMU CMT (Nirenburg et al., 1992; 1994; Frederking et al., 1993; Nirenburg and Frederking, 1994; Frederking and Nirenburg, 1994). At present, it serves as the environment for HAMT from Japanese, Arabic, Russian and Spanish to English. The system includes a general multilingual document handling environment supporting the translator&apos;s workstation and a basic multilingual machine translation architecture. Emphasis is made on reusing off-the-shelf linguistic components and on empirical, corpus-based acquisition of bilingual glossaries. The translator&apos;s workstation is designed to be used concurrently with information extraction and retrieval tools. To this end, the Temple Translator&apos;s Works"
1996.amta-1.10,P92-1017,0,0.0125483,"using the extensive resources available for that language, in particular the hand-corrected parsed text of the Penn Tree Bank (Marcus et al., 1993). See Carroll and Charniak (1992) and Lari and Young (1990) for examples of this approach. In our experiments, a small, core grammar for the languages under development (in our case, Russian and Serbo-Croatian) is used instead of the information provided by the parsed Penn Tree Bank corpus. This 98 grammar will &quot;seed&apos;&apos; the development of a robust, large-scale grammar for these languages. This approach has been discussed, for example, in the work of Pereira and Schabes (1992) or Briscoe and Carroll (1993). We plan to combine this non-automatic top-down approach with any number of bottom-up organizing approaches, for instance, N-gram analyses of corpus which extend the methods applied by Finch and Chater (1991). 4 Rapid Lexical Acquisition MT-related lexicons at CRL include both large bilingual terminological glossaries and deep-coverage computational-semantic lexicons to support various MT engines. For rapid development, acquisition of all these resources must be enhanced. 4.1 Corpus-Based Bilingual Glossary Acquisition Bilingual glossaries support the glossary-ba"
1996.amta-1.10,J91-4003,0,0.131982,"ontaining entries corresponding to word senses, without reference to their part of speech (the senses of both the noun and the verb &quot;walk,&quot; for example, will be listed inside the same superentry). Each word meaning is identified by a unique identifier, or lexeme (Mel&apos;cuk et al., 1984; Onyshkevych and Nirenburg, 1995). No distinction is made between homonyms and polysemous words, and all homonyms and all meaning shifts of polysemous words are under one single superentry, adopting Fillmore&apos;s (1971) rather than Weinreich&apos;s approach (1964). Moreover, for &quot;logically polysemous&quot; words as defined by Pustejovsky (1991; 1995), we keep one entry (as opposed to two in most approaches). The meaning of a lexical entry is encoded in a (lexical) semantic representation language whose primitives are predominantly terms in an independently motivated world model, or ontology (see Carlson and Nirenburg, 1990; Mahesh and Nirenburg, 1995; Mahesh, 1996). The information contained inside a lexeme is divided into zones corresponding to various levels of lexical information. The different zones include, among others, category, morphology, syntactic structure, semantic mapping, lexical relations and rules and stylistics. 4."
1996.amta-1.10,P96-1005,1,0.647079,"shifts as +count to - count or - common to +common. While shifts and modulations are important, we find that the main significance of LRs is in their promise to aid the task of massive lexical acquisition by allowing automatic generation of entries from a set of &quot;core&quot; entries acquired manually. The major problem in using LRs is overgeneration: LR generators suggest inappropriate forms which need to be weeded out by humans. This problem is usually overlooked if the goal of studying the LRs is theoretical (as is the case in many of the above-mentioned approaches). In practice, as we argue in (Viegas et al., 1996b), the costs of using LRs in knowledge acquisition must be carefully weighed against the benefits. The lexicon for which our LRs are introduced is intended mainly to support the computational specification and use of text meaning representations used in the KBMT engine, though the syntactic, morphological, pragmatic, stylistic and other information stored there will also aid other MT engines (see, e.g., Onyshkevych and Nirenburg, 1995). The acquisition of such a lexicon, with or without the LR mechanism, involves a substantial investment of resources. The LR processor applies to each word sen"
1997.tmi-1.1,C92-2070,0,0.0744322,"Missing"
1997.tmi-1.18,P96-1005,1,0.792955,"Missing"
1997.tmi-1.18,C92-2070,0,0.0485992,"s remarkably, they demand detailed knowledge of possible scenarios. It is often prohibitively expensive to acquire such knowledge for general purpose, domain independent NLP. None of these methods used a large scale ontology (because none was available). Nor did they show that they can resolve sense ambiguities in entire texts and at the same time produce complete meaning representations for the texts. We believe that Mikrokosmos is the first successful application of a knowledge-based method for large scale word sense disambiguation and text meaning representation. Statistical methods (e.g., Yarowsky (1992)), work well on carefully chosen domains and training corpora. However, they are not as effective for processing texts from a wide variety of domains in general. Moreover, statistical methods are attractive for solving individual problems such as word sense disambiguation or part of speech tagging. They do not explain why certain meanings were chosen or how the chosen meanings together provide a meaning for a whole sentence or text, something that is often required to carry out further processing (e.g., to generate the meaning in a target language for machine translation). 6 Conclusions Resolv"
1999.tmi-1.2,C94-2203,1,0.820977,"ambiguous and 1 was 4-ways ambiguous among open-class words. The table below is for open-class words only. Sentence Number: 98000 Number of senses in lexicon: 1 2 3 4 5 6 correct: incorrect: total: 12 5 0 0 0 0 0 1 01 00 12 6 0 1 0 0 In next section we turn to the discovery of the relation holding between words. 4 Using Overt Semantics for the Translation of Chinese Compounds Compounding in Chinese is a common phenomenon (Li and Thompson 1981; Palmer and Wu, 1995; Huang 1997; Starosta et al., 1997). It is mainly used to combine 1) characters whose semantics is different and non compositional (Jin, 1994), and 2) sequences of words. In this paper we are concerned with 2) only, with a focus on nouns. The head of the compound can be identified as the last noun in a sequence of Ns. Therefore in the task of translating Chinese compounds into English compounds, where English also makes use of compounds as opposed to say French, one could adopt a transfer-based approach, where each Chinese noun is translated into English in the same sequence, (application software) or (data management system). But it gets a as in bit more complex when there is a large sequence of nouns in English, whereas it is stil"
1999.tmi-1.2,1996.amta-1.10,1,0.42691,"mantics overtly take over in the process. By de-emphasizing syntactic analysis, we mean that there is no need to produce the N-best syntactic parses. In the case of an isolating language, one could not avoid generating the exhaustive list of combinations for an ambiguous parse because of the lack of morpho-syntactic clues. In this paper, we show how an overt semantics along with lexico-syntactic dependencies coded in the lexicon can account for a syntactically ambiguous parse. The study presented here has been conducted within Mikrokosmos, a KBMT system between Spanish and Chinese to English (Nirenburg et al., 1996). In section 2, we briefly present the type of information encoded in the lexicon. In section 3, we present results on the task of WSD. In section 4, we focus on compound generation, showing how to recover the relation between words from the semantics of the co-occurring words. 2 A Brief Overview on Our Computational Lexicons The information encoded in the Mikrokosmos lexicons is distributed among various levels of lexical information (Meyer et al., 1991), relevant to phonology, orthography, 11 morphology, syntax (SYN), semantics (SEM) (as first introduced in Nirenburg and Defrise, 1991)1, syn"
1999.tmi-1.2,C98-2211,1,0.399094,"Missing"
1999.tmi-1.2,W99-0509,1,0.219481,"roscopic economic law under the situation that capital largely inflow.” We marked above in italics the compounds generated, and in bold face the links, absent in Chinese that were added by the generator. Note that our semantic approach will enable the generator to produce the verb coordinating instead of the noun coordination. This is due to what we call a transcategorial approach, where a semantic frame is not linked to a specific part of speech, providing for more room for paraphrasing at the generation level. For instance, explode and explosion will both have the exact same semantics (e.g. Viegas, 1999). RELATIONS have been so far grammaticalized as N of N or as an English compound for nouns NN, and results for our texts arc very reasonable. However, there are cases where one needs to explicitly lexicalized the relation in a compound, as we discuss in section 4. This implies to have a. high confidence on the senses selected during WSD. We present the table for the complex sentence 16 98000 above. We measure complexity in terms of number of words in a compound and numbers of markers present in the sentence. For instance, in 98000, 7 words (out of 28) were 2-ways ambiguous and 1 was 4-ways amb"
2005.mtsummit-papers.9,C94-1012,0,0.0333909,"m. For example, in the text box in Figure 1, notice that the subject “you” is included in the imperative. For content questions, an appropriate pronoun such as “who” or “where” is placed in the clause constituent that is being questioned. Every event is propositionalized. We eliminate most figurative language, except when it has theological implications. Specifically, we eliminate most instances of metonymy, synecdoche, euphemisms and idioms, and metaphors are converted to similes and the point of similarity is supplied. Other standard restrictions (like those described for the Kant system in Baker et al., 1994 and Mitamura, 1991) are employed, such as disallowing reduced relative clauses. Target Language Text Generation In this section we discuss the target language knowledge acquisition process along with a brief overview of the generation process. The knowledge acquisition interface and the text generator are integrated into The Bible Translator’s Assistant (TBTA). TBTA has been tested for English, Korean, Jula (spoken in West Africa) and Kewa (a clause chaining language spoken in Papua New Guinea). Korean, Jula and Kewa differ conceptually and structurally from English, yet in all cases the gene"
2005.mtsummit-papers.9,1991.mtsummit-papers.9,0,\N,Missing
2014.lilt-10.1,de-marneffe-etal-2006-generating,0,0.00697751,"Missing"
2014.lilt-10.1,S13-2025,0,0.0258776,"Missing"
2014.lilt-10.1,P84-1109,0,0.757754,"nder fishing, rather than switch to the compounding repository and seek concept-level generalizations. This is just one example of the larger issue of how to divvy up meaning description across lexicon and ontology, a topic discussed in McShane et al. 2005. Line 10. The next function again involves lexical search, this time attempting to determine if the NN could be a paraphrase of a N+PP construction that is recorded in the lexicon: e.g., restaurant chain can be readily analyzed if chain of X is recorded in the lexicon (and, of course, if restaurant fits the listed semantic constraints) (cf. Isabelle 1984). Recording the meanings of typical N+PP collocations is done as a matter of course in OntoAgent to assist the analyzer with the tremendously diﬃcult challenge of disambiguating prepositions. Continuing with the example chain of X, the lexical sense chain-n2 explicitly lists the PP adjunct “of X” and indicates that if it is present – and if X is of an appropriate semantic type – then the whole structure is to be interpreted as set member-type X. It would be extremely diﬃcult for the semantic analyzer to automatically select this sense of of over a dozen other productive senses of this preposit"
2014.lilt-10.1,D11-1060,0,0.0120141,"f interest and train their systems to automatically choose the relevant relation during analysis of compounds taken outside of context – i.e., presented as a list. Two methods have been used to create the inventory of relations: developer introspection, often with iterative refinement (e.g., Moldovan et al. 2004), and crowd-sourcing, also with iterative refinement (e.g., Tratz and Hovy 2010). A recent direction of development involves using paraphrases as a proxy for semantic analysis: i.e., a paraphrase of a NN that contains a preposition or a verb is treated as the meaning of that NN (e.g., Kim and Nakov 2011). Evaluations of knowledge-lean systems typically compare machine performance with human performance on a relation-selection or paraphrasing task. In most contributions within the knowledge-lean NLP paradigm, the semantics of the component nominals is not directly addressed: i.e., semantic relations are used to link uninterpreted nominals. Although this might seem incongruous from a linguistic perspective, one can find motivations for pursuing NN compounding thus defined. (1) The developers’ purview can be a narrow, technical domain that includes largely monosemous nouns (e.g., medicine, as in"
2014.lilt-10.1,J02-3004,0,0.1406,"pilots and home life could refer to the length of time that a dwelling is suitable to be lived in (by analogy with battery life). So lexical disambiguation is as central to nominal compound analysis as is the establishment of the relationship between the nouns. Nominal compounding has been pursued by descriptive linguists, psycholinguists, and practitioners of natural language processing (NLP). By way of introduction, we will first provide a brief, interpretive glimpse into some lines of work pursued by each of these communities, without repeating the fine surveys already available in, e.g., Lapata 2002, Tratz and Hovy 2010, and Lieber and Štekauer 2009. Descriptive linguists have primarily investigated constraints on the form of NN compounds and the inventory of relations that can hold between the component nouns. They have posited anywhere from 6 to 60 to “innumerable” necessary relations, depending on their evaluation of an appropriate grain-size of semantic analysis. They do not pursue algorithms for disambiguating the component nouns, presumably because the primary consumers of linguistic descriptions are people who carry out such disambiguation automatically; however, they do pay well-"
2014.lilt-10.1,W04-2609,0,0.190491,"iguate component nouns, a topic to be discussed Nominal Compound Interpretation by Intelligent Agents / 3 further below. Within recent mainstream NLP, most practically-oriented work on NN compounding belongs to the knowledge-lean paradigm. Practitioners typically select a medium-sized subset of relations of interest and train their systems to automatically choose the relevant relation during analysis of compounds taken outside of context – i.e., presented as a list. Two methods have been used to create the inventory of relations: developer introspection, often with iterative refinement (e.g., Moldovan et al. 2004), and crowd-sourcing, also with iterative refinement (e.g., Tratz and Hovy 2010). A recent direction of development involves using paraphrases as a proxy for semantic analysis: i.e., a paraphrase of a NN that contains a preposition or a verb is treated as the meaning of that NN (e.g., Kim and Nakov 2011). Evaluations of knowledge-lean systems typically compare machine performance with human performance on a relation-selection or paraphrasing task. In most contributions within the knowledge-lean NLP paradigm, the semantics of the component nominals is not directly addressed: i.e., semantic rela"
2014.lilt-10.1,W01-0511,0,0.341376,"Evaluations of knowledge-lean systems typically compare machine performance with human performance on a relation-selection or paraphrasing task. In most contributions within the knowledge-lean NLP paradigm, the semantics of the component nominals is not directly addressed: i.e., semantic relations are used to link uninterpreted nominals. Although this might seem incongruous from a linguistic perspective, one can find motivations for pursuing NN compounding thus defined. (1) The developers’ purview can be a narrow, technical domain that includes largely monosemous nouns (e.g., medicine, as in Rosario and Hearst 2001), making nominal disambiguation not a central problem.1 (2) The development eﬀort can be squarely application-oriented, with success being defined as near-term improvement to an end system, with no requirement that all aspects of NN analysis be addressed. (3) The work can be method-driven, meaning that its goal is to improve our understanding of machine learning itself, with the NN dataset being of secondary importance. (4) Systems can be built to participate in a field-wide competition, for which the rules of the game are posited externally (cf. the “Free paraphrases of noun compounds” task o"
2014.lilt-10.1,P10-1070,0,0.0300376,"ome life could refer to the length of time that a dwelling is suitable to be lived in (by analogy with battery life). So lexical disambiguation is as central to nominal compound analysis as is the establishment of the relationship between the nouns. Nominal compounding has been pursued by descriptive linguists, psycholinguists, and practitioners of natural language processing (NLP). By way of introduction, we will first provide a brief, interpretive glimpse into some lines of work pursued by each of these communities, without repeating the fine surveys already available in, e.g., Lapata 2002, Tratz and Hovy 2010, and Lieber and Štekauer 2009. Descriptive linguists have primarily investigated constraints on the form of NN compounds and the inventory of relations that can hold between the component nouns. They have posited anywhere from 6 to 60 to “innumerable” necessary relations, depending on their evaluation of an appropriate grain-size of semantic analysis. They do not pursue algorithms for disambiguating the component nouns, presumably because the primary consumers of linguistic descriptions are people who carry out such disambiguation automatically; however, they do pay well-deserved attention to"
2014.lilt-10.1,C94-2125,0,0.221758,"tion find-contextually-appropriate-NN-head-entry returns NN-INTERP 3. then use NN-INTERP. 4. Else if the lexicon contains at least one sense of N1 and N2 5. then create a set of all senses (i.e., ontological interpretations) of N1 and of N2. 6. Use reference-supported sense disambiguation for initial scoring of the likelihood of each available interpretation. 7. Pass on all available senses, with their reference-oriented scores, for subsequent evaluation. [First, narrowly specified patterns are tested.] 3 Of course, OntoAgent is not the first NLP approach to incorporate handwritten rules; see Vanderwende 1994 for discussion. 4 The format of the page precludes the traditional indentation-oriented formatting convention for algorithms. In the presentation that follows, explanatory roadmarks in bracketed italics are provided for guidance at select points. Nominal Compound Interpretation by Intelligent Agents / 11 8. If the function find-contextually-appropriate-lexically-anchoredphrasal returns NN-INTERP then use NN-INTERP. 9. Else if the function find-contextually-appropriate-NN-pattern returns NN-INTERP then use NN-INTERP. 10. Else if the function find-PP-shift-to-NN returns NN-INTERP then use NN-IN"
C96-1016,X93-1018,0,0.0261149,"Missing"
C98-2211,P89-1010,0,0.0506968,"Missing"
C98-2211,W94-0311,1,0.90354,"e Computational Semantic Approach 3 In order to account for the continuum we find in natural languages, we argue for a continuum perspective, spanning the range fl'oln free-combining words to idioms, with semantic collocations and idiosyncrasies in between as defined in (Viegas and B o u i l h m , 1994): • • i d i o s y n c r a s i e s (large coke; green jealousy) • i d i o m s (to kick the (proverbial) bucket) Formally, we go from a purely compositional approach in ""free-combining words"" to a noncompositional approach in idioms, In between, a (semi-)compositional approach is still possible. (Viegas and Bouillon, 1994) showed t h a t we can reduce the set of what are conventionally considered as idiosyncrasies by difl>rentiating ""true"" idiosyncrasies (difficult to derive or calculate) from expressions which have well-defined calculi, being compositional in nature, and t h a t have been called semantic collocations. In this paper, we further distinguish their idiosyncrasies into: • • co-occ'arrcncc L o c - i n ( ( l i s t a n c e ) = at a distance The M T T approach is very interesting as it provides a model of production well suited for generation with its different s t r a t a and also a lot of lexiealsema"
C98-2211,P96-1005,1,0.833826,"ween different classes of cooccurrences (modulo presence of derived forms in the lexicon with same or subsumed semantics). Looking at the following example, A bitter heavy big + N <=> resentment smoker eater V resent smoke eat + V + Adv <=> Adv + oppose strongly strongly oblige morally morally Adv bitterly heavily *bigly Adj-ed opposed obliged we see that after having acquired with human intervention co-occurrences belonging to tile A + N class, we can use lexical rules to deriw; the V + Adv class and also Adv + Adj-ed class. Lexical rules are a useful conceptual tool to extend a dictionary. (Viegas et al., 1996) used derivational lexical rules to extend a Spanish lexicon. We apply their approach to the production of restricted semantic co-occurrences. Note t h a t eat bi91y will be produced but then rejected, as the form bigly does not exist in a dictionary, The rules overgenerate cooccurrences. This is a minor problem for analysis than for generation. To use these derived restricted co-occurrences in generation, the output of the lexical rule processor nmst be checked. This can be (ton(; in different ways: dictionary check, corpus check and ultimately human check. Other classes, such as the ones bel"
C98-2211,J93-1007,0,\N,Missing
C98-2211,C88-2100,1,\N,Missing
I11-2002,P10-4001,0,0.150354,"Missing"
I11-2002,2005.mtsummit-papers.9,1,\N,Missing
mcshane-etal-2004-meaning,W03-0904,1,\N,Missing
nirenburg-etal-2004-rationale,W98-0703,0,\N,Missing
nirenburg-etal-2004-rationale,W98-0707,0,\N,Missing
nirenburg-etal-2004-rationale,W98-0705,0,\N,Missing
nirenburg-etal-2004-rationale,W03-0904,1,\N,Missing
nirenburg-etal-2004-rationale,P98-1013,0,\N,Missing
nirenburg-etal-2004-rationale,C98-1013,0,\N,Missing
nirenburg-etal-2004-rationale,P03-2030,0,\N,Missing
nirenburg-etal-2004-rationale,W98-0701,0,\N,Missing
P98-2216,P89-1010,0,0.0308486,"the co-occurrence is semicompositional between the base and the collocate (strong coffee, pay attention, heavy smoker, ...) • r e s t r i c t e d l e x i c a l c o - o c c u r r e n c e , where the meaning of the collocate is compositional but has a lexical idiosyncratic behavior (lecture ... student; rancid butter; sour milk). co-occurrence L o c - i n ( d i s t a n c e ) = at a distance The M T T approach is very interesting as it provides a model of production well suited for generation with its different strata and also a lot of lexicalsemantic information. It seems nevertheless that all 2Church and Hanks (1989), Smadja (1993) use statistics in their algorithms to extract collocations from texts. 3See (Iordanskaja et al., 1991) and (Ramos et al., 1994) for their use of LFs in M T T and NLG respectively. 4(Held, 1989) contrasts Hausman&apos;s base and collate to Mel&apos;tuk&apos;s keyword and LF values. 5There are about 60 LFs listed said to be universal; the lexicographic approach of Mel&apos;tuk and Zolkovsky has been applied among other languages to Russian, French, German and English. f r e e - c o m b i n i n g w o r d s (the girl ate candies) * s e m a n t i c c o l l o c a t i o n s (fast car; long book) 6 • dist"
P98-2216,W94-0311,1,0.895093,"igning LFs. They distinguish four types of syntagmatic LFs: the collocational information is listed in a static way. We believe that one of the main drawbacks of the approach is the lack of any predictable calculi on the possible expressions which can collocate with each other s e m a n t i c a l l y . 3 • • i d i o s y n c r a s i e s (large coke; green jealousy) • i d i o m s (to kick the (proverbial) bucket) Formally, we go from a purely compositional approach in &quot;free-combining words&quot; to a noncompositional approach in idioms. In between, a (semi-)compositional approach is still possible. (Viegas and Bouillon, 1994) showed that we can reduce the set of what are conventionally considered as idiosyncrasies by differentiating &quot;true&quot; idiosyncrasies (difficult to derive or calculate) from expressions which have well-defined calculi, being compositional in nature, and that have been called semantic collocations. In this paper, we further distinguish their idiosyncrasies into: • r e s t r i c t e d s e m a n t i c c o - o c c u r r e n c e , where the meaning of the co-occurrence is semicompositional between the base and the collocate (strong coffee, pay attention, heavy smoker, ...) • r e s t r i c t e d l e x"
P98-2216,P96-1005,1,0.842501,"ifferent classes of cooccurrences (modulo presence of derived forms in the lexicon with same or subsumed semantics). Looking at the following example, h + N &lt;=&gt; V bitter resentment resent heavy big smoker eater smoke eat v + hdv &lt;=&gt; Adv + oppose strongly strongly morally oblige morally + Adv bitterly heavily *bigly Adj-ed opposed obliged we see t h a t after having acquired with h u m a n intervention co-occurrences belonging to the A + N class, we can use lexical rules to derive the V + Adv class and also Adv + Adj-ed class. Lexical rules are a useful conceptual tool to extend a dictionary. (Viegas et al., 1996) used derivational lexical rules to extend a Spanish lexicon. We apply their approach to the production of restricted semantic co-occurrences. Note t h a t eat bigly will be produced but then rejected, as the form bigly does not exist in a dictionary. The rules overgenerate cooccurrences. This is a minor problem for analysis than for generation. To use these derived restricted co-occurrences in generation, the output of the lexical rule processor must be checked. This can be done in different ways: dictionary check, corpus check and ultimately h u m a n check. Other classes, such as the ones b"
P98-2216,J93-1007,0,\N,Missing
P98-2216,C88-2100,1,\N,Missing
W03-0904,1997.tmi-1.1,1,0.677897,"our example, selectional restrictions on the theme of the proposition head matched successfully: indeed, tools are artifacts. As to the restrictions on the agent, they have been found to be too weak to resolve the ambiguity completely: both the lastname and the first-name (not shown) sense of Patrick fit the selectional restrictions on the proposition head (indeed, Alex Patrick may be also be a double first name). Additional disambiguation means are required in this case. We have developed two general methods for additional sense disambiguation: dynamic tightening of selectional restrictions (Mahesh et al., 1997) and determining weighted distances among ontological concepts activated in the input (using the Ontosearch procedure, e.g., Onyshkevych, 1997). None of these methods will, incidentally, help in our example, so that additional heuristic procedures will have to be built for this type of ambiguity. Incidentally, such heuristic procedures could include evidence from a wide variety of sources, including text corpora. Supporting semantic analysis in this way should become an important direction of work in corpus-oriented computational linguistics (see further discussion below). Residual ambiguity i"
W04-0904,mcshane-etal-2004-meaning,1,0.926534,"language-independent ontology, • • • • • which is written using a metalanguage of description and currently contains around 5,500 concepts, each of which is described by an average of 16 properties. In all, the ontology contains hundreds of properties (which cover the same territory as the Qualia plus much more). Fillers for properties can be other ontological concepts or literals. An OntoSem lexicon for each language processed, which contains syntactic and semantic zones (linked using variables) as well as calls to “meaning procedures” (i.e., programs that carry out procedural semantics, see McShane et al. 2004a) when applicable. The semantic zone most frequently refers to ontological concepts, either directly or with property-based modifications, but can also describe word meaning extraontologically, for example, in terms of modality, aspect, time, etc. The current English lexicon contains approximately 12K senses, including all closed-class items and the most frequent verbs, as indicated by corpus analysis. This English lexicon took less than 1 person year to build and can (as described below) be ported to other languages. An onomasticon, or lexicon of proper names, which contains approximately 35"
W04-0904,W04-0905,1,0.552183,"eflects the meaning of the sentence He asked the UN to authorize the war, is as follows: REQUEST-ACTION-69 AGENT THEME BENEFICIARY SOURCE-ROOT-WORD TIME ACCEPT-70 THEME THEME-OF SOURCE-ROOT-WORD ORGANIZATION-71 HAS-NAME BENEFICIARY-OF SOURCE-ROOT-WORD HUMAN-72 HAS-NAME AGENT-OF SOURCE-ROOT-WORD WAR-73 THEME-OF SOURCE-ROOT-WORD HUMAN-72 ACCEPT-70 ORGANIZATION-71 ask (< (FIND-ANCHOR-TIME)) WAR-73 REQUEST-ACTION-69 authorize UNITED-NATIONS REQUEST-ACTION-69 UN COLIN POWELL REQUEST-ACTION-69 he ; ref. resolution done ACCEPT-70 war Details of this approach to text processing can be found, e.g., in Nirenburg et al. 2004a,b. The ontology itself, a brief ontology tutorial, and an extensive lexicon tutorial can be viewed at http://ilit.umbc.edu. OntoSem has been used with languages including English, Spanish, Chinese, Arabic and Persian, to varying degrees of lexical coverage (e.g., earlier, less fine-grained English and Spanish lexicons contained 40K entries and were used for MT in the Mikrokosmos project). What makes OntoSem amenable to efficient cross-linguistic usage is that many of the resources are either fully language independent (the ontology, the fact repository, the TMR metalanguage) or parameterizab"
W04-0905,W03-0904,1,\N,Missing
W04-0905,J02-3001,0,\N,Missing
W04-0906,J02-3001,0,0.00827318,"ully automatic way. At this point, we rely on TMRs that are obtained automatically but improved through human interaction (see Nirenburg et al. 2004 for details). Note that fully automatic methods for creating structured knowledge of a quality even remotely approaching that needed to support realistic QA do not at this point exist. Few of the numerous current and recent machine learning and statistical processing experiments in NLP deal with the analysis of meaning at all; and those that do address partial tasks (e.g., determining case role fillers in terms of undisambiguated text elements in Gildea and Jurafsky 2002) in a rather “knowledge-lean” manner. The results are very far away indeed from either good quality or good coverage, either in terms of phenomena and text. We believe that our approach, using as it does statistical as well as recorded-knowledge evidence for extracting, representing and manipulating meaning is the most practical and holds the most promise for the future. Indeed, it is not even as expensive as many people believe. 3 The Knowledge Support Infrastructure The process of deriving TMRs from text is implemented in our Ontosem text analyzer. Semantic analysis in OntoSem is described i"
W04-0906,C00-1072,0,0.0423847,"nce. The expressive power of the ontology and the TMR is enhanced by multivalued fillers for properties, implemented using the “facets” DEFAULT, SEM, VALUE, and RELAXABLE-TO, among others. At the time of this writing, the ontology contains about 6,000 concepts (events, objects and properties), with, on average, 16 properties each. Temporally and causally related events are encoded as values of a complex event’s HAS-EVENT-AS-PART property. These are essentially scripts that provide information that is very useful in general reasoning as well as reasoning for NLP (e.g., Schank and Abelson 1977, Lin and Hovy 2000, Clark and Porter 2000). We use scripts in the answer content determination module of the question answering system. Figure 4 illustrates a rather simple script that supports reasoning for our example question answering session. The OntoSem lexicon contains not only semantic information, it also supports morphological and syntactic analysis. Semantically, it specifies what concept, concepts, property or properties of concepts defined in the ontology must be instantiated in the TMR to account for the meaning of a given lexical unit of input. At the time of writing, the latest version of the En"
W04-0906,1997.tmi-1.1,1,0.657351,"or good coverage, either in terms of phenomena and text. We believe that our approach, using as it does statistical as well as recorded-knowledge evidence for extracting, representing and manipulating meaning is the most practical and holds the most promise for the future. Indeed, it is not even as expensive as many people believe. 3 The Knowledge Support Infrastructure The process of deriving TMRs from text is implemented in our Ontosem text analyzer. Semantic analysis in OntoSem is described in some detail in Nirenburg and Raskin 2004; Nirenburg et al., 2004; Beale et al. 1995, 1996, 2003; Mahesh et al. 1997. Our description here will be necessarily brief. Also note that the analysis process is described here as if it were a strict pipeline architecture; in reality, semantic analysis is used to inform and disambiguate syntactic analysis, for example, in cases of prepositional phrase attachment. Text analysis in OntoSem relies on the results of a battery of pre-semantic text processing modules. The preprocessor module deals with mark-up in the input text, finds boundaries of sentences and words, recognizes dates, numbers, named entities and acronyms and performs morphological analysis. Once the mo"
W04-0906,W04-0905,1,\N,Missing
W04-2601,P97-1051,0,0.032261,"– has been widely studied in computational (not to mention other branches of) linguistics, largely because accounting for missing syntactic elements is a crucial aspect of achieving a full parse, and parsing is required for many approaches to NLP.1 Much less attention has been devoted to what we will call semantic ellipsis, or the non-expression of elements that, while not syntactically obligatory, are required for a full semantic interpretation of a text.2 Naturally, semantic ellipsis is important only in truly knowledge-rich ap1 Examples of NLP efforts to resolve syntactic ellipsis include Hobbs and Kehler 1997; Kehler and Shieber 1997; and Lappin 1992, among many others. 2 Some of the types of semantic underspecification treated here are described in the literature (e.g., Pustejovsky 1995) in theoretical terms, not as heuristic algorithms. This is due, in large part, to a lack of knowledge sources for semantic reasoning in those contributions. proaches to NLP, which few current non-toy systems pursue. All definitions of ellipsis derive from a stated or implied notion of completeness. Taking, again, the example of syntactic ellipsis, this means that obligatory verbal arguments must be overt, auxilia"
W04-2601,J97-3005,0,0.0167995,"ied in computational (not to mention other branches of) linguistics, largely because accounting for missing syntactic elements is a crucial aspect of achieving a full parse, and parsing is required for many approaches to NLP.1 Much less attention has been devoted to what we will call semantic ellipsis, or the non-expression of elements that, while not syntactically obligatory, are required for a full semantic interpretation of a text.2 Naturally, semantic ellipsis is important only in truly knowledge-rich ap1 Examples of NLP efforts to resolve syntactic ellipsis include Hobbs and Kehler 1997; Kehler and Shieber 1997; and Lappin 1992, among many others. 2 Some of the types of semantic underspecification treated here are described in the literature (e.g., Pustejovsky 1995) in theoretical terms, not as heuristic algorithms. This is due, in large part, to a lack of knowledge sources for semantic reasoning in those contributions. proaches to NLP, which few current non-toy systems pursue. All definitions of ellipsis derive from a stated or implied notion of completeness. Taking, again, the example of syntactic ellipsis, this means that obligatory verbal arguments must be overt, auxiliary verbs must have comple"
W04-2601,mcshane-etal-2004-meaning,1,0.875495,"Missing"
W04-2601,W03-0904,1,0.817439,"example of a TMR, reflecting the meaning of the sentence The US won the war, is as follows: WIN-3 AGENT NATION-213 THEME WAR-ACTIVITY-7 This TMR is headed by a WIN event – in fact, it is the 3rd instantiation of the concept WIN (WIN-3) in the world “snapshot” being built during the processing of the given text(s). Its agent is NATION-213, which refers to the United States of America in our fact repository. The theme of the event is the 7th instantiation of WARACTIVITY in this text. Details of this approach to text processing can be found, e.g., in Nirenburg and Raskin 2004, Beale et al 2003, Nirenburg et al 2003a,b. The ontology itself, a brief ontology tutorial, and an extensive lexicon tutorial can be viewed at http://ilit.umbc.edu. Since OntoSem text processing attempts to do it all – meaning that any phenomenon in any language we are processing is within the purview of our approach – work on any given problem is carried out in spiral fashion: first at a rough grain size, then at a finer grain size with each iterative improvement of the system. In order both to drive and to organize work, we develop a “microtheory” for each aspect of text processing we treat: e.g., we have microtheories of mood, t"
W04-2601,W04-0905,1,\N,Missing
W05-0310,brants-2000-inter,0,0.0186284,"propriate properties are linked to their heads is quite simple, whereas writing a program for this non-trivial case of reference resolution is not. Thus, in some cases we push through gold standard TMR production while keeping track of – and developing as time permits – the more difficult aspects of text processing that will enhance TMR output in the future. The gold standard TMR for the sentence discussed at length here was produced with only a few manual corrections: changing two part of speech tags and selecting the correct sense for one word. Work took less than the 10 minutes reported by Brants 2000 for their non-semantic tagging. 5 Porting to Other Languages Recently the need for tagged corpora for less commonly taught languages has received much attention. While our group is not currently pursuing such languages, it has in the past: TMRs have been automatically generated for languages such as Chinese, Georgian, Arabic and Persian. We take a short tangent to explain how OntoSem/DEKADE can be extended, at relatively low cost, to the annotation of other languages – showing yet another way in which this approach to annotation reaches beyond the results for any given text or corpus. Whereas"
W05-0310,J02-3001,0,0.00894706,"e at 10 minutes, by the time two taggers carry out the task, their results are compared, difficult issues are resolved, and taggers are trained in the first place. Notably, however, this effort used students as taggers, not professionals. We, by contrast, use professionals to check and correct TMRs and thus reduce to practically zero the training time, the need for multiple annotators (provided the size of a typical annotation task is commensurate with those in current projects), and costly correction of errors. Among past projects that have addressed semantic annotation are the following: 1. Gildea and Jurafsky (2002) created a stochastic system that labels case roles of predicates with either abstract (e.g., AGENT, THEME) or domainspecific (e.g., MESSAGE, TOPIC) roles. The system trained on 50,000 words of hand-annotated text (produced by the FrameNet project). When tasked to segment constituents and identify their semantic roles (with fillers being undisambiguated textual strings) the system scored in the 60’s in precision and recall. Limitations of the system include its reliance on hand-annotated data, and its reliance on prior knowledge of the predicate frame type (i.e., it lacks the capacity to disam"
W05-0310,W04-0904,1,0.829972,"eing treated compositionally.) We do not intend to trivialize the fact that creating a new lexicon is a lot of work. It is, however, compelling to consider that a new lexicon of the same quality of our OntoSem English one could be created with little more work than would be required to build a typical translation dictionary. In fact, we recently carried out an experiment on porting the English lexicon to Polish and found that a) much of it could be done semi-automatically and b) the manual work for a second language is considerably less than for the first language (for further discussion, see McShane et al. 2004). To sum up, the OntoSem ontology and the DEKADE environment are equally suited to any language, and the OntoSem English lexicon and analyzer can be configured to new languages with much less work required than for their initial development. In short, semantic-rich tagging through TMR creation could be a realistic option for languages other than English. 6 Discussion Lack of interannotator agreement presents a significant problem in annotation efforts (see, e.g., Marcus et al. 1993). With the OntoSem semiautomated approach, there is far less possibility of interannotator disagreement since peo"
W05-0310,W04-0905,1,0.834969,"mber of mistakes at each stage of analysis, and the effect that the correction of output at one stage has on the next stage. No methods or standards for such evaluation are readily available since no work of this type has ever been carried out. In the face of the usual pressures of time and manpower, we have made the programmatic decision not to focus on all types of evaluation but, rather, to concentrate our evaluation metrics on the correctness of the automated output of the system, the extent to which manual correction is needed, and the depth and robustness of our knowledge resources (see Nirenburg et al. 2004 for our first evaluation effort). We do not deny the ultimate 74 desirability of additional aspects of evaluation in the future. The main source of variation among knowledge engineers within our approach lies not in reviewing/editing annotations as such, but in building the knowledge sources that give rise to them. To take an actual example we encountered: one member of our group described the phrase weapon of mass destruction in the lexicon as BIOLOGICAL-WEAPON or CHEMICAL-WEAPON, while another described it as a WEAPON with the potential to kill a very large number of people/animals. While b"
W05-0310,W05-0311,0,0.0842338,"Missing"
W05-0310,H01-1035,0,0.0962925,"Missing"
W05-0310,J93-2004,0,\N,Missing
W08-2215,P01-1008,0,0.107062,"Missing"
W08-2215,P04-2006,0,0.0663533,"Missing"
W08-2215,W03-1606,0,0.0443524,"Missing"
W08-2215,W03-1608,0,0.0183392,"of contextually elastic adjectives (such as fast, which means different things in fast highway and fast eater) by semiautomatically constructing paraphrases for phrases that include such adjectives. These paraphrases use the original noun and the adjective (or any of its synonyms, taken from a hand-constructed list) in its adverbial form and add a corpus-derived candidate verb intended to explain the meaning of the adjective. Results are evaluated by human judgments of whether a paraphrase (e.g., highway travel quickly) is appropriate as an explanation of the meaning of fast in fast highway. Ibrahim et al. (2003) pursue the more immediate goal of supporting a questionanswering system. Creating paraphrases for questions helps to expand the queries to the textual resources that are mined for answers. In an early version of this system, such paraphrase rules — which included a combination of lexical and syntactic transformations — were created by hand (Katz and Levin, 1988). The new approach follows the methodology of Lin and Pantel (2001) for dynamically determining paraphrases in a corpus by measuring the similarity of paths between nodes in syntactic deResolving Paraphrases to Support Modeling Languag"
W08-2215,C88-1065,0,0.145448,"candidate verb intended to explain the meaning of the adjective. Results are evaluated by human judgments of whether a paraphrase (e.g., highway travel quickly) is appropriate as an explanation of the meaning of fast in fast highway. Ibrahim et al. (2003) pursue the more immediate goal of supporting a questionanswering system. Creating paraphrases for questions helps to expand the queries to the textual resources that are mined for answers. In an early version of this system, such paraphrase rules — which included a combination of lexical and syntactic transformations — were created by hand (Katz and Levin, 1988). The new approach follows the methodology of Lin and Pantel (2001) for dynamically determining paraphrases in a corpus by measuring the similarity of paths between nodes in syntactic deResolving Paraphrases to Support Modeling Language Perception 181 pendency trees. This method was applied to pairs of sentences from different English translations of the same text. (The idea of using a monolingual “sentence-aligned” corpus is due to Barzilay and McKeown (2001).) Ibrahim et al. (2003) then suggest a set of heuristics for the subsentential-level matching of nouns and pronouns which leads to the"
W08-2215,N01-1009,0,0.0327825,"ent NLP work. As a result, most contributions devoted to paraphrase can be described as syntactic or “light semantic.” In some contributions, processing semantics is constrained to finding synonyms, hyponyms, etc., in a manually constructed word net, like WordNet or any of its progeny. Some others do not rely on a manually constructed knowledge resource but, rather, aim to determine distributional clustering of similar words in corpora (see, e.g. Pereira et al. (1993) or Lin (2001)). A few approaches to dealing with paraphrase actually go beyond the detection and use of synonyms. For example, Lapata (2001) seeks to interpret the meanings of contextually elastic adjectives (such as fast, which means different things in fast highway and fast eater) by semiautomatically constructing paraphrases for phrases that include such adjectives. These paraphrases use the original noun and the adjective (or any of its synonyms, taken from a hand-constructed list) in its adverbial form and add a corpus-derived candidate verb intended to explain the meaning of the adjective. Results are evaluated by human judgments of whether a paraphrase (e.g., highway travel quickly) is appropriate as an explanation of the m"
W08-2215,1997.tmi-1.1,1,0.531795,"EVENT). This preference is the inverse of the lexical choice in text generation off of text meaning representations (for details see Nirenburg and Nirenburg, 1988). OntoSem can yield either of the above basic text meaning representations. In many applications — for example, in interlingua-based machine translation — this would be quite benign. However, it is possible to create extended meaning representations such that the above variability is eliminated. The method we use for this purpose relies on the dynamic tightening or relaxation of selectional restrictions and is described in detail in Mahesh et al. (1997). Note that different paraphrases will still be produced for inputs that, while referring to the same event instance, describe it with a different degree of vagueness or underspecificity (see Section 2.1 above). The fact that the two meaning representations above are paraphrases of one another can be automatically detected using a fairly simple heuristic: the ontological description of AERIAL - MOTION - EVENT includes the following property-value pairs: AERIAL - MOTION - EVENT IS - A MOTION - EVENT INSTRUMENT AIRPLANE HELICOPTER BALLOON Since the head of one of the MRs is an ancestor of the ot"
W08-2215,C88-2100,1,0.424028,"concepts stand in a Nirenburg, McShane, and Beale 188 direct subsumption relation. If one chooses to use a concept that is higher in the ontological hierarchy, one may have to add further overt constraints to the meaning representation (like the one about the INSTRUMENT of the MOTION - EVENT above). If one chooses the lower-level, narrower ontological concept to start with, such constraints may be inherent in its definition (as is the case with AERIAL - MOTION - EVENT). This preference is the inverse of the lexical choice in text generation off of text meaning representations (for details see Nirenburg and Nirenburg, 1988). OntoSem can yield either of the above basic text meaning representations. In many applications — for example, in interlingua-based machine translation — this would be quite benign. However, it is possible to create extended meaning representations such that the above variability is eliminated. The method we use for this purpose relies on the dynamic tightening or relaxation of selectional restrictions and is described in detail in Mahesh et al. (1997). Note that different paraphrases will still be produced for inputs that, while referring to the same event instance, describe it with a differ"
W08-2215,P93-1024,0,0.0618378,"ficult problem: at its deepest, it centrally involves semantics, which, due to its inherent complexity, can be addressed only in limited ways in current NLP work. As a result, most contributions devoted to paraphrase can be described as syntactic or “light semantic.” In some contributions, processing semantics is constrained to finding synonyms, hyponyms, etc., in a manually constructed word net, like WordNet or any of its progeny. Some others do not rely on a manually constructed knowledge resource but, rather, aim to determine distributional clustering of similar words in corpora (see, e.g. Pereira et al. (1993) or Lin (2001)). A few approaches to dealing with paraphrase actually go beyond the detection and use of synonyms. For example, Lapata (2001) seeks to interpret the meanings of contextually elastic adjectives (such as fast, which means different things in fast highway and fast eater) by semiautomatically constructing paraphrases for phrases that include such adjectives. These paraphrases use the original noun and the adjective (or any of its synonyms, taken from a hand-constructed list) in its adverbial form and add a corpus-derived candidate verb intended to explain the meaning of the adjecti"
W08-2215,P04-1019,0,0.0301646,"- EVENT INSTRUMENT AIRPLANE HELICOPTER BALLOON Since the head of one of the MRs is an ancestor of the other, and the property-value pairs in the ancestor-based MR unify with the ontological definition of the descendant (the head of the other MR), these two structures are deemed to be paraphrases. As we see from this example, world knowledge stored in the ontology is leveraged to carry out the reasoning needed to detect that the abovementioned formal structures are paraphrases. Such situations are somewhat similar to “bridging references” in the literature devoted to reference resolution (e.g. Poesio et al., 2004) because a knowledge bridge is needed to aid in the reference resolution of the entity. A common source of this type of paraphrase derives from decisions about how to build the ontology. Ontology building is a complex task with “the lesser of the evils” decisions to be made at every turn. Two ontologies can be equally valid and yet look quite different. One of the most difficult aspects of ontology building is deciding when a new concept is needed. Let us continue with the example of taking a trip. A small excerpt from the MOTION - EVENT subtree of our ontology is as follows: MOTION - EVENT AE"
W08-2225,mcshane-etal-2004-meaning,1,0.852745,"Missing"
W08-2225,W04-0905,1,0.812574,"Missing"
W11-3408,2005.mtsummit-papers.9,1,0.786509,"necessary lexical and grammatical knowledge for the target language, a draft translation of the stories can be produced and checked for naturalness and accuracy. LA has been used in this mode to produce a significant amount of highquality translations in Jula (a Niger-Congo language), Kewa (Papua New Guinea), North Tanna (Vanuatu), Korean and English. Work continues in Vanuatu, with additional languages planned in the near future. We argue that the high quality results achieved in these translations demonstrate the quality and coverage of the underlying language description that LA produces. Beale et al. (2005) and Allman and Beale (2004; 2006) give more information on using LA in translation and for documentation on the evaluations of the translations produced. retical base. LA is somewhere in the late stages of the ""being used and evaluated” step of this cycle. We certainly intend to improve the theoretical basis of each aspect of the product as time goes on, in large part as a result of the feedback, suggestions and criticisms of our users. 2 Introduction to LA Consult Beale (submitted) for details on LA, including the semantic representation language and the visual lexicon and grammar interfaces"
W11-3408,P10-4001,0,0.0281648,"Missing"
W11-3408,I11-2002,1,\N,Missing
W12-1510,W11-3408,1,\N,Missing
W14-2214,2005.mtsummit-papers.9,1,0.893548,"s are visually presented to the user, who checks and/or assigns semantic concepts and relationships. The 103 steps in preparing a semantic analysis of a text or set of elicitation sentences is thus: 1) manually “translate” the text into the controlled language, 2) run this through the automatic analyzer, and 3) manually check and correct the resulting semantic analysis. Although unlimited free text cannot be translated in an LA language project, a wide variety of texts can be semantically authored. This process only needs to be done once and the results can then be used for any language. See (Beale, et al. 2005) for more information on document authoring in the context of endangered languages. We believe that a semantically-based description of a language is the key to the practical description of endangered languages. It provides an inherently efficient framework for language description in the field. The resulting description not only provides invaluable data for future linguists, but also enables present-day translation capabilities that can aid in language revitalization. A document authoring system provides the means for overcoming one of the main drawbacks to a semantically-based system in that"
W14-2214,W12-1510,1,0.853069,"ss itself as well as the underlying semantic representation language need to be refined and validated by our colleagues; we expect such refinements to also improve efficiency. seems an extreme position in practical terms.’ And again, Gippert et al. (2006) add their warning that ‘without theoretical grounding language documentation is in the danger of producing ‘data graveyards’, i.e. large heaps of data with little or no use to anyone.’ We believe that the semantic-based grammar discovery methodology adds this theoretical grounding. We also add the argument that “the proof is in the pudding.” Allman, et al. (2012) documents that a grammar discovery procedure such as described above combined with a capable knowledge acquisition and text generation environment such as found in LA can produce translations that are as accurate and readable to native speakers as manual translations and that these results indicate that the underlying language description is accurate, natural and broad-coverage. 4 Document authoring: a bridge to practical MT (and language description) in endangered languages We have already argued that a semanticallybased language description environment is superior to a transfer-based system"
W14-2214,P10-4001,0,0.0627169,"Missing"
W14-2214,W13-2710,0,\N,Missing
W96-0513,P96-1005,1,0.801543,"end a large-scale lexicon of about 40000 word senses. 2 The main advantage of our approach is that it enabled us to economically multiply the size of the lexicon. The main drawback is that the entries produced automatically need some semimanual checking. We bypass this drawback by We acquired a Spanish semantic lexicon of about 40,000 word meanings, for an MT Project, described in [Beale et al., 1995]. We a u t o m a t e d as much as possible the task of acquisition by providing the lexicographers with 1We use the typed feature structures (tfs) as described in [Pollard and Sag, 1987)9. 2See [Viegas et al., 1996] which describes the advantages and drawbacks of using lexical rules to build lexicons. Reversing an Analysis Lexicon Before addressing the issue of reversing the analysis lexicon, we want first to show how we could acquire a large-scale analysis lexicon. 50 We only show partial entries for superentry of the concept ACQUIIIE, as shown in (Figure 4). ""adquirir-V 1 syn: ""root: [] • [cat: ~Ii_i]P ] -acquire-C 1 subj: [B,cat: lsem: ~]PJ syn: obj: [] [sem: sem: acquire-C2 acquire sem: agent: [~] humav theme: [~] object adquirir-V2 ""root: [] sun: syn: sub j: [] |sere: ]cat: aspect{tefic: yes]] •roo"
W98-0603,C94-2203,1,0.90082,"Missing"
W98-0603,1996.amta-1.10,1,0.70588,"licit semantic relations in nominal compounds. Finally, we show how to translate Chinese nontinals within a knowledge-based framework. 1 Introduction In this paper, we present results of a theoretical and an applied investigation, within a knowledge base framework, on the building and processing of computational semantic lexicons, as reflected by experiments done on Spanish, English and Chinese, with a large scale application on Spanish. The multilingual dictionaries making process (Viegas and Raskin, 1998) has been tested and attested for M i k r o k o s m o s , a machine translation system (Nirenburg et al., 1996) from Spanish and Chinese to English. 2 Here, we focus on Chinese nominals and compounds in terms of representation and processing. In Section2, we briefly present the information carried inside Mikrokosmos lexicons. In Section 3, we show how a semantic-based transcategorial approach is best fitted to account for nominalisations and their derived forms. Formally, we use the conceptual tool of lexical rules as described in (Viegas et al., 1996). In Section 4, we address the Iranslation of Chinese nominal compounds into English using semantic information and word order information. We show the a"
W98-0603,P96-1005,1,0.905152,"ilingual dictionaries making process (Viegas and Raskin, 1998) has been tested and attested for M i k r o k o s m o s , a machine translation system (Nirenburg et al., 1996) from Spanish and Chinese to English. 2 Here, we focus on Chinese nominals and compounds in terms of representation and processing. In Section2, we briefly present the information carried inside Mikrokosmos lexicons. In Section 3, we show how a semantic-based transcategorial approach is best fitted to account for nominalisations and their derived forms. Formally, we use the conceptual tool of lexical rules as described in (Viegas et al., 1996). In Section 4, we address the Iranslation of Chinese nominal compounds into English using semantic information and word order information. We show the advantage of a transcategorial approach to lexicon representation and investigate some trade-offs between an interlingua and transfer approach to nominal compounding. XThis work has been supported in part by DoD under contract number MDA-904-92-C-5189. 2The interested reader can visit the Mikrokosmos site at http://crl.nmsu.edu/Research/Projects/mikro/. 20 Beale A Brief Overview on the Structure of Mikrokosmos Lexicons In Mikrokosmos, the lexic"
W98-0603,A92-1029,0,\N,Missing
W98-1406,J97-2001,0,0.19888,"Missing"
W98-1406,W96-0403,0,0.019266,"Missing"
W98-1406,1996.amta-1.10,1,0.774247,"Missing"
W98-1406,W94-0315,0,0.0593217,"Missing"
W98-1406,P96-1005,1,0.871835,"Missing"
W98-1406,W96-0401,1,0.844947,"Missing"
W98-1406,A92-1006,0,\N,Missing
