2021.newsum-1.7,Evaluation of Abstractive Summarisation Models with Machine Translation in Deliberative Processes,2021,-1,-1,4,0,3112,miguel aranacatania,Proceedings of the Third Workshop on New Frontiers in Summarization,0,"We present work on summarising deliberative processes for non-English languages. Unlike commonly studied datasets, such as news articles, this deliberation dataset reflects difficulties of combining multiple narratives, mostly of poor grammatical quality, in a single text. We report an extensive evaluation of a wide range of abstractive summarisation models in combination with an off-the-shelf machine translation model. Texts are translated into English, summarised, and translated back to the original language. We obtain promising results regarding the fluency, consistency and relevance of the summaries produced. Our approach is easy to implement for many languages for production purposes by simply changing the translation model."
2021.findings-emnlp.200,{G}i{BERT}: Enhancing {BERT} with Linguistic Information using a Lightweight Gated Injection Method,2021,-1,-1,3,1,6925,nicole peinelt,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Large pre-trained language models such as BERT have been the driving force behind recent improvements across many NLP tasks. However, BERT is only trained to predict missing words {--} either through masking or next sentence prediction {--} and has no knowledge of lexical, syntactic or semantic information beyond what it picks up through unsupervised pre-training. We propose a novel method to explicitly inject linguistic information in the form of word embeddings into any layer of a pre-trained BERT. When injecting counter-fitted and dependency-based embeddings, the performance improvements on multiple semantic similarity datasets indicate that such information is beneficial and currently missing from the original model. Our qualitative analysis shows that counter-fitted embedding injection is particularly beneficial, with notable improvements on examples that require synonym resolution."
2021.findings-acl.341,Learning Disentangled Latent Topics for {T}witter Rumour Veracity Classification,2021,-1,-1,2,0,8312,john dougrezlewis,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.eacl-main.21,{CD}{\\^{}}2{CR}: Co-reference resolution across documents and domains,2021,-1,-1,5,1,10537,james ravenscroft,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Cross-document co-reference resolution (CDCR) is the task of identifying and linking mentions to entities and concepts across many text documents. Current state-of-the-art models for this task assume that all documents are of the same type (e.g. news articles) or fall under the same theme. However, it is also desirable to perform CDCR across different domains (type or theme). A particular use case we focus on in this paper is the resolution of entities mentioned across scientific work and newspaper articles that discuss them. Identifying the same entities and corresponding concepts in both scientific articles and news can help scientists understand how their work is represented in mainstream media. We propose a new task and English language dataset for cross-document cross-domain co-reference resolution (CD{\^{}}2CR). The task aims to identify links between entities across heterogeneous document types. We show that in this cross-domain, cross-document setting, existing CDCR models do not perform well and we provide a baseline model that outperforms current state-of-the-art CDCR models on CD{\^{}}2CR. Our data set, annotation tool and guidelines as well as our model for cross-document cross-domain co-reference are all supplied as open access open source resources."
2021.eacl-main.169,Boosting Low-Resource Biomedical {QA} via Entity-Aware Masking Strategies,2021,-1,-1,4,0,3935,gabriele pergola,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Biomedical question-answering (QA) has gained increased attention for its capability to provide users with high-quality information from a vast scientific literature. Although an increasing number of biomedical QA datasets has been recently made available, those resources are still rather limited and expensive to produce; thus, transfer learning via pre-trained language models (LMs) has been shown as a promising approach to leverage existing general-purpose knowledge. However, fine-tuning these large models can be costly and time consuming and often yields limited benefits when adapting to specific themes of specialised domains, such as the COVID-19 literature. Therefore, to bootstrap further their domain adaptation, we propose a simple yet unexplored approach, which we call biomedical entity-aware masking (BEM) strategy, encouraging masked language models to learn entity-centric knowledge based on the pivotal entities characterizing the domain at hand, and employ those entities to drive the LM fine-tuning. The resulting strategy is a downstream process applicable to a wide variety of masked LMs, not requiring additional memory or components in the neural architectures. Experimental results show performance on par with the state-of-the-art models on several biomedical QA datasets."
2021.clpsych-1.15,Automatic Identification of Ruptures in Transcribed Psychotherapy Sessions,2021,-1,-1,6,1,11637,adam tsakalidis,Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access,0,"We present the first work on automatically capturing alliance rupture in transcribed therapy sessions, trained on the text and self-reported rupture scores from both therapists and clients. Our NLP baseline outperforms a strong majority baseline by a large margin and captures client reported ruptures unidentified by therapists in 40{\%} of such cases."
2021.acl-long.530,Evaluation of Thematic Coherence in Microblogs,2021,-1,-1,3,0,13463,iman bilal,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Collecting together microblogs representing opinions about the same topics within the same timeframe is useful to a number of different tasks and practitioners. A major question is how to evaluate the quality of such thematic clusters. Here we create a corpus of microblog clusters from three different domains and time windows and define the task of evaluating thematic coherence. We provide annotation guidelines and human annotations of thematic coherence by journalist experts. We subsequently investigate the efficacy of different automated evaluation metrics for the task. We consider a range of metrics including surface level metrics, ones for topic model coherence and text generation metrics (TGMs). While surface level metrics perform well, outperforming topic coherence metrics, they are not as consistent as TGMs. TGMs are more reliable than all other metrics considered for capturing thematic coherence in microblog clusters due to being less sensitive to the effect of time windows."
2020.emnlp-main.682,Sequential Modelling of the Evolution of Word Representations for Semantic Change Detection,2020,-1,-1,2,1,11637,adam tsakalidis,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Semantic change detection concerns the task of identifying words whose meaning has changed over time. Current state-of-the-art approaches operating on neural embeddings detect the level of semantic change in a word by comparing its vector representation in two distinct time periods, without considering its evolution through time. In this work, we propose three variants of sequential models for detecting semantically shifted words, effectively accounting for the changes in the word representations over time. Through extensive experimentation under various settings with synthetic and real data we showcase the importance of sequential modelling of word vectors through time for semantic change detection. Finally, we compare different approaches in a quantitative manner, demonstrating that temporal modelling of word representations yields a clear-cut advantage in performance."
2020.acl-main.623,Estimating predictive uncertainty for rumour verification models,2020,34,0,2,1,8313,elena kochkina,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"The inability to correctly resolve rumours circulating online can have harmful real-world consequences. We present a method for incorporating model and data uncertainty estimates into natural language processing models for automatic rumour verification. We show that these estimates can be used to filter out model predictions likely to be erroneous so that these difficult instances can be prioritised by a human fact-checker. We propose two methods for uncertainty-based instance rejection, supervised and unsupervised. We also show how uncertainty estimates can be used to interpret model performance as a rumour unfolds."
2020.acl-main.630,t{BERT}: Topic Models and {BERT} Joining Forces for Semantic Similarity Detection,2020,-1,-1,3,1,6925,nicole peinelt,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,Semantic similarity detection is a fundamental task in natural language understanding. Adding topic information has been useful for previous feature-engineered semantic similarity models as well as neural models for other tasks. There is currently no standard way of combining topics with pretrained contextual representations such as BERT. We propose a novel topic-informed BERT-based architecture for pairwise semantic similarity detection and show that our model improves performance over strong neural baselines across a variety of English language datasets. We find that the addition of topics to BERT helps particularly with resolving domain-specific cases.
S19-2147,"{S}em{E}val-2019 Task 7: {R}umour{E}val, Determining Rumour Veracity and Support for Rumours",2019,0,13,3,0,25115,genevieve gorrell,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"Since the first RumourEval shared task in 2017, interest in automated claim validation has greatly increased, as the danger of {``}fake news{''} has become a mainstream concern. However automated support for rumour verification remains in its infancy. It is therefore important that a shared task in this area continues to provide a focus for effort, which is likely to increase. Rumour verification is characterised by the need to consider evolving conversations and news updates to reach a verdict on a rumour{'}s veracity. As in RumourEval 2017 we provided a dataset of dubious posts and ensuing conversations in social media, annotated both for stance and veracity. The social media rumours stem from a variety of breaking news stories and the dataset is expanded to include Reddit as well as new Twitter posts. There were two concrete tasks; rumour stance prediction and rumour verification, which we present in detail along with results achieved by participants. We received 22 system submissions (a 70{\%} increase from RumourEval 2017) many of which used state-of-the-art methodology to tackle the challenges involved."
P19-1268,Aiming beyond the Obvious: Identifying Non-Obvious Cases in Semantic Similarity Datasets,2019,0,0,2,1,6925,nicole peinelt,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,Existing datasets for scoring text pairs in terms of semantic similarity contain instances whose resolution differs according to the degree of difficulty. This paper proposes to distinguish obvious from non-obvious text pairs based on superficial lexical overlap and ground-truth labels. We characterise existing datasets in terms of containing difficult cases and find that recently proposed models struggle to capture the non-obvious cases of semantic similarity. We describe metrics that emphasise cases of similarity which require more complex inference and propose that these are used for evaluating systems for semantic similarity.
P18-4004,{H}arri{GT}: A Tool for Linking News to Science,2018,0,1,3,1,10537,james ravenscroft,"Proceedings of {ACL} 2018, System Demonstrations",0,"Being able to reliably link scientific works to the newspaper articles that discuss them could provide a breakthrough in the way we rationalise and measure the impact of science on our society. Linking these articles is challenging because the language used in the two domains is very different, and the gathering of online resources to align the two is a substantial information retrieval endeavour. We present HarriGT, a semi-automated tool for building corpora of news articles linked to the scientific papers that they discuss. Our aim is to facilitate future development of information-retrieval tools for newspaper/scientific work citation linking. HarriGT retrieves newspaper articles from an archive containing 17 years of UK web content. It also integrates with 3 large external citation networks, leveraging named entity extraction, and document classification to surface relevant examples of scientific literature to the user. We also provide a tuned candidate ranking algorithm to highlight potential links between scientific papers and newspaper articles to the user, in order of likelihood. HarriGT is provided as an open source tool (\url{http://harrigt.xyz})."
C18-1288,All-in-one: Multi-task Learning for Rumour Verification,2018,0,17,2,1,8313,elena kochkina,Proceedings of the 27th International Conference on Computational Linguistics,0,"Automatic resolution of rumours is a challenging task that can be broken down into smaller components that make up a pipeline, including rumour detection, rumour tracking and stance classification, leading to the final outcome of determining the veracity of a rumour. In previous work, these steps in the process of rumour verification have been developed as separate components where the output of one feeds into the next. We propose a multi-task learning approach that allows joint training of the main and auxiliary tasks, improving the performance of rumour verification. We examine the connection between the dataset properties and the outcomes of the multi-task learning models used."
W17-4210,Incongruent Headlines: Yet Another Way to Mislead Your Readers,2017,20,9,2,0,31705,sophie chesney,Proceedings of the 2017 {EMNLP} Workshop: Natural Language Processing meets Journalism,0,"This paper discusses the problem of incongruent headlines: those which do not accurately represent the information contained in the article with which they occur. We emphasise that this phenomenon should be considered separately from recognised problematic headline types such as clickbait and sensationalism, arguing that existing natural language processing (NLP) methods applied to these related concepts are not appropriate for the automatic detection of headline incongruence, as an analysis beyond stylistic traits is necessary. We therefore suggest a number of alternative methodologies that may be appropriate to the task at hand as a foundation for future work in this area. In addition, we provide an analysis of existing data sets which are related to this work, and motivate the need for a novel data set in this domain."
S17-2006,{S}em{E}val-2017 Task 8: {R}umour{E}val: Determining rumour veracity and support for rumours,2017,0,45,3,0,642,leon derczynski,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"Media is full of false claims. Even Oxford Dictionaries named {``}post-truth{''} as the word of 2016. This makes it more important than ever to build systems that can identify the veracity of a story, and the nature of the discourse around it. RumourEval is a SemEval shared task that aims to identify and handle rumours and reactions to them, in text. We present an annotation scheme, a large dataset covering multiple topics {--} each having their own families of claims and replies {--} and use these to pose two concrete challenges as well as the results achieved by participants on these challenges."
S17-2083,{T}uring at {S}em{E}val-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-{LSTM},2017,12,4,2,1,8313,elena kochkina,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This paper describes team Turing{'}s submission to SemEval 2017 RumourEval: Determining rumour veracity and support for rumours (SemEval 2017 Task 8, Subtask A). Subtask A addresses the challenge of rumour stance classification, which involves identifying the attitude of Twitter users towards the truthfulness of the rumour they are discussing. Stance classification is considered to be an important step towards rumour verification, therefore performing well in this task is expected to be useful in debunking false rumours. In this work we classify a set of Twitter posts discussing rumours into either supporting, denying, questioning or commenting on the underlying rumours. We propose a LSTM-based sequential model that, through modelling the conversational structure of tweets, which achieves an accuracy of 0.784 on the RumourEval test set outperforming all other systems in Subtask A."
I17-3006,"{TOTEMSS}: Topic-based, Temporal Sentiment Summarisation for {T}witter",2017,0,4,2,0.681818,7051,bo wang,"Proceedings of the {IJCNLP} 2017, System Demonstrations",0,"We present a system for time sensitive, topic based summarisation of the sentiment around target entities and topics in collections of tweets. We describe the main elements of the system and illustrate its functionality with two examples of sentiment analysis of topics related to the 2017 UK general election."
I17-3011,{C}lassifier{G}uesser: A Context-based Classifier Prediction System for {C}hinese Language Learners,2017,0,0,2,1,6925,nicole peinelt,"Proceedings of the {IJCNLP} 2017, System Demonstrations",0,"Classifiers are function words that are used to express quantities in Chinese and are especially difficult for language learners. In contrast to previous studies, we argue that the choice of classifiers is highly contextual and train context-aware machine learning models based on a novel publicly available dataset, outperforming previous baselines. We further present use cases for our database and models in an interactive demo system."
E17-1046,{TDP}arse: Multi-target-specific sentiment recognition on {T}witter,2017,15,14,2,0.681818,7051,bo wang,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"Existing target-specific sentiment recognition methods consider only a single target per tweet, and have been shown to miss nearly half of the actual targets mentioned. We present a corpus of UK election tweets, with an average of 3.09 entities per tweet and more than one type of sentiment in half of the tweets. This requires a method for multi-target specific sentiment recognition, which we develop by using the context around a target as well as syntactic dependencies involving the target. We present results of our method on both a benchmark corpus of single targets and the multi-target election corpus, showing state-of-the art performance in both corpora and outperforming previous approaches to multi-target sentiment task as well as deep learning models for single-target sentiment."
W16-0307,The language of mental health problems in social media,2016,27,23,5,0,28610,george gkotsis,Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology,0,"Online social media, such as Reddit, has become an important resource to share personal experiences and communicate with others. Among other personal information, some social media users communicate about mental health problems they are experiencing, with the intention of getting advice, support or empathy from other users. Here, we investigate the language of Reddit posts specific to mental health, to define linguistic characteristics that could be helpful for further applications. The latter include attempting to identify posts that need urgent attention due to their nature, e.g. when someone announces their intentions of ending their life by suicide or harming others. Our results show that there are a variety of linguistic features that are discriminative across mental health user communities and that can be further exploited in subsequent classification tasks. Furthermore, while negative sentiment is almost uniformly expressed across the entire data set, we demonstrate that there are also condition-specific vocabularies used in social media to communicate about particular disorders. Source code and related materials are available from: https: //github.com/gkotsis/ reddit-mental-health."
W16-0310,Don{'}t Let Notes Be Misunderstood: A Negation Detection Method for Assessing Risk of Suicide in Mental Health Records,2016,28,13,5,0,28610,george gkotsis,Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology,0,"Mental Health Records (MHRs) contain freetext documentation about patientsxe2x80x99 suicide and suicidality. In this paper, we address the problem of determining whether grammatic variants (inflections) of the word xe2x80x9csuicidexe2x80x9d are affirmed or negated. To achieve this, we populate and annotate a dataset with over 6,000 sentences originating from a large repository of MHRs. The resulting dataset has high InterAnnotator Agreement ( 0.93). Furthermore, we develop and propose a negation detection method that leverages syntactic features of text 1 . Using parse trees, we build a set of basic rules that rely on minimum domain knowledge and render the problem as binary classification (affirmed vs. negated). Since the overall goal is to identify patients who are expected to be at high risk of suicide, we focus on the evaluation of positive (affirmed) cases as determined by our classifier. Our negation detection approach yields a recall (sensitivity) value of 94.6% for the positive cases and an overall accuracy value of 91.9%. We believe that our approach can be integrated with other clinical Natural Language Processing tools in order to further advance information extraction capabilities."
L16-1274,Applying Core Scientific Concepts to Context-Based Citation Recommendation,2016,14,7,2,0,35004,daniel duma,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The task of recommending relevant scientific literature for a draft academic paper has recently received significant interest. In our effort to ease the discovery of scientific literature and augment scientific writing, we aim to improve the relevance of results based on a shallow semantic analysis of the source document and the potential documents to recommend. We investigate the utility of automatic argumentative and rhetorical annotation of documents for this purpose. Specifically, we integrate automatic Core Scientific Concepts (CoreSC) classification into a prototype context-based citation recommendation system and investigate its usefulness to the task. We frame citation recommendation as an information retrieval task and we use the categories of the annotation schemes to apply different weights to the similarity formula. Our results show interesting and consistent correlations between the type of citation and the type of sentence containing the relevant information."
L16-1650,Multi-label Annotation in Scientific Articles - The Multi-label Cancer Risk Assessment Corpus,2016,0,3,4,1,10537,james ravenscroft,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"With the constant growth of the scientific literature, automated processes to enable access to its contents are increasingly in demand. Several functional discourse annotation schemes have been proposed to facilitate information extraction and summarisation from scientific articles, the most well known being argumentative zoning. Core Scientific concepts (CoreSC) is a three layered fine-grained annotation scheme providing content-based annotations at the sentence level and has been used to index, extract and summarise scientific publications in the biomedical literature. A previously developed CoreSC corpus on which existing automated tools have been trained contains a single annotation for each sentence. However, it is the case that more than one CoreSC concept can appear in the same sentence. Here, we present the Multi-CoreSC CRA corpus, a text corpus specific to the domain of cancer risk assessment (CRA), consisting of 50 full text papers, each of which contains sentences annotated with one or more CoreSCs. The full text papers have been annotated by three biology experts. We present several inter-annotator agreement measures appropriate for multi-label annotation assessment. Employing several inter-annotator agreement measures, we were able to identify the most reliable annotator and we built a harmonised consensus (gold standard) from the three different annotators, while also taking concept priority (as specified in the guidelines) into account. We also show that the new Multi-CoreSC CRA corpus allows us to improve performance in the recognition of CoreSCs. The updated guidelines, the multi-label CoreSC CRA corpus and other relevant, related materials are available at the time of publication at http://www.sapientaproject.com/."
C16-1146,{S}enti{H}ood: Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods,2016,13,7,3,0,9998,marzieh saeidi,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"In this paper, we introduce the task of targeted aspect-based sentiment analysis. The goal is to extract fine-grained information with respect to entities mentioned in user comments. This work extends both aspect-based sentiment analysis {--} that assumes a single entity per document {---} and targeted sentiment analysis {---} that assumes a single sentiment towards a target entity. In particular, we identify the sentiment towards each aspect of one or more entities. As a testbed for this task, we introduce the SentiHood dataset, extracted from a question answering (QA) platform where urban neighbourhoods are discussed by users. In this context units of text often mention several aspects of one or more neighbourhoods. This is the first time that a generic social media platform,i.e. QA, is used for fine-grained opinion mining. Text coming from QA platforms are far less constrained compared to text from review specific platforms which current datasets are based on. We develop several strong baselines, relying on logistic regression and state-of-the-art recurrent neural networks"
C16-1230,Stance Classification in Rumours as a Sequential Task Exploiting the Tree Structure of Social Media Conversations,2016,23,4,3,0,2162,arkaitz zubiaga,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Rumour stance classification, the task that determines if each tweet in a collection discussing a rumour is supporting, denying, questioning or simply commenting on the rumour, has been attracting substantial interest. Here we introduce a novel approach that makes use of the sequence of transitions observed in tree-structured conversation threads in Twitter. The conversation threads are formed by harvesting users{'} replies to one another, which results in a nested tree-like structure. Previous work addressing the stance classification task has treated each tweet as a separate unit. Here we analyse tweets by virtue of their position in a sequence and test two sequential classifiers, Linear-Chain CRF and Tree CRF, each of which makes different assumptions about the conversational structure. We experiment with eight Twitter datasets, collected during breaking news, and show that exploiting the sequential structure of Twitter conversations achieves significant improvements over the non-sequential methods. Our work is the first to model Twitter conversations as a tree structure in this manner, introducing a novel way of tackling NLP tasks on Twitter conversations."
C16-1283,Combining Heterogeneous User Generated Data to Sense Well-being,2016,14,8,2,1,11637,adam tsakalidis,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"In this paper we address a new problem of predicting affect and well-being scales in a real-world setting of heterogeneous, longitudinal and non-synchronous textual as well as non-linguistic data that can be harvested from on-line media and mobile phones. We describe the method for collecting the heterogeneous longitudinal data, how features are extracted to address missing information and differences in temporal alignment, and how the latter are combined to yield promising predictions of affect and well-being on the basis of widely used psychological scales. We achieve a coefficient of determination ($R^2$) of 0.71-0.76 and a correlation coefficient of 0.68-0.87 which is higher than the state-of-the art in equivalent multi-modal tasks for affect."
W15-3814,Using word embedding for bio-event extraction,2015,8,18,3,0,9098,chen li,Proceedings of {B}io{NLP} 15,0,"Bio-event extraction is an important phase towards the goal of extracting biological networks from the scientific literature. Recent advances in word embedding make computation of word distribution more ef- ficient and possible. In this study, we investigate methods bringing distributional characteristics of words in the text into event extraction by using the latest word embedding methods. By using bag-ofwords (BOW) features as the baseline, the result has been improved by the introduction of word-embedding features, and is comparable to the state-of-the-art solution."
S15-2110,{W}arwick{DCS}: From Phrase-Based to Target-Specific Sentiment Recognition,2015,20,3,5,0,37267,richard townsend,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"We present and evaluate several hybrid systems for sentiment identification for Twitter, both at the phrase and document (tweet) level. Our approach has been to use a novel combination of lexica, traditional NLP and deep learning features. We also analyse techniques based on syntactic parsing and tokenbased association to handle topic specific sentiment in subtask C. Our strategy has been to identify subphrases relevant to the designated topic/target and assign sentiment according to our subtask A classifier. Our submitted subtask A classifier ranked fourth in the SemEval official results while our BASELINEand xc2xb5PARSE classifiers for subtask C would have ranked second."
S14-2136,{U}niversity{\\_}of{\\_}{W}arwick: {SENTIADAPTRON} - A Domain Adaptable Sentiment Analyser for Tweets - Meets {S}em{E}val,2014,13,1,5,0,37267,richard townsend,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"We give a brief overview of our system, SentiAdaptron, a domain-sensitive and domain adaptable system for twitter analysis in tweets, and discuss performance on SemEval (in both the constrained and unconstrained scenarios), as well as implications arising from comparing the intra- and inter- domain performance on our twitter corpus."
D13-1070,A Discourse-Driven Content Model for Summarising Scientific Articles Evaluated in a Complex Question Answering Task,2013,30,10,1,1,3114,maria liakata,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"We present a method which exploits automatically generated scientific discourse annotations to create a content model for the summarisation of scientific articles. Full papers are first automatically annotated using the CoreSC scheme, which captures 11 contentbased concepts such as Hypothesis, Result, Conclusion etc at the sentence level. A content model which follows the sequence of CoreSC categories observed in abstracts is used to provide the skeleton of the summary, making a distinction between dependent and independent categories. Summary creation is also guided by the distribution of CoreSC categories found in the full articles, in order to adequately represent the article content. Finally, we demonstrate the usefulness of the summaries by evaluating them in a complex question answering task. Results are very encouraging as summaries of papers from automatically obtained CoreSCs enable experts to answer 66% of complex content-related questions designed on the basis of paper abstracts. The questions were answered with a precision of 75%, where the upper bound for human summaries (abstracts) was 95%."
W12-4305,A three-way perspective on scientific discourse annotation for knowledge extraction,2012,48,12,1,1,3114,maria liakata,Proceedings of the Workshop on Detecting Structure in Scholarly Discourse,0,"This paper presents a three-way perspective on the annotation of discourse in scientific literature. We use three different schemes, each of which focusses on different aspects of discourse in scientific articles, to annotate a corpus of three full-text papers, and compare the results. One scheme seeks to identify the core components of scientific investigations at the sentence level, a second annotates meta-knowledge pertaining to bio-events and a third considers how epistemic knowledge is conveyed at the clause level. We present our analysis of the comparison, and a discussion of the contributions of each scheme."
W10-3101,Zones of conceptualisation in scientific papers: a window to negative and speculative statements,2010,15,10,1,1,3114,maria liakata,Proceedings of the Workshop on Negation and Speculation in Natural Language Processing,0,"In view of the increasing need to facilitate processing the content of scientific papers, we present an annotation scheme for annotating full papers with zones of conceptualisation, reflecting the information structure and knowledge types which constitute a scientific investigation. The latter are the Core Scientific Concepts (CoreSCs) and include Hypothesis, Motivation, Goal, Object, Background, Method, Experiment, Model, Observation, Result and Conclusion. The CoreSC scheme has been used to annotate a corpus of 265 full papers in physical chemistry and biochemistry and we are currently automating the recognition of CoreSCs in papers. We discuss how the CoreSC scheme relates to other views of scientific papers and indeed how the former could be used to help identify negation and speculation in scientific texts."
W10-1913,Identifying the Information Structure of Scientific Abstracts: An Investigation of Three Different Schemes,2010,25,44,3,0,4740,yufan guo,Proceedings of the 2010 Workshop on Biomedical Natural Language Processing,0,"Many practical tasks require accessing specific types of information in scientific literature; e.g. information about the objective, methods, results or conclusions of the study in question. Several schemes have been developed to characterize such information in full journal papers. Yet many tasks focus on abstracts instead. We take three schemes of different type and granularity (those based on section names, argumentative zones and conceptual structure of documents) and investigate their applicability to biomedical abstracts. We show that even for the finest-grained of these schemes, the majority of categories appear in abstracts and can be identified relatively reliably using machine learning. We discuss the impact of our results and the need for subsequent task-based evaluation of the schemes."
liakata-etal-2010-corpora,Corpora for the Conceptualisation and Zoning of Scientific Papers,2010,17,76,1,1,3114,maria liakata,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present two complementary annotation schemes for sentence based annotation of full scientific papers, CoreSC and AZ-II, applied to primary research articles in chemistry. AZ-II is the extension of AZ for chemistry papers. AZ has been shown to have been reliably annotated by independent human coders and useful for various information access tasks. Like AZ, AZ-II follows the rhetorical structure of a scientific paper and the knowledge claims made by the authors. The CoreSC scheme takes a different view of scientific papers, treating them as the humanly readable representations of scientific investigations. It seeks to retrieve the structure of the investigation from the paper as generic high-level Core Scientific Concepts (CoreSC). CoreSCs have been annotated by 16 chemistry experts over a total of 265 full papers in physical chemistry and biochemistry. We describe the differences and similarities between the two schemes in detail and present the two corpora produced using each scheme. There are 36 shared papers in the corpora, which allows us to quantitatively compare aspects of the annotation schemes. We show the correlation between the two schemes, their strengths and weeknesses and discuss the benefits of combining a rhetorical based analysis of the papers with a content-based one."
W09-1325,Semantic Annotation of Papers: Interface {\\&} Enrichment Tool ({SAPIENT}),2009,12,14,1,1,3114,maria liakata,Proceedings of the {B}io{NLP} 2009 Workshop,0,"In this paper we introduce a web application (SAPIENT) for sentence based annotation of full papers with semantic information. SAPIENT enables experts to annotate scientific papers sentence by sentence and also to link related sentences together, thus forming spans of interesting regions, which can facilitate text mining applications. As part of the system, we developed an XML-aware sentence splitter (SSSplit) which preserves XML markup and identifies sentences through the addition of in-line markup. SAPIENT has been used in a systematic study for the annotation of scientific papers with concepts representing the Core Information about Scientific Papers (CISP) to create a corpus of 225 annotated papers."
W08-2212,Automatic Fine-Grained Semantic Classification for Domain Adaptation,2008,25,6,1,1,3114,maria liakata,Semantics in Text Processing. {STEP} 2008 Conference Proceedings,0,"Assigning arguments of verbs to different semantic classes ('semantic typing'), or alternatively, checking the 'selectional restrictions' of predicates, is a fundamental component of many natural language processing tasks. However, a common experience has been that general purpose semantic classes, such as those encoded in resources like WordNet, or handcrafted subject-specific ontologies, are seldom quite right when it comes to analysing texts from a particular domain. In this paper we describe a method of automatically deriving fine-grained, domain-specific semantic classes of arguments while simultaneously clustering verbs into semantically meaningful groups: the first step in verb sense induction. We show that in a small pilot study on new examples from the same domain we are able to achieve almost perfect recall and reasonably high precision in the semantic typing of verb arguments in these texts."
O06-5001,Tokenization and Morphological Analysis for {M}alagasy,2006,-1,-1,2,0.753307,50002,mary dalrymple,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 11, Number 4, {D}ecember 2006",0,None
Y05-1008,A Two-level Morphology of {M}alagasy,2005,-1,-1,2,0.753307,50002,mary dalrymple,"Proceedings of the 19th Pacific Asia Conference on Language, Information and Computation",0,None
C04-1027,Learning theories from text,2004,11,7,1,1,3114,maria liakata,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In this paper we describe a method of automatically learning domain theories from parsed corpora of sentences from the relevant domain and use FSA techniques for the graphical representation of such a theory. By a 'domain theory' we mean a collection of facts and generalisations or rules which capture what commonly happens (or does not happen) in some domain of interest. As language users, we implicitly draw on such theories in various disambiguation tasks, such as anaphora resolution and prepositional phrase attachment, and formal encodings of domain theories can be used for this purpose in natural language processing. They may also be objects of interest in their own right, that is, as the output of a knowledge discovery process. The approach is generizable to different domains provided it is possible to get logical forms for the text in the domain."
C02-1105,From Trees to Predicate-argument Structures,2002,10,19,1,1,3114,maria liakata,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"The Penn Treebank encodes valuable information such as grammatical function, semantic roles, and identification of traces. The addition of such information was intended to facilitate the process of predicate-argument extraction. However, even with the enriched annotation this task is far from trivial and, to our knowledge, no complete set of predicate argument structures derived from the Treebank exists. Our paper describes a method for retrieving predicate-argument structures that circumvents the complexity of the tree structures in the corpus, while employing few template rules. Our system operates on a flattened, morphologically enriched version of the corpus. This flattened representation allows access to all levels of the tree simultaneously and thus enables the detection of the main sentence constituents by means of simple template rules. A small number of rules apply to identify the head words of each constituent and the latter fill in the constituent templates, to build the logical forms representative of the predicate argument structure. The system is robust in the face of incomplete syntactic coverage."
demiros-etal-2000-named,Named Entity Recognition in {G}reek Texts,2000,16,12,4,0,52285,iason demiros,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"In this paper, we describe work in progress for the development of a named entity recognizer for Greek. The system aims at information extraction applications where large scale text processing is needed. Speed of analysis, system robustness, and results accuracy have been the basic guidelines for the systemxe2x80x99s design. Our system is an automated pipeline of linguistic components for Greek text processing based on pattern matching techniques. Non-recursive regular expressions have been implemented on top of it in order to capture different types of named entities. For development and testing purposes, we collected a corpus of financial texts from several web sources and manually annotated part of it. Overall precision and recall are 86% and 81% respectively."
