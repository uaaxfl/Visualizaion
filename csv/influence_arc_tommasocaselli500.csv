2020.lrec-1.769,P06-2005,0,0.193435,"Missing"
2020.lrec-1.769,W15-4319,0,0.127835,"Missing"
2020.lrec-1.769,N18-1090,0,0.127352,"Missing"
2020.lrec-1.769,R13-1026,0,0.0538702,"Missing"
2020.lrec-1.769,N13-1037,0,0.141305,"Missing"
2020.lrec-1.769,P11-2008,0,0.170972,"Missing"
2020.lrec-1.769,P11-1038,0,0.148421,"Missing"
2020.lrec-1.769,W15-4313,0,0.334773,"Missing"
2020.lrec-1.769,E17-2068,0,0.0655354,"Missing"
2020.lrec-1.769,Q16-1023,0,0.0438243,"Missing"
2020.lrec-1.769,N18-1088,0,0.0298554,"Missing"
2020.lrec-1.769,W17-1410,0,0.047806,"Missing"
2020.lrec-1.769,D19-5539,0,0.194755,"Missing"
2020.lrec-1.769,D19-5553,0,0.0195383,"(e.g. nn 7→ non); (ii) diacritics not correctly used (e’ 7→ e` ); (iii) orthography based on pronunciation (ke 7→ che). Previous work has shown that lexical normalization can be used to improve performance for a variety of NLP tasks, including POS tagging (Derczynski et al., 2013), parsing (Zhang et al., 2013; Bhat et al., 2018), machine Original text nn e’ nnt joker cmq Joker Normalized version comunque non e` niente Joker actually English not is di ke ! di che ! worthwhile ! Joker is actually not worthwhile ! Figure 1: Example of a normalized sentence in Italian. translation (Rosales N´un˜ ez et al., 2019), and named entity tagging (Schulz et al., 2016). Lexical normalization systems and benchmarks are available for multiple languages, we refer to Sharf and Rahman (2017) and van der Goot (2019) for an overview of available datasets and annotation efforts. Even though there is a rich stream of work on other natural language processing tasks for Italian social media (Bosco et al., 2016; Basile et al., 2016; Ronzano et al., 2018; Bosco et al., 2018), for lexical normalization only a rule-based system with a small test set is reported in the literature (Weber and Zhekova, 2016). In this work, our m"
2020.lrec-1.769,L18-1279,0,0.0135277,"density and save time, we filtered the tweets which contain at least 3 out-of-vocabulary (OOV) words4 to mainly focus on tweets containing nonstandard language. A token is considered OOV if it does not appear in the Aspell5 dictionary for Italian, or if it is either a url, a username, an hashtag, or if it only consists of punctuation. These latter elements have been identified by means of regular expressions. Furthermore, we created a small list of proper nouns from the most frequent OOV words in this dataset, and added them to the vocabulary. We maintained the splits of the original dataset (Sanguinetti et al., 2018). Because of the small size of the resulting data, we merge the training and development data, and perform experiments in a 10-fold setting. Basic statistics of the data after filtering are shown in Table 1. 4 Annotation Annotation was done by four native speakers of Italian.6 They are all male, between the age of 20 and 38 and from a variety of regions (Veneto, Tuscany, Liguria, and Apulia). All annotators have a background in natural language processing and are familiar with the Twitter platform. The annotation has been conducted in two different steps and moments in time: first, the annotat"
2020.lrec-1.769,K18-2011,0,0.0282302,"Missing"
2020.lrec-1.769,N03-1033,0,0.0400461,"Missing"
2020.lrec-1.769,D18-1542,1,0.902963,"Missing"
2020.lrec-1.769,P19-3032,1,0.814988,"Missing"
2020.lrec-1.769,K18-2001,0,0.0567101,"Missing"
2020.lrec-1.769,P13-1114,0,0.0284929,"Missing"
2020.peoples-1.9,D16-1120,0,0.0681053,"Missing"
2020.peoples-1.9,L16-1624,0,0.0228309,"spectrum. Our analysis characterizes the evolution of the discourse around COVID-19 in Dutch speaking communities between February and April 2020, both in the topics that dominated and in the emotions expressed that reflect differences across Twitter communities during this virtual national conversation. 2 Related Works Twitter has been largely used as a proxy of natural language data for the study of different sociodemographics issues of a target population. Previous work ranges from the identification of personality traits (Plank and Hovy, 2015), expressions of emotions (Hagen et al., 2015; Dini and Bittar, 2016), language use (Blodgett et al., 2016), age (Sloan et al., 2015), gender (Rangel et al., 2017), and authorship attributions (Schwartz et al., 2013; Rangel et al., 2017), among others. Previous attempts at community detection of Twitter users have used numerous methods and classification types, categorizing users based on their tweet context (Java et al., 2007), interaction and topics in the network (Darmon et al., 2014; Bakillah et al., 2015), sentiment oriented approaches (Abel et al., 2011; Cao et al., 2015) and followingto-followers ratio (Krishnamurthy et al., 2008). Topic modeling has bee"
2020.peoples-1.9,S15-2097,0,0.0362564,"Missing"
2020.peoples-1.9,W16-0403,0,0.0642262,"Missing"
2020.peoples-1.9,D19-5008,0,0.0221304,"Missing"
2020.peoples-1.9,W15-2913,0,0.0271082,"es, and (3) we identify community differences also in the emotional spectrum. Our analysis characterizes the evolution of the discourse around COVID-19 in Dutch speaking communities between February and April 2020, both in the topics that dominated and in the emotions expressed that reflect differences across Twitter communities during this virtual national conversation. 2 Related Works Twitter has been largely used as a proxy of natural language data for the study of different sociodemographics issues of a target population. Previous work ranges from the identification of personality traits (Plank and Hovy, 2015), expressions of emotions (Hagen et al., 2015; Dini and Bittar, 2016), language use (Blodgett et al., 2016), age (Sloan et al., 2015), gender (Rangel et al., 2017), and authorship attributions (Schwartz et al., 2013; Rangel et al., 2017), among others. Previous attempts at community detection of Twitter users have used numerous methods and classification types, categorizing users based on their tweet context (Java et al., 2007), interaction and topics in the network (Darmon et al., 2014; Bakillah et al., 2015), sentiment oriented approaches (Abel et al., 2011; Cao et al., 2015) and followingto"
2020.peoples-1.9,N10-1020,0,0.0397607,"gel et al., 2017), among others. Previous attempts at community detection of Twitter users have used numerous methods and classification types, categorizing users based on their tweet context (Java et al., 2007), interaction and topics in the network (Darmon et al., 2014; Bakillah et al., 2015), sentiment oriented approaches (Abel et al., 2011; Cao et al., 2015) and followingto-followers ratio (Krishnamurthy et al., 2008). Topic modeling has been widely and successfully used as a distant reading method to explore massive amounts of data and identify clusters of information (Yang et al., 2011; Ritter et al., 2010; Sarioglu et al., 2013; Sch¨och, 2017). The application of these approaches to Twitter data is a challenging task due to the short nature of the texts (Hong and Davison, 2010). Previous work has mainly applied Latent Dirichlet Allocation (LDA) based methods (Blei et al., 2003) with varying degree of success. The main issue is that LDA works under the assumption that a document can belong to multiple topics, where the membership is determined by some probability over the words. Since tweets are short snippets of text communicating often one single thought, they are instead more likely to conta"
2020.peoples-1.9,D19-5005,0,0.0301268,"Missing"
2020.peoples-1.9,P13-3010,0,0.0172515,"ong others. Previous attempts at community detection of Twitter users have used numerous methods and classification types, categorizing users based on their tweet context (Java et al., 2007), interaction and topics in the network (Darmon et al., 2014; Bakillah et al., 2015), sentiment oriented approaches (Abel et al., 2011; Cao et al., 2015) and followingto-followers ratio (Krishnamurthy et al., 2008). Topic modeling has been widely and successfully used as a distant reading method to explore massive amounts of data and identify clusters of information (Yang et al., 2011; Ritter et al., 2010; Sarioglu et al., 2013; Sch¨och, 2017). The application of these approaches to Twitter data is a challenging task due to the short nature of the texts (Hong and Davison, 2010). Previous work has mainly applied Latent Dirichlet Allocation (LDA) based methods (Blei et al., 2003) with varying degree of success. The main issue is that LDA works under the assumption that a document can belong to multiple topics, where the membership is determined by some probability over the words. Since tweets are short snippets of text communicating often one single thought, they are instead more likely to contain only a single topic"
2020.peoples-1.9,D13-1193,0,0.0238311,"both in the topics that dominated and in the emotions expressed that reflect differences across Twitter communities during this virtual national conversation. 2 Related Works Twitter has been largely used as a proxy of natural language data for the study of different sociodemographics issues of a target population. Previous work ranges from the identification of personality traits (Plank and Hovy, 2015), expressions of emotions (Hagen et al., 2015; Dini and Bittar, 2016), language use (Blodgett et al., 2016), age (Sloan et al., 2015), gender (Rangel et al., 2017), and authorship attributions (Schwartz et al., 2013; Rangel et al., 2017), among others. Previous attempts at community detection of Twitter users have used numerous methods and classification types, categorizing users based on their tweet context (Java et al., 2007), interaction and topics in the network (Darmon et al., 2014; Bakillah et al., 2015), sentiment oriented approaches (Abel et al., 2011; Cao et al., 2015) and followingto-followers ratio (Krishnamurthy et al., 2008). Topic modeling has been widely and successfully used as a distant reading method to explore massive amounts of data and identify clusters of information (Yang et al., 2"
2020.peoples-1.9,W11-1513,0,0.0377335,"z et al., 2013; Rangel et al., 2017), among others. Previous attempts at community detection of Twitter users have used numerous methods and classification types, categorizing users based on their tweet context (Java et al., 2007), interaction and topics in the network (Darmon et al., 2014; Bakillah et al., 2015), sentiment oriented approaches (Abel et al., 2011; Cao et al., 2015) and followingto-followers ratio (Krishnamurthy et al., 2008). Topic modeling has been widely and successfully used as a distant reading method to explore massive amounts of data and identify clusters of information (Yang et al., 2011; Ritter et al., 2010; Sarioglu et al., 2013; Sch¨och, 2017). The application of these approaches to Twitter data is a challenging task due to the short nature of the texts (Hong and Davison, 2010). Previous work has mainly applied Latent Dirichlet Allocation (LDA) based methods (Blei et al., 2003) with varying degree of success. The main issue is that LDA works under the assumption that a document can belong to multiple topics, where the membership is determined by some probability over the words. Since tweets are short snippets of text communicating often one single thought, they are instead"
2020.restup-1.4,S19-2007,0,0.0206298,"ng the same phenomena (e.g. offensive language, or hate speech) but applying slightly different definitions, different annotation approaches (e.g. experts vs. crowdsourcing), and different reference domains (e.g., Twitter, Facebook, Reddit). Hate speech, in particular, has been the target of the latest major evaluation campaigns such as SemEval 2019 (Zampieri et al., 2019b; Basile et al., 2019), EVALITA 2018 (Bosco et al., 2018), and IberEVAL 2018 (Fersini et al., 2018) in an attempt to promote both the development of working systems and a better understanding of the phenomenon. Vidgen et al. (2019) and Jurgens et al. (2019) identify a set of pending issues that require attention and care by people in NLP working on this topic. One of them concerns a revision of what actually constitutes abuse. The perspective that has been adopted so far in the definition of abusive language, and most importantly of hate speech, has been limited to specific and narrow types of abusive/hateful behaviors to recognize. For instance, definitions of hate speech • use of communities (Tulkens et al., 2016; Merenda et al., 2018): potentially hateful or abusive messages are extracted by collecting data from on-l"
2020.restup-1.4,P19-1357,0,0.0228063,"sages before finding, very sparse, hateful cases. To circumvent this obstacle, three main strategies have been adopted so far: The automatic detection of abusive and offensive messages in on-line communities has become a pressing issue. The promise of Social Media to create a more open and connected world is challenged by the growth of abusive behaviors, among which cyberbullying, trolling, and hate speech are some of the most known. It has also been shown that awareness of being a victim of some kind of abusive behavior is less widespread than what one actually reports as having experienced (Jurgens et al., 2019). The body of work conducted in the areas of abusive language, hate speech, and offensive language has rapidly grown in the last years, leaving the field with a variety of definitions and a lack of reflection on the intersection among such different phenomena (Waseem et al., 2017; Vidgen et al., 2019). As a direct consequence, there has been a flood of annotated datasets in different languages, 1 all somehow addressing the same phenomena (e.g. offensive language, or hate speech) but applying slightly different definitions, different annotation approaches (e.g. experts vs. crowdsourcing), and d"
2020.restup-1.4,L18-1443,0,0.122229,"Missing"
2020.restup-1.4,W19-3509,0,0.0166277,"rld is challenged by the growth of abusive behaviors, among which cyberbullying, trolling, and hate speech are some of the most known. It has also been shown that awareness of being a victim of some kind of abusive behavior is less widespread than what one actually reports as having experienced (Jurgens et al., 2019). The body of work conducted in the areas of abusive language, hate speech, and offensive language has rapidly grown in the last years, leaving the field with a variety of definitions and a lack of reflection on the intersection among such different phenomena (Waseem et al., 2017; Vidgen et al., 2019). As a direct consequence, there has been a flood of annotated datasets in different languages, 1 all somehow addressing the same phenomena (e.g. offensive language, or hate speech) but applying slightly different definitions, different annotation approaches (e.g. experts vs. crowdsourcing), and different reference domains (e.g., Twitter, Facebook, Reddit). Hate speech, in particular, has been the target of the latest major evaluation campaigns such as SemEval 2019 (Zampieri et al., 2019b; Basile et al., 2019), EVALITA 2018 (Bosco et al., 2018), and IberEVAL 2018 (Fersini et al., 2018) in an a"
2020.restup-1.4,N16-2013,0,0.0872087,"NLP working on this topic. One of them concerns a revision of what actually constitutes abuse. The perspective that has been adopted so far in the definition of abusive language, and most importantly of hate speech, has been limited to specific and narrow types of abusive/hateful behaviors to recognize. For instance, definitions of hate speech • use of communities (Tulkens et al., 2016; Merenda et al., 2018): potentially hateful or abusive messages are extracted by collecting data from on-line communities that are known either to promote or tolerate such types of messages; • use of keywords (Waseem and Hovy, 2016; Basile et al., 2019; Zampieri et al., 2019a): specific keywords which are not hateful or abusive per se but that may be the target of hateful or abusive messages, like for instance the word “migrants”, are selected to collect random messages from Social Media outlets; • use of users (Wiegand et al., 2018; Ribeiro et al., 2018): seed users that have been identified via some heuristics to regularly post abusive or hateful materials are selected and their messages collected. In a variation of this approach, additional potential “hateful” users are identified by applying network analysis to the"
2020.restup-1.4,W17-3012,0,0.0977559,"open and connected world is challenged by the growth of abusive behaviors, among which cyberbullying, trolling, and hate speech are some of the most known. It has also been shown that awareness of being a victim of some kind of abusive behavior is less widespread than what one actually reports as having experienced (Jurgens et al., 2019). The body of work conducted in the areas of abusive language, hate speech, and offensive language has rapidly grown in the last years, leaving the field with a variety of definitions and a lack of reflection on the intersection among such different phenomena (Waseem et al., 2017; Vidgen et al., 2019). As a direct consequence, there has been a flood of annotated datasets in different languages, 1 all somehow addressing the same phenomena (e.g. offensive language, or hate speech) but applying slightly different definitions, different annotation approaches (e.g. experts vs. crowdsourcing), and different reference domains (e.g., Twitter, Facebook, Reddit). Hate speech, in particular, has been the target of the latest major evaluation campaigns such as SemEval 2019 (Zampieri et al., 2019b; Basile et al., 2019), EVALITA 2018 (Bosco et al., 2018), and IberEVAL 2018 (Fersini"
2020.restup-1.4,N18-1095,0,0.15358,"r instance, definitions of hate speech • use of communities (Tulkens et al., 2016; Merenda et al., 2018): potentially hateful or abusive messages are extracted by collecting data from on-line communities that are known either to promote or tolerate such types of messages; • use of keywords (Waseem and Hovy, 2016; Basile et al., 2019; Zampieri et al., 2019a): specific keywords which are not hateful or abusive per se but that may be the target of hateful or abusive messages, like for instance the word “migrants”, are selected to collect random messages from Social Media outlets; • use of users (Wiegand et al., 2018; Ribeiro et al., 2018): seed users that have been identified via some heuristics to regularly post abusive or hateful materials are selected and their messages collected. In a variation of this approach, additional potential “hateful” users are identified by applying network analysis to the seed users. 1 For a more detailed overview of available datasets in different languages please consult https://github.com/leondz/ hatespeechdata. 14 Common advantages of these approaches mainly lie in the reduction of annotation time and a higher density of positive instances, i.e. hateful messages in our"
2020.restup-1.4,N19-1060,0,0.107468,"Missing"
2020.restup-1.4,N19-1144,0,0.0344569,"efinitions and a lack of reflection on the intersection among such different phenomena (Waseem et al., 2017; Vidgen et al., 2019). As a direct consequence, there has been a flood of annotated datasets in different languages, 1 all somehow addressing the same phenomena (e.g. offensive language, or hate speech) but applying slightly different definitions, different annotation approaches (e.g. experts vs. crowdsourcing), and different reference domains (e.g., Twitter, Facebook, Reddit). Hate speech, in particular, has been the target of the latest major evaluation campaigns such as SemEval 2019 (Zampieri et al., 2019b; Basile et al., 2019), EVALITA 2018 (Bosco et al., 2018), and IberEVAL 2018 (Fersini et al., 2018) in an attempt to promote both the development of working systems and a better understanding of the phenomenon. Vidgen et al. (2019) and Jurgens et al. (2019) identify a set of pending issues that require attention and care by people in NLP working on this topic. One of them concerns a revision of what actually constitutes abuse. The perspective that has been adopted so far in the definition of abusive language, and most importantly of hate speech, has been limited to specific and narrow types o"
2020.restup-1.4,S19-2010,0,0.0291805,"efinitions and a lack of reflection on the intersection among such different phenomena (Waseem et al., 2017; Vidgen et al., 2019). As a direct consequence, there has been a flood of annotated datasets in different languages, 1 all somehow addressing the same phenomena (e.g. offensive language, or hate speech) but applying slightly different definitions, different annotation approaches (e.g. experts vs. crowdsourcing), and different reference domains (e.g., Twitter, Facebook, Reddit). Hate speech, in particular, has been the target of the latest major evaluation campaigns such as SemEval 2019 (Zampieri et al., 2019b; Basile et al., 2019), EVALITA 2018 (Bosco et al., 2018), and IberEVAL 2018 (Fersini et al., 2018) in an attempt to promote both the development of working systems and a better understanding of the phenomenon. Vidgen et al. (2019) and Jurgens et al. (2019) identify a set of pending issues that require attention and care by people in NLP working on this topic. One of them concerns a revision of what actually constitutes abuse. The perspective that has been adopted so far in the definition of abusive language, and most importantly of hate speech, has been limited to specific and narrow types o"
2021.case-1.4,D18-1498,0,0.0607472,"Missing"
2021.case-1.4,C18-1139,0,0.0604325,"Missing"
2021.case-1.4,2020.acl-main.740,0,0.0402352,"Missing"
2021.case-1.4,P18-1099,0,0.0639982,"Missing"
2021.case-1.4,D19-1433,0,0.0186634,"cio-demographic aspects, annotation bias, among other unknown factors. Every dataset belonging to a different variety poses a domain adaptation challenge. Unsupervised domain adaptation has a long tradition in NLP (Blitzer et al., 2006; McClosky et al., 2006; Moore and Lewis, 2010; Ganin et al., 2016; Ruder and Plank, 2017; Guo et al., 2018; Miller, 2019; Nishida et al., 2020). The availability of large pre-trained transformer-based language models (TLMs), e.g., BERT (Devlin et al., 2019), has inspired a new trend in domain adaptation, namely domain adaptive retraining (DAR) (Xu et al., 2019; Han and Eisenstein, 2019; Rietzler et al., 2020; Gururangan et al., 2020). The idea behind DAR is as simple as effective: first, additional textual material matching the target domain is selected, then the masked language modeling (MLM) objective is used to further train an existing TLMs. The outcome is a new TLM whose representations are shifted to better suit the target domain. Fine-tuning domain adapted TLMs results in improved performance. This contribution applies this approach to develop a portable system for protest event extraction. Our unsupervised domain adaptation setting investigates two related aspects."
2021.case-1.4,D11-1033,0,0.131843,"Missing"
2021.case-1.4,D17-1038,0,0.110629,"al., 2020). As such, portability is a domain adaptation problem. Following Ramponi and Plank (2020), we consider a domain to be a variety where each corpus, or dataset, can be described as a multidimensional region including notions such as topics, genres, writing styles, years of publication, socio-demographic aspects, annotation bias, among other unknown factors. Every dataset belonging to a different variety poses a domain adaptation challenge. Unsupervised domain adaptation has a long tradition in NLP (Blitzer et al., 2006; McClosky et al., 2006; Moore and Lewis, 2010; Ganin et al., 2016; Ruder and Plank, 2017; Guo et al., 2018; Miller, 2019; Nishida et al., 2020). The availability of large pre-trained transformer-based language models (TLMs), e.g., BERT (Devlin et al., 2019), has inspired a new trend in domain adaptation, namely domain adaptive retraining (DAR) (Xu et al., 2019; Han and Eisenstein, 2019; Rietzler et al., 2020; Gururangan et al., 2020). The idea behind DAR is as simple as effective: first, additional textual material matching the target domain is selected, then the masked language modeling (MLM) objective is used to further train an existing TLMs. The outcome is a new TLM whose rep"
2021.case-1.4,P06-1043,0,0.0948024,"m et al., 2018; Xie et al., 2018; Zhao et al., 2019; Ben-David et al., 2020). As such, portability is a domain adaptation problem. Following Ramponi and Plank (2020), we consider a domain to be a variety where each corpus, or dataset, can be described as a multidimensional region including notions such as topics, genres, writing styles, years of publication, socio-demographic aspects, annotation bias, among other unknown factors. Every dataset belonging to a different variety poses a domain adaptation challenge. Unsupervised domain adaptation has a long tradition in NLP (Blitzer et al., 2006; McClosky et al., 2006; Moore and Lewis, 2010; Ganin et al., 2016; Ruder and Plank, 2017; Guo et al., 2018; Miller, 2019; Nishida et al., 2020). The availability of large pre-trained transformer-based language models (TLMs), e.g., BERT (Devlin et al., 2019), has inspired a new trend in domain adaptation, namely domain adaptive retraining (DAR) (Xu et al., 2019; Han and Eisenstein, 2019; Rietzler et al., 2020; Gururangan et al., 2020). The idea behind DAR is as simple as effective: first, additional textual material matching the target domain is selected, then the masked language modeling (MLM) objective is used to"
2021.case-1.4,N19-1039,0,0.0260144,"ain adaptation problem. Following Ramponi and Plank (2020), we consider a domain to be a variety where each corpus, or dataset, can be described as a multidimensional region including notions such as topics, genres, writing styles, years of publication, socio-demographic aspects, annotation bias, among other unknown factors. Every dataset belonging to a different variety poses a domain adaptation challenge. Unsupervised domain adaptation has a long tradition in NLP (Blitzer et al., 2006; McClosky et al., 2006; Moore and Lewis, 2010; Ganin et al., 2016; Ruder and Plank, 2017; Guo et al., 2018; Miller, 2019; Nishida et al., 2020). The availability of large pre-trained transformer-based language models (TLMs), e.g., BERT (Devlin et al., 2019), has inspired a new trend in domain adaptation, namely domain adaptive retraining (DAR) (Xu et al., 2019; Han and Eisenstein, 2019; Rietzler et al., 2020; Gururangan et al., 2020). The idea behind DAR is as simple as effective: first, additional textual material matching the target domain is selected, then the masked language modeling (MLM) objective is used to further train an existing TLMs. The outcome is a new TLM whose representations are shifted to bett"
2021.case-1.4,P10-2041,0,0.0689049,"al., 2018; Zhao et al., 2019; Ben-David et al., 2020). As such, portability is a domain adaptation problem. Following Ramponi and Plank (2020), we consider a domain to be a variety where each corpus, or dataset, can be described as a multidimensional region including notions such as topics, genres, writing styles, years of publication, socio-demographic aspects, annotation bias, among other unknown factors. Every dataset belonging to a different variety poses a domain adaptation challenge. Unsupervised domain adaptation has a long tradition in NLP (Blitzer et al., 2006; McClosky et al., 2006; Moore and Lewis, 2010; Ganin et al., 2016; Ruder and Plank, 2017; Guo et al., 2018; Miller, 2019; Nishida et al., 2020). The availability of large pre-trained transformer-based language models (TLMs), e.g., BERT (Devlin et al., 2019), has inspired a new trend in domain adaptation, namely domain adaptive retraining (DAR) (Xu et al., 2019; Han and Eisenstein, 2019; Rietzler et al., 2020; Gururangan et al., 2020). The idea behind DAR is as simple as effective: first, additional textual material matching the target domain is selected, then the masked language modeling (MLM) objective is used to further train an existi"
2021.case-1.4,N16-1034,0,0.0315241,"BERT the CLEF 2019 India and China documents. ↓DAR / Test→ India J-S China WPC-Gen WPC-Ev 0.583 0.562 0.594 0.569 the DAR data materials against the India and China test data. As the figures show, the DAR datasets are equally different from the protest event extraction ones. Furthermore, we did not modify BERT original vocabulary by introducing new tokens. More details on the retraining parameters are reported in the Appendix A.1. 4 Event extraction is framed as a token-level classification task. We adopt a joint strategy where triggers’ and arguments’ extent and labels are predicted at once (Nguyen et al., 2016). We used Indiatest to identify the best model (NEWS-BERT vs. PROTEST-ER) and system’s input granularity. With respect to this latter point, we investigate whether processing data at document or sentence level could benefit the TLMs as a strategy to deal with limited training materials. We compare each configuration against a generic BERT counterpart. We fine-tune each model by training all the parameters simultaneously. All models are evaluated using the official script from the ProtestNews Lab. Triggers and arguments are correctly identified only if both the extent and the label are correct."
2021.case-1.4,N19-1242,0,0.0271257,"f publication, socio-demographic aspects, annotation bias, among other unknown factors. Every dataset belonging to a different variety poses a domain adaptation challenge. Unsupervised domain adaptation has a long tradition in NLP (Blitzer et al., 2006; McClosky et al., 2006; Moore and Lewis, 2010; Ganin et al., 2016; Ruder and Plank, 2017; Guo et al., 2018; Miller, 2019; Nishida et al., 2020). The availability of large pre-trained transformer-based language models (TLMs), e.g., BERT (Devlin et al., 2019), has inspired a new trend in domain adaptation, namely domain adaptive retraining (DAR) (Xu et al., 2019; Han and Eisenstein, 2019; Rietzler et al., 2020; Gururangan et al., 2020). The idea behind DAR is as simple as effective: first, additional textual material matching the target domain is selected, then the masked language modeling (MLM) objective is used to further train an existing TLMs. The outcome is a new TLM whose representations are shifted to better suit the target domain. Fine-tuning domain adapted TLMs results in improved performance. This contribution applies this approach to develop a portable system for protest event extraction. Our unsupervised domain adaptation setting investig"
2021.case-1.4,2020.lrec-1.663,0,0.0343131,"problem. Following Ramponi and Plank (2020), we consider a domain to be a variety where each corpus, or dataset, can be described as a multidimensional region including notions such as topics, genres, writing styles, years of publication, socio-demographic aspects, annotation bias, among other unknown factors. Every dataset belonging to a different variety poses a domain adaptation challenge. Unsupervised domain adaptation has a long tradition in NLP (Blitzer et al., 2006; McClosky et al., 2006; Moore and Lewis, 2010; Ganin et al., 2016; Ruder and Plank, 2017; Guo et al., 2018; Miller, 2019; Nishida et al., 2020). The availability of large pre-trained transformer-based language models (TLMs), e.g., BERT (Devlin et al., 2019), has inspired a new trend in domain adaptation, namely domain adaptive retraining (DAR) (Xu et al., 2019; Han and Eisenstein, 2019; Rietzler et al., 2020; Gururangan et al., 2020). The idea behind DAR is as simple as effective: first, additional textual material matching the target domain is selected, then the masked language modeling (MLM) objective is used to further train an existing TLMs. The outcome is a new TLM whose representations are shifted to better suit the target doma"
2021.case-1.4,P11-1157,0,0.0942674,"Missing"
2021.case-1.4,2020.coling-main.603,0,0.0553433,"Missing"
2021.findings-emnlp.56,2021.nlp4if-1.9,1,0.705503,"tions. Some of the larger datasets include the Liar, Liar dataset of 12.8K claims from PolitiFact (Wang, 2017), the ClaimsKG dataset and system (Tchechmedjiev et al., 2019) of 28K claims from eight factchecking organizations, the MultiFC dataset of 38K claims from 26 fact-checking organizations (Augenstein et al., 2019), and the 10K claims Truth of Various Shades dataset (Rashkin et al., 2017). There have been also datasets for other languages, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino"
2021.findings-emnlp.56,N19-1216,1,0.795082,"tweets in Arabic, Bulgarian, Dutch, and English, and we are making it freely available to the research community. We further reported a number of evaluation results for all languages using various transformer architectures. Moreover, we performed advanced experiments, including multilingual training, modeling the Twitter context, the use of propagandistic language, and whether the user is likely to be a bot, as well as multitask learning. In future work, we plan to explore multimodality and explainability (Yu et al., 2021). We further want to model the task as a multitask ordinal regression (Baly et al., 2019), as Q2–Q5 are defined on an ordinal scale. Moreover, we would like to put the data and the system in some practical use; in fact, we have already used them to analyze disinformation about COVID-19 in Bulgaria (Nakov et al., 2021a) and Qatar (Nakov et al., 2021b). Finally, the data will be used in a shared task at the CLEF2022 CheckThat! lab; part of it was used for the NLP4IF-2021 shared task (Shaar et al., 2021a). Acknowledgments We thank Akter Fatema, Al-Awthan Ahmed, AlDobashi Hussein, El Messelmani Jana, Fayoumi 6.3 Multitask Learning Sereen, Mohamed Esraa, Ragab Saleh, and Shurafa For th"
2021.findings-emnlp.56,N18-2004,1,0.90491,"Missing"
2021.findings-emnlp.56,2020.acl-main.747,0,0.0346481,"a URL, and the factuality of the website it points to.4 Models Large-scale pretrained Transformer models have achieved state-of-the-art performance for several NLP tasks. We experimented with several such models to evaluate their efficacy under various training scenarios such as, binary vs. multiclass classification, multilingual setup, etc. We used BERT (Devlin et al., 2019) and RoBERTa for English, AraBERT (Antoun et al., 2020) for Arabic, and BERTje (de Vries et al., 2019) for Dutch. We further used multilingual transformers such as (Liu et al., 2019), multilingual BERT (mBERT) and XLM-r (Conneau et al., 2020). Finally, we used static embeddings from FastText (Joulin et al., 2017). 616 4 From http://mediabiasfactcheck.com English Q. Cls. Arabic Maj. FT BT RT Bulgarian Maj. FT ArBT XLM-r Dutch Maj. FT mBT XLM-r Maj. FT BTje XLM-r Binary (Coarse-grained) Q1 Q2 Q3 Q4 Q5 Q6 Q7 2 2 2 2 2 2 2 Avg. 48.7 91.6 96.3 66.7 67.7 86.7 78.3 77.7 89.0 69.3 96.3 83.8 92.1 80.6 76.5 92.1 96.4 85.6 80.6 88.9 85.5 78.6 92.7 96.9 89.0 84.4 90.5 86.1 76.6 84.1 86.5 88.3 83.8 84.0 96.0 90.3 65.9 88.9 77.4 84.2 83.1 96.3 89.0 66.7 89.8 77.4 58.3 95.0 96.5 86.8 70.5 83.2 80.1 84.0 94.7 96.0 87.7 80.5 84.5 81.6 87.6 95.0 96"
2021.findings-emnlp.56,2020.semeval-1.186,1,0.850189,"., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER"
2021.findings-emnlp.56,2020.acl-demos.32,1,0.926725,"., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER"
2021.findings-emnlp.56,S19-2147,0,0.0285024,"s, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar"
2021.findings-emnlp.56,2021.wanlp-1.9,0,0.0367297,"llected tweets by specifying a target language (English, Arabic, Bulgarian, or Dutch), a set of COVID-19 related keywords, as shown in Figure 2, and different time frames: from January 2020 till March 2021. We collected original tweets (no retweets or replies), we removed duplicates using a similarity-based approach (Alam et al., 2021b), and we filtered out tweets with less than five words. Finally, we selected the most frequently liked and retweeted tweets for annotation. COVID-19 Research There are a number of COVID-19 Twitter datasets: some unlabeled (Chen et al., 2020; Banda et al., 2021; Haouari et al., 2021), some automatically labeled with location information (Abdul-Mageed et al., 2021; Qazi et al., 2020), some labeled using distant supervision (Cinelli et al., 2020; Zhou et al., 2020), and some manually annotated (Song et al., 2020; Vidgen et al., 2020; Shahi and Nandini, 2020; Pulido et al., 2020; Dharawat et al., 2020). There is also work on credibility (Cinelli et al., 2020; Pulido et al., 2020; Zhou et al., 2020), racial prejudices and fear (Medford et al., 2020; Vidgen et al., 2020), as well as situational information, e.g., caution and advice (Li et al., 2020), as well as on detecting me"
2021.findings-emnlp.56,2020.nlpcovid19-2.11,0,0.043054,"Abdul-Mageed et al., 2021; Qazi et al., 2020), some labeled using distant supervision (Cinelli et al., 2020; Zhou et al., 2020), and some manually annotated (Song et al., 2020; Vidgen et al., 2020; Shahi and Nandini, 2020; Pulido et al., 2020; Dharawat et al., 2020). There is also work on credibility (Cinelli et al., 2020; Pulido et al., 2020; Zhou et al., 2020), racial prejudices and fear (Medford et al., 2020; Vidgen et al., 2020), as well as situational information, e.g., caution and advice (Li et al., 2020), as well as on detecting mentions and stance with respect to known misconceptions (Hossain et al., 2020). The closest work to ours is that of Song et al. (2020), who collected false and misleading claims about COVID-19 from IFCN Poynter, and annotated them as (1) Public authority, (2) Community spread and impact, (3) Medical advice, selftreatments, and virus effects, (4) Prominent actors, (5) Conspiracies, (6) Virus transmission, (7) Virus Figure 2: The keywords used to collect the tweets. origins and properties, (8) Public reaction, and (9) Vaccines, medical treatments, and tests. These categories partially overlap with ours, but account 3.2 Annotation Task for less perspectives. Moreover, we c"
2021.findings-emnlp.56,N18-5006,1,0.802211,"tweets (they used claims from news, speeches, political debates, community question answering fora, or were just made up by human annotators; RumourEval is a notable exception), targeted factuality only (we cover a number of other issues), were limited to a single language (typically English; except for CLEF), and did not focus on COVID-19. Check-Worthiness Estimation Another relevant research line is on detecting check-worthy claims in political debates using manual annotations (Hassan et al., 2015) or by observing the selection of fact-checkers (Gencheva et al., 2017; Patwari et al., 2017; Jaradat et al., 2018; Vasileva et al., 2019). 3 3.1 Dataset Data Collection We collected tweets by specifying a target language (English, Arabic, Bulgarian, or Dutch), a set of COVID-19 related keywords, as shown in Figure 2, and different time frames: from January 2020 till March 2021. We collected original tweets (no retweets or replies), we removed duplicates using a similarity-based approach (Alam et al., 2021b), and we filtered out tweets with less than five words. Finally, we selected the most frequently liked and retweeted tweets for annotation. COVID-19 Research There are a number of COVID-19 Twitter data"
2021.findings-emnlp.56,E17-2068,0,0.0257945,"le pretrained Transformer models have achieved state-of-the-art performance for several NLP tasks. We experimented with several such models to evaluate their efficacy under various training scenarios such as, binary vs. multiclass classification, multilingual setup, etc. We used BERT (Devlin et al., 2019) and RoBERTa for English, AraBERT (Antoun et al., 2020) for Arabic, and BERTje (de Vries et al., 2019) for Dutch. We further used multilingual transformers such as (Liu et al., 2019), multilingual BERT (mBERT) and XLM-r (Conneau et al., 2020). Finally, we used static embeddings from FastText (Joulin et al., 2017). 616 4 From http://mediabiasfactcheck.com English Q. Cls. Arabic Maj. FT BT RT Bulgarian Maj. FT ArBT XLM-r Dutch Maj. FT mBT XLM-r Maj. FT BTje XLM-r Binary (Coarse-grained) Q1 Q2 Q3 Q4 Q5 Q6 Q7 2 2 2 2 2 2 2 Avg. 48.7 91.6 96.3 66.7 67.7 86.7 78.3 77.7 89.0 69.3 96.3 83.8 92.1 80.6 76.5 92.1 96.4 85.6 80.6 88.9 85.5 78.6 92.7 96.9 89.0 84.4 90.5 86.1 76.6 84.1 86.5 88.3 83.8 84.0 96.0 90.3 65.9 88.9 77.4 84.2 83.1 96.3 89.0 66.7 89.8 77.4 58.3 95.0 96.5 86.8 70.5 83.2 80.1 84.0 94.7 96.0 87.7 80.5 84.5 81.6 87.6 95.0 96.5 88.4 82.9 85.1 81.7 36.5 64.9 62.3 63.9 44.4 84.7 65.6 75.4 75.1 76.9"
2021.findings-emnlp.56,2020.emnlp-demos.2,0,0.0153692,"80.2 69.2 68.3 Finally, we should note the strong performance Avg. 73.3 73.1 60.7 59.8 71.4 71.5 55.3 54.9 of context-free models such as FastText. We believe that it is suitable for the noisy text of Table 6: Multilingual experiments using mBERT. tweets due to its ability to model not only words Shown are results for monolingual vs. multilingual models (weighted F1 ). Mul is trained on the combined but also character n-grams. In future work, we English, Arabic, Bulgarian, and Dutch data. plan to try transformers specifically trained on tweets and/or on COVID-19 related data such as BERTweet (Nguyen et al., 2020) and COVID5 Twitter-BERT (Müller et al., 2020). We also tried XLM-r, but it performed worse. 618 6.2 Twitter/Propagandistic/Botometer We conducted experiments with Twitter, propaganda, and botness features alongside the posteriors from the BERT classifier, which we combined using XGBoost (Chen and Guestrin, 2016). The results are shown in Table 7. We can see that many of the combinations yielded improvements, with botness being the most useful, followed by propaganda, and finally by the Twitter object features. Binary (Coarse-grained) Q. Cls BERT B+TF B+Prop B+Bot B+All Q1 Q2 Q3 Q4 Q5 Q6 Q7 2"
2021.findings-emnlp.56,D17-1317,0,0.0286494,"onversations with a Ministry of Public Health. Our contributions can be summarized as follows: 2 Related Work Fact-Checking Research on fact-checking claims is largely based on datasets mined from major fact-checking organizations. Some of the larger datasets include the Liar, Liar dataset of 12.8K claims from PolitiFact (Wang, 2017), the ClaimsKG dataset and system (Tchechmedjiev et al., 2019) of 28K claims from eight factchecking organizations, the MultiFC dataset of 38K claims from 26 fact-checking organizations (Augenstein et al., 2019), and the 10K claims Truth of Various Shades dataset (Rashkin et al., 2017). There have been also datasets for other languages, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers"
2021.findings-emnlp.56,2021.nlp4if-1.12,1,0.887854,"We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER task on fact ex1 traction and verification, focusing on claims about https://github.com/firojalam/ COVID-19-disinformation Wikipedia content (Thorne et al., 2018, 2019). 612 Unlike our work, the above datasets did not focus on tweets (they used claims from news, speeches, political debates, community question answering fora, or were just made up by human annotators; RumourEval is a notable exception), targeted factuality only (we cover a number of other issues), were limited to a single language (t"
2021.lchange-1.3,2020.acl-main.51,0,0.020419,"ora They Are a-Changing: a Case Study in Italian Newspapers Pierpaolo Basile Annalina Caputo , Tommaso Caselli , Pierluigi Cassotti , Rossella Varvara Dept. of Computer Science, University of Bari ADAPT Centre School of Computing, Dublin City University CLCG, University of Groningen Dept. of Computer Science, University of Turin {pierpaolo.basile,pierluigi.cassotti}@uniba.it annalina.caputo@dcu.ie, t.caselli@rug.nl rossella.varvara@unito.it Abstract Lapata, 2016), and neural language models (Hamilton et al., 2016a,b; Schlechtweg et al., 2018; Orlikowski et al., 2018; Brandl and Lassner, 2019; Gonen et al., 2020; Giulianelli et al., 2020; Schlechtweg et al., 2020). This has been possible thanks to two factors: increased availability of machine-readable texts covering different periods and increased processing capabilities. The use of computational models for studying LSC is not free from problems, however, as highlighted by Hengchen et al. (2021). Almost every computational model for LSC is grounded on the Distributional Hypothesis of meaning according to which “the meaning of a word is its use” (Wittgenstein, 2010) and the “difference in meaning correlates with difference in distribution” (Harris, 1"
2021.lchange-1.3,W11-2508,0,0.0430287,"tory of Linguistics. Understanding and explaining why a community of speakers “speak” as they do is of primary importance to access one’s cultural heritage and perspectives on the world. In recent years, the Natural Language Processing (NLP) community has developed an interest in historical linguistics, and in particular in the study of lexical semantics change (LSC). Previous work has investigated LSC using different approaches, including statistical tests over time period (Popescu and Strapparava, 2013), supervised methods (Mihalcea and Nastase, 2012), count-based distributional approaches (Gulordava and Baroni, 2011), sense-based methods (Kim et al., 2014; Mitra et al., 2014; Frermann and 14 Proceedings of the 2nd International Workshop on Computational Approaches to Historical Language Change 2021, pages 14–20 August 6, 2021. ©2021 Association for Computational Linguistics 2 Methodology As architecture for automatic LSC detection, we obtain comparable diachronic representations of word meanings by re-implementing the Word2Vec Skipgram model (Mikolov et al., 2013) with Orthogonal Procrustes (OP-SGNS) (Hamilton et al., 2016b). In particular, we adopted the implementation proposed by Kaiser et al. (2020), a"
2021.lchange-1.3,D16-1229,0,0.0152947,"methods (Mihalcea and Nastase, 2012), count-based distributional approaches (Gulordava and Baroni, 2011), sense-based methods (Kim et al., 2014; Mitra et al., 2014; Frermann and 14 Proceedings of the 2nd International Workshop on Computational Approaches to Historical Language Change 2021, pages 14–20 August 6, 2021. ©2021 Association for Computational Linguistics 2 Methodology As architecture for automatic LSC detection, we obtain comparable diachronic representations of word meanings by re-implementing the Word2Vec Skipgram model (Mikolov et al., 2013) with Orthogonal Procrustes (OP-SGNS) (Hamilton et al., 2016b). In particular, we adopted the implementation proposed by Kaiser et al. (2020), a stateof-the-art system that ranked 1st both at DIACRIta and at SemEval 2020 Task 1: Unsupervised Lexical Semantic Change Detection (Schlechtweg et al., 2020). Model parameters are reported in Appendix A. Word embeddings were generated using lemmas to reduce sparseness and facilitate the evaluation against the benchmark. To test benchmark independence and models’ robustness for LSC, we design a set of experiments using two source corpora, a common benchmark, and a common architecture for LSC detection. The firs"
2021.lchange-1.3,P16-1141,0,0.0186846,"methods (Mihalcea and Nastase, 2012), count-based distributional approaches (Gulordava and Baroni, 2011), sense-based methods (Kim et al., 2014; Mitra et al., 2014; Frermann and 14 Proceedings of the 2nd International Workshop on Computational Approaches to Historical Language Change 2021, pages 14–20 August 6, 2021. ©2021 Association for Computational Linguistics 2 Methodology As architecture for automatic LSC detection, we obtain comparable diachronic representations of word meanings by re-implementing the Word2Vec Skipgram model (Mikolov et al., 2013) with Orthogonal Procrustes (OP-SGNS) (Hamilton et al., 2016b). In particular, we adopted the implementation proposed by Kaiser et al. (2020), a stateof-the-art system that ranked 1st both at DIACRIta and at SemEval 2020 Task 1: Unsupervised Lexical Semantic Change Detection (Schlechtweg et al., 2020). Model parameters are reported in Appendix A. Word embeddings were generated using lemmas to reduce sparseness and facilitate the evaluation against the benchmark. To test benchmark independence and models’ robustness for LSC, we design a set of experiments using two source corpora, a common benchmark, and a common architecture for LSC detection. The firs"
2021.lchange-1.3,P16-1000,0,0.217863,"Missing"
2021.lchange-1.3,N18-2027,0,0.023738,"Missing"
2021.lchange-1.3,W14-2517,0,0.0208579,"why a community of speakers “speak” as they do is of primary importance to access one’s cultural heritage and perspectives on the world. In recent years, the Natural Language Processing (NLP) community has developed an interest in historical linguistics, and in particular in the study of lexical semantics change (LSC). Previous work has investigated LSC using different approaches, including statistical tests over time period (Popescu and Strapparava, 2013), supervised methods (Mihalcea and Nastase, 2012), count-based distributional approaches (Gulordava and Baroni, 2011), sense-based methods (Kim et al., 2014; Mitra et al., 2014; Frermann and 14 Proceedings of the 2nd International Workshop on Computational Approaches to Historical Language Change 2021, pages 14–20 August 6, 2021. ©2021 Association for Computational Linguistics 2 Methodology As architecture for automatic LSC detection, we obtain comparable diachronic representations of word meanings by re-implementing the Word2Vec Skipgram model (Mikolov et al., 2013) with Orthogonal Procrustes (OP-SGNS) (Hamilton et al., 2016b). In particular, we adopted the implementation proposed by Kaiser et al. (2020), a stateof-the-art system that ranked 1st"
2021.lchange-1.3,P12-2051,0,0.038592,"c dimension of natural language has played a pivotal role in the history of Linguistics. Understanding and explaining why a community of speakers “speak” as they do is of primary importance to access one’s cultural heritage and perspectives on the world. In recent years, the Natural Language Processing (NLP) community has developed an interest in historical linguistics, and in particular in the study of lexical semantics change (LSC). Previous work has investigated LSC using different approaches, including statistical tests over time period (Popescu and Strapparava, 2013), supervised methods (Mihalcea and Nastase, 2012), count-based distributional approaches (Gulordava and Baroni, 2011), sense-based methods (Kim et al., 2014; Mitra et al., 2014; Frermann and 14 Proceedings of the 2nd International Workshop on Computational Approaches to Historical Language Change 2021, pages 14–20 August 6, 2021. ©2021 Association for Computational Linguistics 2 Methodology As architecture for automatic LSC detection, we obtain comparable diachronic representations of word meanings by re-implementing the Word2Vec Skipgram model (Mikolov et al., 2013) with Orthogonal Procrustes (OP-SGNS) (Hamilton et al., 2016b). In particula"
2021.lchange-1.3,P14-1096,0,0.0445476,"Missing"
2021.lchange-1.3,W18-4501,0,0.0255316,"Missing"
2021.lchange-1.3,I13-1040,0,0.0265295,"lways subject to change and evolution. The diachronic dimension of natural language has played a pivotal role in the history of Linguistics. Understanding and explaining why a community of speakers “speak” as they do is of primary importance to access one’s cultural heritage and perspectives on the world. In recent years, the Natural Language Processing (NLP) community has developed an interest in historical linguistics, and in particular in the study of lexical semantics change (LSC). Previous work has investigated LSC using different approaches, including statistical tests over time period (Popescu and Strapparava, 2013), supervised methods (Mihalcea and Nastase, 2012), count-based distributional approaches (Gulordava and Baroni, 2011), sense-based methods (Kim et al., 2014; Mitra et al., 2014; Frermann and 14 Proceedings of the 2nd International Workshop on Computational Approaches to Historical Language Change 2021, pages 14–20 August 6, 2021. ©2021 Association for Computational Linguistics 2 Methodology As architecture for automatic LSC detection, we obtain comparable diachronic representations of word meanings by re-implementing the Word2Vec Skipgram model (Mikolov et al., 2013) with Orthogonal Procrustes"
2021.nllp-1.5,2021.nlp4posimpact-1.4,1,0.683362,"Missing"
2021.nllp-1.5,C16-2054,0,0.0203303,"Missing"
2021.nlp4if-1.18,2020.findings-emnlp.148,0,0.0162594,"tection shared tasks (Rosenthal et al., 2017; Van Hee et al., 2018). Here, we use a variant of the model, additionally trained on 23M tweets related to the COVID-19 pandemic, collected prior to September 2020. CT-BERT (M¨uller et al., 2020) is a pre-trained BERTlarge model, adapted for use in the twitter setting and specifically the COVID-19 theme by continued unsupervised training on 160M tweets related to the COVID-19 pandemic and collected between January and April 2020. Fine tuned and evaluated on a small range of tasks, it has been shown to slightly outperform the original. T WEET E VAL (Barbieri et al., 2020) is a pretrained RoBERTabase model, further trained with 60M tweets, randomly collected, resulting in a Twitter-domain adapted version. We use a selection of four T WEET E VAL models, each fine tuned for a twitter-specific downstream task: hate speech-, emotion- and irony-detection, and offensive language identification. 3.2 Fine-tuning The affinity between the above models and the task at hand allows us to use them for sentence vectorization as-is, requiring only an inexpensive fine tuning pass. We attach a linear projection on top of each model, which maps its [CLS] token representation to |"
2021.nlp4if-1.18,N19-1423,0,0.0386935,"ure, even more so if one considers the effective loss of data from nan labels and the small proportion of development samples, factors that increase the risk of overfitting. Knowledge grounding with a static external source becomes impractical in view of the rapid pace of events throughout the COVID19 pandemic: a claim would need to be contrasted against a distinct version of the knowledge base depending on when it was expressed, inserting significant overhead and necessitating an additional timestamp input feature.4 In light of the above, we turn our attention to pretrained BERT-like models (Devlin et al., 2019). BERT-like models are the workhorses in NLP, boasting a high capacity for semantic understanding while acting as implicit rudimentary knowledge bases, owing to their utilization of massive amounts of unlabeled data (Petroni et al., 2019; Rogers et al., 2020). Among the many candidate models, the ones confined within the twitter domain make for the most natural choices. Language use in twitter messages differs from the norm, in terms of style, length, and content. A twitter-specific model should then already be accustomed to the particularities of the domain, relieving us from either having to"
2021.nlp4if-1.18,2021.ccl-1.108,0,0.0663488,"Missing"
2021.nlp4if-1.18,2020.emnlp-demos.2,0,0.0163545,"wing criteria: (i) models that have been pre-trained on the language domain (i.e, Twitter); (ii) models that have been pre-trained on data related to the COVID-19 pandemic; and (iii) models that have been pre-trained or fine tuned for high-level tasks (e.g., irony and hate speech detection) expressed by any of the target questions. In this way, we identified and used six variations of three pre-trained models, detailed in the following paragraphs. 4 This is especially relevant in the task’s context, where the training/development and test data are temporally offset by about a year. BERT WEET (Nguyen et al., 2020) is a RoBERTabase model (Liu et al., 2019) trained from scratch on 850M tweets. It is a strong baseline that, fine tuned, achieves state-of-the-art benchmarks on the SemEval 2017 sentiment analysis and the SemEval 2018 irony detection shared tasks (Rosenthal et al., 2017; Van Hee et al., 2018). Here, we use a variant of the model, additionally trained on 23M tweets related to the COVID-19 pandemic, collected prior to September 2020. CT-BERT (M¨uller et al., 2020) is a pre-trained BERTlarge model, adapted for use in the twitter setting and specifically the COVID-19 theme by continued unsupervis"
2021.nlp4if-1.18,D19-1250,0,0.0608314,"Missing"
2021.nlp4if-1.18,D19-5005,0,0.040786,"Missing"
2021.nlp4if-1.18,2020.tacl-1.54,0,0.0235057,"rapid pace of events throughout the COVID19 pandemic: a claim would need to be contrasted against a distinct version of the knowledge base depending on when it was expressed, inserting significant overhead and necessitating an additional timestamp input feature.4 In light of the above, we turn our attention to pretrained BERT-like models (Devlin et al., 2019). BERT-like models are the workhorses in NLP, boasting a high capacity for semantic understanding while acting as implicit rudimentary knowledge bases, owing to their utilization of massive amounts of unlabeled data (Petroni et al., 2019; Rogers et al., 2020). Among the many candidate models, the ones confined within the twitter domain make for the most natural choices. Language use in twitter messages differs from the norm, in terms of style, length, and content. A twitter-specific model should then already be accustomed to the particularities of the domain, relieving us from either having to account for domain adaptation, or relying on external data. We obtain our final set of models by filtering our selection in accordance with a refinement of the tasks, as expressed by the questions of the annotation schemes, and the domain. In particular, we"
2021.nlp4if-1.18,S17-2088,0,0.0663897,"Missing"
2021.nlp4if-1.18,2021.nlp4if-1.12,0,0.0387715,"isinformation is such that manual intervention and curation is not feasible, calling for the development of automatic solutions grounded on Natural Language Processing. The proposed shared task on COVID-19 misinformation presents innovative aspects mirroring the complexity and variation of phenomena that accompanies the spread of misinformation about COVID-19, including fake news, rumors, conspiracy theories, racism, xenophobia and mistrust of science, among others. To embrace the variation of the phenomena, the task organizers have developed a rich annotation scheme based on seven questions (Shaar et al., 2021). Participants are asked to design a system capable of automatically labeling a set of messages from Twitter with a binary value (i.e., yes/no) for each of the seven questions. Train and test data are available in three languages, namely English, Arabic, and Bulgarian. Our team, TOKOFOU , submitted predictions only for the English data by developing an ensemble model based on a combination of different transformer-based pre-trained language encoders. Each pre-trained model has been selected to match the language va1 2 https://bit.ly/3uGjwEr https://bit.ly/3wPqsBg 119 Proceedings of the 4th NLP"
2021.nlp4if-1.18,S18-1005,0,0.0555285,"Missing"
2021.nlp4posimpact-1.4,2020.acl-main.463,0,0.0147923,"community (e.g., developing an API to report bugs might not be appropriate in areas 7 Text is a means rather than an end Introducing PD methods in the design of NLP tools promotes and embraces a philosophical perspective on the interactions between humans and machines, and of Artificial Intelligence in general, as a problem-solving tool rather than as an adaptive mechanism mimicking human abilities (Winograd, 1997; Auernhammer, 2020). On the contrary, current trends in NLP are more oriented towards a rationalist perspective, attempting to develop intelligent systems that understand language (Bender and Koller, 2020). This follows a logic of automation that attempts to ultimately remove human intervention (Crawford, 2021), reinforcing a vision of language as data. Language, however, is not a uniform entity but it adapts to the context where it is used. NLP systems have the potential to support the flow of meanings between contexts but in order to do so, and act as means rather than ends (Auger et al., 2017; Hanna et al., 2017), they must contend with the structural solidity of the categories on which its 3 https://share.hek.ch/en/ participatory-ai-how-to-make-better-ai/ 30 9 algorithms are built (Bender e"
2021.nlp4posimpact-1.4,2021.eacl-main.186,1,0.74118,"lect and requires English translations, GPT-3 produces output with prejudices and negative stereotypes against the community. 4 5 The training of current SOTA language models (LMs) is based on large amounts of written text crawled from the Web, with no or little documentation (Bender et al., 2021). However, the attempt to calibrate a tool to the needs of a specific community demands concrete social interactions. This requires the development of ethical engagement practices based on respect, equity, and reciprocity to gain the trust of the gatekeepers of the community (Le Dantec and Fox, 2015; Hirmer et al., 2021; Bird, 2020). Gaining trust of communities is fundamental, especially when dealing with small groups of people. In that case, all information is sensitive and often considered a currency that can be devalued once made public (Giglitto, 2017). Innovative, flexible and transparent approaches to data collection and annotation should be put in practice. In line with PD methods, the way this cannot be reduced to a check-list valid for each and every community: context-specificity, which affects participation practices, cannot be avoided (Sloan et al., 2020). Documenting, describing, explaining, an"
2021.nlp4posimpact-1.4,W17-5401,0,0.049949,"Missing"
2021.nlp4posimpact-1.4,W19-6001,0,0.104053,"Missing"
2021.nlp4posimpact-1.4,P16-2096,0,0.0651075,"Missing"
2021.nlp4posimpact-1.4,Y08-1039,0,0.0349325,"ssed by and used to create an NLP Data and communities are not separate things As we saw in the first three points, communities represent the core element of PD. One might expect that communities have a prominent role in the development of NLP systems. Indeed, communities are the producers of the oil that runs NLP research: language data. We observe, however, that this is not the case. Searching for the term “community” in the ACL Anthology2 returns 100 papers. However, by manually inspecting each of them, we discovered that only 9 present some sort of engagement with a community of speakers (Garcia et al., 2008; Levin, 2009; Bird et al., 2014; Everson et al., 2019; Kempton, 2017; Susarla and Challa, 2019; Conforti et al., 2 Community involvement is not scraping Accessed on April 30th, 2021 29 system is an essential step. It is up to the NLP researchers to gain trust by describing as best as they can the purpose of the work and the risks and benefits for the community. Additional advantages of designing NLP systems around the needs of a community are the possibilities of challenging existing power dynamics and also reduce risks of dual use. In this context, initiatives such as the Feminist.AI 3 colle"
2021.nlp4posimpact-1.4,W17-0122,0,0.0283832,"ngs As we saw in the first three points, communities represent the core element of PD. One might expect that communities have a prominent role in the development of NLP systems. Indeed, communities are the producers of the oil that runs NLP research: language data. We observe, however, that this is not the case. Searching for the term “community” in the ACL Anthology2 returns 100 papers. However, by manually inspecting each of them, we discovered that only 9 present some sort of engagement with a community of speakers (Garcia et al., 2008; Levin, 2009; Bird et al., 2014; Everson et al., 2019; Kempton, 2017; Susarla and Challa, 2019; Conforti et al., 2 Community involvement is not scraping Accessed on April 30th, 2021 29 system is an essential step. It is up to the NLP researchers to gain trust by describing as best as they can the purpose of the work and the risks and benefits for the community. Additional advantages of designing NLP systems around the needs of a community are the possibilities of challenging existing power dynamics and also reduce risks of dual use. In this context, initiatives such as the Feminist.AI 3 collective and Indigenous data sovereignty practices (Kukutai and Taylor,"
2021.nlp4posimpact-1.4,2020.lrec-1.850,0,0.0133437,"be good samples of language phenomena, but are actually deeply contextbound at different levels (e.g., time period, medium, population sample, among others). It is known that NLP tools struggle with tail phenomena (Ettinger et al., 2017) and are subject to bias (Bender and Friedman, 2018). Solutions are varied and focused on areas such as Domain Adaptation and Transfer Learning (Blitzer et al., 2006; Daum´e III, 2007; Ma et al., 2014; Ganin and Lempitsky, 2015; Wu and Huang, 2016; Ruder et al., 2017; Ruder and Plank, 2017; Ramponi and Plank, 2020) or de-biasing (Gonen and Goldberg, 2019; Paul Panenghat et al., 2020; Liang et al., 2020; Zhou et al., 2021). A PD-aware NLP tool should foresee this community adaptation feature at its design stage. This requires to overcome technical (i.e., access or manipulation of the code) and resource (financial and human) predicaments as well as the use of predatory practices of users’ involvement (i.e., recognize participation as labor). Having access to continuous and updated feedback from a community is paramount for ensuring that tool adaptation effectively addresses their evolving needs. In this context, researchers should put in place appropriate socio-technical s"
2021.nlp4posimpact-1.4,2020.coling-main.603,0,0.0402391,"lied to broader contexts (Sloan et al., 2020). Datasets are assumed to be good samples of language phenomena, but are actually deeply contextbound at different levels (e.g., time period, medium, population sample, among others). It is known that NLP tools struggle with tail phenomena (Ettinger et al., 2017) and are subject to bias (Bender and Friedman, 2018). Solutions are varied and focused on areas such as Domain Adaptation and Transfer Learning (Blitzer et al., 2006; Daum´e III, 2007; Ma et al., 2014; Ganin and Lempitsky, 2015; Wu and Huang, 2016; Ruder et al., 2017; Ruder and Plank, 2017; Ramponi and Plank, 2020) or de-biasing (Gonen and Goldberg, 2019; Paul Panenghat et al., 2020; Liang et al., 2020; Zhou et al., 2021). A PD-aware NLP tool should foresee this community adaptation feature at its design stage. This requires to overcome technical (i.e., access or manipulation of the code) and resource (financial and human) predicaments as well as the use of predatory practices of users’ involvement (i.e., recognize participation as labor). Having access to continuous and updated feedback from a community is paramount for ensuring that tool adaptation effectively addresses their evolving needs. In this c"
2021.nlp4posimpact-1.4,2020.coling-main.303,0,0.0646393,"Missing"
2021.nlp4posimpact-1.4,2009.eamt-1.2,0,0.0799771,"reate an NLP Data and communities are not separate things As we saw in the first three points, communities represent the core element of PD. One might expect that communities have a prominent role in the development of NLP systems. Indeed, communities are the producers of the oil that runs NLP research: language data. We observe, however, that this is not the case. Searching for the term “community” in the ACL Anthology2 returns 100 papers. However, by manually inspecting each of them, we discovered that only 9 present some sort of engagement with a community of speakers (Garcia et al., 2008; Levin, 2009; Bird et al., 2014; Everson et al., 2019; Kempton, 2017; Susarla and Challa, 2019; Conforti et al., 2 Community involvement is not scraping Accessed on April 30th, 2021 29 system is an essential step. It is up to the NLP researchers to gain trust by describing as best as they can the purpose of the work and the risks and benefits for the community. Additional advantages of designing NLP systems around the needs of a community are the possibilities of challenging existing power dynamics and also reduce risks of dual use. In this context, initiatives such as the Feminist.AI 3 collective and Ind"
2021.nlp4posimpact-1.4,D17-1038,0,0.0117036,"can be learned and applied to broader contexts (Sloan et al., 2020). Datasets are assumed to be good samples of language phenomena, but are actually deeply contextbound at different levels (e.g., time period, medium, population sample, among others). It is known that NLP tools struggle with tail phenomena (Ettinger et al., 2017) and are subject to bias (Bender and Friedman, 2018). Solutions are varied and focused on areas such as Domain Adaptation and Transfer Learning (Blitzer et al., 2006; Daum´e III, 2007; Ma et al., 2014; Ganin and Lempitsky, 2015; Wu and Huang, 2016; Ruder et al., 2017; Ruder and Plank, 2017; Ramponi and Plank, 2020) or de-biasing (Gonen and Goldberg, 2019; Paul Panenghat et al., 2020; Liang et al., 2020; Zhou et al., 2021). A PD-aware NLP tool should foresee this community adaptation feature at its design stage. This requires to overcome technical (i.e., access or manipulation of the code) and resource (financial and human) predicaments as well as the use of predatory practices of users’ involvement (i.e., recognize participation as labor). Having access to continuous and updated feedback from a community is paramount for ensuring that tool adaptation effectively addresses their"
2021.nlp4posimpact-1.4,W19-7506,0,0.0107978,"n the first three points, communities represent the core element of PD. One might expect that communities have a prominent role in the development of NLP systems. Indeed, communities are the producers of the oil that runs NLP research: language data. We observe, however, that this is not the case. Searching for the term “community” in the ACL Anthology2 returns 100 papers. However, by manually inspecting each of them, we discovered that only 9 present some sort of engagement with a community of speakers (Garcia et al., 2008; Levin, 2009; Bird et al., 2014; Everson et al., 2019; Kempton, 2017; Susarla and Challa, 2019; Conforti et al., 2 Community involvement is not scraping Accessed on April 30th, 2021 29 system is an essential step. It is up to the NLP researchers to gain trust by describing as best as they can the purpose of the work and the risks and benefits for the community. Additional advantages of designing NLP systems around the needs of a community are the possibilities of challenging existing power dynamics and also reduce risks of dual use. In this context, initiatives such as the Feminist.AI 3 collective and Indigenous data sovereignty practices (Kukutai and Taylor, 2016; Walter and Suina, 20"
2021.nlp4posimpact-1.4,P14-1014,0,0.0178342,"hniques for developing NLP systems adopts a vision where statistical generalizations can be learned and applied to broader contexts (Sloan et al., 2020). Datasets are assumed to be good samples of language phenomena, but are actually deeply contextbound at different levels (e.g., time period, medium, population sample, among others). It is known that NLP tools struggle with tail phenomena (Ettinger et al., 2017) and are subject to bias (Bender and Friedman, 2018). Solutions are varied and focused on areas such as Domain Adaptation and Transfer Learning (Blitzer et al., 2006; Daum´e III, 2007; Ma et al., 2014; Ganin and Lempitsky, 2015; Wu and Huang, 2016; Ruder et al., 2017; Ruder and Plank, 2017; Ramponi and Plank, 2020) or de-biasing (Gonen and Goldberg, 2019; Paul Panenghat et al., 2020; Liang et al., 2020; Zhou et al., 2021). A PD-aware NLP tool should foresee this community adaptation feature at its design stage. This requires to overcome technical (i.e., access or manipulation of the code) and resource (financial and human) predicaments as well as the use of predatory practices of users’ involvement (i.e., recognize participation as labor). Having access to continuous and updated feedback f"
2021.nlp4posimpact-1.4,P16-1029,0,0.0117607,"vision where statistical generalizations can be learned and applied to broader contexts (Sloan et al., 2020). Datasets are assumed to be good samples of language phenomena, but are actually deeply contextbound at different levels (e.g., time period, medium, population sample, among others). It is known that NLP tools struggle with tail phenomena (Ettinger et al., 2017) and are subject to bias (Bender and Friedman, 2018). Solutions are varied and focused on areas such as Domain Adaptation and Transfer Learning (Blitzer et al., 2006; Daum´e III, 2007; Ma et al., 2014; Ganin and Lempitsky, 2015; Wu and Huang, 2016; Ruder et al., 2017; Ruder and Plank, 2017; Ramponi and Plank, 2020) or de-biasing (Gonen and Goldberg, 2019; Paul Panenghat et al., 2020; Liang et al., 2020; Zhou et al., 2021). A PD-aware NLP tool should foresee this community adaptation feature at its design stage. This requires to overcome technical (i.e., access or manipulation of the code) and resource (financial and human) predicaments as well as the use of predatory practices of users’ involvement (i.e., recognize participation as labor). Having access to continuous and updated feedback from a community is paramount for ensuring that"
2021.nlp4posimpact-1.4,2021.eacl-main.274,0,0.0262684,"re actually deeply contextbound at different levels (e.g., time period, medium, population sample, among others). It is known that NLP tools struggle with tail phenomena (Ettinger et al., 2017) and are subject to bias (Bender and Friedman, 2018). Solutions are varied and focused on areas such as Domain Adaptation and Transfer Learning (Blitzer et al., 2006; Daum´e III, 2007; Ma et al., 2014; Ganin and Lempitsky, 2015; Wu and Huang, 2016; Ruder et al., 2017; Ruder and Plank, 2017; Ramponi and Plank, 2020) or de-biasing (Gonen and Goldberg, 2019; Paul Panenghat et al., 2020; Liang et al., 2020; Zhou et al., 2021). A PD-aware NLP tool should foresee this community adaptation feature at its design stage. This requires to overcome technical (i.e., access or manipulation of the code) and resource (financial and human) predicaments as well as the use of predatory practices of users’ involvement (i.e., recognize participation as labor). Having access to continuous and updated feedback from a community is paramount for ensuring that tool adaptation effectively addresses their evolving needs. In this context, researchers should put in place appropriate socio-technical solutions considering the peculiarities o"
2021.woah-1.3,S19-2007,1,0.855659,"eats, and doxxing (Jurgens et al., 2019). Current approaches have focus on a limited range, namely offensive language, abusive language, and hate speech. The connections among these phenomena have only superficially been accounted for, resulting in a fragmented picture, with a variety of definitions, and (in)compatible annotations (Waseem et al., 2017). Poletto et al. (2020) introduce a graphical visualisation (Figure 1) of the connections among abusive language phenomena according to the definitions in previous work (Waseem and Hovy, 2016; Fortuna and Nunes, 2018; Malmasi and Zampieri, 2018; Basile et al., 2019; Zampieri et al., 2019). When it comes to offensive language, abusive language, and hate speech, the distinguishing factor is their level of specificity. This makes offensive language 17 Proceedings of the Fifth Workshop on Online Abuse and Harms, pages 17–25 August 6, 2021. ©2021 Association for Computational Linguistics the most generic form of abusive language phenomena and hate speech the most specific, with abusive language being somewhere in the middle. Such differences are a major issue for the study of portability of models. Previous work (Karan ˇ and Snajder, 2018; Benk, 2019; Pamung"
2021.woah-1.3,2020.lrec-1.760,1,0.892041,"d [someone] with any of the following tokens: “you”, “she”, “he”, “women”, “men” Although not exhaustive, HateBERT consistently present profanities or abusive terms as mask fillers, while this very rarely occurs with the generic BERT. Table 1 illustrates the results for “women”. BERT guage annotation to OffensEval 2019. Abusive language is defined as a specific case of offensive language, namely “hurtful language that a speaker uses to insult or offend another individual or a group of individuals based on their personal qualities, appearance, social status, opinions, statements, or actions.” (Caselli et al., 2020, pg. 6197). The main difference with respect to offensive language is the exclusion of isolated profanities or untargeted messages from the positive class. The size of the dataset is the same as OffensEval 2019.The differences concern the distribution of the positive class which results in 2,749 in training and 178 in test. HatEval (Basile et al., 2019) The English portion of the dataset contains 13,000 tweets annotated for hate speech against migrants and women. The authors define hate speech as “any communication that disparages a person or a group on the basis of some characteristic such a"
2021.woah-1.3,S19-2011,0,0.0357223,"s et al., 2020). We introduce HateBERT, a pre-trained BERT model for abusive language phenomena in social media in English. Introduction The development of systems for the automatic identification of abusive language phenomena has followed a common trend in NLP: feature-based linear classifiers (Waseem and Hovy, 2016; Ribeiro et al., 2018; Ibrohim and Budi, 2019), neural network architectures (e.g., CNN or Bi-LSTM) (Kshirsagar et al., 2018; Mishra et al., 2018; Mitrovi´c et al., 2019; Sigurbergsson and Derczynski, 2020), and fine-tuning pre-trained language models, e.g., BERT, RoBERTa, a.o., (Liu et al., 2019; Swamy et al., 2019). Results vary both across datasets and architectures, with linear classifiers qualifying as very competitive, if not better, when compared to neural networks. On the other hand, systems based on pre-trained language models have reached new state-of-the-art results. One issue with these pretrained models is that the training language variety makes them well suited for general-purpose language understanding tasks, and it highlights their limits with more domain-specific language varieties. To address this, there is a growing interAbusive language phenomena fall along a wide"
2021.woah-1.3,P19-1163,0,0.0259822,"re to harmful content in social media; (ii.) contributing to the creation of healthier online interactions; and (iii.) promoting positive contagious behaviors and interactions (Matias, 2019). Unfortunately, work in this area is not free from potentially negative impacts. The most direct is a risk of promoting misrepresentation. HateBERT is an intrinsically biased pre-trained language model. The fine-tuned models that can be obtained are not overgenerating the positive classes, but they suffer from the biases in the manually annotated data, especially for the offensive language detection task (Sap et al., 2019; Acknowledgements The project on which this report is based was funded by the German Federal Ministry of Education and Research (BMBF) under the funding code 01—S20049. The author is responsible for the content of this publication. Ethical Statement In this paper, the authors introduce HateBERT, a pre-trained language model for the study of abusive language phenomena in social media in English. HateBERT is unique because (i.) it is based on further pre-training of an existing pre-trained language model (i.e., BERT base-uncased) rather than training it from scratch, thus reducing the environme"
2021.woah-1.3,K19-1088,0,0.184491,"Missing"
2021.woah-1.3,W17-3012,0,0.0124076,"anding tasks, and it highlights their limits with more domain-specific language varieties. To address this, there is a growing interAbusive language phenomena fall along a wide spectrum including, a.o., microaggression, stereotyping, offense, abuse, hate speech, threats, and doxxing (Jurgens et al., 2019). Current approaches have focus on a limited range, namely offensive language, abusive language, and hate speech. The connections among these phenomena have only superficially been accounted for, resulting in a fragmented picture, with a variety of definitions, and (in)compatible annotations (Waseem et al., 2017). Poletto et al. (2020) introduce a graphical visualisation (Figure 1) of the connections among abusive language phenomena according to the definitions in previous work (Waseem and Hovy, 2016; Fortuna and Nunes, 2018; Malmasi and Zampieri, 2018; Basile et al., 2019; Zampieri et al., 2019). When it comes to offensive language, abusive language, and hate speech, the distinguishing factor is their level of specificity. This makes offensive language 17 Proceedings of the Fifth Workshop on Online Abuse and Harms, pages 17–25 August 6, 2021. ©2021 Association for Computational Linguistics the most g"
2021.woah-1.3,N16-2013,0,0.478133,"etrained language models, such as AlBERTo (Polignano et al., 2019) or TweetEval (Barbieri et al., 2020) for Twitter, BioBERT for the biomedical domain in English (Lee et al., 2019), FinBERT for the financial domain in English (Yang et al., 2020), and LEGAL-BERT for the legal domain in English (Chalkidis et al., 2020). We introduce HateBERT, a pre-trained BERT model for abusive language phenomena in social media in English. Introduction The development of systems for the automatic identification of abusive language phenomena has followed a common trend in NLP: feature-based linear classifiers (Waseem and Hovy, 2016; Ribeiro et al., 2018; Ibrohim and Budi, 2019), neural network architectures (e.g., CNN or Bi-LSTM) (Kshirsagar et al., 2018; Mishra et al., 2018; Mitrovi´c et al., 2019; Sigurbergsson and Derczynski, 2020), and fine-tuning pre-trained language models, e.g., BERT, RoBERTa, a.o., (Liu et al., 2019; Swamy et al., 2019). Results vary both across datasets and architectures, with linear classifiers qualifying as very competitive, if not better, when compared to neural networks. On the other hand, systems based on pre-trained language models have reached new state-of-the-art results. One issue with"
2021.woah-1.3,N18-1095,0,0.0368395,"Missing"
2021.woah-1.3,S19-2010,0,0.156008,"rgens et al., 2019). Current approaches have focus on a limited range, namely offensive language, abusive language, and hate speech. The connections among these phenomena have only superficially been accounted for, resulting in a fragmented picture, with a variety of definitions, and (in)compatible annotations (Waseem et al., 2017). Poletto et al. (2020) introduce a graphical visualisation (Figure 1) of the connections among abusive language phenomena according to the definitions in previous work (Waseem and Hovy, 2016; Fortuna and Nunes, 2018; Malmasi and Zampieri, 2018; Basile et al., 2019; Zampieri et al., 2019). When it comes to offensive language, abusive language, and hate speech, the distinguishing factor is their level of specificity. This makes offensive language 17 Proceedings of the Fifth Workshop on Online Abuse and Harms, pages 17–25 August 6, 2021. ©2021 Association for Computational Linguistics the most generic form of abusive language phenomena and hate speech the most specific, with abusive language being somewhere in the middle. Such differences are a major issue for the study of portability of models. Previous work (Karan ˇ and Snajder, 2018; Benk, 2019; Pamungkas and Patti, 2019; Riz"
2021.woah-1.6,S19-2007,0,0.0931745,"y create denser datasets, but at the same time risks of developing biased data are very high (Wiegand et al., 2019). Furthermore, according to the specific platform used, some of the methods cannot be reliably applied. For instance, in a platform like Twitter targeting online communities is not trivial. Recently, refinements have been proposed to address limitations of each approach. In some cases controversial posts, videos or keywords are used as proxies for communities (Hammer, 2016; Graumans et al., 2019), in other cases hybrid approaches are proposed by combining keywords and seed users (Basile et al., 2019), others exploit platform pre-filtering functionalities (Zampieri et al., 2019a). DALC v1.0 integrates different bottom-up approaches to collect data providing a first crossfertlization attempt across two social media platforms and paying attention to minimize the introduction of biases. Vidgen and Derczynski (2021) provides a comprehensive survey covering 63 datasets all targeting a specific abusive phenomenon/behavior. The majority of them (25 datasets) is for English, with a long tail of other languages mostly belonging to the Indo-European family, although limited in their diversity. The l"
2021.woah-1.6,2020.lrec-1.760,1,0.813205,"Missing"
2021.woah-1.6,P19-1271,0,0.0554259,"Missing"
2021.woah-1.6,2021.eacl-main.114,0,0.0185085,"ons exist and they mainly concentrate along three dimensions: (i) definitions; (ii) data sources and collection methods; and (iii) language diversity. The development of automatic methods for detecting forms of abusive language has been rapid and has seen a boom of definitions, labels, and phenomena being investigated, including racism (Waseem and Hovy, 2016a; Davidson et al., 2017, 2019), hate speech (Alfina et al., 2017; Founta et al., 2018; Mishra et al., 2018; Basile et al., 2019), toxicity3 and verbal aggression (Kumar et al., 2018), misogyny (Frenda et al., 2018; Pamungkas et al., 2020; Guest et al., 2021), and offensive language (Wiegand et al., 2018; Zampieri et al., 2019a; Rosenthal et al., 2020). Variations in definitions and in annotation guidelines have given rise to isolated datasets, limiting the portability of trained systems and reuse of resources (Swamy et al., 2019; Fortuna et al., 2021). Comprehensive frameworks that integrate and harmonize the variety of definitions and investigate the interactions across the annotated phenomena are still at early stages (Poletto et al., 2020). DALC v1.0 is compatible with existing definitions of abusive language and promotes a multi-layered annot"
2021.woah-1.6,2020.restup-1.4,1,0.724282,"Missing"
2021.woah-1.6,W18-4401,0,0.0281548,"Missing"
2021.woah-1.6,L18-1443,0,0.021205,"focuses on messages from social media platforms, with Twitter being the most used Vidgen and Derczynski (2021). Unlike other language phenomena, e.g., named entities, abusive language is less widespread and cannot be easily captured by means of random sampling. Schematically, we identify three major methods to collect data: namely: (i) use of communities (Tulkens et al., 2016; Del Vigna et al., 2017; Merenda et al., 2018; Kennedy et al., 2018) which targets online communities known to be more likely to have abusive behaviors; (ii) use of keywords (Waseem and Hovy, 2016b; Alfina et al., 2017; Sanguinetti et al., 2018; ElSherief et al., 2018; Founta et al., 2018), where manually compiled lists of words corresponding either to potential targets (e.g, “women”, “migrants”, a.o.) or profanities are employed; (iii) use of seed 3 Data Collection DALC v1.0 is based on a sample of a large ongoing collection of Twitter messages in Dutch at the University of Groningen (Tjong Kim Sang, 2011). For its construction, rather than focusing individually on any of the mentioned approaches, 3 The Toxic Comment Clas- sification Challenge https: //bit.ly/2QuHKD6 55 2018; Gerstenfeld, 2017).4 We use data from the Dutch Centraal"
2021.woah-1.6,P19-1163,0,0.0596307,"Missing"
2021.woah-1.6,C18-1093,0,0.135299,"lies on language-specific resources to train tools to distinguish the “good” messages from the harmful ones. As a contribution in this direction, we have developed the Dutch Abusive Language Corpus, or DALC v1.0, a manually annotated corpus of tweets for abusive language detection in Dutch.2 The resource is unique in the Dutch-speaking panorama because of the approach used to collect the data, the annotation guidelines, and the final data curation. DALC is compatible with previous work on abusive language in other languages (Waseem and Hovy, 2016a; Papegnies et al., 2017; Founta et al., 2018; Mishra et al., 2018; Davidson et al., 2019; Poletto et al., 2020) but presents innovations both with respect to the application of the label “abusive” to messages and the adoption of a multi-layered annotation to distinguish the explicitness of the abusive message and its target (Waseem et al., 2017). Our contributions can be summarized as follows: As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually annotated for a"
2021.woah-1.6,K19-1088,0,0.0314753,"Missing"
2021.woah-1.6,W17-3012,0,0.107455,"in Dutch.2 The resource is unique in the Dutch-speaking panorama because of the approach used to collect the data, the annotation guidelines, and the final data curation. DALC is compatible with previous work on abusive language in other languages (Waseem and Hovy, 2016a; Papegnies et al., 2017; Founta et al., 2018; Mishra et al., 2018; Davidson et al., 2019; Poletto et al., 2020) but presents innovations both with respect to the application of the label “abusive” to messages and the adoption of a multi-layered annotation to distinguish the explicitness of the abusive message and its target (Waseem et al., 2017). Our contributions can be summarized as follows: As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually annotated for abusive language. The resource address a gap in language resources for Dutch and adopts a multi-layer annotation scheme modeling the explicitness and the target of the abusive messages. Baselines experiments on all annotation layers have been conducted, achieving a macro F1 score of"
2021.woah-1.6,N16-2013,0,0.126696,"nt.rug.nl gerbentimmerman@protonmail.com Abstract guage. Such step relies on language-specific resources to train tools to distinguish the “good” messages from the harmful ones. As a contribution in this direction, we have developed the Dutch Abusive Language Corpus, or DALC v1.0, a manually annotated corpus of tweets for abusive language detection in Dutch.2 The resource is unique in the Dutch-speaking panorama because of the approach used to collect the data, the annotation guidelines, and the final data curation. DALC is compatible with previous work on abusive language in other languages (Waseem and Hovy, 2016a; Papegnies et al., 2017; Founta et al., 2018; Mishra et al., 2018; Davidson et al., 2019; Poletto et al., 2020) but presents innovations both with respect to the application of the label “abusive” to messages and the adoption of a multi-layered annotation to distinguish the explicitness of the abusive message and its target (Waseem et al., 2017). Our contributions can be summarized as follows: As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Cor"
2021.woah-1.6,N19-1060,0,0.0588333,"available at https://github.com/ tommasoc80/DALC 1 https://what-europe-does-for-me.eu/ en/portal/2/H19 54 Proceedings of the Fifth Workshop on Online Abuse and Harms, pages 54–66 August 6, 2021. ©2021 Association for Computational Linguistics 2 Related Work users (Wiegand et al., 2018; Ribeiro et al., 2018), which collects messages from users that have been identified to post abusive texts via some heuristics. Each of these methods has advantages and disadvantages. For instance, the use of keywords may create denser datasets, but at the same time risks of developing biased data are very high (Wiegand et al., 2019). Furthermore, according to the specific platform used, some of the methods cannot be reliably applied. For instance, in a platform like Twitter targeting online communities is not trivial. Recently, refinements have been proposed to address limitations of each approach. In some cases controversial posts, videos or keywords are used as proxies for communities (Hammer, 2016; Graumans et al., 2019), in other cases hybrid approaches are proposed by combining keywords and seed users (Basile et al., 2019), others exploit platform pre-filtering functionalities (Zampieri et al., 2019a). DALC v1.0 int"
2021.woah-1.6,N18-1095,0,0.111781,"ated corpus for abusive language detection in Dutch, DALC v1.0; • a series of baseline experiments using different architectures (i.e., a dictionary based approach, a Linear SVM, a Dutch transformerbased language model) showing the complexity of the task. 2 The corpus, the annotation guidelines, and the baselines models are publicly available at https://github.com/ tommasoc80/DALC 1 https://what-europe-does-for-me.eu/ en/portal/2/H19 54 Proceedings of the Fifth Workshop on Online Abuse and Harms, pages 54–66 August 6, 2021. ©2021 Association for Computational Linguistics 2 Related Work users (Wiegand et al., 2018; Ribeiro et al., 2018), which collects messages from users that have been identified to post abusive texts via some heuristics. Each of these methods has advantages and disadvantages. For instance, the use of keywords may create denser datasets, but at the same time risks of developing biased data are very high (Wiegand et al., 2019). Furthermore, according to the specific platform used, some of the methods cannot be reliably applied. For instance, in a platform like Twitter targeting online communities is not trivial. Recently, refinements have been proposed to address limitations of each ap"
2021.woah-1.6,N19-1144,0,0.192267,"a are very high (Wiegand et al., 2019). Furthermore, according to the specific platform used, some of the methods cannot be reliably applied. For instance, in a platform like Twitter targeting online communities is not trivial. Recently, refinements have been proposed to address limitations of each approach. In some cases controversial posts, videos or keywords are used as proxies for communities (Hammer, 2016; Graumans et al., 2019), in other cases hybrid approaches are proposed by combining keywords and seed users (Basile et al., 2019), others exploit platform pre-filtering functionalities (Zampieri et al., 2019a). DALC v1.0 integrates different bottom-up approaches to collect data providing a first crossfertlization attempt across two social media platforms and paying attention to minimize the introduction of biases. Vidgen and Derczynski (2021) provides a comprehensive survey covering 63 datasets all targeting a specific abusive phenomenon/behavior. The majority of them (25 datasets) is for English, with a long tail of other languages mostly belonging to the Indo-European family, although limited in their diversity. The lack of publicly available datasets for any Sino-Tibetan, Niger-Congo, or Afro-"
2021.woah-1.6,S19-2010,0,0.159841,"a are very high (Wiegand et al., 2019). Furthermore, according to the specific platform used, some of the methods cannot be reliably applied. For instance, in a platform like Twitter targeting online communities is not trivial. Recently, refinements have been proposed to address limitations of each approach. In some cases controversial posts, videos or keywords are used as proxies for communities (Hammer, 2016; Graumans et al., 2019), in other cases hybrid approaches are proposed by combining keywords and seed users (Basile et al., 2019), others exploit platform pre-filtering functionalities (Zampieri et al., 2019a). DALC v1.0 integrates different bottom-up approaches to collect data providing a first crossfertlization attempt across two social media platforms and paying attention to minimize the introduction of biases. Vidgen and Derczynski (2021) provides a comprehensive survey covering 63 datasets all targeting a specific abusive phenomenon/behavior. The majority of them (25 datasets) is for English, with a long tail of other languages mostly belonging to the Indo-European family, although limited in their diversity. The lack of publicly available datasets for any Sino-Tibetan, Niger-Congo, or Afro-"
C12-2121,W10-0731,0,0.0171921,"much larger than the sampling allowable by traditional experiments, interesting and hitherto unobserved distributional properties of human behaviors may emerge. In addition to this, for Language Technology, the additional motivation is that a web-based crowd can provide data for the construction of large-scale LRs in a faster, cheaper and still reliable way. So far, annotation works conducted by means of crowsourcing techniques have focused on rather simple linguistic tasks, such as the evaluation of automatic translations (Callison-Burch, 2009), word sense disambiguation (Snow et al., 2008; Akkaya et al., 2010, Rumshinsky, 2011), textual entailment (Snow et.al., 2008; Wang and Callison-Burch, 2010), commonsense knowledge (Gordon et al., 2010), text alignment for machine translations (Ambati and Vogel, 2010) and speech transcriptions (Callison-Burch and Dredze, 2010) among others. Such choices are in line with the idea of using the “wisdom of the crowd” as the tasks can be simplified and presented to the workers as a sort of online game such that a large percentage of the population can be expected to perform the task reliably. In this work we explore the untapped strength of crowdsourcing when the"
C12-2121,D09-1020,0,0.0387399,"Missing"
C12-2121,W10-0710,0,0.0125418,"echnology, the additional motivation is that a web-based crowd can provide data for the construction of large-scale LRs in a faster, cheaper and still reliable way. So far, annotation works conducted by means of crowsourcing techniques have focused on rather simple linguistic tasks, such as the evaluation of automatic translations (Callison-Burch, 2009), word sense disambiguation (Snow et al., 2008; Akkaya et al., 2010, Rumshinsky, 2011), textual entailment (Snow et.al., 2008; Wang and Callison-Burch, 2010), commonsense knowledge (Gordon et al., 2010), text alignment for machine translations (Ambati and Vogel, 2010) and speech transcriptions (Callison-Burch and Dredze, 2010) among others. Such choices are in line with the idea of using the “wisdom of the crowd” as the tasks can be simplified and presented to the workers as a sort of online game such that a large percentage of the population can be expected to perform the task reliably. In this work we explore the untapped strength of crowdsourcing when the linguistic task is a complex and challenging one, trying to understand “how far can go the crowd?”. As mentioned, the received wisdom is that when the tasks are complex, crowdsourced data may be too no"
C12-2121,A97-1052,0,0.0164667,"ral Language Processing (NLP) systems are based on supervised learning approaches relying on large amounts of manually annotated training data collected by domain experts. Such annotation process is highly expensive both in terms of money and time. However, the absence of manually annotated Language Resources (LRs) makes supervised NLP systems subject to the so-called knowledge acquisition bottleneck. In recent years, in order to facilitate the development of LRs, two different approaches have been tackled. The first aims at automatically acquiring LRs, such as lexica, from large corpus data (Briscoe and Carroll, 1997; Korhonen et al., 2006, among others). The second investigates the exploitation of the Web 2.0 through the use of crowdsourcing techniques, i.e. by using non-expert annotators recruited on the Web. The crucial motivation of crowdsourcing is that when a simple linguistic task is performed by a population much larger than the sampling allowable by traditional experiments, interesting and hitherto unobserved distributional properties of human behaviors may emerge. In addition to this, for Language Technology, the additional motivation is that a web-based crowd can provide data for the constructi"
C12-2121,D09-1030,0,0.0355223,"ng is that when a simple linguistic task is performed by a population much larger than the sampling allowable by traditional experiments, interesting and hitherto unobserved distributional properties of human behaviors may emerge. In addition to this, for Language Technology, the additional motivation is that a web-based crowd can provide data for the construction of large-scale LRs in a faster, cheaper and still reliable way. So far, annotation works conducted by means of crowsourcing techniques have focused on rather simple linguistic tasks, such as the evaluation of automatic translations (Callison-Burch, 2009), word sense disambiguation (Snow et al., 2008; Akkaya et al., 2010, Rumshinsky, 2011), textual entailment (Snow et.al., 2008; Wang and Callison-Burch, 2010), commonsense knowledge (Gordon et al., 2010), text alignment for machine translations (Ambati and Vogel, 2010) and speech transcriptions (Callison-Burch and Dredze, 2010) among others. Such choices are in line with the idea of using the “wisdom of the crowd” as the tasks can be simplified and presented to the workers as a sort of online game such that a large percentage of the population can be expected to perform the task reliably. In th"
C12-2121,W10-0701,0,0.067738,"based crowd can provide data for the construction of large-scale LRs in a faster, cheaper and still reliable way. So far, annotation works conducted by means of crowsourcing techniques have focused on rather simple linguistic tasks, such as the evaluation of automatic translations (Callison-Burch, 2009), word sense disambiguation (Snow et al., 2008; Akkaya et al., 2010, Rumshinsky, 2011), textual entailment (Snow et.al., 2008; Wang and Callison-Burch, 2010), commonsense knowledge (Gordon et al., 2010), text alignment for machine translations (Ambati and Vogel, 2010) and speech transcriptions (Callison-Burch and Dredze, 2010) among others. Such choices are in line with the idea of using the “wisdom of the crowd” as the tasks can be simplified and presented to the workers as a sort of online game such that a large percentage of the population can be expected to perform the task reliably. In this work we explore the untapped strength of crowdsourcing when the linguistic task is a complex and challenging one, trying to understand “how far can go the crowd?”. As mentioned, the received wisdom is that when the tasks are complex, crowdsourced data may be too noisy to use. However, the noise may come in two ways. One pos"
C12-2121,W10-0724,0,0.0425813,"Missing"
C12-2121,C92-4177,0,0.213301,"Missing"
C12-2121,korhonen-etal-2006-large,0,0.0141478,"P) systems are based on supervised learning approaches relying on large amounts of manually annotated training data collected by domain experts. Such annotation process is highly expensive both in terms of money and time. However, the absence of manually annotated Language Resources (LRs) makes supervised NLP systems subject to the so-called knowledge acquisition bottleneck. In recent years, in order to facilitate the development of LRs, two different approaches have been tackled. The first aims at automatically acquiring LRs, such as lexica, from large corpus data (Briscoe and Carroll, 1997; Korhonen et al., 2006, among others). The second investigates the exploitation of the Web 2.0 through the use of crowdsourcing techniques, i.e. by using non-expert annotators recruited on the Web. The crucial motivation of crowdsourcing is that when a simple linguistic task is performed by a population much larger than the sampling allowable by traditional experiments, interesting and hitherto unobserved distributional properties of human behaviors may emerge. In addition to this, for Language Technology, the additional motivation is that a web-based crowd can provide data for the construction of large-scale LRs i"
C12-2121,J91-4003,0,0.0319951,"Missing"
C12-2121,W11-0409,0,0.0261567,"Missing"
C12-2121,D08-1027,0,0.63543,"med by a population much larger than the sampling allowable by traditional experiments, interesting and hitherto unobserved distributional properties of human behaviors may emerge. In addition to this, for Language Technology, the additional motivation is that a web-based crowd can provide data for the construction of large-scale LRs in a faster, cheaper and still reliable way. So far, annotation works conducted by means of crowsourcing techniques have focused on rather simple linguistic tasks, such as the evaluation of automatic translations (Callison-Burch, 2009), word sense disambiguation (Snow et al., 2008; Akkaya et al., 2010, Rumshinsky, 2011), textual entailment (Snow et.al., 2008; Wang and Callison-Burch, 2010), commonsense knowledge (Gordon et al., 2010), text alignment for machine translations (Ambati and Vogel, 2010) and speech transcriptions (Callison-Burch and Dredze, 2010) among others. Such choices are in line with the idea of using the “wisdom of the crowd” as the tasks can be simplified and presented to the workers as a sort of online game such that a large percentage of the population can be expected to perform the task reliably. In this work we explore the untapped strength of cr"
C12-2121,W10-0725,0,0.0153057,"g and hitherto unobserved distributional properties of human behaviors may emerge. In addition to this, for Language Technology, the additional motivation is that a web-based crowd can provide data for the construction of large-scale LRs in a faster, cheaper and still reliable way. So far, annotation works conducted by means of crowsourcing techniques have focused on rather simple linguistic tasks, such as the evaluation of automatic translations (Callison-Burch, 2009), word sense disambiguation (Snow et al., 2008; Akkaya et al., 2010, Rumshinsky, 2011), textual entailment (Snow et.al., 2008; Wang and Callison-Burch, 2010), commonsense knowledge (Gordon et al., 2010), text alignment for machine translations (Ambati and Vogel, 2010) and speech transcriptions (Callison-Burch and Dredze, 2010) among others. Such choices are in line with the idea of using the “wisdom of the crowd” as the tasks can be simplified and presented to the workers as a sort of online game such that a large percentage of the population can be expected to perform the task reliably. In this work we explore the untapped strength of crowdsourcing when the linguistic task is a complex and challenging one, trying to understand “how far can go the"
C12-2121,zarcone-lenci-2008-computational,0,0.0717693,"Missing"
caselli-etal-2008-bilingual,W01-1313,0,\N,Missing
caselli-etal-2008-bilingual,W01-1309,0,\N,Missing
caselli-etal-2012-assigning,W10-0206,0,\N,Missing
caselli-etal-2012-assigning,baroni-etal-2004-introducing,0,\N,Missing
caselli-etal-2012-assigning,baccianella-etal-2010-sentiwordnet,0,\N,Missing
caselli-etal-2012-assigning,H05-1116,0,\N,Missing
caselli-etal-2012-assigning,D09-1020,0,\N,Missing
caselli-etal-2012-assigning,W10-1801,0,\N,Missing
caselli-etal-2012-assigning,strapparava-valitutti-2004-wordnet,0,\N,Missing
caselli-etal-2012-assigning,C10-1021,0,\N,Missing
caselli-etal-2012-assigning,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
caselli-etal-2012-customizable,schulte-im-walde-2002-subcategorisation,0,\N,Missing
caselli-etal-2012-customizable,baroni-etal-2004-introducing,0,\N,Missing
caselli-etal-2012-customizable,N07-1049,0,\N,Missing
caselli-etal-2012-customizable,lenci-etal-2008-unsupervised,0,\N,Missing
caselli-etal-2012-customizable,N09-2066,0,\N,Missing
caselli-etal-2012-customizable,W99-0407,0,\N,Missing
caselli-etal-2012-customizable,francopoulo-etal-2006-lexical,0,\N,Missing
caselli-etal-2012-customizable,korhonen-etal-2006-large,0,\N,Missing
caselli-etal-2014-enriching,baroni-etal-2004-introducing,0,\N,Missing
caselli-etal-2014-enriching,alvez-etal-2008-complete,0,\N,Missing
caselli-etal-2014-enriching,W11-0122,0,\N,Missing
caselli-etal-2014-enriching,N09-2066,0,\N,Missing
caselli-etal-2014-enriching,W03-1022,0,\N,Missing
caselli-etal-2014-enriching,E09-1005,0,\N,Missing
caselli-etal-2014-enriching,P06-1014,0,\N,Missing
caselli-etal-2014-enriching,Q13-1013,0,\N,Missing
caselli-etal-2014-enriching,caselli-etal-2012-customizable,1,\N,Missing
caselli-etal-2014-enriching,lenci-etal-2012-lexit,0,\N,Missing
caselli-etal-2014-enriching,I11-1099,0,\N,Missing
caselli-etal-2014-enriching,W13-3816,1,\N,Missing
caselli-etal-2014-enriching,jezek-quochi-2010-capturing,0,\N,Missing
caselli-prodanof-2006-annotating,J95-2003,0,\N,Missing
caselli-prodanof-2006-annotating,J00-4003,0,\N,Missing
caselli-prodanof-2006-annotating,P04-1019,0,\N,Missing
caselli-prodanof-2010-annotating,W03-2120,0,\N,Missing
caselli-prodanof-2010-annotating,hasler-etal-2006-nps,0,\N,Missing
caselli-prodanof-2010-annotating,bejan-harabagiu-2008-linguistic,0,\N,Missing
D14-1046,agirre-etal-2010-exploring,0,0.0898934,"een obtained through two strategies: • lemma label: we extract all normalized domain labels associated to each sense of each lemma in the sense description from MWN. The value of the feature GENERIC corresponds to the sum of the FACTOTUM labels. The value of the feature SPECIFIC corresponds to the sum of all other specific domain labels (e.g. MEDICINE, SPORT etc.) after they have been collapsed into a single value (i.e. NOT-FACTOTUM). • word sense label: for each sense description, we have first performed Word Sense Disambiguation by means of an adapted version to Italian of the UKB package2 (Agirre et al., 2010; Agirre et al., 2014)3 . Only the highest ranked synset, and associated WN Domain(s), was retained as good. Similarly to the lemma label strategy, the sum of the domain label FACTOTUM is assigned to the feature GENERIC, while the sum of all other domain labels collapsed into the single value NOTFACTOTUM is assigned to the feature SPECIFIC. Classifier and feature selection We have developed a training set by manually aligning noun senses between the two lexica. The sense alignment allows us to associate all the information of a synset to a corresponding entry in the SCDM lexicon, including the"
D14-1046,W04-2214,0,0.0428249,"8 38123 Povo, Italy t.caselli@gmail.com Carlo Strapparava FBK / Via Sommarive, 18 38123 Povo, Italy strappa@fbk.eu Abstract The purpose of this work is two folded: first, we experiment on the automatic assignment of domain labels to sense descriptions, and then, evaluate the impact of this information for improving an existing sense aligned dataset for nouns. Previous works has demonstrated that domain labels are a good feature for obtaining high quality alignments of entries (Navigli, 2006; Toral et al., 2009; Navigli and Ponzetto, 2012). The WordNet (WN) Domains (Magnini and Cavaglia, 2000; Bentivogli et al., 2004) have been selected as reference domain labels. We will use as candidate lexico-semantic resources to be aligned two Italian lexica, namely, MultiWordNet (MWN) and the Senso Comune De Mauro Lexicon (SCDM) (Vetere et al., 2011). The two resources differ in terms of modelization: the former, MWN, is an Italian version of WN obtained through the “expand model” (Vossen, 1996) and perfectly aligned to Princeton WN 1.6, while the latter, SCDM, is a machine readable dictionary obtained from a paper-based reference lexicographic dictionary, De Mauro GRADIT. Major issues for WSA of the lexica concern t"
D14-1046,W14-0140,1,0.773603,"od. Similarly to the lemma label strategy, the sum of the domain label FACTOTUM is assigned to the feature GENERIC, while the sum of all other domain labels collapsed into the single value NOTFACTOTUM is assigned to the feature SPECIFIC. Classifier and feature selection We have developed a training set by manually aligning noun senses between the two lexica. The sense alignment allows us to associate all the information of a synset to a corresponding entry in the SCDM lexicon, including the WN Domain label. Concerning the test set, we have used an existing dataset of aligned noun pairs as in (Caselli et al., 2014). We report in Table 1 the figures for the training and test sets. Multiple alignments with the same domain label have been excluded from the training set. Characteristics # lemmas # of aligned pairs # of SCDM senses # of MWN synsets # SCDM with WN Domain label Training Set 131 369 747 675 Test Set 46 166 216 229 350 118 We experimented with two classifiers: Naive Bayes and Maximum Entropy as implemented in the MALLET package (McCallum, 2002). We illustrate the results in Table 2. The classifiers have been evaluated with respect to standard Precision (P), Recall (R) and F1 against the test set"
D14-1046,magnini-cavaglia-2000-integrating,0,0.0945283,"rentoRISE / Via Sommarive, 18 38123 Povo, Italy t.caselli@gmail.com Carlo Strapparava FBK / Via Sommarive, 18 38123 Povo, Italy strappa@fbk.eu Abstract The purpose of this work is two folded: first, we experiment on the automatic assignment of domain labels to sense descriptions, and then, evaluate the impact of this information for improving an existing sense aligned dataset for nouns. Previous works has demonstrated that domain labels are a good feature for obtaining high quality alignments of entries (Navigli, 2006; Toral et al., 2009; Navigli and Ponzetto, 2012). The WordNet (WN) Domains (Magnini and Cavaglia, 2000; Bentivogli et al., 2004) have been selected as reference domain labels. We will use as candidate lexico-semantic resources to be aligned two Italian lexica, namely, MultiWordNet (MWN) and the Senso Comune De Mauro Lexicon (SCDM) (Vetere et al., 2011). The two resources differ in terms of modelization: the former, MWN, is an Italian version of WN obtained through the “expand model” (Vossen, 1996) and perfectly aligned to Princeton WN 1.6, while the latter, SCDM, is a machine readable dictionary obtained from a paper-based reference lexicographic dictionary, De Mauro GRADIT. Major issues for W"
D14-1046,S01-1027,1,0.592062,"P 0.77 0.70 0.77 0.74 R 0.58 0.49 0.58 0.54 F1 0.66 0.58 0.66 0.62 10-Fold F1 0.66 0.63 0.69 0.67 Table 2: Results for the Naive Bayes and Maximum Entropy binary classifiers. synset1 and express a subject field label (e.g. SPORT, MEDICINE). A special label, FACTOTUM, has been used for those synsets which can appear in almost all subject fields. The identification of a domain label to the nominal entries in the SCDM Lexicon is based the “One Domain per Discourse” (ODD) hypothesis applied to the sense descriptions. We have used a reduced set of domains labels (45 normalized domains) following (Magnini et al., 2001). To assign the WN domain label to the SCDM entries, we have developed a hybrid method: first a binary classifier is applied to the SCDM sense descriptions to discriminate between two domain values, FACTOTUM and OTHER, where the OTHER value includes all remaining 44 normalized domains. After this, all entries classified with the OTHER value are analyzed by a rule based system and associated with a specific domain label (i.e. SPORT, MEDICINE, FOOD . . . ). 2.1 GENERIC:val SPECIFIC:val). Feature values have been obtained through two strategies: • lemma label: we extract all normalized domain lab"
D14-1046,R09-1080,0,0.0609121,"Missing"
D14-1046,Q13-1013,0,0.0192327,".g. WordNet (Fellbaum, 1998)). The process of creation of lexical resources is costly both in terms of money and time. To overcome these limits, semi-automatic approaches have been developed (e.g. MultiWordNet (Pianta et al., 2002)) with different levels of success. Furthermore, important information is scattered in different resources and difficult to use. Semantic interoperability between resources could represent a viable solution to allow reusability and develop more robust and powerful resources. Word sense alignment (WSA) qualifies as the preliminary requirement for achieving this goal (Matuschek and Gurevych, 2013). WSA aims at creating lists of pairs of senses from two, or more, (lexical-semantic) resources which denote the same meaning. Different approaches to WSA have been proposed and they all share some common elements, namely: i.) the extensive use of sense descriptions of the words (e.g. WordNet glosses); and ii.) the extension of the basic sense descriptions with additional information such as hypernyms, synonyms and domain or category labels. • SCMD has no structure of word senses (i.e. no taxonomy, no synonymy relations, no distinction between core senses and subsenses for polysemous entries)"
D14-1046,N07-1025,0,0.102901,"Missing"
D14-1046,P06-1014,0,0.0571605,"Missing"
D14-1046,W11-0122,0,0.0379761,"Missing"
D14-1046,P07-2041,0,0.0781805,"Missing"
D14-1046,J14-1003,0,\N,Missing
del-gratta-etal-2008-ufra,W07-1501,0,\N,Missing
del-gratta-etal-2008-ufra,wright-2004-global,0,\N,Missing
E17-2042,P10-2013,0,0.0350608,". 2017 Association for Computational Linguistics News Evaluative Descriptive Expository Instructive Narrative None Other 0.82 0.84 0.86 1.0 - Travel Reports 0.90 0.86 0.93 0.65 0.88 1.0 0.92 To test the comprehensiveness of this scheme, we annotate English texts from two different genres and periods of publication: namely, contemporary news and travel reports published between the end of the XIX Century and the beginning of the XX Century. While the former are taken from already available datasets, i.e., TempEval-3, Penn Discourse Treebank, and MASC (UzZaman et al., 2013; Prasad et al., 2008; Ide et al., 2010), the latter constitute a novel set of texts extracted from the Gutenberg project2 . The corpus is released under the name of Content Types Dataset version 1.0 (CTD v1). The resource is still being extended with new annotated texts, but in the remainder of the paper we will refer to this first version. The annotation was conducted by two expert linguists following a multi-step process and using the web-based tool CAT (Bartalesi Lenzi et al., 2012). In the first phase, annotators were allowed to discuss disagreements based on a trial corpus suggesting revisions to improve the guidelines. In the"
E17-2042,J08-4004,0,0.0992312,"l CAT (Bartalesi Lenzi et al., 2012). In the first phase, annotators were allowed to discuss disagreements based on a trial corpus suggesting revisions to improve the guidelines. In the second phase, inter-annotator agreement was calculated on a subset of the CTD v1 (a total of 5,328 tokens and 526 clauses, with 2,500 tokens and about 250 clauses per genre). Table 1 reports the Cohen’s kappa on the number of tokens for both text genres. With the exception of the INSTRUCTIVE CT, all the classes have high scores, exceeding 0.8, usually set as a threshold that guarantees good annotation quality (Artstein and Poesio, 2008). In the final phase, the whole dataset was annotated using the latest version of the guidelines which includes detailed descriptions of the classes, examples for both genres, and priority rules discriminating when more than one CT class may apply to clauses. Table 2 illustrates the composition of CTD v1. The two genres of texts show, for almost all the CT classes, a statistically significant difference (at p<0.01 and calculated with the z test) in their distribution. Table 1: Inter Annotator Agreement: Cohen’s kappa calculated at token level. scheme to account for undefined or unclear cases:"
E17-2042,baccianella-etal-2010-sentiwordnet,0,0.0169125,"mma of clause adverb, coarse tense values (present, past, future), fine-grained tense values (present perfect, etc.), voice, grammatical aspect (progressive, perfect), WordNet sense and supersense, WordNet hypernyms, length of path to the top node in WordNet, head POS Table 3: Features of the clause components. 3.1 Feature Sets other syntactic relation, and (iv) the clause verb. Details for noun phrase and verb phrase components are reported in Table 3. We extended the basic features with prior sentiment polarity scores for nouns, verbs, adjectives, and adverbs in the clause via SentiWordNet (Baccianella et al., 2010). For each target POS, polarity scores are aggregated per lemma and averaged by the number of senses, thus providing a lemma-based prior polarity. Finally, the lemma-based polarity scores are normalized by the clause length and scaled between 0 and 1. Finally, we introduced a binary feature to mark the presence/absence of a temporal expression in a clause. These two additional blocks of features have been selected following the definition of the CTs in the annotation guidelines. In particular, the presence of temporal expressions in a clause can facilitate the distinction between the NARRATIVE"
E17-2042,bartalesi-lenzi-etal-2012-cat,1,0.813257,"the former are taken from already available datasets, i.e., TempEval-3, Penn Discourse Treebank, and MASC (UzZaman et al., 2013; Prasad et al., 2008; Ide et al., 2010), the latter constitute a novel set of texts extracted from the Gutenberg project2 . The corpus is released under the name of Content Types Dataset version 1.0 (CTD v1). The resource is still being extended with new annotated texts, but in the remainder of the paper we will refer to this first version. The annotation was conducted by two expert linguists following a multi-step process and using the web-based tool CAT (Bartalesi Lenzi et al., 2012). In the first phase, annotators were allowed to discuss disagreements based on a trial corpus suggesting revisions to improve the guidelines. In the second phase, inter-annotator agreement was calculated on a subset of the CTD v1 (a total of 5,328 tokens and 526 clauses, with 2,500 tokens and about 250 clauses per genre). Table 1 reports the Cohen’s kappa on the number of tokens for both text genres. With the exception of the INSTRUCTIVE CT, all the classes have high scores, exceeding 0.8, usually set as a threshold that guarantees good annotation quality (Artstein and Poesio, 2008). In the f"
E17-2042,2007.sigdial-1.16,0,0.040168,"re set extended with the doc2vec clause embeddings. 4 The classification of text passages has been studied in previous works considering different textual units (e.g., clauses, sentences, and paragraphs) or language patterns (Kaufer et al., 2004). Several annotation schemes, often based on genre-specific taxonomies, have been proposed. This is the case, for example, of the detection of the main components in scholarly publications (Teufel et al., 2009; Liakata et al., 2012; De Waard and Maat, 2012; Burns et al., 2016) or the annotation of content zones, i.e., functional constituents of texts (Bieler et al., 2007; Stede and Kuhn, 2009; Baiamonte et al., 2016). On the contrary, the notion of Content Types that we have adopted applies across genres. CTs are based on linguistic theories on discourse/rhetorical strategies but differ from discourse relations. Over the years, different typologies have been proposed (Werlich, 1976; Biber, The SVM models have been implemented using LIBSVM (Chang and Lin, 2011) with Linear Kernel. The CRF models have been implemented with CRF++ toolkit 4 with default parameters. Content and Functional Structure Classification This set of experiments assumes an alternative mode"
E17-2042,P14-5010,0,0.00259154,"We experiment two different types of features: the first relies on distributional information extracted through sentence embeddings (Le and Mikolov, 2014), while the second is linguistically motivated and focuses on syntactic and semantic properties of the main components of the clause, i.e. the noun phrase(s) and the verb phrase. For the first type, we extracted embeddings for each clause using the doc2vec (Le and Mikolov, 2014) implementation in gensim, with vector size = 50 and window = 5. For the second feature type, all documents were pre-processed at clause level with Stanford CoreNLP (Manning et al., 2014), performing tokenization, lemmatization, POS tagging, Named Entity recognition. The extraction of basic syntactic and semantic properties of the clause components has been performed with a syntactic-semantic features toolkit (Friedrich and Pinkal, 2015). This has allowed us to identify four blocks of features for: (i) the noun phrase in subject position (i.e. nsubj and nsubjpass), (ii) the noun phrase in direct object position (i.e. dobj and agent), (iii) the noun phrase in any 262 3.2 Classification Experiments Results are illustrated in Table 4. The content-based classification experiments"
E17-2042,W15-2702,0,0.0145443,"butions of the CTs in the dataset. Furthermore, we plan to study whether information on content types can contribute to other NLP tasks. For example, we believe that identifying NARRATIVE and EVALUATIVE CTs may contribute to discriminating between clauses useful to build a storyline or a timeline of events (the former) and clauses bearing sentiment information (the latter). 1989; Chatman, 1990; Adam, 1985; Longacre, 2013) but have been rarely treated computationally, with the exception of the work by Cocco et al. (2011). The theory of Discourse Modes (DMs) (Smith, 2003) is instead followed by Mavridou et al. (2015) that apply it to a paragraph-based pilot annotation of a variety of documents such as novels, news and European Parliament proceedings. Annotators intuitively labeled DMs relying on a very short manual: as a consequence, no formal guidelines were made available and only a moderate agreement was achieved. Moreover, the final dataset is not publicly available and the recognition of DMs has not been automated yet. Our approach is different: we rely on Werlich’s typology, we provide complete annotation guidelines, we make available the annotated dataset, and we experiment automatic classification"
E17-2042,R11-1059,0,0.0330243,"Missing"
E17-2042,W12-4306,0,0.0583861,"Missing"
E17-2042,P15-1123,0,0.0237978,"es of the main components of the clause, i.e. the noun phrase(s) and the verb phrase. For the first type, we extracted embeddings for each clause using the doc2vec (Le and Mikolov, 2014) implementation in gensim, with vector size = 50 and window = 5. For the second feature type, all documents were pre-processed at clause level with Stanford CoreNLP (Manning et al., 2014), performing tokenization, lemmatization, POS tagging, Named Entity recognition. The extraction of basic syntactic and semantic properties of the clause components has been performed with a syntactic-semantic features toolkit (Friedrich and Pinkal, 2015). This has allowed us to identify four blocks of features for: (i) the noun phrase in subject position (i.e. nsubj and nsubjpass), (ii) the noun phrase in direct object position (i.e. dobj and agent), (iii) the noun phrase in any 262 3.2 Classification Experiments Results are illustrated in Table 4. The content-based classification experiments show that CTs are subject to the functional structure of the sentence and, more generally, of the document. Only the CRF classifiers, i.e. sequence labeling models, can beat the baseline, providing balanced results for Precision and Recall, and improving"
E17-2042,prasad-etal-2008-penn,0,0.016121,"pain, April 3-7, 2017. 2017 Association for Computational Linguistics News Evaluative Descriptive Expository Instructive Narrative None Other 0.82 0.84 0.86 1.0 - Travel Reports 0.90 0.86 0.93 0.65 0.88 1.0 0.92 To test the comprehensiveness of this scheme, we annotate English texts from two different genres and periods of publication: namely, contemporary news and travel reports published between the end of the XIX Century and the beginning of the XX Century. While the former are taken from already available datasets, i.e., TempEval-3, Penn Discourse Treebank, and MASC (UzZaman et al., 2013; Prasad et al., 2008; Ide et al., 2010), the latter constitute a novel set of texts extracted from the Gutenberg project2 . The corpus is released under the name of Content Types Dataset version 1.0 (CTD v1). The resource is still being extended with new annotated texts, but in the remainder of the paper we will refer to this first version. The annotation was conducted by two expert linguists following a multi-step process and using the web-based tool CAT (Bartalesi Lenzi et al., 2012). In the first phase, annotators were allowed to discuss disagreements based on a trial corpus suggesting revisions to improve the"
E17-2042,D09-1155,0,0.0118741,"se model has only basic clause features plus the polarity scores and the presence/absence of temporal expressions. • clause+doc2vec model has the clause model feature set extended with the doc2vec clause embeddings. 4 The classification of text passages has been studied in previous works considering different textual units (e.g., clauses, sentences, and paragraphs) or language patterns (Kaufer et al., 2004). Several annotation schemes, often based on genre-specific taxonomies, have been proposed. This is the case, for example, of the detection of the main components in scholarly publications (Teufel et al., 2009; Liakata et al., 2012; De Waard and Maat, 2012; Burns et al., 2016) or the annotation of content zones, i.e., functional constituents of texts (Bieler et al., 2007; Stede and Kuhn, 2009; Baiamonte et al., 2016). On the contrary, the notion of Content Types that we have adopted applies across genres. CTs are based on linguistic theories on discourse/rhetorical strategies but differ from discourse relations. Over the years, different typologies have been proposed (Werlich, 1976; Biber, The SVM models have been implemented using LIBSVM (Chang and Lin, 2011) with Linear Kernel. The CRF models hav"
E17-2042,S13-2001,0,0.0242654,"260–266, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics News Evaluative Descriptive Expository Instructive Narrative None Other 0.82 0.84 0.86 1.0 - Travel Reports 0.90 0.86 0.93 0.65 0.88 1.0 0.92 To test the comprehensiveness of this scheme, we annotate English texts from two different genres and periods of publication: namely, contemporary news and travel reports published between the end of the XIX Century and the beginning of the XX Century. While the former are taken from already available datasets, i.e., TempEval-3, Penn Discourse Treebank, and MASC (UzZaman et al., 2013; Prasad et al., 2008; Ide et al., 2010), the latter constitute a novel set of texts extracted from the Gutenberg project2 . The corpus is released under the name of Content Types Dataset version 1.0 (CTD v1). The resource is still being extended with new annotated texts, but in the remainder of the paper we will refer to this first version. The annotation was conducted by two expert linguists following a multi-step process and using the web-based tool CAT (Bartalesi Lenzi et al., 2012). In the first phase, annotators were allowed to discuss disagreements based on a trial corpus suggesting rev"
E17-2042,W15-4507,1,0.721559,"rd discourse relations, either based on rhetorical structures or lexically-grounded approaches. Content Types provide cues to access the structure of a document’s types of functional content. They contribute to the overall message or purpose of a text and make explicit the functional role of a discourse segment with respect to its content, i.e. meaning. Their identification may improve the performance of more complex NLP tasks by targeting the portions of the documents that are more relevant. For example, when building a storyline it may be useful to focus on the narrative segments of a text (Vossen et al., 2015), while for sentiment analysis the identification of evaluative clauses may be beneficial (Liu, 2015). Our contribution is threefold: i) we make available annotation guidelines with high reliability in terms of inter-annotator agreement and applicable to texts of different genres and period of publication; ii) we release the first version of a new dataset (whose annotation is still in progress) that takes into consideration both contemporary and historical texts, paving the way to a new NLP task, i.e. 2 Dataset Construction Content Types (henceforth CTs) are text passages with specific semanti"
L16-1063,pianta-etal-2008-textpro,0,0.0371484,"Description In this Section we describe data pre-processing together with PIERINO structure and functionalities. Examples taken from the #buonascuola use case and screenshots are provided. 2.1. Data Selection and Preparation The platform takes in input a list of natural language comments/answers, grouped by question. In our use case, the analysis focused on a set of 10 questions, which were considered as highly relevant by MIUR collaborators. Overall, we processed more than 270,000 free text comments, containing almost 5,000,000 tokens. Each group of comments is first processed with TextPro2 (Pianta et al., 2008), an NLP suite performing several linguistic analyses, and with Mallet3 (McCallum, 2002), a system for the extraction of topic models. Then, we implemented a Web interface to easily navigate the data and query them. The analyses and the visualization strategies were discussed with MIUR and improved iteratively to meet their requirements, which were basically: • for each question, provide a concise representation of the main topics mentioned in the replies; • provide an interface which can be intuitively used without the need of technical support; • allow users to easily make queries to the dat"
L16-1063,rosell-velupillai-2008-revealing,0,0.0120733,"he literature reports the use of different techniques. For example, supervised approaches (Giorgetti et al., 2003) and active learning (Patil and Ravindran, 2015) are employed to perform survey coding, that is matching open-ended answers with a short description associated to a code, thus converting qualitative information (text) into a quantitative format (code). In addition, text clustering and topic modeling methods are applied to explore information contained in open answers, e.g. to summarize and classify them (Wang and Mulrow, 2014) and to identify the intentions of survey participants (Rosell and Velupillai, 2008). In this work, we present the first attempt to apply stateof-the-art Italian Natural Language Processing (NLP) techniques to the outcome of an online public consultation for policy making, with the goal to integrate citizens’ contributions in the Italian school reform. The work is a joint initiative among the Digital Humanites Group at Fondazione Bruno Kessler (FBK), the Vrije Universiteit Amsterdam (VUA) and the Italian Ministry of Education, Universities and Research (MIUR). The goal of the work is the automatic analysis of linguistic data contained in the answers given by the participants"
L16-1187,P11-1059,0,0.0216625,"ent to annotate the event, as a representative of the whole proposition, as the target of factuality. This is because factuality cues can target specific relations within a proposition. To clarify, consider the following example, taken from FactBank: 1182 12 13 TimeBank/FactBank – APW19980227.476-S1 GEN AUTHOR denotes a non-explicit generic source. We call this phenomenon perspective scope, referring to those specific propositional relations associated with an event (or entity) that are affected by a perspective cue. It is strongly related to the scope and focus of negation as investigated by Blanco and Moldovan (2011), but to our knowledge its annotation has not been investigated before in the context of factuality (or sentiment). We believe that perspective scope is an important and innovative aspect of our annotation scheme, and our formal model GRaSP allows for the representation of separate factuality assignments for the event and its relations. In the near future, we will work out the details with respect to its annotation. 4.4. Opinion Layer The final annotation layer that we have included in our scheme is that of opinion or sentiment. As our annotations are largely based on Wiebe et al. (2005) and T"
L16-1187,N15-1146,0,0.0316723,"of factuality. In the factuality layer, each event identified in the event layer is to be annotated as the target of a factuality relation. In the opinion layer, annotators have no clear pre-defined targets; instead, they need to look for cues and understand the text in more detail. The first thing to look for are attributional cues, since they are fairly easy to recognize and some of them will already have been identified in the attribution layer. An example of such an attributional cue is support in our example sentence repeated below, which expresses a positive attitude of Mbeki. Following Deng and Wiebe (2015), we aim to identify the specific entities and events that are the target of the opinion. In this case, there are two targets: the entity denoted by Mugabe and the event expressed by elections. 10. Investors and Western diplomats have saide1 they might interprete2 {Mbeki’s}SENT- SOURCE {supporte3 }SENT- CUE for {Mugabe}SENT- TARGET or the {electionse4 }SENT- TARGET as a sign that Africa is not intent on revitalizinge5 its economies through good governmente6 and expanded international tradee7 . The other two types of cues are also present in our example. An example of a factual opinion cue is e"
L16-1187,W09-3012,0,0.198309,"events and their causes (e.g. conspiracy theories on 9/11). Textual data always provide specific perspectives of the author and quoted sources on the information they contain. Mining information from texts thus implies dealing with these perspectives. In the last decade, different aspects of linguistic encoding of perspectives have been targeted as separated phenomena through different annotation initiatives, each with its own approaches and goals. Targeted aspects of perspectives include, for example, attribution (Prasad et al., 2007; Pareti, 2012), factuality (Saur´ı and Pustejovsky, 2009; Diab et al., 2009), and opinion (Wiebe et al., 2005). Coordinating initiatives such as the Unified Linguistic Annotation project1 have tried to technically combine such annotations into a unique annotation model, but they lack an overarching framework for the various layers of annotation from different resources. Furthermore, annotation initiatives such as those proposed by Prasad et al. (2007) and Pareti (2012) have attempted to tackle the annotation of perspectives in a unified approach, but with different levels of success.2 In our approach, the notion of perspectives lies at the semantic-pragmatic interface"
L16-1187,pareti-2012-database,0,0.1945,"ortion, vaccinations, etc.), and interpretative frames on events and their causes (e.g. conspiracy theories on 9/11). Textual data always provide specific perspectives of the author and quoted sources on the information they contain. Mining information from texts thus implies dealing with these perspectives. In the last decade, different aspects of linguistic encoding of perspectives have been targeted as separated phenomena through different annotation initiatives, each with its own approaches and goals. Targeted aspects of perspectives include, for example, attribution (Prasad et al., 2007; Pareti, 2012), factuality (Saur´ı and Pustejovsky, 2009; Diab et al., 2009), and opinion (Wiebe et al., 2005). Coordinating initiatives such as the Unified Linguistic Annotation project1 have tried to technically combine such annotations into a unique annotation model, but they lack an overarching framework for the various layers of annotation from different resources. Furthermore, annotation initiatives such as those proposed by Prasad et al. (2007) and Pareti (2012) have attempted to tackle the annotation of perspectives in a unified approach, but with different levels of success.2 In our approach, the n"
L16-1187,S15-1009,0,0.0145949,"uting the description of the single ‘real’ event in this sentence expressed by demonstrations. In other words, only one event in this sentence should be put on a timeline. In this framework, syntax is only used to decide on the span of the event. The two main corpora for factuality (or belief) are FactBank (Saur´ı and Pustejovsky, 2009) and the Lexical Understanding (LU) Annotation Corpus (Diab et al., 2009). Although both corpora address the same phenomenon (i.e. the commitment of a source towards the truth of some event/proposition), the annotations are quite different (Werner et al., 2015; Prabhakaran et al., 2015). The LU Corpus so far has only addressed the problem from the perspective of the speaker/writer, in contrast to FactBank, which has fully annotated nested sources. Furthermore, the LU corpus ignores negation (the polarity axis of factuality in FactBank) and does not distinguish between POSSI BLE and PROBABLE (the certainty axis of factuality in FactBank). Finally, there is a subtle difference in the targets. Whereas FactBank has assigned factuality values to events, the LU corpus has assigned belief tags to the head words of propositions, disregarding event-denoting noun phrases (e.g. the col"
L16-1187,pustejovsky-etal-2010-iso,0,0.0225563,"d approach. Section 4 describes the four layers that we have currently defined for the an1177 notation of perspectives: events, attribution, factuality, and opinion. Finally, we conclude and summarize our work and give an outlook on future work in Section 5. 2. Related Work In our annotations, events play an important role because we consider them to be the basic semantic elements that may give rise to or be involved in perspectives. A wellknown specification language for events is TimeML (Pustejovsky et al., 2003a), which has been consolidated as an international cross-language ISO standard (Pustejovsky et al., 2010) and has been used as the annotation language for the TempEval shared task series (Verhagen et al., 2009). Its reference corpus is TimeBank (Pustejovsky et al., 2003b). TimeML defines an event as something that can be said to obtain or hold true, to happen or to occur. TimeML adopts a surface-based annotation of texts and morpho-syntactic information plays a key role for detecting all possible mentions of an event. According to TimeML, both demonstrations and taken (place) in Example 1 are to be annotated as valid event mentions. 1. Several pro-Iraq demonstrations have taken place in the last"
L16-1187,P15-5003,0,0.0165306,"e represented as: PARTY −−−−−−−−−→ MARY OrganizedBy Note that both Examples 3 and 4 mention the O RGA NIZED B Y relation between Mary and his birthday party; however, whereas in the former it functions merely as additional information on the target, in the latter it is the target of the perspective. The schematic representation in Figure 1 illustrates the differences between these targets. Not all attitude dimensions can take the same types of targets. For example, opinion can target entities, events or propositional relations, but factuality can only target events or propositional relations (Rambow and Wiebe, 2015). Whereas the source and target are usually expressed by a single linguistic unit (e.g. an NP or a clause), the attitude may be expressed either by a single linguistic cue or a combination of cues.6 For example, the commitment of a source towards the factual nature of an event or proposition may be expressed by a combination of polarity (e.g. not, never) and modality cues (e.g. could, maybe). In turn, one cue can express multiple attitude dimensions. For example, a verb like hope expresses positive sentiment and uncertainty towards the target at the same time. 5 If the author of a document is"
L16-1187,P10-1059,0,0.0384465,"), but to our knowledge its annotation has not been investigated before in the context of factuality (or sentiment). We believe that perspective scope is an important and innovative aspect of our annotation scheme, and our formal model GRaSP allows for the representation of separate factuality assignments for the event and its relations. In the near future, we will work out the details with respect to its annotation. 4.4. Opinion Layer The final annotation layer that we have included in our scheme is that of opinion or sentiment. As our annotations are largely based on Wiebe et al. (2005) and Toprak et al. (2010), the three main elements are defined as follows: • Source: The entity that has a positive or negative attitude towards some target. • Cue: A linguistic cue that, possibly in combination with other cues, expresses the positive or negative attitude of the source towards the target. We regard cues as belonging to one of the following categories: – Attributional cue: contributes a source while directly expressing the positive or negative attitude of the source towards the embedded target; – Indirect cue: signals the positive or negative attitude of the source by the choice of words; – Factual opi"
L16-1187,W15-1304,0,0.0122515,"merely helps constituting the description of the single ‘real’ event in this sentence expressed by demonstrations. In other words, only one event in this sentence should be put on a timeline. In this framework, syntax is only used to decide on the span of the event. The two main corpora for factuality (or belief) are FactBank (Saur´ı and Pustejovsky, 2009) and the Lexical Understanding (LU) Annotation Corpus (Diab et al., 2009). Although both corpora address the same phenomenon (i.e. the commitment of a source towards the truth of some event/proposition), the annotations are quite different (Werner et al., 2015; Prabhakaran et al., 2015). The LU Corpus so far has only addressed the problem from the perspective of the speaker/writer, in contrast to FactBank, which has fully annotated nested sources. Furthermore, the LU corpus ignores negation (the polarity axis of factuality in FactBank) and does not distinguish between POSSI BLE and PROBABLE (the certainty axis of factuality in FactBank). Finally, there is a subtle difference in the targets. Whereas FactBank has assigned factuality values to events, the LU corpus has assigned belief tags to the head words of propositions, disregarding event-denoting"
L16-1557,W11-0418,1,0.859808,"Missing"
L16-1557,P14-2082,0,0.117095,"Missing"
L16-1557,C12-1129,0,0.471054,"Missing"
L16-1557,sprugnoli-lenci-2014-crowdsourcing,1,0.88389,"Missing"
L16-1557,S13-2001,0,0.0428748,"uation procedures for systems against public benchmark data. Reviewing the performance of systems shows that Temporal Processing is not a trivial task, especially when dealing with temporal relations. In Table 1 we report the results of the best systems for Italian and English with respect to four tasks: a.) Temporal Expressions (TIMEXes) Identification; b.) Event Extraction; c.) Temporal Relation Identification and Classification from raw text (TLINKs raw); and d.) Temporal Relation Classification given Gold entities (TLINKs Gold). The figures have been extracted from TempEval-3 for English (UzZaman et al., 2013) and EVENTI for Italian (Caselli et al., 2014). Although a direct comparison among the test sets cannot be done, the difference in the systems’ performance in the two languages can hardly be explained by making reference only to language specific issues. Furthermore, the systems’ performance seems to be also affected by the language specific annotation guidelines and the quality of the annotated corpora. Concerning the annotation guidelines, the two languages: a.) share the same annotation philosophy (i.e., adherence to the superficial text form), the same set of markables and values; b.) are"
L16-1557,S07-1014,0,0.0433752,"t sets cannot be done, the difference in the systems’ performance in the two languages can hardly be explained by making reference only to language specific issues. Furthermore, the systems’ performance seems to be also affected by the language specific annotation guidelines and the quality of the annotated corpora. Concerning the annotation guidelines, the two languages: a.) share the same annotation philosophy (i.e., adherence to the superficial text form), the same set of markables and values; b.) are compliant with the ISO-TimeML standard; c.) have benefited from a continu1 TempEval 2007 (Verhagen et al., 2007): http://www. timeml.org/tempeval/; TempEval 2010 (Verhagen et al., 2010): http://www.timeml.org/tempeval2/; TempEval 2013: http://www.cs.york.ac.uk/ semeval-2013/task1/ TIMEXes EVENTs TLINK Raw TLINK Gold Lang IT EN IT EN IT EN IT EN IAA P&R=0.95 P&R=0.83 P&R=0.86 P&R=0.78 Dice=0.86 P&R=0.55 K=0.88 K=0.71 System HeidelTime1.8 HeidelTime-t FBK-B1 ATT-1 FBK C1 ClearTK-2 FBK D1 UTTime-1,4 F1 0.709 0.776 0.867 0.81 0.264 0.309 0.736 0.564 Table 1: Inter-annotator agreement (IAA) for Italian (IT) and English (EN) together with system performance comparison (F1) for the two languages. ous collabora"
L16-1557,S10-1010,1,\N,Missing
L16-1625,balahur-etal-2010-sentiment,0,0.0828562,"Missing"
L16-1625,E14-4040,0,0.028681,"tweets that are considered relevant, i.e., the tweets that are linked to the news article. Although the cosine distance proved to be an optimal method to detect novel texts (Kumaran and Allan, 2004), it fails to output fair results on short texts (Sahami and Heilman, 2006), such as tweets. As we mentioned in our methodology, novelty can be also expressed through sentiments. Although crowdsourcing methods for tweets and news novelty detection are still under-developed, research has been done on crowdsourcing sentiments from news or microblogs (Balahur et al., 2010; Rao et al., 2014). Recently (Dunietz and Gillick, 2014) have proposed a new task, entity salience, which merges notions of centrality and referential salience. The task aims at assigning a salience 3965 score to each entity in a document. The authors define salience on the line of (Boguraev and Kennedy, 1999), i.e., as those discourse objects which have a prominent position in the focus of attention of the speaker/hearer9 . Salience labels are automatically generated by exploiting summary pairs from the annotated New York Times corpus (Sandhaus, 2008), containing 1.8 millions of news articles accompanied by a summary written by an expert. 7. Concl"
L16-1625,W10-0727,0,0.0344742,"computed based on related documents and is not tainted by irrelevant documents (Zhao et al., 2006). Various approaches have been developed to compute how relevant two texts are for each other: Local Context Analysis (LCA) (Fern´andez and Losada, 2007), word similarity feature combined with part-of-speech (POS) tagging, Latent Semantic Analysis (LSA) and enrichment with semantic relations from WordNet (Han et al., 2013) among others. Crowdsourcing proved to be a cheap, quick, and reliable alternative for assessing relevance for the data used in the TREC Novelty Task (Alonso and Mizzaro, 2009; Grady and Lease, 2010). Although the approaches yield good results, their crowdsourcing approaches lack a well defined methodology to assess crowd workers and the inherent language ambiguity. For novelty detection in texts, an extensive literature study of automated methods is presented in (Verheij et al., 2012). As another perspective of novelty, in (Wei and Gao, 2014), the authors perform single document summarization for creating news highlights. Their approach combines news articles with tweets in order to extract a set of relevant and novel text snippets from a document. However, the limit of this method is th"
L16-1625,S13-1005,0,0.0723206,"nces in the beginning the average maximum score is 0.79, for those at the end 0.8 while for those in the middle is 0.73. 5.1.1. Automated Relevance for Tweets and News We assume that a good method to automatically derive relevance in our three datasets is text similarity: the more similar a text snippet is to the title or a tweet to the seed words, the more relevant the text fragments with respect to the target event. We thus applied an off-the-shelf tool based on a hybrid approach that combines distributional similarity and Latent Semantic Analysis (LSA) with semantic relations from WordNet (Han et al., 2013). On T weet2014DS1 and T weet2015DS2 we computed the semantic similarity between the tweets and each seed word from the domain experts and then averaged these numbers and computed an overall similarity of the tweet with respect to “whaling”. We performed this comparison on each of the datasets, separately. The reason for this was driven by the fact that the two datasets were collected in different ways. The automated similarity approach for tweets returned very low values: between 0 and 0.24 and an average of 0.11 on the T weet2014DS1 dataset, where the crowd average is 0.84. The top scored tw"
L16-1625,C14-1083,0,0.0308839,"nd enrichment with semantic relations from WordNet (Han et al., 2013) among others. Crowdsourcing proved to be a cheap, quick, and reliable alternative for assessing relevance for the data used in the TREC Novelty Task (Alonso and Mizzaro, 2009; Grady and Lease, 2010). Although the approaches yield good results, their crowdsourcing approaches lack a well defined methodology to assess crowd workers and the inherent language ambiguity. For novelty detection in texts, an extensive literature study of automated methods is presented in (Verheij et al., 2012). As another perspective of novelty, in (Wei and Gao, 2014), the authors perform single document summarization for creating news highlights. Their approach combines news articles with tweets in order to extract a set of relevant and novel text snippets from a document. However, the limit of this method is the use of a very restrictive set of tweets that are considered relevant, i.e., the tweets that are linked to the news article. Although the cosine distance proved to be an optimal method to detect novel texts (Kumaran and Allan, 2004), it fails to output fair results on short texts (Sahami and Heilman, 2006), such as tweets. As we mentioned in our m"
L18-1051,P98-1013,0,0.335983,"Missing"
L18-1051,P14-2082,0,0.264893,"irst. However, differences in results are not statistically significant after performing the McNemar’s test (p&gt;0.05). 4.2. Temporal Relation and Classification The TR task is addressed by means of 3 multi-class CRF classifiers, one for each pair of temporal entities (e-dct, et, and e-e pairs), which predict the 14 TimeML temporal values. Similarly to existing systems, we target only intrasentence relations for e-t and e-e pairs, given that the number of cross-sentence relations in the training data is low. The classifiers are trained with the gold data set only, plus additional relations from Bethard et al. (2014). Different pairs have been normalized with respect to the directionality of the relation in order to reduce the variability of the temporal values. This resulted in the following ordering of pairs: i) relations involving an event and a timex, including the DCT, have been represented as e-t/e-dct pairs; ii) relations involving event pairs have been normalized according to the linear order of presentation of the events in the sentences. In this task the system uses predicted event triggers, which in the learning model are represented with the morphosyntactic and lexical-semantic features used i"
L18-1051,S16-1165,0,0.0203764,"mpaign (UzZaman et al., 2013) is the latest campaign on open-domain TP in English. The major contribution of TempEval-3 is the release of a platform for the development and evaluation of end-to-end TP systems based on the TimeML mark-up language (Pustejovsky et al., 2003). After the TempEval-3 evaluation a new dataset has been released, the TimeBank-Dense corpus (Cassidy et al., 2014). This corpus has been developed to address one of the major shortcomings of the TempEval-3 dataset, namely lack 1 TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), Clinical TempEval (Bethard et al., 2016; Bethard et al., 2017), Q-A TempEval (Llorens et al., 2015). 2 For an extended list of available TimeBanks see (Caselli and Sprugnoli, 2017). of connectivity between all possible e-e and e-t pairs, by completing the transitive closure. The outcome of this approach is a very dense temporal graph consisting of 12,713 annotated TRs, with a 6.3 ratio of relations to events and timexes, which is much higher than the 0.8 ratio of the TempEval-3 data, where 11,098 TRs were manually annotated. Density has been obtained by forcing the annotators to always provide an answer. Additionally, the set of TR"
L18-1051,S13-2002,0,0.256652,"l-3 that targeted either the event extraction and classification subtask only (Task B in the TempEval-3 guidelines) or the end-to-end temporal relation identification and classification subtask (Task C in the TempEval-3 guidelines, which includes Task B as well). In total we review 6 unique systems (5 for event detection and classification only and 4 for the full TP). 3.1. Event detection and classification The event detection and classification task is addressed by all systems using supervised discrete machine learning classifiers such as Conditional Random Fields (CRFs) (Kolya et al., 2013; Bethard, 2013), Logistic Regression (Kolomiyets and Moens, 2013), and Maximum Entropy (Chambers, 2013; Jung and Stent, 2013). Most of the systems (4 out of 5) adopted the same learning model also for event classification. Overall, 17 features are represented in the learning models, which can be aggregated in 5 groups: https://github.com/cltl/TimeMLEventTrigger 340 • Basic morpho-syntactic features: token, lemma, stem, parts-of-speech (POSs), token’s affix and/or suffix, among others. • Syntactic features: constituency/dependency syntax relations; governing verb lemma, verb chunks. • Contextual features: con"
L18-1051,Q14-1022,0,0.0305623,"Missing"
L18-1051,S13-2012,0,0.377252,"n the TempEval-3 guidelines) or the end-to-end temporal relation identification and classification subtask (Task C in the TempEval-3 guidelines, which includes Task B as well). In total we review 6 unique systems (5 for event detection and classification only and 4 for the full TP). 3.1. Event detection and classification The event detection and classification task is addressed by all systems using supervised discrete machine learning classifiers such as Conditional Random Fields (CRFs) (Kolya et al., 2013; Bethard, 2013), Logistic Regression (Kolomiyets and Moens, 2013), and Maximum Entropy (Chambers, 2013; Jung and Stent, 2013). Most of the systems (4 out of 5) adopted the same learning model also for event classification. Overall, 17 features are represented in the learning models, which can be aggregated in 5 groups: https://github.com/cltl/TimeMLEventTrigger 340 • Basic morpho-syntactic features: token, lemma, stem, parts-of-speech (POSs), token’s affix and/or suffix, among others. • Syntactic features: constituency/dependency syntax relations; governing verb lemma, verb chunks. • Contextual features: context windows of token, lemma, POS; and tokens polarity, among others. • Semantic featur"
L18-1051,D17-1190,0,0.0313767,"Missing"
L18-1051,W07-1409,0,0.0201448,"Missing"
L18-1051,S13-2004,0,0.416906,"guidelines) or the end-to-end temporal relation identification and classification subtask (Task C in the TempEval-3 guidelines, which includes Task B as well). In total we review 6 unique systems (5 for event detection and classification only and 4 for the full TP). 3.1. Event detection and classification The event detection and classification task is addressed by all systems using supervised discrete machine learning classifiers such as Conditional Random Fields (CRFs) (Kolya et al., 2013; Bethard, 2013), Logistic Regression (Kolomiyets and Moens, 2013), and Maximum Entropy (Chambers, 2013; Jung and Stent, 2013). Most of the systems (4 out of 5) adopted the same learning model also for event classification. Overall, 17 features are represented in the learning models, which can be aggregated in 5 groups: https://github.com/cltl/TimeMLEventTrigger 340 • Basic morpho-syntactic features: token, lemma, stem, parts-of-speech (POSs), token’s affix and/or suffix, among others. • Syntactic features: constituency/dependency syntax relations; governing verb lemma, verb chunks. • Contextual features: context windows of token, lemma, POS; and tokens polarity, among others. • Semantic features, limited to semantic"
L18-1051,S13-2014,0,0.348341,"traction and classification subtask only (Task B in the TempEval-3 guidelines) or the end-to-end temporal relation identification and classification subtask (Task C in the TempEval-3 guidelines, which includes Task B as well). In total we review 6 unique systems (5 for event detection and classification only and 4 for the full TP). 3.1. Event detection and classification The event detection and classification task is addressed by all systems using supervised discrete machine learning classifiers such as Conditional Random Fields (CRFs) (Kolya et al., 2013; Bethard, 2013), Logistic Regression (Kolomiyets and Moens, 2013), and Maximum Entropy (Chambers, 2013; Jung and Stent, 2013). Most of the systems (4 out of 5) adopted the same learning model also for event classification. Overall, 17 features are represented in the learning models, which can be aggregated in 5 groups: https://github.com/cltl/TimeMLEventTrigger 340 • Basic morpho-syntactic features: token, lemma, stem, parts-of-speech (POSs), token’s affix and/or suffix, among others. • Syntactic features: constituency/dependency syntax relations; governing verb lemma, verb chunks. • Contextual features: context windows of token, lemma, POS; and tokens pola"
L18-1051,S13-2011,0,0.346604,"systems from TempEval-3 that targeted either the event extraction and classification subtask only (Task B in the TempEval-3 guidelines) or the end-to-end temporal relation identification and classification subtask (Task C in the TempEval-3 guidelines, which includes Task B as well). In total we review 6 unique systems (5 for event detection and classification only and 4 for the full TP). 3.1. Event detection and classification The event detection and classification task is addressed by all systems using supervised discrete machine learning classifiers such as Conditional Random Fields (CRFs) (Kolya et al., 2013; Bethard, 2013), Logistic Regression (Kolomiyets and Moens, 2013), and Maximum Entropy (Chambers, 2013; Jung and Stent, 2013). Most of the systems (4 out of 5) adopted the same learning model also for event classification. Overall, 17 features are represented in the learning models, which can be aggregated in 5 groups: https://github.com/cltl/TimeMLEventTrigger 340 • Basic morpho-syntactic features: token, lemma, stem, parts-of-speech (POSs), token’s affix and/or suffix, among others. • Syntactic features: constituency/dependency syntax relations; governing verb lemma, verb chunks. • Contextu"
L18-1051,lopez-de-lacalle-etal-2014-predicate,0,0.0127694,"on and classification; ii) we intend to establish a relation between the errors made by the systems, including ours, and the features used. CRF4TimeML has been designed taking as reference efficient existing systems that incorporate discrete classifiers. In particular, all classifiers we developed share with previous systems basic morpho-syntactic features, such as token, lemma, POS, and dependency relations. However, we have added lexical semantic information by using not only WordNet synsets, but also VerbNet classes and FrameNet frames, obtained from the alignments in the Predicate Matrix (Lacalle et al., 2014). The pre-processing of data is performed with state-of-the-art tools, such as the Stanford CoreNLP (Manning et al., 2014) and the NewsReader NLP pipelines (Agerri et al., 2014). 4 4.1. Event Detection and Classification The event detection and classification task is performed by 4 different classifiers share the basic morpho-syntactic and 4 The timex detection and normalization task is performed using a state-of-the-art system (Bethard, 2013), available at https://bitbucket.org/qwaider/textpro-en. 341 • Adding the automatically generated training data to the manually created training data onl"
L18-1051,S15-2134,0,0.0133911,"n-domain TP in English. The major contribution of TempEval-3 is the release of a platform for the development and evaluation of end-to-end TP systems based on the TimeML mark-up language (Pustejovsky et al., 2003). After the TempEval-3 evaluation a new dataset has been released, the TimeBank-Dense corpus (Cassidy et al., 2014). This corpus has been developed to address one of the major shortcomings of the TempEval-3 dataset, namely lack 1 TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), Clinical TempEval (Bethard et al., 2016; Bethard et al., 2017), Q-A TempEval (Llorens et al., 2015). 2 For an extended list of available TimeBanks see (Caselli and Sprugnoli, 2017). of connectivity between all possible e-e and e-t pairs, by completing the transitive closure. The outcome of this approach is a very dense temporal graph consisting of 12,713 annotated TRs, with a 6.3 ratio of relations to events and timexes, which is much higher than the 0.8 ratio of the TempEval-3 data, where 11,098 TRs were manually annotated. Density has been obtained by forcing the annotators to always provide an answer. Additionally, the set of TRs has been simplified with respect to the one used in TempEv"
L18-1051,P14-5010,0,0.00247954,"he features used. CRF4TimeML has been designed taking as reference efficient existing systems that incorporate discrete classifiers. In particular, all classifiers we developed share with previous systems basic morpho-syntactic features, such as token, lemma, POS, and dependency relations. However, we have added lexical semantic information by using not only WordNet synsets, but also VerbNet classes and FrameNet frames, obtained from the alignments in the Predicate Matrix (Lacalle et al., 2014). The pre-processing of data is performed with state-of-the-art tools, such as the Stanford CoreNLP (Manning et al., 2014) and the NewsReader NLP pipelines (Agerri et al., 2014). 4 4.1. Event Detection and Classification The event detection and classification task is performed by 4 different classifiers share the basic morpho-syntactic and 4 The timex detection and normalization task is performed using a state-of-the-art system (Bethard, 2013), available at https://bitbucket.org/qwaider/textpro-en. 341 • Adding the automatically generated training data to the manually created training data only for the Event Trigger and Class classifiers. lexico-semantic features: one for the identification of event triggers, and"
L18-1051,I17-1085,0,0.0280445,"Missing"
L18-1051,C14-1198,0,0.0542149,"Missing"
L18-1051,C16-1265,0,0.0274861,"Missing"
L18-1051,W17-0222,0,0.0607951,"Missing"
L18-1051,S13-2001,0,0.20146,"timex (et). Temporally aware Natural Language Processing (NLP) systems are crucial not only to generate timelines and storylines (Vossen et al., 2015), but also in decision support systems, summarization and textual entailment applications, question answering systems, and document archiving, among others. Since the release of the TimeBank corpus (Pustejovsky et al., 2003) there has been a renewed interest in the area of TP, which has resulted in the celebration of several evaluation campaigns1 and in the creation of corpora and tools in languages other than English. 2 The TempEval-3 campaign (UzZaman et al., 2013) is the latest campaign on open-domain TP in English. The major contribution of TempEval-3 is the release of a platform for the development and evaluation of end-to-end TP systems based on the TimeML mark-up language (Pustejovsky et al., 2003). After the TempEval-3 evaluation a new dataset has been released, the TimeBank-Dense corpus (Cassidy et al., 2014). This corpus has been developed to address one of the major shortcomings of the TempEval-3 dataset, namely lack 1 TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), Clinical TempEval (Bethard et al., 2016; Bethard"
L18-1051,S07-1014,0,0.0405623,"n the creation of corpora and tools in languages other than English. 2 The TempEval-3 campaign (UzZaman et al., 2013) is the latest campaign on open-domain TP in English. The major contribution of TempEval-3 is the release of a platform for the development and evaluation of end-to-end TP systems based on the TimeML mark-up language (Pustejovsky et al., 2003). After the TempEval-3 evaluation a new dataset has been released, the TimeBank-Dense corpus (Cassidy et al., 2014). This corpus has been developed to address one of the major shortcomings of the TempEval-3 dataset, namely lack 1 TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), Clinical TempEval (Bethard et al., 2016; Bethard et al., 2017), Q-A TempEval (Llorens et al., 2015). 2 For an extended list of available TimeBanks see (Caselli and Sprugnoli, 2017). of connectivity between all possible e-e and e-t pairs, by completing the transitive closure. The outcome of this approach is a very dense temporal graph consisting of 12,713 annotated TRs, with a 6.3 ratio of relations to events and timexes, which is much higher than the 0.8 ratio of the TempEval-3 data, where 11,098 TRs were manually annotated. Density has been obta"
L18-1051,W15-4507,1,0.644984,"meaningfully interpreted by using models of time, which allow to connect events on a timeline (temporal anchoring) and to understand complex sequences of events (temporal ordering). Temporal relations (TRs) provide a model and a set of properties to account for the connections between pairs of entities. Temporal Processing (TP) is a task consisting in automatically identifying and classifying basic entities and their relations, such as event-event (e-e), and event-timex (et). Temporally aware Natural Language Processing (NLP) systems are crucial not only to generate timelines and storylines (Vossen et al., 2015), but also in decision support systems, summarization and textual entailment applications, question answering systems, and document archiving, among others. Since the release of the TimeBank corpus (Pustejovsky et al., 2003) there has been a renewed interest in the area of TP, which has resulted in the celebration of several evaluation campaigns1 and in the creation of corpora and tools in languages other than English. 2 The TempEval-3 campaign (UzZaman et al., 2013) is the latest campaign on open-domain TP in English. The major contribution of TempEval-3 is the release of a platform for the d"
L18-1725,bartalesi-lenzi-etal-2012-cat,0,0.0253447,"were created of which 2244 circumstantial ones and 193 subevents. On average, 7 Subevents are currently not modeled in CEO, but they were annotated for future experiments and evaluations. every ECB+/CEO article contains about 7 new coreference sets and about 5 different circumstantial relations. Instances Coreference sets CEO relations - of which Circumstantial - of which subEvent ECB+ 3323 3323 - ECB+/CEO 3038 3448 2437 2244 193 Table 1: Overview of the annotations made for ECB+/CEO in contrast with ECB+ for the topics annotated For the annotation, we used the CAT annotation tool (Bartalesi Lenzi et al., 2012) which outputs the annotations in XML. In terms of annotation effort, a single article took about 30 minutes to annotate on average. The corpus and the annotation guidelines are publicly available at https: //github.com/newsreader/eso-and-ceo. Inter Annotator Agreement For the calculation of the Inter Annotator Agreement (IAA), we selected 25 articles from five different topics in ECB+/CEO covering variation in article length and complexity. The evaluation was carried out on the CEO links. Agreement was calculated on the existence, or identification, of CEO links. CEO links are created between"
L18-1725,W09-1206,0,0.087529,"Missing"
L18-1725,W17-2712,0,0.030511,"two Abstract Objects (called Arg1 and Arg2), corresponding to discourse units, rather than event mentions. The contingency relation is annotated either in presence of an explicit connective, i.e. a lexical item, between the two abstract objects, or implicitly, by adjacency in discourse. In our approach, contingency relations are one of the possible values which express circumstantial relations, and, most importantly, they are independent of the presence of connectives or adjacency in discourse, but grounded on (shared) properties of events. A related resource is the Rich Event Ontology (REO) (Brown et al., 2017), that provides an independent semantic backbone to different lexical resources such as FrameNet and VerbNet. REO will have explicit causal relations between event classes as well as predefined pre- and post conditions. However, these relations are more strictly defined and on class level. On the other hand, CEO maintains a looser definition in terms of causality, and takes into account the roles affected by the event and the circumstantial relation. A resource such as CEO is envisioned to be of added value for several NLP tasks such as script mining, question answering, information extraction"
L18-1725,L18-1051,1,0.889849,"Missing"
L18-1725,chambers-jurafsky-2010-database,0,0.336561,"perties. The implication is however not necessary. Previous work on the encoding of semantic relations between event pairs has focused on specific subsets of circumstantial relations. For instance, one example is the encoding of the entailment relations in WordNet (Fellbaum, 1998). With respect to the WordNet approach, we abstract from various event types (i.e. lexical items) and do not depend on relations defined at a synset level, by formalizing event knowledge and relations in an ontology. We also provide more details on the property involved. Another related approach are narrative chains (Chambers and Jurafsky, 2010), that provide chains of various event mentions. However, the relation between these mentions is not specified explicitly but based on co-occurrence of participants and a basic precedence relation. Manual inspection of these chains revealed that dissimilar relations are implied within these chains, varying from temporal ordering, to episodic, up to causal. The Penn Discourse TreeBank (PDTB) (Prasad et al., 2007) annotates contingency relations, of which causal relations are a subclass. In PDTB, the focus of the annotation is between two Abstract Objects (called Arg1 and Arg2), corresponding to"
L18-1725,cybulska-vossen-2014-using,1,0.768256,"ication of instances of the calamity classes in CEO, we used Chamber’s narrative chains (Chambers and Jurafsky, 2010). This selection was made manually, based on at least three calamity events per event chain. We also manually selected FrameNet frames that capture calamity events and we used the SUMO ontology as a backbone for modeling our initial list of verbs 3 Contents of CEO The ECB+/CEO Corpus In addition to the CEO, we developed a corpus of annotated circumstantial event relations. For this, we build upon an existing corpus, specifically annotated for event coreference: the ECB+ Corpus (Cybulska and Vossen, 2014). ECB+ consists of 984 news articles divided over 42 topics. From these topics, we manually selected 22 topics (508 articles) that cover calamities such as earthquakes, murders, hijacks and arson. In ECB+, only the most relevant event mentions are manually annotated. For ECB+/CEO, we automatically extended the set of annotated event mentions by applying a state-of-the art machine learning based system 6 . Two linguistically trained annotators were hired for the selection of relevant calamity events and the annotation of circumstantial relations. More specifically, the annotation procedure cons"
L18-1725,L16-1423,0,0.041351,"Missing"
L18-1725,L16-1233,1,0.772607,"is envisioned to be of added value for several NLP tasks such as script mining, question answering, information extraction, and textual entailment, among others. Furthermore, the explicitly defined relations between events can be of help in reconstructing Figure 1: The ESO assertions for the class eso:Damaging storylines (Van den Akker et al., 2010; Vossen et al., 2015) and improve the coherence of existing narrative chain models (Chambers and Jurafsky, 2010). 3. The Circumstantial Event Ontology CEO builds upon an existing event ontology called the Event and Implied Situation Ontology (ESO) (Segers et al., 2016). ESO is designed to run over the output of Semantic Role Labeling systems by making explicit both the ontological type of the predicative element and the situation that holds before, during and after the predicate. Each so called pre-, post- and during situation consists of a set of properties and roles that define what holds true. For instance, as can be seen in Figure 1, the pre- and post-situations of the event class “eso:Damaging” define: • that something is in a “relatively plus (+)” state (presituation); • that this something is in a “relatively less (-)” state, i.e. it underwent a loss"
L18-1725,W17-2706,1,0.142835,"cting” event may, but not necessarily, lead to “ceo:Injuring” or “ceo:Damaging”, which is based on the shared property of some object being damaged. Modeling these relations provides a means to track chains of logically related events and their shared participants within and across documents. Semantic circumstantial relations define possible explanatory sequences of events, but not the actual explanatory sequences. Episodic relations, on the other hand, define circumstantial relations that are dependent on the actual occurrences of events in the world. The Circumstantial Event Ontology (CEO) (Segers et al., 2017) 1 , described in this paper, models such semantic relations, based on shared properties of the event classes with the aim to support the detection of episodic circumstantial relations in texts. Modeling these semantic relations in an ontology will allow us to 1.) abstract over the different lexical realizations of the same concept (i.e. at an event mention level); and 2.) facilitate reasoning between event classes and enrich the extraction of information for event knowledge and event sequences. The remainder of this paper is organized as follows: in section 2., we describe related work; in se"
L18-1725,W15-4507,1,0.837072,"hese relations are more strictly defined and on class level. On the other hand, CEO maintains a looser definition in terms of causality, and takes into account the roles affected by the event and the circumstantial relation. A resource such as CEO is envisioned to be of added value for several NLP tasks such as script mining, question answering, information extraction, and textual entailment, among others. Furthermore, the explicitly defined relations between events can be of help in reconstructing Figure 1: The ESO assertions for the class eso:Damaging storylines (Van den Akker et al., 2010; Vossen et al., 2015) and improve the coherence of existing narrative chain models (Chambers and Jurafsky, 2010). 3. The Circumstantial Event Ontology CEO builds upon an existing event ontology called the Event and Implied Situation Ontology (ESO) (Segers et al., 2016). ESO is designed to run over the output of Semantic Role Labeling systems by making explicit both the ontological type of the predicative element and the situation that holds before, during and after the predicate. Each so called pre-, post- and during situation consists of a set of properties and roles that define what holds true. For instance, as"
R11-1074,lesmo-lombardo-2002-transformed,0,0.0103246,"for verb entries. 4 Evaluation is divided in two experiments that correspond to the objectives of this paper. The Italian TempEval-2 data contains 4,543 events in the training set and 834 in the test set. In Table 1, we report the distributions of the event tokens in the seven TimeML classes for training and test. We set as baseline for the evaluation a previous realization of a TimeML event detector and classification system for Italian, the TimeML TULE Converter2 (Robaldo et al., 2011). The Converter takes as input the syntactic trees of the sentences in a document built by the TULE parser (Lesmo and Lombardo, 2002). The TULE Converter implements two different sets of rules: a group for event recognition which takes into account morphological features (PoS) and dependency relations with a set of “event trigger expressions” and a group for event classification. In particular for classification, the TULE converter exploits the derived event lexicon for having access to the TimeML class(es) associated with each event lemma and then integrates this information with syntactic information. We have developed three data driven models to capture, incrementally, the influence of the features. The basic model, TIPS"
R11-1074,S10-1063,1,0.888614,"Missing"
R11-1074,W04-2404,0,0.0390513,"Missing"
R11-1074,S07-1014,0,0.013118,"Alicante stela@dlsi.ua.es Introduction Recognizing and classifying events is a strategic task in order to improve the performance of many NLP applications such as automatic summarization and question answering (Q.A.). In NLP, different definitions of event can be found regarding the target application. Recently, TimeML (Pustejovsky et al., 2003a) introduced a rich specification language for annotating and classifying events and it has been applied to English documents (the TimeBank corpus (Pustejovsky et al., 2003b)). The SemEval TempEval-1 and TempEval-2 international evaluation excercises (Verhagen et al., 2007; Verhagen et al., 2010), have provided the NLP community with gold standard resources for comparative evaluations of different systems. In addition to this, TempEval-2 made available 2 TimeML specifications for events In TimeML an event is defined as something that happens or holds true. Natural language (NL) offers a variety of means to realize events, such as verbs (andare [to go]), complex VPs (light verb constructions, fare una doccia [to have a shower], or idioms), nouns (nominalizations - volo [flight], 533 Proceedings of Recent Advances in Natural Language Processing, pages 533–538, Hi"
R11-1074,S10-1010,1,\N,Missing
S10-1010,verhagen-2010-brandeis,1,0.752179,"B), precision, recall and the f1-measure are used as evaluation metrics, using the following formulas: X precision recall f -measure Table 1: Corpus size and relation tasks All corpora include event and timex annotation. The French corpus contained a subcorpus with temporal relations but these relations were not split into the four tasks C through F. Annotation proceeded in two phases: a dual annotation phase where two annotators annotate each document and an adjudication phase where a judge resolves disagreements between the annotators. Most languages used BAT, the Brandeis Annotation Tool (Verhagen, 2010), a generic webbased annotation tool that is centered around the notion of annotation tasks. With the task decomposition allowed by BAT, it is possible to structure the complex task of temporal annotation by splitting it up in as many sub tasks as seems useful. As 3 Evaluation Metrics = = = tp/(tp + f p) tp/(tp + f n) 2 ∗ (P ∗ R)/(P + R) Where tp is the number of tokens that are part of an extent in both key and response, fp is the number of tokens that are part of an extent in the response but not in the key, and fn is the number of tokens that are part of an extent in the key but not in the"
S10-1010,taule-etal-2008-ancora,0,0.0880496,"Missing"
S10-1010,S07-1014,1,0.681823,"ons in the text and is identical to the TIMEX3 tag in TimeML. Times can be expressed syntactically by adverbial or prepositional phrases, as shown in the following example. (1) a. on Thursday b. November 15, 2004 c. Thursday evening d. in the late 80’s e. later this afternoon Introduction The ultimate aim of temporal processing is the automatic identification of all temporal referring expressions, events and temporal relations within a text. However, addressing this aim is beyond the scope of an evaluation challenge and a more modest approach is appropriate. The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three. In the rest of this paper, we first introduce the data that we are dealing with. Which gets us in a position to present the lis"
S14-2046,P05-1045,0,0.00507464,"or otherwise). We implemented a simple algorithm to extract the LCS between two given texts. Then we divided the LCS length by the product of normalized lengths of two given texts and used it as a feature. 4.1 5.2 NER aims at identifying and classifying entities in a text with respect to a predefined set of categories such as person names, organizations, locations, time expressions, quantities, monetary values, percentages, etc. By exploring the training set, we observed that there are lot of texts in this task containing named entities. We deployed the Stanford Named Entity Recognizer tool (Finkel et al., 2005) to extract the similar and overlapping named entities between two given texts. Then we divided the number of similar/overlapping named entities by the sum length of two given texts. Analysis Before and After LCS After extracting the LCS between two given texts, we also considered the similarity for the parts before and after the LCS. The similarity between the text portions before and after the LSC has been obtained by means of the Lin measure and the Levenshtein distance. 5 6 Other Features Support Vector Machines (SVMs) Support vector machine (SVM) (Cortes and Vapnik, 1995) is a type of sup"
S14-2046,S13-1030,0,0.0289834,"(Fellbaum, 1999) is a lexical database for the English language in which words are grouped into sets of synonyms (namely synsets, 285 be very similar/related, otherwise dissimilar. We trained our LSA model on the British National Corpus (BNC) 1 and Wikipedia 2 corpora. 4 maximum 500 topics (20, 50, 100, 150, 200, 250, 300, 350, 400, 450 and 500). From the proportion vectors (distribution of documents over topics) of given texts, we applied three different measures to compute the distance between each pair of texts, which are Cosine similarity, Kullback-Leibler and Jensen-Shannon divergences (Gella et al., 2013). String Similarity Measures The Longest Common Substring (LCS) is the longest string in common between two or more strings. Two given texts are considered similar if they are overlapping/covering each other (e.g sentence 1 covers a part of sentence 2, or otherwise). We implemented a simple algorithm to extract the LCS between two given texts. Then we divided the LCS length by the product of normalized lengths of two given texts and used it as a feature. 4.1 5.2 NER aims at identifying and classifying entities in a text with respect to a predefined set of categories such as person names, organ"
S14-2046,S14-2003,0,0.0253575,"between given texts has drawn much attention from the Natural Language Processing community. Especially, the task becomes more interesting when it comes to measuring the semantic similarity between different-sized texts, e.g paragraph-sentence, sentence-phrase, phrase-word, etc. In this paper, we, the FBK-TR team, describe our system participating in Task 3 &quot;Cross-Level Semantic Similarity&quot;, at SemEval 2014. We also report the results obtained by our system, compared to the baseline and other participating systems in this task. 1 At SemEval 2014, the Task 3 &quot;Cross-Level Semantic Similarity&quot; (Jurgens et al., 2014) is to evaluate the semantic similarity across different sizes of texts, in particular, a larger-sized text is compared to a smaller-sized one. The task consists of four types of semantic similarity comparison: paragraph to sentence, sentence to phrase, phrase to word, and word to sense. The degree of similarity ranges from 0 (different meanings) to 4 (similar meanings). For evaluation, systems were evaluated, first, within comparison type and second, across all comparison types. Two methods are used to evaluate between system outputs and gold standard (human annotation), which are Pearson cor"
S14-2046,S12-1060,0,\N,Missing
S14-2046,N04-3012,0,\N,Missing
S14-2047,W13-0117,1,0.746815,"have a different parsing. Each words is replaced with their possible SUMO attributes (Niles and Pease, 2003). Only the following Stanford dependencies are retained as valid [n, nsub]sbj, [d,i,p]obj, prep, [x,c]comp. We considered only the most frequent occurrences of such patterns for each verb. To cluster into a single SDP pattern, all patterns that are sense auto-determinative, we used the OntoNotes (Hovy et al., 2006) and CPA (Hanks, 2008) lexica. Inside each cluster, we searched for the most general hypernyms for each syntactic slot such that there are no common patterns between clusters (Popescu, 2013). However, the patterns thus obtained are not sufficient enough for the task. Some expressions may be the paraFigure 2: Algorithm for computing entailment. 3.1 Entailment on Affirmative Sentences Affirmative sentences use three types of entailment patterns. The switch baseline and hyponym patterns works in this way: If two sentences are matched by the same SDP, and the difference between them is that the second one contains a hypernym on the same syntactic position, then the first one is entailed by the second (i.e. ENTAILMENT). If the two SDPs are such that the difference between them is that"
S14-2047,N06-2015,0,0.0310402,"ontaining at maximum two finite verbs from BNC and Annotated English Gigaword. We parsed this corpus with the Stanford parser, discarding the sentences from the Annotated English Gigaword which have a different parsing. Each words is replaced with their possible SUMO attributes (Niles and Pease, 2003). Only the following Stanford dependencies are retained as valid [n, nsub]sbj, [d,i,p]obj, prep, [x,c]comp. We considered only the most frequent occurrences of such patterns for each verb. To cluster into a single SDP pattern, all patterns that are sense auto-determinative, we used the OntoNotes (Hovy et al., 2006) and CPA (Hanks, 2008) lexica. Inside each cluster, we searched for the most general hypernyms for each syntactic slot such that there are no common patterns between clusters (Popescu, 2013). However, the patterns thus obtained are not sufficient enough for the task. Some expressions may be the paraFigure 2: Algorithm for computing entailment. 3.1 Entailment on Affirmative Sentences Affirmative sentences use three types of entailment patterns. The switch baseline and hyponym patterns works in this way: If two sentences are matched by the same SDP, and the difference between them is that the se"
S14-2047,S14-2001,0,0.0338215,"r University of Trento Trento, Italy ngoc@fbk.eu Octavian Popescu Tommaso Caselli Fondazione Bruno Kessler TrentoRISE Trento, Italy Trento, Italy popescu@fbk.eu t.caselli@trentorise.eu Abstract At SemEval 2014, the Task #1 &quot;Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Entailment&quot; (Marelli et al., 2014a) primarily aimed at evaluating Compositional Distributional Semantic Models (CDSMs) of meaning over two subtasks, namely semantic relatedness and textual entailment (ENTAILMENT, CONTRADICTION and NEUTRAL), over pairs of sentences (Marelli et al., 2014b). Concerning the relatedness subtask, the system outputs are evaluated against gold standard ratings in two ways, using Pearson correlation and Spearman’s rank correlation (rho). The Pearson correlation is used for evaluating and ranking the participating systems. Similarly, for the textual entailment subtask, system outputs are evaluated against a gold standard rating with respect to accuracy. This paper reports the description and scores of our system, FBK-TR, which participated at the SemEval 2014 task #1 &quot;Evaluation of Compositional Distributional Semantic Models on Full Sentences throug"
S14-2047,marelli-etal-2014-sick,0,0.0435581,"r University of Trento Trento, Italy ngoc@fbk.eu Octavian Popescu Tommaso Caselli Fondazione Bruno Kessler TrentoRISE Trento, Italy Trento, Italy popescu@fbk.eu t.caselli@trentorise.eu Abstract At SemEval 2014, the Task #1 &quot;Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Entailment&quot; (Marelli et al., 2014a) primarily aimed at evaluating Compositional Distributional Semantic Models (CDSMs) of meaning over two subtasks, namely semantic relatedness and textual entailment (ENTAILMENT, CONTRADICTION and NEUTRAL), over pairs of sentences (Marelli et al., 2014b). Concerning the relatedness subtask, the system outputs are evaluated against gold standard ratings in two ways, using Pearson correlation and Spearman’s rank correlation (rho). The Pearson correlation is used for evaluating and ranking the participating systems. Similarly, for the textual entailment subtask, system outputs are evaluated against a gold standard rating with respect to accuracy. This paper reports the description and scores of our system, FBK-TR, which participated at the SemEval 2014 task #1 &quot;Evaluation of Compositional Distributional Semantic Models on Full Sentences throug"
S14-2047,N04-3012,0,0.0647765,"erning the Semantic Relatedness subtask our SVM system is built on different linguistic features, ranging from relatedness at the lexical level (WordNet based measures, Wikipedia relatedness and Latent Semantic Analysis), to sentence level, including topic modeling based on Latent Dirichlet allocation (LDA) and string similarity (Longest Common Substring). 2.1 Lexical Features At the lexical level, we built a simple, yet effective Semantic Word Relatedness model, which consists of 3 components: WordNet similarity (based on the Lin measure as implemented in Pedersen package WordNet:Similarity (Pedersen et al., 2004), Wikipedia relatedness (as provided by the Wikipedia Miner package (Milne and Witten, 2013)), and Latent Semantic Analysis (Landauer et al., 1998), with a model trained on the British National Corpus (BNC) 1 and Wikipedia. At this level of analysis, we concentrated only on the best matched (lemma) pairs of content words, i.e. Noun-Noun, Verb-Verb, extracted from each sentence pair. The content words have been automatically extracted by means of part-of-speech tagging (TreeTagger (Schmid, 1994)) and lemmatization. For words which are not present in WordNet, the relatedness score has been obtai"
S15-2077,W10-4001,0,0.0369826,"Missing"
S15-2077,W11-1707,0,0.0696797,"Missing"
S15-2077,P13-1174,0,0.090042,"Missing"
S15-2077,N09-1057,0,0.0259327,"store polarity values. For bag-of-words approaches the polarity of a text depends on the presence/absence of a set of lexical items. This methodology is successful to detect opinions about entities (such as reviews) but it shows mixed results when complex opinions about events - involving perspectives and points of view - are expressed. In terms of parts of speech involved, SA approaches tend to focus on lexical items that explicitly convey opinions - mainly adjectives, adverbs and several nouns - leaving verbs on the foreground. Improvements have been proposed by taking into account syntax (Greene and Resnik 2009) and by investigating the connotative polarity of words (Cambria et al., 2009; Akkaya et al., 2009, Balhaur et Carlo Strapparava Fondazione Bruno Kessler Via Sommarive, 18 38123 Povo (TN) strappa@fbk.eu al., 2011; Russo et al. 2011; Cambria et al., 2012, Deng et al., 2013 among others). One of the key aspects of sentiment analysis, which has been only marginally tackled so far, is the identification of implicit polarity. By implicit polarity we refer to the recognition of subjective textual units where no polarity markers are present but still people are able to state whether the text portion"
S15-2077,W12-3018,0,0.0498221,"Missing"
S15-2077,S14-2009,0,0.0998214,"Missing"
S15-2077,D09-1020,0,\N,Missing
S15-2077,P13-2022,0,\N,Missing
S15-2077,S14-2004,0,\N,Missing
S15-2133,P09-1068,0,0.0302034,"ent levels); such a representation can be further used to explore and discover additional event relations2 ; • Event coreference: we assume that two event mentions (either in the same document or in different documents) are coreferential if they share the same participant set (i.e., entities) and occur at the same time and place (Chen et al., 2011; Cybulska and Vossen, 2013); • Temporal relations: temporal relation processing can benefit from an entity driven approach as sequences of events sharing the same entities (i.e., co-participant events) can be assumed to stand in precedence relation (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010). 2.1 The SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have been developed, namely: 2 We are referring to a broader set of relations that we labeled as “bridging relations” among events which involve coparticipation, primary and secondary causal relations, temporal relations, and entailment relations. 788 • SPINOZA VU 1 uses the output"
S15-2133,chambers-jurafsky-2010-database,0,0.0132799,"ation can be further used to explore and discover additional event relations2 ; • Event coreference: we assume that two event mentions (either in the same document or in different documents) are coreferential if they share the same participant set (i.e., entities) and occur at the same time and place (Chen et al., 2011; Cybulska and Vossen, 2013); • Temporal relations: temporal relation processing can benefit from an entity driven approach as sequences of events sharing the same entities (i.e., co-participant events) can be assumed to stand in precedence relation (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010). 2.1 The SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have been developed, namely: 2 We are referring to a broader set of relations that we labeled as “bridging relations” among events which involve coparticipation, primary and secondary causal relations, temporal relations, and entailment relations. 788 • SPINOZA VU 1 uses the output of a state of the art system,"
S15-2133,I11-1012,0,0.0308787,"nteractions between the participants involved in an event individually; • Event relations: in an entity based representation, event mentions with more than one entity as their participants will be repeated in the final representation (both at in-document at crossdocument levels); such a representation can be further used to explore and discover additional event relations2 ; • Event coreference: we assume that two event mentions (either in the same document or in different documents) are coreferential if they share the same participant set (i.e., entities) and occur at the same time and place (Chen et al., 2011; Cybulska and Vossen, 2013); • Temporal relations: temporal relation processing can benefit from an entity driven approach as sequences of events sharing the same entities (i.e., co-participant events) can be assumed to stand in precedence relation (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010). 2.1 The SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have"
S15-2133,R13-1021,1,0.8536,"the participants involved in an event individually; • Event relations: in an entity based representation, event mentions with more than one entity as their participants will be repeated in the final representation (both at in-document at crossdocument levels); such a representation can be further used to explore and discover additional event relations2 ; • Event coreference: we assume that two event mentions (either in the same document or in different documents) are coreferential if they share the same participant set (i.e., entities) and occur at the same time and place (Chen et al., 2011; Cybulska and Vossen, 2013); • Temporal relations: temporal relation processing can benefit from an entity driven approach as sequences of events sharing the same entities (i.e., co-participant events) can be assumed to stand in precedence relation (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010). 2.1 The SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have been developed, namely: 2 W"
S15-2133,lopez-de-lacalle-etal-2014-predicate,0,0.0274096,"Missing"
S15-2133,S10-1063,0,0.124598,"SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have been developed, namely: 2 We are referring to a broader set of relations that we labeled as “bridging relations” among events which involve coparticipation, primary and secondary causal relations, temporal relations, and entailment relations. 788 • SPINOZA VU 1 uses the output of a state of the art system, TIPSem (Llorens et al., 2010), for event detection and temporal relations; • SPINOZA VU 2 is entirely based on data from the NWR pipeline including the temporal (TLINKs) and causal relation (CLINKs) layers. The final output is based on a dedicated rulebased module, the TimeLine (TML) module. We will describe in the following paragraphs how each subtask has been tackled with respect to each version of the system. Entity identification Entity identification relies on the entity detection and disambiguation layer (NERD) of the NWR pipeline. Each detected entity is associated with a URI (a unique identifier), either from DBpe"
S15-2133,S13-2001,0,0.0340615,"extraction. 1 Introduction This paper reports on a system (SPINOZA VU) for timeline extraction developed at the CLTL Lab of the VU Amsterdam in the context of the SemEval 2015 Task 4: Cross Document TimeLines. In this task, a timeline is defined as a set of chronologically anchored and ordered events extracted from a corpus spanning over a (large) period of time with respect to a target entity. Cross-document timeline extraction benefits from previous works and evaluation campaigns in Temporal Processing, such as the TempEval evaluation campaigns (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013) and aims at promoting research in temporal processing by tackling the following issues: cross-document and cross-temporal event detection and ordering; event coreference (indocument and cross-document); and entity-based temporal processing. The SPINOZA VU system is based on the NewsReader (NWR) NLP pipeline (Agerri et al., 2013; Beloki et al., 2014), which has been developed within the context of the NWR project1 and provides multi-layer annotations over raw texts from tokenization up to temporal relations. The goal of the NWR project is to build structured event indexes from large volumes of"
S15-2133,S07-1014,0,0.171512,"towards a more complex task such as storyline extraction. 1 Introduction This paper reports on a system (SPINOZA VU) for timeline extraction developed at the CLTL Lab of the VU Amsterdam in the context of the SemEval 2015 Task 4: Cross Document TimeLines. In this task, a timeline is defined as a set of chronologically anchored and ordered events extracted from a corpus spanning over a (large) period of time with respect to a target entity. Cross-document timeline extraction benefits from previous works and evaluation campaigns in Temporal Processing, such as the TempEval evaluation campaigns (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013) and aims at promoting research in temporal processing by tackling the following issues: cross-document and cross-temporal event detection and ordering; event coreference (indocument and cross-document); and entity-based temporal processing. The SPINOZA VU system is based on the NewsReader (NWR) NLP pipeline (Agerri et al., 2013; Beloki et al., 2014), which has been developed within the context of the NWR project1 and provides multi-layer annotations over raw texts from tokenization up to temporal relations. The goal of the NWR project is to build"
S15-2133,S10-1010,1,\N,Missing
S16-1193,agerri-etal-2014-ixa,0,0.0317586,"have used the CRF++ tool with default settings for the regularization algorithm (L2)2 for all tasks. The final output is obtained by converting the output of 7 different classifiers into the task representation format, i.e. anafora xml files. In the following subsections, we describe the preprocessing steps, which is common for all subtasks, and the specific system for each subtask. 1 For obtaining scripts and trained models contact the authors. 2 https://taku910.github.io/crfpp/#links 1242 2.1 Preprocessing All text files have been preprocessed by using two different tools: the IXA-pipeline (Agerri et al., 2014)3 and the Stanford CoreNLP tool (Manning et al., 2014). From the IXA pipeline, we used the tokenization, offset and sentence splitting modules. We then passed the tokenized data to the Stanford CoreNLP tool in order to extract additional basic annotation layers such as lemmatization, part-ofspeech tagging, and dependency parsing. The preprocessing step outputs the texts in a tab separated column format. After preprocessing the text files, we merged the preprocessed text with the gold annotations, which were exported from the anafora xml files into a tabcolumn separated files. 2.2 Span Detectio"
S16-1193,S16-1165,0,0.0418554,", 2010; UzZaman et al., 2013) in the framework of several shared tasks where systems were challenged to extract the relevant components of a document timeline: temporal expressions, The setting of the 2015 and 2016 SemEval Clinical TempEval Tasks is similar to previous TempEval campaigns, with the two main differences: i.) the domain , i.e. (colon) cancer clinical notes; and ii.) the annotation scheme, i.e. the THYME annotation scheme (Styler IV et al., 2014), an extended version of TimeML (Pustejovsky et al., 2003a). Similarly to the previous edition, the SemEval 2016 Clinical TempEval task (Bethard et al., 2016) consists of the following six subtasks: temporal expression detection (TS) and attribute classification (TA), event detection (ES) and attribute classification (EA), temporal relation detection and classification of an event with respect to the Document Creation Time (DR), and, finally, narrative container relation identification (CR). Systems are evaluated in two phases: Phase 1, which addressed all six subtasks from raw data, and Phase 2, where target entities, such as events and temporal expressions (including their attributes), were given and the systems were evaluated 1241 Proceedings of"
S16-1193,S13-2002,0,0.0396354,"Missing"
S16-1193,S10-1063,0,0.0201236,"ije Univeristeit Amsterdam De Boelelaan 1105 1081 HV Amsterdam, The Netherlands {t.caselli,r.morantevallejo}@vu.nl Abstract event mentions, and temporal relations. Several evaluations have shown the capabilities and limits of both the annotated resources and the systems. For instance, the best system in TempEval-3 (Bethard, 2013) reports 0.398 F1 on Temporal Relation Detection and Classification from raw text. The development of temporally annotated corpora has boosted research in languages other than English such as Italian (Caselli et al., 2014), French (Arnulphy et al., 2015), and Spanish (Llorens et al., 2010), among others. Recently, interest in temporal processing has moved forward in two directions: cross-document timeline extraction (Minard et al., 2015) and domain adaptation (Sun et al., 2013; Bethard et al., 2015). This paper describes VUACLTL, the system the CLTL Lab submitted to the SemEval 2016 Task Clinical TempEval. The system is based on a purely data-driven approach based on a cascade of seven CRF classifiers which use generic features and little domain knowledge. The challenge consisted in six subtasks related to temporal processing clinical notes from raw text (event and temporal exp"
S16-1193,P14-5010,0,0.00497734,"e regularization algorithm (L2)2 for all tasks. The final output is obtained by converting the output of 7 different classifiers into the task representation format, i.e. anafora xml files. In the following subsections, we describe the preprocessing steps, which is common for all subtasks, and the specific system for each subtask. 1 For obtaining scripts and trained models contact the authors. 2 https://taku910.github.io/crfpp/#links 1242 2.1 Preprocessing All text files have been preprocessed by using two different tools: the IXA-pipeline (Agerri et al., 2014)3 and the Stanford CoreNLP tool (Manning et al., 2014). From the IXA pipeline, we used the tokenization, offset and sentence splitting modules. We then passed the tokenized data to the Stanford CoreNLP tool in order to extract additional basic annotation layers such as lemmatization, part-ofspeech tagging, and dependency parsing. The preprocessing step outputs the texts in a tab separated column format. After preprocessing the text files, we merged the preprocessed text with the gold annotations, which were exported from the anafora xml files into a tabcolumn separated files. 2.2 Span Detection (ES, TS) and Attributes Classification (EA, TA) Task"
S16-1193,S13-2001,0,0.0392726,"or all the subtasks. 1 Introduction Temporal Processing is becoming more and more important for improving access to content. The availability of timelines (either event-centric or entity-centric) can help improving more complex semantically-focused tasks such as Question Answering, Text Summarization, and Textual Entailment, among others. Furthermore, timelines can be further exploited for monitoring the development in time of different phenomena, e.g. the opinions in debates. Temporal Processing research has mainly focused on the newswire domain (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013) in the framework of several shared tasks where systems were challenged to extract the relevant components of a document timeline: temporal expressions, The setting of the 2015 and 2016 SemEval Clinical TempEval Tasks is similar to previous TempEval campaigns, with the two main differences: i.) the domain , i.e. (colon) cancer clinical notes; and ii.) the annotation scheme, i.e. the THYME annotation scheme (Styler IV et al., 2014), an extended version of TimeML (Pustejovsky et al., 2003a). Similarly to the previous edition, the SemEval 2016 Clinical TempEval task (Bethard et al., 2016) consist"
S16-1193,P15-4022,0,0.0428954,"Missing"
S16-1193,S07-1014,0,0.0768811,"e results, which are not equally competitive for all the subtasks. 1 Introduction Temporal Processing is becoming more and more important for improving access to content. The availability of timelines (either event-centric or entity-centric) can help improving more complex semantically-focused tasks such as Question Answering, Text Summarization, and Textual Entailment, among others. Furthermore, timelines can be further exploited for monitoring the development in time of different phenomena, e.g. the opinions in debates. Temporal Processing research has mainly focused on the newswire domain (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013) in the framework of several shared tasks where systems were challenged to extract the relevant components of a document timeline: temporal expressions, The setting of the 2015 and 2016 SemEval Clinical TempEval Tasks is similar to previous TempEval campaigns, with the two main differences: i.) the domain , i.e. (colon) cancer clinical notes; and ii.) the annotation scheme, i.e. the THYME annotation scheme (Styler IV et al., 2014), an extended version of TimeML (Pustejovsky et al., 2003a). Similarly to the previous edition, the SemEval 2016 Clinica"
S16-1193,S10-1010,1,\N,Missing
S16-1193,S15-2132,0,\N,Missing
S16-1193,S15-2136,0,\N,Missing
S16-1193,Q14-1012,0,\N,Missing
W07-1606,W06-2107,0,0.0658224,"Missing"
W07-1606,W04-0208,0,0.0672927,"Missing"
W07-1606,W06-2111,0,0.0647323,"Missing"
W07-1606,W06-2105,0,\N,Missing
W07-1606,W01-1313,0,\N,Missing
W11-0418,2008.jeptalnrecital-recital.2,0,0.027972,"uation of the annotation scheme on the Ita-TimeBank, formed by two corpora independently realized by applying the annotation specifications. Finally, in Section 5 conclusions and extensions to the current annotation effort will be reported. Notice that, for clarity's sake, in this paper the examples will focus only on the tag (or attribute or link) under discussion. Applying an annotation scheme to a language other than the one for which it was initially developed, requires a careful study of the language specific issues related to the linguistic phenomena taken into account (Im et al., 2009; Bittar, 2008). TimeML focuses on Events (i.e. actions, states, and processes - <EVENT> tag), Temporal Expressions (i.e. durations, calendar dates, times of day and sets of time <TIMEX3> tag), Signals (e.g. temporal prepositions and subordinators - <SIGNAL> tag) and various kind of dependencies between Events and/or Temporal Expressions (i.e. temporal, aspectual and subordination relations <TLINK>, <ALINK> and <SLINK> tags respectively). An ISO language-independent specification of TimeML is under development but it is still in the enquiry stage1. For this reason, in the following subsections we will mostly"
W11-0418,W01-1311,0,0.097544,"Missing"
W11-0418,magnini-etal-2006-cab,0,0.606228,"tation strategies have been followed:  in case the multi-token Event expression corresponds to an instance of a collocation or of an idiomatic expression, then only the 146  2.2 head (verbal, nominal or other) of the expression is marked up; in case the multi-token Event is realized by light verb expressions, then two separate <EVENT> tags are to be created both for the verb and the nominal/prepositional complement. The <TIMEX3> tag The TIMEX3 tag relies on and is as much compliant as possible with the TIDES TIMEX2 annotation. The Italian adaptation of this annotation scheme is presented in Magnini et al. (2006). The only difference concerns the annotation of articulated prepositions which are annotated as signals, while in the TIMEX2 specifications they are considered as part of the textual realization of Temporal Expressions: (11a.) <TIMEX2 …> nel 2011 </TIMEX2> [in 2011] (11b.) <SIGNAL …> nel </SIGNAL> <TIMEX3…>2011</TIMEX3> [in 2011] On the other hand, with respect to the TIMEX3 annotation of other languages such as English, we decided to follow the TIMEX2 specification by annotating many adjectives as Temporal Expressions (e.g. recente [recent], ex [former]) and including modifiers like che rima"
W11-0418,P98-1013,0,0.298405,"Missing"
W11-0418,W01-1313,0,0.0830492,"ious kind of dependencies between Events and/or Temporal Expressions (i.e. temporal, aspectual and subordination relations <TLINK>, <ALINK> and <SLINK> tags respectively). An ISO language-independent specification of TimeML is under development but it is still in the enquiry stage1. For this reason, in the following subsections we will mostly compare the Italian annotation guidelines with the latest version of the English annotation guidelines (TimeML Working group, 2010), focusing on the two main tags, i.e <EVENT> and <TIMEX3>, in Italian. previous annotations schemes (Katz and Arosio, 2001, Filatova and Hovy, 2001, Setzer and Gaizauskas, 2001 among other), TimeML allows for annotating as Events not only verbs but also nouns, adjectives and prepositional phrases. In the adaptation to Italian, two annotation principles adopted for English, that is an orientation towards surface linguistic phenomena and the notion of minimal chunk for the tag extent, have been preserved without major modifications. The main differences with respect to the English version rely i.) in the attribute list; and ii.) in the attributes values. In Italian 12 core attributes apply with respect to the 10 attributes in English. The"
W11-0418,W01-1315,0,0.110355,"- <SIGNAL> tag) and various kind of dependencies between Events and/or Temporal Expressions (i.e. temporal, aspectual and subordination relations <TLINK>, <ALINK> and <SLINK> tags respectively). An ISO language-independent specification of TimeML is under development but it is still in the enquiry stage1. For this reason, in the following subsections we will mostly compare the Italian annotation guidelines with the latest version of the English annotation guidelines (TimeML Working group, 2010), focusing on the two main tags, i.e <EVENT> and <TIMEX3>, in Italian. previous annotations schemes (Katz and Arosio, 2001, Filatova and Hovy, 2001, Setzer and Gaizauskas, 2001 among other), TimeML allows for annotating as Events not only verbs but also nouns, adjectives and prepositional phrases. In the adaptation to Italian, two annotation principles adopted for English, that is an orientation towards surface linguistic phenomena and the notion of minimal chunk for the tag extent, have been preserved without major modifications. The main differences with respect to the English version rely i.) in the attribute list; and ii.) in the attributes values. In Italian 12 core attributes apply with respect to the 10 at"
W11-0418,W09-3417,0,0.0491587,"cuses on the evaluation of the annotation scheme on the Ita-TimeBank, formed by two corpora independently realized by applying the annotation specifications. Finally, in Section 5 conclusions and extensions to the current annotation effort will be reported. Notice that, for clarity's sake, in this paper the examples will focus only on the tag (or attribute or link) under discussion. Applying an annotation scheme to a language other than the one for which it was initially developed, requires a careful study of the language specific issues related to the linguistic phenomena taken into account (Im et al., 2009; Bittar, 2008). TimeML focuses on Events (i.e. actions, states, and processes - <EVENT> tag), Temporal Expressions (i.e. durations, calendar dates, times of day and sets of time <TIMEX3> tag), Signals (e.g. temporal prepositions and subordinators - <SIGNAL> tag) and various kind of dependencies between Events and/or Temporal Expressions (i.e. temporal, aspectual and subordination relations <TLINK>, <ALINK> and <SLINK> tags respectively). An ISO language-independent specification of TimeML is under development but it is still in the enquiry stage1. For this reason, in the following subsections"
W11-0418,C98-1013,0,\N,Missing
W11-0418,S10-1010,1,\N,Missing
W11-0418,J08-4004,0,\N,Missing
W11-1720,baccianella-etal-2010-sentiwordnet,0,0.0453724,"Missing"
W11-1720,balahur-etal-2010-sentiment,0,0.0579485,"Missing"
W11-1720,baroni-etal-2004-introducing,0,0.0456845,"Missing"
W11-1720,W10-1801,1,0.899672,"Missing"
W11-1720,C10-1021,0,0.198566,"f three sentences, namely the sentence containing the emotion keyword, the one preceding it and the sentence immediately following. As a justification for this choice, we have assumed that causes are a local focus discourse phenomenon and should not be found at a long distance with respect to their effects (i.e. the emotion keyword). Finally, the corpus is composed by 6,000 text snippets for a total of 738,558 tokens. The corresponding annotation scheme, ItEmoCause, is based on recommendations and previous experience in event annotation (ISOTimeML), emotion event annotation (Lee et al., 2009, Chen et al., 2010), emotion and affective computing annotation (EARL1, the HUMAINE Emotion Annotation and Representation Language, EmotiBlog, Boldrini et al, 2010). The scheme applies at two levels: phrase level and token level and it allows nested tags. Figure 1 reports the BNF description of the scheme. Text consuming markables are &lt;emotionWord&gt;, &lt;causePhrase&gt; and &lt;causeEmotion&gt; tags, which are responsible, respectively, for marking the emotion keyword, the phrase expressing the cause emotion event and the token expressing the cause emotion. The values of the attribute emotionClass is derived from Ekman 1 155"
W11-1720,W10-0206,0,0.128778,"xternal events (pre-events) (Wierzbicka, 1999). In addition to this, emotional states can also be the cause of events (post-events; Chun-Ren, 2010). This suggests to consider emotional states as a pivot and structure the relations between emotional states and related events as a tri-tuple of two pairs: (2) &lt;&lt;pre-events, emotional state&gt; &lt;emotional state, post-event&gt;&gt; This study focuses on the relationship between the first pair of the tri-tuple, namely pre-events (or ECE), and emotional states. Previous works on this task have been carried out for Chinese (Lee et al., 2009, Chen et al., 2009, Lee et al., 2010). ECE can be explicitly expressed as arguments, events, propositions, nominalizations and nominals. Lee et al (2010) restrict the definition of ECE as the immediate cause of the emotional state which does not necessarily correspond to the actual emotional state trigger or what leads to the emotional state. Their work considers all possible linguistic realization of EKs (nouns, verbs, adjectives, prepositional phrases) and ECEs. On the basis of an annotated corpus, correlations between emotional states and ECEs have been studied in terms of different linguistic cues (e.g. position of the cause"
W13-3816,E09-1005,0,0.020113,"uncts, and the semantic type of the complement filler(s). An example of an SFS is reported in example 1. The alignment of senses is based on the notion of lexical overlap. We used the Text::Similarity v.0.09 module2 to obtain the overlap value between two bags of words. Text similarity is based on counting the number of overlapping tokens between the two strings, normalized by the length of the strings. 4.2 Semantic Match: Exploiting Shallow Frames Structures Sense Similarity In the second approach, Sense Similarity, the basis for sense alignment is the Personalized Page Rank (PPR) algorithm (Eneko and Soroa, 2009) relying on a lexical-semantic knowledge base model as a graph G = (V, E) as available in the UKB tool suite3 . As knowledge base we have used WN 3.0 extended with the “Princeton Annotated Gloss Corpus”. Each vertex v of the graph is a 1. Marco ha comprato un libro. [Marco bought a book.] Verb: comprare [to buy] SFS: SUBJ[person] OBJ[artifact] To obtain the SFSs, two different strategies have been used. For the SCL, we have extracted all 1 http://wordnet.princeton.edu/glosstag.shtml 2 http://www.d.umn.edu/∼tpederse/text-similarity.html 3 http://ixa2.si.ehu.es/ukb/ 4 35 We use Google Translate"
W13-3816,jezek-quochi-2010-capturing,0,0.293585,"Missing"
W13-3816,lenci-etal-2012-lexit,0,0.217714,"Missing"
W13-3816,Q13-1013,0,0.198285,"ssing tasks, such as Word Sense Disambiguation, Information Extraction, and Question-Answering, among others. The creation of lexical-sematic resources is costly in terms of manual efforts and time, and often important information is scattered in different lexica and difficult to use. Semantic interoperability between resources could represent the viable solution to allow reusability and develop more robust and powerful resources. Word sense alignment (WSA), a research area which has seen an increasing interest in recent years, qualifies as the preliminary requirement for achieving this goal (Matuschek and Gurevych, 2013). The purpose of this work is to merge two Italian lexical-semantic resources, namely MultiWordNet (Pianta et al., 2002) (MWN) and Senso Comune Lexicon (SCL) (Oltramari et al., 2013), by automatically linking their entries. The final result will be two-folded. On the MWN side, this will provide Italian with a more complete and robust version of this lexicon. On the SCL side, the linking with MWN entries will introduce lexical-semantic relations, thus facilitating its use 2 Task and Resources Following (Matuschek and Gurevych, 2013), WSA can be defined as the identification of pairs of senses f"
W13-3816,N07-1025,0,0.0375074,"o et al., 2002). Ontological classification of verb entries will start in the near future. With respect to MWN, word senses are not hierarchically structured and no semantic relation has been encoded yet. Senses of polysemous entries have a flat representation, one following the other. 3 Related Works Previous works in WSA can be divided into two main groups: a.) approaches and frameworks which aim at linking entries to WN from lexica based on different models (Rigau and Agirre (1995); Navigli (2006); Roventini et al. (2007)) or language resources, such as Wikipedia (RuizCasado et al. (2005); Mihalcea (2007); Niemann and Gurevych (2011)), and b.) approaches towards the merging of different language resources (Navigli and Ponzetto (2012)). Our work clearly fits into the first group. While different methods are employed (similarity-based approaches vs. graphbased approaches), common elements of these works are: i.) the extensive use of the lexical knowledge of the sense descriptions; e.g. the WN glosses or an article first paragraph as in the case of Wikipedia; and ii.) the extension of the basic sense descriptions with additional information 4 Methodology The automatic alignment of senses has been"
W13-3816,N09-2066,0,0.0981197,"Missing"
W13-3816,P06-1014,0,0.0816085,"op level ontology is inspired by DOLCE (Descriptive Ontology for Linguistic and Cognitive Engineering) (Masolo et al., 2002). Ontological classification of verb entries will start in the near future. With respect to MWN, word senses are not hierarchically structured and no semantic relation has been encoded yet. Senses of polysemous entries have a flat representation, one following the other. 3 Related Works Previous works in WSA can be divided into two main groups: a.) approaches and frameworks which aim at linking entries to WN from lexica based on different models (Rigau and Agirre (1995); Navigli (2006); Roventini et al. (2007)) or language resources, such as Wikipedia (RuizCasado et al. (2005); Mihalcea (2007); Niemann and Gurevych (2011)), and b.) approaches towards the merging of different language resources (Navigli and Ponzetto (2012)). Our work clearly fits into the first group. While different methods are employed (similarity-based approaches vs. graphbased approaches), common elements of these works are: i.) the extensive use of the lexical knowledge of the sense descriptions; e.g. the WN glosses or an article first paragraph as in the case of Wikipedia; and ii.) the extension of the"
W13-3816,W11-0122,0,0.405855,"Ontological classification of verb entries will start in the near future. With respect to MWN, word senses are not hierarchically structured and no semantic relation has been encoded yet. Senses of polysemous entries have a flat representation, one following the other. 3 Related Works Previous works in WSA can be divided into two main groups: a.) approaches and frameworks which aim at linking entries to WN from lexica based on different models (Rigau and Agirre (1995); Navigli (2006); Roventini et al. (2007)) or language resources, such as Wikipedia (RuizCasado et al. (2005); Mihalcea (2007); Niemann and Gurevych (2011)), and b.) approaches towards the merging of different language resources (Navigli and Ponzetto (2012)). Our work clearly fits into the first group. While different methods are employed (similarity-based approaches vs. graphbased approaches), common elements of these works are: i.) the extensive use of the lexical knowledge of the sense descriptions; e.g. the WN glosses or an article first paragraph as in the case of Wikipedia; and ii.) the extension of the basic sense descriptions with additional information 4 Methodology The automatic alignment of senses has been conducted by applying three"
W13-3816,W03-1022,0,0.171912,"Missing"
W13-3816,pianta-etal-2008-textpro,0,0.0119654,"een conducted by applying three approaches Lexical Match, Sense Similarity and Semantic Match. 4.1 Lexical Match In the first approach, Lexical Match, for each word w and for each sense s in the given resources R ∈ {MWN, SCL} we constructed a sense description 34 dR (s) as a bag of words in Italian. Provided the different characteristics of the two resources, two different types of bag of words have been built. As for the SCL, the bag of words is represented by the lexical items in the textual definition of sw , automatically lemmatized and part-of-speech analyzed with the TextPro tool suite (Pianta et al., 2008) with standard stopword removal. On the other hand, for each synset, S, the sense description of each MWN synset was built by optionally exploiting: synset, and the edges represent semantic relations between synsets (e.g. hyperonymy, hyponymy, etc.). The PPR algorithm ranks the vertices in a graph according to their importance within the set and assigns stronger initial probabilities to certain kinds of vertices in the graph. The result of the PPR algorithm is a vector whose elements denote the probability for the corresponding vertex that a jumper ends on that vertex if randomly following the"
W13-3816,P07-2041,0,0.0199492,"y is inspired by DOLCE (Descriptive Ontology for Linguistic and Cognitive Engineering) (Masolo et al., 2002). Ontological classification of verb entries will start in the near future. With respect to MWN, word senses are not hierarchically structured and no semantic relation has been encoded yet. Senses of polysemous entries have a flat representation, one following the other. 3 Related Works Previous works in WSA can be divided into two main groups: a.) approaches and frameworks which aim at linking entries to WN from lexica based on different models (Rigau and Agirre (1995); Navigli (2006); Roventini et al. (2007)) or language resources, such as Wikipedia (RuizCasado et al. (2005); Mihalcea (2007); Niemann and Gurevych (2011)), and b.) approaches towards the merging of different language resources (Navigli and Ponzetto (2012)). Our work clearly fits into the first group. While different methods are employed (similarity-based approaches vs. graphbased approaches), common elements of these works are: i.) the extensive use of the lexical knowledge of the sense descriptions; e.g. the WN glosses or an article first paragraph as in the case of Wikipedia; and ii.) the extension of the basic sense descriptions"
W13-5406,bouillon-etal-2002-acquisition,0,0.10668,"Missing"
W13-5406,W91-0217,0,0.438329,"t etc.) of an object. The actual realization of each role is also dependent on the associated semantic type (or ontological class(es)) of the entity analyzed. For instance, an entity denoting a Natural Object (e.g. tree, flower, fruit, etc.) will never have information for the Agentive role. On the contrary, this information is relevant for Artifacts (wheel, pen, table, etc.). The qualia extraction task has been mainly addressed in NLP by means of pattern-based approaches on corpora and from dictionaries. Pattern-based approaches for extracting semantic relations are well known in literature (Calzolari (1991); Montemagni and Vanderwende (1992); Hearst (1992); Bouillon et al. (2002); Cimiano and Wenderoth (2005); Pantel and Pennacchiotti (2006), among others) and have proved highly reliable, namely in terms of precision, for extracting the different types of qualia. One the advantages of our work is that the extracted qualia are associated with both a word sense and an ontological class (see Section 3 for details on the SCDM Dictionary). Furthermore, the SCDM dictionary glosses are richer and more descriptive than the WordNet glosses. The data collected can be exploited in different ways, namely: •"
W13-5406,W05-1004,0,0.141024,"d semantic type (or ontological class(es)) of the entity analyzed. For instance, an entity denoting a Natural Object (e.g. tree, flower, fruit, etc.) will never have information for the Agentive role. On the contrary, this information is relevant for Artifacts (wheel, pen, table, etc.). The qualia extraction task has been mainly addressed in NLP by means of pattern-based approaches on corpora and from dictionaries. Pattern-based approaches for extracting semantic relations are well known in literature (Calzolari (1991); Montemagni and Vanderwende (1992); Hearst (1992); Bouillon et al. (2002); Cimiano and Wenderoth (2005); Pantel and Pennacchiotti (2006), among others) and have proved highly reliable, namely in terms of precision, for extracting the different types of qualia. One the advantages of our work is that the extracted qualia are associated with both a word sense and an ontological class (see Section 3 for details on the SCDM Dictionary). Furthermore, the SCDM dictionary glosses are richer and more descriptive than the WordNet glosses. The data collected can be exploited in different ways, namely: • to reduce the complexity of the lexicographic entries, thus facilitating dictionary entry merging and s"
W13-5406,N03-1011,0,0.0700085,"Missing"
W13-5406,C92-2082,0,0.200257,"role is also dependent on the associated semantic type (or ontological class(es)) of the entity analyzed. For instance, an entity denoting a Natural Object (e.g. tree, flower, fruit, etc.) will never have information for the Agentive role. On the contrary, this information is relevant for Artifacts (wheel, pen, table, etc.). The qualia extraction task has been mainly addressed in NLP by means of pattern-based approaches on corpora and from dictionaries. Pattern-based approaches for extracting semantic relations are well known in literature (Calzolari (1991); Montemagni and Vanderwende (1992); Hearst (1992); Bouillon et al. (2002); Cimiano and Wenderoth (2005); Pantel and Pennacchiotti (2006), among others) and have proved highly reliable, namely in terms of precision, for extracting the different types of qualia. One the advantages of our work is that the extracted qualia are associated with both a word sense and an ontological class (see Section 3 for details on the SCDM Dictionary). Furthermore, the SCDM dictionary glosses are richer and more descriptive than the WordNet glosses. The data collected can be exploited in different ways, namely: • to reduce the complexity of the lexicographic ent"
W13-5406,W09-0205,0,0.060268,"Missing"
W13-5406,P08-2047,0,0.0726717,"Missing"
W13-5406,Q13-1013,0,0.0178622,"distinct the linguistic and the conceptual levels of representation, can be exploited in order to start merging the two resources. The working hypothesis is that the ontologies of the two lexica can be merged together by means of equivalence relations and subsumption. This will allow us to have sets of ontologically compatible entries which can be further aligned for word senses (word sense alignment; WSA) by means of different methods, such as lexical match on the glosses (Niemann and Gurevych (2011); Meyer and Gurevych (2011)), exploitation of qualia information and graph-based approaches (Matuschek and Gurevych (2013); Navigli and Ponzetto (2012)). At this stage of development, we partially tackled the task of aligning the ontological models of the two lexica by restricting our analysis of the PSC semantic types to those which are compatible with or equivalent to the SCDM semantic type ARTIFACT, namely Instrument and Artifact. Notice that we excluded the PSC types Artifactual food and Artifactual drink which in SCDM are assigned to the type SUBSTANCE. We then extracted all qualia information for the 46 lemmas in the Gold Standard which have a corresponding lemma entry and ontological class Artifact or Inst"
W13-5406,I11-1099,0,0.0255977,"ological models, which inform the semantic typing of the word senses and contribute to keep distinct the linguistic and the conceptual levels of representation, can be exploited in order to start merging the two resources. The working hypothesis is that the ontologies of the two lexica can be merged together by means of equivalence relations and subsumption. This will allow us to have sets of ontologically compatible entries which can be further aligned for word senses (word sense alignment; WSA) by means of different methods, such as lexical match on the glosses (Niemann and Gurevych (2011); Meyer and Gurevych (2011)), exploitation of qualia information and graph-based approaches (Matuschek and Gurevych (2013); Navigli and Ponzetto (2012)). At this stage of development, we partially tackled the task of aligning the ontological models of the two lexica by restricting our analysis of the PSC semantic types to those which are compatible with or equivalent to the SCDM semantic type ARTIFACT, namely Instrument and Artifact. Notice that we excluded the PSC types Artifactual food and Artifactual drink which in SCDM are assigned to the type SUBSTANCE. We then extracted all qualia information for the 46 lemmas in"
W13-5406,C92-2083,0,0.289151,"ct. The actual realization of each role is also dependent on the associated semantic type (or ontological class(es)) of the entity analyzed. For instance, an entity denoting a Natural Object (e.g. tree, flower, fruit, etc.) will never have information for the Agentive role. On the contrary, this information is relevant for Artifacts (wheel, pen, table, etc.). The qualia extraction task has been mainly addressed in NLP by means of pattern-based approaches on corpora and from dictionaries. Pattern-based approaches for extracting semantic relations are well known in literature (Calzolari (1991); Montemagni and Vanderwende (1992); Hearst (1992); Bouillon et al. (2002); Cimiano and Wenderoth (2005); Pantel and Pennacchiotti (2006), among others) and have proved highly reliable, namely in terms of precision, for extracting the different types of qualia. One the advantages of our work is that the extracted qualia are associated with both a word sense and an ontological class (see Section 3 for details on the SCDM Dictionary). Furthermore, the SCDM dictionary glosses are richer and more descriptive than the WordNet glosses. The data collected can be exploited in different ways, namely: • to reduce the complexity of the le"
W13-5406,W11-0122,0,0.0241954,"scribed in Section 3. The ontological models, which inform the semantic typing of the word senses and contribute to keep distinct the linguistic and the conceptual levels of representation, can be exploited in order to start merging the two resources. The working hypothesis is that the ontologies of the two lexica can be merged together by means of equivalence relations and subsumption. This will allow us to have sets of ontologically compatible entries which can be further aligned for word senses (word sense alignment; WSA) by means of different methods, such as lexical match on the glosses (Niemann and Gurevych (2011); Meyer and Gurevych (2011)), exploitation of qualia information and graph-based approaches (Matuschek and Gurevych (2013); Navigli and Ponzetto (2012)). At this stage of development, we partially tackled the task of aligning the ontological models of the two lexica by restricting our analysis of the PSC semantic types to those which are compatible with or equivalent to the SCDM semantic type ARTIFACT, namely Instrument and Artifact. Notice that we excluded the PSC types Artifactual food and Artifactual drink which in SCDM are assigned to the type SUBSTANCE. We then extracted all qualia inform"
W13-5406,P07-2041,0,0.0565207,"Missing"
W13-5406,W09-1122,0,0.149649,"re research directions. 2 Related Works In recent years there has been a continuous interest in the NLP community on discovering novel instances of semantic relations. Most of this earlier work was based on surface pattern matching (Hearst (1998); Cimiano and Wenderoth (2005); Yamada and Baldwin (2004) among others). Other works start from matches extracted with this method and then use supervised training data to learn semantic constraints to improve precision (Girju et al. (2003); Katrenko and Adriaans (2010)). Much of previous works concentrated on extracting hypernyms (Snow et al. (2005); Sang and Hofmann (2009). Other works have applied pattern classification approaches to extract larger set of relations. The results obtained proved that extracting a pattern distribution between occurrences and performing supervised classification based on this distribution is a viable and promising solution for extending the range of semantic rela´ S´eaghdha and Coptions beyond hyperonymy (O stake (2007); Herda˘gdelen and Baroni (2009)). With respect to previous research and similarly to what was done in the ACQUILEX Project, we tackle the task of extracting qualia relation from dictionary glosses. However, the SCD"
W13-5406,P07-3013,0,0.0675735,"Missing"
W13-5406,P06-1015,0,0.0237929,"l class(es)) of the entity analyzed. For instance, an entity denoting a Natural Object (e.g. tree, flower, fruit, etc.) will never have information for the Agentive role. On the contrary, this information is relevant for Artifacts (wheel, pen, table, etc.). The qualia extraction task has been mainly addressed in NLP by means of pattern-based approaches on corpora and from dictionaries. Pattern-based approaches for extracting semantic relations are well known in literature (Calzolari (1991); Montemagni and Vanderwende (1992); Hearst (1992); Bouillon et al. (2002); Cimiano and Wenderoth (2005); Pantel and Pennacchiotti (2006), among others) and have proved highly reliable, namely in terms of precision, for extracting the different types of qualia. One the advantages of our work is that the extracted qualia are associated with both a word sense and an ontological class (see Section 3 for details on the SCDM Dictionary). Furthermore, the SCDM dictionary glosses are richer and more descriptive than the WordNet glosses. The data collected can be exploited in different ways, namely: • to reduce the complexity of the lexicographic entries, thus facilitating dictionary entry merging and sense alignment with other lexica"
W13-5406,pianta-etal-2008-textpro,0,0.0284203,"nd additional missing patterns, we developed a manually annotated gold standard. We selected 46 nominal entries in the SCDM lexicon with at least one sense associated with the ontological type ARTIFACT. This has provided us with a set of 50 different senses and a total of 173 different qualia, namely 79 for the constitutive role; 3 for the agentive role; 46 for the telic role; and 45 for the formal role. None of the entries in the Gold Standard is part of the development set described above. We automatically analyzed part-of-speech and lemmas in the glosses by means of the TextPro tool suite (Pianta et al., 2008), applied the pattern extraction script and then evaluated with respect to the Gold Standard. The results are reported in Table 2; all measures have been computed in terms of Precision (P), Recall (R) and F-measure (F1). We evaluated the reliability of the patterns both globally (Overall Evaluation) and for each qualia. Evaluation Type Overall Evaluation Agentive Formal Telic Constitutive P 0.84 1 1 0.92 0.73 R 0.08 0.5 0.01 0.16 0.07 F1 0.14 0.66 0.02 0.27 0.12 Table 2: Evaluation of the patterns with respect to the Gold Standard. The results obtained are quite satisfactory. Precision is extr"
W13-5406,Y04-1012,0,0.023871,"qualia information in the PSC lexicon to identify additional qualia which were not extracted by means of the patterns; and ii.) to evaluate the coverage of the extracted qualia with respect to the entries in the PSC lexicon in order to enrich it. Finally, Section 5 reports on conclusions and highlights on-going and future research directions. 2 Related Works In recent years there has been a continuous interest in the NLP community on discovering novel instances of semantic relations. Most of this earlier work was based on surface pattern matching (Hearst (1998); Cimiano and Wenderoth (2005); Yamada and Baldwin (2004) among others). Other works start from matches extracted with this method and then use supervised training data to learn semantic constraints to improve precision (Girju et al. (2003); Katrenko and Adriaans (2010)). Much of previous works concentrated on extracting hypernyms (Snow et al. (2005); Sang and Hofmann (2009). Other works have applied pattern classification approaches to extract larger set of relations. The results obtained proved that extracting a pattern distribution between occurrences and performing supervised classification based on this distribution is a viable and promising so"
W14-0140,baroni-etal-2004-introducing,0,0.0434723,"Missing"
W14-0140,E09-1005,0,0.0118341,"English). In BabelNet English WN 3.0 synsets have been aligned to their corresponding Wikipedia pages and then extended to other languages, including Italian, by exploiting Wikipedia language links and WN mappings. As for our task, we have retained only those BabelNet entries which have a corresponding synset word in MWN. In this way, we have extended the bag of words representation of nominal entries for MWN synsets by adding the Italian Wikipedia glosses from BabelNet. 4.2 In the second approach, Sense Similarity, the basis for sense alignment is the Personalized Page Rank (PPR) algorithm (Eneko and Soroa, 2009) relying on a lexical-semantic knowledge base model as a graph G = (V, E) as available in the UKB tool suite4 . As knowledge base we have used WN 3.0 extended with the “Princeton Annotated Gloss Corpus”. Each vertex v of the graph is a synset, and the edges represent semantic relations between synsets (e.g. hyperonymy, hyponymy, etc.). The PPR algorithm ranks the vertices in a graph according to their importance within the set and assigns stronger initial probabilities to certain kinds of vertices in the graph. The result of the PPR algorithm is a vector whose elements denotes the probability"
W14-0140,E12-1059,0,0.0264903,"Missing"
W14-0140,jezek-quochi-2010-capturing,0,0.0331949,"Missing"
W14-0140,Q13-1013,0,0.534544,"ty, for word sense alignment between MultiWordNet and a lexicographic dictionary, Senso Comune De Mauro, when having few sense descriptions (MultiWordNet) and no structure over senses (Senso Comune De Mauro). The results obtained from the merging of the two approaches are satisfying, with F1 values of 0.47 for verbs and 0.64 for nouns. 1 Introduction This work is situated in the field of word sense alignment, a research area which has seen an increasing interest in recent years and which is a key requirement for achieving semantic interoperability between different lexical-semantic resources (Matuschek and Gurevych, 2013). Our goal is to automatically import high-quality glosses in Italian in MultiWordNet (Pianta et al., 2002) (MWN) by aligning its synsets to the entries of a lexicographic dictionary, namely the Senso Comune De Mauro (SCDM), thus providing Italian with a more complete and robust version of MWN. For SCDM, the linking of the entries with MWN plays a double role. On the one hand, it will introduce lexical-semantic relations, thus facilitating its use for NLP tasks in Italian, and, on the other hand, it will make SCDM a structurally and semantically interoperable resource for Italian, to which oth"
W14-0140,I11-1099,0,0.0271442,"Missing"
W14-0140,N07-1025,0,0.251904,"Missing"
W14-0140,P06-1014,0,0.0964714,"ering) (Masolo et al., 2002). All nominal entries have been manually classified according to the ontological concepts and an ontological classification of verb entries will start in the near future. With respect to MWN, word senses are not hierarchically structured and no semantic relation is encoded. Senses of polysemous entries have a flat representation, one following the other. 3 Related Works Previous works in word sense alignment can be divided into two main groups: a.) approaches and frameworks which aim at linking lexica based on different models to WN synsets (Rigau and Eneko (1995); Navigli (2006); Roventini et al. (2007)) or language resources, such as Wikipedia (RuizCasado et al. (2005); Mihalcea (2007); Niemann and Gurevych (2011)), and b.) approaches towards the merging of different language resources (Gurevych et al. (2012); Navigli and Ponzetto (2012)). Our work clearly fits into the first group. While different methods are employed (similaritybased approaches vs. graph-based approaches), common elements of these works are: i.) the extensive use of lexical knowledge based on the sense descriptions such as the WN glosses or an article first paragraph as in the case of Wikipedia; a"
W14-0140,W11-0122,0,0.276737,"Missing"
W14-0140,pianta-etal-2008-textpro,0,0.019392,"ying two approaches for constructing the sense representations of the resources and evaluation. 4.1 Lexical Match In the first approach, Lexical Match, for each word w and for each sense s in the given resources R ∈ {MWN, SCDM} we constructed a sense descriptions dR (s) as a bag of words in Italian. Provided the different characteristics of the two resources, two different types of bag of words have been built. As for the SCDM, the bag of words is represented by the lexical items in the textual definition of sw , automatically lemmatized and partof-speech analyzed with the TextPro tool suite (Pianta et al., 2008) with standard stopword removal. On the other hand, for each synset, S, and for each part of speech in analysis, the sense description of each MWN synset was built by optionally exploiting: • the set of synset words in a synset excluding w; • the set of direct hypernyms of s in the taxonomy hierarchy in MWN; • the set of synset words in MWN standing in the relation of nearest synonyms with s; • the set of synset words in MWN composing the manually disambiguated glosses of s from the “Princeton Annotated Gloss Corpus”2 . To extract the corresponding Italian synset(s), we have ported MWN to WN 3"
W14-0140,P07-2041,0,0.365143,"Missing"
W15-4507,P98-1013,0,0.0199579,"lines The timeline extraction is obtained from an NLP pipeline that has been developed in the NewsReader project4 . The pipeline applies a cascade of modules, ranging from tokenization up to temporal and causal relation extraction, to documents (mention level). Next, it generates a semantic representation of the content in SEM (instance level). The NLP modules generate representations of entities mentioned in the text with possible links to DBpedia URIs, time expressions normalized to dates and a semantic role representation with events and participants linked to FrameNet frames and elements (Baker et al., 1998). Furthermore, coreference relations are created to bind participants and events to instances within each document. The NLP modules interpret mentions in the text, i.e. at single document level. However, given a set of documents or a corpus, these mention based representations are combined resolving cross-document coreference for entities and events, anchoring events to time and aggregating event-participant relations and generating an instance level representation. Details about this process can be found in (Agerri et al., 2014). The timeline representation anchors events either to a time anc"
W15-4507,P09-1068,0,0.221452,"t which distinguishes these works concerns the type of datasets, i.e., fictitious or news documents, used or referred to for the storyline generation or modelization. Although such differences are less relevant for the development of models, they are important for the development of systems. Furthermore, the task of storyline extraction is multidisciplinary, concerning different fields such as Multi Document Summarization, Temporal Processing, Topic Detection and Tracking. What follows is a selection of previous works which we consider more strictly related to our work. Chambers and Jurafsky (Chambers and Jurafsky, 2009) extended previous work on the identification of “event narrative chains”, i.e., sets of partially ordered events that involve the same shared participant. They propose an unsupervised method to learn narrative schemas, i.e. coherent sequences of events whose arguments are filled with participants’ semantic roles. The approach can be applied to all text types. The validity of the extracted narrative schemas (event and associated participants) have been evaluated against FrameNet and on a narrative cloze task: a variation of the cloze task defined by (Taylor, 1953). The narrative schema propose"
W15-4507,D13-1127,0,0.0291879,"must be based, at least, on two aspects: informativeness and interest. A good storyline is a storyline which interest the user, provides all relevant and necessary information with respect to a target entity, and it is coherent. We envisage two types of evaluation: direct and indirect. Direct evaluation necessarily needs human interaction. This can be achieved in two methods: using experts and using crowdsourcing techniques. Experts can evaluate the data provided with the storylines with respect to a set of reference documents and check the informativeness and coherence parameters. Following (Xu et al., 2013), two types of questions can be addressed at the microlevel and at the macro-level of knowledge. Both evaluation types address the quality of the generated storylines. The former addresses the efficiency of the storylines in retrieving the information while the latter addresses the quality of the storylines with respect to a certain topic (e.g. the commercial “war” between Boeing and Airbus). Concerning metrics, micro-knowledge can be measured by the time the users need to gather the information, while the macro-knowledge can be measured as the text proportion, i.e. how many sentences of the s"
W15-4507,D13-1068,0,0.0431174,"orpus and then validate if the identified events correlate with the climax events of the storylines. Indirect evaluation can be based on a crossdocument Summarization tasks. The ideal situation is the one in which the storyline contains the most salient and related events. These sets of data can be used either to recover the sentences in a collection of documents and generate an extractive summary (story) or used to produce an abstractive summary. Summarization measures such as ROUGE can then be used to evaluate the quality of summaries and, indirectly, of the storylines (Nguyen et al., 2014; Huang and Huang, 2013; Erkan and Radev, 2004). pants, e.g. Airbus and Airbus 380 are not considered as standing in a co-participation relation by our system because they have different URIs. In other cases, we see more or less the opposite: a storyline reporting on journeys by Boeing is interrupted by a plane crash from Airbus due to overgenerated bridging relations. What is the optimal combination of features still needs to be determined empirically. For this we need a data set, which we will discuss in the next subsection. 4.3 Benchmarking and evaluation In this phase we are not yet able to provide an extensive"
W15-4507,P98-2127,0,0.205584,"Missing"
W15-4507,P09-1025,0,0.00822154,"Missing"
W15-4507,C98-1013,0,\N,Missing
W15-4507,S15-2132,0,\N,Missing
W15-4507,C98-2122,0,\N,Missing
W15-4507,C14-1114,0,\N,Missing
W16-2819,bartalesi-lenzi-etal-2012-cat,0,0.0180456,". 169 sentences (n) 150 Round 1 139 91 100 Round 2 83 42 47 50 42 43 32 14 0 1 Task 1: Pilot annotation 2 3 4 annotators (n) 5 Figure 2: Distribution of annotations. This section reports on a pilot annotation experiment targeted at the first subtask. Five expert annotators were asked to identify those sentences in the editorial article that were COMMENTED UPON in the comment. A set of eight editorial articles (152 unique sentences, including titles) and a total of 62 comments were provided. In total, this came down to 1,186 sentences to be annotated. We used the Content Annotation Tool (CAT) (Lenzi et al., 2012) for the annotations. The experiment was performed in two rounds. First, simple instructions were given to the annotators to explore the data and task. For the second round, the instructions were refined by adding two simple rules: exclude titles (they are part of the meta-data), and include cases where a proposition is simply ‘mentioned’ rather than functioning as part of the argumentation. For example, the fact that the closing of Sweet Briar College is repeated in the comment below without its factual status beA deeper analysis of the annotated data and the annotation distribution shows the"
W16-2819,W10-0214,0,0.0249339,"itly mentioned or at least supposed opponent, as for instance in the rebutting of possible objections.” Therefore, these (implicit) interactions between participants should be given a central role when performing argument mining. In recent years, several studies have addressed the annotation and automatic classification of agreement and disagreement in online debates. The main difference between them is the annotation unit they have targeted, i.e. the textual units that are in (dis)agreement. Some studies focused on global (dis)agreement, i.e. the overall stance towards the main debate topic (Somasundaran and Wiebe, 2010). Other studies focused on local (dis)agreement, comparing pairs of posts (Walker 1 http://argmining2016.arg.tech/index. php/home/call-for-papers/unshared-task 160 Proceedings of the 3rd Workshop on Argument Mining, pages 160–165, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics ernment to reduce teen pregnancies?) and article title describing the author’s stance (e.g. Publicly Funded Birth Control Is Crucial); and ii) Discussions (i.e. collections of comments from different users) about these editorial articles (Variant D). The remainder of this paper is st"
W16-2819,L16-1187,1,0.846655,"Missing"
W16-2819,walker-etal-2012-corpus,0,0.0611907,"Missing"
W16-2819,W14-2617,0,0.0324304,"Missing"
W16-2819,W12-3710,0,0.0423338,"Missing"
W16-2819,andreas-etal-2012-annotating,0,\N,Missing
W16-5708,P15-2137,0,0.0160943,"ifficult challenges for the selection and extraction of relevant information. Relevant information can be missed in this vast amount of data, leading to inconsistencies, fragmented reports, or gaps in the extraction and representation of complex stories. Different solutions have been proposed to deal with this problem ranging from the generation of multi-document extractive summaries (Barzilay et al., 1999), to clustering of news with respect to a topic (Swan and Allan, 2000), to the generation of timelines to monitor relevant events in a topic (Shahaf and Guestrin, 2010; Nguyen et al., 2014; Bauer and Teufel, 2015). In this work, we want to expand on a different approach to select, organize, and represent relevant information from collections of documents clustered around a specific topic. Following Vossen et al. (2015), we adopt the storyline model as a representational device to structure the information, and Previous work in storyline generation is limited and in most cases what is labelled as a storyline is a timeline. The main difference is that storylines and narrative structures exhibit some causal and explanatory relation between events and some tension towards a resolution, or climax. We have i"
W16-5708,P14-2082,0,0.0389167,"limiting inferences. As such, no temporal relation should be annotated on the basis of world knowledge only. 1 https://goo.gl/iWUCFr Furthermore, the set of temporal values has been limited to 8 values (BEFORE, AFTER, OVERLAP, BEFORE OVERLAP, BEGINS ON, ENDS ON, SIMULTANEOUS, INCLUDES). We also annotate temporal relations between events and the Document Creation Time (DCT). The DCT represents a special temporal anchor for actions which expresses a broad temporal dimension (e.g. Present, Past, or Future with respect the time the author created and published the text). Following the proposal in Cassidy et al. (2014), we also annotate transitive closure relations between pairs of events to develop highly connected event graph. This means that in case of two pairs of events A BEFORE B and B BEFORE C, we explicitly mark the transitive closure relation A BEFORE C. Finally, we extend the TLINK tag with the attribute contextualModality, from the RED scheme. It has 4 values: ACTUAL, UNCERTAIN, HYPOTHETICAL, and GENERIC. The attribute allows to represent claims of different sources concerning the reality or certainty of a temporal relation. The assignment of the contextual modality values is connected to the fac"
W16-5708,P09-1068,0,0.0230816,"Missing"
W16-5708,cybulska-vossen-2014-using,1,0.831487,"oints (settings); • the anchoring of events to time and their ordering (a timeline); • bridging relations: a set of relations between events with explanatory and predictive 68 value(s). The proposed annotation scheme aims at grounding these concepts to linguistic elements in document collections. The scheme has been developed to maximize compatibility with existing annotation efforts on event and temporal processing, such as the Richer Event Description (RED) 1 , THYME (Styler IV et al., 2014), and TimeML (Pustejovsky et al., 2003a), and event coreference, such as the Event CorefBank+ (ECB+) (Cybulska and Vossen, 2014b). 2.1 STaR: The Storyline Annotation and Representation Scheme The Storyline Annotation and Representation Scheme (StaR) builds on and extends the ECB+ annotation scheme (Cybulska and Vossen, 2014a). The ECB+ scheme addresses event coreference both at the in- and cross-document levels. Event action coreference is specified as two action mentions which occur/hold true: i.) at the same time; ii.) in the same location; and iii.) with the same actors/participants. Thus, ECB+ data provides access to the first basic elements of the storyline model, i.e., events, participants (actors), locations, a"
W16-5708,D13-1068,0,0.0123716,"h to select, organize, and represent relevant information from collections of documents clustered around a specific topic. Following Vossen et al. (2015), we adopt the storyline model as a representational device to structure the information, and Previous work in storyline generation is limited and in most cases what is labelled as a storyline is a timeline. The main difference is that storylines and narrative structures exhibit some causal and explanatory relation between events and some tension towards a resolution, or climax. We have identified four main contributions (Shahaf et al., 2013; Huang and Huang, 2013; Hu et al., 2014; Laparra et al., 2015) in this area proposing methods to generate storyline datasets. Although each contribution proposes its own definition of storyline, based on the sharing of participants, time and location, one of the commonalities of these works consists of the use of interactions and connections between crossdocument topic threads or events which give rise to timelines, i.e. a basic temporal ordering. Storylines also differ from Narrative Schemas (Chambers and Jurafsky, 2009). Narrative Schemas qualify as sets of partially ordered events with no distinction in relevanc"
W16-5708,W15-4508,0,0.0149152,"evant information from collections of documents clustered around a specific topic. Following Vossen et al. (2015), we adopt the storyline model as a representational device to structure the information, and Previous work in storyline generation is limited and in most cases what is labelled as a storyline is a timeline. The main difference is that storylines and narrative structures exhibit some causal and explanatory relation between events and some tension towards a resolution, or climax. We have identified four main contributions (Shahaf et al., 2013; Huang and Huang, 2013; Hu et al., 2014; Laparra et al., 2015) in this area proposing methods to generate storyline datasets. Although each contribution proposes its own definition of storyline, based on the sharing of participants, time and location, one of the commonalities of these works consists of the use of interactions and connections between crossdocument topic threads or events which give rise to timelines, i.e. a basic temporal ordering. Storylines also differ from Narrative Schemas (Chambers and Jurafsky, 2009). Narrative Schemas qualify as sets of partially ordered events with no distinction in relevance or salience 67 Proceedings of 2nd Work"
W16-5708,C14-1114,0,0.0193278,"ng every day posing difficult challenges for the selection and extraction of relevant information. Relevant information can be missed in this vast amount of data, leading to inconsistencies, fragmented reports, or gaps in the extraction and representation of complex stories. Different solutions have been proposed to deal with this problem ranging from the generation of multi-document extractive summaries (Barzilay et al., 1999), to clustering of news with respect to a topic (Swan and Allan, 2000), to the generation of timelines to monitor relevant events in a topic (Shahaf and Guestrin, 2010; Nguyen et al., 2014; Bauer and Teufel, 2015). In this work, we want to expand on a different approach to select, organize, and represent relevant information from collections of documents clustered around a specific topic. Following Vossen et al. (2015), we adopt the storyline model as a representational device to structure the information, and Previous work in storyline generation is limited and in most cases what is labelled as a storyline is a timeline. The main difference is that storylines and narrative structures exhibit some causal and explanatory relation between events and some tension towards a resolut"
W16-5708,P16-1028,0,0.0125321,"lience 67 Proceedings of 2nd Workshop on Computing News Storylines, pages 67–72, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics of their elements and, most importantly, with no explanatory power of the ways events are connected together, except for precedence relations. A Narrative Schema looks like an un-prioritized set of events which share some participants, thus leading to the development of entity-centric timelines. Furthermore, the use of entity driven relations (e.g. co-participation) to generate the schemas often result in non-coherent chains of events (Peng and Roth, 2016). The remainder of this paper will be structured as follows: in Section 2 we will present the main aspects of the storyline model described in (Vossen et al., 2015) and show how these elements have been used to develop a proposal to annotate storylines. Section 3 will report on the preliminary application of the annotation scheme to a corpus presenting insights on the data and interaction between different layers of annotation ranging from event coreference to storyline. Finally, conclusions and future work will be reported in Section 4. 2 Annotating Storylines: A Proposal The model described"
W16-5708,W15-4507,1,0.934657,"ion and representation of complex stories. Different solutions have been proposed to deal with this problem ranging from the generation of multi-document extractive summaries (Barzilay et al., 1999), to clustering of news with respect to a topic (Swan and Allan, 2000), to the generation of timelines to monitor relevant events in a topic (Shahaf and Guestrin, 2010; Nguyen et al., 2014; Bauer and Teufel, 2015). In this work, we want to expand on a different approach to select, organize, and represent relevant information from collections of documents clustered around a specific topic. Following Vossen et al. (2015), we adopt the storyline model as a representational device to structure the information, and Previous work in storyline generation is limited and in most cases what is labelled as a storyline is a timeline. The main difference is that storylines and narrative structures exhibit some causal and explanatory relation between events and some tension towards a resolution, or climax. We have identified four main contributions (Shahaf et al., 2013; Huang and Huang, 2013; Hu et al., 2014; Laparra et al., 2015) in this area proposing methods to generate storyline datasets. Although each contribution p"
W17-2706,cybulska-vossen-2014-using,1,0.876168,"damaging-undergoer hasDamage damage hasNegativeEffectOn true damage activity 3 Evaluation Figure 3: Explicit chaining of event classes (left) and their shared properties in the pre, post and during situation (right). The CEO will be evaluated against a benchmark corpus to determine precision and recall for both the classes and the semantic circumstantial relations. For this, we plug the CEO into an existing NLP pipeline for text annotation and analysis (Vossen et al., 2016) For this, we are cur3 4 40 https://iptc.org/ https://www.w3.org/2004/02/skos/ rently annotating part of the ECB+ corpus (Cybulska and Vossen, 2014). We selected 24 topics that describe a calamity event. In our annotation, we only use the existing event mention annotations and add new mentions if they realize an event calamity class. In addition to this, the annotators define co-reference sets among event mentions and the semantic circumstantial relations. As such, we can evaluate what events are captured by our ontology and what relations can be successfully reconstructed. For the annotation, we use the CAT annotation tool (Bartalesi Lenzi et al., 2012). Additionally, we are designing a QuestionAnswering task, where systems will have to"
W17-2706,L16-1423,0,0.264153,"Missing"
W17-2706,L16-1233,1,0.576854,"Missing"
W17-2706,bartalesi-lenzi-etal-2012-cat,0,0.309905,"Missing"
W17-2706,W09-1206,0,0.393581,"Missing"
W17-2706,chambers-jurafsky-2010-database,0,0.626859,", C implies D or D is implied by C. The implication is however not necessary. Previous work on the encoding of semantic relations between event pairs has focused on specific subsets of circumstantial relations. For instance, one example is the encoding of the entailment relations in WordNet (Fellbaum, 1998). With respect to the WordNet approach in this work, we abstract from various event types (i.e. lexical items) and do not depend on relations defined at a synset level by formalizing event knowledge and relations in an ontology. Another related approach are narrative chains as described in (Chambers and Jurafsky, 2010) that provide chains of various event mentions. However, the relation between these mentions is not specified explicitly but based on cooccurrence of participants and a basic precedence relation. Manual inspection of these chains revealed that dissimilar relations are implied within these chains, varying from temporal ordering, to episodic, up to causal. The Penn Discourse TreeBank (PDTB) (Prasad et al., 2007) annotates contingency relations, of which causal relations are a subclass. In PDTB, the focus of the annotation is between two Abstract Objects (called Arg1 and Arg2), corresponding to d"
W17-2706,W15-4507,1,\N,Missing
W17-2711,P13-4006,0,0.0295079,"tional Similarity (Jurgens et al., 2012). In particular, we extracted words pairs from the test set Phase1 Answers corresponding to class-8 (CAUSEPURPOSE), retaining only word pairs in the categories Cause:Effect, Cause:Compensatory Action, Action/Activity, and Prevention, where both words express events. This initial set of seed elements has been further extended by looking for “cause”, “enablement”, and “entails” relations in SUMO (Niles and Pease, 2001, 2003) and in WordNet (Miller, 1995). This resulted in a list of 1,609 unique seed pairs. PPMI has been computed using the DISSECT Toolkit (Dinu et al., 2013), and pair frequencies have been extracted from Google bigrams(Brants and Franz, 2006). Rather than identifying a unique threshold for eligible pairs, we looked for a range of PPMI values. Classification 0.744 0.638 Table 2: Inter-annotator agreement: Dice coefficient at token level. One of the most interesting observations on the PLOT LINK analysis is that the agreement may vary according to the type of seminal event. For instance, the highest agreement has been observed for T19: a shooting accident :Dice 0.723 for relation identification, and 0.728 for relation classification. The lowest agr"
W17-2711,J86-2003,0,0.0315621,"annotation framework. 2.1 Basic Components: Events and Temporal Expressions A storyline relation can be best described as a loose causal and temporal relation between a pair of event mentions, where one event mention explains/justifies the occurrence of the other event mention in the pair (more details are reported in Events and temporal expressions are the basic components of the annotation scheme for the ESC v0.9 dataset. The term “event” is used as a cover term to refer to any situations that can happen, occur, or hold. The use of the term event is a synonym to “eventuality” introduced by Bach (1986), covering both dynamic and static situations (i.e. events and states). The annotation of events in NLP is a topic that got a lot of interest and on which yet no consensus has been reached. In this work, we adopted a definition of events that is provided in the ECB+ Annotation Guidelines (Cybulska and Vossen, 2014a), which is compatible with definitions in ACE (Linguistic Data Consortium, 2005) and TimeML. In particular, an event is any punctual, durational, or stative situation which happens or holds, and which results from a combination of four components such as: 1) an action component refe"
W17-2711,W13-1202,1,0.897705,"Missing"
W17-2711,bartalesi-lenzi-etal-2012-cat,0,0.021003,"nging to the class ACTION ASPECTUAL, which functions as lexical morphosyntactic markers of the the internal temporal structure of a situation, and ACTION CAUSATIVE as meta-level event mentions, and thus they are excluded. 6 Event annotation is directly inherited from ECB+, where only sentences containing relevant mentions of the topic were annotated. 7 Note that this can be extended further using the crossdocument event coreference chains of ECB+ 81 3 The annotation of the ESC v0.9 corpus has been conducted by 2 experts following a multistep process and using the web-based tool CAT (Bartalesi Lenzi et al., 2012). In the first phase, both annotators went through a training phase to familiarize with the task, and were allowed to discuss and compare their annotations, especially for the PLOT LINK task. This phase led to a revision of the annotation guidelines, by introducing more specific rules to select event pairs. In the second phase, the inter-annotator agreement was calculated on a subset of the ESC v0.9 dataset. In particular, given that the basic components, i.e. event mentions, temporal expressions, and event co-referential chains, are directly inherited from the ECB+ corpus, the agreement was c"
W17-2711,bethard-etal-2008-building,0,0.0387349,"fically designed to capture the semantics of plot structures. PLOT LINK annotation is conducted in two steps: first, annotators have to identify all eligible relations between event pairs, and then they have to classify each relation as belonging to one of the two classes: rising action, events which are circumstantial to, cause or enable another event, or falling action, which explicitly mark speculations and consequences, i.e. events which are the (anticipated) outcome or the effect of another event. PLOT LINKs are related to causal and temporal relation annotation (Miltsakaki et al., 2004; Bethard et al., 2008; Mirza and Tonelli, 2014; Dunietz et al., 2015), but they differ in three ways: 1) they include the standard causal relations, i.e. cause, enablement, and prevention, but also additional event-event relations such as contingency, sub-event, entailment, and co-participation relations; 2) they are often not explicitly marked in the text through a relational structure; and 3) they are more specific than all events that stand in a temporal relation as they add explanatory information. 4. The earthquake killed 14 and left hundred trapped in collapsed buildings. earthquake rising action killed eart"
W17-2711,D10-1008,0,0.0531712,"lation. In our work both implicit and explicit relations are annotated, allowing the annotation at both intra- and inter-sentential levels. In addition to this, the availability of within- and cross-document event co-reference chains allows the extension of the annotated data across documents, providing access to a larger, “global” level of analysis. cal order, not only a temporal one. However, this does not hold anymore when cross-sentence relations are taken into account. 4 Related Work Frameworks and models for understanding narratives have mainly focused on fictional texts (Lehnert, 1981; Goyal et al., 2010; Mani, 2012) Modern day news reports still reflect narrative structures but they have proven difficult for automatic tools (Rospocher et al., 2016). To the best of our knowledge, previous work on StoryLine Extraction is limited, if we exclude the contribution by Caselli and Vossen (2016). However, there are several related works in NLP dealing with related tasks. The extraction of causal relations is the nearest task. One of the most prominent work is represented by the Penn Discourse Treebank (PDTB) (Miltsakaki et al., 2004), where explicit and implicit causal relations are annotated between"
W17-2711,W16-5708,1,0.90869,"holds, but we are looking for explanations of “why” events happened, according to the information that we are given in the document of analysis. Thus, in example 4, the relation between the events “earthquake” and “trapped” is obtained by answering the question “why were people trapped?” and not by means of transitive relation between the pairs earthquake rising action collapsed and collapsed rising action trapped. Explanatory Relation Annotation (PLOT LINKs) The annotation of explanatory relations between event pairs is encoded in the PLOT LINK tag, following a previous proposal described in Caselli and Vossen (2016). PLOT LINKs are specifically designed to capture the semantics of plot structures. PLOT LINK annotation is conducted in two steps: first, annotators have to identify all eligible relations between event pairs, and then they have to classify each relation as belonging to one of the two classes: rising action, events which are circumstantial to, cause or enable another event, or falling action, which explicitly mark speculations and consequences, i.e. events which are the (anticipated) outcome or the effect of another event. PLOT LINKs are related to causal and temporal relation annotation (Mil"
W17-2711,S12-1047,0,0.0329063,"ion of event pairs in relations that mimic the textual order of presentation; 2) PPMI1: selection of event pairs using Positive Pointwise Mutual Information (PPMI) obtained from a set of selected seed pairs and the manually annotated pairs from the development set; 3) PPMI-CONTAINS: selection of the event pairs using PPMI as in the PPMI1 model but restricting the sets of events to those which share the same temporal anchors, i.e. have a TLINK of type contains. The seed pairs for the PPMI based models have been extracted from the SemEval 2012 Task-2: Measuring Degrees of Relational Similarity (Jurgens et al., 2012). In particular, we extracted words pairs from the test set Phase1 Answers corresponding to class-8 (CAUSEPURPOSE), retaining only word pairs in the categories Cause:Effect, Cause:Compensatory Action, Action/Activity, and Prevention, where both words express events. This initial set of seed elements has been further extended by looking for “cause”, “enablement”, and “entails” relations in SUMO (Niles and Pease, 2001, 2003) and in WordNet (Miller, 1995). This resulted in a list of 1,609 unique seed pairs. PPMI has been computed using the DISSECT Toolkit (Dinu et al., 2013), and pair frequencies"
W17-2711,P14-2082,0,0.0211046,"to annotate temporal expressions, 2) re-introducing the type attribute as part of the temporal expression tag; 3) re-introducing the attribute value for temporal expressions’ normalization. We also allow 3 All examples are taken from the ECB+ Annotation Guidelines or the ECB+ annotated data 79 is also a strategy to avoid the complexity of ordering relations between events. Most of the current solutions are not optimal, as they give the annotators too much freedom in the the selection of the event pairs (e.g. TimeML), or force the annotators to mark all possible relations (e.g. TimeBankDense (Cassidy et al., 2014)), or limit the annotations to the presence of explicit linguistic evidence (e.g. RED). The temporal values in ESC are derived from the RED guidelines. We apply two sets of TLINK values according to the type of anchoring relation annotated: four values apply for relations between events and DCTs (namely before, after, overlap, and contains), while only one value (contains) applies to relations between events and temporal expressions. Annotators are also instructed on the directionality of the TLINK, which should always go from the temporal expression, or DCT, to the target event. 2.3 PLOT LINK"
W17-2711,cybulska-vossen-2014-using,1,0.891471,"ally. Current NLP systems can identify complex information but they lack a method to connect it in a unitary and coherent message. Steps in this direction have been conducted but are very limited and do not cover 77 Proceedings of the Events and Stories in the News Workshop, pages 77–86, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics are compliant with other initiatives for event annotation: temporal processing (TimeML (Pustejovsky et al., 2003a) and Richer Event Description (RED) (O’Gorman et al., 2016)), event coreference (Event Coreference Bank+ (ECB+) (Cybulska and Vossen, 2014b)), and causal relations (Causal-TimeBank (Mirza and Tonelli, 2016), BECauSE (Dunietz et al., 2015), ROCStories (Mostafazadeh et al., 2016b) among others). The remainder of the paper is structured as follows: Section 2 will explain the annotation scheme, describe the annotation layers of the Event StoryLines Corpus (ESC) v0.9, and report on agreement measures. Section 3 will describe experiments related to the development of baselines for the StoryLine Extraction task. In Section 4 a review of previous annotation initiatives is given, showing differences and commonalities between them and the"
W17-2711,miltsakaki-etal-2004-penn,0,0.346239,"16). PLOT LINKs are specifically designed to capture the semantics of plot structures. PLOT LINK annotation is conducted in two steps: first, annotators have to identify all eligible relations between event pairs, and then they have to classify each relation as belonging to one of the two classes: rising action, events which are circumstantial to, cause or enable another event, or falling action, which explicitly mark speculations and consequences, i.e. events which are the (anticipated) outcome or the effect of another event. PLOT LINKs are related to causal and temporal relation annotation (Miltsakaki et al., 2004; Bethard et al., 2008; Mirza and Tonelli, 2014; Dunietz et al., 2015), but they differ in three ways: 1) they include the standard causal relations, i.e. cause, enablement, and prevention, but also additional event-event relations such as contingency, sub-event, entailment, and co-participation relations; 2) they are often not explicitly marked in the text through a relational structure; and 3) they are more specific than all events that stand in a temporal relation as they add explanatory information. 4. The earthquake killed 14 and left hundred trapped in collapsed buildings. earthquake ris"
W17-2711,C14-1198,0,0.226389,"pture the semantics of plot structures. PLOT LINK annotation is conducted in two steps: first, annotators have to identify all eligible relations between event pairs, and then they have to classify each relation as belonging to one of the two classes: rising action, events which are circumstantial to, cause or enable another event, or falling action, which explicitly mark speculations and consequences, i.e. events which are the (anticipated) outcome or the effect of another event. PLOT LINKs are related to causal and temporal relation annotation (Miltsakaki et al., 2004; Bethard et al., 2008; Mirza and Tonelli, 2014; Dunietz et al., 2015), but they differ in three ways: 1) they include the standard causal relations, i.e. cause, enablement, and prevention, but also additional event-event relations such as contingency, sub-event, entailment, and co-participation relations; 2) they are often not explicitly marked in the text through a relational structure; and 3) they are more specific than all events that stand in a temporal relation as they add explanatory information. 4. The earthquake killed 14 and left hundred trapped in collapsed buildings. earthquake rising action killed earthquake rising action trap"
W17-2711,C16-1007,0,0.38978,"ack a method to connect it in a unitary and coherent message. Steps in this direction have been conducted but are very limited and do not cover 77 Proceedings of the Events and Stories in the News Workshop, pages 77–86, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics are compliant with other initiatives for event annotation: temporal processing (TimeML (Pustejovsky et al., 2003a) and Richer Event Description (RED) (O’Gorman et al., 2016)), event coreference (Event Coreference Bank+ (ECB+) (Cybulska and Vossen, 2014b)), and causal relations (Causal-TimeBank (Mirza and Tonelli, 2016), BECauSE (Dunietz et al., 2015), ROCStories (Mostafazadeh et al., 2016b) among others). The remainder of the paper is structured as follows: Section 2 will explain the annotation scheme, describe the annotation layers of the Event StoryLines Corpus (ESC) v0.9, and report on agreement measures. Section 3 will describe experiments related to the development of baselines for the StoryLine Extraction task. In Section 4 a review of previous annotation initiatives is given, showing differences and commonalities between them and the ESC data. Finally, conclusions and future work are reported in Sect"
W17-2711,N16-1098,0,0.34704,"this direction have been conducted but are very limited and do not cover 77 Proceedings of the Events and Stories in the News Workshop, pages 77–86, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics are compliant with other initiatives for event annotation: temporal processing (TimeML (Pustejovsky et al., 2003a) and Richer Event Description (RED) (O’Gorman et al., 2016)), event coreference (Event Coreference Bank+ (ECB+) (Cybulska and Vossen, 2014b)), and causal relations (Causal-TimeBank (Mirza and Tonelli, 2016), BECauSE (Dunietz et al., 2015), ROCStories (Mostafazadeh et al., 2016b) among others). The remainder of the paper is structured as follows: Section 2 will explain the annotation scheme, describe the annotation layers of the Event StoryLines Corpus (ESC) v0.9, and report on agreement measures. Section 3 will describe experiments related to the development of baselines for the StoryLine Extraction task. In Section 4 a review of previous annotation initiatives is given, showing differences and commonalities between them and the ESC data. Finally, conclusions and future work are reported in Section 5. The annotated data, the evaluation scripts, and the baselines mo"
W17-2711,W16-1007,0,0.539168,"this direction have been conducted but are very limited and do not cover 77 Proceedings of the Events and Stories in the News Workshop, pages 77–86, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics are compliant with other initiatives for event annotation: temporal processing (TimeML (Pustejovsky et al., 2003a) and Richer Event Description (RED) (O’Gorman et al., 2016)), event coreference (Event Coreference Bank+ (ECB+) (Cybulska and Vossen, 2014b)), and causal relations (Causal-TimeBank (Mirza and Tonelli, 2016), BECauSE (Dunietz et al., 2015), ROCStories (Mostafazadeh et al., 2016b) among others). The remainder of the paper is structured as follows: Section 2 will explain the annotation scheme, describe the annotation layers of the Event StoryLines Corpus (ESC) v0.9, and report on agreement measures. Section 3 will describe experiments related to the development of baselines for the StoryLine Extraction task. In Section 4 a review of previous annotation initiatives is given, showing differences and commonalities between them and the ESC data. Finally, conclusions and future work are reported in Section 5. The annotated data, the evaluation scripts, and the baselines mo"
W17-2711,W16-5706,0,0.0761097,"Missing"
W17-2711,W11-0419,0,0.0256782,"Temporal Anchoring of Events (TLINKs) Temporal information plays an essential role for StoryLine Extraction. At the same time, the annotation of temporal relations is by no means a trivial task. Two types of temporal relations can be identified: 1) ordering relations, which involve elements of the same ontological type, e.g. pairs of events or temporal expressions; and 2) anchoring relations, which involve cross-type element relations, e.g. pairs of event and related temporal expression. Although both types of temporal relations are useful, they have different informational status. Following Pustejovsky and Stubbs (2011), we assume that the informational level of a temporal relation can be expressed as a function of the information contained in each temporal link and their closure. Under this assumption, anchoring relations expressing when an event mention occurred or its duration, are more informative than ordering relations. The former allow us to put event mentions on a specific point (or interval) on an imaginary timeline and, as a consequence, also gives us the ordering relations between event mentions. The ESC Annotation Scheme expresses temporal relations using the TimeML TLINK tag and restricts them t"
W18-4306,W10-0731,0,0.0704768,"Missing"
W18-4306,P98-1013,0,0.490602,"Missing"
W18-4306,W16-5708,1,0.766538,"e results are presented as structured data based on narrative strategies. We follow, in this This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 1 For a comparison, think about the Ancient Roman tradition of the Annales, concise historical records merely reporting events chronologically. 44 Proceedings of the Workshop on Events and Stories in the News, pages 44–54 Santa Fe, New Mexico, USA, August 20, 2018. respect, the proposal of automatically generating storylines of events (Vossen et al., 2015; Caselli and Vossen, 2016). This paper reports on a crowdsourcing experiment on the annotation of causal relations between pairs of events in news data. The main contributions of this work are: • an analysis of the crowdsourced data, in terms of parameters that may affect the annotation quality, time, and evaluation of the data using the CrowdTruth methodology (Aroyo and Welty, 2014; Aroyo and Welty, 2015); • a comparison between experts and crowdsourced annotated data with respect to a publicly available reference benchmark corpus for storyline evaluation, the Event StoryLine Corpus (ESC) (Caselli and Vossen, 2017); •"
W18-4306,W17-2711,1,0.906506,"5; Caselli and Vossen, 2016). This paper reports on a crowdsourcing experiment on the annotation of causal relations between pairs of events in news data. The main contributions of this work are: • an analysis of the crowdsourced data, in terms of parameters that may affect the annotation quality, time, and evaluation of the data using the CrowdTruth methodology (Aroyo and Welty, 2014; Aroyo and Welty, 2015); • a comparison between experts and crowdsourced annotated data with respect to a publicly available reference benchmark corpus for storyline evaluation, the Event StoryLine Corpus (ESC) (Caselli and Vossen, 2017); • the release of an enhanced version of the Event Storyline Corpus (ESC v1.2). The remainder of the paper is organised as follows: Section 2 provides an overview of related work on the annotation of causal relations in different datasets, highlighting differences and commonalities with our contribution. Section 3 describes the dataset and the crowdsourcing experiment settings based on the CrowdTruth metrics. Section 4 reports on an in-depth analysis of the crowd data and its comparison with existing expert annotated data. Finally, Section 5 summarises our findings and suggests directions for"
W18-4306,L16-1557,1,0.737158,"Missing"
W18-4306,P08-1090,0,0.0714826,"Missing"
W18-4306,W17-0812,0,0.0567243,"Missing"
W18-4306,D10-1008,0,0.0139144,"ns and narrative strategies are one of the major cognitive tools we use to observe the world and, most importantly, to interpret it (Boyd, 2009; Gottschall, 2012). When reporting on an event in the world, or telling someone a personal experience, we do not merely describe what happens, i.e., we do not just list events in the order of occurrence 1 , but we connect them in a set of coherent patterns, or, in other words, we give rise to plot structures (Bal, 1997). Plot structures express a form of reasoning about causal relations between events and states composing the narrative (Lehnert, 1981; Goyal et al., 2010; Mani, 2012). The current stream of data and information is growing everyday and its size and complexity is such that humans may suffer from “information overload”. To minimise such a problem, intelligent content management systems have been developed and they became more and more popular and used. Different methods and approaches have been developed to provide users with personalised and relevant information. However, most of this information is given in the form of full text documents that require the users to read them to identify (i.e., extract) the information. Automatic processing would"
W18-4306,N13-1062,0,0.0656149,"Missing"
W18-4306,kingsbury-palmer-2002-treebank,0,0.383489,"Missing"
W18-4306,D15-1189,0,0.0567309,"Missing"
W18-4306,C16-1007,0,0.194633,"Missing"
W18-4306,N16-1098,0,0.053351,"Missing"
W18-4306,W16-1007,0,0.0477681,"Missing"
W18-4306,W11-0143,0,0.0234577,"bility of a plurality of perspectives on it. Providing an extensive and critical summary of this debate is out of the scope of this work, but, we will review relevant works in the areas of Linguistics and Natural Language Processing that contributed to shape this notion, its annotation in actual natural language data, and the development of automatic systems. We restrict this literature review to approaches in the news domain. One of the distinguishing properties of causality in natural language, shared with other semantic relations such as meronymy and mereology, is granularity (Hobbs, 1985; Mulkar-Mehta et al., 2011). This allows humans to interactively play between coarse-grained and fine-grained levels of causality. Further studies (Talmy, 1976; Comrie, 1981; Girju and Moldovan, 2002) have investigated the variety of lexico and semantic constructions that can express causation in a natural language. At least for English, as well as other Indo-European languages, it is possible to differentiate the set of causative constructions into two big groups: i.) those expressing causality via explicit patterns; and ii.) those using implicit patterns. The difference between these two ways of expressing causality r"
W18-4306,D08-1027,0,0.224582,"Missing"
W18-4306,L16-1338,0,0.058995,"Missing"
W18-4306,W15-4507,1,0.859474,"ial, especially if the results are presented as structured data based on narrative strategies. We follow, in this This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 1 For a comparison, think about the Ancient Roman tradition of the Annales, concise historical records merely reporting events chronologically. 44 Proceedings of the Workshop on Events and Stories in the News, pages 44–54 Santa Fe, New Mexico, USA, August 20, 2018. respect, the proposal of automatically generating storylines of events (Vossen et al., 2015; Caselli and Vossen, 2016). This paper reports on a crowdsourcing experiment on the annotation of causal relations between pairs of events in news data. The main contributions of this work are: • an analysis of the crowdsourced data, in terms of parameters that may affect the annotation quality, time, and evaluation of the data using the CrowdTruth methodology (Aroyo and Welty, 2014; Aroyo and Welty, 2015); • a comparison between experts and crowdsourced annotated data with respect to a publicly available reference benchmark corpus for storyline evaluation, the Event StoryLine Corpus (ESC) (C"
