1999.mtsummit-1.34,C96-1070,0,0.132529,"Missing"
1999.mtsummit-1.34,C96-1075,0,0.0608382,"Missing"
1999.mtsummit-1.34,P89-1013,0,0.0183586,"Missing"
1999.mtsummit-1.34,C88-2118,0,0.0291956,"Missing"
1999.mtsummit-1.34,P91-1024,1,0.749748,"Missing"
1999.mtsummit-1.34,P98-2233,1,0.883543,"Missing"
1999.mtsummit-1.34,W97-0404,0,0.260944,"Missing"
1999.mtsummit-1.34,W99-0207,1,\N,Missing
1999.mtsummit-1.34,C98-2228,1,\N,Missing
2002.tmi-papers.21,J00-1004,0,0.024364,"Missing"
2002.tmi-papers.21,P99-1009,0,0.0235605,"Missing"
2002.tmi-papers.21,1999.mtsummit-1.34,1,0.854328,"Missing"
2002.tmi-papers.21,C94-2116,0,0.0276097,"ful rules in a shorter period of time on a small training set compared to the best systems available. That is, current automatic acquisition methods in some tasks are no more efficient than humans. Consequently, it is important to consider the cooperation of humans and computers from the viewpoint of practicality. Streiter et al. (1999) proposed a strategy that rates manual MT rules by counting occurrence frequencies in a corpus. Some researchers expect this approach to contribute to the disambiguation of languages, but the approach is not applicable to the expansion and maintenance of rules. Tanaka (1994), in contrast, proposed an acquisition model for English case frames given by machine learning. This approach allows optimality to be maintained by confining the human descriptions of rules, but it still suffers from the problem of low knowledge construction efficiency, since it requires a bilingual tagged corpus of high quality. Given the above review, we have been considering what information is useful to humans. As one solution, Shirai et al. (1995) examined methods of making semantic structure dictionaries for Japanese-to-English MT. The results indicated that the most efficient method was"
2002.tmi-papers.21,1993.tmi-1.25,0,0.0444177,"adequate body of knowledge. The current approach to knowledge acquisition demands that a human rule writer, someone who is familiar with general linguistic knowledge and a framework of MT knowledge, manually check and correct the translation examples output by an MT system. This incurs high time and labor costs. While this is reasonable for creating prototype systems, its efficiency is too low to create practical MT systems. One engineering solution to this problem appears to be automatic knowledge acquisition (e.g., Almuallim et al. (1994); Alshawi et al. (2000); Kitamura & Matsumoto (1995); Watanabe (1993)). Such systems are capable of detecting simple rules directly from corpora through automatic learning. This approach is becoming more attractive due to the wide variety of corpora available. The current performance offered by automatic knowledge acquisition is rather suspect; the problem is that low-frequency phenomena are not well handled. Our basic approach is to combine human language skills with the automatic extraction of source and target language information. This is eminently practical since the number of corpora suitable for MT development continues to increase. The voluminous inform"
2002.tmi-papers.21,C94-1006,0,\N,Missing
2005.iwslt-1.16,2003.mtsummit-papers.53,0,\N,Missing
2005.iwslt-1.16,koen-2004-pharaoh,0,\N,Missing
2005.iwslt-1.16,W03-1506,1,\N,Missing
2005.iwslt-1.16,W02-1021,0,\N,Missing
2005.iwslt-1.16,J04-4002,0,\N,Missing
2005.iwslt-1.16,J03-1002,0,\N,Missing
2009.mtsummit-posters.10,2007.mtsummit-papers.63,1,0.920528,"tistical machine translation (SMT) experiments with the corpus and conﬁrmed that the corpus is useful for SMT. 1 Introduction Multilingual parallel corpora are required to support many tasks in natural language processing. For example, statistical machine translation (SMT) requires a parallel corpus for training, and crosslingual processing such as information retrieval and information extraction also use parallel corpora. There is no doubt on the importance of parallel corpora for any language pair. Specially, Japanese-English parallel corpora are very scarce. Although some parallel corpora (Utiyama and Isahara, 2007) are available, the domains and sizes of these corpora are limited. In general, European countries use multiple languages ofﬁcially. Based on this multilingual environment, Koehn (2005) has built a corpus by collecting parallel texts in eleven languages from the proceedings of the European Parliament, which are published on the Web. However, some countries such as Japan have no such language situation, that leads us difﬁculties for Masao Utiyama†† Eiichiro Sumita†† †† MASTAR Project National Institute of Information and Communications Tehnology 3-5, Hikaridai, Seika, Soraku, Kyoto 619-0289, Ja"
2009.mtsummit-posters.10,2005.mtsummit-papers.11,0,0.0444829,"al language processing. For example, statistical machine translation (SMT) requires a parallel corpus for training, and crosslingual processing such as information retrieval and information extraction also use parallel corpora. There is no doubt on the importance of parallel corpora for any language pair. Specially, Japanese-English parallel corpora are very scarce. Although some parallel corpora (Utiyama and Isahara, 2007) are available, the domains and sizes of these corpora are limited. In general, European countries use multiple languages ofﬁcially. Based on this multilingual environment, Koehn (2005) has built a corpus by collecting parallel texts in eleven languages from the proceedings of the European Parliament, which are published on the Web. However, some countries such as Japan have no such language situation, that leads us difﬁculties for Masao Utiyama†† Eiichiro Sumita†† †† MASTAR Project National Institute of Information and Communications Tehnology 3-5, Hikaridai, Seika, Soraku, Kyoto 619-0289, Japan {mutiyama,eiichiro.sumita} @nict.go.jp creating parallel corpora. Hence, more efforts are needed to collect them effectively. Available Japanese-English parallel corpora are scarce."
2009.mtsummit-posters.10,tiedemann-nygaard-2004-opus,0,0.0174316,",eiichiro.sumita} @nict.go.jp creating parallel corpora. Hence, more efforts are needed to collect them effectively. Available Japanese-English parallel corpora are scarce. However, there are a lot of translated texts on the Web. Specially, open source manuals are translated into Japanese from English by volunteer translators. We collected such English and Japanese texts. Then, the sentences in collected texts were automatically aligned, resulting in a parallel corpus made from open source software manuals. Manuals of open source software has been used for making a parallel corpus named OPUS (Tiedemann and Nygaard 2004), which was made from OpenOfﬁce.org documentation1 , KDE manuals including KDE messages2 , and PHP manuals 3 . However, the JapaneseEnglish part of OPUS is not large. In contrast, we collected about 500 thousand sentence pairs. In addition, our work involved extensive human efforts to ensure the quality of our parallel corpus. The original and translated texts often proscribe copy, distribute, display, and make derivative works. Our target texts are open source software manuals. Such open source software manuals are often published under open licenses under which we can modify and distribute t"
2009.mtsummit-posters.10,J93-1004,0,0.306217,", newlines are not deleted because they are regarded as sentence ends. 4.3 Aligning sentences We use Utiyama and Isahara’s alignment method, because their method has been successfully used in aligning noisy Japanese-English parallel texts (Utiyama and Isahara, 2007). Below is a concise description of their algorithm. We begin by obtaining the maximum similarity sentence alignments. Let J and E be a Japanese text ﬁle and an English text ﬁle, respectively. We calculate the maximum similarity sentence alignments (J1 ,E1 ), (J2 ,E2 ), . . ., (Jm ,Em ), using a dynamic programming matching method (Gale and Church, 1993), where (Ji , Ei ) is a Japanese and English sentence alignment pair in J and E. We allow 1-ton, n-to-1 (0 ≤ n ≤ 5), or 2-to-2 alignments when aligning sentences. The similarity between Ji and Ei is calculated based on word overlap (i.e., number of word pairs from Ji and Ei that are translations of each other based on a bilingual dictionary with 450,000+ entries). The similarity between a Japanese document, J, and an English document, E, (noted AVSIM(J,E)) is calculated using: ∑m AVSIM(J, E) = i=1 SIM(Ji , Ei ) m (1) A high AVSIM(J,E) value occurs when the sentence alignments in J and E take o"
2009.mtsummit-posters.10,P07-2045,0,0.0228859,"Missing"
2009.mtsummit-posters.10,P02-1040,0,0.0934553,"Missing"
2009.mtsummit-posters.10,P03-1021,0,0.016259,"ect without performing ﬁlling and painting operations. This includes BIOS settings, kernel conﬁguration and some simpliﬁcations in user land. This signal can be used to perform a remote checkpoint of a session. Xlib may choose to cache font data, loading it only as needed to draw text or compute text dimensions. Table 3: Example of parallel sentences formed to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score (Papinei et al., 2002). The evaluation was done using a single reference. Tuning was performed using the standard technique developed by Och (Och, 2003). The test and development data were extracted from the aligned JF sentences. Each of test and development data consists of 500 sentences. In the following experiments, we simulated a situation where an SMT system was applied to help volunteer translators translate English JF documents into Japanese. We want to use all parallel sentences efﬁciently to help translators. This is a problem of domain adaptation. All of paralell sentences were translated from English to Japanese. Therefore we did MT experiments from English. In the ﬁrst experiment, we used all parallel sentences (excluding developm"
2009.mtsummit-posters.10,J03-1002,0,0.00480786,"Missing"
2011.mtsummit-papers.40,C96-1009,0,0.116742,"Missing"
2011.mtsummit-papers.40,D07-1103,0,0.0336507,"English sentences of the given parallel corpus. (2) Build the phrase-table from the parallel corpus using the Moses toolkit (Koehn et al., 2007). (3) Extract bilingual term candidates from the phrase table that are included in the term candidates obtained in (1). (4) Calculate a statistical measure for each candidate term. (5) Rank the candidates according to the statistical measure, and extract the highly-ranked candidates as valid bilingual terms. We compare three statistical measures, ScoreF , ScoreL and ScoreC , for extracting correct bilingual terms. Fisher’s exact test has been used by Johnson et al. (2007) to select valid phrase pairs from the phrasetable for statistical machine translation. We use the statistic of Fisher’s exact test as ScoreF to measure the validity of each bilingual term candidate. The statistic used in Fisher’s exact test is deﬁned as ScoreF as follows. First, we obtain the contingency table, shown below, for a bilingual term candidate TJ,E consisting of Japanese term J and English term E C(J, E) C(E) − C(J, E) We use functions implemented in TermExtract 2 to extract term candidates. TermExtract is a Perl module for extracting terms. We slightly modiﬁed the POS patterns use"
2011.mtsummit-papers.40,P07-2045,0,0.00258597,"statistical measures. In addition, we compare three statistical measures for extracting bilingual terms. Note that Macken et al. (2008) have also used statistical measures to ﬁlter out invalid bilingual term candidates; however, they did not compare their statistical measures against other measures. 3 Bilingual term extraction (1) Extract the term candidates, which match speciﬁc part-of-speech(POS) patterns (e.g., a single noun or a noun sequence), from the Japanese and English sentences of the given parallel corpus. (2) Build the phrase-table from the parallel corpus using the Moses toolkit (Koehn et al., 2007). (3) Extract bilingual term candidates from the phrase table that are included in the term candidates obtained in (1). (4) Calculate a statistical measure for each candidate term. (5) Rank the candidates according to the statistical measure, and extract the highly-ranked candidates as valid bilingual terms. We compare three statistical measures, ScoreF , ScoreL and ScoreC , for extracting correct bilingual terms. Fisher’s exact test has been used by Johnson et al. (2007) to select valid phrase pairs from the phrasetable for statistical machine translation. We use the statistic of Fisher’s exa"
C02-1056,1993.tmi-1.20,0,0.0631249,"lly says it again using other words, i.e., he paraphrases. In a Chinese-Japanese spoken language translation system, the pre-processing of Chinese utterances is involved and we attempt to apply a paraphrasing approach. This paper ∗ This work was done when the author stayed at ATR Spoken Language Translation Research Laboratories. is focused on the paraphrasing of Chinese utterances. Some cases of paraphrasing research with certain targets have been reported. For example, there has been work on rewriting the source language in machine translation with a focus on reducing syntactic ambiguities (Shirai et al., 1993), research on paraphrasing paper titles with a focus on transforming syntactic structures to achieve readability (Sato, 1999), and research on paraphrasing Japanese in summarization with a focus on transforming a noun modiﬁer into a noun phrase (Kataoka et al., 1999). We have reported some research on Chinese paraphrasing (Zhang and Yamamoto, 2001; Zhang et al., 2001; Zong et al., 2001). The techniques of paraphrasing natural language can be applied not only to the pre-processing of machine translation but also to information retrieval and summarization. 2 Goals and Approach In the pre-process"
C02-1056,C02-1163,1,0.849824,"In addition, a pattern construction method is described through which paraphrasing patterns can be eﬃciently learned from a paraphrase corpus and human experience. Using the implemented paraphraser and the obtained patterns, a paraphrasing experiment was conducted and the results were evaluated. 1 Introduction In spoken language translation one of the key issues is how to deal with unrestricted expressions in spontaneous utterances. To resolve this problem, we have proposed a paraphrasing approach in which the utterances are automatically paraphrased prior to transfer (Yamamoto et al., 2001; Yamamoto, 2002). The paraphrasing process aims to bridge the gap between the unrestricted expressions in the input and the limited expressions that the transfer can translate. In fact, paraphrasing actions are often seen in daily communication. When a listener cannot understand what a speaker said, the speaker usually says it again using other words, i.e., he paraphrases. In a Chinese-Japanese spoken language translation system, the pre-processing of Chinese utterances is involved and we attempt to apply a paraphrasing approach. This paper ∗ This work was done when the author stayed at ATR Spoken Language Tr"
C02-1163,1991.mtsummit-papers.9,0,0.0484619,"tically-acquired information from corpora. Figure 4 shows the average number of paraphrasing trials. It would be a major problem in this design if there were many interaction loops between the paraphraser and the transfer, but we found that such worries are unwarranted in the current system. However, it is necessary to be careful in this measure, since we need to add more functions to the paraphraser in order to avoid zero output. 4 Related Works It is important to reduce the burden of transfer to realize multilingual MT. In this sense, MT using a controlled language, such as the KANT system (Mitamura et al., 1991), has similar principles to our approach. We believe that multilingual MT systems should not place the obligation of transferring the target language on the transfer module. Diﬃcult or ambiguous input should be checked in document translations, while it should somehow be resolved before the transfer module in speech translation, since real-time dialog conversation is a requirement. Although we cannot ﬁnd an MT model where an interactive (that is, feedback) approach between the two sub-modules is implemented, several types of interactive models have been discussed in natural language generation"
C02-1163,1993.tmi-1.20,0,0.944701,"to us, we may try to paraphrase the source input into easier expressions we can translate. In contrast, there is no such machine translation (MT) model so far proposed where the source language module is biased over the bilingual language module. All of the MT models are either those where the bilingual processor takes the initiative over the source language analyzer (conventional analyze-transfer-generate model) or integration models of analyzer and transfer, such as example-based or statistical models. Although some MT models have a paraphraser (also called a ‘pre-editor’), such as that of Shirai et al. (1993), paraphrasing is performed in these models because it is necessary to prepare for the subsequent bilingual process. In other words, the paraphraser operates as a sub-module for successful transfer. We have proposed a new MT model that is more similar to the human translation process than other MT systems (Yamamoto et al., 2001). This model, called the Sandglass model, is designed so that the system can generate a translation through source language paraphrasing, even if the system does not have suﬃcient bilingual knowledge. In this sense, our model design can be considered a nonprofessional t"
C02-1163,C02-1056,1,0.849728,"advantage is task portability, since all of the paraphrasing knowledge, except for lexical paraphrasing knowledge, is independent of the task, so we do not need to ﬁt most of the paraphrasing knowledge to the required task. It is also signiﬁcant that this model’s paraphraser can be employed not only for MT but also for most natural language processing (NLP) applications. This is possible because both the input and output of a paraphraser is the same natural language. We have been building the Sandglass MT system for the Japanese-Chinese, ChineseJapanese language pairs (Yamamoto et al., 2001; Zhang and Yamamoto, 2002). We have already constructed a prototype for JapaneseChinese. In this paper, we report the core concepts of this prototype and discuss issues of both our principle and our implementation. 2 Sandglass Translation Model Figure 1 shows our paradigm for a translation model. In the conventional MT model, the process load and the information used to deal with translation process Conventional paradigm maximum load in a bilingual transfer process Sandglass paradigm maximum load in a monolingual process by paraphrasing load Figure 1: Comparison of the two MT paradigms it are maximized in the transfer"
C04-1102,P97-1017,0,0.261951,"Japanese because of variation in English pronunciation; e.g., between British and American. For example, the                     Kazuhide Yamamoto Nagaoka Univ. of Tech. Nagaoka City, Niigata 940-2188, Japan yamamoto@fw.ipsj.or.jp     English word body corresponds to two words: “ (bodi)” from British and “ (badi)” from American. One may think that if back-transliteration were done precisely, those variants would be backtransliterated into one word, and they would be recognized as variants. However, backtransliteration is known to be a very difficult task(Knight and Graehl, 1997). Not only Japanese but any language that has a phonetic spelling has this problem of transliterated orthographic variants. For example, English has variants for a Chinese proper noun as “Shanhaiguan,” “Shanhaikwan,” or “Shanhaikuan.” Nowadays, it is well recognized that orthographic variant correction is an important processing step for achieving high performance in natural language processing. In order to achieve robust and reliable processing, we have to use many language resources: many types of corpora, dictionaries, thesauri, and so on. Orthographic variants cause many mismatches at any"
C04-1102,takezawa-etal-2002-toward,0,0.0151417,"Here, kwi is a katakana word, ei is an element of the vector corresponding to kwi , f (kwi , ei ) denotes the frequency of the element ei for kwi , sf (kwi ) denotes the frequency of the sentence including kwi , and N denotes the number of katakana words in the corpus. The contextual similarity is defined as the following formula. simc (kwi , kwj ) = cos(vec(kwi ), vec(kwj )) = qP P em ex W (kwi , ex )W (kwj , ex ) W (kwi , em )2 qP en W (kwj , en )2 , (6) where vec(kw) denotes a vector corresponding to the katakana word kw. 4 Experiments We used the ATR Basic Travel Expression Corpus (BTEC)(Takezawa et al., 2002) as a resource for text. BTEC is a multilingual corpus and was mainly developed with English and Japanese. The Japanese part of BTEC contains not only ordinary katakana variants, but also mis-transliterated katakana strings by non-Japanese natives that serve as our target for detection. The BTEC we used consists of almost 200,000 sentences. We used almost 160,000 sentences for the development of the t(i, j) table, other rules used in our method, and parameter estimations for the method. We manually estimated the parameters to achieve the highest F-measure for the development sentences, and est"
C98-1067,C94-1015,1,0.813555,"Missing"
C98-1067,C96-1070,1,0.837774,"Missing"
C98-1067,W97-0404,0,0.695422,"Missing"
C98-2228,P95-1017,0,0.0696926,"Missing"
C98-2228,C96-2137,0,0.0867604,"cue words (Walker and Moore, 1997). This section describes a method to apply a decision-tree learning approach, which is one of the machinelearning approaches, to ellipsis resolution. As described above, our major application for ellipsis resolution is in machine translation. In an MT process, there can be several approaches about the timing of ellipsis resolution: when analyzing the source language, when generating the target language, or at the same time as translating process. Among these candidates, most of the previous works with Japanese chose the source-language approach. For instance, Nakaiwa and Shirai (1996) attempted to resolve Japanese ellipsis in the source language analysis of J-to-E MT, despite utilizing targetdependent resolution candidates. We originally thought that ellipsis resolution in the MT was a generation problem, namely a target-driven problem which utilizes some help, if necessary, of source-language information. This is because the problem is outputdependent and it relies on demands fl'om a target language, in the J-to-Korean or J-toChinese MT, all or most of the ellipses that must be resolved in J-to-E are not necessary to resolve. ttowever, we adopted source-language policy in"
C98-2228,C94-2116,0,0.0298411,"ed the C~.5 algorithm by Quinlan (1993), which is a well-known automatic classifier that produces a binary decision tree. Although it may be necessary to prune decision trees, no pruning is performed throughout this experiment, since we want to concentrate the discussion on the feasibility of machine learning. As shown in the experiment by Aone and BenSince a huge text corlms has become widely available, the machine-learning approach has been utilized for some problems in natural language processing. The most popular touchstone in this field is the verbal case fl'ame or the translation rules (Tanaka, 1994). Machine-learning algorithm has also been attempted to solve some 1429 3.1 Ellipsis Tagging Learning Method Table 2: Number of training attributes Attributes Content words ( p r e d i c a t e ) Content words (case frame) Func. words (case particle) Func. words (conj. particle) Func. words (auxiliary verb) Func. words (other) Exophoric information Total Num. 100 100 9 21 132 4 1 367 nett (1995), which a t t e m p t e d to discuss pruning effects on the decision tree, no more conclusions are expected other than a trade-off between recall and precision. We leave the details of decision-tree lear"
C98-2228,J97-1001,0,\N,Missing
D15-1156,P11-2037,0,0.107753,"ank (Marcus et al., 1993) has annotations on PRO and trace, they provide only labeled bracketing. Johnson (2002) proposed a statistical pattern-matching algorithm for post-processing the results of syntactic parsing based on minimal unlexicalized tree fragments from empty node to its antecedent. Dienes and Dubey (2003) proposed a machine learning-based “trace tagger” as a preprocess of parsing. Campbell (2004) proposed a rule-based post-processing method based on linguistically motivated rules. Gabbard et al. (2006) replaced the rules with machine learning-based classifiers. Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syntactic parsing. Empty category detection for pro (dropped pronouns or zero pronoun) has begun to receive attention as the Chinese Penn Treebank (Xue et al., 2005) has annotations for pro as well as PRO and trace. Xue and Yang (2013) formalized the problem as classifying each pair of the location of empty category and its head word in the dependency structure. Wang et al. (2015) proposed a joint embedding of empty categories and their contexts on dependency structure. Xiang et al. (2013) formalized the problem as classifying each IP node (roughly"
D15-1156,P03-1055,0,0.607948,"proved the accuracy of machine translation both in Korean and in Chinese. Kudo et al. (2014) showed that generating zero subjects in Japanese improved the accuracy of preorderingbased translation. State-of-the-art statistical syntactic parsers had typically ignored empty categories. Although Penn Treebank (Marcus et al., 1993) has annotations on PRO and trace, they provide only labeled bracketing. Johnson (2002) proposed a statistical pattern-matching algorithm for post-processing the results of syntactic parsing based on minimal unlexicalized tree fragments from empty node to its antecedent. Dienes and Dubey (2003) proposed a machine learning-based “trace tagger” as a preprocess of parsing. Campbell (2004) proposed a rule-based post-processing method based on linguistically motivated rules. Gabbard et al. (2006) replaced the rules with machine learning-based classifiers. Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syntactic parsing. Empty category detection for pro (dropped pronouns or zero pronoun) has begun to receive attention as the Chinese Penn Treebank (Xue et al., 2005) has annotations for pro as well as PRO and trace. Xue and Yang (2013) formalized the proble"
D15-1156,N06-1024,0,0.121015,"of-the-art statistical syntactic parsers had typically ignored empty categories. Although Penn Treebank (Marcus et al., 1993) has annotations on PRO and trace, they provide only labeled bracketing. Johnson (2002) proposed a statistical pattern-matching algorithm for post-processing the results of syntactic parsing based on minimal unlexicalized tree fragments from empty node to its antecedent. Dienes and Dubey (2003) proposed a machine learning-based “trace tagger” as a preprocess of parsing. Campbell (2004) proposed a rule-based post-processing method based on linguistically motivated rules. Gabbard et al. (2006) replaced the rules with machine learning-based classifiers. Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syntactic parsing. Empty category detection for pro (dropped pronouns or zero pronoun) has begun to receive attention as the Chinese Penn Treebank (Xue et al., 2005) has annotations for pro as well as PRO and trace. Xue and Yang (2013) formalized the problem as classifying each pair of the location of empty category and its head word in the dependency structure. Wang et al. (2015) proposed a joint embedding of empty categories and their contexts on depen"
D15-1156,P02-1018,0,0.502528,"es such as Japanese, in particular, for the machine translation from pro-drop languages to nonpro-drop languages such as English. Chung and Gildea (2010) reported their recover of empty categories improved the accuracy of machine translation both in Korean and in Chinese. Kudo et al. (2014) showed that generating zero subjects in Japanese improved the accuracy of preorderingbased translation. State-of-the-art statistical syntactic parsers had typically ignored empty categories. Although Penn Treebank (Marcus et al., 1993) has annotations on PRO and trace, they provide only labeled bracketing. Johnson (2002) proposed a statistical pattern-matching algorithm for post-processing the results of syntactic parsing based on minimal unlexicalized tree fragments from empty node to its antecedent. Dienes and Dubey (2003) proposed a machine learning-based “trace tagger” as a preprocess of parsing. Campbell (2004) proposed a rule-based post-processing method based on linguistically motivated rules. Gabbard et al. (2006) replaced the rules with machine learning-based classifiers. Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syntactic parsing. Empty category detection for p"
D15-1156,kawahara-kurohashi-2006-case,0,0.0838813,"Missing"
D15-1156,P14-2091,0,0.145002,"Missing"
D15-1156,P04-1082,0,0.775457,"hat generating zero subjects in Japanese improved the accuracy of preorderingbased translation. State-of-the-art statistical syntactic parsers had typically ignored empty categories. Although Penn Treebank (Marcus et al., 1993) has annotations on PRO and trace, they provide only labeled bracketing. Johnson (2002) proposed a statistical pattern-matching algorithm for post-processing the results of syntactic parsing based on minimal unlexicalized tree fragments from empty node to its antecedent. Dienes and Dubey (2003) proposed a machine learning-based “trace tagger” as a preprocess of parsing. Campbell (2004) proposed a rule-based post-processing method based on linguistically motivated rules. Gabbard et al. (2006) replaced the rules with machine learning-based classifiers. Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syntactic parsing. Empty category detection for pro (dropped pronouns or zero pronoun) has begun to receive attention as the Chinese Penn Treebank (Xue et al., 2005) has annotations for pro as well as PRO and trace. Xue and Yang (2013) formalized the problem as classifying each pair of the location of empty category and its head word in the depende"
D15-1156,levy-andrew-2006-tregex,0,0.0981205,"Missing"
D15-1156,D10-1062,0,0.0906353,"Missing"
D15-1156,D14-1162,0,0.0853881,"and HEAD is:IP-MAT× 連れ (tsure) and the combinations of PATH and CHILD are: IP-MAT×PP-を (wo), IP-MAT×VB, IP-MAT×VB2, IP-MAT×AXD-た (ta) and IP-MAT×PU-。 . 3.1 Using Word Embedding to approximate Case Frame Lexicon A case frame lexicon would be obviously useful for empty category detection because it provides information on the type of argument the verb in question takes. The problem is that case frame lexicon is not usually readily available. We propose a novel method to approximate case frame lexicon for languages with explicit case marking such as Japanese using word embeddings. According to (Pennington et al., 2014), they designed their embedding model GloVe so that the dot product of two word embeddings approximates the logarithm of their co-occurrence counts. Using this characteristic, we can easily make a feature that approximate the case frame of a verb. Given a set of word embeddings for case particles q1 , q2 , · · · , qN ∈ Q, the distributed case frame feature (DCF) for a verb wi is defined as: v˜i = wi · (q1 , q2 , · · · , qN ) v˜i vi = ||v˜i || Proposed model In the proposed model, we use combinations of path features and three other features, namely head word feature, child feature and empty ca"
D15-1156,N07-1051,0,0.0334814,"Missing"
D15-1156,C04-1024,0,0.0843671,"Missing"
D15-1156,P06-1023,0,0.533195,"lthough Penn Treebank (Marcus et al., 1993) has annotations on PRO and trace, they provide only labeled bracketing. Johnson (2002) proposed a statistical pattern-matching algorithm for post-processing the results of syntactic parsing based on minimal unlexicalized tree fragments from empty node to its antecedent. Dienes and Dubey (2003) proposed a machine learning-based “trace tagger” as a preprocess of parsing. Campbell (2004) proposed a rule-based post-processing method based on linguistically motivated rules. Gabbard et al. (2006) replaced the rules with machine learning-based classifiers. Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syntactic parsing. Empty category detection for pro (dropped pronouns or zero pronoun) has begun to receive attention as the Chinese Penn Treebank (Xue et al., 2005) has annotations for pro as well as PRO and trace. Xue and Yang (2013) formalized the problem as classifying each pair of the location of empty category and its head word in the dependency structure. Wang et al. (2015) proposed a joint embedding of empty categories and their contexts on dependency structure. Xiang et al. (2013) formalized the problem as classifying"
D15-1156,P13-1081,0,0.276745,"es with machine learning-based classifiers. Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syntactic parsing. Empty category detection for pro (dropped pronouns or zero pronoun) has begun to receive attention as the Chinese Penn Treebank (Xue et al., 2005) has annotations for pro as well as PRO and trace. Xue and Yang (2013) formalized the problem as classifying each pair of the location of empty category and its head word in the dependency structure. Wang et al. (2015) proposed a joint embedding of empty categories and their contexts on dependency structure. Xiang et al. (2013) formalized the problem as classifying each IP node (roughly corresponds to S and SBAR in Penn Treebank) in the phrase structure. In this paper, we propose a novel method for empty category detection for Japanese that uses conjunction features on phrase structure and word embeddings. We use the Keyaki Treebank (Butler et al., 2012), which is a recent development. As it has annotations for pro and trace, we show our method has substantial improvements over the state-of-the-art machine learning-based method (Xiang et al., 2013) for Chinese empty category detection as well as linguistically-motiv"
D15-1156,N13-1125,0,0.161969,"o its antecedent. Dienes and Dubey (2003) proposed a machine learning-based “trace tagger” as a preprocess of parsing. Campbell (2004) proposed a rule-based post-processing method based on linguistically motivated rules. Gabbard et al. (2006) replaced the rules with machine learning-based classifiers. Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syntactic parsing. Empty category detection for pro (dropped pronouns or zero pronoun) has begun to receive attention as the Chinese Penn Treebank (Xue et al., 2005) has annotations for pro as well as PRO and trace. Xue and Yang (2013) formalized the problem as classifying each pair of the location of empty category and its head word in the dependency structure. Wang et al. (2015) proposed a joint embedding of empty categories and their contexts on dependency structure. Xiang et al. (2013) formalized the problem as classifying each IP node (roughly corresponds to S and SBAR in Penn Treebank) in the phrase structure. In this paper, we propose a novel method for empty category detection for Japanese that uses conjunction features on phrase structure and word embeddings. We use the Keyaki Treebank (Butler et al., 2012), which"
D15-1156,J93-2004,0,\N,Missing
I08-2102,P98-2127,0,0.0107347,"ght according to the type of matched characters. We apply 1.0 to the weight only if Chinese-derived characters (Kanji) are matched. We link phrases as corresponding phrases, where the phrases are the top three similar to a phrase in the similar instance. (iv) Similarity with Mutual Information We finally compute the similarity with mutual information to link syntactically similar phrases. For example, given the following two expressions: “会 議を開く (to hold a meeting)” and “大会を開く (to hold a convention)”, we regard 会議 (a meeting) and 大会 (a convention) as similar. We use the similarity proposed by Lin (1998). The method uses mutual information and dependency relationships as the phrase features. We extend the method to Japanese by using a particle as the dependency relationships. We link phrases as corresponding phrases, where the phrases are the top three similar to a phrase in the similar instance. 3.4 Combination of the Corresponding Phrases Our system forms the one-sentence summary by combining the corresponding phrases. Let us explain this process by using figure 2. We arrange the phrase of the input on the node, where the phrases is judged as the correspondence to the phrase in the similar"
I08-2102,W03-1113,0,0.0279165,"rresponding phrase. For example, in example 2, the phrase “24 日 [DATE] (on the 24th.)” in the input corresponds the phrase “18 日 [DATE] (on the 18th.)” in the similar instance. (iii) Similarity with Enhanced Edit Distance We adopt the enhanced edit distance to link phrases including the same characters, because Japanese abbreviation tends to include the same characters as the original. For example, the abbreviation of “日 2 Comma is also regarded as grammatical case (i.e., null case) here. 3 “obj” is an object case marker. 本銀行 (Bank of Japan)” is “日銀”. The enhanced edit distance is proposed by Yamamoto et al. (2003). The distance is a measure of similarity by counting matching characters between two phrases. Moreover, the distance is assigned a different similarity weight according to the type of matched characters. We apply 1.0 to the weight only if Chinese-derived characters (Kanji) are matched. We link phrases as corresponding phrases, where the phrases are the top three similar to a phrase in the similar instance. (iv) Similarity with Mutual Information We finally compute the similarity with mutual information to link syntactically similar phrases. For example, given the following two expressions: “会"
I08-2102,C98-2122,0,\N,Missing
L18-1072,D16-1034,0,0.0505105,"Missing"
L18-1072,Q16-1029,0,0.0609169,"information for a wide range of users who find it difficult to understand texts that have not been simplified due to their linguistic complexity. Attempts have been made to automate the simplification process for various languages, including English, Spanish, Brazilian Portuguese, and Portuguese. We have been conducted research on simplification since a few years ago (Moku et al., 2012). Recent studies have treated text simplification as a monolingual machine translation problem in which simple and synonymous sentences are generated using statistical machine translation (Wubben et al., 2012; Xu et al., 2016). As with statistical machine translation using bilingual parallel corpora, text simplification therefore requires a monolingual parallel corpus for training. In the case of English, there are PWKP (Zhu et al., 2010) and Wikipedia Datasets (Coster and Kauchak, 2011; Kauchak, 2013) made from wiki and Swiki. In German, a simplification corpus with a scale of 7,000 sentences (Caseli et al., 2009), In Italian the PaCCSS-IT (Brunato et al., 2016) with a scale of 63,000 sentences, In Spanish a simplification corpus do exist ˇ 2014; made by hand by the few rules (Mitkov and Stajner, ˇ Stajner et al.,"
L18-1072,C10-1152,0,0.200109,"arious languages, including English, Spanish, Brazilian Portuguese, and Portuguese. We have been conducted research on simplification since a few years ago (Moku et al., 2012). Recent studies have treated text simplification as a monolingual machine translation problem in which simple and synonymous sentences are generated using statistical machine translation (Wubben et al., 2012; Xu et al., 2016). As with statistical machine translation using bilingual parallel corpora, text simplification therefore requires a monolingual parallel corpus for training. In the case of English, there are PWKP (Zhu et al., 2010) and Wikipedia Datasets (Coster and Kauchak, 2011; Kauchak, 2013) made from wiki and Swiki. In German, a simplification corpus with a scale of 7,000 sentences (Caseli et al., 2009), In Italian the PaCCSS-IT (Brunato et al., 2016) with a scale of 63,000 sentences, In Spanish a simplification corpus do exist ˇ 2014; made by hand by the few rules (Mitkov and Stajner, ˇ Stajner et al., 2015), and so on. However, only English corpora are publicly available, such as a large-scale simplified English corpus obtained from pairs of Wikipedia and Simple English Wikipedia entries. For Japanese, there is n"
L18-1185,P11-2117,0,0.0801229,"Missing"
L18-1185,W16-4912,0,0.118655,"Missing"
L18-1185,P14-2075,0,0.178047,"Missing"
L18-1185,O13-1007,1,0.891401,"Missing"
L18-1185,P15-3006,1,0.87819,"Missing"
L18-1185,L18-1072,1,0.330952,"dy, ˇ Mitkov and Stajner (2014) constructed a simplified corpus with fewer simplification rules. They showed that the SBLEU score of the three annotators is 0.44 to 0.53. Compared their corpus, we observed that our corpus was not dependent on annotators and that it was stable. Table 8 shows a high score of S-BLEU owing to the fact that the simplified corpus consists of only core vocabulary. This restriction helped simple sentences show similarity in expression. 6. Related Works corpus. Therefore, we used crowdsourcing to construct a Japanese simplified corpus containing 34,300 sentence pairs (Katsuta and Yamamoto, 2018). In addition, the corpus contains 100 sentence pairs having 7 references as data for evaluation. The simple sentences consist of only the core vocabulary. We evaluated the crowdsourced corpus from the viewpoint of grammaticality, meaning preservation and interannotator agreement with the same criteria as this paper. Compared to evaluations of the crowdsourced corpus, evaluations in this paper show better results in meaning preservation and inter-annotator agreement. Therefore, the simplified corpus in this paper is higher quality than the simplified corpus constructed using crowdsourcing. 6.2"
L18-1185,W13-2902,0,0.019126,"ting the core vocabulary. The corpus contained 50,000 manually simplified and aligned sentences. This core vocabulary was restricted to 2,000 words, which were selected by accounting for several factors such as meaning preservation, variation, simplicity. Although the vocabulary was restricted, our corpus achieved high quality grammaticality and meaning preservation. In addition, vocabulary restriction helped simplified sentences show similarities of expressions. 6.1. Simplified Corpora There are many simplification resources for various languages (Caseli et al., 2009; Zhu and Bernhard, 2010; Klaper et al., 2013; Brunato et al., 2015; Xu et al., 2015). Text simplification has been researched by various approaches such as lexical simplificaˇ Stajner and Glavaˇs, 2015; tion (Horn et al., 2014; Paetzold, 2016; Paetzold and Specia, 2017), machine translation approaches (Coster and Kauchak, 2011; ˇ ˇ et al., 2015a; Stajner et al., 2015b; Wubben, 2012; Stajner Xu et al., 2016; Nisioi et al., 2017) and rule-based approaches (Siddharthan, 2014) using simplified corpora. However, in Japanese, although attempts have been made for lexical simplification (Kajiwara and Yamamoto, 2013; Imono et al., 2013; Kajiwara"
L18-1185,W14-5604,0,0.0226539,"ed that some low scores were associated with sentences that had become longer and ambiguous owing to vocabulary restriction. 5.3. Inter-Annotator Agreement We asked five simplification annotators to simplify the same 100 sentences, which were selected from the Tanaka Corpus, and evaluated the inter-annotator agreement by SBLEU. These 100 sentences consist of 4 to 16 words from the Tanaka Corpus. In addition, these 100 sentences do not include sentences comprising only the core vocabulary. This evaluation result is shown in Table 8. The values range between 0.58 and 0.63. In a similar study, ˇ Mitkov and Stajner (2014) constructed a simplified corpus with fewer simplification rules. They showed that the SBLEU score of the three annotators is 0.44 to 0.53. Compared their corpus, we observed that our corpus was not dependent on annotators and that it was stable. Table 8 shows a high score of S-BLEU owing to the fact that the simplified corpus consists of only core vocabulary. This restriction helped simple sentences show similarity in expression. 6. Related Works corpus. Therefore, we used crowdsourcing to construct a Japanese simplified corpus containing 34,300 sentence pairs (Katsuta and Yamamoto, 2018). In"
L18-1185,W12-5811,0,0.0506081,"Missing"
L18-1185,P17-2014,0,0.0852167,"Missing"
L18-1185,E17-2006,0,0.464339,"Missing"
L18-1185,E14-1076,0,0.0976449,"show similarities of expressions. 6.1. Simplified Corpora There are many simplification resources for various languages (Caseli et al., 2009; Zhu and Bernhard, 2010; Klaper et al., 2013; Brunato et al., 2015; Xu et al., 2015). Text simplification has been researched by various approaches such as lexical simplificaˇ Stajner and Glavaˇs, 2015; tion (Horn et al., 2014; Paetzold, 2016; Paetzold and Specia, 2017), machine translation approaches (Coster and Kauchak, 2011; ˇ ˇ et al., 2015a; Stajner et al., 2015b; Wubben, 2012; Stajner Xu et al., 2016; Nisioi et al., 2017) and rule-based approaches (Siddharthan, 2014) using simplified corpora. However, in Japanese, although attempts have been made for lexical simplification (Kajiwara and Yamamoto, 2013; Imono et al., 2013; Kajiwara and Yamamoto, 2015; Hading and Matsumoto, 2016), there is no prior research on sentence simplification. The absence of a simplification corpus could be the primary reason for this. Furthermore, there are no large-scale simplified data equivalents to the Simple English Wikipedia; as a result, no attempt has been made to construct a simplification 8. Acknowlegement This work was supported in part by JSPS KAKENHI Grants-in-Aid for"
L18-1185,P15-2011,0,0.0680798,"Missing"
L18-1185,R15-1080,0,0.0185682,"lity grammaticality and meaning preservation. In addition, vocabulary restriction helped simplified sentences show similarities of expressions. 6.1. Simplified Corpora There are many simplification resources for various languages (Caseli et al., 2009; Zhu and Bernhard, 2010; Klaper et al., 2013; Brunato et al., 2015; Xu et al., 2015). Text simplification has been researched by various approaches such as lexical simplificaˇ Stajner and Glavaˇs, 2015; tion (Horn et al., 2014; Paetzold, 2016; Paetzold and Specia, 2017), machine translation approaches (Coster and Kauchak, 2011; ˇ ˇ et al., 2015a; Stajner et al., 2015b; Wubben, 2012; Stajner Xu et al., 2016; Nisioi et al., 2017) and rule-based approaches (Siddharthan, 2014) using simplified corpora. However, in Japanese, although attempts have been made for lexical simplification (Kajiwara and Yamamoto, 2013; Imono et al., 2013; Kajiwara and Yamamoto, 2015; Hading and Matsumoto, 2016), there is no prior research on sentence simplification. The absence of a simplification corpus could be the primary reason for this. Furthermore, there are no large-scale simplified data equivalents to the Simple English Wikipedia; as a result, no attempt has been made to con"
L18-1185,P15-2135,0,0.0142929,"lity grammaticality and meaning preservation. In addition, vocabulary restriction helped simplified sentences show similarities of expressions. 6.1. Simplified Corpora There are many simplification resources for various languages (Caseli et al., 2009; Zhu and Bernhard, 2010; Klaper et al., 2013; Brunato et al., 2015; Xu et al., 2015). Text simplification has been researched by various approaches such as lexical simplificaˇ Stajner and Glavaˇs, 2015; tion (Horn et al., 2014; Paetzold, 2016; Paetzold and Specia, 2017), machine translation approaches (Coster and Kauchak, 2011; ˇ ˇ et al., 2015a; Stajner et al., 2015b; Wubben, 2012; Stajner Xu et al., 2016; Nisioi et al., 2017) and rule-based approaches (Siddharthan, 2014) using simplified corpora. However, in Japanese, although attempts have been made for lexical simplification (Kajiwara and Yamamoto, 2013; Imono et al., 2013; Kajiwara and Yamamoto, 2015; Hading and Matsumoto, 2016), there is no prior research on sentence simplification. The absence of a simplification corpus could be the primary reason for this. Furthermore, there are no large-scale simplified data equivalents to the Simple English Wikipedia; as a result, no attempt has been made to con"
L18-1185,P12-1107,0,0.0566935,"meaning preservation. In addition, vocabulary restriction helped simplified sentences show similarities of expressions. 6.1. Simplified Corpora There are many simplification resources for various languages (Caseli et al., 2009; Zhu and Bernhard, 2010; Klaper et al., 2013; Brunato et al., 2015; Xu et al., 2015). Text simplification has been researched by various approaches such as lexical simplificaˇ Stajner and Glavaˇs, 2015; tion (Horn et al., 2014; Paetzold, 2016; Paetzold and Specia, 2017), machine translation approaches (Coster and Kauchak, 2011; ˇ ˇ et al., 2015a; Stajner et al., 2015b; Wubben, 2012; Stajner Xu et al., 2016; Nisioi et al., 2017) and rule-based approaches (Siddharthan, 2014) using simplified corpora. However, in Japanese, although attempts have been made for lexical simplification (Kajiwara and Yamamoto, 2013; Imono et al., 2013; Kajiwara and Yamamoto, 2015; Hading and Matsumoto, 2016), there is no prior research on sentence simplification. The absence of a simplification corpus could be the primary reason for this. Furthermore, there are no large-scale simplified data equivalents to the Simple English Wikipedia; as a result, no attempt has been made to construct a simpli"
L18-1185,Q16-1029,0,0.0395488,"In addition, vocabulary restriction helped simplified sentences show similarities of expressions. 6.1. Simplified Corpora There are many simplification resources for various languages (Caseli et al., 2009; Zhu and Bernhard, 2010; Klaper et al., 2013; Brunato et al., 2015; Xu et al., 2015). Text simplification has been researched by various approaches such as lexical simplificaˇ Stajner and Glavaˇs, 2015; tion (Horn et al., 2014; Paetzold, 2016; Paetzold and Specia, 2017), machine translation approaches (Coster and Kauchak, 2011; ˇ ˇ et al., 2015a; Stajner et al., 2015b; Wubben, 2012; Stajner Xu et al., 2016; Nisioi et al., 2017) and rule-based approaches (Siddharthan, 2014) using simplified corpora. However, in Japanese, although attempts have been made for lexical simplification (Kajiwara and Yamamoto, 2013; Imono et al., 2013; Kajiwara and Yamamoto, 2015; Hading and Matsumoto, 2016), there is no prior research on sentence simplification. The absence of a simplification corpus could be the primary reason for this. Furthermore, there are no large-scale simplified data equivalents to the Simple English Wikipedia; as a result, no attempt has been made to construct a simplification 8. Acknowlegemen"
L18-1185,C10-1152,0,0.0804299,"as well as through updating the core vocabulary. The corpus contained 50,000 manually simplified and aligned sentences. This core vocabulary was restricted to 2,000 words, which were selected by accounting for several factors such as meaning preservation, variation, simplicity. Although the vocabulary was restricted, our corpus achieved high quality grammaticality and meaning preservation. In addition, vocabulary restriction helped simplified sentences show similarities of expressions. 6.1. Simplified Corpora There are many simplification resources for various languages (Caseli et al., 2009; Zhu and Bernhard, 2010; Klaper et al., 2013; Brunato et al., 2015; Xu et al., 2015). Text simplification has been researched by various approaches such as lexical simplificaˇ Stajner and Glavaˇs, 2015; tion (Horn et al., 2014; Paetzold, 2016; Paetzold and Specia, 2017), machine translation approaches (Coster and Kauchak, 2011; ˇ ˇ et al., 2015a; Stajner et al., 2015b; Wubben, 2012; Stajner Xu et al., 2016; Nisioi et al., 2017) and rule-based approaches (Siddharthan, 2014) using simplified corpora. However, in Japanese, although attempts have been made for lexical simplification (Kajiwara and Yamamoto, 2013; Imono e"
N06-2034,P02-1047,0,0.591589,"Missing"
N06-2034,J05-2005,0,\N,Missing
O13-1007,P02-1028,0,0.131298,"of newspaper article through paraphrasing based on the use of a Japanese dictionary. Fujita et al. [1] and Mino and Tanaka [9] paraphrased the headword of a noun in a dictionary as the headword of another noun by assessing the similarity of the definitions for the two. Yet, as also reported by Mino and Tanaka, the target words acquired by this method are not simpler than the original words. We paraphrase by taking advantage of Japanese dictionary characteristics, namely that “The definition statements are simpler than the headwords” [9], because our aim is lexical simplification. Kaji et al. [3] assumed that the definition statement has an inflectable word as a nominative if the headword is inflectable, and the nominative is placed at the end of the definition statement. Then, they proposed a method for paraphrasing inflectable words. Mino and Tanaka assumed that the last segment of the main sentence in the definition statement represents the meaning of the headword, and they proposed a method for paraphrasing nouns. Kajiwara and Yamamoto [4] assumed that the target word is the same part-of-speech as headword and is placed at the end of the definition statement. They proposed a metho"
O13-1007,W02-1030,0,0.0921216,"Missing"
O13-1007,P01-1046,0,0.0180239,"phrased as the end portion of the definition statement 60 Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) Multiple target word candidates can be acquired by making use of the entire definition statement. Therefore, a process is needed for selecting the most appropriate target words. In the study of the selection of target words, researchers employ various methods such as assessing semantic similarity based on data from a thesaurus [7] or using the statistical information from large resources based on the distributional hypothesis [3][6]. Thesauruses provide hierarchical semantic classifications of words. By measuring the semantic distance between words in the thesaurus, it is possible to measure the proximity of meaning between words. Furthermore, according to the distributional hypothesis [2], words with similar meanings are often used in similar contexts. Based on this hypothesis, Lapata et al. and Keller et al. reported that the plausibility determination of the expression can be achieved by utilizing co-occurrence frequency and n-gram. In this paper, in order to maintain as much of the original meaning as possible in the"
O13-1007,W10-1308,0,0.163054,"inition：【大詰め】芝居の最 後 の場面 paraphrase：大 詰 め の大一番 → 最 後 の大一番 Figure 1: Example of a word that cannot be paraphrased as the end portion of the definition statement 60 Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) Multiple target word candidates can be acquired by making use of the entire definition statement. Therefore, a process is needed for selecting the most appropriate target words. In the study of the selection of target words, researchers employ various methods such as assessing semantic similarity based on data from a thesaurus [7] or using the statistical information from large resources based on the distributional hypothesis [3][6]. Thesauruses provide hierarchical semantic classifications of words. By measuring the semantic distance between words in the thesaurus, it is possible to measure the proximity of meaning between words. Furthermore, according to the distributional hypothesis [2], words with similar meanings are often used in similar contexts. Based on this hypothesis, Lapata et al. and Keller et al. reported that the plausibility determination of the expression can be achieved by utilizing co-occurrence freq"
O13-1007,W12-5811,0,0.0813293,"cquires the target word from the end of the definition statements. Keywords: Lexical Simplification, Lexical Paraphrase. 1. Introduction In the current information age, a various readers have easy access to diverse text data. To achieve information transmission and gathering effectively, we must address the gap in readers’ linguistic skills. The gap of linguistic skills results from differences in age, such as between children and adults, as well as from differences in expert knowledge. In the effort to bridge this gap, and also to facilitate better communication with foreign language speakers[8] and people with disabilities, technology can play an important role. To investigate how technology can be applied toward bridging the gap in readers’ linguistic skills, we simplify the text of newspaper articles containing words that pose difficulties in communication, especially for elementary school students. Children are still developing their language skills, and as such, they have smaller vocabularies than adults. In this paper, we perform text simplification for children by paraphrasing selected newspaper articles using only words found in Basic Vocabulary to Learn (BVL)(3). BVL is a co"
O13-1007,W02-1411,1,0.712847,"(BVL)(3). BVL is a collection of words selected based on a lexical analysis of elementary school textbooks. It contains 5,404 words that can help children write expressively. We define words not included in BVL as Difficult Words (DWs) and those in BVL paraphrased from DW as Simple Words (SWs). Paraphrasing newspaper articles using words that children can understand makes a great contribution to reading assistance for young students. 59 Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 2. Related Works Although there are some methods [10] proposed for automatically acquiring paraphrasable expressions from Web pages, the quality of the results are still unsatisfactory. Hence typical methods use thesauri or dictionaries. Thesaurus is a language resource that contains semantically classified vocabulary words. Methods that utilize a thesaurus have an advantage in that they can measure the semantic relatedness between words (i.e., the distance between meanings). Japanese dictionaries are another language resource that provides the definition of a given lemma. Methods that utilize a dictionary have an advantage in that they are able"
P06-1090,J03-1002,0,0.0599068,"!     !    !    (6) ! where and  are words in the target and source phrases. The phrase alignment based on Equation (5) can be thought of as an extension of word alignment based on the IBM Model 1 to phrase alignment. Note that bilingual phrase segmentation (phrase extraction) is also done using the same criteria. The approximation in Equation (6) is motivated by (Vogel et al., 2003). Here, we added the second !  term    to cope with the asymmetry between !     !2  and    . The word translation probabilities are estimated using the GIZA++ (Och and Ney, 2003). The above search is implemented in the following way:  1. All source word and target word pairs are considered to be initial phrase pairs. 715 2. If the phrase translation probability of the phrase pair is less than the threshold, it is deleted. _ the_light_was_red 3. Each phrase pair is expanded toward the eight neighboring directions as shown in Figure 3. _ (2) _ the_light 4. If the phrase translation probability of the expanded phrase pair is less than the threshold, it is deleted. (3) _ was_red _ 5. The process of expansion and deletion is repeated until no further expansion is possible"
P06-1090,J04-4002,0,0.0601504,"47 Japan nagata.masaaki@labs.ntt.co.jp, saito.kuniko@labs.ntt.co.jp Kazuhide Yamamoto, Kazuteru Ohashi Nagaoka University of Technology 1603-1, Kamitomioka, Nagaoka City Niigata, 940-2188 Japan ykaz@nlp.nagaokaut.ac.jp, ohashi@nlp.nagaokaut.ac.jp Abstract Standard phrase-based translation systems use a word distance-based reordering model in which non-monotonic phrase alignment is penalized based on the word distance between successively translated source phrases without considering the orientation of the phrase alignment or the identities of the source and target phrases (Koehn et al., 2003; Och and Ney, 2004). (Tillmann and Zhang, 2005) introduced the notion of a block (a pair of source and target phrases that are translations of each other), and proposed the block orientation bigram in which the local reordering of adjacent blocks are expressed as a three-valued orientation, namely Right (monotone), Left (swapped), or Neutral. A block with neutral orientation is supposed to be less strongly linked to its predecessor block: thus in their model, the global reordering is not explicitly modeled. In this paper, we present a global reordering model that explicitly models long distance reordering1 . It"
P06-1090,W99-0604,0,0.0446249,"se-English bilingual sentence. For the estimation of the global phrase reordering model, preliminary tests have shown that the appropriate N-best number is 20. In counting the events for the relative frequency estimation, we treat all N-best phrase alignments equally. For comparison, we also implemented a different N-best phrase alignment method, where                 _ _ _ (1) 4.2 Bilingual Phrase Clustering The second approach to cope with the sparseness in Equation (4) is to group the phrases into equivalence classes. We used a bilingual word clustering tool, mkcls (Och et al., 1999) for this purpose. It forms partitions of the vocabulary of the two languages to maximize the joint probability of the training bilingual corpus. In order to perform bilingual phrase clustering, all words in a phrase are concatenated by an underscore ’ ’ to form a pseudo word. We then use the modified bilingual sentences as the input to mkcls. We treat all N-best phrase alignments equally. Thus, the phrase alignments in Figure 4 are converted to the following three bilingual sentence pairs.  the_light_was_red   _  _ _""! _ # _ ""! _ # the_light  _  was_red _ $! _ #"
P06-1090,2005.iwslt-1.16,1,0.828703,"bilingual sentences as the input to mkcls. We treat all N-best phrase alignments equally. Thus, the phrase alignments in Figure 4 are converted to the following three bilingual sentence pairs.  the_light_was_red   _  _ _""! _ # _ ""! _ # the_light  _  was_red _ $! _ #             the_light was red Preliminary tests have shown that the appropriate number of classes for the estimation of the global phrase reordering model is 20. As a comparison, we also tried two phrase classification methods based on the part of speech of the head word (Ohashi et al., 2005). We defined (arguably) the first word of each English phrase and the last word of each Japanese phrase as the 716 shorthand baseline "" e[0] f[0] e[0]f[0] e[-1]f[0] e[0]f[-1,0] e[-1]f[-1,0] e[-1,0]f[0] e[-1,0]f[-1,0] reordering ->K - model G H IJ  JMLN H '$ !2 '$  ! [ '$   ! A !  '$    !5-  A !  '$    ! A !5-  A !  '$    !5-  A !5-  A ![ '$    !5- A  ! A !   '$    !5- A   ! A ! 5- A ![   '$      Japanese English Vocabulary 9,277 6,956 Language Translation) 2005 is an evaluation campaign for spoken language transl"
P06-1090,P02-1040,0,0.107293,"Missing"
P06-1090,P05-1069,0,0.515194,"ki@labs.ntt.co.jp, saito.kuniko@labs.ntt.co.jp Kazuhide Yamamoto, Kazuteru Ohashi Nagaoka University of Technology 1603-1, Kamitomioka, Nagaoka City Niigata, 940-2188 Japan ykaz@nlp.nagaokaut.ac.jp, ohashi@nlp.nagaokaut.ac.jp Abstract Standard phrase-based translation systems use a word distance-based reordering model in which non-monotonic phrase alignment is penalized based on the word distance between successively translated source phrases without considering the orientation of the phrase alignment or the identities of the source and target phrases (Koehn et al., 2003; Och and Ney, 2004). (Tillmann and Zhang, 2005) introduced the notion of a block (a pair of source and target phrases that are translations of each other), and proposed the block orientation bigram in which the local reordering of adjacent blocks are expressed as a three-valued orientation, namely Right (monotone), Left (swapped), or Neutral. A block with neutral orientation is supposed to be less strongly linked to its predecessor block: thus in their model, the global reordering is not explicitly modeled. In this paper, we present a global reordering model that explicitly models long distance reordering1 . It predicts four type of reorde"
P06-1090,N03-1017,0,0.181266,"pan Kanagawa, 239-0847 Japan nagata.masaaki@labs.ntt.co.jp, saito.kuniko@labs.ntt.co.jp Kazuhide Yamamoto, Kazuteru Ohashi Nagaoka University of Technology 1603-1, Kamitomioka, Nagaoka City Niigata, 940-2188 Japan ykaz@nlp.nagaokaut.ac.jp, ohashi@nlp.nagaokaut.ac.jp Abstract Standard phrase-based translation systems use a word distance-based reordering model in which non-monotonic phrase alignment is penalized based on the word distance between successively translated source phrases without considering the orientation of the phrase alignment or the identities of the source and target phrases (Koehn et al., 2003; Och and Ney, 2004). (Tillmann and Zhang, 2005) introduced the notion of a block (a pair of source and target phrases that are translations of each other), and proposed the block orientation bigram in which the local reordering of adjacent blocks are expressed as a three-valued orientation, namely Right (monotone), Left (swapped), or Neutral. A block with neutral orientation is supposed to be less strongly linked to its predecessor block: thus in their model, the global reordering is not explicitly modeled. In this paper, we present a global reordering model that explicitly models long distan"
P06-1090,2003.mtsummit-papers.53,0,0.0154005,"A      C  /     E    C  !M0    N N (5) Phrase translation probabilities are approximated  !  using word translation probabilities    and !     as follows,  /        !     !    !    (6) ! where and  are words in the target and source phrases. The phrase alignment based on Equation (5) can be thought of as an extension of word alignment based on the IBM Model 1 to phrase alignment. Note that bilingual phrase segmentation (phrase extraction) is also done using the same criteria. The approximation in Equation (6) is motivated by (Vogel et al., 2003). Here, we added the second !  term    to cope with the asymmetry between !     !2  and    . The word translation probabilities are estimated using the GIZA++ (Och and Ney, 2003). The above search is implemented in the following way:  1. All source word and target word pairs are considered to be initial phrase pairs. 715 2. If the phrase translation probability of the phrase pair is less than the threshold, it is deleted. _ the_light_was_red 3. Each phrase pair is expanded toward the eight neighboring directions as shown in Figure 3. _ (2) _ the_light 4. If the phrase tra"
P06-1090,C04-1030,0,0.0254999,", 2003). We call this conventional phrase extraction method “grow-diag-final”, and the proposed phrase extraction method “ppicker” (this is intended to stand for phrase picker). The search for consistent Viterbi phrase alignments can be implemented as a phrase-based decoder using a beam search whose outputs are constrained only to the target sentence. The consistent N-best phrase alignment can be obtained by using A* search as described in (Ueffing et al., 2002). We did not use any reordering constraints, such as IBM constraint and ITG constraint in the search for the N-best phrase alignment (Zens et al., 2004). The thresholds used in the search are the following: the minimum phrase translation probability is 0.0001. The maximum number of translation candidates for each phrase is 20. The beam width is 1e-10, the stack size (for each target candidate word length) is 1000. We found that, compared with the decoding of sentence translation, we have to search significantly larger space for the N-best phrase alignment. Figure 3 shows an example of phrase pair expansion toward eight neighbors. If the current phrase pair is ( , of), the expanded phrase , means of), ( , pairs are ( means of), ( , means of),"
P06-1090,W02-1021,0,\N,Missing
P06-1090,2005.iwslt-1.1,0,\N,Missing
P15-3006,S07-1009,0,0.385377,"n system that supports reading comprehension of a wide range of readers, including children and language learners. The other is a dataset for evaluation that enables open discussions with other systems. Both the system and the dataset are made available providing the first such resources for the Japanese language. 1 http://www.jnlp.org/SNOW 2 Previous Work Two datasets for evaluation of English lexical simplification have been published. Both were constructed by transforming a lexical substitution dataset, which was constructed in an English lexical substitution task of SemEval-2007 workshop (McCarthy and Navigli, 2007). Introduction Lexical simplification is a technique that substitutes a complex word or phrase in a sentence with a simpler synonym. This technique supports the reading comprehension of a wide range of readers, including children (Belder and Moens, 2010; Kajiwara et al., 2013) and language learners (Eom et al., 2012; Moku et al., 2012). The recent years have seen a great activity in this field of inquiry, especially for English: At the SemEval-2012 workshop, many systems were participating in the English lexical simplification task (Specia et al., 2012), for which also an evaluation dataset wa"
P15-3006,W12-5811,0,0.126157,"revious Work Two datasets for evaluation of English lexical simplification have been published. Both were constructed by transforming a lexical substitution dataset, which was constructed in an English lexical substitution task of SemEval-2007 workshop (McCarthy and Navigli, 2007). Introduction Lexical simplification is a technique that substitutes a complex word or phrase in a sentence with a simpler synonym. This technique supports the reading comprehension of a wide range of readers, including children (Belder and Moens, 2010; Kajiwara et al., 2013) and language learners (Eom et al., 2012; Moku et al., 2012). The recent years have seen a great activity in this field of inquiry, especially for English: At the SemEval-2012 workshop, many systems were participating in the English lexical simplification task (Specia et al., 2012), for which also an evaluation dataset was constructed. Other resources for statistical learning of simplified rules were built, drawing on the Simple English Wikipedia (Zhu et al., 2010; Horn et al., 2014), e.g. several parallel corpora aligning standard and simple English (Zhu et al., 2010; Kauchak, 2013)1,2 and evaluation datasets (Specia et al., 2012; Belder and Moens, 20"
P15-3006,W12-2038,0,0.0232923,".jnlp.org/SNOW 2 Previous Work Two datasets for evaluation of English lexical simplification have been published. Both were constructed by transforming a lexical substitution dataset, which was constructed in an English lexical substitution task of SemEval-2007 workshop (McCarthy and Navigli, 2007). Introduction Lexical simplification is a technique that substitutes a complex word or phrase in a sentence with a simpler synonym. This technique supports the reading comprehension of a wide range of readers, including children (Belder and Moens, 2010; Kajiwara et al., 2013) and language learners (Eom et al., 2012; Moku et al., 2012). The recent years have seen a great activity in this field of inquiry, especially for English: At the SemEval-2012 workshop, many systems were participating in the English lexical simplification task (Specia et al., 2012), for which also an evaluation dataset was constructed. Other resources for statistical learning of simplified rules were built, drawing on the Simple English Wikipedia (Zhu et al., 2010; Horn et al., 2014), e.g. several parallel corpora aligning standard and simple English (Zhu et al., 2010; Kauchak, 2013)1,2 and evaluation datasets (Specia et al., 2012;"
P15-3006,P14-2075,0,0.095193,"supports the reading comprehension of a wide range of readers, including children (Belder and Moens, 2010; Kajiwara et al., 2013) and language learners (Eom et al., 2012; Moku et al., 2012). The recent years have seen a great activity in this field of inquiry, especially for English: At the SemEval-2012 workshop, many systems were participating in the English lexical simplification task (Specia et al., 2012), for which also an evaluation dataset was constructed. Other resources for statistical learning of simplified rules were built, drawing on the Simple English Wikipedia (Zhu et al., 2010; Horn et al., 2014), e.g. several parallel corpora aligning standard and simple English (Zhu et al., 2010; Kauchak, 2013)1,2 and evaluation datasets (Specia et al., 2012; Belder and Moens, 2012)3,4 . On the other hand, there have been no published resources on Japanese lexical simplification so far. 2.1 McCarthy Substitution Dataset The English lexical substitution task of SemEval2007 requires that the system finds words or phrases that one can substitute for the given target word in the given content. These target words are content words, and their details are shown in Table 1. These contexts are selected from"
P15-3006,S12-1046,0,0.0417308,"on task of SemEval-2007 workshop (McCarthy and Navigli, 2007). Introduction Lexical simplification is a technique that substitutes a complex word or phrase in a sentence with a simpler synonym. This technique supports the reading comprehension of a wide range of readers, including children (Belder and Moens, 2010; Kajiwara et al., 2013) and language learners (Eom et al., 2012; Moku et al., 2012). The recent years have seen a great activity in this field of inquiry, especially for English: At the SemEval-2012 workshop, many systems were participating in the English lexical simplification task (Specia et al., 2012), for which also an evaluation dataset was constructed. Other resources for statistical learning of simplified rules were built, drawing on the Simple English Wikipedia (Zhu et al., 2010; Horn et al., 2014), e.g. several parallel corpora aligning standard and simple English (Zhu et al., 2010; Kauchak, 2013)1,2 and evaluation datasets (Specia et al., 2012; Belder and Moens, 2012)3,4 . On the other hand, there have been no published resources on Japanese lexical simplification so far. 2.1 McCarthy Substitution Dataset The English lexical substitution task of SemEval2007 requires that the system"
P15-3006,C10-1152,0,0.0797595,"ym. This technique supports the reading comprehension of a wide range of readers, including children (Belder and Moens, 2010; Kajiwara et al., 2013) and language learners (Eom et al., 2012; Moku et al., 2012). The recent years have seen a great activity in this field of inquiry, especially for English: At the SemEval-2012 workshop, many systems were participating in the English lexical simplification task (Specia et al., 2012), for which also an evaluation dataset was constructed. Other resources for statistical learning of simplified rules were built, drawing on the Simple English Wikipedia (Zhu et al., 2010; Horn et al., 2014), e.g. several parallel corpora aligning standard and simple English (Zhu et al., 2010; Kauchak, 2013)1,2 and evaluation datasets (Specia et al., 2012; Belder and Moens, 2012)3,4 . On the other hand, there have been no published resources on Japanese lexical simplification so far. 2.1 McCarthy Substitution Dataset The English lexical substitution task of SemEval2007 requires that the system finds words or phrases that one can substitute for the given target word in the given content. These target words are content words, and their details are shown in Table 1. These context"
P15-3006,O13-1007,1,0.853157,"ources for the Japanese language. 1 http://www.jnlp.org/SNOW 2 Previous Work Two datasets for evaluation of English lexical simplification have been published. Both were constructed by transforming a lexical substitution dataset, which was constructed in an English lexical substitution task of SemEval-2007 workshop (McCarthy and Navigli, 2007). Introduction Lexical simplification is a technique that substitutes a complex word or phrase in a sentence with a simpler synonym. This technique supports the reading comprehension of a wide range of readers, including children (Belder and Moens, 2010; Kajiwara et al., 2013) and language learners (Eom et al., 2012; Moku et al., 2012). The recent years have seen a great activity in this field of inquiry, especially for English: At the SemEval-2012 workshop, many systems were participating in the English lexical simplification task (Specia et al., 2012), for which also an evaluation dataset was constructed. Other resources for statistical learning of simplified rules were built, drawing on the Simple English Wikipedia (Zhu et al., 2010; Horn et al., 2014), e.g. several parallel corpora aligning standard and simple English (Zhu et al., 2010; Kauchak, 2013)1,2 and ev"
P15-3006,P13-1151,0,0.0458915,"; Kajiwara et al., 2013) and language learners (Eom et al., 2012; Moku et al., 2012). The recent years have seen a great activity in this field of inquiry, especially for English: At the SemEval-2012 workshop, many systems were participating in the English lexical simplification task (Specia et al., 2012), for which also an evaluation dataset was constructed. Other resources for statistical learning of simplified rules were built, drawing on the Simple English Wikipedia (Zhu et al., 2010; Horn et al., 2014), e.g. several parallel corpora aligning standard and simple English (Zhu et al., 2010; Kauchak, 2013)1,2 and evaluation datasets (Specia et al., 2012; Belder and Moens, 2012)3,4 . On the other hand, there have been no published resources on Japanese lexical simplification so far. 2.1 McCarthy Substitution Dataset The English lexical substitution task of SemEval2007 requires that the system finds words or phrases that one can substitute for the given target word in the given content. These target words are content words, and their details are shown in Table 1. These contexts are selected from the English Internet Corpus, which is a balanced and web-based corpus of English (Sharoff, 2006). This"
P15-3006,N06-1023,0,0.0111353,"ist of sets of the form {predicate, relation, argument}, where the candidate substitutions are used instead of the complex word (so there will be as many of these sets as there are candidate substitutions). These new sets are checked against the Kyoto University Case Frame18 . If the set is found there, the candidate substitution counts as a legitimate substitution; if the set is not found, the candidate substitution is not counted as a legitimate substitution. Kyoto University Case Frame is the list of predicate and argument pairs that have a case relationship, and it is built automatically (Kawahara and Kurohashi, 2006) from Web texts. 5.4 Adjective 3 Table 4: POS of the simplified target words Japanese WordNet Synonyms Database15 , Verb Entailment Database16 , and Case Base for Basic Semantic Relations16 , following previous research (Kajiwara and Yamamoto, 2014). 5.3 Verb 65 • It is {distributed –> dealt} to a {caller –> visitor} from foreign countries. • {Principal –> President} Takagi of the bank presented an idea. 6 Final Remarks We built a Japanese lexical simplification system and a dataset for evaluation of Japanese lexical simplification. Subsequently, we have published these resources on the Web. T"
P15-3006,W04-3230,0,\N,Missing
P15-3006,P11-1081,0,\N,Missing
P98-1070,C94-1015,1,0.803911,"Missing"
P98-1070,C96-1070,1,0.839416,"Missing"
P98-1070,W97-0404,0,0.595116,"Missing"
P98-2233,P95-1017,0,0.0745593,"Missing"
P98-2233,C96-2137,0,0.0886788,"on of ellipses is performed by another module, such as a parser. We only considered ellipses that are commonly and dearly identified. 2 When t o R e s o l v e E l l i p s i s in M T ? As described above, our major application for ellipsis resolution is in machine translation. In an MT process, there can be several approaches about the timing of ellipsis resolution: when analyzing the source language, when generating the target language, or at the same time as translating process. Among these candidates, most of the previous works with Japanese chose the source-language approach. For instance, Nakaiwa and Shirai (1996) attempted to resolve Japanese ellipsis in the source language analysis of J-to-E MT, despite utilizing targetdependent resolution candidates. We originally thought that ellipsis resolution in the MT was a generation problem, namely a target-driven problem which utilizes some help, if necessary, of source-language information. This is because the problem is outputdependent and it relies on demands from a target language. In the J-to-Korean or J-toChinese MT, all or most of the ellipses that must be resolved in J-to-E are not necessary to resolve. However, we adopted source-language policy in t"
P98-2233,C94-2116,0,0.0297388,"psis is anaphoric; in case we need to refer back to the antecedent in the dialogue. In this paper we are not concerned with resolving the antecedent that such ellipses refer to, because it is necessary to have another module to deal with the context for resolving such endophoric ellipses, and the main target of this paper is the exophoric ellipses. 3.2 Since a huge text corpus has become widely available, the machine-learning approach has been utilized for some problems in natural language processing. The most popular touchstone in this field is the verbal case frame or the translation rules (Tanaka, 1994). Machine-learning algorithm has also been attempted to solve some Meaning first person, singular first person, plural second person, singular second person, plural person(s) ~n general anaphoric Learning Method We used the C~.5 algorithm by Quinlan (1993), which is a well-known automatic classifier that produces a binary decision tree. Although it may be necessary to prune decision trees, no pruning is performed throughout this experiment, since we want to concentrate the discussion on the feasibility of machine learning. As shown in the experiment by Aone and BenTable 2: Number of training a"
P98-2233,J97-1001,0,\N,Missing
shirai-etal-2002-towards,1999.tmi-1.21,1,\N,Missing
shirai-etal-2002-towards,2001.mtsummit-road.1,0,\N,Missing
W02-1411,P01-1008,0,0.0808292,"recently. Paraphrasing involves various types of transformations of expressions into the same language, and thus there is generally no all-purpose design and information resource. Among the many types of paraphrasing, a handwritten construction may be best for syntactic paraphrasing knowledge or knowledge of functional words because the number of resulting phenomena can be counted. On the other hand, we need to acquire lexical paraphrasing knowledge automatically or eﬃciently, since there is an enormous number of phenomena observed for an enormous number of content words. Some works, such as Barzilay and McKeown (2001), have acquired paraphrasing knowledge automatically. All of those works found differences from a paraphrase corpus, where each expression is aligned to another expression (or more) with the same meaning and in the same language. Unfortunately, there is no paraphrase corpus widely available except for a few collections such as those prepared by Shirai et al. (2001) and Zhang et al. (2001). Most of those works collected paraphrase corpora by employing special situations, such as multiple news resources from the same events or multiple translations of the same (and well-known) story in other lan"
W02-1411,C94-1015,0,0.0163488,"Missing"
W02-1411,P97-1004,0,0.0618504,"araphrasing knowledge obtained in this process. Not only paraphrases of content words but also paraphrase knowledge of the following types were obtained in this experiment. • insertion and deletion of the particle “の” • paraphrasing for case particles; in Japanese, it may be possible to change a particle under a certain context. • voice conversion • diﬀerent description of the same word, e.g., from a Chinese-origin word to a native Japanese word 5 Related Works Lexical paraphrasing is very useful in information retrieval, since it is necessary to expand terms for improving retrieval coverage. Jacquemin et al. (1997) have proposed acquiring syntactic and morpho-syntactic variations of the multi-word terms using a corpus-based approach. They have searched for variation, i.e., similar expressions using (a part of) the input words, such as technique for measurement against measurement technique, while our target is the paraphrase of a single content word. The goal of our work is to obtain lexical knowledge for paraphrasing. For this purpose we use contextual similarity, which is also used in the sense similarity computation task in the ﬁelds of natural language processing, artiﬁcial intelligence, and cogniti"
W02-1411,W00-0110,0,0.0135672,"perty while paraphrasability does not (explained in 3.3). Second, similarity is a relative measure while paraphrasability is an absolute measure; in many cases, we can answer “Can E1 paraphrase to E2 ?”, but it is hard to answer “Is E1 similar to E2 ?”. In other words, it is important to collect paraphrases while it may be pointless to collect similar words, since the border for the former is clearer than that of the latter. The kind of information used for deﬁning context is important. For this question, Nagamatsu and Tanaka (1996) used a deep case (seen in a semantically tagged corpus), and Kanzaki et al. (2000) only extracted relations of nominal modiﬁcation. The most closely related work in terms of similarity source is the work of Grefenstette (1994), where they obtained subject-verb, verbobject, adjective-noun, and noun-noun relations from a corpus. In contrast, as discussed in subsection 3.1, we propose extracting all of the dependency relations around content words, i.e., nouns, verbs, and adjectives. This is the ﬁrst attempt to introduce these features into a context deﬁnition, and it is obvious that coverage of extracted pairs becomes wider by using various features. However, we have not cond"
W02-1411,P99-1062,0,0.0157231,"Despite the existence of synonymy, even if expression E1 can be replaced by expression E2 , it is unsure whether E2 can be paraphrased by using E1 . We discuss this feature in the experiments. 3 2 3.1 Contextual Similarity vs. Synonymy Paraphrasability is the degree of replacability for two expressions E1 and E2 , which are regarded as diﬀerent from each other in some sense. This deﬁnition implies the notion that E1 should not be judged as the same (or similar) as E2 in the sense of meaning. Of course, similarity of meaning and paraphrasability are very closely correlated with each other, and Kurohashi and Sakai (1999) utilize this feature to paraphrase Japanese expressions (in order to comprehend them more easily). They use a Japanese dictionary written for humans (or more precisely, children) to replace a part of the target expression with a diﬀerent one by judging its local context computed by a thesaurus. We propose that replacability (obtained by the corpus, for example) is a more important factor in judging the paraphrasability of expressions than their meaning as deﬁned in a dictionary. For example, words used only in some special situations, such as for children or in ancient documents, should not b"
W02-1411,Y96-1039,0,0.0121096,"critical diﬀerences between the two tasks. First, similarity satisﬁes the symmetrical property while paraphrasability does not (explained in 3.3). Second, similarity is a relative measure while paraphrasability is an absolute measure; in many cases, we can answer “Can E1 paraphrase to E2 ?”, but it is hard to answer “Is E1 similar to E2 ?”. In other words, it is important to collect paraphrases while it may be pointless to collect similar words, since the border for the former is clearer than that of the latter. The kind of information used for deﬁning context is important. For this question, Nagamatsu and Tanaka (1996) used a deep case (seen in a semantically tagged corpus), and Kanzaki et al. (2000) only extracted relations of nominal modiﬁcation. The most closely related work in terms of similarity source is the work of Grefenstette (1994), where they obtained subject-verb, verbobject, adjective-noun, and noun-noun relations from a corpus. In contrast, as discussed in subsection 3.1, we propose extracting all of the dependency relations around content words, i.e., nouns, verbs, and adjectives. This is the ﬁrst attempt to introduce these features into a context deﬁnition, and it is obvious that coverage of"
W02-1411,C02-1163,1,0.755312,"sed in a paraphrase even though it has synonymy. On the contrary, even if synonymy is not satisﬁed, we still focus on expressions that are replaceable. Hypernymy is one example of this. A hypernym is not a paraphrase in a strict sense due to the loss of information. However, this kind of paraphrasing is still useful from the engineering point of view. For instance, these loose (and therefore many) paraphrases are more eﬀective in the case of reluctant processing, where we must necessarily change an expression for various reasons such as our requirement in paraphrase-based machine translation (Yamamoto, 2002). Moreover, this kind of paraphrase loses nothing when it is used as anaphora or when it is trivial and out of major interest in the context used. Not all hypernyms are always paraphrasable, so we cannot list this type of paraphrase from only a thesaurus. Approach and Implementation This section describes our approach to acquiring paraphrase knowledge from a text corpus. We use Perl programming language to implement all of the following processes and experiments. Collection of context from corpus We ﬁrst deﬁne the term context in this paper. The context of a certain content word is deﬁned as d"
W10-3302,bond-etal-2008-boot,0,0.0416792,"Missing"
W10-3302,C92-2082,0,0.0451113,"n of the concept referred to by the title. We applied language dependent lexicosyntactic patterns to the definition sentence to extract the hypernym. The hypernym of the category name is extracted from the definition sentence if it exists. If there is an article whose title is the same as its category, the hypernym of the article is used as that of the category. As for lexico-syntactic patterns, we used almost the same patterns described in previous work related to Japanese such as (Kobayashi et al., 2008; Sumida et al., 2008), which is basically equivalent to work related to English such as (Hearst, 1992). Here are some examples. Category Alignment For each leaf category in Goi-Taikei, we first make a list of junction category candidates. Wikipedia categories satisfying at least one of the following three conditions are extracted as candidates: • The Goi-Taikei category name exactly matches the Wikipedia category name. • One of the instances of the Goi-Taikei category exactly matches the Wikipedia category name. • More than two instances of the Goi-Taikei category exactly match either instances or subcategories of the Wikipedia category. Here, an instance of a Goi-Taikei category refers to wor"
W10-3302,sumida-etal-2008-boosting,0,0.0194643,"category in advance. We regard the first sentence of each article page as the definition of the concept referred to by the title. We applied language dependent lexicosyntactic patterns to the definition sentence to extract the hypernym. The hypernym of the category name is extracted from the definition sentence if it exists. If there is an article whose title is the same as its category, the hypernym of the article is used as that of the category. As for lexico-syntactic patterns, we used almost the same patterns described in previous work related to Japanese such as (Kobayashi et al., 2008; Sumida et al., 2008), which is basically equivalent to work related to English such as (Hearst, 1992). Here are some examples. Category Alignment For each leaf category in Goi-Taikei, we first make a list of junction category candidates. Wikipedia categories satisfying at least one of the following three conditions are extracted as candidates: • The Goi-Taikei category name exactly matches the Wikipedia category name. • One of the instances of the Goi-Taikei category exactly matches the Wikipedia category name. • More than two instances of the Goi-Taikei category exactly match either instances or subcategories of"
W10-3404,W07-1407,0,0.0753014,"Missing"
W10-3404,W09-3401,0,\N,Missing
W10-3404,P02-1040,0,\N,Missing
W10-3404,P09-1051,0,\N,Missing
W10-3501,sumida-etal-2008-boosting,0,0.0197526,"onductors Structural relation A. The target Wikipedia category label. is-a Category without parent and child Conductors is-a As most category labels and D-hypernyms are common nouns, they are likely to match instances in Goi-Taikei which lists possible semantic categories of words. is-a Composers Announcers Japanese conductors Figure 4: Example of Wikipedia category hierarchy (top) and constructed Wikipedia person category hierarchy (bottom) 5 As for D-hypernym extraction patterns, we used almost the same patterns described in previous works on Japanese sources such as (Kobayashi et al. 2008; Sumida et al., 2008), which are basically equivalent to the works on English sources such as (Hearst, 1992). 4 After the texts located at various structural relations A-F are collected, they are matched to the instances of Goi-Taikei in two different spans: features related to B-Ⅱ, and the relative frequencies used for the feature value related B-Ⅱ are 0, 0.5, 0.5, 0, respectively. In this way, we use 48 relative frequencies calculated from the combinations of structural relation A-F, span Ⅰ and Ⅱ, and semantic type a-d, as the feature vector for the SVM. Span of the text Ⅰ. All character strings of the text Ⅱ. T"
W10-3501,bond-etal-2008-boot,0,0.0313082,"Missing"
W10-3501,C92-2082,0,0.020447,"rent and child Conductors is-a As most category labels and D-hypernyms are common nouns, they are likely to match instances in Goi-Taikei which lists possible semantic categories of words. is-a Composers Announcers Japanese conductors Figure 4: Example of Wikipedia category hierarchy (top) and constructed Wikipedia person category hierarchy (bottom) 5 As for D-hypernym extraction patterns, we used almost the same patterns described in previous works on Japanese sources such as (Kobayashi et al. 2008; Sumida et al., 2008), which are basically equivalent to the works on English sources such as (Hearst, 1992). 4 After the texts located at various structural relations A-F are collected, they are matched to the instances of Goi-Taikei in two different spans: features related to B-Ⅱ, and the relative frequencies used for the feature value related B-Ⅱ are 0, 0.5, 0.5, 0, respectively. In this way, we use 48 relative frequencies calculated from the combinations of structural relation A-F, span Ⅰ and Ⅱ, and semantic type a-d, as the feature vector for the SVM. Span of the text Ⅰ. All character strings of the text Ⅱ. The last word of the text 芸術 Art For the span Ⅱ, the text is segmented into words using"
W10-3906,P06-1045,0,0.729326,"Missing"
W10-3906,P90-1034,0,0.663694,"Missing"
W10-3906,P99-1004,0,0.153337,"Missing"
W10-3906,P98-2127,0,0.945764,"computing distributional similarity is same; for each of two input words a context (i.e., surrounding words) is extracted from a corpus, a vector is made in which an element of the vector is a value or a weight, and two vectors are compared with a formula to compute similarity. Among these processes we have focused on features, that are elements of 32 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 32–39, Beijing, August 2010 the vector, some of which, we think, adversely affect the performance. That is, traditional approaches such as Lin (1998) basically use all of observed words as context, that causes noise in feature vector comparison. One may agree that the number of the characteristic words to determine the meaning of a word is some, not all, of words around the target word. Thus our goal is to detect and reduce such noisy features. Zhitomirsky-Geffet and Dagan (2009) have same motivation with us and introduced a bootstrapping strategy that changes the original features weights. The general idea here is to promote the weights of features that are common for associated words, since these features are likely to be most characteri"
W10-3906,C02-1144,0,0.055583,"Missing"
W10-3906,J05-4002,0,0.0461553,"Missing"
W10-3906,J09-3004,0,0.39388,"focused on features, that are elements of 32 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 32–39, Beijing, August 2010 the vector, some of which, we think, adversely affect the performance. That is, traditional approaches such as Lin (1998) basically use all of observed words as context, that causes noise in feature vector comparison. One may agree that the number of the characteristic words to determine the meaning of a word is some, not all, of words around the target word. Thus our goal is to detect and reduce such noisy features. Zhitomirsky-Geffet and Dagan (2009) have same motivation with us and introduced a bootstrapping strategy that changes the original features weights. The general idea here is to promote the weights of features that are common for associated words, since these features are likely to be most characteristic for determining the word’s meaning. In this paper, we propose instead a method to using features that are both unassociated to the two input words, in addition to use of features that are associated to the input. 2 Method The lexical distributional similarity of the input two words is computed by comparing two vectors that expre"
W10-3906,C98-2122,0,\N,Missing
W16-4615,D10-1062,0,0.654598,"ightly. 1 Introduction Empty categories are phonetically null elements that are used for representing dropped pronouns (“pro” or “small pro”), controlled elements (“PRO” or “big pro”) and traces of movement (“T” or “trace”). Dropped pronouns are one of the major problems caused on machine translation from the pro-drop language such as Japanese to the non-pro-drop language such as English because it is difficult to produce the correct pronouns on the target side when the pronoun is missing on the source side. The effects of empty categories in machine translation have previously been examined (Chung and Gildea, 2010; Taira et al., 2012; Xiang et al., 2013; Kudo et al., 2014; Wang et al., 2016). In this paper, we address two new problems that were not fully discussed in previous work. The first problem is that, even if empty categories are correctly recovered, it is difficult to automatically obtain the correct word alignment for languages with a completely different word order such as Japanese and English. The second problem is that it is not only difficult to translate non-existent pronouns but also relative clauses because relative pronouns do not exist in Japanese. In theory, we can safely ignore PRO"
W16-4615,2005.iwslt-1.1,0,0.0607067,"Missing"
W16-4615,D13-1095,0,0.0186162,"In the following sections, we first briefly describe related works. We then describe empty category detection method (Takeno et al., 2015) and discriminative preordering method (Hoshino et al., 2015) used in the proposed method. We then report experiment results of Japanese-to-English translation on both spoken (IWSLT dataset) and written (KFTT dataset) languages. 2 Related works Conventional approaches to recover zero pronouns in Japanese are to frame it as zero anaphora resolution, which is a sub-problem of predicate argument structure analysis (Nakaiwa and Ikehara, 1995; Iida et al., 2007; Hangyo et al., 2013). Zero anaphora resolution consists of two procedures: zero pronoun detection and anaphora resolution. It is difficult to integrate zero anaphora resolution (or predicate-argument structure analysis) into SMT for two reasons. The first is that anaphora resolution requires context analysis, which complicates the translation method. The second is that predicate argument structure analysis provides semantic relations, not syntactic structure. This makes it difficult to use the information of recovered zero pronouns in SMT, because there is no position information for the zero pronouns in the word"
W16-4615,W11-2123,0,0.0136321,"gories, as described in the previous section. We achieved this by first tokenizing Japanese sentences by using a CRF-based tokenizing and chunking software (Uchimoto and Den, 2008) to obtain the long unit words required by the Japanese parser (Hayashi et al., 2016). We then achieved word alignment by using short unit words in Japanese obtained by using the MeCab morphological analyzer with the UniDic dictionary2 . For the Japanese-to-English translation experiment, we used a phrase-based translation model (Koehn et al., 2007). For all systems we compared, the language model is a 5-gram KenLM (Heafield, 2011), which uses modified Kneser-Ney smoothing and tuning is performed to maximize the BLEU score using minimum error rate training (Och, 2007). Other configurable setting of all tool use default values unless otherwise stated. We compared three translation methods, each with and without empty category detection. BASELINE is a phrase-based machine translation system (Moses) (Koehn et al., 2007) which consists of training data comprising a bilingual dataset without preordering. REORDERING(H) and REORDERING(C) are described in the previous section. For REORDERING(H), 5,319 sentences with manual word"
W16-4615,P15-2023,1,0.909812,"te machine translation outputs. We solve this problem by integrating empty category detection into preordering-based statistical machine translation. We first insert empty categories into the source sentence, and then reorder them such that the word order is similar to that of the target sentence. We find it is effective to filter out unreliable empty category candidates to improve the accuracy of machine translation. In the following sections, we first briefly describe related works. We then describe empty category detection method (Takeno et al., 2015) and discriminative preordering method (Hoshino et al., 2015) used in the proposed method. We then report experiment results of Japanese-to-English translation on both spoken (IWSLT dataset) and written (KFTT dataset) languages. 2 Related works Conventional approaches to recover zero pronouns in Japanese are to frame it as zero anaphora resolution, which is a sub-problem of predicate argument structure analysis (Nakaiwa and Ikehara, 1995; Iida et al., 2007; Hangyo et al., 2013). Zero anaphora resolution consists of two procedures: zero pronoun detection and anaphora resolution. It is difficult to integrate zero anaphora resolution (or predicate-argument"
W16-4615,D10-1092,0,0.0157439,"al dataset without preordering. REORDERING(H) and REORDERING(C) are described in the previous section. For REORDERING(H), 5,319 sentences with manual word alignment 2 http://taku910.github.io/mecab/unidic 161 Figure 3: Characteristic of machine translation evaluation scores to empty categories filtered for development set of the IWSLT dataset is used. These systems are equivalent to Hoshino et al., (2015)’s method. They are taken from both the spoken language (CSJ) and written (KTC) language corpus. As for evaluation measures, we use the standard BLEU (Papineni et al., 2002) as well as RIBES (Isozaki et al., 2010), which is a rank correlation based metric that has been shown to be highly correlated with human evaluations of machine translation systems between languages with a very different word order such as Japanese and English. Result of filtering empty categories In this experiment, we search for the best threshold value to filter out empty categories in Sec 3. Changing the threshold values θ from 0.50 to 1.0, we measure both BLEU and RIBES, where θ = 1.0 corresponds to the result produces by machine translation systems trained from a dataset without empty categories. When decoding the text into En"
W16-4615,P07-2045,0,0.0084825,"detection method to source Japanese sentences to obtain syntactic trees with empty categories, as described in the previous section. We achieved this by first tokenizing Japanese sentences by using a CRF-based tokenizing and chunking software (Uchimoto and Den, 2008) to obtain the long unit words required by the Japanese parser (Hayashi et al., 2016). We then achieved word alignment by using short unit words in Japanese obtained by using the MeCab morphological analyzer with the UniDic dictionary2 . For the Japanese-to-English translation experiment, we used a phrase-based translation model (Koehn et al., 2007). For all systems we compared, the language model is a 5-gram KenLM (Heafield, 2011), which uses modified Kneser-Ney smoothing and tuning is performed to maximize the BLEU score using minimum error rate training (Och, 2007). Other configurable setting of all tool use default values unless otherwise stated. We compared three translation methods, each with and without empty category detection. BASELINE is a phrase-based machine translation system (Moses) (Koehn et al., 2007) which consists of training data comprising a bilingual dataset without preordering. REORDERING(H) and REORDERING(C) are de"
W16-4615,P14-2091,0,0.535961,"ements that are used for representing dropped pronouns (“pro” or “small pro”), controlled elements (“PRO” or “big pro”) and traces of movement (“T” or “trace”). Dropped pronouns are one of the major problems caused on machine translation from the pro-drop language such as Japanese to the non-pro-drop language such as English because it is difficult to produce the correct pronouns on the target side when the pronoun is missing on the source side. The effects of empty categories in machine translation have previously been examined (Chung and Gildea, 2010; Taira et al., 2012; Xiang et al., 2013; Kudo et al., 2014; Wang et al., 2016). In this paper, we address two new problems that were not fully discussed in previous work. The first problem is that, even if empty categories are correctly recovered, it is difficult to automatically obtain the correct word alignment for languages with a completely different word order such as Japanese and English. The second problem is that it is not only difficult to translate non-existent pronouns but also relative clauses because relative pronouns do not exist in Japanese. In theory, we can safely ignore PRO in control structures for translation because they are abse"
W16-4615,1995.tmi-1.7,0,0.321952,"improve the accuracy of machine translation. In the following sections, we first briefly describe related works. We then describe empty category detection method (Takeno et al., 2015) and discriminative preordering method (Hoshino et al., 2015) used in the proposed method. We then report experiment results of Japanese-to-English translation on both spoken (IWSLT dataset) and written (KFTT dataset) languages. 2 Related works Conventional approaches to recover zero pronouns in Japanese are to frame it as zero anaphora resolution, which is a sub-problem of predicate argument structure analysis (Nakaiwa and Ikehara, 1995; Iida et al., 2007; Hangyo et al., 2013). Zero anaphora resolution consists of two procedures: zero pronoun detection and anaphora resolution. It is difficult to integrate zero anaphora resolution (or predicate-argument structure analysis) into SMT for two reasons. The first is that anaphora resolution requires context analysis, which complicates the translation method. The second is that predicate argument structure analysis provides semantic relations, not syntactic structure. This makes it difficult to use the information of recovered zero pronouns in SMT, because there is no position info"
W16-4615,P02-1040,0,0.0948532,"sts of training data comprising a bilingual dataset without preordering. REORDERING(H) and REORDERING(C) are described in the previous section. For REORDERING(H), 5,319 sentences with manual word alignment 2 http://taku910.github.io/mecab/unidic 161 Figure 3: Characteristic of machine translation evaluation scores to empty categories filtered for development set of the IWSLT dataset is used. These systems are equivalent to Hoshino et al., (2015)’s method. They are taken from both the spoken language (CSJ) and written (KTC) language corpus. As for evaluation measures, we use the standard BLEU (Papineni et al., 2002) as well as RIBES (Isozaki et al., 2010), which is a rank correlation based metric that has been shown to be highly correlated with human evaluations of machine translation systems between languages with a very different word order such as Japanese and English. Result of filtering empty categories In this experiment, we search for the best threshold value to filter out empty categories in Sec 3. Changing the threshold values θ from 0.50 to 1.0, we measure both BLEU and RIBES, where θ = 1.0 corresponds to the result produces by machine translation systems trained from a dataset without empty ca"
W16-4615,W12-4213,1,0.892896,"mpty categories are phonetically null elements that are used for representing dropped pronouns (“pro” or “small pro”), controlled elements (“PRO” or “big pro”) and traces of movement (“T” or “trace”). Dropped pronouns are one of the major problems caused on machine translation from the pro-drop language such as Japanese to the non-pro-drop language such as English because it is difficult to produce the correct pronouns on the target side when the pronoun is missing on the source side. The effects of empty categories in machine translation have previously been examined (Chung and Gildea, 2010; Taira et al., 2012; Xiang et al., 2013; Kudo et al., 2014; Wang et al., 2016). In this paper, we address two new problems that were not fully discussed in previous work. The first problem is that, even if empty categories are correctly recovered, it is difficult to automatically obtain the correct word alignment for languages with a completely different word order such as Japanese and English. The second problem is that it is not only difficult to translate non-existent pronouns but also relative clauses because relative pronouns do not exist in Japanese. In theory, we can safely ignore PRO in control structure"
W16-4615,D15-1156,1,0.756541,"ld worsen automatic word alignment and result in less accurate machine translation outputs. We solve this problem by integrating empty category detection into preordering-based statistical machine translation. We first insert empty categories into the source sentence, and then reorder them such that the word order is similar to that of the target sentence. We find it is effective to filter out unreliable empty category candidates to improve the accuracy of machine translation. In the following sections, we first briefly describe related works. We then describe empty category detection method (Takeno et al., 2015) and discriminative preordering method (Hoshino et al., 2015) used in the proposed method. We then report experiment results of Japanese-to-English translation on both spoken (IWSLT dataset) and written (KFTT dataset) languages. 2 Related works Conventional approaches to recover zero pronouns in Japanese are to frame it as zero anaphora resolution, which is a sub-problem of predicate argument structure analysis (Nakaiwa and Ikehara, 1995; Iida et al., 2007; Hangyo et al., 2013). Zero anaphora resolution consists of two procedures: zero pronoun detection and anaphora resolution. It is difficult"
W16-4615,uchimoto-den-2008-word,0,0.0179019,"ade from the “Japanese-English Bilingual Corpus of Wikipedia’s Kyoto Articles”, which is created by manually translating Japanese Wikipedia articles related to Kyoto City into English. The dataset consists of 440,000, 1,235, and 1,160 sentences for training, tuning, and testing, respectively. We built the preordering model by applying the empty category detection method to source Japanese sentences to obtain syntactic trees with empty categories, as described in the previous section. We achieved this by first tokenizing Japanese sentences by using a CRF-based tokenizing and chunking software (Uchimoto and Den, 2008) to obtain the long unit words required by the Japanese parser (Hayashi et al., 2016). We then achieved word alignment by using short unit words in Japanese obtained by using the MeCab morphological analyzer with the UniDic dictionary2 . For the Japanese-to-English translation experiment, we used a phrase-based translation model (Koehn et al., 2007). For all systems we compared, the language model is a 5-gram KenLM (Heafield, 2011), which uses modified Kneser-Ney smoothing and tuning is performed to maximize the BLEU score using minimum error rate training (Och, 2007). Other configurable setti"
W16-4615,N16-1113,0,0.556293,"d for representing dropped pronouns (“pro” or “small pro”), controlled elements (“PRO” or “big pro”) and traces of movement (“T” or “trace”). Dropped pronouns are one of the major problems caused on machine translation from the pro-drop language such as Japanese to the non-pro-drop language such as English because it is difficult to produce the correct pronouns on the target side when the pronoun is missing on the source side. The effects of empty categories in machine translation have previously been examined (Chung and Gildea, 2010; Taira et al., 2012; Xiang et al., 2013; Kudo et al., 2014; Wang et al., 2016). In this paper, we address two new problems that were not fully discussed in previous work. The first problem is that, even if empty categories are correctly recovered, it is difficult to automatically obtain the correct word alignment for languages with a completely different word order such as Japanese and English. The second problem is that it is not only difficult to translate non-existent pronouns but also relative clauses because relative pronouns do not exist in Japanese. In theory, we can safely ignore PRO in control structures for translation because they are absent from both Japanes"
W16-4615,P13-1081,0,0.622535,"phonetically null elements that are used for representing dropped pronouns (“pro” or “small pro”), controlled elements (“PRO” or “big pro”) and traces of movement (“T” or “trace”). Dropped pronouns are one of the major problems caused on machine translation from the pro-drop language such as Japanese to the non-pro-drop language such as English because it is difficult to produce the correct pronouns on the target side when the pronoun is missing on the source side. The effects of empty categories in machine translation have previously been examined (Chung and Gildea, 2010; Taira et al., 2012; Xiang et al., 2013; Kudo et al., 2014; Wang et al., 2016). In this paper, we address two new problems that were not fully discussed in previous work. The first problem is that, even if empty categories are correctly recovered, it is difficult to automatically obtain the correct word alignment for languages with a completely different word order such as Japanese and English. The second problem is that it is not only difficult to translate non-existent pronouns but also relative clauses because relative pronouns do not exist in Japanese. In theory, we can safely ignore PRO in control structures for translation be"
W16-4615,P03-1021,0,\N,Missing
W17-5702,2016.amta-researchers.10,0,0.0592766,"Missing"
W17-5702,P17-2061,0,0.0395071,"Missing"
W17-5702,D10-1062,0,0.0907421,"h as zero pronouns (dropped subject and object) in Japanese and expletives in English (there in thereconstruction, do in interrogative sentence, it in formal subject, etc.). In machine translation, unaligned words in target sentence are problematic because the information required for translation is not explicitly present in the source sentence. There are many works that aim at improving machine translation performance by supplementing unaligned words, but they focus on specific linguistic phenomena such as Japanese case marker (Hisami and Suzuki, 2007), Chinese zero pronoun (empty category) (Chung and Gildea, 2010; 3.3 Domain Adaptation The third example encodes dataset names of a bilingual text for domain adaptation. Here, IWSLT is a travel expression corpus and KFTT is a corpus of Japanese Wikipedia pages on Kyoto and its English translation. 朝食はいくらですか。 → #IWSLT How much is the breakfast ? 妙法蓮華経を根本経典とする。 → #KFTT Its fundamental sutra is lotus sutra . Kobus et al. (2016) proposed a domain adaptation method using side constraints. They used a separate classifier for predicting the domain of a sentence before translation if it is not known. Li et al. (2016) used Speaker IDs of Twitter to 57 Xiang et al."
W17-5702,2005.iwslt-1.1,0,0.0147211,"aiting for you that day . (5) 5 Experiment Our preliminary experiment showed that the scores yielded by Eq. (5) are not reliable for low frequency target words. We therefore use the following equation to filter out low frequency NULLgenerated target words. 5.1 Datasets and Tools 4.3 Prefix Constraints for Unaligned Target Words The experiments used five publicly available Japanese-English parallel corpora, namely IWSLT-2005, KFTT, GVOICES, REUTERS, and TATOEBA, as shown in Table 2. IWSLT-2005 is a dataset for Japanese-English Tasks of the International Workshop on Spoken Language Translation (Eck and Hori, 2005). It is available from ALAGIN2 . KFTT (Kyoto Free Translation Task) is a Japanese-English translation task on Wikipedia articles related to Kyoto3 . Parallel Global Voices is a multilingual corpus created from Global Voices websites which translate social media and blogs (Prokopidis et al., 2016). Tatoeba is a collection of multilingual translated example sentences from Tatoeba website. These last two are available from OPUS (Tiedemann, 2012). Reuters are Japanese-English parallel corpus made by aligning Reuters RCV1 RCV2 multilingual text categorization test collection data set (RCV1 for Engl"
W17-5702,N07-1007,0,0.0160352,"n be caused by specific linguistic phenomena in one language, such as zero pronouns (dropped subject and object) in Japanese and expletives in English (there in thereconstruction, do in interrogative sentence, it in formal subject, etc.). In machine translation, unaligned words in target sentence are problematic because the information required for translation is not explicitly present in the source sentence. There are many works that aim at improving machine translation performance by supplementing unaligned words, but they focus on specific linguistic phenomena such as Japanese case marker (Hisami and Suzuki, 2007), Chinese zero pronoun (empty category) (Chung and Gildea, 2010; 3.3 Domain Adaptation The third example encodes dataset names of a bilingual text for domain adaptation. Here, IWSLT is a travel expression corpus and KFTT is a corpus of Japanese Wikipedia pages on Kyoto and its English translation. 朝食はいくらですか。 → #IWSLT How much is the breakfast ? 妙法蓮華経を根本経典とする。 → #KFTT Its fundamental sutra is lotus sutra . Kobus et al. (2016) proposed a domain adaptation method using side constraints. They used a separate classifier for predicting the domain of a sentence before translation if it is not known."
W17-5702,Q17-1024,0,0.0940404,"Missing"
W17-5702,D16-1140,0,0.0869114,".ntt.co.jp Abstract length (Kikuchi et al., 2016), and target language (Johnson et al., 2016). Meta-textual information include dialogue act (Wen et al., 2015), user personality (Li et al., 2016), topic (Chen et al., 2016), and domain (Kobus et al., 2016) Two approaches can be used to provide additional information to the encoder-decoder model, word-level methods and sentence-level methods. Word-level methods encode the additional information as a vector (embedding) that is input together with a word at each time step of either (or both) encoder and decoder (Wen et al., 2015; Li et al., 2016; Kikuchi et al., 2016). Sentence level methods encode the additional information as special tokens. Side constraints are placed at the end of source sentence (Sennrich et al., 2016; Johnson et al., 2016; Yamagishi et al., 2016), while our proposal, prefix constraints, is placed at the beginning of target sentence. The advantage of sentence-level methods over word-level methods is their simplicity in application. The network structure of the underlying encoder-decoder model does not have to be modified. The problem with side constraints is that, at test time, additional information must be either specified by the us"
W17-5702,P07-2045,0,0.00613964,"age corpora as the IWSLT-2005 dataset is very small. One is the Daionsen parallel sentence database, made by Straightword Inc5 , which is a phrase book for daily conversation. It has 50,709 sentences with 431,258 words in English and 471,677 words in Japanese. The other is the HIT (Harbin Institute of Technology) parallel corpus (Yang et al., 2006) developed for speech translation. It is a collection of 62,727 sentences with 635,809 words in English and 796,200 words in Japanese. We call this dataset IWSLT2005+EXTRA. English sentences are tokenized and lowercased by the scripts used in Moses (Koehn et al., 2007). Japanese sentences are normalized by NFKC (a unicode normalization form) and word segmented by MeCab6 with UniDic. For neural 5 6 Baseline Side Constraints Prefix Constraints None 34.8 33.0 31.7 Oracle 35.4 35.7 Table 3: Comparison between side constraints and prefix constraints on length control As shown in Table 3, Prefix Constraints are comparable to or better than Side Constraints in controlling the length of the target sentence if the correct length is known and provided as an oracle. It is difficult to predict the length of target sentence from source sentence, which lowered the achttp"
W17-5702,P14-2091,0,0.0150655,"lingual text for domain adaptation. Here, IWSLT is a travel expression corpus and KFTT is a corpus of Japanese Wikipedia pages on Kyoto and its English translation. 朝食はいくらですか。 → #IWSLT How much is the breakfast ? 妙法蓮華経を根本経典とする。 → #KFTT Its fundamental sutra is lotus sutra . Kobus et al. (2016) proposed a domain adaptation method using side constraints. They used a separate classifier for predicting the domain of a sentence before translation if it is not known. Li et al. (2016) used Speaker IDs of Twitter to 57 Xiang et al., 2013; Wang et al., 2016), Japanese zero pronoun (Taira et al., 2012; Kudo et al., 2014) and English determiner (Tsvetkov et al., 2013). There are no language independent methods that can cope with unaligned target words. A special token, #GO, is added to delimit the variable length prefix relative to target sentence. In the following examples, words with underline indicate unaligned target words. 赤ワインを頂けますか。 →#i #GO may i have some red wine ? では当日御待ちして居ります。 →#we #you #GO we are waiting for you that day . 4.2 Identifying Unaligned Target Words We first propose a language independent method for automatically identifying unaligned target words. We assume word alignment is given for"
W17-5702,D15-1199,0,0.0663687,"Missing"
W17-5702,N16-1046,0,0.0374182,"Missing"
W17-5702,P16-1007,0,0.210826,"nce. The advantage of sentence-level methods over word-level methods is their simplicity in application. The network structure of the underlying encoder-decoder model does not have to be modified. The problem with side constraints is that, at test time, additional information must be either specified by the user or automatically predicted by some other method. As prefix constraints move the special tokens from source to target, they can be predicted by the network jointly with target sentence. Like side constraints, the user can specify prefix constraints by using prefix-constrained decoding (Wuebker et al., 2016), which can be implemented by a trivial modification of the decoder. The following sections start by describing the framework of prefix constraints. We then show three simple use cases, namely, length control, bidirectional decoding, and domain adaptation. We then show a more sophisticated usage of prefix constraints: unaligned target word generation. We propose prefix constraints, a novel method to enforce constraints on target sentences in neural machine translation. It places a sequence of special tokens at the beginning of target sentence (target prefix), while side constraints (Sennrich e"
W17-5702,2015.iwslt-evaluation.11,0,0.0431552,"length once at the initial state of the decoder. They designed a dedicated network structure for each method. In spirit, our method is similar to the LenInit method, but we don’t have to modify the underlying network structure. Note that we do not tell the network that ’#3’ is the length of the target sentence. The network automatically learns the meaning of the symbol from the regularity of the training data and then calculates its embedding. add personality to a conversational agent. Speaker embeddings are learned jointly with word embeddings and entered into the decoder at each time step. Luong and Manning (2015) proposed a domain adaptation method based on fine tuning in which an out-of-domain model is further trained on in-domain data. Our method can automatically predict domain jointly with target sentence. We don’t have to change the underlying network structure and domain embeddings are jointly learned with word embeddings as a part of target vocabulary. One of the potential benefits of our method is that only one model is made and used for all domains. If multiple domains must be supported, the methods based on fine tuning (Luong and Manning, 2015) have to make a model for each domain. 3.2 Bidir"
W17-5702,P13-1081,0,0.0411866,"Gildea, 2010; 3.3 Domain Adaptation The third example encodes dataset names of a bilingual text for domain adaptation. Here, IWSLT is a travel expression corpus and KFTT is a corpus of Japanese Wikipedia pages on Kyoto and its English translation. 朝食はいくらですか。 → #IWSLT How much is the breakfast ? 妙法蓮華経を根本経典とする。 → #KFTT Its fundamental sutra is lotus sutra . Kobus et al. (2016) proposed a domain adaptation method using side constraints. They used a separate classifier for predicting the domain of a sentence before translation if it is not known. Li et al. (2016) used Speaker IDs of Twitter to 57 Xiang et al., 2013; Wang et al., 2016), Japanese zero pronoun (Taira et al., 2012; Kudo et al., 2014) and English determiner (Tsvetkov et al., 2013). There are no language independent methods that can cope with unaligned target words. A special token, #GO, is added to delimit the variable length prefix relative to target sentence. In the following examples, words with underline indicate unaligned target words. 赤ワインを頂けますか。 →#i #GO may i have some red wine ? では当日御待ちして居ります。 →#we #you #GO we are waiting for you that day . 4.2 Identifying Unaligned Target Words We first propose a language independent method for auto"
W17-5702,D15-1166,0,0.351218,"17. 2017 AFNLP 2 Encoder-Decoder Model with Prefix Constraints 2.3 Prefix Constraints In our proposed method, a sequence of special tokens is placed at the beginning of the target sentence. In other words, they are the prefix to the extended target sentence. Let a sequence of features extracted from a pair of source sentence x and target sentence y be c = c1 . . . ck , and extended target sentence be y ˜ = cy. The baseline encoder-decoder model Eq. (1) is extended as follows. 2.1 Encoder-Decoder Model First, we briefly describe the attention-based encoder-decoder model (Bahdanau et al., 2015; Luong et al., 2015), which is the state-of-the-art neural machine translation method and the baseline of this study. Given input sequence x = x1 . . . xn and model parameters θ, the encoder-decoder model formulates the likelihood of the output sequences y = y1 . . . ym as follows: ∑m log p(y|x) = log p (yj |y<j , x; θ) (1) log p(˜ y |x) = log p(c|x) + log p(y|x, c) log p(c|x) = j=1 log p(y|x, c) = The encoder is a recurrent neural network (RNN) which projects input sequence x into a sequence of hidden states h = h1 . . . hn via non-linear transformation. The decoder is another RNN which predicts target words y s"
W17-5702,W16-4620,0,0.243908,"(Chen et al., 2016), and domain (Kobus et al., 2016) Two approaches can be used to provide additional information to the encoder-decoder model, word-level methods and sentence-level methods. Word-level methods encode the additional information as a vector (embedding) that is input together with a word at each time step of either (or both) encoder and decoder (Wen et al., 2015; Li et al., 2016; Kikuchi et al., 2016). Sentence level methods encode the additional information as special tokens. Side constraints are placed at the end of source sentence (Sennrich et al., 2016; Johnson et al., 2016; Yamagishi et al., 2016), while our proposal, prefix constraints, is placed at the beginning of target sentence. The advantage of sentence-level methods over word-level methods is their simplicity in application. The network structure of the underlying encoder-decoder model does not have to be modified. The problem with side constraints is that, at test time, additional information must be either specified by the user or automatically predicted by some other method. As prefix constraints move the special tokens from source to target, they can be predicted by the network jointly with target sentence. Like side constra"
W17-5702,J03-1002,0,0.00880432,"target words: LEX and COUNT. LEX places a sequence of unaligned target words at the beginning of the target sentence in the same order they appear in the target sentence. The COUNT feature can be thought of a substitute for the fertility of the IBM model (Brown et al., 1993), or the generative model for NULL-generated target words (Schulz et al., 2016). 2 http://alagin.jp/ 3 http://www.phontron.com/kftt/index.html 4 The aligned parallel corpus is available from the homepage of the first author of (Utiyama and Isahara, 2003) Su (w) = p(e = w|f = N U LL) ∗ p(f = N U LL|e = w) (6) We use GIZA++ (Och and Ney, 2003) to obtain word alignment for both translation directions. Word alignment is symmetrized by intersection heuristics, because the word alignment obtained by grow-diag-final-and, is noisy for unaligned words. Table 1 shows the top 50 unaligned target words as determined by Eq. (6) in the IWSLT-2005 Japanese-to-English translation dataset, which is described in the experiment section. We can see that the automatically extracted unaligned target words include zero pronouns (i, you, it), articles (a, the), light verbs (take, get, make), and expletives (do, does). 1 58 i the a you , it to for do ple"
W17-5702,P02-1040,0,0.0983736,"in dev test train dev test train dev test train dev test train dev test Sents. 19,972 506 1,000 440,288 1,235 1,160 43,488 1,000 1,000 54,011 1,000 1,000 185,426 1,000 1,000 753,185 4,741 5,160 len.(ja) 9.9 8.1 8.2 27.0 27.8 24.5 26.3 25.1 28.7 34.3 34.4 34.6 10.1 10.2 11.8 23.3 23.2 22.2 len.(en) 9.4 7.5 7.6 26.3 25.1 23.5 19.8 18.9 21.2 25.2 25.2 25.5 9.14 9.21 9.23 21.2 18.6 17.5 machine translation, we used seq2seq-attn7 , which implements an attention-based encoder-decoder (Luong et al., 2015). We used default settings unless otherwise specified. Translation accuracy is measured by BLEU (Papineni et al., 2002). 5.2 Length Control Table 3 compares side constraints with prefix constraints in terms of length control for IWSLT-2005 dataset. Baseline is a NMT system trained on the parallel corpus without length tag. Side Constraints and Prefix Constraints stand for NMT systems trained on the corpus with length tags placed at the end of source sentence and at the begging of target sentence, respectively. In None, source sentences without length tag are entered into the system at test time. In Oracle, reference length is encoded as length tag and prefix constrained decoding is used in Prefix Constraints."
W17-5702,L16-1144,0,0.0159862,"onstraints for Unaligned Target Words The experiments used five publicly available Japanese-English parallel corpora, namely IWSLT-2005, KFTT, GVOICES, REUTERS, and TATOEBA, as shown in Table 2. IWSLT-2005 is a dataset for Japanese-English Tasks of the International Workshop on Spoken Language Translation (Eck and Hori, 2005). It is available from ALAGIN2 . KFTT (Kyoto Free Translation Task) is a Japanese-English translation task on Wikipedia articles related to Kyoto3 . Parallel Global Voices is a multilingual corpus created from Global Voices websites which translate social media and blogs (Prokopidis et al., 2016). Tatoeba is a collection of multilingual translated example sentences from Tatoeba website. These last two are available from OPUS (Tiedemann, 2012). Reuters are Japanese-English parallel corpus made by aligning Reuters RCV1 RCV2 multilingual text categorization test collection data set (RCV1 for English and RCV2 for other languages) available from NIST (Utiyama and Isahara, 2003)4 . The unaligned target word generation experiments used two additional proprietary spoken We propose here two types of prefix constraints for improving the translation of unaligned target words: LEX and COUNT. LEX"
W17-5702,P16-2028,0,0.0355694,"Missing"
W17-5702,N16-1005,0,0.46565,"), user personality (Li et al., 2016), topic (Chen et al., 2016), and domain (Kobus et al., 2016) Two approaches can be used to provide additional information to the encoder-decoder model, word-level methods and sentence-level methods. Word-level methods encode the additional information as a vector (embedding) that is input together with a word at each time step of either (or both) encoder and decoder (Wen et al., 2015; Li et al., 2016; Kikuchi et al., 2016). Sentence level methods encode the additional information as special tokens. Side constraints are placed at the end of source sentence (Sennrich et al., 2016; Johnson et al., 2016; Yamagishi et al., 2016), while our proposal, prefix constraints, is placed at the beginning of target sentence. The advantage of sentence-level methods over word-level methods is their simplicity in application. The network structure of the underlying encoder-decoder model does not have to be modified. The problem with side constraints is that, at test time, additional information must be either specified by the user or automatically predicted by some other method. As prefix constraints move the special tokens from source to target, they can be predicted by the network"
W17-5702,W12-4213,1,0.874792,"ataset names of a bilingual text for domain adaptation. Here, IWSLT is a travel expression corpus and KFTT is a corpus of Japanese Wikipedia pages on Kyoto and its English translation. 朝食はいくらですか。 → #IWSLT How much is the breakfast ? 妙法蓮華経を根本経典とする。 → #KFTT Its fundamental sutra is lotus sutra . Kobus et al. (2016) proposed a domain adaptation method using side constraints. They used a separate classifier for predicting the domain of a sentence before translation if it is not known. Li et al. (2016) used Speaker IDs of Twitter to 57 Xiang et al., 2013; Wang et al., 2016), Japanese zero pronoun (Taira et al., 2012; Kudo et al., 2014) and English determiner (Tsvetkov et al., 2013). There are no language independent methods that can cope with unaligned target words. A special token, #GO, is added to delimit the variable length prefix relative to target sentence. In the following examples, words with underline indicate unaligned target words. 赤ワインを頂けますか。 →#i #GO may i have some red wine ? では当日御待ちして居ります。 →#we #you #GO we are waiting for you that day . 4.2 Identifying Unaligned Target Words We first propose a language independent method for automatically identifying unaligned target words. We assume word al"
W17-5702,tiedemann-2012-parallel,0,0.0358316,"RS, and TATOEBA, as shown in Table 2. IWSLT-2005 is a dataset for Japanese-English Tasks of the International Workshop on Spoken Language Translation (Eck and Hori, 2005). It is available from ALAGIN2 . KFTT (Kyoto Free Translation Task) is a Japanese-English translation task on Wikipedia articles related to Kyoto3 . Parallel Global Voices is a multilingual corpus created from Global Voices websites which translate social media and blogs (Prokopidis et al., 2016). Tatoeba is a collection of multilingual translated example sentences from Tatoeba website. These last two are available from OPUS (Tiedemann, 2012). Reuters are Japanese-English parallel corpus made by aligning Reuters RCV1 RCV2 multilingual text categorization test collection data set (RCV1 for English and RCV2 for other languages) available from NIST (Utiyama and Isahara, 2003)4 . The unaligned target word generation experiments used two additional proprietary spoken We propose here two types of prefix constraints for improving the translation of unaligned target words: LEX and COUNT. LEX places a sequence of unaligned target words at the beginning of the target sentence in the same order they appear in the target sentence. The COUNT f"
W17-5702,W13-2234,0,0.0154563,"WSLT is a travel expression corpus and KFTT is a corpus of Japanese Wikipedia pages on Kyoto and its English translation. 朝食はいくらですか。 → #IWSLT How much is the breakfast ? 妙法蓮華経を根本経典とする。 → #KFTT Its fundamental sutra is lotus sutra . Kobus et al. (2016) proposed a domain adaptation method using side constraints. They used a separate classifier for predicting the domain of a sentence before translation if it is not known. Li et al. (2016) used Speaker IDs of Twitter to 57 Xiang et al., 2013; Wang et al., 2016), Japanese zero pronoun (Taira et al., 2012; Kudo et al., 2014) and English determiner (Tsvetkov et al., 2013). There are no language independent methods that can cope with unaligned target words. A special token, #GO, is added to delimit the variable length prefix relative to target sentence. In the following examples, words with underline indicate unaligned target words. 赤ワインを頂けますか。 →#i #GO may i have some red wine ? では当日御待ちして居ります。 →#we #you #GO we are waiting for you that day . 4.2 Identifying Unaligned Target Words We first propose a language independent method for automatically identifying unaligned target words. We assume word alignment is given for a bilingual sentence pair, where NULL represen"
W17-5702,P03-1010,0,0.024974,"lation Task) is a Japanese-English translation task on Wikipedia articles related to Kyoto3 . Parallel Global Voices is a multilingual corpus created from Global Voices websites which translate social media and blogs (Prokopidis et al., 2016). Tatoeba is a collection of multilingual translated example sentences from Tatoeba website. These last two are available from OPUS (Tiedemann, 2012). Reuters are Japanese-English parallel corpus made by aligning Reuters RCV1 RCV2 multilingual text categorization test collection data set (RCV1 for English and RCV2 for other languages) available from NIST (Utiyama and Isahara, 2003)4 . The unaligned target word generation experiments used two additional proprietary spoken We propose here two types of prefix constraints for improving the translation of unaligned target words: LEX and COUNT. LEX places a sequence of unaligned target words at the beginning of the target sentence in the same order they appear in the target sentence. The COUNT feature can be thought of a substitute for the fertility of the IBM model (Brown et al., 1993), or the generative model for NULL-generated target words (Schulz et al., 2016). 2 http://alagin.jp/ 3 http://www.phontron.com/kftt/index.html"
W17-5702,N16-1113,0,0.0369129,"main Adaptation The third example encodes dataset names of a bilingual text for domain adaptation. Here, IWSLT is a travel expression corpus and KFTT is a corpus of Japanese Wikipedia pages on Kyoto and its English translation. 朝食はいくらですか。 → #IWSLT How much is the breakfast ? 妙法蓮華経を根本経典とする。 → #KFTT Its fundamental sutra is lotus sutra . Kobus et al. (2016) proposed a domain adaptation method using side constraints. They used a separate classifier for predicting the domain of a sentence before translation if it is not known. Li et al. (2016) used Speaker IDs of Twitter to 57 Xiang et al., 2013; Wang et al., 2016), Japanese zero pronoun (Taira et al., 2012; Kudo et al., 2014) and English determiner (Tsvetkov et al., 2013). There are no language independent methods that can cope with unaligned target words. A special token, #GO, is added to delimit the variable length prefix relative to target sentence. In the following examples, words with underline indicate unaligned target words. 赤ワインを頂けますか。 →#i #GO may i have some red wine ? では当日御待ちして居ります。 →#we #you #GO we are waiting for you that day . 4.2 Identifying Unaligned Target Words We first propose a language independent method for automatically identifyin"
W99-0207,P98-1011,0,0.106169,"Missing"
W99-0207,C88-1021,0,0.529577,"Missing"
W99-0207,J95-2003,0,0.352919,"Missing"
W99-0207,C96-1021,0,0.0509278,"Missing"
W99-0207,P98-2143,0,0.108075,"Missing"
W99-0207,C96-2137,0,0.155725,"Missing"
W99-0207,A88-1003,0,0.032175,"Missing"
W99-0207,P98-2204,0,0.0979801,"Missing"
W99-0207,P95-1017,0,\N,Missing
W99-0207,C98-1011,0,\N,Missing
W99-0207,C98-2138,0,\N,Missing
W99-0207,C98-2199,0,\N,Missing
Y03-1042,N03-1003,0,0.0569012,"ion, and so on. Nowadays, however, many researchers recognize automatic paraphrasing as a very important technique for many areas of NLP, and work in this field has recently become more active. We view paraphrasing as a translation where the source language and the target language are the same. Thus, we can take a similar approach to that of machine translation for paraphrasing. In this study, we use an example-based approach to control paraphrasing. So far, a number of new methods for paraphrase extraction have emerged (e.g., (Lin and Pantel, 2001; Yamamoto, 2002; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Shinyama and Sekine, 2003; Pang et al., 2003)). There are several extracting methods: for example, one extracts paraphrases from a monolingual corpus, and another extracts from a multilingual parallel corpus. However, most of these methods have focused on efficiency or how many paraphrases would be extracted, and there have been few discussions on the types of paraphrases that were extracted. Meanwhile, a rule-based approach seems reasonable in terms of the application of paraphrasing (Takahashi et al., 2001). Therefore, an investigation of how many paraphrasing examples are valuable, or dif"
Y03-1042,P99-1022,0,0.0136304,"J is given, if C(Jja) * C(Jry) or C(Jq) C(47), then the example Jo is included in class-(B). On the other hand, if C(Jia) * C(.10), then the example is included in class-(C). This flow is shown in Figure 3. Applying class-(B) examples to paraphrasing requires the contextual information C(K) of the target sentence K. If C(K) is given properly, we can estimate whether a paraphrase K ia that is applied to sentence K is applicable or not. This is because, if C(K) C(J ia), then the paraphrase is not applicable. To give the context of a sentence, we can use the topic as an approximation of context (Florian and Yarowsky, 1999). This seems to be suitable for traveling conversations, because the target domain is not very broad, and several topic detection methods are available. Fortunately, the corpus that was we employed has such contextual information. The contextual information is given by several topic words. However, currently available topic words are very rough and there are many inconsistencies. Thus the methods shown in above will be very promising if we prepare proper topic words and remove the inconsistencies. 4 4.2 Analysis of paraphrases of passive formed sentences We also analyzed 485 paraphrases of pas"
Y03-1042,P97-1004,0,0.0262285,"ecause the methods or techniques used to extract paraphrases from corpora have been premature, and few people have an interest in collecting paraphrases. However, there are some slightly related works. We mentioned that an example-based paraphrasing method is suitable for passive formed sentences from the viewpoint of paraphrasing honorifics (Ohtake and Yamamoto, 2001). The results shown in this paper support this discussion. The first attempt to derive paraphrasing rules from corpora was undertaken by Jacquemin et al., who investigated morphological and syntactic variants of technical terms (Jacquemin et al., 1997). Although, these rules achieve high accuracy in identifying term paraphrases, the techniques used have not been extended to other types of paraphrasing. Lapata investigated polysemous adjectives whose meanings vary depending on the nouns they modify (Lapata, 2001). Lapata acquired the meanings of these adjectives from a large corpus and proposed a probabilistic model that provides a ranking of the set of possible interpretations. Statistical techniques were also successfully used by Lapata to identify paraphrases of adjective phrases. Barzilay and McKeown presented an unsupervised learning al"
Y03-1042,N01-1009,0,0.0291097,"ssive formed sentences from the viewpoint of paraphrasing honorifics (Ohtake and Yamamoto, 2001). The results shown in this paper support this discussion. The first attempt to derive paraphrasing rules from corpora was undertaken by Jacquemin et al., who investigated morphological and syntactic variants of technical terms (Jacquemin et al., 1997). Although, these rules achieve high accuracy in identifying term paraphrases, the techniques used have not been extended to other types of paraphrasing. Lapata investigated polysemous adjectives whose meanings vary depending on the nouns they modify (Lapata, 2001). Lapata acquired the meanings of these adjectives from a large corpus and proposed a probabilistic model that provides a ranking of the set of possible interpretations. Statistical techniques were also successfully used by Lapata to identify paraphrases of adjective phrases. Barzilay and McKeown presented an unsupervised learning algorithm for the identification of paraphrases from a corpus of multiple English translations of the same source text (Barzilay and McKeown, 2001). They employed literary texts written by foreign authors. Their algorithm produced 9,483 pairs of lexical paraphrases a"
Y03-1042,1999.mtsummit-1.8,0,0.0135874,"hey employed literary texts written by foreign authors. Their algorithm produced 9,483 pairs of lexical paraphrases and 25 morpho-syntactic rules with very high accuracy. However, there was no discussion on extracted paraphrases, and therefore, what kinds of paraphrases are extracted and whether it is difficult to manually write those paraphrases remain undetermined. From the viewpoint of machine translation, to paraphrase an expression into another expression that is easy to translate is closely related to designing a controlled language. Mitamura reported on designing a controlled language (Mitamura, 1999). Although, there is a language specific part, the discussion that touched on the points should be focused on provides helpful information to consider for paraphrasing. 7 Conclusion We analyzed two kinds of paraphrases automatically extracted from a bilingual parallel corpus. These two kinds are focused since the expressions may be ambiguous in their meanings, or they are frequently used in Japanese conversations. By paraphrasing these kinds to simpler and clearer expressions, easier processing becomes possible. We investigated the following points: (a) How many of the paraphrasing examples th"
Y03-1042,N03-1024,0,0.0127317,"recognize automatic paraphrasing as a very important technique for many areas of NLP, and work in this field has recently become more active. We view paraphrasing as a translation where the source language and the target language are the same. Thus, we can take a similar approach to that of machine translation for paraphrasing. In this study, we use an example-based approach to control paraphrasing. So far, a number of new methods for paraphrase extraction have emerged (e.g., (Lin and Pantel, 2001; Yamamoto, 2002; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Shinyama and Sekine, 2003; Pang et al., 2003)). There are several extracting methods: for example, one extracts paraphrases from a monolingual corpus, and another extracts from a multilingual parallel corpus. However, most of these methods have focused on efficiency or how many paraphrases would be extracted, and there have been few discussions on the types of paraphrases that were extracted. Meanwhile, a rule-based approach seems reasonable in terms of the application of paraphrasing (Takahashi et al., 2001). Therefore, an investigation of how many paraphrasing examples are valuable, or difficult-to-write paraphrasing rules that cover t"
Y03-1042,W03-1609,0,0.0125908,", however, many researchers recognize automatic paraphrasing as a very important technique for many areas of NLP, and work in this field has recently become more active. We view paraphrasing as a translation where the source language and the target language are the same. Thus, we can take a similar approach to that of machine translation for paraphrasing. In this study, we use an example-based approach to control paraphrasing. So far, a number of new methods for paraphrase extraction have emerged (e.g., (Lin and Pantel, 2001; Yamamoto, 2002; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Shinyama and Sekine, 2003; Pang et al., 2003)). There are several extracting methods: for example, one extracts paraphrases from a monolingual corpus, and another extracts from a multilingual parallel corpus. However, most of these methods have focused on efficiency or how many paraphrases would be extracted, and there have been few discussions on the types of paraphrases that were extracted. Meanwhile, a rule-based approach seems reasonable in terms of the application of paraphrasing (Takahashi et al., 2001). Therefore, an investigation of how many paraphrasing examples are valuable, or difficult-to-write paraphrasin"
Y03-1042,W02-1411,1,0.848896,"information retrieval, automatic summarization, and so on. Nowadays, however, many researchers recognize automatic paraphrasing as a very important technique for many areas of NLP, and work in this field has recently become more active. We view paraphrasing as a translation where the source language and the target language are the same. Thus, we can take a similar approach to that of machine translation for paraphrasing. In this study, we use an example-based approach to control paraphrasing. So far, a number of new methods for paraphrase extraction have emerged (e.g., (Lin and Pantel, 2001; Yamamoto, 2002; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Shinyama and Sekine, 2003; Pang et al., 2003)). There are several extracting methods: for example, one extracts paraphrases from a monolingual corpus, and another extracts from a multilingual parallel corpus. However, most of these methods have focused on efficiency or how many paraphrases would be extracted, and there have been few discussions on the types of paraphrases that were extracted. Meanwhile, a rule-based approach seems reasonable in terms of the application of paraphrasing (Takahashi et al., 2001). Therefore, an investigation of"
Y07-1007,I05-2030,0,0.018735,"assification approaches have also been proposed, which prepare a semantic orientation dictionary in advance. Tateishi et al. (2004) construct this * Copyright 2007 by Natural Language Processing Laboratory, Department of Electrical Engineering, Nagaoka University of Technology. 76 dictionary in advance by extracting opinion triplets that consist of `object name, attribute expression, evaluative expression’, and classify documents by using the triplets. The method extracts only expressions that appear in a definite pattern, and it is thus difficult to obtain satisfactory coverage and accuracy. Kobayashi et al. (2005) first extract a few opinion pairs of attribute and value with an anaphora resolution technique, and construct a semantic orientation dictionary of the pairs for a target domain. They then gradually expand the entries of the dictionary from the pair seed. However, if the accuracy of the primary pairs is not adequate, the quality of the dictionary becomes gradually poorer due to gradually involving noises. In addition, making the dictionary domain by domain is very expensive. In this paper we do not use a word as an unit of tagging information to extract opinions. As other work does, we also th"
Y07-1007,P02-1053,0,0.00797521,"h the Internet, that include their personal opinions, such as reputation and dissatisfaction with products, complaints about services, and so on. Weblogs and message boards in particular have attracted a great deal of attention as a new information source, since they enable us to obtain subjective opinions easily. In order to automatically extract useful information from these sources, various approaches have been proposed. (Inui and Okumura, 2006) Researchers have been exploring techniques for classifying documents according to sentiment orientation, or positive/negative (p/n) in particular. Turney (2002) extracts phrases containing adjectives or adverbs, and determines their semantic orientation. Further, p/n of a document is judged by computing the average of the semantic orientations. The NEAR operator in the Alta Vista search engine is used in the method. If you search a query like A NEAR B in Alta Vista, the search engine shows pages containing within near words of each other. However, this operator is not provided in Japanese version. Wang and Araki (2007) extend Turney's method into Japanese by collecting and using a set of words that contribute significantly to p/n orientation. Fujimur"
Y08-1030,S07-1072,0,0.0316562,"d be coherent, as emotional coherence affects the naturalness of a sentence. Consider these three sentences: ‧ She receives a present that feels joyous. ‧ She receives a present that is sad. ‧ She receives a present that is afraid. We can select the first sentence ‘She receives a present that feels joyous.’ as the most natural. In this study, we confirm the relationship between the naturalness of a sentence and the coherence of the emotion it expresses. 22nd Pacific Asia Conference on Language, Information and Computation, pages 302–310 302 Related studies on emotion analysis include those of Kozareva et al. (2007), Yang et al. (2007) and Mihalcea and Liu (2006). Kozareva et al. propose an emotional classification approach based on frequency and co-occurrence. They calculate an emotional score for each word using mutual information. A word can express several types of emotions, and we aim to compile these expressions for text generation. Some methods are available for extracting sentences and paragraphs that have emotional content from the blog corpus. Yang et al. used an emoticon as a marker for emotional expression and extracted emotion expressing sentences. Mihalcea and Liu used blog tags such as ‘ha"
Y08-1030,P07-2034,0,\N,Missing
Y10-1074,C96-2202,0,0.0481603,"is annoyed or impatient. However “バカ (stupid)” in (b) means “とても (very)” which is used just for emphasis. Therefore, detection is difficult because we need to consider the neighboring words. Furthermore, a Japanese morpheme analyzer cannot segment the words of BBS posts correctly because they contain several coined words. Therefore, we cannot correctly detect them that are segmented, so nasty words cannot be registered sufficiently. Because of this, we use an n-gram to cope with context and with over-segmented words. The n-gram is used in some natural language processing tasks. Mori and Nagao(Mori and Nagao, 1996) presents a statistical method based on n-gram model for unknown word identification. The method estimates how likely the input string is to be a word. The method cannot cover low frequency unknown words. In the following sections, we present how we detect nasty comments on a BBS with many posts using an n-gram model. 1.1 Definition of Nasty Expression Definition of “nasty” can be vague. In this paper, “nasty” is defined as insults and slander words and phrases that are directed toward another person. In other words, slander that forms part of a story or that is used ironically, is not targete"
Y10-1080,P00-1041,0,0.0423245,"ncluding plural verbs concepts by using definition sentences from a word dictionary (Kondo and Okumura, 1997). This method was useful for handling only verbs and could not handle nouns. Another problem is that it cannot handle summarization of the information that is not described in the definition sentences of a word dictionary. In contrast, we can handle parts of speech other than verbs by using co-occurring words. We can also perform paraphrasing of various kinds of information by using co-occurring words. Banko et al. generated newspaper headlines by using statistical machine translation (Banko et al., 2000). In this study, we handled only the case in which the output summary is one word for simplicity; however, we aim to generate a summary comprising sentences as the output in the future. The case where the output summary comprises two words or sentences can be handled by using an extended version of this method. In this paper, we have only described our idea for handling twoword summaries in Section 4. The summarization process that outputs one-word summaries can also be used to categorize documents, because that one word is representative of the main content and theme of the document. We handl"
Y10-1080,W97-0703,0,0.097702,"of elements in our method was beneficial. Our method obtained 0.75 as the ratio where the top 10 summaries for each document include a correct summary and 0.45 as the mean reciprocal rank (MRR) in the “lenient” case of experiments. Keywords: Summarization, Word-Association Knowledge, Precision, Recall, Generation 1 Introduction Summarization is one of the important techniques in natural language processing more so because of the development of internet-based technologies and the existence of many documents and many kinds of information on the Web (Hovy and Mareu, 1988; Mani and Maybury, 1999; Barzilay and Elhada, 1997; Goldstein et al., 1999; Marcu, 2000; Ker and Chen, 2000; Hongyan, 2000; Radev et al., 2001; Barzilay and Lee, 2004; Radev et al., 2004; Kato et al., 2005). In this study, our purpose was to make a short summary for sentences. For example, we aimed to make a short summary “terror” for sentences “A bomb went off. Some people were killed. This was triggered by rebel campaign.” In this paper, we have proposed a new method that generates summaries that appropriately and adequately express the contents of the original documents using word-association knowledge. In our proposed method, a system jud"
Y10-1080,N04-1015,0,0.0356035,"ument include a correct summary and 0.45 as the mean reciprocal rank (MRR) in the “lenient” case of experiments. Keywords: Summarization, Word-Association Knowledge, Precision, Recall, Generation 1 Introduction Summarization is one of the important techniques in natural language processing more so because of the development of internet-based technologies and the existence of many documents and many kinds of information on the Web (Hovy and Mareu, 1988; Mani and Maybury, 1999; Barzilay and Elhada, 1997; Goldstein et al., 1999; Marcu, 2000; Ker and Chen, 2000; Hongyan, 2000; Radev et al., 2001; Barzilay and Lee, 2004; Radev et al., 2004; Kato et al., 2005). In this study, our purpose was to make a short summary for sentences. For example, we aimed to make a short summary “terror” for sentences “A bomb went off. Some people were killed. This was triggered by rebel campaign.” In this paper, we have proposed a new method that generates summaries that appropriately and adequately express the contents of the original documents using word-association knowledge. In our proposed method, a system judges that a candidate summary that satisfies the following criteria as much as possible is a suitable summary: (i) Th"
Y10-1080,1993.mtsummit-1.10,0,0.63602,"rds, “strict,” and “lenient.” In “strict,” we evaluated only correct candidates as the correct summary. In “lenient” evaluation, we evaluated candidates similar to a correct candidate as the correct summary. We obtained a high Kappa value of 0.78 and 0.75 for “strict” and “lenient,” respectively, when we evaluated the results using two test subjects in a preliminary experiment (Landis and Koch, 1977). 3.2 Results Evaluated results are shown in Tables 1 and 2. Methods 1 to 4 are described in Section 2.6. Method 5 is a comparison method that uses definition sentences in the EDR word dictionary (EDR, 1993). In Method 5, we selected a candidate word whose definition sentence contained a noun that was also present in the input document; we used these words in the definition sentence of a word x as the related words of the word x. The subsequent working is the same as that of our proposed method. Since Method 5 used a definition sentence, it is related to Kondo et al.’s studies (Kondo and Okumura, 1997). However, Kondo et al. did not propose the method of selecting a candidate from plural candidates. We used our candidate scores that were obtained on the basis of criteria (i) and (ii) for selectin"
Y10-1080,A00-1043,0,0.0246578,"e the top 10 summaries for each document include a correct summary and 0.45 as the mean reciprocal rank (MRR) in the “lenient” case of experiments. Keywords: Summarization, Word-Association Knowledge, Precision, Recall, Generation 1 Introduction Summarization is one of the important techniques in natural language processing more so because of the development of internet-based technologies and the existence of many documents and many kinds of information on the Web (Hovy and Mareu, 1988; Mani and Maybury, 1999; Barzilay and Elhada, 1997; Goldstein et al., 1999; Marcu, 2000; Ker and Chen, 2000; Hongyan, 2000; Radev et al., 2001; Barzilay and Lee, 2004; Radev et al., 2004; Kato et al., 2005). In this study, our purpose was to make a short summary for sentences. For example, we aimed to make a short summary “terror” for sentences “A bomb went off. Some people were killed. This was triggered by rebel campaign.” In this paper, we have proposed a new method that generates summaries that appropriately and adequately express the contents of the original documents using word-association knowledge. In our proposed method, a system judges that a candidate summary that satisfies the following criteria as mu"
Y10-1080,P08-4007,0,0.023681,"by the candidate. In this study, we use co-occurring words as word-association knowledge. By using cooccurring words, a summary can be generated containing words that do not appear in the original document (summarization by paraphrasing). In this aspect, our method is completely different Copyright 2010 by Kazuki Takigawa, Masaki Murata, Masaaki Tsuchida, Stijn De Saeger, Kazuhide Yamamoto, and Kentaro Torisawa 693 694 Student Papers from the existing methods that extract some sentences or words from the original document for generating its summary (Knight and Marcu, 2002; Lin and Hovy, 2003; Kang et al., 2008). In terms of related studies pertaining to summarization by paraphrasing, Kondo et al. proposed a method of paraphrasing plural verbs in the original document into a verb having superordinate concepts including plural verbs concepts by using definition sentences from a word dictionary (Kondo and Okumura, 1997). This method was useful for handling only verbs and could not handle nouns. Another problem is that it cannot handle summarization of the information that is not described in the definition sentences of a word dictionary. In contrast, we can handle parts of speech other than verbs by us"
Y10-1080,W00-1100,0,0.390754,"Missing"
Y10-1080,W03-0510,0,0.0335233,"ent is not conveyed by the candidate. In this study, we use co-occurring words as word-association knowledge. By using cooccurring words, a summary can be generated containing words that do not appear in the original document (summarization by paraphrasing). In this aspect, our method is completely different Copyright 2010 by Kazuki Takigawa, Masaki Murata, Masaaki Tsuchida, Stijn De Saeger, Kazuhide Yamamoto, and Kentaro Torisawa 693 694 Student Papers from the existing methods that extract some sentences or words from the original document for generating its summary (Knight and Marcu, 2002; Lin and Hovy, 2003; Kang et al., 2008). In terms of related studies pertaining to summarization by paraphrasing, Kondo et al. proposed a method of paraphrasing plural verbs in the original document into a verb having superordinate concepts including plural verbs concepts by using definition sentences from a word dictionary (Kondo and Okumura, 1997). This method was useful for handling only verbs and could not handle nouns. Another problem is that it cannot handle summarization of the information that is not described in the definition sentences of a word dictionary. In contrast, we can handle parts of speech ot"
Y10-1080,H01-1056,0,0.0238079,"mmaries for each document include a correct summary and 0.45 as the mean reciprocal rank (MRR) in the “lenient” case of experiments. Keywords: Summarization, Word-Association Knowledge, Precision, Recall, Generation 1 Introduction Summarization is one of the important techniques in natural language processing more so because of the development of internet-based technologies and the existence of many documents and many kinds of information on the Web (Hovy and Mareu, 1988; Mani and Maybury, 1999; Barzilay and Elhada, 1997; Goldstein et al., 1999; Marcu, 2000; Ker and Chen, 2000; Hongyan, 2000; Radev et al., 2001; Barzilay and Lee, 2004; Radev et al., 2004; Kato et al., 2005). In this study, our purpose was to make a short summary for sentences. For example, we aimed to make a short summary “terror” for sentences “A bomb went off. Some people were killed. This was triggered by rebel campaign.” In this paper, we have proposed a new method that generates summaries that appropriately and adequately express the contents of the original documents using word-association knowledge. In our proposed method, a system judges that a candidate summary that satisfies the following criteria as much as possible is a"
Y10-1080,W00-1108,0,\N,Missing
Y10-1080,I05-2047,0,\N,Missing
Y14-1073,P01-1008,0,0.102882,"aphrase the headword. We propose lexical paraphrasing based on a variety of contexts obtained from a large corpus without depending on existing lexical resources from such a background. The proposed method is not dependent on language, thus it can perform lexical paraphrases using a corpus of arbitrary languages. In this paper we examine and report on Japanese nouns. 2 Related Works As paraphrase acquisition from a corpus, a study with a parallel corpus and comparable corpus has been performed. Barzilay and McKeown paraphrase text using plural English translations made from the same document (Barzilay and McKeown, 2001). In addition, Shinyama and Sekine paraphrase using plural newspaper articles that report the same event (Shinyama and Sekine, 2003). In a text simCopyright 2014 by Tomoyuki Kajiwara and Kazuhide Yamamoto 28th Pacific Asia Conference on Language, Information and Computation pages 644–649 !644 PACLIC 28 plification task, Coster and Kauchak create a parallel corpus that matches English Wikipedia and Simple English Wikipedia, and they perform text simplification using the framework of statistical machine translation (Coster and Kauchak, 2011). However, the technique of using these parallel corpor"
Y14-1073,P08-1077,0,0.430871,"rate paraphrases using only a single-language corpus so as not to come under these influences. In their research with paraphrasing based on the similarity of the context obtained from a nonparallel corpus, Marton et al. propose a method for paraphrasing unknown words to improve machine translation systems (Marton et al., 2009). They select candidate words with a context common to the subject. Moreover, they calculate cosine similarities of their feature vectors based on the co-occurrence frequency of subjects. Bhagat and Ravichandran extract paraphrases from a massive, 25-billion word corpus (Bhagat and Ravichandran, 2008). They regard English word 5-gram as one phrase, and they generate feature vectors using pointwise mutual information (PMI) scores. They then select the best phrase-paraphrase pairs based on their cosine similarity. Our proposed method is different from these methods in that it does not use co-occurrence frequency or word frequency of conventional features. We focus on the variety of context. Assuming that successful paraphrases have context that is common with their subject, we select paraphrases based only on the number of types of context. 3 Proposed Method In this paper, noun paraphrasing"
Y14-1073,P11-2117,0,0.0200816,"al English translations made from the same document (Barzilay and McKeown, 2001). In addition, Shinyama and Sekine paraphrase using plural newspaper articles that report the same event (Shinyama and Sekine, 2003). In a text simCopyright 2014 by Tomoyuki Kajiwara and Kazuhide Yamamoto 28th Pacific Asia Conference on Language, Information and Computation pages 644–649 !644 PACLIC 28 plification task, Coster and Kauchak create a parallel corpus that matches English Wikipedia and Simple English Wikipedia, and they perform text simplification using the framework of statistical machine translation (Coster and Kauchak, 2011). However, the technique of using these parallel corpora and comparable corpora is problematic in terms of the accuracy of alignment of corresponding expressions and quantity of the corpora that can be used. For example, for Japanese, there is no large-scale parallel corpus in which simplification is possible for use in the framework of statistical machine translation. In this paper, we generate paraphrases using only a single-language corpus so as not to come under these influences. In their research with paraphrasing based on the similarity of the context obtained from a nonparallel corpus,"
Y14-1073,O13-1007,1,0.832845,"exts for children. We believe that vocabulary simplification for children can be realized by paraphrasing text according to Basic Vocabulary to Learn (BVL) (Kai and Matsukawa, 2002) . BVL is a collection of words selected on the basis on a lexical analysis of elementary school textbooks. It contains 5,404 words that can help children write expressively. Kazuhide Yamamoto Department of Electrical Engineering Nagaoka University of Technology Nagaoka City, Niigata, Japan yamamoto@jnlp.org As previous work indicated, there are lexical paraphrases that define statements from a Japanese dictionary (Kajiwara et al., 2013). The definition statements from the Japanese dictionary explain a given headword in several easy words. Therefore, lexical simplification and paraphrasing that conserves a particular meaning are expected by paraphrasing the headword with the words in the definitions. However, definition statements are short sentences that consist of several words. Consequently, there are few paraphrase candidates, and natural paraphrasing is difficult even if we use certain dictionaries together. In addition, the definition statement as a whole is equivalent to the headword; there is no guarantee that any ind"
Y14-1073,W03-1609,0,0.0352864,"n existing lexical resources from such a background. The proposed method is not dependent on language, thus it can perform lexical paraphrases using a corpus of arbitrary languages. In this paper we examine and report on Japanese nouns. 2 Related Works As paraphrase acquisition from a corpus, a study with a parallel corpus and comparable corpus has been performed. Barzilay and McKeown paraphrase text using plural English translations made from the same document (Barzilay and McKeown, 2001). In addition, Shinyama and Sekine paraphrase using plural newspaper articles that report the same event (Shinyama and Sekine, 2003). In a text simCopyright 2014 by Tomoyuki Kajiwara and Kazuhide Yamamoto 28th Pacific Asia Conference on Language, Information and Computation pages 644–649 !644 PACLIC 28 plification task, Coster and Kauchak create a parallel corpus that matches English Wikipedia and Simple English Wikipedia, and they perform text simplification using the framework of statistical machine translation (Coster and Kauchak, 2011). However, the technique of using these parallel corpora and comparable corpora is problematic in terms of the accuracy of alignment of corresponding expressions and quantity of the corpo"
Y14-1073,W04-3230,0,\N,Missing
Y14-1073,D09-1040,0,\N,Missing
Y18-1051,C96-2183,0,0.461687,"Missing"
Y18-1051,W14-3346,0,0.0296892,"e, we manually constructed a paraphrase dictionary where the simplified side consists of only the core vocabulary. We defined the target words to 20,000 high-frequency words in BCCWJ and Tanaka corpus. These words do not include proper nouns, onomatopoeia, numbers and symbols. We asked seven annotators who employed crowdsourcing to simplify these 20,000 words. We divided the 20,000 words into four files such that each file contains 5,000 words, and assigned one file to each an4.2 Evaluation metrics We evaluated the model’s output based on three metrics:1) BLEU with NIST smoothing described by Chen and Cherry (2014), which is a traditional machine translation evaluation metric; 2) SARI (Xu et al., 2016), which is a recent text-simplification metric; and 3) core vocabulary ratio (Core) contained in a sentence, which is considered to be a characteristic of simplicity in this study. SARI has a positive correlation with simplicity, and BLEU has a positive 442 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 4 3 2 1 4 3 2 1 Fluency The sentence is clear. The sentence has several unclear parts, but the overall meaning can"
Y18-1051,C12-1034,0,0.066168,"Missing"
Y18-1051,P15-2011,0,0.0566936,"Missing"
Y18-1051,W16-4912,0,0.0158432,"with the complex word is sometimes necessary. cabulary restriction, which is different from their research. Lexical simplification systems simplify texts mainly by substituting complex words with simpler alternatives (Paetzold and Specia, 2016; Paetzold and Specia, 2017). Lexical simplification involves the following processes: identification of complex words, generating synonyms or similar phrases by using various similarity measures and ranking as well as selecting the best candidate word. Kajiwara and Yamamoto (2015) used several Japanese paraphrasing datasets for generating substitutions. Hading et al. (2016) also utilised Japanese thesaurus and dependency-based word embeddings. In Japanese, substituting particles surrounding a verb is sometimes necessary when the verb is substituted. In these studies, only one complex word is substituted; hence, lexical substitution accompanied by the replacement of particles cannot be performed. In Figure 1, the complex word “富む (rich)” is substituted by “多い (much)”. However, “その土は栄養 に 多 い” is not fluent. When replacing “富む (rich)” with 438 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the au"
Y18-1051,P15-3006,1,0.871721,"Missing"
Y18-1051,L18-1072,1,0.859181,"Missing"
Y18-1051,P13-1151,0,0.0216146,"can be transformed into a simple structure (Siddharthan and Angrosh, 2014; Lee et al., 2017). Recently, machine translation approaches for text simplification present good performance (Wubben et al., 2012; Nisioi et al., 2017; Zhang and Lapata, 2017). The original and simplified sentences can be assumed as two different languages. This approach is a process of translating the original language into a simplified one. This is also known as monolingual machine translation. The data used in these studies are sourced from the Simple English Wikipedia corpus (Wubben et al., 2012; Zhu et al., 2010; Kauchak, 2013) and the Newsela corpus (Xu et al., 2015). Although these datasets are composed of simple vocabulary, they do not explicitly incorporate vocabulary restrictions. Our research concentrates on voFigure 1: Example of a sentence containing complex words, and the words’ substituted candidates. In Japanese, substituting words surrounding a complex word together with the complex word is sometimes necessary. cabulary restriction, which is different from their research. Lexical simplification systems simplify texts mainly by substituting complex words with simpler alternatives (Paetzold and Specia, 201"
Y18-1051,P16-3001,0,0.0718265,"d is substituted; hence, lexical substitution accompanied by the replacement of particles cannot be performed. In Figure 1, the complex word “富む (rich)” is substituted by “多い (much)”. However, “その土は栄養 に 多 い” is not fluent. When replacing “富む (rich)” with 438 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 “多い” (much)”, the particle “に” must be substituted by“が” together with “富む (rich)”. The problem as to why a substitution candidate does not fit the context is also described by Hading et al. (2016) and Kodaira et al. (2016). To address this issue, we use a simple paraphrasing dictionary that covers alternatives for words surrounding the complex word in the substitution generation step. 3 Methods 3.1 Machine translation approach We implemented the encoder-decoder model in (Nisioi et al., 2017). We defined the architecture into two LSTM layers (Hochreiter and Schmidhuber, 1997), 500 hidden units and 0.3 dropout probability (Srivastava et al., 2014). The vocabulary size is defined to 20,000 . We trained the model for 15 epochs by using Adam optimiser (Kingma and Ba, 2015). Further, we saved the current state of the"
Y18-1051,W17-6307,0,0.0118199,"ength and syntactic structure of the source sentence. Generally, multiple simplification approaches work together to simplify a text. Research for automatic text simplification is generally divided into three systems: rule-based, lexical simplification and machine translation. Rule-based systems use rules that are manually created for syntactic simplification and substitute difficult words by using a predefined vocabulary. By analysing a syntactic structure, a sentence with a particular structure or a complex structure can be transformed into a simple structure (Siddharthan and Angrosh, 2014; Lee et al., 2017). Recently, machine translation approaches for text simplification present good performance (Wubben et al., 2012; Nisioi et al., 2017; Zhang and Lapata, 2017). The original and simplified sentences can be assumed as two different languages. This approach is a process of translating the original language into a simplified one. This is also known as monolingual machine translation. The data used in these studies are sourced from the Simple English Wikipedia corpus (Wubben et al., 2012; Zhu et al., 2010; Kauchak, 2013) and the Newsela corpus (Xu et al., 2015). Although these datasets are composed"
Y18-1051,D15-1166,0,0.0337959,", 2017). We defined the architecture into two LSTM layers (Hochreiter and Schmidhuber, 1997), 500 hidden units and 0.3 dropout probability (Srivastava et al., 2014). The vocabulary size is defined to 20,000 . We trained the model for 15 epochs by using Adam optimiser (Kingma and Ba, 2015). Further, we saved the current state of the model and predicted the perplexity of the model on the development set at the end of each epoch. We selected the model resulting from the epoch with the best perplexity to avoid over-fitting. Additionally, we employed global attention and input feeding described by Luong et al. (2015) for the decoder. The architecture is illustrated in Figure 2. For the attention layer, we compute the context vector ct by using the information obtained from the hidden states of the source sentence, and by computing a weighted average with the alignment weights at . The new hidden state h˜t is computed using a concatenation of the previous hidden state ht and the context vector ct . h˜t = tanh W [ct ; ht ] (1) The global alignment weights are computed using a softmax function over the general scoring method for attention. at (s) = ∑ ¯ exp(hT t Wa hs ) T ¯ s′ exp(ht Was′ hs ) (2) Input feedi"
Y18-1051,L18-1185,1,0.834107,"Missing"
Y18-1051,C10-1089,0,0.0294397,"Automatic text simplification is a task that reduces the complexity of vocabulary and expressions whilst preserving the main meaning of a text. This technique can be used to render many text resources available for a wide range of readers, including children, non-native speakers and disabled people. As a pre-processing step, text simplification can improve the performance of natural language processing tasks, such as parsing (Chandrasekar et al., 1996), summarisation (Siddharthan et al., 2004; Xu and Grishman, 2009), semantic role labelling (Vickrey and Koller, 2008), information extraction (Miwa et al., 2010) and machine translation (Chen ˇ et al., 2012; Stajner and Popvi´c, 2016). Automatic text simplification involves several subtasks, such as complex word identification, lexical simplification, syntactic simplification, sentence splitting and sentence compression. Various simplification approaches are employed based on context, sentence length and syntactic structure of the source sentence. Generally, multiple simplification approaches work together to simplify a text. Research for automatic text simplification is generally divided into three systems: rule-based, lexical simplification and mach"
Y18-1051,P17-2014,0,0.0504367,"t. Research for automatic text simplification is generally divided into three systems: rule-based, lexical simplification and machine translation. Rule-based systems use rules that are manually created for syntactic simplification and substitute difficult words by using a predefined vocabulary. By analysing a syntactic structure, a sentence with a particular structure or a complex structure can be transformed into a simple structure (Siddharthan and Angrosh, 2014; Lee et al., 2017). Recently, machine translation approaches for text simplification present good performance (Wubben et al., 2012; Nisioi et al., 2017; Zhang and Lapata, 2017). The original and simplified sentences can be assumed as two different languages. This approach is a process of translating the original language into a simplified one. This is also known as monolingual machine translation. The data used in these studies are sourced from the Simple English Wikipedia corpus (Wubben et al., 2012; Zhu et al., 2010; Kauchak, 2013) and the Newsela corpus (Xu et al., 2015). Although these datasets are composed of simple vocabulary, they do not explicitly incorporate vocabulary restrictions. Our research concentrates on voFigure 1: Example o"
Y18-1051,P15-4015,0,0.0254566,"ext vector ct . h˜t = tanh W [ct ; ht ] (1) The global alignment weights are computed using a softmax function over the general scoring method for attention. at (s) = ∑ ¯ exp(hT t Wa hs ) T ¯ s′ exp(ht Was′ hs ) (2) Input feeding is a process that sends the previous hidden state, obtained by using the alignment method, to the input at the subsequent step. Figure 2: Architecture of model 3.2 Context-based lexical substitution Lexical simplification is generally performed using the following four steps: complex word identification, substitution generation and substitution selection and ranking (Paetzold and Specia, 2015). Our definition of simplicity is based on the inclusion of the core vocabulary in the simplified text. In the complex word identification step, we checked whether all words in the input sentences exist in the core vocabulary list. We defined words other than in the core vocabulary as complex words. In the substitution generation step, we generated the substitution candidates from the Japanese simplified dictionary as described in section 4.1. We also added a copy of the original word into the substitution candidates because sometimes all candidates generated by the dictionary do not fit the c"
Y18-1051,Y18-1000,0,0.272545,"his feature by averaging the semantic similarities between a substitution candidate and each word based on the context of the original word. 3 p(tk |t1 , t2 , ..., tk−1 ) (3) (5) BCCWJ , which is a corpus of various Japanese texts, including books, magazines, newspapers, white papers, blogs, net bulletin boards, textbooks and law. 440 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 Total #sentences Total #tokens Vocabulary size Avg. #characters per sentence Avg. #words per sentence Maruyama and Yamamoto (2018) Original Simplified 50,000 50,000 490,021 516,881 8,786 2,238 Katsuta and Yamamoto (2018) Original Simplified 35,000 35,000 434,544 504,850 16,587 3,939 14.79 15.35 19.18 21.46 9.80 10.34 12.42 14.42 Table 1: Corpora statistics. We presented the number of words in the vocabulary after changing them to the basic form based on the UniDic dictionary. This vocabulary size also includes words, such as proper nouns and symbols. Therefore, the vocabulary size of the simplified version is more than 2,000 words. csim(ω, c) = 1 |C(ω)| ∑ cos(vω′ , vω ) (6) ω ′ ∈C(ω) , where C(ω) is a set of context word"
Y18-1051,E17-2006,0,0.012442,"d the Newsela corpus (Xu et al., 2015). Although these datasets are composed of simple vocabulary, they do not explicitly incorporate vocabulary restrictions. Our research concentrates on voFigure 1: Example of a sentence containing complex words, and the words’ substituted candidates. In Japanese, substituting words surrounding a complex word together with the complex word is sometimes necessary. cabulary restriction, which is different from their research. Lexical simplification systems simplify texts mainly by substituting complex words with simpler alternatives (Paetzold and Specia, 2016; Paetzold and Specia, 2017). Lexical simplification involves the following processes: identification of complex words, generating synonyms or similar phrases by using various similarity measures and ranking as well as selecting the best candidate word. Kajiwara and Yamamoto (2015) used several Japanese paraphrasing datasets for generating substitutions. Hading et al. (2016) also utilised Japanese thesaurus and dependency-based word embeddings. In Japanese, substituting particles surrounding a verb is sometimes necessary when the verb is substituted. In these studies, only one complex word is substituted; hence, lexical"
Y18-1051,P16-2024,0,0.0334355,"Missing"
Y18-1051,E14-1076,0,0.0212733,"ed based on context, sentence length and syntactic structure of the source sentence. Generally, multiple simplification approaches work together to simplify a text. Research for automatic text simplification is generally divided into three systems: rule-based, lexical simplification and machine translation. Rule-based systems use rules that are manually created for syntactic simplification and substitute difficult words by using a predefined vocabulary. By analysing a syntactic structure, a sentence with a particular structure or a complex structure can be transformed into a simple structure (Siddharthan and Angrosh, 2014; Lee et al., 2017). Recently, machine translation approaches for text simplification present good performance (Wubben et al., 2012; Nisioi et al., 2017; Zhang and Lapata, 2017). The original and simplified sentences can be assumed as two different languages. This approach is a process of translating the original language into a simplified one. This is also known as monolingual machine translation. The data used in these studies are sourced from the Simple English Wikipedia corpus (Wubben et al., 2012; Zhu et al., 2010; Kauchak, 2013) and the Newsela corpus (Xu et al., 2015). Although these da"
Y18-1051,C04-1129,0,0.053509,"on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 2 Related Works Automatic text simplification is a task that reduces the complexity of vocabulary and expressions whilst preserving the main meaning of a text. This technique can be used to render many text resources available for a wide range of readers, including children, non-native speakers and disabled people. As a pre-processing step, text simplification can improve the performance of natural language processing tasks, such as parsing (Chandrasekar et al., 1996), summarisation (Siddharthan et al., 2004; Xu and Grishman, 2009), semantic role labelling (Vickrey and Koller, 2008), information extraction (Miwa et al., 2010) and machine translation (Chen ˇ et al., 2012; Stajner and Popvi´c, 2016). Automatic text simplification involves several subtasks, such as complex word identification, lexical simplification, syntactic simplification, sentence splitting and sentence compression. Various simplification approaches are employed based on context, sentence length and syntactic structure of the source sentence. Generally, multiple simplification approaches work together to simplify a text. Researc"
Y18-1051,W16-3411,0,0.0457493,"Missing"
Y18-1051,P08-1040,0,0.0428032,"right 2018 by the authors PACLIC 32 2 Related Works Automatic text simplification is a task that reduces the complexity of vocabulary and expressions whilst preserving the main meaning of a text. This technique can be used to render many text resources available for a wide range of readers, including children, non-native speakers and disabled people. As a pre-processing step, text simplification can improve the performance of natural language processing tasks, such as parsing (Chandrasekar et al., 1996), summarisation (Siddharthan et al., 2004; Xu and Grishman, 2009), semantic role labelling (Vickrey and Koller, 2008), information extraction (Miwa et al., 2010) and machine translation (Chen ˇ et al., 2012; Stajner and Popvi´c, 2016). Automatic text simplification involves several subtasks, such as complex word identification, lexical simplification, syntactic simplification, sentence splitting and sentence compression. Various simplification approaches are employed based on context, sentence length and syntactic structure of the source sentence. Generally, multiple simplification approaches work together to simplify a text. Research for automatic text simplification is generally divided into three systems:"
Y18-1051,N18-2013,0,0.0112128,"uency The sentence is clear. The sentence has several unclear parts, but the overall meaning can be understood. Although a sentence is not clear, its meaning can be inferred. The sentence is not clear and its meaning cannot be understood totally. Adequency The meanings of the two sentences are the same. The main meanings of the two sentences are the same. The meanings of the two sentences are different, but they partially match. The meanings of the two sentences are different. Table 3: Criteria for fluency score and meaning preservation score correlation with fluency and meaning preservation (Vu et al., 2018). In addition, we manually evaluated the randomly selected 50 sentences from the evaluation data of the RARE and NORMAL datasets related to fluency and adequacy. We defined the evaluation criteria to four stages (i.e. higher marks indicate better output) based on the criteria shown in Table 3. We also counted the total number of changes (Total) generated by each system, considering the change of phrase as one change. Instances involving deletion of complex words are considered as a single change. The changes that preserve the original meaning and render the sentences to be more simpler to unde"
Y18-1051,P12-1107,0,0.0668993,"Missing"
Y18-1051,W09-2809,0,0.0242595,"and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 2 Related Works Automatic text simplification is a task that reduces the complexity of vocabulary and expressions whilst preserving the main meaning of a text. This technique can be used to render many text resources available for a wide range of readers, including children, non-native speakers and disabled people. As a pre-processing step, text simplification can improve the performance of natural language processing tasks, such as parsing (Chandrasekar et al., 1996), summarisation (Siddharthan et al., 2004; Xu and Grishman, 2009), semantic role labelling (Vickrey and Koller, 2008), information extraction (Miwa et al., 2010) and machine translation (Chen ˇ et al., 2012; Stajner and Popvi´c, 2016). Automatic text simplification involves several subtasks, such as complex word identification, lexical simplification, syntactic simplification, sentence splitting and sentence compression. Various simplification approaches are employed based on context, sentence length and syntactic structure of the source sentence. Generally, multiple simplification approaches work together to simplify a text. Research for automatic text sim"
Y18-1051,Q15-1021,0,0.0139866,"ture (Siddharthan and Angrosh, 2014; Lee et al., 2017). Recently, machine translation approaches for text simplification present good performance (Wubben et al., 2012; Nisioi et al., 2017; Zhang and Lapata, 2017). The original and simplified sentences can be assumed as two different languages. This approach is a process of translating the original language into a simplified one. This is also known as monolingual machine translation. The data used in these studies are sourced from the Simple English Wikipedia corpus (Wubben et al., 2012; Zhu et al., 2010; Kauchak, 2013) and the Newsela corpus (Xu et al., 2015). Although these datasets are composed of simple vocabulary, they do not explicitly incorporate vocabulary restrictions. Our research concentrates on voFigure 1: Example of a sentence containing complex words, and the words’ substituted candidates. In Japanese, substituting words surrounding a complex word together with the complex word is sometimes necessary. cabulary restriction, which is different from their research. Lexical simplification systems simplify texts mainly by substituting complex words with simpler alternatives (Paetzold and Specia, 2016; Paetzold and Specia, 2017). Lexical si"
Y18-1051,Q16-1029,0,0.0158545,"he core vocabulary. We defined the target words to 20,000 high-frequency words in BCCWJ and Tanaka corpus. These words do not include proper nouns, onomatopoeia, numbers and symbols. We asked seven annotators who employed crowdsourcing to simplify these 20,000 words. We divided the 20,000 words into four files such that each file contains 5,000 words, and assigned one file to each an4.2 Evaluation metrics We evaluated the model’s output based on three metrics:1) BLEU with NIST smoothing described by Chen and Cherry (2014), which is a traditional machine translation evaluation metric; 2) SARI (Xu et al., 2016), which is a recent text-simplification metric; and 3) core vocabulary ratio (Core) contained in a sentence, which is considered to be a characteristic of simplicity in this study. SARI has a positive correlation with simplicity, and BLEU has a positive 442 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 4 3 2 1 4 3 2 1 Fluency The sentence is clear. The sentence has several unclear parts, but the overall meaning can be understood. Although a sentence is not clear, its meaning can be inferred. The senten"
Y18-1051,D17-1062,0,0.0173488,"atic text simplification is generally divided into three systems: rule-based, lexical simplification and machine translation. Rule-based systems use rules that are manually created for syntactic simplification and substitute difficult words by using a predefined vocabulary. By analysing a syntactic structure, a sentence with a particular structure or a complex structure can be transformed into a simple structure (Siddharthan and Angrosh, 2014; Lee et al., 2017). Recently, machine translation approaches for text simplification present good performance (Wubben et al., 2012; Nisioi et al., 2017; Zhang and Lapata, 2017). The original and simplified sentences can be assumed as two different languages. This approach is a process of translating the original language into a simplified one. This is also known as monolingual machine translation. The data used in these studies are sourced from the Simple English Wikipedia corpus (Wubben et al., 2012; Zhu et al., 2010; Kauchak, 2013) and the Newsela corpus (Xu et al., 2015). Although these datasets are composed of simple vocabulary, they do not explicitly incorporate vocabulary restrictions. Our research concentrates on voFigure 1: Example of a sentence containing c"
Y18-1051,C10-1152,0,0.0349618,"complex structure can be transformed into a simple structure (Siddharthan and Angrosh, 2014; Lee et al., 2017). Recently, machine translation approaches for text simplification present good performance (Wubben et al., 2012; Nisioi et al., 2017; Zhang and Lapata, 2017). The original and simplified sentences can be assumed as two different languages. This approach is a process of translating the original language into a simplified one. This is also known as monolingual machine translation. The data used in these studies are sourced from the Simple English Wikipedia corpus (Wubben et al., 2012; Zhu et al., 2010; Kauchak, 2013) and the Newsela corpus (Xu et al., 2015). Although these datasets are composed of simple vocabulary, they do not explicitly incorporate vocabulary restrictions. Our research concentrates on voFigure 1: Example of a sentence containing complex words, and the words’ substituted candidates. In Japanese, substituting words surrounding a complex word together with the complex word is sometimes necessary. cabulary restriction, which is different from their research. Lexical simplification systems simplify texts mainly by substituting complex words with simpler alternatives (Paetzold"
Y18-1057,Y18-1000,0,0.272277,"Missing"
Y18-1057,L18-1215,0,0.0276096,"ding and stress/accents) (Hall and Sproat, 2013; Sproat and Hall, 2014). These are determined on the basis of linguistic analysis with natural language processing technologies. Although an adjacent word information such as a statistical language model is applied for the processing, a few words in narrow contexts are used that sometimes lead to errors. Among the errors, those in pronunciation are noticeable in the TTS situations. Furthermore, it has been reported that the subjective evaluation of TTS by listeners is particularly sensitive to homograph disambiguation errors (Braga et al., 2007; Gorman et al., 2018). Thus, homograph disambiguation has been one of the important tasks in synthesis and has been tackled by many researchers in the past (Yarowsky, 1997; Braga et al., 2007). Homographs are words that share the same written form as other words but have a different meaning, and can be classified into either homonyms or heteronyms, depending on their pronunciation, e.g., homonyms have the same pronunciation, such as lie (untruth) [l´Ai] and lie (to recline) [l´Ai], whereas heteronyms have different pronunciation, such as desert (region) [d´ez@rt] and desert (to leave) [diz’@:rt]. Thus, disambiguat"
Y18-1057,C92-2070,0,0.185084,"ing, and can be classified into either homonyms or heteronyms, depending on their pronunciation, e.g., homonyms have the same pronunciation, such as lie (untruth) [l´Ai] and lie (to recline) [l´Ai], whereas heteronyms have different pronunciation, such as desert (region) [d´ez@rt] and desert (to leave) [diz’@:rt]. Thus, disambiguation of heteronyms is a more important task in TTS synthesis, as the correct pronunciation of the word in its context must be determined. Statistical methods have been considered to be better techniques for disambiguation, such as word sense/homograph disambiguation (Yarowsky, 1992; Yarowsky, 1997; Mihalcea and Moldovan, 1999; Gorman et al., 2018). These methods require large corpora that are manually tagged with correct meanings or pronunciations. The corpora are used to construct models for disambiguation and the disambiguation accuracy is strongly affected by the size of the corpora. However, manual tagging is expensive and causes serious impediments in the application of 495 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 statistical methods to word sense/pronunciation disambi"
Y18-1057,H94-1046,0,0.540887,"· · · 私も今日気になったので、本屋をのぞいてみても · · · · · · = I was also interested today, I looked into the bookstore · · · pronunciation tag is set to /kyoo/ Judged “Appropriate” pronunciation /kyoo/ is tagged to the sentence obtained in step 3 The four steps outlined above are applied for each pronunciation of Wap . The details of the steps are described in the following subsections and Table 1 shows examples of the processing results at each step. 2.1 Word replacement step (Step 1) This step replaces the word with ambiguity in pronunciation (Wap ) by a word with unique pronunciation (Wup ). Wordnet is used (Miller et al., 1994) to find the word (Wup ) whose sense is the same as Wap and whose pronunciation is unique. If the appropriate word is not found in WordNet, Wup is manually specified. Examples are given below using a Japanese word “今日” as a word with ambiguity in pronunciation (Wap ). Hence we use the Japanese WordNet (Isahara et al., 2008). The Japanese word “今日” is a heteronym, i.e., the word whose pronunciation is /kon nichi/ means “recently,” whereas the word whose pronunciation is /kyoo/ means “today.” As the word with both pronunciations functions as an adverb in sentences, it is necessary to disambiguat"
Y18-1081,Y18-1000,0,0.268016,"Missing"
Y18-1081,N15-1146,0,0.0617677,"Missing"
Y18-1081,N18-5012,0,0.0279509,"it has many ways to add. In addition, because only a few reviews of this type were present, remove it are faster but not affect the number of data. • Emotion icon: :)), @-@, :((, etc. • Web link address. All remaining reviews were POS tagged using the same program as in the translation process. The main purpose of this dictionary is to consider ambiguous words, and this can be accomplished by extracting contextual information from each sentiment word. Therefore, understanding the relationship between all words in a sentence is necessary for extraction. Thus, the dependency parser VnCoreNLP by Vu et al. (2018) was used for this purpose, as it proved to have the best performance for this task in the Vietnamese language. Pre-processed data were then used to perform extraction using a rule-based method. As most of the sentiment words were either adjectives or verbs, adjectives and verbs were emphasized 708 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 in producing the dictionary. All adjectives and verbs in the corpus were extracted and assigned a polarity depending on the score of each review. Here, neutral r"
