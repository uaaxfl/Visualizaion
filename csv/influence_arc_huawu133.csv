2005.mtsummit-papers.4,2004.iwslt-evaluation.15,0,0.0278508,"ine Translation (EBMT) method based on Tree String Correspondence (TSC) and statistical generation. In this method, the translation examples are represented as TSC. The translation consists of three steps. The input sentence is first parsed into a tree. Then the TSC forest is searched out if it is best matched with the input tree. Lastly, the translation is generated using a statistical generation model to combine the target language strings in the TSCs. Many EBMT systems use annotated tree structures as translation examples (Watanabe, 1992; Poutsma, 2000; Al-Adhaileh et al., 2002; Way, 2003; Aramaki and Kurohashi, 2004). In these systems, it is necessary to parse both the source 25 translation fragments with a statistical model. The statistical model can improve the fluency by using n-gram co-occurrence statistics. However, the statistical model does not take into account the semantic relation between the translation example and the input sentence. In this paper, we propose a new method to select the translation fragments and generate the translation, which combines the semantic-based approach and the statistical approach. The generation model consists of three parts: the semantic similarity between the tree"
2005.mtsummit-papers.4,2001.mtsummit-papers.12,0,0.0414753,"thm based on TSC to find the TSC forest that is best matched with the input tree. For EBMT systems, there are two major approaches to select the appropriate translation fragments and generate the translation. The semantic-based approach (Aramaki et al., 2003; Armaki and Kurohashi, 2004) obtains an appropriate translation fragment for each part of the input sentence. The final translation is generated by combining the translation fragments in a pre-defined order. This approach does not take into account the fluency between the translation fragments. The statistical approach (Kaki et al., 1999; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Imamura et al., 2004) selects Abstract This paper proposes a novel Example-Based Machine Translation (EBMT) method based on Tree String Correspondence (TSC) and statistical generation. In this method, the translation examples are represented as TSC, which consists of three parts: a parse tree in the source language, a string in the target language, and the correspondences between the leaf nodes of the source language tree and the substrings of the target language string. During the translation, the input sentence is first parsed into a tree. Then the TSC forest is searche"
2005.mtsummit-papers.4,C92-2115,0,0.0608775,"stems. 1 Introduction This paper proposes a novel Example-Based Machine Translation (EBMT) method based on Tree String Correspondence (TSC) and statistical generation. In this method, the translation examples are represented as TSC. The translation consists of three steps. The input sentence is first parsed into a tree. Then the TSC forest is searched out if it is best matched with the input tree. Lastly, the translation is generated using a statistical generation model to combine the target language strings in the TSCs. Many EBMT systems use annotated tree structures as translation examples (Watanabe, 1992; Poutsma, 2000; Al-Adhaileh et al., 2002; Way, 2003; Aramaki and Kurohashi, 2004). In these systems, it is necessary to parse both the source 25 translation fragments with a statistical model. The statistical model can improve the fluency by using n-gram co-occurrence statistics. However, the statistical model does not take into account the semantic relation between the translation example and the input sentence. In this paper, we propose a new method to select the translation fragments and generate the translation, which combines the semantic-based approach and the statistical approach. The"
2005.mtsummit-papers.41,W99-0606,0,0.0770036,"Missing"
2005.mtsummit-papers.41,P98-1004,0,0.0322207,"word aligners. Experimental results indicate that the boosting method proposed in this paper performs much better than the original word aligner, achieving a large error rate reduction. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al., 1993). In recent years, some researchers have employed statistical word alignment models to build alignment links (Brown et al., 1993; Wu, 1997; Och and Ney, 2000; Cherry and Lin, 2003). Other researchers used similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufis and Barbu, 2002). One issue about word alignment is how to improve the performance of a word aligner when the training data is fixed. One possible solution is to use boosting (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 1997 & 2000). Boosting generates the classifiers sequentially and changes the weight of the training instance that is provided as input to each inducer based on the previously built classifiers. The underlying idea of boosting is to combine simple “rules” to form an ensemble such that the 313 including reference set construction, error"
2005.mtsummit-papers.41,P03-1012,0,0.0514034,"dition, the final ensemble takes into account the weights of the alignment links produced by the individual word aligners. Experimental results indicate that the boosting method proposed in this paper performs much better than the original word aligner, achieving a large error rate reduction. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al., 1993). In recent years, some researchers have employed statistical word alignment models to build alignment links (Brown et al., 1993; Wu, 1997; Och and Ney, 2000; Cherry and Lin, 2003). Other researchers used similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufis and Barbu, 2002). One issue about word alignment is how to improve the performance of a word aligner when the training data is fixed. One possible solution is to use boosting (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 1997 & 2000). Boosting generates the classifiers sequentially and changes the weight of the training instance that is provided as input to each inducer based on the previously built classifiers. The underlying idea of boosting is"
2005.mtsummit-papers.41,P98-1083,0,0.0760757,"Missing"
2005.mtsummit-papers.41,A00-2005,0,0.0357531,"Missing"
2005.mtsummit-papers.41,P00-1056,0,0.812121,"training set. In addition, the final ensemble takes into account the weights of the alignment links produced by the individual word aligners. Experimental results indicate that the boosting method proposed in this paper performs much better than the original word aligner, achieving a large error rate reduction. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al., 1993). In recent years, some researchers have employed statistical word alignment models to build alignment links (Brown et al., 1993; Wu, 1997; Och and Ney, 2000; Cherry and Lin, 2003). Other researchers used similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufis and Barbu, 2002). One issue about word alignment is how to improve the performance of a word aligner when the training data is fixed. One possible solution is to use boosting (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 1997 & 2000). Boosting generates the classifiers sequentially and changes the weight of the training instance that is provided as input to each inducer based on the previously built classifiers. The underly"
2005.mtsummit-papers.41,tufis-barbu-2002-lexical,0,0.0267131,"tal results indicate that the boosting method proposed in this paper performs much better than the original word aligner, achieving a large error rate reduction. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al., 1993). In recent years, some researchers have employed statistical word alignment models to build alignment links (Brown et al., 1993; Wu, 1997; Och and Ney, 2000; Cherry and Lin, 2003). Other researchers used similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufis and Barbu, 2002). One issue about word alignment is how to improve the performance of a word aligner when the training data is fixed. One possible solution is to use boosting (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 1997 & 2000). Boosting generates the classifiers sequentially and changes the weight of the training instance that is provided as input to each inducer based on the previously built classifiers. The underlying idea of boosting is to combine simple “rules” to form an ensemble such that the 313 including reference set construction, error rate calculation, and th"
2005.mtsummit-papers.41,N01-1003,0,0.0607072,"Missing"
2005.mtsummit-posters.7,2003.mtsummit-papers.23,0,0.0282081,"n technical document translation, product localization, etc. Now, a number of TMS tools are available at market such as Trados’ Translator’s WorkBench, SDL, IBM Translation Manager/2 and Transit. In general, the traditional TMS retrieves examples matched with the input sentence at the sentence level (Planas and Furuse, 2000). It provides good translation suggestions only when there are closely matched examples in the TM. This leads to low coverage on unseen sentences or texts. In order to solve this problem, many researchers use sub-sentential matching (Brown, 1996; Simard and Langlais, 2001; Huang et al., 2003). Brown (1996) segmented the input sentence into sequences of words and determined the translation of these sequences by performing subsentential alignment on each matched example. Simard and Langlais (2001) ranked the examples according to the length of matched sub-sequence of words. A longest available sub-sequence strategy was adopted to cover as much part of source sentences as possible. Huang et al. (2003) proposed a unified framework to generate translations with statistical confidences. Their experimental results indicated that sub-sentential matching improved translation efficiency and"
2005.mtsummit-posters.7,C92-2101,0,0.15871,"Missing"
2005.mtsummit-posters.7,macklovitch-russell-2000-whats,0,0.0705571,"Missing"
2005.mtsummit-posters.7,2001.mtsummit-ebmt.3,0,0.117687,"measurement data is fixed. 只有 在 测量 数据 被 确定 后，所 描绘 的 区域 才会 闭合。 Translation Suggestion: (9) 所有 数据 都 显示 在 测量 显示 窗 中， 只 有 在 测量 数据 被 确定 后。 where Wt i (k ) is the weight of the example k that Figure 3. An Example Showing Translation Generation containing the sub-sequence s i ; LenSub( s i ) is the length of the sub-sequence s i ; LenSrc(k ) is the length of the source part of the example k; AlignScore(k ) is the alignment score of the example k, which is calculated as shown in (6). patterns were extracted. Carl (1999) used shallow parsing methods to extract patterns. Güvenir and Cicekli (1998) and McTait (2001) used languageneutral methods to extract translation patterns. In this paper, we make use of the word alignment information and a source language parser to extract translation patterns from the examples. The extracted patterns are refined based on their occurring frequency. These patterns are then applied for sentence translation. After obtaining the best example for each subsequence, we locate the translations in the examples using word alignment information and combine the translations to generate the translation suggestion. An example is shown in Figure 3. The input sentence is segmented in"
2005.mtsummit-posters.7,P02-1040,0,0.0712064,"ficiency Evaluation Results Table 1. Statistics of Two Testing Sets 6.2 Baseline System 257 223 240 precsion = recall = |SG ∩ S R | |SG | |SG ∩ S R | |SR | (12) (13) Besides precision and recall, we also use NIST score (Doddington, 2002) for evaluation. The NIST score is calculated by using the statistics of n-gram co-occurrence. It measures both the adequacy and fluency of the translation by comparing it with the translation references. Each sentence can have one or more translation references. It is reported that the NIST score correlates better with the human judgments than the BLEU score (Papineni et al., 2002). Although our system is not an automatic translation system, the adequacy and fluency measures of the translation suggestion are also useful for TMS. For each sentence in the testing sets, we have one translation reference. The evaluation results are shown in Table 4. The results indicate that our system provides higher quality of translation suggestions than the baseline system. Group Two Our System Baseline System Our System Baseline System Our System Table 2. Testing Order Test 1 Test 1 Test 2 Test 2 When the users translate the sentences, the time that they spend on the translation is rec"
2005.mtsummit-posters.7,C00-2090,0,0.0276033,"translators for post-editing. Thus, TMS is also called a Machine Aided Human Translation System (MAHTS). Generally speaking, TMS does not conduct real translation (Macklovitch & Russell, 2000). However, it can avoid repetitive labor and improve translation efficiency. It has been widely applied in technical document translation, product localization, etc. Now, a number of TMS tools are available at market such as Trados’ Translator’s WorkBench, SDL, IBM Translation Manager/2 and Transit. In general, the traditional TMS retrieves examples matched with the input sentence at the sentence level (Planas and Furuse, 2000). It provides good translation suggestions only when there are closely matched examples in the TM. This leads to low coverage on unseen sentences or texts. In order to solve this problem, many researchers use sub-sentential matching (Brown, 1996; Simard and Langlais, 2001; Huang et al., 2003). Brown (1996) segmented the input sentence into sequences of words and determined the translation of these sequences by performing subsentential alignment on each matched example. Simard and Langlais (2001) ranked the examples according to the length of matched sub-sequence of words. A longest available s"
2005.mtsummit-posters.7,2001.mtsummit-papers.60,0,0.0848986,"t has been widely applied in technical document translation, product localization, etc. Now, a number of TMS tools are available at market such as Trados’ Translator’s WorkBench, SDL, IBM Translation Manager/2 and Transit. In general, the traditional TMS retrieves examples matched with the input sentence at the sentence level (Planas and Furuse, 2000). It provides good translation suggestions only when there are closely matched examples in the TM. This leads to low coverage on unseen sentences or texts. In order to solve this problem, many researchers use sub-sentential matching (Brown, 1996; Simard and Langlais, 2001; Huang et al., 2003). Brown (1996) segmented the input sentence into sequences of words and determined the translation of these sequences by performing subsentential alignment on each matched example. Simard and Langlais (2001) ranked the examples according to the length of matched sub-sequence of words. A longest available sub-sequence strategy was adopted to cover as much part of source sentences as possible. Huang et al. (2003) proposed a unified framework to generate translations with statistical confidences. Their experimental results indicated that sub-sentential matching improved trans"
2005.mtsummit-posters.7,C96-1030,0,0.0397118,"efficiency. It has been widely applied in technical document translation, product localization, etc. Now, a number of TMS tools are available at market such as Trados’ Translator’s WorkBench, SDL, IBM Translation Manager/2 and Transit. In general, the traditional TMS retrieves examples matched with the input sentence at the sentence level (Planas and Furuse, 2000). It provides good translation suggestions only when there are closely matched examples in the TM. This leads to low coverage on unseen sentences or texts. In order to solve this problem, many researchers use sub-sentential matching (Brown, 1996; Simard and Langlais, 2001; Huang et al., 2003). Brown (1996) segmented the input sentence into sequences of words and determined the translation of these sequences by performing subsentential alignment on each matched example. Simard and Langlais (2001) ranked the examples according to the length of matched sub-sequence of words. A longest available sub-sequence strategy was adopted to cover as much part of source sentences as possible. Huang et al. (2003) proposed a unified framework to generate translations with statistical confidences. Their experimental results indicated that sub-sentent"
2007.mtsummit-papers.41,C02-1076,0,0.0533523,"Missing"
2007.mtsummit-papers.41,2004.iwslt-evaluation.15,0,0.0446848,"Missing"
2007.mtsummit-papers.41,W03-0312,0,0.0659638,"Missing"
2007.mtsummit-papers.41,2005.mtsummit-ebmt.1,0,0.072749,"Missing"
2007.mtsummit-papers.41,J96-1002,0,0.0325179,"symbol, then the substitution symbol is replaced with the translation of the corresponding substitution node. Log-linear Generation Models We incorporate various features into our log-linear models. Given the input (source language sentence) f=f1J=f1,...,fj,...,fJ, the translation (target language sentence) e=e1I=e1,...,ei,...,eI with the highest probability is chosen from the possible target language sentences according to Eq. 1. e = arg max{ p (e′ |f )} e′ (1) Based on the maximum entropy framework, we directly model the posterior probability p (e |f ) using the same method as described in (Berger et al., 1996). In this framework, there are M feature functions hm(e,f), m=1,...,M. For each feature function, there exists a model parameter λm . We can get the translation probability as described in Eq. 2. M exp[ p(e |f ) = ∑ ∑λ m =1 M exp[ e′ ∑ m hm (e, f )] (2) λm hm (e′, f )] e = arg max{ p(e′ |f )} e′ ∑λ e′ m hm (e′, f )} (3) m =1 Typically, p(e|f) can be decomposed by adding hidden variables. To include the dependence on the hidden variables, we extend the feature functions by including the following hidden variables: the parse tree F of the input sentence and the TSC forest Z with K TSCs z1K=z1,.."
2007.mtsummit-papers.41,J04-4004,0,0.0162922,"used to translate words that cannot be covered by the translation examples. z Language Model: The Chinese language model in our experiments is a word-based trigram model, which is trained using the SRILM toolkit (Stolcke, 2002) on a general Chinese corpus with 228 million words. z English Parser: The English sentence is parsed by the parser of Collins (1999). We use its model 3 and default settings. In the original result of the parser, the punctuation node always occurs as a right sibling of the previous leaf node. If so, the punctuation node cannot always act as a coordinating conjunction (Bikel, 2004). Thus, we change the position of the punctuation node in the tree. For the punctuation node n, if it is the most left/right leaf node, then we set the root node of the tree as the parent node of n. Otherwise, let nr be the nearest right neighbor of n. Then the nearest common ancestor of n and nr is set as the parent node of n. Evaluation of Log-linear Generation Models We conduct four experiments to investigate performance of the log-linear generation models. z the E1 (MS+WTP+LMP) In the experiment, the log-linear generation models use three feature functions: matching score (MS), word transl"
2007.mtsummit-papers.41,P91-1022,0,0.111489,"target language sentence given the source language sentence. p( I |J ) = C(I , J ) C ( I ′, J ) ∑ (17) I′ Here the word number of the sentence is defined as the length of the sentence. The model is trained on the bilingual example database. In the translation process, we need to score the length of the translation fragment, instead of the length of the sentence. The length of the fragment is more flexible. It is difficult to directly model the fragment length selection. The target language length tends to follow a normal distribution on the fixed source language length in the parallel corpus (Brown et al., 1991). Based on the normal distribution, we approximately model the fragment length selection using the ratio of the target language fragment length to the source language fragment length. p N (r ; μ , σ 2 ) = 1 σ 2π exp(− (r − μ ) 2 2σ 2 ) (18) Here r=I/J. We approximately estimate μ and σ2 using the length of the sentences in the bilingual example database. μ and σ2 are 1.03 and 5.56, respectively. Thus, the length selection probability hLSP is calculated as in Eq. 19. hLSP (e, f , F, Z) If e is a sentence ⎧log p( I |J ) ⎪ = ⎨ I 2 ⎪⎩log p N ( J ; μ , σ ) Else (19) Search Here p ( s |t ) is the pr"
2007.mtsummit-papers.41,2001.mtsummit-papers.12,0,0.0291974,"Missing"
2007.mtsummit-papers.41,2005.mtsummit-ebmt.3,0,0.102406,"Missing"
2007.mtsummit-papers.41,W05-0833,0,0.0217629,"Missing"
2007.mtsummit-papers.41,2006.eamt-1.15,0,0.0432219,"Missing"
2007.mtsummit-papers.41,2003.mtsummit-papers.22,0,0.0481467,"Missing"
2007.mtsummit-papers.41,2006.eamt-1.8,0,0.0496657,"Missing"
2007.mtsummit-papers.41,2005.mtsummit-ebmt.9,0,0.0430202,"Missing"
2007.mtsummit-papers.41,C04-1015,0,0.0448845,"Missing"
2007.mtsummit-papers.41,J98-1002,0,0.0203588,") = Sim( f1 , f 2 ) m =1 M In our system, only 1-best parse tree is considered, so we get Eq. 5: (4) 2 × log p(C0 ) log p (C1 ) + log p(C2 ) (7) Here C1 and C2 are the headwords of n1 and n2, respectively; C1 and C2 are the concepts subsuming the words f1 and f2, respectively; C0 is the nearest common ancestor in the semantic hierarchy that subsumes both C1 and C2; p(Ci) is the probability of encountering an instance of Ci in the corpus. Hence, we get the feature function hMS: K hMS (e, f , F, Z) = log ∏ M ( z , F) k (8) k =1 Context Similarity Context similarity is used in various NLP tasks (Karov & Edelman, 1998; Schafer & Yarowsky, 2002). We use it to select the approximate translation example and to further improve translation selection. The context similarity is defined as the word-based cosine distance between sentences. CS (f , z ) = V ⋅ V′ ∑ (v ) 2 i ∑ (v ′ ) × 2 (9) j i I ∏ CS (f , z k ) (10) k =1 Word Translation Probability The quality of word alignment in the translation example affects the quality of the translation. To estimate the quality of the word alignment in TSC, we use the singleword translation probability between the source language matching-tree and the target language string. T"
2007.mtsummit-papers.41,P95-1034,0,0.113767,"Missing"
2007.mtsummit-papers.41,koen-2004-pharaoh,0,0.0478734,"Missing"
2007.mtsummit-papers.41,P03-1021,0,0.0226095,"Missing"
2007.mtsummit-papers.41,P00-1056,0,0.042021,"S(are) … String Replacement 4 VP(are) ○ n-best Selection ○ 3 NPB(employeees) 6 NP-A (backbone) ○ 7 DT(the) ○ 8 JJR(old) ○ 9 NNS(employeers) ○ 5 VBP(are) ○ 11 PP(of) ○ 10 NPB(backbone) ○ 12 DT(the) ○ 14 IN(of) ○ 15 NPB(industry) ○ 13 NN(backbone) ○ 老职工 那些 老 雇员 那些 老 员工 NPB 16 DT(the) ○ 17 NN(industry) ○ &lt;NPB&gt;是&lt;NP-A&gt;。 &lt;NPB&gt;常是&lt;NP-A&gt;。 &lt;NPB&gt;常是&lt;NP-A&gt;的。 &lt;NPB&gt;骨干 &lt;NPB&gt;的 骨干 骨干的&lt;NPB&gt; NP-A 那些 老 雇员 是 行业 的 骨干 。 那些 老 员工 是 该 行业 的骨干。 老 职工 常 是 行业 骨干 。 NPB 行业 该 行业 这 产业 行业 的 骨干 该 行业 的 骨干 行业 骨干 Figure 3. Example of Search English words and Chinese words in the sentence pairs are automatically aligned using GIZA++ (Och & Ney, 2000). Test Set & Development Set: The test set contains 400 English sentences and each sentence has two reference (human) translations. The development set contains 100 and each sentence has two references. Both of them are not included in the translation examples. z Translation Dictionary: A general English-Chinese translation dictionary is used to translate words that cannot be covered by the translation examples. z Language Model: The Chinese language model in our experiments is a word-based trigram model, which is trained using the SRILM toolkit (Stolcke, 2002) on a general Chinese corpus with"
2007.mtsummit-papers.41,P02-1038,0,0.153886,"Missing"
2007.mtsummit-papers.41,J04-4002,0,0.0932334,"Missing"
2007.mtsummit-papers.41,P02-1040,0,0.0743703,"Missing"
2007.mtsummit-papers.41,W02-2026,0,0.0223618,"1 M In our system, only 1-best parse tree is considered, so we get Eq. 5: (4) 2 × log p(C0 ) log p (C1 ) + log p(C2 ) (7) Here C1 and C2 are the headwords of n1 and n2, respectively; C1 and C2 are the concepts subsuming the words f1 and f2, respectively; C0 is the nearest common ancestor in the semantic hierarchy that subsumes both C1 and C2; p(Ci) is the probability of encountering an instance of Ci in the corpus. Hence, we get the feature function hMS: K hMS (e, f , F, Z) = log ∏ M ( z , F) k (8) k =1 Context Similarity Context similarity is used in various NLP tasks (Karov & Edelman, 1998; Schafer & Yarowsky, 2002). We use it to select the approximate translation example and to further improve translation selection. The context similarity is defined as the word-based cosine distance between sentences. CS (f , z ) = V ⋅ V′ ∑ (v ) 2 i ∑ (v ′ ) × 2 (9) j i I ∏ CS (f , z k ) (10) k =1 Word Translation Probability The quality of word alignment in the translation example affects the quality of the translation. To estimate the quality of the word alignment in TSC, we use the singleword translation probability between the source language matching-tree and the target language string. The word translation probabi"
2007.mtsummit-papers.41,zhang-etal-2004-interpreting,0,0.0259646,"Missing"
2007.mtsummit-papers.52,P98-1004,0,0.0606955,"Missing"
2007.mtsummit-papers.52,W02-1601,0,0.055431,"Missing"
2007.mtsummit-papers.52,P06-1009,0,0.143339,"Missing"
2007.mtsummit-papers.52,P03-1012,0,0.0330102,"Missing"
2007.mtsummit-papers.52,2006.eamt-1.10,0,0.0148468,"emmabased method and the stem-based method outperform the word-based method when only smaller training corpus are available. Although the stem-based method and the lemma-based achieve lower AER, they do not achieve much improvement on translation quality when a larger training corpus is available. This result again confirms that large gains in alignment performance can achieve relatively small gains in translation performance (Lopez and Resnik, 2006). In conclusion, lemma-based and stem-based methods are effective to alleviate the problem of data sparseness. This result is similar to that in (Gupta and Federico 2006). However, the lemma-based method outperforms the stembased method on our corpus, which is different from that in (Gupta and Federico 2006). This may be caused by the different kind of language pairs used. 11 10 9 8 30K 90K Word Lemma 150K Stem Figure 4. Translation Results on the WA Test Set by Using Different Sizes of Training Corpus 15 This paper proposed a method to improve statistical word alignment by combining various clues. Our method first trained a baseline statistical IBM word alignment model and then improved it with different clues. The clues are mainly based on features such as l"
2007.mtsummit-papers.52,J97-2004,0,0.090719,"Missing"
2007.mtsummit-papers.52,koen-2004-pharaoh,0,0.0437797,"development set is also from the corpora distributed for the 2005 HTRDP evaluation of machine translation, which includes 278 sentences, with four reference translations for each source sentence. We use the SRILM toolkit (Stolcke, 2002) to train a language model on the Chinese Gigaword Second Edition provided by LDC (catalog number LDC2005T14). Translation Results We conduct phrase-based statistical machine translation (SMT) from English to Chinese. To perform phrase-based SMT, we need a trainer and a decoder. For training, we use Koehn's training scripts 10 . For the decoder, we use Pharaoh (Koehn, 2004). We run the decoder with its default settings (maximum phrase length 7) and then use Koehn's implementation of minimum error rate training (Och, 2003) to tune the feature weights on the development set. The translation quality was evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). The translation results are shown in Table 5. In the translation experiments, our method combines all of the clues to get the alignment results. Based on the alignment results, we extract the phrase pairs used by the Pharaoh decoder. (a) Without Chunks Baseline Our Method Figur"
2007.mtsummit-papers.52,W00-0729,0,0.0589602,"Missing"
2007.mtsummit-papers.52,P05-1057,0,0.218941,"Missing"
2007.mtsummit-papers.52,2006.amta-papers.11,0,0.0423426,"ared with the wordbased method. On smaller training corpus, lemma-based method outperforms the stem-based method. From the translation results, it can be seen that the lemmabased method and the stem-based method outperform the word-based method when only smaller training corpus are available. Although the stem-based method and the lemma-based achieve lower AER, they do not achieve much improvement on translation quality when a larger training corpus is available. This result again confirms that large gains in alignment performance can achieve relatively small gains in translation performance (Lopez and Resnik, 2006). In conclusion, lemma-based and stem-based methods are effective to alleviate the problem of data sparseness. This result is similar to that in (Gupta and Federico 2006). However, the lemma-based method outperforms the stembased method on our corpus, which is different from that in (Gupta and Federico 2006). This may be caused by the different kind of language pairs used. 11 10 9 8 30K 90K Word Lemma 150K Stem Figure 4. Translation Results on the WA Test Set by Using Different Sizes of Training Corpus 15 This paper proposed a method to improve statistical word alignment by combining various c"
2007.mtsummit-papers.52,W97-0311,0,0.0766868,"Missing"
2007.mtsummit-papers.52,W03-0435,0,0.0475076,"Missing"
2007.mtsummit-papers.52,P06-1065,0,0.0416382,"Missing"
2007.mtsummit-papers.52,P04-1066,0,0.0491909,"Missing"
2007.mtsummit-papers.52,P03-1021,0,0.0165469,"nce translations for each source sentence. We use the SRILM toolkit (Stolcke, 2002) to train a language model on the Chinese Gigaword Second Edition provided by LDC (catalog number LDC2005T14). Translation Results We conduct phrase-based statistical machine translation (SMT) from English to Chinese. To perform phrase-based SMT, we need a trainer and a decoder. For training, we use Koehn's training scripts 10 . For the decoder, we use Pharaoh (Koehn, 2004). We run the decoder with its default settings (maximum phrase length 7) and then use Koehn's implementation of minimum error rate training (Och, 2003) to tune the feature weights on the development set. The translation quality was evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). The translation results are shown in Table 5. In the translation experiments, our method combines all of the clues to get the alignment results. Based on the alignment results, we extract the phrase pairs used by the Pharaoh decoder. (a) Without Chunks Baseline Our Method Figure 2. Alignment Examples w/o Chunks Example (2) shows a sentence pair using the chunk clue. (2) the multi-field, multi-level, and multi-channel cooperat"
2007.mtsummit-papers.52,P00-1056,0,0.0699138,"our experiments, we use OAK to recognize the English chunks, and then recognize Chinese chunks based on both the word alignment results and English chunks. We also use a handcraft Chinese-English dictionary included in HowNet (Dong and Dong, 2006), which is a Chinese conceptual database 8 . This dictionary includes 55,462 entries. The handcraft English-Chinese dictionary is collected from various resources, comprising 64,234 entries. Evaluation Metrics The reference of the test data provided by CLDC includes possible links and sure links. So we use the same evaluation metrics as described in (Och and Ney, 2000). If we use A to indicate the alignments identified by the proposed methods, and S and P to denote the sure and possible links in the reference alignments, the precision, recall, and alignment error rate (AER) are calculated as described in Equations (9), (10) and (11). |A ∩S| |S| |A∩P| recall = |P| |A ∩S |+ |A ∩ P | AER = 1 − |A |+ |S| precision = (9) (10) (11) Results With the data described above, we perform bi-directional (source to target and target to source) word alignment based on IBM model 4, and obtain two alignment results on the test set. Based on these two results, we get the ""ref"
2007.mtsummit-papers.52,P02-1040,0,0.0757065,"by LDC (catalog number LDC2005T14). Translation Results We conduct phrase-based statistical machine translation (SMT) from English to Chinese. To perform phrase-based SMT, we need a trainer and a decoder. For training, we use Koehn's training scripts 10 . For the decoder, we use Pharaoh (Koehn, 2004). We run the decoder with its default settings (maximum phrase length 7) and then use Koehn's implementation of minimum error rate training (Och, 2003) to tune the feature weights on the development set. The translation quality was evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). The translation results are shown in Table 5. In the translation experiments, our method combines all of the clues to get the alignment results. Based on the alignment results, we extract the phrase pairs used by the Pharaoh decoder. (a) Without Chunks Baseline Our Method Figure 2. Alignment Examples w/o Chunks Example (2) shows a sentence pair using the chunk clue. (2) the multi-field, multi-level, and multi-channel cooperation 领域 、多 层次 、多 渠道 MT Test Set 0.1426 0.1690 Table 5. English to Chinese Translation Results (b) With Chunks 多 WA Test Set 0.1137 0.1346 From the results, it can be seen"
2007.mtsummit-papers.52,popovic-ney-2004-towards,0,0.0213012,"Missing"
2007.mtsummit-papers.52,2005.eamt-1.29,0,0.0334651,"Missing"
2007.mtsummit-papers.52,J96-1001,0,0.040066,"Missing"
2007.mtsummit-papers.52,C02-1012,0,0.0800603,"Missing"
2007.mtsummit-papers.52,H05-1010,0,0.146777,", 1998; Tiedemann, 1999). Wu and Wang (2004) used a rule-based translation system to identify and disambiguate the multi-word units and improved the multi-word alignment results. Tiedemann (2003) used chunks and n-grams. The third issue is how to make use of more linguistic information. The basic statistical word alignment method works on the word level of the plain text. In recent years, some discriminative methods are proposed to integrate various syntactic and lexical clues into the alignment models to improve alignment quality (Liu et al., 2005; Moore et al., 2006; Blunsom and Cohn, 2006; Taskar et al., 2005). In these methods, part-of-speech (POS), association measure between bilingual words, and translation dictionaries are usually used. More linguistic information, such as named entity and chunk information, may be useful for word alignment. In this paper, we propose an unified method to address all of the three problems mentioned above. The method first trains a weight for each null alignment to improve word alignment. Secondly, we use dictionaries, including human crafted translation dictionaries and automatically trained dictionaries, to improve both precision and recall of word alignment. F"
2007.mtsummit-papers.52,E03-1026,0,0.0397214,"Missing"
2007.mtsummit-papers.52,tufis-barbu-2002-lexical,0,0.0382186,"Missing"
2007.mtsummit-papers.52,J97-3002,0,0.473791,"Missing"
2007.mtsummit-papers.52,C04-1005,1,0.858069,"Missing"
2007.mtsummit-papers.52,P05-1059,0,0.0385058,"Missing"
2007.mtsummit-papers.67,P06-1002,0,0.0116763,"showed in their experiments that the heuristic methods outperform the generative models. Their analysis indicates that the performance gap stems primarily from the segmentation variable of the generative model, which increases the possibility of overfitting during training. Recently, several researches have been conducted to explore the relationship of word alignment quality measures and machine translation quality. The main points are concluded as follows. 1. It is difficult to find a direct correlation between word alignment measures (such as alignment error rate) and automated MT metrics (Ayan and Dorr, 2006; Fraser and Marcu, 2006). 2. Large gains in alignment performance under any metric are confirmed to achieve relatively small gains in translation performance (Lopez and Resnik, 2006). 3. Better feature mining can lead to substantial gain in translation quality (Lopez and Resnik, 2006). 4. It is better to generate alignments adapted to the characteristics of the translation models that will make use of this alignment information (Vilar et al., 2006). However, all of the above conclusions are made on the indomain test sets and never on the out-of-domain test sets. In addition, although Lopez an"
2007.mtsummit-papers.67,N04-4006,0,0.0153766,"d alignment information can be set as the union of the n alignments involved. n As compared with the maximum value, we can get a relative value as described in (7). We only keep those phrase pairs whose relative values are larger than a threshold. 2 G 2 ( f , e) Max( f ) (7) Model Combination Model Interpolation To combine the different phrase tables, we use linear interpolation method in this paper. For the phrase 2 n Another way to combine the phrase pairs extracted by different methods is to use the count merging method, which is widely used in language modeling (Bacchiani and Roark, 2003; Bacchiani et al., 2004). The main idea of count merging is to assign weights to the occurring count of phrase pairs, and then merging them to build translation models. The method to estimate the translation probability is shown in equation (10). e Ratio( f , e) = n ∑ α i = 1 and ∑ β i = 1 . Count Merging According to the contingency table, the log likelihood ratio for each phrase pair is defined as 2 (9) i =1 p ( f |e) = Table 1. Contingency Table for Phrase Pairs nij N (8) i =1 This threshold is determined on a development set. w( f |e) = ∑ β i counti ( f , e) i =1 n ∑ β i ∑ counti ( f , e' ) i =1 (11) e' Where cou"
2007.mtsummit-papers.67,P05-1033,0,0.0530211,"oise filtering and combination of these heuristic methods achieve larger improvement on the out-of-domain test set than on the in-domain test set. Introduction Word or phrase alignment plays a crucial role in statistical machine translation (SMT). During training, the SMT systems produce alignment between words or phrases of existing examples to estimate the statistical parameters. With these estimated parameters, the SMT systems translate source sentences into target sentences. Current state-of-the-art models in machine translation are based on alignments between phrases (Koehn et al., 2003; Chiang, 2005). Phrase-based generative models are first proposed by Marcu and Wong (2002) to extract phrase pairs. Zhao and Waibel (2005) also proposed several generative models to generate phrase pairs for machine translation. An alternative is to first generate word alignments. Phrase alignments are then inferred heuristically from these word alignments (Och et al., 1999; Koehn et al., 2003). DeNero et al. (2006) showed in their experiments that the heuristic methods outperform the generative models. Their analysis indicates that the performance gap stems primarily from the segmentation variable of the g"
2007.mtsummit-papers.67,W06-3105,0,0.0350977,"Missing"
2007.mtsummit-papers.67,J93-1003,0,0.0580581,"xical weight are important features in the phrase translation table. Lopez and Resnik (2006) found that alignment quality has little impact on the lexical weighting feature, which itself provides only a modest improvement in translation quality. Thus, we only filter the phrase pairs using phrase translation statistics. Although the phrase translation probability described in equation (2) can be used to filter the phrase table, translation probability usually overestimates the infrequently occurring pairs. In order to solve this problem, we use association measures to filter some phrase pairs. Dunning (1993) proved that log likelihood ratio performed very well on infrequently occurring data. Thus, we calculate the log likelihood ratio for each phrase pair. First we construct a contingency table as shown in Table 1. Source phrase ~Source phrase Totals Target phrase n11 n21 C1 ~Target phrase n12 n22 C2 Totals R1 R2 N n p w ( f |e, a ) = ∑ β i p w,i ( f |e, a ) Where pi ( f |e) and p w,i ( f |e, a ) ( i = 1,..., n ) are the phrase translation probability and lexical weight estimated by n different methods. α i and β i are interpolation coefficients, ensuring G ( f , e) = −2 log λ = ∑ nij log i, j (5"
2007.mtsummit-papers.67,koen-2004-pharaoh,0,0.232319,"diagonally neighboring alignment points are also included. grow-diag-final: in addition to the alignment points in grow-diag, the non-neighboring alignment points between words, of which at least one is currently unaligned, are added in a final step. grow-final: In addition to the alignment points in grow, the non-neighboring alignment points between words, of which at least one is currently unaligned, are added in a final step. m =1 Where hm (e, f) represents feature functions, and λm is the weight assigned to the corresponding feature function. In this paper, we will use the Pharaoh system (Koehn, 2004). Eight different features are used in this system. 1. a phrase translation probability 2. an inverse phrase translation probability 3. a lexical weight: measuring the quality of word alignment inside the phrase pair 4. an inverse lexical weight 5. language model 6. phrase penalty 7. word penalty 8. reordering For phrase translation probability, lexical weight, and reordering, we use the same models in (Koehn et al., 2003). We use n-grams for language modelling. For the phrase penalty and word penalty, we use the same heuristics in (Zen and Ney, 2004). Phrase Extraction With the word alignment"
2007.mtsummit-papers.67,2005.mtsummit-papers.11,0,0.202781,"he impacts of these heuristics on machine translation quality. And then we will re-evaluate the relationship of word alignment and their impacts on machine translation quality on both indomain and out-of-domain test sets. Furthermore, in order to examine the noise in the phrase pairs extracted using different alignment heuristics, we propose a method to filter the noise in the phrase tables using association measures. And we will also investigate whether combining the phrase tables extracted by different heuristics improves translation quality. We performed experiments on the Europarl corpus (Koehn, 2005; Koehn and Monz, 2006), where a multilingual in-domain training corpus, an in-domain test set, and an out-of-domain test set are available. We obtained the following results: 1. Word alignment results show that the compromise method, which makes compromise between precision and recall, performs the best on both in-domain and out-of-domain alignment test sets. 2. Translation results indicate that the heuristic methods perform differently on the in-domain and out-of-domain test sets. On the in-domain test set, the recall-oriented heuristic methods yield better translation quality. On the out-of"
2007.mtsummit-papers.67,W06-3114,0,0.459203,"these heuristics on machine translation quality. And then we will re-evaluate the relationship of word alignment and their impacts on machine translation quality on both indomain and out-of-domain test sets. Furthermore, in order to examine the noise in the phrase pairs extracted using different alignment heuristics, we propose a method to filter the noise in the phrase tables using association measures. And we will also investigate whether combining the phrase tables extracted by different heuristics improves translation quality. We performed experiments on the Europarl corpus (Koehn, 2005; Koehn and Monz, 2006), where a multilingual in-domain training corpus, an in-domain test set, and an out-of-domain test set are available. We obtained the following results: 1. Word alignment results show that the compromise method, which makes compromise between precision and recall, performs the best on both in-domain and out-of-domain alignment test sets. 2. Translation results indicate that the heuristic methods perform differently on the in-domain and out-of-domain test sets. On the in-domain test set, the recall-oriented heuristic methods yield better translation quality. On the out-of-domain test set, the p"
2007.mtsummit-papers.67,N03-1017,0,0.410847,"ation systems; (3) noise filtering and combination of these heuristic methods achieve larger improvement on the out-of-domain test set than on the in-domain test set. Introduction Word or phrase alignment plays a crucial role in statistical machine translation (SMT). During training, the SMT systems produce alignment between words or phrases of existing examples to estimate the statistical parameters. With these estimated parameters, the SMT systems translate source sentences into target sentences. Current state-of-the-art models in machine translation are based on alignments between phrases (Koehn et al., 2003; Chiang, 2005). Phrase-based generative models are first proposed by Marcu and Wong (2002) to extract phrase pairs. Zhao and Waibel (2005) also proposed several generative models to generate phrase pairs for machine translation. An alternative is to first generate word alignments. Phrase alignments are then inferred heuristically from these word alignments (Och et al., 1999; Koehn et al., 2003). DeNero et al. (2006) showed in their experiments that the heuristic methods outperform the generative models. Their analysis indicates that the performance gap stems primarily from the segmentation va"
2007.mtsummit-papers.67,2006.amta-papers.11,0,0.0240714,"ariable of the generative model, which increases the possibility of overfitting during training. Recently, several researches have been conducted to explore the relationship of word alignment quality measures and machine translation quality. The main points are concluded as follows. 1. It is difficult to find a direct correlation between word alignment measures (such as alignment error rate) and automated MT metrics (Ayan and Dorr, 2006; Fraser and Marcu, 2006). 2. Large gains in alignment performance under any metric are confirmed to achieve relatively small gains in translation performance (Lopez and Resnik, 2006). 3. Better feature mining can lead to substantial gain in translation quality (Lopez and Resnik, 2006). 4. It is better to generate alignments adapted to the characteristics of the translation models that will make use of this alignment information (Vilar et al., 2006). However, all of the above conclusions are made on the indomain test sets and never on the out-of-domain test sets. In addition, although Lopez and Resnik (2006) pointed out that it may be more useful to handle noise in phrase extraction than to improve word alignment quality, they did not provide detailed information to verify"
2007.mtsummit-papers.67,W02-1018,0,0.0223699,"rger improvement on the out-of-domain test set than on the in-domain test set. Introduction Word or phrase alignment plays a crucial role in statistical machine translation (SMT). During training, the SMT systems produce alignment between words or phrases of existing examples to estimate the statistical parameters. With these estimated parameters, the SMT systems translate source sentences into target sentences. Current state-of-the-art models in machine translation are based on alignments between phrases (Koehn et al., 2003; Chiang, 2005). Phrase-based generative models are first proposed by Marcu and Wong (2002) to extract phrase pairs. Zhao and Waibel (2005) also proposed several generative models to generate phrase pairs for machine translation. An alternative is to first generate word alignments. Phrase alignments are then inferred heuristically from these word alignments (Och et al., 1999; Koehn et al., 2003). DeNero et al. (2006) showed in their experiments that the heuristic methods outperform the generative models. Their analysis indicates that the performance gap stems primarily from the segmentation variable of the generative model, which increases the possibility of overfitting during train"
2007.mtsummit-papers.67,W99-0604,0,0.0209715,"e statistical parameters. With these estimated parameters, the SMT systems translate source sentences into target sentences. Current state-of-the-art models in machine translation are based on alignments between phrases (Koehn et al., 2003; Chiang, 2005). Phrase-based generative models are first proposed by Marcu and Wong (2002) to extract phrase pairs. Zhao and Waibel (2005) also proposed several generative models to generate phrase pairs for machine translation. An alternative is to first generate word alignments. Phrase alignments are then inferred heuristically from these word alignments (Och et al., 1999; Koehn et al., 2003). DeNero et al. (2006) showed in their experiments that the heuristic methods outperform the generative models. Their analysis indicates that the performance gap stems primarily from the segmentation variable of the generative model, which increases the possibility of overfitting during training. Recently, several researches have been conducted to explore the relationship of word alignment quality measures and machine translation quality. The main points are concluded as follows. 1. It is difficult to find a direct correlation between word alignment measures (such as align"
2007.mtsummit-papers.67,P02-1040,0,0.0919634,"l. We use the same word alignment evaluation metrics as described in (Och and Ney, 2003). If we use A to indicate the alignments identified by the proposed methods, and S and P to denote the sure and possible links in the reference alignments, the precision, recall, and alignment error rate (AER) are calculated as described in Equations (12), (13) and (14). If we take all links as sure links, then |P |= |S |. |A ∩S| |S| |A∩P| recall = |P| |A ∩S |+ |A ∩ P | AER = 1 − |A |+|S| precision = (12) (13) (14) The translation quality was evaluated using a wellestablished automatic measure: BLEU score (Papineni et al., 2002). And we also use the tool provided in the NAACL/HLT 2006 shared task on SMT to calculate the BLEU scores. We use the same method described in (Koehn and Monz, 2006) to perform the significance test. Word Alignment Results We perform bi-directional (source to target and target to source) word alignments using the GIZA++ toolkit, and obtain the symmetrized alignment results using the six word alignment heuristics described in this paper. The alignment results are shown in Table 4 for the in-domain and out-of-domain test sets. On both of the test sets, the compromise method ""growdiag"" obtains th"
2007.mtsummit-papers.67,2006.iwslt-papers.7,0,0.0819329,"follows. 1. It is difficult to find a direct correlation between word alignment measures (such as alignment error rate) and automated MT metrics (Ayan and Dorr, 2006; Fraser and Marcu, 2006). 2. Large gains in alignment performance under any metric are confirmed to achieve relatively small gains in translation performance (Lopez and Resnik, 2006). 3. Better feature mining can lead to substantial gain in translation quality (Lopez and Resnik, 2006). 4. It is better to generate alignments adapted to the characteristics of the translation models that will make use of this alignment information (Vilar et al., 2006). However, all of the above conclusions are made on the indomain test sets and never on the out-of-domain test sets. In addition, although Lopez and Resnik (2006) pointed out that it may be more useful to handle noise in phrase extraction than to improve word alignment quality, they did not provide detailed information to verify this point. In this paper, we will use different heuristics to generate word alignments, and examine the impacts of these heuristics on machine translation quality. And then we will re-evaluate the relationship of word alignment and their impacts on machine translation"
2007.mtsummit-papers.67,N04-1033,0,0.0359614,"Missing"
2007.mtsummit-papers.67,I05-3011,0,0.0217575,"than on the in-domain test set. Introduction Word or phrase alignment plays a crucial role in statistical machine translation (SMT). During training, the SMT systems produce alignment between words or phrases of existing examples to estimate the statistical parameters. With these estimated parameters, the SMT systems translate source sentences into target sentences. Current state-of-the-art models in machine translation are based on alignments between phrases (Koehn et al., 2003; Chiang, 2005). Phrase-based generative models are first proposed by Marcu and Wong (2002) to extract phrase pairs. Zhao and Waibel (2005) also proposed several generative models to generate phrase pairs for machine translation. An alternative is to first generate word alignments. Phrase alignments are then inferred heuristically from these word alignments (Och et al., 1999; Koehn et al., 2003). DeNero et al. (2006) showed in their experiments that the heuristic methods outperform the generative models. Their analysis indicates that the performance gap stems primarily from the segmentation variable of the generative model, which increases the possibility of overfitting during training. Recently, several researches have been cond"
2008.iwslt-evaluation.18,P07-2045,0,0.0140203,"ranslation. These modules include Chinese word segmentation, word alignment, named entity (NE) translation and language model. The remainder of this paper is organized as follows. Section 2 describes the core algorithms of our systems for the 5 tasks. Section 3 focuses on the specific methods adapted to spoken language translation. Sections 4 to 7 provide the details of our experiments for each task. Section 8 presents the evaluation results of our primary submissions. Section 9 concludes our work for IWSLT 2008. 2. System description 2.1. SMT system We used the phrase-based SMT system: Moses [1]. In Moses, phrase translation probabilities, reordering probabilities, and language model probabilities are combined in the log-linear model to obtain the best translation e best of the source f: e best = arg max e p(e |f ) sentence ≈ arg max e M ∑ λm hm (e, f) (1) m =1 The models or features which are employed by the decoder consist of (a) one or several phrases tables, (b) one or more language models trained with SRILM toolkit [2], (c) distancebased and lexicalized reordering models, (d) word penalty and (e) phrase penalty. The weights are set by a discriminative training method on a held-o"
2008.iwslt-evaluation.18,P03-1021,0,0.0108553,"phrase translation probabilities, reordering probabilities, and language model probabilities are combined in the log-linear model to obtain the best translation e best of the source f: e best = arg max e p(e |f ) sentence ≈ arg max e M ∑ λm hm (e, f) (1) m =1 The models or features which are employed by the decoder consist of (a) one or several phrases tables, (b) one or more language models trained with SRILM toolkit [2], (c) distancebased and lexicalized reordering models, (d) word penalty and (e) phrase penalty. The weights are set by a discriminative training method on a held-out data set [3].  Chinese-English-Spanish: PIVOT_CES (RS and CRR) • BTEC tasks  Chinese-English: BTEC_CE (RS and CRR)  Chinese-Spanish: BTEC_CS (RS and CRR) SS, RS and CRR represent different input conditions, namely spontaneous speech, read speech and correct recognition result, respectively. For different translation directions, we used different translation strategies. For Chinese-English and EnglishChinese translation, we used hybrid systems that combine rule-based machine translation (RBMT) method and statistical machine translation (SMT) method. For ChineseSpanish translation, phrase-based SMT model"
2008.iwslt-evaluation.18,D07-1030,1,0.841151,"mbination of RBMT and SMT We used two MT systems with different translation strategies for Chinese-English and English-Chinese translation. One is a RBMT software - Dr. eye 1 . The other is a phrase-based SMT system - Moses. Firstly we ran the RBMT system as a black box to translate the source texts into the target language. The translations and original source text were used as a synthetic bilingual corpus to train an SMT system. Using the bilingual corpus available for an evaluation task, we built another SMT model. Then these two translation models were combined together as a hybrid system [4]. 1 Available download.shtml - 124 - at http://www.dreye.com.cn/prod/cp-pcProceedings of IWSLT 2008, Hawaii - U.S.A. In our experiments using the development data for evaluation, we used RBMT system to translate the development data to build the synthetic bilingual corpus. In primary runs at IWSLT 2008, we built the synthetic bilingual corpus by translating the test data using RBMT system. For the pivot task, we also used RBMT system on some training sets, which will be described in Section 7. 2.3. Pivot-based SMT system For the pivot task Chinese-English-Spanish translation, we built a pivot"
2008.iwslt-evaluation.18,P07-1108,1,0.842415,"- at http://www.dreye.com.cn/prod/cp-pcProceedings of IWSLT 2008, Hawaii - U.S.A. In our experiments using the development data for evaluation, we used RBMT system to translate the development data to build the synthetic bilingual corpus. In primary runs at IWSLT 2008, we built the synthetic bilingual corpus by translating the test data using RBMT system. For the pivot task, we also used RBMT system on some training sets, which will be described in Section 7. 2.3. Pivot-based SMT system For the pivot task Chinese-English-Spanish translation, we built a pivot translation model as described in [5]. Firstly we trained two translation models on the Chinese-English corpus and English-Spanish corpus, and then built a pivot translation model for Chinese-Spanish translation using English as a pivot language. To use a phrase-based translation system such as Moses, we need to obtain a phrase-table for the ChineseSpanish translation, where two important features are needed: phrase translation probability and lexical weight. 2.3.1. Phrase translation probability the alignment information a inside ( f , e) can be obtained as shown in (4). a = {( f , e) |∃p : ( f , p) ∈ a1 & ( p, e) ∈ a 2 } We est"
2008.iwslt-evaluation.18,N03-1017,0,0.00659339,"slation dictionary extraction, word alignment, NE translation, language model, punctuation restoration, case restoration, and translation selection. Currently, most of Chinese word segmentation systems are not designed for spoken language translation. Thus, we investigated the effect of segmentation granularity and segmentation dictionary for better MT performance on spoken language. Lexical weight 3.1.1. Given a phrase pair ( f , e) and a word alignment a between the source word positions i = 1,..., n and the target word positions j = 1,..., m , the lexical weight can be estimated as follows [6]. p w ( f |e, a ) i =1 1 j |(i, j ) ∈ a ∑ w( f i |e j ) Word segmentation dictionary A Chinese dictionary is a fundamental element for word segmentation. In our segmenter, we used three kinds of dictionaries: basic dictionary, NE dictionary and in-domain dictionary. • The basic dictionary contains some commonly-used words. (3) • The NE dictionary consists of transliterated person names, Japanese first names and last names 3 , and location names. They were extracted from the Chinese-English Name Entity Lists Version 1.0 (LDC2005T34). ∀(i , j )∈a In order to estimate the lexical weight, we first"
2008.iwslt-evaluation.18,W08-0336,0,0.0137922,"in bilingual dictionaries, we added them into the final alignment set. • For the links conflicting with the links in the final alignment set, we simply deleted them. • For the remained links, we kept them in the bidirectional results. 5 For the tasks using Spanish as the target language, we only used in-domain dictionaries extracted from training data. EC Figure 1. An example of improving word alignment results Word granularity There are lots of arguments about the definition of a Chinese word. In fact, only a few researchers investigated the effect of word granularity on machine translation [8]. In this work, we followed the guidelines shown below to define what is a word. • Its translation is a word in the target language. 3.1.4. 我们 想 要 张 靠 窗 的 桌子 。 CE 6 Available at http://www.fjoch.com/GIZA++.html - 126 - Proceedings of IWSLT 2008, Hawaii - U.S.A. Training with NE categories Decoding with NE categories 玛丽 和 亨利 不 一样 大 。 mary is not so old as henry . ---------------------------------------<nr&gt;#0 和 <nr&gt;#1 不 一样 大 。 <nr&gt;#0 is not so old as <nr&gt;#1 . 苏珊 和 比尔 不 一样 大 。 ---------------------------------------<nr&gt;#0 和 <nr&gt;#1 不 一样 大 。 <nr&gt;#0 is not so old as <nr&gt;#1 . ------------------------"
2008.iwslt-evaluation.18,W07-0718,0,0.0369648,"Missing"
2008.iwslt-evaluation.18,2006.iwslt-evaluation.7,0,\N,Missing
2015.mtsummit-papers.23,J93-2003,0,0.0470688,"x, we note that the number of possible translation candidates Tx is much smaller compared to the target vocabulary size Ky . Instead of calculating the Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 303 Figure 3: Deep Output Layer of RNN Decoder translation probability for every word in the target vocabulary, we only calculate the probability for a limited set of target candidates which are relevant to the given x. The problem is how to get such a candidate set. Intuitively, we can make use of the word alignment information generated with IBM model (Brown et al., 1993). And we found that there are often too many candidates especially for the common words, which makes the improvement limited. Actually, we can get more accurate translation candidates from the phrase pairs of the phrase-based translation model (Koehn et al., 2003). Firstly, we train a phrase-based translation model with the word aligned bilingual sentence pairs. Then we segment the input sentence x into a number of continuous source phrases. Next, we search the corresponding target phrases for all source phrases from the phrase-based translation model, namely phrase table. After that, we build"
2015.mtsummit-papers.23,W14-4012,0,0.0860863,"Missing"
2015.mtsummit-papers.23,P14-1129,0,0.0728845,"Missing"
2015.mtsummit-papers.23,P07-1019,0,0.173351,"Missing"
2015.mtsummit-papers.23,koen-2004-pharaoh,0,0.21634,"existing system, Cho et al. (2014) proposed a whole new RNN Encoder-Decoder approach which generates the target translation with a neural network directly. Bahdanau et al. (2014) extended the above approach by allowing an RNN model to automatically (soft-)search for parts of a source sentence to predict a target word, and achieved a translation performance comparable to the existing state-of-the-art phrase-based systems (Koehn et al., 2003). Although NMT shows high potential, its decoding efficiency is still challenging. Cho et al. (2014) implemented a standard beam search decoding algorithm (Koehn, 2004) for an RNN Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 297 Encoder-Decoder system published as GroundHog 1 . The beam search algorithm successfully reduces the search space from exponential size to polynomial size, and it is able to translate about ten words per second. Even though the speed is acceptable for most research tasks, it is not yet efficient enough to meet the requirement of commercial systems for providing real-time translation service. Huang and Chiang (2007) proposed the forest rescoring algorithm which succeed to accelerate the dec"
2015.mtsummit-papers.23,P07-2045,0,0.0122377,"of corpus for NIST08 task, containing 67M Chinese words and 74M English words, to train our models. No monolingual corpus are used to help the model training. The NIST MT03 Chineseto-English test set is used as the development set, and NIST MT08 Chinese-to-English test set is used to evaluate our translation results. The development set is used to choose the best RNN Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 304 model in history, because the performance of RNN model will fluctuate during training. 4.2 Toolkits The open source SMT system Moses 2 (Koehn et al., 2007) is used to train a phrase-based machine translation system. The phrase table trained with Moses will be used to constrain the softmax translation. We use the open source RNN Encoder-Decoder toolkits GroundHog which is implemented with Theano (Bergstra et al., 2010; Bastien et al., 2012) to train a neural machine translation model. It is re-implemented with C++ in the GPU Environment and named NetTrans, which is well optimized and faster than GroundHog which is implemented in Python. Given the same model, there is no translation differences between GroundHog and NetTrans. We will report our ex"
2015.mtsummit-papers.23,N03-1017,0,0.112704,"models into the traditional translation decoders as additional features in a log-linear framework (Och and Ney, 2002). Rather than applying a neural network as a part of the existing system, Cho et al. (2014) proposed a whole new RNN Encoder-Decoder approach which generates the target translation with a neural network directly. Bahdanau et al. (2014) extended the above approach by allowing an RNN model to automatically (soft-)search for parts of a source sentence to predict a target word, and achieved a translation performance comparable to the existing state-of-the-art phrase-based systems (Koehn et al., 2003). Although NMT shows high potential, its decoding efficiency is still challenging. Cho et al. (2014) implemented a standard beam search decoding algorithm (Koehn, 2004) for an RNN Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 297 Encoder-Decoder system published as GroundHog 1 . The beam search algorithm successfully reduces the search space from exponential size to polynomial size, and it is able to translate about ten words per second. Even though the speed is acceptable for most research tasks, it is not yet efficient enough to meet the requiremen"
2015.mtsummit-papers.23,P02-1038,0,0.42432,"Missing"
2015.mtsummit-papers.23,D12-1089,0,0.0168434,"s. Finally, we constrain the softmax operation with the candidate word set for all the hypothesis extension of x. One disadvantage of above approach is that it is too costly to load the whole phrase table into memory. Fortunately, we can reduce the memory cost by pruning phrase table carefully. The phrase table filtering techniques have been widely used in machine translation. Instead of considering both the phrase coverage and translation features in the filtering technique, our system only considers the word coverage, which is much easier to implement. We investigated the histogram pruning (Zens et al., 2012) and length-based pruning method to reduce the memory used in our system. For histogram pruning, we preserves the X target phrases with highest probability for each source phrase. For length-based pruning, we prune the phrase pairs which contain more than X words in source or target side. Where X is the threshold. We found that even the basic length-based pruning method works well enough in our task. 4 Experimental Settings In this section, we evaluate the improved beam search algorithm on the task of Chinese-toEnglish translation. 4.1 Dataset In this paper, we use Chinese-to-English bilingual"
2020.acl-main.166,P19-1535,1,0.762994,"2016b; Zhang et al., 2018b), previous works decouple policy learning from response generation, and then use utterance-level latent variables (Zhao et al., 2019) or keywords (Yao et al., 2018) as RL actions to guide response generation. In this work, we investigate how to use prior dialog-transition information to facilitate dialog policy learning. Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Dinan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Zhou et al., 2018; Liu et al., 2019; Bao et al., 2019; Xu et al., 2020). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn policy learning, instead of dialog informativeness improvement. Specifically, we are motivated by (Xu et al., 2020). The method in (Xu et al., 2020) has the issue of cross-domain transfer since it relies on labor-intensive knowledge graph grounded multiturn dialog datasets for model training. Compared with them, our conversational graph is automatically built from dialog datasets, which introduces very low cost for train"
2020.acl-main.166,D18-1256,0,0.0169801,", 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et al., 2018; Yao et al., 2018; Zhao et al., 2019). However, these utterance-level methods tend to produce less coherent multi-turn dialogs since it is quite challenging to learn semantic transitions in a dialog flow merely from dialog data without the help of prior information. In this paper, we propose to represent prior information about dialog transition (between a message and its response) as a graph, and optimize dialog policy based on the graph, to foster a more coherent dialog. To this end, we propose a novel conversational graph (CG) grounded policy learning frame1835 Proceedings of the 58th Annu"
2020.acl-main.166,D17-1259,0,0.0192369,"onse “It’s so ...” with “sleepy” and r¯ as input. Notice all the howvertices are from the same set rather than completely independent of each other. How to effectively learn dialog strategies is an enduring challenge for open-domain multi-turn conversation generation. To address this challenge, previous works investigate word-level policy models that simultaneously learn dialog policy and language generation from dialog corpora (Li et al., 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et al., 2018; Yao et al., 2018; Zhao et al., 2019). However, these utterance-level methods tend to produce less coherent multi-turn dialogs since it is quite challenging to l"
2020.acl-main.166,N16-1014,0,0.66715,"t response with two sub-steps: firstly, obtains a response representation r¯ using both M3 and a message representation (from a message-encoder); Next, produces a response “It’s so ...” with “sleepy” and r¯ as input. Notice all the howvertices are from the same set rather than completely independent of each other. How to effectively learn dialog strategies is an enduring challenge for open-domain multi-turn conversation generation. To address this challenge, previous works investigate word-level policy models that simultaneously learn dialog policy and language generation from dialog corpora (Li et al., 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et"
2020.acl-main.166,D16-1127,0,0.489962,"t response with two sub-steps: firstly, obtains a response representation r¯ using both M3 and a message representation (from a message-encoder); Next, produces a response “It’s so ...” with “sleepy” and r¯ as input. Notice all the howvertices are from the same set rather than completely independent of each other. How to effectively learn dialog strategies is an enduring challenge for open-domain multi-turn conversation generation. To address this challenge, previous works investigate word-level policy models that simultaneously learn dialog policy and language generation from dialog corpora (Li et al., 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et"
2020.acl-main.166,D16-1230,0,0.114845,"Missing"
2020.acl-main.166,D19-1187,1,0.845143,"models (Li et al., 2016b; Zhang et al., 2018b), previous works decouple policy learning from response generation, and then use utterance-level latent variables (Zhao et al., 2019) or keywords (Yao et al., 2018) as RL actions to guide response generation. In this work, we investigate how to use prior dialog-transition information to facilitate dialog policy learning. Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Dinan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Zhou et al., 2018; Liu et al., 2019; Bao et al., 2019; Xu et al., 2020). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn policy learning, instead of dialog informativeness improvement. Specifically, we are motivated by (Xu et al., 2020). The method in (Xu et al., 2020) has the issue of cross-domain transfer since it relies on labor-intensive knowledge graph grounded multiturn dialog datasets for model training. Compared with them, our conversational graph is automatically built from dialog datasets, which introduces very"
2020.acl-main.166,W15-4640,0,0.0973246,"Missing"
2020.acl-main.166,N19-1123,0,0.0334427,"Missing"
2020.acl-main.166,D18-1255,0,0.0342656,"egeneration issue of word-level policy models (Li et al., 2016b; Zhang et al., 2018b), previous works decouple policy learning from response generation, and then use utterance-level latent variables (Zhao et al., 2019) or keywords (Yao et al., 2018) as RL actions to guide response generation. In this work, we investigate how to use prior dialog-transition information to facilitate dialog policy learning. Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Dinan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Zhou et al., 2018; Liu et al., 2019; Bao et al., 2019; Xu et al., 2020). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn policy learning, instead of dialog informativeness improvement. Specifically, we are motivated by (Xu et al., 2020). The method in (Xu et al., 2020) has the issue of cross-domain transfer since it relies on labor-intensive knowledge graph grounded multiturn dialog datasets for model training. Compared with them, our conversational graph is automatically built from di"
2020.acl-main.166,C16-1316,0,0.253008,"rocedure, they randomly select a mechanism for response generation. As shown in Figure 3, the generator consists of a RNN based message encoder, a set of responding mechanisms, and a decoder. First, given a dialog message, the message-encoder represents it as a vector x. Second, the generator uses a responding mechanism (selected by policy) to convert x into a response representation r¯. Finally, r¯ and a keyword (selected by policy) are fed into the decoder for response generation. To ensure that the given keyword will appear in generated responses, we introduce another Seq2BF based decoder (Mou et al., 2016) to replace the original RNN decoder. Moreover, this generator is trained on a dataset with pairs of [the message, a keyword extracted from a response]-the response.3 3.2 CG Construction Given a dialog corpus D, we construct the CG with three steps: what-vertex construction, how-vertex construction, and edge construction. 3 If multiple keywords are extracted from the response, we randomly choose one; and if no keyword exists in the response, we randomly sample a word from the response to serve as “keyword”. 1837 What-vertex construction To extract content words from D as what-vertices, we use"
2020.acl-main.166,D14-1162,0,0.0831011,"Missing"
2020.acl-main.166,P15-1152,0,0.0691534,"t by grid search. The weights of the third/sixth factors are set as 0 by default because they are proposed for target-guided conversation. 1839 rameters, and the parameters of other modules stay intact during RL training. 3.8 NLG As described in Section 3.1, we use the mechanism selected by how-policy to convert x into a response representation r¯. Then we feed the keyword in the selected what-vertex and r¯ into a Seq2BF decoder (Mou et al., 2016) for response generation. Experiments and Results9 4 4.1 Datasets We conduct experiments on two widely used opendomain dialog corpora. Weibo corpus (Shang et al., 2015). This is a large micro-blogging corpora. After data cleaning, we obtain 2.6 million pairs for training, 10k pairs for validation and 10k pairs for testing. We use publicly-available lexical analysis tools10 to obtain POS tag features for this dataset and then we further use this feature to extract keywords from utterances. We use Tencent AI Lab Embedding11 for embedding initialization in models. Persona dialog corpus (Zhang et al., 2018a). This ia a crowd-sourced dialog corpora where each participant plays the part of an assigned persona. To evaluate policy controllability brought by CGPolicy"
2020.acl-main.166,P18-1205,0,0.27686,"o sub-steps: firstly, obtains a response representation r¯ using both M3 and a message representation (from a message-encoder); Next, produces a response “It’s so ...” with “sleepy” and r¯ as input. Notice all the howvertices are from the same set rather than completely independent of each other. How to effectively learn dialog strategies is an enduring challenge for open-domain multi-turn conversation generation. To address this challenge, previous works investigate word-level policy models that simultaneously learn dialog policy and language generation from dialog corpora (Li et al., 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et al., 2018; Yao et al."
2020.acl-main.374,W11-0705,0,0.0924445,"ed for sentiment analysis, including sentencelevel sentiment classification (Taboada et al., 2011; Shin et al., 2017; Lei et al., 2018; Barnes et al., 2019), aspect-level sentiment classification (Vo and Zhang, 2015), opinion extraction (Li and Lam, 2017), emotion analysis (Gui et al., 2017; Fan et al., 2019) and so on. Lexicon-based method (Turney, 2002; Taboada et al., 2011) directly utilizes polarity of sentiment words for classification. Traditional feature-based approaches encode sentiment word information in manually-designed features to improve the supervised models (Pang et al., 2008; Agarwal et al., 2011). In contrast, deep learning approaches enhance the embedding representation with the help of sentiment words (Shin et al., 2017), or absorb the sentiment knowledge through linguistic regularization (Qian et al., 2017; Fan et al., 2019). Aspect-sentiment pair knowledge is also useful for aspect-level classification and opinion extraction. Previous works often provide weak supervision by this type of knowledge, either for aspect7 Conclusion In this paper, we propose Sentiment Knowledge Enhanced Pre-training for sentiment analysis. Sentiment masking and three sentiment pre-training objectives ar"
2020.acl-main.374,W19-6119,0,0.0200401,"t al., 2019) uses WordNet super-senses to improve word-in-context tasks. A different ERNIE (Zhang et al., 2019) exploits entity knowledge for entity-linking and relation classification. 6 Related Work Sentiment Analysis with Knowledge Various types of sentiment knowledge, including sentiment words, word polarity, aspect-sentiment pairs, have been proved to be useful for a wide range of sentiment analysis tasks. Sentiment words with their polarity are widely used for sentiment analysis, including sentencelevel sentiment classification (Taboada et al., 2011; Shin et al., 2017; Lei et al., 2018; Barnes et al., 2019), aspect-level sentiment classification (Vo and Zhang, 2015), opinion extraction (Li and Lam, 2017), emotion analysis (Gui et al., 2017; Fan et al., 2019) and so on. Lexicon-based method (Turney, 2002; Taboada et al., 2011) directly utilizes polarity of sentiment words for classification. Traditional feature-based approaches encode sentiment word information in manually-designed features to improve the supervised models (Pang et al., 2008; Agarwal et al., 2011). In contrast, deep learning approaches enhance the embedding representation with the help of sentiment words (Shin et al., 2017), or a"
2020.acl-main.374,N19-1423,0,0.64,"timent analysis refers to the identification of sentiment and opinion contained in the input texts that are often user-generated comments. In practice, sentiment analysis involves a wide range of specific tasks (Liu, 2012), such as sentence-level sentiment classification, aspect-level sentiment classification, opinion extraction and so on. Traditional methods often study these tasks separately and design specific models for each task, based on manuallydesigned features (Liu, 2012) or deep learning (Zhang et al., 2018). Recently, pre-training methods (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019) have shown their powerfulness in learning general semantic representations, and have remarkably improved most natural language processing (NLP) tasks like sentiment analysis. These methods build unsupervised objectives at word-level, such as masking strategy (Devlin et al., 2019), next-word prediction (Radford et al., 2018) or permutation (Yang et al., 2019). Such wordprediction-based objectives have shown great abilities to capture dependency between words and syntactic structures (Jawahar et al., 2019). However, as the sentiment information of a text is seldom explicitly"
2020.acl-main.374,D19-1563,0,0.104905,"from other NLP tasks in that it deals mainly with user reviews other than news texts. There are many specific sentiment tasks, and these tasks usually depend on different types of sentiment knowledge including sentiment words, word polarity and aspect-sentiment pairs. The importance of these knowledge has been verified by tasks at different level, for instance, sentence-level sentiment classification (Taboada et al., 2011; Shin et al., 2017; Lei et al., 2018), aspect-level sentiment classification (Vo and Zhang, 2015; Zeng et al., 2019), opinion extraction (Li and Lam, 2017; Gui et al., 2017; Fan et al., 2019) and so on. Therefore, we assume that, by integrating these knowledge into the pre-training process, the learned representation would be more sentimentspecific and appropriate for sentiment analysis. In order to learn a unified sentiment representation for multiple sentiment analysis tasks, we propose Sentiment Knowledge Enhanced Pre-training (SKEP), where sentiment knowledge about words, polarity, and aspect-sentiment pairs are included to guide the process of pre-training. The sentiment knowledge is first automatically mined from unlabeled data (Section 3.1). With the knowledge 4067 Proceedi"
2020.acl-main.374,D17-1167,0,0.177221,"analysis differs from other NLP tasks in that it deals mainly with user reviews other than news texts. There are many specific sentiment tasks, and these tasks usually depend on different types of sentiment knowledge including sentiment words, word polarity and aspect-sentiment pairs. The importance of these knowledge has been verified by tasks at different level, for instance, sentence-level sentiment classification (Taboada et al., 2011; Shin et al., 2017; Lei et al., 2018), aspect-level sentiment classification (Vo and Zhang, 2015; Zeng et al., 2019), opinion extraction (Li and Lam, 2017; Gui et al., 2017; Fan et al., 2019) and so on. Therefore, we assume that, by integrating these knowledge into the pre-training process, the learned representation would be more sentimentspecific and appropriate for sentiment analysis. In order to learn a unified sentiment representation for multiple sentiment analysis tasks, we propose Sentiment Knowledge Enhanced Pre-training (SKEP), where sentiment knowledge about words, polarity, and aspect-sentiment pairs are included to guide the process of pre-training. The sentiment knowledge is first automatically mined from unlabeled data (Section 3.1). With the know"
2020.acl-main.374,D19-1355,0,0.0379178,"al., 2019), XLNet (Yang et al., 2019) and so on. Among them, BERT pre-trains a bidirectional transformer by randomly masked word prediction, and have shown strong performance gains. RoBERTa (Liu et al., 2019) further improves BERT by robust optimization, and become one of the best pre-training methods. Inspired by BERT, some works propose finegrained objectives beyond random word masking. SpanBERT (Joshi et al., 2019) masks the span of words at the same time. ERNIE (Sun et al., 2019) proposes to mask entity words. On the other hand, pre-training for specific tasks is also studied. GlossBERT (Huang et al., 2019) exploits gloss knowledge to improve word sense disambiguation. SenseBERT (Levine et al., 2019) uses WordNet super-senses to improve word-in-context tasks. A different ERNIE (Zhang et al., 2019) exploits entity knowledge for entity-linking and relation classification. 6 Related Work Sentiment Analysis with Knowledge Various types of sentiment knowledge, including sentiment words, word polarity, aspect-sentiment pairs, have been proved to be useful for a wide range of sentiment analysis tasks. Sentiment words with their polarity are widely used for sentiment analysis, including sentencelevel se"
2020.acl-main.374,2021.ccl-1.108,0,0.22006,"Missing"
2020.acl-main.374,N18-1054,0,0.0617166,"Missing"
2020.acl-main.374,N18-1202,0,0.312695,"/github.com/baidu/Senta. 1 Introduction Sentiment analysis refers to the identification of sentiment and opinion contained in the input texts that are often user-generated comments. In practice, sentiment analysis involves a wide range of specific tasks (Liu, 2012), such as sentence-level sentiment classification, aspect-level sentiment classification, opinion extraction and so on. Traditional methods often study these tasks separately and design specific models for each task, based on manuallydesigned features (Liu, 2012) or deep learning (Zhang et al., 2018). Recently, pre-training methods (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019) have shown their powerfulness in learning general semantic representations, and have remarkably improved most natural language processing (NLP) tasks like sentiment analysis. These methods build unsupervised objectives at word-level, such as masking strategy (Devlin et al., 2019), next-word prediction (Radford et al., 2018) or permutation (Yang et al., 2019). Such wordprediction-based objectives have shown great abilities to capture dependency between words and syntactic structures (Jawahar et al., 2019). However, as the sentiment"
2020.acl-main.374,S14-2004,0,0.0866188,"3 Aspect-Level Sem-L Sem-R 78.11 84.93 78.89 85.77 80.13 86.92 80.32 87.25 81.32 87.92 81.19 87.71 Opinion Role MPQA-Holder MPQA-Target 81.89/77.34 80.23/72.19 82.71/77.71 80.86/73.01 82.95/77.63 81.18/73.15 82.97/77.82 81.09/73.24 84.25/79.03 82.77/74.82 84.01/78.36 82.69/74.36 Table 4: Effectiveness of objectives. SW, WP, AP refers to pre-training objectives: Sentiment Word prediction, Word Polarity prediction and Aspect-sentiment Pair prediction. “Random Token” denotes random token masking used in RoBERTa. AP-I denotes predicting words in an Aspect-sentiment Pair Independently. 2014 Task4 (Pontiki et al., 2014). This task contains both restaurant domain and laptop domain, whose accuracy is evaluated separately. (3) For opinion role labeling, MPQA 2.0 dataset (Wiebe et al., 2005; Wilson, 2008) is used. MPQA aims to extract the targets or the holders of the opinions. Here we follow the method of evaluation in SRL4ORL (Marasovi´c and Frank, 2018), which is released and available online. 4-folder crossvalidation is performed, and the F-1 scores of both holder and target are reported. To perform sentiment pre-training of SKEP, the training part of Amazon-2 is used, which is the largest dataset among the"
2020.acl-main.374,P17-1154,0,0.0171678,"nion extraction (Li and Lam, 2017), emotion analysis (Gui et al., 2017; Fan et al., 2019) and so on. Lexicon-based method (Turney, 2002; Taboada et al., 2011) directly utilizes polarity of sentiment words for classification. Traditional feature-based approaches encode sentiment word information in manually-designed features to improve the supervised models (Pang et al., 2008; Agarwal et al., 2011). In contrast, deep learning approaches enhance the embedding representation with the help of sentiment words (Shin et al., 2017), or absorb the sentiment knowledge through linguistic regularization (Qian et al., 2017; Fan et al., 2019). Aspect-sentiment pair knowledge is also useful for aspect-level classification and opinion extraction. Previous works often provide weak supervision by this type of knowledge, either for aspect7 Conclusion In this paper, we propose Sentiment Knowledge Enhanced Pre-training for sentiment analysis. Sentiment masking and three sentiment pre-training objectives are designed to incorporate various types of knowledge for pre-training model. Thought conceptually simple, SKEP is empirically highly effective. SKEP significantly outperforms strong pre-training baseline RoBERTa, and"
2020.acl-main.374,P19-1356,0,0.0235741,"ently, pre-training methods (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019) have shown their powerfulness in learning general semantic representations, and have remarkably improved most natural language processing (NLP) tasks like sentiment analysis. These methods build unsupervised objectives at word-level, such as masking strategy (Devlin et al., 2019), next-word prediction (Radford et al., 2018) or permutation (Yang et al., 2019). Such wordprediction-based objectives have shown great abilities to capture dependency between words and syntactic structures (Jawahar et al., 2019). However, as the sentiment information of a text is seldom explicitly studied, it is hard to expect such pre-trained general representations to deliver optimal results for sentiment analysis (Tang et al., 2014). Sentiment analysis differs from other NLP tasks in that it deals mainly with user reviews other than news texts. There are many specific sentiment tasks, and these tasks usually depend on different types of sentiment knowledge including sentiment words, word polarity and aspect-sentiment pairs. The importance of these knowledge has been verified by tasks at different level, for instan"
2020.acl-main.374,W17-5220,0,0.101293,"tly studied, it is hard to expect such pre-trained general representations to deliver optimal results for sentiment analysis (Tang et al., 2014). Sentiment analysis differs from other NLP tasks in that it deals mainly with user reviews other than news texts. There are many specific sentiment tasks, and these tasks usually depend on different types of sentiment knowledge including sentiment words, word polarity and aspect-sentiment pairs. The importance of these knowledge has been verified by tasks at different level, for instance, sentence-level sentiment classification (Taboada et al., 2011; Shin et al., 2017; Lei et al., 2018), aspect-level sentiment classification (Vo and Zhang, 2015; Zeng et al., 2019), opinion extraction (Li and Lam, 2017; Gui et al., 2017; Fan et al., 2019) and so on. Therefore, we assume that, by integrating these knowledge into the pre-training process, the learned representation would be more sentimentspecific and appropriate for sentiment analysis. In order to learn a unified sentiment representation for multiple sentiment analysis tasks, we propose Sentiment Knowledge Enhanced Pre-training (SKEP), where sentiment knowledge about words, polarity, and aspect-sentiment pair"
2020.acl-main.374,D13-1170,0,0.0175205,"Labeling This task is to detect fine-grained opinion, such as holder and target, from input texts. Following SRL4ORL (Marasovi´c and Frank, 2018), this task is converted into sequence labeling, which uses BIOS scheme for labeling, and a CRF-layer is added to predict the labels.5 5 Experiment 5.1 Dataset and Evaluation A variety of English sentiment analysis datasets are used in this paper. Table 1 summarizes the statistics of the datasets used in the experiments. These datasets contain three types of tasks: (1) For sentence-level sentiment classification, Standford Sentiment Treebank (SST-2) (Socher et al., 2013) and Amazon-2 (Zhang et al., 2015) are used. In Amazon-2, 400k of the original training data are reserved for development. The performance is evaluated in terms of accuracy. (2) Aspect-level sentiment classification is evaluated on Semantic Eval 5 All the pretraining models, including our SKEP and baselines use CRF-Layer here, thus their performances are comparable. 4071 Model Previous SOTA RoBERTabase RoBERTabase + SKEP RoBERTalarge RoBERTalarge + SKEP Sentence-Level SST-2 Amazon-2 97.11∗ 97.372 94.9 96.61 96.7 96.94 96.5 97.33 97.0 97.56 Aspect-Level Sem-L Sem-R 81.353 87.894 78.11 84.93 81."
2020.acl-main.374,J11-2001,0,0.138897,"text is seldom explicitly studied, it is hard to expect such pre-trained general representations to deliver optimal results for sentiment analysis (Tang et al., 2014). Sentiment analysis differs from other NLP tasks in that it deals mainly with user reviews other than news texts. There are many specific sentiment tasks, and these tasks usually depend on different types of sentiment knowledge including sentiment words, word polarity and aspect-sentiment pairs. The importance of these knowledge has been verified by tasks at different level, for instance, sentence-level sentiment classification (Taboada et al., 2011; Shin et al., 2017; Lei et al., 2018), aspect-level sentiment classification (Vo and Zhang, 2015; Zeng et al., 2019), opinion extraction (Li and Lam, 2017; Gui et al., 2017; Fan et al., 2019) and so on. Therefore, we assume that, by integrating these knowledge into the pre-training process, the learned representation would be more sentimentspecific and appropriate for sentiment analysis. In order to learn a unified sentiment representation for multiple sentiment analysis tasks, we propose Sentiment Knowledge Enhanced Pre-training (SKEP), where sentiment knowledge about words, polarity, and as"
2020.acl-main.374,P14-1146,0,0.0354299,"ed most natural language processing (NLP) tasks like sentiment analysis. These methods build unsupervised objectives at word-level, such as masking strategy (Devlin et al., 2019), next-word prediction (Radford et al., 2018) or permutation (Yang et al., 2019). Such wordprediction-based objectives have shown great abilities to capture dependency between words and syntactic structures (Jawahar et al., 2019). However, as the sentiment information of a text is seldom explicitly studied, it is hard to expect such pre-trained general representations to deliver optimal results for sentiment analysis (Tang et al., 2014). Sentiment analysis differs from other NLP tasks in that it deals mainly with user reviews other than news texts. There are many specific sentiment tasks, and these tasks usually depend on different types of sentiment knowledge including sentiment words, word polarity and aspect-sentiment pairs. The importance of these knowledge has been verified by tasks at different level, for instance, sentence-level sentiment classification (Taboada et al., 2011; Shin et al., 2017; Lei et al., 2018), aspect-level sentiment classification (Vo and Zhang, 2015; Zeng et al., 2019), opinion extraction (Li and"
2020.acl-main.374,P02-1053,0,0.122795,"iment knowledge in pre-training by virtue of a relatively common mining method. We believe that a more fine-grained method would further improve the quality of knowledge, and this is something we will be exploring in the nearest future. 3.1 3.2 Unsupervised Sentiment Knowledge Mining SKEP mines the sentiment knowledge from unlabeled data. As sentiment knowledge has been the central subject of extensive research, SKEP finds a way to integrate former technique of knowledge mining with pre-training. This paper uses a simple and effective mining method based on Pointwise Mutual Information (PMI) (Turney, 2002). PMI method depends only on a small number of sentiment seed words and the word polarity WP(s) of each seed word s is given. It first builds a collection of candidate word-pairs where each word-pair contains a seed word, and meet with pre-defined part-of-speech patterns as Turney (2002). Then, the co-occurrence of a word-pair is calculated by PMI as follows: PMI(w1 , w2 ) = log p(w1 , w2 ) p(w1 )p(w2 ) Sentiment Masking Sentiment masking aims to construct a corrupted version for each input sequence where sentiment information is masked. Our sentiment masking is directed by sentiment knowledge"
2020.acl-main.374,W18-5446,0,0.0628817,"Missing"
2020.acl-main.374,N19-1036,0,0.100892,"ults for sentiment analysis (Tang et al., 2014). Sentiment analysis differs from other NLP tasks in that it deals mainly with user reviews other than news texts. There are many specific sentiment tasks, and these tasks usually depend on different types of sentiment knowledge including sentiment words, word polarity and aspect-sentiment pairs. The importance of these knowledge has been verified by tasks at different level, for instance, sentence-level sentiment classification (Taboada et al., 2011; Shin et al., 2017; Lei et al., 2018), aspect-level sentiment classification (Vo and Zhang, 2015; Zeng et al., 2019), opinion extraction (Li and Lam, 2017; Gui et al., 2017; Fan et al., 2019) and so on. Therefore, we assume that, by integrating these knowledge into the pre-training process, the learned representation would be more sentimentspecific and appropriate for sentiment analysis. In order to learn a unified sentiment representation for multiple sentiment analysis tasks, we propose Sentiment Knowledge Enhanced Pre-training (SKEP), where sentiment knowledge about words, polarity, and aspect-sentiment pairs are included to guide the process of pre-training. The sentiment knowledge is first automaticall"
2020.acl-main.374,P19-1139,0,0.0399134,"u et al., 2019) further improves BERT by robust optimization, and become one of the best pre-training methods. Inspired by BERT, some works propose finegrained objectives beyond random word masking. SpanBERT (Joshi et al., 2019) masks the span of words at the same time. ERNIE (Sun et al., 2019) proposes to mask entity words. On the other hand, pre-training for specific tasks is also studied. GlossBERT (Huang et al., 2019) exploits gloss knowledge to improve word sense disambiguation. SenseBERT (Levine et al., 2019) uses WordNet super-senses to improve word-in-context tasks. A different ERNIE (Zhang et al., 2019) exploits entity knowledge for entity-linking and relation classification. 6 Related Work Sentiment Analysis with Knowledge Various types of sentiment knowledge, including sentiment words, word polarity, aspect-sentiment pairs, have been proved to be useful for a wide range of sentiment analysis tasks. Sentiment words with their polarity are widely used for sentiment analysis, including sentencelevel sentiment classification (Taboada et al., 2011; Shin et al., 2017; Lei et al., 2018; Barnes et al., 2019), aspect-level sentiment classification (Vo and Zhang, 2015), opinion extraction (Li and La"
2020.acl-main.374,D17-1310,0,\N,Missing
2020.acl-main.374,P18-2120,0,\N,Missing
2020.acl-main.555,J05-3002,0,0.302043,"raph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al.,"
2020.acl-main.555,P11-1049,0,0.0446647,"Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (L"
2020.acl-main.555,P15-1153,0,0.155763,"lutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply tr"
2020.acl-main.555,N18-1150,0,0.0346674,"ed (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering th"
2020.acl-main.555,N13-1136,0,0.770568,"ecting redundancy and generating overall coherent summaries for MDS. Graphs that capture ∗ Corresponding author. relations between textual units have great benefits to MDS, which can help generate more informative, concise and coherent summaries from multiple documents. Moreover, graphs can be easily constructed by representing text spans (e.g. sentences, paragraphs etc.) as graph nodes and the semantic links between them as edges. Graph representations of documents such as similarity graph based on lexical similarities (Erkan and Radev, 2004) and discourse graph based on discourse relations (Christensen et al., 2013), have been widely used in traditional graph-based extractive MDS models. However, they are not well studied by most abstractive approaches, especially the end-to-end neural approaches. Few work has studied the effectiveness of explicit graph representations on neural abstractive MDS. In this paper, we develop a neural abstractive MDS model which can leverage explicit graph representations of documents to more effectively process multiple input documents and distill abstractive summaries. Our model augments the end-toend neural architecture with the ability to incorporate well-established grap"
2020.acl-main.555,N19-1423,0,0.474382,"ontent. Benefiting from the graph modeling, our model can extract salient information from long documents and generate coherent summaries more effectively. We experiment with three types of graph representations, including similarity graph, topic graph and discourse graph, which all significantly improve the MDS performance. 6232 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6232–6243 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Additionally, our model is complementary to most pre-trained language models (LMs), like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019b). They can be easily combined with our model to process much longer inputs. The combined model adopts the advantages of both our graph model and pre-trained LMs. Our experimental results show that our graph model significantly improves the performance of pre-trained LMs on MDS. The contributions of our paper are as follows: • Our work demonstrates the effectiveness of graph modeling in neural abstractive MDS. We show that explicit graph representations are beneficial for both document representation and summary generation. • We propose"
2020.acl-main.555,N19-1409,0,0.022001,"ument relations, thus achieves significantly better performance.We also leverage the graph structure to guide the summary decoding process, which is beneficial for long summary generation. Additionally, we combine the advantages of pretrained LMs into our model. 2.3 Summarization with Pretrained LMs Pretrained LMs (Peters et al., 2018; Radford et al.; Devlin et al., 2019; Dong et al., 2019; Sun et al., 2019) have recently emerged as a key technology for achieving impressive improvements in a wide variety of natural language tasks, including both language understanding and language generation (Edunov et al., 2019; Rothe et al., 2019). Liu and Lapata (2019b) attempt to incorporate pre-trained BERT encoder into SDS model and achieves significant improvements. Dong et al. (2019) further propose a unified LM for both language understanding and language generation tasks, which achieves state-of-the-art results on several generation tasks including SDS. In this work, we propose an effective method to combine pretrained LMs with our graph model and make them be able to process much longer inputs effectively. 3 Model Description In order to process long source documents more effectively, we follow Liu and Lap"
2020.acl-main.555,P19-1102,0,0.415565,"ansfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input documents as a concatenated flat sequence. Fan et al. (2019) further propose to construct a local knowledge graph from documents and then linearize the graph into a sequence to better sale Seq2Seq models to multidocument inputs. Fabbri et al. (2019) also introduce a middle-scale (about 50K) MDS news dataset (namely MultiNews), and propose an end-to-end model by incorporating traditional MMR-based 6233 Graph Encoding Layer Graph Decoding Layer Add & Normalize Add & Normalize Feed Forward Feed Forward Feed Forward Feed Forward Add & Normalize Add & Normalize Graph-informed Self-Attention Hierarchical Graph Attention PARAGRAPH POSITION ENCODING Add & Normalize Transformer Masked Self-Attention Transformer TOKEN POSITION ENCODING POSITIONAL ENCODING first paragraph last paragraph token1 END Figure 1: Illustration of our model, which follows"
2020.acl-main.555,D19-1428,0,0.119197,"9a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input documents as a concatenated flat sequence. Fan et al. (2019) further propose to construct a local knowledge graph from documents and then linearize the graph into a sequence to better sale Seq2Seq models to multidocument inputs. Fabbri et al. (2019) also introduce a middle-scale (about 50K) MDS news dataset (namely MultiNews), and propose an end-to-end model by incorporating traditional MMR-based 6233 Graph Encoding Layer Graph Decoding Layer Add & Normalize Add & Normalize Feed Forward Feed Forward Feed Forward Feed Forward Add & Normalize Add & Normalize Graph-informed Self-Attention Hierarchical Graph Attention PARAGRAPH POSITION ENCODING Add & Norm"
2020.acl-main.555,D08-1019,0,0.248382,"l. (2017) propose a neural graph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan"
2020.acl-main.555,D18-1443,0,0.0774954,"de: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2S"
2020.acl-main.555,W11-1608,0,0.0265118,"The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack o"
2020.acl-main.555,D18-1446,0,0.40232,"Missing"
2020.acl-main.555,D15-1219,1,0.902847,"structed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 20"
2020.acl-main.555,D18-1205,1,0.787509,"Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input"
2020.acl-main.555,D18-1441,1,0.688871,"Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input"
2020.acl-main.555,C18-1101,0,0.0528075,"s between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu"
2020.acl-main.555,P04-1077,0,0.0143827,"l Lead LexRank FT BERT+FT XLNet+FT RoBERTa+FT T-DMCA HT GraphSum GraphSum+RoBERTa R-1 38.22 36.12 40.56 41.49 40.85 42.05 40.77 41.53 42.63 42.99 R-2 16.85 11.67 25.35 25.73 25.29 27.00 25.60 26.52 27.70 27.83 Model Lead LexRank PG-BRNN HiMAP FT RoBERTa+FT HT GraphSum R-L 26.89 22.52 34.73 35.59 35.20 36.56 34.90 35.76 36.97 37.36 G.S.(Similarity)+RoBERTa G.S.(Topic)+RoBERTa G.S.(Discourse)+RoBERTa Table 1: Evaluation results on the WikiSum test set using ROUGE F1 . R-1, R-2 and R-L are abbreviations for ROUGE-1, ROUGE-2 and ROUGE-L, respectively. rization quality is evaluated using ROUGE F1 (Lin and Och, 2004). We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) between system summaries and gold references as a means of assessing informativeness, and the longest common subsequence (ROUGE-L2 ) as a means of accessing fluency. Results on WikiSum Table 6 summarizes the evaluation results on the WikiSum dataset. Several strong extractive baselines and abstractive baselines are also evaluated and compared with our models. The first block in the table shows the results of extractive methods Lead and LexRank (Erkan and Radev, 2004). The second block shows the results of abstractive methods: (1) FT"
2020.acl-main.555,P19-1500,0,0.195307,"n Transformer TOKEN POSITION ENCODING POSITIONAL ENCODING first paragraph last paragraph token1 END Figure 1: Illustration of our model, which follows the encoder-deocder architecture. The encoder is a stack of transformer layers and graph encoding layers, while the decoder is a stack of graph decoding layers. We incorporate explicit graph representations into both the graph encoding layers and graph decoding layers. extractive model with a standard Seq2Seq model. The above Seq2Seq models haven’t study the importance of cross-document relations and graph representations in MDS. Most recently, Liu and Lapata (2019a) propose a hierarchical transformer model to utilize the hierarchical structure of documents. They propose to learn cross-document relations based on selfattention mechanism. They also propose to incorporate explicit graph representations into the model by simply replacing the attention weights with a graph matrix, however, it doesn’t achieve obvious improvement according to their experiments. Our work is partly inspired by this work, but our approach is quite different from theirs. In contrast to their approach, we incorporate explicit graph representations into the encoding process via a g"
2020.acl-main.555,D19-1387,0,0.0797698,"n Transformer TOKEN POSITION ENCODING POSITIONAL ENCODING first paragraph last paragraph token1 END Figure 1: Illustration of our model, which follows the encoder-deocder architecture. The encoder is a stack of transformer layers and graph encoding layers, while the decoder is a stack of graph decoding layers. We incorporate explicit graph representations into both the graph encoding layers and graph decoding layers. extractive model with a standard Seq2Seq model. The above Seq2Seq models haven’t study the importance of cross-document relations and graph representations in MDS. Most recently, Liu and Lapata (2019a) propose a hierarchical transformer model to utilize the hierarchical structure of documents. They propose to learn cross-document relations based on selfattention mechanism. They also propose to incorporate explicit graph representations into the model by simply replacing the attention weights with a graph matrix, however, it doesn’t achieve obvious improvement according to their experiments. Our work is partly inspired by this work, but our approach is quite different from theirs. In contrast to their approach, we incorporate explicit graph representations into the encoding process via a g"
2020.acl-main.555,2021.ccl-1.108,0,0.172335,"Missing"
2020.acl-main.555,C16-1143,0,0.164968,"Missing"
2020.acl-main.555,W04-3252,0,0.204168,"ing that graph modeling enables our model process longer inputs with better performance, and graphs with richer relations are more beneficial for MDS.1 2 Related Work 2.1 Graph-based MDS Most previous MDS approaches are extractive, which extract salient textual units from documents based on graph-based representations of sentences. Various ranking methods have been developed to rank textual units based on graphs to select most salient ones for inclusion in the final summary. Erkan and Radev (2004) propose LexRank to compute sentence importance based on a lexical similarity graph of sentences. Mihalcea and Tarau (2004) propose a graph-based ranking model to extract salient sentences from documents. Wan (2008) further proposes to incorporate documentlevel information and sentence-to-document relations into the graph-based ranking process. A series of variants of the PageRank algorithm has been 1 Codes and results are in: https://github.com/ PaddlePaddle/Research/tree/master/NLP/ ACL2020-GraphSum further developed to compute the salience of textual units recursively based on various graph representations of documents (Wan and Xiao, 2009; Cai and Li, 2012). More recently, Yasunaga et al. (2017) propose a neura"
2020.acl-main.555,P14-1084,0,0.061278,"ased on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straigh"
2020.acl-main.555,W00-1009,0,0.690742,"he summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines. 1 Introduction Multi-document summarization (MDS) brings great challenges to the widely used sequence-tosequence (Seq2Seq) neural architecture as it requires effective representation of multiple input documents and content organization of long summaries. For MDS, different documents may contain the same content, include additional information, and present complementary or contradictory information (Radev, 2000). So different from single document summarization (SDS), cross-document links are very important in extracting salient information, detecting redundancy and generating overall coherent summaries for MDS. Graphs that capture ∗ Corresponding author. relations between textual units have great benefits to MDS, which can help generate more informative, concise and coherent summaries from multiple documents. Moreover, graphs can be easily constructed by representing text spans (e.g. sentences, paragraphs etc.) as graph nodes and the semantic links between them as edges. Graph representations of docu"
2020.acl-main.555,P17-1099,0,0.154693,"ss. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely Wiki"
2020.acl-main.555,D19-1323,0,0.029316,"lay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input documents as a concatenated flat sequence. Fan et al. (2019) furth"
2020.acl-main.555,D18-1206,0,0.0452166,"e, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input documents as a concatenat"
2020.acl-main.555,D18-1108,0,0.0263741,"nd the Transformer with a hierarchical graph attention mechanism to utilize explicit graph structure to guide the summary decoding process. In the following, we will focus on the graph encoding layer and graph decoding layer of our model. 3.1 Graph Encoding Layer As shown in Figure 1, based on the output of the token-level transformer encoding layers, the graph encoding layer is used to encode all documents globally. Most existing neural work only utilizes attention mechanism to learn latent graph representations of documents where the graph edges are attention weights (Liu and Lapata, 2019a; Niculae et al., 2018; Fernandes et al., 2018). However, much work in traditional MDS has shown that explicit graph representations are very beneficial to MDS. Different types of graphs capture different kinds of semantic relations (e.g. lexical relations or discourse relations), which can help the model focus on different facets of the summarization task. In this work, we propose to incorporate explicit graph representations into the neural encoding process via a graph-informed attention mechanism. It takes advantage of the explicit relations in graphs to learn better inter-paragraph relations. Each paragraph can"
2020.acl-main.555,P19-1504,0,0.0828261,"on extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input documents as a concatenated flat sequence. Fan et al. (2019) further propose to construct a local k"
2020.acl-main.555,N18-1202,0,0.0230266,"pproach is quite different from theirs. In contrast to their approach, we incorporate explicit graph representations into the encoding process via a graphinformed attention mechanism. Under the guidance of explicit relations in graphs, our model can learn better and richer cross-document relations, thus achieves significantly better performance.We also leverage the graph structure to guide the summary decoding process, which is beneficial for long summary generation. Additionally, we combine the advantages of pretrained LMs into our model. 2.3 Summarization with Pretrained LMs Pretrained LMs (Peters et al., 2018; Radford et al.; Devlin et al., 2019; Dong et al., 2019; Sun et al., 2019) have recently emerged as a key technology for achieving impressive improvements in a wide variety of natural language tasks, including both language understanding and language generation (Edunov et al., 2019; Rothe et al., 2019). Liu and Lapata (2019b) attempt to incorporate pre-trained BERT encoder into SDS model and achieves significant improvements. Dong et al. (2019) further propose a unified LM for both language understanding and language generation tasks, which achieves state-of-the-art results on several generat"
2020.acl-main.555,P13-1137,0,0.124191,"Missing"
2020.acl-main.555,K17-1045,0,0.0407222,"f sentences. Mihalcea and Tarau (2004) propose a graph-based ranking model to extract salient sentences from documents. Wan (2008) further proposes to incorporate documentlevel information and sentence-to-document relations into the graph-based ranking process. A series of variants of the PageRank algorithm has been 1 Codes and results are in: https://github.com/ PaddlePaddle/Research/tree/master/NLP/ ACL2020-GraphSum further developed to compute the salience of textual units recursively based on various graph representations of documents (Wan and Xiao, 2009; Cai and Li, 2012). More recently, Yasunaga et al. (2017) propose a neural graph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova"
2020.acl-main.555,D08-1079,0,\N,Missing
2020.acl-main.639,D14-1181,0,\N,Missing
2020.acl-main.639,D17-1230,0,\N,Missing
2020.acl-main.639,D18-1041,1,\N,Missing
2020.acl-main.9,P19-1535,1,0.785728,"e obtained through the following inference procedure: 1) Candidate Response Generation – Conditioned on each latent value z ∈ [1, K], generate corresponding candidate response r. 2) Response Selection – Calculate the probability for each response p(lr = 1|c, r) and select the one with highest coherence value as the final response. It is worth noting that the above fine-tuning and inference procedures are set up for the dialogue generation without any specific objectives. If there exists a specific objective within the conversation, such as letting both participants know more about each other (Bao et al., 2019), the fine-tuning can proceed to maximize the pre-defined rewards with reinforcement learning (RL). Under such circumstances, our latent discrete variable can be naturally treated as action within RL, and thus the response selection can be straightforwardly solved by selecting the action that results in the maximum reward. 3 3.1 3.1.2 Compared Methods The following models have been compared in the experiments. Baseline. Sequence to sequence with attention (Seq2Seq) (Vinyals and Le, 2015) is employed as the baseline for the experiments on Persona-Chat and Daily Dialog. DSTC7-AVSD has provided a"
2020.acl-main.9,W14-3346,0,0.0119552,"BERT Bi-direction Bi-direction Twitter & Reddit ✓ 15.080 12.936 12.285 Table 5: Perplexity of different pre-trained models on Persona-Chat, with best value written in bold. 3.1.3 Evaluation Metrics the response’s final score is determined via majority voting. The average Fleiss’s kappa (Fleiss and Cohen, 1973) on Persona-Chat and Daily Dialog is 0.515 and 0.480 respectively, indicating annotators have reached moderate agreement. Both automatic and human evaluations are employed to assess the performance of compared methods. In automatic evaluation, the following metrics are included: • BLEU (Chen and Cherry, 2014) measures the n-gram overlap between generated response and the target response. • Distinct-1/2 (Li et al., 2016) measures the generation diversity, which is defined as the number of distinct uni- or bi-grams divided by the total amount of generated words. • Knowledge Recall/Precision/F1 (Dinan et al., 2019b) measures the degree of informativeness w.r.t. background knowledge. • In DSTC7-AVSD, the MSCOCO platform (Chen et al., 2015) is employed for evaluation. It compares the generated response with six ground truth responses, using metrics of BLEU, METEOR, ROUGH-L and CIDEr. In human evaluatio"
2020.acl-main.9,I17-1099,0,0.0585933,"former blocks in our model L is 12 and the hidden embedding dimension D is 768. The batch size is set to 64 and K is set to 20 for the discrete latent variable. Adam optimizer (Kingma and Ba, 2015) is employed for optimization with a learning rate of 5e-5. The pretraining of dialogue generation was carried out on 8 Nvidia Telsa V100 32G GPU cards for 3.5M steps, taking about two weeks to reach convergence. 2.5 both manually annotated conversations and corresponding persona profiles (background knowledge), where two participants chat naturally and try to get to know each other. • Daily Dialog (Li et al., 2017) is a chit-chat dataset, which contains high-quality human conversations about daily life. • DSTC7-AVSD (Alamri et al., 2019), short for Audio Visual Scene-aware Dialog of the DSTC7 challenge, is a conversational question answering dataset. In DSTC7-AVSD, the system need to generate an answer given dialogue context and background knowledge. There are two available options of knowledge utilization: 1) using singlemodal information of text only, including video’s caption and summary; 2) relying on multi-modal information, including text, audio and visual features. The single-modal option is adop"
2020.acl-main.9,D19-1407,0,0.306322,"Missing"
2020.acl-main.9,D16-1230,0,0.0969151,"Missing"
2020.acl-main.9,N19-1125,0,0.0341595,"Missing"
2020.acl-main.9,P19-1534,0,0.0578899,"Missing"
2020.acl-main.9,P19-1608,0,0.188466,"straightforwardly solved by selecting the action that results in the maximum reward. 3 3.1 3.1.2 Compared Methods The following models have been compared in the experiments. Baseline. Sequence to sequence with attention (Seq2Seq) (Vinyals and Le, 2015) is employed as the baseline for the experiments on Persona-Chat and Daily Dialog. DSTC7-AVSD has provided a baseline system, which is built upon hierarchical recurrent encoders with multi-modal features. State of the art. Persona-Chat was also utilized in the ConvAI2 challenge (Dinan et al., 2019a), where the team of Lost in Conversation (LIC) (Golovanov et al., 2019) obtains the best performance. LIC is also one transformer based generation method and fine-tuned upon the pre-trained model of GPT (Radford et al., 2018). For the dataset of Daily Dialog, its best results are reported by the recently developed method – iVAEMI (Fang et al., 2019), which generates diverse responses with samplebased latent representation. In DSTC7-AVSD, the team of CMU (Sanabria et al., 2019) obtains the best performance across all the evaluation metrics. Our method. To better analyze the effects of our latent discrete variable, we also compare to the version without latent vari"
2020.acl-main.9,N18-2008,0,0.0436583,"ifference in training mode, a flexible paradigm integrating uni- and bi-directional processing is employed in this work, which is inspired by the latest unified language modeling (Dong et al., 2019). Thirdly, a discrete latent variable is introduced to model the one-to-many relationship among utterances in conversations. Each value of the latent variable corresponds to the particular conversational intent of one response, which is referred as latent speech act. Distinct with those controllable dialogue generation based on explicit labels (including emotion, keywords, domain codes, and so on) (Huang et al., 2018; Keskar et al., 2019), our latent variable gets Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. In this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. We also introduce discrete latent variables to tackle the inherent one"
2020.acl-main.9,N16-1014,0,0.0833488,"dels on Persona-Chat, with best value written in bold. 3.1.3 Evaluation Metrics the response’s final score is determined via majority voting. The average Fleiss’s kappa (Fleiss and Cohen, 1973) on Persona-Chat and Daily Dialog is 0.515 and 0.480 respectively, indicating annotators have reached moderate agreement. Both automatic and human evaluations are employed to assess the performance of compared methods. In automatic evaluation, the following metrics are included: • BLEU (Chen and Cherry, 2014) measures the n-gram overlap between generated response and the target response. • Distinct-1/2 (Li et al., 2016) measures the generation diversity, which is defined as the number of distinct uni- or bi-grams divided by the total amount of generated words. • Knowledge Recall/Precision/F1 (Dinan et al., 2019b) measures the degree of informativeness w.r.t. background knowledge. • In DSTC7-AVSD, the MSCOCO platform (Chen et al., 2015) is employed for evaluation. It compares the generated response with six ground truth responses, using metrics of BLEU, METEOR, ROUGH-L and CIDEr. In human evaluation, we randomly select 100 dialogue contexts and generate responses with compared methods. Three crowd-sourcing wo"
2020.acl-main.9,P18-1205,0,0.0327722,"rted by the recently developed method – iVAEMI (Fang et al., 2019), which generates diverse responses with samplebased latent representation. In DSTC7-AVSD, the team of CMU (Sanabria et al., 2019) obtains the best performance across all the evaluation metrics. Our method. To better analyze the effects of our latent discrete variable, we also compare to the version without latent variable (Our w/o Latent).2 Experiments Settings 3.1.1 Datasets To evaluate the performance of our proposed method, comprehensive experiments have been carried out on three publicly available datasets. • Persona-Chat (Zhang et al., 2018) is a knowledge grounded conversation dataset. It provides 2 It shares the same training settings as our method with latent variables: network parameters are first initialized with BERTBASE , and the pre-training is further carried out on Reddit and Twitter. The only difference lies in the incorporation of latent variable. 89 Dataset Type Knowledge # Train # Valid # Test Persona-Chat Chit-chat with persona Persona profiles 8,939 dialogues 131,438 turns 1,000 dialogues 15,602 turns 968 dialogues 15,024 turns Daily Dialog Chit-chat N/A 11,118 dialogues 87,170 turns 1,000 dialogues 8,069 turns 1,"
2020.acl-main.9,P18-1101,0,0.0622304,"Missing"
2020.acl-main.9,P17-1061,0,0.305634,"rectional decoding flexibly via specific selfattention masks. Both response generation and latent act recognition are carried out under the unified network with shared parameters. Their detailed implementations are described as follows. Given the context c and a specific speech act z, the response generation can be estimated as Dialogue Generation Pre-training Given a piece of context, there exist multiple appropriate responses, leading to diverse conversation flows. It is widely recognized that the capability of modeling one-to-many relationship is crucial for the dialogue generation system (Zhao et al., 2017; Chen et al., 2019). To this end, we propose to encode discrete latent variables into transformer blocks for one-to-many relationship modeling, where two reciprocal tasks of response generation and latent act recognition are collaboratively carried out. 2.1 $()|&quot;, #) It is snowing outside. exempted from the restriction of human annotations and can be learned automatically from the corpus in an unsupervised way. In the pre-training of dialogue generation, response generation and latent act recognition are carried out simultaneously within a shared network. Based on the context and latent varia"
2020.acl-main.98,N16-1014,0,0.232101,"nate the goal predicted by our model, all the related knowledge and the dialog context as its input. Turn-level results Dialog-level results Methods↓ Metrics→ Fluency Appro. Infor. Proactivity Goal success rate Coherence S2S +gl. +kg. MGCG R +gl. +kg. MGCG G +gl. +kg. 1.08 1.98 1.94 0.23 0.60 0.75 0.37 1.28 1.68 0.94 1.22 1.34 0.37 0.68 0.82 0.49 0.83 0.91 Table 5: Human evaluation results at the level of turns and dialogs. 5.3 Automatic Evaluations Metrics For automatic evaluation, we use several common metrics such as BLEU (Papineni et al., 2002), F1, perplexity (PPL), and DISTINCT (DIST2) (Li et al., 2016) to measure the relevance, fluency, and diversity of generated responses. Following the setting in previous work (Wu et al., 2019; Zhang et al., 2018a), we also measure the performance of all models using Hits@1 and Hits@3.6 Here we let each model to select the best response from 10 candidates. Those 10 candidate responses consist of the ground-truth response generated by humans and nine randomly sampled ones from the training set. Moreover, we also evaluate the knowledge-selection capability of each model by calculating knowledge precision/recall/F1 scores as done in Wu et al. (2019).7 In add"
2020.acl-main.98,D18-1255,0,0.356097,"The goal-planning module can conduct dialog management to control the dialog 1037 • We identify the task of conversational recommendation over multi-type dialogs. • To facilitate the study of this task, we create a novel dialog dataset DuRecDial, with rich variability of dialog types and domains as shown in Table 1. • We propose a conversation generation framework with a novel mixed-goal driven dialog policy mechanism. Datasets↓ Metrics→ Facebook Rec (Dodge et al., 2016) REDIAL (Li et al., 2018) GoRecDial (Kang et al., 2019) OpenDialKG (Moon et al., 2019) CMU DoG (Zhou et al., 2018a) IIT DoG (Moghe et al., 2018) Wizard-of-wiki (Dinan et al., 2019) OpenDialKG (Moon et al., 2019) DuConv (Wu et al., 2019) KdConv (Zhou et al., 2020) DuRecDial #Dialogs #Utterances Dialog types Domains 1M 10k 9k 12k 4k 9k 22k 3k 29k 4.5k 10.2k 6M 163k 170k 143k 130k 90k 202k 38k 270k 86k 156k Rec. Rec., chitchat Rec. Rec. Chitchat Chitchat Chitchat Chitchat Chitchat Chitchat Rec., chitchat, QA, task Movie Movie Movie Movie, book Movie Movie 1365 Wikipedia topics Sports, music Movie Movie, music, travel Movie, music, movie star, food, restaurant, news, weather User profile No No Yes No No No No No No No Yes Table 1: Compari"
2020.acl-main.98,P19-1081,0,0.263116,"oal planning module and a goal-guided responding module. The goal-planning module can conduct dialog management to control the dialog 1037 • We identify the task of conversational recommendation over multi-type dialogs. • To facilitate the study of this task, we create a novel dialog dataset DuRecDial, with rich variability of dialog types and domains as shown in Table 1. • We propose a conversation generation framework with a novel mixed-goal driven dialog policy mechanism. Datasets↓ Metrics→ Facebook Rec (Dodge et al., 2016) REDIAL (Li et al., 2018) GoRecDial (Kang et al., 2019) OpenDialKG (Moon et al., 2019) CMU DoG (Zhou et al., 2018a) IIT DoG (Moghe et al., 2018) Wizard-of-wiki (Dinan et al., 2019) OpenDialKG (Moon et al., 2019) DuConv (Wu et al., 2019) KdConv (Zhou et al., 2020) DuRecDial #Dialogs #Utterances Dialog types Domains 1M 10k 9k 12k 4k 9k 22k 3k 29k 4.5k 10.2k 6M 163k 170k 143k 130k 90k 202k 38k 270k 86k 156k Rec. Rec., chitchat Rec. Rec. Chitchat Chitchat Chitchat Chitchat Chitchat Chitchat Rec., chitchat, QA, task Movie Movie Movie Movie, book Movie Movie 1365 Wikipedia topics Sports, music Movie Movie, music, travel Movie, music, movie star, food, restaurant, news, weather User p"
2020.acl-main.98,P02-1040,0,0.106605,"nts “with(without) knowledge”. For “S2S +gl.+kg.”, we simply concatenate the goal predicted by our model, all the related knowledge and the dialog context as its input. Turn-level results Dialog-level results Methods↓ Metrics→ Fluency Appro. Infor. Proactivity Goal success rate Coherence S2S +gl. +kg. MGCG R +gl. +kg. MGCG G +gl. +kg. 1.08 1.98 1.94 0.23 0.60 0.75 0.37 1.28 1.68 0.94 1.22 1.34 0.37 0.68 0.82 0.49 0.83 0.91 Table 5: Human evaluation results at the level of turns and dialogs. 5.3 Automatic Evaluations Metrics For automatic evaluation, we use several common metrics such as BLEU (Papineni et al., 2002), F1, perplexity (PPL), and DISTINCT (DIST2) (Li et al., 2016) to measure the relevance, fluency, and diversity of generated responses. Following the setting in previous work (Wu et al., 2019; Zhang et al., 2018a), we also measure the performance of all models using Hits@1 and Hits@3.6 Here we let each model to select the best response from 10 candidates. Those 10 candidate responses consist of the ground-truth response generated by humans and nine randomly sampled ones from the training set. Moreover, we also evaluate the knowledge-selection capability of each model by calculating knowledge p"
2020.acl-main.98,P13-2089,0,0.526547,"ender proactively leads a multi-type dialog to approach recommendation targets and then makes multiple recommendations with rich interaction behavior. This dataset allows us to systematically investigate different parts of the overall problem, e.g., how to naturally lead a dialog, how to interact with users for recommendation. Finally we establish baseline results on DuRecDial for future studies.1 1 Introduction In recent years, there has been a significant increase in the work of conversational recommendation due to the rise of voice-based bots (Christakopoulou et al., 2016; Li et al., 2018; Reschke et al., 2013; Warnestal, 2005). They focus on how to provide high-quality recommendations through dialog-based interactions with users. These work fall into two categories: (1) task-oriented dialogmodeling approaches (Christakopoulou et al., 2016; Sun and Zhang, 2018; Warnestal, 2005); (2) nontask dialog-modeling approaches with more freeform interactions (Kang et al., 2019; Li et al., 2018). ∗ This work was done at Baidu. Corresponding author: Wanxiang Che. 1 Dataset and codes are publicly available at https://github.com/PaddlePaddle/models/ tree/develop/PaddleNLP/Research/ACL2020-DuRecDial. † Almost all"
2020.acl-main.98,P19-1565,0,0.111519,"Missing"
2020.acl-main.98,D19-1203,0,0.344356,"on DuRecDial for future studies.1 1 Introduction In recent years, there has been a significant increase in the work of conversational recommendation due to the rise of voice-based bots (Christakopoulou et al., 2016; Li et al., 2018; Reschke et al., 2013; Warnestal, 2005). They focus on how to provide high-quality recommendations through dialog-based interactions with users. These work fall into two categories: (1) task-oriented dialogmodeling approaches (Christakopoulou et al., 2016; Sun and Zhang, 2018; Warnestal, 2005); (2) nontask dialog-modeling approaches with more freeform interactions (Kang et al., 2019; Li et al., 2018). ∗ This work was done at Baidu. Corresponding author: Wanxiang Che. 1 Dataset and codes are publicly available at https://github.com/PaddlePaddle/models/ tree/develop/PaddleNLP/Research/ACL2020-DuRecDial. † Almost all these work focus on a single type of dialogs, either task oriented dialogs for recommendation, or recommendation oriented open-domain conversation. Moreover, they assume that both sides in the dialog (especially the user) are aware of the conversational goal from the beginning. In many real-world applications, there are multiple dialog types in human-bot conver"
2020.acl-main.98,D14-1181,0,0.0103428,"Current goal prediction (a) Goal-planning module Figure 3: The architecture of our multi-goal driven conversation generation framework (denoted as MGCG). K. These goals will be used as answers for training of the goal-planning module, while the tuples of [context, a ground-truth goal, K, response] will be used for training of the responding module. 4.2 Goal-planning Model As shown in Figure 3(a), we divide the task of goal planning into two sub-tasks, goal completion estimation, and current goal prediction. Goal completion estimation For this subtask, we use Convolutional neural network (CNN)(Kim, 2014) to estimate the probability of goal completion by: PGC (l = 1|X, gt−1 ). (1) Current goal prediction If gt−1 is not completed (PGC &lt; 0.5), then gc = gt−1 , where gc is the goal for Y . Otherwise, we use CNN based multi-task classification to predict the current goal by maximizing the following probability: gt = arg max PGP (g ty , g tp |X, G 0 , Pi k , K), (2) gc = gt , (3) s g ty ,g tp where g ty is a candidate dialog type and g tp is a candidate dialog topic. 4.3 Retrieval-based Response Model we modify the original retrieval model to suit our task by emphasizing the use of goals. As shown"
2020.acl-main.98,D14-1007,1,0.863561,"m/PaddlePaddle/models/ tree/develop/PaddleNLP/Research/ACL2020-DuRecDial. † Almost all these work focus on a single type of dialogs, either task oriented dialogs for recommendation, or recommendation oriented open-domain conversation. Moreover, they assume that both sides in the dialog (especially the user) are aware of the conversational goal from the beginning. In many real-world applications, there are multiple dialog types in human-bot conversations (called multi-type dialogs), such as chit-chat, task oriented dialogs, recommendation dialogs, and even question answering (Ram et al., 2018; Wang et al., 2014; Zhou et al., 2018b). Therefore it is crucial to study how to proactively and naturally make conversational recommendation by the bots in the context of multi-type human-bot communication. For example, the bots could proactively make recommendations after question answering or a task dialog to improve user experience, or it could lead a dialog from chitchat to approach a given product as commercial advertisement. However, to our knowledge, there is less previous work on this problem. To address this challenge, we present a novel task, conversational recommendation over multitype dialogs, wher"
2020.acl-main.98,P19-1369,1,0.765759,"the task of conversational recommendation over multi-type dialogs. • To facilitate the study of this task, we create a novel dialog dataset DuRecDial, with rich variability of dialog types and domains as shown in Table 1. • We propose a conversation generation framework with a novel mixed-goal driven dialog policy mechanism. Datasets↓ Metrics→ Facebook Rec (Dodge et al., 2016) REDIAL (Li et al., 2018) GoRecDial (Kang et al., 2019) OpenDialKG (Moon et al., 2019) CMU DoG (Zhou et al., 2018a) IIT DoG (Moghe et al., 2018) Wizard-of-wiki (Dinan et al., 2019) OpenDialKG (Moon et al., 2019) DuConv (Wu et al., 2019) KdConv (Zhou et al., 2020) DuRecDial #Dialogs #Utterances Dialog types Domains 1M 10k 9k 12k 4k 9k 22k 3k 29k 4.5k 10.2k 6M 163k 170k 143k 130k 90k 202k 38k 270k 86k 156k Rec. Rec., chitchat Rec. Rec. Chitchat Chitchat Chitchat Chitchat Chitchat Chitchat Rec., chitchat, QA, task Movie Movie Movie Movie, book Movie Movie 1365 Wikipedia topics Sports, music Movie Movie, music, travel Movie, music, movie star, food, restaurant, news, weather User profile No No Yes No No No No No No No Yes Table 1: Comparison of our dataset DuRecDial to recommendation dialog datasets and knowledge grounded dialog"
2020.acl-main.98,D18-1076,0,\N,Missing
2020.acl-main.98,2020.acl-main.635,0,\N,Missing
2020.emnlp-main.178,D18-1337,0,0.2376,"Missing"
2020.emnlp-main.178,P19-1126,0,0.373191,"Missing"
2020.emnlp-main.178,N18-2079,0,0.0758748,"el adaptive segmentation policy for simultaneous translation. The policy learns to segment the source text by considering possible translations produced by the translation model, maintaining consistency between the segmentation and translation. Experimental results on Chinese-English and German-English translation show that our method achieves a better accuracy-latency trade-off over recently proposed state-of-the-art methods. • Fixed Policies are hard policies that follow a pre-defined schedule independent of the context. They segment the source text based on a fixed length (Ma et al., 2019; Dalvi et al., 2018). For example, the wait-k method (Ma et al., 2019) first reads k source words, and then generates one target word immediately after each subsequent word is received. Policies of this type are simple and easy to implement. However, they do not consider contextual information and usually result in a drop in translation accuracy. • Adaptive Policies learn to do segmentation according to dynamic contextual information. They either use a specific model to chunk the streaming source text (Sridhar et al., 2013; Oda et al., 2014; Cho and Esipova, 2016; Gu et al., 2017; Zheng et al., 2019a, 2020) or jo"
2020.emnlp-main.178,D14-1140,0,0.262784,"Missing"
2020.emnlp-main.178,E17-1099,0,0.488904,"ixed length (Ma et al., 2019; Dalvi et al., 2018). For example, the wait-k method (Ma et al., 2019) first reads k source words, and then generates one target word immediately after each subsequent word is received. Policies of this type are simple and easy to implement. However, they do not consider contextual information and usually result in a drop in translation accuracy. • Adaptive Policies learn to do segmentation according to dynamic contextual information. They either use a specific model to chunk the streaming source text (Sridhar et al., 2013; Oda et al., 2014; Cho and Esipova, 2016; Gu et al., 2017; Zheng et al., 2019a, 2020) or jointly learn segmentation and translation in an end-to-end framework (Arivazhagan et al., 2019; Zheng et al., 2019b; Ma et al., 2020). The adaptive methods are more flexible than the fixed ones and achieve state-of-the-art. 1 Introduction In recent years, simultaneous translation has attracted increasing interest both in research and industry community. It aims at a real-time translation that demands high translation quality and an as-short-as-possible delay between speech and translation output. A typical simultaneous translation system consists of an auto-spe"
2020.emnlp-main.178,P14-2090,0,0.329879,"They segment the source text based on a fixed length (Ma et al., 2019; Dalvi et al., 2018). For example, the wait-k method (Ma et al., 2019) first reads k source words, and then generates one target word immediately after each subsequent word is received. Policies of this type are simple and easy to implement. However, they do not consider contextual information and usually result in a drop in translation accuracy. • Adaptive Policies learn to do segmentation according to dynamic contextual information. They either use a specific model to chunk the streaming source text (Sridhar et al., 2013; Oda et al., 2014; Cho and Esipova, 2016; Gu et al., 2017; Zheng et al., 2019a, 2020) or jointly learn segmentation and translation in an end-to-end framework (Arivazhagan et al., 2019; Zheng et al., 2019b; Ma et al., 2020). The adaptive methods are more flexible than the fixed ones and achieve state-of-the-art. 1 Introduction In recent years, simultaneous translation has attracted increasing interest both in research and industry community. It aims at a real-time translation that demands high translation quality and an as-short-as-possible delay between speech and translation output. A typical simultaneous tr"
2020.emnlp-main.178,P02-1040,0,0.111276,"uture words, which is less than m. Our training follows the pre-training and fine-tuning framework (Devlin et al., 2018; Sun et al., 2019). • MILk (Arivazhagan et al., 2019): using hard attention to schedule the policy and train the policy together with the NMT model in an end-to-end framework. It uses a weight λ in the loss function to balance translation quality and latency. 9 3 Experiments We carry out experiments on two translation tasks: the NIST Chinese-English (Zh-En) translation task (2M sentences), and the WMT 2015 German-English (De-En) translation task (4.5M sentences).we use BLEU (Papineni et al., 2002) score to evaluate translation quality, and Average Lagging 5 (Ma et al., 2019) to measure latency. 3.1 Data Preprocess We use an open-source Chinese Tokenizer 6 to preprocess Chinese and apply Moses Tokenizer 7 to preprocess English and German. For Zh-En, we validate on NIST newstest 2006 and report results on newstest 2002, 2003, 2004, 2005, and 2008. We use SententcePiece 8 to implement byte-pair encoding (BPE) (Sennrich et al., 2016) for both Chinese and English by setting the vocabulary size to 20K and 18K, respectively. For De-En, we validate on newstest 2013 and then report results on n"
2020.emnlp-main.178,N13-1023,0,0.212308,"ndent of the context. They segment the source text based on a fixed length (Ma et al., 2019; Dalvi et al., 2018). For example, the wait-k method (Ma et al., 2019) first reads k source words, and then generates one target word immediately after each subsequent word is received. Policies of this type are simple and easy to implement. However, they do not consider contextual information and usually result in a drop in translation accuracy. • Adaptive Policies learn to do segmentation according to dynamic contextual information. They either use a specific model to chunk the streaming source text (Sridhar et al., 2013; Oda et al., 2014; Cho and Esipova, 2016; Gu et al., 2017; Zheng et al., 2019a, 2020) or jointly learn segmentation and translation in an end-to-end framework (Arivazhagan et al., 2019; Zheng et al., 2019b; Ma et al., 2020). The adaptive methods are more flexible than the fixed ones and achieve state-of-the-art. 1 Introduction In recent years, simultaneous translation has attracted increasing interest both in research and industry community. It aims at a real-time translation that demands high translation quality and an as-short-as-possible delay between speech and translation output. A typic"
2020.emnlp-main.178,P16-1162,0,0.0613232,"asks: the NIST Chinese-English (Zh-En) translation task (2M sentences), and the WMT 2015 German-English (De-En) translation task (4.5M sentences).we use BLEU (Papineni et al., 2002) score to evaluate translation quality, and Average Lagging 5 (Ma et al., 2019) to measure latency. 3.1 Data Preprocess We use an open-source Chinese Tokenizer 6 to preprocess Chinese and apply Moses Tokenizer 7 to preprocess English and German. For Zh-En, we validate on NIST newstest 2006 and report results on newstest 2002, 2003, 2004, 2005, and 2008. We use SententcePiece 8 to implement byte-pair encoding (BPE) (Sennrich et al., 2016) for both Chinese and English by setting the vocabulary size to 20K and 18K, respectively. For De-En, we validate on newstest 2013 and then report results on newstest 2015. We utilize a joint vocabulary, with a vocabulary size of 32K. Notably, translation quality in all experiments is measured using detokenized, cased BLEU. • MU: our proposed basic method of translating after detecting a meaning unit. • MU++: our proposed refined method to detect fine-grained meaning units. The training of segmentation models for chunk, MU and MU++ are based on the classification task of BERT 10 and ERNIE 11 ,"
2020.emnlp-main.178,2020.acl-main.254,0,0.171224,"Missing"
2020.emnlp-main.178,D19-1137,0,0.344659,"t al., 2019; Dalvi et al., 2018). For example, the wait-k method (Ma et al., 2019) first reads k source words, and then generates one target word immediately after each subsequent word is received. Policies of this type are simple and easy to implement. However, they do not consider contextual information and usually result in a drop in translation accuracy. • Adaptive Policies learn to do segmentation according to dynamic contextual information. They either use a specific model to chunk the streaming source text (Sridhar et al., 2013; Oda et al., 2014; Cho and Esipova, 2016; Gu et al., 2017; Zheng et al., 2019a, 2020) or jointly learn segmentation and translation in an end-to-end framework (Arivazhagan et al., 2019; Zheng et al., 2019b; Ma et al., 2020). The adaptive methods are more flexible than the fixed ones and achieve state-of-the-art. 1 Introduction In recent years, simultaneous translation has attracted increasing interest both in research and industry community. It aims at a real-time translation that demands high translation quality and an as-short-as-possible delay between speech and translation output. A typical simultaneous translation system consists of an auto-speech-recognition (ASR"
2020.emnlp-main.178,P19-1582,0,0.300752,"t al., 2019; Dalvi et al., 2018). For example, the wait-k method (Ma et al., 2019) first reads k source words, and then generates one target word immediately after each subsequent word is received. Policies of this type are simple and easy to implement. However, they do not consider contextual information and usually result in a drop in translation accuracy. • Adaptive Policies learn to do segmentation according to dynamic contextual information. They either use a specific model to chunk the streaming source text (Sridhar et al., 2013; Oda et al., 2014; Cho and Esipova, 2016; Gu et al., 2017; Zheng et al., 2019a, 2020) or jointly learn segmentation and translation in an end-to-end framework (Arivazhagan et al., 2019; Zheng et al., 2019b; Ma et al., 2020). The adaptive methods are more flexible than the fixed ones and achieve state-of-the-art. 1 Introduction In recent years, simultaneous translation has attracted increasing interest both in research and industry community. It aims at a real-time translation that demands high translation quality and an as-short-as-possible delay between speech and translation output. A typical simultaneous translation system consists of an auto-speech-recognition (ASR"
2020.emnlp-main.570,Q18-1002,0,0.0367208,"Missing"
2020.emnlp-main.570,D18-1403,0,0.0507268,"Missing"
2020.emnlp-main.570,D17-1047,0,0.0506624,"Missing"
2020.emnlp-main.570,D16-1011,0,0.0666235,"Missing"
2020.emnlp-main.570,P19-1052,0,0.0905592,"Missing"
2020.emnlp-main.570,C18-1079,0,0.0226271,"vel sentiment classification performance. • Comprehensive experiments are conducted on the BeerAdvocate and TripAdvisor benchmark datasets. The results verify the necessity and advantages of both our framework and diversified regularizations. Meanwhile, our D-MILN outperforms previous weakly supervised methods significantly and is also comparable to the supervised method with thousands of labeled instances per aspect. 2 Related Work Document-level multi-aspect sentiment classification In previous studies, DMSC is usually done by supervised learning methods (Lei et al., 2016; Yin et al., 2017; Li et al., 2018; Wang et al., 2019), where aspect-level annotations should be provided. However, human annotation of aspectlevel sentiment is laborious and expensive, therefore, some researches focus on weakly supervised DMSC. This approach can be further categorized into knowledge-supervised and document-level supervised methods. As for knowledge-supervised methods, Zeng et al. (2019) propose to use aspectopinion word pairs as knowledge for supervision. The aspect-level sentiment classification is achieved by accomplishing another relevant objective: to predict an opinion word when given an aspect. However,"
2020.emnlp-main.570,P14-1062,0,0.0266984,"k ck qjk , where ck encodes the importance of each keyword for the given aspect: ck = P exp(wc · qjk ) k0 exp(wc · qj 0 ) (2) k and wc is the parameter to learn. Document encoding We first convert the words in the given document into a sequence of embedding vectors E = [e1 , e2 , · · · , eI ]. Usually, the sentiments are expressed through phrases in the document (Fei et al., 2004). For example, “a lovely room” expresses a positive sentiment towards the aspect room. Since one-dimension convolutional 7014 layers can serve as linguistic feature detectors to extract specific patterns of n-grams (Kalchbrenner et al., 2014), we apply several one-dimension convolutional layers on top of the word embeddings and obtain the final contextual features for the input words: H = [h1 , h2 , · · · , hI ]. Aspect-specific representations We obtain the aspect-specific representation by a weighted sum of contextual features: raj = I X αji hi (3) i=1 where αji encodes the importance of word wi to determine the sentiment towards aspect aj . αji is calculated through attention mechanism: exp(qj T Wa hi ) T i0 exp(qj Wa hi0 ) αji = P (4) where Wa is a bilinear term to capture the relevance between qj and hi . Prediction The aspec"
2020.emnlp-main.570,D14-1162,0,0.0854893,"ith negative overall sentiment and that with positive overall sentiment are balanced. Table 1 shows the statistics of the two datasets. Both datasets are split into train/development/test sets with proportions 8:1:1. The development set is used to tune the hyper-parameters for all methods. We use accuracy 7016 as the evaluation metric. Note that both aspectlevel and document-level sentiment annotations are provided in the datasets, but our D-MILN only uses document-level annotations for training. 4.2 Implementation Details We adopt the pre-trained uncased GloVe 300dimensional word embeddings (Pennington et al., 2014), which are set to be trainable during the training process3 . In document encoding, we apply three one-dimension convolutional layers with kernel widths of 3, 5, and 7 respectively4 . The number of filters is 200 for each convolutional layer. Batch normalization is applied on the output of the convolutional layers. The dimension of all hidden layers is 200. Dropout is applied on the embedding layer and the final representations of aspects and document words with dropout rate being 0.4. The values of α, β, γ in Equation 13 are 0.999, 0.1 and −0.1 respectively. The probabilistic margin t is 0.7"
2020.emnlp-main.570,2020.acl-main.374,1,0.84929,"Missing"
2020.emnlp-main.570,D19-1560,0,0.0187428,"ssification performance. • Comprehensive experiments are conducted on the BeerAdvocate and TripAdvisor benchmark datasets. The results verify the necessity and advantages of both our framework and diversified regularizations. Meanwhile, our D-MILN outperforms previous weakly supervised methods significantly and is also comparable to the supervised method with thousands of labeled instances per aspect. 2 Related Work Document-level multi-aspect sentiment classification In previous studies, DMSC is usually done by supervised learning methods (Lei et al., 2016; Yin et al., 2017; Li et al., 2018; Wang et al., 2019), where aspect-level annotations should be provided. However, human annotation of aspectlevel sentiment is laborious and expensive, therefore, some researches focus on weakly supervised DMSC. This approach can be further categorized into knowledge-supervised and document-level supervised methods. As for knowledge-supervised methods, Zeng et al. (2019) propose to use aspectopinion word pairs as knowledge for supervision. The aspect-level sentiment classification is achieved by accomplishing another relevant objective: to predict an opinion word when given an aspect. However, their model heavily"
2020.emnlp-main.570,2020.acl-main.295,0,0.0191515,"eA great location to stay at. The room was ordinary ingthe overly consistent with document-level senbathroom looked like an after thought, the timent. Experimental results shower was extremely small. Alsoon theTripAdvifront desk sorclerks and BeerAdvocate datasets that Dprovided minimum service.show Besides, the price remarkably is very expensive. MILN outperforms recent weaklysupervised baselines, and is also comparable Ratings to the supervised method. overall: 1 location: 0.85 room: Introduction 0.21 overall: location: 0.15 value: location:+ room: - corresponding author. service: 1 value: Wang et al., 2020). Despite the advantages, the acquisition of aspect-level sentiment annotations remains a laborious and expensive endeavor. Fortunately, the overall document-level sentiment annotations are relatively easy to obtain thanks to the widespread online reviews with overall star ratings. Therefore, it is practically meaningful to perform DMSC by weak supervision from document-level sentiment signals. However, this problem is far from solved. To the best of our knowledge, there is no neural model that is able to achieve DMSC with only document-level signals. There are mainly two challenges need to be"
2020.emnlp-main.570,P18-1234,0,0.0373356,"Missing"
2020.emnlp-main.570,D17-1217,0,0.0492225,"Missing"
2020.emnlp-main.570,N19-1036,0,0.331244,"hod with thousands of labeled instances per aspect. 2 Related Work Document-level multi-aspect sentiment classification In previous studies, DMSC is usually done by supervised learning methods (Lei et al., 2016; Yin et al., 2017; Li et al., 2018; Wang et al., 2019), where aspect-level annotations should be provided. However, human annotation of aspectlevel sentiment is laborious and expensive, therefore, some researches focus on weakly supervised DMSC. This approach can be further categorized into knowledge-supervised and document-level supervised methods. As for knowledge-supervised methods, Zeng et al. (2019) propose to use aspectopinion word pairs as knowledge for supervision. The aspect-level sentiment classification is achieved by accomplishing another relevant objective: to predict an opinion word when given an aspect. However, their model heavily depends on the performance of dependency parsing and manually designed rules. As for document-level supervised methods, Wang et al. (2010, 2011) propose to use the document-level sentiment as supervision which is similar to ours. Specifically, they propose a probabilistic graphical model for the task, which assumes the overall rating is generated bas"
2020.findings-emnlp.69,W12-3010,0,0.865822,"Missing"
2020.findings-emnlp.69,P15-1034,0,0.440267,", ..., hm ) = BiLST M (x1 , x2 , ..., xm ); 3) predict the probability of assigning label yi to a word wi using a fully connected feedforward classifier: P (ˆ yi |S, p, wi ) = sof tmax(W hi + b); 4) finally decode the full label sequence Yˆ using a beamsearch algorithm, e.g., RnnOIE will decode the label sequence [B-ARG1 , B-P, B-ARG2 , IARG2 , I-ARG2 , I-ARG2 , O, O, O, O, O] to extract (Parragon; operates; more than 35 markets). In open IE, all extracted tuples are ranked according to their confidence scores, which is important for downstream tasks, such as QA (Fader et al., 2011) and KBP (Angeli et al., 2015). RnnOIE uses average log probabilities as the confidence of an extracted tuple: Pm logP (ˆ yi |S, p, wi ) ˆ c(S, p, Y ) = i=1 (1) m Given a training corpus, RnnOIE can be supervisedly learned by maximum log-likelihood estiFigure 3: An overview of syntactic patterns as data labelling functions. Two training instances are automatically generated using dependency pattern for predicates “operates” and “has”. mation (MLE): log P (Y|S, p) = m X log P (yi |S, p, wi ) (2) i=1 where Y = (y1 , y2 , ..., ym ) are the gold labels. As discussed above, Y are expensive and labourintensive to obtain and have"
2020.findings-emnlp.69,D15-1075,0,0.042156,"incorrect. For semantic consistency, given an extracted relation and its original sentence, Sem(Yˆ , S) is computed as: Sem(Yˆ , S) = P (positive|Yˆ , S) 785 (6) where P (positive|Yˆ , S) is the semantic similarity between the predicted label sequence Yˆ and its original sentence S. This paper estimates this semantic similarity using a BERT-based classifier, which assigns a similarity score to each sentencetuple pair. Because multiple tuples can be extracted from a single sentence (see Figure 3 for example), we train the classifier using the Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015), so that a high similarity score will be assigned if the original sentence entails the extracted tuple. This semantic consistency can provide useful supervision signals for open IE models. For example, because (Parragon; has; 10 offices) has higher semantic similarity than (has; 10 offices) to sentence “Parragon operates more than 35 markets and has 10 offices.”, the model will be guided to more complete extractions. Semantic-Based Confidence Estimation. In RnnOIE, the confidence score c(S, p, Yˆ ) is estimated only using extraction probabilities. This paper further considers the semantic con"
2020.findings-emnlp.69,P18-2065,0,0.550639,", 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019; Roy et al., 2019) or generating the target tuples token-by-token (Zhang et al., 2017; Cui et al., 2018; Sun et al., 2018). For example, to extract (Parragon; operates; more than 35 markets), neural open IE systems will label the sentence as [B-ARG1 , B-P, B-ARG2 , I-ARG2 , IARG2 , I-ARG2 , O, O, O, O, O] or generate a token sequence [&lt;ARG1 &gt;, Parragon, &lt;P &gt;, operates, &lt;ARG2 &gt;, more, than, 35, markets]. The neural open IE systems, unfortunately, rely on the large labelled corpus to achieve good performance, which is often expensive and labourintensive to obtain. Furthermore, open IE needs to extract relations of unlimited types from open domain corpus, which further exacerbates the need for lar"
2020.findings-emnlp.69,N19-1423,0,0.406093,"ency between a tuple and its original sentence. For example, Figure 2 shows the ARG1 “Parragon” and the ARG2 “more than 35 markets” follow the nsubj and dobj dependency structure, respectively. Meanwhile, the extracted tuple (Parragon; operates; more than 35 markets) has a high semantic similarity with its original sentence “Parragon operates more than 35 markets and has 10 offices.”. And we found that the syntactic regularities can be effectively captured using syntactic rules, and the semantic consistency can be effectively modelled using the recent powerful pre-trained models such as BERT (Devlin et al., 2019). Based on the above observations, we propose two learning strategies to exploit syntactic and semantic knowledge for model learning. Figure 1 illustrates the framework of our method. Firstly, syntactic open IE patterns are used as data labelling functions, and a base model is pretrained using the noisy training corpus generated by these labelling functions. Secondly, because the pattern-based labels are often noisy and with limited coverage, we further propose a reinforcement learning algorithm which uses syntactic and semantic-driven reward functions, which can effectively generalize the bas"
2020.findings-emnlp.69,D11-1142,0,0.881246,"is used to generalize the base model to open situations. Open information extraction (Open IE) aims to extract open-domain textual tuples consisting of a predicate and a set of arguments from massive and heterogeneous corpora (Sekine, 2006; Banko et al., 2007). For example, a system will extract a tuple (Parragon; operates; more than 35 markets) from the sentence “Parragon operates more than 35 markets and has 10 offices.”. In contrary to the traditional IE, open IE is completely domain-independent and does not require the predetermined relations. Recently, open IE has gained much attention (Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Corro and Gemulla, 2013; Moro and Navigli, 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019"
2020.findings-emnlp.69,D15-1076,0,0.0307563,"RL. By modelling the extraction task as a Markov Decision Process (MDP), we have the following definitions: &lt; S, A, T , R &gt;: Reward Function. The reward function, i.e., the goodness of extracted tuples, is critical in our RL algorithm. This paper estimates the reward R(Yˆ , S) by considering both syntactic constraint and semantic consistency: 2.3 2 The dependency relations are defined https://nlp.stanford.edu/software/ dependencies_manual.pdf in R(Yˆ , S) = Syn(Yˆ ) ∗ Sem(Yˆ , S) (4) where Syn(Yˆ ) is the syntactic constraint score and Sem(Yˆ , S) is the semantic consistency score. Following He et al. (2015); Stanovsky et al. (2018); Jiang et al. (2019), we judge an extracted tuple as correct if and only if it’s predicate and arguments include their corresponding syntactic headwords (Headwords Match). Otherwise, the extracted tuples are judged as incorrect. That is: ( 1, Headwords Match Syn(Yˆ ) = (5) −1, Else where 1 means the predicted label sequence Yˆ is correct and -1 for incorrect. For semantic consistency, given an extracted relation and its original sentence, Sem(Yˆ , S) is computed as: Sem(Yˆ , S) = P (positive|Yˆ , S) 785 (6) where P (positive|Yˆ , S) is the semantic similarity between"
2020.findings-emnlp.69,P19-1523,0,0.652449,"Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Corro and Gemulla, 2013; Moro and Navigli, 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019; Roy et al., 2019) or generating the target tuples token-by-token (Zhang et al., 2017; Cui et al., 2018; Sun et al., 2018). For example, to extract (Parragon; operates; more than 35 markets), neural open IE systems will label the sentence as [B-ARG1 , B-P, B-ARG2 , I-ARG2 , IARG2 , I-ARG2 , O, O, O, O, O] or generate a token sequence [&lt;ARG1 &gt;, Parragon, &lt;P &gt;, operates, &lt;ARG2 &gt;, more, than, 35, markets]. The neural open IE systems, unfortunately, rely on the large labelled corpus to achieve good performance, which is often expensive and labourintensive to obtain. Furthermore, open IE needs to"
2020.findings-emnlp.69,D12-1048,0,0.898816,"n situations. Open information extraction (Open IE) aims to extract open-domain textual tuples consisting of a predicate and a set of arguments from massive and heterogeneous corpora (Sekine, 2006; Banko et al., 2007). For example, a system will extract a tuple (Parragon; operates; more than 35 markets) from the sentence “Parragon operates more than 35 markets and has 10 offices.”. In contrary to the traditional IE, open IE is completely domain-independent and does not require the predetermined relations. Recently, open IE has gained much attention (Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Corro and Gemulla, 2013; Moro and Navigli, 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019; Roy et al., 2019) or generating the target"
2020.findings-emnlp.69,D13-1043,0,0.611067,"ic and semantic-driven reward functions, which can effectively generalize the base model to open situations with high accuracy. These two strategies together will ensure the effective learning of open IE models: the data labelling function can pretrain a reasonable initial model so that the RL algorithm can optimize model more effectively; although the pattern-based labels are often noisy and with low coverage, the RL algorithm can generalize the model to open situations with high accuracy. We conducted experiments on three open IE benchmarks: OIE2016 (Stanovsky and Dagan, 2016), WEB and NYT (Mesquita et al., 2013). Experimental results show that the proposed framework significantly outperforms the supervised counterparts, and can even achieve competitive performance with the supervised SoA approach. 1 The main contributions of this paper are: • We propose a syntactic and semantic-driven learning algorithm which can leverage syntactic and semantic knowledge as noisier, higherlevel supervisions and learn neural open IE models without any human-labelled data. • We design two effective learning strategies for exploiting syntactic and semantic knowledge as supervisions: one is to use as data labelling funct"
2020.findings-emnlp.69,P09-1113,0,0.205826,"and many syntactic patterns have been designed for extracting tuples, such as TEXTRUNNER (Banko et al., 2007) and ReVerb (Fader et al., 2011). However, it is difficult to design high coverage syntactic patterns, although many extensions have been proposed, such as WOE (Wu and Weld, 2010), OLLIE (Mausam et al., 2012), ClausIE (Corro and Gemulla, 2013), Standford Open IE (Angeli et al., 2015), PropS (Stanovsky et al., 2016) and OpenIE4 (Mausam, 2016). This paper leverages the power of patterns differently. Inspired by the ideas of data programming (Ratner et al., 2016) and distant supervision (Mintz et al., 2009), we use syntactic patterns 784 • S = {s} are states used to capture the information from the current sentence. Specifically, S are hidden states H obtained by stacked BiLSTM. as data labelling functions, rather than to directly extracting tuples. Concretely, this paper uses dependency patterns from Standford Open IE (Angeli et al., 2015) to design hand-crafted patterns as data labelling functions. As shown in Figure 3, given a sentence and its dependency parse, two training instances are generated: 1) We first identify all its predicates using part of speech (POS) tags. For example, “operates"
2020.findings-emnlp.69,D16-1261,0,0.112935,"en-domain textual tuples consisting of a predicate and a set of arguments from massive and heterogeneous corpora (Sekine, 2006; Banko et al., 2007). For example, a system will extract a tuple (Parragon; operates; more than 35 markets) from the sentence “Parragon operates more than 35 markets and has 10 offices.”. In contrary to the traditional IE, open IE is completely domain-independent and does not require the predetermined relations. Recently, open IE has gained much attention (Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Corro and Gemulla, 2013; Moro and Navigli, 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019; Roy et al., 2019) or generating the target tuples token-by-token (Zhang et al., 2017; Cui et al., 2018; Sun et al., 2"
2020.findings-emnlp.69,W16-1307,0,0.0191463,"e (Parragon; operates; more than 35 markets) from the sentence “Parragon operates more than 35 markets and has 10 offices.”. In contrary to the traditional IE, open IE is completely domain-independent and does not require the predetermined relations. Recently, open IE has gained much attention (Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Corro and Gemulla, 2013; Moro and Navigli, 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019; Roy et al., 2019) or generating the target tuples token-by-token (Zhang et al., 2017; Cui et al., 2018; Sun et al., 2018). For example, to extract (Parragon; operates; more than 35 markets), neural open IE systems will label the sentence as [B-ARG1 , B-P, B-ARG2 , I-ARG2 , IARG2 , I-ARG2 , O, O, O, O, O"
2020.findings-emnlp.69,P18-1046,0,0.0201502,"sers’ expressions or domain heuristics as a generative model. Distant supervision paradigm (Mintz et al., 2009) heuristically generates labelled dataset by aligning facts in KB with sentences in the corpus. The proposed data labelling functions are also motivated by the ideas of data programming and distant supervision. Reinforcement Learning for IE. Reinforcement learning (RL) (Sutton and Barto, 1998) follows the explore and exploit paradigm and is apt for optimizing non-derivative learning objectives in NLP (Wu et al., 2018). Recently, RL has gained much attention in information extraction (Qin et al., 2018b,a; Takanobu et al., 2019). In open IE, Narasimhan et al. (2016) firstly using traditional Q-learning method to extract textual tuples. However, their reward function is chosen to maximize the final extraction accuracy which still relies on human-labelled datasets and can not capture the syntactic and semantic supervisions explicitly. 5 Conclusions This paper proposes an open IE learning approach, which can learn neural models without any humanlabelled data by leveraging syntactic and semantic knowledge as noisier, higher-level supervisions. Specifically, two effective learning strategies are"
2020.findings-emnlp.69,P18-1199,0,0.0143311,"sers’ expressions or domain heuristics as a generative model. Distant supervision paradigm (Mintz et al., 2009) heuristically generates labelled dataset by aligning facts in KB with sentences in the corpus. The proposed data labelling functions are also motivated by the ideas of data programming and distant supervision. Reinforcement Learning for IE. Reinforcement learning (RL) (Sutton and Barto, 1998) follows the explore and exploit paradigm and is apt for optimizing non-derivative learning objectives in NLP (Wu et al., 2018). Recently, RL has gained much attention in information extraction (Qin et al., 2018b,a; Takanobu et al., 2019). In open IE, Narasimhan et al. (2016) firstly using traditional Q-learning method to extract textual tuples. However, their reward function is chosen to maximize the final extraction accuracy which still relies on human-labelled datasets and can not capture the syntactic and semantic supervisions explicitly. 5 Conclusions This paper proposes an open IE learning approach, which can learn neural models without any humanlabelled data by leveraging syntactic and semantic knowledge as noisier, higher-level supervisions. Specifically, two effective learning strategies are"
2020.findings-emnlp.69,D19-1067,0,0.729588,"Akbik and L¨oser, 2012; Mausam et al., 2012; Corro and Gemulla, 2013; Moro and Navigli, 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019; Roy et al., 2019) or generating the target tuples token-by-token (Zhang et al., 2017; Cui et al., 2018; Sun et al., 2018). For example, to extract (Parragon; operates; more than 35 markets), neural open IE systems will label the sentence as [B-ARG1 , B-P, B-ARG2 , I-ARG2 , IARG2 , I-ARG2 , O, O, O, O, O] or generate a token sequence [&lt;ARG1 &gt;, Parragon, &lt;P &gt;, operates, &lt;ARG2 &gt;, more, than, 35, markets]. The neural open IE systems, unfortunately, rely on the large labelled corpus to achieve good performance, which is often expensive and labourintensive to obtain. Furthermore, open IE needs to extract relations o"
2020.findings-emnlp.69,P06-2094,0,0.0212662,"ta labeling Noisy Training Corpus [Parragon]ARG1 [operates]P [more than 35 markets]ARG2 and has 10 offices. Open IE Model Figure 1: The proposed open IE framework, which consists of two learning strategies: 1) syntactic patterns are used as data labelling functions and a base model is pretrained using the generated labels; 2) a syntactic and semantic-driven RL algorithm is used to generalize the base model to open situations. Open information extraction (Open IE) aims to extract open-domain textual tuples consisting of a predicate and a set of arguments from massive and heterogeneous corpora (Sekine, 2006; Banko et al., 2007). For example, a system will extract a tuple (Parragon; operates; more than 35 markets) from the sentence “Parragon operates more than 35 markets and has 10 offices.”. In contrary to the traditional IE, open IE is completely domain-independent and does not require the predetermined relations. Recently, open IE has gained much attention (Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Corro and Gemulla, 2013; Moro and Navigli, 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introducti"
2020.findings-emnlp.69,D16-1252,0,0.0668223,"ent learning algorithm which uses syntactic and semantic-driven reward functions, which can effectively generalize the base model to open situations with high accuracy. These two strategies together will ensure the effective learning of open IE models: the data labelling function can pretrain a reasonable initial model so that the RL algorithm can optimize model more effectively; although the pattern-based labels are often noisy and with low coverage, the RL algorithm can generalize the model to open situations with high accuracy. We conducted experiments on three open IE benchmarks: OIE2016 (Stanovsky and Dagan, 2016), WEB and NYT (Mesquita et al., 2013). Experimental results show that the proposed framework significantly outperforms the supervised counterparts, and can even achieve competitive performance with the supervised SoA approach. 1 The main contributions of this paper are: • We propose a syntactic and semantic-driven learning algorithm which can leverage syntactic and semantic knowledge as noisier, higherlevel supervisions and learn neural open IE models without any human-labelled data. • We design two effective learning strategies for exploiting syntactic and semantic knowledge as supervisions:"
2020.findings-emnlp.69,N18-1081,0,0.201594,"gained much attention (Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Corro and Gemulla, 2013; Moro and Navigli, 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019; Roy et al., 2019) or generating the target tuples token-by-token (Zhang et al., 2017; Cui et al., 2018; Sun et al., 2018). For example, to extract (Parragon; operates; more than 35 markets), neural open IE systems will label the sentence as [B-ARG1 , B-P, B-ARG2 , I-ARG2 , IARG2 , I-ARG2 , O, O, O, O, O] or generate a token sequence [&lt;ARG1 &gt;, Parragon, &lt;P &gt;, operates, &lt;ARG2 &gt;, more, than, 35, markets]. The neural open IE systems, unfortunately, rely on the large labelled corpus to achieve good performance, which is often expensive and labourintensive to obtain. Furthermor"
2020.findings-emnlp.69,D18-1236,0,0.0952876,"et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019; Roy et al., 2019) or generating the target tuples token-by-token (Zhang et al., 2017; Cui et al., 2018; Sun et al., 2018). For example, to extract (Parragon; operates; more than 35 markets), neural open IE systems will label the sentence as [B-ARG1 , B-P, B-ARG2 , I-ARG2 , IARG2 , I-ARG2 , O, O, O, O, O] or generate a token sequence [&lt;ARG1 &gt;, Parragon, &lt;P &gt;, operates, &lt;ARG2 &gt;, more, than, 35, markets]. The neural open IE systems, unfortunately, rely on the large labelled corpus to achieve good performance, which is often expensive and labourintensive to obtain. Furthermore, open IE needs to extract relations of unlimited types from open domain corpus, which further exacerbates the need for large labelled corpus."
2020.findings-emnlp.69,P10-1013,0,0.819567,"ning using Syntactic Pattern-based Data Labelling Functions The first strategy is to use syntactic extraction patterns as data labelling functions, and then the heuristically labelled training corpus will be used to pretrain a neural open IE model. It has long been observed that most relation tuples follow syntactic regularity, and many syntactic patterns have been designed for extracting tuples, such as TEXTRUNNER (Banko et al., 2007) and ReVerb (Fader et al., 2011). However, it is difficult to design high coverage syntactic patterns, although many extensions have been proposed, such as WOE (Wu and Weld, 2010), OLLIE (Mausam et al., 2012), ClausIE (Corro and Gemulla, 2013), Standford Open IE (Angeli et al., 2015), PropS (Stanovsky et al., 2016) and OpenIE4 (Mausam, 2016). This paper leverages the power of patterns differently. Inspired by the ideas of data programming (Ratner et al., 2016) and distant supervision (Mintz et al., 2009), we use syntactic patterns 784 • S = {s} are states used to capture the information from the current sentence. Specifically, S are hidden states H obtained by stacked BiLSTM. as data labelling functions, rather than to directly extracting tuples. Concretely, this paper"
2020.findings-emnlp.69,D18-1397,0,0.0286055,"aradigm (Ratner et al., 2016) creates training datasets by explicitly representing users’ expressions or domain heuristics as a generative model. Distant supervision paradigm (Mintz et al., 2009) heuristically generates labelled dataset by aligning facts in KB with sentences in the corpus. The proposed data labelling functions are also motivated by the ideas of data programming and distant supervision. Reinforcement Learning for IE. Reinforcement learning (RL) (Sutton and Barto, 1998) follows the explore and exploit paradigm and is apt for optimizing non-derivative learning objectives in NLP (Wu et al., 2018). Recently, RL has gained much attention in information extraction (Qin et al., 2018b,a; Takanobu et al., 2019). In open IE, Narasimhan et al. (2016) firstly using traditional Q-learning method to extract textual tuples. However, their reward function is chosen to maximize the final extraction accuracy which still relies on human-labelled datasets and can not capture the syntactic and semantic supervisions explicitly. 5 Conclusions This paper proposes an open IE learning approach, which can learn neural models without any humanlabelled data by leveraging syntactic and semantic knowledge as noi"
2020.findings-emnlp.69,I17-1086,0,0.0139316,"from the sentence “Parragon operates more than 35 markets and has 10 offices.”. In contrary to the traditional IE, open IE is completely domain-independent and does not require the predetermined relations. Recently, open IE has gained much attention (Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Corro and Gemulla, 2013; Moro and Navigli, 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019; Roy et al., 2019) or generating the target tuples token-by-token (Zhang et al., 2017; Cui et al., 2018; Sun et al., 2018). For example, to extract (Parragon; operates; more than 35 markets), neural open IE systems will label the sentence as [B-ARG1 , B-P, B-ARG2 , I-ARG2 , IARG2 , I-ARG2 , O, O, O, O, O] or generate a token sequence [&lt;ARG1 &gt;,"
2020.findings-emnlp.69,E17-2011,0,0.0526786,"Missing"
2021.acl-long.136,N16-1014,0,0.0183544,"ces and information inconsistency. “0” means that there are more than two incoherence errors in a session. “1” means that there are only one error. “2” means that there are no errors. Finally, we compute the average score of all the sessions. (2) Dialog engagement (Enga.) This metric measures how interesting a dialogs is. It is “1” if a dialog is interesting and the human is willing to continue the conversation, otherwise “0”. (3) Length of high-quality dialog (Length) A high-quality dialog ends if the model tends to produce dull responses or two consecutive utterances are highly overlapping (Li et al., 2016b). Single-turn Metrics. We use the following metrics: (1) Single-turn Coherence (Single.T.-Coh.) “0” if a response is inappropriate as an reply, otherwise “1”; (2) Informativeness (Info.) “0” if a response is a “safe” response, e.g. “I don’t know”, or it is highly overlapped with context, otherwise “1”; (3) Distinct (Dist.-i) It is an automatic metric for response diversity (Li et al., 2016a). 5.3 Experiment Results As shown in Table 2, GCS significantly outperforms all the baselines in terms of all the metrics except “Length-of-dialog” (sign test, p-value &lt; 0.01). It indicates that GCS can g"
2021.acl-long.136,D16-1127,0,0.0292141,"ces and information inconsistency. “0” means that there are more than two incoherence errors in a session. “1” means that there are only one error. “2” means that there are no errors. Finally, we compute the average score of all the sessions. (2) Dialog engagement (Enga.) This metric measures how interesting a dialogs is. It is “1” if a dialog is interesting and the human is willing to continue the conversation, otherwise “0”. (3) Length of high-quality dialog (Length) A high-quality dialog ends if the model tends to produce dull responses or two consecutive utterances are highly overlapping (Li et al., 2016b). Single-turn Metrics. We use the following metrics: (1) Single-turn Coherence (Single.T.-Coh.) “0” if a response is inappropriate as an reply, otherwise “1”; (2) Informativeness (Info.) “0” if a response is a “safe” response, e.g. “I don’t know”, or it is highly overlapped with context, otherwise “1”; (3) Distinct (Dist.-i) It is an automatic metric for response diversity (Li et al., 2016a). 5.3 Experiment Results As shown in Table 2, GCS significantly outperforms all the baselines in terms of all the metrics except “Length-of-dialog” (sign test, p-value &lt; 0.01). It indicates that GCS can g"
2021.acl-long.136,D19-1187,1,0.845921,"models (Chotimongkol, 2008; Ritter et al., 2010; Zhai and Williams, 2014) or variational auto-encoder (Shi et al., 2019). However, the number of their dialog states is limited to only dozens or hundreds, which cannot cover fine-grained semantics in chitchat. Moreover, our method can discover a hierarchical dialog structure, which is different from the non-hierarchical dialog structures in most previous work. 2.2 Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Moghe et al., 2018; Dinan et al., 2019; Liu et al., 2019; Xu et al., 2020c,a). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn dialog modeling. 2.3 Latent variable models for chitchat Recently, latent variables are utilized to improve diversity (Serban et al., 2017; Zhao et al., 2017; Gu et al., 2019; Gao et al., 2019; Ghandeharioun et al., 2019), control responding styles (Zhao et al., 2018; Li et al., 2020) and incorporate knowledge (Kim et al., 2020) in dialogs. Our work differs from 1727 4 ai.baidu.com/tech/nlp basic/dependency parsing Em"
2021.acl-long.136,D18-1255,0,0.0143329,"task-oriented dialogs via hidden Markov models (Chotimongkol, 2008; Ritter et al., 2010; Zhai and Williams, 2014) or variational auto-encoder (Shi et al., 2019). However, the number of their dialog states is limited to only dozens or hundreds, which cannot cover fine-grained semantics in chitchat. Moreover, our method can discover a hierarchical dialog structure, which is different from the non-hierarchical dialog structures in most previous work. 2.2 Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Moghe et al., 2018; Dinan et al., 2019; Liu et al., 2019; Xu et al., 2020c,a). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn dialog modeling. 2.3 Latent variable models for chitchat Recently, latent variables are utilized to improve diversity (Serban et al., 2017; Zhao et al., 2017; Gu et al., 2019; Gao et al., 2019; Ghandeharioun et al., 2019), control responding styles (Zhao et al., 2018; Li et al., 2020) and incorporate knowledge (Kim et al., 2020) in dialogs. Our work differs from 1727 4 ai.baidu.co"
2021.acl-long.136,P02-1040,0,0.110099,"evaluate the quality of the initialized graph (denoted as Phrase Graph) that consists of only phrases (as vertices) and initial edges (between phrases) in Section 3.2. For more details, please refer to Appendix A.1. 4.2 Evaluation Metrics We evaluate discovered dialog structure graph with both automatic evaluation and human evaluation. For automatic evaluation, we use two metrics to evaluate the performance of reconstruction: (1) NLL is the negative log likelihood of dialog utterances; (2) BLEU-1/2 measures how much that reconstructed sentences contains 1/2-gram overlaps with input sentences (Papineni et al., 2002). The two metrics indicate how well the learned dialog structure graph can capture important semantic information in dialog dataset. Further, we manually evaluate the quality of edges and vertices in the graph. For edges, (1) S-U Appr. for multi-turn dialog coherence. It measures the appropriateness of Sess-Utter edges, where these edges provide crucial prior information to ensure multi-turn dialog coherence (see results in Section 5.4). “1” if an utterance-level vertex is relevant to its session-level vertex (topic), otherwise “0”. (2) U-U Appr. for single-turn dialog coherence: It measures t"
2021.acl-long.136,N10-1020,0,0.0678486,". (2) we propose a novel model, DVAE-GNN, for hierarchical dialog struc3 Co-occurrence means that two utterance-level vertices are mapped by two adjacent utterances in a session. 7 预定了酒店 Have booked a hotel room 找到房子了么 Have you found a place to live? Speak 1: 嗯，我 提前订好了酒店。 [Yes, I have booked a hotel in advance.] Response Utterance-level semantic vertex Related Work 2.1 Utterance-level semantic vertex 6 Dialog structure learning for task-oriented dialogs There are previous work on discovering humanreadable dialog structure for task-oriented dialogs via hidden Markov models (Chotimongkol, 2008; Ritter et al., 2010; Zhai and Williams, 2014) or variational auto-encoder (Shi et al., 2019). However, the number of their dialog states is limited to only dozens or hundreds, which cannot cover fine-grained semantics in chitchat. Moreover, our method can discover a hierarchical dialog structure, which is different from the non-hierarchical dialog structures in most previous work. 2.2 Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Moghe et al., 2018; Dinan et al., 2019; Liu et al., 2019; Xu et al., 2020c,a). In this"
2021.acl-long.136,2020.emnlp-main.378,0,0.027344,"e growing interests in leveraging knowledge bases for generation of more informative responses (Moghe et al., 2018; Dinan et al., 2019; Liu et al., 2019; Xu et al., 2020c,a). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn dialog modeling. 2.3 Latent variable models for chitchat Recently, latent variables are utilized to improve diversity (Serban et al., 2017; Zhao et al., 2017; Gu et al., 2019; Gao et al., 2019; Ghandeharioun et al., 2019), control responding styles (Zhao et al., 2018; Li et al., 2020) and incorporate knowledge (Kim et al., 2020) in dialogs. Our work differs from 1727 4 ai.baidu.com/tech/nlp basic/dependency parsing Embedding space of session-level semantic vertices … Embedding space of utterance-level semantic vertices … Vectors of session-level semantic vertices Vectors of utterance-level semantic vertices Vectors of utterance phrases 放假过来找我玩啊 [Let‘s gather on holiday] 好啊，好久没见面了 [Yep, long time no see] 我明天准备去长沙上班 [I’ll go to Changsha tomorrow] 1 放假过来找我玩啊 [Let‘s gather on holiday] 1 3 2 RNN Encoder FFN 1 GNN 2 5 1 你租房子了么 [Oh, have you rent a room yet?] 2 3 4 Emb RNN Decode"
2021.acl-long.136,P15-1152,0,0.0107824,"hierarchical latent dialog states (at the level of both session and utterance) and their transitions from corpus as a dialog structure graph. Then we leverage it as background knowledge to facilitate dialog management in a RL based dialog system. Experimental results on two benchmark corpora confirm that DVAE-GNN can discover meaningful dialog structure graph, and the use of dialog structure as background knowledge can significantly improve multi-turn coherence. 1 Introduction With the aim of building a machine to converse with humans naturally, some work investigate neural generative models (Shang et al., 2015; Serban et al., 2017). While these models can generate locally relevant dialogs, they struggle to organize individual utterances into globally coherent flow (Yu et al., 2016; Xu et al., 2020b). The possible reason is that it is difficult to control the overall dialog flow without background knowledge about dialog structure.1 However, due to the complexity of opendomain conversation, it is laborious and costly to annotate dialog structure manually. Therefore, it is ∗ Equal contribution. Corresponding author: Wanxiang Che. 1 Dialog structure means dialog states and their transitions. of great i"
2021.acl-long.136,N19-1178,0,0.326662,"The possible reason is that it is difficult to control the overall dialog flow without background knowledge about dialog structure.1 However, due to the complexity of opendomain conversation, it is laborious and costly to annotate dialog structure manually. Therefore, it is ∗ Equal contribution. Corresponding author: Wanxiang Che. 1 Dialog structure means dialog states and their transitions. of great importance to discover open-domain dialog structure from corpus in an unsupervised way for coherent dialog generation. Some studies tried to discover dialog structure from task-oriented dialogs (Shi et al., 2019). However, the number of their dialog states is limited to only dozens or hundreds, which cannot cover finegrained semantics in open-domain dialogs. Furthermore, the dialog structures they discovered generally only contain utterance-level semantics (non-hierarchical), without session-level semantics (chatting topics) that are essential in open-domain dialogs (Wu et al., 2019; Kang et al., 2019; Xu et al., 2020c).2 Thus, in order to provide a full picture of open-domain dialog structure, it is desirable to discover a two-layer directed graph that contains session-level semantics in the upper-la"
2021.acl-long.136,P19-1371,0,0.0113493,"in Appendix B. 5 Experiments for Graph Grounded Dialog Generation To confirm the benefits of discovered dialog structure graph for coherent conversation generation, we conduct experiments on the graph discovered from Weibo corpus. All the systems (including baselines) are trained on Weibo corpus. 5.1 Models We carefully select the following six baselines. MMPMS It is the multi-mapping based neural open-domain conversational model with posterior mapping selection mechanism (Chen et al., 2019), which is a SOTA model on the Weibo Corpus. MemGM It is the memory-augmented opendomain dialog model (Tian et al., 2019), which learns to cluster U-R pairs for response generation. HRED It is the hierarchical recurrent encoderdecoder model (Serban et al., 2016). CVAE It is the Conditional Variational AutoEncoder based neural open-domain conversational model (Zhao et al., 2017). VHCR-EI This variational hierarchical RNN model can learn hierarchical latent variables from open-domain dialogs (Ghandeharioun et al., 2019). It is a SOTA dialog model with hierarchical VAE. DVRNN-RL It discovers dialog structure graph for task-oriented dialog modeling (Shi et al., 2019). GCS It is our proposed dialog structure graph gr"
2021.acl-long.136,P19-1369,1,0.807053,"eir transitions. of great importance to discover open-domain dialog structure from corpus in an unsupervised way for coherent dialog generation. Some studies tried to discover dialog structure from task-oriented dialogs (Shi et al., 2019). However, the number of their dialog states is limited to only dozens or hundreds, which cannot cover finegrained semantics in open-domain dialogs. Furthermore, the dialog structures they discovered generally only contain utterance-level semantics (non-hierarchical), without session-level semantics (chatting topics) that are essential in open-domain dialogs (Wu et al., 2019; Kang et al., 2019; Xu et al., 2020c).2 Thus, in order to provide a full picture of open-domain dialog structure, it is desirable to discover a two-layer directed graph that contains session-level semantics in the upper-layer vertices, utterance-level semantics in the lower-layer vertices, and edges among these vertices. In this paper, we propose a novel discrete variational auto-encoder with graph neural network (DVAE-GNN) to discover a two-layer dialog structure from chitchat corpus. Intuitively, since discrete dialog states are easier to capture transitions for dialog coherence, we use dis"
2021.acl-long.202,2020.acl-main.703,0,0.0295517,"rained on the ImageNet dataset. Recently, contrastive self-supervised learning like SimCLR (Chen et al., 2020a) and MoCo (He et al., 2020) also greatly improve the performance of visual representation learning. These pre-trained models only focus on visual tasks (e.g. image classification etc.), however, they cannot be used in textual or multimodal (i.e., with both text and image) tasks. The language pre-training methods based on the Transformer architecture are also very popular in NLP models, such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and BART (Lewis et al., 2020). However, they mainly focus on textual tasks. They cannot effectively deal with the multi-modal tasks, such as image-text retrieval, image captioning, multimodal machine translation (Lin et al., 2020a; Su et al., 2021) and visual dialog (Murahari et al., 2020). Multi-Modal Pre-training Recently, multimodal pre-training methods have been more and more popular for solving the multi-modal tasks. All of them are trained on a corpus of image-text pairs, such as ViLBERT (Lu et al., 2019), VisualBERT (Li et al., 2019b), VL-BERT (Su et al., 2019), Unicoder-VL (Li et al., 2019a) and UNITER (Chen et al"
2021.acl-long.202,2021.ccl-1.108,0,0.0544888,"Missing"
2021.acl-long.202,D16-1264,0,0.0350289,"UNIMO-base by initializing from RoBERTa-base, and UNIMO-large by initializing from RoBERTa-large. Both UNIMObase and UNIMO-large are trained for at least 500K steps. An Adam optimizer with initial learning rate 3.3 Finetuning Tasks We fine-tune our model on two categories of downstream tasks: (1) single-modal language understanding and generation tasks; (2) multimodal vision-language understanding and generation tasks. The single-modal generation tasks include: generative conversational question answering on the CoQA dataset (Reddy et al., 2019), question generation on the SQuAD 1.1 dataset (Rajpurkar et al., 2016), abstractive summarization on the CNN/DailyMail (CNNDM) dataset (Hermann et al., 2015), and sentence compression on the Gigaword dataset (Rush et al., 2015). The single-modal understanding tasks include: sentiment classification on the SST-2 dataset (Socher et al., 2013), natural language inference on the MNLI dataset (Williams et al., 2017), linguistic acceptability analysis on the CoLA dataset (Warstadt et al., 2019) and semantic similarity analysis on the STS-B dataset (Cer et al., 2017). The multi-modal tasks include: visual question answering (VQA) on the VQA v2.0 dataset (Goyal et al.,"
2021.acl-long.202,Q19-1016,0,0.0140192,"-region features are set as 512 and 100, respectively. We pre-train UNIMO-base by initializing from RoBERTa-base, and UNIMO-large by initializing from RoBERTa-large. Both UNIMObase and UNIMO-large are trained for at least 500K steps. An Adam optimizer with initial learning rate 3.3 Finetuning Tasks We fine-tune our model on two categories of downstream tasks: (1) single-modal language understanding and generation tasks; (2) multimodal vision-language understanding and generation tasks. The single-modal generation tasks include: generative conversational question answering on the CoQA dataset (Reddy et al., 2019), question generation on the SQuAD 1.1 dataset (Rajpurkar et al., 2016), abstractive summarization on the CNN/DailyMail (CNNDM) dataset (Hermann et al., 2015), and sentence compression on the Gigaword dataset (Rush et al., 2015). The single-modal understanding tasks include: sentiment classification on the SST-2 dataset (Socher et al., 2013), natural language inference on the MNLI dataset (Williams et al., 2017), linguistic acceptability analysis on the CoLA dataset (Warstadt et al., 2019) and semantic similarity analysis on the STS-B dataset (Cer et al., 2017). The multi-modal tasks include:"
2021.acl-long.202,D15-1044,0,0.04454,"eps. An Adam optimizer with initial learning rate 3.3 Finetuning Tasks We fine-tune our model on two categories of downstream tasks: (1) single-modal language understanding and generation tasks; (2) multimodal vision-language understanding and generation tasks. The single-modal generation tasks include: generative conversational question answering on the CoQA dataset (Reddy et al., 2019), question generation on the SQuAD 1.1 dataset (Rajpurkar et al., 2016), abstractive summarization on the CNN/DailyMail (CNNDM) dataset (Hermann et al., 2015), and sentence compression on the Gigaword dataset (Rush et al., 2015). The single-modal understanding tasks include: sentiment classification on the SST-2 dataset (Socher et al., 2013), natural language inference on the MNLI dataset (Williams et al., 2017), linguistic acceptability analysis on the CoLA dataset (Warstadt et al., 2019) and semantic similarity analysis on the STS-B dataset (Cer et al., 2017). The multi-modal tasks include: visual question answering (VQA) on the VQA v2.0 dataset (Goyal et al., 2017), image caption on the Microsoft COCO Captions dataset (Chen et al., 2015), visual entailment on the SNLI-VE dataset (Xie et al., 2019) and image-text r"
2021.acl-long.202,P16-1162,0,0.0126841,"learn representations that capture modality-invariant information at the semantic level. Different from previous methods, UNIMO learns from different modalities of data, including images, texts and image-text pairs, thus achieving more robust and generalizable representations for both textual and visual input. As shown in Figure 2, UNIMO employs multi-layer self-attention Transformers to learn unified semantic representations for both textual and visual data. For a textual input W, it is firstly split into a sequence of subwords W = {[CLS], w1 , ..., wn , [SEP ]} by Byte-Pair Encoding (BPE) (Sennrich et al., 2016), and then the self-attention mechanism is leveraged to learn contextual token representations {h[CLS] , hw1 , ..., hwn , h[SEP ] }. The special tokens [CLS] and [SEP ] denote the start and end of the textual sequence, respectively. Similarly, for an image V, it is firstly converted to a sequence of region features V = {[IM G], v1 , ..., vt } ([IM G] denotes the representation of the entire image), and then the self-attention mechanism is leveraged to learn contextual region representations {h[IM G] , hv1 , ..., hvt }. Similar to previous work (Chen et al., 2020b), we use Faster R-CNN (Ren et"
2021.acl-long.202,P18-1238,0,0.0530716,"Missing"
2021.acl-long.227,Q18-1021,0,0.0715666,"Missing"
2021.acl-long.227,D19-1521,0,0.0282896,"n et al., 2018) to predict an answer with the maximum sum of start and end logits across multiple segments of a sample. In addition, we use a modified cross-entropy loss (Clark and Gardner, 2017) for the TQA dataset and use a two-stage model (Groeneveld et al., 2020) with the backbone of ERNIE-D OC for the HQA dataset. Tab. 4. shows that ERNIE-D OC outperforms RoBERTa and Longformer by a considerable margin on these two datasets, and is comparable to current SOTA long-document model, i.e., BigBird on HQA in large-size model setting. Results on the Keyphrase Extraction Task. We include OpenKP (Xiong et al., 2019) dataset to evaluate ERNIE-D OC’s ability to extract keyphrases from a long document. Each document contains up to three short keyphrases and we follow the model setting of JointKPE (Sun et al., 2020a) and ETC (Ainslie et al., 2020) by applying CNNs on BERT’s output to compose n-gram embeddings for classification. We report the results of basesize models in Tab. 5 under no-visual-features setting for easy and fair comparison with baselines. ERNIE-D OC performs stably better on all metrics on the OpenKP dataset. 4.2.4 Results on Chinese Tasks We conducted extensive experiments on seven Chinese"
2021.acl-long.227,L18-1431,0,0.0219061,"short keyphrases and we follow the model setting of JointKPE (Sun et al., 2020a) and ETC (Ainslie et al., 2020) by applying CNNs on BERT’s output to compose n-gram embeddings for classification. We report the results of basesize models in Tab. 5 under no-visual-features setting for easy and fair comparison with baselines. ERNIE-D OC performs stably better on all metrics on the OpenKP dataset. 4.2.4 Results on Chinese Tasks We conducted extensive experiments on seven Chinese natural language understanding (NLU) tasks, including machine reading comprehension (CMRC2018 (Cui et al., 2018), DRCD (Shao et al., 2018), DuReader (He et al., 2017), C3 (Sun et al., 2019a)), semantic similarity (CAIL2019SCM (CAIL) (Xiao et al., 2019)), and long-text classification (IFLYTEK (IFK) (Xu et al., 2020), THUCNews (THU)5 (Sun et al., 2016)). The documents in all the aforementioned datasets are sufficiently long to be used to evaluate the effectiveness of ERNIE-D OC on long-context tasks (see detailed datasets statistics in Tab. 9). We reported the mean results with five runs for the seven Chinese tasks in Tab. 6, and summarized the hyperparameters in Tab. 16. ERNIE-D OC outperforms previous models across these Chinese"
2021.acl-long.227,N18-2074,0,0.0262244,"limited the length of the sentences in each mini-batch to 512 tokens and the length of the memory to 128. The models were trained for 500K/400K/100K steps using a batch size of 2,560/2,560/3,920 sentences for the small/base/large configurations. ERNIE-D OC was optimized with the Adam (Kingma and Ba, 2014) optimizer. The learning rate was warmed up over the first 4,000 steps to a peak value of 1e-4, and then it linearly decayed. The remaining pretraining hyperparameters were the same as those of RoBERTa (Liu et al., 2019) (see Tab. 12). Additionally, we employed relative positional embedding (Shaw et al., 2018) in our model pretraining because it is necessary for reusing hidden state without causing temporal confusion (Dai et al., 2019). Finetune. In contrast to previous models, such as BERT, RoBERTa, and XLNet, the proposed model employs the retrospective feed mechanism and the enhanced recurrence mechanism during the finetuning phase to fully utilize the advantages of these two strategies. 4.2.3 Results on English Tasks Results on Long-Text Classification Tasks. We consider two datasets: IMDB reviews (Maas et al., 2011) and Hyperpartisan News Detection (HYP) (Kiesel et al., 2019). The former is a"
2021.acl-long.472,2020.acl-main.175,0,0.0125372,"odels as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summarization Explicit structures play an important role in recent deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection"
2021.acl-long.472,D17-1209,0,0.0607661,"Missing"
2021.acl-long.472,2020.acl-main.461,0,0.0198281,"Missing"
2021.acl-long.472,N18-1150,0,0.0212707,"ent representation and summary generation process of the Seq2Seq architecture by leveraging the graph structure. • Automatic and human evaluation on both long-document summarization and MDS outperform several strong baselines and validate the effectiveness of our graph-based model. 2 2.1 Related Works Abstractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 201"
2021.acl-long.472,P18-1063,0,0.0157292,"emantic relevance of the generated summaries and references, as the BERTScore improvements of BASS is obvious. Results on SDS Table 3 shows our experiment results along with other SDS baselines. Similar to WikiSUM, we also report LexRank, TransS2S, and RoBERTaS2S. Besides, we report the performance of several other baselines. ORACLE is the upper-bound of current extrative models. Seq2seq is based on LSTM encoder-decoder with attention mechanism (Bahdanau et al., 2015). Pointer and Pointer+cov are pointer-generation (See et al., 2017) with and without coverage mechanism, respectively. FastAbs (Chen and Bansal, 2018) is an abstractive method by jointly training sentence extraction and compression. TLM (Pilault et al., 2020) is a recent long-document summarization method based on language model. We also report the performances of recent pretrianing-based SOTA 6058 text generation models BART (large) and Peaguasus (base) on BIGPATENT, which both contain a parameter size of 406M . The last block shows the results of our model, which contains a parameter size of 201M . The results show that BASS consistently outperforms RoBERTaS2S, and comparable with current large SOTA models with only half of the parameter"
2021.acl-long.472,D19-5412,0,0.0207023,"(Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summariz"
2021.acl-long.472,2020.acl-main.703,0,0.0383412,"hysics Nobel Prize in 1921. The great prize was for his explanation of the photoelectric effect. The Unified Semantic Graph nomod:of Work is done during an internship at Baidu Inc. Corresponding author. cop obj won nsubj a German physicist appos nsubj published Albert Einstein dobj obl 1912 the theory of relativity Human Written Summary Albert Einstein received the physics Nobel Prize in 1912 for his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was for the photoelectric eﬀect Figure 1: Ill"
2021.acl-long.472,2020.acl-main.555,1,0.901075,"representing them as nodes and their relations as edges. This greatly benefits global structure learning and longdistance relation modeling. Several previous works have attempted to leverage sentence-relation graph to improve long sequence summarization, where nodes are sentences and edges are similarity or dis6052 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6052–6067 August 1–6, 2021. ©2021 Association for Computational Linguistics course relations between sentences (Li et al., 2020). However, the sentence-relation graph is not flexible for fine-grained (such as entities) information aggregation and relation modeling. Some other works also proposed to construct local knowledge graph by OpenIE to improve Seq2Seq models (Fan et al., 2019; Huang et al., 2020). However, the OpenIE-based graph only contains sparse relations between partially extracted phrases, which cannot reflect the global structure and rich relations of the overall sequence. For better modeling the long-distance relations and global structure of a long sequence, we propose to apply a phrase-level unified se"
2021.acl-long.472,D18-1205,1,0.790714,"ls to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summarization Explicit structures play an important role in recent deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection 6053 Input Length #Nodes #Edges 800 140 154 1600 291 332 2400 467 568 3000 579 703 Table 1: Illustration of how the average number of nodes and edges in the graph changes when the input sequence becomes longer on WikiSUM. and compression for extractive models. Cao et al. (2015) propose to extract salient sentences based on their constituency parsing trees. Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient cont"
2021.acl-long.472,D18-1441,1,0.763064,"ls to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summarization Explicit structures play an important role in recent deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection 6053 Input Length #Nodes #Edges 800 140 154 1600 291 332 2400 467 568 3000 579 703 Table 1: Illustration of how the average number of nodes and edges in the graph changes when the input sequence becomes longer on WikiSUM. and compression for extractive models. Cao et al. (2015) propose to extract salient sentences based on their constituency parsing trees. Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient cont"
2021.acl-long.472,C18-1101,0,0.0214588,"sing helps summarization models in semantic understanding. Jin et al. (2020) incorporate semantic dependency graphs of input sentences to help the summarization models generate sentences with better semantic relevance . Besides sentence-level structures, document-level structures also attract a lot of attention. Fernandes et al. (2019) build a simple graph consisting of sentences, tokens and POS for summary generation. By incorporating RST trees, Xu et al. (2020) propose a discourse-aware model to extract sentences. Similarly, structures from semantic analysis also help. Liu et al. (2015) and Liao et al. (2018) propose to guide summarization with Abstract Meaning Representation (AMR) for a better comprehension of the input context. (Li and Zhuge, 2019) propose semantic link networks based MDS but without graph neural networks. Recently, the local knowledge graph by OpenIE attracts great attention. Leveraging OpenIE extracted tuples, Fan et al. (2019) compress and reduce redundancy in multi-document inputs in MDS. Their work mainly focus on the efficiency in processing long sequences. Huang et al. (2020) utilize OpenIEbased graph for boosting the faithfulness of the generated summaries. Compared with"
2021.acl-long.472,N15-1114,0,0.0238886,"rules. Dependency parsing helps summarization models in semantic understanding. Jin et al. (2020) incorporate semantic dependency graphs of input sentences to help the summarization models generate sentences with better semantic relevance . Besides sentence-level structures, document-level structures also attract a lot of attention. Fernandes et al. (2019) build a simple graph consisting of sentences, tokens and POS for summary generation. By incorporating RST trees, Xu et al. (2020) propose a discourse-aware model to extract sentences. Similarly, structures from semantic analysis also help. Liu et al. (2015) and Liao et al. (2018) propose to guide summarization with Abstract Meaning Representation (AMR) for a better comprehension of the input context. (Li and Zhuge, 2019) propose semantic link networks based MDS but without graph neural networks. Recently, the local knowledge graph by OpenIE attracts great attention. Leveraging OpenIE extracted tuples, Fan et al. (2019) compress and reduce redundancy in multi-document inputs in MDS. Their work mainly focus on the efficiency in processing long sequences. Huang et al. (2020) utilize OpenIEbased graph for boosting the faithfulness of the generated s"
2021.acl-long.472,P19-1500,0,0.47225,"luation on both long-document summarization and MDS outperform several strong baselines and validate the effectiveness of our graph-based model. 2 2.1 Related Works Abstractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the r"
2021.acl-long.472,D19-1387,0,0.342053,"luation on both long-document summarization and MDS outperform several strong baselines and validate the effectiveness of our graph-based model. 2 2.1 Related Works Abstractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the r"
2021.acl-long.472,N19-1173,0,0.398581,"et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summarization Explicit structures play an important role in recent deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection 6053 Input Length #Nodes #Edges 800 140 154 1600 291 332 2400 467 568 3000 579 703 Table 1: Illustration of how the average number of nodes and edges in the graph changes when the input sequence becomes longer on WikiSUM. and compression for extractive models. Cao et al. (2015) propose to extract salient sentences based on their constituency parsing trees. Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient content based on syntax s"
2021.acl-long.472,2021.ccl-1.108,0,0.0427439,"Missing"
2021.acl-long.472,P19-1212,0,0.133693,"12 the theory of relativity Human Written Summary Albert Einstein received the physics Nobel Prize in 1912 for his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was for the photoelectric eﬀect Figure 1: Illustration of a unified semantic graph and its construction procedure for a document containing three sentences. In Graph Construction, underlined tokens represent phrases., co-referent phrases are represented in the same color. In The Unified Semantic Graph, nodes of different colors indic"
2021.acl-long.472,P14-5010,0,0.00266212,", two-hop meta-path represents more complex semantic relations in graph. For example, N-V-N like [Albert Einstein]-[won]-[the physics Nobel Prize] indicates SVO (subject–verb–object) relation. It is essential to effectively model the two-hop meta-path for complex semantic relation modeling. 3.2 Graph Construction In this section, we introduce the definition and construction of the unified semantic graph. To construct the semantic graph, we extract phrases and their relations from sentences by first merging tokens into phrases and then merging co-referent phrases into nodes. We employ CoreNLP (Manning et al., 2014) to obtain coreference chains of the input sequence and the dependency parsing tree of each sentence. Based on the dependency parsing tree, we merge consecutive tokens that form a complete semantic unit into a phrase. Afterwards, we merge the same phrases from different positions and phrases in the same coreference chain to form the nodes in the semantic graph. The final statistics of the unified semantic graph on WikiSUM are illustrated in table 1, which indicates that the scale of the graph expands moderately with the inputs. This also demonstrates how the unified semantic graph compresses l"
2021.acl-long.472,2020.emnlp-main.748,0,0.0134901,"Results on SDS Table 3 shows our experiment results along with other SDS baselines. Similar to WikiSUM, we also report LexRank, TransS2S, and RoBERTaS2S. Besides, we report the performance of several other baselines. ORACLE is the upper-bound of current extrative models. Seq2seq is based on LSTM encoder-decoder with attention mechanism (Bahdanau et al., 2015). Pointer and Pointer+cov are pointer-generation (See et al., 2017) with and without coverage mechanism, respectively. FastAbs (Chen and Bansal, 2018) is an abstractive method by jointly training sentence extraction and compression. TLM (Pilault et al., 2020) is a recent long-document summarization method based on language model. We also report the performances of recent pretrianing-based SOTA 6058 text generation models BART (large) and Peaguasus (base) on BIGPATENT, which both contain a parameter size of 406M . The last block shows the results of our model, which contains a parameter size of 201M . The results show that BASS consistently outperforms RoBERTaS2S, and comparable with current large SOTA models with only half of the parameter size. This further demonstrates the effectiveness of our graph-augmented model on long-document summarization"
2021.acl-long.472,2020.findings-emnlp.217,0,0.0113767,"stractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al.,"
2021.acl-long.472,W00-1009,0,0.480591,"or his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was for the photoelectric eﬀect Figure 1: Illustration of a unified semantic graph and its construction procedure for a document containing three sentences. In Graph Construction, underlined tokens represent phrases., co-referent phrases are represented in the same color. In The Unified Semantic Graph, nodes of different colors indicate different types, according to section 3.1. on the long source sequence (Shao et al., 2017). Thus"
2021.acl-long.472,2020.tacl-1.18,0,0.0241394,"cument summarization and MDS outperform several strong baselines and validate the effectiveness of our graph-based model. 2 2.1 Related Works Abstractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several larg"
2021.acl-long.472,D19-1324,0,0.0156427,"deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection 6053 Input Length #Nodes #Edges 800 140 154 1600 291 332 2400 467 568 3000 579 703 Table 1: Illustration of how the average number of nodes and edges in the graph changes when the input sequence becomes longer on WikiSUM. and compression for extractive models. Cao et al. (2015) propose to extract salient sentences based on their constituency parsing trees. Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient content based on syntax structure and syntax rules. Dependency parsing helps summarization models in semantic understanding. Jin et al. (2020) incorporate semantic dependency graphs of input sentences to help the summarization models generate sentences with better semantic relevance . Besides sentence-level structures, document-level structures also attract a lot of attention. Fernandes et al. (2019) build a simple graph consisting of sentences, tokens and POS for summary generation. By incorporating RST trees, Xu et al. (2020) propo"
2021.acl-long.472,2020.acl-main.451,0,0.191789,"Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient content based on syntax structure and syntax rules. Dependency parsing helps summarization models in semantic understanding. Jin et al. (2020) incorporate semantic dependency graphs of input sentences to help the summarization models generate sentences with better semantic relevance . Besides sentence-level structures, document-level structures also attract a lot of attention. Fernandes et al. (2019) build a simple graph consisting of sentences, tokens and POS for summary generation. By incorporating RST trees, Xu et al. (2020) propose a discourse-aware model to extract sentences. Similarly, structures from semantic analysis also help. Liu et al. (2015) and Liao et al. (2018) propose to guide summarization with Abstract Meaning Representation (AMR) for a better comprehension of the input context. (Li and Zhuge, 2019) propose semantic link networks based MDS but without graph neural networks. Recently, the local knowledge graph by OpenIE attracts great attention. Leveraging OpenIE extracted tuples, Fan et al. (2019) compress and reduce redundancy in multi-document inputs in MDS. Their work mainly focus on the efficie"
2021.acl-long.472,2020.acl-main.640,0,0.0218044,"d by the graph. Node Initialization Similar to graph construction in section 3.2, we initialize graph representations following the two-level merging, token merging and phrase merging. The token merging compresses and abstracts local token features into higher-level phrase representations. The phrase merging aggregates co-referent phrases in a wide context, which captures long-distance and crossdocument relations. To be simple, these two merging steps are implemented by average pooling. Graph Encoding Layer Following previous works in graph-to-sequence learning (KoncelKedziorski et al., 2019; Yao et al., 2020), we apply Transformer layers for graph modeling by applying the graph adjacent matrix as self-attention mask. Graph Augmentation Following previous works (Bastings et al., 2017; Koncel-Kedziorski et al., 2019), we add reverse edges and self-loop edges in graph as the original directed edges are 6055 not enough for learning backward information. For better utilizing the properties of the united semantic graph, we further propose two novel graph augmentation methods. Supernode As the graph becomes larger, noises introduced by imperfect graph construction also increase, which may cause disconnec"
2021.acl-long.472,D15-1044,0,0.251484,"ion tasks. 1 relativity. He won the physics Nobel Prize in 1921. The great prize was for his explanation of the photoelectric effect. The Unified Semantic Graph nomod:of Work is done during an internship at Baidu Inc. Corresponding author. cop obj won nsubj a German physicist appos nsubj published Albert Einstein dobj obl 1912 the theory of relativity Human Written Summary Albert Einstein received the physics Nobel Prize in 1912 for his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was fo"
2021.acl-long.472,P17-1099,0,0.353644,"vity. He won the physics Nobel Prize in 1921. The great prize was for his explanation of the photoelectric effect. The Unified Semantic Graph nomod:of Work is done during an internship at Baidu Inc. Corresponding author. cop obj won nsubj a German physicist appos nsubj published Albert Einstein dobj obl 1912 the theory of relativity Human Written Summary Albert Einstein received the physics Nobel Prize in 1912 for his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was for the photoelectri"
2021.acl-short.120,D17-1215,0,0.109534,"ior of existing models on the challenge test set, which may provide suggestions for future model development. The dataset and codes are publicly available at https://github.com/baidu/ DuReader. 1 Introduction Machine reading comprehension (MRC) requires machines to comprehend text and answer questions about it. With the development of deep learning, the recent studies of MRC have achieved remarkable advancements (Seo et al., 2017; Wang and Jiang, 2017; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020). However, previous studies show that most of the neural models are not robust enough (Jia and Liang, 2017; Ribeiro et al., 2018b; Talmor and Berant, 2019a; Welbl et al., 2020) and fail to generalize well (Talmor and Berant, 2019b). ∗ This work was done while the first author was doing internship at Baidu Inc. † Corresponding authors To further promote the studies of robust and well generalized MRC, we construct a Chinese dataset – DuReaderrobust which comprises natural questions and documents. In this paper, we focus on evaluating the robustness and generalization from the following aspects, where robustness consists of over-sensitivity and over-stability: (1) Over-sensitivity denotes that MRC mo"
2021.acl-short.120,2021.ccl-1.108,0,0.0944696,"Missing"
2021.acl-short.120,P18-1079,0,0.124182,"s on the challenge test set, which may provide suggestions for future model development. The dataset and codes are publicly available at https://github.com/baidu/ DuReader. 1 Introduction Machine reading comprehension (MRC) requires machines to comprehend text and answer questions about it. With the development of deep learning, the recent studies of MRC have achieved remarkable advancements (Seo et al., 2017; Wang and Jiang, 2017; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020). However, previous studies show that most of the neural models are not robust enough (Jia and Liang, 2017; Ribeiro et al., 2018b; Talmor and Berant, 2019a; Welbl et al., 2020) and fail to generalize well (Talmor and Berant, 2019b). ∗ This work was done while the first author was doing internship at Baidu Inc. † Corresponding authors To further promote the studies of robust and well generalized MRC, we construct a Chinese dataset – DuReaderrobust which comprises natural questions and documents. In this paper, we focus on evaluating the robustness and generalization from the following aspects, where robustness consists of over-sensitivity and over-stability: (1) Over-sensitivity denotes that MRC models provide different"
2021.acl-short.120,P19-1485,0,0.348476,"set, which may provide suggestions for future model development. The dataset and codes are publicly available at https://github.com/baidu/ DuReader. 1 Introduction Machine reading comprehension (MRC) requires machines to comprehend text and answer questions about it. With the development of deep learning, the recent studies of MRC have achieved remarkable advancements (Seo et al., 2017; Wang and Jiang, 2017; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020). However, previous studies show that most of the neural models are not robust enough (Jia and Liang, 2017; Ribeiro et al., 2018b; Talmor and Berant, 2019a; Welbl et al., 2020) and fail to generalize well (Talmor and Berant, 2019b). ∗ This work was done while the first author was doing internship at Baidu Inc. † Corresponding authors To further promote the studies of robust and well generalized MRC, we construct a Chinese dataset – DuReaderrobust which comprises natural questions and documents. In this paper, we focus on evaluating the robustness and generalization from the following aspects, where robustness consists of over-sensitivity and over-stability: (1) Over-sensitivity denotes that MRC models provide different answers to the paraphrase"
2021.acl-short.120,2020.findings-emnlp.103,0,0.0277015,"ggestions for future model development. The dataset and codes are publicly available at https://github.com/baidu/ DuReader. 1 Introduction Machine reading comprehension (MRC) requires machines to comprehend text and answer questions about it. With the development of deep learning, the recent studies of MRC have achieved remarkable advancements (Seo et al., 2017; Wang and Jiang, 2017; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020). However, previous studies show that most of the neural models are not robust enough (Jia and Liang, 2017; Ribeiro et al., 2018b; Talmor and Berant, 2019a; Welbl et al., 2020) and fail to generalize well (Talmor and Berant, 2019b). ∗ This work was done while the first author was doing internship at Baidu Inc. † Corresponding authors To further promote the studies of robust and well generalized MRC, we construct a Chinese dataset – DuReaderrobust which comprises natural questions and documents. In this paper, we focus on evaluating the robustness and generalization from the following aspects, where robustness consists of over-sensitivity and over-stability: (1) Over-sensitivity denotes that MRC models provide different answers to the paraphrased questions. It means"
2021.autosimtrans-1.5,2021.autosimtrans-1.5,1,0.0530913,"Missing"
2021.autosimtrans-1.5,2020.acl-demos.34,0,0.0687097,"Missing"
2021.autosimtrans-1.5,P19-1126,0,0.512702,"Missing"
2021.autosimtrans-1.5,cho-etal-2014-corpus,0,0.473052,"Missing"
2021.autosimtrans-1.5,2020.lrec-1.517,0,0.114315,"Missing"
2021.autosimtrans-1.5,P14-2090,0,0.435886,"Missing"
2021.autosimtrans-1.5,W17-4608,0,0.282237,"Missing"
2021.autosimtrans-1.5,2013.iwslt-papers.14,0,0.144286,"Missing"
2021.autosimtrans-1.5,2020.emnlp-main.178,1,0.639518,"Missing"
2021.autosimtrans-1.5,P16-1162,0,0.0317362,"Missing"
2021.autosimtrans-1.5,shimizu-etal-2014-collection,0,0.278144,"Missing"
2021.autosimtrans-1.5,Q19-1020,0,0.0906152,"Missing"
2021.autosimtrans-1.5,N13-1023,0,0.444736,"Missing"
2021.autosimtrans-1.6,2021.autosimtrans-1.6,1,0.0530913,"Missing"
2021.autosimtrans-1.6,P19-1126,0,0.146797,"Missing"
2021.autosimtrans-1.6,W05-0909,0,0.103299,"Missing"
2021.autosimtrans-1.6,N19-1202,0,0.173048,"Missing"
2021.autosimtrans-1.6,E17-1099,0,0.275927,"Missing"
2021.autosimtrans-1.6,2021.autosimtrans-1.5,1,0.84463,"Missing"
2021.autosimtrans-1.6,L18-1001,0,0.276747,"Missing"
2021.autosimtrans-1.6,2020.autosimtrans-1.1,1,0.73397,"Missing"
2021.autosimtrans-1.6,2020.emnlp-main.178,1,0.859154,"Missing"
2021.autosimtrans-1.6,1983.tc-1.13,0,0.644431,"Missing"
2021.autosimtrans-1.6,2020.aacl-main.58,0,0.698322,"Missing"
2021.autosimtrans-1.6,P02-1040,0,0.119352,"Missing"
2021.autosimtrans-1.6,2020.acl-main.350,0,0.514198,"Missing"
2021.autosimtrans-1.6,P16-1009,0,0.120801,"Missing"
2021.autosimtrans-1.6,2020.lrec-1.517,0,0.288828,"Missing"
2021.emnlp-main.224,N19-1423,0,0.0315612,"ipedia articles as the collection of passages. In our experiments, we reuse the NQ version created by DPR. Evaluation Metrics Following previous work, we adopt Mean Reciprocal Rank (MRR) and Recall at top k ranks (Recall@k) to evaluate the performance of passage retrieval. MRR calculates the averaged reciprocal of the rank at which the first positive passage is retrieved. Recall@k calculates the proportion of questions to which the top k retrieved passages contain positives. Model Specifications Our retriever and reranker largely follow ERNIE-2.0 base (Sun et al., 2020), which is a BERT-like (Devlin et al., 2019) model with 12-layer transformers and introduces a continual pre-training framework on multiple pretrained tasks. As described in previous section, the retriever is initialized with the parameters of the dual-encoder in the first step of RocketQA, and the re-ranker is initialized with the parameters of the cross-encoder in the second step of RocketQA. In this section, we first describe the experimental Implementation Details We conduct experisettings, then report the main experimental results, 2829 MSMARCO Dev MRR@10 R@50 R@1000 Natural Questions Test R@5 R@20 R@100 Methods PLM BM25 (anserini)"
2021.emnlp-main.224,2020.emnlp-main.342,0,0.0288834,"(Izacard and Grave, 2020; Yang and Seo, 2020; Qu et al., 2021; Ren et al., 2021). Based on the retrieved passages from a retriever, PLM-based rerankers with the cross-encoder architecture have recently been applied on passage re-ranking to improve the retrieval results (Qiao et al., 2019; Nogueira and Cho, 2019; Wang et al., 2019; Yan et al., 2019), and yield substantial improvements over the traditional methods. Apart from separately considering the above two tasks, it has been proved that passage retrieval and passage re-ranking are actually highly related and dependent (Huang et al., 2020; Gao et al., 2020; Khattab and Zaharia, 2020). The retriever needs to capture the relevance knowledge from the re-ranker, and the re-ranker should be specially optimized according to the preceding results of the retriever. Some efforts studied the possibility of leveraging the dependency of retriever and re-ranker, and try to enhance the connection between them in an alternative way (Qu et al., 2021; Yang et al., 2020; Huang et al., 2020). Furthermore, several studies attempted to jointly train the retriever and the reader for Open-domain Question Answering (Guu et al., 2020; Sachan et al., 2021; Karpukhin et"
2021.emnlp-main.224,2021.naacl-main.241,0,0.037037,"Missing"
2021.emnlp-main.224,K19-1049,0,0.0195356,"e relevance can be measured via embedding similarity. Additionally, a subsequent procedure of passage re-ranking is widely adopted to further improve the retrieval results by incorporating a reranker (Qu et al., 2021; Luan et al., 2021). Such a two-stage procedure is particularly useful in a variety of natural language processing tasks, including question answering (Mao et al., 2021; Xiong ∗ Equal contribution. The work was done when Ruiyang Ren was doing internship at Baidu. † Corresponding authors. et al., 2020b), dialogue system (Ji et al., 2014; Henderson et al., 2017) and entity linking (Gillick et al., 2019; Wu et al., 2020). Following a retrieve-then-rerank way, the dense retriever in passage retrieval and the re-ranker in passage re-ranking jointly contribute to the final performance. Despite the fact that the two modules work as a pipeline during the inference stage, it has been found useful to train them in a correlated manner. For example, the retriever with a dual-encoder can be improved by distilling from the re-ranker with a more capable cross-encoder architecture (Qu et al., 2021; Yang et al., 2020), and the re-ranker can be improved with training instances generated from the retriever"
2021.emnlp-main.224,P19-1612,0,0.0282384,"marized as follows: • We propose a novel approach that jointly trains the dense passage retriever and passage re-ranker. It is the first time that joint training has been implemented for the two modules. • We make two major technical contributions by introducing dynamic listwise distillation and hybrid data augmentation to support the proposed joint learning approach. • Extensive experiments show the effectiveness of our proposed approach on both MSMARCO and Natural Questions datasets. 2 Related Work vided into two categories: (1) self-supervised pretraining for retrieval (Chang et al., 2020; Lee et al., 2019; Guu et al., 2020) and (2) fine-tuning pre-trained language models (PLMs) on labeled data (Lu et al., 2020; Karpukhin et al., 2020; Xiong et al., 2020a; Luan et al., 2021; Qu et al., 2021) . Our work follows the second class of approaches, which show better performance with less cost. There are two important tricks to train an effective dense retriever: (1) incorporating hard negatives during training (Karpukhin et al., 2020; Xiong et al., 2020a; Qu et al., 2021) and (2) distilling the knowledge from cross-encoder-based reranker into dual-encoder-based retriever (Izacard and Grave, 2020; Yang"
2021.emnlp-main.224,2020.emnlp-main.550,0,0.0611717,"for both the retriever and the re-ranker. During the dynamic distillation, the retriever and the re-ranker can be adaptively improved according to each other’s relevance information. We also propose a hybrid data augmentation strategy to construct diverse training instances for listwise training approach. Extensive experiments show the effectiveness of our approach on both MSMARCO and Natural Questions datasets. Our code is available at https:// github.com/PaddlePaddle/RocketQA. 1 Introduction Recently, dense passage retrieval has become an important approach in the task of passage retrieval (Karpukhin et al., 2020) to identify relevant contents from a large corpus. The underlying idea is to represent both queries and passages as low-dimensional vectors (a.k.a., embeddings), so that the relevance can be measured via embedding similarity. Additionally, a subsequent procedure of passage re-ranking is widely adopted to further improve the retrieval results by incorporating a reranker (Qu et al., 2021; Luan et al., 2021). Such a two-stage procedure is particularly useful in a variety of natural language processing tasks, including question answering (Mao et al., 2021; Xiong ∗ Equal contribution. The work was"
2021.emnlp-main.224,2021.naacl-main.466,1,0.0736875,"uestions datasets. Our code is available at https:// github.com/PaddlePaddle/RocketQA. 1 Introduction Recently, dense passage retrieval has become an important approach in the task of passage retrieval (Karpukhin et al., 2020) to identify relevant contents from a large corpus. The underlying idea is to represent both queries and passages as low-dimensional vectors (a.k.a., embeddings), so that the relevance can be measured via embedding similarity. Additionally, a subsequent procedure of passage re-ranking is widely adopted to further improve the retrieval results by incorporating a reranker (Qu et al., 2021; Luan et al., 2021). Such a two-stage procedure is particularly useful in a variety of natural language processing tasks, including question answering (Mao et al., 2021; Xiong ∗ Equal contribution. The work was done when Ruiyang Ren was doing internship at Baidu. † Corresponding authors. et al., 2020b), dialogue system (Ji et al., 2014; Henderson et al., 2017) and entity linking (Gillick et al., 2019; Wu et al., 2020). Following a retrieve-then-rerank way, the dense retriever in passage retrieval and the re-ranker in passage re-ranking jointly contribute to the final performance. Despite the"
2021.emnlp-main.224,2021.findings-acl.191,1,0.783125,"tuning pre-trained language models (PLMs) on labeled data (Lu et al., 2020; Karpukhin et al., 2020; Xiong et al., 2020a; Luan et al., 2021; Qu et al., 2021) . Our work follows the second class of approaches, which show better performance with less cost. There are two important tricks to train an effective dense retriever: (1) incorporating hard negatives during training (Karpukhin et al., 2020; Xiong et al., 2020a; Qu et al., 2021) and (2) distilling the knowledge from cross-encoder-based reranker into dual-encoder-based retriever (Izacard and Grave, 2020; Yang and Seo, 2020; Qu et al., 2021; Ren et al., 2021). Based on the retrieved passages from a retriever, PLM-based rerankers with the cross-encoder architecture have recently been applied on passage re-ranking to improve the retrieval results (Qiao et al., 2019; Nogueira and Cho, 2019; Wang et al., 2019; Yan et al., 2019), and yield substantial improvements over the traditional methods. Apart from separately considering the above two tasks, it has been proved that passage retrieval and passage re-ranking are actually highly related and dependent (Huang et al., 2020; Gao et al., 2020; Khattab and Zaharia, 2020). The retriever needs to capture the"
2021.emnlp-main.224,2021.acl-long.519,0,0.0233286,"ang et al., 2020; Gao et al., 2020; Khattab and Zaharia, 2020). The retriever needs to capture the relevance knowledge from the re-ranker, and the re-ranker should be specially optimized according to the preceding results of the retriever. Some efforts studied the possibility of leveraging the dependency of retriever and re-ranker, and try to enhance the connection between them in an alternative way (Qu et al., 2021; Yang et al., 2020; Huang et al., 2020). Furthermore, several studies attempted to jointly train the retriever and the reader for Open-domain Question Answering (Guu et al., 2020; Sachan et al., 2021; Karpukhin et al., 2020). Different from the prior studies, our method is a joint learning architecture of the dense passage retriever and the re-ranker. 3 Methodology In this section, we describe a novel joint training approach for dense passage retrieval and passage re-ranking (called RocketQAv2) Recently, dense passage retrieval has demonstrated better performance than traditional sparse 3.1 Overview retrieval methods (e.g., TF-IDF and BM25) on In this work, we consider two tasks including the task of passage retrieval. Existing approaches dense passage retrieval and passage re-ranking, of"
2021.emnlp-main.224,D19-1599,0,0.0282163,"t. There are two important tricks to train an effective dense retriever: (1) incorporating hard negatives during training (Karpukhin et al., 2020; Xiong et al., 2020a; Qu et al., 2021) and (2) distilling the knowledge from cross-encoder-based reranker into dual-encoder-based retriever (Izacard and Grave, 2020; Yang and Seo, 2020; Qu et al., 2021; Ren et al., 2021). Based on the retrieved passages from a retriever, PLM-based rerankers with the cross-encoder architecture have recently been applied on passage re-ranking to improve the retrieval results (Qiao et al., 2019; Nogueira and Cho, 2019; Wang et al., 2019; Yan et al., 2019), and yield substantial improvements over the traditional methods. Apart from separately considering the above two tasks, it has been proved that passage retrieval and passage re-ranking are actually highly related and dependent (Huang et al., 2020; Gao et al., 2020; Khattab and Zaharia, 2020). The retriever needs to capture the relevance knowledge from the re-ranker, and the re-ranker should be specially optimized according to the preceding results of the retriever. Some efforts studied the possibility of leveraging the dependency of retriever and re-ranker, and try to enha"
2021.emnlp-main.224,2020.emnlp-main.519,0,0.0184809,"sured via embedding similarity. Additionally, a subsequent procedure of passage re-ranking is widely adopted to further improve the retrieval results by incorporating a reranker (Qu et al., 2021; Luan et al., 2021). Such a two-stage procedure is particularly useful in a variety of natural language processing tasks, including question answering (Mao et al., 2021; Xiong ∗ Equal contribution. The work was done when Ruiyang Ren was doing internship at Baidu. † Corresponding authors. et al., 2020b), dialogue system (Ji et al., 2014; Henderson et al., 2017) and entity linking (Gillick et al., 2019; Wu et al., 2020). Following a retrieve-then-rerank way, the dense retriever in passage retrieval and the re-ranker in passage re-ranking jointly contribute to the final performance. Despite the fact that the two modules work as a pipeline during the inference stage, it has been found useful to train them in a correlated manner. For example, the retriever with a dual-encoder can be improved by distilling from the re-ranker with a more capable cross-encoder architecture (Qu et al., 2021; Yang et al., 2020), and the re-ranker can be improved with training instances generated from the retriever (Qu et al., 2021;"
2021.emnlp-main.333,P19-1098,0,0.0852126,"2 for training, validation and testing and truncate each document to 768 tokens. DUC Dataset We use the benchmark datasets from the Document Understanding Conferences (DUC) containing clusters of English news articles and human reference summaries. We use DUC 2002, 2003 and 2004 datesets which contain 60, 30 and 50 clusters of nearly 10 documents respectively. Four human reference summaries have been created for each document cluster by NIST assessors. Our model is trained on DUC 2002, validated on DUC 2003, and tested on DUC 2004. We apply the similar preprocessing method with previous work (Cho et al., 2019) and truncate each document to 768 tokens Training Configuration We use the base version of RoBERTa (Liu et al., 2019b) to initialize our models in all experiments. The optimizer is Adam (Kingma and Ba, 2014) with β1=0.9 and β2=0.999, and the learning rate is 0.03 for MultiNews and 0.015 for DUC. We apply learning rate warmup over the first 10000 steps and decay as in (Kingma and Ba, 2014). Gradient clipping with maximum gradient norm 2.0 is also utilized during training. All models are trained on 4 GPUs (Tesla V100) for about 10 epochs. We apply dropout with probability 0.1 before all linear"
2021.emnlp-main.333,N13-1136,0,0.056924,"he graph structure is effective to model relations methods such as similarity graph and discourse between sentences which is an essential point to graph. Sentences are the basic information units select interrelated summary-worthy sentences in and represented as nodes in the graph. And relaextractive summarization. Erkan and Radev (2004) tions between sentences are represented as edges. utilize a similarity graph to construct an unsuper- For example, a similarity graph can be built based vised summarization methods called LexRank. G- on cosine similarities between tf-idf representations Flow (Christensen et al., 2013) and DISCOBERT of sentences. Let G denotes a graph representation (Xu et al., 2020) both use discourse graphs to gen- matrix of the input documents, where G[i][j] indierate concise and informative summaries. Li et al. cates the tf-idf weights between sentence Si and Sj . 4064 source documents. Figure 2 illustrates the overall architecture of SgSum. Sub-graph Ranking Layer Graph Pooling Graph Pooling Graph Pooling Graph Pooling Sub-graph Encoder Graph Encoder Transformer … Doc1 Transformer DocN Figure 2: Model architecture of SgSum. Graph-based multidocument encoder takes tokenized documents as"
2021.emnlp-main.333,P19-1102,0,0.103266,". Thus, our model can be viewed as a sub-graph selection framework which means selecting a proper sub-graph from a whole graph. Furthermore, the graph structure can help to reorder the sentences in the summary to obtain a more coherent summary (Christensen et al., 2013). We order the summary by placing sentences with discourse relations next to each other. 4 4.1 Experiments Experimental Setup stated, we use the similarity graph by default as it is the most widely used in previous work. MultiNews Dataset The MultiNews dataset is a large-scale multi-document summarization dataset introduced by (Fabbri et al., 2019). It contains 56,216 articles-summary pairs and each example consists of 2-10 source documents and a humanwritten summary. Following their experimental settings, we split the dataset into 44,972/5,622/5,622 for training, validation and testing and truncate each document to 768 tokens. DUC Dataset We use the benchmark datasets from the Document Understanding Conferences (DUC) containing clusters of English news articles and human reference summaries. We use DUC 2002, 2003 and 2004 datesets which contain 60, 30 and 50 clusters of nearly 10 documents respectively. Four human reference summaries h"
2021.emnlp-main.333,N10-1131,0,0.0435273,"(2013) build multi-document graphs to identify pairwise ordering constraints over the Different from above studies, some work focus sentences by accounting for discourse relation- on the summary-level selection. Wan et al. (2015) ships between sentences. More recently, Yasunaga optimize the summarization performance directly 4070 based on the characteristics of summaries and rank summaries directly during inference. Bae et al. (2019), Paulus et al. (2017) and Celikyilmaz et al. (2018) use reinforcement learning to globally optimize summary-level performance. Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019) have attempted to a build two-stage document summarization. The first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Mendes et al. (2019) follow the extract-then-compress paradigm to train an extractor for content selection. Zhong et al. (2020) propose a novel extract-then-match framework which employs a sentence extractor to prune unnecessary information, then outputs a summary by matching models. These methods consider summary as a whole rather than individual sentences. Howev"
2021.emnlp-main.333,N18-1065,0,0.0474057,"Missing"
2021.emnlp-main.333,N09-1041,0,0.0748764,"30.95 34.52 34.02 35.41 Table 2: Evaluation results on the DUC2004 test set. We report R-1, R-2 and R-L scores, and follow the ROUGE setting of Cho et al. (2019).3 as an extra training resource to improve our model. The results show that single-document data boost the performance of our unified model a further step and achieve a new SOTA result on Multinews. Results on DUC Table 2 summarizes the evaluation results on the DUC2004 dataset. The first block shows four popular unsupervised baselines, and the second block shows several strong supervised baselines. We report the results of KSLSumm (Haghighi and Vanderwende, 2009), LexRank (Erkan and Radev, 2004), DPP (Kulesza and Taskar, 2011), Sim-DPP (Cho et al., 2019) following Cho et al. (2019). Besides, we also report the results of SubModular (Lin and Bilmes, 2010), StructSVM (Sipos et al., 2012) and PG (See et al., 2017) as strong baselines. The last block shows the results of our models. The results indicate that our model SgSum consistently outperforms most baselines, which further demonstrate the effectiveness of our model on different types of corpora. Additionally, we also test the performance of SgSum-extra which add CNN/DM data as a supplement. It is com"
2021.emnlp-main.333,D18-1446,0,0.0384836,"Missing"
2021.emnlp-main.333,C16-1023,1,0.8911,"Missing"
2021.emnlp-main.333,2020.acl-main.555,1,0.918979,"blems. In this paper, we encode source documents by a Hierarchical Transformer, which consists of several sharedweight single Transformers (Vaswani et al., 2017) that process each document independently. Each Transformer takes a tokenized document as input and outputs its sentence representations. This architecture enables our model to process much longer input. Graph Encoding To effectively capture the relations between sentences in source documents, we incorporate explicit graph representations of documents into the neural encoding process via a graph-informed attention mechanism similar to Li et al. (2020). Each sentence can collect information from other related sentences to capture global information from the whole input. The graphinformed attention mechanism extends the vanilla self-attention mechanism to consider the pairwise relations in explicit graph representations as: αij = Softmax(eij + Rij ) (1) where eij denotes the origin self-attention weights between sentences Si and Sj , αij denotes the adjusted weights by graph structure. The key point of the graph-based self-attention is the additional pairwise relation bias Rij , which is computed as a Gaussian bias of the weights of graph re"
2021.emnlp-main.333,D18-1205,1,0.825796,"del SgSum consistently outperforms most baselines, which further demonstrate the effectiveness of our model on different types of corpora. Additionally, we also test the performance of SgSum-extra which add CNN/DM data as a supplement. It is comparable to Sim-DPP baseline which also uses extra CNN/DM data to train a similarity model. And the results again show that singledocument data greatly improves the performance of our model. 4.3 Transfer Performances It is commonly known that deep neural networks achieved great improvement on SDS task recently (Liu and Lapata, 2019b; Zhong et al., 2020; Li et al., 2018a,b). However, such supervised models can not work well on MDS task because parallel data for mulit-document are scarce and costly to obtain. For example, the DUC dataset only contains tens of parallel MDS data. There is a pressing need 3 -n 2 -m -w 1.2 -c 95 -r 1000 -l 250 4068 -n 2 -m -w 1.2 -c 95 -r 1000 -l 100 Models Lead LexRank BERTSUMEXT SgSum R-1 40.21 40.27 41.28 43.61 R-2 12.13 12.63 12.05 14.07 R-L 37.13 37.50 37.18 39.50 Table 3: Transfer performance on MultiNews dataset Figure 3: Results on different graph types. Models KLSumm LexRank Extract+Rewrite BERTSUMEXT PG-MMR SgSum R-1 31"
2021.emnlp-main.333,D18-1441,1,0.819301,"del SgSum consistently outperforms most baselines, which further demonstrate the effectiveness of our model on different types of corpora. Additionally, we also test the performance of SgSum-extra which add CNN/DM data as a supplement. It is comparable to Sim-DPP baseline which also uses extra CNN/DM data to train a similarity model. And the results again show that singledocument data greatly improves the performance of our model. 4.3 Transfer Performances It is commonly known that deep neural networks achieved great improvement on SDS task recently (Liu and Lapata, 2019b; Zhong et al., 2020; Li et al., 2018a,b). However, such supervised models can not work well on MDS task because parallel data for mulit-document are scarce and costly to obtain. For example, the DUC dataset only contains tens of parallel MDS data. There is a pressing need 3 -n 2 -m -w 1.2 -c 95 -r 1000 -l 250 4068 -n 2 -m -w 1.2 -c 95 -r 1000 -l 100 Models Lead LexRank BERTSUMEXT SgSum R-1 40.21 40.27 41.28 43.61 R-2 12.13 12.63 12.05 14.07 R-L 37.13 37.50 37.18 39.50 Table 3: Transfer performance on MultiNews dataset Figure 3: Results on different graph types. Models KLSumm LexRank Extract+Rewrite BERTSUMEXT PG-MMR SgSum R-1 31"
2021.emnlp-main.333,P02-1058,0,0.294435,"ified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. However, these works only consider the graph structure of source documents, but neglect the graph structures of summaries which are also important to generate coherent and informative summaries. 5.2 Sentence or Summary-level Extraction Extractive summarization methods usually produce a summary by selecting some original sentences in the document set by a sentence-level extractor. Early models employ rule-based methods to score and select sentenecs (Lin and Hovy, 2002; Lin and Bilmes, 2011; Takamura and Okumura, 2009; 5 Related Work Schilder and Kondadadi, 2008). Recently, SUM5.1 Graph-based Summarization MARUNNER (Nallapati et al., 2017) adopt an encoder based on Recurrent Neural Networks which Most previous graph extractive MDS approaches aim to extract salient textual units from docu- is the earliest neural summarization model. SUMO (Liu et al., 2019a) capitalizes on the notion of strucments based on graph structure representations tured attention to induce a multi-root dependency of sentences. Erkan and Radev (2004) introduce tree representation of the"
2021.emnlp-main.333,P11-1052,0,0.0483401,", which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. However, these works only consider the graph structure of source documents, but neglect the graph structures of summaries which are also important to generate coherent and informative summaries. 5.2 Sentence or Summary-level Extraction Extractive summarization methods usually produce a summary by selecting some original sentences in the document set by a sentence-level extractor. Early models employ rule-based methods to score and select sentenecs (Lin and Hovy, 2002; Lin and Bilmes, 2011; Takamura and Okumura, 2009; 5 Related Work Schilder and Kondadadi, 2008). Recently, SUM5.1 Graph-based Summarization MARUNNER (Nallapati et al., 2017) adopt an encoder based on Recurrent Neural Networks which Most previous graph extractive MDS approaches aim to extract salient textual units from docu- is the earliest neural summarization model. SUMO (Liu et al., 2019a) capitalizes on the notion of strucments based on graph structure representations tured attention to induce a multi-root dependency of sentences. Erkan and Radev (2004) introduce tree representation of the document. However, al"
2021.emnlp-main.333,P19-1500,0,0.0845786,"tion and a high-way layer normalization are 4065 applied after the graph-informed attention mechanism. These three components form the graph encoding layers. Graph Pooling In the MDS task, information is more massive and relations between sentences are much more complex. So it is necessary to have an overview of the central meaning of multi-document input. Zhong et al. (2020) generate a document representation with Siamese-BERT to guide the training and inference process. In this paper, based on the graph representation of documents, we apply a multi-head weighted-pooling operation similar to Liu and Lapata (2019a) to capture the global semantic information of source documents. It takes sentence representations in the source graph as input and outputs an overall representation of them (denoted as D), which provides global information of documents for both the sentence and summary selection processes. Let xi denotes the graph representation of sentence Si . For each head z ∈ {1, ..., nhead }, we first transform xi into attention scores azi and value vectors bzi , then we calculate an attention distribution a ˆzi over all sentences in the source graph based on attention scores: azi = Waz xi (3) bzi = Wb"
2021.emnlp-main.333,D19-1387,0,0.0905449,"tion and a high-way layer normalization are 4065 applied after the graph-informed attention mechanism. These three components form the graph encoding layers. Graph Pooling In the MDS task, information is more massive and relations between sentences are much more complex. So it is necessary to have an overview of the central meaning of multi-document input. Zhong et al. (2020) generate a document representation with Siamese-BERT to guide the training and inference process. In this paper, based on the graph representation of documents, we apply a multi-head weighted-pooling operation similar to Liu and Lapata (2019a) to capture the global semantic information of source documents. It takes sentence representations in the source graph as input and outputs an overall representation of them (denoted as D), which provides global information of documents for both the sentence and summary selection processes. Let xi denotes the graph representation of sentence Si . For each head z ∈ {1, ..., nhead }, we first transform xi into attention scores azi and value vectors bzi , then we calculate an attention distribution a ˆzi over all sentences in the source graph based on attention scores: azi = Waz xi (3) bzi = Wb"
2021.emnlp-main.333,P08-2052,0,0.0316995,"range of context and conveys rich relations between phrases. However, these works only consider the graph structure of source documents, but neglect the graph structures of summaries which are also important to generate coherent and informative summaries. 5.2 Sentence or Summary-level Extraction Extractive summarization methods usually produce a summary by selecting some original sentences in the document set by a sentence-level extractor. Early models employ rule-based methods to score and select sentenecs (Lin and Hovy, 2002; Lin and Bilmes, 2011; Takamura and Okumura, 2009; 5 Related Work Schilder and Kondadadi, 2008). Recently, SUM5.1 Graph-based Summarization MARUNNER (Nallapati et al., 2017) adopt an encoder based on Recurrent Neural Networks which Most previous graph extractive MDS approaches aim to extract salient textual units from docu- is the earliest neural summarization model. SUMO (Liu et al., 2019a) capitalizes on the notion of strucments based on graph structure representations tured attention to induce a multi-root dependency of sentences. Erkan and Radev (2004) introduce tree representation of the document. However, all LexRank to compute sentence importance based these models belong to sent"
2021.emnlp-main.333,P17-1099,0,0.03234,"he performance of our unified model a further step and achieve a new SOTA result on Multinews. Results on DUC Table 2 summarizes the evaluation results on the DUC2004 dataset. The first block shows four popular unsupervised baselines, and the second block shows several strong supervised baselines. We report the results of KSLSumm (Haghighi and Vanderwende, 2009), LexRank (Erkan and Radev, 2004), DPP (Kulesza and Taskar, 2011), Sim-DPP (Cho et al., 2019) following Cho et al. (2019). Besides, we also report the results of SubModular (Lin and Bilmes, 2010), StructSVM (Sipos et al., 2012) and PG (See et al., 2017) as strong baselines. The last block shows the results of our models. The results indicate that our model SgSum consistently outperforms most baselines, which further demonstrate the effectiveness of our model on different types of corpora. Additionally, we also test the performance of SgSum-extra which add CNN/DM data as a supplement. It is comparable to Sim-DPP baseline which also uses extra CNN/DM data to train a similarity model. And the results again show that singledocument data greatly improves the performance of our model. 4.3 Transfer Performances It is commonly known that deep neural"
2021.emnlp-main.333,N19-1173,0,0.112281,"sets from the Document Understanding Conferences (DUC) containing clusters of English news articles and human reference summaries. We use DUC 2002, 2003 and 2004 datesets which contain 60, 30 and 50 clusters of nearly 10 documents respectively. Four human reference summaries have been created for each document cluster by NIST assessors. Our model is trained on DUC 2002, validated on DUC 2003, and tested on DUC 2004. We apply the similar preprocessing method with previous work (Cho et al., 2019) and truncate each document to 768 tokens Training Configuration We use the base version of RoBERTa (Liu et al., 2019b) to initialize our models in all experiments. The optimizer is Adam (Kingma and Ba, 2014) with β1=0.9 and β2=0.999, and the learning rate is 0.03 for MultiNews and 0.015 for DUC. We apply learning rate warmup over the first 10000 steps and decay as in (Kingma and Ba, 2014). Gradient clipping with maximum gradient norm 2.0 is also utilized during training. All models are trained on 4 GPUs (Tesla V100) for about 10 epochs. We apply dropout with probability 0.1 before all linear layers. The number of hidden units in our models is set as 256, the feedforward hidden size is 1,024, and the number"
2021.emnlp-main.333,E12-1023,0,0.0352659,"single-document data boost the performance of our unified model a further step and achieve a new SOTA result on Multinews. Results on DUC Table 2 summarizes the evaluation results on the DUC2004 dataset. The first block shows four popular unsupervised baselines, and the second block shows several strong supervised baselines. We report the results of KSLSumm (Haghighi and Vanderwende, 2009), LexRank (Erkan and Radev, 2004), DPP (Kulesza and Taskar, 2011), Sim-DPP (Cho et al., 2019) following Cho et al. (2019). Besides, we also report the results of SubModular (Lin and Bilmes, 2010), StructSVM (Sipos et al., 2012) and PG (See et al., 2017) as strong baselines. The last block shows the results of our models. The results indicate that our model SgSum consistently outperforms most baselines, which further demonstrate the effectiveness of our model on different types of corpora. Additionally, we also test the performance of SgSum-extra which add CNN/DM data as a supplement. It is comparable to Sim-DPP baseline which also uses extra CNN/DM data to train a similarity model. And the results again show that singledocument data greatly improves the performance of our model. 4.3 Transfer Performances It is commo"
2021.emnlp-main.333,2021.ccl-1.108,0,0.045059,"Missing"
2021.emnlp-main.333,C18-1146,0,0.0222071,"e an end-to-end model which is trained on single-document data but can work well with multiple-document input. In this section we do further experiments to verify the transfer ability of our model from single to multi-document task. We follow the experiment setups of Lebanoff et al. (2018), and compare with several strong baseline models: (1) BERTSUMEXT (Liu and Lapata, 2019b), an extractive method with pre-trained LM model; (2) PG-MMR (Lebanoff et al., 2018), an encoder-decoder model which exploits the maximal marginal relevance method to select representative sentences; (3) Extract+Rewrite (Song et al., 2018), is a recent approach that scores sentences using LexRank and generates a title-like summary for each sentence using an encoder-decoder model. We follow the results of Lebanoff et al. (2018). Table 3 and Table 4 demonstrate the results on MultiNews and DUC2004 respectively. Models SgSum w/o s.g. enc w/o s.g. rank w/o s.g. enc&rank w/o graph enc w/o all R-1 47.36 46.87 46.91 46.69 46.21 45.43 R-2 18.61 17.93 17.97 17.64 17.12 16.62 R-L 43.13 42.67 42.80 42.48 42.11 41.32 Table 5: Ablation study on the MultiNews test set. s.g. is the abbreviation for sub-graph. fer ability which can reduce the"
2021.emnlp-main.333,N19-1397,0,0.0189253,"ly, Yasunaga optimize the summarization performance directly 4070 based on the characteristics of summaries and rank summaries directly during inference. Bae et al. (2019), Paulus et al. (2017) and Celikyilmaz et al. (2018) use reinforcement learning to globally optimize summary-level performance. Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019) have attempted to a build two-stage document summarization. The first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Mendes et al. (2019) follow the extract-then-compress paradigm to train an extractor for content selection. Zhong et al. (2020) propose a novel extract-then-match framework which employs a sentence extractor to prune unnecessary information, then outputs a summary by matching models. These methods consider summary as a whole rather than individual sentences. However, they neglect the relations between sentences during both scoring and selecting. 5.3 Conclusion We propose a novel framework SgSum which transforms the MDS task into the problem of sub-graph selection. SgSum captures the relations between sentences by"
2021.emnlp-main.333,N18-1158,0,0.0213222,"salient textual units from docu- is the earliest neural summarization model. SUMO (Liu et al., 2019a) capitalizes on the notion of strucments based on graph structure representations tured attention to induce a multi-root dependency of sentences. Erkan and Radev (2004) introduce tree representation of the document. However, all LexRank to compute sentence importance based these models belong to sentence-level extractors on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Chris- which select high score sentences individually and might raise redundancy (Narayan et al., 2018). tensen et al. (2013) build multi-document graphs to identify pairwise ordering constraints over the Different from above studies, some work focus sentences by accounting for discourse relation- on the summary-level selection. Wan et al. (2015) ships between sentences. More recently, Yasunaga optimize the summarization performance directly 4070 based on the characteristics of summaries and rank summaries directly during inference. Bae et al. (2019), Paulus et al. (2017) and Celikyilmaz et al. (2018) use reinforcement learning to globally optimize summary-level performance. Recent studies (Aly"
2021.emnlp-main.333,2020.acl-main.553,0,0.0666884,"sub-graph view is more appropriate to generate a coherent and concise summary. This is also the key point of our framework. Additionally, important sentences usually build up crucial sub-graphs. So it is a simple but efficient way to generate candidate sub-graphs based on those salient sentences. 3 3.1 Methodology Graph-based Multi-document Encoder Hierarchical Transformer Most previous works (Cao et al., 2017; Jin et al., 2020; Wang et al., 2017) did not consider the multi-document structure. They simply concatenate all documents together and treat the MDS as a special SDS with longer input. Wang et al. (2020) preprocess the multi-document input by truncating lead sentences averagely from each document, then concatenating them together as the MDS input. These preprocessing methods are simple ways to help the model encode multi-document inputs. But they do not make full use of the source document structures. Lead sentences extracted from each document might be similar with each other and result in redundant and incoherent problems. In this paper, we encode source documents by a Hierarchical Transformer, which consists of several sharedweight single Transformers (Vaswani et al., 2017) that process ea"
2021.emnlp-main.333,D17-1020,0,0.0195624,"he sub-graph structures, we can distinguish the quality of different candidate summaries and finally select the best one. Compared with the whole document graph view, sub-graph view is more appropriate to generate a coherent and concise summary. This is also the key point of our framework. Additionally, important sentences usually build up crucial sub-graphs. So it is a simple but efficient way to generate candidate sub-graphs based on those salient sentences. 3 3.1 Methodology Graph-based Multi-document Encoder Hierarchical Transformer Most previous works (Cao et al., 2017; Jin et al., 2020; Wang et al., 2017) did not consider the multi-document structure. They simply concatenate all documents together and treat the MDS as a special SDS with longer input. Wang et al. (2020) preprocess the multi-document input by truncating lead sentences averagely from each document, then concatenating them together as the MDS input. These preprocessing methods are simple ways to help the model encode multi-document inputs. But they do not make full use of the source document structures. Lead sentences extracted from each document might be similar with each other and result in redundant and incoherent problems. In"
2021.emnlp-main.333,2021.acl-long.472,1,0.782914,"our proposed sub-graph selection framework. et al. (2017) build on the approximate discourse graph model and account for macro-level features in sentences to improve sentence salience prediction. Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes an entity linking graph to capture the global dependencies between sentences. Li et al. (2020) incorporate explicit graph representations to the neural architecture based on a novel graph-informed selfattention mechanism. It is the first work to effectively combine graph structures with abstractive MDS model. Wu et al. (2021) present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. However, these works only consider the graph structure of source documents, but neglect the graph structures of summaries which are also important to generate coherent and informative summaries. 5.2 Sentence or Summary-level Extraction Extractive summarization methods usually produce a summary by selecting some original sentences in the document set by a sentence-le"
2021.emnlp-main.333,2020.acl-main.451,0,0.0429933,"ourse between sentences which is an essential point to graph. Sentences are the basic information units select interrelated summary-worthy sentences in and represented as nodes in the graph. And relaextractive summarization. Erkan and Radev (2004) tions between sentences are represented as edges. utilize a similarity graph to construct an unsuper- For example, a similarity graph can be built based vised summarization methods called LexRank. G- on cosine similarities between tf-idf representations Flow (Christensen et al., 2013) and DISCOBERT of sentences. Let G denotes a graph representation (Xu et al., 2020) both use discourse graphs to gen- matrix of the input documents, where G[i][j] indierate concise and informative summaries. Li et al. cates the tf-idf weights between sentence Si and Sj . 4064 source documents. Figure 2 illustrates the overall architecture of SgSum. Sub-graph Ranking Layer Graph Pooling Graph Pooling Graph Pooling Graph Pooling Sub-graph Encoder Graph Encoder Transformer … Doc1 Transformer DocN Figure 2: Model architecture of SgSum. Graph-based multidocument encoder takes tokenized documents as input and outputs sentence representations after graph encoding layers. Candidate"
2021.emnlp-main.333,K17-1045,0,0.0334523,"Missing"
2021.emnlp-main.333,K19-1074,0,0.0180512,"to identify pairwise ordering constraints over the Different from above studies, some work focus sentences by accounting for discourse relation- on the summary-level selection. Wan et al. (2015) ships between sentences. More recently, Yasunaga optimize the summarization performance directly 4070 based on the characteristics of summaries and rank summaries directly during inference. Bae et al. (2019), Paulus et al. (2017) and Celikyilmaz et al. (2018) use reinforcement learning to globally optimize summary-level performance. Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019) have attempted to a build two-stage document summarization. The first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Mendes et al. (2019) follow the extract-then-compress paradigm to train an extractor for content selection. Zhong et al. (2020) propose a novel extract-then-match framework which employs a sentence extractor to prune unnecessary information, then outputs a summary by matching models. These methods consider summary as a whole rather than individual sentences. However, they neglect the"
2021.emnlp-main.333,2020.acl-main.552,0,0.0746797,"hen,xiaoxinyan wu_hua,wanghaifeng}@baidu.com Abstract These models (called sentence-level extractors) do not consider summary as a whole but a combinaMost of existing extractive multi-document tion of independent sentences. This may cause summarization (MDS) methods score each incoherent and redundant problem, and result in sentence individually and extract salient sena poor summary even if the summary consists of tences one by one to compose a summary, high score sentences. Some works (Wan et al., which have two main drawbacks: (1) neglecting both the intra and cross-document relations 2015; Zhong et al., 2020) treat summary as a whole between sentences; (2) neglecting the coherunit and try to solve the weakness of sentenceence and conciseness of the whole summary. level extractors by using a summary-level extracIn this paper, we propose a novel MDS frametor. However, these models neglect the intra and work (SgSum) to formulate the MDS task as a cross-document relations between sentences which sub-graph selection problem, in which source also have benefits for extracting salient sentences, documents are regarded as a relation graph of detecting redundancy and generating overall cohersentences (e.g.,"
2021.emnlp-main.356,2020.tacl-1.30,0,0.0230087,"lingual corpora can gual, and cross-lingual conversational recombring performance improvement in comparison mendation baselines on DuRecDial 2.0. Experwith monolingual task setting, such as for the tasks iment results show that the use of additional English data can bring performance improveof task-oriented dialog (Schuster et al., 2019b), ment for Chinese conversational recommensemantic parsing (Li et al., 2021), QA and readdation, indicating the benefits of DuRecDial ing comprehension (Jing et al., 2019; Lewis et al., 2.0. Finally, this dataset provides a challeng2020; Artetxe et al., 2020; Clark et al., 2020; Hu ing testbed for future studies of monolingual, et al., 2020; Hardalov et al., 2020), machine transmultilingual, and cross-lingual conversational 1 lation(Johnson et al., 2017), document classificarecommendation. tion (Lewis et al., 2004; Klementiev et al., 2012; 1 Introduction Schwenk and Li, 2018), semantic role labelling In recent years, there has been a significant in- (Akbik et al., 2015) and NLI (Conneau et al., 2018). crease in the research topic of conversational rec- Therefore it is necessary to create multilingual conversational recommendation dataset that might enommendation due"
2021.emnlp-main.356,2020.acl-main.747,0,0.0765699,"Missing"
2021.emnlp-main.356,D18-1269,0,0.0242468,"ddation, indicating the benefits of DuRecDial ing comprehension (Jing et al., 2019; Lewis et al., 2.0. Finally, this dataset provides a challeng2020; Artetxe et al., 2020; Clark et al., 2020; Hu ing testbed for future studies of monolingual, et al., 2020; Hardalov et al., 2020), machine transmultilingual, and cross-lingual conversational 1 lation(Johnson et al., 2017), document classificarecommendation. tion (Lewis et al., 2004; Klementiev et al., 2012; 1 Introduction Schwenk and Li, 2018), semantic role labelling In recent years, there has been a significant in- (Akbik et al., 2015) and NLI (Conneau et al., 2018). crease in the research topic of conversational rec- Therefore it is necessary to create multilingual conversational recommendation dataset that might enommendation due to the rise of voice-based bots (Kang et al., 2019; Li et al., 2018; Sun and Zhang, hance model performance when compared with 2018; Christakopoulou et al., 2016; Warnestal, monolingual training setting, and it could provide 2005). These works focus on how to provide recom- a new benchmark dataset for the study of multilingual modeling techniques. mendation service in a more user-friendly manner To facilitate the study of this"
2021.emnlp-main.356,2020.emnlp-main.438,0,0.0859544,"Missing"
2021.emnlp-main.356,2020.emnlp-main.654,0,0.0324503,"ntents and slots (Li et al., dataset (DuRecDial 2.0) to enable researchers 2018; Kang et al., 2019). Recently more and more to explore a challenging task of multilingual and cross-lingual conversational recommendaefforts are devoted to the research line of the section. The difference between DuRecDial 2.0 ond category and many datasets have been created, and existing conversational recommendation including English dialog datasets (Dodge et al., datasets is that the data item (Profile, Goal, 2016; Li et al., 2018; Kang et al., 2019; Moon Knowledge, Context, Response) in DuRecDial et al., 2019; Hayati et al., 2020) and Chinese dialog 2.0 is annotated in two languages, both Endatasets (Liu et al., 2020b; Zhou et al., 2020). glish and Chinese, while other datasets are built with the setting of a single language. We However, to the best of our knowledge, almost collect 8.2k dialogs aligned across English and all these datasets are constructed in the setting Chinese languages (16.5k dialogs and 255k utof a single language, and there is no publicly terances in total) that are annotated by crowdavailable multilingual dataset for conversational sourced workers with strict quality control prorecommendation. Pre"
2021.emnlp-main.356,D19-1249,0,0.0164608,"ndation. Previous work on other NLP cedure. We then build monolingual, multilintasks have proved that multilingual corpora can gual, and cross-lingual conversational recombring performance improvement in comparison mendation baselines on DuRecDial 2.0. Experwith monolingual task setting, such as for the tasks iment results show that the use of additional English data can bring performance improveof task-oriented dialog (Schuster et al., 2019b), ment for Chinese conversational recommensemantic parsing (Li et al., 2021), QA and readdation, indicating the benefits of DuRecDial ing comprehension (Jing et al., 2019; Lewis et al., 2.0. Finally, this dataset provides a challeng2020; Artetxe et al., 2020; Clark et al., 2020; Hu ing testbed for future studies of monolingual, et al., 2020; Hardalov et al., 2020), machine transmultilingual, and cross-lingual conversational 1 lation(Johnson et al., 2017), document classificarecommendation. tion (Lewis et al., 2004; Klementiev et al., 2012; 1 Introduction Schwenk and Li, 2018), semantic role labelling In recent years, there has been a significant in- (Akbik et al., 2015) and NLI (Conneau et al., 2018). crease in the research topic of conversational rec- Therefo"
2021.emnlp-main.356,D19-1203,0,0.210984,"ng2 , Zheng-Yu Niu2 , Hua Wu2 , Wanxiang Che1† 1 Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, Harbin, China 2 Baidu Inc., Beijing, China {zmliu, car}@ir.hit.edu.cn {wanghaifeng, niuzhengyu, wu_hua}@baidu.com Abstract et al., 2016; Sun and Zhang, 2018); (2) non-task dialog-modeling approaches that can conduct more In this paper, we provide a bilingual paralfree-form interactions for recommendation, withlel human-to-human recommendation dialog out pre-defined user intents and slots (Li et al., dataset (DuRecDial 2.0) to enable researchers 2018; Kang et al., 2019). Recently more and more to explore a challenging task of multilingual and cross-lingual conversational recommendaefforts are devoted to the research line of the section. The difference between DuRecDial 2.0 ond category and many datasets have been created, and existing conversational recommendation including English dialog datasets (Dodge et al., datasets is that the data item (Profile, Goal, 2016; Li et al., 2018; Kang et al., 2019; Moon Knowledge, Context, Response) in DuRecDial et al., 2019; Hayati et al., 2020) and Chinese dialog 2.0 is annotated in two languages, both Endatasets (Liu et"
2021.emnlp-main.356,C12-1089,0,0.0384622,"a can bring performance improveof task-oriented dialog (Schuster et al., 2019b), ment for Chinese conversational recommensemantic parsing (Li et al., 2021), QA and readdation, indicating the benefits of DuRecDial ing comprehension (Jing et al., 2019; Lewis et al., 2.0. Finally, this dataset provides a challeng2020; Artetxe et al., 2020; Clark et al., 2020; Hu ing testbed for future studies of monolingual, et al., 2020; Hardalov et al., 2020), machine transmultilingual, and cross-lingual conversational 1 lation(Johnson et al., 2017), document classificarecommendation. tion (Lewis et al., 2004; Klementiev et al., 2012; 1 Introduction Schwenk and Li, 2018), semantic role labelling In recent years, there has been a significant in- (Akbik et al., 2015) and NLI (Conneau et al., 2018). crease in the research topic of conversational rec- Therefore it is necessary to create multilingual conversational recommendation dataset that might enommendation due to the rise of voice-based bots (Kang et al., 2019; Li et al., 2018; Sun and Zhang, hance model performance when compared with 2018; Christakopoulou et al., 2016; Warnestal, monolingual training setting, and it could provide 2005). These works focus on how to provi"
2021.emnlp-main.356,2020.acl-main.653,0,0.0690059,"Missing"
2021.emnlp-main.356,2021.eacl-main.257,0,0.0464825,"Missing"
2021.emnlp-main.356,N16-1014,0,0.0884497,"Missing"
2021.emnlp-main.356,2020.tacl-1.47,0,0.0591359,", 2019). Recently more and more to explore a challenging task of multilingual and cross-lingual conversational recommendaefforts are devoted to the research line of the section. The difference between DuRecDial 2.0 ond category and many datasets have been created, and existing conversational recommendation including English dialog datasets (Dodge et al., datasets is that the data item (Profile, Goal, 2016; Li et al., 2018; Kang et al., 2019; Moon Knowledge, Context, Response) in DuRecDial et al., 2019; Hayati et al., 2020) and Chinese dialog 2.0 is annotated in two languages, both Endatasets (Liu et al., 2020b; Zhou et al., 2020). glish and Chinese, while other datasets are built with the setting of a single language. We However, to the best of our knowledge, almost collect 8.2k dialogs aligned across English and all these datasets are constructed in the setting Chinese languages (16.5k dialogs and 255k utof a single language, and there is no publicly terances in total) that are annotated by crowdavailable multilingual dataset for conversational sourced workers with strict quality control prorecommendation. Previous work on other NLP cedure. We then build monolingual, multilintasks have proved tha"
2021.emnlp-main.356,L18-1560,0,0.012717,"riented dialog (Schuster et al., 2019b), ment for Chinese conversational recommensemantic parsing (Li et al., 2021), QA and readdation, indicating the benefits of DuRecDial ing comprehension (Jing et al., 2019; Lewis et al., 2.0. Finally, this dataset provides a challeng2020; Artetxe et al., 2020; Clark et al., 2020; Hu ing testbed for future studies of monolingual, et al., 2020; Hardalov et al., 2020), machine transmultilingual, and cross-lingual conversational 1 lation(Johnson et al., 2017), document classificarecommendation. tion (Lewis et al., 2004; Klementiev et al., 2012; 1 Introduction Schwenk and Li, 2018), semantic role labelling In recent years, there has been a significant in- (Akbik et al., 2015) and NLI (Conneau et al., 2018). crease in the research topic of conversational rec- Therefore it is necessary to create multilingual conversational recommendation dataset that might enommendation due to the rise of voice-based bots (Kang et al., 2019; Li et al., 2018; Sun and Zhang, hance model performance when compared with 2018; Christakopoulou et al., 2016; Warnestal, monolingual training setting, and it could provide 2005). These works focus on how to provide recom- a new benchmark dataset for"
2021.emnlp-main.356,2020.acl-main.98,1,0.911879,", 2019). Recently more and more to explore a challenging task of multilingual and cross-lingual conversational recommendaefforts are devoted to the research line of the section. The difference between DuRecDial 2.0 ond category and many datasets have been created, and existing conversational recommendation including English dialog datasets (Dodge et al., datasets is that the data item (Profile, Goal, 2016; Li et al., 2018; Kang et al., 2019; Moon Knowledge, Context, Response) in DuRecDial et al., 2019; Hayati et al., 2020) and Chinese dialog 2.0 is annotated in two languages, both Endatasets (Liu et al., 2020b; Zhou et al., 2020). glish and Chinese, while other datasets are built with the setting of a single language. We However, to the best of our knowledge, almost collect 8.2k dialogs aligned across English and all these datasets are constructed in the setting Chinese languages (16.5k dialogs and 255k utof a single language, and there is no publicly terances in total) that are annotated by crowdavailable multilingual dataset for conversational sourced workers with strict quality control prorecommendation. Previous work on other NLP cedure. We then build monolingual, multilintasks have proved tha"
2021.emnlp-main.356,2020.lrec-1.494,0,0.0246919,"Missing"
2021.emnlp-main.356,P19-1081,0,0.0642011,"Missing"
2021.emnlp-main.356,P19-1369,1,0.858284,"Missing"
2021.emnlp-main.356,P17-1163,0,0.0377515,"Missing"
2021.emnlp-main.356,2020.coling-main.365,0,0.0251136,"ore and more to explore a challenging task of multilingual and cross-lingual conversational recommendaefforts are devoted to the research line of the section. The difference between DuRecDial 2.0 ond category and many datasets have been created, and existing conversational recommendation including English dialog datasets (Dodge et al., datasets is that the data item (Profile, Goal, 2016; Li et al., 2018; Kang et al., 2019; Moon Knowledge, Context, Response) in DuRecDial et al., 2019; Hayati et al., 2020) and Chinese dialog 2.0 is annotated in two languages, both Endatasets (Liu et al., 2020b; Zhou et al., 2020). glish and Chinese, while other datasets are built with the setting of a single language. We However, to the best of our knowledge, almost collect 8.2k dialogs aligned across English and all these datasets are constructed in the setting Chinese languages (16.5k dialogs and 255k utof a single language, and there is no publicly terances in total) that are annotated by crowdavailable multilingual dataset for conversational sourced workers with strict quality control prorecommendation. Previous work on other NLP cedure. We then build monolingual, multilintasks have proved that multilingual corpor"
2021.emnlp-main.356,Q17-1022,0,0.0479985,"Missing"
2021.emnlp-main.356,P02-1040,0,0.113776,"Missing"
2021.emnlp-main.356,N19-1380,0,0.108235,"and there is no publicly terances in total) that are annotated by crowdavailable multilingual dataset for conversational sourced workers with strict quality control prorecommendation. Previous work on other NLP cedure. We then build monolingual, multilintasks have proved that multilingual corpora can gual, and cross-lingual conversational recombring performance improvement in comparison mendation baselines on DuRecDial 2.0. Experwith monolingual task setting, such as for the tasks iment results show that the use of additional English data can bring performance improveof task-oriented dialog (Schuster et al., 2019b), ment for Chinese conversational recommensemantic parsing (Li et al., 2021), QA and readdation, indicating the benefits of DuRecDial ing comprehension (Jing et al., 2019; Lewis et al., 2.0. Finally, this dataset provides a challeng2020; Artetxe et al., 2020; Clark et al., 2020; Hu ing testbed for future studies of monolingual, et al., 2020; Hardalov et al., 2020), machine transmultilingual, and cross-lingual conversational 1 lation(Johnson et al., 2017), document classificarecommendation. tion (Lewis et al., 2004; Klementiev et al., 2012; 1 Introduction Schwenk and Li, 2018), semantic role"
2021.emnlp-main.378,E17-1075,0,0.0511693,"Missing"
2021.emnlp-main.378,2020.acl-main.749,0,0.0219971,"Missing"
2021.emnlp-main.378,P18-1009,0,0.14216,"ve correct labels: where Ws is the weight parameter, Wa is the attribute embedding (i.e., word embedding of attribute words). We use cosine distance to measure similarity and employ ReLU to activate attributes. Then we induce new labels by reasoning over the activated attributes as: (j) scoreVl Learning LA = − |L| X (j) scoreVl ∗ yj (15) j=1 ( 1 yj = −1 , vj ∈ Y , vj ∈ /Y (16) Final Loss. The final loss is a combination of set loss and BAG loss: L = LS + λLA https://pypi.org/project/stanza/ 4615 (17) where λ is the relative weight of these two losses3 . 5 5.1 Model P without label dependency *Choi et al. (2018) 47.1 *ELMo(Onoe and Durrett, 2019) 51.5 BERT(Onoe and Durrett, 2019) 51.6 BERT[in-house] 55.9 with label dependency *LABELGCN (Xiong et al., 2019) 50.3 61.2 LRN w/o IR LRN 54.5 Experiments Settings Datasets We conduct experiments on two standard fine-grained entity typing datasets4 : UltraFine as primary dataset and OntoNotes as complementary dataset. Ultra-Fine contains 6K manuallyannotated examples, 2519 categories, and 5.4 labels per sample on average. Followed Choi et al. (2018) we use the same 2K/2K/2K train/dev/test splits and evaluate using macro precision, recall and F-score. Original"
2021.emnlp-main.378,D15-1103,0,0.0261385,"ning data, and therefore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods (Ma et al., 2016; Huang et al., 2016; Zhou et al., 2018; Yuan and Downey, 2018; Obeidat et al., 2019; Zhang et al., 2020b; Ren et al., 2020), or using data augmentation with denosing strategies (Ren et al., 2016b; Onoe and Durrett, 2019; Zhang et al., 2020a; Ali et al., 2020) or utilizing external knowledge (Corro et al., 2015; Dai et al., 2019) to introduce more external knowledge. In this paper, we propose Label Reasoning Network, which is significantly different from previous methods because 1) by introducing deductive reasoning, LRN can capture extrinsic dependencies between labels in an end-to-end manner without predefined structures; 2) by introducing inductive reasoning, LRN can leverage intrinsic dependencies to predict long tail labels; 3) Through the sequenceto-set framework, LRN can consider two kinds of label dependencies simultaneously to jointly reason frequent and long tail labels. mechanisms: deduct"
2021.emnlp-main.378,D19-1643,0,0.0167883,"fore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods (Ma et al., 2016; Huang et al., 2016; Zhou et al., 2018; Yuan and Downey, 2018; Obeidat et al., 2019; Zhang et al., 2020b; Ren et al., 2020), or using data augmentation with denosing strategies (Ren et al., 2016b; Onoe and Durrett, 2019; Zhang et al., 2020a; Ali et al., 2020) or utilizing external knowledge (Corro et al., 2015; Dai et al., 2019) to introduce more external knowledge. In this paper, we propose Label Reasoning Network, which is significantly different from previous methods because 1) by introducing deductive reasoning, LRN can capture extrinsic dependencies between labels in an end-to-end manner without predefined structures; 2) by introducing inductive reasoning, LRN can leverage intrinsic dependencies to predict long tail labels; 3) Through the sequenceto-set framework, LRN can consider two kinds of label dependencies simultaneously to jointly reason frequent and long tail labels. mechanisms: deductive reasoning for e"
2021.emnlp-main.378,E17-2119,0,0.0371948,"Missing"
2021.emnlp-main.378,P19-1511,1,0.89873,"Missing"
2021.emnlp-main.378,D19-1646,1,0.884217,"Missing"
2021.emnlp-main.378,2020.emnlp-main.592,1,0.893659,"Missing"
2021.emnlp-main.378,D19-1641,0,0.192756,"nt worker musician scientist police terrorism expert scientist ac-vate Ac-vate Func-on ✔ ✗ Only such a potent force, [they] theorize, could collapse some of the war ship … Figure 2: Overview of the process for LRN which contains an encoder, a deductive reasoning-based decoder and an inductive reasoning-based decoder. The figure shows: at step 1, the label person is predicted by deductive reasoning, and the attribute human is activated; at step 3, the label scientist is generated by inductive reasoning. 2019), or requiring latent label representation to reconstruct the co-occurrence structure (Lin and Ji, 2019). However, these methods require predefined label structures or statistics from training data, and therefore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods (Ma et al., 2016; Huang et al., 2016; Zhou et al., 2018; Yuan and Downey, 2018; Obeidat et al., 2019; Zhang et al., 2020b; Ren et al., 2020), or using data augmentation with denosing strategies (Ren et al., 2016b; Onoe and Dur"
2021.emnlp-main.378,2020.findings-emnlp.42,0,0.225694,"an auto-regressive network to predict labels et al. (2016a); Xu and Barbosa (2018); Wu et al. based on both the context and previous labels. For (2019); Chen et al. (2020) design new loss function example, given previously-generated label person to exploit label hierarchies. Abhishek et al. (2017) of the mention they, as well as the context they enhance the label representation by sharing paramtheorize, LRN will deduce its new label theorist eters. Shimaoka et al. (2017); Murty et al. (2018); based on the extrinsic dependency between person and theorist derived from data. For intrinsic depen- López and Strube (2020) embed labels into a highdencies, LRN introduces inductive reasoning (i.e., dimension or a new space. And the studies exploit gather generalized information to a conclusion), co-occurrence structures including limiting the label range during label set prediction (Rabinovich and utilizes a bipartite attribute graph to reason labels based on current activated attributes of previ- and Klein, 2017), enriching the label representaous labels. For example, if the attributes {expert, tion by introducing associated labels (Xiong et al., 1 scholar} have been activated, LRN will induce Our source codes a"
2021.emnlp-main.378,C16-1017,0,0.0299401,"easoning, and the attribute human is activated; at step 3, the label scientist is generated by inductive reasoning. 2019), or requiring latent label representation to reconstruct the co-occurrence structure (Lin and Ji, 2019). However, these methods require predefined label structures or statistics from training data, and therefore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods (Ma et al., 2016; Huang et al., 2016; Zhou et al., 2018; Yuan and Downey, 2018; Obeidat et al., 2019; Zhang et al., 2020b; Ren et al., 2020), or using data augmentation with denosing strategies (Ren et al., 2016b; Onoe and Durrett, 2019; Zhang et al., 2020a; Ali et al., 2020) or utilizing external knowledge (Corro et al., 2015; Dai et al., 2019) to introduce more external knowledge. In this paper, we propose Label Reasoning Network, which is significantly different from previous methods because 1) by introducing deductive reasoning, LRN can capture extrinsic dependencies between labels in an end-to-end manner"
2021.emnlp-main.378,P18-1010,0,0.018346,"d formulates it us- hierarchy and co-occurrence structures estimated from data to enhance the models. To this end, Ren ing an auto-regressive network to predict labels et al. (2016a); Xu and Barbosa (2018); Wu et al. based on both the context and previous labels. For (2019); Chen et al. (2020) design new loss function example, given previously-generated label person to exploit label hierarchies. Abhishek et al. (2017) of the mention they, as well as the context they enhance the label representation by sharing paramtheorize, LRN will deduce its new label theorist eters. Shimaoka et al. (2017); Murty et al. (2018); based on the extrinsic dependency between person and theorist derived from data. For intrinsic depen- López and Strube (2020) embed labels into a highdencies, LRN introduces inductive reasoning (i.e., dimension or a new space. And the studies exploit gather generalized information to a conclusion), co-occurrence structures including limiting the label range during label set prediction (Rabinovich and utilizes a bipartite attribute graph to reason labels based on current activated attributes of previ- and Klein, 2017), enriching the label representaous labels. For example, if the attributes {"
2021.emnlp-main.378,N19-1087,0,0.0132746,"is generated by inductive reasoning. 2019), or requiring latent label representation to reconstruct the co-occurrence structure (Lin and Ji, 2019). However, these methods require predefined label structures or statistics from training data, and therefore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods (Ma et al., 2016; Huang et al., 2016; Zhou et al., 2018; Yuan and Downey, 2018; Obeidat et al., 2019; Zhang et al., 2020b; Ren et al., 2020), or using data augmentation with denosing strategies (Ren et al., 2016b; Onoe and Durrett, 2019; Zhang et al., 2020a; Ali et al., 2020) or utilizing external knowledge (Corro et al., 2015; Dai et al., 2019) to introduce more external knowledge. In this paper, we propose Label Reasoning Network, which is significantly different from previous methods because 1) by introducing deductive reasoning, LRN can capture extrinsic dependencies between labels in an end-to-end manner without predefined structures; 2) by introducing inductive reasoning, LRN can lever"
2021.emnlp-main.378,N19-1250,0,0.152787,"nd Ji, 2019). However, these methods require predefined label structures or statistics from training data, and therefore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods (Ma et al., 2016; Huang et al., 2016; Zhou et al., 2018; Yuan and Downey, 2018; Obeidat et al., 2019; Zhang et al., 2020b; Ren et al., 2020), or using data augmentation with denosing strategies (Ren et al., 2016b; Onoe and Durrett, 2019; Zhang et al., 2020a; Ali et al., 2020) or utilizing external knowledge (Corro et al., 2015; Dai et al., 2019) to introduce more external knowledge. In this paper, we propose Label Reasoning Network, which is significantly different from previous methods because 1) by introducing deductive reasoning, LRN can capture extrinsic dependencies between labels in an end-to-end manner without predefined structures; 2) by introducing inductive reasoning, LRN can leverage intrinsic dependencies to predict long tail labels; 3) Through the sequenceto-set framework, LRN can consider two kinds of label dep"
2021.emnlp-main.378,D14-1162,0,0.0839453,"Missing"
2021.emnlp-main.378,P17-2052,0,0.0534636,"Missing"
2021.emnlp-main.378,D16-1144,0,0.0219023,"ce structure (Lin and Ji, 2019). However, these methods require predefined label structures or statistics from training data, and therefore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods (Ma et al., 2016; Huang et al., 2016; Zhou et al., 2018; Yuan and Downey, 2018; Obeidat et al., 2019; Zhang et al., 2020b; Ren et al., 2020), or using data augmentation with denosing strategies (Ren et al., 2016b; Onoe and Durrett, 2019; Zhang et al., 2020a; Ali et al., 2020) or utilizing external knowledge (Corro et al., 2015; Dai et al., 2019) to introduce more external knowledge. In this paper, we propose Label Reasoning Network, which is significantly different from previous methods because 1) by introducing deductive reasoning, LRN can capture extrinsic dependencies between labels in an end-to-end manner without predefined structures; 2) by introducing inductive reasoning, LRN can leverage intrinsic dependencies to predict long tail labels; 3) Through the sequenceto-set framework, LRN can consid"
2021.emnlp-main.378,E17-1119,0,0.0222479,"ises) between labels, and formulates it us- hierarchy and co-occurrence structures estimated from data to enhance the models. To this end, Ren ing an auto-regressive network to predict labels et al. (2016a); Xu and Barbosa (2018); Wu et al. based on both the context and previous labels. For (2019); Chen et al. (2020) design new loss function example, given previously-generated label person to exploit label hierarchies. Abhishek et al. (2017) of the mention they, as well as the context they enhance the label representation by sharing paramtheorize, LRN will deduce its new label theorist eters. Shimaoka et al. (2017); Murty et al. (2018); based on the extrinsic dependency between person and theorist derived from data. For intrinsic depen- López and Strube (2020) embed labels into a highdencies, LRN introduces inductive reasoning (i.e., dimension or a new space. And the studies exploit gather generalized information to a conclusion), co-occurrence structures including limiting the label range during label set prediction (Rabinovich and utilizes a bipartite attribute graph to reason labels based on current activated attributes of previ- and Klein, 2017), enriching the label representaous labels. For example"
2021.emnlp-main.378,D18-1121,0,0.0123657,"tation is very efficient (Zupan et al., 1999). In local BAG, we collect attributes in two ways: (1) We mask the entity mention in the sentence, and predict the [MASK] token using masked language model (this paper uses BERT-base-uncased), and the non-stop words whose prediction scores greater than a confidence threshold θc will be used as attributes — we denote them as context attributes; Since PLM usually predicts high-frequency words, the attributes are usually not long-tailed, which facilitates modeling dependencies between head and tail labels. This mask-prediction strategy is also used in Xin et al. (2018), for collecting additional semantic evidence of entity labels. (2) We directly segment the entity mention into words using Stanza2 , and all non-stop words are used as attributes — we denote them as entity attributes. Figure 3 shows several attribute examples. Given attributes, we compute the attribute-label relatedness (i.e. E in G) using the cosine similarity between their GloVe embeddings (Pennington et al., 2014). Reasoning over BAG. At each time step, we activate attributes in BAG by calculating their similarities to the current hidden state of decoder st . For the ith attribute node Va"
2021.emnlp-main.378,N19-1084,0,0.286146,"independently recognize each entity label without considering their dependencies. For this, existing approaches use the predefined label hierarchies (Ren et al., 1 Introduction 2016a; Shimaoka et al., 2017; Abhishek et al., Fine-grained entity typing (FET) aims to classify 2017; Karn et al., 2017; Xu and Barbosa, 2018; Wu entity mentions to a fine-grained semantic label et al., 2019; Chen et al., 2020; Ren, 2020) or label set, e.g., classify “FBI agents"" in “They were ar- co-occurrence statistics from training data (Rabirested by FBI agents."" as {organization, adminis- novich and Klein, 2017; Xiong et al., 2019; Lin tration, force, agent, police}. By providing fine- and Ji, 2019) as external constraints. Unfortunately, grained semantic labels, FET is critical for entity these label structures or statistics are difficult to recognition (Lin et al., 2019a,b, 2020; Zhang et al., obtain when transferring to new scenarios. Sec2021b,a) and can benefit many NLP tasks, such ond, because of the fine-grained and large-scale as relation extraction (Yaghoobzadeh et al., 2017; label set, many long tail labels are only provided Zhang et al., 2019), entity linking (Onoe and Dur- with several or even no training in"
2021.emnlp-main.378,N18-1002,0,0.0145477,"sequentially generate 2 Related Work fine-grained labels in an end-to-end, sequence-toset manner. Figure 1(c) shows several examples. One main challenge for FET is how to exploit complex label dependencies in the large-scale label To capture extrinsic dependencies, LRN introduces set. Previous studies typically use predefined label deductive reasoning (i.e., draw a conclusion based on premises) between labels, and formulates it us- hierarchy and co-occurrence structures estimated from data to enhance the models. To this end, Ren ing an auto-regressive network to predict labels et al. (2016a); Xu and Barbosa (2018); Wu et al. based on both the context and previous labels. For (2019); Chen et al. (2020) design new loss function example, given previously-generated label person to exploit label hierarchies. Abhishek et al. (2017) of the mention they, as well as the context they enhance the label representation by sharing paramtheorize, LRN will deduce its new label theorist eters. Shimaoka et al. (2017); Murty et al. (2018); based on the extrinsic dependency between person and theorist derived from data. For intrinsic depen- López and Strube (2020) embed labels into a highdencies, LRN introduces inductive"
2021.emnlp-main.378,E17-1111,0,0.0372287,"Missing"
2021.emnlp-main.378,C18-1330,0,0.0227817,"nodes Vl , use mt = [ct + g; ut + st ] as input, and calculate and edges E only exist between attributes nodes the probability distribution over label set L: and labels nodes, with the edge weight indicatst = LSTM(st−1 , Wb yt−1 ) (7) ing the attribute-label relatedness. Attributes are represented using natural language words in BAG. ot = Wo mt (8) Figure 2 shows a BAG where Va contains words yt = sof tmax(ot + It ) (9) {scholar, expert, historian, ...}, Vl are all where Wo and Wb are weight parameters and we entity labels in label set L, containing {student, muuse the mask vector It ∈ RL+1 (Yang et al., 2018) sician, scientist, ...} 4614 … the RTC would be forced until [cash] could be raised … object, money, currency, income, resource, financing cash fund, capital, interest, revenue … owner of the technology, receives [royalty payments]. Label object, money, award, payment, gift royalty, payment Entity Attribute fund, award, assistance, support Context Attribute Figure 3: Examples of attributes. BAG Construction. Because there are many labels and many attributes, we dynamically build a local BAG during the decoding for each instance. In this way the BAG is very compact and the computation is very"
2021.emnlp-main.378,D16-1015,0,0.0627348,"Missing"
2021.emnlp-main.378,2020.coling-main.7,0,0.0114082,"ive reasoning. 2019), or requiring latent label representation to reconstruct the co-occurrence structure (Lin and Ji, 2019). However, these methods require predefined label structures or statistics from training data, and therefore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods (Ma et al., 2016; Huang et al., 2016; Zhou et al., 2018; Yuan and Downey, 2018; Obeidat et al., 2019; Zhang et al., 2020b; Ren et al., 2020), or using data augmentation with denosing strategies (Ren et al., 2016b; Onoe and Durrett, 2019; Zhang et al., 2020a; Ali et al., 2020) or utilizing external knowledge (Corro et al., 2015; Dai et al., 2019) to introduce more external knowledge. In this paper, we propose Label Reasoning Network, which is significantly different from previous methods because 1) by introducing deductive reasoning, LRN can capture extrinsic dependencies between labels in an end-to-end manner without predefined structures; 2) by introducing inductive reasoning, LRN can leverage intrinsic depend"
2021.emnlp-main.378,2021.acl-long.371,1,0.831157,"Missing"
2021.emnlp-main.378,P19-1139,0,0.0404571,"Missing"
2021.emnlp-main.378,D18-1231,0,0.014152,"activated; at step 3, the label scientist is generated by inductive reasoning. 2019), or requiring latent label representation to reconstruct the co-occurrence structure (Lin and Ji, 2019). However, these methods require predefined label structures or statistics from training data, and therefore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods (Ma et al., 2016; Huang et al., 2016; Zhou et al., 2018; Yuan and Downey, 2018; Obeidat et al., 2019; Zhang et al., 2020b; Ren et al., 2020), or using data augmentation with denosing strategies (Ren et al., 2016b; Onoe and Durrett, 2019; Zhang et al., 2020a; Ali et al., 2020) or utilizing external knowledge (Corro et al., 2015; Dai et al., 2019) to introduce more external knowledge. In this paper, we propose Label Reasoning Network, which is significantly different from previous methods because 1) by introducing deductive reasoning, LRN can capture extrinsic dependencies between labels in an end-to-end manner without predefined structures; 2) by i"
2021.emnlp-main.707,P01-1008,0,0.0422567,"e., ciency with augmented data. In summary, we make data augmentation, which addresses both chal- the following contributions. lenges discussed above in a resource-cheap way. • We present a simple and resource-cheap data augThe idea of data augmentation is automatically mentation framework for cross-domain text-togenerating noisy labeled data using some delibSQL parsing with no human intervention.1 erately designed method, and the technique has • As the key component for our framework, we probeen successfully applied to a wide range of NLP pose a hierarchical SQL-to-question generation tasks (Barzilay and McKeown, 2001; Jia and Liang, model to obtain more reliable NL questions. 2016). In our cross-domain text-to-SQL task, we can directly generate labeled data over unseen DBs • In order to improve training efficiency, we proas extra training data. The key of data augmenpose a simple sampling strategy to utilize genertation is how to improve the quality of generated ated data, which is of relatively larger scale than data. As two prior works, Yu et al. (2018a) manuoriginal training data. ally align question tokens and DB elements in the 1 We release the code at https://github.com/ corresponding SQL query, in"
2021.emnlp-main.707,N19-1423,0,0.017965,"r each dataset, the first major row shows previously reported results, and the second major row gives results of our base parsers without and with data augmentation. To compare previous data augmentation methHyper-parameter settings. For each parser, we ods, we also re-implement the flat one-stage genuse default parameter settings in their released code. eration approach (FLAT) proposed by Guo et al. All these parsers are enhanced with vanilla (in (2018). We do not implement the pattern-based contrast to task-specific) pretraining models, i.e., data augmentation approach (PATTERN) of Yu BERT (Devlin et al., 2019), including IRNet-Ext. et al. (2018a) due to its requirement of human inIn order to avoid the effect of performance vitervention. Moreover, their large performance imbrations9 , we run each model for 5 times with provement is obtained over a very weak baseline. 8 The comparison of V2 and V3 is discussed at https: Performance of our baseline parsers. On Wik//github.com/microsoft/rat-sql/issues/12. iSQL, the averaged performance of our SQLova 9 Please see issues proposed at the github of RATSQL parser is lower than their reported performance by model, such as https://github.com/microsoft/ rat-sq"
2021.emnlp-main.707,P17-2090,0,0.0298472,"ta of parsing models. We conduct this experiment augmentation has been widely and successfully on the Spider dataset using IRNet model based on adopted in the computer vision field (Szegedy et al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentation for semantic parsing. Given an NL question and a knowledge base, semantic parsing aims to generate a"
2021.emnlp-main.707,P16-1154,0,0.0353237,"mainly considers as illustrated by the last two examples in Figure 3. the informativeness aspect of generated NL questions. We leave such evaluation and analysis as From the perspective of SQL syntax, HAVING and GROUP_BY, are naturally bundled together, future work, which will certainly help us better understand our proposed approach. and thus are put into one clause, as shown in the third example of Figure 3. LIMIT and ORDER_BY 2.3 Clause-to-subquestion Translation Model are similarly handled. We adopt the standard Seq2Seq model with From the second perspective, some keywords copy mechanism (Gu et al., 2016) for clause-toare not explicitly expressed in NL questions. In subquestion translation, which is also used in our other words, there is a mismatch between intents expressed in NL questions and the implementa- baseline, i.e., flat SQL-to-question translation, with tion details in SQL queries. To better align them, the same hyper-parameter settings. In the input layer, we represent every SQL towe follow IRNet (Guo et al., 2019) and combine GROUP_BY with either SELECT or ORDER_BY. ken by concatenating two embeddings, i.e., word (token as string) embedding, and token type (colFor a nested SQL quer"
2021.emnlp-main.707,D18-1188,0,0.105257,"generated questions, since very complex questions are rare in the training data. Please kindly note that our simple ASTG-based generation procedure can produce a lot of patterns unseen in the original data, because our generation is at production rule level. This is advantageous from the data variety perspective. Moreover, given a DB, we only keep executable SQL queries for correctness check. 2.2 Hierarchical SQL-to-Question Generation Given an SQL query, especially a complex one, it is difficult to generate an NL question that represents exactly same meaning. In their data augmentation work, Guo et al. (2018) use a vanilla Seq2Seq model to translate SQL queries into NL questions and obtain performance boost on WikiSQL consisting of simple queries. However, as shown in Table 2, we find performance consistently drops on all datasets over our strong baselines, which is largely due to the quality issue of generated NL questions, as illustrated in Table 3. This work proposes a hierarchical SQL-toBeing a program language, all SQL queries can be represented as nested tree structures, as depicted in Figure 2-B according to some context-free grammar. In fact, most text-to-SQL parsers proposed recently adop"
2021.emnlp-main.707,P19-1444,0,0.336098,"ikiSQL consisting of simple queries. However, as shown in Table 2, we find performance consistently drops on all datasets over our strong baselines, which is largely due to the quality issue of generated NL questions, as illustrated in Table 3. This work proposes a hierarchical SQL-toBeing a program language, all SQL queries can be represented as nested tree structures, as depicted in Figure 2-B according to some context-free grammar. In fact, most text-to-SQL parsers proposed recently adopt the abstract syntax tree representation at the decoding stage (Yin and Neubig, 2018; Yu et al., 2018a; Guo et al., 2019; Wang et al., 2020a). Following those works, we design a general ASTG that can cover all SQL patterns in our adopted benchmark datasets. Due to space limita2 As discussed in the logic form-based semantic parsing tion, Figure 2 shows a fraction of the production work of Herzig and Berant (2019), distribution mismatch is rules. mainly caused by insufficient coverage of logical form temAccording to our ASTG, the SQL query in Fig- plates. 8976 question generation model to produce higher- much easier to translate clauses to subquestions quality NL questions. The idea is motivated by compared with"
2021.emnlp-main.707,D19-1394,0,0.0189092,"ing a program language, all SQL queries can be represented as nested tree structures, as depicted in Figure 2-B according to some context-free grammar. In fact, most text-to-SQL parsers proposed recently adopt the abstract syntax tree representation at the decoding stage (Yin and Neubig, 2018; Yu et al., 2018a; Guo et al., 2019; Wang et al., 2020a). Following those works, we design a general ASTG that can cover all SQL patterns in our adopted benchmark datasets. Due to space limita2 As discussed in the logic form-based semantic parsing tion, Figure 2 shows a fraction of the production work of Herzig and Berant (2019), distribution mismatch is rules. mainly caused by insufficient coverage of logical form temAccording to our ASTG, the SQL query in Fig- plates. 8976 question generation model to produce higher- much easier to translate clauses to subquestions quality NL questions. The idea is motivated by compared with direct SQL-to-question translation. our observation that there is a strong segment-level We use a standard copy-based Seq2Seq model (Gu mapping between SQL queries and corresponding et al., 2016) for clause-to-subquestion generation. questions, as shown in Figure 3. For example, the The details"
2021.emnlp-main.707,2020.acl-main.677,0,0.257833,"uSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement. Figure 1: An example of the text-to-SQL parsing task. where all question/SQL pairs of train/dev/test sets are generated against the same DB. In order to deal with the more realistic setting where DBs in the evaluation phase are unseen in the training data, researchers propose several cross-domain datasets, such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b) in English, and DuSQL (Wang et al., 2020b) in Chinese. All three datasets adopt the DB-level data splitting, meaning that a DB and all its corresponding question/SQL pairs can appear in only one of the train/dev/test sets. Cross-domain text-to-SQL parsing has two major challenges. First, unseen DBs usually introduce 1 Introduction new schemas, such as new table/column names and unknown semantics of inter-table relationships. Given a natural language (NL) question and a relaTherefore, it is crucial for a parsing model to have tional database (DB), the text-to-SQL parsing task strong generalization ability. The second challenge aims t"
2021.emnlp-main.707,2020.emnlp-main.562,1,0.725224,"uSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement. Figure 1: An example of the text-to-SQL parsing task. where all question/SQL pairs of train/dev/test sets are generated against the same DB. In order to deal with the more realistic setting where DBs in the evaluation phase are unseen in the training data, researchers propose several cross-domain datasets, such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b) in English, and DuSQL (Wang et al., 2020b) in Chinese. All three datasets adopt the DB-level data splitting, meaning that a DB and all its corresponding question/SQL pairs can appear in only one of the train/dev/test sets. Cross-domain text-to-SQL parsing has two major challenges. First, unseen DBs usually introduce 1 Introduction new schemas, such as new table/column names and unknown semantics of inter-table relationships. Given a natural language (NL) question and a relaTherefore, it is crucial for a parsing model to have tional database (DB), the text-to-SQL parsing task strong generalization ability. The second challenge aims t"
2021.emnlp-main.707,2020.acl-main.398,0,0.0547679,"Missing"
2021.emnlp-main.707,D15-1306,0,0.0153771,"sion field (Szegedy et al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentation for semantic parsing. Given an NL question and a knowledge base, semantic parsing aims to generate a semantically equivalent formal representation, such as SQL query, logic form (LF), or task-oriented dialogue slots. Based on LF-based representation, Jia and Liang (2016"
2021.emnlp-main.707,C18-1105,0,0.102154,"ne the corresponding subquestion as the shortest question segment that contains all DB elements in the clause. Finally, we discard low-confidence clause/subquestion pairs to reduce noises, such as subquestions having large overlap with others. We keep overlapping subquestions, unless one subquestion fully contains another. In that case, we only keep the shorter subquestion. We find that a portion of collected clauses have multiple subquestion translations. For example, the clause “ORDER_BY age ASC” are translated as both “in ascending order of the age” and “from youngest to oldest”. We follow Hou et al. (2018) and use them as two independent clause/subquestion pairs for training. 2.4 Three Strategies for Utilizing Generated Data Given a set of DBs, the generated question/SQL pairs are usually of larger scale than the original training data (see Table 1), which may greatly increase training time. In this work, we compare the following three strategies for parser training. • The pre-training strategy first pre-trains the model with only generated data, and then finetunes the model with labeled training data. • The directly merging strategy trains the model with all generated data and labeled training"
2021.emnlp-main.707,D19-1670,0,0.0162761,"ve way the number of augmented pairs affects the accuracy to address the sparseness of labeled data, data of parsing models. We conduct this experiment augmentation has been widely and successfully on the Spider dataset using IRNet model based on adopted in the computer vision field (Szegedy et al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentat"
2021.emnlp-main.707,P17-1089,0,0.156978,"ion ability. The second challenge aims to produce a legal and executable SQL query is that the scale of labeled data is quite small for to get the correct answer (Date and Darwen, 1997), such a complex task, since it is extremely diffias depicted in Figure 1. A DB usually consists of cult to construct DBs and manually annotate cormultiple tables interconnected via foreign keys. responding question/SQL pairs. For example, the Early research on text-to-SQL parsing mainly Spider dataset has only 200 DBs and 10K quesfocuses on the in-domain setting (Li and Jagadish, tion/SQL pairs in total. 2014; Iyer et al., 2017; Yaghmazadeh et al., 2017), To deal with the first challenge, many previous * Work done during an internship at Baidu Inc. works focus on how to better encode the matching 8974 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8974–8983 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 2: An overview of our approach containing 3 stages: SQL query generation based on ASTG (Section §2.1), question hierarchical generation according to SQL structure (Section §2.2), model training via data augmentation (Section §2.4). among que"
2021.emnlp-main.707,P16-1002,0,0.0206393,"t al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentation for semantic parsing. Given an NL question and a knowledge base, semantic parsing aims to generate a semantically equivalent formal representation, such as SQL query, logic form (LF), or task-oriented dialogue slots. Based on LF-based representation, Jia and Liang (2016) train a synchronous"
2021.emnlp-main.707,D18-1545,0,0.0126,"ess the sparseness of labeled data, data of parsing models. We conduct this experiment augmentation has been widely and successfully on the Spider dataset using IRNet model based on adopted in the computer vision field (Szegedy et al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentation for semantic parsing. Given an NL question and a knowledge"
2021.emnlp-main.707,2020.acl-main.745,0,0.0314504,"Missing"
2021.emnlp-main.707,D18-1193,0,0.351191,".e., WikiSQL and Spider in English, and DuSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement. Figure 1: An example of the text-to-SQL parsing task. where all question/SQL pairs of train/dev/test sets are generated against the same DB. In order to deal with the more realistic setting where DBs in the evaluation phase are unseen in the training data, researchers propose several cross-domain datasets, such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b) in English, and DuSQL (Wang et al., 2020b) in Chinese. All three datasets adopt the DB-level data splitting, meaning that a DB and all its corresponding question/SQL pairs can appear in only one of the train/dev/test sets. Cross-domain text-to-SQL parsing has two major challenges. First, unseen DBs usually introduce 1 Introduction new schemas, such as new table/column names and unknown semantics of inter-table relationships. Given a natural language (NL) question and a relaTherefore, it is crucial for a parsing model to have tional database (DB), the text-to-SQL parsing task strong generali"
2021.emnlp-main.707,D18-1425,0,0.0470955,"Missing"
2021.findings-acl.191,E17-1005,0,0.0224974,"Missing"
2021.findings-acl.191,P19-1595,0,0.0356742,"Missing"
2021.findings-acl.191,N19-1423,0,0.0102182,"ct about 1.8 million unlabeled queries from Yahoo! Answers4 , ORCAS (Craswell et al., 2020), SQuAD (Rajpurkar et al.), TriviaQA (Joshi et al., 2017) and HotpotQA (Yang et al., 2018). In the pre-training stage, we reuse the passage collections from the labeled corpus (MSMARCO and NQ). 4.2 Implementation Details We conduct experiments with the deep learning framework PaddlePaddle (Ma et al., 2019) on up to eight NVIDIA Tesla V100 GPUs (with 32G RAM). Pre-trained LMs The dual-encoder is initialized with the parameters of ERNIE-2.0 base (Sun et al., 2020). ERNIE-2.0 has the same networks as BERT (Devlin et al., 2019), and it introduces a continual pre-training framework on multiple pretrained tasks. The cross-encoder setting follows the cross-encoder in RocketQA (Qu et al., 2020) Hyper-parameters (a) batch size: Our dualencoder is trained with a batch size of 512 × 1 in fine-tuning stage on NQ and 512 × 8 in other settings. We use the in-batch negative setting (Karpukhin et al., 2020) on NQ and crossbatch negative setting (Qu et al., 2020) on MSMARCO. (b) training epochs: The number of training epochs is set up to 10 for both pre-training and fine-tuning for dual-encoder. (c) warm-up and learning rate: Th"
2021.findings-acl.191,K19-1049,0,0.119052,") (b) Figure 1: An illustrative case of a query q, its positive passage p+ and negative passage p− : (a) Query-centric similarity relation enforces s(q, p+ ) &gt; s(q, p− ); (b) Passage-centric similarity relation further enforces s(p+ , q) &gt; s(p+ , p− ), where s(p+ , q) = s(q, p+ ). We use the distance (i.e., dissimilarity) for visualization: the longer the distance is, the less similar it is. ing question answering (Lee et al., 2019; Xiong et al., 2020b), information retrieval (Luan et al., 2021; Khattab and Zaharia, 2020), dialogue (Ji et al., 2014; Henderson et al., 2017) and entity linking (Gillick et al., 2019; Wu et al., 2020). With the recent advances of pre-trained language models, dense passage retrieval techniques (representing queries and passages in low-dimensional semantic space) have significantly outperformed traditional term-based techniques (Guu et al., 2020; Karpukhin et al., 2020). As the key step of finding the relevant information, it has been shown that dense passage retrieval can effectively improve the performance in a variety of tasks, includEqual contribution. The work was done when Ruiyang Ren was doing internship at Baidu. ‡ Corresponding authors. 1 Our code is available at h"
2021.findings-acl.191,P17-1147,0,0.0201338,"BERTbase ERNIEbase RoBERTabase BERTlarge BERTbase BERTbase ERNIEbase 32.5 33.0 34.3 31.1 36.0 37.0 82.2 82.9 85.5 97.3 95.9 97.7 96.8 97.9 68.4 73.3 74.0 78.4 80.7 81.9 82.8 82.7 85.4 87.3 87.5 88.4 88.5 PAIR (Ours) ERNIEbase 37.9 86.4 98.2 74.9 83.5 89.1 Table 2: Experimental results on MSMARCO and Natural Questions datasets. Note that we copy the results from original papers and we leave it blank if the original paper does not report the result. tion data, we collect about 1.8 million unlabeled queries from Yahoo! Answers4 , ORCAS (Craswell et al., 2020), SQuAD (Rajpurkar et al.), TriviaQA (Joshi et al., 2017) and HotpotQA (Yang et al., 2018). In the pre-training stage, we reuse the passage collections from the labeled corpus (MSMARCO and NQ). 4.2 Implementation Details We conduct experiments with the deep learning framework PaddlePaddle (Ma et al., 2019) on up to eight NVIDIA Tesla V100 GPUs (with 32G RAM). Pre-trained LMs The dual-encoder is initialized with the parameters of ERNIE-2.0 base (Sun et al., 2020). ERNIE-2.0 has the same networks as BERT (Devlin et al., 2019), and it introduces a continual pre-training framework on multiple pretrained tasks. The cross-encoder setting follows the cross"
2021.findings-acl.191,2020.emnlp-main.550,0,0.0584937,"e the distance (i.e., dissimilarity) for visualization: the longer the distance is, the less similar it is. ing question answering (Lee et al., 2019; Xiong et al., 2020b), information retrieval (Luan et al., 2021; Khattab and Zaharia, 2020), dialogue (Ji et al., 2014; Henderson et al., 2017) and entity linking (Gillick et al., 2019; Wu et al., 2020). With the recent advances of pre-trained language models, dense passage retrieval techniques (representing queries and passages in low-dimensional semantic space) have significantly outperformed traditional term-based techniques (Guu et al., 2020; Karpukhin et al., 2020). As the key step of finding the relevant information, it has been shown that dense passage retrieval can effectively improve the performance in a variety of tasks, includEqual contribution. The work was done when Ruiyang Ren was doing internship at Baidu. ‡ Corresponding authors. 1 Our code is available at https://github.com/ PaddlePaddle/Research/tree/master/NLP/ ACL2021-PAIR † p+ q pIntroduction ∗ p+ q Typically, the dual-encoder architecture is used to learn the dense representations of queries and passages, and the dot-product similarity between the representations of queries and passages"
2021.findings-acl.191,Q19-1026,0,0.0137076,"etter retrieval performance. In this stage, we use both ground-truth labels and pseudo labels derived from the labeled corpus for training. 4 Pre-training with learning both query-centric similarity relation (QSR) and passage-centric similarity relation (PSR) Experiments In this section, we first describe the experimental settings, then report the main experimental results, ablation study and detailed analysis. 4.1 Experimental Settings Datasets This paper focuses on the passage retrieval task. We conduct experiments on two public datasets: MSMARCO (Nguyen et al., 2016) and Natural Questions (Kwiatkowski et al., 2019). The statistics of the datasets are listed in Table 1. MSMARCO was originally designed for multiple passage machine reading comprehension, and its queries were sampled from Bing search logs. Based on the queries and passages in MSMARCO Question Answering, a dataset for passage retrieval and ranking was created, namely MSMARCO Passage Ranking. Natural Questions (NQ) was originally introduced as a dataset for open-domain QA. The queries were collected from Google search logs. DPR (Karpukhin et al., 2020) selected the queries that had short answers, and processed all the Wikipedia articles as th"
2021.findings-acl.191,P19-1612,0,0.189578,"relation constraint. Extensive experiments show that our approach significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions datasets1 . 1 p(a) (b) Figure 1: An illustrative case of a query q, its positive passage p+ and negative passage p− : (a) Query-centric similarity relation enforces s(q, p+ ) &gt; s(q, p− ); (b) Passage-centric similarity relation further enforces s(p+ , q) &gt; s(p+ , p− ), where s(p+ , q) = s(q, p+ ). We use the distance (i.e., dissimilarity) for visualization: the longer the distance is, the less similar it is. ing question answering (Lee et al., 2019; Xiong et al., 2020b), information retrieval (Luan et al., 2021; Khattab and Zaharia, 2020), dialogue (Ji et al., 2014; Henderson et al., 2017) and entity linking (Gillick et al., 2019; Wu et al., 2020). With the recent advances of pre-trained language models, dense passage retrieval techniques (representing queries and passages in low-dimensional semantic space) have significantly outperformed traditional term-based techniques (Guu et al., 2020; Karpukhin et al., 2020). As the key step of finding the relevant information, it has been shown that dense passage retrieval can effectively improve"
2021.findings-acl.191,2020.emnlp-main.519,0,0.291894,"ustrative case of a query q, its positive passage p+ and negative passage p− : (a) Query-centric similarity relation enforces s(q, p+ ) &gt; s(q, p− ); (b) Passage-centric similarity relation further enforces s(p+ , q) &gt; s(p+ , p− ), where s(p+ , q) = s(q, p+ ). We use the distance (i.e., dissimilarity) for visualization: the longer the distance is, the less similar it is. ing question answering (Lee et al., 2019; Xiong et al., 2020b), information retrieval (Luan et al., 2021; Khattab and Zaharia, 2020), dialogue (Ji et al., 2014; Henderson et al., 2017) and entity linking (Gillick et al., 2019; Wu et al., 2020). With the recent advances of pre-trained language models, dense passage retrieval techniques (representing queries and passages in low-dimensional semantic space) have significantly outperformed traditional term-based techniques (Guu et al., 2020; Karpukhin et al., 2020). As the key step of finding the relevant information, it has been shown that dense passage retrieval can effectively improve the performance in a variety of tasks, includEqual contribution. The work was done when Ruiyang Ren was doing internship at Baidu. ‡ Corresponding authors. 1 Our code is available at https://github.com/"
2021.findings-acl.191,D18-1259,0,0.019867,"Tlarge BERTbase BERTbase ERNIEbase 32.5 33.0 34.3 31.1 36.0 37.0 82.2 82.9 85.5 97.3 95.9 97.7 96.8 97.9 68.4 73.3 74.0 78.4 80.7 81.9 82.8 82.7 85.4 87.3 87.5 88.4 88.5 PAIR (Ours) ERNIEbase 37.9 86.4 98.2 74.9 83.5 89.1 Table 2: Experimental results on MSMARCO and Natural Questions datasets. Note that we copy the results from original papers and we leave it blank if the original paper does not report the result. tion data, we collect about 1.8 million unlabeled queries from Yahoo! Answers4 , ORCAS (Craswell et al., 2020), SQuAD (Rajpurkar et al.), TriviaQA (Joshi et al., 2017) and HotpotQA (Yang et al., 2018). In the pre-training stage, we reuse the passage collections from the labeled corpus (MSMARCO and NQ). 4.2 Implementation Details We conduct experiments with the deep learning framework PaddlePaddle (Ma et al., 2019) on up to eight NVIDIA Tesla V100 GPUs (with 32G RAM). Pre-trained LMs The dual-encoder is initialized with the parameters of ERNIE-2.0 base (Sun et al., 2020). ERNIE-2.0 has the same networks as BERT (Devlin et al., 2019), and it introduces a continual pre-training framework on multiple pretrained tasks. The cross-encoder setting follows the cross-encoder in RocketQA (Qu et al.,"
2021.findings-acl.198,C10-1041,0,0.115298,"Missing"
2021.findings-acl.198,D19-5522,0,0.724457,"tains an incorrect word “的(de, of)”. The CSC model produces a fluent but incorrect sentence by replacing “的(de, 1 In this paper, we ignore the tone of Pinyin, and use homophone to refer to characters with the same pinyin spellings. 2250 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2250–2261 August 1–6, 2021. ©2021 Association for Computational Linguistics of)” with “英(ying, English)”. However, the pronunciations of these two words are totally different, because the model ignores phonetic features. Recent studies tackle the issue using deep neural networks. Hong et al. (2019) used a pre-trained language model BERT (Devlin et al., 2019) to generate candidates and train a classifier with phonetic features to select the final correction. Wang et al. (2019) considered CSC as a sequence-to-sequence task and generated candidates from a confusion set 2 instead of the entire vocabulary. These methods take phonetic information as external knowledge but the discrete candidate selection obstructs the language model from learning directly via backpropagation. Zhang et al. (2020) proposed an endto-end CSC model by modifying the mask mechanism of BERT. However, they did not use"
2021.findings-acl.198,W13-4416,0,0.167613,"ically similar characters. As illustrated in Figure 1, the character “德(de, German)” is incorrectly typed as one of its homophone1 “的(de, of)”. Traditional methods of CSC firstly detect misspelled characters and generate candidates via a language model, and then use a phonetic model or rules to filter wrong candidates (Chang, 1995; Chen et al., 2013; Dong et al., 2016). To improve CSC performance, studies mainly focus on two issues: 1) how to improve the language model (Wu et al., 2010; Dong et al., 2016; Zhang et al., 2020) and 2) how to utilize external knowledge of phonological similarity (Jia et al., 2013; Yu and Li, 2014; Wang et al., 2018; Cheng et al., 2020). The language model is used to generate fluent sentences and the phonetic features can prevent the model from producing predictions whose pronunciation deviates from that of the original word. As illustrated in Fig. 1, the original Wrong sentence contains an incorrect word “的(de, of)”. The CSC model produces a fluent but incorrect sentence by replacing “的(de, 1 In this paper, we ignore the tone of Pinyin, and use homophone to refer to characters with the same pinyin spellings. 2250 Findings of the Association for Computational Linguisti"
2021.findings-acl.198,C10-2085,0,0.725247,"Missing"
2021.findings-acl.198,W13-4409,0,0.0258715,"l that incorporates phonetic features into language representation. The model encodes the Chinese characters and Pinyin tokens in a shared space. • The integration of phonological information greatly facilitates CSC. Experimental results on the benchmark SIGHAN datasets show that 2 Confusion set is a set of similar characters. our method significantly outperforms the previous state-of-the-art methods. 2 Related work Earier work on CSC follows the pipeline of error detection, candidate generation, and candidate selection (Wu et al., 2010; Jia et al., 2013; Chen et al., 2013; Chiu et al., 2013; Liu et al., 2013; Xin et al., 2014; Yu and Li, 2014; Dong et al., 2016; Wang et al., 2018). These methods mainly employ unsupervised language models and rules to select candidates. With the development of end-to-end networks, some work proposed to optimize the error correction performance directly as a sequence-labeling task with conditional random fields (CRF) (Wu et al., 2018) and recurrent neural networks (RNN) (Zheng et al., 2016; Yang et al., 2017). Wang et al. (2019) used a sequence-to-sequence framework with copy mechanism to copy the correction results directly from a prepared confusion set for the er"
2021.findings-acl.198,2020.acl-main.81,0,0.600889,"the character “德(de, German)” is incorrectly typed as one of its homophone1 “的(de, of)”. Traditional methods of CSC firstly detect misspelled characters and generate candidates via a language model, and then use a phonetic model or rules to filter wrong candidates (Chang, 1995; Chen et al., 2013; Dong et al., 2016). To improve CSC performance, studies mainly focus on two issues: 1) how to improve the language model (Wu et al., 2010; Dong et al., 2016; Zhang et al., 2020) and 2) how to utilize external knowledge of phonological similarity (Jia et al., 2013; Yu and Li, 2014; Wang et al., 2018; Cheng et al., 2020). The language model is used to generate fluent sentences and the phonetic features can prevent the model from producing predictions whose pronunciation deviates from that of the original word. As illustrated in Fig. 1, the original Wrong sentence contains an incorrect word “的(de, of)”. The CSC model produces a fluent but incorrect sentence by replacing “的(de, 1 In this paper, we ignore the tone of Pinyin, and use homophone to refer to characters with the same pinyin spellings. 2250 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2250–2261 August 1–6, 2021. ©2"
2021.findings-acl.198,W13-4408,0,0.0240902,"end-to-end CSC model that incorporates phonetic features into language representation. The model encodes the Chinese characters and Pinyin tokens in a shared space. • The integration of phonological information greatly facilitates CSC. Experimental results on the benchmark SIGHAN datasets show that 2 Confusion set is a set of similar characters. our method significantly outperforms the previous state-of-the-art methods. 2 Related work Earier work on CSC follows the pipeline of error detection, candidate generation, and candidate selection (Wu et al., 2010; Jia et al., 2013; Chen et al., 2013; Chiu et al., 2013; Liu et al., 2013; Xin et al., 2014; Yu and Li, 2014; Dong et al., 2016; Wang et al., 2018). These methods mainly employ unsupervised language models and rules to select candidates. With the development of end-to-end networks, some work proposed to optimize the error correction performance directly as a sequence-labeling task with conditional random fields (CRF) (Wu et al., 2018) and recurrent neural networks (RNN) (Zheng et al., 2016; Yang et al., 2017). Wang et al. (2019) used a sequence-to-sequence framework with copy mechanism to copy the correction results directly from a prepared confus"
2021.findings-acl.198,C12-1144,0,0.073701,"Missing"
2021.findings-acl.198,W10-4107,0,0.198678,"e spelling errors on the Internet results from Corresponding author. de He German language speak 1 Introduction ∗ ta phonologically similar characters. As illustrated in Figure 1, the character “德(de, German)” is incorrectly typed as one of its homophone1 “的(de, of)”. Traditional methods of CSC firstly detect misspelled characters and generate candidates via a language model, and then use a phonetic model or rules to filter wrong candidates (Chang, 1995; Chen et al., 2013; Dong et al., 2016). To improve CSC performance, studies mainly focus on two issues: 1) how to improve the language model (Wu et al., 2010; Dong et al., 2016; Zhang et al., 2020) and 2) how to utilize external knowledge of phonological similarity (Jia et al., 2013; Yu and Li, 2014; Wang et al., 2018; Cheng et al., 2020). The language model is used to generate fluent sentences and the phonetic features can prevent the model from producing predictions whose pronunciation deviates from that of the original word. As illustrated in Fig. 1, the original Wrong sentence contains an incorrect word “的(de, of)”. The CSC model produces a fluent but incorrect sentence by replacing “的(de, 1 In this paper, we ignore the tone of Pinyin, and use"
2021.findings-acl.198,W14-6835,0,0.712709,"racters. As illustrated in Figure 1, the character “德(de, German)” is incorrectly typed as one of its homophone1 “的(de, of)”. Traditional methods of CSC firstly detect misspelled characters and generate candidates via a language model, and then use a phonetic model or rules to filter wrong candidates (Chang, 1995; Chen et al., 2013; Dong et al., 2016). To improve CSC performance, studies mainly focus on two issues: 1) how to improve the language model (Wu et al., 2010; Dong et al., 2016; Zhang et al., 2020) and 2) how to utilize external knowledge of phonological similarity (Jia et al., 2013; Yu and Li, 2014; Wang et al., 2018; Cheng et al., 2020). The language model is used to generate fluent sentences and the phonetic features can prevent the model from producing predictions whose pronunciation deviates from that of the original word. As illustrated in Fig. 1, the original Wrong sentence contains an incorrect word “的(de, of)”. The CSC model produces a fluent but incorrect sentence by replacing “的(de, 1 In this paper, we ignore the tone of Pinyin, and use homophone to refer to characters with the same pinyin spellings. 2250 Findings of the Association for Computational Linguistics: ACL-IJCNLP 20"
2021.findings-acl.198,2020.acl-main.82,0,0.197201,"sults from Corresponding author. de He German language speak 1 Introduction ∗ ta phonologically similar characters. As illustrated in Figure 1, the character “德(de, German)” is incorrectly typed as one of its homophone1 “的(de, of)”. Traditional methods of CSC firstly detect misspelled characters and generate candidates via a language model, and then use a phonetic model or rules to filter wrong candidates (Chang, 1995; Chen et al., 2013; Dong et al., 2016). To improve CSC performance, studies mainly focus on two issues: 1) how to improve the language model (Wu et al., 2010; Dong et al., 2016; Zhang et al., 2020) and 2) how to utilize external knowledge of phonological similarity (Jia et al., 2013; Yu and Li, 2014; Wang et al., 2018; Cheng et al., 2020). The language model is used to generate fluent sentences and the phonetic features can prevent the model from producing predictions whose pronunciation deviates from that of the original word. As illustrated in Fig. 1, the original Wrong sentence contains an incorrect word “的(de, of)”. The CSC model produces a fluent but incorrect sentence by replacing “的(de, 1 In this paper, we ignore the tone of Pinyin, and use homophone to refer to characters with t"
2021.findings-acl.198,W16-4907,0,0.0267098,"Earier work on CSC follows the pipeline of error detection, candidate generation, and candidate selection (Wu et al., 2010; Jia et al., 2013; Chen et al., 2013; Chiu et al., 2013; Liu et al., 2013; Xin et al., 2014; Yu and Li, 2014; Dong et al., 2016; Wang et al., 2018). These methods mainly employ unsupervised language models and rules to select candidates. With the development of end-to-end networks, some work proposed to optimize the error correction performance directly as a sequence-labeling task with conditional random fields (CRF) (Wu et al., 2018) and recurrent neural networks (RNN) (Zheng et al., 2016; Yang et al., 2017). Wang et al. (2019) used a sequence-to-sequence framework with copy mechanism to copy the correction results directly from a prepared confusion set for the erroneous words. Cheng et al. (2020) built a graph convolution network (GCN) on top of BERT (Devlin et al., 2019) and the graph was constructed from a confusion set. Zhang et al. (2020) proposed a soft-masked BERT model that first predicts the probability of spelling error for each word, and then uses the probabilities to perform a soft-masked word embedding for correction. However, they did not use any phonetic informa"
2021.findings-acl.198,W13-4406,0,0.498195,"Missing"
2021.findings-acl.222,N16-1014,0,0.272785,"mpared to DialoGPT, as they have similar model scales. • PLATO-2 93M parameter model is a tiny version in English, which is trained with Reddit comments. As it is difficult to scale up PLATO, we use this version to compare with PLATO. • PLATO-2 336M parameter Chinese model2 will be compared to XiaoIce in the experiments. 2 This model has 24 transformer blocks and 16 attention 3.3.2 Evaluation Metrics We carry out both automatic and human evaluations in the experiments. In automatic evaluation, to assess the model’s capacity on lexical diversity, we use the corpus-level metric of distinct-1/2 (Li et al., 2016a), which is defined as the number of distinct uni- or bi-grams divided by the total number of generated words. In human evaluation, we employ four utterancelevel and dialogue-level metrics, including coherence, informativeness, engagingness and humanness. Three crowd-sourcing workers are asked to score the response/dialogue quality on a scale of [0, 1, 2], with the final score determined through majority voting. The higher score, the better. These criteria are discussed as follows, with scoring details provided in the Appendix. • Coherence is an utterance-level metric, measuring whether the r"
2021.findings-acl.222,D16-1127,0,0.126928,"mpared to DialoGPT, as they have similar model scales. • PLATO-2 93M parameter model is a tiny version in English, which is trained with Reddit comments. As it is difficult to scale up PLATO, we use this version to compare with PLATO. • PLATO-2 336M parameter Chinese model2 will be compared to XiaoIce in the experiments. 2 This model has 24 transformer blocks and 16 attention 3.3.2 Evaluation Metrics We carry out both automatic and human evaluations in the experiments. In automatic evaluation, to assess the model’s capacity on lexical diversity, we use the corpus-level metric of distinct-1/2 (Li et al., 2016a), which is defined as the number of distinct uni- or bi-grams divided by the total number of generated words. In human evaluation, we employ four utterancelevel and dialogue-level metrics, including coherence, informativeness, engagingness and humanness. Three crowd-sourcing workers are asked to score the response/dialogue quality on a scale of [0, 1, 2], with the final score determined through majority voting. The higher score, the better. These criteria are discussed as follows, with scoring details provided in the Appendix. • Coherence is an utterance-level metric, measuring whether the r"
2021.findings-acl.222,I17-1099,0,0.0192107,"ents over XiaoIce across all the human evaluation metrics. Model Static Evaluation Besides the interactive evaluation, we also employ static evaluation to analyze the model’s performance. In static evaluation, each model will produce a response towards the given multi-turn context. Those powerful models are involved in the evaluation: Meena, Blender, DialoGPT and PLATO-2 1.6B. To compare with Meena, we include their provided 60 static samples in the Appendix of the paper and generate corresponding responses with other models. We also include 60 test samples about daily life from Daily Dialog (Li et al., 2017) and 60 test samples about in-depth discussion from Reddit. Given that the measurement of humanness usually needs multi-turn interaction, this metric is excluded from static evaluation. The evaluation results are summarized in Table 3. It can be observed that PLATO-2 is able to produce coherent, informative and engaging responses across different chat scenarios. The average Fleiss’s kappa (Fleiss, 1971) of human evaluation is 0.466, indicating annotators have reached moderate agreement. Informativeness Engagingness Meena 1.750 1.617 1.583 DialoGPT 1.233 1.067 1.017 Blender 1.800 1.767 1.683 PL"
2021.findings-acl.222,2021.eacl-main.24,0,0.577455,"eat success in natural language processing (Devlin et al., 2019), especially open-domain dialogue generation. For instance, based on the general language model GPT-2 (Radford et al., 2019), DialoGPT (Zhang et al., 2020) is further trained for response generation using Reddit comments. To obtain a human-like open-domain chatbot, Meena (Adiwardana et al., 2020) scales up the network parameters to 2.6B and employs more social media conversations in the training process, leading to significant improvement on response quality. To mitigate undesirable toxic or bias traits of large corpora, Blender (Roller et al., 2021) fine-tunes the pretrained model with human annotated datasets and emphasizes desirable conversational skills of engagingness, knowledge, empathy and personality. In addition to the attempts from model scale and data selection, PLATO (Bao et al., 2020) aims Equal contribution. Stage 2.2 Evaluation Response Coherence Estimation Introduction ∗ One-to-Many Mapping Diverse Response Generation to tackle the inherent one-to-many mapping problem to improve response quality. The one-to-many mapping refers to that one dialogue context might correspond to multiple appropriate responses. It is widely rec"
2021.findings-acl.222,P16-1162,0,0.0583211,"The English training data is extracted from Reddit comments, which are collected by a third party and made publicly available on pushshift.io (Baumgartner et al., 2020). To improve the generation quality, we carry out elaborate data cleaning, as discussed in the Appendix. After filtering, the data is split into training and validation sets in chronological order. The training set contains 684M (context, response) samples, ranging from December 2005 to July 2019. For the validation set, 0.2M samples are selected from the rest data after July 2019. The English vocabulary contains 8K BPE tokens (Sennrich et al., 2016), constructed with the SentencePiece library. The Chinese training data is collected from public domain social medias. After filtering, there are 1.2B (context, response) samples in the training set, 0.1M samples in the validation set, and 0.1M samples in the test set. As for the Chinese vocabulary, it contains 30K BPE tokens. 3.2 Training Details PLATO-2 has three model sizes: a standard version of 1.6B parameters, a small version of 314M parameters, and a tiny version of 93M parameters. Detailed network and training configurations are summarized in the Appendix. The main hyperparameters used"
2021.findings-acl.222,2020.acl-main.183,0,0.02038,"020) is trained on the basis of BERTBASE using 8.3M Twitter and Reddit conversations (Cho et al., 2014; Zhou et al., 2018; Galley et al., 2019). There are 132M network parameters in this model. • DialoGPT (Zhang et al., 2020) is trained on the basis of GPT-2 (Radford et al., 2019) using Reddit comments. There are three model sizes: 117M, 345M and 762M. Since the 345M parameter model obtains the best performance in their evaluations, we compare with this version. • Blender (Roller et al., 2021) is first trained using Reddit comments and then fine-tuned with human annotated conversations – BST (Smith et al., 2020), to help emphasize desirable conversational skills of engagingness, knowledge, empathy and personality. Blender has three model sizes: 90M, 2.7B and 9.4B. Since the 2.7B parameter model obtains the best performance in their evaluations, we compare with this version. • Meena (Adiwardana et al., 2020) is an opendomain chatbot trained with social media conversations. There are 2.6B network parameters in Meena. Since Meena has not released the model or provided a service interface, it is difficult to perform comprehensive comparison. In the experiments, we include the provided samples in their pa"
2021.findings-acl.222,2020.acl-demos.30,0,0.484041,"ct the best response, respectively. PLATO-2 was trained on both Chinese and English data, whose effectiveness and superiority are verified through comprehensive evaluations, achieving new state-of-the-art results. 1 One-to-One Mapping General Response Generation Figure 1: Curriculum learning process in PLATO-2. Recently, task agnostic pre-training with largescale transformer models has achieved great success in natural language processing (Devlin et al., 2019), especially open-domain dialogue generation. For instance, based on the general language model GPT-2 (Radford et al., 2019), DialoGPT (Zhang et al., 2020) is further trained for response generation using Reddit comments. To obtain a human-like open-domain chatbot, Meena (Adiwardana et al., 2020) scales up the network parameters to 2.6B and employs more social media conversations in the training process, leading to significant improvement on response quality. To mitigate undesirable toxic or bias traits of large corpora, Blender (Roller et al., 2021) fine-tunes the pretrained model with human annotated datasets and emphasizes desirable conversational skills of engagingness, knowledge, empathy and personality. In addition to the attempts from mod"
2021.findings-acl.222,P17-1061,0,0.116345,"engagingness, knowledge, empathy and personality. In addition to the attempts from model scale and data selection, PLATO (Bao et al., 2020) aims Equal contribution. Stage 2.2 Evaluation Response Coherence Estimation Introduction ∗ One-to-Many Mapping Diverse Response Generation to tackle the inherent one-to-many mapping problem to improve response quality. The one-to-many mapping refers to that one dialogue context might correspond to multiple appropriate responses. It is widely recognized that the capability of modeling one-to-many relationship is crucial for opendomain dialogue generation (Zhao et al., 2017; Chen et al., 2019). PLATO explicitly models this one-to-many relationship via discrete latent variables, aiming to boost the quality of dialogue generation. PLATO has a modest scale of 132M network parameters and trained with 8M samples, achieving relatively good performance among conversation models on a similar scale. However, scaling up PLATO directly encounters training instability and efficiency issues, which might result from the difficulty to capture the one-to-many semantic relationship from scratch. In this work, we try to scale up PLATO to PLATO-2 and introduce an effective trainin"
2021.findings-emnlp.29,D18-1149,0,0.0658785,"Missing"
2021.findings-emnlp.29,P19-1177,0,0.039526,"Missing"
2021.findings-emnlp.29,W19-6622,0,0.063172,"Missing"
2021.findings-emnlp.29,W18-6301,0,0.0672122,"Missing"
2021.findings-emnlp.29,W16-2323,0,0.0803376,"Missing"
2021.findings-emnlp.29,2020.emnlp-main.82,1,0.798934,"Missing"
2021.naacl-main.136,S17-2001,0,0.0162971,"ined models. 4.3 Results on GLUE Benchmark The General Language Understanding Evaluation (GLUE; Wang et al., 2018) is a multi-task benchmark consisting of various NLU tasks, which contains 1) pairwise classification tasks like language inference (MNLI; Williams et al., 2018, RTE; Dagan et al., 2006), question answering (QNLI; Rajpurkar et al., 2016) and paraphrase detection (QQP, MRPC; Dolan and Brockett, 2005), 2) singlesentence classification tasks like linguistic acceptability (CoLA; Warstadt et al., 2019), sentiment analysis (SST-2; Socher et al., 2013) and 3) text similarity task (STS-B; Cer et al., 2017). The fine-tuning results on GLUE of ERNIEGram and various strong baselines are presented in Table 1. For fair comparison, the listed models are all in base size and fine-tuned without any data augmentation. Pre-trained with base-scale text corpora, ERNIE-Gram outperforms recent models such as TUPE and F-TFM by 1.7 and 1.3 points on average. As for large-scale text corpora, ERNIEGram achieves average score increase of 1.7 and 0.6 over RoBERTa and ELECTRA, demonstrating the effectiveness of ERNIE-Gram. 4.4 Results on Question Answering (SQuAD) The Stanford Question Answering (SQuAD) tasks are d"
2021.naacl-main.136,D18-1269,0,0.020724,"020). passages. We also evaluate ERNIE-Gram on two large scaled text classification tasks that involve long text and reasoning, including sentiment analysis datasets IMDb (Maas et al., 2011) and topic classification dataset AG’s News (Zhang et al., 2015). The results are reported in Table 3. It can be seen that ERNIE-Gram consistently outperforms previous models, showing the advantage of ERNIEGram on tasks involving long text and reasoning. 4.6 Results on Chinese NLU Tasks We execute extensive experiments on six Chinese language understanding tasks, including natural language inference (XNLI; Conneau et al., 2018), 1707 Models XNLI LCQMC Acc Acc Dev Test Dev Test DRCD EM / F1 Dev Test CMRC2018 DuReader M-NER EM / F1 EM / F1 F1 Dev Dev Dev Test RoBERTa-wwn-ext∗LARGE 82.1 81.2 90.4 87.0 89.6 / 94.8 89.6 / 94.5 68.5 / 88.4 NEZHALARGE (Wei et al., 2019) 82.2 81.2 90.9 87.9 -/-/-/MacBERTLARGE (Cui et al., 2020) 82.4 81.3 90.6 87.6 91.2 / 95.6 91.7 / 95.6 70.7 / 88.9 -/-/-/- - - BERT-wwn-ext∗BASE RoBERTa-wwn-ext∗BASE Z ENBASE (Diao et al., 2020) NEZHABASE (Wei et al., 2019) MacBERTBASE (Cui et al., 2020) ERNIE1.0BASE (Sun et al., 2019b) ERNIE2.0BASE (Sun et al., 2020) 79.4 80.0 80.5 81.4 80.3 79.9 81.2 ERNIE"
2021.naacl-main.136,2020.findings-emnlp.58,0,0.0474497,"asking and train the standard model θ to predict the original n-grams from fake ones in coarse-grained and fine-grained manners, as shown in Figure 3(a), which is efficient to model the pair relationships between similar n-grams. The generator model θ0 will not be used during fine-tuning, where the hidden size Hθ0 of θ0 has Hθ0 = Hθ /3 empirically. As shown in Figure 3(b), n-grams of different length can be sampled to mask original n-grams according to the prediction distributions of θ0 , which is more flexible and sufficient for constructing ngram pairs than previous synonym masking methods (Cui et al., 2020) that require synonyms and original words to be of the same length. Note that our method needs a large embedding layer E ∈ R|hVF ,VN i|×h to obtain n-gram vectors in pretraining. To keep the number of parameters consisMoreover, we incorporate the replaced token detection objective (RTD) to further distinguish 0 fake n-grams from the mix-grained context z¯M for interactions among explicit n-grams and finegrained contextual tokens, as shown in the right part of Figure 3(a). Formally, we donate zˆM to be the sequence after replacing masked n-grams with target n-gram identities yM , the RTD obje"
2021.naacl-main.136,L18-1431,0,0.0273221,".2 86.6 / 92.5 -/-/89.4 / 94.3 84.6 / 90.9 88.5 / 93.8 83.6 / 90.4 85.6 / 92.0 -/-/89.5 / 93.8 84.0 / 90.5 88.0 / 93.4 67.1 / 85.7 -/67.4 / 87.2 -/-/-/-/-/68.5 / 87.9 -/65.1 / 85.1 57.9 / 72.1 95.0 93.8 69.1 / 88.6 61.3 / 74.9 95.2 93.8 Table 4: Results on six Chinese NLU tasks for base-size pre-trained models. Results of models with asterisks “∗ ” are from Cui et al., 2019. M-NER is in short for MSRA-NER dataset. “BASE” and “LARGE” donate different sizes of pre-training models. Large size models have L = 24, H = 1024, A = 16 and total Parameters=340M. machine reading comprehension (CMRC2018; Cui et al., 2018, DRCD; Shao et al., 2018 and DuReader; He et al., 2018), named entity recognition (MSRA-NER; Gao et al., 2005) and semantic similarity (LCQMC; Liu et al., 2018). Results on six Chinese tasks are presented in Table 4. It is observed that ERNIE-Gram significantly outperforms previous models across tasks by a large margin and achieves new state-of-theart results on these Chinese NLU tasks in basesize model group. Besides, ERNIE-GramBASE are also better than various large-size models on XNLI, LCQMC and CMRC2018 datasets. 4.7 Ablation Studies We further conduct ablation experiments to analyze the"
2021.naacl-main.136,J05-4005,0,0.108501,"0 / 93.4 67.1 / 85.7 -/67.4 / 87.2 -/-/-/-/-/68.5 / 87.9 -/65.1 / 85.1 57.9 / 72.1 95.0 93.8 69.1 / 88.6 61.3 / 74.9 95.2 93.8 Table 4: Results on six Chinese NLU tasks for base-size pre-trained models. Results of models with asterisks “∗ ” are from Cui et al., 2019. M-NER is in short for MSRA-NER dataset. “BASE” and “LARGE” donate different sizes of pre-training models. Large size models have L = 24, H = 1024, A = 16 and total Parameters=340M. machine reading comprehension (CMRC2018; Cui et al., 2018, DRCD; Shao et al., 2018 and DuReader; He et al., 2018), named entity recognition (MSRA-NER; Gao et al., 2005) and semantic similarity (LCQMC; Liu et al., 2018). Results on six Chinese tasks are presented in Table 4. It is observed that ERNIE-Gram significantly outperforms previous models across tasks by a large margin and achieves new state-of-theart results on these Chinese NLU tasks in basesize model group. Besides, ERNIE-GramBASE are also better than various large-size models on XNLI, LCQMC and CMRC2018 datasets. 4.7 Ablation Studies We further conduct ablation experiments to analyze the major components of ERNIE-Gram. Effect of Explicitly N-gram MLM. We compare two models pre-trained with contigu"
2021.naacl-main.136,W18-2605,1,0.825381,"3.6 / 90.4 85.6 / 92.0 -/-/89.5 / 93.8 84.0 / 90.5 88.0 / 93.4 67.1 / 85.7 -/67.4 / 87.2 -/-/-/-/-/68.5 / 87.9 -/65.1 / 85.1 57.9 / 72.1 95.0 93.8 69.1 / 88.6 61.3 / 74.9 95.2 93.8 Table 4: Results on six Chinese NLU tasks for base-size pre-trained models. Results of models with asterisks “∗ ” are from Cui et al., 2019. M-NER is in short for MSRA-NER dataset. “BASE” and “LARGE” donate different sizes of pre-training models. Large size models have L = 24, H = 1024, A = 16 and total Parameters=340M. machine reading comprehension (CMRC2018; Cui et al., 2018, DRCD; Shao et al., 2018 and DuReader; He et al., 2018), named entity recognition (MSRA-NER; Gao et al., 2005) and semantic similarity (LCQMC; Liu et al., 2018). Results on six Chinese tasks are presented in Table 4. It is observed that ERNIE-Gram significantly outperforms previous models across tasks by a large margin and achieves new state-of-theart results on these Chinese NLU tasks in basesize model group. Besides, ERNIE-GramBASE are also better than various large-size models on XNLI, LCQMC and CMRC2018 datasets. 4.7 Ablation Studies We further conduct ablation experiments to analyze the major components of ERNIE-Gram. Effect of Explicitly N-g"
2021.naacl-main.136,2020.tacl-1.5,0,0.10719,"al context. However, BERT’s MLM focuses on the representations of fine-grained text units (e.g. words or subwords in English and characters in Chinese), rarely considering the coarse-grained linguistic information (e.g. named entities or phrases in English and words in Chinese) thus incurring inadequate representation learning. Many efforts have been devoted to integrate coarse-grained semantic information by independently masking and predicting contiguous sequences of n tokens, namely n-grams, such as named entities, phrases (Sun et al., 2019b), whole words (Cui et al., 2019) and text spans (Joshi et al., 2020). We argue that such contiguously masking strategies are less effective and reliable since the prediction of tokens in masked n-grams are independent of each other, which neglects the intradependencies of n-grams. Specifically, given a masked n-gramQw = {x1 , ..., xn }, x ∈ VF , we maximize p(w) = ni=1 p(xi |c) for n-gram learning, where models learn to recover w in a huge and n sparse prediction space F ∈ R|VF |. Note that VF is the fine-grained vocabulary1 and c is the context. We propose ERNIE-Gram, an explicitly ngram masked language modeling method in which n-grams are masked with single"
2021.naacl-main.136,D17-1082,0,0.049529,"Missing"
2021.naacl-main.136,P19-1314,0,0.0138094,"phrases to enhance contextual representations, BERT-wwm (Cui et al., 2019) masks whole Chinese words to achieve better Chinese representations, SpanBERT (Joshi et al., 2020) masks contiguous spans to improve the performance on span selection tasks. A few studies attempt to inject the coarsegrained n-gram representations into fine-grained contextualized representations explicitly, such as Z EN (Diao et al., 2020) and AMBERT (Zhang and Li, 2020), in which additional transformer encoders and computations for explicit n-gram representations are incorporated into both pre-training and fine-tuning. Li et al., 2019 demonstrate that explicit n-gram representations are not sufficiently reliable for NLP tasks because of n-gram data sparsity and the ubiquity of out-of-vocabulary n-grams. Differently, we only incorporate n-gram information by leveraging auxiliary n-gram classifier and embedding weights in pre-training, which will be completely removed during fine-tuning, so our method maintains the same parameters and computations as BERT. 3 Proposed Method In this section, we present the detailed implementation of ERNIE-Gram, including n-gram lexicon 1703 (a) VN extraction in Section 3.5, explicitly n-gram"
2021.naacl-main.136,C18-1166,0,0.0214755,"Missing"
2021.naacl-main.136,2021.ccl-1.108,0,0.094593,"Missing"
2021.naacl-main.136,P11-1015,0,0.0681582,"questions and 72.0 77.7 70.3 75.6 76.3 78.8 4.4 3.9 4.9 Table 3: Comparison on the test sets of RACE, IMDb and AG. The listed models are all in base-size. In the results of RACE, “High” and “Middle” represent the training and evaluation sets for high schools and middle schools respectively, “Total” is the full training and evaluation set. a (Devlin et al., 2019); b (Yang et al., 2019); c (Song et al., 2020); d (Dai et al., 2020). passages. We also evaluate ERNIE-Gram on two large scaled text classification tasks that involve long text and reasoning, including sentiment analysis datasets IMDb (Maas et al., 2011) and topic classification dataset AG’s News (Zhang et al., 2015). The results are reported in Table 3. It can be seen that ERNIE-Gram consistently outperforms previous models, showing the advantage of ERNIEGram on tasks involving long text and reasoning. 4.6 Results on Chinese NLU Tasks We execute extensive experiments on six Chinese language understanding tasks, including natural language inference (XNLI; Conneau et al., 2018), 1707 Models XNLI LCQMC Acc Acc Dev Test Dev Test DRCD EM / F1 Dev Test CMRC2018 DuReader M-NER EM / F1 EM / F1 F1 Dev Dev Dev Test RoBERTa-wwn-ext∗LARGE 82.1 81.2 90.4"
2021.naacl-main.466,W02-1033,0,0.356347,"Missing"
2021.naacl-main.466,P17-1171,0,0.154647,"ategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever 1 . 1 Introduction Open-domain question answering (QA) aims to find the answers to natural language questions from a large collection of documents. Early QA systems (Brill et al., 2002; Dang et al., 2007; Ferrucci et al., 2010) constructed complicated pipelines consisting of multiple components, including question understanding, document retrieval, passage ranking and answer extraction. Recently, inspired by the advancements of machine reading comprehension (MRC), Chen et al. (2017) proposed a simplified two-stage approach, where a traditional IR sim(q, p) = Eq(q) Ep(p) Eq(q) Ep(p) ... ... ... [CLS] q(1) ... q(k) ... [CLS] question p(1) ... p(l) passage (a) A dual-encoder based on pre-trained LMs. sim(q, p) [CLS] q(1) ... ... ... ... ... question q(k) [SEP] p(1) ... p(l) passage (b) A cross-encoder based on pre-trained LMs. Figure 1: The comparison of dual-encoder and crossencoder architectures. retriever (e.g., TF-IDF or BM25) first selects a few relevant passages as contexts, and then a neural reader reads the contexts and extracts the answers. As the recall component,"
2021.naacl-main.466,D19-5801,0,0.0177207,"ely. The dualencoders are trained on NQ for 30 epochs in all steps of RocketQA. The cross-encoders are trained for 2 epochs on both MSMARCO and NQ. Optimizers We use ADAM optimizer. Warmup and learning rate The learning rate of the dual-encoder is set to 3e-5 and the rate of linear scheduling warm-up is set to 0.1, while the learning rate of the cross-encoder is set to 1e-5. Maximal length We set the maximal length of questions and passages as 32 and 128, respectively. Unlabeled questions We collect 1.7 million unlabeled questions from Yahoo! Answers5 , ORCAS (Craswell et al., 2020) and MRQA (Fisch et al., 2019). We use the questions from Yahoo! Answers, ORCAS and NQ as new questions in the experiments of MSMARCO. We only use the questions from MRQA as the new questions in the experiments of NQ. Since both NQ and MRQA mainly contain factoid-questions, while other datasets contain both factoid and non-factoid questions. 4.2 Experimental Results In our experiments, we first examine the effectiveness of our retriever on MSMARCO and NQ datasets. Then, we conduct extensive experiments to examine the effects of the three proposed training strategies. We also show the performance of endto-end QA based on ou"
2021.naacl-main.466,2020.emnlp-main.342,0,0.255114,"re-ranking for open-domain QA Based on the retrieved passages from a first-stage retriever, BERT-based rerankers have recently been applied to retrieval-based question answering and search-related tasks (Wang et al., 2019; Nogueira and Cho, 2019; Nogueira et al., 2019b; Yan et al., 2019), and yield substantial improvements over the traditional methods. Although effective to some extent, these rankers employ the cross-encoder architecture (as shown in Figure 1b) that is impractical to be applied to all passages in a corpus with respect to a question. The re-rankers (Khattab and Zaharia, 2020; Gao et al., 2020) with light weight interaction based on the representations of dense retrievers have been studied. However, these techniques still rely on a separate retriever which provides candidates and representations. As a comparison, we focus on developing dual-encoder based retrievers. 3 Approach 3.1 Task Description The task of open-domain QA is described as follows. Given a natural language question, a system is required to answer it based on a large collection of documents. Let C denote the corpus, consisting of N documents. We split the N documents into M passages, denoted by p1 , p2 , ..., pM , wh"
2021.naacl-main.466,K19-1049,0,0.458038,"exists the discrepancy between training and inference for the dual-encoder retriever. During inference, the retriever needs to identify positive (or relevant) passages for each question from a large collection containing millions of candidates. However, during training, the model is learned to estimate the probabilities of positive passages in a small candidate set for each question, due to the limited memory of a single GPU (or other device). To reduce such a discrepancy, previous work tried to design specific mechanisms for selecting a few hard negatives from the top-k retrieved candidates (Gillick et al., 2019; Wu et al., 2020; Karpukhin et al., 2020; Luan et al., 2020; Xiong et al., 2020). However, it suffers from the false negative issue due to the following challenge. Second, there might be a large number of unlabeled positives. Usually, it is infeasible to completely annotate all the candidate passages for one question. By only examining the the top-K passages retrieved by a specific retrieval approach (e.g. BM25), the annotators are likely to miss relevant passages to a question. Taking the MSMARCO dataset (Nguyen et al., 2016) as an example, each question has only 1.1 annotated positive passa"
2021.naacl-main.466,2020.emnlp-main.550,0,0.0498813,"retriever (e.g., TF-IDF or BM25) first selects a few relevant passages as contexts, and then a neural reader reads the contexts and extracts the answers. As the recall component, the first-stage retriever significantly affects the final QA performance. Though efficient with an inverted index, traditional IR retrievers with term-based sparse representations have limited capabilities in matching questions and passages, e.g., term mismatch. To deal with the issue of term mismatch, the dual-encoder architecture (as shown in Figure 1a) has been widely explored (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020; Luan et al., 2020; Xiong et al., 2020) to learn dense representations of questions and passages in an end-to-end manner, which provides better representations for semantic matching. These studies first separately encode questions and passages to obtain their dense ∗ representations, and then compute the similarity Corresponding authors. † The work was done when Ruiyang Ren was doing inbetween the dense representations using similarity ternship at Baidu. functions such as cosine or dot product. Typically, 1 Our code is available at https://github.com/ the dual-encoder is trained by using in-b"
2021.naacl-main.466,Q19-1026,0,0.0329179,".58 100.0 Table 1: The statistics of datasets MSMARCO and Natural Questions. Here, “p” and “q” are the abbreviations of questions and passages, respectively. The length is in tokens. encoder. The cross-encoder is used both STEP 3 and STEP 4 with different purposes to promote the performance of the dual encoder. The implementation details of denoising hard negatives and data augmentation can be found in Section 4. 4 Experiments 4.1 4.1.1 Experimental Setup Datasets We conduct the experiments on two popular QA benchmarks: MSMARCO Passage Ranking (Nguyen et al., 2016) and Natural Questions (NQ) (Kwiatkowski et al., 2019). The statistics of the datasets are listed in Table 1. MSMARCO Passage Ranking MSMARCO is originally designed for multiple passage MRC, and its questions were sampled from Bing search logs. Based on the questions and passages in MSMARCO Question Answering, a dataset for passage ranking was created, namely MSMARCO Passage Ranking, consisting of about 8.8 million passages. The goal is to find positive passages that answer the questions. Natural Question (NQ) Kwiatkowski et al. (2019) introduces a large dataset for open-domain QA. The original dataset contains more than 300, 000 questions collec"
2021.naacl-main.466,P19-1612,0,0.475888,"oder and crossencoder architectures. retriever (e.g., TF-IDF or BM25) first selects a few relevant passages as contexts, and then a neural reader reads the contexts and extracts the answers. As the recall component, the first-stage retriever significantly affects the final QA performance. Though efficient with an inverted index, traditional IR retrievers with term-based sparse representations have limited capabilities in matching questions and passages, e.g., term mismatch. To deal with the issue of term mismatch, the dual-encoder architecture (as shown in Figure 1a) has been widely explored (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020; Luan et al., 2020; Xiong et al., 2020) to learn dense representations of questions and passages in an end-to-end manner, which provides better representations for semantic matching. These studies first separately encode questions and passages to obtain their dense ∗ representations, and then compute the similarity Corresponding authors. † The work was done when Ruiyang Ren was doing inbetween the dense representations using similarity ternship at Baidu. functions such as cosine or dot product. Typically, 1 Our code is available at https://github.com/"
2021.naacl-main.466,D19-1284,0,0.093921,"Missing"
2021.naacl-main.466,D19-1599,0,0.0992005,"Missing"
2021.naacl-main.466,2020.emnlp-main.519,0,0.731378,"between training and inference for the dual-encoder retriever. During inference, the retriever needs to identify positive (or relevant) passages for each question from a large collection containing millions of candidates. However, during training, the model is learned to estimate the probabilities of positive passages in a small candidate set for each question, due to the limited memory of a single GPU (or other device). To reduce such a discrepancy, previous work tried to design specific mechanisms for selecting a few hard negatives from the top-k retrieved candidates (Gillick et al., 2019; Wu et al., 2020; Karpukhin et al., 2020; Luan et al., 2020; Xiong et al., 2020). However, it suffers from the false negative issue due to the following challenge. Second, there might be a large number of unlabeled positives. Usually, it is infeasible to completely annotate all the candidate passages for one question. By only examining the the top-K passages retrieved by a specific retrieval approach (e.g. BM25), the annotators are likely to miss relevant passages to a question. Taking the MSMARCO dataset (Nguyen et al., 2016) as an example, each question has only 1.1 annotated positive passages on average, w"
2021.nlp4convai-1.14,2021.tacl-1.6,0,0.033141,"ese approaches has difficulties to hit the knowledge contained in the target response, and deteriorates the learning of knowledge utilization. Our top-k selection improves the robustness of prior knowledge selection. Some other works (Lian et al., 2019; Zhao et al., 2020; Ren et al., 2020) employ the target response to identify the grounded knowledge. Since the posterior knowledge selection is involved, it will inevitably cause discrepancy between the training and inference stages (Zhao et al., 2019). With end-to-end modeling and optimization, PLATO-KAG gets exempt from this discrepancy. KIF (Fan et al., 2021) explicitly selects external knowledge through a retrieval module, and fuses into one integrated representation to assist dialogue generation. While some knowledge details might be obscured with this fusion. As comparison, the knowledge keeps its independence and integrity in our response generation, which helps reduce the hallucination. More recently, Shuster et al. (2021) attempts to utilize the pre-trained retriever DPR (Karpukhin et al., 2020). DPR has been trained on Wikipedia which includes the knowledge sets of WoW and Holl-E. Due to the concern of potential data contamination, we choos"
2021.nlp4convai-1.14,2020.emnlp-main.550,0,0.0167985,"etween the training and inference stages (Zhao et al., 2019). With end-to-end modeling and optimization, PLATO-KAG gets exempt from this discrepancy. KIF (Fan et al., 2021) explicitly selects external knowledge through a retrieval module, and fuses into one integrated representation to assist dialogue generation. While some knowledge details might be obscured with this fusion. As comparison, the knowledge keeps its independence and integrity in our response generation, which helps reduce the hallucination. More recently, Shuster et al. (2021) attempts to utilize the pre-trained retriever DPR (Karpukhin et al., 2020). DPR has been trained on Wikipedia which includes the knowledge sets of WoW and Holl-E. Due to the concern of potential data contamination, we choosed to initialize our knowledge selection module with a general dialogue model which is pre-trained on Reddit. Thus, we facilitated an unbiased setting for our experiments and the analysis of framework generalization. Knowledge-grounded conversation is becoming a more important and popular topic, with several datasets (Zhang et al., 2018; Moghe et al., 2018; Zhou et al., 2018; Dinan et al., 2019; Gopalakrishnan et al., 2019; Komeili et al., 2021) c"
2021.nlp4convai-1.14,N16-1014,0,0.0399779,"the pre-training models used for initialization6 . The average Fleiss’s kappa (Fleiss, 1971) in human evaluation is 0.502, indicating that annotators have reached moderate agreement. The evaluation results on the Holl-E test set are summarized in Table 2. In the evaluation on the multiple reference test set, we took the best score over multiple reference responses for each dialogue context. The results demonstrate that PLATO-KAG also achieves competitive results in Holl-E. PostKS obtains a slightly higher value on Unigram F1 than PLATO-KAG and supervised TMN. While the values on Distinct-1/2 (Li et al., 2016) indicate the PLATO-KAG and supervised TMN might have better capacity on lexical diversity. 3.3 3.3.1 Discussions Case Analysis For further qualitative analysis, two examples of generated responses from the WoW test set are provided in Table 3. It can be observed that unsupervised TMN suffers from low-quality response generation, such as generic replies with little information or statements with factual errors. In comparison, PostKS and KnowledGPT are able to generate much more informative responses, depicting contents from the selected knowledge. However, the responses fail to be coherent wit"
2021.nlp4convai-1.14,P19-1002,0,0.0159839,"ed learning difficulty of knowledge-grounded conver- for end-to-end knowledge grounded conversation sation. However, given that manual annotation is modeling. There are two main components in our expensive and time-consuming, it is not feasible to method: knowledge selection and response gencarry out the knowledge labelling on a large scale. eration. Given a dialogue context, top-k relevant Unsupervised approaches have been introduced knowledge elements are selected and utilized for to model knowledge-grounded conversation. Some response generation. The generation probability of these such as Li et al. (2019); Yavuz et al. can in turn provide training signal for the prece(2019); Lin et al. (2020) perform implicit soft fu- dent knowledge selection. Joint balanced training sion over provided knowledge elements and do not is further introduced for the effective optimization select knowledge explicitly. Some attempts have of these two components. Comprehensive experibeen made to learn the unsupervised selection of ments have been carried out on WoW and Holl-E, external knowledge based on semantic similarity verifying the effectiveness and superiority of the (Ghazvininejad et al., 2018; Dinan et al., 2"
2021.nlp4convai-1.14,2020.acl-main.6,0,0.0312241,"onversation sation. However, given that manual annotation is modeling. There are two main components in our expensive and time-consuming, it is not feasible to method: knowledge selection and response gencarry out the knowledge labelling on a large scale. eration. Given a dialogue context, top-k relevant Unsupervised approaches have been introduced knowledge elements are selected and utilized for to model knowledge-grounded conversation. Some response generation. The generation probability of these such as Li et al. (2019); Yavuz et al. can in turn provide training signal for the prece(2019); Lin et al. (2020) perform implicit soft fu- dent knowledge selection. Joint balanced training sion over provided knowledge elements and do not is further introduced for the effective optimization select knowledge explicitly. Some attempts have of these two components. Comprehensive experibeen made to learn the unsupervised selection of ments have been carried out on WoW and Holl-E, external knowledge based on semantic similarity verifying the effectiveness and superiority of the (Ghazvininejad et al., 2018; Dinan et al., 2019). proposed method. 150 Acknowledgements We would like to thank the anonymous reviewer"
2021.nlp4convai-1.14,D18-1255,0,0.162148,"aptive normalization on the second term, our method successfully maintains the balance between knowledge selection and knowledge-grounded response generation. More analyses on the component weight are included in the experiments. 3 3.1 Experiments Settings 3.1.1 Datasets In the sequence form of Equation (5a), it relies We conducted experiments on two knowledgeon one knowledge element to predict the whole grounded conversation datasets: Wizard of sequence of the target response. In the token form Wikipedia (WoW) (Dinan et al., 2019) and Holl-E of Equation (5b), the generative process can rely (Moghe et al., 2018). on different knowledge elements independently for In Wizard of Wikipedia, two participants conduct each token. in-depth discussion on a chosen beginning topic. With the sequence form, the selection of knowl- One of the participants has access to relevant knowledge just weight like the generation of one response edge and plays the role of an expert (wizard). The token. Given the long responses in knowledge- other one acts as a curious learner (apprentice). grounded conversation2 , the module of knowledge There are 18,430/1,948/1,933 dialogues in the trainselection is at a distinct disadvantag"
2021.nlp4convai-1.14,2021.eacl-main.24,0,0.0346446,"and tal results on two publicly available datasets KnowledGPT (Zhao et al., 2020) rely on the tarvalidate the superiority of PLATO-KAG. get response to identify the grounded knowledge. 1 Introduction However, involving the posterior knowledge selection will inevitably cause discrepancy between the Recently, the capability of large-scale pre-trained training and inference stages (Zhao et al., 2019). models has been verified in open-domain dialogue In this paper, we propose an unsupervised generation, including Meena (Adiwardana et al., approach for end-to-end knowledge-grounded 2020), Blender (Roller et al., 2021), and PLATO-2 conversation modeling, namely PLATO-KAG (Bao et al., 2020). Without introducing explicit (Knowledge-Augmented Generation). As shown knowledge in learning process, substantive knowlin Figure 1, given each dialogue context, the edge is implicitly embedded into parameters from top-k relevant knowledge elements are selected the training corpus. However, these models are for the subsequent response generation. Then, found to suffer from knowledge hallucinations (Roller et al., 2021; Marcus, 2020), producing plau- the model learns to generate the target response grounded on each of the"
2021.nlp4convai-1.14,2021.findings-emnlp.320,0,0.023837,"r knowledge selection is involved, it will inevitably cause discrepancy between the training and inference stages (Zhao et al., 2019). With end-to-end modeling and optimization, PLATO-KAG gets exempt from this discrepancy. KIF (Fan et al., 2021) explicitly selects external knowledge through a retrieval module, and fuses into one integrated representation to assist dialogue generation. While some knowledge details might be obscured with this fusion. As comparison, the knowledge keeps its independence and integrity in our response generation, which helps reduce the hallucination. More recently, Shuster et al. (2021) attempts to utilize the pre-trained retriever DPR (Karpukhin et al., 2020). DPR has been trained on Wikipedia which includes the knowledge sets of WoW and Holl-E. Due to the concern of potential data contamination, we choosed to initialize our knowledge selection module with a general dialogue model which is pre-trained on Reddit. Thus, we facilitated an unbiased setting for our experiments and the analysis of framework generalization. Knowledge-grounded conversation is becoming a more important and popular topic, with several datasets (Zhang et al., 2018; Moghe et al., 2018; Zhou et al., 201"
2021.nlp4convai-1.14,W19-5917,0,0.0308804,"Missing"
2021.nlp4convai-1.14,P18-1205,0,0.0251644,"the hallucination. More recently, Shuster et al. (2021) attempts to utilize the pre-trained retriever DPR (Karpukhin et al., 2020). DPR has been trained on Wikipedia which includes the knowledge sets of WoW and Holl-E. Due to the concern of potential data contamination, we choosed to initialize our knowledge selection module with a general dialogue model which is pre-trained on Reddit. Thus, we facilitated an unbiased setting for our experiments and the analysis of framework generalization. Knowledge-grounded conversation is becoming a more important and popular topic, with several datasets (Zhang et al., 2018; Moghe et al., 2018; Zhou et al., 2018; Dinan et al., 2019; Gopalakrishnan et al., 2019; Komeili et al., 2021) collected to 5 Conclusion study it. Besides interactive dialogues, some of these datasets have annotated the corresponding knowledge for each response, aiming to ease the In this paper, an unsupervised approach is proposed learning difficulty of knowledge-grounded conver- for end-to-end knowledge grounded conversation sation. However, given that manual annotation is modeling. There are two main components in our expensive and time-consuming, it is not feasible to method: knowledge se"
2021.nlp4convai-1.14,N19-1123,0,0.150515,"ontained in the target response, deterigeneration are optimized jointly and effecorating the learning of knowledge utilization. As tively under a balanced objective. Experimenan improvement, PostKS (Lian et al., 2019) and tal results on two publicly available datasets KnowledGPT (Zhao et al., 2020) rely on the tarvalidate the superiority of PLATO-KAG. get response to identify the grounded knowledge. 1 Introduction However, involving the posterior knowledge selection will inevitably cause discrepancy between the Recently, the capability of large-scale pre-trained training and inference stages (Zhao et al., 2019). models has been verified in open-domain dialogue In this paper, we propose an unsupervised generation, including Meena (Adiwardana et al., approach for end-to-end knowledge-grounded 2020), Blender (Roller et al., 2021), and PLATO-2 conversation modeling, namely PLATO-KAG (Bao et al., 2020). Without introducing explicit (Knowledge-Augmented Generation). As shown knowledge in learning process, substantive knowlin Figure 1, given each dialogue context, the edge is implicitly embedded into parameters from top-k relevant knowledge elements are selected the training corpus. However, these models a"
2021.nlp4convai-1.14,2020.emnlp-main.272,0,0.213758,"relevant knowledge elements context. The prior top-1 knowledge selection emare selected and then employed in knowledgeployed by these approaches (Ghazvininejad et al., grounded response generation. The two com2018; Dinan et al., 2019) has difficulties to hit the ponents of knowledge selection and response knowledge contained in the target response, deterigeneration are optimized jointly and effecorating the learning of knowledge utilization. As tively under a balanced objective. Experimenan improvement, PostKS (Lian et al., 2019) and tal results on two publicly available datasets KnowledGPT (Zhao et al., 2020) rely on the tarvalidate the superiority of PLATO-KAG. get response to identify the grounded knowledge. 1 Introduction However, involving the posterior knowledge selection will inevitably cause discrepancy between the Recently, the capability of large-scale pre-trained training and inference stages (Zhao et al., 2019). models has been verified in open-domain dialogue In this paper, we propose an unsupervised generation, including Meena (Adiwardana et al., approach for end-to-end knowledge-grounded 2020), Blender (Roller et al., 2021), and PLATO-2 conversation modeling, namely PLATO-KAG (Bao et"
2021.nlp4convai-1.14,D18-1076,0,0.0195368,"er et al. (2021) attempts to utilize the pre-trained retriever DPR (Karpukhin et al., 2020). DPR has been trained on Wikipedia which includes the knowledge sets of WoW and Holl-E. Due to the concern of potential data contamination, we choosed to initialize our knowledge selection module with a general dialogue model which is pre-trained on Reddit. Thus, we facilitated an unbiased setting for our experiments and the analysis of framework generalization. Knowledge-grounded conversation is becoming a more important and popular topic, with several datasets (Zhang et al., 2018; Moghe et al., 2018; Zhou et al., 2018; Dinan et al., 2019; Gopalakrishnan et al., 2019; Komeili et al., 2021) collected to 5 Conclusion study it. Besides interactive dialogues, some of these datasets have annotated the corresponding knowledge for each response, aiming to ease the In this paper, an unsupervised approach is proposed learning difficulty of knowledge-grounded conver- for end-to-end knowledge grounded conversation sation. However, given that manual annotation is modeling. There are two main components in our expensive and time-consuming, it is not feasible to method: knowledge selection and response gencarry out the k"
2021.nlp4convai-1.8,2020.acl-main.53,0,0.527371,"n the double-checking process and alleviates unnecessary error propagation. Experimental results show that AGDST significantly outperforms previous works in two active DST datasets (MultiWOZ 2.2 and WOZ 2.0), achieving new state-of-the-art performances. 1 On the merits of utilizing the previous dialogue state as a compact representation of the previous dialogue history, some recent methods choose to take the previous dialogue state into consideration when generating the slot values. One direction is to decompose DST into two explicit sub-tasks: State Operation Prediction and Value Generation (Kim et al., 2020; Zeng and Nie, 2020). At each turn, whether or how to modify the value in the previous dialogue state is determined by the discrete operations from the state operation prediction, so the accuracy of state operation prediction holds back the overall DST performance (Kim et al., 2020). Another direction of recent works recasts dialogue state tracking into a single causal language model by using the dialogue of the current turn and the previous dialogue state as input sequence (Lin et al., 2020; Yang et al., 2021), where the current dialogue state is generated by jointly modeling the state opera"
2021.nlp4convai-1.8,2020.emnlp-main.273,0,0.546412,"tion is to decompose DST into two explicit sub-tasks: State Operation Prediction and Value Generation (Kim et al., 2020; Zeng and Nie, 2020). At each turn, whether or how to modify the value in the previous dialogue state is determined by the discrete operations from the state operation prediction, so the accuracy of state operation prediction holds back the overall DST performance (Kim et al., 2020). Another direction of recent works recasts dialogue state tracking into a single causal language model by using the dialogue of the current turn and the previous dialogue state as input sequence (Lin et al., 2020; Yang et al., 2021), where the current dialogue state is generated by jointly modeling the state operation prediction and value generation in a implicit fashion. While it is more effective and reasonable to use the previous dialogue state under the Markov assumption, the mistakes of these models made during the prediction of the current turn are prone to be carried over to the next turn, causing error propagation. These carried-over mistakes are unlikely to be fixed in the next turn. Essentially, these models perform a one-pass generation process and lack a double-checking process to amend th"
2021.nlp4convai-1.8,P17-1163,0,0.0342867,"Missing"
2021.nlp4convai-1.8,P18-1135,0,0.29244,"ing process to amend the mistakes of Introduction Dialogue state tracking (DST) is a crucial task in task-oriented dialogue systems, as it affects database query results as well as the subsequent policy prediction (Chen et al., 2017). It extracts users’ goals at each turn of the conversation and represents them in the form of a set of (slot, value) pairs, i.e., dialogue state. Traditional methods of DST mainly rely on a predefined ontology which includes all possible slots and corresponding values. These models predict the value for each slot as a classification problem (Mrkši´c et al., 2017; Zhong et al., 2018; Ramadan et al., 2018). However, in practical applications, some slot values appearing in the conversations cannot be predefined, and it is infeasible 80 Proceedings of the Third Workshop on Natural Language Processing for Conversational AI, pages 80–92 November 10, 2021. ©2021 Association for Computational Linguistics Ok, and what day and time would you like that reservation? I would like to make a reservation for saturday at 11:45. And there has been a change in plans, I will be dining alone. Last Dialogue State Primitive Dialogue State Amended Dialogue State Slot Value Slot Value Slot Valu"
2021.nlp4convai-1.8,D18-1299,0,0.0243137,"Missing"
2021.nlp4convai-1.8,2020.findings-emnlp.68,0,0.0147908,"0; Yang et al., 2021). Lin et al. (2020) utilizes an encoder-decoder framework to generate dialogue state and system response sequentially, where minimal slot value pairs are generated for efficiently tracking. Yang et al. (2021) models task-oriented dialogs on a dialog session level, which generates dialogue state, dialogue action and system response sequentially based on the whole previous dialogue context, including the generated dialogue states and dialogue actions. Moreover, some schemaguided DST methods leverage schema descriptions to deal with unseen schemas with new domains and slots (Zhu et al., 2020; Feng et al., 2021; Noroozi et al., 2020). Table 7: The ablation study of the absence of different special tokens on MultiWOZ 2.2 with joint goal accuracy. with the PLATO-2 initialization. This indicates our approach’s ability to universally improve the performances of other pretrained models. 4.4 Effect of Special Tokens Special tokens are important for identifying different input components (Hosseini-Asl et al., 2020). In our experiments, similar to prior works, we use the utterance special token and dialogue state special token to differentiate each part. Besides, to boost the extraction o"
2021.nlp4convai-1.8,E17-1042,0,0.0690482,"Missing"
2021.nlp4convai-1.8,P19-1078,0,0.0183715,"d by <nm>, <dc> or some wrong values. During training, the dialogue state after negative sampling is used as the primitive ˜t in the amending generation to dialogue state B encourage the model to lay more emphasis on error correction. 2.3 Statistics # domains # slots # dialogues # train dialogues # valid dialogues # test dialogues Avg. turns per dialogue Avg. tokens per turn Training Objective The training objective of the basic generation is the negative log-likelihood loss given the dialogue of the current turn and the previous dialogue state: Lbasic = − log P (Bt |Dt , Bt−1 ) (5) Following Wu et al. (2019), due to the absence of hospital and police domains in the validation and test datasets, there are only 5 domains (attraction, hotel, restaurant, taxi and train) and 30 corresponding domain-slot pairs in our experiments. WOZ 2.0 (Wen et al., 2017) is a well-known singledomain DST dataset, where 3 slots (area, food and price range) are involved in the restaurant domain. Table 1 summarizes the statistics of the above two datasets. (6) The total objective of AG-DST is to minimize the sum of the above two losses: L = Lbasic + Lamending 3 3.1 WOZ 2.0 1 3 1200 600 200 400 8.35 13.18 Table 1: Statist"
2021.nlp4convai-1.8,2020.nlp4convai-1.13,0,0.0380338,"Missing"
C00-2174,W96-0411,0,\N,Missing
C04-1005,P98-1004,0,0.354377,"termediate result in statistical machine translation (SMT) (Brown et al. 1993). Besides being used in SMT, it is also used in translation lexicon building (Melamed 1996), transfer rule learning (Menezes and Richardson 2001), example-based machine translation (Somers 1999), etc. In previous alignment methods, some researches modeled the alignments as hidden parameters in a statistical translation model (Brown et al. 1993; Och and Ney 2000) or directly modeled them given the sentence pairs (Cherry and Lin 2003). Some researchers used similarity and association measures to build alignment links (Ahrenberg et al. 1998; Tufis and Barbu 2002). In addition, Wu (1997) used a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Generally speaking, there are four cases in word alignment: word to word alignment, word to multi-word alignment, multi-word to word alignment, and multi-word to multi-word alignment. One of the most difficult tasks in word alignment is to find out the alignments that include multi-word units. For example, the statistical word alignment in IBM translation models (Brown et al. 1993) can only handle word to word and mult"
C04-1005,ahrenberg-etal-2000-evaluation,0,0.0355324,"Missing"
C04-1005,P03-1012,0,0.18936,"ecision and recall of word alignment. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al. 1993). Besides being used in SMT, it is also used in translation lexicon building (Melamed 1996), transfer rule learning (Menezes and Richardson 2001), example-based machine translation (Somers 1999), etc. In previous alignment methods, some researches modeled the alignments as hidden parameters in a statistical translation model (Brown et al. 1993; Och and Ney 2000) or directly modeled them given the sentence pairs (Cherry and Lin 2003). Some researchers used similarity and association measures to build alignment links (Ahrenberg et al. 1998; Tufis and Barbu 2002). In addition, Wu (1997) used a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Generally speaking, there are four cases in word alignment: word to word alignment, word to multi-word alignment, multi-word to word alignment, and multi-word to multi-word alignment. One of the most difficult tasks in word alignment is to find out the alignments that include multi-word units. For example, the sta"
C04-1005,1996.amta-1.13,0,0.0369898,"get and target to source), and then uses the translation information in the rule-based machine translation system to improve the statistical word alignment. The improved alignments allow the word(s) in the source language to be aligned to one or more words in the target language. Experimental results show a significant improvement in precision and recall of word alignment. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al. 1993). Besides being used in SMT, it is also used in translation lexicon building (Melamed 1996), transfer rule learning (Menezes and Richardson 2001), example-based machine translation (Somers 1999), etc. In previous alignment methods, some researches modeled the alignments as hidden parameters in a statistical translation model (Brown et al. 1993; Och and Ney 2000) or directly modeled them given the sentence pairs (Cherry and Lin 2003). Some researchers used similarity and association measures to build alignment links (Ahrenberg et al. 1998; Tufis and Barbu 2002). In addition, Wu (1997) used a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get t"
C04-1005,J00-2004,0,0.103013,"can only handle word to word and multi-word to word alignments. Some studies have been made to tackle this problem. Och and Ney (2000) performed translation in both directions (source to target and target to source) to extend word alignments. Their results showed that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Some researchers used preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al. 1998; Tiedemann 1999; Melamed 2000). The methods obtained multi-word candidates based on continuous N-gram statistics. The main limitation of these methods is that they cannot handle separated phrases and multi-word units in low frequencies. In order to handle all of the four cases in word alignment, our approach uses both the alignment information in statistical translation models and translation information in a rule-based machine translation system. It includes three steps. (1) A statistical translation model is employed to perform word alignment in two directions1 (English to Chinese, Chinese to English). (2) A rule-based E"
C04-1005,W01-1406,0,0.0244656,"s the translation information in the rule-based machine translation system to improve the statistical word alignment. The improved alignments allow the word(s) in the source language to be aligned to one or more words in the target language. Experimental results show a significant improvement in precision and recall of word alignment. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al. 1993). Besides being used in SMT, it is also used in translation lexicon building (Melamed 1996), transfer rule learning (Menezes and Richardson 2001), example-based machine translation (Somers 1999), etc. In previous alignment methods, some researches modeled the alignments as hidden parameters in a statistical translation model (Brown et al. 1993; Och and Ney 2000) or directly modeled them given the sentence pairs (Cherry and Lin 2003). Some researchers used similarity and association measures to build alignment links (Ahrenberg et al. 1998; Tufis and Barbu 2002). In addition, Wu (1997) used a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Generally speaking, ther"
C04-1005,P00-1056,0,0.735936,"t language. Experimental results show a significant improvement in precision and recall of word alignment. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al. 1993). Besides being used in SMT, it is also used in translation lexicon building (Melamed 1996), transfer rule learning (Menezes and Richardson 2001), example-based machine translation (Somers 1999), etc. In previous alignment methods, some researches modeled the alignments as hidden parameters in a statistical translation model (Brown et al. 1993; Och and Ney 2000) or directly modeled them given the sentence pairs (Cherry and Lin 2003). Some researchers used similarity and association measures to build alignment links (Ahrenberg et al. 1998; Tufis and Barbu 2002). In addition, Wu (1997) used a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Generally speaking, there are four cases in word alignment: word to word alignment, word to multi-word alignment, multi-word to word alignment, and multi-word to multi-word alignment. One of the most difficult tasks in word alignment is to fin"
C04-1005,tufis-barbu-2002-lexical,0,0.622409,"atistical machine translation (SMT) (Brown et al. 1993). Besides being used in SMT, it is also used in translation lexicon building (Melamed 1996), transfer rule learning (Menezes and Richardson 2001), example-based machine translation (Somers 1999), etc. In previous alignment methods, some researches modeled the alignments as hidden parameters in a statistical translation model (Brown et al. 1993; Och and Ney 2000) or directly modeled them given the sentence pairs (Cherry and Lin 2003). Some researchers used similarity and association measures to build alignment links (Ahrenberg et al. 1998; Tufis and Barbu 2002). In addition, Wu (1997) used a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Generally speaking, there are four cases in word alignment: word to word alignment, word to multi-word alignment, multi-word to word alignment, and multi-word to multi-word alignment. One of the most difficult tasks in word alignment is to find out the alignments that include multi-word units. For example, the statistical word alignment in IBM translation models (Brown et al. 1993) can only handle word to word and multi-word to word alignmen"
C04-1005,J97-3002,0,0.109237,"(Brown et al. 1993). Besides being used in SMT, it is also used in translation lexicon building (Melamed 1996), transfer rule learning (Menezes and Richardson 2001), example-based machine translation (Somers 1999), etc. In previous alignment methods, some researches modeled the alignments as hidden parameters in a statistical translation model (Brown et al. 1993; Och and Ney 2000) or directly modeled them given the sentence pairs (Cherry and Lin 2003). Some researchers used similarity and association measures to build alignment links (Ahrenberg et al. 1998; Tufis and Barbu 2002). In addition, Wu (1997) used a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Generally speaking, there are four cases in word alignment: word to word alignment, word to multi-word alignment, multi-word to word alignment, and multi-word to multi-word alignment. One of the most difficult tasks in word alignment is to find out the alignments that include multi-word units. For example, the statistical word alignment in IBM translation models (Brown et al. 1993) can only handle word to word and multi-word to word alignments. Some studies have be"
C04-1005,W03-1610,1,0.813124,"d to the Chinese word “习惯”, and “is” and “to” have null links in S . But in the translation set S 3 , “is used to&quot; is a phrase. Thus, we combine the three alignment links into a new link. The words “is”, “used” and “ to” are all aligned to the Chinese word “习惯”, denoted as (is used to, 习惯). Figure 2 describes the algorithm employed to improve the word alignment in the intersection set S . Word Similarity Calculation This section describes the method for monolingual word similarity calculation. This method calculates word similarity by using a bilingual dictionary, which is first introduced by Wu and Zhou (2003). The basic assumptions of this method are that the translations of a word can express its meanings and that two words are similar in meanings if they have mutual translations. Given a Chinese word, we get its translations with a Chinese-English bilingual dictionary. The translations of a word are used to construct its feature vector. The similarity of two words is estimated through their feature vectors with the cosine measure as shown in (Wu and Zhou 2003). If there are a Chinese word or phrase w and a Chinese word set Z , the word similarity between them is calculated as shown in Equation ("
C04-1005,1987.mtsummit-1.11,0,\N,Missing
C04-1005,J93-2003,0,\N,Missing
C04-1005,2001.mtsummit-ebmt.4,0,\N,Missing
C04-1005,C98-1004,0,\N,Missing
C08-1125,J93-2003,0,0.0149626,"main monolingual corpora to improve the indomain performance. We propose an algorithm to combine these different resources in a unified framework. Experimental results indicate that our method achieves absolute improvements of 8.16 and 3.36 BLEU scores on Chinese to English translation and English to French translation respectively, as compared with the baselines using only out-ofdomain corpora. 1 Introduction In statistical machine translation (SMT), the translation process is modeled to obtain the translation e best of the source sentence f by maximizing the following posterior probability (Brown et al., 1993). e best = arg max e p (e |f ) = arg max e p (f |e ) pLM (e ) (1) State-of-the-art SMT systems are trained on large collections of bilingual corpora for the C 2008. Licensed under the Creative Commons Attri○ bution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. Chengqing Zong NLPR, Institute of Automation Chinese Academy of Sciences Beijing 100080, China cqzong@nlpr.ia.ac.cn translation model p (f |e ) and monolingual target language corpora for the language model (LM) pLM (e ) . The trained SMT systems are suitable for"
C08-1125,W07-0718,0,0.0184073,"Missing"
C08-1125,W07-0722,0,0.0694584,"adaptation has also been studied for SMT (Bulyko et al., 2007). They explored discriminative estimation of language model weights by directly optimizing machine translation performances such as BLEU score (Papineni et al., 2002). Their experiments indicated about 0.4 BLEU score improvement. A shared task is organized as part of the Second Workshop on Statistical Machine Translation. A part of this shared task focused on domain adaptation for machine translation among European languages. Several studies investigated mixture model adaptation for both translation model and language model in SMT (Civera and Juan, 2007; Foster and Kuhn, 2007). Koehn and Schroeder (2007) investigated different adaptation methods for SMT. Their experiments indicate an absolute improvement of more than 1 BLEU score. To enlarge the in-domain bilingual corpus, Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from comparable corpora. Adding the extracted bilingual corpus to the training data improved the performance of the MT system. In addition, Ueffing et al. (2007) explored transductive learning for SMT, where source language corpora are used to train the models. They repeatedly translated s"
C08-1125,W07-0717,0,0.0622145,"n studied for SMT (Bulyko et al., 2007). They explored discriminative estimation of language model weights by directly optimizing machine translation performances such as BLEU score (Papineni et al., 2002). Their experiments indicated about 0.4 BLEU score improvement. A shared task is organized as part of the Second Workshop on Statistical Machine Translation. A part of this shared task focused on domain adaptation for machine translation among European languages. Several studies investigated mixture model adaptation for both translation model and language model in SMT (Civera and Juan, 2007; Foster and Kuhn, 2007). Koehn and Schroeder (2007) investigated different adaptation methods for SMT. Their experiments indicate an absolute improvement of more than 1 BLEU score. To enlarge the in-domain bilingual corpus, Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from comparable corpora. Adding the extracted bilingual corpus to the training data improved the performance of the MT system. In addition, Ueffing et al. (2007) explored transductive learning for SMT, where source language corpora are used to train the models. They repeatedly translated source sentences from the"
C08-1125,W06-3114,0,0.015664,"Missing"
C08-1125,P07-2045,0,0.00675131,"rmance of the SMT system. This kind of transductive learning can be seen as a means to adapt the SMT system to a new type of texts. In this paper, we use an in-domain translation dictionary and/or in-domain monolingual corpora (in both source language and target language) to improve the performance of a SMT system trained on the out-of-domain corpora. Thus, our method uses these resources, instead of an indomain bilingual corpus, to adapt a baseline system trained on the out-of-domain corpora to indomain texts. 3 Baseline MT System The phrase-based SMT system used in our experiments is Moses (Koehn et al., 2007). In Moses, phrase translation probabilities, reordering probabilities, and language model probabilities are combined in the log-linear model to obtain the best translation e best of the source sentence f : e best = arg max e p (e |f ) ≈ arg max e M ∑ λm hm (e, f) (2) m =1 The weights are set by a discriminative training method using a held-out data set as described in (Och, 2003). The models or features which are employed by the decoder are (a) one or several phrases tables, (b) one or more language models trained with SRILM toolkit (Stolcke, 2002), (c) distance-based and lexicalized distorti"
C08-1125,W07-0733,0,0.126911,"o et al., 2007). They explored discriminative estimation of language model weights by directly optimizing machine translation performances such as BLEU score (Papineni et al., 2002). Their experiments indicated about 0.4 BLEU score improvement. A shared task is organized as part of the Second Workshop on Statistical Machine Translation. A part of this shared task focused on domain adaptation for machine translation among European languages. Several studies investigated mixture model adaptation for both translation model and language model in SMT (Civera and Juan, 2007; Foster and Kuhn, 2007). Koehn and Schroeder (2007) investigated different adaptation methods for SMT. Their experiments indicate an absolute improvement of more than 1 BLEU score. To enlarge the in-domain bilingual corpus, Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from comparable corpora. Adding the extracted bilingual corpus to the training data improved the performance of the MT system. In addition, Ueffing et al. (2007) explored transductive learning for SMT, where source language corpora are used to train the models. They repeatedly translated source sentences from the development set and test se"
C08-1125,J05-4003,0,0.0482352,"s indicated about 0.4 BLEU score improvement. A shared task is organized as part of the Second Workshop on Statistical Machine Translation. A part of this shared task focused on domain adaptation for machine translation among European languages. Several studies investigated mixture model adaptation for both translation model and language model in SMT (Civera and Juan, 2007; Foster and Kuhn, 2007). Koehn and Schroeder (2007) investigated different adaptation methods for SMT. Their experiments indicate an absolute improvement of more than 1 BLEU score. To enlarge the in-domain bilingual corpus, Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from comparable corpora. Adding the extracted bilingual corpus to the training data improved the performance of the MT system. In addition, Ueffing et al. (2007) explored transductive learning for SMT, where source language corpora are used to train the models. They repeatedly translated source sentences from the development set and test set. Then the generated translations are used to improve the performance of the SMT system. This kind of transductive learning can be seen as a means to adapt the SMT system to a new type of texts. In"
C08-1125,P03-1021,0,0.00513523,"instead of an indomain bilingual corpus, to adapt a baseline system trained on the out-of-domain corpora to indomain texts. 3 Baseline MT System The phrase-based SMT system used in our experiments is Moses (Koehn et al., 2007). In Moses, phrase translation probabilities, reordering probabilities, and language model probabilities are combined in the log-linear model to obtain the best translation e best of the source sentence f : e best = arg max e p (e |f ) ≈ arg max e M ∑ λm hm (e, f) (2) m =1 The weights are set by a discriminative training method using a held-out data set as described in (Och, 2003). The models or features which are employed by the decoder are (a) one or several phrases tables, (b) one or more language models trained with SRILM toolkit (Stolcke, 2002), (c) distance-based and lexicalized distortion models, (d) word penalty, (e) phrase penalty. 994 Input Out-of-domain training data LO In-domain translation dictionary DI In-domain target language corpus TI (optional) In-domain source language corpus S I (optional) Begin Assign translation probabilities to D I If TI is available Training step: π = Estimate ( LO , DI , TI ) , where π represents the general model. Else Trainin"
C08-1125,P02-1040,0,0.105146,"ing indomain dictionary and monolingual corpora. And then we present the experimental results in sections 5. In the last section, we conclude this paper. 2 Related Work Translation model and language model adaptation are usually used in domain adaptation for SMT. Language model adaptation has been widely used in speech recognition (Bacchiani and Roark, 2003). In recent years, language model adaptation has also been studied for SMT (Bulyko et al., 2007). They explored discriminative estimation of language model weights by directly optimizing machine translation performances such as BLEU score (Papineni et al., 2002). Their experiments indicated about 0.4 BLEU score improvement. A shared task is organized as part of the Second Workshop on Statistical Machine Translation. A part of this shared task focused on domain adaptation for machine translation among European languages. Several studies investigated mixture model adaptation for both translation model and language model in SMT (Civera and Juan, 2007; Foster and Kuhn, 2007). Koehn and Schroeder (2007) investigated different adaptation methods for SMT. Their experiments indicate an absolute improvement of more than 1 BLEU score. To enlarge the in-domain"
C08-1125,P07-1004,0,0.228266,"y. Moreover, if an indomain source language corpus (SLC) is available, we automatically translate it and obtain a synthetic in-domain bilingual corpus. By adding this synthetic bilingual corpus to the training data, we rebuild the translation model to improve 993 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 993–1000 Manchester, August 2008 translation quality. We can repeatedly translate the in-domain source language corpus with the improved model until no more improvement can be made. This is similar to transductive learning described in (Ueffing et al., 2007). We perform domain adaptation experiments on two tasks: one is the Chinese to English translation, using the test set released by the International Workshop on Spoken Language Translation 2006 (IWSLT 2006), and the other is the English to French translation, using the data released by the Second Workshop on Statistical Machine Translation (WMT 2007) (CallisonBurch et al., 2007). Experimental results indicate that our method achieves absolute improvements of 8.16 and 3.36 BLEU scores on Chinese to English translation and English to French translation respectively, as compared with the baseline"
C08-1125,2007.mtsummit-papers.67,1,0.744835,"8.16 BLEU score (Model 7 vs. Model 1). Comparison of Different Dictionaries We compare the effects of different dictionaries with concern to the translation quality. Besides the manually-made in-domain dictionary, we use other two dictionaries: the LDC dictionary and an automatically built dictionary, which is extracted from the BTEC corpus. This extracted dictionary only contains Chinese words and their translations. The extraction method is as follows: • • Build a phrase table with the in-domain bilingual corpus. Filter those phrase pairs whose values are below a threshold as described in (Wu and Wang, 2007). • • From the filtered phrase table, extract the Chinese words and their translations. Assign constant translation probabilities to the entries of the extracted dictionary. Table 6 shows the translation results. All of the methods use the out-of-domain corpus, the in-domain target language corpus, and the corresponding translation dictionaries with constant translation probabilities. The results indicate that using the general-domain dictionary also improves translation quality, achieving an improvement of about 2 BLEU score as compared with Model 2 in Table 5. It can also be seen that the in"
C08-1125,H93-1040,0,\N,Missing
C08-1125,1993.mtsummit-1.24,0,\N,Missing
C16-1100,D10-1051,0,0.018129,"scribes some previous work on poetry generation and compares our work with previous methods. Section 3 describes our planning based poetry generation framework. We introduce the datasets and experimental results in Section 4. Section 5 concludes the paper. 2 Related Work Poetry generation is a challenging task in NLP. Oliveira (2009; 2012) proposed a Spanish poem generation method based on semantic and grammar templates. Netzer et al. (2009) employed a method based on word association measures. Tosa et al. (2008) and Wu et al. (2009) used a phrase search approach for Japanese poem generation. Greene et al. (2010) applied statistical methods to analyze, generate and translate rhythmic poetry. Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constrains. Yan et al. (2013) considered the poetry generation as an optimization problem based on a summarization framework with several constraints. Manurung (2004; 2012) and Zhou et al. (2010) used genetic algorithms for generating poems. An important approach to poem generation is based on statistical machine translation (SMT). Jiang and Zhou (2008) used an SMT-based model in gen"
C16-1100,C08-1048,0,0.154329,"ward tone); the last character of the second and last line in a quatrain must belong to the same rhyme category (Wang, 2002). With such strict restrictions, the well-written quatrain is full of rhythmic beauty. In recent years, the research of automatic poetry generation has received great attention. Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al., 2010; Manurung et al., 2012), summarization methods (Yan et al., 2013) and statistical machine translation methods (Jiang and Zhou, 2008; He et al., 2012) to generate poems. More recently, deep learning methods have emerged as a promising discipline, which considers the poetry generation as a sequence-to-sequence generation problem (Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016). These methods usually generate the first line by selecting one line from the dataset of poems according to the user’s writing intents (usually a set of keywords), and the other three lines are generated based on the first line and the previous lines. The user’s writing intent can only affect the first line, and the rest three lines may ha"
C16-1100,D16-1230,0,0.00880819,"Missing"
C16-1100,D16-1126,0,0.139197,"Missing"
C16-1100,W04-3252,0,0.0142967,"according to one sub-topic and all the preceding lines. 3.2 Poem Planning 3.2.1 Keyword Extraction The user’s input writing intent can be represented as a sequence of words. There is an assumption in the Poem Planning stage that the number of keywords extracted from the input query Q must be equal to the number of lines N in the poem, which can ensure each line takes just one keyword as the sub-topic. If the user’s input query Q is too long, we need to extract the most important N words and keep the original order as the keywords sequence to satisfy the requirement. We use TextRank algorithm (Mihalcea and Tarau, 2004) to evaluate the importance of words. It is a graph-based ranking algorithm based on PageRank (Brin and Page, 1998). Each candidate word is represented by a vertex in the graph and edges are added between two words according to their cooccurrence; the edge weight is set according to the total count of co-occurrence strength of the two words. The TextRank score S(Vi ) is initialized to a default value (e.g. 1.0) and computed iteratively until convergence according to the following equation: S(Vi ) = (1 − d) + d X Vj ∈E(Vi ) P wji Vk ∈E(Vj ) wjk S(Vj ), (1) where wij is the weight of the edge be"
C16-1100,C16-1316,0,0.0286406,"ent hidden layers of the decoder and two encoders contained 512 hidden units. Parameters of our model were randomly initialized over a uniform distribution with support [-0.08,0.08]. The model was trained with the AdaDelta algorithm (Zeiler, 2012), where the minibatch was set to be 128. The final model is selected according to the perplexity on the validation set. 4.3 Evaluation 4.3.1 Evaluation Metrics It is well known that accurate evaluation of text generation system is difficult, such as the poetry generation and dialog response generation (Zhang and Lapata, 2014; Schatzmann et al., 2005; Mou et al., 2016). There are thousands of ways to generate an appropriate and relative poem or dialog response given a specific topic, the limited references are impossible to cover all the correct results. Liu et al. (2016) has recently shown that the overlap-based automatic evaluation metrics adapted for dialog responses, such 1 A collaborative online encyclopedia provided by Chinese search engine Baidu: http://baike.baidu.com. 1056 Poeticness Fluency Coherence Meaning Does the poem follow the rhyme and tone requirements ? Does the poem read smoothly and fluently? Is the poem coherent across lines? Does the"
C16-1100,W09-2005,0,0.0455963,"r genres of poetry in China. The principles of a quatrain include: The poem consists of four lines and each line has five or seven characters; every character has a particular tone, Ping (the level tone) or Ze (the downward tone); the last character of the second and last line in a quatrain must belong to the same rhyme category (Wang, 2002). With such strict restrictions, the well-written quatrain is full of rhythmic beauty. In recent years, the research of automatic poetry generation has received great attention. Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al., 2010; Manurung et al., 2012), summarization methods (Yan et al., 2013) and statistical machine translation methods (Jiang and Zhou, 2008; He et al., 2012) to generate poems. More recently, deep learning methods have emerged as a promising discipline, which considers the poetry generation as a sequence-to-sequence generation problem (Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016). These methods usually generate the first line by selecting one line from the dataset of poems according to the user’s wr"
C16-1100,2005.sigdial-1.6,0,0.0113308,"et al., 2013). The recurrent hidden layers of the decoder and two encoders contained 512 hidden units. Parameters of our model were randomly initialized over a uniform distribution with support [-0.08,0.08]. The model was trained with the AdaDelta algorithm (Zeiler, 2012), where the minibatch was set to be 128. The final model is selected according to the perplexity on the validation set. 4.3 Evaluation 4.3.1 Evaluation Metrics It is well known that accurate evaluation of text generation system is difficult, such as the poetry generation and dialog response generation (Zhang and Lapata, 2014; Schatzmann et al., 2005; Mou et al., 2016). There are thousands of ways to generate an appropriate and relative poem or dialog response given a specific topic, the limited references are impossible to cover all the correct results. Liu et al. (2016) has recently shown that the overlap-based automatic evaluation metrics adapted for dialog responses, such 1 A collaborative online encyclopedia provided by Chinese search engine Baidu: http://baike.baidu.com. 1056 Poeticness Fluency Coherence Meaning Does the poem follow the rhyme and tone requirements ? Does the poem read smoothly and fluently? Is the poem coherent acro"
C16-1100,D14-1074,0,0.669329,"t years, the research of automatic poetry generation has received great attention. Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al., 2010; Manurung et al., 2012), summarization methods (Yan et al., 2013) and statistical machine translation methods (Jiang and Zhou, 2008; He et al., 2012) to generate poems. More recently, deep learning methods have emerged as a promising discipline, which considers the poetry generation as a sequence-to-sequence generation problem (Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016). These methods usually generate the first line by selecting one line from the dataset of poems according to the user’s writing intents (usually a set of keywords), and the other three lines are generated based on the first line and the previous lines. The user’s writing intent can only affect the first line, and the rest three lines may have no association with the main topic of the poem, which may lead to semantic inconsistency when generating poems. In addition, topics of poems are usually represented by the words from the collected poems in the training"
C16-1253,P14-2134,0,0.0296287,"ds which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. Mikolov presented several extensions of Skip-gram that improve both the quality of the vectors and the training speed (Mikolov et al., 2013b) (Mikolov et al., 2013a). Paragraph vector that is an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts was proposed in (Le and Mikolov, 2014). Word embedding is adapted for incorporating contextual information in learning vector-space representations of situated language (Bamman et al., 2014). A more relevant work is (Liu et al., 2015), which inputs the result of topic modeling into word embedding models to learn the topical word embedding. The major difference between this work and ours is that they did not aim to integrate topic modeling and word embedding and yet only utilizes the result of topic modeling as the input of word embedding models. Recently, (Nguyen et al., 2015) extended two Dirichlet multinomial topic models by incorporating word embeddings to improve the word-topic mapping. (Li et al., 2016) proposed a generative model that replaces the Multinomial word generatio"
C16-1253,P16-1063,0,0.0210258,"ation in learning vector-space representations of situated language (Bamman et al., 2014). A more relevant work is (Liu et al., 2015), which inputs the result of topic modeling into word embedding models to learn the topical word embedding. The major difference between this work and ours is that they did not aim to integrate topic modeling and word embedding and yet only utilizes the result of topic modeling as the input of word embedding models. Recently, (Nguyen et al., 2015) extended two Dirichlet multinomial topic models by incorporating word embeddings to improve the word-topic mapping. (Li et al., 2016) proposed a generative model that replaces the Multinomial word generation assumption of LDA with embedding based assumption. Although topic modeling and word embedding receive intensive attention in recent years, to the best of our knowledge, there is no previous endeavor on integrating them together as a joint learning task to enhance each other. LTE paves the way for collectively modeling of word co-occurrence information at different granularity levels while retaining the topic modeling result as well as the word embedding result. 3 Generative Process of Latent Topic Embedding Latent Topic"
C16-1253,Q15-1022,0,0.268041,"from variable-length pieces of texts was proposed in (Le and Mikolov, 2014). Word embedding is adapted for incorporating contextual information in learning vector-space representations of situated language (Bamman et al., 2014). A more relevant work is (Liu et al., 2015), which inputs the result of topic modeling into word embedding models to learn the topical word embedding. The major difference between this work and ours is that they did not aim to integrate topic modeling and word embedding and yet only utilizes the result of topic modeling as the input of word embedding models. Recently, (Nguyen et al., 2015) extended two Dirichlet multinomial topic models by incorporating word embeddings to improve the word-topic mapping. (Li et al., 2016) proposed a generative model that replaces the Multinomial word generation assumption of LDA with embedding based assumption. Although topic modeling and word embedding receive intensive attention in recent years, to the best of our knowledge, there is no previous endeavor on integrating them together as a joint learning task to enhance each other. LTE paves the way for collectively modeling of word co-occurrence information at different granularity levels while"
C16-1253,S10-1032,0,0.0301478,"opics. This observation verifies our assumption that the collectively utilizing co-occurrence information of different granularity has the potential of improving the performance of topic models. 5.3 Topical Word Extraction We now evaluate the performance of LTE in the scenario of topical word extraction, which is critical for natural language understanding in modern search engines. Given a document, the goal of topical word extraction is to find some words that are highly relevant to the document theme. Conventionally, LDA plays an important role in topical word extraction (Zhao et al., 2011)(Pasquier, 2010). The existing methods based LDA are usually plagued by the weakness of capturing the semantics of words with low frequency. In this section, we study whether the embeddings generated by LTE are able to alleviate this problem. Ten thousands Web pages are utilized for this evaluation and the ground truth (i.e., the words that are highly relevant to the document theme) is manually prepared by human experts. To derive the topical words for a document d, we first calculate the score of each word w in d and the score reflect the relevance between w and the themes of d. Then we sort all the words ac"
C16-1253,P11-1039,0,\N,Missing
D07-1030,J00-1004,0,0.0191954,"section 2 we summarize the related work. We then describe our method Using RBMT systems to produce bilingual corpus for SMT in section 3. Section 4 describes the resources used in the experiments. Section 5 presents the experiment result, followed by the discussion in section 6. Finally, we conclude and present the future work in section 7. 2 Related Work In the MT field, by far the most dominant paradigm is SMT. SMT has evolved from the original word-based approach (Brown et al., 1993) into phrase-based approaches (Koehn et al., 2003; Och and Ney, 2004) and syntax-based approaches (Wu, 1997; Alshawi et al., 2000; Yamada and Knignt, 2001; Chiang, 2005). On the other hand, much important work continues to be carried out in Example-Based Machine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids has increased over the last few years. Some research focuse"
D07-1030,J93-2003,0,0.00855924,"Missing"
D07-1030,W03-0310,0,0.0577463,"Missing"
D07-1030,E06-1032,0,0.0473225,"Missing"
D07-1030,2005.mtsummit-ebmt.3,0,0.0352193,"ed in the experiments. Section 5 presents the experiment result, followed by the discussion in section 6. Finally, we conclude and present the future work in section 7. 2 Related Work In the MT field, by far the most dominant paradigm is SMT. SMT has evolved from the original word-based approach (Brown et al., 1993) into phrase-based approaches (Koehn et al., 2003; Och and Ney, 2004) and syntax-based approaches (Wu, 1997; Alshawi et al., 2000; Yamada and Knignt, 2001; Chiang, 2005). On the other hand, much important work continues to be carried out in Example-Based Machine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids has increased over the last few years. Some research focused on the hybrid of various corpus-based MT methods, such as SMT and EBMT (Vogel and Ney, 2000; Marcu, 2001; Groves and Way, 2006; Menezes and Quirk, 2005). Others tried to"
D07-1030,P05-1033,0,0.037433,"n describe our method Using RBMT systems to produce bilingual corpus for SMT in section 3. Section 4 describes the resources used in the experiments. Section 5 presents the experiment result, followed by the discussion in section 6. Finally, we conclude and present the future work in section 7. 2 Related Work In the MT field, by far the most dominant paradigm is SMT. SMT has evolved from the original word-based approach (Brown et al., 1993) into phrase-based approaches (Koehn et al., 2003; Och and Ney, 2004) and syntax-based approaches (Wu, 1997; Alshawi et al., 2000; Yamada and Knignt, 2001; Chiang, 2005). On the other hand, much important work continues to be carried out in Example-Based Machine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids has increased over the last few years. Some research focused on the hybrid of various corpus-based"
D07-1030,A94-1016,0,0.0396351,"ased methods. Habash et al. (2006) built an Arabic-English generationheavy MT system and boosted it with SMT components. METIS-II is a hybrid machine translation system, in which insights from SMT, EBMT, and RBMT are used (Vandeghinste et al., 2006). Seneff et al. (2006) combined an interlingual translation 288 framework with phrase-based SMT for spoken language translation in a limited domain. They automatically generated a corpus of EnglishChinese pairs from the same interlingual representation by parsing the English corpus and then paraphrasing each utterance into both English and Chinese. Frederking and Nirenburg (1994) produced the first MEMT system by combining outputs from three different MT engines based on their knowledge of the inner workings of the engines. Nomoto (2004) used voted language models to select the best output string at sentence level. Some recent approaches to MEMT used word alignment techniques for comparison between the MT systems (Jayaraman and Lavie, 2005; Zaanen and Somers, 2005; Matusov et al. 2006). All the above MEMT systems operate on MT outputs for complete input sentences. Mellebeek et al. (2006) presented a different approach, using a recursive decomposition algorithm that pr"
D07-1030,2006.eamt-1.15,0,0.0246764,"in Example-Based Machine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids has increased over the last few years. Some research focused on the hybrid of various corpus-based MT methods, such as SMT and EBMT (Vogel and Ney, 2000; Marcu, 2001; Groves and Way, 2006; Menezes and Quirk, 2005). Others tried to exploit the advantages of both rule-based and corpus-based methods. Habash et al. (2006) built an Arabic-English generationheavy MT system and boosted it with SMT components. METIS-II is a hybrid machine translation system, in which insights from SMT, EBMT, and RBMT are used (Vandeghinste et al., 2006). Seneff et al. (2006) combined an interlingual translation 288 framework with phrase-based SMT for spoken language translation in a limited domain. They automatically generated a corpus of EnglishChinese pairs from the same interlingual representation"
D07-1030,P05-3026,0,0.0135751,"ion in a limited domain. They automatically generated a corpus of EnglishChinese pairs from the same interlingual representation by parsing the English corpus and then paraphrasing each utterance into both English and Chinese. Frederking and Nirenburg (1994) produced the first MEMT system by combining outputs from three different MT engines based on their knowledge of the inner workings of the engines. Nomoto (2004) used voted language models to select the best output string at sentence level. Some recent approaches to MEMT used word alignment techniques for comparison between the MT systems (Jayaraman and Lavie, 2005; Zaanen and Somers, 2005; Matusov et al. 2006). All the above MEMT systems operate on MT outputs for complete input sentences. Mellebeek et al. (2006) presented a different approach, using a recursive decomposition algorithm that produces simple chunks as input to the MT engines. A consensus translation is produced by combining the best chunk translation. This paper uses RBMT outputs to improve the performance of SMT systems. Instead of RBMT outputs, other researchers have used SMT outputs to boost translation quality. Callision-Burch and Osborne (2003) used co-training to extend existing par"
D07-1030,koen-2004-pharaoh,0,0.25782,"rase. d ( a i − bi −1 ) is the distortion probability. p w ( f i |e i , a) is the lexical weight, and λ is the strength of the lexical weight. 3.2 Interpolated Models We train synthetic models with the synthetic bilingual corpus produced by the RBMT systems. We can also train a translation model, namely standard model, if a real bilingual corpus is available. In order to make full use of these two kinds of corpora, we conduct linear interpolation between them. In this paper, the distortion probability in equation (2) is estimated during decoding, using the same method as described in Pharaoh (Koehn, 2004). For the phrase translation probability and lexical weight, we interpolate them as shown in (3) and (4). n φ ( f |e) = ∑ α i φ i ( f |e) (3) i =0 n p w ( f |e, a ) = ∑ β i p w,i ( f |e, a ) ∑α i =0 Where the translation model p (f |e ) can be decomposed into I ficients, ensuring (4) i =0 Where φ 0 ( f |e) and p w,0 ( f |e, a) denote the phrase translation probability and lexical weight trained with the real bilingual corpus, respectively. φi ( f |e) and p w,i ( f |e, a) ( i = 1,..., n ) are the phrase translation probability and lexical weight estimated by n synthetic corpora produced by the"
D07-1030,W06-3114,0,0.127593,"For the decoder, we use Pharaoh (Koehn, 2004). We run the decoder with its default settings (maximum phrase length 7) and then use Koehn's implementation of minimum error rate training (Och, 2003) to tune the feature weights on the de2 The full name of HTRDP is National High Technology Research and Development Program of China, also named as 863 Program. 3 It is located at http://www.statmt.org/wmt06/sharedtask/baseline.html. velopment set. The translation quality is evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). We use the same method described in (Koehn and Monz, 2006) to perform the significance test. 5 Experimental Results 5.1 Results on Synthetic Corpus Only With the monolingual English corpus and the English side of the real bilingual corpus, we translate them into Chinese using the two commercial RBMT systems and produce two synthetic bilingual corpora. With the corpora, we train two synthetic models as described in section 3.1. Based on the synthetic models, we also perform linear interpolation as shown in section 3.2, without the standard models. We tune the interpolation weights using the development set, and achieve the best performance when α 1 ="
D07-1030,N03-1017,0,0.0752916,"h the standard model. The remainder of this paper is organized as follows. In section 2 we summarize the related work. We then describe our method Using RBMT systems to produce bilingual corpus for SMT in section 3. Section 4 describes the resources used in the experiments. Section 5 presents the experiment result, followed by the discussion in section 6. Finally, we conclude and present the future work in section 7. 2 Related Work In the MT field, by far the most dominant paradigm is SMT. SMT has evolved from the original word-based approach (Brown et al., 1993) into phrase-based approaches (Koehn et al., 2003; Och and Ney, 2004) and syntax-based approaches (Wu, 1997; Alshawi et al., 2000; Yamada and Knignt, 2001; Chiang, 2005). On the other hand, much important work continues to be carried out in Example-Based Machine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Rese"
D07-1030,P01-1050,0,0.0226073,"e carried out in Example-Based Machine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids has increased over the last few years. Some research focused on the hybrid of various corpus-based MT methods, such as SMT and EBMT (Vogel and Ney, 2000; Marcu, 2001; Groves and Way, 2006; Menezes and Quirk, 2005). Others tried to exploit the advantages of both rule-based and corpus-based methods. Habash et al. (2006) built an Arabic-English generationheavy MT system and boosted it with SMT components. METIS-II is a hybrid machine translation system, in which insights from SMT, EBMT, and RBMT are used (Vandeghinste et al., 2006). Seneff et al. (2006) combined an interlingual translation 288 framework with phrase-based SMT for spoken language translation in a limited domain. They automatically generated a corpus of EnglishChinese pairs from the same interl"
D07-1030,E06-1005,0,0.0133574,"d a corpus of EnglishChinese pairs from the same interlingual representation by parsing the English corpus and then paraphrasing each utterance into both English and Chinese. Frederking and Nirenburg (1994) produced the first MEMT system by combining outputs from three different MT engines based on their knowledge of the inner workings of the engines. Nomoto (2004) used voted language models to select the best output string at sentence level. Some recent approaches to MEMT used word alignment techniques for comparison between the MT systems (Jayaraman and Lavie, 2005; Zaanen and Somers, 2005; Matusov et al. 2006). All the above MEMT systems operate on MT outputs for complete input sentences. Mellebeek et al. (2006) presented a different approach, using a recursive decomposition algorithm that produces simple chunks as input to the MT engines. A consensus translation is produced by combining the best chunk translation. This paper uses RBMT outputs to improve the performance of SMT systems. Instead of RBMT outputs, other researchers have used SMT outputs to boost translation quality. Callision-Burch and Osborne (2003) used co-training to extend existing parallel corpora, wherein machine translations are"
D07-1030,2006.amta-papers.13,0,0.0257063,"Missing"
D07-1030,2005.mtsummit-ebmt.13,0,0.0148516,"ine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids has increased over the last few years. Some research focused on the hybrid of various corpus-based MT methods, such as SMT and EBMT (Vogel and Ney, 2000; Marcu, 2001; Groves and Way, 2006; Menezes and Quirk, 2005). Others tried to exploit the advantages of both rule-based and corpus-based methods. Habash et al. (2006) built an Arabic-English generationheavy MT system and boosted it with SMT components. METIS-II is a hybrid machine translation system, in which insights from SMT, EBMT, and RBMT are used (Vandeghinste et al., 2006). Seneff et al. (2006) combined an interlingual translation 288 framework with phrase-based SMT for spoken language translation in a limited domain. They automatically generated a corpus of EnglishChinese pairs from the same interlingual representation by parsing the English cor"
D07-1030,P04-1063,0,0.0201967,"insights from SMT, EBMT, and RBMT are used (Vandeghinste et al., 2006). Seneff et al. (2006) combined an interlingual translation 288 framework with phrase-based SMT for spoken language translation in a limited domain. They automatically generated a corpus of EnglishChinese pairs from the same interlingual representation by parsing the English corpus and then paraphrasing each utterance into both English and Chinese. Frederking and Nirenburg (1994) produced the first MEMT system by combining outputs from three different MT engines based on their knowledge of the inner workings of the engines. Nomoto (2004) used voted language models to select the best output string at sentence level. Some recent approaches to MEMT used word alignment techniques for comparison between the MT systems (Jayaraman and Lavie, 2005; Zaanen and Somers, 2005; Matusov et al. 2006). All the above MEMT systems operate on MT outputs for complete input sentences. Mellebeek et al. (2006) presented a different approach, using a recursive decomposition algorithm that produces simple chunks as input to the MT engines. A consensus translation is produced by combining the best chunk translation. This paper uses RBMT outputs to imp"
D07-1030,P03-1021,0,0.0116753,"ource sentence in the test set and the development set has 4 different references. 4.2 Tools In this paper, we use two off-the-shelf commercial English to Chinese RBMT systems to produce the synthetic bilingual corpus. We also need a trainer and a decoder to perform phrase-based SMT. We use Koehn's training scripts 3 to train the translation model, and the SRILM toolkit (Stolcke, 2002) to train language model. For the decoder, we use Pharaoh (Koehn, 2004). We run the decoder with its default settings (maximum phrase length 7) and then use Koehn's implementation of minimum error rate training (Och, 2003) to tune the feature weights on the de2 The full name of HTRDP is National High Technology Research and Development Program of China, also named as 863 Program. 3 It is located at http://www.statmt.org/wmt06/sharedtask/baseline.html. velopment set. The translation quality is evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). We use the same method described in (Koehn and Monz, 2006) to perform the significance test. 5 Experimental Results 5.1 Results on Synthetic Corpus Only With the monolingual English corpus and the English side of the real bilingual co"
D07-1030,J04-4002,0,0.019069,". The remainder of this paper is organized as follows. In section 2 we summarize the related work. We then describe our method Using RBMT systems to produce bilingual corpus for SMT in section 3. Section 4 describes the resources used in the experiments. Section 5 presents the experiment result, followed by the discussion in section 6. Finally, we conclude and present the future work in section 7. 2 Related Work In the MT field, by far the most dominant paradigm is SMT. SMT has evolved from the original word-based approach (Brown et al., 1993) into phrase-based approaches (Koehn et al., 2003; Och and Ney, 2004) and syntax-based approaches (Wu, 1997; Alshawi et al., 2000; Yamada and Knignt, 2001; Chiang, 2005). On the other hand, much important work continues to be carried out in Example-Based Machine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids"
D07-1030,P02-1040,0,0.0775652,"nd the SRILM toolkit (Stolcke, 2002) to train language model. For the decoder, we use Pharaoh (Koehn, 2004). We run the decoder with its default settings (maximum phrase length 7) and then use Koehn's implementation of minimum error rate training (Och, 2003) to tune the feature weights on the de2 The full name of HTRDP is National High Technology Research and Development Program of China, also named as 863 Program. 3 It is located at http://www.statmt.org/wmt06/sharedtask/baseline.html. velopment set. The translation quality is evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). We use the same method described in (Koehn and Monz, 2006) to perform the significance test. 5 Experimental Results 5.1 Results on Synthetic Corpus Only With the monolingual English corpus and the English side of the real bilingual corpus, we translate them into Chinese using the two commercial RBMT systems and produce two synthetic bilingual corpora. With the corpora, we train two synthetic models as described in section 3.1. Based on the synthetic models, we also perform linear interpolation as shown in section 3.2, without the standard models. We tune the interpolation weights using the d"
D07-1030,2006.amta-papers.24,0,0.0339716,"Missing"
D07-1030,2006.iwslt-papers.3,0,0.0519484,"m that produces simple chunks as input to the MT engines. A consensus translation is produced by combining the best chunk translation. This paper uses RBMT outputs to improve the performance of SMT systems. Instead of RBMT outputs, other researchers have used SMT outputs to boost translation quality. Callision-Burch and Osborne (2003) used co-training to extend existing parallel corpora, wherein machine translations are selectively added to training corpora with multiple source texts. They also created training data for a language pair without a parallel corpus by using multiple source texts. Ueffing (2006) explored monolingual source-language data to improve an existing machine translation system via selftraining. The source data is translated by a SMT system, and the reliable translations are automatically identified. Both of the methods improved translation quality. 3 Method In this paper, we use the synthetic and real bilingual corpus to train the phrase-based translation models. 3.1 Phrase-Based Models According to the translation model presented in (Koehn et al., 2003), given a source sentence f , the best target translation e best can be obtained using the following model e best = arg max"
D07-1030,vandeghinste-etal-2006-metis,0,0.0197189,"of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids has increased over the last few years. Some research focused on the hybrid of various corpus-based MT methods, such as SMT and EBMT (Vogel and Ney, 2000; Marcu, 2001; Groves and Way, 2006; Menezes and Quirk, 2005). Others tried to exploit the advantages of both rule-based and corpus-based methods. Habash et al. (2006) built an Arabic-English generationheavy MT system and boosted it with SMT components. METIS-II is a hybrid machine translation system, in which insights from SMT, EBMT, and RBMT are used (Vandeghinste et al., 2006). Seneff et al. (2006) combined an interlingual translation 288 framework with phrase-based SMT for spoken language translation in a limited domain. They automatically generated a corpus of EnglishChinese pairs from the same interlingual representation by parsing the English corpus and then paraphrasing each utterance into both English and Chinese. Frederking and Nirenburg (1994) produced the first MEMT system by combining outputs from three different MT engines based on their knowledge of the inner workings of the engines. Nomoto (2004) used voted language models to select the best output str"
D07-1030,C00-2172,0,0.0315135,"t work continues to be carried out in Example-Based Machine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids has increased over the last few years. Some research focused on the hybrid of various corpus-based MT methods, such as SMT and EBMT (Vogel and Ney, 2000; Marcu, 2001; Groves and Way, 2006; Menezes and Quirk, 2005). Others tried to exploit the advantages of both rule-based and corpus-based methods. Habash et al. (2006) built an Arabic-English generationheavy MT system and boosted it with SMT components. METIS-II is a hybrid machine translation system, in which insights from SMT, EBMT, and RBMT are used (Vandeghinste et al., 2006). Seneff et al. (2006) combined an interlingual translation 288 framework with phrase-based SMT for spoken language translation in a limited domain. They automatically generated a corpus of EnglishChinese pairs from th"
D07-1030,J97-3002,0,0.0330266,"llows. In section 2 we summarize the related work. We then describe our method Using RBMT systems to produce bilingual corpus for SMT in section 3. Section 4 describes the resources used in the experiments. Section 5 presents the experiment result, followed by the discussion in section 6. Finally, we conclude and present the future work in section 7. 2 Related Work In the MT field, by far the most dominant paradigm is SMT. SMT has evolved from the original word-based approach (Brown et al., 1993) into phrase-based approaches (Koehn et al., 2003; Och and Ney, 2004) and syntax-based approaches (Wu, 1997; Alshawi et al., 2000; Yamada and Knignt, 2001; Chiang, 2005). On the other hand, much important work continues to be carried out in Example-Based Machine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids has increased over the last few years"
D07-1030,P01-1067,0,0.123664,"Missing"
D07-1030,2005.mtsummit-papers.23,0,0.110715,"Missing"
D07-1030,2006.amta-papers.7,0,\N,Missing
D07-1030,2005.eamt-1.20,0,\N,Missing
D09-1051,P06-1120,0,0.174786,"w. To avoid explosion, these approaches generally limit the window size to a small number. As a result, long-span collocations can not be extracted 1 . In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high. The above problems could be partially solved by introducing more resources into collocation extraction, such as chunker (Wermter and Hahn, 2004), parser (Lin, 1998; Seretan and Wehrli, 2006) and WordNet (Pearce, 2001). This paper proposes a novel monolingual word alignment (MWA) method to extract collocation of higher quality and with longer spans only from monolingual corpus, without using any additional resources. The difference between MWA and bilingual word alignment (Brown et al., 1993) is that the MWA method works on monolingual parallel corpus instead of bilingual corpus used by bilingual word alignment. The 1 Here, &quot;span of collocation&quot; means the distance of two words in a collocation. For example, if the span of the collocation (w1, w2) is 6, it means there are 5 words i"
D09-1051,C04-1141,0,0.115627,"cations from the word pairs in a given window. To avoid explosion, these approaches generally limit the window size to a small number. As a result, long-span collocations can not be extracted 1 . In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high. The above problems could be partially solved by introducing more resources into collocation extraction, such as chunker (Wermter and Hahn, 2004), parser (Lin, 1998; Seretan and Wehrli, 2006) and WordNet (Pearce, 2001). This paper proposes a novel monolingual word alignment (MWA) method to extract collocation of higher quality and with longer spans only from monolingual corpus, without using any additional resources. The difference between MWA and bilingual word alignment (Brown et al., 1993) is that the MWA method works on monolingual parallel corpus instead of bilingual corpus used by bilingual word alignment. The 1 Here, &quot;span of collocation&quot; means the distance of two words in a collocation. For example, if the span of the collocati"
D09-1051,J93-2003,0,0.0505828,"Missing"
D09-1051,J93-1003,0,0.0325009,"ns in this paper include phrasal verbs (e.g. &quot;put on&quot;), proper nouns (e.g. &quot;New York&quot;), idioms (e.g. &quot;dry run&quot;), compound nouns (e.g. &quot;ice cream&quot;), correlative conjunctions (e.g. &quot;either … or&quot;), and the other commonly used combinations in following types: verb+noun, adjective+noun, adverb+verb, adverb+adjective and adjective+preposition (e.g. &quot;break rules&quot;, &quot;strong tea&quot;, &quot;softly whisper&quot;, &quot;fully aware&quot;, and &quot;fond of&quot;). Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts (Choueka et al., 1983; Church and Hanks, 1990; Smadja, 1993; Dunning, 1993; Pearce, 2002; Evert, 2004). These approaches use association measures to discover collocations from the word pairs in a given window. To avoid explosion, these approaches generally limit the window size to a small number. As a result, long-span collocations can not be extracted 1 . In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high. The above problems could be part"
D09-1051,pearce-2002-comparative,0,\N,Missing
D09-1051,J90-1003,0,\N,Missing
D09-1051,J93-1007,0,\N,Missing
D13-1050,P05-1074,0,0.0485877,"nodes (s source phrases and p pivot phrases) to represent the translation graph. (6) A =  gij  ( s + p )×( s + p ) where gij is the i,j-th elements of matrix A. We can split the matrix A into 4 sub-matrixes: 0 s×s Asp  (7) A=  A 0 ps p × p   where the sub-matrix Asp = [ pik ]s× p represents the translation probabilities from source to pivot language, and Aps represents the similar meaning. Take 3 steps walks as an example: Step1: 0 s×s Asp  A=   Aps 0 p× p  Step2:  Asp × Aps A2 =   0 p× s 4.3  Aps × Asp  Step3: 0 s× s  A3 =   Aps × Asp × Aps analogous to paraphrasing (Bannard and Callison-Burch, 2005). For the example shown in figure 1 as an example, the hidden relation between “很可口 henkekou” and “非常好吃 feichanghaochi” can be found through Step 2. 3. The third step describes the following procedure: S-P-S’-P’. An extended source-pivot phrase table is generated by 3-step random walks. Compared with the initial phrase table in Step1, although the number of phrases is not increased, the relations between phrase pairs are increased and more translation rules can be obtained. Still for the example in Figure 1 , the hidden relation between “很可口 henkekou” and “really delicious” can be generated in"
D13-1050,I11-1154,0,0.0328767,"Missing"
D13-1050,I11-1153,0,0.02207,"ly, they can be classified into 3 kinds of methods: Transfer Method: Within the transfer framework (Utiyama and Isahara, 2007; Wang et al., 2008; Costa-jussà et al., 2011), a source sentence is first translated to n pivot sentences via a sourcepivot translation system, and then each pivot sentence is translated to m target sentences via a pivot-target translation system. At each step (source to pivot and pivot to target), multiple translation outputs will be generated, thus a minimum Bayesrisk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). A problem with the transfer method is that it needs to decode twice. On one 525 hand, the time cost is doubled; on the other hand, the translation error of the source-pivot translation system will be transferred to the pivot-target translation. Synthetic Method: A synthetic method creates a synthetic source-target corpus using source-pivot translation model or pivot-target translation model (Utiyama et al., 2008; Wu and Wang, 2009). For example, we can translate each pivot sentence in the pivot-target corpus to source language with a pivot-source model, and then combine the translated source"
D13-1050,2005.mtsummit-papers.11,0,0.217955,"Missing"
D13-1050,C00-2163,0,0.176928,"Missing"
D13-1050,N07-1061,0,0.224847,"ional pivot-based method, triangulation method. The remainder of this paper is organized as follows. In section 2, we describe the related work. We review the triangulation method for pivotbased machine translation in section 3. Section 4 describes the random walk models. In section 5 and section 6, we describe the experiments and analyze the performance, respectively. Section 7 gives a conclusion of the paper. 2 Related Work Several methods have been proposed for pivotbased translation. Typically, they can be classified into 3 kinds of methods: Transfer Method: Within the transfer framework (Utiyama and Isahara, 2007; Wang et al., 2008; Costa-jussà et al., 2011), a source sentence is first translated to n pivot sentences via a sourcepivot translation system, and then each pivot sentence is translated to m target sentences via a pivot-target translation system. At each step (source to pivot and pivot to target), multiple translation outputs will be generated, thus a minimum Bayesrisk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). A problem with the transfer method is that it needs to decode twice. On one 525 hand, the time cost is dou"
D13-1050,2008.iwslt-evaluation.18,1,0.886415,"Missing"
D13-1050,P09-1018,1,0.897372,"ranslation outputs will be generated, thus a minimum Bayesrisk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). A problem with the transfer method is that it needs to decode twice. On one 525 hand, the time cost is doubled; on the other hand, the translation error of the source-pivot translation system will be transferred to the pivot-target translation. Synthetic Method: A synthetic method creates a synthetic source-target corpus using source-pivot translation model or pivot-target translation model (Utiyama et al., 2008; Wu and Wang, 2009). For example, we can translate each pivot sentence in the pivot-target corpus to source language with a pivot-source model, and then combine the translated source sentence with the target sentence to obtain a synthetic source-target corpus, and vice versa. However, it is difficult to build a high quality translation system with a corpus created by a machine translation system. Triangulation Method: The triangulation method obtains source-target model by combining source-pivot and pivot-target translation models (Wu and Wang, 2007; Cohn and Lapata 2007), which has been shown to work better tha"
D13-1050,D07-1103,0,\N,Missing
D13-1050,P02-1040,0,\N,Missing
D13-1050,P07-1108,1,\N,Missing
D13-1050,P07-2045,0,\N,Missing
D13-1050,2008.iwslt-evaluation.11,0,\N,Missing
D13-1050,N03-1017,0,\N,Missing
D13-1050,P09-2031,1,\N,Missing
D13-1050,W04-3250,0,\N,Missing
D14-1007,W13-4035,0,0.0301826,"Missing"
D14-1007,N07-2038,0,0.164032,"Missing"
D14-1007,W10-4323,0,0.0266216,"Missing"
D14-1007,W06-1302,0,0.0591506,"Processes (POMDPs) (Thomson and Young, 2010; Young et al., 2010; Williams and Young, 2007), in conjunction with reinforcement learning techniques (Jurˇc´ıcˇ ek et al., 2011; Jurˇc´ıcˇ ek et al., 2012; Gaˇsi´c et al., 2013a) to seek optimal dialogue policies that maximise long-term expected (discounted) rewards and are robust to ASR errors. However, to the best of our knowledge, most of the existing multi-domain SDS in public use are rule-based (e.g. (Gruber et al., 2012; Mirkovic and Cavedon, 2006)). The application of statistical models in multi-domain dialogue systems is still preliminary. Komatani et al. (2006) and Nakano et al. (2011) utilised a distributed architecture (Lin et al., 1999) to integrate expert dialogue systems in different domains into a unified framework, where a central controller trained as a data-driven classifier selects a domain expert at each turn to address user’s query. Alternatively, Hakkani-T¨ur et al. (2012) adopted the well-known Information State mechanism (Traum and Larsson, 2003) to construct a multi-domain SDS and proposed a discriminative classification model for more accurate state updates. More recently, Gaˇsi´c et al. (2013b) proposed that by a simple expansion o"
D14-1007,W11-2004,0,0.0505052,"Missing"
D14-1015,P00-1054,0,0.0612324,"for single languages. Recently, a lot of progress has 142 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–146, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tween bilingual embedding representation is considered as score function. The score function should be discriminative between target phrases and other candidate phrases. Our score function is in the form: been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word s"
D14-1015,D10-1005,0,0.02413,"2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–146, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tween bilingual embedding representation is considered as score function. The score function should be discriminative between target phrases and other candidate phrases. Our score function is in the form: been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this"
D14-1015,P07-1066,0,0.0239875,"ethods in Natural Language Processing (EMNLP), pages 142–146, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tween bilingual embedding representation is considered as score function. The score function should be discriminative between target phrases and other candidate phrases. Our score function is in the form: been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this paper, we learn bil"
D14-1015,D07-1007,0,0.035916,"h as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this paper, we learn bilingual semantic embeddings for source content and target phrase, and incorporate it into a phrasebased SMT system to improve translation quality. 3 f (x, y; W, U) = cos(WT x, UT y) where x is contextual feature vector in source sentence, and y is the representation of target phrase, W ∈ R|X|×k , U ∈ R|Y|×k are low rank matrix. In our model, we allow y to be bag-of-words representation. Our embedding model is memoryefficient in that dimensionality of x and y can be very large in practical setting. We use |X |and |Y| means dimensionality o"
D14-1015,N13-1011,0,0.0470984,"Missing"
D14-1015,P14-1066,0,0.232256,"u1 Daxiang Dong1 Wei He1 Xiaoguang Hu1 Dianhai Yu1 Hua Wu1 Haifeng Wang1 Ting Liu2 1 Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China 2 Harbin Institute of Technology, Harbin, China wuhaiyang,dongdaxiang,hewei,huxiaoguang,yudianhai, wu hua,wanghaifeng@baidu.com tliu@ir.hit.edu.cn Abstract where labels are automatically generated from phrase-pairs. For each source phrase, the aligned target phrase is marked as the positive label whereas other phrases in our phrase table are treated as negative labels. Different from previous work in bilingual embedding learning(Zou et al., 2013; Gao et al., 2014), our framework is a supervised model that utilizes contextual information in source sentence as features and make use of phrase pairs as weak labels. Bilingual semantic embeddings are trained automatically from our supervised learning task. Our learned bilingual semantic embedding model is used to measure the similarity of phrase pairs which is treated as a feature in decoding. We integrate our learned model into a phrase-based translation system and experimental results indicate that our system significantly outperform the baseline system. On the NIST08 Chinese-English translation task, we o"
D14-1015,P12-1092,0,0.0602901,"guation highly depends on the language model which is trained only on target corpus. To solve this problem, we present to learn context-sensitive bilingual semantic embedding. Our methodology is to train a supervised model 2 Related Work Using vectors to represent word meanings is the essence of vector space models (VSM). The representations capture words’ semantic and syntactic information which can be used to measure semantic similarities by computing distance between the vectors. Although most VSMs represent one word with only one vector, they fail to capture homonymy and polysemy of word. Huang et al. (2012) introduced global document context and multiple word prototypes which distinguishes and uses both local and global context via a joint training objective. Much of the research focus on the task of inducing representations for single languages. Recently, a lot of progress has 142 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–146, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tween bilingual embedding representation is considered as score function. The score function should be discriminative between"
D14-1015,P06-2124,0,0.0365331,"uage Processing (EMNLP), pages 142–146, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tween bilingual embedding representation is considered as score function. The score function should be discriminative between target phrases and other candidate phrases. Our score function is in the form: been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this paper, we learn bilingual semantic embedding"
D14-1015,D13-1141,0,0.491805,"ng Model Haiyang Wu1 Daxiang Dong1 Wei He1 Xiaoguang Hu1 Dianhai Yu1 Hua Wu1 Haifeng Wang1 Ting Liu2 1 Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China 2 Harbin Institute of Technology, Harbin, China wuhaiyang,dongdaxiang,hewei,huxiaoguang,yudianhai, wu hua,wanghaifeng@baidu.com tliu@ir.hit.edu.cn Abstract where labels are automatically generated from phrase-pairs. For each source phrase, the aligned target phrase is marked as the positive label whereas other phrases in our phrase table are treated as negative labels. Different from previous work in bilingual embedding learning(Zou et al., 2013; Gao et al., 2014), our framework is a supervised model that utilizes contextual information in source sentence as features and make use of phrase pairs as weak labels. Bilingual semantic embeddings are trained automatically from our supervised learning task. Our learned bilingual semantic embedding model is used to measure the similarity of phrase pairs which is treated as a feature in decoding. We integrate our learned model into a phrase-based translation system and experimental results indicate that our system significantly outperform the baseline system. On the NIST08 Chinese-English tra"
D14-1015,N12-1005,0,0.0245693,"n bilingual embedding representation is considered as score function. The score function should be discriminative between target phrases and other candidate phrases. Our score function is in the form: been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this paper, we learn bilingual semantic embeddings for source content and target phrase, and incorporate it into a phrasebased SMT system to improve translation quality. 3"
D14-1015,J03-1002,0,0.00507761,"mantic model into the phrase-based SMT system. Experimental results show that our method achieves significant improvements over the baseline on large scale Chinese-English translation task. Our model is memory-efficient and practical for industrial usage that training can be done on large scale data set with large number of classes. Prediction time is also negligible with regard to SMT decoding phase. In the future, we will explore more features to refine the model and try to utilize contextual information in target sentences. For word alignment, we align all of the training data with GIZA++ (Och and Ney, 2003), using the grow-diag-final heuristic to improve recall. For language model, we train a 5-gram modified Kneser-Ney language model and use Minimum Error Rate Training (Och, 2003) to tune the SMT. For both OpenMT08 task and WebData task, we use NIST06 as the tuning set, and use NIST08 as the testing set. Our baseline system is a standard phrase-based SMT system, and a language model is trained with the target side of bilingual corpus. Results on Chinese-English translation task are reported in Table 1. Word position features and partof-speech tagging features are both useful for our bilingual se"
D14-1015,P03-1021,0,0.0157548,"ask. Our model is memory-efficient and practical for industrial usage that training can be done on large scale data set with large number of classes. Prediction time is also negligible with regard to SMT decoding phase. In the future, we will explore more features to refine the model and try to utilize contextual information in target sentences. For word alignment, we align all of the training data with GIZA++ (Och and Ney, 2003), using the grow-diag-final heuristic to improve recall. For language model, we train a 5-gram modified Kneser-Ney language model and use Minimum Error Rate Training (Och, 2003) to tune the SMT. For both OpenMT08 task and WebData task, we use NIST06 as the tuning set, and use NIST08 as the testing set. Our baseline system is a standard phrase-based SMT system, and a language model is trained with the target side of bilingual corpus. Results on Chinese-English translation task are reported in Table 1. Word position features and partof-speech tagging features are both useful for our bilingual semantic embedding learning. Based on our trained bilingual embedding model, we can easily compute a translation score between any bilingual phrase pair. We list some cases in tab"
D14-1015,N10-1135,0,0.0461665,"Missing"
D14-1016,P03-1021,0,0.010994,"ested our system on WMT test sets from 2010 to 2013. The baseline systems are trained on the training corpus with initial word alignment, which was obtained via GIZA++ and “grow-diag-final” method. Based on the initial word alignment, we computed word translation probabilities and used the proposed method to obtain a refined word alignment. Then we used the refined word alignment to train our SMT systems. The translation results are evaluated by caseinsensitive BLEU-4 (Papineni et al., 2002). The feature weights of the translation system are tuned with the standard minimum-error-ratetraining (Och, 2003) to maximize the systems BLEU score on the development set. Experiment To demonstrate the effect of the proposed method, we use the state-of-the-art phrase-based system and hierarchical phrase-based system implemented in Moses (Koehn et al., 2007). The phrasebased system uses continuous phrase pair as the main translation knowledge. While the hierarchical phrase-based system uses both continuous and discontinuous phrase pairs, which has an ability to capture long distance phrase reordering. we carried out experiments on two translation tasks: the Chinese-to-English task comes from the NIST Ope"
D14-1016,J93-2003,0,0.0521236,"Continuous Word Alignment Improves Translation Quality Zhongjun He1 Hua Wu1 Haifeng Wang1 Ting Liu2 1 Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China 2 Harbin Institute of Technology, Harbin, China {hezhongjun,wu hua,wanghaifeng}@baidu.com tliu@ir.hit.edu.cn Abstract of the word alignment has big impact on the quality of translation output. Word alignments are usually automatically obtained from a large amount of bilingual training corpus. The most widely used toolkit for word alignment in SMT community is GIZA++ (Och and Ney, 2004), which implements the well known IBM models (Brown et al., 1993) and the HMM model (Vogel and Ney, 1996). Koehn et al. (2003) proposed some heuristic methods (e.g. the “grow-diag-final” method) to refine word alignments trained by GIZA++. Another group of word alignment methods (Liu et al., 2005; Moore et al., 2006; Riesa and Marcu, 2010) define feature functions to describe word alignment. They need manually aligned bilingual texts to train the model. However, the manually annotated data is too expensive to be available for all languages. Although these models reported high accuracy, the GIZA++ and “grow-diag-final” method are dominant in practice. Howeve"
D14-1016,P02-1040,0,0.0889285,"h translation quality of the phrase-based system. the shared translation task 2013. We used WMT08 as the development set and tested our system on WMT test sets from 2010 to 2013. The baseline systems are trained on the training corpus with initial word alignment, which was obtained via GIZA++ and “grow-diag-final” method. Based on the initial word alignment, we computed word translation probabilities and used the proposed method to obtain a refined word alignment. Then we used the refined word alignment to train our SMT systems. The translation results are evaluated by caseinsensitive BLEU-4 (Papineni et al., 2002). The feature weights of the translation system are tuned with the standard minimum-error-ratetraining (Och, 2003) to maximize the systems BLEU score on the development set. Experiment To demonstrate the effect of the proposed method, we use the state-of-the-art phrase-based system and hierarchical phrase-based system implemented in Moses (Koehn et al., 2007). The phrasebased system uses continuous phrase pair as the main translation knowledge. While the hierarchical phrase-based system uses both continuous and discontinuous phrase pairs, which has an ability to capture long distance phrase re"
D14-1016,P05-1033,0,0.501326,"glish translation tasks. Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems. Our method produces a gain of +1.68 BLEU on NIST OpenMT04 for the phrase-based system, and a gain of +1.28 BLEU on NIST OpenMT06 for the hierarchical phrase-based system. 1 Introduction Word alignment, indicating the correspondence between the source and target words in bilingual sentences, plays an important role in statistical machine translation (SMT). Almost all of the SMT models, not only phrase-based (Koehn et al., 2003), but also syntax-based (Chiang, 2005; Liu et al., 2006; Huang et al., 2006), derive translation knowledge from large amount bilingual text annotated with word alignment. Therefore, the quality 147 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 147–152, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 1 {I meiguo The 1 2 ´ shi United 2 3 ê shaoshu States 3 was 4 4 A‡ jige among 5 5 Ý tou the 6 6 e xia 7 ‡é fandui handful 7 of 8 8 ¦ piao nations 9 9 10 I[ guojia de that 10 cast 11 a 12 11 ƒ˜ zhiyi nay 13 note 14 Figure 1: An example of word al"
D14-1016,P10-1017,0,0.0192555,"n Abstract of the word alignment has big impact on the quality of translation output. Word alignments are usually automatically obtained from a large amount of bilingual training corpus. The most widely used toolkit for word alignment in SMT community is GIZA++ (Och and Ney, 2004), which implements the well known IBM models (Brown et al., 1993) and the HMM model (Vogel and Ney, 1996). Koehn et al. (2003) proposed some heuristic methods (e.g. the “grow-diag-final” method) to refine word alignments trained by GIZA++. Another group of word alignment methods (Liu et al., 2005; Moore et al., 2006; Riesa and Marcu, 2010) define feature functions to describe word alignment. They need manually aligned bilingual texts to train the model. However, the manually annotated data is too expensive to be available for all languages. Although these models reported high accuracy, the GIZA++ and “grow-diag-final” method are dominant in practice. However, automatic word alignments are usually very noisy. The example in Figure 1 shows a Chinese and English sentence pair, with word alignment automatically trained by GIZA++ and the “grow-diag-final” method. We find many errors (dashed links) are caused by discontinuous alignme"
D14-1016,C96-2141,0,0.376364,"slation Quality Zhongjun He1 Hua Wu1 Haifeng Wang1 Ting Liu2 1 Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China 2 Harbin Institute of Technology, Harbin, China {hezhongjun,wu hua,wanghaifeng}@baidu.com tliu@ir.hit.edu.cn Abstract of the word alignment has big impact on the quality of translation output. Word alignments are usually automatically obtained from a large amount of bilingual training corpus. The most widely used toolkit for word alignment in SMT community is GIZA++ (Och and Ney, 2004), which implements the well known IBM models (Brown et al., 1993) and the HMM model (Vogel and Ney, 1996). Koehn et al. (2003) proposed some heuristic methods (e.g. the “grow-diag-final” method) to refine word alignments trained by GIZA++. Another group of word alignment methods (Liu et al., 2005; Moore et al., 2006; Riesa and Marcu, 2010) define feature functions to describe word alignment. They need manually aligned bilingual texts to train the model. However, the manually annotated data is too expensive to be available for all languages. Although these models reported high accuracy, the GIZA++ and “grow-diag-final” method are dominant in practice. However, automatic word alignments are usually"
D14-1016,2006.amta-papers.8,0,0.030158,"ental results show statistically significant improvements of BLEU score in both cases over the baseline systems. Our method produces a gain of +1.68 BLEU on NIST OpenMT04 for the phrase-based system, and a gain of +1.28 BLEU on NIST OpenMT06 for the hierarchical phrase-based system. 1 Introduction Word alignment, indicating the correspondence between the source and target words in bilingual sentences, plays an important role in statistical machine translation (SMT). Almost all of the SMT models, not only phrase-based (Koehn et al., 2003), but also syntax-based (Chiang, 2005; Liu et al., 2006; Huang et al., 2006), derive translation knowledge from large amount bilingual text annotated with word alignment. Therefore, the quality 147 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 147–152, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 1 {I meiguo The 1 2 ´ shi United 2 3 ê shaoshu States 3 was 4 4 A‡ jige among 5 5 Ý tou the 6 6 e xia 7 ‡é fandui handful 7 of 8 8 ¦ piao nations 9 9 10 I[ guojia de that 10 cast 11 a 12 11 ƒ˜ zhiyi nay 13 note 14 Figure 1: An example of word alignment between a Chinese and English s"
D14-1016,D07-1103,0,0.0248816,"tandard definition of phrase in SMT, phrase pairs cannot be extracted from the discontinuous alignments. By transforming discontinuous alignments into continuous alignment, we can extract more phrase pairs. Table 7 shows the number of standard phrases and hierarchical phrases extracted from the initial and refined word alignments. We find that the number of both phrases and hierarchical phrases grows heavily. This is because that the word alignment constraint for phrase extraction is loosed by removing noisy links. Although the phrase table becomes larger, fortunately, there are some methods (Johnson et al., 2007; He et al., 2009) to prune phrase table without hurting translation quality. For further illustration, we compare the phrase pairs extracted from the initial alignment and refined alignment in Figure 1. From the initial alignments, we extracted only 3 standard phrase pairs and no hierarchical phrase pairs (Table 8). After discarding noisy alignments (dashed links) by using the proposed method, we extracted 21 standard phrase pairs and 36 hierarchical phrases. Table 9 and Table 10 show selected phrase pairs and hierarchical phrase pairs, respectively. Acknowlegement This paper is supported by"
D14-1016,N03-1017,0,0.0381442,"1 Haifeng Wang1 Ting Liu2 1 Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China 2 Harbin Institute of Technology, Harbin, China {hezhongjun,wu hua,wanghaifeng}@baidu.com tliu@ir.hit.edu.cn Abstract of the word alignment has big impact on the quality of translation output. Word alignments are usually automatically obtained from a large amount of bilingual training corpus. The most widely used toolkit for word alignment in SMT community is GIZA++ (Och and Ney, 2004), which implements the well known IBM models (Brown et al., 1993) and the HMM model (Vogel and Ney, 1996). Koehn et al. (2003) proposed some heuristic methods (e.g. the “grow-diag-final” method) to refine word alignments trained by GIZA++. Another group of word alignment methods (Liu et al., 2005; Moore et al., 2006; Riesa and Marcu, 2010) define feature functions to describe word alignment. They need manually aligned bilingual texts to train the model. However, the manually annotated data is too expensive to be available for all languages. Although these models reported high accuracy, the GIZA++ and “grow-diag-final” method are dominant in practice. However, automatic word alignments are usually very noisy. The exam"
D14-1016,P07-2045,0,0.0254523,"any external knowledge, as the word translation probabilities can be estimated from the bilingual corpus with the original word alignment. We notice that the discontinuous alignment is helpful for hierarchical phrase-based model, as the model allows discontinuous phrases. Thus, for the hierarchical phrase-based model, our method may lost some discontinuous phrases. To solve the problem, we keep the original discontinuous alignment in the training corpus. We carry out experiment with the state-of-theart phrase-based and hierarchical phrase-based (Chiang, 2005) SMT systems implemented in Moses (Koehn et al., 2007). Experiments on large scale Chinese-to-English and German-to-English translation tasks demonstrate significant improvements in both cases over the baseline systems. 2 into several continuous groups, and select the best group with the highest score computed by word translation probabilities as the final alignment. For further understanding, we first describe some definitions. Given a word-aligned sentence pair (F1I , E1J , A), an alignment set Aset (i) is the set of target word positions that aligned to the source word Fii : Aset (i) = {j|(i, j) ∈ A} For example, in Figure 1, the alignment set"
D14-1016,P05-1057,0,0.0305017,"nghaifeng}@baidu.com tliu@ir.hit.edu.cn Abstract of the word alignment has big impact on the quality of translation output. Word alignments are usually automatically obtained from a large amount of bilingual training corpus. The most widely used toolkit for word alignment in SMT community is GIZA++ (Och and Ney, 2004), which implements the well known IBM models (Brown et al., 1993) and the HMM model (Vogel and Ney, 1996). Koehn et al. (2003) proposed some heuristic methods (e.g. the “grow-diag-final” method) to refine word alignments trained by GIZA++. Another group of word alignment methods (Liu et al., 2005; Moore et al., 2006; Riesa and Marcu, 2010) define feature functions to describe word alignment. They need manually aligned bilingual texts to train the model. However, the manually annotated data is too expensive to be available for all languages. Although these models reported high accuracy, the GIZA++ and “grow-diag-final” method are dominant in practice. However, automatic word alignments are usually very noisy. The example in Figure 1 shows a Chinese and English sentence pair, with word alignment automatically trained by GIZA++ and the “grow-diag-final” method. We find many errors (dashe"
D14-1016,P06-1077,0,0.0327986,"ion tasks. Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems. Our method produces a gain of +1.68 BLEU on NIST OpenMT04 for the phrase-based system, and a gain of +1.28 BLEU on NIST OpenMT06 for the hierarchical phrase-based system. 1 Introduction Word alignment, indicating the correspondence between the source and target words in bilingual sentences, plays an important role in statistical machine translation (SMT). Almost all of the SMT models, not only phrase-based (Koehn et al., 2003), but also syntax-based (Chiang, 2005; Liu et al., 2006; Huang et al., 2006), derive translation knowledge from large amount bilingual text annotated with word alignment. Therefore, the quality 147 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 147–152, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 1 {I meiguo The 1 2 ´ shi United 2 3 ê shaoshu States 3 was 4 4 A‡ jige among 5 5 Ý tou the 6 6 e xia 7 ‡é fandui handful 7 of 8 8 ¦ piao nations 9 9 10 I[ guojia de that 10 cast 11 a 12 11 ƒ˜ zhiyi nay 13 note 14 Figure 1: An example of word alignment between a"
D14-1016,P06-1065,0,0.0610096,"Missing"
D14-1016,J04-4002,0,0.277293,"Missing"
D14-1174,2008.iwslt-papers.1,0,0.0679481,"mental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems. 1 Introduction Statistical Machine Translation (SMT) relies on large bilingual parallel data to produce high quality translation results. Unfortunately, for some language pairs, large bilingual corpora are not readily available. To alleviate the parallel data scarceness, a conventional solution is to introduce a “bridge” language (named pivot language) to connect the source and target language (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Paul et al., 2011; El Kholy et al., 2013; Zahabi et al., 2013), where there are large amounts of source-pivot and pivot-target parallel corpora. Among various pivot-based approaches, the triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007) is a representative work in * pivot-based machine translation. The approach proposes to build a source-target phrase table by merging the source-pivot and pivot-target phrase table. One of the key issues in this method is to estimate the translation probabilities for the generated source-target phrase pairs. Conventionally, the probabilities are"
D14-1174,P07-1092,0,0.593184,"igh quality translation results. Unfortunately, for some language pairs, large bilingual corpora are not readily available. To alleviate the parallel data scarceness, a conventional solution is to introduce a “bridge” language (named pivot language) to connect the source and target language (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Paul et al., 2011; El Kholy et al., 2013; Zahabi et al., 2013), where there are large amounts of source-pivot and pivot-target parallel corpora. Among various pivot-based approaches, the triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007) is a representative work in * pivot-based machine translation. The approach proposes to build a source-target phrase table by merging the source-pivot and pivot-target phrase table. One of the key issues in this method is to estimate the translation probabilities for the generated source-target phrase pairs. Conventionally, the probabilities are estimated by multiplying the posterior probabilities of source-pivot and pivottarget phrase pairs. However, it has been shown that the generated probabilities are not accurate enough (Cui et al., 2013). One possible reason may lie"
D14-1174,I11-1154,0,0.515643,"t translation model. See Figure 2. (b) and (c) for further illustration. 1666 The remainder of this paper is organized as follows. In Section 2, we describe the related work. We introduce the co-occurrence count method in Section 3, and the mixed model in Section 4. In Section 5 and Section 6, we describe and analyze the experiments. Section 7 gives a conclusion of the paper. 2 Related Work Several methods have been proposed for pivotbased translation. Typically, they can be classified into 3 kinds as follows: Transfer Method: The transfer method (Utiyama and Isahara, 2007; Wang et al., 2008; Costa-jussà et al., 2011) connects two translation systems: a source-pivot MT system and a pivottarget MT system. Given a source sentence, (1) the source-pivot MT system translates it into the pivot language, (2) and the pivot-target MT system translates the pivot sentence into the target sentence. During each step (source to pivot and pivot to target), multiple translation outputs will be generated, thus a minimum Bayes-risk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). The problem with the transfer method is that it needs to decode twice. On o"
D14-1174,I11-1153,0,0.0607625,"Missing"
D14-1174,P11-1127,0,0.0142768,"s as follows: Transfer Method: The transfer method (Utiyama and Isahara, 2007; Wang et al., 2008; Costa-jussà et al., 2011) connects two translation systems: a source-pivot MT system and a pivottarget MT system. Given a source sentence, (1) the source-pivot MT system translates it into the pivot language, (2) and the pivot-target MT system translates the pivot sentence into the target sentence. During each step (source to pivot and pivot to target), multiple translation outputs will be generated, thus a minimum Bayes-risk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). The problem with the transfer method is that it needs to decode twice. On one hand, the time cost is doubled; on the other hand, the translation error of the source-pivot translation system will be transferred to the pivot-target translation. Synthetic Method: It aims to create a synthetic source-target corpus by: (1) translate the pivot part in source-pivot corpus into target language with a pivot-target model; (2) translate the pivot part in pivot-target corpus into source language with a pivot-source model; (3) combine the source sentences with translated target sentenc"
D14-1174,N03-1017,0,0.372818,"the co-occurrence count of source-target phrase pairs to estimate phrase translation probabilities more precisely. Different from the triangulation method, which merges the source-pivot and pivot-target phrase pairs after training the translation model, we propose to merge the source-pivot and pivot-target phrase pairs immediately after the phrase extraction step, and estimate the co-occurrence count of the source-pivot-target phrase pairs. Finally, we compute the translation probabilities according to the estimated co-occurrence counts, using the standard training method in phrase-based SMT (Koehn et al., 2003). As Figure 1. (b) shows, the This work was done when the first author was visiting Baidu. 1665 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1665–1675, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics (a) the triangulation method (b) the co-occurrence count method Figure 1: An example of probability space evolution in pivot translation. Standard ST corpus Large SP corpus Large PT corpus Standard ST corpus Large SP corpus Phrase Extraction Phrase Extraction Phrase Extraction SP phrase pairs PT phrase pair"
D14-1174,W04-3250,0,0.269372,"Missing"
D14-1174,2005.mtsummit-papers.11,0,0.294746,"Missing"
D14-1174,2010.iwslt-papers.12,0,0.0373385,"Missing"
D14-1174,C00-2163,0,0.0997045,"parison of different merging methods on out-of-domain test set. 5 Experiments on Europarl Corpus Our first experiment is carried out on Europarl1 corpus, which is a multi-lingual corpus including 21 European languages (Koehn, 2005). In our work, we perform translations among French (fr), German (de) and Spanish (es). Due to the richness of available language resources, we choose English (en) as the pivot language. Table 1 summarized the statistics of training data. For the language model, the same monolingual data extracted from the Europarl are used. The word alignment is obtained by GIZA++ (Och and Ney, 2000) and the heuristics “growdiag-final” refinement rule (Koehn et al., 2003). Our translation system is an in-house phrasebased system analogous to Moses (Koehn et al., 2007). The baseline system is the triangulation method (Wu and Wang, 2007), including an interpolated model which linearly interpolate the direct and pivot translation model. 1 http://www.statmt.org/europarl We use WMT082 as our test data, which contains 2000 in-domain sentences and 2051 out-ofdomain sentences with single reference. The translation results are evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). The"
D14-1174,W11-2601,0,0.017343,"arl data and web data show that our method leads to significant improvements over the baseline systems. 1 Introduction Statistical Machine Translation (SMT) relies on large bilingual parallel data to produce high quality translation results. Unfortunately, for some language pairs, large bilingual corpora are not readily available. To alleviate the parallel data scarceness, a conventional solution is to introduce a “bridge” language (named pivot language) to connect the source and target language (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Paul et al., 2011; El Kholy et al., 2013; Zahabi et al., 2013), where there are large amounts of source-pivot and pivot-target parallel corpora. Among various pivot-based approaches, the triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007) is a representative work in * pivot-based machine translation. The approach proposes to build a source-target phrase table by merging the source-pivot and pivot-target phrase table. One of the key issues in this method is to estimate the translation probabilities for the generated source-target phrase pairs. Conventionally, the probabilities are estimated by multi"
D14-1174,I11-1091,0,0.0343951,"Missing"
D14-1174,P02-1040,0,0.0885743,"Missing"
D14-1174,E12-1015,0,0.0948513,"Missing"
D14-1174,P07-2050,0,0.0731859,"Missing"
D14-1174,N07-1061,0,0.872022,"ce-pivot and pivot-target phrase pairs. Experimental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems. 1 Introduction Statistical Machine Translation (SMT) relies on large bilingual parallel data to produce high quality translation results. Unfortunately, for some language pairs, large bilingual corpora are not readily available. To alleviate the parallel data scarceness, a conventional solution is to introduce a “bridge” language (named pivot language) to connect the source and target language (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Paul et al., 2011; El Kholy et al., 2013; Zahabi et al., 2013), where there are large amounts of source-pivot and pivot-target parallel corpora. Among various pivot-based approaches, the triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007) is a representative work in * pivot-based machine translation. The approach proposes to build a source-target phrase table by merging the source-pivot and pivot-target phrase table. One of the key issues in this method is to estimate the translation probabilities for the generated source-target phrase pa"
D14-1174,2008.iwslt-evaluation.11,0,0.0527301,"Missing"
D14-1174,P07-1108,1,0.954596,"hrase pairs. Experimental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems. 1 Introduction Statistical Machine Translation (SMT) relies on large bilingual parallel data to produce high quality translation results. Unfortunately, for some language pairs, large bilingual corpora are not readily available. To alleviate the parallel data scarceness, a conventional solution is to introduce a “bridge” language (named pivot language) to connect the source and target language (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Paul et al., 2011; El Kholy et al., 2013; Zahabi et al., 2013), where there are large amounts of source-pivot and pivot-target parallel corpora. Among various pivot-based approaches, the triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007) is a representative work in * pivot-based machine translation. The approach proposes to build a source-target phrase table by merging the source-pivot and pivot-target phrase table. One of the key issues in this method is to estimate the translation probabilities for the generated source-target phrase pairs. Conventionally"
D14-1174,P09-1018,1,0.899443,"one hand, the time cost is doubled; on the other hand, the translation error of the source-pivot translation system will be transferred to the pivot-target translation. Synthetic Method: It aims to create a synthetic source-target corpus by: (1) translate the pivot part in source-pivot corpus into target language with a pivot-target model; (2) translate the pivot part in pivot-target corpus into source language with a pivot-source model; (3) combine the source sentences with translated target sentences or/and combine the target sentences with translated source sentences (Utiyama et al., 2008; Wu and Wang, 2009). However, it is difficult to build a high quality translation system with a corpus created by a machine translation system. Triangulation Method: The triangulation method obtains source-target phrase table by merging source-pivot and pivot-target phrase table entries with identical pivot language phrases and multiplying corresponding posterior probabilities (Wu and Wang, 2007; Cohn and Lapata, 2007), which has been shown to work better than the other pivot approaches (Utiyama and Isahara, 2007). A problem of this approach is that the probability space of the source-target phrase pairs is non-"
D14-1174,2008.iwslt-evaluation.18,1,\N,Missing
D14-1174,P07-2045,0,\N,Missing
D14-1174,C08-2032,0,\N,Missing
D14-1174,P13-2057,0,\N,Missing
D14-1174,I13-1167,0,\N,Missing
D14-1174,P13-2073,0,\N,Missing
D14-1174,2009.mtsummit-papers.7,0,\N,Missing
D16-1036,J86-3001,0,0.494876,"chnology of China o n zhouxiangyang, dongdaxiang, wu hua, zhaoshiqi, @baidu.com yudianhai, tianhao, liuxuan, yanrui Abstract these work, context and response are taken as two separate word sequences without considering the relationship among utterances in the context and response. The response selection in these models is largely influenced by word-level information. We called this kind of models as word sequence model in this paper. Besides word-level dependencies, utterance-level semantic and discourse information are also very important to catch the conversation topics to ensure coherence (Grosz and Sidner, 1986). For example an utterance can be an affirmation, negation or deduction to the previous utterances, or starts a new topic for discussion. This kind of utterance-level information is generally ignored in word sequence model, which may be helpful for selecting the next response. Therefore, it is necessary to take each utterance as a unit and model the context and response from the view of utterance sequence. In this paper, we study the task of response selection for multi-turn human-computer conversation. Previous approaches take word as a unit and view context and response as sequences of words"
D16-1036,D14-1181,0,0.0104234,"er hand, research on multi-turn response selection usually takes the whole context into consideration and views the context and response as word sequences. Lowe et al., (2015) proposed a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) based response selection model for multi-turn conversation, where words from context and response are modeled with LSTM. The selection of a response is based on the similarity of embeddings between the context and response. Similar to the work of Lowe et al., Kadlec et al., (2015) replaced LSTM with Temporal Convolutional Neural Networks (TCNN) (Kim, 2014) and BidirectLSTM. Their experimental results show that models with LSTM perform better than other neural networks. However, the utterance-level discourse information and dependencies have been left out in these studies since they view the context and response as word sequences. 2.3 Response Generation Another line of related research focuses on generating responses for human-computer conversation. Ritter et al., (2011) trained a phrase-based statistical machine translation model on a corpus of utterance pairs extracted from Twitter human-human conversation and used it as a response generator"
D16-1036,W15-4640,0,0.618745,"oduction Selecting a potential response from a set of candidates is an important and challenging task for open-domain human-computer conversation, especially for the retrieval-based human-computer conversation. In general, a set of candidate responses from the indexed conversation corpus are retrieved, and then the best one is selected from the candidates as the system’s response (Ji et al., 2014). Previous Deep Neural Network (DNN) based approaches to response selection represent context and response as two embeddings. The response is selected based on the similarity of these two embeddings (Lowe et al., 2015; Kadlec et al., 2015). In ∗ These two authors contributed equally This paper proposes a multi-view response selection model, which integrates information from both word sequence view and utterance sequence view. Our assumption is that each view can represent relationships between context and response from a particular aspect, and features extracted from the word sequence and the utterance sequence provide complementary information for response selection. An effective integration of these two views is expected to improve the model performance. To the best of our knowledge, this is the first wo"
D16-1036,D14-1162,0,0.116556,"Missing"
D16-1036,D11-1054,0,0.46192,"the similarity of embeddings between the context and response. Similar to the work of Lowe et al., Kadlec et al., (2015) replaced LSTM with Temporal Convolutional Neural Networks (TCNN) (Kim, 2014) and BidirectLSTM. Their experimental results show that models with LSTM perform better than other neural networks. However, the utterance-level discourse information and dependencies have been left out in these studies since they view the context and response as word sequences. 2.3 Response Generation Another line of related research focuses on generating responses for human-computer conversation. Ritter et al., (2011) trained a phrase-based statistical machine translation model on a corpus of utterance pairs extracted from Twitter human-human conversation and used it as a response generator for single-turn conversation. Vinyals and Le (2015) regarded single-turn conversation as a sequence-tosequence problem and proposed an encoder-decoder based response generation model, where the post response is first encoded using LSTM and its embedding used as the initialization state of another LSTM to generate the response. Shang et al., (2015) improved the encoder-decoder based model using attention signals. Sordoni"
D16-1036,2005.sigdial-1.6,0,0.00892499,"ng positive response, 9 negative responses are randomly selected for further evaluation. 4.2 Experiment Setup Following the work of Lowe et al., (2015), the evaluation metric is 1 in m Recall@k (denoted 1 in m 2 Preprocessing includes tokenization, recognition of named entity, urls and numbers. 377 R@k), where a response selection model is designed to select k most likely responses among m candidates, and it gets the score “1” if the correct response is in the k selected ones. This metric can be seen as an adaptation of the precision and recall metrics previously applied to dialogue datasets (Schatzmann et al., 2005). It is worth noticing that 1 in 2 R@1 equals to precision and recall in binary classification. 4.3 Model Training and Hyper-parameters We initialize word embeddings with a pre-trained embedding matrix through GloVe (Pennington et al., 2014) 3 . We use Stochastic Gradient Descent (SGD) for optimizing. Hidden size for a gated recurrent unit is set to 200 in both word sequence model and utterance sequence model. The number of convolutional kernels is set to 200. Our initial learning rate is 0.01 with mini-batch size of 32. Other hyperparameters are set exactly the same as the baseline. We train"
D16-1036,P15-1152,0,0.102237,"esearch focuses on generating responses for human-computer conversation. Ritter et al., (2011) trained a phrase-based statistical machine translation model on a corpus of utterance pairs extracted from Twitter human-human conversation and used it as a response generator for single-turn conversation. Vinyals and Le (2015) regarded single-turn conversation as a sequence-tosequence problem and proposed an encoder-decoder based response generation model, where the post response is first encoded using LSTM and its embedding used as the initialization state of another LSTM to generate the response. Shang et al., (2015) improved the encoder-decoder based model using attention signals. Sordoni et al., (2015) proposed a context-sensitive response generation model, where the context is represented by bag-of-words and fed into a recurrent language model to generate the next response. In this paper, we focused on the task of response selection. ; !&quot; &lt; = 1 &, ) = $ (+&quot; + ℎ/ ⨀ (&quot; &&quot; ℎ341 ℎ3 ) ⨀ )&quot; ℎ/ ... 01 02 0341 03 ... A1 A2 91 92 & 015 01 02 06 07 08 A15 A1 96 A8 ) Figure 1: Word sequence model for response selection 3 Response Selection Model 3.1 In the task of response selection, a conventional DNN-based arch"
D16-1036,N15-1020,0,0.0117317,"(2011) trained a phrase-based statistical machine translation model on a corpus of utterance pairs extracted from Twitter human-human conversation and used it as a response generator for single-turn conversation. Vinyals and Le (2015) regarded single-turn conversation as a sequence-tosequence problem and proposed an encoder-decoder based response generation model, where the post response is first encoded using LSTM and its embedding used as the initialization state of another LSTM to generate the response. Shang et al., (2015) improved the encoder-decoder based model using attention signals. Sordoni et al., (2015) proposed a context-sensitive response generation model, where the context is represented by bag-of-words and fed into a recurrent language model to generate the next response. In this paper, we focused on the task of response selection. ; !&quot; &lt; = 1 &, ) = $ (+&quot; + ℎ/ ⨀ (&quot; &&quot; ℎ341 ℎ3 ) ⨀ )&quot; ℎ/ ... 01 02 0341 03 ... A1 A2 91 92 & 015 01 02 06 07 08 A15 A1 96 A8 ) Figure 1: Word sequence model for response selection 3 Response Selection Model 3.1 In the task of response selection, a conventional DNN-based architecture represents the context and response as low dimensional embeddings with deep lear"
D16-1036,P01-1066,0,0.102718,"hen we move on to a detailed description of our model in Section 3. Experimental results are described in Section 4. Analysis of our models is shown in Section 5. We conclude the paper in Section 6. 2 2.1 Related Work Conversation System Establishing a machine that can interact with human beings via natural language is one of the most challenging problems in Artificial Intelligent (AI). Early studies of conversation models are generally designed for specific domain, like booking restaurant, and require numerous domain knowledge as well as human efforts in model design and feature engineering (Walker et al., 2001). Hence it is too costly to adapt those models to other domains. Recently leveraging “big dialogs” for open domain conversation draws increasing research attentions. One critical issue for open domain conversation is to produce a reasonable response. Responding to this challenge, two promising solutions have been proposed: 1) retrieval-based model which selects a response from a large corpus (Ji et al., 2014; Yan et al., 2016; Yan et al., ). 2) generation-based model which directly generates the next utterance (Wen et al., 2015a; Wen et al., 2015b). 2.2 Response Selection Research on response"
D16-1036,W15-4639,0,0.0215736,"Missing"
D16-1036,D15-1199,0,0.00773748,"Missing"
D18-1036,D14-1179,0,0.0198241,"Missing"
D18-1036,D17-1148,0,0.0190034,"ublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in Zhang and Zong (2015). Later, the researchers transfer to NMT framework, e.g. (Wang et al., 2017b; Zhang and Zong, 2016; Zhou et al., 2017; Tu et al., 2016; Mi et al., 2016; He et al., 2016; Dahlmann et al., 2017; Wang et al., 2017c,d; Gu et al., 2018; Zhao et al., 2018). The most relevant studies are Arthur et al. (2016) and Results on EN-DE Translation We also test our method on EN-DE translation and the results are reported in Table 6. We can see that our method is still effective on EN-DE translation. Specifically, when the translation unit is word, the proposed method improves the baseline by 1.13 BLEU points. The improvement is 0.76 BLEU points when the translation unit is sub-word. 398 Feng et al. (2017). They incorporate the lexicon pairs into NMT to improve the translation quality. There are"
D18-1036,N13-1073,0,0.0374239,"(hm 1 , h2 , ..., hT x ) by using m stacked Long Short Term Memory (LSTM) layers (Hochreiter and Schmidhuber, 1997) . hm j is the hidden state of the top layer in encoder. The bottom layer of encoder is a bi-direction LSTM layer to collect the context from the left side and right side. The decoder generates one target word at a time by computing pN i (yi |y<i , C) as follows: (1) where zei is the attention output: zei = tanh(Wz [zim ; ci ]) aij hm i j=1 Neural Machine Translation pN i (yi |y<i , C) = sof tmax(Wyi zei + bs ) Tx X 2 The word alignments A is extracted using the fast-align tool (Dyer et al., 2013) on the bilingual training data with both source-to-target and target-to-source directions. (2) 392 Arthur: alc percent in fo Baseline+ME 30 percent i 30 percent in fourth quarter of last year source sentence y1 y2 y3 input NMT model output probability of each N gold target word P i (y)i 0.80 0.18 0.02 ... 0.35 0.34 0.31 ... 0.75 0.20 0.05 ... alignment y1 y2 y3 x1 x3 Source: ae chengzhan Reference: last year gr Baseline: he in fourth q Arthur: al percent in Baseline+M 30 percent alignments NMT model If PNi (y)i word satisfied the exception criterion and x j aligns to y i x1 x j is an exceptio"
D18-1036,D17-1146,0,0.0730537,"lesome words. The extensive experiments on Chineseto-English and English-to-German translation tasks demonstrate that our method significantly outperforms the strong baseline models in translation quality, especially in handling troublesome words. 1 Figure 1: The NMT model produces a wrong translation for the low-frequency word “aerkat”. While introducing an external lexicon table without contextual information, the model incorrectly translates the ambiguous word “chengzhang” into “growth”. that the low-frequency words can be represented by frequent subword sequences. Arthur et al. (2016) and Feng et al. (2017) try to incorporate a translation lexicon into NMT in order to obtain the correct translation of low-frequency words. However, the former method still faces the lowfrequency problem of subwords. And the latter one has a drawback that they use lexicons without considering specific contexts. Fig. 1 shows an example, in which “aerkate” is an infrequent word and the baseline NMT incorrectly translates it into a pronoun “he”. Incorporation of bilingual lexicon rectifies the mistake but wrongly converts “chengzhang” into an incorrect target word “growth” since an entry “(chengzhang, growth)” in the"
D18-1036,D16-1162,0,0.547059,"ectly translate the troublesome words. The extensive experiments on Chineseto-English and English-to-German translation tasks demonstrate that our method significantly outperforms the strong baseline models in translation quality, especially in handling troublesome words. 1 Figure 1: The NMT model produces a wrong translation for the low-frequency word “aerkat”. While introducing an external lexicon table without contextual information, the model incorrectly translates the ambiguous word “chengzhang” into “growth”. that the low-frequency words can be represented by frequent subword sequences. Arthur et al. (2016) and Feng et al. (2017) try to incorporate a translation lexicon into NMT in order to obtain the correct translation of low-frequency words. However, the former method still faces the lowfrequency problem of subwords. And the latter one has a drawback that they use lexicons without considering specific contexts. Fig. 1 shows an example, in which “aerkate” is an infrequent word and the baseline NMT incorrectly translates it into a pronoun “he”. Incorporation of bilingual lexicon rectifies the mistake but wrongly converts “chengzhang” into an incorrect target word “growth” since an entry “(cheng"
D18-1036,P15-1001,0,0.062259,"st all methods based on two granularities: words and sub-words. For word granularity, we limit the vocabulary to 30K (CH-EN) and 50K (EN-DE) for both the source and target languages. For subword granularity, we use the BPE method (Sennrich et al., 2016) to merge 30K (CH-EN) and 32K (EN-DE) steps. The beam size is set to 12. We use case-insensitive 4-gram BLEU (Papineni et al., 2002) for translation quality evaluation. We compare our method with other relevant methods as follows: (12) j 1) Baseline: It is the baseline NMT system with global attention (Luong et al., 2015; Zoph and Knight, 2016; Jean et al., 2015). where βγ is a learnable parameter. From Eq. (12), the dynamic weight λi is determined by both of the attention weight ai,j , and the exception rate r(xj ). Training the parameters. As discussed above, our method contains some parameters (vd , Wh , Wc and βγ ) to be learned. We denote the parameters introduced by our method by θM and the parameters in NMT by θN . To make it efficient, given the aligned training data D =  (d) (d) |D| X ,Y , we keep θN unchanged and opd=1 M timize θ by maximizing the following objective function. 2) Arthur: It is the state-of-the-art method which incorporates"
D18-1036,D07-1007,0,0.0487669,"inspired by the Neural Turing Machine (NTM) (Graves et al., 2014, 2016) and memory network (Weston et al., 2014). (Wang et al., 2017a) used special NTM memory to extend the decoder in the attention-based NMT. In their method, the memory is used to provide temporary information from source to assist the decoding process. In contrast, our work uses memory to store contextual knowledge in the training data. Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous words (Vickrey et al., 2005; Zhai et al., 2013; Rios et al., 2017; Carpuat and Wu, 2007; Li et al., 2016). Among them, the most relevant is the work that decomposes the low-frequency words into smaller granularities, e.g, hybrid word-character model (Luong and Manning, 2016), sub-word model (Sennrich et al., 2016) or word piece model (Wu et al., 2016). These methods mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also"
D18-1036,P16-1100,0,0.0442821,"the attention-based NMT. In their method, the memory is used to provide temporary information from source to assist the decoding process. In contrast, our work uses memory to store contextual knowledge in the training data. Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous words (Vickrey et al., 2005; Zhai et al., 2013; Rios et al., 2017; Carpuat and Wu, 2007; Li et al., 2016). Among them, the most relevant is the work that decomposes the low-frequency words into smaller granularities, e.g, hybrid word-character model (Luong and Manning, 2016), sub-word model (Sennrich et al., 2016) or word piece model (Wu et al., 2016). These methods mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in Zhang and Zong (2015). Later,"
D18-1036,1983.tc-1.13,0,0.743972,"Missing"
D18-1036,D15-1166,0,0.347565,"esome words can be correctly translated. The contributions are listed as follows: 1) We are the first to define and handle the troublesome words in neural machine translation. 2) We propose to memorize not only the bilingual lexicons but also their contexts with a contextual memory. 3) We design a dynamic approach to correctly translate the troublesome words by combining the contextual memory and the NMT model. 2 ci = (3) where ai,j is the attention weight: m hm j zi ai,j = P m m j hj zi (4) where zim is the hidden state of the top layer in decoder. More detailed introduction can be found in (Luong et al., 2015). Notation. In this paper, we denote the whole |VS | source vocabulary by VS = {sm }m=1 and target |VT | vocabulary by VT = {tn }n=1 , where sm is the source word and tn is the target word. We denote a source sentence by X and a target sentence by Y . Each source word in X is denoted by xj . Each target word in Y is denoted by yi . Accordingly, a target word can be denoted not only by tn , but also by yi . This does not contradict. tn means this target word is the nth word in vocabulary VT , and yi means this target word is the ith word in sentence Y . Similarly, we denote a source word by sm"
D18-1036,D16-1096,0,0.059563,"that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in Zhang and Zong (2015). Later, the researchers transfer to NMT framework, e.g. (Wang et al., 2017b; Zhang and Zong, 2016; Zhou et al., 2017; Tu et al., 2016; Mi et al., 2016; He et al., 2016; Dahlmann et al., 2017; Wang et al., 2017c,d; Gu et al., 2018; Zhao et al., 2018). The most relevant studies are Arthur et al. (2016) and Results on EN-DE Translation We also test our method on EN-DE translation and the results are reported in Table 6. We can see that our method is still effective on EN-DE translation. Specifically, when the translation unit is word, the proposed method improves the baseline by 1.13 BLEU points. The improvement is 0.76 BLEU points when the translation unit is sub-word. 398 Feng et al. (2017). They incorporate the lexicon pairs into NMT to imp"
D18-1036,P13-1111,1,0.846978,"ng Machine for NMT. Our idea is first inspired by the Neural Turing Machine (NTM) (Graves et al., 2014, 2016) and memory network (Weston et al., 2014). (Wang et al., 2017a) used special NTM memory to extend the decoder in the attention-based NMT. In their method, the memory is used to provide temporary information from source to assist the decoding process. In contrast, our work uses memory to store contextual knowledge in the training data. Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous words (Vickrey et al., 2005; Zhai et al., 2013; Rios et al., 2017; Carpuat and Wu, 2007; Li et al., 2016). Among them, the most relevant is the work that decomposes the low-frequency words into smaller granularities, e.g, hybrid word-character model (Luong and Manning, 2016), sub-word model (Sennrich et al., 2016) or word piece model (Wu et al., 2016). These methods mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. C"
D18-1036,P02-1040,0,0.101068,"liable. Thus we design the dynamic weight λi according to the exception rate r(xj ): λi = sigmoid(βγ ∗ γi ) γi = Tx X ai,j ∗ r(xj ) sentence pairs whose length exceeds 100. We run a total of 20 iterations for all translation tasks. We test all methods based on two granularities: words and sub-words. For word granularity, we limit the vocabulary to 30K (CH-EN) and 50K (EN-DE) for both the source and target languages. For subword granularity, we use the BPE method (Sennrich et al., 2016) to merge 30K (CH-EN) and 32K (EN-DE) steps. The beam size is set to 12. We use case-insensitive 4-gram BLEU (Papineni et al., 2002) for translation quality evaluation. We compare our method with other relevant methods as follows: (12) j 1) Baseline: It is the baseline NMT system with global attention (Luong et al., 2015; Zoph and Knight, 2016; Jean et al., 2015). where βγ is a learnable parameter. From Eq. (12), the dynamic weight λi is determined by both of the attention weight ai,j , and the exception rate r(xj ). Training the parameters. As discussed above, our method contains some parameters (vd , Wh , Wc and βγ ) to be learned. We denote the parameters introduced by our method by θM and the parameters in NMT by θN ."
D18-1036,W17-4702,0,0.0202248,"Our idea is first inspired by the Neural Turing Machine (NTM) (Graves et al., 2014, 2016) and memory network (Weston et al., 2014). (Wang et al., 2017a) used special NTM memory to extend the decoder in the attention-based NMT. In their method, the memory is used to provide temporary information from source to assist the decoding process. In contrast, our work uses memory to store contextual knowledge in the training data. Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous words (Vickrey et al., 2005; Zhai et al., 2013; Rios et al., 2017; Carpuat and Wu, 2007; Li et al., 2016). Among them, the most relevant is the work that decomposes the low-frequency words into smaller granularities, e.g, hybrid word-character model (Luong and Manning, 2016), sub-word model (Sennrich et al., 2016) or word piece model (Wu et al., 2016). These methods mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NM"
D18-1036,D16-1160,1,0.858302,"., 2016). These methods mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in Zhang and Zong (2015). Later, the researchers transfer to NMT framework, e.g. (Wang et al., 2017b; Zhang and Zong, 2016; Zhou et al., 2017; Tu et al., 2016; Mi et al., 2016; He et al., 2016; Dahlmann et al., 2017; Wang et al., 2017c,d; Gu et al., 2018; Zhao et al., 2018). The most relevant studies are Arthur et al. (2016) and Results on EN-DE Translation We also test our method on EN-DE translation and the results are reported in Table 6. We can see that our method is still effective on EN-DE translation. Specifically, when the translation unit is word, the proposed method improves the baseline by 1.13 BLEU points. The improvement is 0.76 BLEU points when the translation unit is sub-word. 398 Feng et al. (2017"
D18-1036,P16-1162,0,0.809658,"achine translation (NMT) based on the encoder-decoder architecture becomes the new state-of-the-art due to distributed representation and end-to-end learning (Cho et al., 2014; Bahdanau et al., 2015; Junczys-Dowmunt et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). However, the current NMT is a global model that maximizes the performance on the overall data and has problems in handling low-frequency words and ambiguous words1 , we refer these words as troublesome words and define them in Section 3.1. Some previous work attempt to tackle the translation problem of low-frequency words. Sennrich et al. (2016) propose to decompose the words into subwords which are used as translation units so 1 In this work, we consider a source word is ambiguous if it has multiple translations with high entropy of probability distribution. 391 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 391–400 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ci can be calculated as follows: address them. Our method first investigates different strategies to define the troublesome words. Then, these words and their contexts in the t"
D18-1036,P16-5005,0,0.0197893,"owfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in Zhang and Zong (2015). Later, the researchers transfer to NMT framework, e.g. (Wang et al., 2017b; Zhang and Zong, 2016; Zhou et al., 2017; Tu et al., 2016; Mi et al., 2016; He et al., 2016; Dahlmann et al., 2017; Wang et al., 2017c,d; Gu et al., 2018; Zhao et al., 2018). The most relevant studies are Arthur et al. (2016) and Results on EN-DE Translation We also test our method on EN-DE translation and the results are reported in Table 6. We can see that our method is still effective on EN-DE translation. Specifically, when the translation unit is word, the proposed method improves the baseline by 1.13 BLEU points. The improvement is 0.76 BLEU points when the translation unit is sub-word. 398 Feng et al. (2017). They incorporate the lexicon pair"
D18-1036,P17-2060,1,0.804071,"s mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in Zhang and Zong (2015). Later, the researchers transfer to NMT framework, e.g. (Wang et al., 2017b; Zhang and Zong, 2016; Zhou et al., 2017; Tu et al., 2016; Mi et al., 2016; He et al., 2016; Dahlmann et al., 2017; Wang et al., 2017c,d; Gu et al., 2018; Zhao et al., 2018). The most relevant studies are Arthur et al. (2016) and Results on EN-DE Translation We also test our method on EN-DE translation and the results are reported in Table 6. We can see that our method is still effective on EN-DE translation. Specifically, when the translation unit is word, the proposed method improves the baseline by 1.13 BLEU points. The improvement is 0.76 BLEU points when the translation unit is sub-word. 398 Feng et al. (2017). They incorporate"
D18-1036,N16-1004,0,0.0190829,"ranslation tasks. We test all methods based on two granularities: words and sub-words. For word granularity, we limit the vocabulary to 30K (CH-EN) and 50K (EN-DE) for both the source and target languages. For subword granularity, we use the BPE method (Sennrich et al., 2016) to merge 30K (CH-EN) and 32K (EN-DE) steps. The beam size is set to 12. We use case-insensitive 4-gram BLEU (Papineni et al., 2002) for translation quality evaluation. We compare our method with other relevant methods as follows: (12) j 1) Baseline: It is the baseline NMT system with global attention (Luong et al., 2015; Zoph and Knight, 2016; Jean et al., 2015). where βγ is a learnable parameter. From Eq. (12), the dynamic weight λi is determined by both of the attention weight ai,j , and the exception rate r(xj ). Training the parameters. As discussed above, our method contains some parameters (vd , Wh , Wc and βγ ) to be learned. We denote the parameters introduced by our method by θM and the parameters in NMT by θN . To make it efficient, given the aligned training data D =  (d) (d) |D| X ,Y , we keep θN unchanged and opd=1 M timize θ by maximizing the following objective function. 2) Arthur: It is the state-of-the-art method"
D18-1036,H05-1097,0,0.0707789,"s follows: Neural Turing Machine for NMT. Our idea is first inspired by the Neural Turing Machine (NTM) (Graves et al., 2014, 2016) and memory network (Weston et al., 2014). (Wang et al., 2017a) used special NTM memory to extend the decoder in the attention-based NMT. In their method, the memory is used to provide temporary information from source to assist the decoding process. In contrast, our work uses memory to store contextual knowledge in the training data. Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous words (Vickrey et al., 2005; Zhai et al., 2013; Rios et al., 2017; Carpuat and Wu, 2007; Li et al., 2016). Among them, the most relevant is the work that decomposes the low-frequency words into smaller granularities, e.g, hybrid word-character model (Luong and Manning, 2016), sub-word model (Sennrich et al., 2016) or word piece model (Wu et al., 2016). These methods mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could allevi"
D18-1036,D17-1149,0,0.0883335,"n cases (Deterio) when rectifying the troublesome words. As a comparison, we also count the total rectification and deterioration numbers of Arthur(test). The results are reported in Table 5. These results show that our method could rectify more words (51 vs. 70) with less deterioration (17 vs. 11) than Arthur(test). 6 Unit 7 Related Work The related work can be divided into three categories and we describe each of them as follows: Neural Turing Machine for NMT. Our idea is first inspired by the Neural Turing Machine (NTM) (Graves et al., 2014, 2016) and memory network (Weston et al., 2014). (Wang et al., 2017a) used special NTM memory to extend the decoder in the attention-based NMT. In their method, the memory is used to provide temporary information from source to assist the decoding process. In contrast, our work uses memory to store contextual knowledge in the training data. Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous words (Vickrey et al., 2005; Zhai et al., 2013; Rios et al., 2017; Carpuat and Wu, 2007; Li et al., 2016). Among them, the most relevant is the work that decomposes the low-frequency words into small"
D18-1036,I17-1039,1,0.885788,"Missing"
D19-1047,W02-0109,0,0.0595018,"Table 4 is the summary of the experimental results. We use underscores to represent the best published models, and bold the best records. Best models in our proposed architecture beat previous state-of-the-art models on all eight text classification benchmarks. For published models, best results are achieved almost all by local feature driven models including Region-emb, VDCNN and DRNN. Self-Attention model SANet performs well, but does not achieve advantageous results as in sequence to sequence Model Settings For data preprocessing, all the texts of datasets are tokenized by NLTKs tokenizer (Loper and Bird, 2002). For model hyperparameters, notations are following in 3.2 and 3.3. We adopt CNN, GRU and Attention for Encoder1. In CNN (Encoder1), we use filter windows (h) of [3, 5, 7] with 128 feature maps (k) each. The hidden unit number is 128 in GRU (Encoder1) and Attention (Encoder1). For Encoder2, we conduct experiments based on two types of local feature 500 Model Yelp P. Yelp F. Amz. P. Amz. F. AG Sogou Yah. A. DBP bigram-FastText (Joulin et al., 2016) Region-emb (Qiao et al., 2018) 95.7 96.2 63.9 64.5 94.6 95.3 60.2 60.8 92.5 92.8 96.8 97.3 72.3 73.4 98.6 98.9 SANet(big) (Letarte et al., 2018) 95"
D19-1047,W02-1011,0,0.0214788,"pecially in the case of insufficient corpus. Text classification is a fundamental task in natural language processing, which is widely used in various applications such as spam detection, sentiment analysis and topic classification. One of the mainstream approaches firstly utilizes explicit local extractors to identity key local patterns and classifies based on them afterwards. In this paper, we call this line of research as local feature driven models. Lots of proposed methods can be grouped into this scope. Ngrams have been traditionally exploited in statistical machine learning approaches (Pang et al., 2002; Wang and Manning, 2012). For deep neural networks, encoding local features into low-dimensional distributed ngrams To address this issue, we believe a more efficient approach is to optimize the local extraction process directly. In this paper, we propose ? These authors contributed equally to this work. † This work was done while the author was an intern at Baidu Inc. 496 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 496–506, c Hong Kong, China, November 3–7, 2019. 2019 A"
D19-1047,D14-1162,0,0.0875604,"Missing"
D19-1047,N18-1202,0,0.0286707,"o sentiment analysis, and QA refers to question answering. 3.5 Table 3: Model settings. We limit the vocabulary size and set maximum sequence length. We also show the window size in DRNN following Wang (2018). extractor, corresponding to CNN and DRNN respectively. In CNN (Encoder2), window sizes of filters are of [3, 5, 7] with 128 feature maps each. In DRNN (Encoder2), all the dimensions of hidden states are set to 300. Other settings are shown in Table 3, all trainable parameters including embeddings of words are initialized randomly without any pre-trained techniques (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2018). Classification Layer After incorporating the global information obtained from Encoder1 into the local feature extraction of Encoder2, the output vector of latter can be regarded as the representation of the entire text. The vector is then fed into a softmax classifier to predict the probability of each category and cross entropy is used as loss function: yˆ = sof tmax(Wc enc2 + bc ) X H(y, yˆ) = yi log yˆi Training and Validation For each dataset, we randomly split the full training corpus into training and validation set, where the validation size is the same as the co"
D19-1047,P17-1052,0,0.530675,"windows and has reported best results on several benchmarks. Introduction Despite having good interpretability and remarkable performance, current local feature extraction still has one shortcoming. As shown in Table 1, the real meaning of Apple can only be correctly recognized from overall view instead of narrow window. If the local extractor in charge of Apple cannot receive camera and nutritional from the very beginning, it would require complicated and costly upper structures to help revise the imprecisely local representation and create newer high-level features, such as deeply stacking (Johnson and Zhang, 2017; Conneau et al., 2016) and hybrid integration (Xiao and Cho, 2016). To a certain extend, it is inefficient and hard to train especially in the case of insufficient corpus. Text classification is a fundamental task in natural language processing, which is widely used in various applications such as spam detection, sentiment analysis and topic classification. One of the mainstream approaches firstly utilizes explicit local extractors to identity key local patterns and classifies based on them afterwards. In this paper, we call this line of research as local feature driven models. Lots of propos"
D19-1047,D14-1181,0,0.0155923,"Missing"
D19-1047,N16-1176,0,0.453804,"Missing"
D19-1047,W18-5429,0,0.0235627,"Missing"
D19-1047,P15-1150,0,0.0445647,"Missing"
D19-1047,D15-1167,0,0.0857808,"Missing"
D19-1047,P16-2034,0,0.0209808,"ontrols the flow of previous information. The hidden state ht is computed iteratively based on ht 1 and xt . As a result, the all previous information can be encoded. For saving space, here we abbreviate it as: ht = GRU (x1 , x2 , . . . , xt ) (4) The global representation produced by GRU is hidden states of all time steps: Encoder1: Global Information Provider Without loss of generality, we introduce three types of models for Encoder1 in our architecture, enc1 = [h1 ; h2 ; . . . ; hn ] 498 (5) Attention We also introduce attention mechanism on GRU for enhancing valuable information following Zhou et al. (2016). Define a context vector uw to measure the importance of each hidden state ht in GRU, which is randomly initialized and learned during training. A normalized importance weight ↵t is obtained through a softmax function: exp(tanh(ht )&gt; uw ) ↵t = P &gt; t exp(tanh(ht ) uw ) To maintain translation invariant, a max-overtime pooling layer is then applied to CNN or DRNN layer, the pooling result is regarded as the output of Encoder2: enc2 = maxpool([h1 ; h2 ; . . . ; hn ]) 3.4 (6) 3.3 gt = G(enc1 , xt (7) Vanilla local feature extractor strictly focuses on a limited size region. Here we propose a vari"
D19-1047,P18-1215,0,0.172598,"s demonstrate that our proposed architecture promotes local feature driven models by a substantial margin and outperforms the previous best models in the fully-supervised setting. 1 Table 1: Topic classification examples for Technology and Health, where Apple is ambiguous within local context. embeddings (Joulin et al., 2016; Qiao et al., 2018) and simply bagging of them have been proved effective and highly efficient. Convolutional Neural Networks (CNN) (LeCun et al., 2010) are promising methods for their strong capacities in capturing local invariant regularities (Kim, 2014). More recently, Wang (2018) proposes the Disconnected Recurrent Neural Network (DRNN), which utilizes RNN to extract local features for larger windows and has reported best results on several benchmarks. Introduction Despite having good interpretability and remarkable performance, current local feature extraction still has one shortcoming. As shown in Table 1, the real meaning of Apple can only be correctly recognized from overall view instead of narrow window. If the local extractor in charge of Apple cannot receive camera and nutritional from the very beginning, it would require complicated and costly upper structures"
D19-1047,P12-2018,0,0.0156346,"e of insufficient corpus. Text classification is a fundamental task in natural language processing, which is widely used in various applications such as spam detection, sentiment analysis and topic classification. One of the mainstream approaches firstly utilizes explicit local extractors to identity key local patterns and classifies based on them afterwards. In this paper, we call this line of research as local feature driven models. Lots of proposed methods can be grouped into this scope. Ngrams have been traditionally exploited in statistical machine learning approaches (Pang et al., 2002; Wang and Manning, 2012). For deep neural networks, encoding local features into low-dimensional distributed ngrams To address this issue, we believe a more efficient approach is to optimize the local extraction process directly. In this paper, we propose ? These authors contributed equally to this work. † This work was done while the author was an intern at Baidu Inc. 496 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 496–506, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computatio"
D19-1047,N16-1174,0,0.0688166,"ven model whose output is directly fed into the classifier. (3) Mode is the interaction manner between them. S and A are abbreviation of SAME and ATTEND respectively. ument representation. Tai et al. (2015) introduce a tree-structured LSTM for sentiment classification. The attention mechanism proposed by Bahdanau et al. (2014) has achieved great success in machine translation (Vaswani et al., 2017). For text classification which only has single input sequence, attention based models mainly focus on applying attention mechanism on top of CNN or RNN for selecting the more important information (Yang et al., 2016; Er et al., 2016). Letarte et al. (2018) and Shen et al. (2018) also explore self-attention networks which is CNN/RNN free. 3 3.1 each of which can be an independent global information provider and they are compared in our experiments. CNN Let xt be the d-dimensional word vector corresponding to the t-th word in a sequence of length n, xt h+1:t refers to the concatenation of words xt h+1 , xt h+2 , . . . , xt with size h and k number of filters are applied to the input sequence to generate features. Formally, filters Wf are applied to window xt h+1:t to compute ht : ht = Conv(xt = relu(Wf xt"
D19-1047,C18-1173,0,0.0185048,"eep pyramid CNN Johnson and Zhang (2017) and convolution-recurrent networks Xiao and Cho (2016), in which recurrent layers are designed on top of convolutional layers for learning long-term dependencies between local features. CNN use simple linear operations on n-gram vectors of each window, which enlightens researchers to capture higher order local non-linear feature using RNN. Shi et al. (2016) first replace convolution filters with LSTM for query classification. Wang (2018) proposes DRNN, which exploits large window size equipped with GRU. To make full use of local and global information, Zhao et al. (2018) propose a sandwich network by carding a CNN in the middle of two LSTM layers, where the output of CNN provides local semantic representations while the top LSTM supplies global structure representations. However, the global information they mainly focus on is the syntax part, which is produced by reorganizing the already obtained local features. Besides, both of them are directly used for final classification, while we use pre-acquired global representations to help capture better local features. To the best of our knowledge, we are the first to incorporate global representation into the extr"
D19-1079,D16-1139,0,0.123117,"Missing"
D19-1079,N16-1046,0,0.140054,"Missing"
D19-1079,P18-1129,0,0.305262,"nse disambiguation (Tang et al., 2018; Domhan, 2018). In this paper, we investigate the effects of two teams of multi-agents: a team of alternative agents mentioned above, and a uniform team of different initialization for the same model. • We extend the study on training with two agents to the multi-agent scenario, and propose a general learning strategy to train multiple agents. To resolve the second problem, we simplify the many-to-many learning to the one-to-many (one teacher vs. many students) learning, extending ensemble knowledge distillation (Fukuda et al., 2017; Freitag et al., 2017; Liu et al., 2018; Zhu et al., 2018). During the training, each agent performs better by learning from the ensemble model (Teacher) of all agents integrating the knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016) into the training objective. This procedure can be viewed as the introduction of an additional regularization term into the training objective, with which each agent can learn advantages from the ensemble model gradually. With this method, each agent is optimized not only to the maximization of the likelihood of the training data, but also to the minimization of the divergence between it"
D19-1079,E17-2060,0,0.0560241,"Missing"
D19-1079,P18-1167,0,0.02985,"requires an effective learning strategy. There have been many alternatives to improve the diversity of models even based on the Transformer model (Vaswani et al., 2017). For example, decoding in the opposite direction usually results in different preferences: good prefixes and bad prefixes (Zhang et al., 2019b). Rather, selfattention with relative position representations enhances the generalization to sequence lengths unseen during training (Shaw et al., 2018). Furthermore, increasing the size of layers in the encoder is expected to specialize in word sense disambiguation (Tang et al., 2018; Domhan, 2018). In this paper, we investigate the effects of two teams of multi-agents: a team of alternative agents mentioned above, and a uniform team of different initialization for the same model. • We extend the study on training with two agents to the multi-agent scenario, and propose a general learning strategy to train multiple agents. To resolve the second problem, we simplify the many-to-many learning to the one-to-many (one teacher vs. many students) learning, extending ensemble knowledge distillation (Fukuda et al., 2017; Freitag et al., 2017; Liu et al., 2018; Zhu et al., 2018). During the trai"
D19-1079,P16-1162,0,0.0912544,"ss: LiKD ← (q(yt ), p(yti ), Yt , Ysi , Xg , Yg ); Compute NLL loss: LN LL ← (p(yti ), Xg , Yg ) ; Compute Agent loss: Lia ← λLN LL + (1 − λ)LiKD ; end P i Model loss: Lf inal = N i=1 La ; Update gradients for each agent; end until convergence; 4 Experiments In this paper, we evaluate our model on four translation tasks: NIST Chinese-English Translation Task, IWSLT 2014 German-English Translation Task, WMT 2014 English-German Translation Task and large-scale Chinese-English Translation Task. Joint Learning 4.1 Data Preprocessing To compare with previous studies, we conduct byte-pair encoding (Sennrich et al., 2016) for Chinese, English and German sentences, setting the In the work of Zhang et al. (2019b), they proposed a relatively complex joint learning framework for training two agents. In this paper, according to the 860 MODEL Wang et al. (2018) a.L2R b.R2L c.Enc d.Rel a+b a+c a+d c+d a×2 d×2 a+b+c a+b+d a+c+d a×3 d×3 a+b+c+d MT02 MT03 MT04 MT08 Results for Best Agent 46.60 47.73 48.53 47.07 48.43 42.21 47.06 45.58 47.14 41.04 48.86 47.54 48.57 42.93 48.12 48.19 48.33 42.51 48.82 47.65 48.45 42.49 48.79 48.30 49.32 43.44 48.76 48.40 48.74 43.27 49.45 49.01 49.52 43.71 48.64 47.98 49.08 43.07 48.23 48"
D19-1079,P12-1000,0,0.244265,"Missing"
D19-1079,D18-1045,0,0.0255087,"by the pretraining of the i − th agent, and the Bavg is the average BLEU score of all agents. The above formula suggests the agent learns more from the ensemble model as its performance 2 LDC2002E18, LDC2002L27, LDC2002T01, LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24, LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92, LDC2006E93, LDC2004T08(HK News, HK Hansards ) 3 https://github.com/paddlepaddle/ paddle 861 Models ConvS2S Gehring et al. (2017) Transformer Vaswani et al. (2017) Rel Shaw et al. (2018) DynamicConv Wu et al. (2019) Back-translation Sergey et al. (2018) Dual-3 Wang et al. (2019) Dual-3 + Mono Data L2R Rel Rel-4 Task L2R Rel KD-4 Dual-5 Rel-4 De-En 33.63 34.91 35.53 34.70 36.27 Table 3: BLEU score on IWSLT 2014 German-English translation. KD-4 stands for ensemble knowledge distillation with four agents. Dual-5 is the SOTA model from the work of Wang et al. (2019). And Rel-4 is our best model (Rel) training with four diverse agents. is worse than the majority vote, rather than focusing on exploring by its own prediction. We train our model with parallelization at data batch level. For NIST Chinese-English task, it takes about 1.5 days to train"
D19-1079,N18-2074,0,0.0640489,"al., 2013) Second, learning in multi-agent scenario is many-to-many, as opposed to the relatively simpler one-to-one learning in two-agent training, and requires an effective learning strategy. There have been many alternatives to improve the diversity of models even based on the Transformer model (Vaswani et al., 2017). For example, decoding in the opposite direction usually results in different preferences: good prefixes and bad prefixes (Zhang et al., 2019b). Rather, selfattention with relative position representations enhances the generalization to sequence lengths unseen during training (Shaw et al., 2018). Furthermore, increasing the size of layers in the encoder is expected to specialize in word sense disambiguation (Tang et al., 2018; Domhan, 2018). In this paper, we investigate the effects of two teams of multi-agents: a team of alternative agents mentioned above, and a uniform team of different initialization for the same model. • We extend the study on training with two agents to the multi-agent scenario, and propose a general learning strategy to train multiple agents. To resolve the second problem, we simplify the many-to-many learning to the one-to-many (one teacher vs. many students)"
D19-1079,D18-1458,0,0.0233259,"Missing"
D19-1079,C18-1124,0,0.0535505,"Missing"
D19-1187,E17-1013,0,0.0246841,"rom Wikipedia or user generated content, some work focus on either modeling of conversation generation with unstructured texts (Ghazvininejad et al., 2018; Vougiouklis et al., 2016; Xu et al., 2017), or building benchmark dialogue data grounded on knowledge (Dinan et al., 2019; Moghe et al., 2018). In comparison with them, we adopt a graph based representation scheme for unstructured texts, which enables better explainability and generalization capability of our system. Knowledge Graph Reasoning: Previous studies on KG reasoning can be categorized into three lines, path-based symbolic models (Das et al., 2017a; Lao et al., 2011), embedding-based neural models (Bordes et al., 2013; Wang et al., 2014), and models in unifying embedding and path-based 1783 technology (Das et al., 2018; Lin et al., 2018; Xiong et al., 2017), which can predict missing links for completion of KG. In this work, for knowledge selection on a graph, we follow the third line of works. Furthermore, our problem setting is different from theirs in that some of our vertices contain long texts, which motivates the use of machine reading technology for graph reasoning. Fusion of KG triples and texts: In the task of QA, combination"
D19-1187,P17-2057,0,0.0194558,"rom Wikipedia or user generated content, some work focus on either modeling of conversation generation with unstructured texts (Ghazvininejad et al., 2018; Vougiouklis et al., 2016; Xu et al., 2017), or building benchmark dialogue data grounded on knowledge (Dinan et al., 2019; Moghe et al., 2018). In comparison with them, we adopt a graph based representation scheme for unstructured texts, which enables better explainability and generalization capability of our system. Knowledge Graph Reasoning: Previous studies on KG reasoning can be categorized into three lines, path-based symbolic models (Das et al., 2017a; Lao et al., 2011), embedding-based neural models (Bordes et al., 2013; Wang et al., 2014), and models in unifying embedding and path-based 1783 technology (Das et al., 2018; Lin et al., 2018; Xiong et al., 2017), which can predict missing links for completion of KG. In this work, for knowledge selection on a graph, we follow the third line of works. Furthermore, our problem setting is different from theirs in that some of our vertices contain long texts, which motivates the use of machine reading technology for graph reasoning. Fusion of KG triples and texts: In the task of QA, combination"
D19-1187,P18-1138,0,0.133721,"(Ritter et al., 2011; Shang et al., 2015). 1 Data and codes are available at https://github. com/PaddlePaddle/models/tree/develop/ PaddleNLP/Research/EMNLP2019-AKGCM However, these models tend to produce generic responses or incoherent responses for a given topic, since it is quite challenging to learn semantic interactions merely from dialogue data without help of background knowledge. Recently, some previous studies have been conducted to introduce external knowledge, either unstructured knowledge texts (Ghazvininejad et al., 2018; Vougiouklis et al., 2016) or structured knowledge triples (Liu et al., 2018; Young et al., 2018; Zhou et al., 2018) to help open-domain conversation generation by producing responses conditioned on selected knowledge. In the first research line, their knowledge graph can help narrowing down knowledge candidates for conversation generation with the use of prior information, e.g., triple attributes or graph paths. Moreover, these prior information can enhance generalization capability of knowledge selection models. But it suffers from information insufficiency for response generation since there is simply a single word or entity to facilitate generation. In the second"
D19-1187,D18-1255,0,0.442446,"and integrate more explainable and flexible multi-hop graph reasoning models into conversation systems. Wu et al. (2018) used document reasoning network for modeling of conversational contexts, but not for knowledge selection. Conversation with Unstructured Texts: With availability of a large amount of knowledge texts from Wikipedia or user generated content, some work focus on either modeling of conversation generation with unstructured texts (Ghazvininejad et al., 2018; Vougiouklis et al., 2016; Xu et al., 2017), or building benchmark dialogue data grounded on knowledge (Dinan et al., 2019; Moghe et al., 2018). In comparison with them, we adopt a graph based representation scheme for unstructured texts, which enables better explainability and generalization capability of our system. Knowledge Graph Reasoning: Previous studies on KG reasoning can be categorized into three lines, path-based symbolic models (Das et al., 2017a; Lao et al., 2011), embedding-based neural models (Bordes et al., 2013; Wang et al., 2014), and models in unifying embedding and path-based 1783 technology (Das et al., 2018; Lin et al., 2018; Xiong et al., 2017), which can predict missing links for completion of KG. In this work"
D19-1187,P02-1040,0,0.103053,"Missing"
D19-1187,D11-1054,0,0.0255171,"ne reading comprehension technology. We demonstrate the effectiveness of our system on two datasets in comparison with state-of-the-art models1 . 1 Introduction One of the key goals of AI is to build a machine that can talk with humans when given an initial topic. To achieve this goal, the machine should be able to understand language with background knowledge, recall knowledge from memory or external resource, reason about these concepts together, and finally output appropriate and informative responses. Lots of research efforts have been devoted to chitchat oriented conversation generation (Ritter et al., 2011; Shang et al., 2015). 1 Data and codes are available at https://github. com/PaddlePaddle/models/tree/develop/ PaddleNLP/Research/EMNLP2019-AKGCM However, these models tend to produce generic responses or incoherent responses for a given topic, since it is quite challenging to learn semantic interactions merely from dialogue data without help of background knowledge. Recently, some previous studies have been conducted to introduce external knowledge, either unstructured knowledge texts (Ghazvininejad et al., 2018; Vougiouklis et al., 2016) or structured knowledge triples (Liu et al., 2018; You"
D19-1187,P17-1099,0,0.32399,"P (ASt )), PKS (vd |vX , G, X) = P (AST −1 ). Please see Section 3.1 for definition of PKS (∗). When the agent finally arrives at ST , we obtain vT as the answer vY for response generation. Training: For the policy network (πθ ) described above, we want to find parameters θ that maximize the expected reward: (8) (9) (10) J(θ) = E(v0 ,X,vgt )∼D EA0 ,...,AT −1 ∼πθ [R(ST )|S0 = (v0 , v0 , X, vgt )], (11) where we assume there is a true underlying distribution D, and (v0 , X, vgt ) ∼ D. 3.4 Knowledge Aware Generation Following the work of Moghe et al. (2018), we modify a text summarization model (See et al., 2017) to suit this generation task. In the summarization task, its input is a document and its output is a summary, but in our case the input is a [selected knowledge, message] pair and the output is a response. Therefore we introduce two RNNs: one is for computing the representation of the selected knowledge, and the other for the message. The decoder accepts the two representations and its own internal state representation as input, and then compute (1) a probability score which indicates whether the next word should be generated or copied, (2) a probability distribution over the vocabulary if th"
D19-1187,W15-4616,0,0.135638,"reover, our graph differs from previous KGs in that: some vertices in ours contain long texts, not a single entity or word. To fully leverage this long text information, we improve the reasoning algorithm with machine reading comprehension (MRC) technology (Seo et al., 2017) to conduct fine-grained semantic matching between an input message and candidate vertices. Finally, for response generation, we use an encoder-decoder model to produce responses conditioned on selected knowledge. 2 Related Work Conversation with Knowledge Graph: There are growing interests in leveraging factoid knowledge (Han et al., 2015; Liu et al., 2018; Zhu et al., 2017) or commonsense knowledge (Young et al., 2018; Zhou et al., 2018) with graph based representation for generation of appropriate and informative responses. Compared with them, we augment previous KGs with knowledge texts and integrate more explainable and flexible multi-hop graph reasoning models into conversation systems. Wu et al. (2018) used document reasoning network for modeling of conversational contexts, but not for knowledge selection. Conversation with Unstructured Texts: With availability of a large amount of knowledge texts from Wikipedia or user"
D19-1187,P15-1152,0,0.252756,"Missing"
D19-1187,D11-1049,0,0.020697,"er generated content, some work focus on either modeling of conversation generation with unstructured texts (Ghazvininejad et al., 2018; Vougiouklis et al., 2016; Xu et al., 2017), or building benchmark dialogue data grounded on knowledge (Dinan et al., 2019; Moghe et al., 2018). In comparison with them, we adopt a graph based representation scheme for unstructured texts, which enables better explainability and generalization capability of our system. Knowledge Graph Reasoning: Previous studies on KG reasoning can be categorized into three lines, path-based symbolic models (Das et al., 2017a; Lao et al., 2011), embedding-based neural models (Bordes et al., 2013; Wang et al., 2014), and models in unifying embedding and path-based 1783 technology (Das et al., 2018; Lin et al., 2018; Xiong et al., 2017), which can predict missing links for completion of KG. In this work, for knowledge selection on a graph, we follow the third line of works. Furthermore, our problem setting is different from theirs in that some of our vertices contain long texts, which motivates the use of machine reading technology for graph reasoning. Fusion of KG triples and texts: In the task of QA, combination of a KG and a text c"
D19-1187,W04-1013,0,0.0594973,"Missing"
D19-1187,P04-1077,0,0.0682962,"Missing"
D19-1187,D18-1362,0,0.0491171,"2017), or building benchmark dialogue data grounded on knowledge (Dinan et al., 2019; Moghe et al., 2018). In comparison with them, we adopt a graph based representation scheme for unstructured texts, which enables better explainability and generalization capability of our system. Knowledge Graph Reasoning: Previous studies on KG reasoning can be categorized into three lines, path-based symbolic models (Das et al., 2017a; Lao et al., 2011), embedding-based neural models (Bordes et al., 2013; Wang et al., 2014), and models in unifying embedding and path-based 1783 technology (Das et al., 2018; Lin et al., 2018; Xiong et al., 2017), which can predict missing links for completion of KG. In this work, for knowledge selection on a graph, we follow the third line of works. Furthermore, our problem setting is different from theirs in that some of our vertices contain long texts, which motivates the use of machine reading technology for graph reasoning. Fusion of KG triples and texts: In the task of QA, combination of a KG and a text corpus has been studied with a strategy of late fusion (Gardner and Krishnamurthy, 2017; Ryu et al., 2014) or early fusion (Das et al., 2017b; Sun et al., 2018), which can he"
D19-1187,D18-1455,0,0.0405211,"Missing"
D19-1187,C16-1318,0,0.265115,"e been devoted to chitchat oriented conversation generation (Ritter et al., 2011; Shang et al., 2015). 1 Data and codes are available at https://github. com/PaddlePaddle/models/tree/develop/ PaddleNLP/Research/EMNLP2019-AKGCM However, these models tend to produce generic responses or incoherent responses for a given topic, since it is quite challenging to learn semantic interactions merely from dialogue data without help of background knowledge. Recently, some previous studies have been conducted to introduce external knowledge, either unstructured knowledge texts (Ghazvininejad et al., 2018; Vougiouklis et al., 2016) or structured knowledge triples (Liu et al., 2018; Young et al., 2018; Zhou et al., 2018) to help open-domain conversation generation by producing responses conditioned on selected knowledge. In the first research line, their knowledge graph can help narrowing down knowledge candidates for conversation generation with the use of prior information, e.g., triple attributes or graph paths. Moreover, these prior information can enhance generalization capability of knowledge selection models. But it suffers from information insufficiency for response generation since there is simply a single word"
D19-1187,N18-1186,0,0.0166739,"or response generation, we use an encoder-decoder model to produce responses conditioned on selected knowledge. 2 Related Work Conversation with Knowledge Graph: There are growing interests in leveraging factoid knowledge (Han et al., 2015; Liu et al., 2018; Zhu et al., 2017) or commonsense knowledge (Young et al., 2018; Zhou et al., 2018) with graph based representation for generation of appropriate and informative responses. Compared with them, we augment previous KGs with knowledge texts and integrate more explainable and flexible multi-hop graph reasoning models into conversation systems. Wu et al. (2018) used document reasoning network for modeling of conversational contexts, but not for knowledge selection. Conversation with Unstructured Texts: With availability of a large amount of knowledge texts from Wikipedia or user generated content, some work focus on either modeling of conversation generation with unstructured texts (Ghazvininejad et al., 2018; Vougiouklis et al., 2016; Xu et al., 2017), or building benchmark dialogue data grounded on knowledge (Dinan et al., 2019; Moghe et al., 2018). In comparison with them, we adopt a graph based representation scheme for unstructured texts, which"
D19-1187,D17-1060,0,0.0207557,"benchmark dialogue data grounded on knowledge (Dinan et al., 2019; Moghe et al., 2018). In comparison with them, we adopt a graph based representation scheme for unstructured texts, which enables better explainability and generalization capability of our system. Knowledge Graph Reasoning: Previous studies on KG reasoning can be categorized into three lines, path-based symbolic models (Das et al., 2017a; Lao et al., 2011), embedding-based neural models (Bordes et al., 2013; Wang et al., 2014), and models in unifying embedding and path-based 1783 technology (Das et al., 2018; Lin et al., 2018; Xiong et al., 2017), which can predict missing links for completion of KG. In this work, for knowledge selection on a graph, we follow the third line of works. Furthermore, our problem setting is different from theirs in that some of our vertices contain long texts, which motivates the use of machine reading technology for graph reasoning. Fusion of KG triples and texts: In the task of QA, combination of a KG and a text corpus has been studied with a strategy of late fusion (Gardner and Krishnamurthy, 2017; Ryu et al., 2014) or early fusion (Das et al., 2017b; Sun et al., 2018), which can help address the issue"
D19-5828,P18-1157,0,0.0118879,"dsourcing workers, examine writers, search logs, synthetics, etc. Machine reading comprehension (MRC) requires machines to understand text and answer questions about the text, and it is an important task in natural language processing (NLP). With the increasing availability of large-scale datasets for MRC (Rajpurkar et al., 2016; Bajaj et al., 2016; Dunn et al., 2017; Joshi et al., 2017; He et al., 2018) and the development of deep learning techniques, MRC has achieved remarkable advancements in the last few years (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Liu et al., 2018; Wang et al., 2018; Yu et al., 2018). Although a number of neural models obtain even human parity performance on several datasets, these models may generalize poorly on other datasets (Talmor and Berant, 2019). We expect that a truly effective question answering system works well on both the examples drawn from the same distribution as the training • Documents: They involve passages from different sources, e.g. wikipedia, news, movies, textbook, etc. • Language Understanding Ability: They might require different language understanding abilities, e.g. matching, reasoning and arithmetic. To add"
D19-5828,P19-1620,0,0.0163425,", 2019) uses multi-layer Transformer encoding blocks as its encoder. The pre-training tasks include masked language model and next sentence prediction, which enable the model to capture bidirectional and global information. In our system, we use the BERT large configuration that contains 24 Transformer encoding blocks, each with 16 self attention heads and 1024 hidden units. Note that we use this pre-trained model for experimental purpose, and it is not included in the final submission. In our experiments, we initialize the parameters of the encoding layers from the checkpoint 2 of the model (Alberti et al., 2019) namely BERT + N-Gram Masking + Synthetic Self-Training. The model is initialized from Whole Word Masking BERT (BERTwwm ), further fine-tuned on the SQuAD 2.0 task with synthetic generated question answering corpora. In our experiments, we find that this model performs consistently better than the original BERTlarge and 3 2 The checkpoint can be downloaded from https: //worksheets.codalab.org/worksheets/ 0xd7b08560b5b24bd1874b9429d58e2df1 https://github.com/google-research/ bert 4 https://github.com/zihangdai/xlnet/ 214 Model ID BERT Pre-trained XLNET Model ERNIE In-domain Masked Search Snippe"
D19-5828,D16-1264,0,0.030307,"els and we conduct experiments to examine the effectiveness of these strategies. Our system is ranked at top 1 of all the participants in terms of averaged F1 score. Our codes and models will be released at PaddleNLP 1 . 1 Introduction • Questions: They come from different sources, e.g. crowdsourcing workers, examine writers, search logs, synthetics, etc. Machine reading comprehension (MRC) requires machines to understand text and answer questions about the text, and it is an important task in natural language processing (NLP). With the increasing availability of large-scale datasets for MRC (Rajpurkar et al., 2016; Bajaj et al., 2016; Dunn et al., 2017; Joshi et al., 2017; He et al., 2018) and the development of deep learning techniques, MRC has achieved remarkable advancements in the last few years (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Liu et al., 2018; Wang et al., 2018; Yu et al., 2018). Although a number of neural models obtain even human parity performance on several datasets, these models may generalize poorly on other datasets (Talmor and Berant, 2019). We expect that a truly effective question answering system works well on both the examples drawn from"
D19-5828,N19-1213,0,0.343888,"C models based on the same pre-trained models. • Multi-task Learning: Since the pre-training is usually performed on corpus with restricted domains, it is expected that increasing the domain diversity by further pretraining on other corpus may improve the generalization capability. Hence, we incorporate masked language model by using corpus from various domains as an auxiliary task in the fine-tuning phase, along with MRC. The side effect of adding a language modeling objective to MRC is that it can avoid catastrophic forgetting and keep the most useful features learned from pretraining task (Chronopoulou et al., 2019). Additionally, we explore multi-task learning (Liu et al., 2019) by incorporating the supervised dataset from other NLP tasks (e.g. natural language inference and paragraph ranking) to learn better language representation. • The auxiliary task of masked language model can help improve the generalization of MRC models. • We do not observe much improvements from the auxiliary tasks of natural language inference and paragraph ranking. The remainder of this paper is structured as follows: Section 2 describes the detailed overview of our system. Section 3 shows the experimental settings and result"
D19-5828,P19-1485,0,0.0135196,"k in natural language processing (NLP). With the increasing availability of large-scale datasets for MRC (Rajpurkar et al., 2016; Bajaj et al., 2016; Dunn et al., 2017; Joshi et al., 2017; He et al., 2018) and the development of deep learning techniques, MRC has achieved remarkable advancements in the last few years (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Liu et al., 2018; Wang et al., 2018; Yu et al., 2018). Although a number of neural models obtain even human parity performance on several datasets, these models may generalize poorly on other datasets (Talmor and Berant, 2019). We expect that a truly effective question answering system works well on both the examples drawn from the same distribution as the training • Documents: They involve passages from different sources, e.g. wikipedia, news, movies, textbook, etc. • Language Understanding Ability: They might require different language understanding abilities, e.g. matching, reasoning and arithmetic. To address the above challenge, we introduce a simple framework of pre-training and fine-tuning, namely D-NET, for improving the generalization of MRC models by exploring the following techniques: • Pre-trained Model"
D19-5828,N19-1300,0,0.122698,"the pre-trained models, the corpus for the masked language model task, the types of supervised NLP tasks. The hyper-parameters include the max sequence length, batch size and the mix ratio λ used the auxiliary tasks in multi-task learning. version 5 . task (Chronopoulou et al., 2019). Supervised Tasks Motivated by (Liu et al., 2019), we explore multi-task learning by incorporating the supervised datasets from other NLP tasks to learn more general language representation. Specifically, we incorporate natural language inference and paragraph ranking as auxiliary tasks to MRC. (1) Previous work (Clark et al., 2019; Liu et al., 2019) show that MNLI (Williams et al., 2017) (a popular natural language inference dataset) can help improve the performance of the major task in a multi-task setting. In our system, we also leverage MNLI as an auxiliary task. (2) Previous work (Tan et al., 2017; Wang et al., 2018) examine the effectiveness of the joint learning of MRC and paragraph ranking. In our system, we also leverage paragraph ranking as an auxiliary task. We generate the datasets of paragraph ranking from MRQA in-domain datasets. The generated data and the details of data generation will be released at Pad"
D19-5828,N19-1423,0,0.121965,"ystem works well on both the examples drawn from the same distribution as the training • Documents: They involve passages from different sources, e.g. wikipedia, news, movies, textbook, etc. • Language Understanding Ability: They might require different language understanding abilities, e.g. matching, reasoning and arithmetic. To address the above challenge, we introduce a simple framework of pre-training and fine-tuning, namely D-NET, for improving the generalization of MRC models by exploring the following techniques: • Pre-trained Models: We leverage multiple pre-trained models, e.g. BERT (Devlin et al., 2019), XLNET (Yang et al., 2019) and ERNIE 2.0 (Sun et al., 2019). Since different pre-trained models are trained on various 1 https://github.com/PaddlePaddle/ models/tree/develop/PaddleNLP/Research/ MRQA2019-D-NET 212 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 212–219 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics Dataset SQuAD NewsQA TriviaQA SearchQA HotpotQA NaturalQuestions BioASQ DROP DuoRC RACE RelationExtraction TextbookQA BioProcess ComplexWebQuestions MCTest QAMR QAST TREC Question Sources Crowdsourced Crowdsou"
D19-5828,P17-1018,0,0.0292485,"sources, e.g. crowdsourcing workers, examine writers, search logs, synthetics, etc. Machine reading comprehension (MRC) requires machines to understand text and answer questions about the text, and it is an important task in natural language processing (NLP). With the increasing availability of large-scale datasets for MRC (Rajpurkar et al., 2016; Bajaj et al., 2016; Dunn et al., 2017; Joshi et al., 2017; He et al., 2018) and the development of deep learning techniques, MRC has achieved remarkable advancements in the last few years (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Liu et al., 2018; Wang et al., 2018; Yu et al., 2018). Although a number of neural models obtain even human parity performance on several datasets, these models may generalize poorly on other datasets (Talmor and Berant, 2019). We expect that a truly effective question answering system works well on both the examples drawn from the same distribution as the training • Documents: They involve passages from different sources, e.g. wikipedia, news, movies, textbook, etc. • Language Understanding Ability: They might require different language understanding abilities, e.g. matching, reasoning and"
D19-5828,P18-1178,1,0.840226,"examine writers, search logs, synthetics, etc. Machine reading comprehension (MRC) requires machines to understand text and answer questions about the text, and it is an important task in natural language processing (NLP). With the increasing availability of large-scale datasets for MRC (Rajpurkar et al., 2016; Bajaj et al., 2016; Dunn et al., 2017; Joshi et al., 2017; He et al., 2018) and the development of deep learning techniques, MRC has achieved remarkable advancements in the last few years (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Liu et al., 2018; Wang et al., 2018; Yu et al., 2018). Although a number of neural models obtain even human parity performance on several datasets, these models may generalize poorly on other datasets (Talmor and Berant, 2019). We expect that a truly effective question answering system works well on both the examples drawn from the same distribution as the training • Documents: They involve passages from different sources, e.g. wikipedia, news, movies, textbook, etc. • Language Understanding Ability: They might require different language understanding abilities, e.g. matching, reasoning and arithmetic. To address the above chal"
D19-5828,W18-2605,1,0.872523,"r system is ranked at top 1 of all the participants in terms of averaged F1 score. Our codes and models will be released at PaddleNLP 1 . 1 Introduction • Questions: They come from different sources, e.g. crowdsourcing workers, examine writers, search logs, synthetics, etc. Machine reading comprehension (MRC) requires machines to understand text and answer questions about the text, and it is an important task in natural language processing (NLP). With the increasing availability of large-scale datasets for MRC (Rajpurkar et al., 2016; Bajaj et al., 2016; Dunn et al., 2017; Joshi et al., 2017; He et al., 2018) and the development of deep learning techniques, MRC has achieved remarkable advancements in the last few years (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Liu et al., 2018; Wang et al., 2018; Yu et al., 2018). Although a number of neural models obtain even human parity performance on several datasets, these models may generalize poorly on other datasets (Talmor and Berant, 2019). We expect that a truly effective question answering system works well on both the examples drawn from the same distribution as the training • Documents: They involve passages from"
D19-5828,P17-1147,0,0.0547512,"these strategies. Our system is ranked at top 1 of all the participants in terms of averaged F1 score. Our codes and models will be released at PaddleNLP 1 . 1 Introduction • Questions: They come from different sources, e.g. crowdsourcing workers, examine writers, search logs, synthetics, etc. Machine reading comprehension (MRC) requires machines to understand text and answer questions about the text, and it is an important task in natural language processing (NLP). With the increasing availability of large-scale datasets for MRC (Rajpurkar et al., 2016; Bajaj et al., 2016; Dunn et al., 2017; Joshi et al., 2017; He et al., 2018) and the development of deep learning techniques, MRC has achieved remarkable advancements in the last few years (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Liu et al., 2018; Wang et al., 2018; Yu et al., 2018). Although a number of neural models obtain even human parity performance on several datasets, these models may generalize poorly on other datasets (Talmor and Berant, 2019). We expect that a truly effective question answering system works well on both the examples drawn from the same distribution as the training • Documents: They inv"
I05-1041,P98-1004,0,0.0718758,"erforms consistently better than unweighted voting on different sizes of training sets. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) [3]. Besides being used in SMT, it is also used in translation lexicon building [9], transfer rule learning [10], example-based machine translation [14], etc. In previous alignment methods, some researchers employed statistical word alignment models to build alignment links [3], [4], [8], [11], [16]. Some researchers used similarity and association measures to build alignment links [1], [15]. One issue about word alignment is how to improve the performance of a word aligner when the training data are fixed. One possible solution is to use ensemble methods [5], [6]. The ensemble methods were proposed to improve the performance of classifiers. An ensemble of classifiers is a set of classifiers whose individual decisions are combined in some way (weighted or unweighted voting) to classify new examples. Many methods for constructing ensembles have been developed [5]. One kind of methods is to resample the training examples. These methods include bagging [2], cross-validation co"
I05-1041,J93-2003,0,0.00630544,"these two methods, both weighted voting and unweighted voting are compared under the word alignment task. In addition, we analyze the effect of different sizes of training sets on the bagging method. Experimental results indicate that both bagging and cross-validation committees improve the word alignment results regardless of weighted voting or unweighted voting. Weighted voting performs consistently better than unweighted voting on different sizes of training sets. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) [3]. Besides being used in SMT, it is also used in translation lexicon building [9], transfer rule learning [10], example-based machine translation [14], etc. In previous alignment methods, some researchers employed statistical word alignment models to build alignment links [3], [4], [8], [11], [16]. Some researchers used similarity and association measures to build alignment links [1], [15]. One issue about word alignment is how to improve the performance of a word aligner when the training data are fixed. One possible solution is to use ensemble methods [5], [6]. The ensemble methods were propo"
I05-1041,P03-1012,0,0.0365007,"improve the word alignment results regardless of weighted voting or unweighted voting. Weighted voting performs consistently better than unweighted voting on different sizes of training sets. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) [3]. Besides being used in SMT, it is also used in translation lexicon building [9], transfer rule learning [10], example-based machine translation [14], etc. In previous alignment methods, some researchers employed statistical word alignment models to build alignment links [3], [4], [8], [11], [16]. Some researchers used similarity and association measures to build alignment links [1], [15]. One issue about word alignment is how to improve the performance of a word aligner when the training data are fixed. One possible solution is to use ensemble methods [5], [6]. The ensemble methods were proposed to improve the performance of classifiers. An ensemble of classifiers is a set of classifiers whose individual decisions are combined in some way (weighted or unweighted voting) to classify new examples. Many methods for constructing ensembles have been developed [5]. One kin"
I05-1041,C04-1032,0,0.0254337,"ove the word alignment results regardless of weighted voting or unweighted voting. Weighted voting performs consistently better than unweighted voting on different sizes of training sets. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) [3]. Besides being used in SMT, it is also used in translation lexicon building [9], transfer rule learning [10], example-based machine translation [14], etc. In previous alignment methods, some researchers employed statistical word alignment models to build alignment links [3], [4], [8], [11], [16]. Some researchers used similarity and association measures to build alignment links [1], [15]. One issue about word alignment is how to improve the performance of a word aligner when the training data are fixed. One possible solution is to use ensemble methods [5], [6]. The ensemble methods were proposed to improve the performance of classifiers. An ensemble of classifiers is a set of classifiers whose individual decisions are combined in some way (weighted or unweighted voting) to classify new examples. Many methods for constructing ensembles have been developed [5]. One kind of"
I05-1041,1996.amta-1.13,0,0.0738635,"r the word alignment task. In addition, we analyze the effect of different sizes of training sets on the bagging method. Experimental results indicate that both bagging and cross-validation committees improve the word alignment results regardless of weighted voting or unweighted voting. Weighted voting performs consistently better than unweighted voting on different sizes of training sets. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) [3]. Besides being used in SMT, it is also used in translation lexicon building [9], transfer rule learning [10], example-based machine translation [14], etc. In previous alignment methods, some researchers employed statistical word alignment models to build alignment links [3], [4], [8], [11], [16]. Some researchers used similarity and association measures to build alignment links [1], [15]. One issue about word alignment is how to improve the performance of a word aligner when the training data are fixed. One possible solution is to use ensemble methods [5], [6]. The ensemble methods were proposed to improve the performance of classifiers. An ensemble of classifiers is a s"
I05-1041,W01-1406,0,0.0657252,"n addition, we analyze the effect of different sizes of training sets on the bagging method. Experimental results indicate that both bagging and cross-validation committees improve the word alignment results regardless of weighted voting or unweighted voting. Weighted voting performs consistently better than unweighted voting on different sizes of training sets. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) [3]. Besides being used in SMT, it is also used in translation lexicon building [9], transfer rule learning [10], example-based machine translation [14], etc. In previous alignment methods, some researchers employed statistical word alignment models to build alignment links [3], [4], [8], [11], [16]. Some researchers used similarity and association measures to build alignment links [1], [15]. One issue about word alignment is how to improve the performance of a word aligner when the training data are fixed. One possible solution is to use ensemble methods [5], [6]. The ensemble methods were proposed to improve the performance of classifiers. An ensemble of classifiers is a set of classifiers whose indiv"
I05-1041,P00-1056,0,0.15695,"he word alignment results regardless of weighted voting or unweighted voting. Weighted voting performs consistently better than unweighted voting on different sizes of training sets. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) [3]. Besides being used in SMT, it is also used in translation lexicon building [9], transfer rule learning [10], example-based machine translation [14], etc. In previous alignment methods, some researchers employed statistical word alignment models to build alignment links [3], [4], [8], [11], [16]. Some researchers used similarity and association measures to build alignment links [1], [15]. One issue about word alignment is how to improve the performance of a word aligner when the training data are fixed. One possible solution is to use ensemble methods [5], [6]. The ensemble methods were proposed to improve the performance of classifiers. An ensemble of classifiers is a set of classifiers whose individual decisions are combined in some way (weighted or unweighted voting) to classify new examples. Many methods for constructing ensembles have been developed [5]. One kind of method"
I05-1041,J96-1001,0,0.0489376,"alignment results on the training data. As described in Section 3.1, on each bootstrap replicate j, we train a word aligner M stj in the source to target direction and a word aligner M tsj in the target to source direction. That is to say, we obtain two different word alignment sets S stj and S tsj for each of the bootstrap replicate. For each word alignment link ( s, t ) produced by M stj or M tsj , we calculate its weight as shown in (3). This weight measures the association of the source part and the target part in an alignment link. This measure is like the Dice Coefficient. Smadja et al. [13] showed that the Dice Coefficient is a good indicator of translation association. Wi ( s, t ) = 2 * count ( s, t ) ∑ count(s, t &apos; ) + ∑ count(s&apos; , t ) t&apos; (3) s&apos; Where, count ( s, t ) is the occurring frequency of the alignment link ( s, t ) ∈ S stj ∪ S tsj . 6 Experiments 6.1 Training and Testing Set We perform experiments on a sentence aligned English-Chinese bilingual corpus in general domain. There are about 320,000 bilingual sentence pairs in the corpus, from which, we randomly select 1,000 sentence pairs as testing data. The remainder is used as training data. In the sentence pairs, the a"
I05-1041,tufis-barbu-2002-lexical,0,0.0266233,"ms consistently better than unweighted voting on different sizes of training sets. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) [3]. Besides being used in SMT, it is also used in translation lexicon building [9], transfer rule learning [10], example-based machine translation [14], etc. In previous alignment methods, some researchers employed statistical word alignment models to build alignment links [3], [4], [8], [11], [16]. Some researchers used similarity and association measures to build alignment links [1], [15]. One issue about word alignment is how to improve the performance of a word aligner when the training data are fixed. One possible solution is to use ensemble methods [5], [6]. The ensemble methods were proposed to improve the performance of classifiers. An ensemble of classifiers is a set of classifiers whose individual decisions are combined in some way (weighted or unweighted voting) to classify new examples. Many methods for constructing ensembles have been developed [5]. One kind of methods is to resample the training examples. These methods include bagging [2], cross-validation committe"
I05-1041,J97-3002,0,0.061287,"d alignment results regardless of weighted voting or unweighted voting. Weighted voting performs consistently better than unweighted voting on different sizes of training sets. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) [3]. Besides being used in SMT, it is also used in translation lexicon building [9], transfer rule learning [10], example-based machine translation [14], etc. In previous alignment methods, some researchers employed statistical word alignment models to build alignment links [3], [4], [8], [11], [16]. Some researchers used similarity and association measures to build alignment links [1], [15]. One issue about word alignment is how to improve the performance of a word aligner when the training data are fixed. One possible solution is to use ensemble methods [5], [6]. The ensemble methods were proposed to improve the performance of classifiers. An ensemble of classifiers is a set of classifiers whose individual decisions are combined in some way (weighted or unweighted voting) to classify new examples. Many methods for constructing ensembles have been developed [5]. One kind of methods is t"
I05-1041,wu-wang-2004-improving-domain,1,0.343875,"Missing"
I05-1041,2001.mtsummit-ebmt.4,0,\N,Missing
I05-1041,C98-1004,0,\N,Missing
P03-1016,P01-1008,0,0.168805,"Missing"
P03-1016,J93-2003,0,0.00566892,"Missing"
P03-1016,P98-2127,0,0.28292,"ap between the query space and the document space. For instance, “buy book” extracted from the users’ query should also in some way match “order book” indexed in the documents. Besides, the synonymous expressions are also important in language generation (Langkilde and Knight, 1998) and computer assisted authoring to produce vivid texts. Up to now, there have been few researches which directly address the problem of extracting synonymous collocations. However, a number of studies investigate the extraction of synonymous words from monolingual corpora (Carolyn et al., 1992; Grefenstatte, 1994; Lin, 1998; Gasperin et al., 2001). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous. In addition, some studies investigate the extraction of synonymous words and/or patterns from bilingual corpora (Barzilay and Mckeown, 2001; Shimohata and Sumita, 2002). However, these methods can only extract synonymous expressions which occur in the bilingual corpus. Due to the limited size of the bi"
P03-1016,shimohata-sumita-2002-automatic,0,0.0239679,"Missing"
P03-1016,O01-2001,1,0.862821,"Missing"
P03-1016,J96-1001,0,\N,Missing
P03-1016,J93-1007,0,\N,Missing
P03-1016,J96-1002,0,\N,Missing
P03-1016,P04-1022,1,\N,Missing
P03-1016,P98-1116,0,\N,Missing
P03-1016,C98-1112,0,\N,Missing
P03-1016,O04-2001,0,\N,Missing
P03-1016,O04-1027,0,\N,Missing
P03-1016,C02-1084,0,\N,Missing
P03-1016,E99-1005,0,\N,Missing
P03-1016,C98-2122,0,\N,Missing
P04-3002,P98-1004,0,0.081698,"Missing"
P04-3002,J93-2003,0,0.00754756,"Missing"
P04-3002,P03-1012,0,0.0492941,"Missing"
P04-3002,J93-1003,0,0.0206975,"order to filter some noise caused by the error alignment links, we only retain those translation pairs whose translation probabilities are above a threshold δ 1 or co-occurring frequencies are above a threshold δ 2 . When we train the IBM statistical word alignment model with a limited bilingual corpus in the specific domain, we build another translation dictionary D 2 with the same method as for the dictionary D1 . But we adopt a different filtering strategy for the translation dictionary D 2 . We use log-likelihood ratio to estimate the association strength of each translation pair because Dunning (1993) proved that log-likelihood ratio performed very well on small-scale data. Thus, we get the translation dictionary D 2 by keeping those entries whose log-likelihood ratio scores are greater than a threshold δ 3 . 2.3 Word Alignment Adaptation Algorithm Based on the bi-directional word alignment, we define SI as SI = SG ∩ SF and UG as UG = PG ∪ PF − SI . The word alignment links in the set SI are very reliable. Thus, we directly accept them as correct links and add them into the final alignment set WA . Input: Alignment set SI and UG (1) For alignment links in SI , we directly add them into the"
P04-3002,J97-2004,0,0.0743463,"Missing"
P04-3002,P00-1056,0,0.645519,"in the specific domain respectively, and then improves the domain-specific word alignment with these two models. Experimental results show a significant improvement in terms of both alignment precision and recall. And the alignment results are applied in a computer assisted translation system to improve human translation efficiency. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al., 1993). In previous alignment methods, some researchers modeled the alignments with different statistical models (Wu, 1997; Och and Ney, 2000; Cherry and Lin, 2003). Some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufis and Barbu, 2002). However, All of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997). However, few works address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available. This paper addres"
P04-3002,2001.mtsummit-papers.60,0,0.029265,"Missing"
P04-3002,tufis-barbu-2002-lexical,0,0.0244294,"Missing"
P04-3002,J97-3002,0,0.048517,"Missing"
P04-3002,C98-1004,0,\N,Missing
P05-1058,J93-1003,0,0.0333044,"When we train the bi-directional statistical word alignment models with the training data, we get two word alignment results for the training data. By taking the intersection of the two word alignment results, we build a new alignment set. The alignment links in this intersection set are extended by iteratively adding word alignment links into it as described in (Och and Ney, 2000). Based on the extended alignment links, we build a translation dictionary. In order to filter the noise caused by the error alignment links, we only retain those translation pairs whose log-likelihood ratio scores (Dunning, 1993) are above a threshold. Based on the alignment results on the 470 out-of-domain corpus, we build a translation dictionary D1 filtered with a threshold δ 1 . Based on the alignment results on a small-scale in-domain corpus, we build another translation dictionary D 2 filtered with a threshold δ 2 . After obtaining the two dictionaries, we combine two dictionaries through linearly interpolating the translation probabilities in the two dictionaries, which is shown in (11). The symbols f and e represent a single word or a phrase in the source and target languages. This differs from the translation"
P05-1058,J97-2004,0,0.0597441,"on Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links. In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997). However, only a few studies (Wu and Wang, 2004) directly address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available. In this paper, we address the problem of word alignment in a specific domain, in which only a small-scale corpus is available. In the domain-specific (in-domain) corpus, there are two kinds of words: general words, which also frequently occur in the out-of-domain corpus, and domain-specific words, which only occur in the specific domain. Thus, we can use the out"
P05-1058,P97-1063,0,0.0166212,"orithms include two parts: a training algorithm and a testing algorithm. The training algorithm is shown in Figure 1. After getting the two adaptation models and the translation dictionary, we apply them to the in-domain corpus to perform word alignment. Here we call it testing algorithm. The detailed algorithm is shown in Figure 2. For each sentence pair, there are two different word alignment results, from which the final alignment links are selected according to their translation probabilities in the dictionary D. The selection order is similar to that in the competitive linking algorithm (Melamed, 1997). The difference is that we allow many-to-one and one-to-many alignments. 6 Evaluation We compare our method with four other methods. The first method is descried in (Wu and Wang, 2004). We call it “Result Adaptation (ResAdapt)”. 3 We also tried an adaptation coefficient to calculate the interpolation weight as in (8). However, the alignment results are not improved by using this coefficient for the dictionary. models using the training data. Then we build a translation dictionary based on the alignment results on the training data and filter it using log-likelihood ratio as described in secti"
P05-1058,P00-1056,0,0.487452,"j ≠ 0 j =1, a j ≠ 0 1 A cept is defined as the set of target words connected to a source word (Brown et al., 1993). 468 , ρ i = max{i &apos; : φ i &apos; > 0 ∧ 0 &lt; i &apos; &lt; i} ; else ρ i = 0 . (τ ,π ) l |{i &apos; : φ i &apos; > 0 ∧ 0 &lt; i &apos; &lt; i} |> 0 If However, both model 3 and model 4 do not take the multiword cept into account. Only one-to-one and many-to-one word alignments are considered. Thus, some multi-word units in the domain-specific corpus cannot be correctly aligned. In order to deal with this problem, we perform word alignment in two directions (source to target, and target to source) as described in (Och and Ney, 2000). The GIZA++ toolkit2 is used to perform statistical word alignment. We use SG1 and SG2 to represent the bi-directional alignment sets, which are shown in Equation (4) and (5). For alignment in both sets, we use j for source words and i for target words. If a target word in position i is connected to source words in positions j1 and j 2 , then Ai = { j1 , j 2 } . We call an element in the alignment set an alignment link. 3 SG1 = {( Ai , i) |Ai = { j |a j = i, a j ≥ 0}} (4) SG 2 = {( j , A j ) |A j = {i |i = a j , a j ≥ 0}} (5) Word Alignment Model Adaptation In this paper, we first train two m"
P05-1058,J96-1001,0,0.0535151,"Missing"
P05-1058,tufis-barbu-2002-lexical,0,0.0255598,"Missing"
P05-1058,J97-3002,0,0.0809803,"Missing"
P05-1058,wu-wang-2004-improving-domain,1,0.643727,"iate result of statistical machine translation (Brown et al., 1993). In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links. In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997). However, only a few studies (Wu and Wang, 2004) directly address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available. In this paper, we address the problem of word alignment in a specific domain, in which only a small-scale corpus is available. In the domain-specific (in-domain) corpus, there are two kinds of words: general words, which also frequently occur in the out-of-domain corpus, and domain-specific words, which only occur in the specific domain. Thus, we can use the out-of-domain bilingual corpus to improve the alignm"
P05-1058,J93-2003,0,\N,Missing
P05-1058,P03-1012,0,\N,Missing
P05-1058,P98-1004,0,\N,Missing
P05-1058,C98-1004,0,\N,Missing
P06-2112,W05-0819,0,0.133738,"Missing"
P06-2112,J93-2003,0,0.0221963,"Missing"
P06-2112,P03-1012,0,0.0439111,"Missing"
P06-2112,J97-2004,0,0.0671268,"nly using the small bilingual corpus in L1 and L2. 1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). Many researchers build alignment links with bilingual corpora (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Zhang and Gildea, 2005). In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is unavailable, some researchers acquired class-based alignment rules with existing dictionaries to improve word alignment (Ker and Chang, 1997). Wu et al. (2005) used a large-scale bilingual corpus in general domain to improve domain-specific word alignment when only a small-scale domainspecific bilingual corpus is available. This paper proposes an approach to improve word alignment for languages with scarce resources using bilingual corpora of other language pairs. To perform word alignment between languages L1 and L2, we introduce a third language L3 as the pivot language. Although only small amounts of bilingual data are available for the desired language pair L1-L2, large-scale bilingual corpora in L1-L3 and L2-L3 are available."
P06-2112,W05-0812,0,0.0732401,"Missing"
P06-2112,W05-0809,0,0.0478672,"Missing"
P06-2112,W02-2026,0,0.0964339,"nse of an instance of the ambiguous English word e can be determined by the context in which the instance appears. Thus, the cross-language word similarity between the Chinese word c and the Japanese word f can be calculated according to the contexts of their English translation e. We use the feature vector constructed using the context words in the English sentence to represent the context. So we can calculate the cross-language word similarity using the feature vectors. The detailed algorithm is shown in figure 1. This idea is similar to translation lexicon extraction via a bridge language (Schafer and Yarowsky, 2002). For example, the Chinese word ""河岸"" and its English translation ""bank"" (the border of a river) appears in the following Chinese-English sentence pair: (a) 他们沿着河岸走回家。 (b) They walked home along the river bank. The Japanese word "" 銀 行 "" and its English translation ""bank"" (a financial organization) appears in the following English-Japanese sentence pair: (c) He has plenty of money in the bank. (d) 彼は銀行預金が相当ある。 The context words of the English word ""bank"" in sentences (b) and (c) are quite different. The difference indicates the cross language word similarity of the Chinese word ""河岸"" and the Japa"
P06-2112,W05-0817,0,0.14113,"Missing"
P06-2112,P00-1056,0,0.245002,"et Words 237,834 4,480,034 1,685,204 Besides the training data, we also have heldout data and testing data. The held-out data includes 500 Chinese-Japanese sentence pairs, which is used to set the interpolated weights described in section 5. We use another 1,000 Chinese-Japanese sentence pairs as testing data, which is not included in the training data and the held-out data. The alignment links in the held-out data and the testing data are manually annotated. Testing data includes 4,926 alignment links 2 . We use the same metrics as described in Wu et al. (2005), which is similar to those in (Och and Ney, 2000). The difference lies in that Wu et al. (2005) took all alignment links as sure links. If we use S G to represent the set of alignment links identified by the proposed methods and S C to denote the reference alignment set, the methods to calculate the precision, recall, f-measure, and alignment error rate (AER) are shown in equations (18), (19), (20), and (21), respectively. It can be seen that the higher the f-measure is, the lower the alignment error rate is. Thus, we will only show precision, recall and AER scores in the evaluation results. recall = |SG ∩ SC | |SG | |SG ∩ SC | |SC | (18) (1"
P06-2112,J03-1002,0,0.0118618,"Missing"
P06-2112,J97-3002,0,0.110144,"Missing"
P06-2112,P05-1058,1,0.888651,"Missing"
P06-2112,P05-1059,0,0.0222278,"Missing"
P06-2117,J93-2003,0,0.0177669,"Missing"
P06-2117,P03-1012,0,0.0418338,"Missing"
P06-2117,W99-0613,0,0.0619412,"by automatically building a pseudo reference set for the unlabeled data to improve alignment results. In fact, large amounts of unlabeled data are available without difficulty, while labeled data is costly to obtain. However, labeled data is valuable to improve performance of learners. Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation (Yarowsky, 1995; Pham et al., 2005), classification (Blum and Mitchell, 1998; Thorsten, 1999), clustering (Basu et al., 2004), named entity classification (Collins and Singer, 1999), and parsing (Sarkar, 2001). In this paper, we propose a semi-supervised boosting method to improve statistical word alignment with both limited labeled data and large amounts of unlabeled data. The proposed approach modifies the supervised AdaBoost algorithm to a semi-supervised learning algorithm by incorporating the unlabeled data. Therefore, it should address the following three problems. The first is to build a word alignment model with both labeled and unlabeled data. In this paper, with the labeled data, we build a supervised model by directly estimating the parameters in 913 Proceedin"
P06-2117,P00-1056,0,0.779615,"ly 2006. 2006 Association for Computational Linguistics the model instead of using the Expectation Maximization (EM) algorithm in Brown et al. (1993). With the unlabeled data, we build an unsupervised model by estimating the parameters with the EM algorithm. Based on these two word alignment models, an interpolated model is built through linear interpolation. This interpolated model is used as a learner in the semi-supervised AdaBoost algorithm. The second is to build a reference set for the unlabeled data. It is automatically built with a modified &quot;refined&quot; combination method as described in Och and Ney (2000). The third is to calculate the error rate on each round. Although we build a reference set for the unlabeled data, it still contains alignment errors. Thus, we use the reference set of the labeled data instead of that of the entire training data to calculate the error rate on each round. With the interpolated model as a learner in the semi-supervised AdaBoost algorithm, we investigate two boosting methods in this paper to improve statistical word alignment. The first method uses the unlabeled data only in the interpolated model. During training, it only changes the distribution of the labeled"
P06-2117,J03-1002,0,0.0172725,"Missing"
P06-2117,N01-1023,0,0.0160004,"ence set for the unlabeled data to improve alignment results. In fact, large amounts of unlabeled data are available without difficulty, while labeled data is costly to obtain. However, labeled data is valuable to improve performance of learners. Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation (Yarowsky, 1995; Pham et al., 2005), classification (Blum and Mitchell, 1998; Thorsten, 1999), clustering (Basu et al., 2004), named entity classification (Collins and Singer, 1999), and parsing (Sarkar, 2001). In this paper, we propose a semi-supervised boosting method to improve statistical word alignment with both limited labeled data and large amounts of unlabeled data. The proposed approach modifies the supervised AdaBoost algorithm to a semi-supervised learning algorithm by incorporating the unlabeled data. Therefore, it should address the following three problems. The first is to build a word alignment model with both labeled and unlabeled data. In this paper, with the labeled data, we build a supervised model by directly estimating the parameters in 913 Proceedings of the COLING/ACL 2006 Ma"
P06-2117,J97-3002,0,0.0577505,"Missing"
P06-2117,2005.mtsummit-papers.41,1,0.850989,"he performances of the word aligners with available data and available alignment models. One possible solution is to use the boosting method (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 2000). The underlying idea of boosting is to combine simple &quot;rules&quot; to form an ensemble such that the performance of the single ensemble is improved. The AdaBoost (Adaptive Boosting) algorithm by Freund and Schapire (1996) was developed for supervised learning. When it is applied to word alignment, it should solve the problem of building a reference set for the unlabeled data. Wu and Wang (2005) developed an unsupervised AdaBoost algorithm by automatically building a pseudo reference set for the unlabeled data to improve alignment results. In fact, large amounts of unlabeled data are available without difficulty, while labeled data is costly to obtain. However, labeled data is valuable to improve performance of learners. Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation (Yarowsky, 1995; Pham et al., 2005), classification (Blum and Mitchell, 1998; Thorsten, 1999), clustering (Bas"
P06-2117,P05-1058,1,0.884677,"Missing"
P06-2117,P95-1026,0,0.0797415,"alignment, it should solve the problem of building a reference set for the unlabeled data. Wu and Wang (2005) developed an unsupervised AdaBoost algorithm by automatically building a pseudo reference set for the unlabeled data to improve alignment results. In fact, large amounts of unlabeled data are available without difficulty, while labeled data is costly to obtain. However, labeled data is valuable to improve performance of learners. Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation (Yarowsky, 1995; Pham et al., 2005), classification (Blum and Mitchell, 1998; Thorsten, 1999), clustering (Basu et al., 2004), named entity classification (Collins and Singer, 1999), and parsing (Sarkar, 2001). In this paper, we propose a semi-supervised boosting method to improve statistical word alignment with both limited labeled data and large amounts of unlabeled data. The proposed approach modifies the supervised AdaBoost algorithm to a semi-supervised learning algorithm by incorporating the unlabeled data. Therefore, it should address the following three problems. The first is to build a word alignmen"
P06-2117,P05-1059,0,0.0246094,"Missing"
P07-1108,W05-0819,0,0.0168646,"word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some researchers (Aswani and Gaizauskas, 2005; Lopez and Resnik, 2005; Tufis et al., 2005) used language-dependent resources such as dictionary, thesaurus, and dependency parser to improve word alignment results. 857 In this paper, we address the translation problem for language pairs with scarce resources by bringing in a pivot language, via which we can make use of large bilingual corpora. Our method does not need language-dependent resources or deep linguistic processing. Thus, the method is easy to be adapted to any language pair where a pivot language and corresponding large bilingual corpora are available. 3 Phrase-Based SMT Accord"
P07-1108,J93-2003,0,0.0513623,"Missing"
P07-1108,N06-1003,0,0.0365215,"two kinds of methods: those using pivot language and those using a small bilingual corpus or scarce resources. For the first kind, pivot languages are employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001; Kishida and Kando, 2003). These methods only used the available dictionaries to perform word by word translation. In addition, NTCIR 4 workshop organized a shared task for CLIR using pivot language. Machine translation systems are used to translate queries into pivot language sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-sy"
P07-1108,2000.eamt-1.5,0,0.099614,"Missing"
P07-1108,C00-1015,0,0.0314286,"ind, pivot languages are employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001; Kishida and Kando, 2003). These methods only used the available dictionaries to perform word by word translation. In addition, NTCIR 4 workshop organized a shared task for CLIR using pivot language. Machine translation systems are used to translate queries into pivot language sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) us"
P07-1108,P05-1033,0,0.0520512,"Missing"
P07-1108,P03-1021,0,0.0523928,"and English-Le since Europarl corpus is a multilingual corpus. For the language models, we use the same data provided in the shared task. We also use the same development set and test set provided by the shared task. The in-domain test set includes 2,000 sentences and the out-of-domain test set includes 1,064 sentences for each language. 6.2 Translation Method System and Evaluation To perform phrase-based SMT, we use Koehn&apos;s training scripts1 and the Pharaoh decoder (Koehn, 2004). We run the decoder with its default settings and then use Koehn&apos;s implementation of minimum error rate training (Och, 2003) to tune the feature weights on the development set. The translation quality was evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). And we also use the tool provided in the NAACL/HLT 2006 shared task on SMT to calculate the BLEU scores. 6.3 Comparison of Different Lexical Weights As described in section 4, we employ two methods to estimate the lexical weight in the translation model. In order to compare the two methods, we translate from French to Spanish, using English as the pivot language. We use the French-English and English-Spanish corpora described"
P07-1108,P02-1033,0,0.00633302,"sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some researchers (Aswani and Gaizauskas, 2005; Lopez and Resnik, 200"
P07-1108,J04-4002,0,0.0289727,"Missing"
P07-1108,koen-2004-pharaoh,0,0.0244223,"3,134,411 12,155,876 15,222,105 16,052,269 14,362,615 Table 1. Training Corpus for European Languages extracted from Lf-English and English-Le since Europarl corpus is a multilingual corpus. For the language models, we use the same data provided in the shared task. We also use the same development set and test set provided by the shared task. The in-domain test set includes 2,000 sentences and the out-of-domain test set includes 1,064 sentences for each language. 6.2 Translation Method System and Evaluation To perform phrase-based SMT, we use Koehn&apos;s training scripts1 and the Pharaoh decoder (Koehn, 2004). We run the decoder with its default settings and then use Koehn&apos;s implementation of minimum error rate training (Och, 2003) to tune the feature weights on the development set. The translation quality was evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). And we also use the tool provided in the NAACL/HLT 2006 shared task on SMT to calculate the BLEU scores. 6.3 Comparison of Different Lexical Weights As described in section 4, we employ two methods to estimate the lexical weight in the translation model. In order to compare the two methods, we translate"
P07-1108,2005.mtsummit-papers.11,0,0.0230044,"( f |e) and p w,0 ( f |e, a) denote the phrase translation probability and lexical weight trained with the Lf-Le bilingual corpus, respectively. φi ( f |e) and p w,i ( f |e, a ) ( i = 1,..., n ) are the phrase translation probability and lexical weight estimated by using the pivot languages. α i and β i are the interpolation coefficients. 6 6.1 Experiments on the Europarl Corpus Data A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006). The shared task used the Europarl corpus (Koehn, 2005), in which four languages are involved: English, French, Spanish, and German. The shared task performed translation between English and the other three languages. In our work, we perform translation from French to the other three languages. We select French to Spanish and French to German translation that are not in the shared task because we want to use English as the pivot language. In general, for most of the languages, there exist bilingual corpora between these languages and English since English is an internationally used language. Table 1 shows the information about the bilingual traini"
P07-1108,W06-3114,0,0.0171445,"=0 n p w ( f |e, a) = ∑ β i p w,i ( f |e, a) (11) i =0 Where φ 0 ( f |e) and p w,0 ( f |e, a) denote the phrase translation probability and lexical weight trained with the Lf-Le bilingual corpus, respectively. φi ( f |e) and p w,i ( f |e, a ) ( i = 1,..., n ) are the phrase translation probability and lexical weight estimated by using the pivot languages. α i and β i are the interpolation coefficients. 6 6.1 Experiments on the Europarl Corpus Data A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006). The shared task used the Europarl corpus (Koehn, 2005), in which four languages are involved: English, French, Spanish, and German. The shared task performed translation between English and the other three languages. In our work, we perform translation from French to the other three languages. We select French to Spanish and French to German translation that are not in the shared task because we want to use English as the pivot language. In general, for most of the languages, there exist bilingual corpora between these languages and English since English is an internationally used language."
P07-1108,N03-1017,0,0.28741,"5) used language-dependent resources such as dictionary, thesaurus, and dependency parser to improve word alignment results. 857 In this paper, we address the translation problem for language pairs with scarce resources by bringing in a pivot language, via which we can make use of large bilingual corpora. Our method does not need language-dependent resources or deep linguistic processing. Thus, the method is easy to be adapted to any language pair where a pivot language and corresponding large bilingual corpora are available. 3 Phrase-Based SMT According to the translation model presented in (Koehn et al., 2003), given a source sentence f , the best target translation e best can be obtained according to the following model e best = arg max e p (e |f ) (1) = arg max e p (f |e ) pLM (e )ω length ( e ) Where the translation model p (f |e ) can be decomposed into I I p ( f 1 |e1 ) = I ∏φ( f i |e i )d (a i − bi −1 ) p w ( f i |e i , a) λ (2) i =1 Where φ ( f i |e i ) and d (ai − bi −1 ) denote phrase translation probability and distortion probability, respectively. p w ( f i |e i , a ) is the lexical weight, and λ is the strength of the lexical weight. 4 Phrase-Based SMT Via Pivot Language This section wi"
P07-1108,W05-0812,0,0.00821932,"iab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some researchers (Aswani and Gaizauskas, 2005; Lopez and Resnik, 2005; Tufis et al., 2005) used language-dependent resources such as dictionary, thesaurus, and dependency parser to improve word alignment results. 857 In this paper, we address the translation problem for language pairs with scarce resources by bringing in a pivot language, via which we can make use of large bilingual corpora. Our method does not need language-dependent resources or deep linguistic processing. Thus, the method is easy to be adapted to any language pair where a pivot language and corresponding large bilingual corpora are available. 3 Phrase-Based SMT According to the translation m"
P07-1108,W05-0809,0,0.0684983,"Missing"
P07-1108,P04-1083,0,0.0852707,"Missing"
P07-1108,P05-1034,0,0.0286919,"Missing"
P07-1108,W02-2026,0,0.0103997,"ystems are used to translate queries into pivot language sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some researchers"
P07-1108,P06-2112,1,0.843722,"ges are employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001; Kishida and Kando, 2003). These methods only used the available dictionaries to perform word by word translation. In addition, NTCIR 4 workshop organized a shared task for CLIR using pivot language. Machine translation systems are used to translate queries into pivot language sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictiona"
P07-1108,W05-0817,0,0.00821302,"nd so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some researchers (Aswani and Gaizauskas, 2005; Lopez and Resnik, 2005; Tufis et al., 2005) used language-dependent resources such as dictionary, thesaurus, and dependency parser to improve word alignment results. 857 In this paper, we address the translation problem for language pairs with scarce resources by bringing in a pivot language, via which we can make use of large bilingual corpora. Our method does not need language-dependent resources or deep linguistic processing. Thus, the method is easy to be adapted to any language pair where a pivot language and corresponding large bilingual corpora are available. 3 Phrase-Based SMT According to the translation model presented in (Ko"
P07-1108,J97-3002,0,0.0177418,"Missing"
P07-1108,P01-1067,0,0.020506,"Missing"
P07-1108,J03-3002,0,\N,Missing
P07-1108,P02-1040,0,\N,Missing
P07-1108,N07-1061,0,\N,Missing
P07-1108,J04-2003,0,\N,Missing
P07-1108,P02-1038,0,\N,Missing
P07-1108,2006.eamt-1.24,0,\N,Missing
P07-1108,vandeghinste-etal-2006-metis,0,\N,Missing
P07-1108,2006.amta-papers.11,0,\N,Missing
P09-1006,abeille-etal-2000-building,0,0.0765618,"Missing"
P09-1006,W00-1201,0,0.0822755,"Missing"
P09-1006,N07-1051,0,0.068459,"Missing"
P09-1006,D08-1092,0,0.11849,"Missing"
P09-1006,A00-2018,0,0.602877,"Missing"
P09-1006,P07-1078,0,0.0304062,"Missing"
P09-1006,P05-1022,0,0.22895,"Missing"
P09-1006,N03-1027,0,0.0627931,"Missing"
P09-1006,P94-1034,0,0.283773,"a higher performance when in-domain data was weighted more heavily than out-of-domain data. McClosky et al. (2006b) used self-training and corpus weighting to adapt their parser trained on WSJ corpus to Brown corpus. Their results indicated that both unlabeled in-domain data and labeled out-of-domain data can help domain adaptation. In comparison with these works, we conduct our study in a different setting where we work with multiple heterogeneous treebanks. Grammar formalism conversion makes it possible to reuse existing source treebanks for the study of target grammar parsing. Wang et al. (1994) employed a parser to help conversion of a treebank from a simple phrase structure to a more informative phrase structure and then used this converted treebank to train their parser. Collins et al. (1999) performed statistical constituency parsing of Czech on a treebank that was converted from the Prague Dependency Treebank under the guidance of conversion rules and heuristic rules, e.g., one level of projection for any category, minimal projection for any dependents, and fixed position of attachment. Xia and Palmer (2001) adopted better heuristic rules to build converted trees, which Availabl"
P09-1006,C02-1126,0,0.0609279,"Missing"
P09-1006,W01-0904,0,0.0582808,"Missing"
P09-1006,P99-1065,0,0.772284,"). It is important to acquire additional labeled data for the target grammar parsing through exploitation of existing source treebanks since there is often a shortage of labeled data. However, to our knowledge, there is no previous study on this issue. Recently there have been some works on using multiple treebanks for domain adaptation of parsers, where these treebanks have the same grammar formalism (McClosky et al., 2006b; Roark and Bacchiani, 2003). Other related works focus on converting one grammar formalism of a treebank to another and then conducting studies on the converted treebank (Collins et al., 1999; Forst, 2003; Wang et al., 1994; Watkinson and Manandhar, 2001). These works were done either on multiple treebanks with the same grammar formalism or on only one converted treebank. We see that their scenarios are different from ours as we work with multiple heterogeneous treebanks. For the use of heterogeneous treebanks1 , we propose a two-step solution: (1) converting the grammar formalism of the source treebank to the target one, (2) refining converted trees and using them as additional training data to build a target grammar parser. For grammar formalism conversion, we choose the DS to P"
P09-1006,H01-1014,0,0.695341,"works were done either on multiple treebanks with the same grammar formalism or on only one converted treebank. We see that their scenarios are different from ours as we work with multiple heterogeneous treebanks. For the use of heterogeneous treebanks1 , we propose a two-step solution: (1) converting the grammar formalism of the source treebank to the target one, (2) refining converted trees and using them as additional training data to build a target grammar parser. For grammar formalism conversion, we choose the DS to PS direction for the convenience of the comparison with existing works (Xia and Palmer, 2001; Xia et al., 2008). Specifically, we assume that the source grammar formalism is dependency We address the issue of using heterogeneous treebanks for parsing by breaking it down into two sub-problems, converting grammar formalisms of the treebanks to the same one, and parsing on these homogeneous treebanks. First we propose to employ an iteratively trained target grammar parser to perform grammar formalism conversion, eliminating predefined heuristic rules as required in previous methods. Then we provide two strategies to refine conversion results, and adopt a corpus weighting technique for p"
P09-1006,W03-2404,0,0.113977,"Missing"
P09-1006,han-etal-2002-development,0,0.0632138,"Missing"
P09-1006,I05-1007,0,0.0333223,"Missing"
P09-1006,P03-1056,0,0.0665153,"Missing"
P09-1006,J93-2004,0,0.0348317,"Missing"
P09-1006,N06-1020,0,0.0892901,"Missing"
P09-1006,P06-1043,0,0.102023,"Missing"
P09-1006,P06-1054,0,\N,Missing
P09-1006,N04-1032,0,\N,Missing
P09-1018,P07-1038,0,0.0535445,"to the pseudo references) to a score that indicates the quality of the translation. Scores are first generated independently for each translation, then the translations are ranked by their respective scores. The candidate with the highest score is selected as the final translation. This is achieved by optimizing the regression learning model’s output to correlate against a set of training examples, where the source sentences are provided with several reference translations, instead of manually labeling the translations produced by various systems with quantitative assessments as described in (Albrecht and Hwa, 2007; Duh, 2008). The advantage of our method is that we do not need to manually label the translations produced by each translation system, therefore enabling our method suitable for translation selection among any systems without additional manual work. 2 Pivot Methods for Phrase-based SMT 2.1 Triangulation Method Following the method described in Wu and Wang (2007), we train the source-pivot and pivot-target translation models using the source-pivot and pivot-target corpora, respectively. Based on these two models, we induce a source-target translation model, in which two important elements nee"
P09-1018,P03-1021,0,0.0771557,"m target sentences ti1 , ti2 , ..., tim . We rescore all the n × m candidates using both the source-pivot and pivot-target translation scores following the method described in Utiyama and Isahara (2007). If we use hf p and hpt to denote the features in the source-pivot and pivot-target systems, respectively, we get the optimal target translation according to the following formula. tˆ = argmax t L X sp pt pt (λsp k hk (s, p)+λk hk (p, t)) (4) k=1 Where L is the number of features used in SMT systems. λsp and λpt are feature weights set by performing minimum error rate training as described in Och (2003). 2.3 Synthetic Method There are two possible methods to obtain a sourcetarget corpus using the source-pivot and pivottarget corpora. One is to obtain target translations for the source sentences in the source-pivot corpus. This can be achieved by translating the pivot sentences in source-pivot corpus to target sentences with the pivot-target SMT system. The other is to obtain source translations for the target sentences in the pivot-target corpus using the pivot-source SMT system. And we can combine these two source-target corpora to produced a final synthetic corpus. Given a pivot sentence,"
P09-1018,2008.iwslt-papers.1,0,0.240307,"d Lapata 2007; Wu and Wang, 2007). It multiples corresponding translation probabilities and lexical weights in source-pivot and pivot-target translation models to induce a new source-target phrase table. We name it the triangulation method. The second is the sentence translation strategy, which first translates the source sentence to the pivot sentence, and then to the target sentence (Utiyama and Isahara, 2007; Khalilov et al., 2008). We name it the transfer method. The third is to use existing models to build a synthetic source-target corpus, from which a source-target model can be trained (Bertoldi et al., 2008). For example, we can obtain a source-pivot corpus by translating the pivot sentence in the source-pivot corpus into the target language with pivot-target translation models. We name it the synthetic method. The working condition with the pivot language approach is that the source-pivot and pivot-target parallel corpora are independent, in the sense that they are not derived from the same set of sentences, namely independently sourced corpora. Thus, some linguistic phenomena in the sourcepivot corpus will lost if they do not exist in the pivot-target corpus, and vice versa. In order to fill up"
P09-1018,P02-1040,0,0.0974401,"ative values of the scores assigned by the subject systems may change. In order to train a reliable learner, we need to prepare a balanced training set, where the translations produced by different systems under different conditions are required to be manually evaluated. In extreme cases, we need to relabel the training data to obtain better performance. In this paper, we modify the method in Albrecht and Hwa (2007) to only prepare human reference translations for the training examples, and then evaluate the translations produced by the subject systems against the references using BLEU score (Papineni et al., 2002). We use smoothed sentence-level BLEU score to replace the human assessments, where we use additive smoothing to avoid zero BLEU scores when we calculate the n-gram precisions. In this case, we 9-12 13-14 15-19 Description n-gram precisions against pseudo references (1 ≤ n ≤ 4) PER and WER precision, recall, fragmentation from METEOR (Lavie and Agarwal, 2007) precisions and recalls of nonconsecutive bigrams with a gap size of m (1 ≤ m ≤ 2) longest common subsequences n-gram precision against a target corpus (1 ≤ n ≤ 5) Table 1: Feature sets for regression learning can easily retrain the learne"
P09-1018,P07-1092,0,0.410823,"}@rdc.toshiba.com.cn Abstract approach (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang 2007; Bertoldi et al., 2008). This approach introduces a third language, named the pivot language, for which there exist large source-pivot and pivot-target bilingual corpora. A pivot task was also designed for spoken language translation in the evaluation campaign of IWSLT 2008 (Paul, 2008), where English is used as a pivot language for Chinese to Spanish translation. Three different pivot strategies have been investigated in the literature. The first is based on phrase table multiplication (Cohn and Lapata 2007; Wu and Wang, 2007). It multiples corresponding translation probabilities and lexical weights in source-pivot and pivot-target translation models to induce a new source-target phrase table. We name it the triangulation method. The second is the sentence translation strategy, which first translates the source sentence to the pivot sentence, and then to the target sentence (Utiyama and Isahara, 2007; Khalilov et al., 2008). We name it the transfer method. The third is to use existing models to build a synthetic source-target corpus, from which a source-target model can be trained (Bertoldi et a"
P09-1018,W08-0331,0,0.0612152,") to a score that indicates the quality of the translation. Scores are first generated independently for each translation, then the translations are ranked by their respective scores. The candidate with the highest score is selected as the final translation. This is achieved by optimizing the regression learning model’s output to correlate against a set of training examples, where the source sentences are provided with several reference translations, instead of manually labeling the translations produced by various systems with quantitative assessments as described in (Albrecht and Hwa, 2007; Duh, 2008). The advantage of our method is that we do not need to manually label the translations produced by each translation system, therefore enabling our method suitable for translation selection among any systems without additional manual work. 2 Pivot Methods for Phrase-based SMT 2.1 Triangulation Method Following the method described in Wu and Wang (2007), we train the source-pivot and pivot-target translation models using the source-pivot and pivot-target corpora, respectively. Based on these two models, we induce a source-target translation model, in which two important elements need to be indu"
P09-1018,N07-1061,0,0.569659,"where English is used as a pivot language for Chinese to Spanish translation. Three different pivot strategies have been investigated in the literature. The first is based on phrase table multiplication (Cohn and Lapata 2007; Wu and Wang, 2007). It multiples corresponding translation probabilities and lexical weights in source-pivot and pivot-target translation models to induce a new source-target phrase table. We name it the triangulation method. The second is the sentence translation strategy, which first translates the source sentence to the pivot sentence, and then to the target sentence (Utiyama and Isahara, 2007; Khalilov et al., 2008). We name it the transfer method. The third is to use existing models to build a synthetic source-target corpus, from which a source-target model can be trained (Bertoldi et al., 2008). For example, we can obtain a source-pivot corpus by translating the pivot sentence in the source-pivot corpus into the target language with pivot-target translation models. We name it the synthetic method. The working condition with the pivot language approach is that the source-pivot and pivot-target parallel corpora are independent, in the sense that they are not derived from the same"
P09-1018,D07-1030,1,0.916827,"Missing"
P09-1018,2008.iwslt-evaluation.18,1,0.934056,"tive is to infer a function that maps a feature vector (which measures the similarity of a translation from one system to the pseudo references) to a score that indicates the quality of the translation. Scores are first generated independently for each translation, then the translations are ranked by their respective scores. The candidate with the highest score is selected. The similar ideas have been explored in previous studies. Albrecht and Hwa (2007) proposed a method to evaluate MT outputs with pseudo references using support vector regression as the learner to evaluate translations. Duh (2008) proposed a ranking method to compare the translations proposed by several systems. These two methods require quantitative quality assessments by human judges for the translations produced by various systems in the training set. When we apply such methods to translation selection, the relative values of the scores assigned by the subject systems are important. In different data conditions, the relative values of the scores assigned by the subject systems may change. In order to train a reliable learner, we need to prepare a balanced training set, where the translations produced by different sy"
P09-1018,P07-1108,1,0.717108,"bstract approach (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang 2007; Bertoldi et al., 2008). This approach introduces a third language, named the pivot language, for which there exist large source-pivot and pivot-target bilingual corpora. A pivot task was also designed for spoken language translation in the evaluation campaign of IWSLT 2008 (Paul, 2008), where English is used as a pivot language for Chinese to Spanish translation. Three different pivot strategies have been investigated in the literature. The first is based on phrase table multiplication (Cohn and Lapata 2007; Wu and Wang, 2007). It multiples corresponding translation probabilities and lexical weights in source-pivot and pivot-target translation models to induce a new source-target phrase table. We name it the triangulation method. The second is the sentence translation strategy, which first translates the source sentence to the pivot sentence, and then to the target sentence (Utiyama and Isahara, 2007; Khalilov et al., 2008). We name it the transfer method. The third is to use existing models to build a synthetic source-target corpus, from which a source-target model can be trained (Bertoldi et al., 2008). For examp"
P09-1018,N03-1017,0,0.00975991,"models using the source-pivot and pivot-target corpora, respectively. Based on these two models, we induce a source-target translation model, in which two important elements need to be induced: phrase translation probability and lexical weight. Phrase Translation Probability We induce the phrase translation probability by assuming the independence between the source and target phrases when given the pivot phrase. X φ(¯ s|t¯) = φ(¯ s|¯ p)φ(¯ p|t¯) (1) p¯ Where s¯, p¯ and t¯ represent the phrases in the languages Ls , Lp and Lt , respectively. Lexical Weight According to the method described in Koehn et al. (2003), there are two important elements in the lexical weight: word alignment information a in a phrase pair (¯ s, t¯) and lexical translation probability w(s|t). Let a1 and a2 represent the word alignment information inside the phrase pairs (¯ s, p¯) and (¯ p, t¯) respectively, then the alignment information inside (¯ s, t¯) can be obtained as shown in Eq. (2). We conducted experiments for spoken language translation on the pivot task in the IWSLT 2008 evaluation campaign, where Chinese sentences in travel domain need to be translated into Spanish, with English as the pivot language. Experimental"
P09-1018,W07-0734,0,0.0278038,"Missing"
P09-1018,2008.iwslt-evaluation.17,0,\N,Missing
P09-1018,P07-2045,0,\N,Missing
P09-1018,2008.iwslt-evaluation.1,0,\N,Missing
P10-1085,ahrenberg-etal-2000-evaluation,0,0.0345317,"ng decision rule. Chinese words 6.3M To investigate the quality of the generated word alignments, we randomly selected a subset from the bilingual corpus as test set, including 500 sentence pairs. Then word alignments in the subset were manually labeled, referring to the guideline of the Chinese-to-English alignment (LDC2006E93), but we made some modifications for the guideline. For example, if a preposition appears after a verb as a phrase aligned to one single word in the corresponding sentence, then they are glued together. There are several different evaluation metrics for word alignment (Ahrenberg et al., 2000). We use precision (P), recall (R) and alignment error ratio (AER), which are similar to those in Och and Ney (2000), except that we consider each alignment as a sure link. 828 Single word alignments Multi-word alignments P R AER P R AER Baseline 0.77 0.45 0.43 0.23 0.71 0.65 CM-1 0.70 0.50 0.42 0.35 0.86 0.50 Improved BWA methods CM-2 0.73 0.48 0.42 0.36 0.89 0.49 CM-3 0.73 0.48 0.41 0.39 0.78 0.47 Experiments Table 2. English-to-Chinese word alignment results 中国 的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就 。 China&apos;s science and technology research has made achievements which have gained the attention of t"
P10-1085,J93-2003,0,0.0187108,"Missing"
P10-1085,P03-1012,0,0.0613872,"Missing"
P10-1085,J07-2003,0,0.12447,"Missing"
P10-1085,P09-1105,0,0.0386071,"Missing"
P10-1085,W04-3250,0,0.0422656,"the baseline phrase-based SMT system. We use SRI language modeling toolkit (Stolcke, 2002) to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST MT-2002 set as the development set and the NIST MT-2004 test set as the test set. And Koehn&apos;s implementation of minimum error rate training (Och, 2003) is used to tune the feature weights on the development set. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using paired bootstrap re-sample method (Koehn, 2004). 6.2 Effect of improved word alignment on phrase-based SMT We investigate the effectiveness of the improved word alignments on the phrase-based SMT system. The bi-directional alignments are obtained 830 T1: We must adopt effective measures in order to avoid problems . 我们 必须 wo-men bi-xu we must T2: 采取 有效 措施 才能 避免 出 问题 。 cai-qu you-xiao cuo-shi cai-neng bi-mian chu use effective measure can avoid out wen-ti . problem . We must adopt effective measures can we avoid out of the question . Figure 3. Example of the translations generated by the baseline system and the system where the phrase colloc"
P10-1085,2005.iwslt-1.8,0,0.0194514,"we use the collocation probability of the whole source sentence, r (F ) , as the collocation probability of one-word cept. 3.2 Improving bi-directional bilingual word alignments In word alignment models implemented in GIZA++, only one-to-one and many-to-one word alignment links can be found. Thus, some multiword units cannot be correctly aligned. The symmetrization method is used to effectively overcome this deficiency (Och and Ney, 2003). Bi-directional alignments are generally obtained from source-to-target alignments As 2t and targetto-source alignments At 2 s , using some heuristic rules (Koehn et al., 2005). This method ignores the correlation of the words in the same alignment unit, so an alignment may include many unrelated words 2 , which influences the performances of SMT systems. 1 2 827 http://www.fjoch.com/GIZA++.html In our experiments, a multi-word unit may include up to 40 words. In order to solve the above problem, we incorporate the collocation probabilities into the bidirectional word alignment process. Given alignment sets As 2t and At 2 s . We can obtain the union As t  As 2t  At 2s . The source sentence f f m 1 Corpora Bilingual corpus Additional monolingual corpora can be se"
P10-1085,N03-1017,0,0.122261,"Missing"
P10-1085,P09-4007,0,0.0287087,"Missing"
P10-1085,P05-1057,0,0.0448216,"Missing"
P10-1085,D09-1051,1,0.637339,"f the words in a phrase. Some This work was partially done at Toshiba (China) Research and Development Center. researches used soft syntactic constraints to predict whether source phrase can be translated together (Marton and Resnik, 2008; Xiong et al., 2009). However, the constraints were learned from the parsed corpus, which is not available for many languages. In this paper, we propose to use monolingual collocations to improve SMT. We first identify potentially collocated words and estimate collocation probabilities from monolingual corpora using a Monolingual Word Alignment (MWA) method (Liu et al., 2009), which does not need any additional resource or linguistic preprocessing, and which outperforms previous methods on the same experimental data. Then the collocation information is employed to improve Bilingual Word Alignment (BWA) for various kinds of SMT systems and to improve phrase table for phrase-based SMT. To improve BWA, we re-estimate the alignment probabilities by using the collocation probabilities of words in the same cept. A cept is the set of source words that are connected to the same target word (Brown et al., 1993). An alignment between a source multi-word cept and a target wo"
P10-1085,W02-1018,0,0.0462638,"Missing"
P10-1085,P08-1114,0,0.0333448,"Missing"
P10-1085,P00-1056,0,0.0899882,"subset from the bilingual corpus as test set, including 500 sentence pairs. Then word alignments in the subset were manually labeled, referring to the guideline of the Chinese-to-English alignment (LDC2006E93), but we made some modifications for the guideline. For example, if a preposition appears after a verb as a phrase aligned to one single word in the corresponding sentence, then they are glued together. There are several different evaluation metrics for word alignment (Ahrenberg et al., 2000). We use precision (P), recall (R) and alignment error ratio (AER), which are similar to those in Och and Ney (2000), except that we consider each alignment as a sure link. 828 Single word alignments Multi-word alignments P R AER P R AER Baseline 0.77 0.45 0.43 0.23 0.71 0.65 CM-1 0.70 0.50 0.42 0.35 0.86 0.50 Improved BWA methods CM-2 0.73 0.48 0.42 0.36 0.89 0.49 CM-3 0.73 0.48 0.41 0.39 0.78 0.47 Experiments Table 2. English-to-Chinese word alignment results 中国 的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就 。 China&apos;s science and technology research has made achievements which have gained the attention of the people of the world . 中国 的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就 。 zhong-guo de china DE ke-xue-ji-shu yan-jiu science and"
P10-1085,P03-1021,0,0.0152963,"nces of Moses using the different bi-directional word alignments (Significantly better than baseline with p &lt; 0.01) 6 6.1 Experiments on Phrase-Based SMT Experimental settings We use FBIS corpus to train the Chinese-toEnglish SMT systems. Moses (Koehn et al., 2007) is used as the baseline phrase-based SMT system. We use SRI language modeling toolkit (Stolcke, 2002) to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST MT-2002 set as the development set and the NIST MT-2004 test set as the test set. And Koehn&apos;s implementation of minimum error rate training (Och, 2003) is used to tune the feature weights on the development set. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using paired bootstrap re-sample method (Koehn, 2004). 6.2 Effect of improved word alignment on phrase-based SMT We investigate the effectiveness of the improved word alignments on the phrase-based SMT system. The bi-directional alignments are obtained 830 T1: We must adopt effective measures in order to avoid problems . 我们 必须 wo-men bi-xu we must T2: 采取 有效 措施 才能 避免 出 问题"
P10-1085,J03-1002,0,0.00555258,"To calculate the collocation probability of the alignment sequence, we should also consider the collocation probabilities of such one-to-one alignments. To solve this problem, we use the collocation probability of the whole source sentence, r (F ) , as the collocation probability of one-word cept. 3.2 Improving bi-directional bilingual word alignments In word alignment models implemented in GIZA++, only one-to-one and many-to-one word alignment links can be found. Thus, some multiword units cannot be correctly aligned. The symmetrization method is used to effectively overcome this deficiency (Och and Ney, 2003). Bi-directional alignments are generally obtained from source-to-target alignments As 2t and targetto-source alignments At 2 s , using some heuristic rules (Koehn et al., 2005). This method ignores the correlation of the words in the same alignment unit, so an alignment may include many unrelated words 2 , which influences the performances of SMT systems. 1 2 827 http://www.fjoch.com/GIZA++.html In our experiments, a multi-word unit may include up to 40 words. In order to solve the above problem, we incorporate the collocation probabilities into the bidirectional word alignment process. Given"
P10-1085,P02-1040,0,0.0794477,"er than baseline with p &lt; 0.01) 6 6.1 Experiments on Phrase-Based SMT Experimental settings We use FBIS corpus to train the Chinese-toEnglish SMT systems. Moses (Koehn et al., 2007) is used as the baseline phrase-based SMT system. We use SRI language modeling toolkit (Stolcke, 2002) to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST MT-2002 set as the development set and the NIST MT-2004 test set as the test set. And Koehn&apos;s implementation of minimum error rate training (Och, 2003) is used to tune the feature weights on the development set. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using paired bootstrap re-sample method (Koehn, 2004). 6.2 Effect of improved word alignment on phrase-based SMT We investigate the effectiveness of the improved word alignments on the phrase-based SMT system. The bi-directional alignments are obtained 830 T1: We must adopt effective measures in order to avoid problems . 我们 必须 wo-men bi-xu we must T2: 采取 有效 措施 才能 避免 出 问题 。 cai-qu you-xiao cuo-shi cai-neng bi-mian chu use effective measure can avoid out wen-ti . prob"
P10-1085,J97-3002,0,0.38215,"Missing"
P10-1085,P09-1036,0,0.0240816,"Missing"
P10-1085,P07-2045,0,\N,Missing
P11-1104,P06-1067,0,0.0460858,"Missing"
P11-1104,J93-2003,0,0.0367397,"Missing"
P11-1104,P05-1033,0,0.640525,"EU score over the baseline methods. 1 Introduction Reordering for SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source"
P11-1104,N10-1127,0,0.0615614,"n IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignment (MWA)"
P11-1104,W04-3250,0,0.0191744,"toolkit (Stolcke, 2002) is used to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST evaluation set of 2002 as the development set to tune the feature weights of the SMT system and the interpolation parameters, based on the minimum error rate training method (Och, 2003), and the NIST evaluation sets of 2004 and 2008 (MT04 and MT08) as the test sets. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using the paired bootstrap resample method (Koehn, 2004). 4.3 Translation results We compare the proposed method with various reordering methods in previous work. Monotone model: no reordering model is used. Distortion based reordering (DBR) model: a distortion based reordering method (AlOnaizan & Papineni, 2006). In this method, the distortion cost is defined in terms of words, rather than phrases. This method considers outbound, inbound, and pairwise distortions that Reorder models Monotone model DBR model MSDR model (Baseline) DBR model SCBR Model 1 SCBR Model 2 MSDR+ SCBR Model 3 SCBR models (1+2) SCBR models (1+2+3) MT04 MT08 26.99 18.30 26.64"
P11-1104,N03-1017,0,0.0420923,"rce token is covered when it is translated into a new target token. In 1997, another model called ITG constraint was presented, in which the reordering order can be hierarchically modeled as straight or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our metho"
P11-1104,2005.iwslt-1.8,0,0.0286277,"or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straig"
P11-1104,D09-1051,1,0.918596,"ence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignment (MWA) method without employing additional resources (Liu et al., 2009), and then the reordering model based on the detected collocations is learned from the word-aligned bilingual corpus. The source collocation based reordering model is integrated into SMT systems as an additional feature to softly constrain the translation orders of the source collocations in the sentence to be translated, so as to constrain the translation orders of those source phrases containing these collocated words. This method has two advantages: (1) it can automatically detect and leverage collocated words in a sentence, including long-distance collocated words; (2) such a reordering mo"
P11-1104,P08-1114,0,0.0864168,"r SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignm"
P11-1104,P03-1021,0,0.00926147,"ic function for estimating future score be determined according to the relative positions of the words and the corresponding reordering probability is employed. 4.2 Settings We use the FBIS corpus (LDC2003E14) to train a Chinese-to-English phrase-based translation model. And the SRI language modeling toolkit (Stolcke, 2002) is used to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST evaluation set of 2002 as the development set to tune the feature weights of the SMT system and the interpolation parameters, based on the minimum error rate training method (Och, 2003), and the NIST evaluation sets of 2004 and 2008 (MT04 and MT08) as the test sets. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using the paired bootstrap resample method (Koehn, 2004). 4.3 Translation results We compare the proposed method with various reordering methods in previous work. Monotone model: no reordering model is used. Distortion based reordering (DBR) model: a distortion based reordering method (AlOnaizan & Papineni, 2006). In this method, the distortion cost i"
P11-1104,J03-1002,0,0.00360503,"in (10). L Input: Input sentence F  f 1 Initialization: Score = 0 for each uncovered word f i do e4 e3 for each word f j ( j  ci or r ( f i , f j )   ) do e2 if f j is covered then e1 f1 f2 f3 if i > j then Score+= r ( f i , f j ) log p (o  straight |f i , f j ) f4 f5 else Score+= r ( f i , f j ) log p (o  inverted |f i , f j ) Figure 2. An example for reordering 4 4.1 else Score += arg max o r ( f i , f j ) log p (o |f i , f j ) Evaluation of Our Method Output: Score Implementation We implemented our method in a phrase-based SMT system (Koehn et al., 2007). Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection. Thus, given a sentence to be translated, we first identify the collocations in the sentence, and then estimate the reordering score according to the translation hypothesis. For a translation option to be expanded, the reordering score inside this source phrase is calculated according to their translation orders of the collocations in the corresponding target phrase. The reordering score crossing the current translation option and the covered parts can be calculated according to the relative position of the collocated words. If the source p"
P11-1104,P02-1040,0,0.0825739,"d the corresponding reordering probability is employed. 4.2 Settings We use the FBIS corpus (LDC2003E14) to train a Chinese-to-English phrase-based translation model. And the SRI language modeling toolkit (Stolcke, 2002) is used to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST evaluation set of 2002 as the development set to tune the feature weights of the SMT system and the interpolation parameters, based on the minimum error rate training method (Och, 2003), and the NIST evaluation sets of 2004 and 2008 (MT04 and MT08) as the test sets. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using the paired bootstrap resample method (Koehn, 2004). 4.3 Translation results We compare the proposed method with various reordering methods in previous work. Monotone model: no reordering model is used. Distortion based reordering (DBR) model: a distortion based reordering method (AlOnaizan & Papineni, 2006). In this method, the distortion cost is defined in terms of words, rather than phrases. This method considers outbound, inbound, and pairwise distortions t"
P11-1104,N04-4026,0,0.0295724,"led as straight or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and their translation orders in the target languages, which are used to constrain the ordering models with the estimat"
P11-1104,P05-1069,0,0.0191896,"though the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straight or inverted) score. Moreover, our method allows flexible reordering by con"
P11-1104,C10-1126,0,0.0841098,"ls (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignment (MWA) method without employing add"
P11-1104,J97-3002,0,0.675665,"ng decoding, the model is employed to softly constrain the translation orders of the source language collocations, so as to constrain the translation orders of those source phrases containing these collocated words. The experimental results show that the proposed method significantly improves the translation quality, achieving the absolute improvements of 1.1~1.4 BLEU score over the baseline methods. 1 Introduction Reordering for SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the"
P11-1104,P06-1066,0,0.120655,"ted in the same order as in the source language, or in the inverted order. We name the first case as straight, and the second inverted. Based on the observation that some collocations tend to have fixed translation orders such as “金融 jin-rong „financial‟ 危 机 wei-ji „crisis‟” (financial crisis) whose English translation order is usually straight, and “ 法 律 fa-lv „law‟ 范 围 fan-wei „scope‟” (scope of law) whose English translation order is generally inverted, some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al., 2006). We further notice that some words are translated in different orders when they are collocated with different words. For instance, when “潮流 chao-liu „trend‟” is collocated with “时代 shi-dai „times‟”, they are often translated into the “trend of times”; when collocated with “历史 li-shi „history‟”, the translation usually becomes the “historical trend”. Thus, if we can automatically detect the collocations in the sentence to be translated and their orders in the target language, the reordering information of the collocations could be used to constrain the reordering of phrases during decoding. Th"
P11-1104,P03-1019,0,0.0237065,"n quality. 6 Related Work Reordering was first proposed in the IBM models (Brown et al., 1993), later was named IBM constraint by Berger et al. (1996). This model treats the source word sequence as a coverage set that is processed sequentially and a source token is covered when it is translated into a new target token. In 1997, another model called ITG constraint was presented, in which the reordering order can be hierarchically modeled as straight or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target"
P11-1104,W06-3108,0,0.0263897,"allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straight or inverted) score. Moreover, our method allows flexible reordering by considering both consecu"
P11-1104,D07-1056,0,0.0844338,"uction Reordering for SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using"
P11-1104,P07-2045,0,\N,Missing
P12-1048,J05-4003,0,0.0943735,"Missing"
P12-1048,W09-0432,0,0.0300305,"t Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of"
P12-1048,D07-1007,0,0.0296716,") also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of addit"
P12-1048,P07-1005,0,0.406273,"uccessfully in NLP community. Based on the “bag-of-words” assumption that the order of words can be ignored, these methods model the text corpus by using a co-occurrence matrix of words and documents, and build generative models to infer the latent aspects or topics. Using these models, the words can be clustered into the derived topics with a probability distribution, and the correlation between words can be automatically captured via topics. However, the “bag-of-words” assumption is an unrealistic oversimplification because it ignores the order of words. To remedy this problem, Gruber et al.(2007) propose HTMM, which models the topics of words in the document as a Markov chain. Based on the assumption that all words in the same sentence have the same topic and the successive sentences are more likely to have the same topic, HTMM incorporates the local dependency between words by Hidden Markov Model for better topic estimation. HTMM can also be viewed as a soft clustering tool for words in training corpus. That is, HTMM can estimate the probability distribution of a topic over words, i.e. the topic-word distribution P (word|topic) during training. Besides, HTMM derives inherent topics i"
P12-1048,P10-1086,0,0.0111327,"in SMT. Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity. Tam et al.(2007) proposed a bilingual LSA, which enforces one-to-one topic correspondence and enables latent topic distributions to be efficiently transferred across languages, to crosslingual language modeling and translation lexicon adaptation. Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; C"
P12-1048,P10-1146,0,0.0129201,"ship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely ∗ Part of this work was done during the first author’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific tra"
P12-1048,W07-0722,0,0.159911,"dapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpo"
P12-1048,eck-etal-2004-language,0,0.0665867,"Missing"
P12-1048,2005.mtsummit-papers.30,0,0.060786,"Missing"
P12-1048,W07-0717,0,0.227606,"re we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Lingui"
P12-1048,D10-1044,0,0.0292696,"in SMT. Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity. Tam et al.(2007) proposed a bilingual LSA, which enforces one-to-one topic correspondence and enables latent topic distributions to be efficiently transferred across languages, to crosslingual language modeling and translation lexicon adaptation. Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; C"
P12-1048,P06-1121,0,0.0055969,"ility estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely ∗ Part of this work was done during the first author’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-d"
P12-1048,D08-1039,0,0.0258649,"Missing"
P12-1048,C08-1041,1,0.583545,"odeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of additional in-domain m"
P12-1048,2005.eamt-1.19,0,0.0557771,"Missing"
P12-1048,J03-1002,0,0.00333165,"conduct topic model training. During this process, we empirically set the same parameter values for the HTMM training of different corpora: topics = 50, α = 1.5, β = 1.01, iters = 100. See (Gruber et al., 2007) for the meanings of these parameters. Besides, we set the interpolation weight θ in formula (10) to 0.5 by observing the results on development set in the additional experiments. We choose MOSES, a famous open-source 1 2 http://blog.sohu.com/ http://u.cs.biu.ac.il/ koppel/BlogCorpus.html phrase-based machine translation system (Koehn et al., 2007), as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al.,"
P12-1048,J04-4002,0,0.647358,"translation probability estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely ∗ Part of this work was done during the first author’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the"
P12-1048,N03-1017,0,0.0215688,"Missing"
P12-1048,W04-3250,0,0.0780006,"generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Result and Analysis 4.2.1 Effect of Different Smoothing Methods Our first experiments investigate the effect of different smoothing methods for the in-domain phrasetopic distribution: “Noisy-OR” and “Averaging”. We build adapted phrase tables with these two methods, and then respectively use them in place of the out-of-domain phrase table to test the system performance. For the purpose of studying the generality of our approach, we carry out comparative experiments on two sizes of in-domain monolingual corpora: 5K and 40K. Adaptation Met"
P12-1048,P07-2045,0,0.00336531,"vely, we use HTMM tool developed by Gruber et al.(2007) to conduct topic model training. During this process, we empirically set the same parameter values for the HTMM training of different corpora: topics = 50, α = 1.5, β = 1.01, iters = 100. See (Gruber et al., 2007) for the meanings of these parameters. Besides, we set the interpolation weight θ in formula (10) to 0.5 by observing the results on development set in the additional experiments. We choose MOSES, a famous open-source 1 2 http://blog.sohu.com/ http://u.cs.biu.ac.il/ koppel/BlogCorpus.html phrase-based machine translation system (Koehn et al., 2007), as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evalu"
P12-1048,P06-1077,1,0.129484,"method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely ∗ Part of this work was done during the first author’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual co"
P12-1048,D07-1036,1,0.789966,"l, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpora. Our approach i"
P12-1048,D09-1022,0,0.0366086,"l language modeling and translation lexicon adaptation. Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to"
P12-1048,D09-1074,0,0.027418,"Missing"
P12-1048,W11-2133,0,0.0654102,"gnoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inh´ ang”, and occurs in the sentences related to the geography topic when translated to “h´ ea `n”. Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases. In a monolingual corpus, if “bank” occurs more often in the sentences rela"
P12-1048,P02-1040,0,0.105802,"ch and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Result and Analysis 4.2.1 Effect of Different Smoothing Methods Our first experiments investigate the effect of different smoothing methods for the in-domain phrasetopic distribution: “Noisy-OR” and “Averaging”. We build adapted phrase tables with these two methods, and then respectively use them in place of the out-of-domain phrase table to test the system performance. For the purpose of studying the generality of our approach, we carry out comparative experiments on two sizes"
P12-1048,2009.mtsummit-posters.17,0,0.0128304,"ion emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computati"
P12-1048,2007.mtsummit-tutorials.1,0,0.0760626,"rpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inh´ ang”, and occurs in the sentences related to the geography topic when translated to “h´ ea `n”. Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases. In a monolingual corpus, if"
P12-1048,C08-1125,1,0.73343,"or’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings"
P12-1048,N04-1033,0,0.157585,"which is decomposed into the topic posterior distribution at the word level, and θ is the interpolation weight that can be optimized over the development data. Given the number of the phrase f˜ in sentence f denoted as countf (f˜), we compute the in-domain phrase-topic distribution in the following way: Pmle (tf P = f ∈Cf P tf in ˜ in |f ) countf (f˜) · P (tf in |f ) in countf (f˜) · P (tf P f ∈Cf in in |f ) (11) Under the assumption that the topics of all words in the same phrase are independent, we consider two methods to calculate Pword (tf in |f˜). One is a “Noisy-OR” combination method (Zens and Ney, 2004) which has shown good performance in calculating similarities between bags-of-words in different languages. Using this method, Pword (tf in |f˜) is defined as: Pword (tf in |f˜) = 1 − Pword (t¯f in |f˜) Y ≈ 1− P (t¯f in |fj ) fj ∈f˜ = 1− Y (1 − P (tf in |fj )) (12) fj ∈f˜ where Pword (t¯f in |f˜) represents the probability that tf in is not the topic of the phrase f˜. Similarly, P (t¯f in |fj ) indicates the probability that tf in is not the topic of the word fj . The other method is an “Averaging” combination one. With the assumption that tf in is the topic of f˜ if at least one of the words"
P12-1048,W06-1626,0,0.0730106,"Missing"
P12-1048,C04-1059,0,0.0592364,"Missing"
P12-1048,P06-2124,0,0.82898,"hes focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inh´ ang”, and occurs in the sentences related to the geography topic when translated to “h´ ea `n”. Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates"
P12-1048,D08-1010,1,0.848211,"in adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of additional in-domain monolingual corpora,"
P12-1048,J07-2003,0,\N,Missing
P12-1048,2006.iwslt-evaluation.15,0,\N,Missing
P12-1103,2008.iwslt-papers.2,0,0.110087,"ranslation models, either from the input side, which targets on rewriting the input sentences to the MT-favored expressions, or from the side of translation models, which tries to enrich the translation models to cover more expressions. In recent years, paraphrasing has been proven useful for improving SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use"
P12-1103,N06-1003,0,0.0203314,"SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods, only word or phrasal paraphrases are used for input sentence rewriting. No structured paraphrases on the sentence level have been investigated. However, the information in the sentence level is very important for d"
P12-1103,P05-1033,0,0.063803,"eel interest that N/A blue handbag 欢迎 乘坐 I to that N/A blue handbag have interest 我 对 那 只 蓝色 手提包 有 兴趣 。 我 很 感 兴趣 那 个 蓝色 手提包 I very feel interest that N/A blue handbag 乘坐 ride 。 Figure 2: Example for Word Alignment Filtration 2. Stop words (including some function words and punctuations) can only be aligned to either stop words or null. Figure 2 illustrates an example of using the heuristics to filter alignment. 3.4 Extracting Paraphrase Rules From the word-aligned sentence pairs, we then extract a set of rules that are consistent with the word alignments. We use the rule extracting methods of Chiang (2005). Take the sentence pair in Figure 2 as an example, two initial phrase pairs 感 兴趣 那 个 蓝色 手提包” are identified, and PP1 is contained by PP2, then we could form the rule: 对 X1 有 兴趣  很 感 兴趣 X1 to have interest very feel interest 4 Paraphrasing the Input Sentences The extracted paraphrase rules aim to rewrite the input sentences to an MT-favored form which may lead to a better translation. However, it is risky to directly replace the input sentence with a paraphrased sentence, since the errors in automatic paraphrase substitution may jeopardize the translation result seriously. To avoid such damag"
P12-1103,D10-1041,0,0.0787359,"), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods, only word or phrasal paraphrases are used for input sentence rewriting. No structured paraphrases on the sentence level have been investigated. However, the information in the sentence level is very important for disambiguation. For example, we can only substitute play with drama in a context related to stage or theatre. Phrasal paraphrase substitutions can hardly solve such kind of problems. In this paper, we propose a method that rewrites This work was done when the first author was visit"
P12-1103,I11-1090,1,0.511378,"entences and the translation models, either from the input side, which targets on rewriting the input sentences to the MT-favored expressions, or from the side of translation models, which tries to enrich the translation models to cover more expressions. In recent years, paraphrasing has been proven useful for improving SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and D"
P12-1103,N03-1017,0,0.0378794,"Missing"
P12-1103,C10-1069,0,0.0127161,"e MT-favored expressions, or from the side of translation models, which tries to enrich the translation models to cover more expressions. In recent years, paraphrasing has been proven useful for improving SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods,"
P12-1103,D09-1040,0,0.0211672,"can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods, only word or phrasal paraphrases are used for input sentence rewriting. No structured paraphrases on the sentence level have been investigated. However, the information in the sentence level is very important for disambiguation. For exampl"
P12-1103,P00-1056,0,0.0382139,"e is: T1 = SYS_ST(S0), S1 = SYS_TS(T0), T2 = SYS_ST(S1). Finally we compute BLEU (Papineni et al. 2002) score for every sentence in T2 and T1, using the corresponding sentence in T0 as reference. If the sentence in T2 has a higher BLEU score than the aligned sentence in T1, the corresponding sentences in S0 and S1 are selected as candidate paraphrase sentence pairs, which are used in the following steps of paraphrase extractions. 3.3 Word Alignments Filtering We can construct word alignment between S0 and S1 through T0. On the initial corpus of (S0, T0), we conduct word alignment with Giza++ (Och and Ney, 2000) in both directions and then apply the grow-diag-final heuristic (Koehn et al., 2005) for symmetrization. Because S1 is generated by feeding T0 into the PBMT system SYS_TS, the word alignment between T0 and S1 can be acquired from the verbose information of the decoder. The word alignments of S0 and S1 contain noises which are produced by either wrong alignment of GIZA++ or translation errors of SYS_TS. To ensure the alignment quality, we use some heuristics to filter the alignment between S0 and S1: 1. If two identical words are aligned in S0 and S1, then remove all the other links to the two"
P12-1103,P03-1021,0,0.0399546,". Taking α as an example, firstly, p – 1 nodes are created, and then p edges labeled with αj (1 ≤ j ≤ p) are generated to connect node θx-1, p-1 nodes and θy-1. Via step 2, word lattices are generated by adding new nodes and edges coming from paraphrases. 983 Experiments Experimental Data In our experiments, we used Moses (Koehn et al., 2007) as the baseline system which can support lattice decoding. The alignment was obtained using GIZA++ (Och and Ney, 2003) and then we symmetrized the word alignment using the growdiag-final heuristic. Parameters were tuned using Minimum Error Rate Training (Och, 2003). To comprehensively evaluate the proposed methods in different domains, two groups of experiments were carried out, namely, the oral group (Goral) and the news group (Gnews). The experiments were conducted in both Chinese-English and EnglishChinese directions for the oral group, and ChineseEnglish direction for the news group. The English sentences were all tokenized and lowercased, and the Chinese sentences were segmented into words by Language Technology Platform (LTP) 1 . We used SRILM2 for the training of language models (5-gram in all the experiments). The metrics for automatic evaluatio"
P12-1103,P02-1040,0,0.0861493,"n be paraphrased to RHS. Taking Chinese as a case 981 study, some examples of paraphrase rules are shown in Table 1. 3.2 Selecting Paraphrase Sentence Pairs Following the methods in Section 2, the initial bilingual corpus is (S0, T0). We train a source-totarget PBMT system (SYS_ST) and a target-tosource PBMT system (SYS_TS) on the parallel corpus. Then a Forward-Translation is performed on S0 using SYS_ST, and a Back-Translation is performed on T0 using SYS_TS and SYS_ST. As mentioned above, the detailed procedure is: T1 = SYS_ST(S0), S1 = SYS_TS(T0), T2 = SYS_ST(S1). Finally we compute BLEU (Papineni et al. 2002) score for every sentence in T2 and T1, using the corresponding sentence in T0 as reference. If the sentence in T2 has a higher BLEU score than the aligned sentence in T1, the corresponding sentences in S0 and S1 are selected as candidate paraphrase sentence pairs, which are used in the following steps of paraphrase extractions. 3.3 Word Alignments Filtering We can construct word alignment between S0 and S1 through T0. On the initial corpus of (S0, T0), we conduct word alignment with Giza++ (Och and Ney, 2000) in both directions and then apply the grow-diag-final heuristic (Koehn et al., 2005)"
P12-1103,P09-2034,0,0.0583384,"Missing"
P12-1103,2010.eamt-1.34,0,0.0705043,"Missing"
P12-1103,D10-1064,0,\N,Missing
P12-1103,P07-2045,0,\N,Missing
P12-1103,P09-1089,0,\N,Missing
P12-1103,P10-2001,0,\N,Missing
P12-1103,W04-3250,0,\N,Missing
P12-1103,2005.iwslt-1.8,0,\N,Missing
P15-1166,W14-4012,0,0.0364514,"Missing"
P15-1166,P07-1092,0,0.0299357,"iments that demonstrate the effectiveness of our framework will be described in section 4. Lastly, we will conclude our work in section 5. 2 Related Work Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality. Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus. Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang, 2007; Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014; Gao et al., 2014; Sundermeyer et al., 2014; Liu et al., 2014). On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation. A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a ”bridge” to generate source-target translation for language pair with few training corpus. Pivot based statistical machine translation is crucial in machine translation for resource-poor language pa"
P15-1166,D13-1107,0,0.0175694,"work, where the convolutional neural network model was used. Hatori et al. (2012) proposed to jointly train word segmentation, POS tagging and dependency parsing, which can also be seen as a multi-task learning approach. Similar idea has also been proposed by Li et al. (2014) in Chinese dependency parsing. Most of multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005) where they jointly trained models and shared center parameters in NLP tasks. Researchers have also explored similar approaches (Sennrich et al., 2013; Cui et al., 2013) in statistical machine translation which are often refered as domain adaption. Our work explores the possibility of machine translation under the multitask framework by using the recurrent neural networks. To the best of our knowledge, this is the first trial of end to end machine translation under multi-task learning framework. 3 Multi-task Model for Multiple Language Translation Our model is a general framework for translating from one source language to many targets. The model we build in this section is a recurrent neural network based encoder-decoder model with multiple target tasks, and"
P15-1166,P14-1129,0,0.016516,"ework will be described in section 4. Lastly, we will conclude our work in section 5. 2 Related Work Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality. Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus. Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang, 2007; Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014; Gao et al., 2014; Sundermeyer et al., 2014; Liu et al., 2014). On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation. A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a ”bridge” to generate source-target translation for language pair with few training corpus. Pivot based statistical machine translation is crucial in machine translation for resource-poor language pairs, such as Spanish to Chinese. Considering the p"
P15-1166,P14-1066,0,0.0224282,"ed in section 4. Lastly, we will conclude our work in section 5. 2 Related Work Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality. Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus. Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang, 2007; Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014; Gao et al., 2014; Sundermeyer et al., 2014; Liu et al., 2014). On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation. A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a ”bridge” to generate source-target translation for language pair with few training corpus. Pivot based statistical machine translation is crucial in machine translation for resource-poor language pairs, such as Spanish to Chinese. Considering the problem of translat"
P15-1166,P12-1110,0,0.00596837,"s model. As a specific example model in this paper, we adopt a RNN encoder-decoder neural machine translation model for multi-task learning, though all neural network based model can be adapted in our framework. 1724 In the natural language processing field, a notable work related with multi-task learning was proposed by Collobert et al. (2011) which shared common representation for input words and solve different traditional NLP tasks such as part-of-Speech tagging, name entity recognition and semantic role labeling within one framework, where the convolutional neural network model was used. Hatori et al. (2012) proposed to jointly train word segmentation, POS tagging and dependency parsing, which can also be seen as a multi-task learning approach. Similar idea has also been proposed by Li et al. (2014) in Chinese dependency parsing. Most of multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005) where they jointly trained models and shared center parameters in NLP tasks. Researchers have also explored similar approaches (Sennrich et al., 2013; Cui et al., 2013) in statistical machine translation which are often refered as d"
P15-1166,D13-1176,0,0.1771,"to pivot language bilingual corpus and large-scale pivot language to target languages corpus. However, in reality, language pairs between English and many other target languages may not be large enough, and pivot-based SMT sometimes fails to handle this problem. Our approach handles one to many target language translation in a different way that we directly learn an end to multi-end translation system that does not need a pivot language based on the idea of neural machine translation. Neural Machine translation is a emerging new field in machine translation, proposed by several work recently (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014), aiming at end-to-end machine translation without phrase table extraction and language model training. Different from traditional statistical machine translation, neural machine translation encodes a variable-length source sentence with a recurrent neural network into a fixed-length vector representation and decodes it with another recurrent neural network from a fixed-length vector into variable-length target sentence. A typical model is the RNN encoder-decoder approach proposed by Bahdanau et al. (2014), which utilizes a bidirectional recurren"
P15-1166,koen-2004-pharaoh,0,0.00875525,"mization for end to multi-end model 3.4 Translation with Beam Search Although parallel corpora are available for the encoder and the decoder modeling in the training phrase, the ground truth is not available during test time. During test time, translation is produced by finding the most likely sequence via beam search. ˆ = argmax p(YTp |STp ) Y Y (15) Given the target direction we want to translate to, beam search is performed with the shared encoder and a specific target decoder where search space belongs to the decoder Tp . We adopt beam search algorithm similar as it is used in SMT system (Koehn, 2004) except that we only utilize scores produced by each decoder as features. The size of beam is 10 in our experiments for speedup consideration. Beam search is ended until the endof-sentence eos symbol is generated. Dataset The Europarl corpus is a multi-lingual corpus including 21 European languages. Here we only choose four language pairs for our experiments. The source language is English for all language pairs. And the target languages are Spanish (Es), French (Fr), Portuguese (Pt) and Dutch (Nl). To demonstrate the validity of our learning framework, we do some preprocessing on the training"
P15-1166,P14-1140,0,0.0169606,"Missing"
P15-1166,P02-1040,0,0.123027,"er is set to 1000. We trained our multi-task model with a multiGPU implementation due to the limitation of Graphic memory. And each target decoder is trained within one GPU card, and we synchronize our source encoder every 1000 batches among all GPU card. Our model costs about 72 hours on full large parallel corpora training until convergence and about 24 hours on partial parallel corpora training. During decoding, our implementation on GPU costs about 0.5 second per sentence. 4.3 Evaluation We evaluate the effectiveness of our method with EuroParl Common testset and WMT 2013 dataset. BLEU-4 (Papineni et al., 2002) is used as the evaluation metric. We evaluate BLEU scores on EuroParl Common test set with multi-task NMT models and single NMT models to demonstrate the validity of our multi-task learning framework. On the WMT 2013 data sets, we compare performance of separately trained NMT models, multi-task NMT models and Moses. We use the EuroParl Common test set as a development set in both neural machine translation experiments and Moses experiments. For single NMT models and multi-task NMT models, we select the best model with the highest BLEU score in the EuroParl Common testset and apply it to the W"
P15-1166,P13-1082,0,0.0405335,"beling within one framework, where the convolutional neural network model was used. Hatori et al. (2012) proposed to jointly train word segmentation, POS tagging and dependency parsing, which can also be seen as a multi-task learning approach. Similar idea has also been proposed by Li et al. (2014) in Chinese dependency parsing. Most of multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005) where they jointly trained models and shared center parameters in NLP tasks. Researchers have also explored similar approaches (Sennrich et al., 2013; Cui et al., 2013) in statistical machine translation which are often refered as domain adaption. Our work explores the possibility of machine translation under the multitask framework by using the recurrent neural networks. To the best of our knowledge, this is the first trial of end to end machine translation under multi-task learning framework. 3 Multi-task Model for Multiple Language Translation Our model is a general framework for translating from one source language to many targets. The model we build in this section is a recurrent neural network based encoder-decoder model with multipl"
P15-1166,D14-1003,0,0.0161313,"astly, we will conclude our work in section 5. 2 Related Work Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality. Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus. Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang, 2007; Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014; Gao et al., 2014; Sundermeyer et al., 2014; Liu et al., 2014). On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation. A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a ”bridge” to generate source-target translation for language pair with few training corpus. Pivot based statistical machine translation is crucial in machine translation for resource-poor language pairs, such as Spanish to Chinese. Considering the problem of translating one source language to"
P15-1166,P07-1108,1,0.525798,"rning method. Experiments that demonstrate the effectiveness of our framework will be described in section 4. Lastly, we will conclude our work in section 5. 2 Related Work Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality. Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus. Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang, 2007; Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014; Gao et al., 2014; Sundermeyer et al., 2014; Liu et al., 2014). On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation. A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a ”bridge” to generate source-target translation for language pair with few training corpus. Pivot based statistical machine translation is crucial in machine translation for r"
P16-1033,D14-1108,0,0.0232974,"nnotated sentence, where only the heads of “saw” and “with” are decided. upsurge of web data (e.g., tweets, blogs, and product comments) imposes great challenges to existing parsing techniques. Meanwhile, previous research on out-of-domain dependency parsing gains little success (Dredze et al., 2007; Petrov and McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial"
P16-1033,C10-1011,0,0.131693,"Missing"
P16-1033,W09-2307,0,0.0617144,"Missing"
P16-1033,P10-1001,0,0.0312588,"lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences provide more flexibility in choosing potentially informative structures to annotate. Beyond previous work, this paper endeavors to more thoroughly study this issue, and has made substantial progress from the following perspectives. In this work, we for the first time apply a probabilistic CRF-based pa"
P16-1033,D07-1015,0,0.0416854,"Missing"
P16-1033,N06-1019,0,0.0701527,"Missing"
P16-1033,W08-1301,0,0.158099,"Missing"
P16-1033,C12-2067,0,0.0659251,"Missing"
P16-1033,C14-1075,1,0.946592,"certain metric for AL for sequence labeling problems. In the case of dependency parsing, the marginal probability of a dependency is the sum of probabilities of all legal trees that contain the dependency. ∑ p(h ↷ m|x; w) = p(d|x; w) (4) d∈Y(x):h↷m∈d Intuitively, marginal probability is a more principled metric for measuring reliability of a dependency since it considers all legal parses in the search space, compared to previous methods based on scores of local classifiers (Sassano and Kurohashi, 2010; Flannery and Mori, 2015) or votes of n-best parses (Mirroshandel and Nasr, 2011). Moreover, Li et al. (2014) find strong correlation between marginal probability and correctness of a dependency in cross-lingual syntax projection. Score(x, d∗ ) (5) n1.5 Normalized tree probability. The CRF-based parser allows us, for the first time in AL for dependency parsing, to directly use tree probabilities for uncertainty measurement. Unlike previous approximate methods based on k-best parses (Mirroshandel and Nasr, 2011), tree probabilities globally consider all parse trees in the search space, and thus are intuitively more consistent and proper for measuring the reliability of a tree. Our initial assumption i"
P16-1033,U12-1005,0,0.0219821,"2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They f"
P16-1033,P14-1126,0,0.0479024,"Missing"
P16-1033,W15-2202,0,0.24441,"notation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting"
P16-1033,I11-1087,0,0.0377813,"Missing"
P16-1033,W13-5711,0,0.102764,"arsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences pro"
P16-1033,D14-1097,0,0.0830651,"Missing"
P16-1033,I11-1100,0,0.0281049,"Missing"
P16-1033,P15-1119,1,0.88885,"Missing"
P16-1033,E06-1011,0,0.0810264,"4th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences provide more flexibility in choosing potentially informative structures to annotate. Beyond previous work, this paper endeavors to more thoroughly study this issue, and has made substantial progress from the following perspectives. In this work, we for the first time apply a probabilistic CRF-based parsing model to AL for dependency parsing. We adopt the second-order graphbased model of McDonald and Pereira (2006), which casts the problem as finding an optimal tree from a fully-connect directed graph and factors the score of a dependency tree into scores of pairs of sibling dependencies. (1) This is the first work that applies a stateof-the-art probabilistic parsing model to AL for dependency parsing. The CRF-based dependency parser on the one hand allows us to use probabilities of trees or marginal probabilities of single dependencies for uncertainty measurement, and on the other hand can directly learn parameters from partially annotated trees. Using probabilistic models may be ubiquitous in AL for r"
P16-1033,P99-1010,0,0.894454,"Missing"
P16-1033,W07-2216,0,0.0829738,"Missing"
P16-1033,J04-3001,0,0.119634,"d McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and"
P16-1033,N12-1053,0,0.0241059,"omain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing perf"
P16-1033,C08-1113,0,0.0347094,"Missing"
P16-1033,P15-1134,0,0.075254,"Missing"
P16-1033,D14-1122,0,0.0228308,"where only the heads of “saw” and “with” are decided. upsurge of web data (e.g., tweets, blogs, and product comments) imposes great challenges to existing parsing techniques. Meanwhile, previous research on out-of-domain dependency parsing gains little success (Dredze et al., 2007; Petrov and McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which"
P16-1033,W11-2917,0,0.448477,"es full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units r"
P16-1033,P11-2033,1,0.841112,"ng trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences provide more flexibility in choosing potentially informative structures to annotate. Beyond previous work, this paper endeavors to more thoroughly study this issue, and has made substantial progress from the following perspectives. In this work, we for the first time apply a probabilistic CRF-based parsing model to AL for de"
P16-1033,P92-1017,0,0.8556,"Missing"
P16-1033,D15-1039,0,0.0453421,"Missing"
P16-1033,P02-1035,0,0.254628,"e, and may be asked to annotate another selected word in the same sentence in next AL iteration. Obviously, frequently switching sentences incurs great waste of cognitive effort, 3.4 Learning from PA A major challenge for AL with PA is how to learn from partially labeled sentences, as depicted in Figure 1. Li et al. (2014) show that a probabilistic CRF-based parser can naturally and effectively learn from PA. The basic idea is converting a partial tree into a forest as shown in Figure 2, 347 and using the forest as the gold-standard reference during training, also known as ambiguous labeling (Riezler et al., 2002; T¨ackstr¨om et al., 2013). For each remaining word without head, we add all dependencies linking to it as long as the new dependency does not violate the existing dependencies. We denote the resulting forest as Fj, whose probability is naturally the sum of probabilities of each tree d in F. ∑ p(F|x; w) = p(d|x; w) d∈F ∑ eScore(x,d;w) = ∑ d∈F Score(x,d′ ;w) d′ ∈Y(x) e Train Chinese English ∑N i=1 log p(Fi |xi ; w) #Sentences 14,304 803 1,910 #Tokens 318,408 20,454 50,319 #Sentences 39,115 1,700 2,416 #Tokens 908,154 40,117 56,684 are selected and annotated at each iteration. In the case of si"
P16-1033,P10-1037,0,0.419473,"09). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Lin"
P16-1033,D08-1112,0,0.139903,"Missing"
P16-1033,D07-1014,0,0.220985,"Missing"
P16-1033,N13-1126,0,0.0792451,"Missing"
P16-1033,P02-1016,0,0.0777853,"l., 2007; Petrov and McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; F"
P16-1159,J93-2003,0,0.0998959,"Missing"
P16-1159,P05-1033,0,0.394114,"d-to-end neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has attracted increasing attention from the community. Providing a new paradigm for machine translation, NMT aims at training a single, large neural network that directly transforms a sourcelanguage sentence to a target-language sentence without explicitly modeling latent structures (e.g., word alignment, phrase segmentation, phrase reordering, and SCFG derivation) that are vital in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005). Current NMT models are based on the encoderdecoder framework (Cho et al., 2014; Sutskever et al., 2014), with an encoder to read and encode a source-language sentence into a vector, from which a decoder generates a target-language sentence. While early efforts encode the input into a ∗ Corresponding author: Yang Liu. fixed-length vector, Bahdanau et al. (2015) advocate the attention mechanism to dynamically generate a context vector for a target word being generated. Although NMT models have achieved results on par with or better than conventional SMT, they still suffer from a major drawback"
P16-1159,P14-1066,0,0.055646,"ages over MLE: 1. Direct optimization with respect to evaluation metrics: MRT introduces evaluation metrics as loss functions and aims to minimize expected loss on the training data. 2. Applicable to arbitrary loss functions: our approach allows arbitrary sentence-level loss functions, which are not necessarily differentiable. 3. Transparent to architectures: MRT does not assume the specific architectures of NMT and can be applied to any end-to-end NMT systems. While MRT has been widely used in conventional SMT (Och, 2003; Smith and Eisner, 2006; He and Deng, 2012) and deep learning based MT (Gao et al., 2014), to the best of our knowledge, this work is the first effort to introduce MRT 1683 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1683–1692, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics into end-to-end NMT. Experiments on a variety of language pairs (Chinese-English, English-French, and English-German) show that MRT leads to significant improvements over MLE on a state-ofthe-art NMT system (Bahdanau et al., 2015). 2 Background Given a source sentence x = x1 , . . . , xm , . . . , xM and a target sentence y"
P16-1159,P12-1031,0,0.0465582,"the training data. MRT has the following advantages over MLE: 1. Direct optimization with respect to evaluation metrics: MRT introduces evaluation metrics as loss functions and aims to minimize expected loss on the training data. 2. Applicable to arbitrary loss functions: our approach allows arbitrary sentence-level loss functions, which are not necessarily differentiable. 3. Transparent to architectures: MRT does not assume the specific architectures of NMT and can be applied to any end-to-end NMT systems. While MRT has been widely used in conventional SMT (Och, 2003; Smith and Eisner, 2006; He and Deng, 2012) and deep learning based MT (Gao et al., 2014), to the best of our knowledge, this work is the first effort to introduce MRT 1683 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1683–1692, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics into end-to-end NMT. Experiments on a variety of language pairs (Chinese-English, English-French, and English-German) show that MRT leads to significant improvements over MLE on a state-ofthe-art NMT system (Bahdanau et al., 2015). 2 Background Given a source sentence x = x1 , ."
P16-1159,P15-1001,0,0.46654,"missing. By optimizing model parameters directly with respect to sentence-level BLEU, RNN SEARCH -MRT seems to be able to generate translations more consistently at the sentence level. 4.7 Results on English-French Translation Table 7 shows the results on English-French translation. We list existing end-to-end NMT systems that are comparable to our system. All these systems use the same subset of the WMT 2014 training corpus and adopt MLE as the training criterion. They differ in network architectures and vocabulary sizes. Our RNN SEARCH -MLE system achieves a BLEU score comparable to that of Jean et al. (2015). RNN SEARCH -MRT achieves the highest BLEU score in this setting even with a vocabulary size smaller than Luong et al. (2015b) and Sutskever et al. (2014). Note that our approach does not assume specific architectures and can in principle be applied to any NMT systems. 4.8 Results on English-German Translation Table 8 shows the results on English-German translation. Our approach still significantly out1689 Source Reference M OSES RNN SEARCH -MLE RNN SEARCH -MRT meiguo daibiao tuan baokuo laizi shidanfu daxue de yi wei zhongguo zhuanjia , liang ming canyuan waijiao zhengce zhuli yiji yi wei fu"
P16-1159,D15-1166,0,0.616781,"se-English, the training data consists of 2.56M pairs of sentences with 67.5M Chinese words and 74.8M English words, respectively. We ˜ ∂ R(θ) used the NIST 2006 dataset as the validation set ∂θi (hyper-parameter optimization and model selec"" S (s) X tion) and the NIST 2002, 2003, 2004, 2005, and ∂P (y|x ; θ)/∂θi =α Ey|x(s) ;θ,α × 2008 datasets as test sets. (s) P (y|x ; θ) s=1  For English-French, to compare with the results ∆(y, y(s) ) − reported by previous work on end-to-end NMT # (Sutskever et al., 2014; Bahdanau et al., 2015;  0 (s) Ey0 |x(s) ;θ,α [∆(y , y )] . (14) Jean et al., 2015; Luong et al., 2015b), we used the same subset of the WMT 2014 training corpus that contains 12M sentence pairs with 304M Since |S(x(s) ) | |Y(x(s) )|, the expectations English words and 348M French words. The conin Eq. (14) can be efficiently calculated by excatenation of news-test 2012 and news-test 2013 plicitly enumerating all candidates in S(x(s) ). In serves as the validation set and news-test 2014 as our experiments, we find that approximating the the test set. full space with 100 samples (i.e., k = 100) works For English-German, to compare with the very well and further increasing sample size does resul"
P16-1159,P15-1002,0,0.727498,"se-English, the training data consists of 2.56M pairs of sentences with 67.5M Chinese words and 74.8M English words, respectively. We ˜ ∂ R(θ) used the NIST 2006 dataset as the validation set ∂θi (hyper-parameter optimization and model selec"" S (s) X tion) and the NIST 2002, 2003, 2004, 2005, and ∂P (y|x ; θ)/∂θi =α Ey|x(s) ;θ,α × 2008 datasets as test sets. (s) P (y|x ; θ) s=1  For English-French, to compare with the results ∆(y, y(s) ) − reported by previous work on end-to-end NMT # (Sutskever et al., 2014; Bahdanau et al., 2015;  0 (s) Ey0 |x(s) ;θ,α [∆(y , y )] . (14) Jean et al., 2015; Luong et al., 2015b), we used the same subset of the WMT 2014 training corpus that contains 12M sentence pairs with 304M Since |S(x(s) ) | |Y(x(s) )|, the expectations English words and 348M French words. The conin Eq. (14) can be efficiently calculated by excatenation of news-test 2012 and news-test 2013 plicitly enumerating all candidates in S(x(s) ). In serves as the validation set and news-test 2014 as our experiments, we find that approximating the the test set. full space with 100 samples (i.e., k = 100) works For English-German, to compare with the very well and further increasing sample size does resul"
P16-1159,P03-1021,0,0.667288,"the expected loss (i.e., risk) on the training data. MRT has the following advantages over MLE: 1. Direct optimization with respect to evaluation metrics: MRT introduces evaluation metrics as loss functions and aims to minimize expected loss on the training data. 2. Applicable to arbitrary loss functions: our approach allows arbitrary sentence-level loss functions, which are not necessarily differentiable. 3. Transparent to architectures: MRT does not assume the specific architectures of NMT and can be applied to any end-to-end NMT systems. While MRT has been widely used in conventional SMT (Och, 2003; Smith and Eisner, 2006; He and Deng, 2012) and deep learning based MT (Gao et al., 2014), to the best of our knowledge, this work is the first effort to introduce MRT 1683 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1683–1692, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics into end-to-end NMT. Experiments on a variety of language pairs (Chinese-English, English-French, and English-German) show that MRT leads to significant improvements over MLE on a state-ofthe-art NMT system (Bahdanau et al., 2015). 2 B"
P16-1159,P02-1040,0,0.11537,". . . , yN , end-to-end NMT directly models the translation probability: P (y|x; θ) = N Y P (yn |x, y<n ; θ), (1) n=1 First, while the models are trained only on the training data distribution, they are used to generate target words on previous model predictions, which can be erroneous, at test time. This is referred to as exposure bias (Ranzato et al., 2015). Second, MLE usually uses the cross-entropy loss focusing on word-level errors to maximize the probability of the next correct word, which might hardly correlate well with corpus-level and sentence-level evaluation metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). As a result, it is important to introduce new training algorithms for end-to-end NMT to include model predictions during training and optimize model parameters directly with respect to evaluation metrics. where θ is a set of model parameters and y<n = y1 , . . . , yn−1 is a partial translation. 3 Minimum Risk Training for Neural Predicting the n-th target word can be modeled Machine Translation by using a recurrent neural network: n o P (yn |x, y<n ; θ) ∝ exp q(yn−1 , zn , cn , θ) , (2) Minimum risk training (MRT), which aims to minimize the expected loss on the"
P16-1159,P06-2101,0,0.836096,"ed loss (i.e., risk) on the training data. MRT has the following advantages over MLE: 1. Direct optimization with respect to evaluation metrics: MRT introduces evaluation metrics as loss functions and aims to minimize expected loss on the training data. 2. Applicable to arbitrary loss functions: our approach allows arbitrary sentence-level loss functions, which are not necessarily differentiable. 3. Transparent to architectures: MRT does not assume the specific architectures of NMT and can be applied to any end-to-end NMT systems. While MRT has been widely used in conventional SMT (Och, 2003; Smith and Eisner, 2006; He and Deng, 2012) and deep learning based MT (Gao et al., 2014), to the best of our knowledge, this work is the first effort to introduce MRT 1683 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1683–1692, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics into end-to-end NMT. Experiments on a variety of language pairs (Chinese-English, English-French, and English-German) show that MRT leads to significant improvements over MLE on a state-ofthe-art NMT system (Bahdanau et al., 2015). 2 Background Given a source"
P16-1159,2006.amta-papers.25,0,0.0949686,"ectly models the translation probability: P (y|x; θ) = N Y P (yn |x, y<n ; θ), (1) n=1 First, while the models are trained only on the training data distribution, they are used to generate target words on previous model predictions, which can be erroneous, at test time. This is referred to as exposure bias (Ranzato et al., 2015). Second, MLE usually uses the cross-entropy loss focusing on word-level errors to maximize the probability of the next correct word, which might hardly correlate well with corpus-level and sentence-level evaluation metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). As a result, it is important to introduce new training algorithms for end-to-end NMT to include model predictions during training and optimize model parameters directly with respect to evaluation metrics. where θ is a set of model parameters and y<n = y1 , . . . , yn−1 is a partial translation. 3 Minimum Risk Training for Neural Predicting the n-th target word can be modeled Machine Translation by using a recurrent neural network: n o P (yn |x, y<n ; θ) ∝ exp q(yn−1 , zn , cn , θ) , (2) Minimum risk training (MRT), which aims to minimize the expected loss on the training data, has been widel"
P16-1159,D13-1176,0,0.0837584,"on. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differentiable. Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to architectures, our approach can be applied to more neural networks and potentially benefit more NLP tasks. 1 Introduction Recently, end-to-end neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has attracted increasing attention from the community. Providing a new paradigm for machine translation, NMT aims at training a single, large neural network that directly transforms a sourcelanguage sentence to a target-language sentence without explicitly modeling latent structures (e.g., word alignment, phrase segmentation, phrase reordering, and SCFG derivation) that are vital in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005). Current NMT models are based on the encoderdecoder framewo"
P16-1159,D07-1091,0,0.0128786,"s with 91M English words and To build the subset, an alternative to sampling is computing top-k translations. We prefer sampling to comput87M German words. The concatenation of newsing top-k translations for efficiency: sampling is more effitest 2012 and news-test 2013 is used as the validacient and easy-to-implement than calculating k-best lists, estion set and news-test 2014 as the test set. pecially given the extremely parallel architectures of GPUs. 1686 40 35 35 30 30 BLEU score BLEU score 40 25 20 15 10 0 5 10 15 20 25 30 Training time (hours) k=100 k=50 k=25 5 α=1×10 35 0 40 1. M OSES (Koehn and Hoang, 2007): a phrasebased SMT system using minimum error rate training (Och, 2003). 2. RNN SEARCH (Bahdanau et al., 2015): an attention-based NMT system using maximum likelihood estimation. M OSES uses the parallel corpus to train a phrase-based translation model and the target part to train a 4-gram language model using the SRILM toolkit (Stolcke, 2002). 2 The log-linear model Moses uses is trained by the minimum error rate training (MERT) algorithm (Och, 2003) that directly optimizes model parameters with respect to evaluation metrics. RNN SEARCH uses the parallel corpus to train an attention-based ne"
P16-1159,N03-1017,0,0.112266,"duction Recently, end-to-end neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has attracted increasing attention from the community. Providing a new paradigm for machine translation, NMT aims at training a single, large neural network that directly transforms a sourcelanguage sentence to a target-language sentence without explicitly modeling latent structures (e.g., word alignment, phrase segmentation, phrase reordering, and SCFG derivation) that are vital in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005). Current NMT models are based on the encoderdecoder framework (Cho et al., 2014; Sutskever et al., 2014), with an encoder to read and encode a source-language sentence into a vector, from which a decoder generates a target-language sentence. While early efforts encode the input into a ∗ Corresponding author: Yang Liu. fixed-length vector, Bahdanau et al. (2015) advocate the attention mechanism to dynamically generate a context vector for a target word being generated. Although NMT models have achieved results on par with or better than conventional SMT, they still suffer from a"
P16-1159,W04-1013,0,0.0217352,"late reinforcement reward while MRT generates multiple samples to calculate the expected risk. Figure 2 indicates that multiple samples potentially increases MRT’s capability of discriminating between diverse candidates and thus benefit translation quality. Our experiments confirm their finding that taking evaluation metrics into account when optimizing model parameters does help to improve sentence-level text generation. More recently, our approach has been successfully applied to summarization (Ayana et al., 2016). They optimize neural networks for headline generation with respect to ROUGE (Lin, 2004) and also achieve significant improvements, confirming the effectiveness and applicability of our approach. 5 6 Related Work Our work originated from the minimum risk training algorithms in conventional statistical machine translation (Och, 2003; Smith and Eisner, 2006; He and Deng, 2012). Och (2003) describes a smoothed error count to allow calculating gradients, which directly inspires us to use a parameter α to adjust the smoothness of the objective function. As neural networks are non-linear, our approach has to minimize the expected loss on the sentence level rather than the loss of 1-bes"
P16-1159,D14-1179,0,\N,Missing
P16-1185,J93-2003,0,0.0781716,"Missing"
P16-1185,P05-1033,0,0.174897,"t show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems. 1 Introduction End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model∗ Yang Liu is the corresponding author. ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al., 2015). However, most existing NMT approaches suffer from a major drawback: they heavily rely on parallel corpora for training translation models. This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language model like SMT"
P16-1185,W14-4012,0,0.0289659,"Missing"
P16-1185,D14-1061,0,0.0330026,"Missing"
P16-1185,P15-1001,0,0.069953,"Missing"
P16-1185,D13-1176,0,0.0583835,"arget and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the ChineseEnglish dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems. 1 Introduction End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model∗ Yang Liu is the corresponding author. ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al., 2015). However, most existing NMT approaches suffer from a major drawback: they h"
P16-1185,P13-1140,0,0.0205566,"Missing"
P16-1185,E12-1014,0,0.015191,"pora for machine translation and (2) autoencoders in unsupervised and semi-supervised learning. 4.1 Exploiting Monolingual Corpora for Machine Translation Exploiting monolingual corpora for conventional SMT has attracted intensive attention in recent years. Several authors have introduced transductive learning to make full use of monolingual corpora (Ueffing et al., 2007; Bertoldi and Federico, 2009). They use an existing translation model to translate unseen source text, which can be paired with its translations to form a pseudo parallel corpus. This process iterates until convergence. While Klementiev et al. (2012) propose an approach to estimating phrase translation probabilities from monolingual corpora, Zhang and Zong (2013) directly extract parallel phrases from monolingual corpora using retrieval techniques. Another important line of research is to treat translation on monolingual corpora as a decipherment problem (Ravi and Knight, 2011; Dou et al., 2014). 1972 Closely related to Gulccehre et al. (2015) and Sennrich et al. (2015), our approach focuses on learning birectional NMT models via autoencoders on monolingual corpora. The major advantages of our approach are the transparency to network arch"
P16-1185,N03-1017,0,0.145099,"hineseEnglish dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems. 1 Introduction End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model∗ Yang Liu is the corresponding author. ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al., 2015). However, most existing NMT approaches suffer from a major drawback: they heavily rely on parallel corpora for training translation models. This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language"
P16-1185,P07-2045,0,0.0608607,"ts serve as test sets. Each Chinese sentence has four reference translations. For English-to-Chinese translation, we use the NIST datasets in a reverse direction: treating the first English sentence in the four reference translations as a source sentence and the original input Chinese sentence as the single reference translation. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script. We compared our approach with two state-ofthe-art SMT and NMT systems: 2. RNN SEARCH (Bahdanau et al., 2015): an attention-based NMT system. 1. M OSES (Koehn et al., 2007): a phrase-based SMT system; For M OSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003). We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models. For RNNS EARCH, we use the parallel corpus to train the attention-based neural translation models. We set the vocabulary size of word embeddings to 30K for both Chinese and English. We follow Luong et al. (2015) to address rare words. On top of RNNS EARCH, our approach is capable of tra"
P16-1185,P15-1002,0,0.100715,"N SEARCH (Bahdanau et al., 2015): an attention-based NMT system. 1. M OSES (Koehn et al., 2007): a phrase-based SMT system; For M OSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003). We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models. For RNNS EARCH, we use the parallel corpus to train the attention-based neural translation models. We set the vocabulary size of word embeddings to 30K for both Chinese and English. We follow Luong et al. (2015) to address rare words. On top of RNNS EARCH, our approach is capable of training bidirectional attention-based neural translation models on the concatenation of parallel and monolingual corpora. The sample size k is set to 10. We set the hyper-parameter λ1 = 0.1 and 1969 λ2 = 0 when we add the target monolingual corpus, and λ1 = 0 and λ2 = 0.1 for source monolingual corpus incorporation. The threshold of gradient clipping is set to 0.05. The parameters of our model are initialized by the model trained on parallel corpus. 3.2 Effect of Sample Size k As the inference of our approach is intracta"
P16-1185,P03-1021,0,0.0280237,"sentence and the original input Chinese sentence as the single reference translation. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script. We compared our approach with two state-ofthe-art SMT and NMT systems: 2. RNN SEARCH (Bahdanau et al., 2015): an attention-based NMT system. 1. M OSES (Koehn et al., 2007): a phrase-based SMT system; For M OSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003). We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models. For RNNS EARCH, we use the parallel corpus to train the attention-based neural translation models. We set the vocabulary size of word embeddings to 30K for both Chinese and English. We follow Luong et al. (2015) to address rare words. On top of RNNS EARCH, our approach is capable of training bidirectional attention-based neural translation models on the concatenation of parallel and monolingual corpora. The sample size k is set to 10. We set the hyper-parameter λ1 = 0.1 and 1969 λ2 = 0 when we add the target monolingua"
P16-1185,P02-1040,0,0.119118,"nese validation set. For Chinese-to-English translation, we use the NIST 2006 Chinese-English dataset as the validation set for hyper-parameter optimization and model selection. The NIST 2002, 2003, 2004, and 2005 datasets serve as test sets. Each Chinese sentence has four reference translations. For English-to-Chinese translation, we use the NIST datasets in a reverse direction: treating the first English sentence in the four reference translations as a source sentence and the original input Chinese sentence as the single reference translation. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script. We compared our approach with two state-ofthe-art SMT and NMT systems: 2. RNN SEARCH (Bahdanau et al., 2015): an attention-based NMT system. 1. M OSES (Koehn et al., 2007): a phrase-based SMT system; For M OSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003). We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models. For RNNS EARCH, we use the parallel corpus to train the attention-base"
P16-1185,P11-1002,0,0.0466152,"Missing"
P16-1185,D11-1014,0,0.0792271,"icted to government documents and news reports. Therefore, the availability of large-scale, high-quality, and wide-coverage parallel corpora becomes a major obstacle for NMT. 2.2 the observed source sentence via a latent target sentence: → − ← − P (x0 |x; θ , θ ) X ← − = P (x0 , y|x; θ ) y Autoencoders on Monolingual Corpora It is appealing to explore the more readily available, abundant monolingual corpora to improve NMT. Let us first consider an unsupervised setting: how to train NMT models on a monolingual corpus T = {y(t) }Tt=1 ? Our idea is to leverage autoencoders (Vincent et al., 2010; Socher et al., 2011): (1) encoding an observed target sentence into a latent source sentence using a target-to-source translation model and (2) decoding the source sentence to reconstruct the observed target sentence using a source-to-target model. For example, as shown in Figure 1(b), given an observed English sentence “Bush held a talk with Sharon”, a target-to-source translation model (i.e., encoder) transforms it into a Chinese translation “bushi yu shalong juxing le huitan” that is unobserved on the training data (highlighted in grey). Then, a source-to-target translation model (i.e., decoder) reconstructs t"
P16-1185,P07-1004,0,0.451948,"y of the observed source sentence x0 from the latent target sentence. As a result, monolingual corpora can be combined with parallel corpora to train bidirectional NMT models in a semi-supervised setting. and attention model are fixed when training on these pseudo parallel sentence pairs. In the second approach, they first train a nerual translation model on the parallel corpus and then use the learned model to translate a monolingual corpus. The monolingual corpus and its translations constitute an additional pseudo parallel corpus. Similar ideas have also been suggested in conventional SMT (Ueffing et al., 2007; Bertoldi and Federico, 2009). Sennrich et al. (2015) report that their approach significantly improves translation quality across a variety of language pairs. In this paper, we propose semi-supervised learning for neural machine translation. Given labeled (i.e., parallel corpora) and unlabeled (i.e., monolingual corpora) data, our approach jointly trains source-to-target and target-to-source translation models. The key idea is to append a reconstruction term to the training objective, which aims to reconstruct the observed monolingual corpora using an autoencoder. In the autoencoder, the sou"
P16-1185,W09-0432,0,\N,Missing
P17-1021,D13-1160,0,0.664994,"only be familiar with the particular language grammars, but also be aware of the architectures of the KBs. By contrast, knowledge base-based question answering (KB-QA) (Unger et al., 2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers. Recently, with the progress of deep learning, neural n"
P17-1021,D14-1067,0,0.627295,"e-based question answering (KB-QA) (Unger et al., 2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers. Recently, with the progress of deep learning, neural network-based (NN-based) methods have been introduced to the KB-QA task (Bordes et al., 2014b). Different from previous methods, NNbase"
P17-1021,P13-1042,0,0.0164781,"s are required to not only be familiar with the particular language grammars, but also be aware of the architectures of the KBs. By contrast, knowledge base-based question answering (KB-QA) (Unger et al., 2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers. Recently, with the progress of de"
P17-1021,J84-3009,0,0.10831,"Missing"
P17-1021,P15-1026,0,0.527104,"-QA) (Unger et al., 2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers. Recently, with the progress of deep learning, neural network-based (NN-based) methods have been introduced to the KB-QA task (Bordes et al., 2014b). Different from previous methods, NNbased methods represent both of"
P17-1021,P15-1033,0,0.00797898,"n different words of the same question. The extent of attention can be measured by the relatedness between each word representation hj and an answer aspect embedding ei . We propose the following formulas to calculate the weights. KB Embedding Matrix ?? ?6 ?? ?? ?? ?? Figure 2: The architecture of the proposed crossattention based neural network. Note that only one aspect(in orange color) is depicted for clarity. The other three aspects follow the same way. be effective in many natural language processing (NLP) tasks such as machine translation (Sutskever et al., 2014) and dependency parsing (Dyer et al., 2015), and it is adept in harnessing long sentences. Note that if we use unidirectional LSTM, the outcome of a specific word contains only the information of the words before it, whereas the words after it are not taken into account. To avoid this, we employ bidirectional LSTM as Bahdanau (2015) does, which consists of both forward and backward networks. The forward LSTM handles the question from left to right, and the backward LSTM processes in the reverse order. Thus, we could acquire two hidden state sequences, one − → − → − → from the forward one (h1 , h2 , ..., hn ) and the other ← − ← − ← − f"
P17-1021,P16-1122,1,0.323366,"tion mechanism has been widely used in different areas. Bahdanau et al. (2015) first applied attention model in NLP. They improved 228 References the encoder-decoder Neural Machine Translation (NMT) framework by jointly learning align and translation. They argued that representing source sentence by a fixed vector is unreasonable, and proposed a soft-align method, which could be understood as attention mechanism. Rush et al. (2015) implemented sentence-level summarization task. They utilized local attention-based model that generated each word of the summary conditioned on the input sentence. Wang et al. (2016) proposed an inner attention mechanism that the attention was imposed directly to the input. And their experiment on answer selection showed the advantage of inner attention compared with traditional attention methods. Yin et al. (2016) tackled simple question answering by an attentive convolutional neural network. They stacked an attentive max-pooling above convolution layer to model the relationship between predicates and question patterns. Our approach differs from previous work in that we use attentions to help represent questions dynamically, not generating current word from vocabulary as"
P17-1021,D11-1142,0,0.0123893,"ry networks framework (Sukhbaatar et al., 2015), and achieves the state-of-the-art performance of endto-end methods. Our approach employs bidirectional LSTM, cross-attention model and global KB information. From the results, we observe that our approach achieves the best performance of all the end-to-end methods on WebQuestions. Bordes et al. (2014b; 2014a; 2015) all utilize BOW model to represent the questions, while ours takes advantage of the attention of answer aspects to dynamically represent the questions. Also note that Bordes et al. (2015) uses additional training data such as Reverb (Fader et al., 2011) and their original dataset SimpleQuestions. Dong et al. (2015) employs three fixed CNNs to represent questions, while ours is able to express the focus of each unique answer aspect to the words in the question. Besides, our approach employs the global KB information. So, we believe that the results faithfully show that the proposed approach is more effective than the other competitive methods. Model Analysis In this part, we further discuss the impacts of the components of our model. Table 2 indicates the effectiveness of different parts in the model. Methods es the last hidden state as the q"
P17-1021,C16-1226,0,0.0424091,"2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers. Recently, with the progress of deep learning, neural network-based (NN-based) methods have been introduced to the KB-QA task (Bordes et al., 2014b). Different from previous methods, NNbased methods represent both of the questions and"
P17-1021,P16-1220,0,0.128615,"2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers. Recently, with the progress of deep learning, neural network-based (NN-based) methods have been introduced to the KB-QA task (Bordes et al., 2014b). Different from previous methods, NNbased methods represent both of the questions and"
P17-1021,D14-1071,0,0.0745986,"KB facts. Bordes et al. (2014a) further improved their work by proposing the concept of subgraph embeddings. The key idea was to involve as much information as possible in the answer end. Besides the answer triple, the subgraph contained all the entities and relations connected to the answer entity. The final vector was also obtained by bag-of-words strategy. Yih et al. (2014) focused on single-relation questions. The KB-QA task was divided into two steps. Firstly, they found the topic entity of the question. Then, the rest of the question was represented by CNNs and used to match relations. Yang et al. (2014) tackled entity and relation mapping as joint procedures. Actually, these two methods followed the SP-based manner, but they took advantage of neural networks to obtain intermediate mapping results. The most similar work to ours is Dong et al. (2015). They considered the different aspects of answers, using three columns of CNNs to represent questions respectively. The difference is that our approach uses cross-attention mechanism for each unique answer aspect, so the question representation is not fixed to only three types. Moreover, we utilize the global KB information. Xu et al. (2016a; 2016"
P17-1021,D13-1161,0,0.0155682,"such query languages, users are required to not only be familiar with the particular language grammars, but also be aware of the architectures of the KBs. By contrast, knowledge base-based question answering (KB-QA) (Unger et al., 2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers. Recently, wi"
P17-1021,P14-1090,0,0.199154,"Missing"
P17-1021,Q16-1010,0,0.0486223,"Missing"
P17-1021,P15-1128,0,0.348258,"h the particular language grammars, but also be aware of the architectures of the KBs. By contrast, knowledge base-based question answering (KB-QA) (Unger et al., 2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers. Recently, with the progress of deep learning, neural network-based (NN-b"
P17-1021,D15-1044,0,0.0237223,"ssive power of Semantic Web data while at the same time hiding their complexity behind an intuitive and easy-to-use interface. At the same time the 5.2 Attention-based Model The attention mechanism has been widely used in different areas. Bahdanau et al. (2015) first applied attention model in NLP. They improved 228 References the encoder-decoder Neural Machine Translation (NMT) framework by jointly learning align and translation. They argued that representing source sentence by a fixed vector is unreasonable, and proposed a soft-align method, which could be understood as attention mechanism. Rush et al. (2015) implemented sentence-level summarization task. They utilized local attention-based model that generated each word of the summary conditioned on the input sentence. Wang et al. (2016) proposed an inner attention mechanism that the attention was imposed directly to the input. And their experiment on answer selection showed the advantage of inner attention compared with traditional attention methods. Yin et al. (2016) tackled simple question answering by an attentive convolutional neural network. They stacked an attentive max-pooling above convolution layer to model the relationship between pred"
P17-1021,P14-2105,0,0.0745915,"imilarity could be used to find the most possible answer. BOW method was employed to obtain a single vector for both the questions and the answers. Pairwise training was utilized, and the negative examples were randomly selected from the KB facts. Bordes et al. (2014a) further improved their work by proposing the concept of subgraph embeddings. The key idea was to involve as much information as possible in the answer end. Besides the answer triple, the subgraph contained all the entities and relations connected to the answer entity. The final vector was also obtained by bag-of-words strategy. Yih et al. (2014) focused on single-relation questions. The KB-QA task was divided into two steps. Firstly, they found the topic entity of the question. Then, the rest of the question was represented by CNNs and used to match relations. Yang et al. (2014) tackled entity and relation mapping as joint procedures. Actually, these two methods followed the SP-based manner, but they took advantage of neural networks to obtain intermediate mapping results. The most similar work to ours is Dong et al. (2015). They considered the different aspects of answers, using three columns of CNNs to represent questions respectiv"
P17-1021,P16-2033,0,0.0279007,"Missing"
P17-1021,C16-1164,0,0.445608,"Missing"
P17-1021,P09-1110,0,0.0250529,"Seaborne, 2008). However, to handle such query languages, users are required to not only be familiar with the particular language grammars, but also be aware of the architectures of the KBs. By contrast, knowledge base-based question answering (KB-QA) (Unger et al., 2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from"
P18-1103,P12-3007,0,0.0686957,"Missing"
P18-1103,D17-1230,0,0.0913954,"Missing"
P18-1103,P17-1103,0,0.0235977,"Missing"
P18-1103,W15-4640,0,0.724654,"atently points to “what packages are installed on my system”, the question that speaker A wants to double-check. Previous studies show that capturing those matched segment pairs at different granularities across context and response is the key to multiturn response selection (Wu et al., 2017). However, existing models only consider the textual relevance, which suffers from matching response that latently depends on previous turns. Moreover, Recurrent Neural Networks (RNN) are conveniently used for encoding texts, which is too costly to use for capturing multi-grained semantic representations (Lowe et al., 2015; Zhou et al., 2016; Wu et al., 2017). As an alternative, we propose to match a response with multi-turn context using dependency information based entirely on attention mechanism. Our solution is inspired by the recently proposed Transformer in machine translation (Vaswani et al., 2017), which addresses the issue of sequence-to-sequence generation only using attention, and we extend the key attention mechanism of Transformer in two ways: self-attention By making a sentence attend to itself, we can capture its intra word-level dependencies. Phrases, such as “debian package manager”, can be mod"
P18-1103,Q16-1019,0,0.0606777,"Missing"
P18-1103,D11-1054,0,0.315852,"Missing"
P18-1103,D16-1036,1,0.816807,"in retrieval-based chatbots (Ji et al., 2014), response selection models have been used in automatic evaluation of dialogue generation ∗ Equally contributed. Work done as a visiting scholar at Baidu. Wayne Xin Zhao is an associate professor of Renmin University of China and can be reached at batmanfly@ruc.edu.cn. † Early studies on response selection only use the last utterance in context for matching a reply, which is referred to as single-turn response selection (Wang et al., 2013). Recent works show that the consideration of a multi-turn context can facilitate selecting the next utterance (Zhou et al., 2016; Wu et al., 2017). The reason why richer contextual information works is that human generated responses are heavily dependent on the previous dialogue segments at different granularities (words, phrases, sentences, etc), both semantically and functionally, over multiple turns rather than one turn (Lee et al., 2006; Traum and Heeman, 1996). Figure 1 illustrates semantic connectivities between segment pairs across context and response. As demonstrated, generally there are two kinds of matched segment pairs at different granularities across context and response: (1) surface text relevance, for e"
P18-1103,J00-3003,0,0.701924,"Missing"
P18-1103,D13-1096,0,0.724319,"to select the bestmatched response from a set of candidates given the context of a conversation. Besides playing a critical role in retrieval-based chatbots (Ji et al., 2014), response selection models have been used in automatic evaluation of dialogue generation ∗ Equally contributed. Work done as a visiting scholar at Baidu. Wayne Xin Zhao is an associate professor of Renmin University of China and can be reached at batmanfly@ruc.edu.cn. † Early studies on response selection only use the last utterance in context for matching a reply, which is referred to as single-turn response selection (Wang et al., 2013). Recent works show that the consideration of a multi-turn context can facilitate selecting the next utterance (Zhou et al., 2016; Wu et al., 2017). The reason why richer contextual information works is that human generated responses are heavily dependent on the previous dialogue segments at different granularities (words, phrases, sentences, etc), both semantically and functionally, over multiple turns rather than one turn (Lee et al., 2006; Traum and Heeman, 1996). Figure 1 illustrates semantic connectivities between segment pairs across context and response. As demonstrated, generally there"
P18-1103,P17-1046,0,0.55238,"chatbots (Ji et al., 2014), response selection models have been used in automatic evaluation of dialogue generation ∗ Equally contributed. Work done as a visiting scholar at Baidu. Wayne Xin Zhao is an associate professor of Renmin University of China and can be reached at batmanfly@ruc.edu.cn. † Early studies on response selection only use the last utterance in context for matching a reply, which is referred to as single-turn response selection (Wang et al., 2013). Recent works show that the consideration of a multi-turn context can facilitate selecting the next utterance (Zhou et al., 2016; Wu et al., 2017). The reason why richer contextual information works is that human generated responses are heavily dependent on the previous dialogue segments at different granularities (words, phrases, sentences, etc), both semantically and functionally, over multiple turns rather than one turn (Lee et al., 2006; Traum and Heeman, 1996). Figure 1 illustrates semantic connectivities between segment pairs across context and response. As demonstrated, generally there are two kinds of matched segment pairs at different granularities across context and response: (1) surface text relevance, for example the lexical"
P18-1178,P16-1223,0,0.0449865,"answer based on three factors: the answer boundary, the answer content and the cross-passage answer verification. The experimental results show that our method outperforms the baseline by a large margin and achieves the state-of-the-art performance on the English MS-MARCO dataset and the Chinese DuReader dataset, both of which are designed for MRC in real-world settings. 1 Introduction Machine reading comprehension (MRC), empowering computers with the ability to acquire knowledge and answer questions from textual data, is believed to be a crucial step in building a general intelligent agent (Chen et al., 2016). Recent years have seen rapid growth in the MRC community. With the release of various datasets, the MRC task has evolved from the early cloze-style test (Hermann et al., 2015; Hill et al., 2015) to answer extraction from a single passage (Rajpurkar et al., 2016) and to the latest more complex question answering on web data (Nguyen et al., 2016; Dunn et al., 2017; He et al., 2017). Great efforts have also been made to develop models for these MRC tasks , especially for the answer extraction on single passage (Wang and Jiang, 2016; Seo et al., 2016; Pan et al., 2017). A significant milestone i"
P18-1178,W18-2605,1,0.891557,"Missing"
P18-1178,D17-1215,0,0.0195692,"h question, they use the search engine to retrieve multiple passages and the MRC models are required to read these passages in order to give the final answer. One of the intrinsic challenges for such multipassage MRC is that since all the passages are question-related but usually independently written, it’s probable that multiple confusing answer candidates (correct or incorrect) exist. Table 1 shows an example from MS-MARCO. We can see that all the answer candidates have semantic matching with the question while they are literally different and some of them are even incorrect. As is shown by Jia and Liang (2017), these confusing answer candidates could be quite difficult for MRC models to distinguish. Therefore, special consideration is required for such multi-passage MRC problem. In this paper, we propose to leverage the answer candidates from different passages to verify the final correct answer and rule out the noisy incorrect answers. Our hypothesis is that the cor* This work was done while the first author was doing internship at Baidu Inc. 1 https://rajpurkar.github.io/SQuAD-explorer/ 1918 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pag"
P18-1178,P17-1147,0,0.0608865,"ages. For the model training, Xiong et al. (2017) argues that the boundary loss encourages exact answers at the cost of penalizing overlapping answers. Therefore they propose a mixed objective that incorporates rewards derived from word overlap. Our joint training approach has a similar function. By taking the content and verification loss into consideration, our model will give less loss for overlapping answers than those unmatched answers, and our loss function is totally differentiable. Recently, we also see emerging interests in multi-passage MRC from both the academic (Dunn et al., 2017; Joshi et al., 2017) and industrial community (Nguyen et al., 2016; He et al., 2017). Early studies (Shen et al., 2017; Wang et al., 2017c) usually concat those passages and employ the same models designed for singlepassage MRC. However, more and more latest studies start to design specific methods that can read multiple passages more effectively. In the aspect of passage selection, Wang et al. (2017a) introduced a pipelined approach that rank the passages first and then read the selected passages for answering questions. Tan et al. (2017) treats the passage ranking as an auxiliary task that can be trained jointl"
P18-1178,P14-5010,0,0.00415096,"s one single answer here. Therefore, we also report the proportion of questions that have multiple answer spans to match with the human-generated answers. A span is taken as valid if it can achieve F1 score larger than 0.7 compared with any reference answer. From these statistics, we can see that the phenomenon of multiple answers is quite common for both MS-MARCO and DuReader. These answers will provide strong signals for answer verification if we can leverage them properly. 3.2 Implementation Details For MS-MARCO, we preprocess the corpus with the reversible tokenizer from Stanford CoreNLP (Manning et al., 2014) and we choose the span that achieves the highest ROUGE-L score with the reference answers as the gold span for training. We employ the 300-D pre-trained Glove embeddings (Pennington et al., 2014) and keep it fixed during training. The character embeddings are randomly initialized with its dimension as 30. For DuReader, we follow the preprocessing described in He et al. (2017). We tune the hyper-parameters according to the 1922 Model FastQA Ext (Weissenborn et al., 2017) Prediction (Wang and Jiang, 2016) ReasoNet (Shen et al., 2017) R-Net (Wang et al., 2017c) S-Net (Tan et al., 2017) Our Model"
P18-1178,P02-1040,0,0.101008,"or “No”. For DuReader, the retrieved document usually contains a large number of paragraphs that cannot be fed into MRC models directly (He et al., 2017). The original paper employs a simple a simple heuristic strategy to select a representative paragraph for each document, while we train a paragraph ranking model for this. We will demonstrate the effects of these two technologies later. 3.3 Results on MS-MARCO Table 3 shows the results of our system and other state-of-the-art models on the MS-MARCO test set. We adopt the official evaluation metrics, including ROUGE-L (Lin, 2004) and BLEU-1 (Papineni et al., 2002). As we can see, for both metrics, our single model outperforms all the other competing models with an evident margin, which is extremely hard considering the near-human perModel Match-LSTM BiDAF PR + BiDAF Our Model Human BLEU-4 31.8 31.9 37.55 40.97 56.1 ROUGE-L 39.0 39.2 41.81 44.18 57.4 Table 4: Performance on the DuReader test set Model Complete Model Answer Verification Content Modeling Joint Training YesNo Classification Boundary Baseline ROUGE-L 45.65 44.38 44.27 44.12 41.87 38.95 ∆ -1.27 -1.38 -1.53 -3.78 -6.70 Table 5: Ablation study on MS-MARCO development set formance. If we ensemb"
P18-1178,D14-1162,0,0.0827217,"ieve F1 score larger than 0.7 compared with any reference answer. From these statistics, we can see that the phenomenon of multiple answers is quite common for both MS-MARCO and DuReader. These answers will provide strong signals for answer verification if we can leverage them properly. 3.2 Implementation Details For MS-MARCO, we preprocess the corpus with the reversible tokenizer from Stanford CoreNLP (Manning et al., 2014) and we choose the span that achieves the highest ROUGE-L score with the reference answers as the gold span for training. We employ the 300-D pre-trained Glove embeddings (Pennington et al., 2014) and keep it fixed during training. The character embeddings are randomly initialized with its dimension as 30. For DuReader, we follow the preprocessing described in He et al. (2017). We tune the hyper-parameters according to the 1922 Model FastQA Ext (Weissenborn et al., 2017) Prediction (Wang and Jiang, 2016) ReasoNet (Shen et al., 2017) R-Net (Wang et al., 2017c) S-Net (Tan et al., 2017) Our Model S-Net (Ensemble) Our Model (Ensemble) Human ROUGE-L 33.67 37.33 38.81 42.89 45.23 46.15 46.65 46.66 47 BLEU-1 33.93 40.72 39.86 42.22 43.78 44.47 44.78 45.41 46 Table 3: Performance of our method"
P18-1178,C02-1169,0,0.176529,"Missing"
P18-1178,P17-1018,0,0.637994,"we formally present the details of modeling the question and passages. Encoding We first map each word into the vector space by concatenating its word embedding and sum of its character embeddings. Then we employ bi-directional LSTMs (BiLSTM) to encode the question Q and passages {Pi } as follows: Q Q Q uQ t = BiLSTMQ (ut−1 , [et , ct ]) (1) i uPt i = BiLSTMP (uPt−1 , [ePt i , cPt i ]) (2) Q Pi Pi where eQ t , ct , et , ct are the word-level and character-level embeddings of the tth word. uQ t and uPt i are the encoding vectors of the tth words in Q and Pi respectively. Unlike previous work (Wang et al., 2017c) that simply concatenates all the passages, we process the passages independently at the encoding and matching steps. Q-P Matching One essential step in MRC is to match the question with passages so that important information can be highlighted. We use the Attention Flow Layer (Seo et al., 2016) to conduct the Q-P matching in two directions. The similarity matrix S ∈ R|Q|×|Pi |between the question and passage i is changed to a simpler version, where the similarity between the tth word in the question and the k th word in passage i is computed as: | Pi St,k = uQ t · uk (3) Then the context-to"
P18-1178,K17-1028,0,0.0888184,"Missing"
P18-1178,D16-1264,0,\N,Missing
P19-1135,D18-3004,0,0.0283887,"Missing"
P19-1135,P16-1200,0,0.723213,"New Conﬁdent Attention Based Conﬁdence Calculation Pattern Set Instances Selecting Conﬁdent Unconﬁdent New Conﬁdent Dataset Epoch k Figure 3: An overview of our ARNOR framework. It is based on a BiLSTM with attention mechanism and utilizes attention regularization to force the model to attend the corresponding relation patterns. Then, an instance selector calculates a confidence score for each training instance to generate a new redistributed training set and a new trustable pattern set. These two steps are run iteratively to form a bootstrap learning procedure. learning (Riedel et al., 2010; Lin et al., 2016; Surdeanu et al., 2012; Zeng et al., 2015). However, it models noise problem on a bag of instances and is not suitable for sentence-level prediction. The second kind of approach utilizes RL (Feng et al., 2018b; Xiangrong et al., 2018; Qin et al., 2018b) or adversarial training (Qin et al., 2018a; Han et al., 2018) to select trustable instances. The third research line relies on patterns (Hearst, 1992; Hamon and Nazarenko, 2001). Takamatsu et al. (2012) directly models the labeling process of DS to find noisy patterns. Ratner et al. (2016, 2017) proposes to fuse DS-based labels and manual rela"
P19-1135,P09-1113,0,0.949698,"tural language processing (NLP) and is particularly important for knowledge base construction. The goal of RC (Zelenko et al., 2003) is to identify the relation type of a given entity pair in a sentence. Generally, a relation should be explicitly expressed by some clue words. See the first sentence in Figure 1. The phrase “was born in” explains the relation type “place of birth” for “Bill Lockyer” and “California”. Such indicating words is called patterns (Hearst, 1992; Hamon and Nazarenko, 2001). In order to cheaply obtain a large amount of labeled RC training data, Distant Supervision (DS) (Mintz et al., 2009) was proposed to automatically generate training data by aligning a knowledge base with an unlabeled corpus. It is built on a weak assumption that if an entity pair have a relationship in a knowledge base, all sentences that contain this pair will express the corresponding relation. Unfortunately, DS obviously brings plenty of noisy data, which may significantly reduce the 1399 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1399–1408 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics performance of an RC m"
P19-1135,P18-1046,0,0.604067,"multiinstance learning (Riedel et al., 2010; Lin et al., 2016; Surdeanu et al., 2012; Zeng et al., 2015) relaxes the DS assumption as at-least-one. In a bag of sentences that mention the same entity pair, it assumes that at least one sentence expresses the relation. Multi-instance learning carries out classification on bag-level and often fails to perform well on sentence-level prediction (Feng et al., 2018b). Secondly, in order to reduce noise for sentence-level prediction, researchers then resort to reinforcement learning or adversarial training to select trustable data (Feng et al., 2018b; Qin et al., 2018a; Han et al., 2018; Xiangrong et al., 2018; Qin et al., 2018b). This line of research selects confident relation labels by matching the predicted label of the learned model with DSgenerated label. As the model is also learned from DS data, it might still fail when model predictions and DS-generated labels are both wrong. The third method relies on relation patterns. Pattern-based extraction is widely used in information extraction (Hearst, 1992; Hamon and Nazarenko, 2001). Among them, the generative model (Takamatsu et al., 2012) directly models the labeling process of DS and finds noisy patt"
P19-1135,C92-2082,0,0.585652,"m the fact that DS method only depends on entities for labeling data. Introduction Relation Classification (RC) is a fundamental task in natural language processing (NLP) and is particularly important for knowledge base construction. The goal of RC (Zelenko et al., 2003) is to identify the relation type of a given entity pair in a sentence. Generally, a relation should be explicitly expressed by some clue words. See the first sentence in Figure 1. The phrase “was born in” explains the relation type “place of birth” for “Bill Lockyer” and “California”. Such indicating words is called patterns (Hearst, 1992; Hamon and Nazarenko, 2001). In order to cheaply obtain a large amount of labeled RC training data, Distant Supervision (DS) (Mintz et al., 2009) was proposed to automatically generate training data by aligning a knowledge base with an unlabeled corpus. It is built on a weak assumption that if an entity pair have a relationship in a knowledge base, all sentences that contain this pair will express the corresponding relation. Unfortunately, DS obviously brings plenty of noisy data, which may significantly reduce the 1399 Proceedings of the 57th Annual Meeting of the Association for Computation"
P19-1135,P11-1055,0,0.158327,"ich is a news corpus sampled from 294k 1989-2007 New York Times news articles and is first presented in (Riedel et al., 2010). Most previous work commonly generates training instances by aligning entity pairs from Freebase and adopt held-out evaluation to evaluate without costly human annotation. Such an evaluation can only provide an approximate measure due to the noisy test set that is also generated by distant supervision. In contrast, Ren et al. (2017) publishes a training set which is also generated by distant supervision, but a manuallyannotated test set that contains 395 sentences from Hoffmann et al. (2011). However, we find that this test set was annotated with only one entity pair for one sentence. Not all of the triplets in these sentences are marked out. In addition, although there are enough test instances (3,880 including “None” type), the number of positive ones is relatively small (only 396). Moreover, the test set only contains half of the relation types of the training set. To address these issues and evaluate our ARNOR framework more precisely, we annotate and publish a new sentence-level test set (the source address is in section 1) on the basis of the one released by Ren et al. (201"
P19-1135,P18-1199,0,0.691974,"multiinstance learning (Riedel et al., 2010; Lin et al., 2016; Surdeanu et al., 2012; Zeng et al., 2015) relaxes the DS assumption as at-least-one. In a bag of sentences that mention the same entity pair, it assumes that at least one sentence expresses the relation. Multi-instance learning carries out classification on bag-level and often fails to perform well on sentence-level prediction (Feng et al., 2018b). Secondly, in order to reduce noise for sentence-level prediction, researchers then resort to reinforcement learning or adversarial training to select trustable data (Feng et al., 2018b; Qin et al., 2018a; Han et al., 2018; Xiangrong et al., 2018; Qin et al., 2018b). This line of research selects confident relation labels by matching the predicted label of the learned model with DSgenerated label. As the model is also learned from DS data, it might still fail when model predictions and DS-generated labels are both wrong. The third method relies on relation patterns. Pattern-based extraction is widely used in information extraction (Hearst, 1992; Hamon and Nazarenko, 2001). Among them, the generative model (Takamatsu et al., 2012) directly models the labeling process of DS and finds noisy patt"
P19-1135,D12-1042,0,0.209971,"tion Based Conﬁdence Calculation Pattern Set Instances Selecting Conﬁdent Unconﬁdent New Conﬁdent Dataset Epoch k Figure 3: An overview of our ARNOR framework. It is based on a BiLSTM with attention mechanism and utilizes attention regularization to force the model to attend the corresponding relation patterns. Then, an instance selector calculates a confidence score for each training instance to generate a new redistributed training set and a new trustable pattern set. These two steps are run iteratively to form a bootstrap learning procedure. learning (Riedel et al., 2010; Lin et al., 2016; Surdeanu et al., 2012; Zeng et al., 2015). However, it models noise problem on a bag of instances and is not suitable for sentence-level prediction. The second kind of approach utilizes RL (Feng et al., 2018b; Xiangrong et al., 2018; Qin et al., 2018b) or adversarial training (Qin et al., 2018a; Han et al., 2018) to select trustable instances. The third research line relies on patterns (Hearst, 1992; Hamon and Nazarenko, 2001). Takamatsu et al. (2012) directly models the labeling process of DS to find noisy patterns. Ratner et al. (2016, 2017) proposes to fuse DS-based labels and manual relation patterns for reduc"
P19-1135,P12-1076,0,0.0211777,"ining set and a new trustable pattern set. These two steps are run iteratively to form a bootstrap learning procedure. learning (Riedel et al., 2010; Lin et al., 2016; Surdeanu et al., 2012; Zeng et al., 2015). However, it models noise problem on a bag of instances and is not suitable for sentence-level prediction. The second kind of approach utilizes RL (Feng et al., 2018b; Xiangrong et al., 2018; Qin et al., 2018b) or adversarial training (Qin et al., 2018a; Han et al., 2018) to select trustable instances. The third research line relies on patterns (Hearst, 1992; Hamon and Nazarenko, 2001). Takamatsu et al. (2012) directly models the labeling process of DS to find noisy patterns. Ratner et al. (2016, 2017) proposes to fuse DS-based labels and manual relation patterns for reducing noise. Feng et al. (2018a) presents a pattern extractor based on RL and uses extracted patterns as features for RC. 3 The ARNOR Framework In this paper, we reduce DS noise and make the model more interpretable according to the observation that a relation should be expressed by its sentence context. Generally, RC classifier should rely on relation patterns to decide the relation type for a pair of entities. Thus, for a training"
P19-1135,D15-1203,0,0.756746,"lculation Pattern Set Instances Selecting Conﬁdent Unconﬁdent New Conﬁdent Dataset Epoch k Figure 3: An overview of our ARNOR framework. It is based on a BiLSTM with attention mechanism and utilizes attention regularization to force the model to attend the corresponding relation patterns. Then, an instance selector calculates a confidence score for each training instance to generate a new redistributed training set and a new trustable pattern set. These two steps are run iteratively to form a bootstrap learning procedure. learning (Riedel et al., 2010; Lin et al., 2016; Surdeanu et al., 2012; Zeng et al., 2015). However, it models noise problem on a bag of instances and is not suitable for sentence-level prediction. The second kind of approach utilizes RL (Feng et al., 2018b; Xiangrong et al., 2018; Qin et al., 2018b) or adversarial training (Qin et al., 2018a; Han et al., 2018) to select trustable instances. The third research line relies on patterns (Hearst, 1992; Hamon and Nazarenko, 2001). Takamatsu et al. (2012) directly models the labeling process of DS to find noisy patterns. Ratner et al. (2016, 2017) proposes to fuse DS-based labels and manual relation patterns for reducing noise. Feng et a"
P19-1135,C14-1220,0,0.772359,"can be explained by the model. 2. Our ARNOR framework achieves significant improvement over state-of-the-art noise reduction methods, in terms of both RC performance and noise reduction effect. 3. We publish a better manually labeled sentence-level test set1 for evaluating the performance of RC models. This test set contains 1,024 sentences and 4,543 entity pairs, and is carefully annotated to ensure accuracy. 2 Related Work We deal with DS-based RC in this paper. For RC task, various models are recently proposed based on different neural architectures, such as convolutional neural networks (Zeng et al., 2014, 2015) and recurrent neural network (Zhang et al., 2015; Zhou et al., 2016). To automatically obtain a large training dataset, DS has been proposed (Mintz et al., 2009). However, DS also introduces noisy data, making DS-based RC more challenging. Previous studies make attempts on kinds of methods to solve the noise problem. The first widely studied method is based on multi-instance 1 The dataset used in this paper is on https: //github.com/PaddlePaddle/models/tree/ develop/PaddleNLP/Research/ACL2019-ARNOR 1400 Pattern Set Relation Output FC Epoch k-1 Attention Regularization Conﬁdent Attentio"
P19-1135,Y15-1009,0,0.125667,"achieves significant improvement over state-of-the-art noise reduction methods, in terms of both RC performance and noise reduction effect. 3. We publish a better manually labeled sentence-level test set1 for evaluating the performance of RC models. This test set contains 1,024 sentences and 4,543 entity pairs, and is carefully annotated to ensure accuracy. 2 Related Work We deal with DS-based RC in this paper. For RC task, various models are recently proposed based on different neural architectures, such as convolutional neural networks (Zeng et al., 2014, 2015) and recurrent neural network (Zhang et al., 2015; Zhou et al., 2016). To automatically obtain a large training dataset, DS has been proposed (Mintz et al., 2009). However, DS also introduces noisy data, making DS-based RC more challenging. Previous studies make attempts on kinds of methods to solve the noise problem. The first widely studied method is based on multi-instance 1 The dataset used in this paper is on https: //github.com/PaddlePaddle/models/tree/ develop/PaddleNLP/Research/ACL2019-ARNOR 1400 Pattern Set Relation Output FC Epoch k-1 Attention Regularization Conﬁdent Attention Target Attention BiLSTM Target Attention . Clifornia i"
P19-1135,P16-2034,0,0.45033,"improvement over state-of-the-art noise reduction methods, in terms of both RC performance and noise reduction effect. 3. We publish a better manually labeled sentence-level test set1 for evaluating the performance of RC models. This test set contains 1,024 sentences and 4,543 entity pairs, and is carefully annotated to ensure accuracy. 2 Related Work We deal with DS-based RC in this paper. For RC task, various models are recently proposed based on different neural architectures, such as convolutional neural networks (Zeng et al., 2014, 2015) and recurrent neural network (Zhang et al., 2015; Zhou et al., 2016). To automatically obtain a large training dataset, DS has been proposed (Mintz et al., 2009). However, DS also introduces noisy data, making DS-based RC more challenging. Previous studies make attempts on kinds of methods to solve the noise problem. The first widely studied method is based on multi-instance 1 The dataset used in this paper is on https: //github.com/PaddlePaddle/models/tree/ develop/PaddleNLP/Research/ACL2019-ARNOR 1400 Pattern Set Relation Output FC Epoch k-1 Attention Regularization Conﬁdent Attention Target Attention BiLSTM Target Attention . Clifornia in born was Lockyer E"
P19-1226,D18-1454,0,0.469513,"ar, presenting new state-of-the-art results in MRC and a wide variety of other language understanding tasks. Owing to the large amounts of unlabeled data and the sufficiently deep architectures used during pre-training, advanced LMs such as BERT are able to capture complex linguistic phenomena, understanding language better than previously appreciated (Peters et al., 2018b; Goldberg, 2019). However, as widely recognized, genuine reading comprehension requires not only language understanding, but also knowledge that supports sophisticated reasoning (Chen et al., 2016; Mihaylov and Frank, 2018; Bauer et al., 2018; Zhong 2346 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2346–2357 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018). Thereby, we argue that pre-trained LMs, despite their powerfulness, could be further improved for MRC by integrating background knowledge. Fig. 1 gives a motivating example from ReCoRD (Zhang et al., 2018). In this example, the passage describes that Sudan faces trade sanctions from US due to its past support for North Korea. The cloze-style question states that Sudan is s"
P19-1226,P04-3031,0,0.16502,"2017), where the WordNet embeddings were pre-trained on a subset consisting of 151,442 triples with 40,943 synsets and 18 relations, and the NELL embeddings pre-trained on a subset containing 180,107 entities and 258 3 In this paper, we restrict ourselves to improvements involving a single model, and hence do not consider ensembles. 2350 concepts. Both groups of embeddings are 100-D. Refer to (Yang and Mitchell, 2017) for details. Then we retrieve knowledge from the two KBs. For WordNet, we employ the BasicTokenizer built in BERT to tokenize text, and look up synsets for each word using NLTK (Bird and Loper, 2004). Synsets within the 40,943 subset are returned as candidate KB concepts for the word. For NELL, we link entity mentions to the whole KB, and return associated concepts within the 258 subset as candidate KB concepts. Entity mentions are given as answer candidates on ReCoRD, and recognized by Stanford CoreNLP (Manning et al., 2014) on SQuAD1.1. Finally, we follow Devlin et al. (2018) and use the FullTokenizer built in BERT to segment words into wordpieces. The maximum question length is set to 64. Questions longer than that are truncated. The maximum input length (|S|) is set to 384. Input sequ"
P19-1226,P16-1223,0,0.0693758,"Missing"
P19-1226,P18-1224,0,0.0556635,"., 2018), ARC (Clark et al., 2018), MCScript (Ostermann et al., 2018), OpenBookQA (Mihaylov et al., 2018) and CommonsenseQA (Talmor et al., 2018). ReCoRD can be viewed as an extractive MRC dataset, while the later four are multi-choice MRC datasets, with relatively smaller size than ReCoRD. In this paper, we focus on the extractive MRC task. Hence, we choose ReCoRD and SQuAD in the experiments. Some previous work attempts to leverage structured knowledge from KBs to deal with the tasks of MRC and QA. Weissenborn et al. (2017), Bauer et al. (2018), Mihaylov and Frank (2018), Pan et al. (2019), Chen et al. (2018), Wang et al. (2018) follow a retrieve-then-encode paradigm, i.e., they first retrieve relevant knowledge from KBs, and only the retrieved knowledge relevant locally to the reading text will be encoded and integrated. By contrast, we leverage pre-trained KB embeddings which encode whole KBs. Then we use attention mechanisms to select and integrate knowledge that is relevant locally to the reading text. Zhong et al. (2018) try to leverage pre-trained KB embeddings to solve the multi-choice MRC task. However, the knowledge and text modules are not integrated,but used independently to predict the"
P19-1226,P18-1078,0,0.012744,"UN 1. nongovorganization: 0.986 2. sentinel: 0.012 3. terroristorganization: 0.001 Concepts from WordNet: ban 1. forbidding_NN_1: 0.861 2. proscription_NN_1: 0.135 3. ban_VB_2: 0.002 sanctions 1. sanction_VB_1: 0.336 2. sanction_NN_3: 0.310 3. sanction_NN_4: 0.282 (a) (a) KT-NET (b) BERT Figure 3: Case study. Heat maps present similarities between question (row) and passage (column) words. Line charts show probabilities of answer boundaries. In KT-NET, top 3 most relevant KB concepts are further given. with 12 self-attention heads and 768 hidden units); (iii) DocQA (Liu et al., 2018) and SAN (Clark and Gardner, 2018) are two previous state-of-the-art MRC models; (iv) the pre-trained LM ELMo (Peters et al., 2018a) is further used in DocQA. All these models, except for DCReader+BERT, were re-implemented by the creators of the dataset and provided as official baselines (Zhang et al., 2018). On SQuAD6 (Table 3): (i) BERT+TriviaQA is the former best model officially submitted by Google. It is an uncased, large model, and further uses data augmentation with TriviaQA (Joshi et al., 2017); (ii) WD, nlnet, and MARS are three competitive models that have not been published; (iii) QANet is a well performing MRC mode"
P19-1226,P17-1055,0,0.0245036,". We observed similar phenomena on SQuAD1.1 and report the results in Appendix B. 5 Related Work Machine Reading Comprehension In the last few years, a number of datasets have been created for MRC, e.g., CNN/DM (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016, 2018), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), and MS-MARCO (Nguyen et al., 2016). These 7 During visualization, we use a row-wise softmax operation to normalize similarity scores over all passage tokens. datasets have led to advances like Match-LSTM (Wang and Jiang, 2017), BiDAF (Seo et al., 2017), AoA Reader (Cui et al., 2017), DCN (Xiong et al., 2017), R-Net (Wang et al., 2017), and QANet (Yu et al., 2018). These end-to-end neural models have similar architectures, starting off with an encoding layer to encode every question/passage word as a vector, passing through various attention-based interaction layers and finally a prediction layer. More recently, LMs such as ELMo (Peters et al., 2018b), GPT (Radford et al., 2018), and BERT (Devlin et al., 2018) have been devised. They pre-train deep LMs on large-scale unlabeled corpora to obtain contextual representations of text. When used in downstream tasks including MR"
P19-1226,P17-1147,0,0.402474,"n) WordNet: (sanctions, common-hypernym-with, ban) Figure 1: An example from ReCoRD, with answer candidates marked (underlined) in the passage. The vanilla BERT model fails to predict the correct answer. But it succeeds after integrating background knowledge collected from WordNet and NELL. Introduction Machine reading comprehension (MRC), which requires machines to comprehend text and answer questions about it, is a crucial task in natural language processing. With the development of deep learning and the increasing availability of datasets (Rajpurkar et al., 2016, 2018; Nguyen et al., 2016; Joshi et al., 2017), MRC has achieved remarkable advancements in the last few years. Recently language model (LM) pre-training has caused a stir in the MRC community. These LMs * This work was done while the first author was an intern at Baidu Inc. † Co-corresponding authors: Hua Wu and Sujian Li. 1 Our code will be available at http://github. com/paddlepaddle/models/tree/develop/ PaddleNLP/Research/ACL2019-KTNET are pre-trained on unlabeled text and then applied to MRC, in either a feature-based (Peters et al., 2018a) or a fine-tuning (Radford et al., 2018) manner, both offering substantial performance boosts."
P19-1226,P18-1157,0,0.0363982,"122 3. organization: 0.003 UN 1. nongovorganization: 0.986 2. sentinel: 0.012 3. terroristorganization: 0.001 Concepts from WordNet: ban 1. forbidding_NN_1: 0.861 2. proscription_NN_1: 0.135 3. ban_VB_2: 0.002 sanctions 1. sanction_VB_1: 0.336 2. sanction_NN_3: 0.310 3. sanction_NN_4: 0.282 (a) (a) KT-NET (b) BERT Figure 3: Case study. Heat maps present similarities between question (row) and passage (column) words. Line charts show probabilities of answer boundaries. In KT-NET, top 3 most relevant KB concepts are further given. with 12 self-attention heads and 768 hidden units); (iii) DocQA (Liu et al., 2018) and SAN (Clark and Gardner, 2018) are two previous state-of-the-art MRC models; (iv) the pre-trained LM ELMo (Peters et al., 2018a) is further used in DocQA. All these models, except for DCReader+BERT, were re-implemented by the creators of the dataset and provided as official baselines (Zhang et al., 2018). On SQuAD6 (Table 3): (i) BERT+TriviaQA is the former best model officially submitted by Google. It is an uncased, large model, and further uses data augmentation with TriviaQA (Joshi et al., 2017); (ii) WD, nlnet, and MARS are three competitive models that have not been published; (iii) Q"
P19-1226,P14-5010,0,0.00277388,"s. 2350 concepts. Both groups of embeddings are 100-D. Refer to (Yang and Mitchell, 2017) for details. Then we retrieve knowledge from the two KBs. For WordNet, we employ the BasicTokenizer built in BERT to tokenize text, and look up synsets for each word using NLTK (Bird and Loper, 2004). Synsets within the 40,943 subset are returned as candidate KB concepts for the word. For NELL, we link entity mentions to the whole KB, and return associated concepts within the 258 subset as candidate KB concepts. Entity mentions are given as answer candidates on ReCoRD, and recognized by Stanford CoreNLP (Manning et al., 2014) on SQuAD1.1. Finally, we follow Devlin et al. (2018) and use the FullTokenizer built in BERT to segment words into wordpieces. The maximum question length is set to 64. Questions longer than that are truncated. The maximum input length (|S|) is set to 384. Input sequences longer than that are segmented into chunks with a stride of 128. The maximum answer length at inference time is set to 30. Comparison Setting We evaluate our approach in three settings: KT-NETWordNet, KT-NETNELL, and KT-NETBOTH, to incorporate knowledge from WordNet, NELL, and both of the two KBs, respectively. We take BERT"
P19-1226,D18-1260,0,0.0467552,"ra to obtain contextual representations of text. When used in downstream tasks including MRC, the pre-trained contextual representations greatly improve the performance in either a fine-tuning or feature-based way. Built upon pre-trained LMs, our work further explores the potential of incorporating structured knowledge from KBs, combining the strengths of both text and knowledge representations. Incorporating KBs Several MRC datasets that require external knowledge have been proposed, such as ReCoRD (Zhang et al., 2018), ARC (Clark et al., 2018), MCScript (Ostermann et al., 2018), OpenBookQA (Mihaylov et al., 2018) and CommonsenseQA (Talmor et al., 2018). ReCoRD can be viewed as an extractive MRC dataset, while the later four are multi-choice MRC datasets, with relatively smaller size than ReCoRD. In this paper, we focus on the extractive MRC task. Hence, we choose ReCoRD and SQuAD in the experiments. Some previous work attempts to leverage structured knowledge from KBs to deal with the tasks of MRC and QA. Weissenborn et al. (2017), Bauer et al. (2018), Mihaylov and Frank (2018), Pan et al. (2019), Chen et al. (2018), Wang et al. (2018) follow a retrieve-then-encode paradigm, i.e., they first retrieve"
P19-1226,P18-1076,0,0.485861,"y the most successful by far, presenting new state-of-the-art results in MRC and a wide variety of other language understanding tasks. Owing to the large amounts of unlabeled data and the sufficiently deep architectures used during pre-training, advanced LMs such as BERT are able to capture complex linguistic phenomena, understanding language better than previously appreciated (Peters et al., 2018b; Goldberg, 2019). However, as widely recognized, genuine reading comprehension requires not only language understanding, but also knowledge that supports sophisticated reasoning (Chen et al., 2016; Mihaylov and Frank, 2018; Bauer et al., 2018; Zhong 2346 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2346–2357 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018). Thereby, we argue that pre-trained LMs, despite their powerfulness, could be further improved for MRC by integrating background knowledge. Fig. 1 gives a motivating example from ReCoRD (Zhang et al., 2018). In this example, the passage describes that Sudan faces trade sanctions from US due to its past support for North Korea. The cloze-style question st"
P19-1226,P18-2124,0,0.023982,"tes questions that require external knowledge and reasoning. It also filters out questions that can be answered simply by pattern matching, posing further challenges to current MRC systems. We take it as the major testbed for evaluating our approach. SQuAD1.1 (Rajpurkar et al., 2016) is a wellknown extractive MRC dataset that consists of questions created by crowdworkers for Wikipedia articles. The golden answer to each question is a span from the corresponding passage. In this paper, we focus more on answerable questions than unanswerable ones. Hence, we choose SQuAD1.1 rather than SQuAD2.0 (Rajpurkar et al., 2018). Table 1 provides the statistics of ReCoRD and SQuAD1.1. On both datasets, the training and development (dev) sets are publicly available, but the test set is hidden. One has to submit the code to retrieve the final test score. As frequent submissions to probe the unseen test set are not encouraged, we only submit our best single model for testing,3 and conduct further analysis on the dev set. Both datasets use Exact Match (EM) and (macro-averaged) F1 as the evaluation metrics (Zhang et al., 2018). 3.2 Experimental Setups Data Preprocessing We first prepare pre-trained KB embeddings. We use t"
P19-1226,D16-1264,0,0.525999,"rdNet: (government, same-synset-with, administration) WordNet: (sanctions, common-hypernym-with, ban) Figure 1: An example from ReCoRD, with answer candidates marked (underlined) in the passage. The vanilla BERT model fails to predict the correct answer. But it succeeds after integrating background knowledge collected from WordNet and NELL. Introduction Machine reading comprehension (MRC), which requires machines to comprehend text and answer questions about it, is a crucial task in natural language processing. With the development of deep learning and the increasing availability of datasets (Rajpurkar et al., 2016, 2018; Nguyen et al., 2016; Joshi et al., 2017), MRC has achieved remarkable advancements in the last few years. Recently language model (LM) pre-training has caused a stir in the MRC community. These LMs * This work was done while the first author was an intern at Baidu Inc. † Co-corresponding authors: Hua Wu and Sujian Li. 1 Our code will be available at http://github. com/paddlepaddle/models/tree/develop/ PaddleNLP/Research/ACL2019-KTNET are pre-trained on unlabeled text and then applied to MRC, in either a feature-based (Peters et al., 2018a) or a fine-tuning (Radford et al., 2018) manner"
P19-1226,S18-1119,0,0.0163245,"ep LMs on large-scale unlabeled corpora to obtain contextual representations of text. When used in downstream tasks including MRC, the pre-trained contextual representations greatly improve the performance in either a fine-tuning or feature-based way. Built upon pre-trained LMs, our work further explores the potential of incorporating structured knowledge from KBs, combining the strengths of both text and knowledge representations. Incorporating KBs Several MRC datasets that require external knowledge have been proposed, such as ReCoRD (Zhang et al., 2018), ARC (Clark et al., 2018), MCScript (Ostermann et al., 2018), OpenBookQA (Mihaylov et al., 2018) and CommonsenseQA (Talmor et al., 2018). ReCoRD can be viewed as an extractive MRC dataset, while the later four are multi-choice MRC datasets, with relatively smaller size than ReCoRD. In this paper, we focus on the extractive MRC task. Hence, we choose ReCoRD and SQuAD in the experiments. Some previous work attempts to leverage structured knowledge from KBs to deal with the tasks of MRC and QA. Weissenborn et al. (2017), Bauer et al. (2018), Mihaylov and Frank (2018), Pan et al. (2019), Chen et al. (2018), Wang et al. (2018) follow a retrieve-then-encode"
P19-1226,S18-1120,0,0.0120194,"et al., 2018), MCScript (Ostermann et al., 2018), OpenBookQA (Mihaylov et al., 2018) and CommonsenseQA (Talmor et al., 2018). ReCoRD can be viewed as an extractive MRC dataset, while the later four are multi-choice MRC datasets, with relatively smaller size than ReCoRD. In this paper, we focus on the extractive MRC task. Hence, we choose ReCoRD and SQuAD in the experiments. Some previous work attempts to leverage structured knowledge from KBs to deal with the tasks of MRC and QA. Weissenborn et al. (2017), Bauer et al. (2018), Mihaylov and Frank (2018), Pan et al. (2019), Chen et al. (2018), Wang et al. (2018) follow a retrieve-then-encode paradigm, i.e., they first retrieve relevant knowledge from KBs, and only the retrieved knowledge relevant locally to the reading text will be encoded and integrated. By contrast, we leverage pre-trained KB embeddings which encode whole KBs. Then we use attention mechanisms to select and integrate knowledge that is relevant locally to the reading text. Zhong et al. (2018) try to leverage pre-trained KB embeddings to solve the multi-choice MRC task. However, the knowledge and text modules are not integrated,but used independently to predict the answer. And the mod"
P19-1226,D19-5804,0,0.116397,"ReCoRD (Zhang et al., 2018), ARC (Clark et al., 2018), MCScript (Ostermann et al., 2018), OpenBookQA (Mihaylov et al., 2018) and CommonsenseQA (Talmor et al., 2018). ReCoRD can be viewed as an extractive MRC dataset, while the later four are multi-choice MRC datasets, with relatively smaller size than ReCoRD. In this paper, we focus on the extractive MRC task. Hence, we choose ReCoRD and SQuAD in the experiments. Some previous work attempts to leverage structured knowledge from KBs to deal with the tasks of MRC and QA. Weissenborn et al. (2017), Bauer et al. (2018), Mihaylov and Frank (2018), Pan et al. (2019), Chen et al. (2018), Wang et al. (2018) follow a retrieve-then-encode paradigm, i.e., they first retrieve relevant knowledge from KBs, and only the retrieved knowledge relevant locally to the reading text will be encoded and integrated. By contrast, we leverage pre-trained KB embeddings which encode whole KBs. Then we use attention mechanisms to select and integrate knowledge that is relevant locally to the reading text. Zhong et al. (2018) try to leverage pre-trained KB embeddings to solve the multi-choice MRC task. However, the knowledge and text modules are not integrated,but used independ"
P19-1226,N18-1202,0,0.720563,"and the increasing availability of datasets (Rajpurkar et al., 2016, 2018; Nguyen et al., 2016; Joshi et al., 2017), MRC has achieved remarkable advancements in the last few years. Recently language model (LM) pre-training has caused a stir in the MRC community. These LMs * This work was done while the first author was an intern at Baidu Inc. † Co-corresponding authors: Hua Wu and Sujian Li. 1 Our code will be available at http://github. com/paddlepaddle/models/tree/develop/ PaddleNLP/Research/ACL2019-KTNET are pre-trained on unlabeled text and then applied to MRC, in either a feature-based (Peters et al., 2018a) or a fine-tuning (Radford et al., 2018) manner, both offering substantial performance boosts. Among different pre-training mechanisms, BERT (Devlin et al., 2018), which uses Transformer encoder (Vaswani et al., 2017) and trains a bidirectional LM, is undoubtedly the most successful by far, presenting new state-of-the-art results in MRC and a wide variety of other language understanding tasks. Owing to the large amounts of unlabeled data and the sufficiently deep architectures used during pre-training, advanced LMs such as BERT are able to capture complex linguistic phenomena, understanding"
P19-1226,D18-1179,0,0.320665,"and the increasing availability of datasets (Rajpurkar et al., 2016, 2018; Nguyen et al., 2016; Joshi et al., 2017), MRC has achieved remarkable advancements in the last few years. Recently language model (LM) pre-training has caused a stir in the MRC community. These LMs * This work was done while the first author was an intern at Baidu Inc. † Co-corresponding authors: Hua Wu and Sujian Li. 1 Our code will be available at http://github. com/paddlepaddle/models/tree/develop/ PaddleNLP/Research/ACL2019-KTNET are pre-trained on unlabeled text and then applied to MRC, in either a feature-based (Peters et al., 2018a) or a fine-tuning (Radford et al., 2018) manner, both offering substantial performance boosts. Among different pre-training mechanisms, BERT (Devlin et al., 2018), which uses Transformer encoder (Vaswani et al., 2017) and trains a bidirectional LM, is undoubtedly the most successful by far, presenting new state-of-the-art results in MRC and a wide variety of other language understanding tasks. Owing to the large amounts of unlabeled data and the sufficiently deep architectures used during pre-training, advanced LMs such as BERT are able to capture complex linguistic phenomena, understanding"
P19-1226,P17-1018,0,0.0227181,"rt the results in Appendix B. 5 Related Work Machine Reading Comprehension In the last few years, a number of datasets have been created for MRC, e.g., CNN/DM (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016, 2018), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), and MS-MARCO (Nguyen et al., 2016). These 7 During visualization, we use a row-wise softmax operation to normalize similarity scores over all passage tokens. datasets have led to advances like Match-LSTM (Wang and Jiang, 2017), BiDAF (Seo et al., 2017), AoA Reader (Cui et al., 2017), DCN (Xiong et al., 2017), R-Net (Wang et al., 2017), and QANet (Yu et al., 2018). These end-to-end neural models have similar architectures, starting off with an encoding layer to encode every question/passage word as a vector, passing through various attention-based interaction layers and finally a prediction layer. More recently, LMs such as ELMo (Peters et al., 2018b), GPT (Radford et al., 2018), and BERT (Devlin et al., 2018) have been devised. They pre-train deep LMs on large-scale unlabeled corpora to obtain contextual representations of text. When used in downstream tasks including MRC, the pre-trained contextual representations greatly"
P19-1226,P17-1132,0,0.462652,"and Text fusion NET), a new approach to MRC which improves pre-trained LMs with additional knowledge from knowledge bases (KBs). The aim here is to take full advantage of both linguistic regularities covered by deep LMs and high-quality knowledge derived from curated KBs, towards better MRC. We leverage two KBs: WordNet (Miller, 1995) that records lexical relations between words and NELL (Carlson et al., 2010) that stores beliefs about entities. Both are useful for the task (see Fig. 1). Instead of introducing symbolic facts, we resort to distributed representations (i.e., embeddings) of KBs (Yang and Mitchell, 2017). With such KB embeddings, we could (i) integrate knowledge relevant not only locally to the reading text but also globally about the whole KBs; and (ii) easily incorporate multiple KBs at the same time, with minimal task-specific engineering (see § 2.2 for detailed explanation). As depicted in Fig. 2, given a question and passage, KT-NET first retrieves potentially relevant KB embeddings and encodes them in a knowledge memory. Then, it employs, in turn, (i) a BERT encoding layer to compute deep, context-aware representations for the reading text; (ii) a knowledge integration layer to select d"
P19-1289,D18-1337,0,0.06727,"ranslation” model which also outputs target-side words before the whole input sentence is fed in, but there are several crucial differences: (a) their work still aims to translate full sentences using beam search, and is therefore, as the authors admit, “not a simultaneous translation model”; (b) their work does not anticipate future words; and (c) they use word alignments to learn the reordering and achieve it in decoding by emitting the  token, while our work integrates reordering into a single wait-k prediction model that is agnostic of, yet capable of, reordering. In another recent work, Alinejad et al. (2018) adds a prediction action to the work of Gu et al. (2017). Unlike Grissom II et al. (2014) who predict the source verb which might come after several words, they instead predict the immediate next source words, which we argue is not as useful in SOV-to-SVO translation. 4 In any case, we are the first to predict directly on the target side, thus integrating anticipation in a single translation model. Jaitly et al. (2016) propose an online neural transducer for speech recognition that is conditioned on prefixes. This problem does not have reorderings and thus no anticipation is needed. 8 Conclus"
P19-1289,J82-2005,0,0.638759,"Missing"
P19-1289,D14-1140,0,0.371479,"Missing"
P19-1289,K16-1010,0,0.0322637,"Missing"
P19-1289,E17-1099,0,0.472658,"hu`ıw`u. (a) Our wait-k policy (here k = 2) translates concurrently with the source sentence, but always k words behind. It correclty predicts the English verb given just the first 4 Chinese words (in bold), lit. “Bush president in Moscow”, because it is trained in a prefix-to-prefix fashion (Sec. 3), and the training data contains many prefix-pairs in the form of (X z`ai Y ..., X met ...). (c) The test-time wait-k decoding (Sec. 3.2) using the full-sentence model in (b) can not anticipate and produces nonsense translation. (d) A simultaneous translator without anticipation such as Gu et al. (2017) has to wait 5 words. (Grissom II et al., 2016), and have attempted to reduce latency by explicitly predicting the sentencefinal German (Grissom II et al., 2014) or English verbs (Matsubarayx et al., 2000), which is limited to this particular case, or unseen syntactic constituents (Oda et al., 2015; He et al., 2015), which requires incremental parsing on the source sentence. Some researchers propose to translate on an optimized sentence segment level to get better translation accuracy (Oda et al., 2014; Fujita et al., 2013; Bangalore et al., 2012). More recently, Gu et al. (2017) propose a two"
P19-1289,D15-1006,0,0.0494593,"Missing"
P19-1289,D17-1227,1,0.906619,"Missing"
P19-1289,P17-4012,0,0.0164908,"German↔English evaluation, we use newstest-2013 (dev) as our dev set and newstest-2015 (test) as our test set, with 3,000 and 2,169 sentence pairs, respectively. For Chinese↔English evaluation, we use NIST 2006 and NIST 2008 as our dev and test sets. They contain 616 and 691 Chinese sentences, each with 4 English references. When translating from Chinese to English, we report 4-reference BLEU scores, and in the reverse direction, we use the second among the four English references as the source text, and report 1-reference BLEU scores. Our implementation is adapted from PyTorchbased OpenNMT (Klein et al., 2017). Our Transformer is essentially the same as the base model from the original paper (Vaswani et al., 2017). 6.2 Quality and Latency of Wait-k Model Tab. 1 shows the results of a model trained with wait-k 0 but decoded with wait-k (where ∞ means full-sentence). Our wait-k is the diagonal, and the last row is the “test-time wait-k” decoding. Also, the best results of wait-k decoding is often from a model trained with a slightly larger k 0 . Figs. 5–8 plot translation quality (in BLEU) against latency (in AL and CW) for full-sentence baselines, our wait-k, test-time wait-k (using fullsentence mod"
P19-1289,P14-2090,0,0.0597317,"nd produces nonsense translation. (d) A simultaneous translator without anticipation such as Gu et al. (2017) has to wait 5 words. (Grissom II et al., 2016), and have attempted to reduce latency by explicitly predicting the sentencefinal German (Grissom II et al., 2014) or English verbs (Matsubarayx et al., 2000), which is limited to this particular case, or unseen syntactic constituents (Oda et al., 2015; He et al., 2015), which requires incremental parsing on the source sentence. Some researchers propose to translate on an optimized sentence segment level to get better translation accuracy (Oda et al., 2014; Fujita et al., 2013; Bangalore et al., 2012). More recently, Gu et al. (2017) propose a two-stage model whose base model is a full-sentence model, On top of that, they use a READ/WRITE (R/W) model to decide, at every step, whether to wait for another source word (READ) or to emit a target word using the pretrained base model (WRITE), and this R/W model is trained by reinforcement learning to prefer (rather than enforce) a specific latency, without updating the base model. All these efforts have the following major limitations: (a) none of them can achieve any arbitrary given latency such as"
P19-1289,P15-1020,0,0.0583849,"hion (Sec. 3), and the training data contains many prefix-pairs in the form of (X z`ai Y ..., X met ...). (c) The test-time wait-k decoding (Sec. 3.2) using the full-sentence model in (b) can not anticipate and produces nonsense translation. (d) A simultaneous translator without anticipation such as Gu et al. (2017) has to wait 5 words. (Grissom II et al., 2016), and have attempted to reduce latency by explicitly predicting the sentencefinal German (Grissom II et al., 2014) or English verbs (Matsubarayx et al., 2000), which is limited to this particular case, or unseen syntactic constituents (Oda et al., 2015; He et al., 2015), which requires incremental parsing on the source sentence. Some researchers propose to translate on an optimized sentence segment level to get better translation accuracy (Oda et al., 2014; Fujita et al., 2013; Bangalore et al., 2012). More recently, Gu et al. (2017) propose a two-stage model whose base model is a full-sentence model, On top of that, they use a READ/WRITE (R/W) model to decide, at every step, whether to wait for another source word (READ) or to emit a target word using the pretrained base model (WRITE), and this R/W model is trained by reinforcement learnin"
P19-1289,D18-1342,1,0.898779,"Missing"
P19-1289,P19-1582,1,0.895955,"Missing"
P19-1369,P18-1138,0,0.0708081,"formation (Zhang et al., 2018), recommending something (Li et al., 2018), and completing tasks (Bordes et al., 2016), most of which rely on background knowledge. However, many 1 https://github.com/PaddlePaddle/models/tree/develop/ PaddleNLP/Research/ACL2019-DuConv dialogue systems only rely on utterances and responses as training data, without explicitly exploiting knowledge associated with them, which sometimes results in uninformative and inappropriate responses (Wang et al., 2018). Although there exist some work that use external background knowledge to generate more informative responses (Liu et al., 2018; Yin et al., 2015; Zhu et al., 2017), these systems usually generate responses to answer questions instead of asking questions or leading the conversation. In order to solve the above problems, some new datasets have been created, where external background knowledge is explicitly linked to utterances (Dinan et al., 2019; Moghe et al., 2018), to facilitate the development of knowledge aware conversation models. With these datasets, conversation systems can be built to talk with humans given a topic based on the provided external knowledge. Unlike taskoriented systems (Bordes et al., 2016), the"
P19-1369,D18-1255,0,0.122597,"thout explicitly exploiting knowledge associated with them, which sometimes results in uninformative and inappropriate responses (Wang et al., 2018). Although there exist some work that use external background knowledge to generate more informative responses (Liu et al., 2018; Yin et al., 2015; Zhu et al., 2017), these systems usually generate responses to answer questions instead of asking questions or leading the conversation. In order to solve the above problems, some new datasets have been created, where external background knowledge is explicitly linked to utterances (Dinan et al., 2019; Moghe et al., 2018), to facilitate the development of knowledge aware conversation models. With these datasets, conversation systems can be built to talk with humans given a topic based on the provided external knowledge. Unlike taskoriented systems (Bordes et al., 2016), these conversation systems don’t have an explicit goal to achieve, thereof not able to plan over the background knowledge. In this paper, we take a radical step towards building another type of human-like conversational agent: endowing it with the ability of proactively leading the conversation with an explicit conversation goal. To this end, w"
P19-1369,C16-1318,0,0.128485,"tate the development of such conversation systems. 2.2 # dialogs # utterances average # utterances per dialog average # words per utterance average # words per dialog average # knowledge per dialogue Proactive Conversation Knowledge Grounded Conversation Leveraging knowledge for better dialogue modeling has drawn lots of research interests in past years and researchers have shown the multi-fold benefits of exploiting knowledge in dialogue modeling. One major research line is using knowledge to generate engaging, meaningful or personalized responses in chitchatting (Ghazvininejad et al., 2018; Vougiouklis et al., 2016; Zhou et al., 2018a; Zhang et al., 2018). In addition to proposing better conversation models, researchers also released several knowledge grounded datasets (Dinan et al., 2019; Moghe et al., 2018). Our work is most related to Mogh et al., (2018) and Dinan et al., (2019), where each utterance in their released datasets is aligned to the related knowledge, including both structured triplets and unstructured sentences. We extend their work, by including the whole knowledge graph into dialogue modeling and propose a new task of proactively leading the conversation via planning over the knowledge"
P19-1369,P18-1204,0,0.150476,"l agent is one of long-cherished goals in Artificial Intelligence (AI) (Turing, 2009). Typical conversations involve exchanging information (Zhang et al., 2018), recommending something (Li et al., 2018), and completing tasks (Bordes et al., 2016), most of which rely on background knowledge. However, many 1 https://github.com/PaddlePaddle/models/tree/develop/ PaddleNLP/Research/ACL2019-DuConv dialogue systems only rely on utterances and responses as training data, without explicitly exploiting knowledge associated with them, which sometimes results in uninformative and inappropriate responses (Wang et al., 2018). Although there exist some work that use external background knowledge to generate more informative responses (Liu et al., 2018; Yin et al., 2015; Zhu et al., 2017), these systems usually generate responses to answer questions instead of asking questions or leading the conversation. In order to solve the above problems, some new datasets have been created, where external background knowledge is explicitly linked to utterances (Dinan et al., 2019; Moghe et al., 2018), to facilitate the development of knowledge aware conversation models. With these datasets, conversation systems can be built to"
P19-1369,P17-1046,0,0.06064,"se generation model. We will give a detailed description of those two knowledgeaware models in next two sub-sections. 4.1 Retrieval-based Model Given a dialogue context X, the retrieval-based dialogue system responds to that context via searching for the best response Y from DuConv. Thus retrieval-based dialogue system often has a pipeline structure with two major steps: 1) retrieve response candidates from a database and 2) select the best one from the response candidates (Zhou et al., 2018b). In our retrieval-based method, the candidate responses are collected similar to most existing work (Wu et al., 2017; Zhou et al., 2018b) with one notable difference that we normalize the entities with their entity types in the knowledge graph to improve generalization capabilities. For each retrieved candidate response Y , the goal of our response ranker is to measure if Y is a good response to the context X considering the given dialogue goal G = [start, topic a, topic b] and related knowledge K. The matching 4 The workers are collected from a Chinese crowdsourcing platform http://test.baidu.com/. The workers are paid 2.5 Chinese Yuan per conversation. 3797 NLL Loss knowledge1 p(k1 x) knowledge2 p(k2 x) k"
P19-1369,D17-1233,0,0.0244852,"P ([x; y])) p(ki |x, y) = PN (3) j=1 exp(kj · M LP ([x; y])) exp(ki · x) p(ki |x) = PN j=1 exp(kj · x) (4) N 1 X p(ki |x, y) LKL (θ) = p(ki |x, y)log N p(ki |x) (5) i=1 Given that knowledge distribution p(ki |x) and p(ki |x, y), we fused all related P knowledge information into a vector kc = i p(ki |x, y) ∗ ki , same as our retrieval-based method, and feed it to the decoder for response generation. In the testing phase, the fused Pknowledge is estimated by the formula kc = i p(ki |x) ∗ ki without gold responses . The decoder is implemented with the Hierarchical Gated Fusion Unit described in (Yao et al., 2017), which is a standard GRU based decoder enhanced with external knowledge gates. Besides the KLDivLoss, our knowledge-aware generator introduces two additional loss functions: NLL Loss: the Negative Log-Likelihood (NLL) LN LL (θ) measures the difference between the true response and the response generated by our model. BOW Loss: We use the BOW loss proposed by Zhao et al., (2017), to ensure the accuracy of the fused knowledge kc by enforcing the relevancy between the knowledge and the true response. Specifically, let w = MLP(kc ) ∈ R|V |, where |V |is the vocabulary size and we define: exp(wyt"
P19-1369,P18-1205,0,0.377324,"d plan over the given knowledge graph. We establish baseline results on this dataset (about 270K utterances and 30k dialogues) using several state-of-the-art models. Experimental results show that dialogue models that plan over the knowledge graph can make full use of related knowledge to generate more diverse multi-turn conversations. The baseline systems along with the dataset are publicly available 1 . 1 Introduction Building a human-like conversational agent is one of long-cherished goals in Artificial Intelligence (AI) (Turing, 2009). Typical conversations involve exchanging information (Zhang et al., 2018), recommending something (Li et al., 2018), and completing tasks (Bordes et al., 2016), most of which rely on background knowledge. However, many 1 https://github.com/PaddlePaddle/models/tree/develop/ PaddleNLP/Research/ACL2019-DuConv dialogue systems only rely on utterances and responses as training data, without explicitly exploiting knowledge associated with them, which sometimes results in uninformative and inappropriate responses (Wang et al., 2018). Although there exist some work that use external background knowledge to generate more informative responses (Liu et al., 2018; Yin et al.,"
P19-1369,P17-1061,0,0.0525952,"r response generation. In the testing phase, the fused Pknowledge is estimated by the formula kc = i p(ki |x) ∗ ki without gold responses . The decoder is implemented with the Hierarchical Gated Fusion Unit described in (Yao et al., 2017), which is a standard GRU based decoder enhanced with external knowledge gates. Besides the KLDivLoss, our knowledge-aware generator introduces two additional loss functions: NLL Loss: the Negative Log-Likelihood (NLL) LN LL (θ) measures the difference between the true response and the response generated by our model. BOW Loss: We use the BOW loss proposed by Zhao et al., (2017), to ensure the accuracy of the fused knowledge kc by enforcing the relevancy between the knowledge and the true response. Specifically, let w = MLP(kc ) ∈ R|V |, where |V |is the vocabulary size and we define: exp(wyt ) p(yt |kc ) = P v exp(wv ) (6) Then, the BOW loss is defined to minimize: m LBOW (θ) = − 1 X logp(yt |kc ) m (7) t=1 In summary, the final loss of our generative model is: L(θ) = LKL (θ) + LN LL (θ) + LBOW (θ) 5 5.1 (8) Experiments Setting Our proposed models are tested under two settings: 1) automatic evaluation and 2) human evaluation. For automatic evaluation, we leverage se"
P19-1369,P18-1103,1,\N,Missing
P19-1535,W14-4012,0,0.0140135,"Missing"
P19-1535,D17-1259,0,0.2257,"1 = T X Oθ log p(zi |ct ) R(τ ) − b  Settings (7) t=1 + T X • The KG-Net (Lian et al., 2019) makes use of posterior knowledge distribution in the training process for accurate informative response generation and achieves the state-of-the-art results on PersonaChat.  Oθ log p(ut |zi , ut−1 ) R(τ ) − b , t=1 where b is the reward baseline estimated with P K (k)times Monte Carlo sampling: b = )/K. In Equation (7), the first term is k R(τ about the dialogue strategy of appropriate knowledge selection and the second term is about the decoding process with the selected knowledge. As suggested in (Lewis et al., 2017; Yarats and Lewis, 2018), applying reinforcement learning on the decoder might lead to poor linguistic quality. As such, in this paper, the focus is on the strategy evolution and gradient update is further simplified: Oθ J(θ) = T X t=1  Oθ log p(zi |ct ) R(τ ) − b . (8) • Li et al. (2016b) first employed reinforcement learning for dialogue generation (RLDG), where simple Seq2Seq was used as the generation model. In the experiments, to improve RL-DG’s performance, KG-Net is utilized as the base model for informative generation. In our strategic knowledge interaction, the parameters of knowled"
P19-1535,N16-1014,0,0.736684,"R(τ ) − b , t=1 where b is the reward baseline estimated with P K (k)times Monte Carlo sampling: b = )/K. In Equation (7), the first term is k R(τ about the dialogue strategy of appropriate knowledge selection and the second term is about the decoding process with the selected knowledge. As suggested in (Lewis et al., 2017; Yarats and Lewis, 2018), applying reinforcement learning on the decoder might lead to poor linguistic quality. As such, in this paper, the focus is on the strategy evolution and gradient update is further simplified: Oθ J(θ) = T X t=1  Oθ log p(zi |ct ) R(τ ) − b . (8) • Li et al. (2016b) first employed reinforcement learning for dialogue generation (RLDG), where simple Seq2Seq was used as the generation model. In the experiments, to improve RL-DG’s performance, KG-Net is utilized as the base model for informative generation. In our strategic knowledge interaction, the parameters of knowledge encoder, utterance encoder and decoder were pre-trained with supervised learning. For the learnable parameters (Blue 5386 areas in Figure 2), the context encoder was initialized with the utterance encoder and random initialization was employed for the rest layers1 . The training process"
P19-1535,D16-1127,0,0.502156,"R(τ ) − b , t=1 where b is the reward baseline estimated with P K (k)times Monte Carlo sampling: b = )/K. In Equation (7), the first term is k R(τ about the dialogue strategy of appropriate knowledge selection and the second term is about the decoding process with the selected knowledge. As suggested in (Lewis et al., 2017; Yarats and Lewis, 2018), applying reinforcement learning on the decoder might lead to poor linguistic quality. As such, in this paper, the focus is on the strategy evolution and gradient update is further simplified: Oθ J(θ) = T X t=1  Oθ log p(zi |ct ) R(τ ) − b . (8) • Li et al. (2016b) first employed reinforcement learning for dialogue generation (RLDG), where simple Seq2Seq was used as the generation model. In the experiments, to improve RL-DG’s performance, KG-Net is utilized as the base model for informative generation. In our strategic knowledge interaction, the parameters of knowledge encoder, utterance encoder and decoder were pre-trained with supervised learning. For the learnable parameters (Blue 5386 areas in Figure 2), the context encoder was initialized with the utterance encoder and random initialization was employed for the rest layers1 . The training process"
P19-1535,I17-1099,0,0.0256352,"ther state-of-the-art approaches significantly. 1 Introduction Intelligent dialogue systems have become popular in our daily life, such as the chit-chat XiaoIce and the task-oriented Echo. These systems serve as smart agents to facilitate more effective interaction with users in various situations, like ticket booking or recreation offering. Primary dialogue systems (Vinyals and Le, 2015; Shang et al., 2015) try to mimic human beings to generate fluent utterances, whereas paying little attention to the intrinsic factors of human conversations: exchanging information and enhancing interaction (Li et al., 2017). Therefore, they are prone to generate dull and generic responses. To address this problem, in recent years, several approaches have been developed to generate informative responses based on external knowledge. Recently, a knowledge grounded model is proposed in Ghazvininejad et al. (2018), where relevant factual texts are encoded into memory and replies are decoded via attention mechanism. Instead of using unstructured text knowledge, CCM (Zhou et al., 2018) relies on structured knowledge to generate rich-information response. However, all these approaches are designed for the singleround se"
P19-1535,P16-1008,0,0.0245257,"Missing"
P19-1535,P17-1046,0,0.0166362,"MLP MLP (' Figure 4: Illustration of coherence assessment, where H-GRU refers to hierarchical GRU and the symbol ⊕ denotes vector concatenation. the coherence assessment also evaluates the conversation consistency with the backgrounds. The motivation to enforce background consistency is to confine the massive and loose interactive responses into a reasonable space. Considering that the essence of coherence is semantic relevance between two inputs and many deep learning based approaches have demonstrated their superiority at capturing semantic relevance, such as DSSM (Huang et al., 2013), SMN (Wu et al., 2017) and BERT (Devlin et al., 2018), we use a symmetric neural network for the coherence assessment in this paper. As shown in Figure 4, for a generated utterance ut , its coherence with the context ct and corresponding backgrounds Z can be estimated through this symmetric network. The utterance is fed into the embedding layer, followed by gated recurrent unit (GRU) (Cho et al., 2014) and multilayer perceptron (MLP) to capture discriminative representation. As for the context and backgrounds, they are fed into the embedding layer and the hierarchical GRU for better feature extractions (Sordoni et"
P19-1535,P18-1205,0,0.0648267,"dge selection within a conversation can be regarded as sequential actions taken within a trajectory. As such, the objective of knowledge grounded dialogue generation can be written as: max J(θ) = Eτ ∼p(τ ;θ) R(τ ), (6) where θ refers to the network parameters of dialogue generation, τ ∼ p(τ ; θ) is a multi-turn conversation generated under the deployed strategy and R(τ ) is the compound assessment of strategy evaluation. Gradient update of the above objective can be further derived as follows: Oθ J(θ) = T X All experiments have been carried out on the publicly available dataset – PersonaChat (Zhang et al., 2018), which provides both human annotated conversations and the participants’ background knowledge (persona profiles). PersonaChat has separated training and testing set. In total, there are 8,939 dialogues (131,438 turns) in the training set and 968 dialogues (15,024 turns) in the testing set. Comprehensive comparisons have been made to the following methods: • Sequence to sequence with attention (Seq2Seq) (Vinyals and Le, 2015) is the classic response generation approach, without using any extra knowledge. • The knowledge grounded memory network (Mem-Net) (Ghazvininejad et al., 2018) encodes tex"
P19-1535,D16-1230,0,0.20258,"Missing"
P19-1535,P17-1061,0,0.107177,"Missing"
P19-1535,P17-1099,0,0.0242127,"Missing"
P19-1535,P15-1152,0,0.113903,"Missing"
W03-1610,W97-0703,0,0.0093278,"of extracting synonymous English words (synonyms) from multiple resources: a monolingual dictionary, a parallel bilingual corpus, and a monolingual corpus. The extracted synonyms can be used in a number of NLP applications. In information retrieval and question answering, the synonymous words are employed to bridge the expressions gaps between the query space and the document space (Mandala et al., 1999; Radev et al., 2001; Kiyota et al., 2002). In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem"
W03-1610,P01-1008,0,0.203179,"Missing"
W03-1610,W02-1029,0,0.445681,"ut not synonymous. Other resources are also used for synonym extraction. Barzilay and Mckeown (2001), and Shimohata and Sumita (2002) used bilingual corpora to extract synonyms. However, these methods can only extract synonyms which occur in the bilingual corpus. Thus, the extracted synonyms are limited. Besides, Blondel and Sennelart (2002) used monolingual dictionaries to extract synonyms. Although the precision of this method is high, the coverage is low because the result of this method heavily depends on the definitions of words. In order to improve the performance of synonym extraction, Curran (2002) used an ensemble method to combine the results of different methods using a monolingual corpus. Although Curran (2002) showed that the ensemble extractors outperformed the individual extractors, it still cannot overcome the deficiency of the methods using the monolingual corpus. To overcome the deficiencies of the methods using only one resource, our approach combines both monolingual and bilingual resources to automatically extract synonymous words. By combining the synonyms extracted by the individual extractors using the three resources, our approach can combine the merits of the individua"
W03-1610,P90-1034,0,0.349868,"al., 2001; Kiyota et al., 2002). In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous. Other resources are also used for synonym extraction. Barzilay and Mckeown (2001), and Shimohata and Sumita (2002) used bilingual corpora to extract synonyms. However, these methods can only extract synonyms which occur in th"
W03-1610,C02-1084,0,0.015652,"ction, and that the ensemble method we used is very effective to improve both precisions and recalls of extracted synonyms. 1 Introduction This paper addresses the problem of extracting synonymous English words (synonyms) from multiple resources: a monolingual dictionary, a parallel bilingual corpus, and a monolingual corpus. The extracted synonyms can be used in a number of NLP applications. In information retrieval and question answering, the synonymous words are employed to bridge the expressions gaps between the query space and the document space (Mandala et al., 1999; Radev et al., 2001; Kiyota et al., 2002). In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1"
W03-1610,P98-1116,0,0.0214685,"parallel bilingual corpus, and a monolingual corpus. The extracted synonyms can be used in a number of NLP applications. In information retrieval and question answering, the synonymous words are employed to bridge the expressions gaps between the query space and the document space (Mandala et al., 1999; Radev et al., 2001; Kiyota et al., 2002). In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many w"
W03-1610,P98-2127,0,0.347812,"tify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous. Other resources are also used for synonym extraction. Barzilay and Mckeown (2001), and Shimohata and Sumita (2002) used bilingual corpora to extract synonyms. However, these methods can only extract synonyms which occur in the bilingual corpus. Thus, the extracted synonyms are limited. Besides, Blondel and Sennelart (2002) us"
W03-1610,shimohata-sumita-2002-automatic,0,0.392551,"nym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous. Other resources are also used for synonym extraction. Barzilay and Mckeown (2001), and Shimohata and Sumita (2002) used bilingual corpora to extract synonyms. However, these methods can only extract synonyms which occur in the bilingual corpus. Thus, the extracted synonyms are limited. Besides, Blondel and Sennelart (2002) used monolingual dictionaries to extract synonyms. Although the precision of this method is high, the coverage is low because the result of this method heavily depends on the definitions of words. In order to improve the performance of synonym extraction, Curran (2002) used an ensemble method to combine the results of different methods using a monolingual corpus. Although Curran (2002)"
W03-1610,C98-1112,0,\N,Missing
W03-1610,C98-2122,0,\N,Missing
W13-5708,C10-1011,0,0.109851,"Missing"
W13-5708,J08-4003,0,0.015643,"iments in this paper. In a typical transition-based parsing process, the input words are stored in a queue and partially built dependency structures (e.g., sub-trees) are organized by a configuration (or state). A parser configuration (or, state) can be represented by a tuple &lt; S, N, A &gt;, where S is the stack, N is the queue of incoming words, and A is the set of dependency arcs that have been built. A set of shift-reduce actions are defined, which are used to construct new dependency arcs by connecting the top word of the queue and the top word of the stack. We adopt the arc-standard system (Nivre, 2008), whose actions include: about an exponential number of semantically neighbouring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. The general process of neural language model based word embedding is as follows: • associate with each word in the vocabulary a distributed word feature vector (a real valued vector in Rm ); • express the joint probability function of word sequences in terms of the feature vectors of these words in the sequence; and, • sh"
W13-5708,W09-2307,0,0.0605908,"Missing"
W13-5708,Q13-1025,0,0.0225926,"Missing"
W13-5708,P12-2003,0,0.0222484,"Missing"
W13-5708,Q13-1012,0,0.0292666,"Missing"
W13-5708,P10-1110,0,0.397686,"who study the effects of different clustering algorithms for POS tagging and named entity recognition (NER). We designed feature templates by making use of words, POS tags, CPOS tags, NLMWEbased word classes and their combinations. NLMWEbased word classes is shown to be an important supplement of POS-tags. Experiments on a Query treebank, CTB5 and CTB7 show that the combinations of features from CPOS-tags, POS-tags, and NLMWEbased word classes yield the best UASs. 1.1 Shift-reduce parsing We use a transition-based shift-reduce parser (Kudo and Matsumoto, 2002; Nivre, 2003; Nivre et al., 2006; Huang and Sagae, 2010) to 73 perform all the experiments in this paper. In a typical transition-based parsing process, the input words are stored in a queue and partially built dependency structures (e.g., sub-trees) are organized by a configuration (or state). A parser configuration (or, state) can be represented by a tuple &lt; S, N, A &gt;, where S is the stack, N is the queue of incoming words, and A is the set of dependency arcs that have been built. A set of shift-reduce actions are defined, which are used to construct new dependency arcs by connecting the top word of the queue and the top word of the stack. We ado"
W13-5708,P08-1068,0,0.113017,"a correct tree if there are out-of-vocabulary (OOV) words (compared to the training data of the treebank) and/or the POS-tags are wrongly annotated? Words need to be generalized to solve this problem in a sense. Indeed, POS-tag itself is a way to generalize words into word classes. This is because POS-taggers can be trained on larger-scale data compared with treebanks. Annotating trees is far more difficult than annotating POS-tags. Considering that unsupervised word clustering methods can make use of TB/PB-level Web data, these approaches have been shown to be helpful for dependency parsing (Koo et al., 2008). In this paper, we investigate the influence of generalization of words to the accuracies of Chinese dependency parsing. Specially, in our shift-reduce parser, we use a neural language model based word embedding method (Bengio et al., 2003) to generate distributed word feature vectors and then perform Kmeans based word clustering (Yu et al., 2013) to generate word classes. Our usage of word embedding is in line with Turian et al. (2010) and Yu et al. (2013), who study the effects of different clustering algorithms for POS tagging and named entity recognition (NER). We designed feature templat"
W13-5708,W02-2016,0,0.0433687,"is in line with Turian et al. (2010) and Yu et al. (2013), who study the effects of different clustering algorithms for POS tagging and named entity recognition (NER). We designed feature templates by making use of words, POS tags, CPOS tags, NLMWEbased word classes and their combinations. NLMWEbased word classes is shown to be an important supplement of POS-tags. Experiments on a Query treebank, CTB5 and CTB7 show that the combinations of features from CPOS-tags, POS-tags, and NLMWEbased word classes yield the best UASs. 1.1 Shift-reduce parsing We use a transition-based shift-reduce parser (Kudo and Matsumoto, 2002; Nivre, 2003; Nivre et al., 2006; Huang and Sagae, 2010) to 73 perform all the experiments in this paper. In a typical transition-based parsing process, the input words are stored in a queue and partially built dependency structures (e.g., sub-trees) are organized by a configuration (or state). A parser configuration (or, state) can be represented by a tuple &lt; S, N, A &gt;, where S is the stack, N is the queue of incoming words, and A is the set of dependency arcs that have been built. A set of shift-reduce actions are defined, which are used to construct new dependency arcs by connecting the to"
W13-5708,C12-1103,0,0.287843,"Missing"
W13-5708,P13-2020,0,0.0212508,"Missing"
W13-5708,P10-1040,0,0.141137,"onsidering that unsupervised word clustering methods can make use of TB/PB-level Web data, these approaches have been shown to be helpful for dependency parsing (Koo et al., 2008). In this paper, we investigate the influence of generalization of words to the accuracies of Chinese dependency parsing. Specially, in our shift-reduce parser, we use a neural language model based word embedding method (Bengio et al., 2003) to generate distributed word feature vectors and then perform Kmeans based word clustering (Yu et al., 2013) to generate word classes. Our usage of word embedding is in line with Turian et al. (2010) and Yu et al. (2013), who study the effects of different clustering algorithms for POS tagging and named entity recognition (NER). We designed feature templates by making use of words, POS tags, CPOS tags, NLMWEbased word classes and their combinations. NLMWEbased word classes is shown to be an important supplement of POS-tags. Experiments on a Query treebank, CTB5 and CTB7 show that the combinations of features from CPOS-tags, POS-tags, and NLMWEbased word classes yield the best UASs. 1.1 Shift-reduce parsing We use a transition-based shift-reduce parser (Kudo and Matsumoto, 2002; Nivre, 200"
W13-5708,N13-1063,1,0.760145,"ared with treebanks. Annotating trees is far more difficult than annotating POS-tags. Considering that unsupervised word clustering methods can make use of TB/PB-level Web data, these approaches have been shown to be helpful for dependency parsing (Koo et al., 2008). In this paper, we investigate the influence of generalization of words to the accuracies of Chinese dependency parsing. Specially, in our shift-reduce parser, we use a neural language model based word embedding method (Bengio et al., 2003) to generate distributed word feature vectors and then perform Kmeans based word clustering (Yu et al., 2013) to generate word classes. Our usage of word embedding is in line with Turian et al. (2010) and Yu et al. (2013), who study the effects of different clustering algorithms for POS tagging and named entity recognition (NER). We designed feature templates by making use of words, POS tags, CPOS tags, NLMWEbased word classes and their combinations. NLMWEbased word classes is shown to be an important supplement of POS-tags. Experiments on a Query treebank, CTB5 and CTB7 show that the combinations of features from CPOS-tags, POS-tags, and NLMWEbased word classes yield the best UASs. 1.1 Shift-reduce"
W13-5708,D08-1059,0,0.579374,"tion of word sequences in terms of the feature vectors of these words in the sequence; and, • shift, which removes the top word in the queue and pushes it onto the top of the stack; • left-arc, which pops the top item off the stack, and adds it as a modifier to the front of the queue; • right-arc, which removes the front of the queue, and adds it as a modifier to the top of the stack. In addition, the top of the stack is popped and added to the front of the queue. We follow Kudo and Matsumoto (2002) and use the Support Vector Machines (SVMs) for action classification training and beam search (Zhang and Clark, 2008) for decoding. 2 Neural Language Model Based Word Embedding Following (Bengio et al., 2003), we use a neural network with two hidden layers to learn distributed word feature vectors from large-scale training data. Recall that, the goal of statistical language modelling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training (so called OOV words and/or sequences). Ngram based a"
W13-5708,D12-1030,0,0.0295383,"Missing"
W13-5708,P11-2033,0,0.0280779,"periments. 3 Feature Templates At each step during shift-reducing, a parser configuration (or, state) can be represented by a tuple &lt; S, N, A &gt;. We denote the top of stack with S0 , the front items from the queue with N0 , N1 , N2 , and N3 , the leftmost and rightmost modifiers of S0 (if any) with S0l and S0r , respectively, and the leftmost modifier of N0 (if any) with N0l (refer to Figure 1). The baseline feature templates without any word class level information (such as POS-tags) are shown in Table 1. These features are mostly taken from Zhang and Clark (2008), Huang and Sagae (2010), and Zhang and Nivre (2011). In this table, w, l and d represents the word, dependency label, and the distance between S0 and N0 , respectively. For example, S0 wN0 w represents the feature template that takes the word of S0 , and combines it with the word of N0 . In Table 1, (S/N )0l2 , (S/N )0r2 , and (S/N )0rn refer to the second leftmost modifier, the second rightmost modifier, and the right nearest modifier of (S/N )0 , respectively. It should be mentioned that, for the arc-standard algorithm used in this paper, S0 or N0 never contain a head word. The reason is that, once the head is found for a node, that node wil"
W13-5708,E06-1011,0,0.055922,"Missing"
W13-5708,nivre-etal-2006-maltparser,0,0.11463,"Missing"
W13-5708,W03-3017,0,0.058527,"al. (2010) and Yu et al. (2013), who study the effects of different clustering algorithms for POS tagging and named entity recognition (NER). We designed feature templates by making use of words, POS tags, CPOS tags, NLMWEbased word classes and their combinations. NLMWEbased word classes is shown to be an important supplement of POS-tags. Experiments on a Query treebank, CTB5 and CTB7 show that the combinations of features from CPOS-tags, POS-tags, and NLMWEbased word classes yield the best UASs. 1.1 Shift-reduce parsing We use a transition-based shift-reduce parser (Kudo and Matsumoto, 2002; Nivre, 2003; Nivre et al., 2006; Huang and Sagae, 2010) to 73 perform all the experiments in this paper. In a typical transition-based parsing process, the input words are stored in a queue and partially built dependency structures (e.g., sub-trees) are organized by a configuration (or state). A parser configuration (or, state) can be represented by a tuple &lt; S, N, A &gt;, where S is the stack, N is the queue of incoming words, and A is the set of dependency arcs that have been built. A set of shift-reduce actions are defined, which are used to construct new dependency arcs by connecting the top word of the"
W18-2605,P17-1055,0,0.0293444,"Dataset from Real-world Applications Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, Haifeng Wang Baidu Inc., Beijing, China {hewei06, liukai20, liujing46, lvyajuan, zhaoshiqi, xiaoxinyan, liuyuan04, wangyizhong01, wu hua, sheqiaoqiao, liuxuan, wutian, wanghaifeng}@baidu.com Abstract 2016; Nguyen et al., 2016). In recent years, a number of datasets have been developed for MRC, as shown in Table 1. These datasets have led to advances such as Match-LSTM (Wang and Jiang, 2017), BiDAF (Seo et al., 2016), AoA Reader (Cui et al., 2017), DCN (Xiong et al., 2017) and RNet (Wang et al., 2017). This paper hopes to advance MRC even further with the release of DuReader, challenging the community to deal with more realistic data sources, more types of questions and more scale, as illustrated in Tables 1-4. Table 1 highlights DuReader’s advantages over previous datasets in terms of data sources and scale. Tables 2-4 highlight DuReader’s advantages in the range of questions. Ideally, a good dataset should be based on questions from real applications. However, many existing datasets have been forced to make various compromises such a"
W18-2605,C16-1167,0,0.21916,"Reader, challenging the community to deal with more realistic data sources, more types of questions and more scale, as illustrated in Tables 1-4. Table 1 highlights DuReader’s advantages over previous datasets in terms of data sources and scale. Tables 2-4 highlight DuReader’s advantages in the range of questions. Ideally, a good dataset should be based on questions from real applications. However, many existing datasets have been forced to make various compromises such as: (1) cloze task: Data is synthesized missing a keyword. The task is to fill in the missing keyword (Hermann et al., 2015; Cui et al., 2016; Hill et al., 2015). (2) multiple-choice exams: Richardson et al. (2013) collect both fictional stories and the corresponding multiple-choice questions by crowdsourcing. Lai et al. (2017) collect the multiple-choice questions from English exams. (3) crowdsourcing: Turkers are given documents (e.g., articles from the news and/or Wikipedia) and are asked to construct questions after reading the documents(Trischler et al., 2017; Rajpurkar et al., 2016; Koˇcisk`y et al., 2017). The limitations of the datasets lead to build datasets based on queries that real users submitted to real search engines"
W18-2605,P17-1147,0,0.0514491,"du.com) is the largest Chinese community-based question answering (CQA) site in the world. 2 http://ai.baidu.com/broad/download? dataset=dureader 3 https://github.com/baidu/DuReader 37 Proceedings of the Workshop on Machine Reading for Question Answering, pages 37–46 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics Dataset CNN/DM (Hermann et al., 2015) HLF-RC (Cui et al., 2016) CBT (Hill et al., 2015) RACE (Lai et al., 2017) MCTest (Richardson et al., 2013) NewsQA (Trischler et al., 2017) SQuAD (Rajpurkar et al., 2016) SearchQA (Dunn et al., 2017) TrivaQA (Joshi et al., 2017) NarrativeQA (Koˇcisk`y et al., 2017) MS-MARCO (Nguyen et al., 2016) DuReader (this paper) Lang EN ZH EN EN EN EN EN EN EN EN EN ZH #Que. 1.4M 100K 688K 870K 2K 100K 100K 140K 40K 46K 100K 200k #Docs 300K 28K 108 50K 500 10K 536 6.9M 660K 1.5K 200K1 1M Source of Que. Synthetic cloze Synthetic cloze Synthetic cloze English exam Crowdsourced Crowdsourced Crowdsourced QA site Trivia websites Crowdsourced User logs User logs Source of Docs News Fairy/News Children’s books English exam Fictional stories CNN Wiki. Web doc. Wiki./Web doc. Book&movie Web doc. Web doc./CQA Answer Type Fill in entity Fi"
W18-2605,W04-1013,0,0.0483886,"paragraph or passage. In contrast, DuReader provides the full body text of each document to stimulate the research in a real-world setting. get a vector representation for each position. Implementation Details We randomly initialize the word embeddings with a dimension of 300 and set the hidden vector size as 150 for all layers. We use the Adam algorithm (Kingma and Ba, 2014) to train both MRC models with an initial learning rate of 0.001 and a batch size of 32. 4.2 Results and Analysis We evaluate the reading comprehension task via character-level BLEU-4 (Papineni et al., 2002) and Rouge-L (Lin, 2004), which are widely used for evaluating the quality of language generation. The experimental results on test set are shown in Table 6. For comparison, we also evaluate the Selected Paragraph that has the largest overlap with the question among all documents. We also assess human performance by involving a new annotator to annotate on the test data and treat his first answer as the prediction. The results demonstrate that current reading comprehension models can achieve an impressive improvement compared with the selected paragraph baseline, which approves the effectiveness of these models. Howe"
W18-2605,P02-1040,0,0.100891,"uppose to find the answer in a small paragraph or passage. In contrast, DuReader provides the full body text of each document to stimulate the research in a real-world setting. get a vector representation for each position. Implementation Details We randomly initialize the word embeddings with a dimension of 300 and set the hidden vector size as 150 for all layers. We use the Adam algorithm (Kingma and Ba, 2014) to train both MRC models with an initial learning rate of 0.001 and a batch size of 32. 4.2 Results and Analysis We evaluate the reading comprehension task via character-level BLEU-4 (Papineni et al., 2002) and Rouge-L (Lin, 2004), which are widely used for evaluating the quality of language generation. The experimental results on test set are shown in Table 6. For comparison, we also evaluate the Selected Paragraph that has the largest overlap with the question among all documents. We also assess human performance by involving a new annotator to annotate on the test data and treat his first answer as the prediction. The results demonstrate that current reading comprehension models can achieve an impressive improvement compared with the selected paragraph baseline, which approves the effectivene"
W18-2605,D16-1264,0,0.485744,"ke various compromises such as: (1) cloze task: Data is synthesized missing a keyword. The task is to fill in the missing keyword (Hermann et al., 2015; Cui et al., 2016; Hill et al., 2015). (2) multiple-choice exams: Richardson et al. (2013) collect both fictional stories and the corresponding multiple-choice questions by crowdsourcing. Lai et al. (2017) collect the multiple-choice questions from English exams. (3) crowdsourcing: Turkers are given documents (e.g., articles from the news and/or Wikipedia) and are asked to construct questions after reading the documents(Trischler et al., 2017; Rajpurkar et al., 2016; Koˇcisk`y et al., 2017). The limitations of the datasets lead to build datasets based on queries that real users submitted to real search engines. MS-MARCO (Nguyen et al., 2016) is based on Bing logs (in English), and DuReader (this paper) is based on the logs of Baidu Search (in Chinese). Besides question sources, DuReader complements MS-MARCO and other datasets in the following ways: question types: DuReader contains a richer inThis paper introduces DuReader, a new large-scale, open-domain Chinese machine reading comprehension (MRC) dataset, designed to address real-world MRC. DuReader has"
W18-2605,D13-1020,0,0.161908,"a sources, more types of questions and more scale, as illustrated in Tables 1-4. Table 1 highlights DuReader’s advantages over previous datasets in terms of data sources and scale. Tables 2-4 highlight DuReader’s advantages in the range of questions. Ideally, a good dataset should be based on questions from real applications. However, many existing datasets have been forced to make various compromises such as: (1) cloze task: Data is synthesized missing a keyword. The task is to fill in the missing keyword (Hermann et al., 2015; Cui et al., 2016; Hill et al., 2015). (2) multiple-choice exams: Richardson et al. (2013) collect both fictional stories and the corresponding multiple-choice questions by crowdsourcing. Lai et al. (2017) collect the multiple-choice questions from English exams. (3) crowdsourcing: Turkers are given documents (e.g., articles from the news and/or Wikipedia) and are asked to construct questions after reading the documents(Trischler et al., 2017; Rajpurkar et al., 2016; Koˇcisk`y et al., 2017). The limitations of the datasets lead to build datasets based on queries that real users submitted to real search engines. MS-MARCO (Nguyen et al., 2016) is based on Bing logs (in English), and"
W18-2605,W17-2623,0,0.246887,"s have been forced to make various compromises such as: (1) cloze task: Data is synthesized missing a keyword. The task is to fill in the missing keyword (Hermann et al., 2015; Cui et al., 2016; Hill et al., 2015). (2) multiple-choice exams: Richardson et al. (2013) collect both fictional stories and the corresponding multiple-choice questions by crowdsourcing. Lai et al. (2017) collect the multiple-choice questions from English exams. (3) crowdsourcing: Turkers are given documents (e.g., articles from the news and/or Wikipedia) and are asked to construct questions after reading the documents(Trischler et al., 2017; Rajpurkar et al., 2016; Koˇcisk`y et al., 2017). The limitations of the datasets lead to build datasets based on queries that real users submitted to real search engines. MS-MARCO (Nguyen et al., 2016) is based on Bing logs (in English), and DuReader (this paper) is based on the logs of Baidu Search (in Chinese). Besides question sources, DuReader complements MS-MARCO and other datasets in the following ways: question types: DuReader contains a richer inThis paper introduces DuReader, a new large-scale, open-domain Chinese machine reading comprehension (MRC) dataset, designed to address real"
W18-2605,P17-1018,0,0.0958802,"Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, Haifeng Wang Baidu Inc., Beijing, China {hewei06, liukai20, liujing46, lvyajuan, zhaoshiqi, xiaoxinyan, liuyuan04, wangyizhong01, wu hua, sheqiaoqiao, liuxuan, wutian, wanghaifeng}@baidu.com Abstract 2016; Nguyen et al., 2016). In recent years, a number of datasets have been developed for MRC, as shown in Table 1. These datasets have led to advances such as Match-LSTM (Wang and Jiang, 2017), BiDAF (Seo et al., 2016), AoA Reader (Cui et al., 2017), DCN (Xiong et al., 2017) and RNet (Wang et al., 2017). This paper hopes to advance MRC even further with the release of DuReader, challenging the community to deal with more realistic data sources, more types of questions and more scale, as illustrated in Tables 1-4. Table 1 highlights DuReader’s advantages over previous datasets in terms of data sources and scale. Tables 2-4 highlight DuReader’s advantages in the range of questions. Ideally, a good dataset should be based on questions from real applications. However, many existing datasets have been forced to make various compromises such as: (1) cloze task: Data is synthesized missing a keywor"
W19-5341,N13-1073,0,0.0800459,"a, increasing the model diversity for the model ensemble. The experimental results indicate that this method achieves absolute improvements over the single system (at most a 1.7 BLEU point improvements). 2.9 Experiments and Results 3.1 Pre-processing and Post-processing The Chinese data has been tokenized using the Jieba tokenizer3 . For English data, punctuation normalization, aggressive tokenization and truecasing are applied orderly to all sentences with the scripts provided in Moses. We also filter the parallel sentences which are duplicated or bad alignment scores obtained by fast-align (Dyer et al., 2013), and then we have a preprocessed bilingual training data consisting of 18M parallel sentences. In post-processing phase, the English translations are true-cased and de-tokenized with the scripts provided in Moses. We use simple rules to normalize the punctuations and Arabic numerals in the Chinese translations. Re-ranking In order to get better translation results, we generate n-best hypotheses with an ensemble model and then train a re-ranker using k-best MIRA (Cherry and Foster, 2012) on the validation set. K-best MIRA is a version of MIRA (Chiang et al., 2008) that works with a batch tunin"
W19-5341,D18-1045,0,0.109803,"er improved. In the next iteration, the two improved models can potentially generate better synthetic parallel data. This procedure can be applied in several iterations until no further improvement can be obtained. In addition, we also augment the training data by exploring the bilingual corpus rather than the monolingual corpus. Specifically, we translate the sentences in the target language back into the source language by diverse training models, such as Left-to-right model and Right-to-left model. This procedure can be viewed as one alternative Large-scale Back-Translation In recent work, Edunov et al. (2018) proposed an effective approach to improve the translation quality by exploiting back-translation mechanism on the large-scale monolingual corpus. Following their work, we also train our model on the synthetic bilingual corpus to further improve the performance. However, the provided monolingual data contains a certain amount of noise and out-ofdomain data which may affect the translation quality implicitly. Therefore, we use a language model to select high-quality and in-domain data from the 376 Source CWMT UN Wiki Titles Total solution for alleviating the exposure bias problem (Ranzato et al"
W19-5341,D16-1139,0,0.0541292,"ck-translation mechanism on the large-scale monolingual corpus. Following their work, we also train our model on the synthetic bilingual corpus to further improve the performance. However, the provided monolingual data contains a certain amount of noise and out-ofdomain data which may affect the translation quality implicitly. Therefore, we use a language model to select high-quality and in-domain data from the 376 Source CWMT UN Wiki Titles Total solution for alleviating the exposure bias problem (Ranzato et al., 2016). 2.6 Knowledge Distillation The early adoption of knowledge distillation (Kim and Rush, 2016) is for model compression, where the goal is to deliver a compact student model that matches the accuracy of a large teacher model or the ensemble of models. In our knowledge distillation approach, we translate the source side of the bilingual data with a Right-to-Left (R2L) (Liu et al., 2016) model teacher and different architecture NMT teachers to use the translations as additional training data for the student network. Considering that distillation from a bad teacher model is likely to hurt the student model and thus result in inferior accuracy, we selectively use distillation in the traini"
W19-5341,N16-1046,0,0.0898531,"Missing"
W19-5341,N12-1047,0,0.0413295,"Moses. We also filter the parallel sentences which are duplicated or bad alignment scores obtained by fast-align (Dyer et al., 2013), and then we have a preprocessed bilingual training data consisting of 18M parallel sentences. In post-processing phase, the English translations are true-cased and de-tokenized with the scripts provided in Moses. We use simple rules to normalize the punctuations and Arabic numerals in the Chinese translations. Re-ranking In order to get better translation results, we generate n-best hypotheses with an ensemble model and then train a re-ranker using k-best MIRA (Cherry and Foster, 2012) on the validation set. K-best MIRA is a version of MIRA (Chiang et al., 2008) that works with a batch tuning to learn a re-ranker for the n-best hypotheses. The features we use for re-ranking are: • NMT Features: Ensemble model score and Right-to-Left model score. 3.2 • Language Model Features: Multiple n-gram language models and backward n-gram language models. For Chinese→English task, we do not use all of the 18M preprocessed parallel sentences, in that there is much out-of-domain data in UN corpus. Table 1 shows that the 6.7M CWMT corpus and 9M UN corpus which are selected ran• Length Fea"
W19-5341,2015.iwslt-evaluation.11,0,0.0627984,"a teacher model are filtered if BLEU scores are below a threshold τ . According to our previous empirical results, we select English translations with BLEU score higher than 30 and Chinese translations with BLEU score higher than 42. There are two kinds of teacher models to help a student model improve translation performance: En→Chn 6.7M 3.5M 0.6M 10.8M Table 1: Statistics of the bilingual training data (Chn indicates Chinese while En indicates English). data. The dominant approach for domain adaptation is training on large-scale out-of-domain data and then fine-tuning on the in-domain data (Luong and Manning, 2015). Thus the effectiveness of the domain adaptation depends on the selected in-domain data. According to our previous empirical results, using the WMT 18 dev set to fine-tune the models straightforwardly achieves the best results. In our final submission, we set the batch size to 1,024 and fine-tune the model for a few iterations on the WMT 18 dev set. It is surprising to find a gain of almost +2 BLEU improvement on WMT 18 Chinese→English test set. However, on WMT 18 English→Chinese test set, the improvement is not significant. In WMT 17 and 18, the source side of both dev set and test set are c"
W19-5341,D08-1024,0,0.0139368,"scores obtained by fast-align (Dyer et al., 2013), and then we have a preprocessed bilingual training data consisting of 18M parallel sentences. In post-processing phase, the English translations are true-cased and de-tokenized with the scripts provided in Moses. We use simple rules to normalize the punctuations and Arabic numerals in the Chinese translations. Re-ranking In order to get better translation results, we generate n-best hypotheses with an ensemble model and then train a re-ranker using k-best MIRA (Cherry and Foster, 2012) on the validation set. K-best MIRA is a version of MIRA (Chiang et al., 2008) that works with a batch tuning to learn a re-ranker for the n-best hypotheses. The features we use for re-ranking are: • NMT Features: Ensemble model score and Right-to-Left model score. 3.2 • Language Model Features: Multiple n-gram language models and backward n-gram language models. For Chinese→English task, we do not use all of the 18M preprocessed parallel sentences, in that there is much out-of-domain data in UN corpus. Table 1 shows that the 6.7M CWMT corpus and 9M UN corpus which are selected ran• Length Features: Length ratio and length difference between source sentences and hypothe"
W19-5341,N18-1202,0,0.00961915,"ion heads to 16. Both Chinese and English pre-training took 7 days to complete. In the fine-tuning procedure of the translation task, we employ a pre-trained language model as encoder of NMT, and the parameters of decoders are learned during fine-tuning. The decoder has 6 self-attention layers, and the hidden size is 1024, which is same with the decoder of standard big Transformer. During fine-tuning, we only fix the parameters of the language model for the first 10,000 steps. Pre-trained Transformer Recent empirical improvements with language models have showed that unsupervised pretraining (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018; Dai et al., 2019; Sun et al., 2019) on very large corpora is an integral part of many NLP tasks. We implement a big Transformer language model using PaddlePaddle1 , an end-to-end open source deep learning platform developed by Baidu. It provides a complete suite of deep learning libraries, tools and service platforms to make the research and development of deep learning simple and reliable. The language model is pre-trained only with masked language model task (Taylor, 1953; Devlin et al., 2018; Sun et al., 2019) on a monolingual corpus of the sourc"
W19-5341,P19-1285,0,0.0714852,"The Transformer model (Vaswani et al., 2017), which exploits self-attention mechanism both in the encoder and decoder, has significantly improved the translation quality in recent years. It is also adopted by most participants as the basic Neural Machine Translation (NMT) system in the previous translation campaigns (Bojar et al., 2018; Niehues et al., 2018). In this year’s translation task, we focus on the improvement of single system, and propose three novel Transformer variants: • Pre-trained Transformer: We train a big Transformer language model (Radford et al., 2018; Devlin et al., 2018; Dai et al., 2019; Sun et al., 2019) on monolingual corpora, and use the language model as the encoder of the Transformer model. 2 • Deeper Transformer: We increase the encoder layers to better learn the representation of the source sentences. Specifically, we increase the number of encoder layers from 6 to 30 for the base version, and from 6 to 15 layers for the big version. System Overview Figure 1 depicts the overall process of our submissions in this year’s evaluation task, in which we train our advanced Transformer models on the bilingual corpus together with synthetic corpora, fine-tune them on the well-"
W19-5341,P16-1009,0,0.0741956,"Missing"
W19-5341,D18-1458,0,0.0491168,"Missing"
W19-5341,W18-6429,0,0.0370671,"e also use an iterative approach (Zhang et al., 2018) to extend the back translation method by jointly training source-totarget and target-to-source NMT models. For bilingual data augmentation, a target-to-source baseline system is used to translate the target of the bilingual corpus as the synthetic data. Moreover, the sequence-level knowledge distillation (Hassan et al., 2018) mechanism is employed to boost the performance by means of using the model decoding from right to left (Right-to-Left) and the aforementioned Transformer variants to generate synthetic data for training the NMT model (Wang et al., 2018). The remainder of paper is structured as follows: Section 2 describes the detailed overview of our training strategy. Section 3 shows the experimental settings and results. Finally, we conclude our work in Section 4. Introduction The Transformer model (Vaswani et al., 2017), which exploits self-attention mechanism both in the encoder and decoder, has significantly improved the translation quality in recent years. It is also adopted by most participants as the basic Neural Machine Translation (NMT) system in the previous translation campaigns (Bojar et al., 2018; Niehues et al., 2018). In this"
wu-wang-2004-improving-domain,J96-1001,0,\N,Missing
wu-wang-2004-improving-domain,ahrenberg-etal-2000-evaluation,0,\N,Missing
wu-wang-2004-improving-domain,W02-1405,0,\N,Missing
wu-wang-2004-improving-domain,J93-2003,0,\N,Missing
wu-wang-2004-improving-domain,W01-1406,0,\N,Missing
wu-wang-2004-improving-domain,2001.mtsummit-ebmt.4,0,\N,Missing
wu-wang-2004-improving-domain,P03-1012,0,\N,Missing
wu-wang-2004-improving-domain,2001.mtsummit-papers.60,0,\N,Missing
wu-wang-2004-improving-domain,J97-2004,0,\N,Missing
wu-wang-2004-improving-domain,1996.amta-1.13,0,\N,Missing
wu-wang-2004-improving-domain,J97-3002,0,\N,Missing
wu-wang-2004-improving-domain,P98-1004,0,\N,Missing
wu-wang-2004-improving-domain,C98-1004,0,\N,Missing
wu-wang-2004-improving-domain,tufis-barbu-2002-lexical,0,\N,Missing
wu-wang-2004-improving-domain,P00-1056,0,\N,Missing
