2007.tmi-papers.28,W05-0909,0,0.0472932,"lar for the optimisation of the weights of the log-linear model), and the final evaluation is conducted using the test set (using the CRR=Correct Recognition Result input condition). For both Chinese and Italian, POS-tagging is performed using the M X P OST tagger (Ratnaparkhi, 1996). Table 1 summarizes the various corpus statistics. The number of training/test examples refers to the examples involved in the classification task. For all experiments, the quality of the translation output is evaluated using the accuracy measures BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), using 7 references and ignoring case information. For BLEU and NIST, we also Train. Sentences Running words Vocabulary size Train. examples Dev. Sentences Running words Vocabulary size Test examples Eval. Sentences Running words Vocabulary size Test examples Chinese–English Italian–English 44,501 323,958 351,303 11,421 10,363 434,442 21,484 156,237 169,476 10,418 7,359 391,626 489 (7 refs.) 5,214 39,183 1,137 1,821 8,004 489 (7 refs.) 4,976 39,368 1,234 1,776 7,993 500 (7 refs.) 5,550 44,089 1,328 2,038 8,301 500 (7 refs.) 5,787 44,271 1,467 1,976 9,103 Table 1: Chinese–English and Italian–E"
2007.tmi-papers.28,J93-2003,0,0.00624254,"ontext-informed features, i.e. the sourcesimilarity exploitation, results in an improvement in translation quality, for Italian-to-English and Chinese-to-English translations tasks. 2 Log-Linear Phrase-Based SMT In statistical machine translation (SMT), translation is modeled as a decision process, in which the translation eI1 = e1 . . . ei . . . eI of a source sentence f1J = f1J = f1 . . . fj . . . fJ is chosen to maximize: argmax P(eI1 |f1J ) = argmax P(f1J |eI1 ).P(eI1 ), I,eI1 I,eI1 (1) where P(f1J |eI1 ) and P(eI1 ) denote respectively the translation model and the target language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P(eI1 |f1J ) is directly modeled as a (log-linear) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language 231 model: log P(eI1 |f1J ) = m X λm hm (f1J , eI1 , sK 1 ) m=1 + λLM log P(eI1 ), (2) where sK 1 = s1 . . . sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (e˜1 , . . . , e˜k ) and (f˜1 , . . . , f˜k ) such that (we set i0 := 0): ∀1 ≤ k ≤ K, sk := (ik ; bk , jk ), e˜k := eik−1 +1 . . . eik , f˜k := fbk . . . fjk . K X ˜"
2007.tmi-papers.28,2003.mtsummit-papers.4,0,0.0138252,"the translation of these fragments (transfer) (Nagao, 1984; Somers, 1999; Carl and Way, 2003). A number of matching techniques and notions of similarity have been proposed. Consequently, EBMT crucially relies on the retrieval of source sentences similar to f in the bilingual training corpus; in other words, EBMT is source-similarity based. Let us also mention (Somers et al., 1994), which marks the fragments to translate with their (left and right) contexts. Source and Target Similarity While the use of target-similarity may avoid problems such as boundary-friction usually encountered in EBMT (Brown et al., 2003), the use of source-similarity may limit ambiguity problems (cf. Section 3). By exploiting the two types of similarity, we hope to benefit from the strength of both aspects. 5 5.1 Experimental Results Data, Tasks, and Baseline The experiments were carried out using the Chinese–English and Italian–English datasets provided within the IWSLT 2006 evaluation campaign (Paul, 2006), extracted from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002). This multilingual speech corpus contains sentences similar to those that are usually found in phrase-books for tourists going abroad. Trai"
2007.tmi-papers.28,P05-1048,0,0.0497272,"t the accuracy and the flexibility of discriminative learning (Cowan et al., 2006; Liang et al., 2006; Tillmann and Zhang, 2006; Wellington et al., 2006). These papers generally require one to redefine one’s training procedures; on the contrary our approach introduces new features while keeping the strength of existing state-of-the-art systems. The exploitation of source-similarity is one of the key components of EBMT (Nagao, 1984; Somers, 1999; Carl and Way, 2003); one could say that our approach is a combination of EBMT and SMT since we exploit both source similarity and target similarity. (Carpuat and Wu, 2005) present an attempt to use word-sense disambiguation techniques to MT in order to enhance lexical selection; in a sense, we are also performing some sort of word-sense disambiguation, even if the handling of lexical selection is performed totally implicitly in our case. 7 Conclusion In this paper, we have introduced new features for log-linear phrase-based SMT, that take into account contextual information about the source phrases to translate. This contextual information can take the form of left and right context words, as well as other source of knowledge such as Part-Of-Speech information."
2007.tmi-papers.28,W02-1001,0,0.0225766,"This does not affect our general line of reasoning. 3.1 Context-Informed Features Context-Based Disambiguation The optimization of the feature weights λm can be performed in a discriminative learning setting (Och and Ney, 2002). However, it is important to note that these weights are meta-parameters. Indeed, the dependencies between the parameters of the standard phrase-based approach consist of: (i) relationships between single phrases (modeled by ˜ (ii) relationships between consecutive target h), words (modeled by the language model), which is generally characteristic of generative models (Collins, 2002; Dietterich, 2002). Notably, dependencies between consecutive source phrases are not directly expressed. Discriminative frameworks usually allow for the introduction of (relatively) unrestricted dependencies that are relevant to the decision process. In particular, disambiguation problems can be solved by taking the direct context of the entity to disambiguate into account (e.g. Dietterich (2002)). In the translation example displayed in Figure 1, the source right context is sufficient to solve the ambiguity: when followed by di baseball, the (Italian) word partita is very likely to correspon"
2007.tmi-papers.28,W06-1628,0,0.070058,"Missing"
2007.tmi-papers.28,W06-1607,0,0.0273569,"f source phrases into account. Word-based features A feature that includes the direct left and right context words (resp. fbk −1 and fjk +1 ) of a given phrase f˜k = fbk . . . fjk takes the following form: 4 Memory-Based Disambiguation 4.1 A Classification Approach The direct estimation of P(e˜k |f˜k , CI(f˜k )), for example using relative frequencies, is problematic. Indeed, it is well known that the estimation of P(e˜k |f˜k ) using relative frequencies results in the overestimation of the probabilities of long phrases K X J I K ˜ ˜ hm (fk , fbk −1 , fjk +1 , e˜k , sk ). (Zens and Ney, 2004; Foster et al., 2006); a frehm (f1 , e1 , s1 ) = quent remedy consists of introducing a smoothing k=1 factor, which takes the form of lexical-based feaIn this case, the contextual information can be tures (Zens and Ney, 2004). Similar issues and seen as a window of size 3 (focus phrase + left a variety of smoothing techniques are discussed context word + right context word), centered on in (Foster et al., 2006). In the case of contextthe source phrase f˜k . Larger contexts may also be informed features, since the context is also taken considered. More generally, we have: into account, this estimation problem can o"
2007.tmi-papers.28,N03-1017,0,0.116106,"Missing"
2007.tmi-papers.28,koen-2004-pharaoh,0,0.292053,"is a feature that applies to a single where h phrase-pair.1 It thus follows: m X m=1 λm K X ˜ m (f˜k , e˜k , sk ) = h K X ˜ f˜k , e˜k , sk ), h( k=1 k=1 ˜= with h m X ˜ m . (4) λm h m=1 In this context, the translation process amounts to: (i) choosing a segmentation of the source sentence, (ii) translating each source phrase, and possibly (iii) re-ordering the target segments obtained. The target language model is used to guide the decision process; in case no particular constraints are assumed, it is common to employ beam search techniques to reduce the number of hypotheses to be considered (Koehn, 2004). Equations (2) and (4) characterize what is referred to as the standard phrase-based approach in the following. 1 Figure 1: Examples of ambiguity for the (Italian) word partita, easily solved when considering its context 3 A remarkable property of this approach is that the usual translational features involved in those models only depend on a pair of source/target phrases, i.e. they do not take into account the contexts of those phrases. This means that each feature hm in equation (2) can be rewritten as: hm (f1J , eI1 , sK 1 )= C’`e una partita di baseball oggi ? (⇔ Is there a baseball game"
2007.tmi-papers.28,P06-1096,0,0.0564009,"Missing"
2007.tmi-papers.28,P02-1038,0,0.762423,"ork that enables the estimation of these features while avoiding sparseness problems. We evaluate the performance of our approach on Italian-to-English and Chineseto-English translation tasks using a state-of-the-art phrase-based SMT system, and report significant improvements for both BLEU and NIST scores when adding the context-informed features. 1 Introduction In log-linear phrase-based SMT, the probability P(eI1 |f1J ) of target phrase eI1 given a source phrase f1J is modeled as a (log-linear) combination of features that usually comprise some translational features, and a language model (Och and Ney, 2002). The usual translational features involved in those models express dependencies between source and target phrases, but not dependencies between source phrases themselves. In particular, the context in which those phrases occur is never taken into account during translation. While the language model can be seen as a way to exploit target similarity (between the translation and Andy Way Dublin City University, Dublin, Ireland away@ computing.dcu.ie other target sentences), one could ask whether it is also possible to exploit source similarity, i.e. to take into account the context in which the"
2007.tmi-papers.28,P03-1021,0,0.0346756,"xt is an intermediary result of the estimation of P(e˜k |f˜k , CI(f˜k )). In ˜ m (f˜k , CI(f˜k ), e˜k , sk ) = addition to the feature h log P(e˜k |f˜k , CI(f˜k )), we consider a simple binary feature based on this intermediary result: ˜ best h   1 if e˜k is (one of) the target phrases = with the most support,   0 otherwise, where “most support” means the highest probability according to P(e˜k |f˜k , CI(f˜k )). The two ˜ m and h ˜ best are integrated in the logfeatures h linear model. As for the standard phrasebased approach, their weights are optimized using minimum-error-rate training (Och, 2003). 4.4 Implementation Issues When predicting a target phrase given a source phrase and its context, the source phrase is intuitively the feature with the highest prediction power; in all our experiments, it is the feature with the highest IG. In the trie constructed by IGT REE, this is thus the feature on which the first branching decision is taken. Consequently, when classifying a source phrase f˜k with its context, there are two possible situations, depending on f˜k being in the training material or not. In the first case, f˜k is matched, and we proceed further down the trie. At this stage, i"
2007.tmi-papers.28,P02-1040,0,0.110535,"pment set (devset 4) was used for tuning purposes (in particular for the optimisation of the weights of the log-linear model), and the final evaluation is conducted using the test set (using the CRR=Correct Recognition Result input condition). For both Chinese and Italian, POS-tagging is performed using the M X P OST tagger (Ratnaparkhi, 1996). Table 1 summarizes the various corpus statistics. The number of training/test examples refers to the examples involved in the classification task. For all experiments, the quality of the translation output is evaluated using the accuracy measures BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), using 7 references and ignoring case information. For BLEU and NIST, we also Train. Sentences Running words Vocabulary size Train. examples Dev. Sentences Running words Vocabulary size Test examples Eval. Sentences Running words Vocabulary size Test examples Chinese–English Italian–English 44,501 323,958 351,303 11,421 10,363 434,442 21,484 156,237 169,476 10,418 7,359 391,626 489 (7 refs.) 5,214 39,183 1,137 1,821 8,004 489 (7 refs.) 4,976 39,368 1,234 1,776 7,993 500 (7 refs.) 5,550 44,089 1,328 2,038 8,301 500 (7 refs.) 5,787"
2007.tmi-papers.28,W96-0213,0,0.361764,"Missing"
2007.tmi-papers.28,W05-0908,0,0.0612328,"e Train. examples Dev. Sentences Running words Vocabulary size Test examples Eval. Sentences Running words Vocabulary size Test examples Chinese–English Italian–English 44,501 323,958 351,303 11,421 10,363 434,442 21,484 156,237 169,476 10,418 7,359 391,626 489 (7 refs.) 5,214 39,183 1,137 1,821 8,004 489 (7 refs.) 4,976 39,368 1,234 1,776 7,993 500 (7 refs.) 5,550 44,089 1,328 2,038 8,301 500 (7 refs.) 5,787 44,271 1,467 1,976 9,103 Table 1: Chinese–English and Italian–English corpus statistics report statistical significance p-values, estimated using approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005).7 To assess the validity of our approach, we use the state-of-the-art phrase-based SMT system M OSES (Koehn et al., 2007).8 The baseline system is composed of the usual features: phrase-based probabilities and lexical weighting in both directions, phrase and word penalties, and re-ordering. Our system additionally includes the memory-based features described in Sections 3 and 4. 5.2 Translation Results The results obtained for the Italian–English and Chinese–English translation tasks using the IWSLT data are summarized in Table 2. The contextual information may include the (context) words, th"
2007.tmi-papers.28,takezawa-etal-2002-toward,0,0.0141726,"xts. Source and Target Similarity While the use of target-similarity may avoid problems such as boundary-friction usually encountered in EBMT (Brown et al., 2003), the use of source-similarity may limit ambiguity problems (cf. Section 3). By exploiting the two types of similarity, we hope to benefit from the strength of both aspects. 5 5.1 Experimental Results Data, Tasks, and Baseline The experiments were carried out using the Chinese–English and Italian–English datasets provided within the IWSLT 2006 evaluation campaign (Paul, 2006), extracted from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002). This multilingual speech corpus contains sentences similar to those that are usually found in phrase-books for tourists going abroad. Training was performed using the default training set, to which we added the sets devset1, devset2, and devset3. The development set (devset 4) was used for tuning purposes (in particular for the optimisation of the weights of the log-linear model), and the final evaluation is conducted using the test set (using the CRR=Correct Recognition Result input condition). For both Chinese and Italian, POS-tagging is performed using the M X P OST tagger (Ratnaparkhi, 1"
2007.tmi-papers.28,P06-1091,0,0.069543,"will select the correct translation baseball game as the most 232 probable among all the possible combinations of target words: gone of baseball, game of baseball, baseball partita, baseball game, etc., but this solution appears to be more expensive than simply looking at the context. In particular, the context can be used to early prune weak candidates, which allows spending more time on promising candidates. Several discriminative frameworks have been proposed recently in the context of MT to fully exploit the flexibility of discriminative approaches (Cowan et al., 2006; Liang et al., 2006; Tillmann and Zhang, 2006; Wellington et al., 2006). Unfortunately, this flexibility usually comes at the price of training complexity. An alternative in-between approach, pursued in this paper, consists of introducing context-informed features in the original log-linear framework. This enables us to take the context of source phrases into accounts, while benefiting from the existing training and optimization procedures of the standard phrase-based approach. 3.2 Class-based features In addition to the context words themselves, it is possible to exploit several knowledge sources characterizing the context. For example,"
2007.tmi-papers.28,2006.amta-papers.28,0,0.0677454,"anslation baseball game as the most 232 probable among all the possible combinations of target words: gone of baseball, game of baseball, baseball partita, baseball game, etc., but this solution appears to be more expensive than simply looking at the context. In particular, the context can be used to early prune weak candidates, which allows spending more time on promising candidates. Several discriminative frameworks have been proposed recently in the context of MT to fully exploit the flexibility of discriminative approaches (Cowan et al., 2006; Liang et al., 2006; Tillmann and Zhang, 2006; Wellington et al., 2006). Unfortunately, this flexibility usually comes at the price of training complexity. An alternative in-between approach, pursued in this paper, consists of introducing context-informed features in the original log-linear framework. This enables us to take the context of source phrases into accounts, while benefiting from the existing training and optimization procedures of the standard phrase-based approach. 3.2 Class-based features In addition to the context words themselves, it is possible to exploit several knowledge sources characterizing the context. For example, we can consider the Part-"
2007.tmi-papers.28,N04-1033,0,0.0389915,"t take the context of source phrases into account. Word-based features A feature that includes the direct left and right context words (resp. fbk −1 and fjk +1 ) of a given phrase f˜k = fbk . . . fjk takes the following form: 4 Memory-Based Disambiguation 4.1 A Classification Approach The direct estimation of P(e˜k |f˜k , CI(f˜k )), for example using relative frequencies, is problematic. Indeed, it is well known that the estimation of P(e˜k |f˜k ) using relative frequencies results in the overestimation of the probabilities of long phrases K X J I K ˜ ˜ hm (fk , fbk −1 , fjk +1 , e˜k , sk ). (Zens and Ney, 2004; Foster et al., 2006); a frehm (f1 , e1 , s1 ) = quent remedy consists of introducing a smoothing k=1 factor, which takes the form of lexical-based feaIn this case, the contextual information can be tures (Zens and Ney, 2004). Similar issues and seen as a window of size 3 (focus phrase + left a variety of smoothing techniques are discussed context word + right context word), centered on in (Foster et al., 2006). In the case of contextthe source phrase f˜k . Larger contexts may also be informed features, since the context is also taken considered. More generally, we have: into account, this es"
2007.tmi-papers.28,2006.iwslt-evaluation.1,0,\N,Missing
2009.eamt-1.25,W05-0909,0,0.0235009,"nteresting since the decoding algorithm is the same as the one used in our constraint satisfaction system. Therefore, the differences that are observed can be attributed to the modelling choices underlying the two systems. First, the constraint satisfaction system uses a richer objective function based on the constraint model that replaces the translation model. Second, constraint satisfaction inference searches a smaller solution space than the ReWrite system, which does not restrict its solution space in advance. Table 1 lists the BLEU scores (Papineni et al., 2002) and exact Meteor scores (Banerjee and Lavie, 2005) for both systems on each of the four data sets. The two systems are closest in performance on the EuroParl data, though constraint satisfaction inference outperforms ReWrite in terms of both metrics. On EMEA, ReWrite outperforms constraint satisfaction inference; on JRC-Acquis and OpenSubtitles, constraint satisfaction inference outperforms ReWrite again. The relative performance differences are rather diverse among the four data sets. This may be attributed to the underlying search algorithm, a greedy hill-climbing search, which is known to risk ending up in suboptimal local optima. Constrai"
2009.eamt-1.25,J96-1002,0,0.0355965,"e, the constraints of a constraint satisfaction problem specify which simultaneous value combinations over a number of variables are allowed. Here, we Proceedings of the 13th Annual Conference of the EAMT, pages 182–189, Barcelona, May 2009 182 adopt a weighted constraint satisfaction approach. Candidate solutions to a weighted constraint satisfaction problem are scored according to the sum of weights of the constraints they satisfy, and the highest scoring solution is selected. The constraint satisfaction approach presented here is formulated as an extension of statistical log-linear models (Berger et al., 1996; Papineni et al., 1998)1 . A typical log-linear model for machine translation combines a number of feature functions, each of which measures the quality of a candidate translation according to some aspect. Two feature functions tend to be part of any SMT system; a translation model (TM), and a target language model (LM), measuring the faithfulness and the fluency of the translation, respectively. Both are probability distributions, obtained by maximum-likelihood estimation from training data. The best translation is determined by maximising a weighted sum of those feature functions: argmax λT"
2009.eamt-1.25,D07-1007,0,0.113971,"inference) component finds the output structure that maximises the objective function. Within statistical machine translation systems, the probabilistic underpinning of all involved components acts as a kind of industrial standard. This has the positive effect that a substantial body of work could be built using the same universal language. At the same time, it makes the integration of non-stochastic components into statistical machine translation systems sometimes unwieldy. Yet, recent experiments on mixing local classifications of non-stochastic machine learners with statistical MT models (Carpuat and Wu, 2007; Stroppa et al., 2007), has shown its potential. Arguably, it makes sense to investigate the gathering of a mix of views on the objective function as input to the final search or inference process, and this mix should be eclectic—that is, theory-neutral—to allow extremely different but successful partial solutions to the objective function to participate. As we argue in Section 2, the classic constraint satisfaction framework offers the right apparatus to offer such a theory-neutral basis. In Section 3 we describe our experimental setup; the outcomes of a comparative study with an unconstrain"
2009.eamt-1.25,P07-1005,0,0.0134537,"xample sentence pair. Middle: trigram predictions for the words in the Dutch sentence. Bottom: the complete graph connecting all Dutch words. Any valid translation is a directed cycle in this graph that starts and ends in the BEGIN/END node. 3.2 Constraint prediction For predicting the soft constraints of our translation model, we need to map each word of the source sentence to a trigram of target words. The middle word of that trigram is the translation of the source word in focus; the left and right words are the two target words surrounding it. Several recent studies (Carpuat and Wu, 2007; Chan et al., 2007; Gim´enez and M`arquez, 2007; Stroppa et al., 2007) have experimented with classification-based alternatives to traditional translation models that take into account contextual information of the word in the source sentence, similarly to the way word-sense disambiguation is performed. Our constraint predictor is similar to the classifiers used in these studies in the sense that contextual information is used to improve the suggested translations. We follow (Stroppa et al., 2007) in using the k-nearest neighbor classifier as implemented in the TiMBL software package (Daelemans et al., The targ"
2009.eamt-1.25,N03-1010,0,0.105988,"pace of the constraint satisfaction problem defined in this section has immense proportions. Even if a base classifier perfectly predicts the correct translations of all source words, which is already overly optimistic, the inference procedure still has to consider every possible permutation of those translated words as a candidate translation. Unfortunately, no further restrictions or assumptions can be made that would restrict the solution space sufficiently to allow for exhaustive solving. Approximate solving is the only option. With this in mind, we choose the greedy decoding algorithm of Germann (2003) as the basis for the constraint solver. The algorithm starts with a complete candidate translation; for example, one where all source words are mapped to their most likely translations and added to the target sentence in original order. Subsequently, a hill-climbing search is started in which simple transformations of the current translation are attempted and the one leading to the highest score increase is actually applied. New transformations are tried until no further improvement can be attained. The following transformations are considered: • Change the translation of a source-language wo"
2009.eamt-1.25,W07-0719,0,0.040061,"Missing"
2009.eamt-1.25,2005.mtsummit-papers.11,0,0.00501454,"proposed in the context of statistical machine translation, it can more generally be seen as optimising an arbitrary objective function defined over candidate translations. By replacing the noisy-channel equations optimised originally by a credit function based on constraint weights, the algorithm can be employed for solving our constraint satisfaction problem. 3 Experimental setup 3.1 Data For our study we use four different corpora covering a diverse range of genres. From each of the four corpora, we prepare data sets for the translation pair Dutch to English: EuroParl The EuroParl corpus (Koehn, 2005) is a multi-lingual parallel corpus extracted from the proceedings of the European Parliament. The Dutch-English parallel subcorpus consists of 1,313,111 sentence pairs. JRC-Acquis The JRC-Acquis corpus (Steinberger et al., 2006) comprises a large collection of legislative texts extracted from the Acquis Communautaire. The Dutch-English parallel subcorpus provides 1,235,878 bilingual sentence pairs. EMEA The EMEA data set is composed of texts made available by the European Medicines Agency. It is one of the corpora included in the OPUS parallel corpus (Tiedemann and Nygaard, 2004). The paralle"
2009.eamt-1.25,P02-1040,0,0.0765224,"003). Comparing with this system is especially interesting since the decoding algorithm is the same as the one used in our constraint satisfaction system. Therefore, the differences that are observed can be attributed to the modelling choices underlying the two systems. First, the constraint satisfaction system uses a richer objective function based on the constraint model that replaces the translation model. Second, constraint satisfaction inference searches a smaller solution space than the ReWrite system, which does not restrict its solution space in advance. Table 1 lists the BLEU scores (Papineni et al., 2002) and exact Meteor scores (Banerjee and Lavie, 2005) for both systems on each of the four data sets. The two systems are closest in performance on the EuroParl data, though constraint satisfaction inference outperforms ReWrite in terms of both metrics. On EMEA, ReWrite outperforms constraint satisfaction inference; on JRC-Acquis and OpenSubtitles, constraint satisfaction inference outperforms ReWrite again. The relative performance differences are rather diverse among the four data sets. This may be attributed to the underlying search algorithm, a greedy hill-climbing search, which is known to"
2009.eamt-1.25,steinberger-etal-2006-jrc,0,0.0120954,"ptimised originally by a credit function based on constraint weights, the algorithm can be employed for solving our constraint satisfaction problem. 3 Experimental setup 3.1 Data For our study we use four different corpora covering a diverse range of genres. From each of the four corpora, we prepare data sets for the translation pair Dutch to English: EuroParl The EuroParl corpus (Koehn, 2005) is a multi-lingual parallel corpus extracted from the proceedings of the European Parliament. The Dutch-English parallel subcorpus consists of 1,313,111 sentence pairs. JRC-Acquis The JRC-Acquis corpus (Steinberger et al., 2006) comprises a large collection of legislative texts extracted from the Acquis Communautaire. The Dutch-English parallel subcorpus provides 1,235,878 bilingual sentence pairs. EMEA The EMEA data set is composed of texts made available by the European Medicines Agency. It is one of the corpora included in the OPUS parallel corpus (Tiedemann and Nygaard, 2004). The parallel texts for Dutch and English cover 751,602 sentence pairs. OpenSubtitles The OpenSubtitles corpus, also part of OPUS, provides aligned movie subtitles in various different languages. For the language pair Dutch-English, it compr"
2009.eamt-1.25,2007.tmi-papers.28,1,0.898212,"Missing"
2009.eamt-1.25,tiedemann-nygaard-2004-opus,0,0.0151206,"uroParl The EuroParl corpus (Koehn, 2005) is a multi-lingual parallel corpus extracted from the proceedings of the European Parliament. The Dutch-English parallel subcorpus consists of 1,313,111 sentence pairs. JRC-Acquis The JRC-Acquis corpus (Steinberger et al., 2006) comprises a large collection of legislative texts extracted from the Acquis Communautaire. The Dutch-English parallel subcorpus provides 1,235,878 bilingual sentence pairs. EMEA The EMEA data set is composed of texts made available by the European Medicines Agency. It is one of the corpora included in the OPUS parallel corpus (Tiedemann and Nygaard, 2004). The parallel texts for Dutch and English cover 751,602 sentence pairs. OpenSubtitles The OpenSubtitles corpus, also part of OPUS, provides aligned movie subtitles in various different languages. For the language pair Dutch-English, it comprises 288,160 sentence pairs. In four experiments, the translation system has been trained and tested on texts within the same corpus. For this evaluation, as well as for tuning the system, from each of the four corpora, two sets of 1,000 sentences each have been selected for testing and development purposes respectively; the remainder is used for training."
2010.amta-papers.23,J96-1002,0,0.0209051,"a brief overview of HPB. In Section 4 we describe the context-informed features contained in our baseline HPB model. In Section 5 we describe our memory-based classification approach. Section 6 describes experimental set-ups. Section 7 presents the results obtained, and offers a brief qualitative analysis. In Section 8 we formulate our conclusions, and offer some avenues for further work. 2 Related Work MT research on incorporating contexts into SMT models can be broadly divided into two categories: source-context modelling such as (Stroppa et al., 2007), and target-context modelling such as (Berger et al., 1996; Hasan et al., 2008). The present study relates to the first category, which further divides into the following approaches: Discriminative word alignment: Garc´ıa-Varea et al. (2001) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Subsequently, more recent discriminative approaches employ source-side contexts for creating finer-grained word-to-word lexicons (Brunning et al., 2009; Mauser et al., 2009; Patry and Langlais, 2009). Phrase-based SMT: Vickrey et al. (2005) b"
2010.amta-papers.23,2009.mtsummit-papers.12,0,0.0121654,"l., 2007), and target-context modelling such as (Berger et al., 1996; Hasan et al., 2008). The present study relates to the first category, which further divides into the following approaches: Discriminative word alignment: Garc´ıa-Varea et al. (2001) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Subsequently, more recent discriminative approaches employ source-side contexts for creating finer-grained word-to-word lexicons (Brunning et al., 2009; Mauser et al., 2009; Patry and Langlais, 2009). Phrase-based SMT: Vickrey et al. (2005) build WSD-inspired classifiers to fill in blanks in partially completed translations. Stroppa et al. (2007) were the first to add source-side contextual features into a state-of-the-art log-linear PBSMT system by incorporating context-dependent phrasal translation probabilities learned using a decision-tree classifier (Daelemans and van den Bosch, 2005). Significant improvements over a baseline PBSMT system were obtained on Italian-to-English and Chinese-toEnglish IWSLT tasks. Discriminative learning approaches in SMT such as (Cowan et al., 2006) gener"
2010.amta-papers.23,D09-1022,0,0.0362701,"such as (Stroppa et al., 2007), and target-context modelling such as (Berger et al., 1996; Hasan et al., 2008). The present study relates to the first category, which further divides into the following approaches: Discriminative word alignment: Garc´ıa-Varea et al. (2001) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Subsequently, more recent discriminative approaches employ source-side contexts for creating finer-grained word-to-word lexicons (Brunning et al., 2009; Mauser et al., 2009; Patry and Langlais, 2009). Phrase-based SMT: Vickrey et al. (2005) build WSD-inspired classifiers to fill in blanks in partially completed translations. Stroppa et al. (2007) were the first to add source-side contextual features into a state-of-the-art log-linear PBSMT system by incorporating context-dependent phrasal translation probabilities learned using a decision-tree classifier (Daelemans and van den Bosch, 2005). Significant improvements over a baseline PBSMT system were obtained on Italian-to-English and Chinese-toEnglish IWSLT tasks. Discriminative learning approaches in SMT such as"
2010.amta-papers.23,C92-2066,0,0.242831,"α+1 , ..., wα+i ) of a given source phrase α. In our experiments, we consider a context size of 2 (i.e., i := 2). It also includes boundary words (wntstart j and wntend ) of subphrases covered by nonterminals j in the α. Like (Chiang, 2007), we restrict the number of nonterminals to two (i.e., j := 2). The resultant lexical features form a window of size 2(i+j) features. Thus, lexical contextual information (CIlex ) can be described as in (4): Figure 1: CCG and LTAG supertag sequences. In our experiments two kinds of supertags are employed: those from lexicalized tree-adjoining grammar, LTAG (Joshi and Schabes, 1992), and combinatory categorial grammar, CCG (Steedman, 2000). Both the LTAG and the CCG supertag sets were acquired from the WSJ section of the Penn-II Treebank using hand-built extraction rules. Here we use both the LTAG and CCG supertaggers. In LTAG, a lexical item is associated with an elementary tree, while in CCG the supertag constitutes a CCG lexical category with a set of word-to-word dependencies. The two alternative supertag descriptions can be viewed as closely related functional descriptors of words. Like CIpos , we define the contextual information (CIst ) defining supertags as in (6"
2010.amta-papers.23,W06-1628,0,0.0642608,"Missing"
2010.amta-papers.23,J07-2003,0,0.394007,"ssfully employed in PBSMT by taking various contextual information of the source phrase into account. These contextual features may include lexical features of words appearing in the context and bearing sense discriminatory information, position-specific neighbouring words (Gim´enez and M`arquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features (Gimpel and Smith, 2008), full sentential context (Carpuat and Wu, 2007), lexical syntactic descriptions in the form of supertags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b). A limitation that Hiero (Chiang, 2007) shares with the PBSMT model (Koehn et al., 2003) is that it does not take into account the contexts in which the source-sides of the rules appear. In other words, it can be argued that rule selection in Hiero is suboptimally modelled. So far, a small number of studies have made use of source-language context for improving rule selection in Hiero. Position-specific neighbouring words and their part-of-speech (POS) prove to be effective source contexts in the HPB model (He et al., 2008). In a study involving PBSMT, Haque et al. (2009b) showed that the translations of ambiguous words are also in"
2010.amta-papers.23,N09-1025,0,0.0267371,"focused on solving ambiguities for those Chinese phrases that consist of only one or two terminal symbols. More recently, Shen et al. (2009) proposed a method to include linguistic and contextual information in the HPB system. The features employed in the system are non-terminal labels, non-terminal length distribution, source context and a language model created from source-side grammatical dependency structures. While their source-side dependency language model does not produce any improvement, the other features seem to be effective in Arabic-to-English and Chinese-to-English translation. Chiang et al. (2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system. In order to limit the the size of their model, they restrict words to being among the 100 most frequently occurring words from the training data; all other words are replaced with a special token. One final paper in this strand of research is that of (He et al., 2008), who despite not mentioning the link between the two pieces of work, show that the low-level source-language features used by (Stroppa et al., 2007) are also of benefi"
2010.amta-papers.23,H05-1097,0,0.0642552,"as (Berger et al., 1996; Hasan et al., 2008). The present study relates to the first category, which further divides into the following approaches: Discriminative word alignment: Garc´ıa-Varea et al. (2001) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Subsequently, more recent discriminative approaches employ source-side contexts for creating finer-grained word-to-word lexicons (Brunning et al., 2009; Mauser et al., 2009; Patry and Langlais, 2009). Phrase-based SMT: Vickrey et al. (2005) build WSD-inspired classifiers to fill in blanks in partially completed translations. Stroppa et al. (2007) were the first to add source-side contextual features into a state-of-the-art log-linear PBSMT system by incorporating context-dependent phrasal translation probabilities learned using a decision-tree classifier (Daelemans and van den Bosch, 2005). Significant improvements over a baseline PBSMT system were obtained on Italian-to-English and Chinese-toEnglish IWSLT tasks. Discriminative learning approaches in SMT such as (Cowan et al., 2006) generally require a redefinition of the traini"
2010.amta-papers.23,P02-1038,0,0.80659,"based SMT model (Chiang, 2007) uses the bilingual phrase pairs of phrase-based SMT (PBSMT) (Koehn et al., 2003) as a starting point to learn hierarchial rules using probabilistic synchronous context-free grammar (PSCFG). The decoding process in the hierarchical phrase-based SMT (HPB) model is based on bottom-up chart parsing (Chiang, 2007). This chart parsing decoder, also known as Hiero, does not require explicit syntactic representation on either side of the phrases in rules. State-of-the-art SMT models (Koehn et al., 2003; Chiang, 2007) can be viewed as log-linear combinations of features (Och and Ney, 2002) that usually comprise translational features and the language model. The translational features typically involved in these models express dependencies between the source and target phrases, but not dependencies between the phrases in the source language themselves, i.e. they do not take into account the contexts of those phrases. Word sense disambiguation (WSD), a task intricately related to MT, typically employs rich contextsensitive features to determine contextually the most likely sense of a polysemous word. Inspired by these context-rich WSD techniques, researchers have tried to integra"
2010.amta-papers.23,W06-1607,0,0.0192106,"e, when supertags are combined with lexical features, the CI is S formed by the union of these features, i.e., CI= CIst CIlex . 5 Memory-Based Disambiguation As Stroppa et al. (2007) point out, directly estimating context-dependent phrase translation probabilities using relative frequencies is problematic. Indeed, (Zens and Ney, 2004) showed that the estimation of phrase translation probabilities using relative frequencies results in overestimation of the probabilities of long phrases. Accordingly, smoothing factors in the form of lexical-based features are often used to counteract this bias (Foster et al., 2006). In the case of context-informed features, since the context is also taken into account, this estimation problem can only become worse. As an alternative, in this work we make use of memory-based machine learning classifiers able to estimate P(γ|α, CI(α)) by similarity-based reasoning over memorized nearest-neighbour examples of source–target phrase translations, matched to a new source phrase to be translated. In this work we use the approximate memory-based classifier IGTree1 (Daelemans and van den Bosch, 2005). IGTree makes a heuristic approximation of knearest neighbour search by storing"
2010.amta-papers.23,P01-1027,0,0.0761219,"Missing"
2010.amta-papers.23,N09-1013,0,0.0447881,"Missing"
2010.amta-papers.23,W07-0719,0,0.0564061,"Missing"
2010.amta-papers.23,2008.eamt-1.17,0,0.0184333,"nse of a polysemous word. Inspired by these context-rich WSD techniques, researchers have tried to integrate various contextual knowledge sources into state-of-the-art SMT models. In recent years, source context modelling has been successfully employed in PBSMT by taking various contextual information of the source phrase into account. These contextual features may include lexical features of words appearing in the context and bearing sense discriminatory information, position-specific neighbouring words (Gim´enez and M`arquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features (Gimpel and Smith, 2008), full sentential context (Carpuat and Wu, 2007), lexical syntactic descriptions in the form of supertags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b). A limitation that Hiero (Chiang, 2007) shares with the PBSMT model (Koehn et al., 2003) is that it does not take into account the contexts in which the source-sides of the rules appear. In other words, it can be argued that rule selection in Hiero is suboptimally modelled. So far, a small number of studies have made use of source-language context for improving rule selection in Hiero. Position-specific neighb"
2010.amta-papers.23,D09-1023,0,0.0210909,"009a) and grammatical dependency relations (Haque et al., 2009b) have been modelled as useful source context to improve phrase selection in PBSMT. Alternative SMT architectures: Bangalore et al. (2007) propose an SMT architecture based on stochastic finite state transducers, that addresses global lexical selection in which parameters are discriminatively trained using a MaxEnt model considering n-gram features from the source sentence. Specia et al. (2008) integrate WSD predictions for the reranking of n-best translations, limited to a small set of words from different grammatical categories. Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasisynchronous grammar that can incorporate arbitrary features from both source and target sentences. Hierarchial phrase-based SMT: Chan et al. (2007) were the first to use a WSD system to integrate additional features in the state-of-the-art HPB system (Chiang, 2007), achieving statistically significant performance improvements for several automatic measures for Chinese-to-English translation. However, they only focused on solving ambiguities for those Chinese phrases that consist of only one or two terminal symbols. More recently, Sh"
2010.amta-papers.23,D09-1008,0,0.0200964,"9) present an MT framework based on lattice parsing with a quasisynchronous grammar that can incorporate arbitrary features from both source and target sentences. Hierarchial phrase-based SMT: Chan et al. (2007) were the first to use a WSD system to integrate additional features in the state-of-the-art HPB system (Chiang, 2007), achieving statistically significant performance improvements for several automatic measures for Chinese-to-English translation. However, they only focused on solving ambiguities for those Chinese phrases that consist of only one or two terminal symbols. More recently, Shen et al. (2009) proposed a method to include linguistic and contextual information in the HPB system. The features employed in the system are non-terminal labels, non-terminal length distribution, source context and a language model created from source-side grammatical dependency structures. While their source-side dependency language model does not produce any improvement, the other features seem to be effective in Arabic-to-English and Chinese-to-English translation. Chiang et al. (2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated i"
2010.amta-papers.23,D07-1007,0,0.1589,"t-rich WSD techniques, researchers have tried to integrate various contextual knowledge sources into state-of-the-art SMT models. In recent years, source context modelling has been successfully employed in PBSMT by taking various contextual information of the source phrase into account. These contextual features may include lexical features of words appearing in the context and bearing sense discriminatory information, position-specific neighbouring words (Gim´enez and M`arquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features (Gimpel and Smith, 2008), full sentential context (Carpuat and Wu, 2007), lexical syntactic descriptions in the form of supertags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b). A limitation that Hiero (Chiang, 2007) shares with the PBSMT model (Koehn et al., 2003) is that it does not take into account the contexts in which the source-sides of the rules appear. In other words, it can be argued that rule selection in Hiero is suboptimally modelled. So far, a small number of studies have made use of source-language context for improving rule selection in Hiero. Position-specific neighbouring words and their part-of-speech (POS) prov"
2010.amta-papers.23,2007.tmi-papers.28,1,0.703467,"Missing"
2010.amta-papers.23,W04-3250,0,0.0940844,"Missing"
2010.amta-papers.23,N03-1017,0,0.00659903,"contextual information of the source phrase into account. These contextual features may include lexical features of words appearing in the context and bearing sense discriminatory information, position-specific neighbouring words (Gim´enez and M`arquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features (Gimpel and Smith, 2008), full sentential context (Carpuat and Wu, 2007), lexical syntactic descriptions in the form of supertags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b). A limitation that Hiero (Chiang, 2007) shares with the PBSMT model (Koehn et al., 2003) is that it does not take into account the contexts in which the source-sides of the rules appear. In other words, it can be argued that rule selection in Hiero is suboptimally modelled. So far, a small number of studies have made use of source-language context for improving rule selection in Hiero. Position-specific neighbouring words and their part-of-speech (POS) prove to be effective source contexts in the HPB model (He et al., 2008). In a study involving PBSMT, Haque et al. (2009b) showed that the translations of ambiguous words are also influenced by more distant words in the sentence. S"
2010.amta-papers.23,Y09-1019,1,0.821505,"Missing"
2010.amta-papers.23,2009.eamt-1.32,1,0.68043,"owledge sources into state-of-the-art SMT models. In recent years, source context modelling has been successfully employed in PBSMT by taking various contextual information of the source phrase into account. These contextual features may include lexical features of words appearing in the context and bearing sense discriminatory information, position-specific neighbouring words (Gim´enez and M`arquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features (Gimpel and Smith, 2008), full sentential context (Carpuat and Wu, 2007), lexical syntactic descriptions in the form of supertags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b). A limitation that Hiero (Chiang, 2007) shares with the PBSMT model (Koehn et al., 2003) is that it does not take into account the contexts in which the source-sides of the rules appear. In other words, it can be argued that rule selection in Hiero is suboptimally modelled. So far, a small number of studies have made use of source-language context for improving rule selection in Hiero. Position-specific neighbouring words and their part-of-speech (POS) prove to be effective source contexts in the HPB model (He et al., 2008). In a st"
2010.amta-papers.23,N04-1033,0,0.0277164,"a window of size 2(i + j). We compare the effect of supertag features in contrastive experiments using words and POS tags as context in order to observe the relative effects of different features. In addition, we combine the syntactic features with the lexical features. For instance, when supertags are combined with lexical features, the CI is S formed by the union of these features, i.e., CI= CIst CIlex . 5 Memory-Based Disambiguation As Stroppa et al. (2007) point out, directly estimating context-dependent phrase translation probabilities using relative frequencies is problematic. Indeed, (Zens and Ney, 2004) showed that the estimation of phrase translation probabilities using relative frequencies results in overestimation of the probabilities of long phrases. Accordingly, smoothing factors in the form of lexical-based features are often used to counteract this bias (Foster et al., 2006). In the case of context-informed features, since the context is also taken into account, this estimation problem can only become worse. As an alternative, in this work we make use of memory-based machine learning classifiers able to estimate P(γ|α, CI(α)) by similarity-based reasoning over memorized nearest-neighb"
2010.amta-papers.23,D08-1039,0,0.0238295,"Missing"
2010.amta-papers.23,P07-1020,0,0.0166708,"training procedure; in contrast, Stroppa et al. (2007) introduce new features while retaining the strength of existing stateof-the-art systems. Other recent approaches to integrate state-of-the-art WSD methods into PBSMT (Gim´enez and M`arquez, 2007; Carpuat and Wu, 2007) have met with success as well. Following the work of (Stroppa et al., 2007), rich and complex syntactic structures such as supertags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b) have been modelled as useful source context to improve phrase selection in PBSMT. Alternative SMT architectures: Bangalore et al. (2007) propose an SMT architecture based on stochastic finite state transducers, that addresses global lexical selection in which parameters are discriminatively trained using a MaxEnt model considering n-gram features from the source sentence. Specia et al. (2008) integrate WSD predictions for the reranking of n-best translations, limited to a small set of words from different grammatical categories. Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasisynchronous grammar that can incorporate arbitrary features from both source and target sentences. Hierarchial phras"
2010.amta-papers.23,P07-1005,0,0.0391009,"MT architecture based on stochastic finite state transducers, that addresses global lexical selection in which parameters are discriminatively trained using a MaxEnt model considering n-gram features from the source sentence. Specia et al. (2008) integrate WSD predictions for the reranking of n-best translations, limited to a small set of words from different grammatical categories. Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasisynchronous grammar that can incorporate arbitrary features from both source and target sentences. Hierarchial phrase-based SMT: Chan et al. (2007) were the first to use a WSD system to integrate additional features in the state-of-the-art HPB system (Chiang, 2007), achieving statistically significant performance improvements for several automatic measures for Chinese-to-English translation. However, they only focused on solving ambiguities for those Chinese phrases that consist of only one or two terminal symbols. More recently, Shen et al. (2009) proposed a method to include linguistic and contextual information in the HPB system. The features employed in the system are non-terminal labels, non-terminal length distribution, source cont"
2010.amta-papers.23,C08-1041,0,0.211341,"rtags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b). A limitation that Hiero (Chiang, 2007) shares with the PBSMT model (Koehn et al., 2003) is that it does not take into account the contexts in which the source-sides of the rules appear. In other words, it can be argued that rule selection in Hiero is suboptimally modelled. So far, a small number of studies have made use of source-language context for improving rule selection in Hiero. Position-specific neighbouring words and their part-of-speech (POS) prove to be effective source contexts in the HPB model (He et al., 2008). In a study involving PBSMT, Haque et al. (2009b) showed that the translations of ambiguous words are also influenced by more distant words in the sentence. Syntactic contexts that capture long-distance dependencies between words in a sentence can be a useful means to disambiguate among translations. Accordingly, integration of such syntactic contexts could lead to improved translation quality in PBSMT. For instance, Haque et al. (2009a) showed that supertags are more powerful source contexts than neighbouring words and part-of-speech tags to disambiguate a source phrase in PBSMT. Inspired by"
2016.lilt-14.7,P09-2041,0,0.0356438,"actor, because it implicitly changes the polarity of a message. The detection of sarcasm is therefore important, if not crucial, for the development and refinement of sentiment analysis systems, but is at the same time a serious conceptual and technical challenge. Most current approaches, which are mostly statistical and datadriven in nature, test their algorithms on publicly available social media data such as Twitter or product reviews (Carvalho et al., 2009, González-Ibánez et al., 2011, Reyes et al., 2013, Vanin et al., 2013, Davidov et al., 2010, Tsur et al., 2010, Kunneman et al., 2015, Burfoot and Baldwin, 2009) and make use of categorical labels such as hashtags to collect their corpus (for example, Reyes et al., 2013 collected tweets 6 / LiLT volume 14, issue 7 August 2016 with the hashtag ‘#irony’ and González-Ibánez et al., 2011 collected tweets with ‘#sarcasm’ and ‘#sarcastic’). Reyes and Rosso (2012) identify humorous and ironic patterns in social media by automatically evaluating features that concern ambiguity, polarity, unexpectedness and emotional scenarios. They show that ironic (and humorous) texts deviate from other messages (political, technical or general tweets). Reyes et al. (2013) p"
2016.lilt-14.7,W10-2914,0,0.0454854,"tell the polarity of a sentiment. Sarcasm can be a disruptive factor, because it implicitly changes the polarity of a message. The detection of sarcasm is therefore important, if not crucial, for the development and refinement of sentiment analysis systems, but is at the same time a serious conceptual and technical challenge. Most current approaches, which are mostly statistical and datadriven in nature, test their algorithms on publicly available social media data such as Twitter or product reviews (Carvalho et al., 2009, González-Ibánez et al., 2011, Reyes et al., 2013, Vanin et al., 2013, Davidov et al., 2010, Tsur et al., 2010, Kunneman et al., 2015, Burfoot and Baldwin, 2009) and make use of categorical labels such as hashtags to collect their corpus (for example, Reyes et al., 2013 collected tweets 6 / LiLT volume 14, issue 7 August 2016 with the hashtag ‘#irony’ and González-Ibánez et al., 2011 collected tweets with ‘#sarcasm’ and ‘#sarcastic’). Reyes and Rosso (2012) identify humorous and ironic patterns in social media by automatically evaluating features that concern ambiguity, polarity, unexpectedness and emotional scenarios. They show that ironic (and humorous) texts deviate from other me"
2016.lilt-14.7,filatova-2012-irony,0,0.0432035,"Missing"
2016.lilt-14.7,P11-2102,0,0.0423623,"e field of sentiment analysis and opinion mining aims to automatically tell the polarity of a sentiment. Sarcasm can be a disruptive factor, because it implicitly changes the polarity of a message. The detection of sarcasm is therefore important, if not crucial, for the development and refinement of sentiment analysis systems, but is at the same time a serious conceptual and technical challenge. Most current approaches, which are mostly statistical and datadriven in nature, test their algorithms on publicly available social media data such as Twitter or product reviews (Carvalho et al., 2009, González-Ibánez et al., 2011, Reyes et al., 2013, Vanin et al., 2013, Davidov et al., 2010, Tsur et al., 2010, Kunneman et al., 2015, Burfoot and Baldwin, 2009) and make use of categorical labels such as hashtags to collect their corpus (for example, Reyes et al., 2013 collected tweets 6 / LiLT volume 14, issue 7 August 2016 with the hashtag ‘#irony’ and González-Ibánez et al., 2011 collected tweets with ‘#sarcasm’ and ‘#sarcastic’). Reyes and Rosso (2012) identify humorous and ironic patterns in social media by automatically evaluating features that concern ambiguity, polarity, unexpectedness and emotional scenarios. Th"
2016.lilt-14.7,P15-2124,0,0.0164371,"hm, experience a lot of difficulties in assessing the ironic intent on the basis of isolated fragments. They achieve higher results when the fragments are presented in context. Recent work on automatic sarcasm detection feed a classifier with more complex features of sarcasm. Riloff et al. (2013) observe that sarcasm is often characterized by a positive sentiment in relation to a negative state or situation. They collect a bootstrapped lexicon of negative situations and positive phrases. Training a machine learning classifier on the co-occurrence of these two yields the best result. Likewise, Joshi et al. (2015) make use of the positive and negative weights of words in a sentiment lexicon to recognize implicit and explicit incongruities in tweets and messages on online fora. Rajadesingan et al. (2015) and Bamman and Smith (2015) extend the scope to the context outside of a textual unit, and model characteristics of the sender (Rajadesingan et al., 2015, Bamman and Smith, 2015), the addressee and the conversation (Bamman and Smith, 2015) for sarcastic tweets that contain a user mention (’@user’). Several characteristics of the past tweets and user profile of the sender and addressee are included as fe"
2016.lilt-14.7,D13-1066,0,0.0356732,"Missing"
2016.lilt-14.7,walker-etal-2012-corpus,0,0.0653238,"Missing"
2016.lilt-14.7,P14-2084,0,0.0697787,"findings underline what has also been stated by Wallace (2013) and Reyes et al. (2013) who concur that unless an irony predicting algorithm accounts for an explicit model of the communicator and the communicative situation, automatic irony detection will remain a challenge. It follows from our results that in the case of communicative situations where there is a relatively high degree of mutual knowledge, for instance for Instant Messaging services such as Whatsapp and Facebook Messenger, statistical methods for sarcasm detection solely based on explicit cues are very likely to lack accuracy (Wallace et al., 2014). Future research should therefore explore new ways of operationalizing context. For instance, the number of followers a Twitter user has may be correlated to the number of irony markers she uses, simply because it is impossible to share mutual knowledge with all addressees. Acknowledgements This research was funded by the Dutch national program COMMIT. We wish to thank Mathilde Blom for assisting with the coding of the predicting elements, Erik Tjong Kim Sang for the development and support of the http://twiqs.nl service, and BuzzFeed for the YouTube skit entitled &quot;Sarcastic Soulmates&quot;, https"
2016.lilt-14.7,P01-1033,0,\N,Missing
2020.cogalex-1.4,I05-3017,0,0.11792,". 3 https://github.com/fxsjy/jieba 40 pervised LiB (LiB(sup)) for the word segmentation task. LiB(sup) skips the training phase. Instead, it counts all the ground-truth words in the training set and adds them as the chunk types to L. The higher the frequency of a type in the training set, the smaller its ordinal in L. We trained and tested the models on CTB8. To test the generalization performance of the models in the word segmentation task, we also test the training result on two additional corpora: MSR and PKU (Table 1) provided by the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005). The segmentation rules are slightly different among MSR, PKU, and CTB8. MSR and PKU are news domain, which is different from CTB8. MSR and PKU were preprocessed in the same way as CTB8. Table 6b shows that the scores of the unsupervised original version of LiB are lower than the supervised models4 , but the scores of the supervised version of LiB are close to the supervised models and are even higher on MSR. Due to the low out-of-vocabulary (OOV) rate of MSR (Emerson, 2005), the good performance on MSR shows that the lexicon is important for LiB. The only difference between the two versions"
2020.cogalex-1.4,N09-1036,0,0.280375,"for NLP preprocessing. It iteratively searches for the most common n-gram pairs and adds them into the ngram lexicon. Some other models such as the Chunk-Based Learner (McCauley and Christiansen, 2019) and PARSER (Perruchet and Vinter, 1998) are also based on the local statistics of tokens (e.g., token frequency, mutual information, or transitional probability). • Model the grammar: Some studies attempted to analyze the grammar patterns of sentences and then parse/segment the sentences based on these patterns. To find the optimal grammar, de Marcken (1996) used Minimum Description Length, and Johnson and Goldwater (2009) used the Hierarchical Dirichlet Process. • Model the sequences: Recurrent neural networks and its variations are able to learn the sequential patterns in language and to perform text segmentation (Chung et al., 2017; Kawakami et al., 2019; Sun and Deng, 2018; Zhikov et al., 2013). In general, lexicon models capture only the local statistics of the tokens so they tend to be short-sighted at the global level (e.g. long-distance dependencies). The other two types of models, in contrast, learn how the tokens co-occur globally. Yet, the ways grammar models and sequence models learn the global info"
2020.cogalex-1.4,P19-1645,0,0.0115619,"also based on the local statistics of tokens (e.g., token frequency, mutual information, or transitional probability). • Model the grammar: Some studies attempted to analyze the grammar patterns of sentences and then parse/segment the sentences based on these patterns. To find the optimal grammar, de Marcken (1996) used Minimum Description Length, and Johnson and Goldwater (2009) used the Hierarchical Dirichlet Process. • Model the sequences: Recurrent neural networks and its variations are able to learn the sequential patterns in language and to perform text segmentation (Chung et al., 2017; Kawakami et al., 2019; Sun and Deng, 2018; Zhikov et al., 2013). In general, lexicon models capture only the local statistics of the tokens so they tend to be short-sighted at the global level (e.g. long-distance dependencies). The other two types of models, in contrast, learn how the tokens co-occur globally. Yet, the ways grammar models and sequence models learn the global information makes them more complicated and computing-intensive than the lexicon models. In this paper we propose a model that builds a lexicon, but does so by using both local and global information. Our model is not only a computational mode"
2020.cogalex-1.4,D18-2012,0,0.0123016,"s are transcribed into English words for ease of presentation. Here, #s denotes the number of unique symbols s in L (either as a single-symbol chunk or as part of a larger chunk); Freq(si ) and P (si ) are the occurrence count and ratio of si in L; #u denotes the number of unique units u in the corpus; Freq(uj ) and P (uj ) are the occurrence count and ratio of uj in the corpus. As benchmarks, we used Symbol (the indivisible units; in our two corpora, phonemes and characters respectively), Word (the words presegmented in the corpora), and BPE subword (the Byte Pair generated by SentencePiece (Kudo and Richardson, 2018) with default parameters setting). The DL result (Table 4) shows that LiB chunks result in shortest DL; they minimze the information; they are the most concise encodings. 4.4 Language model evaluation Besides the DL, which compares the information efficiencies of different lexicons, we are also interested in whether the LiB lexicon can reflect the mental lexicon. We lack a ground truth of what is in the putative mental lexicon. However, we can regard natural language material as a large-scale result of human language use and language behavior. Trained on a very large corpus, a recent study by"
2020.cogalex-1.4,E12-1041,1,0.732149,"odels (LMs) can closely predict human performance on various language tasks. LMs capture the probabilistic constraints in natural language and perform the tasks by making predictions, which is a fundamental cognitive function (Bar, 2007). So, by measuring the prediction surprisal in the corpus segmented by different lexicons, we can evaluate different lexicons from a cognitive view, and we presume that the lexicon that gets the best LM performance is a better approximation of the mental lexicon. Many studies have shown that word surprisal is positively correlated with human word-reading time (Monsalve et al., 2012; Smith and Levy, 2013) and size of the N400 component in EEG (Frank et al., 2015). From the cognitive principle of least effort, it follows that readers try to minimize reading time. 39 Corpus BRphono CTB8 Evaluation metric Average length Lexicon size DL(lexicon) DL(corpus) DL(total) Average length Lexicon size DL(lexicon) DL(corpus) DL(total) Symbol 1 50 <1 490 490 1 4,697 57 21,864 21,921 BPE subword 2.8 5,574 173 278 451 1.4 7,980 133 18,229 18,362 Segmentation Word LiB subchunk 2.9 2.9 1,321 1,119 28 24 262 258 289 282 1.7 1.7 65,410 24,763 1,767 621 15,669 16,188 17,436 16,809 LiB chunk"
2020.cogalex-1.4,P16-1162,0,0.0189709,"en phrased as the task of unsupervised segmentation. Several types of computational models or NLP algorithms have been proposed for segmentation, taking different approaches: This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 33 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 33–45 Barcelona, Spain (Online), December 12, 2020 • Model the lexicon: A straightforward basis for segmentation is to build a lexicon. One of the lexicon-building algorithms, Byte pair encoding (BPE) (Sennrich et al., 2016), is popular for NLP preprocessing. It iteratively searches for the most common n-gram pairs and adds them into the ngram lexicon. Some other models such as the Chunk-Based Learner (McCauley and Christiansen, 2019) and PARSER (Perruchet and Vinter, 1998) are also based on the local statistics of tokens (e.g., token frequency, mutual information, or transitional probability). • Model the grammar: Some studies attempted to analyze the grammar patterns of sentences and then parse/segment the sentences based on these patterns. To find the optimal grammar, de Marcken (1996) used Minimum Description"
2020.cogalex-1.4,D18-1531,0,0.0216019,"statistics of tokens (e.g., token frequency, mutual information, or transitional probability). • Model the grammar: Some studies attempted to analyze the grammar patterns of sentences and then parse/segment the sentences based on these patterns. To find the optimal grammar, de Marcken (1996) used Minimum Description Length, and Johnson and Goldwater (2009) used the Hierarchical Dirichlet Process. • Model the sequences: Recurrent neural networks and its variations are able to learn the sequential patterns in language and to perform text segmentation (Chung et al., 2017; Kawakami et al., 2019; Sun and Deng, 2018; Zhikov et al., 2013). In general, lexicon models capture only the local statistics of the tokens so they tend to be short-sighted at the global level (e.g. long-distance dependencies). The other two types of models, in contrast, learn how the tokens co-occur globally. Yet, the ways grammar models and sequence models learn the global information makes them more complicated and computing-intensive than the lexicon models. In this paper we propose a model that builds a lexicon, but does so by using both local and global information. Our model is not only a computational model but also a cogniti"
2020.lrec-1.756,W11-3702,0,0.0620564,"ted to our study, in Section 3 we present the data and the prediction method used in this study. Section 4 describes the annotation process. In Section 5 we explain the filters we used and show some results. In Section 6 we draw conclusions, and we wrap up with a discussion. Related Work A strand of recent work reports on the application of sentiment analysis in the prediction of election results based on tweets. Results are mixed. Berhingam et al. use the recent Irish General Election as a case study for investigating the potential to model political sentiment through mining of social media (Bermingham and Smeaton, 2011). Their approach combines sentiment analysis using supervised learning and volume-based measures. They conclude that ”Twitter does appear to display a predictive quality which is marginally augmented by the inclusion of sentiment analysis”. Burnap et. al. present their ’baseline’ model of prediction that incorporates sentiment analysis and prior party support to generate a true forecast of parliament seat allocation of the 2015 UK General Election (Burnap et al., 2016). The effect of the sentiment analysis is not clear from their analysis. Almeida et. al. also use sentiment analysis among othe"
buchholz-van-den-bosch-2000-integrating,C96-1047,0,\N,Missing
buchholz-van-den-bosch-2000-integrating,W99-0613,0,\N,Missing
buchholz-van-den-bosch-2000-integrating,W99-0612,0,\N,Missing
C18-1188,E09-1046,0,0.021524,"e extracted from each review text by means of Frog5 , an off-the-shelf NLP system for Dutch (Van den Bosch et al., 2007). We matched the automatic analyses with a set of syntactic patterns of which the lexical realizations (the words associated with the PoS-tags) may reflect statements on the valence of an aspect. Examples of patterns and example lexical realizations are given in Table 1. To assess the presence and direction of the valence in the lexical realizations (henceforth referred to as phrases) found with matching syntactic patterns, we matched them to the Duoman subjectivity lexicon (Jijkoun and Hofmann, 2009). This a lexicon of nouns and adjectives rated by human annotators as ‘Very negative’, ‘Negative’, ‘Neutral’, ‘Positive’ and ‘Very positive’. We labeled each phrase with a positive or very positive word as ‘pro’, and each phrase with a negative or very negative word as ‘con’. Phrases in which no word has a positive or negative valence in Duoman lexicon were discarded, as well as phrases that matched an equal amount of positive and negative words. A phrase that carries a combination of a ‘positive’ and ‘very negative’ is labeled as ‘con’, while phrases with a ‘very positive’ and ‘negative’ word"
C18-1188,P06-2063,0,0.0613142,"Missing"
C18-1188,W14-5905,0,0.0335956,"standard is compared to system output through human evaluation. After discussing related work, this paper will proceed with a description of the methods. Subsequently, the experimental set-up will be outlined, followed by the results. The paper ends with a conclusion and discussion. 2 Related Work Studies in aspect-based sentiment analysis can roughly be divided into rule-based and machine learningbased approaches. Rule-based approaches implement searching for combinations of aspects and evaluations on the sentence level. Such combinations are commonly identified based on syntactic patterns (Poria et al., 2014; Zhang et al., 2014; Chinsha and Joseph, 2015; Rana and Cheah, 2015; Yan et al., 2015) while the evaluation is scored based on a sentiment lexicon (Chinsha and Joseph, 2015; Rana and Cheah, 2015; Poria et al., 2014), a PageRank-based procedure (Yan et al., 2015), or optimization based on review labels (Zhang et al., 2014). We apply a knowledge-driven approach to aspect-based sentiment analysis, comparable to the approach by Rana and Cheah (2015). Candidate phrases are identified based on predefined syntactic patterns and a sentiment lexicon is used to identify patterns that likely signify a p"
C18-1188,D13-1170,0,0.0102213,"Missing"
C18-1188,L16-1652,0,0.0194187,"e, the labels are the human generated aspect sentences. We treat each aspect as a potential class to predict. The chosen architecture is similar to Joulin et al. (2016) which is a modification of the cbow model by Mikolov et al. (2013), but instead of predicting a hidden word, a hidden class is predicted. We use softmax to compute the probability distribution over the aspect classes. The model is trained using stochastic gradient descent and a linearly decaying learning rate. To pretrain the model we use 320-dimensional word embeddings derived from the corpora from the web (COW) introduced in Tulkens et al. (2016). 3.3 System 3: Shallow neural classification with clustering Because users can state positive and negative aspects in any way they want, we end up with a large number of classes in a long-tail distribution. From inspection of the development data we observe that many of the different classes are in fact paraphrases of the same semantic content (’great coffee’, ’coffee tastes great’, etc.) To try to alleviate this problem, we add a preprocessing step where we cluster the aspect summaries based on averaging over word embedding vectors. Using k-means clustering we then cluster the summaries into"
E03-1051,P90-1003,0,0.0613331,"ct prosodic phrasing may impede the listener Antal van den Bosch and ErwinNIarsi ILK/Comp. Ling. and AT Tilburg University P.O. Box90153,NL-5000LETilburg The Netherlands {A.vdnBosch,E.Marsi}@uvt.nl in the correct understanding of the spoken utterance (Sanderman and Collier, 1997). A major factor causing difficulties in appropriate phrase boundary placement is the lack of reliable information about syntactic structure. Even if there is no one-to-one mapping between syntax and prosody, the placement of prosodic phrase boundaries is nevertheless dependent on syntactic information (Selkirk, 1984; Bear and Price, 1990; van Herwijnen and Terken, 2001b). To cope with this lack of syntactic information that a speech synthesis developer may face currently, e.g. in the absence of a reliable parser, several strategies have been applied to allocate phrase boundaries. One strategy is to allocate phrase boundaries on the basis of punctuation only. In general, however, this results in too few phrase boundaries (and some incorrect ones, e.g. in enumerations). A clear example of information about syntactic structure being useful for placing phrase boundaries is the attachment of prepositional phrases (PPs). When a PP"
E03-1051,C94-2195,0,0.0422969,"et al., 1997). Deciding about noun versus verb attachment of PPs is a known hard task in parsing, since it is un139 derstood to involve knowing lexical preferences, verb subcategorization, fixed phrases, but also semantic and pragmatic &quot;world&quot; knowledge. A typical current parser (e.g., statistical parsers such as (Collins, 1996; Ratnaparkhi, 1997; Charniak, 2000)) interleaves PP attachment with all its other disambiguation tasks. However, because of its interesting complexity, a line of work has concentrated on studying the task in isolation (Hindle and Rooth, 1993; Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Franz, 1996; Zavrel et al., 1997). Our study can be seen as following these lines of isolation studies, pursuing the same process for another language, Dutch. At present there are no parsers available for Dutch that disambiguate PP attachment, which leaves the comparison between PP attachment as an embedded subtask of a full parser with our approach as future work. In line with these earlier studies, we assume that at least two sources of information should be used as features in training data: (i) lexical features (e.g. unigrams and bigrams of head words), and (ii)"
E03-1051,A00-2018,0,0.006562,"e verb in the clause (verb attachment), as in the structure ... eats pizza with a fork, an intervening phrase boundary between the PP and its preceding NP or PP (between pizza and with) is optional, and when placed, usually judged appropriate (Marsi et al., 1997). Deciding about noun versus verb attachment of PPs is a known hard task in parsing, since it is un139 derstood to involve knowing lexical preferences, verb subcategorization, fixed phrases, but also semantic and pragmatic &quot;world&quot; knowledge. A typical current parser (e.g., statistical parsers such as (Collins, 1996; Ratnaparkhi, 1997; Charniak, 2000)) interleaves PP attachment with all its other disambiguation tasks. However, because of its interesting complexity, a line of work has concentrated on studying the task in isolation (Hindle and Rooth, 1993; Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Franz, 1996; Zavrel et al., 1997). Our study can be seen as following these lines of isolation studies, pursuing the same process for another language, Dutch. At present there are no parsers available for Dutch that disambiguate PP attachment, which leaves the comparison between PP attachment as an embedded subtask"
E03-1051,W95-0103,0,0.103032,"about noun versus verb attachment of PPs is a known hard task in parsing, since it is un139 derstood to involve knowing lexical preferences, verb subcategorization, fixed phrases, but also semantic and pragmatic &quot;world&quot; knowledge. A typical current parser (e.g., statistical parsers such as (Collins, 1996; Ratnaparkhi, 1997; Charniak, 2000)) interleaves PP attachment with all its other disambiguation tasks. However, because of its interesting complexity, a line of work has concentrated on studying the task in isolation (Hindle and Rooth, 1993; Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Franz, 1996; Zavrel et al., 1997). Our study can be seen as following these lines of isolation studies, pursuing the same process for another language, Dutch. At present there are no parsers available for Dutch that disambiguate PP attachment, which leaves the comparison between PP attachment as an embedded subtask of a full parser with our approach as future work. In line with these earlier studies, we assume that at least two sources of information should be used as features in training data: (i) lexical features (e.g. unigrams and bigrams of head words), and (ii) word cooccurrence strengt"
E03-1051,P96-1025,0,0.0141571,"wever, when a PP is attached to the verb in the clause (verb attachment), as in the structure ... eats pizza with a fork, an intervening phrase boundary between the PP and its preceding NP or PP (between pizza and with) is optional, and when placed, usually judged appropriate (Marsi et al., 1997). Deciding about noun versus verb attachment of PPs is a known hard task in parsing, since it is un139 derstood to involve knowing lexical preferences, verb subcategorization, fixed phrases, but also semantic and pragmatic &quot;world&quot; knowledge. A typical current parser (e.g., statistical parsers such as (Collins, 1996; Ratnaparkhi, 1997; Charniak, 2000)) interleaves PP attachment with all its other disambiguation tasks. However, because of its interesting complexity, a line of work has concentrated on studying the task in isolation (Hindle and Rooth, 1993; Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Franz, 1996; Zavrel et al., 1997). Our study can be seen as following these lines of isolation studies, pursuing the same process for another language, Dutch. At present there are no parsers available for Dutch that disambiguate PP attachment, which leaves the comparison between"
E03-1051,J93-1003,0,0.0350175,". There are six possible combinations of the four heads: N1-P, N1-V, .... The example construction is thus stored in the data set as the following comma-separated 10-feature instance labelled with the VERB attachment class: Dults, om, slaan, oren, Dults-om, Duits-slaan, Duits-oren, om-slaan, om-oren, slaan-oren, VERB 3.2 Cooccurrence strength values Several metrics are available that estimate to what extent words or phrases belong together informationally. Well known examples of such cooccurrence strength metrics are mutual information (Church and Hanks, 1991), chi-square and log likelihood (Dunning, 1993). Cooccurrence strength values are typically estimated from a very large corpus. Often, these corpora are static and do not contain neologisms and names from later periods. In this paper, we explore an alternative by estimating cooccurrence strength values from the WWW. The WWW can be seen as a dynamic corpus: it contains new words that are not yet incorporated in other (static) corpora. Another advantage of using the WWW as a corpus is that it is the largest freely and electronically accessible corpus (for most languages including Dutch). Consequently, frequency counts obtained from the = Not"
E03-1051,J93-1005,0,0.105817,"and when placed, usually judged appropriate (Marsi et al., 1997). Deciding about noun versus verb attachment of PPs is a known hard task in parsing, since it is un139 derstood to involve knowing lexical preferences, verb subcategorization, fixed phrases, but also semantic and pragmatic &quot;world&quot; knowledge. A typical current parser (e.g., statistical parsers such as (Collins, 1996; Ratnaparkhi, 1997; Charniak, 2000)) interleaves PP attachment with all its other disambiguation tasks. However, because of its interesting complexity, a line of work has concentrated on studying the task in isolation (Hindle and Rooth, 1993; Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Franz, 1996; Zavrel et al., 1997). Our study can be seen as following these lines of isolation studies, pursuing the same process for another language, Dutch. At present there are no parsers available for Dutch that disambiguate PP attachment, which leaves the comparison between PP attachment as an embedded subtask of a full parser with our approach as future work. In line with these earlier studies, we assume that at least two sources of information should be used as features in training data: (i) lexical features ("
E03-1051,H94-1048,0,0.75653,"judged appropriate (Marsi et al., 1997). Deciding about noun versus verb attachment of PPs is a known hard task in parsing, since it is un139 derstood to involve knowing lexical preferences, verb subcategorization, fixed phrases, but also semantic and pragmatic &quot;world&quot; knowledge. A typical current parser (e.g., statistical parsers such as (Collins, 1996; Ratnaparkhi, 1997; Charniak, 2000)) interleaves PP attachment with all its other disambiguation tasks. However, because of its interesting complexity, a line of work has concentrated on studying the task in isolation (Hindle and Rooth, 1993; Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Franz, 1996; Zavrel et al., 1997). Our study can be seen as following these lines of isolation studies, pursuing the same process for another language, Dutch. At present there are no parsers available for Dutch that disambiguate PP attachment, which leaves the comparison between PP attachment as an embedded subtask of a full parser with our approach as future work. In line with these earlier studies, we assume that at least two sources of information should be used as features in training data: (i) lexical features (e.g. unigrams and bigrams"
E03-1051,W97-0301,0,0.0130729,"P is attached to the verb in the clause (verb attachment), as in the structure ... eats pizza with a fork, an intervening phrase boundary between the PP and its preceding NP or PP (between pizza and with) is optional, and when placed, usually judged appropriate (Marsi et al., 1997). Deciding about noun versus verb attachment of PPs is a known hard task in parsing, since it is un139 derstood to involve knowing lexical preferences, verb subcategorization, fixed phrases, but also semantic and pragmatic &quot;world&quot; knowledge. A typical current parser (e.g., statistical parsers such as (Collins, 1996; Ratnaparkhi, 1997; Charniak, 2000)) interleaves PP attachment with all its other disambiguation tasks. However, because of its interesting complexity, a line of work has concentrated on studying the task in isolation (Hindle and Rooth, 1993; Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Franz, 1996; Zavrel et al., 1997). Our study can be seen as following these lines of isolation studies, pursuing the same process for another language, Dutch. At present there are no parsers available for Dutch that disambiguate PP attachment, which leaves the comparison between PP attachment as an"
E03-1051,van-der-wouden-etal-2002-syntactic,0,0.0630738,"Missing"
E03-1051,W97-1016,0,0.699246,"Ps is a known hard task in parsing, since it is un139 derstood to involve knowing lexical preferences, verb subcategorization, fixed phrases, but also semantic and pragmatic &quot;world&quot; knowledge. A typical current parser (e.g., statistical parsers such as (Collins, 1996; Ratnaparkhi, 1997; Charniak, 2000)) interleaves PP attachment with all its other disambiguation tasks. However, because of its interesting complexity, a line of work has concentrated on studying the task in isolation (Hindle and Rooth, 1993; Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Franz, 1996; Zavrel et al., 1997). Our study can be seen as following these lines of isolation studies, pursuing the same process for another language, Dutch. At present there are no parsers available for Dutch that disambiguate PP attachment, which leaves the comparison between PP attachment as an embedded subtask of a full parser with our approach as future work. In line with these earlier studies, we assume that at least two sources of information should be used as features in training data: (i) lexical features (e.g. unigrams and bigrams of head words), and (ii) word cooccurrence strength values (the probability that two"
E03-1051,J90-1003,0,\N,Missing
E12-1057,oostdijk-2000-spoken,0,0.0316412,"Missing"
E12-1057,W03-2502,0,0.439497,"Missing"
E14-1034,W97-0504,0,0.296868,"he number of keystrokes we have to make, thus saving time and preventing mistakes. With the rise of smartphones word prediction has become widely known and used. Preceding this 1 The system is available as an interactive demo at http://soothsayer.cls.ru.nl/ and its source code is publicly available at https://github.com/ woseseltops/soothsayer 318 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 318–327, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics that to limit the pool of suggestions (Carlberger et al., 1997; Fazly and Hirst, 2003; Copestake, 1997; Matiasek et al., 2002; Garay-Vitoria and Gonzalez-Abascal, 1997). Interestingly, most authors conclude that including linguistic knowledge improves the results, but only slightly (GarayVitoria and Gonzalez-Abascal, 1997; Fazly and Hirst, 2003). Fazly and Hirst (2003) note that adding explicit linguistic knowledge ’might not be considered worth the considerable extra cost that it requires’. In the current study we have not used any explicit linguistic knowledge, thus making our system language-independent. There have also been more successful optimizati"
E14-1034,W00-0507,0,0.155182,"Missing"
E14-1034,C00-1027,0,0.0234618,"ediction could be displayed, which in many cases will be the word with the second most likely suffix. We use this early prediction switching method throughout our experiments. • From the set of users mentioned in tweets already harvested and not yet tracked, the most frequently mentioned user is selected. This ensures that the new person communicates with at least one of the persons the system is already following. • From the set of users mentioned by more than a single person already being tracked, the most frequently mentioned user is selected. This ensures the new person is well Recency As Church (2000) showed, the probability that a word recurs twice in a short stretch of text is far higher than its frequency in language would suggests, which is mainly related to the 4 322 http://www.twitter.com An ANOVA for repeated measures showed that there is a significant effect of the training material F (2, 198) = 109.495, p < .001 and whether the recency buffer was used F (1, 99) = 469.648, p < .001. Contrast analyses revealed that both the differences between the results of the general model and the idiolect model F (1, 99) = 41.902, p < .001 and the idiolect model and the idiolect model with the b"
E14-1034,W97-0506,0,0.197613,"ving time and preventing mistakes. With the rise of smartphones word prediction has become widely known and used. Preceding this 1 The system is available as an interactive demo at http://soothsayer.cls.ru.nl/ and its source code is publicly available at https://github.com/ woseseltops/soothsayer 318 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 318–327, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics that to limit the pool of suggestions (Carlberger et al., 1997; Fazly and Hirst, 2003; Copestake, 1997; Matiasek et al., 2002; Garay-Vitoria and Gonzalez-Abascal, 1997). Interestingly, most authors conclude that including linguistic knowledge improves the results, but only slightly (GarayVitoria and Gonzalez-Abascal, 1997; Fazly and Hirst, 2003). Fazly and Hirst (2003) note that adding explicit linguistic knowledge ’might not be considered worth the considerable extra cost that it requires’. In the current study we have not used any explicit linguistic knowledge, thus making our system language-independent. There have also been more successful optimizations of word completion systems. One is t"
E14-1034,W03-2502,0,0.25205,"e have to make, thus saving time and preventing mistakes. With the rise of smartphones word prediction has become widely known and used. Preceding this 1 The system is available as an interactive demo at http://soothsayer.cls.ru.nl/ and its source code is publicly available at https://github.com/ woseseltops/soothsayer 318 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 318–327, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics that to limit the pool of suggestions (Carlberger et al., 1997; Fazly and Hirst, 2003; Copestake, 1997; Matiasek et al., 2002; Garay-Vitoria and Gonzalez-Abascal, 1997). Interestingly, most authors conclude that including linguistic knowledge improves the results, but only slightly (GarayVitoria and Gonzalez-Abascal, 1997; Fazly and Hirst, 2003). Fazly and Hirst (2003) note that adding explicit linguistic knowledge ’might not be considered worth the considerable extra cost that it requires’. In the current study we have not used any explicit linguistic knowledge, thus making our system language-independent. There have also been more successful optimizations of word completion"
E14-1034,E12-1057,1,0.890735,"Missing"
E93-1007,C88-1028,1,0.789302,"Missing"
E93-1007,E93-1023,0,0.0195413,"Missing"
L16-1003,W09-1904,0,0.0383851,"in the highest possible general performance system (Huck et al., 2015). As expected, however, it is outperformed by using the domain-specific test set. System EN-EL EN-PT EN-IT TraMOOC Test 28.0 28.5 27.9 29.1 32.6 33.0 Table 2: BLEU scores for the initial translation prototypes 5. Size (million words) 2.7 1.5 4.8 2.4 1.3 1.5 1.4 0.2 1.7 2.3 8.7 Crowdsourcing Crowdsourcing has been employed extensively for the implementation of human intelligence natural language processing (NLP) tasks in recent years (Callison, 2009; Zaidan & Burch, 2011; Zbib et al., 2012; Ambati, 2012; Finin et al., 2010; Hsueh et al., 2009). TraMOOC involves crowdsourcing for realizing sub-goals that require human intervention in order to meet its high-quality output standards against upcoming challenges, including the large number of targeted languages, the fragmentary or weak SMT infrastructure support for the majority of the languages, and the multiple domains and text genres involved. The CrowdFlower 14 platform was chosen for the implementation of the crowdsourcing activities because of (a) its configurability, (b) its robust infrastructure, (c) its densely populated crowd channels and the evaluation and ranking process the"
L16-1003,2015.mtsummit-papers.19,1,0.892142,"i et al., 2013), binary features indicating absolute occurrence count classes of phrase pairs, sparse phrase length features, and sparse lexical features for the top-200 words. The models were optimised to maximise BLEU (Papineni et al., 2002) with batch MIRA (Cherry & Foster, 2012) on 1000-best lists. In Table 1 we compare the BLEU score performance of the systems on the TraMOOC test sets, when tuned on a mixed domain tuning set, or with the TraMOOC tuning set. The mixed tuning set includes tuning sets from TED, Europarl, and News to result in the highest possible general performance system (Huck et al., 2015). As expected, however, it is outperformed by using the domain-specific test set. System EN-EL EN-PT EN-IT TraMOOC Test 28.0 28.5 27.9 29.1 32.6 33.0 Table 2: BLEU scores for the initial translation prototypes 5. Size (million words) 2.7 1.5 4.8 2.4 1.3 1.5 1.4 0.2 1.7 2.3 8.7 Crowdsourcing Crowdsourcing has been employed extensively for the implementation of human intelligence natural language processing (NLP) tasks in recent years (Callison, 2009; Zaidan & Burch, 2011; Zbib et al., 2012; Ambati, 2012; Finin et al., 2010; Hsueh et al., 2009). TraMOOC involves crowdsourcing for realizing sub-g"
L16-1003,2005.mtsummit-papers.11,0,0.0298517,"onsist of (a) translation experts, (b) an internal group of workers with a known background in linguistics and/or translation, and (c) a group of external Table 1: Size of parallel data for all language pairs 4. Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev TraMOOC Dev 25.5 27.9 34.1 36.5 34.1 35.9 Initial Translation Results For the initial TraMOOC prototypes we focused on three language pairs: EN-IT, EN-PT and EN-EL. Phrase-based models were trained on a large amount of parallel and monolingual data, including TED (Cettolo et al., 2012), Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), various OPUS corpora (Tiedemann, 2012), WMT News Commentary and Common Crawl 12, and SETimes 10 https://www.translectures.eu/web/project-summary/ http://videolectures.net/ 12 http://www.statmt.org/wmt15/translation-task.html 11 13 14 18 http://dumps.wikimedia.org (April 2015) www.crowdflower.com contributors from the platform's crowd channels. For the latter crowd category, apart from the standard channel evaluation processes applied by the platform to isolate spammers and contributors with poor language skills, further quality assurance measures are ta"
L16-1003,P02-1040,0,0.0956189,"EN-CR EN-PL EN-IT EN-ZH (Tyers and Alperen, 2010). These were supplemented with monolingual Wikipedia corpora13 for all three target languages. The phrase-based models include many features which make them strong baselines. These models include standard features plus a hierarchical lexicalised reordering model (Galley & Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013), binary features indicating absolute occurrence count classes of phrase pairs, sparse phrase length features, and sparse lexical features for the top-200 words. The models were optimised to maximise BLEU (Papineni et al., 2002) with batch MIRA (Cherry & Foster, 2012) on 1000-best lists. In Table 1 we compare the BLEU score performance of the systems on the TraMOOC test sets, when tuned on a mixed domain tuning set, or with the TraMOOC tuning set. The mixed tuning set includes tuning sets from TED, Europarl, and News to result in the highest possible general performance system (Huck et al., 2015). As expected, however, it is outperformed by using the domain-specific test set. System EN-EL EN-PT EN-IT TraMOOC Test 28.0 28.5 27.9 29.1 32.6 33.0 Table 2: BLEU scores for the initial translation prototypes 5. Size (millio"
L16-1003,P11-1138,0,0.0602764,"Missing"
L16-1003,steinberger-etal-2006-jrc,0,0.0272335,"experts, (b) an internal group of workers with a known background in linguistics and/or translation, and (c) a group of external Table 1: Size of parallel data for all language pairs 4. Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev TraMOOC Dev 25.5 27.9 34.1 36.5 34.1 35.9 Initial Translation Results For the initial TraMOOC prototypes we focused on three language pairs: EN-IT, EN-PT and EN-EL. Phrase-based models were trained on a large amount of parallel and monolingual data, including TED (Cettolo et al., 2012), Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), various OPUS corpora (Tiedemann, 2012), WMT News Commentary and Common Crawl 12, and SETimes 10 https://www.translectures.eu/web/project-summary/ http://videolectures.net/ 12 http://www.statmt.org/wmt15/translation-task.html 11 13 14 18 http://dumps.wikimedia.org (April 2015) www.crowdflower.com contributors from the platform's crowd channels. For the latter crowd category, apart from the standard channel evaluation processes applied by the platform to isolate spammers and contributors with poor language skills, further quality assurance measures are taken like  access control using quiz da"
L16-1003,tiedemann-2012-parallel,0,0.0663706,"known background in linguistics and/or translation, and (c) a group of external Table 1: Size of parallel data for all language pairs 4. Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev TraMOOC Dev 25.5 27.9 34.1 36.5 34.1 35.9 Initial Translation Results For the initial TraMOOC prototypes we focused on three language pairs: EN-IT, EN-PT and EN-EL. Phrase-based models were trained on a large amount of parallel and monolingual data, including TED (Cettolo et al., 2012), Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), various OPUS corpora (Tiedemann, 2012), WMT News Commentary and Common Crawl 12, and SETimes 10 https://www.translectures.eu/web/project-summary/ http://videolectures.net/ 12 http://www.statmt.org/wmt15/translation-task.html 11 13 14 18 http://dumps.wikimedia.org (April 2015) www.crowdflower.com contributors from the platform's crowd channels. For the latter crowd category, apart from the standard channel evaluation processes applied by the platform to isolate spammers and contributors with poor language skills, further quality assurance measures are taken like  access control using quiz data that are far from straightforward to"
L16-1003,abdelali-etal-2014-amara,0,0.284209,"Missing"
L16-1003,D09-1030,0,0.252764,"Missing"
L16-1003,2012.eamt-1.60,0,0.0756545,"king field. The targeted crowds consist of (a) translation experts, (b) an internal group of workers with a known background in linguistics and/or translation, and (c) a group of external Table 1: Size of parallel data for all language pairs 4. Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev TraMOOC Dev 25.5 27.9 34.1 36.5 34.1 35.9 Initial Translation Results For the initial TraMOOC prototypes we focused on three language pairs: EN-IT, EN-PT and EN-EL. Phrase-based models were trained on a large amount of parallel and monolingual data, including TED (Cettolo et al., 2012), Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), various OPUS corpora (Tiedemann, 2012), WMT News Commentary and Common Crawl 12, and SETimes 10 https://www.translectures.eu/web/project-summary/ http://videolectures.net/ 12 http://www.statmt.org/wmt15/translation-task.html 11 13 14 18 http://dumps.wikimedia.org (April 2015) www.crowdflower.com contributors from the platform's crowd channels. For the latter crowd category, apart from the standard channel evaluation processes applied by the platform to isolate spammers and contributors with poor language skills, further quality a"
L16-1003,N12-1047,0,0.0685374,"Missing"
L16-1003,N13-1001,0,0.0129109,"TraMOOC’s industrial partner, Iversity, are also included since student forums will also be automatically translated for the purposes of implicit translation evaluation. Language pair EN-DE EN-BG EN-PT EN-EL EN-NL EN-CZ EN-RU EN-CR EN-PL EN-IT EN-ZH (Tyers and Alperen, 2010). These were supplemented with monolingual Wikipedia corpora13 for all three target languages. The phrase-based models include many features which make them strong baselines. These models include standard features plus a hierarchical lexicalised reordering model (Galley & Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013), binary features indicating absolute occurrence count classes of phrase pairs, sparse phrase length features, and sparse lexical features for the top-200 words. The models were optimised to maximise BLEU (Papineni et al., 2002) with batch MIRA (Cherry & Foster, 2012) on 1000-best lists. In Table 1 we compare the BLEU score performance of the systems on the TraMOOC test sets, when tuned on a mixed domain tuning set, or with the TraMOOC tuning set. The mixed tuning set includes tuning sets from TED, Europarl, and News to result in the highest possible general performance system (Huck et al., 20"
L16-1003,W10-0713,0,0.0292656,", and News to result in the highest possible general performance system (Huck et al., 2015). As expected, however, it is outperformed by using the domain-specific test set. System EN-EL EN-PT EN-IT TraMOOC Test 28.0 28.5 27.9 29.1 32.6 33.0 Table 2: BLEU scores for the initial translation prototypes 5. Size (million words) 2.7 1.5 4.8 2.4 1.3 1.5 1.4 0.2 1.7 2.3 8.7 Crowdsourcing Crowdsourcing has been employed extensively for the implementation of human intelligence natural language processing (NLP) tasks in recent years (Callison, 2009; Zaidan & Burch, 2011; Zbib et al., 2012; Ambati, 2012; Finin et al., 2010; Hsueh et al., 2009). TraMOOC involves crowdsourcing for realizing sub-goals that require human intervention in order to meet its high-quality output standards against upcoming challenges, including the large number of targeted languages, the fragmentary or weak SMT infrastructure support for the majority of the languages, and the multiple domains and text genres involved. The CrowdFlower 14 platform was chosen for the implementation of the crowdsourcing activities because of (a) its configurability, (b) its robust infrastructure, (c) its densely populated crowd channels and the evaluation an"
L16-1003,D08-1089,0,0.0114402,"ments, slides, and other course materials. The forum data of TraMOOC’s industrial partner, Iversity, are also included since student forums will also be automatically translated for the purposes of implicit translation evaluation. Language pair EN-DE EN-BG EN-PT EN-EL EN-NL EN-CZ EN-RU EN-CR EN-PL EN-IT EN-ZH (Tyers and Alperen, 2010). These were supplemented with monolingual Wikipedia corpora13 for all three target languages. The phrase-based models include many features which make them strong baselines. These models include standard features plus a hierarchical lexicalised reordering model (Galley & Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013), binary features indicating absolute occurrence count classes of phrase pairs, sparse phrase length features, and sparse lexical features for the top-200 words. The models were optimised to maximise BLEU (Papineni et al., 2002) with batch MIRA (Cherry & Foster, 2012) on 1000-best lists. In Table 1 we compare the BLEU score performance of the systems on the TraMOOC test sets, when tuned on a mixed domain tuning set, or with the TraMOOC tuning set. The mixed tuning set includes tuning sets from TED, Europarl, and News to result in the hi"
L16-1003,N12-1006,0,0.0606257,"Missing"
L16-1003,ambati-etal-2010-active,0,\N,Missing
L16-1203,broeder-etal-2010-data,0,0.0325953,"Missing"
L16-1203,odijk-2010-clarin,0,0.0314935,"lowing scholars to simultaneously search and analyze data from texts spanning the full recorded history of the Netherlands. An early vision of the project was described, in Dutch, as a ‘Portal to a Research Paradise’ by (van der Sijs, 2012). Partners actively involved in the construction of Nederlab are Meertens Institute, Institute for Dutch Lexicology, Huygens ING and Radboud University. The project ties in with other major projects and initiatives: for collections Nederlab collaborates with academic libraries and institutions in the Netherlands and Flanders, for infrastructure with CLARIN (Odijk, 2010) and CLARIAH2 , for tools with eHumanities programmes such as NWO CATCH3 and IM2 http://www.clariah.nl/en/ http://www.nwo.nl/en/ research-and-results/programmes/Continuous+ Access+To+Cultural+Heritage+%28CATCH%29 1277 3 PACT4 , and with language technologists from scientific institutions, again mainly in the Netherlands and Flanders. The Nederlab project is currently about halfway. We have a solid collection pipeline in place for processing metadata, text content, and vocabularies, and have tested and applied this pipeline on three collections. The Nederlab index now contains 13.5 million sear"
L16-1203,C14-2027,1,0.738438,"Missing"
L16-1203,stehouwer-etal-2012-federated,0,0.0302114,"semi-automatic data curation and interlinking. For texts that originate from Optical Character Recognition automatic spelling correction, OCR post-correction and text normalization is applied. To all texts we apply sentence splitting, tokenization, lemmatization, part of speech tagging, and named entity recognition, resulting in massive amounts of linguistic annotations. This uniform annotation treatment enables us to obtain the necessary statistics for word tokens and types, lemmata, PoS tags and entities across all incorporated cor1 Online at http://www.nederlab.nl/ pora. Federated Search (Stehouwer et al., 2012) across nonuniformly annotated corpora simply cannot provide this. Metadata, text and linguistic annotations are indexed in one large, powerful search index. Finally, a broker orchestrates searching and other data services, and provides a uniform web-based access point to all Nederlab collections for enduser tools. The most important of these tools is Nederlab’s own virtual research environment (see Section 6.). Section 2. of this paper introduces the Nederlab project. Section 3. discusses the wide range of collections that play a role in the project. Section 4. presents our collection pipelin"
L18-1073,abdelali-etal-2014-amara,0,0.0313475,"has the main benefit of access to people speaking the different languages. 3. Data Selection Our aim was to collect wikification annotations for 500 to 1,000 sentences from parallel educational texts for each language pair. We used three existing parallel text resources as the basis for the sentence selection so as to cover a broad range of online courses and to cover all eleven language pairs. In particular, we use parallel texts from course material of the Coursera MOOC platform, the Iversity MOOC platform, and the QCRI Educational Domain (QED) Corpus (formerly known as QCRI AMARA Corpus) (Abdelali et al., 2014). The Iversity data consists of (i) manually translated MOOC data, and (ii) MT output of English MOOC data produced by the first MT software prototype that was developed in the first year of the TraMOOC project. Both Coursera and QED material consist of MOOC subtitles that were translated using crowdsourcing in other projects unrelated to TraMOOC. The QED corpus consists of a large collection of files and each file contains a number of subtitles from MOOC video lectures in a particular language. The aligned files (parallel corpus) share the same first part of the file name. However, not all fi"
L18-1073,S16-1081,0,0.0258532,"We use the data set for an in-depth analysis and evaluation of machine translation output. In addition to word-based evaluations (e.g., BLEU), a semantic evaluation can be performed, as the links to the Wikipedia pages in the various target languages provide an additional source of information. Such implicit MT evaluation aims to judge the MT quality between source and target language without using an explicit (manual) translation step. Alternatively, the data set is suited for other multilingual tasks that focus on semantic aspects such as the cross-lingual Semantic Textual Similarity task (Agirre et al., 2016; Cer et al., 2017). Furthermore, the data set can be used as multilingual training material for the development of novel wikification tools (e.g., Tsai and Roth (2016)), or tools that automatically detect and link topics in a text to their respective Wikipedia pages. In the remainder of this paper we discuss related work in Section 2., the creation of this data set via crowdsourcing (Section 3.), the difficulties that we encountered using crowdsourcing for such complex annotation task (Section 4.) and the outcomes of this process in Section 5.. We also briefly discuss the use case of implicit"
L18-1073,S17-2001,0,0.03374,"for an in-depth analysis and evaluation of machine translation output. In addition to word-based evaluations (e.g., BLEU), a semantic evaluation can be performed, as the links to the Wikipedia pages in the various target languages provide an additional source of information. Such implicit MT evaluation aims to judge the MT quality between source and target language without using an explicit (manual) translation step. Alternatively, the data set is suited for other multilingual tasks that focus on semantic aspects such as the cross-lingual Semantic Textual Similarity task (Agirre et al., 2016; Cer et al., 2017). Furthermore, the data set can be used as multilingual training material for the development of novel wikification tools (e.g., Tsai and Roth (2016)), or tools that automatically detect and link topics in a text to their respective Wikipedia pages. In the remainder of this paper we discuss related work in Section 2., the creation of this data set via crowdsourcing (Section 3.), the difficulties that we encountered using crowdsourcing for such complex annotation task (Section 4.) and the outcomes of this process in Section 5.. We also briefly discuss the use case of implicit translation evalua"
L18-1073,W08-0336,0,0.024088,"of speech (such as “mmm”, [NOISE], or [MUSIC]). Also, particularly long paragraphs were not selected, so as to maintain the microtasking nature of the activity. The selected source and target sentences were automatically tokenized using the multilingual tokenizer Ucto1 (van Gompel et al., 2017). Ucto has language-specific rules for the tokenization of several languages including Dutch, English, German, Italian, Portuguese, and Russian. For the other languages, generic language-independent settings of Ucto were used except for the Chinese language where we applied the Stanford Word Segmenter (Chang et al., 2008). 4. Annotation via Crowdsourcing Crowdsourcing has been used extensively for annotating corpora due to being a cheap and fast means to collecting human intelligence input, compared to requesting expertbased intervention (Wang et al., 2013). Crowdsourcing approaches vary from voluntary work and gaming to paid microtasks (Bougrine et al., 2017). Applications involve, but are not limited to, the annotation of speech corpora (Bougrine et al., 2017; Su et al., 2017), of named entities (Bontcheva et al., 2017), of domain-specific concepts (Good et al., 2014) of relation extraction (Liu et al., 2016"
L18-1073,D13-1184,0,0.0441826,"Missing"
L18-1073,N16-1104,0,0.0316436,"ng et al., 2008). 4. Annotation via Crowdsourcing Crowdsourcing has been used extensively for annotating corpora due to being a cheap and fast means to collecting human intelligence input, compared to requesting expertbased intervention (Wang et al., 2013). Crowdsourcing approaches vary from voluntary work and gaming to paid microtasks (Bougrine et al., 2017). Applications involve, but are not limited to, the annotation of speech corpora (Bougrine et al., 2017; Su et al., 2017), of named entities (Bontcheva et al., 2017), of domain-specific concepts (Good et al., 2014) of relation extraction (Liu et al., 2016). Researchers have shown particular interest in issues pertaining to ethical implications (Cohen et al., 2016), as well as best practices for obtaining optimal quality output (Sabou et al., 2014). Best practice guidelines are followed in the present work also, after experimentation with varying 1 Ucto is freely available at languagemachines.github.io/ucto/. 468 https:// Figure 1: CrowdFlower interface for manual annotation. parameterization schemata, and involve task decomposition into simple microtasks, the appropriate crowd choice, the appropriate crowd reward choice, data preparation, task"
L18-1073,S15-2049,0,0.0331012,"e also briefly discuss the use case of implicit translation evaluation for which we created the data set in Section 6. and conclude in Section 7.. 2. Related Work Comparable and related types of data sets are those created for the evaluation of wikification tools (Mihalcea and Csomai, 2007). The Illinois Wikifier was evaluated on English material annotated with Wikipedia links (Ratinov et al., 2011). This evaluation set consists of Wikipedia pages and news articles. In the context of automatic word sense disambiguation, there is another related multilingual data set from Semeval-2015 task 13 (Moro and Navigli, 2015) containing links to BabelNet (Navigli and Ponzetto, 2012) and Wikipedia pages with news articles in three languages (of which only Italian matches the languages targeted in the TraMOOC project). These data sets do not cover all the languages we are interested in and, additionally, do not target the educational domain. Therefore, a new data set needed to be created. This data set is intended as both tuning material for the developed implicit machine translation system evaluation tool and for testing the final machine translation systems. The data set also gives us insights into the coverage of"
L18-1073,P11-1138,0,0.042925,"2., the creation of this data set via crowdsourcing (Section 3.), the difficulties that we encountered using crowdsourcing for such complex annotation task (Section 4.) and the outcomes of this process in Section 5.. We also briefly discuss the use case of implicit translation evaluation for which we created the data set in Section 6. and conclude in Section 7.. 2. Related Work Comparable and related types of data sets are those created for the evaluation of wikification tools (Mihalcea and Csomai, 2007). The Illinois Wikifier was evaluated on English material annotated with Wikipedia links (Ratinov et al., 2011). This evaluation set consists of Wikipedia pages and news articles. In the context of automatic word sense disambiguation, there is another related multilingual data set from Semeval-2015 task 13 (Moro and Navigli, 2015) containing links to BabelNet (Navigli and Ponzetto, 2012) and Wikipedia pages with news articles in three languages (of which only Italian matches the languages targeted in the TraMOOC project). These data sets do not cover all the languages we are interested in and, additionally, do not target the educational domain. Therefore, a new data set needed to be created. This data"
L18-1073,sabou-etal-2014-corpus,0,0.0251481,"ared to requesting expertbased intervention (Wang et al., 2013). Crowdsourcing approaches vary from voluntary work and gaming to paid microtasks (Bougrine et al., 2017). Applications involve, but are not limited to, the annotation of speech corpora (Bougrine et al., 2017; Su et al., 2017), of named entities (Bontcheva et al., 2017), of domain-specific concepts (Good et al., 2014) of relation extraction (Liu et al., 2016). Researchers have shown particular interest in issues pertaining to ethical implications (Cohen et al., 2016), as well as best practices for obtaining optimal quality output (Sabou et al., 2014). Best practice guidelines are followed in the present work also, after experimentation with varying 1 Ucto is freely available at languagemachines.github.io/ucto/. 468 https:// Figure 1: CrowdFlower interface for manual annotation. parameterization schemata, and involve task decomposition into simple microtasks, the appropriate crowd choice, the appropriate crowd reward choice, data preparation, task design, task completion time, quality control, task monitoring and crowd evaluation. Given pairs of aligned texts, the entity annotation (wikification) task consists of identifying and annotating"
L18-1073,N16-1072,0,0.0225831,"an be performed, as the links to the Wikipedia pages in the various target languages provide an additional source of information. Such implicit MT evaluation aims to judge the MT quality between source and target language without using an explicit (manual) translation step. Alternatively, the data set is suited for other multilingual tasks that focus on semantic aspects such as the cross-lingual Semantic Textual Similarity task (Agirre et al., 2016; Cer et al., 2017). Furthermore, the data set can be used as multilingual training material for the development of novel wikification tools (e.g., Tsai and Roth (2016)), or tools that automatically detect and link topics in a text to their respective Wikipedia pages. In the remainder of this paper we discuss related work in Section 2., the creation of this data set via crowdsourcing (Section 3.), the difficulties that we encountered using crowdsourcing for such complex annotation task (Section 4.) and the outcomes of this process in Section 5.. We also briefly discuss the use case of implicit translation evaluation for which we created the data set in Section 6. and conclude in Section 7.. 2. Related Work Comparable and related types of data sets are those"
L18-1521,P16-2050,1,0.627018,"Missing"
L18-1521,P14-5010,0,0.00299399,"grape varieties to be predicted. Much variation can be seen in the number of training instances per grape, ranging from 5,706 reviews for chardonnay to 222 reviews for carmen`ere. The third classifier aims at predicting among 47 different countries of origin. Again, the class distribution is unbalanced, with some countries represented very well (e.g., US: 25,104 reviews, Italy: 9,912 reviews, France: 8,568 reviews) to countries only occurring once (Tunisia, South Korea, Montenegro, India) in the training set. All wine reviews were linguistically preprocessed by means of the Stanford toolkit (Manning et al., 2014) involving tokenization, lemmatization and Part-of-Speech tagging. From the preprocessed review text, three different feature types were extracted to model the three classification tasks: lexical, semantic and terminology features. 4.1.1. Lexical features We extracted a list of bag-of-words (BoW) unigram features from the review text containing lowercased lemmas. These BoW features were filtered on Part-of-Speech category to filter out function words and only keep content words (nouns, adjectives, verbs, and adverbs). The BoW features were incorporated as binary features, meaning that each BoW"
P01-1012,P99-1040,0,0.0605618,"Missing"
P01-1012,A00-2028,0,0.0274759,"Missing"
P02-1055,W99-0621,0,\N,Missing
P02-1055,A00-2018,0,\N,Missing
P02-1055,W96-0102,0,\N,Missing
P02-1055,W99-0707,1,\N,Missing
P02-1055,W99-0629,1,\N,Missing
P02-1055,J93-2004,0,\N,Missing
P02-1055,W99-0706,0,\N,Missing
P02-1055,W01-0706,0,\N,Missing
P02-1055,W00-0735,1,\N,Missing
P02-1055,C96-1058,0,\N,Missing
P02-1055,A88-1019,0,\N,Missing
P02-1055,W00-0726,1,\N,Missing
P02-1055,P96-1025,0,\N,Missing
P02-1055,P01-1005,0,\N,Missing
P02-1055,P98-1010,0,\N,Missing
P02-1055,C98-1010,0,\N,Missing
P03-1062,daelemans-hoste-2002-evaluation,1,0.825919,"onstrated by (Daelemans et al., 1999), exceptions do typically reoccur in language data. Hence, machine learning algorithms that retain a memory trace of individual instances, like memory-based learning algorithms based on the k-nearest neighbour classifier, outperform decision tree or rule inducers precisely for this reason. Comparing the performance of machine learning algorithms is not straightforward, and deserves careful methodological consideration. For a fair comparison, both algorithms should be objectively and automatically optimized for the task to be learned. This point is made by (Daelemans and Hoste, 2002), who show that, for tasks such as word-sense disambiguation and part-of-speech tagging, tuning algorithms in terms of feature selection and classifier parameters gives rise to significant improvements in performance. In this paper, therefore, we optimize both CART and MBL individually and per task, using a heuristic optimization method called iterative deepening. The second issue, that of task combination, stems from the intuition that the two tasks have a lot in common. For instance, (Hirschberg, 1993) reports that knowledge of the location of breaks facilitates accent placement. Although pi"
P03-1062,W96-0102,1,0.89273,". An excerpt of the annotated data with all generated symbolic and numeric1 features is presented in Table 1. Word forms (Wrd) – The word form tokens form the central unit to which other features are added. Pre- and post-punctuation – All punctuation marks in the data are transferred to two separate features: a pre-punctuation feature (PreP) for punctuation marks such as quotation marks appearing before the token, and a post-punctuation feature (PostP) for punctuation marks such as periods, commas, and question marks following the token. Part-of-speech (POS) tagging – We used MBT version 1.0 (Daelemans et al., 1996) to develop a memory-based POS tagger trained on the Eindhoven corpus of written Dutch, which does not overlap with our base data. We split up the full POS tags into two features, the first (PosC) containing the main POS category, the second (PosF) the POS subfeatures. Diacritical accent – Some tokens bear an orthographical diacritical accent put there by the author to particularly emphasize the token in question. These accents were stripped off the accented letter, and transferred to a binary feature (DiA). NP and VP chunking (NpC & VpC) – An approximation of the syntactic structure is provid"
P03-1062,P00-1030,0,0.0146539,"1997). Predicting prosody is known to be a hard problem that is thought to require information on syntactic boundaries, syntactic and semantic relations between constituents, discourse-level knowledge, and phonological well-formedness constraints (Hirschberg, 1993). However, producing all this information – using full parsing, including establishing semanto-syntactic relations, and full discourse analysis – is currently infeasible for a realtime system. Resolving this dilemma has been the topic of several studies in pitch accent placement (Hirschberg, 1993; Black, 1995; Pan and McKeown, 1999; Pan and Hirschberg, 2000; Marsi et al., 2002) and in prosodic boundary placement (Wang and Hirschberg, 1997; Taylor and Black, 1998). The commonly adopted solution is to use shallow information sources that approximate full syntactic, semantic and discourse information, such as the words of the text themselves, their part-of-speech tags, or their information content (in general, or in the text at hand), since words with a high (semantic) information content or load tend to receive pitch accents (Ladd, 1996). Within this research paradigm, we investigate pitch accent and prosodic boundary placement for Dutch, using an"
P03-1062,W99-0619,0,0.0486747,"erance (Cutler et al., 1997). Predicting prosody is known to be a hard problem that is thought to require information on syntactic boundaries, syntactic and semantic relations between constituents, discourse-level knowledge, and phonological well-formedness constraints (Hirschberg, 1993). However, producing all this information – using full parsing, including establishing semanto-syntactic relations, and full discourse analysis – is currently infeasible for a realtime system. Resolving this dilemma has been the topic of several studies in pitch accent placement (Hirschberg, 1993; Black, 1995; Pan and McKeown, 1999; Pan and Hirschberg, 2000; Marsi et al., 2002) and in prosodic boundary placement (Wang and Hirschberg, 1997; Taylor and Black, 1998). The commonly adopted solution is to use shallow information sources that approximate full syntactic, semantic and discourse information, such as the words of the text themselves, their part-of-speech tags, or their information content (in general, or in the text at hand), since words with a high (semantic) information content or load tend to receive pitch accents (Ladd, 1996). Within this research paradigm, we investigate pitch accent and prosodic boundary pla"
P12-1107,P05-1074,0,0.065489,"Missing"
P12-1107,D08-1021,0,0.0128255,"Missing"
P12-1107,E99-1042,0,0.91382,"put. We also argue that text readability metrics such as the Flesch-Kincaid grade level should be used with caution when evaluating the output of simplification systems. 1 Introduction Sentence simplification can be defined as the process of producing a simplified version of a sentence by changing some of the lexical material and grammatical structure of that sentence, while still preserving the semantic content of the original sentence, in order to ease its understanding. Particularly language learners (Siddharthan, 2002), people with reading disabilities (Inui et al., 2003) such as aphasia (Carroll et al., 1999), and low-literacy readers (Watanabe et al., 2009) can benefit from this application. It can serve to generate output in a specific limited format, such as subtitles (Daelemans et al., 2004). Sentence simplification can also serve to preprocess the input of other tasks, such as summarization (Knight and Marcu, 2000), parsing, machine translation (Chandrasekar et al., 1996), semantic role labeling (Vickrey and Koller, 2008) or sentence fusion (Filippova and Strube, 2008). The goal of simplification is to achieve an improvement in readability, defined as the ease with which a text can be underst"
P12-1107,C96-2183,0,0.415994,", while still preserving the semantic content of the original sentence, in order to ease its understanding. Particularly language learners (Siddharthan, 2002), people with reading disabilities (Inui et al., 2003) such as aphasia (Carroll et al., 1999), and low-literacy readers (Watanabe et al., 2009) can benefit from this application. It can serve to generate output in a specific limited format, such as subtitles (Daelemans et al., 2004). Sentence simplification can also serve to preprocess the input of other tasks, such as summarization (Knight and Marcu, 2000), parsing, machine translation (Chandrasekar et al., 1996), semantic role labeling (Vickrey and Koller, 2008) or sentence fusion (Filippova and Strube, 2008). The goal of simplification is to achieve an improvement in readability, defined as the ease with which a text can be understood. Some of the factors that are known to help increase the readability of text are the vocabulary used, the length of the sentences, the syntactic structures present in the text, and the usage of discourse markers. One effort to create a simple version of English at the vocabulary level has been the creation of Basic English by Charles Kay Ogden. Basic English is a contr"
P12-1107,H05-1098,0,0.0447449,"Missing"
P12-1107,W11-1601,0,0.584213,"l., 2000; Vickrey and Koller, 2008). There has also been work on lexical substitution for simplification, where the aim is to substitute difficult words with simpler synonyms, derived from WordNet or dictionaries (Inui et al., 2003). Zhu et al. (2010) examine the use of paired documents in English Wikipedia and Simple Wikipedia for a data-driven approach to the sentence simplification task. They propose a probabilistic, syntaxbased machine translation approach to the problem and compare against a baseline of no simplification and a phrase-based machine translation approach. In a similar vein, Coster and Kauchak (2011) use a parallel corpus of paired documents from Simple Wikipedia and Wikipedia to train a phrase-based machine translation model coupled with a deletion model. Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al., 2010). Woodsend and Lapata (2011) investigate the use of Simple Wikipedia edit histories and an aligned Wikipedia– Simple Wikipedia corpus to induce a model based on quasi-synchronous grammar. They select the most appropriate simplification by using integer linear programming. We follow Zhu et al. (2010) and Coster"
P12-1107,W96-0102,0,0.0146636,"ng to a language model. For each noun, adjective and verb in the sentence this model takes that word and its part-of-speech tag and retrieves from WordNet all synonyms from all synsets the word occurs in. The word is then replaced by all of its synset words, and each replacement is scored by a SRILM language model (Stolcke, 2002) with probabilities that are obtained from training on the Simple Wikipedia data. The alternative that has the highest probability according to the language model is kept. If no relevant alternative is found, the word is left unchanged. We use the Memory-Based Tagger (Daelemans et al., 1996) trained on the Brown corpus to compute the part-ofspeech tags. The WordNet::QueryData2 Perl mod2 2.2 2.3 RevILP Woodsend and Lapata’s (2011) model is based on quasi-synchronous grammar (Smith and Eisner, 2006). Quasi-synchronous grammar generates a loose alignment between parse trees. It operates on individual sentences annotated with syntactic information in the form of phrase structure trees. Quasisynchronous grammar is used to generate all possible rewrite operations, after which integer linear programming is employed to select the most appropriate simplification. Their model is trained on"
P12-1107,D08-1019,0,0.0239748,"standing. Particularly language learners (Siddharthan, 2002), people with reading disabilities (Inui et al., 2003) such as aphasia (Carroll et al., 1999), and low-literacy readers (Watanabe et al., 2009) can benefit from this application. It can serve to generate output in a specific limited format, such as subtitles (Daelemans et al., 2004). Sentence simplification can also serve to preprocess the input of other tasks, such as summarization (Knight and Marcu, 2000), parsing, machine translation (Chandrasekar et al., 1996), semantic role labeling (Vickrey and Koller, 2008) or sentence fusion (Filippova and Strube, 2008). The goal of simplification is to achieve an improvement in readability, defined as the ease with which a text can be understood. Some of the factors that are known to help increase the readability of text are the vocabulary used, the length of the sentences, the syntactic structures present in the text, and the usage of discourse markers. One effort to create a simple version of English at the vocabulary level has been the creation of Basic English by Charles Kay Ogden. Basic English is a controlled language with a basic vocabulary consisting of 850 words. According to Ogden, 90 percent of a"
P12-1107,W03-1602,0,0.817529,", while generating better formed output. We also argue that text readability metrics such as the Flesch-Kincaid grade level should be used with caution when evaluating the output of simplification systems. 1 Introduction Sentence simplification can be defined as the process of producing a simplified version of a sentence by changing some of the lexical material and grammatical structure of that sentence, while still preserving the semantic content of the original sentence, in order to ease its understanding. Particularly language learners (Siddharthan, 2002), people with reading disabilities (Inui et al., 2003) such as aphasia (Carroll et al., 1999), and low-literacy readers (Watanabe et al., 2009) can benefit from this application. It can serve to generate output in a specific limited format, such as subtitles (Daelemans et al., 2004). Sentence simplification can also serve to preprocess the input of other tasks, such as summarization (Knight and Marcu, 2000), parsing, machine translation (Chandrasekar et al., 1996), semantic role labeling (Vickrey and Koller, 2008) or sentence fusion (Filippova and Strube, 2008). The goal of simplification is to achieve an improvement in readability, defined as th"
P12-1107,W09-0424,0,0.0281928,"Missing"
P12-1107,W07-0716,0,0.0153875,"Missing"
P12-1107,E06-1021,0,0.00949,"ired by syntax-based SMT (Yamada and Knight, 2001) and consists of a language model, a translation model and a decoder. The four mentioned simplification operations together form the translation model. Their model is trained on a corpus containing aligned sentences from English Wikipedia and English Simple Wikipedia called PWKP. The PWKP dataset consists of 108,016 pairs of aligned lines from 65,133 Wikipedia and Simple Wikipedia articles. These articles were paired by following the “interlanguage link”3 . TF*IDF at the sentence level was used to align the sentences in the different articles (Nelken and Shieber, 2006). Zhu et al. (2010) evaluate their system using BLEU and NIST scores, as well as various readability scores that only take into account the output sentence, such as the Flesch Reading Ease test and n-gram language model perplexity. Although their system outperforms several baselines at the level of these readability metrics, they do not achieve better when evaluated with BLEU or NIST. In this work we aim to investigate the use of phrasebased machine translation modified with a dissimilarity component for the task of sentence simplification. While Zhu et al. (2010) have demonstrated that their"
P12-1107,J03-1002,0,0.00859772,"R We use the Moses software to train a PBMT model (Koehn et al., 2007). The data we use is the PWKP dataset created by Zhu et al. (2010). In general, a statistical machine translation model finds a best translation e˜ of a text in language f to a text in language e by combining a translation model that 1018 finds the most likely translation p(f |e) with a language model that outputs the most likely sentence p(e): e˜ = arg max p(f |e)p(e) ∗ e∈e The GIZA++ statistical alignment package is used to perform the word alignments, which are later combined into phrase alignments in the Moses pipeline (Och and Ney, 2003) to build the sentence simplification model. GIZA++ utilizes IBM Models 1 to 5 and an HMM word alignment model to find statistically motivated alignments between words. We first tokenize and lowercase all data and use all unique sentences from the Simple Wikipedia part of the PWKP training set to train an n-gram language model with the SRILM toolkit to learn the probabilities of different n-grams. Then we invoke the GIZA++ aligner using the training simplification pairs. We run GIZA++ with standard settings and we perform no optimization. This results in a phrase table containing phrase pairs"
P12-1107,W04-3219,0,0.0390606,"Missing"
P12-1107,W06-3104,0,0.010711,"ord is then replaced by all of its synset words, and each replacement is scored by a SRILM language model (Stolcke, 2002) with probabilities that are obtained from training on the Simple Wikipedia data. The alternative that has the highest probability according to the language model is kept. If no relevant alternative is found, the word is left unchanged. We use the Memory-Based Tagger (Daelemans et al., 1996) trained on the Brown corpus to compute the part-ofspeech tags. The WordNet::QueryData2 Perl mod2 2.2 2.3 RevILP Woodsend and Lapata’s (2011) model is based on quasi-synchronous grammar (Smith and Eisner, 2006). Quasi-synchronous grammar generates a loose alignment between parse trees. It operates on individual sentences annotated with syntactic information in the form of phrase structure trees. Quasisynchronous grammar is used to generate all possible rewrite operations, after which integer linear programming is employed to select the most appropriate simplification. Their model is trained on two different datasets: one containing alignments between Wikipedia and English Simple Wikipedia (AlignILP), and one containing alignments between edits in the revision history of Simple Wikipedia (RevILP). Re"
P12-1107,P08-1040,0,0.315897,"original sentence, in order to ease its understanding. Particularly language learners (Siddharthan, 2002), people with reading disabilities (Inui et al., 2003) such as aphasia (Carroll et al., 1999), and low-literacy readers (Watanabe et al., 2009) can benefit from this application. It can serve to generate output in a specific limited format, such as subtitles (Daelemans et al., 2004). Sentence simplification can also serve to preprocess the input of other tasks, such as summarization (Knight and Marcu, 2000), parsing, machine translation (Chandrasekar et al., 1996), semantic role labeling (Vickrey and Koller, 2008) or sentence fusion (Filippova and Strube, 2008). The goal of simplification is to achieve an improvement in readability, defined as the ease with which a text can be understood. Some of the factors that are known to help increase the readability of text are the vocabulary used, the length of the sentences, the syntactic structures present in the text, and the usage of discourse markers. One effort to create a simple version of English at the vocabulary level has been the creation of Basic English by Charles Kay Ogden. Basic English is a controlled language with a basic vocabulary consisting o"
P12-1107,D11-1038,0,0.801355,"ple Wikipedia for a data-driven approach to the sentence simplification task. They propose a probabilistic, syntaxbased machine translation approach to the problem and compare against a baseline of no simplification and a phrase-based machine translation approach. In a similar vein, Coster and Kauchak (2011) use a parallel corpus of paired documents from Simple Wikipedia and Wikipedia to train a phrase-based machine translation model coupled with a deletion model. Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al., 2010). Woodsend and Lapata (2011) investigate the use of Simple Wikipedia edit histories and an aligned Wikipedia– Simple Wikipedia corpus to induce a model based on quasi-synchronous grammar. They select the most appropriate simplification by using integer linear programming. We follow Zhu et al. (2010) and Coster and Kauchak (2011) in proposing that sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning. We differ from the approach of Zhu et al. (2010) in the s"
P12-1107,W10-4223,1,0.848009,"Missing"
P12-1107,P01-1067,0,0.0213427,"scores from the translation model, and the target language model. In principle, all of this should be transportable to a data-driven machine translation account of sentence simplification, provided that a parallel corpus is available that pairs text to simplified versions of that text. ule is used to query WordNet (Fellbaum, 1998). 1.2 This study Zhu et al. (2010) learn a sentence simplification model which is able to perform four rewrite operations on the parse trees of the input sentences, namely substitution, reordering, splitting, and deletion. Their model is inspired by syntax-based SMT (Yamada and Knight, 2001) and consists of a language model, a translation model and a decoder. The four mentioned simplification operations together form the translation model. Their model is trained on a corpus containing aligned sentences from English Wikipedia and English Simple Wikipedia called PWKP. The PWKP dataset consists of 108,016 pairs of aligned lines from 65,133 Wikipedia and Simple Wikipedia articles. These articles were paired by following the “interlanguage link”3 . TF*IDF at the sentence level was used to align the sentences in the different articles (Nelken and Shieber, 2006). Zhu et al. (2010) evalu"
P12-1107,N10-1056,0,0.0432683,"glish Wikipedia and Simple Wikipedia for a data-driven approach to the sentence simplification task. They propose a probabilistic, syntaxbased machine translation approach to the problem and compare against a baseline of no simplification and a phrase-based machine translation approach. In a similar vein, Coster and Kauchak (2011) use a parallel corpus of paired documents from Simple Wikipedia and Wikipedia to train a phrase-based machine translation model coupled with a deletion model. Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al., 2010). Woodsend and Lapata (2011) investigate the use of Simple Wikipedia edit histories and an aligned Wikipedia– Simple Wikipedia corpus to induce a model based on quasi-synchronous grammar. They select the most appropriate simplification by using integer linear programming. We follow Zhu et al. (2010) and Coster and Kauchak (2011) in proposing that sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning. We differ from the approach o"
P12-1107,P09-1094,0,0.0435845,"Missing"
P12-1107,C10-1152,0,0.650897,"d to reduce overall sentence length, splits long sentences on the basis of syntactic 1015 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1015–1024, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics information (Chandrasekar and Srinivas, 1997; Carroll et al., 1998; Canning et al., 2000; Vickrey and Koller, 2008). There has also been work on lexical substitution for simplification, where the aim is to substitute difficult words with simpler synonyms, derived from WordNet or dictionaries (Inui et al., 2003). Zhu et al. (2010) examine the use of paired documents in English Wikipedia and Simple Wikipedia for a data-driven approach to the sentence simplification task. They propose a probabilistic, syntaxbased machine translation approach to the problem and compare against a baseline of no simplification and a phrase-based machine translation approach. In a similar vein, Coster and Kauchak (2011) use a parallel corpus of paired documents from Simple Wikipedia and Wikipedia to train a phrase-based machine translation model coupled with a deletion model. Another useful resource is the edit history of Simple Wikipedia, f"
P12-1107,P07-2045,0,\N,Missing
P12-1107,daelemans-etal-2004-automatic,0,\N,Missing
P14-1082,W04-3250,0,0.0124248,"tion to be incorporated that influences the weights of the classifier output, successfully forcing the system to select alternatives. This combination of a classifier with context size one and trigrambased language model proves to be most effective and reaches the best results so far. We have not conducted experiments with language models of other orders. (configuration l1r1+LM in the results in Table 2). It appears that the classifier approach and the L2 language model are able to complement each other. Statistical significance on the BLEU scores was tested using pairwise bootstrap sampling (Koehn, 2004). All significance tests were performed with 5, 000 iterations. We compared the outcomes of several key configurations. We first tested l1r1 against both baselines; both differences are significant at p < 0.01 for both. The same significance level was found when comparing l1r1+LM against l1r1, auto+LM against auto, as well as the LM baseline against the MLF baseline. Automatic feature selection auto was found to perform statistically better than l1r1, but only at p < 0.05. Conclusions with regard to context width may have to be tempered somewhat, as the performance of the l1r1 configuration wa"
P14-1082,J09-1002,0,0.0382624,"Missing"
P14-1082,2005.mtsummit-papers.11,0,0.0449107,"simply left both weights set to one, thereby assigning equal importance to translation model and language model. 4 Baselines Evaluation Several automated metrics exist for the evaluation of L2 system output against the L2 reference out874 tomary in MT decoders. Computing this baseline is done in the same fashion as previously illustrated in Equation 1, where scoreT then represents the normalised p(t|s) score from the phrasetranslation table rather than the class probability from the classifier. 6 Experiments & Results The data for our experiments were drawn from the Europarl parallel corpus (Koehn, 2005) from which we extracted two sets of 200, 000 sentence pairs each for several language pairs. These were used to form the training and test sets. The final test sets are a randomly sampled 5, 000 sentence pairs from the 200, 000-sentence test split for each language pair. All input data for the experiments in this section are publicly available2 . Let us first zoom in to convey a sense of scale on a specific language pair. The actual Europarl training set we generate for English (L1) to Spanish (L2), i.e. English fallback in a Spanish context, consists of 5, 608, 015 sentence pairs. This numbe"
P14-1082,D07-1007,0,0.0114974,"(Hoste et al., 2002; Decadt et al., 2004), in particular in cross-lingual word sense disambiguation, a task closely resembling our current task (van Gompel and van den Bosch, 2013). It has also been used in machine translation studies in which local source context is used to classify source phrases into target phrases, rather than looking them up in a phrase table (Stroppa et al., 2007; Haque et al., 2011). The idea of local phrase selection with a discriminative machine learning classifier using additional local (source-language) context was introduced in parallel to Stroppa et al. (2007) by Carpuat and Wu (2007) and Gim´enez and M´arquez (2007); cf. Haque et al. (2011) for an overview of more recent methods. The feature vector for the classifiers represents a local context of neighbouring words, and optionally also global context keywords in a binaryvalued bag-of-words configuration. The local context consists of an X number of L2 words to the left of the L1 fragment, and Y words to the right. When presented with test data, in which the L1 fragment is explicitly marked, we first check whether there is ambiguity for this L1 fragment and if a direct translation is available in our simple mapping table."
P14-1082,P96-1006,0,0.114879,"r of fragments in the input, regardless of whether the fragment is translated correctly or not. The system may skip fragments for which it can find no solution at all. In addition to these, the system’s output can be compared against the L2 reference translation(s) using established Machine Translation evaluation metrics. We report on BLEU, NIST, METEOR, and word error rate metrics WER and PER. These scores should generally be much better than the typical MT system performances as only local changes are made to otherwise “perfect” L2 sentences. translation. We used the method of extraction by Ng and Lee (1996) and encoded all keywords in a binary bag of words model. The experiments however showed that inclusion of such keywords did not make any noticeable impact on any of the results, so we restrict ourselves to mentioning this negative result. Our full system, including the scripts for data preparation, training, and evaluation, is implemented in Python and freely available as open-source from http://github.com/ proycon/colibrita/ . Version tag v0.2.1 is representative for the version used in this research. 3.1 Language Model We also implement a statistical language model as an optional component"
P14-1082,W04-0827,1,0.820495,"Missing"
P14-1082,J03-1002,0,0.00356806,"sampled into two large and equally-sized parts. One is the basis for the training set, and the other is the basis for the test set. The reason for such a large test split shall become apparent soon. From each of the splits (S), a phrase-translation table is constructed automatically in an unsupervised fashion. This is done using the scripts provided by the Statistical Machine Translation system Moses (Koehn et al., 2007). It invokes GIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003). The result, independent for each set, will be a phrase-translation table (T ) that maps phrases in L1 to L2. For each phrase-pair (fs , ft ) this phrase-translation table holds the computed translation probabilities P (fs |ft ) and P (ft |fs ). Given these phrase-translation tables, we can now extract both training data and test data using the algorithm in Figure 1. In our discourse, the source language (s) corresponds to L1, the fallback language used for by the end-user for inserting fragments, whilst the target language (t) is L2. Step 4 is effectively a filter: two thresholds can be conf"
P14-1082,2012.iwslt-evaluation.1,0,0.0993873,"Missing"
P14-1082,W07-0719,0,0.0713265,"Missing"
P14-1082,2007.tmi-papers.28,1,0.876241,"Missing"
P14-1082,S13-2033,1,0.890621,"Missing"
P14-1082,P07-2045,0,0.0117884,"L2. No further linguistic processing such as part-ofspeech tagging or lemmatisation takes place in our experiments; adding this remains open for future research. The parallel corpus is randomly sampled into two large and equally-sized parts. One is the basis for the training set, and the other is the basis for the test set. The reason for such a large test split shall become apparent soon. From each of the splits (S), a phrase-translation table is constructed automatically in an unsupervised fashion. This is done using the scripts provided by the Statistical Machine Translation system Moses (Koehn et al., 2007). It invokes GIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003). The result, independent for each set, will be a phrase-translation table (T ) that maps phrases in L1 to L2. For each phrase-pair (fs , ft ) this phrase-translation table holds the computed translation probabilities P (fs |ft ) and P (ft |fs ). Given these phrase-translation tables, we can now extract both training data and test data using the algorithm in Figure 1. In our discourse, the source langua"
P16-2023,P09-2085,0,0.0161352,"p We train 4-gram language model on the two training corpora, the Google 1 billion word benchmark and the Mediargus corpus. We do not perform any preprocessing on the data except tokenisation. The models are trained with a HPYLM. We do not use sentence beginning and end markers. The results for the ngram backoff strategy are obtained by training without skipgrams; for limited and full we added skipgram features during training. At the core of our experimental framework we use cpyp,1 which is an existing library for nonparametric Bayesian modelling with PY priors with histogram-based sampling (Blunsom et al., 2009). This library has an example application to showcase its performance with n-gram based language modelling. Limitations of the library, such as not natively supporting skipgrams, and the lack of other functionality such as thresholding and discarding of certain patterns, led us to extend the library with Colibri Core,2 a pattern modelling library. Colibri Core resolves the limitations, and together the libraries are a complete language model that handles skipgrams: cococpyp.3 Each model is run for 50 iterations (without an explicit burn-in phase), with hyperparameters θ = 1.0 and γ = 0.8. The"
P16-2023,oostdijk-2000-spoken,0,0.162811,"Missing"
P16-2023,P14-1108,0,0.041166,"Missing"
P16-2023,steinberger-etal-2006-jrc,0,0.125815,"Missing"
P16-2023,P06-1124,0,0.278325,"ons, such as the Dirichlet process and its two-parameter generalisation, the Pitman-Yor process. Both are widely studied in the statistics and probability theory communities. Interestingly, language modelling has acquired the status of a “fruit fly” problem in these communities, to benchmark the performance of statistical models. In this paper we approach language modelling from a computational linguistics point of view, and consider the statistical methods to be the tool with the future goal of improving language models for extrinsic tasks such as speech recognition. We derive our model from Teh (2006), and propose an extension with skipgrams. A frequentist approach to language modelling with skipgrams is described by Pickhardt et al. (2014), who introduce an approach using skip-n-grams which are interpolated using modified Kneser-Ney smoothing. In this paper we show that a Bayesian skip-ngram approach outperforms a frequentist skip-ngram model. In this paper we improve over the hierarchical Pitman-Yor processes language model in a cross-domain setting by adding skipgrams as features. We find that adding skipgram features reduces the perplexity. This reduction is substantial when models are"
P16-2050,P14-5010,0,0.0072621,"Missing"
P99-1037,J94-3007,1,0.632688,"by storing examples of a task in memory. Computational effort is invested on a &quot;call-by-need&quot; basis for solving new examples (henceforth called instances) of the same task. When new instances are presented to a memory-based learner, it searches for the bestmatching instances in memory, according to a task-dependent similarity metric. When it has found the best matches (the nearest neighbors), it transfers their solution (classification, label) to the new instance. Memory-based learning has been shown to be quite adequate for various natural-language processing tasks such as stress assignment (Daelemans et al., 1994), grapheme-phoneme conversion (Daelemans and Van den Bosch, 1996; Van den Bosch, 1997), and part-of-speech tagging (Daelemans et al., 1996b). The paper is structured as follows. First, we give a brief overview of Dutch morphology in Section 2. We then turn to a description of MBMA in Section 3. In Section 4 we present 285 the experimental outcomes of our study with MBMA. Section 5 summarizes our findings, reports briefly on a partial study of English showing that the approach is applicable to other languages, and lists our conclusions. 2 Dutch Morphology The processes of Dutch morphology inclu"
P99-1037,W96-0102,1,0.869778,"lled instances) of the same task. When new instances are presented to a memory-based learner, it searches for the bestmatching instances in memory, according to a task-dependent similarity metric. When it has found the best matches (the nearest neighbors), it transfers their solution (classification, label) to the new instance. Memory-based learning has been shown to be quite adequate for various natural-language processing tasks such as stress assignment (Daelemans et al., 1994), grapheme-phoneme conversion (Daelemans and Van den Bosch, 1996; Van den Bosch, 1997), and part-of-speech tagging (Daelemans et al., 1996b). The paper is structured as follows. First, we give a brief overview of Dutch morphology in Section 2. We then turn to a description of MBMA in Section 3. In Section 4 we present 285 the experimental outcomes of our study with MBMA. Section 5 summarizes our findings, reports briefly on a partial study of English showing that the approach is applicable to other languages, and lists our conclusions. 2 Dutch Morphology The processes of Dutch morphology include inflection, derivation, and compounding. Inflection of verbs, adjectives, and nouns is mostly achieved by suffixation, but a circumfix"
P99-1037,E93-1023,0,0.0748031,"+en, we stop); there is variation between s and z and f and v (e.g. huis, house, versus huizen, houses). Finally, between the parts of a compound, a linking morpheme may appear (e.g. staat+s+loterij, state lottery). For a detailed discussion of morphological phenomena in Dutch, see De Haas and Trommelen (1993). Previous approaches to Dutch morphological analysis have been based on finite-state transducers (e.g., XEROX&apos;es morphological analyzer), or on parsing with context-free word grammars interleaved with exploration of possible spelling changes (e.g. Heemskerk and van Heuven (1993); or see Heemskerk (1993) for a probabilistic variant). 286 3 Applying memory-based learning to morphological a n a l y s i s Most linguistic problems can be seen as,contextsensitive mappings from one representation to another (e.g., from text to speech; from a sequence of spelling words to a parse tree; from a parse tree to logical form, from source language to target language, etc.) (Daelemans, 1995). This is also the case for morphological analysis. Memory-based learning algorithms can learn mappings (classifications) if a sufficient number of instances of these mappings is presented to them. We drew our instances"
R09-1051,W06-2924,1,0.87021,"training and processing efficiency to the number of class labels [4]. Memory-based algorithms have been previously applied to processing semantic and syntactic dependencies separately. As for semantic role labeling, [10] describes a memory-based semantic role labeling system for Spanish based on gold standard dependency syntax; [11] report on a semantic role labeling system for English based on syntactic dependencies produced by the MaltParser system of Nivre et al. [13]. As for dependency parsing, MaltParser uses memorybased learning as one of its optional local classifiers. Canisius et al. [2] present another type of memorybased dependency parser, extended later in [3] to a constraint satisfaction-based dependency parser. The latter parser combines local memory-based classification with a global optimization method based on soft weighted constraint-satisfaction inference, where the local classifiers estimate syntactic relations between pairs of words, the direction of the relation from children to parents, and the relations that parents have with children. Our current joint system adopts a similar strategy, but uses ranking rather than weighted constraint satisfaction inference. 1"
R09-1051,D07-1121,0,0.0469762,"-based algorithms have been previously applied to processing semantic and syntactic dependencies separately. As for semantic role labeling, [10] describes a memory-based semantic role labeling system for Spanish based on gold standard dependency syntax; [11] report on a semantic role labeling system for English based on syntactic dependencies produced by the MaltParser system of Nivre et al. [13]. As for dependency parsing, MaltParser uses memorybased learning as one of its optional local classifiers. Canisius et al. [2] present another type of memorybased dependency parser, extended later in [3] to a constraint satisfaction-based dependency parser. The latter parser combines local memory-based classification with a global optimization method based on soft weighted constraint-satisfaction inference, where the local classifiers estimate syntactic relations between pairs of words, the direction of the relation from children to parents, and the relations that parents have with children. Our current joint system adopts a similar strategy, but uses ranking rather than weighted constraint satisfaction inference. 1 http://ufal.mff.cuni.cz/conll2009-st/ 275 International Conference RANLP 2009"
R09-1051,W09-1202,0,0.0423312,"t A is the argument with the A0 role of predicate B. Thus, we merge the class labels of the two 276 tasks into single labels, and present the classifiers with examples with these labels. Further on the system, as we describe in the next section, we do make use of the compositionality of the labels, as in the end we have to produce syntactic dependency graphs and semantic role assignments separately. Apart from our system, three more joint systems participated in the CoNLL Shared Task 2009. The system described by [9] extends the Eisner parser to accomodate semantic dependencies. The system of [5] decomposes the joint learning task in four subtasks: semantic dependency identification and labeling, and syntactic dependency identification and labeling. A pipeline approach is set up in order to use the output of one task as input of another, and features not available at a certain step are incorporated iteratively. The system described by [7] is based on an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for both types of dependency structures. Fig. 1: Architecture of the joint system for dependency parsing and semantic role labe"
R09-1051,N09-1037,0,0.032308,"ion performance as compared to learning the letter-phoneme task and the stress marker assignment task separately. Buchholz [1] transposes this idea to shallow parsing, and shows that POS tagging and base phrase chunking could be learnd as a single task without any significant performance loss. Wang et al. [17] jointly learn Chinese word segmentation, named entity recognition, and part-of-speech tagging, outperforming a pipeline architecture baseline. Recently, Finkel and Manning show that joint learning of parsing and named entity recognition produce mildly improved performance for both tasks [6]. The merging of two tasks will typically lead to an increase in the number of class labels, and generally a more complex class space. In the worst case, the number of classes in the new class space is the product of the number of classes in the original tasks. In practice, if two combined tasks are to some extent related, the increase will tend to be limited, as class labels from the original tasks will tend to correlate. For instance, the POS tag for “determiner” will typically co-occur with the chunk marker for “beginning of noun phrase”, and less so, or not at all with other chunk markers."
R09-1051,W09-1205,0,0.0965718,"ency graphs and semantic role assignments separately. Apart from our system, three more joint systems participated in the CoNLL Shared Task 2009. The system described by [9] extends the Eisner parser to accomodate semantic dependencies. The system of [5] decomposes the joint learning task in four subtasks: semantic dependency identification and labeling, and syntactic dependency identification and labeling. A pipeline approach is set up in order to use the output of one task as input of another, and features not available at a certain step are incorporated iteratively. The system described by [7] is based on an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for both types of dependency structures. Fig. 1: Architecture of the joint system for dependency parsing and semantic role labeling 3 System description In this section we describe the joint system, and compare it to the isolated version of the system. The joint system operates in three phases (see Figure 1): a classification phase in which three memory-based classifiers predict different aspects of joint syntactic and semantic labeling; a ranking phase in which the outpu"
R09-1051,W09-1201,0,0.0653145,"do not signify the same, the “subject” dependency relation, for example, often co-occurs with the “A0” label that denotes the agent role in the PropBank annotation scheme [14]. Overlaps such as these naturally suggest the possibility of jointly learning the two labeling tasks as if they were one. In this paper we present a system that performs dependency parsing and semantic role labeling jointly, Antal van den Bosch Tilburg Centre for Creative Computing Tilburg University P.O. Box 90153 NL-5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl which we submitted to the CoNLL Shared Task 2009 [8]. The task combines the identification and labeling of syntactic dependencies and semantic roles for seven languages. Details about the task setting and the data sets used can be found in the web page of the task1 . Additionally, we present a comparison of the joint system with another version of the system (“isolated” system) that processes semantic and syntactic dependencies separately. In this way, we are able to evaluate whether and where the joint learning approach is more efficient and successful than the isolated approach. As far as we know, this is the first time that such a comparison"
R09-1051,W09-1212,0,0.0661215,"is the modifier in a subject dependency relation with its head B, as well as that A is the argument with the A0 role of predicate B. Thus, we merge the class labels of the two 276 tasks into single labels, and present the classifiers with examples with these labels. Further on the system, as we describe in the next section, we do make use of the compositionality of the labels, as in the end we have to produce syntactic dependency graphs and semantic role assignments separately. Apart from our system, three more joint systems participated in the CoNLL Shared Task 2009. The system described by [9] extends the Eisner parser to accomodate semantic dependencies. The system of [5] decomposes the joint learning task in four subtasks: semantic dependency identification and labeling, and syntactic dependency identification and labeling. A pipeline approach is set up in order to use the output of one task as input of another, and features not available at a certain step are incorporated iteratively. The system described by [7] is based on an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for both types of dependency structures. Fig."
R09-1051,morante-2008-semantic,1,0.900396,"mber of labels increases, and the average number of examples per label decreases. This does not rule out the application of a machine learning classifier to the joint task, but the classifier should not be too sensitive to a fragmented class space with many labels. This is the main reason our system relies on local memory-based classifiers: they are largely insensitive in terms of training and processing efficiency to the number of class labels [4]. Memory-based algorithms have been previously applied to processing semantic and syntactic dependencies separately. As for semantic role labeling, [10] describes a memory-based semantic role labeling system for Spanish based on gold standard dependency syntax; [11] report on a semantic role labeling system for English based on syntactic dependencies produced by the MaltParser system of Nivre et al. [13]. As for dependency parsing, MaltParser uses memorybased learning as one of its optional local classifiers. Canisius et al. [2] present another type of memorybased dependency parser, extended later in [3] to a constraint satisfaction-based dependency parser. The latter parser combines local memory-based classification with a global optimizatio"
R09-1051,W08-2128,1,0.901491,"cation of a machine learning classifier to the joint task, but the classifier should not be too sensitive to a fragmented class space with many labels. This is the main reason our system relies on local memory-based classifiers: they are largely insensitive in terms of training and processing efficiency to the number of class labels [4]. Memory-based algorithms have been previously applied to processing semantic and syntactic dependencies separately. As for semantic role labeling, [10] describes a memory-based semantic role labeling system for Spanish based on gold standard dependency syntax; [11] report on a semantic role labeling system for English based on syntactic dependencies produced by the MaltParser system of Nivre et al. [13]. As for dependency parsing, MaltParser uses memorybased learning as one of its optional local classifiers. Canisius et al. [2] present another type of memorybased dependency parser, extended later in [3] to a constraint satisfaction-based dependency parser. The latter parser combines local memory-based classification with a global optimization method based on soft weighted constraint-satisfaction inference, where the local classifiers estimate syntactic"
R09-1051,W09-1203,1,0.864082,"vel. Two classifiers consider pairs of words, and predict the identity or the presence, respectively, of a joint semantic and syntactic dependency between them. The third classifier focus on single words only, and predicts the relations one word has with other words, without making reference to these other words. The hyperparameters of the classifiers were optimized on English, by training on the full training set and testing on the development set; these optimized settings were then used for the other six languages as well. The hyperparameters and features used per classifier can be found in [12]. Classifier 1: Pairwise semantic and syntactic dependencies. Classifier 1 predicts the merged semantic and syntactic dependencies that hold between two tokens. Instances represent combinations of pairs of tokens within a sentence. Each token is combined with all other tokens in the sentence. The class predicted is a joint < dependencyrelation &gt;:< semanticrole &gt; label, or NONE if no relation is present between the tokens. The amount of occurring classes for all seven languages is shown in Table 2. C1 C2 Cat 111 111 Chi 309 1209 Cze 395 1221 Eng 551 1957 Ger 152 300 Jap 103 505 Spa 124 124 Tabl"
R09-1051,J05-1004,0,0.144165,"ntences, while semantic role assignments center around individual predicates. Yet, the spaces overlap; in a dependency graph verbal predicates will tend to have dependency relations with the same modifiers that have a semantic role as argument of that predicate. In general, even though the labels are different, syntactic dependencies between two words often co-occur with the existence of certain semantic roles. Although they do not signify the same, the “subject” dependency relation, for example, often co-occurs with the “A0” label that denotes the agent role in the PropBank annotation scheme [14]. Overlaps such as these naturally suggest the possibility of jointly learning the two labeling tasks as if they were one. In this paper we present a system that performs dependency parsing and semantic role labeling jointly, Antal van den Bosch Tilburg Centre for Creative Computing Tilburg University P.O. Box 90153 NL-5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl which we submitted to the CoNLL Shared Task 2009 [8]. The task combines the identification and labeling of syntactic dependencies and semantic roles for seven languages. Details about the task setting and the data sets used"
R15-1043,N10-1021,0,0.10136,"Missing"
R15-1043,D13-1100,0,0.0503397,"Missing"
R15-1043,P12-1056,0,0.0328029,"any hashtag as event term. Commonness is formulated as the prior probability of a concept c (the n-gram) to be used as an anchor text q in Wikipedia (Meij et al., 2012): The automatic identification of periodic patterns related to events has not been applied in the context of Twitter. The detection of single events, on the other hand, is a popular strand of research. Many studies have leveraged the notion of burstiness, the sudden rise and fall of word frequency, to find events from Twitter. Either by looking at the rapid growth of tweet clusters (Petrovi´c et al., 2010; McMinn et al., 2013; Diao et al., 2012) or words with peaky behavior (Weng and Lee, 2011; Li et al., 2012). The explicit reference to events in tweets has also been shown to help find scheduled events; social events in particular (Ritter et al., 2012). A possible reason the aforementioned approaches have not been employed to search for periodically recurring events, is that it requires a longitudinal effort to increase the chances of observing periodic behavior. To this end, we make use of TwiNL (Tjong Kim Sang and van den Bosch, 2013), a database of IDs of Dutch tweets gathered from December 2010 onwards. 3 3.1 Examples (English)"
R15-1043,D11-1141,0,0.0893327,"Missing"
R15-1043,S10-1071,0,0.0949923,"Missing"
R19-1070,S17-2051,0,0.128753,"this work and of most other studies in this field, consists of reranking the most likely candidate questions with a more fine-grained, domain-specific approach. Optionally, the system could also return whether a candidate question is a duplicate of the query. The reranking task has been included as a benchmark task (Task 3 - Subtask B) in SemEval2016/2017 (Nakov et al., 2016, 2017). Using the domain of Qatar Living4 , it consisted of re-ranking ten candidate questions retrieved by Google for a target question. Several promising approaches were proposed for this challenge, most notably SimBOW (Charlet and Damnati, 2017) based on the SoftCosine metric and winner of SemEval-2017, and KeLP (Filice et al., 2016), which is based on Tree Kernels and provided top results for all the subtasks in the challenge. However, little is known about the effects of particular design choices for these models, especially concerning the preprocessing methods and wordsimilarity metrics. Moreover, we know little about Community Question Answering forums are popular among Internet users, and a basic problem they encounter is trying to find out if their question has already been posed before. To address this issue, NLP researchers h"
R19-1070,D11-1096,0,0.0328394,"sk by many studies. We used the implementation of BM25 provided by gensim5 as a baseline. Translation-Based Language Model (TRLM) is a question similarity ranking function, first introduced by Xue et al. (2008). The method combines a language model with a word translation system technique, and is known to obtain better results on the question similarity task than BM25 and only the language model (Jeon et al., 2005). Equation 1 summarizes the TRLM ranking score between questions Q1 and Q2 : Smoothed Partial Tree Kernels (SPTK) are the basis of KeLP (Filice et al., 2016), a system introduced by Croce et al. (2011). SPTK applies the kernel trick by computing the similarity of question pairs based on the number of common substructures their parse trees share. The difference with Partial Tree Kernels (PTK) (Moschitti, 2006) is that SPTK also considers word relations. Besides the different variations of the model, which are well explained in Moschitti (2006) and 5 https://radimrehurek.com/gensim/ summarization/bm25.html 594 Filice et al. (2016), we designed SPTK in the following form. Equation 3 portrays the notation of the similarity metric among two questions’ constituency trees, i.e. TQ1 and TQ2 . X X n"
R19-1070,S16-1172,0,0.10683,"te questions with a more fine-grained, domain-specific approach. Optionally, the system could also return whether a candidate question is a duplicate of the query. The reranking task has been included as a benchmark task (Task 3 - Subtask B) in SemEval2016/2017 (Nakov et al., 2016, 2017). Using the domain of Qatar Living4 , it consisted of re-ranking ten candidate questions retrieved by Google for a target question. Several promising approaches were proposed for this challenge, most notably SimBOW (Charlet and Damnati, 2017) based on the SoftCosine metric and winner of SemEval-2017, and KeLP (Filice et al., 2016), which is based on Tree Kernels and provided top results for all the subtasks in the challenge. However, little is known about the effects of particular design choices for these models, especially concerning the preprocessing methods and wordsimilarity metrics. Moreover, we know little about Community Question Answering forums are popular among Internet users, and a basic problem they encounter is trying to find out if their question has already been posed before. To address this issue, NLP researchers have developed methods to automatically detect question-similarity, which was one of the sh"
R19-1070,S16-1126,0,0.0310224,"ates than EnsSPTK. The results are different in the test set of Semeval 2017: the latter approach is slightly better than the former on re-ranking similar questions according to the MAP metric, but shows a nonsignificant difference in classifying duplicates according to the F-1 score metric. Although the results between our two best approaches are inconclusive, we argue that the inclusion of SPTK in the ensemble is not beneficial due to the trade-off between efficiency and performance. The SPTK approach, mainly its kernel, In SemEval-2016, the UH-PRHLT model was the winner of the shared-task (Franco-Salvador et al., 2016). This system is based on a range of lexical (cosine similarity, word, noun and ngram overlap) and semantic (word representations, alignments, knowledge graphs and common frames) features. In turn, our best model, Ensemble, with considerably less features, obtains competitive results in terms of MAP. The same pattern is seen for the SemEval-2017 test set: the Ensemble approach obtained competitive results with the winner SimBOW, also based on the SoftCosine metric, in terms of MAP, and outperforms it in FScore. 597 Quora results Based on the previous results, we also evaluated the performance"
R19-1070,N13-1090,0,0.0558413,"n X Xi Mij Yj X tM X = Sof tCos(X, Y ) = √ We compare two traditional and two recent approaches in this study: BM25, TranslationBased Language Model (TRLM), SoftCosine and Smoothed Partial Tree Kernels (SPTK - Syntactic Tree Kernels). (2) i=1 j=1 Mij = max(0, cosine(Vi , Vj ))2 As Sim(w, t) in Equation 1, Mij represents the similarity between the i-th word of question Q1 and the j-th one in question Q2 . cosine is the cosine similarity, and Vi and Vj are originally 300-dimension embedding representations of the words, trained on the unannotated part of the Qatar living corpus using Word2Vec (Mikolov et al., 2013) with a context window size of 10. BM25 is a fast information retrieval technique (Robertson et al., 2009) used as a search engine in the first step of the shared task by many studies. We used the implementation of BM25 provided by gensim5 as a baseline. Translation-Based Language Model (TRLM) is a question similarity ranking function, first introduced by Xue et al. (2008). The method combines a language model with a word translation system technique, and is known to obtain better results on the question similarity task than BM25 and only the language model (Jeon et al., 2005). Equation 1 summ"
R19-1070,P04-3031,0,0.181064,"cased data. The table also shows the results of the best baseline (e.g., Google) and the winners of the SemEval 20162017 challenges. As expected, our best models were the ensemble approaches (e.g., Ensemble and EnsSPTK), which combine the ranking scores of all the other evaluated approaches and outperform Experiment 2: Word-Similarity A central component of all of the evaluated models except BM25 is the use of a word-similarity metric. To evaluate which distribution better captures the similarity between two words for the 8 We used the list of English stopwords provided by the NLTK framework (Bird and Loper, 2004) 596 Preproc. L.S.P. L.S. L.P. S.P. L. S. P. Metric Translation Word2Vec fastText Word2Vec+ELMo fastText+ELMo BM25 68.80 67.31 69.95 66.03 67.07 63.77 65.05 63.52 BM25 - TRLM 68.43 63.25 68.42 68.65 66.42 64.53 64.38 64.95 TRLM 68.43 72.90 70.93 71.41 70.56 SoftCosine 72.75 69.15 65.33 68.56 63.68 67.01 60.04 60.66 SoftCosine 70.75 72.75 71.07 73.89 73.43 SPTK 54.34 54.44 SPTK 48.10 54.44 53.49 54.78 54.77 Ensemble 71.62 69.50 68.70 68.67 67.04 67.85 65.31 63.08 Ensemble 70.80 71.40 71.92 73.90 73.73 EnsSPTK 72.40 71.29 69.16 70.37 67.41 68.36 66.66 64.31 EnsSPTK 70.80 72.64 71.92 74.63 73.73"
R19-1070,S17-2003,0,0.0629111,"Missing"
R19-1070,Q17-1010,0,0.0391958,"s, hyperparameters of the models such as σ and α of TRLM and γ of Ensemble with SPTK were optimized in the development split of the data through Grid Search. Moreover, Support Vector Machines in SPTK and Logistic Regression in Ensemble were implemented based on the Scikit-Learn toolkit (Pedregosa et al., 2011) and had their hyperparameters tuned by crossvalidation on the training set. 3.3 task, we evaluated all the models using the wordtranslation probabilities, plus the cosine similarity measure depicted in Equation 2. In the latter, besides Word2Vec representations, we also tested fastText (Bojanowski et al., 2017), a distribution which takes character-level information and tends to overcome spelling variations, and the top layer of ELMo (Peters et al., 2018). To equalize the trials, the data used by the models were lowercased and stripped of stop words and punctuation. 4 Evaluation The first section of Table 1 lists the MAP of the preprocessing methods in the development part of the corpus for each model. Although the best combination of preprocessing methods differs between models, we see that preprocessing the data is beneficial for the performance of all models, except for SPTK. Between the best res"
R19-1070,H93-1039,0,0.0544812,"ent datasets. Results show that the choice of a preprocessing method and a word-similarity metric have a considerable impact on the final results. We also show that the combination of all the analyzed approaches leads to results competitive with related work in question-similarity. 2 T RLM (Q1 , Q2 ) = Y (1 − σ)Ptr (w|Q2 ) + σPlm (w|C) w∈Q1 X Ptr (w|Q2 ) = α Sim(w, t)Plm (t|Q2 )+ t∈Q2 (1 − α)Plm (w|Q2 ) (1) Sim(w, t) denotes a similarity score among words w and t. In the original study, this similarity metric is the word-translation probability P (w|t) obtained by the IBM Translation Model 1 (Brown et al., 1993). Furthermore, C denotes a background corpus to compute unigram probabilities in order to avoid 0 scores. SoftCosine is the ranking function used by SimBOW (Charlet and Damnati, 2017), the winning system of the question similarity re-ranking task of SemEval 2017 (Nakov et al., 2017). The method is similar to a cosine similarity between the tf-idf bag-of-words of the pair of questions, except that it also takes into account word-level similarities as a matrix M . Given X and Y as the respective tfidf bag-of-words for questions Q1 and Q2 , Equation 2 summarizes the SoftCosine metric. Models X tM"
R19-1070,N18-1202,0,0.0135032,"earch. Moreover, Support Vector Machines in SPTK and Logistic Regression in Ensemble were implemented based on the Scikit-Learn toolkit (Pedregosa et al., 2011) and had their hyperparameters tuned by crossvalidation on the training set. 3.3 task, we evaluated all the models using the wordtranslation probabilities, plus the cosine similarity measure depicted in Equation 2. In the latter, besides Word2Vec representations, we also tested fastText (Bojanowski et al., 2017), a distribution which takes character-level information and tends to overcome spelling variations, and the top layer of ELMo (Peters et al., 2018). To equalize the trials, the data used by the models were lowercased and stripped of stop words and punctuation. 4 Evaluation The first section of Table 1 lists the MAP of the preprocessing methods in the development part of the corpus for each model. Although the best combination of preprocessing methods differs between models, we see that preprocessing the data is beneficial for the performance of all models, except for SPTK. Between the best results, we see that suppression of punctuation is beneficial for all the models, while the removal of stopwords and lowercasing are detrimental to BM"
S01-1003,W96-0102,0,0.0517625,"n information can be found immediately adjacent to the ambiguous word.It has been found that a four-word window, two words before the target word and two words after gives good results; cf. (Veenstra et al., 2000). Second, information about the grammatical category of the target word and its direct context words can also be valuable. Consequently, each sentence of the Dutch corpus was tagged and the part-of-speech (POS) tags of the word and its direct context (two left, two right) are included in the representation of the sentence. Part-of-speech tagging was done with the Memory Based Tagger (Daelemans et al., 1996). Third, informative words in the context ('keywords') are detected based on the statistical chi-squared test. Chi-square estimates the significance, or degree of surprise, of the number of keyword occurrences with respect to the expected number of occurrences (apriori probability): 4.9~9 9319 6.702 54 Table 1: Basic corpus statistics metjmeLprepositie geweld/= opgelostjoplossen_probleem wordenjworden_hww ,"" /= zeiden/zeggen_praten de/= koningenjkoning .J= toenjtoen_adv verklaardenjverklaren_oorlog zej= elkaar/=de/= oorlog/= .J= The dataset needed some adaptations to make it fully usable for c"
S07-1039,W96-0102,0,0.0707357,"ous way to model their lexical semantics was by utilizing WordNet3.0 (Fellbaum, 1998) (WN). One of the systems followed this route. We also entered a second system, which did not rely on WN but instead made use of automatically 2 System Description The development of the system consists of a preprocessing phase to extract the features, and the classification phase. 2.1 Preprocessing Each sentence is preprocessed automatically in the following steps. First, the sentence is tokenized with a rule-based tokenizer. Next a part-of-speech tagger and text chunker that use the memory-based tagger MBT (Daelemans et al., 1996) produces partof-speech tags and NP chunk labels for each token. Then a memory-based shallow parser predicts grammatical relations between verbs and NP chunks such as subject, object or modifier (Buchholz, 2002). The tagger, chunker and parser were all trained on the WSJ Corpus (Marcus et al., 1993). We also use a memory-based lemmatizer (Van den Bosch et al., 1996) trained on Celex (Baayen et al., 1993) to predict the lemma of each word. 187 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 187–190, c Prague, June 2007. 2007 Association for Computatio"
S07-1039,J93-2004,0,0.0321182,"ing phase to extract the features, and the classification phase. 2.1 Preprocessing Each sentence is preprocessed automatically in the following steps. First, the sentence is tokenized with a rule-based tokenizer. Next a part-of-speech tagger and text chunker that use the memory-based tagger MBT (Daelemans et al., 1996) produces partof-speech tags and NP chunk labels for each token. Then a memory-based shallow parser predicts grammatical relations between verbs and NP chunks such as subject, object or modifier (Buchholz, 2002). The tagger, chunker and parser were all trained on the WSJ Corpus (Marcus et al., 1993). We also use a memory-based lemmatizer (Van den Bosch et al., 1996) trained on Celex (Baayen et al., 1993) to predict the lemma of each word. 187 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 187–190, c Prague, June 2007. 2007 Association for Computational Linguistics The features extracted are of three types: semantic, lexical, and morpho-syntactic. The features that apply to the entities in a relation (e1,e2) are extracted for term 1 (t1) and term 2 (t2) of the relation, where t1 is the first term in the relation name, and t2 is the second term."
S13-2033,atserias-etal-2006-freeling,0,0.0450861,"Missing"
S13-2033,S13-2034,0,0.229605,"ntly assess the idea of feature selection, without hyperparameter optimisation on the classifiers. This proves to be a good idea. However, the fourth configuration was not yet available for the actual competition. This incidentally would have had no impact on the final ranking between competitors. When we run these systems on the actual test data of the shared task, we obtain the results in Table 3. The best score amongst the other competitors is mentioned in the last row for reference, this is the HLTDI team (Rudnick et al., 2013) for all but Best-Spanish, which goes to the NRC contribution (Carpuat, 2013). BEST baseline c1l c1lN var varN best.comp OUT-OF-5 baseline c1l c1lN var varN best.comp ES 23.23 28.40 28.65 23.3 29.05 32.16 ES 53.07 58.23 57.62 55.70 58.61 61.69 FR 25.74 29.88 30.11 25.89 30.15 28.23 FR 51.36 59.07 59.80 59.19 59.26 58.20 IT 20.21 25.43 25.66 20.38 24.90 24.62 IT 42.63 52.22 52.73 51.18 50.89 53.57 NL 20.66 23.14 23.61 17.17 23.57 22.36 NL 43.59 47.83 47.62 46.85 50.42 46.55 DE 17.42 20.70 20.82 16.2 21.98 19.92 DE 38.86 43.17 43.24 41.46 43.34 43.66 Table 3: Results on the test set A major factor in this task is the accuracy of lemmatisation, and to lesser extent of PoS"
S13-2033,P07-2045,0,0.00285319,"matisation, and to lesser extent of PoS tagging. We conducted additional experiments on German and French without lemmatisation, tested on the trial data. Results immediately fell below baseline. Another main factor is the quality of the word alignments, and the degree to which the found word alignments correspond with the translations the human annotators could choose from in preparing the gold standard. An idea we tested is, instead of relying on the mere intersection of word alignments, to use a phrase-translation table generated by and for the Statistical Machine Translation system Moses (Koehn et al., 2007), which uses the grow-diag-final heuristic to extract phrase pairs. This results in more phrases, and whilst this is a good idea for MT, in the current task it has a detrimental effect, as it creates too many translation options and we do not have an MT decoder to discard ineffective options in this task. The grow-diag-final heuristic incorporates unaligned words to the end of a translation in the trans186 lation option, a bad idea for CLWSD. 5 Conclusion In this study we have taken parameter optimisation one step further compared to our previous research (van Gompel, 2010), namely by selectin"
S13-2033,2005.mtsummit-papers.11,0,0.0240428,"Missing"
S13-2033,S10-1003,0,0.111541,"winning scores for four of them when asked to predict the best translation(s). We tested various configurations of our system, focusing on various levels of hyperparameter optimisation and feature selection. Our final results indicate that hyperparameter optimisation did not lead to the best results, indicating overfitting by our optimisation method in this aspect. Feature selection does have a modest positive impact. 1 2 Introduction WSD2 is a rewrite and extension of our previous system (van Gompel, 2010) that participated in the Cross-Lingual Word Sense Disambiguation task in SemEval 2010 (Lefever and Hoste, 2010). In WSD2 we introduce and test a new level of hyperparameter optimisation. Unlike the previous occasion, we participate in all five target languages (Dutch, Spanish, Italian, French, and German). The task presents twenty polysemous nouns with fifty instances each to be mapped onto normalised (lemmatised) translations in all languages. The task is described in detail by Lefever and Hoste (2013). Trial data is provided and has been used to optimise system parameters. Due to the unsupervised System Description The WSD2 system, like its predecessor, distributes the task over word experts. Each wo"
S13-2033,S13-2029,0,0.117883,"rs preparing the test data could have never picked. Systems may output several senses per instance, rather than producing just one sense prediction. These are evaluated in two different ways. The scoring type “best” expects that the system outputs the sense it considers the most likely, or a number of senses in the order of its confidence in these senses being correct. Multiple guesses are penalised, however. In contrast, the scoring type “out of five” expects five guesses, in which each answer carries the same weight. These metrics are more extensively described in Mihalcea et al. (2010) and Lefever and Hoste (2013). We present our system WSD2 which participated in the Cross-Lingual Word-Sense Disambiguation task for SemEval 2013 (Lefever and Hoste, 2013). The system closely resembles our winning system for the same task in SemEval 2010. It is based on k-nearest neighbour classifiers which map words with local and global context features onto their translation, i.e. their cross-lingual sense. The system participated in the task for all five languages and obtained winning scores for four of them when asked to predict the best translation(s). We tested various configurations of our system, focusing on vari"
S13-2033,P96-1006,0,0.380361,"acted; these are a set of keywords per lemma and per translation which are found occurring above certain occurrence thresholds at arbitrary positions in the same sentence, as this is the widest context supplied in the task data. The global context features are represented as a binary bag-of-words model in which the presence of each of the keywords that may be indicative for a given mapping of the focus word to a sense is represented by a boolean value. Such a set of keywords is constructed for each of the twenty nouns, per language. The method used to extract these keywords (k) is proposed by Ng and Lee (1996) and used also by Hoste et al. (2002). Assume we have a focus word f , more precisely, a lemma of one of the target nouns. We also have one of its aligned translations/senses s, also a lemma. We can now estimate P (s|k), the probability of sense s, given a keyword k. Let Ns,klocal . be the number of occurrences of a possible local context word k with particular focus word lemma-PoS combination and with a particular 184 sense s. Let Nklocal be the number of occurrences of a possible local context keyword k with a particular focus word-PoS combination regardless of its sense. If we also take int"
S13-2033,S13-2031,0,0.163603,"training data. Therefore a fourth configuration (varN) was tried later to independently assess the idea of feature selection, without hyperparameter optimisation on the classifiers. This proves to be a good idea. However, the fourth configuration was not yet available for the actual competition. This incidentally would have had no impact on the final ranking between competitors. When we run these systems on the actual test data of the shared task, we obtain the results in Table 3. The best score amongst the other competitors is mentioned in the last row for reference, this is the HLTDI team (Rudnick et al., 2013) for all but Best-Spanish, which goes to the NRC contribution (Carpuat, 2013). BEST baseline c1l c1lN var varN best.comp OUT-OF-5 baseline c1l c1lN var varN best.comp ES 23.23 28.40 28.65 23.3 29.05 32.16 ES 53.07 58.23 57.62 55.70 58.61 61.69 FR 25.74 29.88 30.11 25.89 30.15 28.23 FR 51.36 59.07 59.80 59.19 59.26 58.20 IT 20.21 25.43 25.66 20.38 24.90 24.62 IT 42.63 52.22 52.73 51.18 50.89 53.57 NL 20.66 23.14 23.61 17.17 23.57 22.36 NL 43.59 47.83 47.62 46.85 50.42 46.55 DE 17.42 20.70 20.82 16.2 21.98 19.92 DE 38.86 43.17 43.24 41.46 43.34 43.66 Table 3: Results on the test set A major fact"
S13-2033,S10-1053,1,0.894937,"Missing"
S13-2033,W09-2413,0,\N,Missing
S13-2033,W09-2412,0,\N,Missing
S13-2033,S10-1002,0,\N,Missing
S14-2005,S13-2029,1,0.709155,"y, to ensure all fragments are correct and sensible, a manual selection from this • Input (L1=French,L2=English): “I rentre a` la maison because I am tired.” Desired output: “I return home because I am tired.” • Input (L1=Dutch, L2=English): “Workers are facing a massive aanval op their employment and social rights.” Desired output: “Workers are facing a massive attack on their employment and social rights.” The task can be related to two tasks that were offered in previous years of SemEval: Lexical Substitution (Mihalcea et al., 2010) and most notably Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2013). When comparing our task to the Cross-Lingual Word-Sense Disambiguation task, one notable difference is the fact that our task concerns not just words, but also phrases. Another essential difference is the nature of the context; our context is in L2 instead of L1. Unlike the Cross-Lingual Word Sense Disambiguation task, we do not constrain the L1 words or phrases that may be used for translation, except for a maximum length which we set to 6 tokens, whereas Lefever and Hoste (2013) only tested a select number of nouns. Our task emphasizes a correct meaning-preserving choice 37 site1 . We crea"
S14-2005,S10-1002,0,0.0325185,"k to L1 in an L2 context. The full L2 sentence acts as reference sentence. Finally, to ensure all fragments are correct and sensible, a manual selection from this • Input (L1=French,L2=English): “I rentre a` la maison because I am tired.” Desired output: “I return home because I am tired.” • Input (L1=Dutch, L2=English): “Workers are facing a massive aanval op their employment and social rights.” Desired output: “Workers are facing a massive attack on their employment and social rights.” The task can be related to two tasks that were offered in previous years of SemEval: Lexical Substitution (Mihalcea et al., 2010) and most notably Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2013). When comparing our task to the Cross-Lingual Word-Sense Disambiguation task, one notable difference is the fact that our task concerns not just words, but also phrases. Another essential difference is the nature of the context; our context is in L2 instead of L1. Unlike the Cross-Lingual Word Sense Disambiguation task, we do not constrain the L1 words or phrases that may be used for translation, except for a maximum length which we set to 6 tokens, whereas Lefever and Hoste (2013) only tested a select number o"
S14-2005,S14-2030,0,0.0733812,"nd Nanyang Technological University (Singapore) – all language pairs 6. TeamZ - Anubhav Gupta - Universit´e de Franche-Comt´e (France) – English-Spanish, English-German Participants implemented distinct methodologies and implementations. One obvious avenue of tackling the problem is through standard Statistical Machine Translation (SMT). The CNRC team takes a pure SMT approach with few modifications. They employ their own Portage decoder and directly send an L1 fragment in an L2 context, corresponding to a partial translation hypothesis with only one fragment left to decode, to their decoder (Goutte et al., 2014). The UEdin team applies a similar method using the Moses decoder, marking the L2 context so that the decoder leaves this context as is. In addition they add a context similarity feature for every phrase pair in the phrase translation table, which expresses topical similarity with the test context. In order to properly decode, the phrase table is filtered per test sentence (Hasler, 2014). The IUCL and UNAL teams do make use of the information from word alignments or phrase translation tables, but do not use a standard SMT decoder. The IUCL system combines various information sources in a log-l"
S14-2005,S14-2110,0,0.0209095,"uesses. Only the best match is counted. This metric does not count how many of the five are valid. Participants could submit up to three runs per language pair and evaluation type. 5 Participants Six teams submitted systems, three of which participated for all language pairs. In alphabetic order, these are: 1. CNRC - Cyril Goutte, Michel Simard, Marine Carpuat - National Research Council Canada – All language pairs 39 by attempting to emulate the manual post-editing process human translators employ to correct MT output (Tan et al., 2014), whereas TeamZ relies on Wiktionary as the sole source (Gupta, 2014). 6 higher. These scores can be viewed on the result website at http://github.com/proycon/ semeval2014task5/. The result website also holds the system output and evaluation scripts with which all graphs and tables can be reproduced. We observe that the best scoring team in the task (UEdin), as well as the CNRC team, both employ standard Statistical Machine Translation and achieve high results. From this we can conclude that standard SMT techniques are suitable for this task. Teams IUCL and UNAL achieve similarly good results, building on word and phrase alignment data as does SMT, yet not usin"
S14-2005,S14-2060,0,0.037367,"Missing"
S14-2005,S14-2123,0,0.0228464,"tions. They employ their own Portage decoder and directly send an L1 fragment in an L2 context, corresponding to a partial translation hypothesis with only one fragment left to decode, to their decoder (Goutte et al., 2014). The UEdin team applies a similar method using the Moses decoder, marking the L2 context so that the decoder leaves this context as is. In addition they add a context similarity feature for every phrase pair in the phrase translation table, which expresses topical similarity with the test context. In order to properly decode, the phrase table is filtered per test sentence (Hasler, 2014). The IUCL and UNAL teams do make use of the information from word alignments or phrase translation tables, but do not use a standard SMT decoder. The IUCL system combines various information sources in a log-linear model: phrase table, L2 Language Model, Multilingual Dictionary, and a dependency-based collocation model, although this latter source was not finished in time for the system submission (Rudnick et al., 2014). The UNAL system extracts syntactic features as a means to relate L1 fragments with L2 context to their L2 fragment translations, and uses memorybased classifiers to achieve t"
S14-2005,S14-2132,0,0.0206542,"IUCL and UNAL teams do make use of the information from word alignments or phrase translation tables, but do not use a standard SMT decoder. The IUCL system combines various information sources in a log-linear model: phrase table, L2 Language Model, Multilingual Dictionary, and a dependency-based collocation model, although this latter source was not finished in time for the system submission (Rudnick et al., 2014). The UNAL system extracts syntactic features as a means to relate L1 fragments with L2 context to their L2 fragment translations, and uses memorybased classifiers to achieve this (Silva-Schlenker et al., 2014). The two systems on the lower end of the result spectrum use different techniques altogether. The Sensible team approaches the problem • Best - The system may only output one, its best, translation; • Out of Five - The system may output up to five alternatives, effectively allowing 5 guesses. Only the best match is counted. This metric does not count how many of the five are valid. Participants could submit up to three runs per language pair and evaluation type. 5 Participants Six teams submitted systems, three of which participated for all language pairs. In alphabetic order, these are: 1. C"
S14-2005,P07-2045,0,0.0150004,"Missing"
S14-2005,S14-2094,0,0.0289288,"- The system may output up to five alternatives, effectively allowing 5 guesses. Only the best match is counted. This metric does not count how many of the five are valid. Participants could submit up to three runs per language pair and evaluation type. 5 Participants Six teams submitted systems, three of which participated for all language pairs. In alphabetic order, these are: 1. CNRC - Cyril Goutte, Michel Simard, Marine Carpuat - National Research Council Canada – All language pairs 39 by attempting to emulate the manual post-editing process human translators employ to correct MT output (Tan et al., 2014), whereas TeamZ relies on Wiktionary as the sole source (Gupta, 2014). 6 higher. These scores can be viewed on the result website at http://github.com/proycon/ semeval2014task5/. The result website also holds the system output and evaluation scripts with which all graphs and tables can be reproduced. We observe that the best scoring team in the task (UEdin), as well as the CNRC team, both employ standard Statistical Machine Translation and achieve high results. From this we can conclude that standard SMT techniques are suitable for this task. Teams IUCL and UNAL achieve similarly good results,"
S14-2005,P14-1082,1,0.682767,"Missing"
S14-2005,2005.mtsummit-papers.11,0,0.00919859,"d in a simple XML format that explicitly marks the fragments. System output of participants adheres to the same format. The trial set, released early on in the task, was used by participants to develop and tune their systems on. The test set corresponds to the final data released for the evaluation period; the final evaluation was conducted on this data. The trial data was constructed in an automated fashion in the way described in our pilot study (van Gompel and van den Bosch, 2014). First a phrase-translation table is constructed from a parallel corpus. We used the Europarl parallel corpus (Koehn, 2005) and the Moses tools (Koehn et al., 2007), which in turn makes use of GIZA++ (Och and Ney, 2000). Only strong phrase pairs (exceeding a set threshold) were retained and weaker ones were pruned. This phrase-translation table was then used to create input sentences in which the L2 fragments are swapped for their L1 counterparts, effectively mimicking a fall-back to L1 in an L2 context. The full L2 sentence acts as reference sentence. Finally, to ensure all fragments are correct and sensible, a manual selection from this • Input (L1=French,L2=English): “I rentre a` la maison because I am tired.”"
sporleder-etal-2006-identifying,A97-1028,0,\N,Missing
sporleder-etal-2006-identifying,E99-1001,0,\N,Missing
sporleder-etal-2006-identifying,W02-1017,0,\N,Missing
sporleder-etal-2006-identifying,W03-0425,0,\N,Missing
sporleder-etal-2006-identifying,W03-1505,0,\N,Missing
sporleder-etal-2006-identifying,buchholz-van-den-bosch-2000-integrating,1,\N,Missing
sporleder-etal-2006-identifying,W03-0419,0,\N,Missing
sporleder-etal-2006-identifying,W99-0613,0,\N,Missing
sporleder-etal-2006-identifying,W99-0612,0,\N,Missing
van-den-bosch-etal-2006-transferring,W96-0102,0,\N,Missing
van-den-bosch-etal-2006-transferring,zavrel-daelemans-2000-bootstrapping,0,\N,Missing
van-den-bosch-etal-2006-transferring,P03-1062,1,\N,Missing
van-den-bosch-etal-2006-transferring,J95-4004,0,\N,Missing
van-den-bosch-etal-2006-transferring,J01-2002,0,\N,Missing
vossen-etal-2012-dutchsemcor,gorog-vossen-2010-computer,1,\N,Missing
vossen-etal-2012-dutchsemcor,S10-1053,0,\N,Missing
vossen-etal-2012-dutchsemcor,N06-1016,0,\N,Missing
vossen-etal-2012-dutchsemcor,D07-1082,0,\N,Missing
vossen-etal-2012-dutchsemcor,S10-1013,1,\N,Missing
vossen-etal-2012-dutchsemcor,E09-1005,0,\N,Missing
vossen-etal-2012-dutchsemcor,H93-1061,0,\N,Missing
vossen-etal-2012-dutchsemcor,J07-4005,0,\N,Missing
vossen-etal-2012-dutchsemcor,oostdijk-etal-2008-coi,0,\N,Missing
vossen-etal-2012-dutchsemcor,P04-1075,0,\N,Missing
vossen-etal-2012-dutchsemcor,P96-1006,0,\N,Missing
vossen-etal-2012-dutchsemcor,W04-0827,1,\N,Missing
vossen-etal-2012-dutchsemcor,vossen-etal-2008-integrating,1,\N,Missing
W00-0713,W95-0107,0,0.0364087,"Missing"
W00-0713,H94-1048,0,0.042129,"Missing"
W00-0713,W97-0301,0,0.0130521,"rrent issue, e.g. RIPPER&apos;s ability to represent sets of values at left-hand side conditions, and its flexibility in producing larger or smaller numbers of rules. Second, other rule induction algorithms exist that may play RIPPER&apos;S role, such as C4.5RULES (Quinlan, 1993). More generally, further research should focus on the scaling properties of the approach (including the scaling of the external ruleinduction algorithm), should investigate more and larger language data sets, and should seek comparisons with other existing methods that claim to handle complex features efficiently (Brill, 1993; Ratnaparkhi, 1997; Roth, 1998; Brants, 2000). GPSM causes a slight drop in performance - and a slowdown. 4 IBI-IG Discussion Representing instances by complex features that have been induced by a rule induction algorithm appears, in view of the measured accuracies, a viable alternative approach to using rules, as compared to standard rule induction. This result is in line with results reported by Domingos on the RISE algorithm (Domingos, 1995; Domingos, 1996). A marked difference is that in RISE, the rules are the instances in kNN classification (and due to the careful general±sat±on strategy of RISE, they can"
W00-0713,W97-1016,0,0.035592,"Missing"
W00-0713,A00-1031,0,0.00991924,"ity to represent sets of values at left-hand side conditions, and its flexibility in producing larger or smaller numbers of rules. Second, other rule induction algorithms exist that may play RIPPER&apos;S role, such as C4.5RULES (Quinlan, 1993). More generally, further research should focus on the scaling properties of the approach (including the scaling of the external ruleinduction algorithm), should investigate more and larger language data sets, and should seek comparisons with other existing methods that claim to handle complex features efficiently (Brill, 1993; Ratnaparkhi, 1997; Roth, 1998; Brants, 2000). GPSM causes a slight drop in performance - and a slowdown. 4 IBI-IG Discussion Representing instances by complex features that have been induced by a rule induction algorithm appears, in view of the measured accuracies, a viable alternative approach to using rules, as compared to standard rule induction. This result is in line with results reported by Domingos on the RISE algorithm (Domingos, 1995; Domingos, 1996). A marked difference is that in RISE, the rules are the instances in kNN classification (and due to the careful general±sat±on strategy of RISE, they can be very instance-specific)"
W00-0713,W95-0103,0,0.0343381,"Missing"
W00-0713,W96-0102,0,0.0907032,"Missing"
W00-0735,W99-0629,1,0.875306,"Missing"
W00-0735,J93-2004,0,\N,Missing
W02-0809,S01-1003,1,\N,Missing
W02-0809,J01-3001,0,\N,Missing
W02-0809,P97-1056,1,\N,Missing
W02-0809,W02-0814,1,\N,Missing
W02-0809,P96-1006,0,\N,Missing
W02-0814,P99-1037,1,0.884605,"Missing"
W02-0814,P01-1005,0,0.0641157,"Missing"
W02-0814,W96-0102,1,0.832954,"Missing"
W02-0814,S01-1020,1,0.844583,"Missing"
W02-0814,J98-1001,0,0.0345256,"Missing"
W02-0814,P96-1006,0,0.0730626,"polysemy and sense distributions. Iris Hendrickx and Antal van den Bosch ILK Computational Linguistics Tilburg University, The Netherlands I.H.E.Hendrickx,antalb @kub.nl  1 Introduction The task of word sense disambiguation (WSD) is to assign a sense label to a word in context. Both knowledge-based and statistical methods have been applied to the problem. See (Ide and V´eronis, 1998) for an introduction to the area. Recently (both S ENSEVAL competitions), various machine learning (ML) approaches have been demonstrated to produce relatively successful WSD systems, e.g. memory-based learning (Ng and Lee, 1996; Veenstra et al., 2000), decision lists (Yarowsky, 2000), boosting (Escudero et al., 2000). In this paper, we evaluate the results of a memorybased learning approach to WSD. We ask ourselves whether we can learn lessons from the errors made in the S ENSEVAL -2 competition. More particularly, we are interested whether there are words or categories of words which are more difficult to predict than other words. If so, do these words have certain characteristic features? We furthermore investigate the interaction between the use of different information sources and the part-of-speech categories o"
W02-0814,J01-3001,0,0.0788235,"Missing"
W02-0814,1995.iwpt-1.8,0,\N,Missing
W02-0814,J95-4002,0,\N,Missing
W02-0814,C00-2098,0,\N,Missing
W02-0814,H94-1020,0,\N,Missing
W02-0814,P00-1058,0,\N,Missing
W02-0814,P90-1035,0,\N,Missing
W02-0814,P83-1017,0,\N,Missing
W02-0814,P98-2156,0,\N,Missing
W02-0814,C98-2151,0,\N,Missing
W03-0427,buchholz-van-den-bosch-2000-integrating,1,0.898249,"Missing"
W03-0427,W02-2004,0,0.0192722,"the final test set of 78.20 on English and 63.02 on German. 2 Data and features The CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003) supplied datasets in two languages, English and German, using four named entity categories: persons, organisations, locations, and “miscellany names”. Manual annotation has been performed at the University of Antwerp. Apart from tokenized wordforms, the data provides predicted PoS-tags and chunks. Additionally we computed the following features with each wordform, largely following those used by the bestperforming submission of the 2002 shared task (Carreras et al., 2002): • Orthographic features represented as binary features: Begin cap, All caps, Internal cap, Contains digit, Contains digit en alpha, Initial, Lower case, First word • The wordform’s first letter and last three letters (as three separate features) • The direct output of the memory-based lemmatizer (Van den Bosch and Daelemans, 1999), providing PoS tag, morphological features, and spelling change information • PoS tag from a slow but accurate version of the memory-based tagger trained on a portion of the British National Corpus, according to the CLAWS-5 tagset (for English data only) For exampl"
W03-0427,W99-0612,0,0.015855,"imilarities between instances using these extra features, it is able, in principle, to recognise and correct reoccurring patterns of errors within sub-sentential sequences. This could correct errors made due to the “blindness” of the first-stage classifier, which is unaware of its own classifications left or right of the wordform in the current focus position. We used stacking on top of the first extension. 4.3 Unannotated data For both languages a large unannotated dataset was made available for extracting data or information. Alternative to using this data to expand or bootstrap seed lists (Cucerzan and Yarowsky, 1999; Buchholz and Van den Bosch, 2000), we use the unannotated corpus to select useful instances to be added directly to the training set. Not unlike (Yarowsky, 1995) we use confidence of our classifier on unannotated data to enrich itself; that is, by adding confidently-classified instances to the memory. We make the simple assumption that entropy in the class distribution in the nearest neighbour set computed in the classification of a new instance is correlated with the reliability of the classification, when k &gt; 1. When k nearest neighbours all vote for the same class, the entropy of that cla"
W03-0427,W03-0419,0,0.0772698,"Missing"
W03-0427,P99-1037,1,0.85853,"Missing"
W03-0427,P95-1026,0,0.0280531,"is could correct errors made due to the “blindness” of the first-stage classifier, which is unaware of its own classifications left or right of the wordform in the current focus position. We used stacking on top of the first extension. 4.3 Unannotated data For both languages a large unannotated dataset was made available for extracting data or information. Alternative to using this data to expand or bootstrap seed lists (Cucerzan and Yarowsky, 1999; Buchholz and Van den Bosch, 2000), we use the unannotated corpus to select useful instances to be added directly to the training set. Not unlike (Yarowsky, 1995) we use confidence of our classifier on unannotated data to enrich itself; that is, by adding confidently-classified instances to the memory. We make the simple assumption that entropy in the class distribution in the nearest neighbour set computed in the classification of a new instance is correlated with the reliability of the classification, when k &gt; 1. When k nearest neighbours all vote for the same class, the entropy of that class vote is 0.0. Alternatively, when the votes tie, the entropy is maximal. A secondary heuristic assumption is that it is probably not useful to add (almost) exact"
W03-2710,N01-1027,0,\N,Missing
W03-2710,P95-1016,0,\N,Missing
W03-2710,P01-1012,1,\N,Missing
W03-2710,P99-1030,0,\N,Missing
W03-2710,J00-3003,0,\N,Missing
W03-2710,W02-0213,0,\N,Missing
W03-2710,P99-1026,0,\N,Missing
W03-2710,P98-2188,0,\N,Missing
W03-2710,C98-2183,0,\N,Missing
W04-0827,C02-1039,0,0.0184143,"and value weighting methods, different neighborhood size and weighting parameters, etc., that should be optimized for each word expert independently. See (Daelemans et al., 2003b) for more information. It has been claimed, e.g. in (Daelemans et al., 1999), that lazy learning has the right bias for learning natural language processing tasks as it makes possible learning from atypical and low-frequency events that are usually discarded by eager learning methods. Architecture. Previous work on memory-based WSD includes work from Ng and Lee (1996), Veenstra et al. (2000), Hoste et al. (2002) and Mihalcea (2002). The current design of our WSD system is largely based on Hoste et al. (2002). Figure 1 gives an overview of the design of our WSD system: the training text is first linguistically analyzed. For each word-lemma–POS-tag combination, we check if it (i) is in our sense lexicon, (ii) has more than one sense and (iii) has a frequency in the training text above a certain threshold. For all combinations matching these three conditions, we train a word expert module. To all combinations with only one sense, or with more senses and a frequency below the threshold, we assign the default sense, which is"
W04-0827,P96-1006,0,0.452921,"ice between different statistical and information-theoretic feature and value weighting methods, different neighborhood size and weighting parameters, etc., that should be optimized for each word expert independently. See (Daelemans et al., 2003b) for more information. It has been claimed, e.g. in (Daelemans et al., 1999), that lazy learning has the right bias for learning natural language processing tasks as it makes possible learning from atypical and low-frequency events that are usually discarded by eager learning methods. Architecture. Previous work on memory-based WSD includes work from Ng and Lee (1996), Veenstra et al. (2000), Hoste et al. (2002) and Mihalcea (2002). The current design of our WSD system is largely based on Hoste et al. (2002). Figure 1 gives an overview of the design of our WSD system: the training text is first linguistically analyzed. For each word-lemma–POS-tag combination, we check if it (i) is in our sense lexicon, (ii) has more than one sense and (iii) has a frequency in the training text above a certain threshold. For all combinations matching these three conditions, we train a word expert module. To all combinations with only one sense, or with more senses and a fre"
W04-2414,W99-0629,1,0.820692,"role labeling: Optimizing features, algorithm, and output Antal van den Bosch, Sander Canisius, Walter Daelemans, Iris Hendrickx Erik Tjong Kim Sang ILK / Computational Linguistics CNTS / Department of Linguistics Tilburg University, P.O. Box 90153, University of Antwerp, Universiteitsplein 1, NL-5000 LE Tilburg, The Netherlands B-2610 Antwerpen, Belgium {Antal.vdnBosch,S.V.M.Canisius, {Walter.Daelemans, I.H.E.Hendrickx}@uvt.nl 1 Introduction In this paper we interpret the semantic role labeling problem as a classification task, and apply memory-based learning to it in an approach similar to Buchholz et al. (1999) and Buchholz (2002) for grammatical relation labeling. We apply feature selection and algorithm parameter optimization strategies to our learner. In addition, we investigate the effect of two innovations: (i) the use of sequences of classes as classification output, combined with a simple voting mechanism, and (ii) the use of iterative classifier stacking which takes as input the original features and a pattern of outputs of a first-stage classifier. Our claim is that both methods avoid errors in sequences of predictions typically made by simple classifiers that are unaware of their previous"
W04-2414,W04-2412,0,0.0763194,"Missing"
W05-0611,W02-2004,0,0.0774683,"Missing"
W05-0611,W96-0102,1,0.827984,"lier. This well-known problem has triggered at least the following three main types of solutions. Feedback loop Each training or test example may represent not only the regular windowed input, but also a copy of previously made classifications, to allow the classifier to be more consistent with its previous decisions. Direct feedback loops that copy a predicted output label to the input representation of the next example have been used in symbolic machine-learning architectures such as the the maximum-entropy tagger described by Ratnaparkhi (1996) and the memory-based tagger (MBT) proposed by Daelemans et al. (1996). This solution assumes that processing is directed, e.g. from left to right. A noted problem of this approach is the label bias problem (Lafferty et al., 2001), which is that a feedback-loop classifier may be driven to be consistent with its previous decision also in the case this decision was wrong; sequences of errors may result. Stacking, boosting, and voting The partly incorrect concatenated output sequence of a single classifier may serve as input to a second-stage classifier in a stacking architecture, a common machine-learning optimization technique (Wolpert, 1992). Although less elega"
W05-0611,W03-0425,0,0.0607795,"Missing"
W05-0611,W95-0107,0,0.073809,"semantic nature: English base phrase chunking (henceforth CHUNK), English named-entity recognition (NER), and disfluency chunking in transcribed spoken Dutch utterances (DISFL). C HUNK is the task of splitting sentences into non-overlapping syntactic phrases or constituents. The used data set, extracted from the WSJ Penn Treebank, contains 211,727 training examples and 47,377 test instances. The examples represent seven-word windows of words and their respective (predicted) part-of-speech tags, and each example is labeled with a class using the IOB type of segmentation coding as introduced by Ramshaw and Marcus (1995), marking whether the middle word is inside (I), outside (O), or at the beginning (B) of a chunk. Words occuring less than ten times in the training material are attenuated (converted into a more general string that retains some of the word’s surface form). Generalization performance is measured by the F-score on correctly identified and labeled constituents in test data, using the evaluation method originally used in the “shared task” subevent of the CoNLL-2000 conference (Tjong Kim Sang and Buchholz, 2000) in which this particular training and test set were used. An example sentence with bas"
W05-0611,N03-1028,0,0.0984263,"Missing"
W05-0611,W00-0726,0,0.0475029,"le is labeled with a class using the IOB type of segmentation coding as introduced by Ramshaw and Marcus (1995), marking whether the middle word is inside (I), outside (O), or at the beginning (B) of a chunk. Words occuring less than ten times in the training material are attenuated (converted into a more general string that retains some of the word’s surface form). Generalization performance is measured by the F-score on correctly identified and labeled constituents in test data, using the evaluation method originally used in the “shared task” subevent of the CoNLL-2000 conference (Tjong Kim Sang and Buchholz, 2000) in which this particular training and test set were used. An example sentence with base phrases marked and labeled is the following: [He]N P [reckons]V P [the current account deficit]N P [will narrow]V P [to]P P [only $ 1.8 billion]N P [in]P P [September]N P . 82 N ER, named-entity recognition, is to recognize and type named entities in text. We employ the English NER shared task data set used in the CoNLL2003 conference, again using the same evaluation method as originally used in the shared task (Tjong Kim Sang and De Meulder, 2003). This data set discriminates four name types: persons, org"
W05-0611,W03-0419,0,0.0324592,"Missing"
W05-0611,A00-2007,0,0.0555511,"Missing"
W05-0637,W05-0620,0,0.130271,"Missing"
W05-0637,J02-3001,0,0.369891,"Missing"
W05-0637,P03-1004,0,0.0299625,".1 Machine learning The core machine learning technique employed, is memory-based learning, a supervised inductive algorithm for learning classification tasks based on the k-nn algorithm. We use the TiMBL system (Daelemans et al., 2003), version 5.0.0, patch-2 with uniform feature weighting and random tiebreaking (options: -w 0 -R 911). We have also evaluated two alternative learning techniques. First, Maximum Entropy Models, for which we employed Zhang Le’s Maximum Entropy Toolkit, version 20041229 with default parameters. Second, Support Vector Machines for which we used Taku Kudo’s YamCha (Kudo and Matsumoto, 2003), with one-versus-all voting and option -V which enabled us to ignore predicted classes with negative distances. 230 3.2 Feature selection In previous research, we have found that memorybased learning is rather sensitive to the chosen features. In particular, irrelevant or redundant features may lead to reduced performance. In order to minimise the effects of this sensitivity, we have employed bi-directional hill-climbing (Caruana and Freitag, 1994) for finding the features that were most suited for this task. This process starts with an empty feature set, examines the effect of adding or remo"
W05-0637,N04-1030,0,0.0907971,"Missing"
W05-0637,W04-2414,1,0.545326,"Missing"
W05-0637,W04-3212,0,0.106377,"Missing"
W05-0701,P98-1018,0,0.0500889,"Missing"
W05-0701,A00-2012,1,0.929171,"Missing"
W05-0701,W96-0102,0,0.122978,"in fact be correct alternatives. Since instances are generated for each type rather than for each token in the data, the effect of token frequency on classification is lost. For example, instances from frequent tokens are more likely to occur in the k-NN set, and therefore their (partial) analyses will show up more frequently. This is an issue to explore in future work. Depending on the application, it may also make sense to optimize on the correct prediction of unkown words, or on increasing only the recall. 4 Part-of-speech tagging We employ MBT, a memory-based tagger-generator and tagger (Daelemans et al., 1996) to produce a part-of-speech (PoS) tagger based on the ATB 1 corpus2 . We first describe how we prepared the corpus data. We then describe how we generated the tagger (a two-module tagger with a module for known words and one for unknown words), and subsequently we report on the accuracies obtained on test material by the generated tagger. We conclude this 2 In our experiments we used the MBT software package, version 2 (Daelemans et al., 2003), available from http://ilk.uvt.nl/. w bdA styfn knt nHylA jdA , AlA >n ... CONJ VERB_PERFECT NOUN_PROP NOUN_PROP ADJ+NSUFF_MASC_SG_ACC_INDEF ADV PUNC A"
W05-0701,W04-3246,0,0.0434305,"Missing"
W05-0701,N04-4038,0,0.0808253,"Missing"
W05-0701,E87-1002,0,0.365437,"Missing"
W05-0701,C94-1029,0,0.0737791,"Missing"
W05-0701,P99-1037,1,0.919473,"Missing"
W05-0701,C98-1018,0,\N,Missing
W06-2602,W95-0107,0,0.0334661,"method, and the trigram method combined both with majority voting, and with constraint satisfaction inference. The last column shows the performance of the (hypothetical) oracle inference procedure. spelling changes due to compounding, derivation, or inflection that would enable the reconstruction of the appropriate root forms of the involved morphemes. For CHUNK, and the three information extraction tasks, instances represent a seven-token window of words and their (predicted) part-of-speech tags. Each token is labelled with a class using the IOB type of segmentation coding as introduced by Ramshaw and Marcus (1995), marking whether the middle word is inside (I), outside (O), or at the beginning (B) of a chunk, or named entity. Performance is measured by the F-score on correctly identified and labelled chunks, or named entities. Instances for PHONEME, and MORPHO consist of a seven-letter window of letters only. The labels assigned to an instance are task-specific and have been introduced above, together with the tasks themselves. Generalisation performance is measured on the word accuracy level: if the entire phonological transcription of the word is predicted correctly, or if all three aspects of the mo"
W06-2602,W00-0726,0,0.0385326,"ing tasks, we composed a benchmark set consisting of six different tasks, covering four areas in natural language processing: syntax (syntactic chunking), morphology (morphological analysis), phonology (grapheme-to-phoneme conversion), and information extraction (general, medical, and biomedical named-entity recognition). Below, the six data sets used for these tasks are introduced briefly. C HUNK is the task of splitting sentences into non-overlapping syntactic phrases or constituents. The data set, extracted from the WSJ Penn Treebank, and first used in the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000), contains 211,727 training examples and 47,377 test instances. N ER, named-entity recognition, involves identifying and labelling named entities in text. We employ the English NER shared task data set used in the CoNLL-2003 conference (Tjong Kim Sang and De Meulder, 2003). This data set discriminates four name types: persons, organisations, locations, and a rest category of “miscellany names”. The data set is a collection of newswire 12 articles from the Reuters Corpus, RCV11 . The given training set contains 203,621 examples; as test set we use the “testb” evaluation set which contains 46,43"
W06-2602,W03-0419,0,0.0388559,"Missing"
W06-2602,W05-0611,1,0.795576,"Missing"
W06-2924,W06-2920,0,0.0662995,"E Tilburg, The Netherlands NL-1098 SJ Amsterdam, The Netherlands {S.V.M.Canisius,A.M.Bogers, erikt@science.uva.nl Antal.vdnBosch,J.Geertzen}@uvt.nl 1 Introduction 2 As more and more syntactically-annotated corpora become available for a wide variety of languages, machine learning approaches to parsing gain interest as a means of developing parsers without having to repeat some of the labor-intensive and languagespecific activities required for traditional parser development, such as manual grammar engineering, for each new language. The CoNLL-X shared task on multi-lingual dependency parsing (Buchholz et al., 2006) aims to evaluate and advance the state-ofthe-art in machine learning-based dependency parsing by providing a standard benchmark set comprising thirteen languages1 . In this paper, we describe two different machine learning approaches to the CoNLL-X shared task. Before introducing the two learning-based approaches, we first describe a number of baselines, which provide simple reference scores giving some sense of the difficulty of each language. Next, we present two machine learning systems: 1) an approach that directly predicts all dependency relations in a single run over the input sentence,"
W06-2924,W03-3023,0,0.186072,"Missing"
W06-2924,W03-2403,0,\N,Missing
W06-2924,dzeroski-etal-2006-towards,0,\N,Missing
W06-2924,W03-2405,0,\N,Missing
W06-2924,afonso-etal-2002-floresta,0,\N,Missing
W06-3206,W04-3246,0,0.0182711,"ord such as being as a prefix followed by an inflection, making the locally most likely guesses, it generates an analysis that could never exist, since it lacks a stem. Global models that coordinate, mediate, or enforce that the output is a valid sequence are typically formulated in the form of linguistic rules, applied during processing or in post-processing, that constrain the space of possible output sequences. Some present-day research in machine learning of morpho-phonology indeed focuses on satisfying linguistically-motivated constraints as a postprocessing or filtering step; e.g., see (Daya et al., 2004) on identifying roots in Hebrew word forms. Optimality Theory (Prince and Smolensky, 2004) can also be seen as a constraint-based approach to language processing based on linguistically motivated constraints. In contrast to being motivated by linguistic theory, constraints in a global model can be learned automatically from data as well. In this paper we propose such a data-driven constraint satisfaction inference method, that finds a globally appropriate output sequence on the basis of a space of possible sequences generated by a locally-operating classifier predicting output subsequences. We"
W06-3206,W05-0616,0,0.0133827,"ange of n-gram widths exceeding the current trigrams. Preliminary results suggest that the method retains a positive effect over the baseline with n &gt; 3, but it does not outperform the n = 3 case. We also intend to test the method with a range of different machine learning methods, since as we noted before the constraintsatisfaction inference method and its underlying ngram output subsequence classification method can be applied to any machine learning classification algorithm in principle, as is already supported by preliminary work in this direction. Also, we plan comparisons to the work of Stroppa and Yvon (2005) and Damper and Eastmond (1997) on sequence-global analogy-based models for morpho-phonological processing, since the main difference between this related work and ours is that both alternatives are based on working units of variable width, rather than our fixedwidth n-grams, and also their analogical reasoning is based on interestingly different principles than our k-nearest neighbor classification rule, such as the use of analogical proportions by Stroppa and Yvon (2005). Acknowledgements This research was funded by NWO, the Netherlands Organization for Scientific Research, as part of the IM"
W06-3206,E93-1007,1,0.705216,"Missing"
W06-3206,P99-1037,1,0.85824,"Missing"
W06-3206,W05-0611,1,0.810894,"Missing"
W06-3604,P01-1005,0,0.220327,"ion as the ultimate confusable disambiguation Antal van den Bosch ILK / Dept. of Language and Information Science, Tilburg University P.O. Box 90153, NL-5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl Abstract (Even-Zohar and Roth, 2000). It is usually not an engineering end in itself to predict the next word in a sequence, or fill in a blanked-out word in a sequence. Yet, it could be an asset in higher-level proofing or authoring tools, e.g. to be able to automatically discern among confusables and thereby to detect confusable errors (Golding and Roth, 1999; Even-Zohar and Roth, 2000; Banko and Brill, 2001; Huang and Powers, 2001). It could alleviate problems with lowfrequency and unknown words in natural language processing and information retrieval, by replacing them with likely and higher-frequency alternatives that carry similar information. And also, since the task of word prediction is a direct interpretation of language modeling, a word prediction system could provide useful information for to be used in speech recognition systems. We present a classification-based word prediction model based on IGT REE, a decision-tree induction algorithm with favorable scaling abilities and a functiona"
W06-3604,A00-2017,0,0.0194746,"06 Association for Computational Linguistics this word might be a confusable error, and the classifier’s prediction might be its correction. Confusable prediction and correction is a strong asset in proofing tools. In this paper we generalize the word prediction task to predicting any word in context. This is basically the task of a generic language model. An explicit choice for the particular study on “all-words” prediction is to encode context only by words, and not by any higher-level linguistic non-terminals which have been investigated in related work on word prediction (Wu et al., 1999; Even-Zohar and Roth, 2000). This choice leaves open the question how the same tasks can be learned from examples when non-terminal symbols are taken into account as well. The choice for our algorithm, a decision-tree approximation of k-nearest-neigbor (k-NN) based or memory-based learning, is motivated by the fact that, as we describe later in this paper, this particular algorithm can scale up to predicting tens of thousands of words, while simultaneously being able to scale up to tens of millions of examples as training material, predicting words at useful rates of hundreds to thousands of words per second. Another mo"
W06-3604,W95-0104,0,0.0932375,"Missing"
W06-3604,J93-2004,0,0.0256752,"imentation. In the remainder of the article we make no difference between words and punctuation markers; both are regarded as tokens. We separated the final 100,000 tokens as a held-out test set, henceforth referred to as REUTERS, and kept the rest as training set, henceforth TRAIN - REUTERS. Additionally, we selected two test sets taken from different corpora. First, we used the Project Gutenberg2 version of the novel Alice’s Adventures in Wonderland by Lewis Carroll (Carroll, 1865), henceforth ALICE. As the third test set we selected all tokens of the Brown corpus part of the Penn Treebank (Marcus et al., 1993), a selected portion of the original one-million word Brown corpus (Kuˇcera and Francis, 1967), a collection of samples of American English in many different genres, from sources printed in 1961; we refer to this test set as BROWN. In sum, we have three test sets, covering texts from the same genre and source as the training data, a fictional novel, and a mix of genres wider than the training set. Table 1 summarizes the key training and test set statistics. As the table shows, the cross-domain coverages for unigrams and bigrams are rather low; not only are these numbers the best-case performan"
W06-3604,P94-1013,0,0.125187,"Missing"
W06-3604,P97-1056,0,0.156263,"oice for our algorithm, a decision-tree approximation of k-nearest-neigbor (k-NN) based or memory-based learning, is motivated by the fact that, as we describe later in this paper, this particular algorithm can scale up to predicting tens of thousands of words, while simultaneously being able to scale up to tens of millions of examples as training material, predicting words at useful rates of hundreds to thousands of words per second. Another motivation for our choice is that our decision-tree approximation of k-nearest neighbor classification is functionally equivalent to back-off smoothing (Zavrel and Daelemans, 1997); not only does it share its performance capacities with n-gram models with back-off smoothing, it also shares its scaling abilities with these models, while being able to handle large values of n. The article is structured as follows. In Section 2 we describe what data we selected for our experiments, and we provide an overview of the experimental methodology used throughout the experiments, including a description of the IGT REE algorithm central to our study. In Section 3 the results of the word prediction experiments are presented, and the subsequent Section 4 contains the experimental res"
W09-0308,W99-0707,0,0.0470416,"ng two values from two database columns are sent to the system. The system processes each term pair in four steps. A schematic overview of the system is given in Figure 1. Term 1 Term 2 Indexed Wikipedia corpus Step 1 Term 1 Interm. Term Interm. Term Term 2 Step 4 Step 2 Art. 1 find path length if path length == 2 if path length == 1 Step 3 Art. 2 if found find intermediate value in database ...&lt;term 1>...&lt;term 2>... extract &lt;term 1>&lt;relation>&lt;term 2> Figure 1: Schematic overview of the system Subsequently, the selected sentences are POStagged and parsed using the Memory Based Shallow Parser (Daelemans et al., 1999). This parser provides tokenisation, POS-tagging, chunking, and grammatical relations such as subject and direct object between verbs and phrases, and is based on memory-based classification as implemented in TiMBL (Daelemans et al., 2004). The five most frequently recurring phrases that occur Step 1 We look for the most relevant Wikipedia page for each term, by looking up the term in titles of Wikipedia articles. As Wikipedia formatting requires the article title to be an informative and concise description of the article’s main topic, we assume that querying only for article titles will yiel"
W09-0308,J06-1003,0,0.00742194,"Relation discovery between terms (instantiations of different ontological classes) that have a page in Wikipedia is best performed after establishing if a sufficiently strong relation between the two terms under consideration actually exists. To do this, the semantic relatedness of those two terms or concepts needs to be computed first. Semantic relatedness can denote every possible relation between two concepts, unlike semantic similarity, which typically denotes only certain hierarchical relations (like hypernymy and synonymy) and is often computed using hierarchical networks like WordNet (Budanitsky and Hirst, 2006). A simple and effective way of computing semantic relatedness between two concepts c1 and c2 is measuring their distance in a semantic network. This results in a semantic distance metric, which can be inversed to yield a semantic relatedness metric. Computing the path-length between terms c1 and c2 can be done using Formula 1 where P is the set of paths connecting c1 to c2 and N p is the number of nodes in path p. As the database was created manually, it was necessary to normalise spelling errors, as well as variations on diacritics, names and date formats. The database values were also strip"
W09-0308,radev-etal-2002-evaluating,0,0.0120723,"between the ontological class pairs that were extracted from Wikipedia. Evaluating semantic relations automatically is hard, if not impossible, since the same relation can be expressed in many ways, and would require a gold standard of some sort, which for this domain (as well as for many cultural heritage domains) is not available. The judges were presented with the five highestranked candidate labels per column pair, as well a longer snippet of text containing the candidate label, to resolve possible ambiguity. The items in each list were scored according to the total reciprocal rank (TRR) (Radev et al., 2002). For every correct answer 1/n points are given, where n denotes the position of the answer in the ranked list. If there is more than 1 correct answer the points will be added up. For example, if in a list of five, two correct answers occur on positions 2 and 4, the TRR would be calculated as (1/2 + 1/4) = .75. The TRR scores were normalised for the number of relation candidates that were retrieved, as for some column pairs less than five relation candidates were retrieved. As an example, for the column pair “Province” and “Genus”, the judges were presented with the relations shown in Table 2."
W09-0308,W06-0503,0,0.0249279,"cles are under constant scrutiny of their viewers, most links in Wikipedia are indeed relevant (Blohm and Cimiano, 2007; Kamps and Koolen, 2008). The structure and breadth of Wikipedia is a potentially powerful resource for information extraction which has not gone unnoticed in the natural language processing (NLP) community. Preprocessing of Wikipedia content in order to extract non-trivial relations has been addressed in a number of studies. (Syed et al., 2008) for instance utilise the category structure in Wikipedia as an upper ontology to predict concepts common to a set of documents. In (Suchanek et al., 2006) an ontology is constructed by combining entities and relations between these extracted from Wikipedia through Wikipedia’s category structure and WordNet. This results in a large “is-a” hierarchy, drawing on the basis of WordNet, while further relation enrichments come from Wikipedia’s category The idea of using Wikipedia for relation extraction is not new (Auer and Lehmann, 2007; Nakayama et al., 2008; Nguyen et al., 2007; Suchanek et al., 2006; Syed et al., 2008). However, most studies so far focus on the structured information already explicit in Wikipedia, such as its infoboxes and categor"
W09-0621,C04-1051,0,0.116471,"Barzilay, 2006; Zhou et al., 2006), but also for text simplification and explanation. In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation. News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News. These services collect multiple articles covering the same event. Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases (Dolan et al., 2004; Nelken and Shieber, 2006). We use this method to collect a large amount of aligned paraphrases in an automatic fashion. For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based"
W09-0621,N06-1058,0,0.0926057,"Missing"
W09-0621,E06-1021,0,0.526588,"u et al., 2006), but also for text simplification and explanation. In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation. News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News. These services collect multiple articles covering the same event. Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases (Dolan et al., 2004; Nelken and Shieber, 2006). We use this method to collect a large amount of aligned paraphrases in an automatic fashion. For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching. We show that the"
W09-0621,N06-4007,0,0.0198878,"ntence is viewed as a Clustering Our first approach is to use a clustering algorithm to cluster similar headlines. The original Google News headline clusters are reclustered into finer grained sub-clusters. We use the k-means implementation in the CLUTO1 software package. The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space. The total intra-cluster variances is minimized by the function V = k X X (xj − µi )2 i=1 xj ∈Si where µi is the centroid of all the points xj ∈ Si . The PK1 cluster-stopping algorithm as proposed by Pedersen and Kulkarni (2006) is used to find the optimal k for each sub-cluster: P K1(k) = Cr(k) − mean(Cr[1...∆K]) std(Cr[1...∆K]) Here, Cr is a criterion function, which measures the ratio of withincluster similarity to betweencluster similarity. As soon as P K1(k) exceeds a threshold, k − 1 is selected as the optimum number of clusters. To find the optimal threshold value for clusterstopping, optimization is performed on the development data. Our optimization function is an F score: Fβ = 1 123 (1 + β 2 ) · (precision · recall) (β 2 · precision + recall) http://glaros.dtc.umn.edu/gkhome/views/cluto/ Type k-means cluste"
W09-0621,P07-1059,0,0.0744359,"Missing"
W09-0621,W03-1004,0,0.769439,"rces for text-to-text generation. Paraphrase generation has already proven to be valuable for Question Answering (Lin and Pantel, 2001; Riezler et al., 2 Method We aim to build a high-quality paraphrase corpus. Considering the fact that this corpus will be the basic resource of a paraphrase generation system, we need it to be as free of errors as possible, because errors will propagate throughout the system. This implies that we focus on obtaining a high precision in the paraphrases collection process. Where previous work has focused on aligning news-items at the paragraph and sentence level (Barzilay and Elhadad, 2003), we choose to focus on aligning the headlines of news articles. We think this approach will enable us to harvest reliable training material for paraphrase generation quickly and efficiently, without having to worry too much about the problems that arise when trying to align complete news articles. For the development of our system we use data which was obtained in the DAESO-project. This project is an ongoing effort to build a Parallel Monolingual Treebank for Dutch (Marsi Proceedings of the 12th European Workshop on Natural Language Generation, pages 122–125, c Athens, Greece, 30 – 31 March"
W09-0621,W06-1610,0,0.0901955,"Missing"
W09-0621,N06-1003,0,\N,Missing
W09-1203,burchardt-etal-2006-salsa,0,0.0646198,"Missing"
W09-1203,D07-1121,0,0.17029,"Missing"
W09-1203,kawahara-etal-2002-construction,0,0.0175209,"performing the identification and labeling of syntactic and semantic dependencies in multiple languages. Dependencies are truly jointly learned, i.e. as if they were a single task. The system works in two phases: a classification phase in which three classifiers predict different types of information, and a ranking phase in which the output of the classifiers is combined. 1 Introduction In this paper we present the machine learning system submitted to the CoNLL Shared Task 2009 (Hajiˇc et al., 2009). The task is an extension to multiple languages (Burchardt et al., 2006; Hajiˇc et al., 2006; Kawahara et al., 2002; Palmer and Xue, 2009; Surdeanu et al., 2008; Taul´e et al., 2008) of the CoNLL Shared Task 2008, combining the identification and labeling of syntactic dependencies and semantic roles. Our system is a joint-learning system tested in the “closed” challenge, i.e. without making use of external resources. Our system operates in two phases: a classification phase in which three memory-based classifiers predict different types of information, and a ranking phase in which the output of the classifiers is combined by ranking the predictions. Semantic and syntactic dependencies are jointly learned a"
W09-1203,W08-2128,1,0.860736,"Missing"
W09-1203,W08-2121,0,0.11314,"Missing"
W09-1203,taule-etal-2008-ancora,0,0.0626567,"Missing"
W09-3743,J06-1003,0,0.12859,"set we show that a shortest path metric run on Wikipedia attains a better correlation than WordNet-based metrics. ConceptNet attains a good correlation as well, but suffers from a low concept coverage. 1 Introduction In this paper we propose a new estimate metric for semantic relatedness, by finding the shortest path between two concepts in a semantic network. The semantic networks that we exploit are the extracted free link structures of Wikipedia and the ConceptNet 3 database, a commonsense knowledgebase [3]. Various metrics of calculating semantic similarity have been developed for WordNet [1] and applied to Wikipedia’s category graph [7]. These measures tend to perform well on semantic similarity (how synonymous two words are), but not very well on semantic relatedness (how related two words are). 2 Free-link pathfinding The graph extracted from Wikipedia contains over 2 million nodes and 55 million edges, from respectively the number of articles and the number of internal links. ConceptNet contains over 18 thousand usable concepts (nodes) 355 Proceedings of the 8th International Conference on Computational Semantics, pages 355–358, c Tilburg, January 2009. 2009 International Conf"
W09-3743,N04-3012,0,0.146916,"Missing"
W10-4223,W05-0909,0,0.0771031,"ollection of data is required. In this case, this would have to be pairs of sentences that are paraphrases of each other. So far, paraphrasing data sets of sufficient size have been mostly lacking. We argue that the headlines aggregated by Google News offer an attractive avenue. 2 Data Collection Currently not many resources are available for paraphrasing; one example is the Microsoft Paraphrase Corpus (MSR) (Dolan et al., 2004; Nelken and Shieber, 2006), which with its 139,000 aligned lines are presented to human judges, whose ratings are compared to the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) automatic evaluation metrics. Police investigate Doherty drug pics Doherty under police investigation Police to probe Pete pics Pete Doherty arrested in drug-photo probe 3.1 Rocker photographed injecting unconscious fan Doherty ʼinjected unconscious fan with drugʼ Photos may show Pete Doherty injecting passed-out fan Doherty ʼinjected female fanʼ Figure 1: Part of a sample headline cluster, with aligned paraphrases paraphrases can be considered relatively small. In this study we explore the use of a large, automatically acquired aligned paraphrase corpus. Our method cons"
W10-4223,N03-1003,0,0.0318701,"at in their original form cannot be answered (Lin and Pantel, 2001; Riezler et al., 2007), or to generate paraphrases of sentences that failed to translate (Callison-Burch et al., 2006). Paraphrasing has also been used in the evaluation of machine translation system output (Russo-Lassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006). Adding certain constraints to paraphrasing allows for additional useful applications. When a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subtitle generation (Daelemans et al., 2004). We regard SPG as a monolingual machine translation task, where the source and target languages are the same (Quirk et al., 2004). However, there are two problems that have to be dealt with to make this approach work, namely obtaining a sufficient amount of examples, and a proper evaluation methodology. As Callison-Burch et al. (2008) argue, automatic evaluation of paraphrasing is problematic. The essence of SPG is to generate a sentence that is structurally different from the source. Aut"
W10-4223,N06-1003,0,0.0317805,"hine translation, and paraphrase generation. Sentential paraphrase generation (SPG) is the process of transforming a source sentence into a target sentence in the same language which differs in form from the source sentence, but approximates its meaning. Paraphrasing is often used as a subtask in more complex NLP applications to allow for more variation in text strings presented as input, for example to generate paraphrases of questions that in their original form cannot be answered (Lin and Pantel, 2001; Riezler et al., 2007), or to generate paraphrases of sentences that failed to translate (Callison-Burch et al., 2006). Paraphrasing has also been used in the evaluation of machine translation system output (Russo-Lassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006). Adding certain constraints to paraphrasing allows for additional useful applications. When a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subtitle generation (Daelemans et al., 2004). We regard SPG as a monolingual machine translation tas"
W10-4223,C08-1013,0,0.095526,"hen a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subtitle generation (Daelemans et al., 2004). We regard SPG as a monolingual machine translation task, where the source and target languages are the same (Quirk et al., 2004). However, there are two problems that have to be dealt with to make this approach work, namely obtaining a sufficient amount of examples, and a proper evaluation methodology. As Callison-Burch et al. (2008) argue, automatic evaluation of paraphrasing is problematic. The essence of SPG is to generate a sentence that is structurally different from the source. Automatic evaluation metrics in related fields such as machine translation operate on a notion of similarity, while paraphrasing centers around achieving dissimilarity. Besides the evaluation issue, another problem is that for an datadriven MT account of paraphrasing to work, a large collection of data is required. In this case, this would have to be pairs of sentences that are paraphrases of each other. So far, paraphrasing data sets of suff"
W10-4223,W96-0102,0,0.0711676,"Missing"
W10-4223,C04-1051,0,0.0443599,"imilarity, while paraphrasing centers around achieving dissimilarity. Besides the evaluation issue, another problem is that for an datadriven MT account of paraphrasing to work, a large collection of data is required. In this case, this would have to be pairs of sentences that are paraphrases of each other. So far, paraphrasing data sets of sufficient size have been mostly lacking. We argue that the headlines aggregated by Google News offer an attractive avenue. 2 Data Collection Currently not many resources are available for paraphrasing; one example is the Microsoft Paraphrase Corpus (MSR) (Dolan et al., 2004; Nelken and Shieber, 2006), which with its 139,000 aligned lines are presented to human judges, whose ratings are compared to the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) automatic evaluation metrics. Police investigate Doherty drug pics Doherty under police investigation Police to probe Pete pics Pete Doherty arrested in drug-photo probe 3.1 Rocker photographed injecting unconscious fan Doherty ʼinjected unconscious fan with drugʼ Photos may show Pete Doherty injecting passed-out fan Doherty ʼinjected female fanʼ Figure 1: Part of a sample headlin"
W10-4223,N06-1058,0,0.0525483,"sentence in the same language which differs in form from the source sentence, but approximates its meaning. Paraphrasing is often used as a subtask in more complex NLP applications to allow for more variation in text strings presented as input, for example to generate paraphrases of questions that in their original form cannot be answered (Lin and Pantel, 2001; Riezler et al., 2007), or to generate paraphrases of sentences that failed to translate (Callison-Burch et al., 2006). Paraphrasing has also been used in the evaluation of machine translation system output (Russo-Lassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006). Adding certain constraints to paraphrasing allows for additional useful applications. When a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subtitle generation (Daelemans et al., 2004). We regard SPG as a monolingual machine translation task, where the source and target languages are the same (Quirk et al., 2004). However, there are two problems that have to be dealt with to make t"
W10-4223,P07-2045,0,0.0348047,"September 2006. Using this method we end up with a corpus of 7,400,144 pairwise alignments of 1,025,605 unique headlines1 . 3 Paraphrasing methods In our approach we use the collection of automatically obtained aligned headlines to train a paraphrase generation model using a PhraseBased MT framework. We compare this approach to a word substitution baseline. The generated paraphrases along with their source head1 This list of aligned pairs is http://ilk.uvt.nl/∼swubben/resources.html available at Phrase-Based MT We use the MOSES package to train a Phrase-Based Machine Translation model (PBMT) (Koehn et al., 2007). Such a model normally finds a best translation e˜ of a text in language f to a text in language e by combining a translation model p(f |e) with a language model p(e): e˜ = arg max p(f |e)p(e) ∗ e∈e GIZA++ is used to perform the word alignments (Och and Ney, 2003) which are then used in the Moses pipeline to generate phrase alignments in order to build the paraphrase model. We first tokenize our data before training a recaser. We then lowercase all data and use all unique headlines in the training data to train a language model with the SRILM toolkit (Stolcke, 2002). Then we invoke the GIZA++"
W10-4223,W04-1013,0,0.0132683,"s case, this would have to be pairs of sentences that are paraphrases of each other. So far, paraphrasing data sets of sufficient size have been mostly lacking. We argue that the headlines aggregated by Google News offer an attractive avenue. 2 Data Collection Currently not many resources are available for paraphrasing; one example is the Microsoft Paraphrase Corpus (MSR) (Dolan et al., 2004; Nelken and Shieber, 2006), which with its 139,000 aligned lines are presented to human judges, whose ratings are compared to the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) automatic evaluation metrics. Police investigate Doherty drug pics Doherty under police investigation Police to probe Pete pics Pete Doherty arrested in drug-photo probe 3.1 Rocker photographed injecting unconscious fan Doherty ʼinjected unconscious fan with drugʼ Photos may show Pete Doherty injecting passed-out fan Doherty ʼinjected female fanʼ Figure 1: Part of a sample headline cluster, with aligned paraphrases paraphrases can be considered relatively small. In this study we explore the use of a large, automatically acquired aligned paraphrase corpus. Our method consists of crawling the h"
W10-4223,E06-1021,0,0.0694444,"aphrasing centers around achieving dissimilarity. Besides the evaluation issue, another problem is that for an datadriven MT account of paraphrasing to work, a large collection of data is required. In this case, this would have to be pairs of sentences that are paraphrases of each other. So far, paraphrasing data sets of sufficient size have been mostly lacking. We argue that the headlines aggregated by Google News offer an attractive avenue. 2 Data Collection Currently not many resources are available for paraphrasing; one example is the Microsoft Paraphrase Corpus (MSR) (Dolan et al., 2004; Nelken and Shieber, 2006), which with its 139,000 aligned lines are presented to human judges, whose ratings are compared to the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) automatic evaluation metrics. Police investigate Doherty drug pics Doherty under police investigation Police to probe Pete pics Pete Doherty arrested in drug-photo probe 3.1 Rocker photographed injecting unconscious fan Doherty ʼinjected unconscious fan with drugʼ Photos may show Pete Doherty injecting passed-out fan Doherty ʼinjected female fanʼ Figure 1: Part of a sample headline cluster, with aligned par"
W10-4223,J03-1002,0,0.0366185,"l using a PhraseBased MT framework. We compare this approach to a word substitution baseline. The generated paraphrases along with their source head1 This list of aligned pairs is http://ilk.uvt.nl/∼swubben/resources.html available at Phrase-Based MT We use the MOSES package to train a Phrase-Based Machine Translation model (PBMT) (Koehn et al., 2007). Such a model normally finds a best translation e˜ of a text in language f to a text in language e by combining a translation model p(f |e) with a language model p(e): e˜ = arg max p(f |e)p(e) ∗ e∈e GIZA++ is used to perform the word alignments (Och and Ney, 2003) which are then used in the Moses pipeline to generate phrase alignments in order to build the paraphrase model. We first tokenize our data before training a recaser. We then lowercase all data and use all unique headlines in the training data to train a language model with the SRILM toolkit (Stolcke, 2002). Then we invoke the GIZA++ aligner using the 7M training paraphrase pairs. We run GIZA++ with standard settings and we perform no optimization. Finally, we use the MOSES decoder to generate paraphrases for our test data. Instead of assigning equal weights to language and translation model,"
W10-4223,P02-1040,0,0.0872487,"paraphrasing to work, a large collection of data is required. In this case, this would have to be pairs of sentences that are paraphrases of each other. So far, paraphrasing data sets of sufficient size have been mostly lacking. We argue that the headlines aggregated by Google News offer an attractive avenue. 2 Data Collection Currently not many resources are available for paraphrasing; one example is the Microsoft Paraphrase Corpus (MSR) (Dolan et al., 2004; Nelken and Shieber, 2006), which with its 139,000 aligned lines are presented to human judges, whose ratings are compared to the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) automatic evaluation metrics. Police investigate Doherty drug pics Doherty under police investigation Police to probe Pete pics Pete Doherty arrested in drug-photo probe 3.1 Rocker photographed injecting unconscious fan Doherty ʼinjected unconscious fan with drugʼ Photos may show Pete Doherty injecting passed-out fan Doherty ʼinjected female fanʼ Figure 1: Part of a sample headline cluster, with aligned paraphrases paraphrases can be considered relatively small. In this study we explore the use of a large, automatically acquired aligned"
W10-4223,W04-3219,0,0.0466146,"machine translation system output (Russo-Lassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006). Adding certain constraints to paraphrasing allows for additional useful applications. When a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subtitle generation (Daelemans et al., 2004). We regard SPG as a monolingual machine translation task, where the source and target languages are the same (Quirk et al., 2004). However, there are two problems that have to be dealt with to make this approach work, namely obtaining a sufficient amount of examples, and a proper evaluation methodology. As Callison-Burch et al. (2008) argue, automatic evaluation of paraphrasing is problematic. The essence of SPG is to generate a sentence that is structurally different from the source. Automatic evaluation metrics in related fields such as machine translation operate on a notion of similarity, while paraphrasing centers around achieving dissimilarity. Besides the evaluation issue, another problem is that for an datadrive"
W10-4223,P07-1059,0,0.0181436,"as summarization (Knight and Marcu, 2002), question-answering (Lin and Pantel, 2001), machine translation, and paraphrase generation. Sentential paraphrase generation (SPG) is the process of transforming a source sentence into a target sentence in the same language which differs in form from the source sentence, but approximates its meaning. Paraphrasing is often used as a subtask in more complex NLP applications to allow for more variation in text strings presented as input, for example to generate paraphrases of questions that in their original form cannot be answered (Lin and Pantel, 2001; Riezler et al., 2007), or to generate paraphrases of sentences that failed to translate (Callison-Burch et al., 2006). Paraphrasing has also been used in the evaluation of machine translation system output (Russo-Lassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006). Adding certain constraints to paraphrasing allows for additional useful applications. When a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subt"
W10-4223,W09-0621,1,0.548134,"Missing"
W10-4223,W06-1610,0,0.0581942,"Missing"
W10-4223,daelemans-etal-2004-automatic,0,\N,Missing
W11-1507,J93-1003,0,0.0611157,"gical (Heid, 1998) rule patterns, often in combination with terminological or other lexical resources (Gaizauskas et al., 2000) and are typically language and domain specific. Statistical approaches typically combine linguistic information with statistical measures. These measures can be coarsely classified into two categories: unithood-based and termhood-based. Unithood-based approaches measure the attachment strength among the constituents of a candidate term. For example, some unithood-based measures are frequency of co-occurrence, hypothesis testing statistics, log-likelihood ratios test (Dunning, 1993) and pointwise mutual information (Church and Hanks, 1990). Termhood-based approaches attempt to measure the degree up to which a candidate expression is a valid term, i.e. refers to a specialised concept. They attempt to measure this degree by considering nestedness information, namely the frequencies of candidate terms and their subsequences. Examples of such approaches are C-Value and NCValue (Frantzi et al., 2000) and the statistical barrier method (Nakagawa, 2000). It has been experimentally shown that termhoodbased approaches to automatic term extraction outperform unithood-based ones an"
W11-1507,2005.mtsummit-papers.11,0,0.00421473,"respective category (orlanguage Dutch German English French snippets 50,363 41,334 19,767 6,182 language Spanish Danish Italian Swedish snippets 3,430 2,478 1,100 699 Table 1: Number of snippets per identified language. ganisations, persons, geographic locations and subject terms) and are compared to our term results list. 4 Figure 2: Length of snippets per identified language. Experimental Setting For training the language identification component, we used the European Parliament Proceedings Parallel Corpus (Europarl) which covers the proceedings of the European Parliament from 1996 to 2006 (Koehn, 2005). The corpus size is 40 million words per language and is translated in Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish. In our experiments, we take as input for subsequent term recognition only the snippets identified as English text. In the experiments reported in this work, we accept as term candidates morpho-syntactic pattern sequences which consist of adjectives and nouns, and end with a noun. The C-Value algorithm (cf. Section 3.3) was implemented under two different settings: i. one only considering as term candidates adjective and noun s"
W11-1507,J90-1003,0,\N,Missing
W11-1604,P05-1074,0,0.0381435,"The Netherlands emarsi@idi.ntnu.no antal.vdnbosch@uvt.nl e.j.krahmer@uvt.nl Abstract The similarity between Paraphrase Generation and MT suggests that methods and tools originally developed for MT could be exploited for Paraphrase Generation. One popular approach – arguably the most successful so far – is Statistical Phrase-based Machine Translation (PBMT), which learns phrase translation rules from aligned bilingual text corpora (Och et al., 1999; Vogel et al., 2000; Zens et al., 2002; Koehn et al., 2003). Prior work has explored the use of PBMT for paraphrase generation (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Madnani et al., 2007; Callison-Burch, 2008; Zhao et al., 2009; Wubben et al., 2010) Paraphrase generation can be regarded as machine translation where source and target language are the same. We use the Moses statistical machine translation toolkit for paraphrasing, comparing phrase-based to syntax-based approaches. Data is derived from a recently released, large scale (2.1M tokens) paraphrase corpus for Dutch. Preliminary results indicate that the phrase-based approach performs better in terms of NIST scores and produces paraphrases at a greater distance from the source. 1 Antal van den Bos"
W11-1604,N03-1003,0,0.0482554,"ed translations of literary texts and then applied machine learning or multi-sequence alignment for extracting paraphrases. In a similar vein, Pang et al. (2003) used a corpus of alternative English translations of Chinese news stories in combination with a syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified. So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences 28 exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles. However, it is rather small and contains no subsentential allignments. Cohn et al. (2008) developed a parallel monolingual corpus of 900 sentence pairs annotated at the word and phrase level. However, all of these corpora are small from an SMT perspective. Recently a new large-scale paraphrase corpus for Dutch, the DAESO corpus, was released. The corpus"
W11-1604,P01-1008,0,0.0653452,"neration methods and the experimental setup. Results are presented in Section 4. In Section 5 we discuss our findings and formulate our conclusions. 2 Corpus The main bottleneck in building SMT systems is the need for a substantial amount of parallel aligned text. Likewise, exploiting SMT for paraphrasing requires large amounts of monolingual parallel text. However, paraphrase corpora are scarce; the situation is more dire than in MT, and this has caused some studies to focus on the automatic harvesting of paraphrase corpora. The use of monolingual parallel text corpora was first suggested by Barzilay and McKeown (2001), who built their corpus using various alternative human-produced translations of literary texts and then applied machine learning or multi-sequence alignment for extracting paraphrases. In a similar vein, Pang et al. (2003) used a corpus of alternative English translations of Chinese news stories in combination with a syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified. So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences 28 exhibit partial semant"
W11-1604,D08-1021,0,0.0189038,"e.j.krahmer@uvt.nl Abstract The similarity between Paraphrase Generation and MT suggests that methods and tools originally developed for MT could be exploited for Paraphrase Generation. One popular approach – arguably the most successful so far – is Statistical Phrase-based Machine Translation (PBMT), which learns phrase translation rules from aligned bilingual text corpora (Och et al., 1999; Vogel et al., 2000; Zens et al., 2002; Koehn et al., 2003). Prior work has explored the use of PBMT for paraphrase generation (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Madnani et al., 2007; Callison-Burch, 2008; Zhao et al., 2009; Wubben et al., 2010) Paraphrase generation can be regarded as machine translation where source and target language are the same. We use the Moses statistical machine translation toolkit for paraphrasing, comparing phrase-based to syntax-based approaches. Data is derived from a recently released, large scale (2.1M tokens) paraphrase corpus for Dutch. Preliminary results indicate that the phrase-based approach performs better in terms of NIST scores and produces paraphrases at a greater distance from the source. 1 Antal van den Bosch Tilburg University P.O. Box 90135 5000 LE"
W11-1604,H05-1098,0,0.0190688,"s such as insertion, deletion and many-to-one, one-to-many or many-to-many translation are all covered in the structure of the phrase table. Phrase-based models have been used most prominently in the past decade, as they have shown to outperform other approaches 29 (Callison-Burch et al., 2009). One issue with the phrase-based approach is that recursion is not handled explicitly. It is generally acknowledged that language contains recursive structures up to certain depths. So-called hierarchical models have introduced the inclusion of nonterminals in the mapping rules, to allow for recursion (Chiang et al., 2005). However, using a generic non-terminal X can introduce many substitutions in translations that do not make sense. By making the non-terminals explicit, using syntactic categories such as N P s and V P s, this phenomenon is constrained, resulting in syntax-based translation. Instead of phrase translations, translation rules in terms of syntactic constituents or subtrees are extracted, presupposing the availability of syntactic structures for source, target, or both languages. Incorporating syntax can guide the translation process and unlike phrase-based MT syntax it enables the modeling of lon"
W11-1604,J08-4005,0,0.0153637,"ases can be identified. So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences 28 exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles. However, it is rather small and contains no subsentential allignments. Cohn et al. (2008) developed a parallel monolingual corpus of 900 sentence pairs annotated at the word and phrase level. However, all of these corpora are small from an SMT perspective. Recently a new large-scale paraphrase corpus for Dutch, the DAESO corpus, was released. The corpus contains both samples of parallel and comparable text in which similar sentences, phrases and words are aligned. One part of the corpus is manually aligned, whereas another part is automatically aligned using a data-driven aligner trained on the first part. The DAESO corpus is extensively described in (Marsi and Krahmer, 2011); the"
W11-1604,C04-1051,0,0.0434847,"sed a corpus of alternative English translations of Chinese news stories in combination with a syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified. So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences 28 exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles. However, it is rather small and contains no subsentential allignments. Cohn et al. (2008) developed a parallel monolingual corpus of 900 sentence pairs annotated at the word and phrase level. However, all of these corpora are small from an SMT perspective. Recently a new large-scale paraphrase corpus for Dutch, the DAESO corpus, was released. The corpus contains both samples of parallel and comparable text in which similar sentences, phrases and words are aligned. One part of the corpus is manually aligned, wh"
W11-1604,E09-1049,0,0.0160286,"e Translation (MT) are instances of Text-To-Text Generation, which involves transforming one text into another, obeying certain restrictions. Here these restrictions are that the generated text must be grammatically well-formed and semantically/translationally equivalent to the source text. Addionally Paraphrase Generation requires that the output should differ from the input to a certain degree. However, since many researchers believe that PBMT has reached a performance ceiling, ongoing research looks into more structural approaches to statistical MT (Marcu and Wong, 2002; Och and Ney, 2004; Khalilov and Fonollosa, 2009). Syntaxbased MT attempts to extract translation rules in terms of syntactic constituents or subtrees rather than arbitrary phrases, presupposing syntactic structures for source, target or both languages. Syntactic information might lead to better results in the area of grammatical well-formedness, and unlike phrasebased MT that uses contiguous n-grams, syntax enables the modeling of long-distance translation patterns. While the verdict on whether or not this approach leads to any significant performance gain is still out, a similar line of reasoning would suggest that syntax-based paraphrasin"
W11-1604,N03-1017,0,0.0505385,"Missing"
W11-1604,W07-0734,0,0.0376284,"als to make the word order match the surface word order. 3 http://www.vf.utwente.nl/˜druid/TwNC/ TwNC-main.html 30 average score. This implies that we penalize systems that provide output at Levenshtein distance 0, which are essentially copies of the input, and not paraphrases. Formally, the score is computed as follows: X N ISTweightedLD = α (i ∗ Ni ∗ N ISTi ) i=LD(1..8) X (i ∗ Ni ) i=LD(1..8) where α is the percentage of output phrases that have a sentence Levenshtein Distance higher than 0. Instead of NIST scores, other MT evaluation scores can be plugged into this formula, such as METEOR (Lavie and Agarwal, 2007) for languages for which paraphrase data is available. 4 Results Figure 1 shows NIST scores per Levenshtein Distance. It can be observed that overall the NIST score decreases as the distance to the input increases, indicating that more distant paraphrases are of less quality. The relaxed syntax-based approach (SAMT) performs mildly better than the standard syntaxbased approach, but performs worse than the phrasebased approach. The distribution of generated paraphrases per Levenshtein Distance is shown in Figure 2. It reveals that the Syntax-based approaches tend to stay closer to the source th"
W11-1604,J10-3003,0,0.0204038,"s better in terms of NIST scores and produces paraphrases at a greater distance from the source. 1 Antal van den Bosch Tilburg University P.O. Box 90135 5000 LE Tilburg The Netherlands Introduction One of the challenging properties of natural language is that the same semantic content can typically be expressed by many different surface forms. As the ability to deal with paraphrases holds great potential for improving the coverage of NLP systems, a substantial body of research addressing recognition, extraction and generation of paraphrases has emerged (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010). Paraphrase Generation can be regarded as a translation task in which source and target language are the same. Both Paraphrase Generation and Machine Translation (MT) are instances of Text-To-Text Generation, which involves transforming one text into another, obeying certain restrictions. Here these restrictions are that the generated text must be grammatically well-formed and semantically/translationally equivalent to the source text. Addionally Paraphrase Generation requires that the output should differ from the input to a certain degree. However, since many researchers believe that PBMT h"
W11-1604,W07-0716,0,0.0211701,"antal.vdnbosch@uvt.nl e.j.krahmer@uvt.nl Abstract The similarity between Paraphrase Generation and MT suggests that methods and tools originally developed for MT could be exploited for Paraphrase Generation. One popular approach – arguably the most successful so far – is Statistical Phrase-based Machine Translation (PBMT), which learns phrase translation rules from aligned bilingual text corpora (Och et al., 1999; Vogel et al., 2000; Zens et al., 2002; Koehn et al., 2003). Prior work has explored the use of PBMT for paraphrase generation (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Madnani et al., 2007; Callison-Burch, 2008; Zhao et al., 2009; Wubben et al., 2010) Paraphrase generation can be regarded as machine translation where source and target language are the same. We use the Moses statistical machine translation toolkit for paraphrasing, comparing phrase-based to syntax-based approaches. Data is derived from a recently released, large scale (2.1M tokens) paraphrase corpus for Dutch. Preliminary results indicate that the phrase-based approach performs better in terms of NIST scores and produces paraphrases at a greater distance from the source. 1 Antal van den Bosch Tilburg University"
W11-1604,W02-1018,0,0.0253057,"me. Both Paraphrase Generation and Machine Translation (MT) are instances of Text-To-Text Generation, which involves transforming one text into another, obeying certain restrictions. Here these restrictions are that the generated text must be grammatically well-formed and semantically/translationally equivalent to the source text. Addionally Paraphrase Generation requires that the output should differ from the input to a certain degree. However, since many researchers believe that PBMT has reached a performance ceiling, ongoing research looks into more structural approaches to statistical MT (Marcu and Wong, 2002; Och and Ney, 2004; Khalilov and Fonollosa, 2009). Syntaxbased MT attempts to extract translation rules in terms of syntactic constituents or subtrees rather than arbitrary phrases, presupposing syntactic structures for source, target or both languages. Syntactic information might lead to better results in the area of grammatical well-formedness, and unlike phrasebased MT that uses contiguous n-grams, syntax enables the modeling of long-distance translation patterns. While the verdict on whether or not this approach leads to any significant performance gain is still out, a similar line of rea"
W11-1604,J03-1002,0,0.00539719,"ra are still lacking for paraphrase generation, using more linguistically motivated methods might prove beneficial for paraphrase generation. At the same time, automatic syntactic analysis introduces errors in the parse trees, as no syntactic parser is perfect. Likewise, automatic alignment of syntactic phrases may be prone to errors. The main contribution of this paper is a systematic comparison between phrase-based and syntax-based paraphrase generation using an off-the-shelf statistical machine translation (SMT) decoder, namely Moses (Koehn et al., 2007) and the word-alignment tool GIZA++ (Och and Ney, 2003). Training data derives from a new, large scale (2.1M tokens) paraphrase corpus for Dutch, which has been recently released. The paper is organized as follows. Section 2 reviews the paraphrase corpus from which provides training and test data. Next, Section 3 describes the paraphrase generation methods and the experimental setup. Results are presented in Section 4. In Section 5 we discuss our findings and formulate our conclusions. 2 Corpus The main bottleneck in building SMT systems is the need for a substantial amount of parallel aligned text. Likewise, exploiting SMT for paraphrasing requir"
W11-1604,J04-4002,0,0.17308,"neration and Machine Translation (MT) are instances of Text-To-Text Generation, which involves transforming one text into another, obeying certain restrictions. Here these restrictions are that the generated text must be grammatically well-formed and semantically/translationally equivalent to the source text. Addionally Paraphrase Generation requires that the output should differ from the input to a certain degree. However, since many researchers believe that PBMT has reached a performance ceiling, ongoing research looks into more structural approaches to statistical MT (Marcu and Wong, 2002; Och and Ney, 2004; Khalilov and Fonollosa, 2009). Syntaxbased MT attempts to extract translation rules in terms of syntactic constituents or subtrees rather than arbitrary phrases, presupposing syntactic structures for source, target or both languages. Syntactic information might lead to better results in the area of grammatical well-formedness, and unlike phrasebased MT that uses contiguous n-grams, syntax enables the modeling of long-distance translation patterns. While the verdict on whether or not this approach leads to any significant performance gain is still out, a similar line of reasoning would sugges"
W11-1604,W99-0604,0,0.0978911,"ilburg The Netherlands s.wubben@uvt.nl Erwin Marsi NTNU Sem Saelandsvei 7-9 NO-7491 Trondheim Norway Emiel Krahmer Tilburg University P.O. Box 90135 5000 LE Tilburg The Netherlands emarsi@idi.ntnu.no antal.vdnbosch@uvt.nl e.j.krahmer@uvt.nl Abstract The similarity between Paraphrase Generation and MT suggests that methods and tools originally developed for MT could be exploited for Paraphrase Generation. One popular approach – arguably the most successful so far – is Statistical Phrase-based Machine Translation (PBMT), which learns phrase translation rules from aligned bilingual text corpora (Och et al., 1999; Vogel et al., 2000; Zens et al., 2002; Koehn et al., 2003). Prior work has explored the use of PBMT for paraphrase generation (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Madnani et al., 2007; Callison-Burch, 2008; Zhao et al., 2009; Wubben et al., 2010) Paraphrase generation can be regarded as machine translation where source and target language are the same. We use the Moses statistical machine translation toolkit for paraphrasing, comparing phrase-based to syntax-based approaches. Data is derived from a recently released, large scale (2.1M tokens) paraphrase corpus for Dutch. Pr"
W11-1604,N03-1024,0,0.0375578,"mount of parallel aligned text. Likewise, exploiting SMT for paraphrasing requires large amounts of monolingual parallel text. However, paraphrase corpora are scarce; the situation is more dire than in MT, and this has caused some studies to focus on the automatic harvesting of paraphrase corpora. The use of monolingual parallel text corpora was first suggested by Barzilay and McKeown (2001), who built their corpus using various alternative human-produced translations of literary texts and then applied machine learning or multi-sequence alignment for extracting paraphrases. In a similar vein, Pang et al. (2003) used a corpus of alternative English translations of Chinese news stories in combination with a syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified. So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences 28 exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus"
W11-1604,W04-3219,0,0.0646921,"Missing"
W11-1604,P06-2096,0,0.019951,"ary texts and then applied machine learning or multi-sequence alignment for extracting paraphrases. In a similar vein, Pang et al. (2003) used a corpus of alternative English translations of Chinese news stories in combination with a syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified. So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences 28 exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles. However, it is rather small and contains no subsentential allignments. Cohn et al. (2008) developed a parallel monolingual corpus of 900 sentence pairs annotated at the word and phrase level. However, all of these corpora are small from an SMT perspective. Recently a new large-scale paraphrase corpus for Dutch, the DAESO corpus, was released. The corpus contains both samp"
W11-1604,W09-0621,1,0.896169,"Missing"
W11-1604,W10-4223,1,0.888359,"Missing"
W11-1604,2002.tmi-tutorials.2,0,0.0383049,"Erwin Marsi NTNU Sem Saelandsvei 7-9 NO-7491 Trondheim Norway Emiel Krahmer Tilburg University P.O. Box 90135 5000 LE Tilburg The Netherlands emarsi@idi.ntnu.no antal.vdnbosch@uvt.nl e.j.krahmer@uvt.nl Abstract The similarity between Paraphrase Generation and MT suggests that methods and tools originally developed for MT could be exploited for Paraphrase Generation. One popular approach – arguably the most successful so far – is Statistical Phrase-based Machine Translation (PBMT), which learns phrase translation rules from aligned bilingual text corpora (Och et al., 1999; Vogel et al., 2000; Zens et al., 2002; Koehn et al., 2003). Prior work has explored the use of PBMT for paraphrase generation (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Madnani et al., 2007; Callison-Burch, 2008; Zhao et al., 2009; Wubben et al., 2010) Paraphrase generation can be regarded as machine translation where source and target language are the same. We use the Moses statistical machine translation toolkit for paraphrasing, comparing phrase-based to syntax-based approaches. Data is derived from a recently released, large scale (2.1M tokens) paraphrase corpus for Dutch. Preliminary results indicate that the phr"
W11-1604,P09-1094,0,0.0134462,"stract The similarity between Paraphrase Generation and MT suggests that methods and tools originally developed for MT could be exploited for Paraphrase Generation. One popular approach – arguably the most successful so far – is Statistical Phrase-based Machine Translation (PBMT), which learns phrase translation rules from aligned bilingual text corpora (Och et al., 1999; Vogel et al., 2000; Zens et al., 2002; Koehn et al., 2003). Prior work has explored the use of PBMT for paraphrase generation (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Madnani et al., 2007; Callison-Burch, 2008; Zhao et al., 2009; Wubben et al., 2010) Paraphrase generation can be regarded as machine translation where source and target language are the same. We use the Moses statistical machine translation toolkit for paraphrasing, comparing phrase-based to syntax-based approaches. Data is derived from a recently released, large scale (2.1M tokens) paraphrase corpus for Dutch. Preliminary results indicate that the phrase-based approach performs better in terms of NIST scores and produces paraphrases at a greater distance from the source. 1 Antal van den Bosch Tilburg University P.O. Box 90135 5000 LE Tilburg The Nether"
W11-1604,W06-3119,0,0.0349578,"or subtrees are extracted, presupposing the availability of syntactic structures for source, target, or both languages. Incorporating syntax can guide the translation process and unlike phrase-based MT syntax it enables the modeling of long-distance translation patterns. Syntax-based systems may parse the data on the target side (string-to-tree), source side (tree-tostring), or both (tree-to-tree). In our experiments we use tree-to-tree syntaxbased MT. We also experiment with relaxing the parses by a method proposed under the label of syntax-augmented machine translation (SAMT), described in (Zollmann and Venugopal, 2006). This method combines any neighboring nodes and labels previously unlabeled nodes, removing the syntactic constraint on the grammar1 . We train all systems on the DAESO data (218,102 lines of aligned sentences) and test on a held-out set consisting of manually aligned headlines that ap1 This method is implemented in the Moses package in the program relax-parse as option SAMT 4 Table 2: Examples of output of the phrase-based and syntax-based systems Source Phrase-based Syntax-based jongen ( 7 ) zwaargewond na aanrijding 7-jarige gewond na botsing jongen ( 7 ) zwaar gewond na aanrijding boy (7)"
W11-1604,C00-2163,0,\N,Missing
W11-1604,W09-0401,0,\N,Missing
W11-1604,P07-2045,0,\N,Missing
W11-1708,N06-1038,0,0.0338219,"the social networks of the characters in a novel. Though this work is most related regarding the type of data used, their method can be considered complementary to ours: where they relate entities based on their conversational interaction without further analysis of the content, we try to find connections based solely on the words that occur in the text. Efforts in more general relation extraction from text have focused on finding recurring patterns and transforming them into triples (RDF). Relation types and labels are then deduced from the most common patterns (Ravichandran and Hovy, 2002; Culotta et al, 2006). These approaches work well for the induction and verification of straightforwardly verbalized factoids, but they are too restricted to capture the multitude of aspects that surround human interaction; a case in point is the kind of relationship between two persons, which people can usually infer from the text, but is rarely explicitly described in a single triple. 2.2 Sentiment Analysis Sentiment analysis is concerned with locating and classifying the subjective information contained in a source. Subjectivity is inherently dependent on human interpretation and emotion. A machine can be taugh"
W11-1708,P10-1015,0,0.0719236,"th entities are mentioned. A more elaborate approach to network mining is taken by Mika (2005) in his presentation 62 of the Flink system. In addition to Web cooccurrence counts of person names, the system uses data mined from other—highly structured—sources such as email headers, publication archives and socalled Friend-Of-A-Friend (FOAF) profiles. Cooccurrence counts of a name and different interests taken from a predefined set are used to determine a person’s expertise and to enrich their profile. These profiles are then used to resolve named entity coreference and to find new connections. Elson et al (2010) use quoted speech attribution to reconstruct the social networks of the characters in a novel. Though this work is most related regarding the type of data used, their method can be considered complementary to ours: where they relate entities based on their conversational interaction without further analysis of the content, we try to find connections based solely on the words that occur in the text. Efforts in more general relation extraction from text have focused on finding recurring patterns and transforming them into triples (RDF). Relation types and labels are then deduced from the most c"
W11-1708,P06-2063,0,0.0149461,"understand, for instance, that a sarcastic comment is not meant to be taken literally. Although the general distinction between negative and positive is intuitive for humans to make, subjectivity and sentiment are very much domain and context dependent. Depending on the domain and context, a single sentence can have opposite meanings (Pang and Lee, 2008). Many of the approaches to automatically solving tasks like these involve using lists of positively and negatively polarized words or phrases to calculate the overall sentiment of a clause, sentence or document (Pang et al, 2002). As shown by Kim and Hovy (2006), the order of the words potentially influences the interpretation of a text. Pang et al (2002) also found that the simple presence of a word is more important than the number of times it appears. Word sense disambiguation can be a useful tool in determining polarity. Turney (2002) proposed a simple, but seemingly effective way to determine polarity at the word level. He calculates the difference between the mutual information gain of a phrase and the word ’excellent’ and of the same phrase and the word ’poor’. 3 Method, Data, and Annotation 3.1 Method In contrast to most previous work regardi"
W11-1708,W02-1011,0,0.0195857,"o is what makes humans able to understand, for instance, that a sarcastic comment is not meant to be taken literally. Although the general distinction between negative and positive is intuitive for humans to make, subjectivity and sentiment are very much domain and context dependent. Depending on the domain and context, a single sentence can have opposite meanings (Pang and Lee, 2008). Many of the approaches to automatically solving tasks like these involve using lists of positively and negatively polarized words or phrases to calculate the overall sentiment of a clause, sentence or document (Pang et al, 2002). As shown by Kim and Hovy (2006), the order of the words potentially influences the interpretation of a text. Pang et al (2002) also found that the simple presence of a word is more important than the number of times it appears. Word sense disambiguation can be a useful tool in determining polarity. Turney (2002) proposed a simple, but seemingly effective way to determine polarity at the word level. He calculates the difference between the mutual information gain of a phrase and the word ’excellent’ and of the same phrase and the word ’poor’. 3 Method, Data, and Annotation 3.1 Method In contr"
W11-1708,P02-1006,0,0.0580415,"ch attribution to reconstruct the social networks of the characters in a novel. Though this work is most related regarding the type of data used, their method can be considered complementary to ours: where they relate entities based on their conversational interaction without further analysis of the content, we try to find connections based solely on the words that occur in the text. Efforts in more general relation extraction from text have focused on finding recurring patterns and transforming them into triples (RDF). Relation types and labels are then deduced from the most common patterns (Ravichandran and Hovy, 2002; Culotta et al, 2006). These approaches work well for the induction and verification of straightforwardly verbalized factoids, but they are too restricted to capture the multitude of aspects that surround human interaction; a case in point is the kind of relationship between two persons, which people can usually infer from the text, but is rarely explicitly described in a single triple. 2.2 Sentiment Analysis Sentiment analysis is concerned with locating and classifying the subjective information contained in a source. Subjectivity is inherently dependent on human interpretation and emotion."
W11-1708,P02-1053,0,0.00595127,"t, a single sentence can have opposite meanings (Pang and Lee, 2008). Many of the approaches to automatically solving tasks like these involve using lists of positively and negatively polarized words or phrases to calculate the overall sentiment of a clause, sentence or document (Pang et al, 2002). As shown by Kim and Hovy (2006), the order of the words potentially influences the interpretation of a text. Pang et al (2002) also found that the simple presence of a word is more important than the number of times it appears. Word sense disambiguation can be a useful tool in determining polarity. Turney (2002) proposed a simple, but seemingly effective way to determine polarity at the word level. He calculates the difference between the mutual information gain of a phrase and the word ’excellent’ and of the same phrase and the word ’poor’. 3 Method, Data, and Annotation 3.1 Method In contrast to most previous work regarding social network extraction, we do not possess any explicit record of the network we are after. Although the documents we work with are available online, the number of hyperlinks between them is minimal and all personal relations are expressed only in running text. We aim to train"
W12-2034,W12-2006,0,0.037301,"us, the first two classifiers determine the presence of a determiner or a preposition between all words in a text in which the actual determiners and prepositions are masked. The second pair of classifiers determines which is the most likely correction given a masked determiner or preposition. The hyperparameters that govern the classifiers are optimized on the shared task training data. We point out a number of obvious improvements to boost the medium-level scores attained by the system. 1 Introduction Our Valkuil.net team entry, known under the abbreviation ’VA’ in the HOO 2012 Shared Task (Dale et al., 2012), is a simplistic text correction system based on four memory-based classifiers. The goal of the system is to be lightweight: simple to set up and train, fast in execution. It requires a (preferably very large) corpus to train on, and a closed list of words which together form the category of interest—in the HOO 2012 Shared Task context, the two categories of interest are prepositions and determiners. As a corpus we used the Google 1TB 5-gram corpus (Brants and Franz, 2006), and we used two lists, one consisting of 47 prepositions and one consisting of 24 determiners, both extracted from the H"
W12-2034,W06-3604,1,0.564718,"Missing"
W13-1605,P09-2041,0,0.111105,"Missing"
W13-1605,W09-1304,0,0.0142208,"Missing"
W13-1605,D08-1075,0,0.0119069,"Missing"
W13-2702,J03-1002,0,0.00938034,"on approach based on character bigrams performs best. 2 More work has been done in the area of translating between closely related languages and dealing with data sparsity that occurs within these language pairs (Hajiˇc et al., 2000; Van Huyssteen and Pilon, 2009). Koehn et al. (2003) have shown that there is a direct negative correlation between the size of the vocabulary of a language and the accuracy of the translation. Alignment models are directly affected by data sparsity. Uncommon words are more likely to be aligned incorrectly to other words or, even worse, to large segments of words (Och and Ney, 2003). Out of vocabulary (OOV) words also pose a problem in the translation process, as systems are unable to provide translations for these words. A standard heuristic is to project them into the translated sentence untranslated. Related work Language transformation by machine translation within a language is a task that has not been studied extensively before. Related work is the study by Xu et al. (2012). They evaluate paraphrase systems that attempt to paraphrase a specific style of writing into another style. The plays of William Shakespeare and the modern translations of these works are used"
W13-2702,W12-4406,0,0.0413929,"Missing"
W13-2702,popovic-ney-2004-towards,0,0.0269097,"n from the Modern and Middle Polish Bible. The correspondences are extracted using machine translation with the aim of deriving historical grammar and lexical items. A larger amount of work has been published about spelling normalization of historical texts. Baron and Rayson (2008) developed tools for research in Early Modern English. Their tool, VARD 2, finds candidate modern form replacements for spelling variants in historical texts. It makes use of a Various solutions to data sparsity have been studied, among them the use of part-of-speech tags, suffixes and word stems to normalize words (Popovic and Ney, 2004; De Gispert and Marino, 2006), the treatment of compound words in translation (Koehn and Knight, 2003), transliteration of names and named entities, and advanced models that combine transliteration and translation (Kondrak et al., 2003; Finch et al., 2012) or learn unknown words by analogical reasoning (Langlais and Patry, 2007). Vilar et al. (2007) investigate a way to handle data sparsity in machine translation between closely related languages by translating between characters as opposed to words. The words in the parallel sentences are segmented into characters. Spaces between words are m"
W13-2702,A00-1002,0,0.107651,"Missing"
W13-2702,2009.eamt-1.3,0,0.0217281,"ters as opposed to words. The words in the parallel sentences are segmented into characters. Spaces between words are marked with a special character. The sequences of characters are then used to train a standard machine translation model and a language model with n-grams up to n = 16. They apply their system to the translation between the related languages Spanish and Catalan, and find that a word based system outperforms their 12 tion 7. We close with a discussion of our results in Section 8. letter-based system. However, a combined system performs marginally better in terms of BLEU scores. Tiedemann (2009) shows that combining character-based translation with phrase-based translation improves machine translation quality in terms of BLEU and NIST scores when translating between Swedish and Norwegian if the OOV-words are translated beforehand with the character-based model. Nakov and Tiedemann (2012) investigate the use of character-level models in the translation between Macedonian and Bulgarian movie subtitles. Their aim is to translate between the resource poor language Macedonian to the related language Bulgarian, in order to use Bulgarian as a pivot in order to translate to other languages s"
W13-2702,P03-1040,0,0.0383485,"n with the aim of deriving historical grammar and lexical items. A larger amount of work has been published about spelling normalization of historical texts. Baron and Rayson (2008) developed tools for research in Early Modern English. Their tool, VARD 2, finds candidate modern form replacements for spelling variants in historical texts. It makes use of a Various solutions to data sparsity have been studied, among them the use of part-of-speech tags, suffixes and word stems to normalize words (Popovic and Ney, 2004; De Gispert and Marino, 2006), the treatment of compound words in translation (Koehn and Knight, 2003), transliteration of names and named entities, and advanced models that combine transliteration and translation (Kondrak et al., 2003; Finch et al., 2012) or learn unknown words by analogical reasoning (Langlais and Patry, 2007). Vilar et al. (2007) investigate a way to handle data sparsity in machine translation between closely related languages by translating between characters as opposed to words. The words in the parallel sentences are segmented into characters. Spaces between words are marked with a special character. The sequences of characters are then used to train a standard machine t"
W13-2702,N03-1017,0,0.00334008,"s a dictionary to improve the statistical alignment process. The PBMT approach based on character bigrams rather than translating words, transliterates character bigrams and in this way improves the transformation process. We demonstrate that these two approaches outperform standard PBMT in this task, and that the PBMT transliteration approach based on character bigrams performs best. 2 More work has been done in the area of translating between closely related languages and dealing with data sparsity that occurs within these language pairs (Hajiˇc et al., 2000; Van Huyssteen and Pilon, 2009). Koehn et al. (2003) have shown that there is a direct negative correlation between the size of the vocabulary of a language and the accuracy of the translation. Alignment models are directly affected by data sparsity. Uncommon words are more likely to be aligned incorrectly to other words or, even worse, to large segments of words (Och and Ney, 2003). Out of vocabulary (OOV) words also pose a problem in the translation process, as systems are unable to provide translations for these words. A standard heuristic is to project them into the translated sentence untranslated. Related work Language transformation by m"
W13-2702,W07-0705,0,0.0206163,"2, finds candidate modern form replacements for spelling variants in historical texts. It makes use of a Various solutions to data sparsity have been studied, among them the use of part-of-speech tags, suffixes and word stems to normalize words (Popovic and Ney, 2004; De Gispert and Marino, 2006), the treatment of compound words in translation (Koehn and Knight, 2003), transliteration of names and named entities, and advanced models that combine transliteration and translation (Kondrak et al., 2003; Finch et al., 2012) or learn unknown words by analogical reasoning (Langlais and Patry, 2007). Vilar et al. (2007) investigate a way to handle data sparsity in machine translation between closely related languages by translating between characters as opposed to words. The words in the parallel sentences are segmented into characters. Spaces between words are marked with a special character. The sequences of characters are then used to train a standard machine translation model and a language model with n-grams up to n = 16. They apply their system to the translation between the related languages Spanish and Catalan, and find that a word based system outperforms their 12 tion 7. We close with a discussion"
W13-2702,W10-4223,1,0.910792,"Missing"
W13-2702,C12-1177,0,0.0160675,"of the translation. Alignment models are directly affected by data sparsity. Uncommon words are more likely to be aligned incorrectly to other words or, even worse, to large segments of words (Och and Ney, 2003). Out of vocabulary (OOV) words also pose a problem in the translation process, as systems are unable to provide translations for these words. A standard heuristic is to project them into the translated sentence untranslated. Related work Language transformation by machine translation within a language is a task that has not been studied extensively before. Related work is the study by Xu et al. (2012). They evaluate paraphrase systems that attempt to paraphrase a specific style of writing into another style. The plays of William Shakespeare and the modern translations of these works are used in this study. They show that their models outperform baselines based on dictionaries and out-of-domain parallel text. Their work differs from our work in that they target writing in a specific literary style and we are interested in translating between diachronic variants of a language. Work that is slightly comparable is the work by Zeldes (2007), who extrapolates correspondences in a small parallel"
W13-2702,2005.mtsummit-papers.11,0,0.00496556,"@let.ru.nl Abstract area that is now defined as the Netherlands and parts of Belgium. One of the factors that make Middle Dutch difficult to read is the fact that at the time no overarching standard language existed. Modern Dutch is defined as Dutch as spoken from 1500. The variant we investigate is contemporary Dutch. An important difference with regular paraphrasing is the amount of parallel data available. The amount of parallel data for the variant pair Middle Dutch - Modern Dutch is several orders of magnitude smaller than bilingual parallel corpora typically used in machine translation (Koehn, 2005) or monolingual parallel corpora used for paraphrase generation by machine translation (Wubben et al., 2010). We do expect many etymologically related words to show a certain amount of character overlap between the Middle and Modern variants. An example of the data is given below, from the work ’Van den vos Reynaerde’ (‘About Reynard the Fox’), part of the Comburg manuscript that was written between 1380-1425. Here, the first text is the original text, the second text is a modern translation in Dutch by Walter Verniers and a translation in English is added below that for clarity. Language tran"
W13-2702,N03-2016,0,0.0458324,"of historical texts. Baron and Rayson (2008) developed tools for research in Early Modern English. Their tool, VARD 2, finds candidate modern form replacements for spelling variants in historical texts. It makes use of a Various solutions to data sparsity have been studied, among them the use of part-of-speech tags, suffixes and word stems to normalize words (Popovic and Ney, 2004; De Gispert and Marino, 2006), the treatment of compound words in translation (Koehn and Knight, 2003), transliteration of names and named entities, and advanced models that combine transliteration and translation (Kondrak et al., 2003; Finch et al., 2012) or learn unknown words by analogical reasoning (Langlais and Patry, 2007). Vilar et al. (2007) investigate a way to handle data sparsity in machine translation between closely related languages by translating between characters as opposed to words. The words in the parallel sentences are segmented into characters. Spaces between words are marked with a special character. The sequences of characters are then used to train a standard machine translation model and a language model with n-grams up to n = 16. They apply their system to the translation between the related langu"
W13-2702,D07-1092,0,0.0148357,"English. Their tool, VARD 2, finds candidate modern form replacements for spelling variants in historical texts. It makes use of a Various solutions to data sparsity have been studied, among them the use of part-of-speech tags, suffixes and word stems to normalize words (Popovic and Ney, 2004; De Gispert and Marino, 2006), the treatment of compound words in translation (Koehn and Knight, 2003), transliteration of names and named entities, and advanced models that combine transliteration and translation (Kondrak et al., 2003; Finch et al., 2012) or learn unknown words by analogical reasoning (Langlais and Patry, 2007). Vilar et al. (2007) investigate a way to handle data sparsity in machine translation between closely related languages by translating between characters as opposed to words. The words in the parallel sentences are segmented into characters. Spaces between words are marked with a special character. The sequences of characters are then used to train a standard machine translation model and a language model with n-grams up to n = 16. They apply their system to the translation between the related languages Spanish and Catalan, and find that a word based system outperforms their 12 tion 7. We clo"
W13-2702,P12-2059,0,0.0542666,"hey apply their system to the translation between the related languages Spanish and Catalan, and find that a word based system outperforms their 12 tion 7. We close with a discussion of our results in Section 8. letter-based system. However, a combined system performs marginally better in terms of BLEU scores. Tiedemann (2009) shows that combining character-based translation with phrase-based translation improves machine translation quality in terms of BLEU and NIST scores when translating between Swedish and Norwegian if the OOV-words are translated beforehand with the character-based model. Nakov and Tiedemann (2012) investigate the use of character-level models in the translation between Macedonian and Bulgarian movie subtitles. Their aim is to translate between the resource poor language Macedonian to the related language Bulgarian, in order to use Bulgarian as a pivot in order to translate to other languages such as English. Their research shows that using character bigrams shows improvement over a word-based baseline. It seems clear that character overlap can be used to improve translation quality in related languages. We therefore use character overlap in language transformation between two diachroni"
W13-2702,P07-2045,0,\N,Missing
W13-3614,W12-2006,0,0.0606542,"Nijmegen P.O. Box 90153 P.O. Box 9103 NL-6500 HD Nijmegen, The Netherlands NL-5000 LE Tilburg, The Netherlands a.vandenbosch@let.ru.nl p.j.berck@tilburguniversity.edu Abstract real-world situations and languages. The system described in this article takes plain text as input and produces plain text as output. Memory-based classifiers have been applied to similar tasks before. (Van den Bosch, 2006) describes memory based classifiers used for confusible disambiguation, and (Stehouwer and Van den Bosch, 2009) shows how agreement errors can be detected. In the 2012 shared task ’Helping Our Own’ (Dale et al., 2012) memory based classifiers were used to solve the problem of missing and incorrect determiners and prepositions (Van den Bosch and Berck, 2012). The C ON LL-2013 Shared Task context limited the grammatical error correction task to detecting and correcting five error types: We describe the ’TILB’ team entry for the C ON LL-2013 Shared Task. Our system consists of five memory-based classifiers that generate correction suggestions for center positions in small text windows of two words to the left and to the right. Trained on the Google Web 1T corpus, the first two classifiers determine the presen"
W13-3614,W12-2034,1,0.45963,"Missing"
W13-3614,W06-3604,1,0.878128,"Missing"
W13-3614,N12-1067,0,\N,Missing
W14-1302,P06-1145,0,0.0111338,"as the Web by means of time queries (Baeza Yates, 2005). Various studies have used temporal expression elements as features in an automatic setting to improve the relevance estimation of a web document (Dias et al., 2011; Jatowt and Au Yeung, 2011). Information relevant to event times has been the focus of studies such as those by Becker et al. (2012) and Kawai et al. (2010). Our research is aimed at estimating the time to event of an upcoming event as precisely as possible. Radinsky et al. (2012) approach this problem by learning from causality pairs in texts from longranging news articles. Noro et al. (2006) describe a machine-learning-based system for the identification of the time period in which an event will happen, such as in the morning or at night. Some case studies are focused on detecting events as early as possible as their unfolding is fast. The study by Sakaki et al. (2010) describes a system which analyzes the flow of tweets in time and place mentioning an earthquake, to predict the unfolding quake pattern which may in turn provide just-in-time alerts to people residing in the locations that are likely to be struck shortly. Zielinski et al. (2012) developed an early warning system to"
W14-1304,W11-0705,0,0.0401995,"p://en.wikipedia.org/wiki/ Gezelligheid 31 Figure 1: Precision at {1 . . . 250} on the classes #zinin (top left), #fml (top right), #geenzin (bottom left), and #omg (bottom right). as hyperboles, exclamations and emoticons to help readers to correctly interpret the message (Burgers et al., 2012; Liebrecht et al., 2013). We argue that this is also the case for emotional messages. surrogates for vocal segregates (hmmm) (Carey, 1980). Later he also recognizes emoticons as nonverbal emotion cues (Walther and D’Addario, 2001). Emoticons can serve many purposes, one of which is expressing emotions (Agarwal et al., 2011; Davidov et al., 2010). Tweets are written messages with a strongly restricted length. Authors compensate the lack of non-verbal cues by adding emotion markers. This hypothesis is supported by research in the field of Computer-Mediated Communication (CMC), where many studies have been carried out on (the lack of) non-verbal emotional cues in (electronic) messages. Walther (1992) introduced the Social Information Processing Perspective: a theory that users can develop relationships via CMC if they have sufficient time and message exchanges and if communicative cues, such as non-verbal emotiona"
W14-1304,H05-1073,0,0.164368,"iversity of Amsterdam {f.kunneman,a.vandenbosch}@let. c.c.liebrecht@uva.nl ru.nl Abstract stand the sentiment underlying real world events and topics. Potentially, Twitter offers a vast amount of data to exploit for the construction of computational models able to detect certain sentiments or emotions in unseen tweets. Yet, in the typical scenario of applying supervised machine learning classifiers, some annotation effort will be required to label sentiments and emotions reliably. Currently there are two main approaches to labeling tweets. The first is the annotation of data by human experts (Alm et al., 2005; Aman and Szpakowicz, 2007). This approach is known to result in highprecision annotated data, but is labor-intensive and time-consuming. The second approach is to use the annotations that Twitter users themselves add to a tweet: hashtags. A hashtag (a word prefixed by the typographical hashmark #) is an explicitly marked keyword that may also serve as a word in the context of the other non-tagged words of the post. The usage of a hashtag in Twitter serves many purposes beyond mere categorization, most of which are conversational (Huang et al., 2010). Hashtags expressing emotions are often us"
W14-1304,P80-1018,0,0.185125,"itter have other cues at their disposal. Previous studies show, for example, that they might mark the irony or sarcasm in their message by using linguistic markers such 2 See http://en.wikipedia.org/wiki/ Gezelligheid 31 Figure 1: Precision at {1 . . . 250} on the classes #zinin (top left), #fml (top right), #geenzin (bottom left), and #omg (bottom right). as hyperboles, exclamations and emoticons to help readers to correctly interpret the message (Burgers et al., 2012; Liebrecht et al., 2013). We argue that this is also the case for emotional messages. surrogates for vocal segregates (hmmm) (Carey, 1980). Later he also recognizes emoticons as nonverbal emotion cues (Walther and D’Addario, 2001). Emoticons can serve many purposes, one of which is expressing emotions (Agarwal et al., 2011; Davidov et al., 2010). Tweets are written messages with a strongly restricted length. Authors compensate the lack of non-verbal cues by adding emotion markers. This hypothesis is supported by research in the field of Computer-Mediated Communication (CMC), where many studies have been carried out on (the lack of) non-verbal emotional cues in (electronic) messages. Walther (1992) introduced the Social Informati"
W14-1304,C10-2028,0,0.025112,"iki/ Gezelligheid 31 Figure 1: Precision at {1 . . . 250} on the classes #zinin (top left), #fml (top right), #geenzin (bottom left), and #omg (bottom right). as hyperboles, exclamations and emoticons to help readers to correctly interpret the message (Burgers et al., 2012; Liebrecht et al., 2013). We argue that this is also the case for emotional messages. surrogates for vocal segregates (hmmm) (Carey, 1980). Later he also recognizes emoticons as nonverbal emotion cues (Walther and D’Addario, 2001). Emoticons can serve many purposes, one of which is expressing emotions (Agarwal et al., 2011; Davidov et al., 2010). Tweets are written messages with a strongly restricted length. Authors compensate the lack of non-verbal cues by adding emotion markers. This hypothesis is supported by research in the field of Computer-Mediated Communication (CMC), where many studies have been carried out on (the lack of) non-verbal emotional cues in (electronic) messages. Walther (1992) introduced the Social Information Processing Perspective: a theory that users can develop relationships via CMC if they have sufficient time and message exchanges and if communicative cues, such as non-verbal emotional cues, are available."
W14-1304,pak-paroubek-2010-twitter,0,0.0680855,"and emotions are important aspects of status updates and conversations in Twitter messages (Ritter et al., 2010; Dann, 2010). Many Twitter messages (tweets) express an emotion of the sender: according to Roberts et al. (2012), 43 percent of the 7,000 tweets they collected are an emotional expression. Automatically detecting the emotion in tweets is key to under26 Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 26–34, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics vision. In the field of sentiment analysis, Pak and Paroubek (2010) and Go, Bhayani and Huang (2009) select emoticons representing positive and negative sentiment to collect tweets with either of the polarities. Several studies focusing on the specific task of emotion detection in Twitter also apply distant supervision. The studies in which it is applied vary in a number of ways. First, the type of markers by which data is collected differs. Most often only hashtags are used, occasionally combined with emoticons. Davidov, Tsur and Rappoport (2010) use hashtags and emoticons as distinct prediction labels and find that they are equally useful. Suttles and Ide ("
W14-1304,E12-1049,0,0.03458,"ith its conventions for hashtags as extra-linguistic markers, Twitter is a potentially suitable platform for implementing classification based on distant super1 27 http://en.wikipedia.org/wiki/Emoji tweet) would predict the hashtag. If this is the case, our classifier should score well on the retrieval of unseen tweets containing the hashtag (the first evaluation). Consistency can arise from many different types of features, ranging from topical words to emotionbearing words. classification is performed, distinguishing the different target emotions and optionally an emotionally neutral class (Purver and Battersby, 2012; Wang et al., 2012). The multitude of classes, class imbalance, and the possibility of single tweets conveying multiple emotions make this a challenging task. The alternative is to train a binary classifier for each emotion (Mohammad, 2012; Qadir and Riloff, 2013; Suttles and Ide, 2013), deciding for each unseen tweet whether it conveys the trained emotion. We apply the latter type of classification. The fourth and final variation is the way in which classification is evaluated. In the discussed papers, evaluation is either performed in a ten-fold cross-validation setting or by testing the tr"
W14-1304,W13-1602,0,0.081748,"should score well on the retrieval of unseen tweets containing the hashtag (the first evaluation). Consistency can arise from many different types of features, ranging from topical words to emotionbearing words. classification is performed, distinguishing the different target emotions and optionally an emotionally neutral class (Purver and Battersby, 2012; Wang et al., 2012). The multitude of classes, class imbalance, and the possibility of single tweets conveying multiple emotions make this a challenging task. The alternative is to train a binary classifier for each emotion (Mohammad, 2012; Qadir and Riloff, 2013; Suttles and Ide, 2013), deciding for each unseen tweet whether it conveys the trained emotion. We apply the latter type of classification. The fourth and final variation is the way in which classification is evaluated. In the discussed papers, evaluation is either performed in a ten-fold cross-validation setting or by testing the trained classifier on a small, manually annotated set of tweets. We deviate from these approaches by testing our classifiers on a large set of uncontrolled tweets gathered in a single day, thereby approximating the real world scenario in which emotion detection is a"
W14-1304,P11-2102,0,0.0275934,"words of the post. The usage of a hashtag in Twitter serves many purposes beyond mere categorization, most of which are conversational (Huang et al., 2010). Hashtags expressing emotions are often used in tweets and are therefore potentially useful annotations for training data. Wang et al. (2012) state that annotating interpretative labels by humans other than the author is not as reliable as having the data annotated by the author himself. As far as emotions can be self-observed and self-reported, authors arguably have the best information about their own emotions. Following Gonz´alez-Ib´an˜ ez et al. (2011), Mohammad (2012) presents several experiments to validate that the emotional labels in tweets are consistent and match intuitions of trained judges. Therefore, using hashtags as annotated training data may be useful for generating emotion detectors. Yet, not all hashtags are equally suitable for this task. Even a high level of consistency and predictability in hashtag usage might not be sufficient. Hashtags in Twitter posts may carry different semantic payloads. Their dual form (word and label) may serve to categorize the tweet, but may also add content to the message, or strengthen it. Some"
W14-1304,N10-1020,0,0.0213625,"a precision-at-250 of .7 for only two of the hashtags. We observe that some hashtags are predictable from their tweets, and strengthen the emotion already expressed in the tweets. Other hashtags are added to messages that do not predict them, presumably to provide emotional information that was not yet in the tweet. 1 Introduction Since the launch of Twitter in 2006 the microblogging service has proven to be a valuable source of research on the linguistic expression of sentiment and affect. Sentiments and emotions are important aspects of status updates and conversations in Twitter messages (Ritter et al., 2010; Dann, 2010). Many Twitter messages (tweets) express an emotion of the sender: according to Roberts et al. (2012), 43 percent of the 7,000 tweets they collected are an emotional expression. Automatically detecting the emotion in tweets is key to under26 Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 26–34, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics vision. In the field of sentiment analysis, Pak and Paroubek (2010) and Go, Bhayani and Huang (2009) select emoticons representing positive and negative se"
W14-1304,roberts-etal-2012-empatweet,0,0.162774,"tweets, and strengthen the emotion already expressed in the tweets. Other hashtags are added to messages that do not predict them, presumably to provide emotional information that was not yet in the tweet. 1 Introduction Since the launch of Twitter in 2006 the microblogging service has proven to be a valuable source of research on the linguistic expression of sentiment and affect. Sentiments and emotions are important aspects of status updates and conversations in Twitter messages (Ritter et al., 2010; Dann, 2010). Many Twitter messages (tweets) express an emotion of the sender: according to Roberts et al. (2012), 43 percent of the 7,000 tweets they collected are an emotional expression. Automatically detecting the emotion in tweets is key to under26 Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 26–34, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics vision. In the field of sentiment analysis, Pak and Paroubek (2010) and Go, Bhayani and Huang (2009) select emoticons representing positive and negative sentiment to collect tweets with either of the polarities. Several studies focusing on the specific task of emotion"
W14-1304,W13-1605,1,0.450817,"Missing"
W14-1304,P09-1113,0,0.0189253,"e-learning-based text classification. We put this method into practice for a number of hashtags expressing emotion in Dutch tweets. The novel contribution of this research lies in the fact that we offer an objective, empirical handle of the two usages of emotion hashtags as formulated by Mohammad (2012). Furthermore, we exemplify a new type of study that tests our hypothesis in the realistic scenario of testing on a full day of streaming tweets with no filtering. 2 Related research Leveraging uncontrolled labeling to obtain large amounts of training data is referred to as distant supervision (Mintz et al., 2009). With its conventions for hashtags as extra-linguistic markers, Twitter is a potentially suitable platform for implementing classification based on distant super1 27 http://en.wikipedia.org/wiki/Emoji tweet) would predict the hashtag. If this is the case, our classifier should score well on the retrieval of unseen tweets containing the hashtag (the first evaluation). Consistency can arise from many different types of features, ranging from topical words to emotionbearing words. classification is performed, distinguishing the different target emotions and optionally an emotionally neutral clas"
W14-1304,S12-1033,0,0.5909,"The usage of a hashtag in Twitter serves many purposes beyond mere categorization, most of which are conversational (Huang et al., 2010). Hashtags expressing emotions are often used in tweets and are therefore potentially useful annotations for training data. Wang et al. (2012) state that annotating interpretative labels by humans other than the author is not as reliable as having the data annotated by the author himself. As far as emotions can be self-observed and self-reported, authors arguably have the best information about their own emotions. Following Gonz´alez-Ib´an˜ ez et al. (2011), Mohammad (2012) presents several experiments to validate that the emotional labels in tweets are consistent and match intuitions of trained judges. Therefore, using hashtags as annotated training data may be useful for generating emotion detectors. Yet, not all hashtags are equally suitable for this task. Even a high level of consistency and predictability in hashtag usage might not be sufficient. Hashtags in Twitter posts may carry different semantic payloads. Their dual form (word and label) may serve to categorize the tweet, but may also add content to the message, or strengthen it. Some hashtags are rela"
W15-2414,P13-1130,0,0.0272052,"Somehow and at some point in language acquisition, children learn these preferences, but it takes several years before children approximate adult language use. Monitoring and modeling this process of development may shed light on the inner workings of language learning in general, but to keep experiments under control, most studies, including the one presented here, zoom in on a representative but specific phenomenon. The dative alternation has been the topic of several studies in which computational models are trained on naturalistic data (Perfors et al., 2010; Parisien and Stevenson, 2010; Villavicencio et al., 2013; Conwell et al., 2011), such as offered by the Child Language Data Exchange System (CHILDES) (MacWhinney, 2000), a publicly available database of children’s speech produced in a natural environment. These approaches address what is conventionally known as “Baker’s paradox” (Baker, 1979; Pinker, 1989), which can be phrased as the question how children learn not to generalize a syntactic alternation to cases that block alternation, such as the verb ’donate’, which only allows the prepositional dative construction. In contrast, the present contribution continues a line of research introduced by"
W15-2414,E14-1034,1,0.88527,"Missing"
W16-6608,P05-1074,0,0.0273893,"ble rewrites. By using discriminative training, a weight is assigned to each grammar rule. These grammar rules are then used to generate compressions by a decoder. In contrast to the large body of work on extractive sentence compression, work on abstractive sentence compression is relatively sparse. (Cohn et al., 2008) propose an abstractive sentence compression method based on a parse tree transduction grammar and Integer Linear Programming. For their abstractive model, the grammar that is extracted is augmented with paraphrasing rules obtained from a pivoting approach to a bilingual corpus (Bannard and Burch, 2005). They show that the abstractive model outperforms an extractive model on their dataset.(Cohn and Lapata, 2013) follow up on earlier work and describe a discriminative tree-to-tree transduction model that can handle mismatches on the structural and lexical level. There has been some work on the related task of sentence simplification. (Coster and Kauchak, 2011; Zhu et al., 2010) develop models using data from Simple English Wikipedia paired with English Wikipedia. Their models were able to perform rewording, reordering, insertion and deletion actions. (Woodsend and Lapata, 2011) use Simple Wik"
W16-6608,D14-1179,0,0.0286808,"Missing"
W16-6608,D07-1008,0,0.644752,"years (Lloret and Palomar, 2012). Extractive sentence compression entails finding a subset of words in the source sentence that can be dropped to create a new, shorter sentence that is still grammatical and contains the most important information. More formally, the aim is to shorten a sentence x = x1 , x2 , ..., xn into a substring y = y1 , y2 , ..., ym where all words in y also occur in x in the same order and m < n. A number of techniques have been used for extractive sentence compression, ranging from the noisy-channel model (Knight and Marcu, 2002), large-margin learning (McDonald, 2006; Cohn and Lapata, 2007) to Integer Linear Programming (Clarke and Lapata, 2008). (Marsi et al., 2010) characterize these approaches in terms of two assumptions: (1) only word deletions are allowed and (2) the word order is fixed. They argue that these constraints rule out more complicated operations such as reordering, substitution and insertion, and reduce the sentence compression task to a word deletion task. This does not model human sentence compression accurately, as humans tend to paraphrase when summarizing (Jing and McKeown, 2000), resulting in an abstractive compression of the source sentence. Recent advanc"
W16-6608,C08-1018,0,0.500283,"et al., 2011), we selected only those cases with rougly equal character compression rate (we limited this by selecting within a 0.1 CCR resolution). From this selection, we randomly selected 30 source sentences with their corresponding system outputs and one short human description which served as the human compression. 46 We used Crowdflower6 to perform the evaluation study. CrowdFlower is a platform for data annotation by the crowd. We allowed only native English speakers with a trust level of minimally 90 percent to partcipate. Following earlier evaluation studies (Clarke and Lapata, 2008; Cohn and Lapata, 2008; Wubben et al., 2012) we asked 25 participants to evaluate Fluency and Importance of the target compressions on a seven point Likert scale. Fluency was defined in the instructions as the extent to which a sentence is in proper, grammatical English. Importance was defined as the extent to which the sentence has retained the important information from the source sentence. The order of the output of the various systems was randomized. The participants saw 30 source descriptions and for each source description they evaluated all three compressions: the aRNN, Moses and Human compression. They were"
W16-6608,J08-4005,0,0.0119509,"ing is used to combine the features and weight 42 their contribution to a successful compression. (Cohn and Lapata, 2007) cast the sentence compression problem as a tree-to-tree rewriting task. For this task, they train a synchronous tree substitution grammar, which dictates the space of all possible rewrites. By using discriminative training, a weight is assigned to each grammar rule. These grammar rules are then used to generate compressions by a decoder. In contrast to the large body of work on extractive sentence compression, work on abstractive sentence compression is relatively sparse. (Cohn et al., 2008) propose an abstractive sentence compression method based on a parse tree transduction grammar and Integer Linear Programming. For their abstractive model, the grammar that is extracted is augmented with paraphrasing rules obtained from a pivoting approach to a bilingual corpus (Bannard and Burch, 2005). They show that the abstractive model outperforms an extractive model on their dataset.(Cohn and Lapata, 2013) follow up on earlier work and describe a discriminative tree-to-tree transduction model that can handle mismatches on the structural and lexical level. There has been some work on the"
W16-6608,W11-1601,0,0.157528,"aper we focus on abstractive sentence compression Proceedings of The 9th International Natural Language Generation conference, pages 41–50, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics with RNNs. In order to be applied to sentence compression, RNNs typically need to be trained on large data sets of aligned sequences. In the domain of abstractive sentence compression, not many of such data sets are available. For the related task of sentence simplification, data sets are available of aligned sentences from Wikipedia and Simple Wikipedia (Zhu et al., 2010; Coster and Kauchak, 2011). Recently, (Rush et al., 2015) used the Gigaword corpus to construct a large corpus containing headlines paired with the article’s first sentence. Here, we present a data set compiled from scene descriptions taken from the MSCOCO dataset (Lin et al., 2014). These descriptions are generally only one sentence long, and humans tend to describe photos in different ways, which makes this task suitable for abstractive sentence compression. For each image, we align long descriptions with shorter descriptions to construct a corpus of abstractive compressions . We employ an Attentive Recurrent Neural"
W16-6608,W05-0901,0,0.0410566,"rable variation between sentences. The general pattern for Fluency, in Figure 3, is comparable, but much more pronounced: Fluency scores for Moses are (much) lower than for aRNN, and the latter are very similar to those for the Human descriptions. 5.3 Correlations Interestingly, we found no significant correlations between the automatic measures and the human 7 https://github.com/cgevans/scikits-bootstrap 47 fluency 5.2 6 4 3 2 1 aRNN Moses system Human Figure 3: Fluency scores given by human subjects to the two systems and human description. judgements. This is in line with earlier findings (Dorr et al., 2005). We did find correlations between human judgements, as can be observed in Table 6. Strong correlations are reported between the Fluency and Importance for the systems, and moderate correlation for the Human compression. This indicates some difference in the nature of the errors the systems and the humans make. 5.4 Qualitative analysis When we look at the output in Table , we can observe a few interesting things. First, the human written descriptions sometimes contain errors, i.e. ’many toilets without its upper top part’. The aRNN system is robust to these errors as it can abstract away from"
W16-6608,D15-1042,0,0.281873,"Missing"
W16-6608,N07-1023,0,0.0299514,"n that the aRNN outperforms the Moses system and even performs on par with the human generated description. We also show that automatic measures such as ROUGE that are used generally to evaluate compression tasks do not correlate with human judgements. 2 Related work A large body of work is devoted to extractive sentence compression. Here, we mention a few. (Knight and Marcu, 2002) propose two models to generate a short sentence by deleting a subset of words: the decision tree model and the noisy channel model, both based on a synchronous context free grammar. (Turner and Charniak, 2005) and (Galley and McKeown, 2007) build upon this model reporting improved results. (McDonald, 2006) develop a system using largemargin online learning combined with a decoding algorithm that searches the compression space to produce a compressed sentence. Discriminative learning is used to combine the features and weight 42 their contribution to a successful compression. (Cohn and Lapata, 2007) cast the sentence compression problem as a tree-to-tree rewriting task. For this task, they train a synchronous tree substitution grammar, which dictates the space of all possible rewrites. By using discriminative training, a weight i"
W16-6608,W11-2123,0,0.0132897,"ranslation P (X|Y ) with a language model that outputs the most likely sentence P (Y ): Y˜ = arg max∗ P (X|Y )P (Y ) Y ∈Y Moses augments this model by regarding logP (X|Y ) as a loglinear model with added features and weights. During decoding, the sentence X is segmented into a sequence of I phrases. Each phrase is then translated into a phrase to form sentence Y . During this process phrases may be reordered. The GIZA++ statistical alignment package is used to perform the word alignments, which are later combined into phrase alignments in the Moses pipeline (Och and Ney, 2003) and the KenLM (Heafield, 2011) package is used to do language modelling on the target sentences. Because Moses performs Phrase-based Machine Translation where it is often not optimal to delete unaligned phrases from the source sentence, we pad the source sentence with special EMPTY tokens until the source and target sentences contain equally many tokens. We train the Moses system with default parameters on the 900,000 padded training pairs. Additionally, we train a KenLM language model on the target side sentences from the training set. We perform MERT tuning on the development set and manually set the word penalty weight"
W16-6608,A00-2024,0,0.0631613,"-channel model (Knight and Marcu, 2002), large-margin learning (McDonald, 2006; Cohn and Lapata, 2007) to Integer Linear Programming (Clarke and Lapata, 2008). (Marsi et al., 2010) characterize these approaches in terms of two assumptions: (1) only word deletions are allowed and (2) the word order is fixed. They argue that these constraints rule out more complicated operations such as reordering, substitution and insertion, and reduce the sentence compression task to a word deletion task. This does not model human sentence compression accurately, as humans tend to paraphrase when summarizing (Jing and McKeown, 2000), resulting in an abstractive compression of the source sentence. Recent advances in Recurrent Neural Networks (RNNs) have boosted interest in text-to-text generation tasks (Sutskever et al., 2014). In this paper we focus on abstractive sentence compression Proceedings of The 9th International Natural Language Generation conference, pages 41–50, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics with RNNs. In order to be applied to sentence compression, RNNs typically need to be trained on large data sets of aligned sequences. In the domain of abstractive sente"
W16-6608,W04-1013,0,0.0949412,"utomatic scores (BLEU scores, various ROUGE scores and character compression rates) as well as human judgements on two different dimensions (Fluency and Importance). model ARNN Moses Human BLEU 0.21 0.13 0.17 ROUGE 1 0.70 0.69 0.72 ROUGE 2 0.40 0.38 0.41 ROUGE 3 0.28 0.25 0.28 ROUGE 4 0.22 0.19 0.21 ROUGE SU4 0.49 0.48 0.50 Table 4: BLEU and ROUGE scores 4.2.1 Automatic Evaluation First, we perform automatic evaluation using regular summarization and text generation evaluation metrics, such as BLEU (Papineni et al., 2002), which is generally used for Machine Translation and variants of ROUGE (Lin, 2004), which is generally used for summarization evaluation. Both take into account reference sentences and calculate overlap on the n-gram level. ROUGE also accounts for compression. ROUGE 1-4 take into account unigrams up to four-grams and ROUGE SU4 also takes into account skipgrams. For BLEU we use multi-bleu.pl, and for ROUGE we used pyrouge. We also compute compression rate on the character level, as this tells us how much the source sentence has been compressed. We simply compute this by dividing the number of characters in the target sentence by the number of characters in the source sentenc"
W16-6608,W01-0100,0,0.341702,"ty of its output to a Phrasebased Machine Translation (PBMT) model and a human generated short description. An extensive evaluation is done using automatic measures and human judgements. We show that the neural model outperforms the PBMT model. Additionally, we show that automatic measures are not very well suited for evaluating this text-to-text generation task. 1 Introduction Text summarization is an important, yet challenging subfield of Natural Language Processing. Summarization can be defined as the process of finding the important items in a text and presenting them in a condensed form (Mani, 2001; Knight and Marcu, 2002). Summarization on the sentence level is called sentence compression. Sentence compression approaches can be classified into two categories: extractive and abstractive sentence compression. Most successful sentence compression models consist of extractive approaches that select the most relevant fragments from the source document and generate a shorter representation of this document by stitching the selected fragments together. In contrast, abstractive sentence compression is the process of producing a representation of the original sentence in a bottom-up manner. Thi"
W16-6608,E06-1038,0,0.484011,"ntion in recent years (Lloret and Palomar, 2012). Extractive sentence compression entails finding a subset of words in the source sentence that can be dropped to create a new, shorter sentence that is still grammatical and contains the most important information. More formally, the aim is to shorten a sentence x = x1 , x2 , ..., xn into a substring y = y1 , y2 , ..., ym where all words in y also occur in x in the same order and m < n. A number of techniques have been used for extractive sentence compression, ranging from the noisy-channel model (Knight and Marcu, 2002), large-margin learning (McDonald, 2006; Cohn and Lapata, 2007) to Integer Linear Programming (Clarke and Lapata, 2008). (Marsi et al., 2010) characterize these approaches in terms of two assumptions: (1) only word deletions are allowed and (2) the word order is fixed. They argue that these constraints rule out more complicated operations such as reordering, substitution and insertion, and reduce the sentence compression task to a word deletion task. This does not model human sentence compression accurately, as humans tend to paraphrase when summarizing (Jing and McKeown, 2000), resulting in an abstractive compression of the source"
W16-6608,W11-1611,0,0.196739,"Missing"
W16-6608,J03-1002,0,0.00483215,"model that finds the most likely translation P (X|Y ) with a language model that outputs the most likely sentence P (Y ): Y˜ = arg max∗ P (X|Y )P (Y ) Y ∈Y Moses augments this model by regarding logP (X|Y ) as a loglinear model with added features and weights. During decoding, the sentence X is segmented into a sequence of I phrases. Each phrase is then translated into a phrase to form sentence Y . During this process phrases may be reordered. The GIZA++ statistical alignment package is used to perform the word alignments, which are later combined into phrase alignments in the Moses pipeline (Och and Ney, 2003) and the KenLM (Heafield, 2011) package is used to do language modelling on the target sentences. Because Moses performs Phrase-based Machine Translation where it is often not optimal to delete unaligned phrases from the source sentence, we pad the source sentence with special EMPTY tokens until the source and target sentences contain equally many tokens. We train the Moses system with default parameters on the 900,000 padded training pairs. Additionally, we train a KenLM language model on the target side sentences from the training set. We perform MERT tuning on the development set and manual"
W16-6608,P02-1040,0,0.0952011,"order to evaluate our models. 45 Evaluation To evaluate the output of our systems we collect automatic scores (BLEU scores, various ROUGE scores and character compression rates) as well as human judgements on two different dimensions (Fluency and Importance). model ARNN Moses Human BLEU 0.21 0.13 0.17 ROUGE 1 0.70 0.69 0.72 ROUGE 2 0.40 0.38 0.41 ROUGE 3 0.28 0.25 0.28 ROUGE 4 0.22 0.19 0.21 ROUGE SU4 0.49 0.48 0.50 Table 4: BLEU and ROUGE scores 4.2.1 Automatic Evaluation First, we perform automatic evaluation using regular summarization and text generation evaluation metrics, such as BLEU (Papineni et al., 2002), which is generally used for Machine Translation and variants of ROUGE (Lin, 2004), which is generally used for summarization evaluation. Both take into account reference sentences and calculate overlap on the n-gram level. ROUGE also accounts for compression. ROUGE 1-4 take into account unigrams up to four-grams and ROUGE SU4 also takes into account skipgrams. For BLEU we use multi-bleu.pl, and for ROUGE we used pyrouge. We also compute compression rate on the character level, as this tells us how much the source sentence has been compressed. We simply compute this by dividing the number of"
W16-6608,D15-1044,0,0.64711,"compression Proceedings of The 9th International Natural Language Generation conference, pages 41–50, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics with RNNs. In order to be applied to sentence compression, RNNs typically need to be trained on large data sets of aligned sequences. In the domain of abstractive sentence compression, not many of such data sets are available. For the related task of sentence simplification, data sets are available of aligned sentences from Wikipedia and Simple Wikipedia (Zhu et al., 2010; Coster and Kauchak, 2011). Recently, (Rush et al., 2015) used the Gigaword corpus to construct a large corpus containing headlines paired with the article’s first sentence. Here, we present a data set compiled from scene descriptions taken from the MSCOCO dataset (Lin et al., 2014). These descriptions are generally only one sentence long, and humans tend to describe photos in different ways, which makes this task suitable for abstractive sentence compression. For each image, we align long descriptions with shorter descriptions to construct a corpus of abstractive compressions . We employ an Attentive Recurrent Neural Network (aRNN) to the task of s"
W16-6608,P05-1036,0,0.422164,"ve automatic and human evaluation that the aRNN outperforms the Moses system and even performs on par with the human generated description. We also show that automatic measures such as ROUGE that are used generally to evaluate compression tasks do not correlate with human judgements. 2 Related work A large body of work is devoted to extractive sentence compression. Here, we mention a few. (Knight and Marcu, 2002) propose two models to generate a short sentence by deleting a subset of words: the decision tree model and the noisy channel model, both based on a synchronous context free grammar. (Turner and Charniak, 2005) and (Galley and McKeown, 2007) build upon this model reporting improved results. (McDonald, 2006) develop a system using largemargin online learning combined with a decoding algorithm that searches the compression space to produce a compressed sentence. Discriminative learning is used to combine the features and weight 42 their contribution to a successful compression. (Cohn and Lapata, 2007) cast the sentence compression problem as a tree-to-tree rewriting task. For this task, they train a synchronous tree substitution grammar, which dictates the space of all possible rewrites. By using disc"
W16-6608,D11-1038,0,0.0129165,"ilingual corpus (Bannard and Burch, 2005). They show that the abstractive model outperforms an extractive model on their dataset.(Cohn and Lapata, 2013) follow up on earlier work and describe a discriminative tree-to-tree transduction model that can handle mismatches on the structural and lexical level. There has been some work on the related task of sentence simplification. (Coster and Kauchak, 2011; Zhu et al., 2010) develop models using data from Simple English Wikipedia paired with English Wikipedia. Their models were able to perform rewording, reordering, insertion and deletion actions. (Woodsend and Lapata, 2011) use Simple Wikipedia edit histories and an aligned Wikipedia– Simple Wikipedia corpus to induce a model based on quasi-synchronous grammar and integer linear programming. (Wubben et al., 2012) propose a model for simplifying sentences using monolingual Phrase-Based Machine Translation obtaining state of the art results. Recently, significant advances have been made in sequence to sequence learning. The paradigm has shifted from traditional approaches that are more focused on optimizing the parameters of several subsystems, to a single model that learns mappings between sequences by learning f"
W16-6608,P12-1107,1,0.882858,"Missing"
W16-6608,Q14-1006,0,0.0527031,"ion-based RNN is unable to outperform a Moses system. Only after additional tuning on extractive compresssions do they get better ROUGE scores. This can be attributed to the fact that additional extractive features bias the system towards retaining more input words, which is beneficial for higher ROUGE scores. Following this work, we employ an attentive Recurrent Network as described in (Bahdanau et al., 2014) to the task of abstractive summarization of scene descriptions. 3 Data set To construct the data set to train the models on, we use the image descriptions in the MSCOCO1 and FLICKR30K2 (Young et al., 2014) data sets. These data sets contain images paired with multiple descriptions provided by human subjects. The FLICKR30K data set contains 158,915 captions describing 31,783 images and the MSCOCO data set contains over a million captions describing over 160,000 images. For this work, we assume that the shorter descriptions of the images are abstractive summaries of the longer descriptions. We constrain the long-short relation by stating that a short description should be at least 10 percent shorter than a long 1 2 http://mscoco.org/dataset/ http://shannon.cs.illinois.edu/DenotationGraph/ descrip"
W16-6608,C10-1152,0,0.0585816,", 2014). In this paper we focus on abstractive sentence compression Proceedings of The 9th International Natural Language Generation conference, pages 41–50, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics with RNNs. In order to be applied to sentence compression, RNNs typically need to be trained on large data sets of aligned sequences. In the domain of abstractive sentence compression, not many of such data sets are available. For the related task of sentence simplification, data sets are available of aligned sentences from Wikipedia and Simple Wikipedia (Zhu et al., 2010; Coster and Kauchak, 2011). Recently, (Rush et al., 2015) used the Gigaword corpus to construct a large corpus containing headlines paired with the article’s first sentence. Here, we present a data set compiled from scene descriptions taken from the MSCOCO dataset (Lin et al., 2014). These descriptions are generally only one sentence long, and humans tend to describe photos in different ways, which makes this task suitable for abstractive sentence compression. For each image, we align long descriptions with shorter descriptions to construct a corpus of abstractive compressions . We employ an"
W16-6608,N03-1031,0,\N,Missing
W17-1224,W13-1727,0,0.057438,"Missing"
W17-1224,W16-4829,0,0.0114077,"y. Sum-rule meta-classifier The probabilistic outputs of the most accurate text statistics, syntactic, and content-specific classifier are summed, and the language variety with the highest sum is chosen. Classification methods The five machine learning algorithms used in this study are AdaBoost with a decision tree core, C4.5, Naive Bayes, Random Forest Classifier, and Linear-kernel SVM. These types of algorithms have been used frequently for Language Identification tasks. SVM algorithms (Goutte et al., 2014; Malmasi and Dras, 2015; Jauhiainen et al., 2016) and Naive Bayes (King et al., 2014; Franco-Penya and Sanchez, 2016) are amongst the most popuProduct-rule meta-classifier The product is calculated for the probabilistic outputs of the most accurate lexical, syntactic and contentspecific classifier, and the language variety with the highest product is chosen. Algorithm-based meta-classifier The probabilistic outputs of the most accurate lexical, syntactic and content-specific classifier are 193 Group Lexical Category Average words per minute Average characters per minute Average word length Average sentence length in terms of words Average sentence length in terms of characters Type/token ratio Hapax legomena"
W17-1224,W14-5316,0,0.0928377,"Missing"
W17-1224,W14-4204,0,0.0349131,"Missing"
W17-1224,L16-1284,0,0.171715,"Missing"
W17-1224,N15-1160,0,0.0252495,"s of feature category, are merged into a single vector to predict the language variety. Sum-rule meta-classifier The probabilistic outputs of the most accurate text statistics, syntactic, and content-specific classifier are summed, and the language variety with the highest sum is chosen. Classification methods The five machine learning algorithms used in this study are AdaBoost with a decision tree core, C4.5, Naive Bayes, Random Forest Classifier, and Linear-kernel SVM. These types of algorithms have been used frequently for Language Identification tasks. SVM algorithms (Goutte et al., 2014; Malmasi and Dras, 2015; Jauhiainen et al., 2016) and Naive Bayes (King et al., 2014; Franco-Penya and Sanchez, 2016) are amongst the most popuProduct-rule meta-classifier The product is calculated for the probabilistic outputs of the most accurate lexical, syntactic and contentspecific classifier, and the language variety with the highest product is chosen. Algorithm-based meta-classifier The probabilistic outputs of the most accurate lexical, syntactic and content-specific classifier are 193 Group Lexical Category Average words per minute Average characters per minute Average word length Average sentence length in"
W17-1224,W13-1714,0,0.0716425,"Missing"
W17-1224,W16-4801,0,0.271932,"Missing"
W17-1224,W16-4820,0,0.0674345,"Missing"
W17-1224,W14-5317,0,0.0221979,"the language variety. Sum-rule meta-classifier The probabilistic outputs of the most accurate text statistics, syntactic, and content-specific classifier are summed, and the language variety with the highest sum is chosen. Classification methods The five machine learning algorithms used in this study are AdaBoost with a decision tree core, C4.5, Naive Bayes, Random Forest Classifier, and Linear-kernel SVM. These types of algorithms have been used frequently for Language Identification tasks. SVM algorithms (Goutte et al., 2014; Malmasi and Dras, 2015; Jauhiainen et al., 2016) and Naive Bayes (King et al., 2014; Franco-Penya and Sanchez, 2016) are amongst the most popuProduct-rule meta-classifier The product is calculated for the probabilistic outputs of the most accurate lexical, syntactic and contentspecific classifier, and the language variety with the highest product is chosen. Algorithm-based meta-classifier The probabilistic outputs of the most accurate lexical, syntactic and content-specific classifier are 193 Group Lexical Category Average words per minute Average characters per minute Average word length Average sentence length in terms of words Average sentence length in terms of character"
W17-1224,U13-1003,0,0.102474,"NNP NN NNP PRP$ NN FW , NNP Personal pronouns ratio . PRP$ CD VB Function words ratio , Content-specific nou zandloper plots jij hen amuseren orde vinden lief helpen ’t Table 6: Top 10 most important features per feature category. techniques (AdaBoost, C4.5, Naive Bayes, Random Forest Classifier, and Linear SVM), and three feature categories (text statistics, syntactic features, and content-specific features) focusing on the Netherlandic and Flemish variants of Dutch. Subtitles collected in the SUBTIEL corpus were used to train and test the classifiers on. With the exception of a few studies (Lui and Cook, 2013; Lui et al., 2014; Windisch and Csink, 2005; Zampieri et al., 2013), text statistics and syntactic features have rarely been explored in language identification tasks. Additionally, there are not many classification studies focusing on Dutch language varieties, exceptions being Trieschnigg et al. (2012) and Tulkens et al. (2016). The highest accuracy score was obtained when using a meta-classifier approach with a machinelearning algorithm, AdaBoost. In this approach the probabilistic scores obtained from classifiers trained exclusively on text statistics features, syntactic features, and cont"
W17-1224,W03-2414,0,0.132401,"Missing"
W17-1224,C12-1160,0,0.0735336,"Missing"
W17-1224,L16-1652,0,0.107995,"episode or a movie. The main focus of the studio are movies and television shows, and to a smaller degree documentaries. After filtering out the English subtitles and the Dutch subtitles without information on whether they were intended for Dutch or Flemish television, 110.278 documents remain; cf. Table 1. A document in this context is the subtitles for one movie, or one episode of a television show. For the subtitles used in this study, a distinction is made between subtitles that were shown on a Dutch or a Flemish television network. In comparison to similar work (Trieschnigg et al., 2012; Tulkens et al., 2016), the number of documents and tokens that is used in the current study is relatively large. Using an automated mining tool, the subtitles in the corpus were scanned for a match in the Internet Movie Database (IMDb)1 , which provides additional information about the show or movie (e.g. genre, year, actors). The main interest was genre, since a vastly different genre distribution per language variety could have an impact on classification accuracy. An IMDb match was found for roughly half of the subtitles. The genre distribution for these matches did show minor difThe current study will explore"
W17-1224,W14-5307,0,0.308068,"Missing"
W17-1224,W15-5411,0,0.0669112,"Missing"
W18-3901,W18-3913,0,0.217545,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W18-3919,0,0.254393,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W18-3932,0,0.129693,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W17-1223,0,0.131933,"data to reduce the transcriber effects seen last year. 7 • The LaMa system is a blend (weighted vote) of eight classifiers being stochastic gradient descent (hinge and modified Huber), multinomial Na¨ıve Bayes, both counts and tf-idf, FastText, and modified Kneser-Ney smoothing. The classifiers were trained using word n-grams (1-6) and character n-grams (1-8). The hyperparameters were determined with cross-validation and searching on the development set. • XAC system is a refined version of the n-gram-based Bayesline system described in last year’s XAC submission to the VarDial shared tasks (Barbaresi, 2017), and previously used as a baseline for the DSL shared task (Tan et al., 2014). The XAC team achieved their best results using a Na¨ıve Bayes classifier. • The GDI classification system is based on an ensemble of multiple SVM classifiers. The system was trained on various word- and character-level features. • The dkosmajac system is based on a normalized Euclidean distance measure. The distances are calculated between a sample and each class profile. The class profiles are generated by selecting the most frequent features for each class, which results in profiles that are of the same length fo"
W18-3901,W18-3918,0,0.0590619,"Missing"
W18-3901,W18-3925,0,0.142653,"Missing"
W18-3901,W17-1214,0,0.138486,"was part of the first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions gen"
W18-3901,W18-3909,0,0.15091,"Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants and in the overall interest from the NLP community. This motivated the organizers to turn the shared tasks at VarDial into a more comprehensive evaluation exercise with four shared tasks in 2017. This year, the VarDial workshop featured the second edition of the VarDial evaluation campaign w"
W18-3901,W18-3933,1,0.889149,"Missing"
W18-3901,W18-3920,1,0.880795,"Missing"
W18-3901,W18-3926,0,0.0553879,"Missing"
W18-3901,W18-3921,0,0.054375,"Missing"
W18-3901,W17-1225,0,0.134424,"r of hours. 5.2 Participants and Approaches In this section, we present a short description of the systems that competed in the ADI shared task: • UnibucKernel system (Butnaru and Ionescu, 2018) combines three kernel matrices: one calculated using just the lexical features, another one computed on embeddings, and a combined kernel computed on the phonetic features. The final matrix is the mean of these three matrices. As a classifier, they used Kernel Ridge Regression. The approach is similar to the systems that ranked second and first in the previous two ADI tasks (Ionescu and Popescu, 2016; Ionescu and Butnaru, 2017). • Safina system (Ali, 2018a) accepts a sequence of 256 characters as input in addition to the acoustic embedding vectors. First, the sequence of characters is one-hot encoded, then it is passed to a GRU layer, which is followed by a convolutional layer with different filter sizes ranging from 2 to 7. The convolutional layer is followed by batch normalizations, max-pooling, and dropout layers, and finally a softmax layer. In contrast, the acoustic embedding vectors go directly to another softmax layer. The final output is the average between these two softmax layers, which represents the prob"
W18-3901,W16-4818,0,0.048944,", duration (Dur.), in number of hours. 5.2 Participants and Approaches In this section, we present a short description of the systems that competed in the ADI shared task: • UnibucKernel system (Butnaru and Ionescu, 2018) combines three kernel matrices: one calculated using just the lexical features, another one computed on embeddings, and a combined kernel computed on the phonetic features. The final matrix is the mean of these three matrices. As a classifier, they used Kernel Ridge Regression. The approach is similar to the systems that ranked second and first in the previous two ADI tasks (Ionescu and Popescu, 2016; Ionescu and Butnaru, 2017). • Safina system (Ali, 2018a) accepts a sequence of 256 characters as input in addition to the acoustic embedding vectors. First, the sequence of characters is one-hot encoded, then it is passed to a GRU layer, which is followed by a convolutional layer with different filter sizes ranging from 2 to 7. The convolutional layer is followed by batch normalizations, max-pooling, and dropout layers, and finally a softmax layer. In contrast, the acoustic embedding vectors go directly to another softmax layer. The final output is the average between these two softmax layer"
W18-3901,W18-3915,0,0.0733204,"Missing"
W18-3901,W18-3929,0,0.411409,"Missing"
W18-3901,W18-3907,0,0.178349,"Missing"
W18-3901,W18-3922,0,0.0589219,"Missing"
W18-3901,W18-3928,0,0.0556119,"Missing"
W18-3901,kumar-2012-challenges,1,0.824307,"ed printed stories, novels and essays in books, magazines, and newspapers. We scanned the printed materials, then we performed OCR, and finally we asked native speakers of the respective languages to correct the OCR output. Since there are no specific OCR models available for these languages, we used the Google OCR for Hindi, part of the Drive API. Since all the languages used the Devanagari script, we expected the OCR to work reasonably well, and overall it did. We further managed to get some blogs in Magahi and Bhojpuri. There are several corpora already available for Modern Standard Hindi (Kumar, 2012; Kumar, 2014a; Kumar, 2014b; Choudhary and Jha, 2011). However, in order to keep the domain the same as for the other languages, we collected data from blogs that mainly contain stories and novels. Thus, the Modern Standard Hindi data collected for this study is also from the literature domain.5 9.2 Participants and Approaches • The SUKI team used HeLI with adaptive language models based on character n-grams from 1 to 6, as previously described in Section 5. The also used an iterative version of the language model adaptation technique, with three additional adaptation epochs. ¨ • Tubingen-Osl"
W18-3901,kumar-2014-developing,1,0.832345,"ories, novels and essays in books, magazines, and newspapers. We scanned the printed materials, then we performed OCR, and finally we asked native speakers of the respective languages to correct the OCR output. Since there are no specific OCR models available for these languages, we used the Google OCR for Hindi, part of the Drive API. Since all the languages used the Devanagari script, we expected the OCR to work reasonably well, and overall it did. We further managed to get some blogs in Magahi and Bhojpuri. There are several corpora already available for Modern Standard Hindi (Kumar, 2012; Kumar, 2014a; Kumar, 2014b; Choudhary and Jha, 2011). However, in order to keep the domain the same as for the other languages, we collected data from blogs that mainly contain stories and novels. Thus, the Modern Standard Hindi data collected for this study is also from the literature domain.5 9.2 Participants and Approaches • The SUKI team used HeLI with adaptive language models based on character n-grams from 1 to 6, as previously described in Section 5. The also used an iterative version of the language model adaptation technique, with three additional adaptation epochs. ¨ • Tubingen-Oslo team submit"
W18-3901,W14-0405,1,0.912975,"Missing"
W18-3901,W18-3917,1,0.897664,"Missing"
W18-3901,L16-1242,1,0.902855,"Missing"
W18-3901,L16-1676,1,0.914666,"Missing"
W18-3901,W16-4814,1,0.86288,"-Oslo system (C ¸ o¨ ltekin et al., 2018) is trained on word and character n-grams using a single SVM classifier, which is fine-tuned using cross-validation. It is similar to the submissions by the same authors to previous VarDial shared tasks (C¸o¨ ltekin and Rama, 2017; C¸o¨ ltekin and Rama, 2016). They also tried an approach based on RNN, which worked worse. • Arabic Identification system is based on an ensemble of SVM classifiers trained on character and word n-grams. The approach is similar to the systems ranked second and first in the previous two ADI tasks (Malmasi and Zampieri, 2017a; Malmasi and Zampieri, 2016). 5 5.3 Results Six teams submitted runs for the ADI shared task and the results are shown in Table 3. The best result, an F1 score of 0.589, was achieved by UnibucKernel,1 followed by safina, with an F1 score of 0.575. The following three teams are tied for the third place as they are not statistically different. Rank 1 2 3 3 3 4 Team F1 (Macro) UnibucKernel safina BZU SYSTRAN T¨ubingen-Oslo Arabic Identification 0.589 0.576 0.534 0.529 0.514 0.500 Table 3: ADI results: ranked taking statistical significance into account. 5.4 Summary We introduced multi-phoneme representation for the dialecta"
W18-3901,W17-1222,1,0.833986,"e first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions generated using large-vocabular"
W18-3901,W17-1220,1,0.80558,"e first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions generated using large-vocabular"
W18-3901,W16-4801,1,0.733081,"Missing"
W18-3901,W18-3927,0,0.0554913,"Missing"
W18-3901,W18-3914,0,0.175754,"ES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants and in the overall interest from the NLP community. This motivated the organizers to turn the shared tasks at VarDial into a more comprehensive eva"
W18-3901,W18-3924,0,0.0833746,"and the number of submissions varied widely across the tasks, ranging from 6 entries for ADI and MTT to 12 entries for DFS. Table 1 lists the participating teams, the shared tasks they took part in, and a reference to the system description paper. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participate"
W18-3901,L16-1641,1,0.509642,"Missing"
W18-3901,W17-1224,1,0.744067,"Missing"
W18-3901,W18-3923,1,0.879634,"Missing"
W18-3901,W14-5307,1,0.857122,"Missing"
W18-3901,W15-5401,1,0.855743,"Missing"
W18-3901,W17-1201,1,0.607978,"Missing"
W19-2903,D08-1102,0,0.0337886,"ating Spanish-English Code-Switching: El Modelo Est´a Generating Code-Switches Chara Tsoukala Stefan L. Frank Antal van den Bosch Centre for Language Studies, Centre for Language Studies, KNAW Meertens Institute; Radboud University Radboud University Centre for Language Studies, c.tsoukala@let.ru.nl s.frank@let.ru.nl Radboud University a.vandenbosch@let.ru.nl Jorge Vald´es Kroff Department of Spanish and Portuguese Studies, University of Florida jvaldeskroff@ufl.edu Abstract ral language processing (NLP) research community. Several NLP applications have emerged, e.g., to detect code-switches (Solorio and Liu, 2008; Guzm´an et al., 2017), or to automatically recognize code-switched speech (Yılmaz et al., 2016; Gonen and Goldberg, 2018). Moreover, there are a small number of cognitive computational models relevant to code-switching: Filippi et al. (2014) developed a model of code-switched word production and Janciauskas and Chang (2018), while simulating age of acquisition effects on native Korean speakers of English, reported that the models that had been exposed to English later produced code-switches, i.e., occasionally used Korean words in their predominantly English production. Multilingual speakers"
W19-2909,2006.jeptalnrecital-invite.2,0,0.133927,"Missing"
W19-2909,schafer-bildhauer-2012-building,0,0.0344733,"Missing"
W19-2909,W15-2402,0,0.154482,"structures has an effect on fixation durations and regression probabilities. Demberg and Keller (2008) compared linguistic integration cost computed as a function of dependency relations distances and word surprisal as predictors of gaze duration. They showed that distance is not a significant predictor of reading times except for nouns. On the other hand, they demonstrate that surprisal can predict reading times for arbitrary words in the corpus, concluding that the two predictors may capture distinct aspects of naturalistic language processing. In the context of Natural Language Processing, Klerke et al. (2015) used eye-tracker data as a metric for the quality of automatic text simplification and compression, which are operations used in machine translation and automatic summarization. Their proposal is grounded in the hypothesis that eye movements are related to perceived text diffi79 5 nobj ROOT Materials and Methods det 5.1 amod advmod nsubj amod Eye-tracker data culty (Rayner and Pollatsek, 1995), one of the two hypothesis we have introduced in Section 2 above. The eye tracker data used in this study was originally collected for a study on mental stimulation during literary reading by Mak and Wi"
W19-3503,W17-1101,0,0.0271692,"Missing"
W19-3503,W16-5618,0,0.104309,"be&t=39m38s 19 Proceedings of the Third Workshop on Abusive Language Online, pages 19–24 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics ing algorithms such as Support Vector Machines, Naive Bayes, Logistic Regression, and Random Forests (see Schmidt and Wiegand (2017) for an overview) and focus on manually extracted features. Besides word or character n-grams and POS tags, the approaches typically make use of features such as punctuation, word and document length, capitalization, and gender identity of the speaker (Davidson et al., 2017; Nobata et al., 2016; Waseem, 2016; Waseem and Hovy, 2016). Many of these approaches have the advantage of explainability (to a certain extent), but struggle when harassment is implicit (Dinakar et al., 2011) or when harassment-related words have multiple meanings (Kwok and Wang, 2013; Davidson et al., 2017). tomated system that makes this estimate retroactively would not be of much added value. Instead, toxic players should be detected as the conversation develops, as early as possible, making it possible for gaming companies to intervene in one way or the other (like warning, muting or banning a player). Translated to a mach"
W19-3503,N16-2013,0,0.0324699,"Proceedings of the Third Workshop on Abusive Language Online, pages 19–24 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics ing algorithms such as Support Vector Machines, Naive Bayes, Logistic Regression, and Random Forests (see Schmidt and Wiegand (2017) for an overview) and focus on manually extracted features. Besides word or character n-grams and POS tags, the approaches typically make use of features such as punctuation, word and document length, capitalization, and gender identity of the speaker (Davidson et al., 2017; Nobata et al., 2016; Waseem, 2016; Waseem and Hovy, 2016). Many of these approaches have the advantage of explainability (to a certain extent), but struggle when harassment is implicit (Dinakar et al., 2011) or when harassment-related words have multiple meanings (Kwok and Wang, 2013; Davidson et al., 2017). tomated system that makes this estimate retroactively would not be of much added value. Instead, toxic players should be detected as the conversation develops, as early as possible, making it possible for gaming companies to intervene in one way or the other (like warning, muting or banning a player). Translated to a machine learning task, this"
W19-3503,P18-1125,0,0.131326,"adboud University w.stoop@let.ru.nl Florian Kunneman CLST, Radboud University f.kunneman@let.ru.nl Antal van den Bosch KNAW Meertens Instituut Ben Miller Emory University b.j.miller@emory.edu antal.van.den.bosch@meertens.knaw.nl Abstract toxicity to ”the average person just having a bad day” (Maher, 2016). As encounters with harassment are a major predictor for players quitting a video game2 , creating healthy communities is an important focus point for many video game developers3 . There has been an increase recently in the number of academic papers on automatically detecting harassment; see Zhang et al. (2018b) and van Aken et al. (2018) for overviews. Many of these works focus on datasets with relatively short conversations (often <20 turns), consisting of longer utterances (often multiple full sentences). As a result, most of these studies approach detecting verbal harassment as a classical text classification task, where each individual comment is considered a document on its own that should be assigned one of two or more categories. Conversations in video games, on the other hand, are different in nature: they consist of up to several hundreds of utterances, depending on the length of a match"
W98-1223,C88-1028,1,0.825637,"Missing"
W98-1223,J94-3007,1,0.894369,"Missing"
W98-1223,J76-4008,0,0.257262,"se. Mean results can be employed further in significance tests. In our experiments, n = 10, and one-tailed ttests axe performed. 3 Three word-pronunciation architectures Out experiments axe grouped in three series, each involving the application of IGTR~.B to a paxticula~ word-pronunciation system. The a~chitectures of these systems axe displayed in Figure 1. In the following subsections, each system is introduced, an outline is given of the experiments performed on the system, and the results a~e briefly discussed. 3.1 M-A-G-Y-S The axchitectu~e of the M-A-G-Y-S system is inspixed by SGUND1 (Hunnicutt, 1976; Hunnicutt, 1980), the word-pronunciation subsystem of the MIT~kLK text-to-speech system (Allen, Hunnicutt, and Klatt, 1987). When the MITALK system is faced with an unknown word, sounD1 produces on the basis of that word a phonemic transcription with stress markers (Allen, Hunnieutt, and Klatt, 1987). This wordpronunciation process is divided into the following five processing components: 1. morphological segmentalion, which we implement as the module referred to as M; 2. graphemic parsing, module A; 3. grapheme-phoneme conversion, module G; 4. sfllabifica~ion, module y; 5. stress assignment"
W98-1223,P84-1038,0,0.0599276,"ssential levels of abstraction in language processing tasks. Appl;ed to morpho-phonology, the argument states that generic learning methods are not able to discover morphology, graphematies, and stress patterns autonomonsly when learning word pronunciation, although this knowledge appears essential. Phonological and morphological theories, influenced by Chomskyan theory across the board since the publication of spy. (Chomsky and Halle, 1968), have generally adopted the idea of abstraction levels in various guises (e.g., levels, tapes, tiers, grids) (Goldsmith, 1976; Liberman and Prince, 1977; Koskenniemi, 1984; Mohanan, 1986). Although there is no general consensus on which levels of abstraction can be discerned in phonology and morphology, there is a rough, global agreement on the fact that words can be represented on different abstraction levels as Modul,~rity in Word Pronunciation systems Antal van den Bosch, Ton Weijters and Walter Daelemans (1998) Modularity in Inductively-Learned Word Pronunciation Systems. In D.M.W. Powers (ed.) NeMLaP3/CoNLL98: New Methods in Language Processing and Computational Natural Language Learning, ACL, pp 185-194. I strings ofletters, graphemes, morphemes, phonemes"
W98-1223,E93-1007,1,0.88377,"Missing"
W98-1223,W83-0114,0,\N,Missing
W98-1224,J94-3007,1,0.877197,"Missing"
wubben-etal-2014-creating,W96-0102,0,\N,Missing
wubben-etal-2014-creating,W04-3219,0,\N,Missing
wubben-etal-2014-creating,W10-4223,1,\N,Missing
wubben-etal-2014-creating,E06-1021,0,\N,Missing
wubben-etal-2014-creating,C04-1051,0,\N,Missing
wubben-etal-2014-creating,J10-3003,0,\N,Missing
wubben-etal-2014-creating,W03-1004,0,\N,Missing
wubben-etal-2014-creating,W07-1007,0,\N,Missing
wubben-etal-2014-creating,D09-1040,0,\N,Missing
wubben-etal-2014-creating,P02-1040,0,\N,Missing
wubben-etal-2014-creating,P07-1059,0,\N,Missing
wubben-etal-2014-creating,P01-1008,0,\N,Missing
wubben-etal-2014-creating,D11-1038,0,\N,Missing
wubben-etal-2014-creating,D08-1021,0,\N,Missing
wubben-etal-2014-creating,C10-1152,0,\N,Missing
wubben-etal-2014-creating,W06-1610,0,\N,Missing
wubben-etal-2014-creating,N03-1024,0,\N,Missing
wubben-etal-2014-creating,P07-2045,0,\N,Missing
wubben-etal-2014-creating,P09-1034,0,\N,Missing
wubben-etal-2014-creating,D10-1090,0,\N,Missing
wubben-etal-2014-creating,N03-1003,0,\N,Missing
wubben-etal-2014-creating,N06-1058,0,\N,Missing
wubben-etal-2014-creating,P05-1074,0,\N,Missing
wubben-etal-2014-creating,N06-1003,0,\N,Missing
wubben-etal-2014-creating,N03-1017,0,\N,Missing
wubben-etal-2014-creating,P12-1107,1,\N,Missing
wubben-etal-2014-creating,J03-1002,0,\N,Missing
wubben-etal-2014-creating,P11-1020,0,\N,Missing
wubben-etal-2014-creating,daelemans-etal-2004-automatic,0,\N,Missing
wubben-etal-2014-creating,C08-1013,0,\N,Missing
wubben-etal-2014-creating,J08-4005,0,\N,Missing
wubben-etal-2014-creating,vossen-etal-2008-integrating,0,\N,Missing
Y09-1019,W05-0909,0,0.059657,"Missing"
Y09-1019,P07-1020,0,0.209869,"Missing"
Y09-1019,J96-1002,0,0.0373506,"describe the features used in the experiments, and the pre-processing required. Section 7 presents the results obtained, and offers some analysis. In Section 8 we formulate our conclusions, and offer some avenues for further work. 2 Related Work Brown et al. (1991) were the first to propose the use of dedicated WSD models in word-based SMT systems. Results were limited to the case of binary disambiguation, i.e., deciding between only two possible translation candidates, and to a reduced set of common words. A significant improvement in translation was reported according to manual evaluation. Berger et al. (1996) suggested context-sensitive modeling of word translations in order to integrate local contextual information into their IBM translation models using a Maximum Entropy (MaxEnt) model, but the work is not supported by any significant evaluation results. García Varea et al. (2001) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Using such a model on the German—English Verbmobil corpus, they obtained better alignment quality in terms of improved alignment error rate (AER)."
Y09-1019,J93-2003,0,0.0107764,"ng. In the second group, predictions are allowed to interact with other models (e.g., language, distortion, additional translation models etc.) during decoding time. The present work falls into the second type of interaction methods. 3 Log-Linear PB-SMT Translation is modelled in PB-SMT as a decision process, in which the translation e1I = e1 . . . e I of a source sentence f1J = f1 . . . f J is chosen to maximize (1): arg max P(e1I |f1J ) = arg max P( f 1J |e1I ) P(e1I ) I ,e1I (1) I ,e1I where P( f 1J |e1I ) and P(e1I ) denote respectively the translation model and the target-language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P( f 1I |e1J ) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): M log P ( e1I |f 1 J ) = ∑λ I m h m ( f 1 J , e1I , s1K ) + λLM log P(e1 ) (2) m =1 where s1K = s1...sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (eˆ ,..., eˆ ) and ( fˆ ,..., fˆ ) such that (we set i0 = 0) (3): 1 k ∀1 ≤ k ≤ K , sk = (ik ; bk, jk), 1 k eˆk = eik−1 +1...eik , fˆk = f b ... f j k k ("
Y09-1019,H91-1025,0,0.219377,"79 170 The remainder of the paper is organized as follows. In Section 2 we discuss related work. Section 3 provides a brief overview of PB-SMT. In Section 4 we describe how we model dependency information as context-informed features in our baseline log-linear PB-SMT system. Section 5 describes the memory-based classification approach. In Section 6 we describe the features used in the experiments, and the pre-processing required. Section 7 presents the results obtained, and offers some analysis. In Section 8 we formulate our conclusions, and offer some avenues for further work. 2 Related Work Brown et al. (1991) were the first to propose the use of dedicated WSD models in word-based SMT systems. Results were limited to the case of binary disambiguation, i.e., deciding between only two possible translation candidates, and to a reduced set of common words. A significant improvement in translation was reported according to manual evaluation. Berger et al. (1996) suggested context-sensitive modeling of word translations in order to integrate local contextual information into their IBM translation models using a Maximum Entropy (MaxEnt) model, but the work is not supported by any significant evaluation re"
Y09-1019,I05-2021,0,0.158454,"-SMT systems, improved AER scores do not necessarily result in improved translation quality, as noted by a number of researchers. Vickrey et al. (2005) built classifiers inspired by those used in WSD to fill in any blanks in a partially completed translation. Giménez and Màrquez (2007) extended this work by considering the more general case of frequent phrases and moved to full translation rather than blank-filling on the target side. Attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches of integrating state-of-theart WSD methods into SMT to improve the overall translation quality have met with more success (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007, 2009). Recently, Bangalore et al. (2008) employed an SMT architecture based on stochastic finitestate transducers that addresses global lexical selection, i.e. dedicated word selection. Specia et al. (2008) use dedicated predictions for the re-ranking of n-best translations, limited to a small set of words from different grammatical categories. Significant BLEU improvement"
Y09-1019,2007.mtsummit-papers.11,0,0.220443,"ses. Approaches to include source context for proper selection of target phrases have been inspired by methods for word sense disambiguation (WSD), that employ rich context-sensitive features to determine the contextually most likely sense of a polysemous word. These contextual features may include lexical features of words appearing in the context and bearing sensediscriminatory information, position-specific neighbouring words (Giménez and Márquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features of the sentential context (Gimpel and Smith, 2008) and full sentential context (Carpuat and Wu, 2007). Most of the work on syntactic features has made use of part-of-speech taggers (Stroppa et al., 2007), supertaggers (Haque et al., 2009) and shallow and deep syntactic parsers (Gimpel and Smith, 2008). In the present work, we explore how the local sentential context information from a dependency parse can be modeled as source context features to be integrated into a PB-SMT model. Copyright 2009 by Rejwanul Haque, Sudip Kumar Naskar, Antal van den Bosch, and Andy Way 23rd Pacific Asia Conference on Language, Information and Computation, pages 170–179 170 The remainder of the paper is organized"
Y09-1019,P07-1005,0,0.234639,"to fill in any blanks in a partially completed translation. Giménez and Màrquez (2007) extended this work by considering the more general case of frequent phrases and moved to full translation rather than blank-filling on the target side. Attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches of integrating state-of-theart WSD methods into SMT to improve the overall translation quality have met with more success (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007, 2009). Recently, Bangalore et al. (2008) employed an SMT architecture based on stochastic finitestate transducers that addresses global lexical selection, i.e. dedicated word selection. Specia et al. (2008) use dedicated predictions for the re-ranking of n-best translations, limited to a small set of words from different grammatical categories. Significant BLEU improvements were reported in both approaches. Hasan et al. (2008) present target context modeling into SMT using a triplet lexicon model that captures long-distance (global) dependencies. Their approach is"
Y09-1019,W06-1628,0,0.185399,"Missing"
Y09-1019,P01-1027,0,0.430501,"Missing"
Y09-1019,W07-0719,0,0.377407,"p a refined context-dependent lexicon model. Using such a model on the German—English Verbmobil corpus, they obtained better alignment quality in terms of improved alignment error rate (AER). However, since alignment is not an end task in itself and is most often used as an intermediate task to generate phrase pairs for the t-tables in PB-SMT systems, improved AER scores do not necessarily result in improved translation quality, as noted by a number of researchers. Vickrey et al. (2005) built classifiers inspired by those used in WSD to fill in any blanks in a partially completed translation. Giménez and Màrquez (2007) extended this work by considering the more general case of frequent phrases and moved to full translation rather than blank-filling on the target side. Attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches of integrating state-of-theart WSD methods into SMT to improve the overall translation quality have met with more success (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007, 2009). Recently, Bangalore et al. (2008)"
Y09-1019,W08-0302,0,0.104114,"influence the weighting and selection of target phrases. Approaches to include source context for proper selection of target phrases have been inspired by methods for word sense disambiguation (WSD), that employ rich context-sensitive features to determine the contextually most likely sense of a polysemous word. These contextual features may include lexical features of words appearing in the context and bearing sensediscriminatory information, position-specific neighbouring words (Giménez and Márquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features of the sentential context (Gimpel and Smith, 2008) and full sentential context (Carpuat and Wu, 2007). Most of the work on syntactic features has made use of part-of-speech taggers (Stroppa et al., 2007), supertaggers (Haque et al., 2009) and shallow and deep syntactic parsers (Gimpel and Smith, 2008). In the present work, we explore how the local sentential context information from a dependency parse can be modeled as source context features to be integrated into a PB-SMT model. Copyright 2009 by Rejwanul Haque, Sudip Kumar Naskar, Antal van den Bosch, and Andy Way 23rd Pacific Asia Conference on Language, Information and Computation, pages"
Y09-1019,2009.eamt-1.32,1,0.872115,"n (WSD), that employ rich context-sensitive features to determine the contextually most likely sense of a polysemous word. These contextual features may include lexical features of words appearing in the context and bearing sensediscriminatory information, position-specific neighbouring words (Giménez and Márquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features of the sentential context (Gimpel and Smith, 2008) and full sentential context (Carpuat and Wu, 2007). Most of the work on syntactic features has made use of part-of-speech taggers (Stroppa et al., 2007), supertaggers (Haque et al., 2009) and shallow and deep syntactic parsers (Gimpel and Smith, 2008). In the present work, we explore how the local sentential context information from a dependency parse can be modeled as source context features to be integrated into a PB-SMT model. Copyright 2009 by Rejwanul Haque, Sudip Kumar Naskar, Antal van den Bosch, and Andy Way 23rd Pacific Asia Conference on Language, Information and Computation, pages 170–179 170 The remainder of the paper is organized as follows. In Section 2 we discuss related work. Section 3 provides a brief overview of PB-SMT. In Section 4 we describe how we model d"
Y09-1019,D08-1039,0,0.0899696,"integrating state-of-theart WSD methods into SMT to improve the overall translation quality have met with more success (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007, 2009). Recently, Bangalore et al. (2008) employed an SMT architecture based on stochastic finitestate transducers that addresses global lexical selection, i.e. dedicated word selection. Specia et al. (2008) use dedicated predictions for the re-ranking of n-best translations, limited to a small set of words from different grammatical categories. Significant BLEU improvements were reported in both approaches. Hasan et al. (2008) present target context modeling into SMT using a triplet lexicon model that captures long-distance (global) dependencies. Their approach is evaluated in a re-ranking framework; slight improvements are observed over IBM model 1 in terms of BLEU and TER (Snover et al., 2006). Target-language models arguably play the most significant role in today’s PB-SMT systems. However, for some time now people have believed that some incorporation of source language information into SMT systems was bound to help. Stroppa et al. (2007) added source-side contextual features to a state-of-the-art log-linear PB"
Y09-1019,N03-1017,0,0.0255589,"Missing"
Y09-1019,P07-2045,0,0.00699313,"features from the head-words of the SMT phrases, identified from the dependency graph generated for the source sentence (as described earlier in Section 4). (ii) They filter out phrases from phrase table entries for which P( eˆk |fˆk ) < 0.0002. In contrast, we keep all phrase pairs for more discrimination. (iii) Their experimental data contains 95K English-to-French training pairs, while we trained our models on about three times as many (286K) Dutch-to-English translation pairs, a less explored direction. 6.2 Pre-processing As (Stroppa et al., 2007) point out, PB-SMT decoders such as Moses (Koehn et al., 2007) rely on a static phrase table, represented as a list of aligned phrases accompanied by several estimated metrics. Since these features do not express the context information in which those phrases occur, no dependency information is kept in the phrase table, and there is no way to recover this information from the phrase table. In order to take into account the dependency information features within such decoders, the test text to be translated is pre-processed. Each word appearing in the test set (and, during development, the development set) is assigned a unique identifier. First we prepare"
Y09-1019,P06-1096,0,0.0168677,"es learned using decision trees. They considered up to two words and/or POS tags on either side of the source focus word as contextual features. In order to overcome problems of estimation of such features, they used a decision-tree classifier (Daelemans et al., 2005) that implicitly smoothes the probability estimates. Significant improvements over a baseline state-of-the-art PB-SMT system were obtained on Italian— English and Chinese—English IWSLT tasks. Several proposals have recently been made to fully exploit the accuracy and the flexibility of discriminative learning (Cowan et al., 2006; Liang et al., 2006). Work of this type generally 171 requires a redefinition of the training procedure; in contrast, our approach introduces new features while retaining the strength of existing state-of-the-art systems. Like the work of (Max et al., 2008), the present work is directly motivated by and is an extension of the approach of (Stroppa et al., 2007). The work of both (Max et al., 2008) and (Gimpel and Smith, 2008) focuses on language pairs where the target is not English. While (Gimpel and Smith, 2008) are unable to show any improvements for English-to-German, (Max et al., 2008) conduct experiments fro"
Y09-1019,2008.eamt-1.17,0,0.39666,"fier (Daelemans et al., 2005) that implicitly smoothes the probability estimates. Significant improvements over a baseline state-of-the-art PB-SMT system were obtained on Italian— English and Chinese—English IWSLT tasks. Several proposals have recently been made to fully exploit the accuracy and the flexibility of discriminative learning (Cowan et al., 2006; Liang et al., 2006). Work of this type generally 171 requires a redefinition of the training procedure; in contrast, our approach introduces new features while retaining the strength of existing state-of-the-art systems. Like the work of (Max et al., 2008), the present work is directly motivated by and is an extension of the approach of (Stroppa et al., 2007). The work of both (Max et al., 2008) and (Gimpel and Smith, 2008) focuses on language pairs where the target is not English. While (Gimpel and Smith, 2008) are unable to show any improvements for English-to-German, (Max et al., 2008) conduct experiments from English-to-French. Using the same sorts of local contextual features as (Stroppa et al., 2007), as well as using broader context in addition to grammatical dependency information, (Max et al., 2008) show modest gains over a PB-SMT base"
Y09-1019,P03-1021,0,0.0118444,"ized, these weights can be seen as the posterior probabilities of the target phrases eˆ k , which thus give access to P( eˆk |fˆk ,DI( fˆk )). Therefore, the expected feature is derived as in (7): hˆmbl = log P( eˆk |fˆk ,DI( fˆk )) (7) In addition to the above feature, we derived a simple binary feature hˆbest . The feature hˆbest is defined as in (8): hˆbest = 1 0 if eˆk maximizes P( eˆk |fˆk ,CI( fˆk )) otherwise, (8) We performed experiments by integrating these two features hˆmbl and hˆbest directly into the log-linear model. Their weights are optimized using minimum error-rate training (Och, 2003) on a held-out development set for each of the experiments. Our approach in terms of experimental set-up and classification of a source phrase along with contextual dependency features differs from Stroppa et al., (2009) and Haque et al., (2009) in the following respects: (i) Stroppa et al. (2007) and Haque et al., (2009) integrate local, position-specific contextual features into the log-linear framework. Here we integrate a feature encoding positionindependent dependency information; (ii) Haque et al. (2009) interpolate the context-dependent phrase translation probability with the forward ph"
Y09-1019,P02-1038,0,0.32297,"g time. The present work falls into the second type of interaction methods. 3 Log-Linear PB-SMT Translation is modelled in PB-SMT as a decision process, in which the translation e1I = e1 . . . e I of a source sentence f1J = f1 . . . f J is chosen to maximize (1): arg max P(e1I |f1J ) = arg max P( f 1J |e1I ) P(e1I ) I ,e1I (1) I ,e1I where P( f 1J |e1I ) and P(e1I ) denote respectively the translation model and the target-language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P( f 1I |e1J ) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): M log P ( e1I |f 1 J ) = ∑λ I m h m ( f 1 J , e1I , s1K ) + λLM log P(e1 ) (2) m =1 where s1K = s1...sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (eˆ ,..., eˆ ) and ( fˆ ,..., fˆ ) such that (we set i0 = 0) (3): 1 k ∀1 ≤ k ≤ K , sk = (ik ; bk, jk), 1 k eˆk = eik−1 +1...eik , fˆk = f b ... f j k k (3) The translational features depend only on pairs of source/target phrases and do not take into account any context of these phrases, i.e. each feature hm i"
Y09-1019,P02-1040,0,0.0773823,"Missing"
Y09-1019,2006.amta-papers.25,0,0.0152464,"astic finitestate transducers that addresses global lexical selection, i.e. dedicated word selection. Specia et al. (2008) use dedicated predictions for the re-ranking of n-best translations, limited to a small set of words from different grammatical categories. Significant BLEU improvements were reported in both approaches. Hasan et al. (2008) present target context modeling into SMT using a triplet lexicon model that captures long-distance (global) dependencies. Their approach is evaluated in a re-ranking framework; slight improvements are observed over IBM model 1 in terms of BLEU and TER (Snover et al., 2006). Target-language models arguably play the most significant role in today’s PB-SMT systems. However, for some time now people have believed that some incorporation of source language information into SMT systems was bound to help. Stroppa et al. (2007) added source-side contextual features to a state-of-the-art log-linear PB-SMT system by incorporating contextdependent phrasal translation probabilities learned using decision trees. They considered up to two words and/or POS tags on either side of the source focus word as contextual features. In order to overcome problems of estimation of such"
Y09-1019,2007.tmi-papers.28,1,0.904233,"Missing"
Y09-1019,tiedemann-nygaard-2004-opus,0,0.0233867,"l. Thus we create a dynamic phrase table. A lexicalized reordering model is used for all the experiments undertaken on development and test texts. The source phrase in the reordering table is replaced by the sequence of unique identifiers when the new phrase table is created. After replacing all words by their unique identifyers, we perform MERT using our new phrase table to optimize the feature weights. 7 Results and Analysis The experiments were carried out on the Dutch-to-English Open Subtitles corpus,2 which is collected as part of the Opus collection of freely available parallel corpora (Tiedemann and Nygaard, 2004). The corpus contains user-contributed translations of movie subtitles. The training text contains 286,160 sentences; the development set and test set each contain 1,000 sentences. Dutch sentences were parsed using Tadpole 3 , a morphosyntactic analyzer and dependency parser (Van den Bosch et al., 2007). 2 3 http://urd.let.rug.nl/tiedeman/OPUS/OpenSubtitles.php http://ilk.uvt.nl/tadpole/ 175 Table 1: Experiments with words and part-of-speech. Experiments Baseline Word±2 POS±2 POS±2* Word±2+POS±2 BLEU 32.39 32.48 33.07 33.29 32.59 NIST 6.11 6.11 6.13 6.17 6.09 METEOR 55.39 55.72 56.17 55.72 55."
Y09-1019,H05-1097,0,0.0687212,"present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Using such a model on the German—English Verbmobil corpus, they obtained better alignment quality in terms of improved alignment error rate (AER). However, since alignment is not an end task in itself and is most often used as an intermediate task to generate phrase pairs for the t-tables in PB-SMT systems, improved AER scores do not necessarily result in improved translation quality, as noted by a number of researchers. Vickrey et al. (2005) built classifiers inspired by those used in WSD to fill in any blanks in a partially completed translation. Giménez and Màrquez (2007) extended this work by considering the more general case of frequent phrases and moved to full translation rather than blank-filling on the target side. Attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches of integrating state-of-theart WSD methods into SMT to improve the overall translation quality h"
Y09-1019,N04-1033,0,0.0677221,"his feature is denoted as PW (parent word). Together we refer to these dependency features as the grammatical dependency information (DI) of the focus phrase fˆk , DI ( fˆk ). They are expressed as the conditional probability of the target phrase given the source phrase fˆk and its grammatical dependency information DI ( fˆk ), as in (6): (6) hˆm ( fˆk , DI ( fˆk ), eˆk , sk) = log P ( eˆk |fˆk , DI( fˆk )) 5 Memory-Based Classification As (Stroppa et al., 2007) point out, directly estimating context-dependent phrase translation probabilities using relative frequencies is problematic. Indeed, Zens and Ney (2004) showed that the estimation of P( eˆk |fˆk ) using relative frequencies results in the overestimation of the probabilities of long phrases. In the case of grammatical dependency-informed features, which include the identity of the parent word of the focus phrase, this estimation problem can only become worse. As an alternative, in this work we make use of memory-based machine learning classifiers that are able to estimate P ( eˆk |fˆk , DI ( fˆk )) by similarity-based reasoning over memorized nearest-neighbour examples of source—target phrase translations to a new source phrase to be translate"
