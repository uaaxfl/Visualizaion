2020.coling-main.509,W11-1401,0,0.0688523,"). While systems for such applications are usually designed to generate a particular, task-specific set of questions, some also try to generate as many different questions as possible. Such generation traditionally involves a form of transformation, be it based on shallow rules created manually (Liu et al., 2010) or learned from data (Curto et al., 2012), syntax-based transformation rules (Heilman, 2011), or transformations based on semantic representations (Mannem et al., 2010; Yao and Zhang, 2010; Yao et al., 2012; Chali and Hasan, 2012), with some proposals also integrating discourse cues (Agarwal et al., 2011). There is much less QG research for languages other than English, such as German. G¨utl et al. (2011) focus on the extraction of concepts from German text, reporting very little on how questions are actually constructed. As far as we are aware, Kolditz (2015) is the only systematic exploration of the characteristics and challenges of QG for German. The rule-based QG system he implemented selects a potential answer phrase (NPs, PPs and embedded clauses) based on a syntactic analysis of the input sentence, replaces it with an appropriate question phrase, and transforms the syntactic representat"
2020.coling-main.509,Q17-1010,0,0.00672226,"ace-form) tokens of the source sentence, their part-of-speech tags, and the span of the answer phrase were used as inputs to the model. spaCy (https://spacy.io) with the de core news sm pretrained model was used for tokenization, tagging, and parsing. The answer span was encoded in IOB format. All input sequences were padded with special leading and trailing tokens to indicate their beginning and end. In the encoder stage of the model, the input at each timestep was the concatenation of the embeddings of the token and the POS tag, and the answer span indicator. Pretrained fastText embeddings (Bojanowski et al., 2017) were used to initialize the token embedding matrix, which was then frozen during training. The embedding matrix for the POS tags was randomly initialized. A fixed vocabulary was used for both input and target sequences, which is generated from 100K most frequent words in the corpus. Out-of-vocabulary (OOV) tokens were replaced with a special marker token. The model hyperparameters we used are given in Table 3 in the appendix. We observed that many generated questions could be considered well-formed were it not for the appearance of the special token in place of OOV words. Most such occurrence"
2020.coling-main.509,H05-1103,0,0.101,"ons is left to future work. 3 3.1 Automated question generation Rule-based question generation In computational linguistics, question generation (QG) has been tackled in several, usually applied contexts, mostly focusing on English. Automatically generating questions is a challenging task involving methods such as parsing, coreference resolution, and the transformation of syntactic structures reflecting complex linguistic characteristics. A variety of QG systems were developed, often for educational purposes, e.g., assisting students in reading (Mazidi and Nielsen, 2015), vocabulary learning (Brown et al., 2005; Mostow et al., 2004), or the assessment of reading comprehension (Le et al., 2014). While systems for such applications are usually designed to generate a particular, task-specific set of questions, some also try to generate as many different questions as possible. Such generation traditionally involves a form of transformation, be it based on shallow rules created manually (Liu et al., 2010) or learned from data (Curto et al., 2012), syntax-based transformation rules (Heilman, 2011), or transformations based on semantic representations (Mannem et al., 2010; Yao and Zhang, 2010; Yao et al.,"
2020.coling-main.509,P16-1014,0,0.0175208,"hen is the selection of the specific question that is the appropriate QUD in a given discourse. As an alternative, we also plan to investigate whether the neural question generation approach can be extended with discourse context to support QUD generation in a single step. In terms of future work on the neural network approach, we also plan to leverage the insight that the vocabulary of the generated questions almost entirely derives from that of the source sentence by explicitly encoding it in the neural network’s architecture using pointer networks (Vinyals et al., 2015) or copy mechanisms (Gulcehre et al., 2016). Using pretraining methods (Devlin et al., 2018) to impart the model with a deeper understanding of the problem domain is another potential avenue for this future research. Developing an approach supporting the generation of a specific QUD for each sentence in a given discourse can also be seen as a step towards a normal form realization of QUDs. Such normal forms could facilitate automatic comparison of discourse annotations, which currently is hampered by the fact that the pragmatic principles formulated by De Kuthy et al. (2019) characterize the meaning to be encoded by the question, so of"
2020.coling-main.509,levy-andrew-2006-tregex,0,0.0917938,"G for German. The rule-based QG system he implemented selects a potential answer phrase (NPs, PPs and embedded clauses) based on a syntactic analysis of the input sentence, replaces it with an appropriate question phrase, and transforms the syntactic representation of the declarative input sentence into question form. This is realized using a complex NLP pipeline performing constituency and dependency parsing, morphological analysis, and identification of relevant semantic characteristics. The collected information supports answer phrase selection using a set of 18 rules formulated as Tregex (Levy and Andrew, 2006) patterns. For each answer phrase thus identified, a second set of rules identifies an appropriate question word or phrase. Finally, transformation rules with a 5788 linearization component realize the actual QG. A particular challenge is the correct insertion of the initial constituent of the input sentence into the rest of the sentence so that the initial position of the question can be occupied by the question phrase, as required by German syntax. Failure to generate questions or incorrectly generated questions can result from a number of sources. The NLP pipeline can introduce errors in th"
2020.coling-main.509,D15-1166,0,0.435375,"he approach is not sufficiently robust to produce a broad range of question types for any input sentence. 3.2 Neural question generation Complementing rule-based QG approaches, recent QG research has focused on developing deep neural QG methods. Sequence-to-sequence (Seq2Seq) architectures (Sutskever et al., 2014), where an encoder network learns a representation of the source sequence and a decoder network generates target words, have seen significant success in sequence learning tasks such as machine translation. The inclusion of global and local attention mechanisms (Bahdanau et al., 2014; Luong et al., 2015) and self-attention (Vaswani et al., 2017), has resulted in state-of-the-art performance in several NLP tasks (Edunov et al., 2018; Raffel et al., 2019). Indeed, a recent survey of neural question generation (NQG) research (Pan et al., 2019) shows that Seq2Seq architectures also form the basis of many current QG approaches. Du et al. (2017) conditioned their generative model on target answers by encoding the position of the answer in the context as an input feature. Where generating questions takes entire paragraphs for which a question is to be generated as input, extra information about the"
2020.coling-main.509,D18-1427,0,0.0223874,"l., 2018; Raffel et al., 2019). Indeed, a recent survey of neural question generation (NQG) research (Pan et al., 2019) shows that Seq2Seq architectures also form the basis of many current QG approaches. Du et al. (2017) conditioned their generative model on target answers by encoding the position of the answer in the context as an input feature. Where generating questions takes entire paragraphs for which a question is to be generated as input, extra information about the intent of asking reduces the ambiguity of the task. Another important challenge of QG is question word generation so that Sun et al. (2018) and Du et al. (2017) split the QG task into determining the question word or type and then generating the rest of the question. The former approach employs a template-based approach with two Seq2Seq models while the latter proposes a more flexible approach that involves learning an additional parameter during decoding that explicitly generates the question word. To handle larger contexts and deal with the problem of out-of-vocabulary words, Zhao et al. (2018) implemented a gated self-attention encoder with a Copy/Maxout pointer mechanism. Kumar et al. (2018) leveraged linguistic features such"
2020.coling-main.509,telljohann-etal-2004-tuba,0,0.185343,"Missing"
2020.coling-main.509,D18-1424,0,0.342031,"tion about the intent of asking reduces the ambiguity of the task. Another important challenge of QG is question word generation so that Sun et al. (2018) and Du et al. (2017) split the QG task into determining the question word or type and then generating the rest of the question. The former approach employs a template-based approach with two Seq2Seq models while the latter proposes a more flexible approach that involves learning an additional parameter during decoding that explicitly generates the question word. To handle larger contexts and deal with the problem of out-of-vocabulary words, Zhao et al. (2018) implemented a gated self-attention encoder with a Copy/Maxout pointer mechanism. Kumar et al. (2018) leveraged linguistic features such as POS and NER tags and deep reinforcement learning techniques such as policy gradient methods to add additional task-specific rewards to the training objective. 4 Obtaining data for a neural question generation approach The productivity and rich compositionality of human language makes handcrafting rule-based systems for robust question generation very difficult. Deep neural QG methods, on the other hand, can learn latent representations of syntactic and sem"
2020.coling-main.509,W14-4922,1,0.694069,"attempting to analyze the information structure of naturally occurring data (Ritz et al., 2008; Calhoun et al., 2010). Yet, these approaches were only rewarded with limited success in terms of inter-annotator agreement, arguably because the task of This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons. org/licenses/by/4.0/. 5786 Proceedings of the 28th International Conference on Computational Linguistics, pages 5786–5798 Barcelona, Spain (Online), December 8-13, 2020 identifying QUDs was not made explicit. More recently, Ziai and Meurers (2014) and De Kuthy et al. (2016) showed that for data collected in task contexts including explicit questions, such as answers to reading comprehension questions, focus annotation becomes more reliable. The explicit question context enables experts and non-experts to reach substantial agreement in the annotation of discourse functions such as focus. In addition, automated annotation of information structure becomes feasible when explicit questions are given (Ziai and Meurers, 2018). Bridging the gap from corpora already containing explicit questions to the analysis of any type of authentic language"
2020.coling-main.509,N18-1011,1,0.931855,"tuebingen.de Abstract Questions under Discussion (QUD; Roberts, 2012) are emerging as a conceptually fruitful approach to spelling out the connection between the information structure of a sentence and the nature of the discourse in which the sentence can function. To make this approach useful for analyzing authentic data, Riester, Brunetti & De Kuthy (2018) presented a discourse annotation framework based on explicit pragmatic principles for determining a QUD for every assertion in a text. De Kuthy et al. (2018) demonstrate that this supports more reliable discourse structure annotation, and Ziai and Meurers (2018) show that based on explicit questions, automatic focus annotation becomes feasible. But both approaches are based on manually specified questions. In this paper, we present an automatic question generation approach to partially automate QUD annotation by generating all potentially relevant questions for a given sentence. While transformation rules can concisely capture the typical question formation process, a rule-based approach is not sufficiently robust for authentic data. We therefore employ the transformation rules to generate a large set of sentence-question-answer triples and train a n"
2021.bea-1.3,W16-0521,1,0.885804,"Missing"
2021.bea-1.3,D14-1162,0,0.0860443,"Missing"
2021.bea-1.3,C04-1146,0,0.0172341,"Missing"
2021.bea-1.3,W14-1817,0,0.309398,"anguage, later the selection of texts to be read can in principle follow the individual interests of the student or adult, which boosts the motivation to engage with the book. Linking language learning to a functional goal that someone actually wants to 26 Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications, pages 26–36 April 20, 2021 ©2021 Association for Computational Linguistics and Kay, 2010) showing their growing mastery of the book’s lexical space. Lexical learning is fostered and monitored through automatically generated multi-gap activities (Zesch and Melamud, 2014) that support learning and revision of words in the contexts in which they occur in the book. In section 2 we discuss how a book or other text chosen by the learner is turned in to a graph encoding the lexical space that the learner needs to engage with to read the book, and how words that are morphologically related as word families (Bauer and Nation, 1993) are automatically identified and compactly represented in the graph (2.1.1). In section 3 we then turn to the use of the graph representation of the lexical semantic space of the book to determine the reader’s learning path and represent t"
2021.bea-1.5,E17-1104,0,0.084745,"Missing"
2021.bea-1.5,W13-5608,0,0.0650068,"Missing"
2021.bea-1.5,Q19-1028,0,0.0443662,"Missing"
2021.bea-1.5,D12-1043,0,0.0890443,"Missing"
2021.bea-1.5,W14-1821,0,0.0386411,"Missing"
2021.bea-1.5,D19-1489,0,0.0623543,"Missing"
2021.bea-1.5,W16-4106,0,0.0311344,"Missing"
2021.bea-1.5,C12-1065,1,0.69379,"al., 2019). For second language learners, texts at an appropriate level of proficiency are of particular importance, with a substantial strand of computational linguistic research focusing on this target group, e.g., (Franc¸ois and Fairon, 2012; Pil´an et al., 2014; Xia et al., 2016), and parallel automated proficiency evaluation approaches analyzing L2 writing, e.g., (Lu, 2010; Giagkou et al., 2015; Weiss and Meurers, 2019b). While much of the initial computational linguistic research on readability focused on English, research targeting other languages is also increasingly emerging, e.g., (Hancke et al., 2012; Falkenjack et al., 2013; Dell’Orletta et al., 2014; Giagkou et al., 2017). Some recent approaches employ neural networks and deep learning for classification-related tasks (Martinc et al., 2019; Conneau et al., 2017). For instance, Martinc et al. (2019) employed neural classifiers on English and Slovenian corpora using unsupervised and supervised techniques to assess readability, outperforming some previous state-of-the-art approaches. In this paper, we focus on the Greek language and investigate readability classification using a This paper explores the linguistic complexity of Greek textbo"
2021.bea-1.5,E17-1090,0,0.046189,"Missing"
2021.bea-1.5,W18-0535,0,0.0376473,"Missing"
2021.bea-1.5,W12-2019,1,0.440891,"Missing"
2021.bea-1.5,W19-4440,1,0.901247,"esearch (Howcroft and Demberg, 2017), training neural networks (Azpiazu and Pera, 2019), broadening the corpus basis (Vajjala and Luˇci´c, 2018), and externally grounding the ratings (Redmiles et al., 2019). For second language learners, texts at an appropriate level of proficiency are of particular importance, with a substantial strand of computational linguistic research focusing on this target group, e.g., (Franc¸ois and Fairon, 2012; Pil´an et al., 2014; Xia et al., 2016), and parallel automated proficiency evaluation approaches analyzing L2 writing, e.g., (Lu, 2010; Giagkou et al., 2015; Weiss and Meurers, 2019b). While much of the initial computational linguistic research on readability focused on English, research targeting other languages is also increasingly emerging, e.g., (Hancke et al., 2012; Falkenjack et al., 2013; Dell’Orletta et al., 2014; Giagkou et al., 2017). Some recent approaches employ neural networks and deep learning for classification-related tasks (Martinc et al., 2019; Conneau et al., 2017). For instance, Martinc et al. (2019) employed neural classifiers on English and Slovenian corpora using unsupervised and supervised techniques to assess readability, outperforming some previ"
2021.bea-1.5,W16-0502,0,0.0451783,"Missing"
2021.inlg-1.3,D18-1269,0,0.0300674,"n deployed to address the problem of rare and out-ofvocabulary words and larger contexts (Zhao et al., 2018). 3 Data In terms of datasets for neural question generation models, contemporary approaches are generally trained on datasets created in the question answering context. These datasets, such as SQuAD (Rajpurkar et al., 2016), Quac (Choi et al., 2018), and Coqa (Reddy et al., 2019), are not well-suited for training models for tasks requiring high questionanswer congruence, and they focus on English. Multilingual datasets like XQUAD (Artetxe et al., 2019), MLQA (Lewis et al., 2019), XNLI (Conneau et al., 2018), and TyDi QA (Clark et al., 2020) are similarly unsuitable as they contain only little data, intended as benchmark for the evaluation of question answering systems. Given these limitations of the established English datasets for the research goals we are pursuing, we instead obtained the German QA answer corpus created by De Kuthy et al. (2020) and base our explorations on that dataset. The corpus contains 5.24 million sentence-question-answer triples which were generated by a transformation-based question generation system (Kolditz, 2015) on articles from the German newspaper Die Tageszeitun"
2021.inlg-1.3,2020.coling-main.509,1,0.809927,"Missing"
2021.inlg-1.3,P17-1123,0,0.0250497,"gether with the relative morpho-syntactic richness and partially flexible word order of the German language make it an interesting experimental setting for exploring the potential advantages of character and subword representations. Neural question generation is generally realised as a sequence learning problem, so a sequence-tosequence (seq2seq) architecture (Sutskever et al., 2014) is a logical fit for this type of task. Here, the encoder network learns the latent representation of the source sentence and the decoder network generates the target question one word at a time. The work done by Du et al. (2017) introduces two such models, which are provided with the source sentence and paragraph-level information that encodes the context of the generated question. Borrowing from reinforcement learning, the work by Kumar et al. (2018) introduces policy gradients along with POS tags and named entity mentions to assign task-specific rewards to the training objective. Pointer-generator networks (Gu et al., 2016; See et al., 2017) with gated self-attention have been deployed to address the problem of rare and out-ofvocabulary words and larger contexts (Zhao et al., 2018). 3 Data In terms of datasets for"
2021.inlg-1.3,D16-1166,0,0.0307202,"primarily focus on generating questions in English and consider words to be the atomic unit of meaning. They consequently approach the representation learning and text generation tasks at the word level. This assumption does not necessarily hold for languages such as Chinese, where the individual characters contain rich internal information. Neural language models that are trained on characterlevel inputs have been shown to capture more salient information about morphology than their word-level counterparts (Huang et al., 2016; Marra et al., 2018). Character-aware question answering systems (Golub and He, 2016; Lukovnikov et al., 2017) have similarly been shown to be resilient to the unknown word problem. To capture and combine information about language form and meaning, Bojanowski et al. (2017) proposed treating words as bags of character n-grams to enrich word embeddings with subword information. Byte-pair encoding (Shibata et al., 1999) has seen a recent resurgence in the context of generative language models where it is employed to perform subword segmentation without the necessity of tokenization or mor26 4 Our Character and Subword-based Neural QG Approach current unit in the encoder as we e"
2021.inlg-1.3,P16-1154,0,0.0380124,"or this type of task. Here, the encoder network learns the latent representation of the source sentence and the decoder network generates the target question one word at a time. The work done by Du et al. (2017) introduces two such models, which are provided with the source sentence and paragraph-level information that encodes the context of the generated question. Borrowing from reinforcement learning, the work by Kumar et al. (2018) introduces policy gradients along with POS tags and named entity mentions to assign task-specific rewards to the training objective. Pointer-generator networks (Gu et al., 2016; See et al., 2017) with gated self-attention have been deployed to address the problem of rare and out-ofvocabulary words and larger contexts (Zhao et al., 2018). 3 Data In terms of datasets for neural question generation models, contemporary approaches are generally trained on datasets created in the question answering context. These datasets, such as SQuAD (Rajpurkar et al., 2016), Quac (Choi et al., 2018), and Coqa (Reddy et al., 2019), are not well-suited for training models for tasks requiring high questionanswer congruence, and they focus on English. Multilingual datasets like XQUAD (Ar"
2021.inlg-1.3,W11-1401,0,0.0388487,"ation and in terms of a qualitative analysis of the questions. Going beyond the particular QG task, our results contribute to the general endeavour of exploring the best choices of form or meaning-based input and output representations for neural approaches for a range of NLP tasks depending on their characteristics. A second strand of work for which the relation between the question and the answer sentence as expressed in the text plays a crucial role is the research interested in discourse. An early example of research investigating the role of discourse structure for question generation is Agarwal et al. (2011). They identify discourse relations in a text as cues motivating the generation of a question and then formulate questions that can be answered by the sentences with those discourse relations, while ensuring direct question answer congruence. In a related vein, approaches making use of socalled Questions under Discussion (QuDs) to identify the information structure of a sentence in a given discourse also rely on such a direct relationship between question and answer. In a recent paper pursuing this perspective, De Kuthy et al. (2020) show that a seq2seq based neural approach can successfully g"
2021.inlg-1.3,L18-1473,0,0.0769453,"Missing"
2021.inlg-1.3,Q17-1010,0,0.0520128,"s at the word level. This assumption does not necessarily hold for languages such as Chinese, where the individual characters contain rich internal information. Neural language models that are trained on characterlevel inputs have been shown to capture more salient information about morphology than their word-level counterparts (Huang et al., 2016; Marra et al., 2018). Character-aware question answering systems (Golub and He, 2016; Lukovnikov et al., 2017) have similarly been shown to be resilient to the unknown word problem. To capture and combine information about language form and meaning, Bojanowski et al. (2017) proposed treating words as bags of character n-grams to enrich word embeddings with subword information. Byte-pair encoding (Shibata et al., 1999) has seen a recent resurgence in the context of generative language models where it is employed to perform subword segmentation without the necessity of tokenization or mor26 4 Our Character and Subword-based Neural QG Approach current unit in the encoder as we expect the contextual information provided by the backward pass to not only enrich the sentential representation learned in the encoder but also lower the effective reduction in learnable par"
2021.inlg-1.3,D19-5821,0,0.0283608,"Missing"
2021.inlg-1.3,Q19-1016,0,0.020907,"Missing"
2021.inlg-1.3,P18-1007,0,0.0178889,"l to perform word-level tokenization and part-of-speech (POS) tagging, 3) a second tokenization pass was performed on each word token to generate character and subword tokens, and 4) each character and subword token pertaining to a given word token was assigned the latter’s POS tag and the answer phrase indicator. For character-level tokenization, each word was decomposed into a list of its component Unicode codepoints. Subword tokenization was performed with the HuggingFace Tokenizer library (Wolf et al., 2020). The library provides byte-pair encoding (BPE, Shibata et al., 1999) and unigram (Kudo, 2018) tokenization algorithms. BPE first constructs a baseline vocabulary with all unique symbols in a corpus. Then, merge rules that combine two symbols in the base vocabulary into a new symbol are learned iteratively until a desired final vocabulary size is reached. Conversely, unigram tokenization starts with a large initial vocabulary from which it repeatedly removes symbols that have the least effect on a loss function defined over the training data of a unigram language model. To reduce the size of the base vocabulary in both models, base symbols are directly derived from bytes rather than (a"
2021.inlg-1.3,D18-2012,0,0.0124374,"with all unique symbols in a corpus. Then, merge rules that combine two symbols in the base vocabulary into a new symbol are learned iteratively until a desired final vocabulary size is reached. Conversely, unigram tokenization starts with a large initial vocabulary from which it repeatedly removes symbols that have the least effect on a loss function defined over the training data of a unigram language model. To reduce the size of the base vocabulary in both models, base symbols are directly derived from bytes rather than (all) Unicode codepoints. The library also includes the SentencePiece (Kudo and Richardson, 2018) algorithm, which processes the input as raw string sequences obviating the need for pre-tokenization. Finally, bidirectional LSTM was used as the re5 Evaluation For a comprehensive comparison, we trained five models: a word-level model to replicate De Kuthy et al. (2020), three subword models with different tokenization algorithms (byte-level BPE, SentencePiece BPE, and SentencePiece Unigram), and a character model. All models were trained on the same 400K training samples from the QA corpus for 20 epochs, and validation was performed on 40K samples. For each type of input representation, the"
2021.inlg-1.3,P17-1099,0,0.0366995,"ask. Here, the encoder network learns the latent representation of the source sentence and the decoder network generates the target question one word at a time. The work done by Du et al. (2017) introduces two such models, which are provided with the source sentence and paragraph-level information that encodes the context of the generated question. Borrowing from reinforcement learning, the work by Kumar et al. (2018) introduces policy gradients along with POS tags and named entity mentions to assign task-specific rewards to the training objective. Pointer-generator networks (Gu et al., 2016; See et al., 2017) with gated self-attention have been deployed to address the problem of rare and out-ofvocabulary words and larger contexts (Zhao et al., 2018). 3 Data In terms of datasets for neural question generation models, contemporary approaches are generally trained on datasets created in the question answering context. These datasets, such as SQuAD (Rajpurkar et al., 2016), Quac (Choi et al., 2018), and Coqa (Reddy et al., 2019), are not well-suited for training models for tasks requiring high questionanswer congruence, and they focus on English. Multilingual datasets like XQUAD (Artetxe et al., 2019)"
2021.inlg-1.3,2021.bea-1.17,0,0.019488,"on Natural Language Generation (INLG), pages 24–34, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Linguistics at generating questions that can be answered by a sentence as given in the text, putting a premium on question-answer congruence. This includes QG work in the educational application domain, where the perspective of the question is supposed to reflect the perspective of the author of a given text passage that the student is supposed to learn about (Heilman and Smith, 2010; Heilman, 2011; Rus et al., 2012). Recent work under this perspective includes Stasaski et al. (2021), who propose a neural question generation architecture for the generation of cause-and-effect questions. They extract cause and effect relations from text, which are then used as answers for the neural question generation, aiming at direct question-answer congruence. All the words that the generated question consists of are already given, so only the question word that matches the answer phrase needs to be generated anew. The sentence-question pair in example (1) taken from De Kuthy et al. (2020) illustrates this. (1) A: Auch Otto Graf Lambsdorf ist gegen zweierlei also Otto Graf Lambsdorf is"
2021.inlg-1.3,D15-1166,0,0.0085891,"f the encoder (hidden state, sequences, cell state) is the concatenation of the respective backward and forward layers of each output. For the character-level models, a fixed-size vocabulary consisting of all the unique codepoints in the QA corpus was generated. Similarly, the subword tokenizers were trained on the entire corpus to generate vocabularies with 10K symbols each.1 As the starting point and baseline of our approach, we take the same basic architecture as De Kuthy et al. (2020), a word-embedding based sequence-tosequence model (Sutskever et al., 2014) with multiplicative attention (Luong et al., 2015). This was done in order to ensure comparability of our results with theirs. Furthermore, any fundamental changes to the neural architecture – such as using a Transformer (Vaswani et al., 2017) or a pointer-generator (Zhao et al., 2018) network – would make it more difficult to distinguish between any improvements offered exclusively by the change in input representation and those by the change in architecture. To introduce character– and subword–level tokens, we defined an input pipeline consisting of the following steps: 1) UTF-8 text normalization was performed on the input sentence, 2) the"
2021.inlg-1.3,W18-6319,0,0.0134864,"lemented a post-processing copy module to replace OOV marker tokens in the generated question with the original tokens from the source sentence; this behaviour was replicated for our word-level model. As model hyperparameters, we used: batch size: 128, encoder: Bi-LSTM, decoder: LSTM, encoder/decoder hidden size: 256/512, encoder/decoder dropout: 0.5, word/subword/character embedding dim: 300, decoder beam search width: 5. Table 1 shows the BLEU scores from comparing the ground-truth questions of the test set with corresponding modelgenerated questions. We used the standard SacreBLEU library (Post, 2018)2 for the calculation of the BLEU scores. 1 The subword vocabularies also include the base symbols found in the character vocabulary. In both cases, special meta tokens such as unknown, sentence-start and end markers were additionally added to each vocabulary. 2 Version 1.4.10 with default parameters. 27 Model BLEU 1/2/3/4 Cumulative Word 93.8/86.5/81.0/76.5 (De Kuthy et al., 2020) 84.24 Word (replication) Subword (Byte BPE) Subword (SentPiece BPE) Subword (SentPiece Unigram) Character 93.8/86.5/81.0/76.5 84.20 98.2/93.4/90.0/87.4 91.97 97.0/91.4/87.3/84.1 89.35 98.1/93.3/89.8/87.2 91.76 97.2/"
2021.inlg-1.3,2020.emnlp-main.224,0,0.0217153,"answered by the sentences with those discourse relations, while ensuring direct question answer congruence. In a related vein, approaches making use of socalled Questions under Discussion (QuDs) to identify the information structure of a sentence in a given discourse also rely on such a direct relationship between question and answer. In a recent paper pursuing this perspective, De Kuthy et al. (2020) show that a seq2seq based neural approach can successfully generate meaningful, well-formed questions that can function as Questions under Discussions in a formal theory of discourse. Similarly, Pyatkin et al. (2020) showed that using questionanswer pairs obtained through crowdsourcing can be used reliably to annotate discourse. Based on their crowdsourced data, they train a pipeline of neural models to directly generate such questionanswer pairs from text. The overall goal of question generation supporting discourse analysis is to generate a question for every sentence in a text to explicitly characterize the evolving discourse. Viewed from the perspective of question generation for tasks requiring question-answer congruence, the QG task in essence consists of two steps: (i) replace the answer phrase in"
2021.inlg-1.3,2020.coling-main.228,0,0.0994382,"Missing"
2021.inlg-1.3,D18-1424,0,0.0869086,"ne word at a time. The work done by Du et al. (2017) introduces two such models, which are provided with the source sentence and paragraph-level information that encodes the context of the generated question. Borrowing from reinforcement learning, the work by Kumar et al. (2018) introduces policy gradients along with POS tags and named entity mentions to assign task-specific rewards to the training objective. Pointer-generator networks (Gu et al., 2016; See et al., 2017) with gated self-attention have been deployed to address the problem of rare and out-ofvocabulary words and larger contexts (Zhao et al., 2018). 3 Data In terms of datasets for neural question generation models, contemporary approaches are generally trained on datasets created in the question answering context. These datasets, such as SQuAD (Rajpurkar et al., 2016), Quac (Choi et al., 2018), and Coqa (Reddy et al., 2019), are not well-suited for training models for tasks requiring high questionanswer congruence, and they focus on English. Multilingual datasets like XQUAD (Artetxe et al., 2019), MLQA (Lewis et al., 2019), XNLI (Conneau et al., 2018), and TyDi QA (Clark et al., 2020) are similarly unsuitable as they contain only little"
2021.nlp4call-1.2,W16-0521,1,0.762973,"ial authors and organized as learning sequences. As we will see in section 5, it also allows us to monitor and make explicit learner competencies by enriching the learner model and ultimately perform adaptive sequencing. As a starting point, we describe three resources that facilitate the automatization of this process: (i) a hierarchical structure of language phenomena relevant for English as a Foreign Language, (ii) a general linguistic annotation module, and (iii) a rule-based module for the annotation of language structures. guistic properties to support language-aware document retrieval (Chinkina and Meurers, 2016) or input enrichment and enhancement (Meurers et al., 2010), the work so far fell short of generating tutoring system activities that are pedagogically linked to a linguistic syllabus or curriculum. The approach we are presenting in this paper goes a step further in automatically deriving finegrained metalinguistic characterizations of the language used in or elicited by some given learning material, including both the linguistic phenomena targeted by the materials as well as those incidentally occurring in it. 3 Implementation context and resources The research presented here is being carried"
2021.nlp4call-1.2,P12-2071,0,0.0866193,"Missing"
2021.nlp4call-1.2,W10-1002,1,0.516269,"n section 5, it also allows us to monitor and make explicit learner competencies by enriching the learner model and ultimately perform adaptive sequencing. As a starting point, we describe three resources that facilitate the automatization of this process: (i) a hierarchical structure of language phenomena relevant for English as a Foreign Language, (ii) a general linguistic annotation module, and (iii) a rule-based module for the annotation of language structures. guistic properties to support language-aware document retrieval (Chinkina and Meurers, 2016) or input enrichment and enhancement (Meurers et al., 2010), the work so far fell short of generating tutoring system activities that are pedagogically linked to a linguistic syllabus or curriculum. The approach we are presenting in this paper goes a step further in automatically deriving finegrained metalinguistic characterizations of the language used in or elicited by some given learning material, including both the linguistic phenomena targeted by the materials as well as those incidentally occurring in it. 3 Implementation context and resources The research presented here is being carried out in the context of the development of Didi (http: //did"
2021.nlp4call-1.2,W17-0305,1,0.762614,"Missing"
2021.nlp4call-1.2,W18-0513,1,0.86335,"Missing"
2021.nlp4call-1.2,W19-6310,1,0.80213,"Missing"
2021.nlp4call-1.2,W18-7110,1,0.789308,"Missing"
2021.nlp4call-1.4,D12-1133,0,0.0304298,"ke explicit cohesion measures, such as the number of particular connectives, they are directly applicable across languages. NLP Pipeline We calculate our complexity features following a three-step procedure. First, we run a pipeline of Natural Language Processing (NLP) tools to provide linguistic annotations for the data. The annotation pipeline primarily relies on Stanford CoreNLP (Manning et al., 2014) which we use for sentence segmentation, tokenization, parts-ofspeech (POS) tagging, constituency parsing, and dependency parsing for English and German. We additionally employ the Mate tools (Bohnet and Nivre, 2012) for lemmatization, because CoreNLP only provides a lemmatizer for English but not for German. We also use the OpenNLP Snowball stemmer to extract stems for English and German. For all annotations, we use the respective default models provided with the NLP tools. Second, we count linguistic constructs using a set of extraction rules as well as word frequencies. This procedure is fully identical across languages except for syllable counts, POS-based counts, and syntactic complexity counts which we designed to be comparable across languages as described in 4.1.6 Language Use (U SE) Word frequenc"
2021.nlp4call-1.4,W19-4443,0,0.0289222,"Missing"
2021.nlp4call-1.4,R13-1025,0,0.0384678,"Missing"
2021.nlp4call-1.4,W19-6305,1,0.86174,"Missing"
2021.nlp4call-1.4,P16-4002,1,0.840825,"e come to the conclusion (Section 8) and outlook (Section 9). 2 Related Work Automatic readability assessment has a long history dating back to the first readability formulas developed in the early 20th century, see DuBay (2006) for an overview. Traditional readability formulas employ few surface text characteristics such as text, sentence, and word length (Flesch, 1948; Dale and Chall, 1948). They are still widely used especially in non-linguistic studies on web accessibility (Esfahani et al., 2016; GrootensWiegers et al., 2015), in information retrieval systems (Miltsakaki and Troutt, 2007; Chinkina et al., 2016), and for confirming the compliance of reading materials with specific accessibility guidelines (Weiss et al., 2018; Yaneva et al., 2016), such as Easy-to-Read materials.2 Over the last two decades, there has been a shift towards computational readability classification approaches based on machine learning techniques employing feature engineering with Natural Language Processing (NLP) methods, see Collins-Thompson (2014) and Benjamin (2012) for an overview. Among others, linguistic complexity features from SLA research (Vajjala and Meurers, 2012), word frequency measures (Chen and Meurers, 201"
2021.nlp4call-1.4,D12-1043,0,0.0920119,"Missing"
2021.nlp4call-1.4,C14-1033,0,0.0462083,"Missing"
2021.nlp4call-1.4,P14-5010,0,0.00316022,"ch feature is calculated locally (between neighboring sentences) and globally (across all sentences in the text). These implicit cohesion features were originally proposed in CohMetrix (McNamara et al., 2014). Unlike explicit cohesion measures, such as the number of particular connectives, they are directly applicable across languages. NLP Pipeline We calculate our complexity features following a three-step procedure. First, we run a pipeline of Natural Language Processing (NLP) tools to provide linguistic annotations for the data. The annotation pipeline primarily relies on Stanford CoreNLP (Manning et al., 2014) which we use for sentence segmentation, tokenization, parts-ofspeech (POS) tagging, constituency parsing, and dependency parsing for English and German. We additionally employ the Mate tools (Bohnet and Nivre, 2012) for lemmatization, because CoreNLP only provides a lemmatizer for English but not for German. We also use the OpenNLP Snowball stemmer to extract stems for English and German. For all annotations, we use the respective default models provided with the NLP tools. Second, we count linguistic constructs using a set of extraction rules as well as word frequencies. This procedure is fu"
2021.nlp4call-1.4,C12-1065,1,0.776136,"r and were created to be maximally comparable. To mitigate effects due to the different sizes of the underlying corpora, we only use word frequencies per million words. Based on this, we calculate 56 word frequency features including the mean (log) frequency of all words, lexical words, and function words and their standard deviations as well as frequencies for verbs, nouns, adjectives, and adverbs. 4.1.4 Morphological Complexity (M OR) Morphological complexity has been argued to be an important feature for readability assessment of morphological richer languages than English, such as German (Hancke et al., 2012; Weiss and Meurers, 2018) or Basque (Gonzalez-Dios et al., 2014). However, few measures have been used in readability assessment that are applicable across languages with different morphological systems. We use the Morphological Complexity Index (MCI) proposed by Brezina and Pallotti (2019) to assess morphological complexity independent of language by measuring the variability of morphological exponents of specific parts-of-speech within a text. These morphological exponents can be identified by contrasting word forms with their stems which makes the features applicable across languages. We a"
2021.nlp4call-1.4,levy-andrew-2006-tregex,0,0.0560513,"(complex) NPs per clause, t-unit and sentence, and the length of NPs in words. It also entails features measuring the complexity of verb phrases (VPs) including the number of verb clusters and VPs per clause, t-unit and sentence and the length of verb clusters in words. We also measure the complexity of prepositional phrases (PPs) such as the number of (complex) PPs per clause, t-unit and sentence or the length of PPs in words. Finally, this includes measures of coordinate phrases per clause, t-unit and sentence. While these syntactic features are identified based on language-specific TregEx (Levy and Andrew, 2006) patterns for constituency trees, we carefully designed all extraction rules to yield equivalent results across languages. We also measure syntactic variation based on 12 measures of parse tree edit distances following Chen (2018). 4 Automatic Complexity Analysis 4.1 Complexity Features We calculate 312 features of linguistic complexity merging the feature collections proposed by us in our previous work on German (Weiss and Meurers, 2018) and English (Chen, 2018). These have been successfully used for the tasks of readability assessment (Chen and Meurers, 2018; Weiss and Meurers, 2018; K¨uhber"
2021.nlp4call-1.4,W16-4106,0,0.0507431,"Missing"
2021.nlp4call-1.4,Q19-1028,0,0.0128332,"iate, and advanced language learners which the publisher equates with the Common European Framework of Reference (CEFR) levels A2 (level: easy), B1/B2 (level: medium) and C1 (level: advanced). We extracted all articles from the PDF versions of the respective issues provided to us for research purposes by the publisher.The type setting of the magazines made it impossible to diOnly little research has been conducted on multilingual readability classification. While there are some neural classification approaches that are developed to be applicable across languages (Martinc et al., 2019; Madrazo Azpiazu and Pera, 2019), feature-based approaches are usually languagespecific. An exception is the study by De Clercq and Hoste (2016), who compare the informativeness of lexical, semantic and syntactic features for English and Dutch readability classification. The 3 https://github.com/ionmadrazo/VikiWiki https://www.spotlight-online.de 5 https://www.deutsch-perfekt.com 6 https://www.spotlight-verlag.de 4 Proceedings of the 10th Workshop on Natural Language Processing for Computer Assisted Language Learning (NLP4CALL 2021) 40 rectly extract the individual articles with a PDF converter without loosing the informatio"
2021.nlp4call-1.4,P19-1355,0,0.0257454,"y Assessment research (Crossley, 2020) were shown to be valuable features for readabilty assessemnt. While most readability research focuses on English (Collins-Thompson, 2014), to a lesser degree these approaches have also been employed for other languages such as Russian (Reynolds, 2016), 1. Can we train a successful readability classifier for German and for English using broad complexity modeling? 2. Can these classifiers generalize beyond their training language to cross-lingual contexts? 3. Which linguistic features are relevant for the distinction of reading levels and how do they 1 See Strubell et al. (2019) for a discussion of the considerable energy demands of deep learning approaches in NLP. 2 https://www.inclusion-europe.eu/easy-to-read/ Proceedings of the 10th Workshop on Natural Language Processing for Computer Assisted Language Learning (NLP4CALL 2021) 39 cross-lingual applicability of multilingual models has so far not been investigated, except for a series of studies by Madrazo Azpiazu and Pera on the VikiWiki corpus, which distinguishes simplified Vikidia.org texts for 8 to 13 year old children from regular Wikipedia.org texts for Basque, Catalan, Dutch, English, French, Italian, and Sp"
2021.nlp4call-1.4,W18-0535,0,0.036858,"Missing"
2021.nlp4call-1.4,W19-4437,0,0.0204219,"Missing"
2021.nlp4call-1.4,W12-2019,1,0.766098,"ion retrieval systems (Miltsakaki and Troutt, 2007; Chinkina et al., 2016), and for confirming the compliance of reading materials with specific accessibility guidelines (Weiss et al., 2018; Yaneva et al., 2016), such as Easy-to-Read materials.2 Over the last two decades, there has been a shift towards computational readability classification approaches based on machine learning techniques employing feature engineering with Natural Language Processing (NLP) methods, see Collins-Thompson (2014) and Benjamin (2012) for an overview. Among others, linguistic complexity features from SLA research (Vajjala and Meurers, 2012), word frequency measures (Chen and Meurers, 2017), and features of text cohesion (Crossley et al., 2017) from Writing Quality Assessment research (Crossley, 2020) were shown to be valuable features for readabilty assessemnt. While most readability research focuses on English (Collins-Thompson, 2014), to a lesser degree these approaches have also been employed for other languages such as Russian (Reynolds, 2016), 1. Can we train a successful readability classifier for German and for English using broad complexity modeling? 2. Can these classifiers generalize beyond their training language to c"
2021.nlp4call-1.4,C18-1026,1,0.899392,"lity assessment. Proceedings of the 10th Workshop on Natural Language Processing for Computer Assisted Language Learning (NLP4CALL 2021). Linköping Electronic Conference Proceedings 177: 38– 54. 38 differ between English and German? languages. The lack of suitable training corpora for other languages remains as one major limiting factor (Collins-Thompson, 2014), despite some research efforts to facilitate unsupervised readability assessments (Benzahra and Franc¸ois, 2019; Martinc et al., 2019). For example, there has been some recent work on German readability classifiers for native speakers (Weiss and Meurers, 2018; Weiss et al., 2018; Dittrich et al., 2019). Yet, a lack of corpus resources has so far hindered the development of a readability classifier for German as a second or foreign language (L2) learners. In this article, we introduce a novel crosslingual feature collection for broad linguistic modeling of German and English complexity. Although neural classification approaches have been strongly represented in readability assessment, our literature review (see Section 2) shows that their success has been very much limited on the benchmark data we use for this study and fallen behind the feature-ba"
boyd-etal-2014-merlin,W13-1706,0,\N,Missing
boyd-etal-2014-merlin,I13-1200,1,\N,Missing
boyd-etal-2014-merlin,C12-1065,1,\N,Missing
boyd-etal-2014-merlin,W12-2019,1,\N,Missing
C04-1025,P81-1022,0,0.269584,"riginal rules could be combined without introducing domain compaction. M¨uller (2004) discusses the combinatorial explosion of rules that results for an analysis of German if one wants to flatten the trees in this way. If recursive rules such as adjunction are included – which is necessary since adjuncts and complements can be freely intermixed in the German Mittelfeld – such flattening will not even lead to a finite number of rules. We will return to this issue in section 6. 5 A Parsing Algorithm for GIDLP We have developed a GIDLP parser based on Earley’s algorithm for context-free parsing (Earley, 1970). In Earley’s original algorithm, each edge encodes the interval of the input string it covers. With discontinuous constituents, however, that is no longer an option. In the spirit of Johnson (1985) and Reape (1991), and following Ramsay (1999), we represent edge coverage with bitvectors, stored as integers. For instance, 00101 represents an edge covering words one and three of a five-word sentence.4 Our parsing algorithm begins by seeding the chart with passive edges corresponding to each word in the input and then predicting a compacted instance of the start symbol covering the entire input;"
C04-1025,W97-1506,0,0.0520301,"Missing"
C04-1025,P85-1015,0,0.488142,"ions. Some simply do not use a parser, such as the work based on ConTroll (G¨otz and Meurers, 1997); as a consequence, the eﬃciency and termination properties of parsers cannot be taken for granted in such approaches. The other approaches use a minimal parser that can only take advantage of a small subset of the requisite constraints. Such parsers are typically limited to the general concept of resource sensitivity – every element in the input needs to be found exactly once – and the ability to require certain categories to dominate a contiguous segment of the input. Some of these approaches (Johnson, 1985; Reape, 1991) lack word order constraints altogether. Others (van Noord, 1991; Ramsay, 1999) have the grammar writer provide a combinatory predicate (such as concatenate, shuﬄe, or head-wrap) for each rule specifying how the string coverage of the mother is determined from the string coverages of the daughter. In either case, the task of constructing a word order domain and enforcing word order constraints in that domain is left out of the parsing algorithm; as a result, constraints on word order domains either cannot be stated or are tested in a separate clean-up phase. 4 Defining GIDLP Gram"
C04-1025,P95-1024,0,0.484344,"y contains all and only the terminal yield of the nodes it dominates; there are no holes or additional strings. LP Locality: Precedence statements only constrain the order among elements within the same compacted domain. In other words, precedence constraints cannot look into a compacted domain. Note that these are two distinct functions of domain compaction: defining a domain as covering a contiguous stretch of terminals is in principle independent of defining a domain of elements for LP constraints to apply to. In linearization-based HPSG, domain compaction encodes both aspects. Later work (Kathol and Pollard, 1995; Kathol, 1995; Yatabe, 1996) introduced the notion of partial compaction, in which only a portion of the daughter’s order domain is compacted; the remaining elements are domain unioned. 1 2 Linearization-based HPSG The idea of discontinuous constituency was first introduced into HPSG in a series of papers by Mike Apart from Reape’s approach, there have been proposals for a more complete separation of word order and syntactic structure in HPSG (see, for example, Richter and Sailer, 2001 and Penn, 1999). In this paper, we focus on the majority of linearization-based HPSG approaches, which follo"
C04-1025,P91-1015,0,0.0758081,"Missing"
C04-1025,W89-0206,0,\N,Missing
C10-2031,W07-1604,0,0.0672454,"ta is available, in a surprising number of cases it thus is possible to obtain sufficient information from the relatively narrow window of context provided by n-grams which are small enough to frequently occur but large enough to contain enough predictive information about preposition usage. Introduction The correct use of prepositions is a well-known difficulty for learners of English, and correspondingly the computational analysis of preposition usage has attracted significant attention in recent years (De Felice and Pulman, 2007; De Felice, 2008; Lee and Knutsson, 2008; Gamon et al., 2008; Chodorow et al., 2007; Tetreault and Chodorow, 2008a, 2008b). As a point of reference for the detection of preposition errors in learner language, most of the research starts out by developing a model of preposition usage for native English. For this purpose, virtually all previous approaches employ a machine learning setup combining a range of features, from surface-based evidence to deep linguistically-informed properties. The overall task is approached as a classification problem where the classes are the prepositions and the instances to be classified are the contexts, i.e., the sentences with the prepositions"
C10-2031,W07-1607,0,0.141397,"Missing"
C10-2031,E03-1068,1,0.795801,"blems, in this paper we want to explore the limits of a purely surface-based prediction of prepositions. Essentially, our question is how much predictive information can be found in the immediate distributional context of the preposition. Is it possible to obtain n-gram contexts for prepositions which are small enough to occur frequently enough in the available training data but large enough to contain enough predictive information about preposition usage? This perspective is related to that underlying the variation-n-gram approach for detecting errors in the linguistic annotation of corpora (Dickinson and Meurers, 2003; Dickinson and Meurers, 2005; Boyd et al., 2008). Under that approach, errors in the annotation of linguistic properties (lexical, constituency, or dependency information) are detected by identifying units which recur in the corpus with sufficient identical context so as to make variation in their annotation unlikely to be correct. In a sense, the recurring n-gram contexts are used as exemplar references for the local domains in which the complex linguistic properties are established. The question now is to what extent basic1 n-gram contexts can also be successfully used to capture the lingui"
C10-2031,P05-1040,1,0.923238,"to explore the limits of a purely surface-based prediction of prepositions. Essentially, our question is how much predictive information can be found in the immediate distributional context of the preposition. Is it possible to obtain n-gram contexts for prepositions which are small enough to occur frequently enough in the available training data but large enough to contain enough predictive information about preposition usage? This perspective is related to that underlying the variation-n-gram approach for detecting errors in the linguistic annotation of corpora (Dickinson and Meurers, 2003; Dickinson and Meurers, 2005; Boyd et al., 2008). Under that approach, errors in the annotation of linguistic properties (lexical, constituency, or dependency information) are detected by identifying units which recur in the corpus with sufficient identical context so as to make variation in their annotation unlikely to be correct. In a sense, the recurring n-gram contexts are used as exemplar references for the local domains in which the complex linguistic properties are established. The question now is to what extent basic1 n-gram contexts can also be successfully used to capture the linguistic properties and relations"
C10-2031,I08-1059,0,0.274047,"ets. Where enough data is available, in a surprising number of cases it thus is possible to obtain sufficient information from the relatively narrow window of context provided by n-grams which are small enough to frequently occur but large enough to contain enough predictive information about preposition usage. Introduction The correct use of prepositions is a well-known difficulty for learners of English, and correspondingly the computational analysis of preposition usage has attracted significant attention in recent years (De Felice and Pulman, 2007; De Felice, 2008; Lee and Knutsson, 2008; Gamon et al., 2008; Chodorow et al., 2007; Tetreault and Chodorow, 2008a, 2008b). As a point of reference for the detection of preposition errors in learner language, most of the research starts out by developing a model of preposition usage for native English. For this purpose, virtually all previous approaches employ a machine learning setup combining a range of features, from surface-based evidence to deep linguistically-informed properties. The overall task is approached as a classification problem where the classes are the prepositions and the instances to be classified are the contexts, i.e., the sentence"
C10-2031,W08-1205,0,0.235698,"urprising number of cases it thus is possible to obtain sufficient information from the relatively narrow window of context provided by n-grams which are small enough to frequently occur but large enough to contain enough predictive information about preposition usage. Introduction The correct use of prepositions is a well-known difficulty for learners of English, and correspondingly the computational analysis of preposition usage has attracted significant attention in recent years (De Felice and Pulman, 2007; De Felice, 2008; Lee and Knutsson, 2008; Gamon et al., 2008; Chodorow et al., 2007; Tetreault and Chodorow, 2008a, 2008b). As a point of reference for the detection of preposition errors in learner language, most of the research starts out by developing a model of preposition usage for native English. For this purpose, virtually all previous approaches employ a machine learning setup combining a range of features, from surface-based evidence to deep linguistically-informed properties. The overall task is approached as a classification problem where the classes are the prepositions and the instances to be classified are the contexts, i.e., the sentences with the prepositions omitted. A focus of the previ"
C10-2031,C08-1109,0,0.105159,"urprising number of cases it thus is possible to obtain sufficient information from the relatively narrow window of context provided by n-grams which are small enough to frequently occur but large enough to contain enough predictive information about preposition usage. Introduction The correct use of prepositions is a well-known difficulty for learners of English, and correspondingly the computational analysis of preposition usage has attracted significant attention in recent years (De Felice and Pulman, 2007; De Felice, 2008; Lee and Knutsson, 2008; Gamon et al., 2008; Chodorow et al., 2007; Tetreault and Chodorow, 2008a, 2008b). As a point of reference for the detection of preposition errors in learner language, most of the research starts out by developing a model of preposition usage for native English. For this purpose, virtually all previous approaches employ a machine learning setup combining a range of features, from surface-based evidence to deep linguistically-informed properties. The overall task is approached as a classification problem where the classes are the prepositions and the instances to be classified are the contexts, i.e., the sentences with the prepositions omitted. A focus of the previ"
C12-1027,brooke-hirst-2012-measuring,0,0.139788,"Missing"
C12-1027,P05-1040,1,0.829444,"Missing"
C12-1027,W07-0602,0,0.44284,"Missing"
C12-1027,U09-1008,0,0.117504,"tion, Second Language Acquisition, Learner Corpora. KEYWORDS IN G ERMAN : Muttersprachenerkennung, Autoren-Profiling, Textklassifkation, Zweitspracherwerb, Lernerkorpora. Proceedings of COLING 2012: Technical Papers, pages 425–440, COLING 2012, Mumbai, December 2012. 425 1 Introduction Inferring characteristics of an author by automatically analyzing that author’s texts is a task that is increasingly drawing attention in recent years. Traits such as gender, age, level of education or native language are some of the properties targeted thus far (e.g., Koppel et al., 2005; Estival et al., 2007; Wong and Dras, 2009). The work presented in this paper examines one particular characteristic, namely the author’s native language, with the task being to infer it from a text written in a second language. So we explore the task of Native Language Identification (NLI), which is of interest for a number of reasons. The impact of one’s native language on a second language is studied in Second Language Acquisition (SLA) research, aimed at understanding how languages are acquired and how language works in general. Of particular relevance here is the notion of Transfer: “Transfer is the influence resulting from simila"
C12-1027,D11-1148,0,0.468064,"Missing"
C12-1027,E03-1068,1,\N,Missing
C12-1065,W10-1001,0,0.286712,"Missing"
C12-1065,N04-1025,0,0.438833,"Missing"
C12-1065,W11-2308,0,0.203513,"Missing"
C12-1065,D12-1043,0,0.221952,"Missing"
C12-1065,N07-1058,0,0.252867,"Missing"
C12-1065,W08-0909,0,0.387414,"Missing"
C12-1065,klerke-sogaard-2012-dsim,0,0.0467617,"Missing"
C12-1065,levy-andrew-2006-tregex,0,0.0394675,"Missing"
C12-1065,E09-2013,0,0.100464,"Missing"
C12-1065,W10-0406,0,0.0892198,"Missing"
C12-1065,W08-1006,0,0.0490854,"Missing"
C12-1065,C08-1098,0,0.0189569,"Missing"
C12-1065,P05-1065,0,0.43747,"Missing"
C12-1065,A97-1014,0,0.0176422,"Missing"
C12-1065,W12-2019,1,0.523806,"Missing"
C12-1065,sato-etal-2008-automatic,0,\N,Missing
C14-1185,C10-3009,0,0.0339601,"Missing"
C14-1185,C12-1025,0,0.359564,"t-Free Grammar (CFG) features, i.e., local trees. Their approach with a binary representation of non-lexicalized rules (except for those rules lexicalized with function words and punctuation) outperformed a setup using only lexical features, such as n-grams, on data from the International Corpus of Learner English (ICLE; Granger et al., 2002). Swanson and Charniak (2012) used binary feature representations of CFG and Tree Substitution Grammar (TSG) rules replacing terminals (except for function words) by a special symbol. TSG outperformed CFG features in their settings. Among several options, Brooke and Hirst (2012) explored using non-lexicalized CFG production rules in a binary feature encoding on three corpora: ICLE, FCE (Yannakoudakis et al., 2011), and Lang-8 (Brooke and Hirst, 2013a). The authors conclude that including CFG features generally boosts the performance of the system. In the context of the First NLI Shared Task, in Bykh et al. (2013) we showed that non-lexicalized frequency-based CFG features contribute relevant information. Other recent work has focused on TSGs (Tetreault et al., 2012; Brooke and Hirst, 2013b; Swanson and Charniak, 2012; Swanson and Charniak, 2013; Swanson, 2013; Malmas"
C14-1185,W13-1725,0,0.0627545,"and punctuation) outperformed a setup using only lexical features, such as n-grams, on data from the International Corpus of Learner English (ICLE; Granger et al., 2002). Swanson and Charniak (2012) used binary feature representations of CFG and Tree Substitution Grammar (TSG) rules replacing terminals (except for function words) by a special symbol. TSG outperformed CFG features in their settings. Among several options, Brooke and Hirst (2012) explored using non-lexicalized CFG production rules in a binary feature encoding on three corpora: ICLE, FCE (Yannakoudakis et al., 2011), and Lang-8 (Brooke and Hirst, 2013a). The authors conclude that including CFG features generally boosts the performance of the system. In the context of the First NLI Shared Task, in Bykh et al. (2013) we showed that non-lexicalized frequency-based CFG features contribute relevant information. Other recent work has focused on TSGs (Tetreault et al., 2012; Brooke and Hirst, 2013b; Swanson and Charniak, 2012; Swanson and Charniak, 2013; Swanson, 2013; Malmasi et al., 2013). Before extending syntactic modeling further, in this paper we want to systematically explore the range of options involving CFG rule features for NLI. We con"
C14-1185,C12-1027,1,0.862375,"or domain dependence. Yet, NLI research has since established that lexical features, such as word-based n-grams, are among the best performing features both in singleThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1962 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1962–1973, Dublin, Ireland, August 23-29 2014. and in cross-corpus settings (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Jarvis and Crossley, 2012; Brooke and Hirst, 2013b; Bykh et al., 2013; Gebre et al., 2013; Jarvis et al., 2013; Lynum, 2013), making them an essential component of any approach with state-of-the-art performance. At the same time, the question whether an NLI approach and its results capture general characteristics of language and language learning instead of only encoding the characteristics of a specific data set remains an essential concern. In the experiments in this paper, we thus include experiments on both a topic-balanced single-corpus and on a highly heterogeneous cross-corpus data se"
C14-1185,W13-1726,1,0.875629,"Swanson and Charniak (2012) used binary feature representations of CFG and Tree Substitution Grammar (TSG) rules replacing terminals (except for function words) by a special symbol. TSG outperformed CFG features in their settings. Among several options, Brooke and Hirst (2012) explored using non-lexicalized CFG production rules in a binary feature encoding on three corpora: ICLE, FCE (Yannakoudakis et al., 2011), and Lang-8 (Brooke and Hirst, 2013a). The authors conclude that including CFG features generally boosts the performance of the system. In the context of the First NLI Shared Task, in Bykh et al. (2013) we showed that non-lexicalized frequency-based CFG features contribute relevant information. Other recent work has focused on TSGs (Tetreault et al., 2012; Brooke and Hirst, 2013b; Swanson and Charniak, 2012; Swanson and Charniak, 2013; Swanson, 2013; Malmasi et al., 2013). Before extending syntactic modeling further, in this paper we want to systematically explore the range of options involving CFG rule features for NLI. We consider non-lexicalized and lexicalized CFG features, and different feature representations, from binary encodings to a normalized frequency encoding inspired by a varia"
C14-1185,W13-1727,0,0.185991,"Missing"
C14-1185,W13-1728,0,0.0720721,"rd-based n-grams, are among the best performing features both in singleThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1962 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1962–1973, Dublin, Ireland, August 23-29 2014. and in cross-corpus settings (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Jarvis and Crossley, 2012; Brooke and Hirst, 2013b; Bykh et al., 2013; Gebre et al., 2013; Jarvis et al., 2013; Lynum, 2013), making them an essential component of any approach with state-of-the-art performance. At the same time, the question whether an NLI approach and its results capture general characteristics of language and language learning instead of only encoding the characteristics of a specific data set remains an essential concern. In the experiments in this paper, we thus include experiments on both a topic-balanced single-corpus and on a highly heterogeneous cross-corpus data set. The range of feature types used in NLI research raises a further question, namely how th"
C14-1185,W13-1714,0,0.5112,"e among the best performing features both in singleThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1962 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1962–1973, Dublin, Ireland, August 23-29 2014. and in cross-corpus settings (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Jarvis and Crossley, 2012; Brooke and Hirst, 2013b; Bykh et al., 2013; Gebre et al., 2013; Jarvis et al., 2013; Lynum, 2013), making them an essential component of any approach with state-of-the-art performance. At the same time, the question whether an NLI approach and its results capture general characteristics of language and language learning instead of only encoding the characteristics of a specific data set remains an essential concern. In the experiments in this paper, we thus include experiments on both a topic-balanced single-corpus and on a highly heterogeneous cross-corpus data set. The range of feature types used in NLI research raises a further question, namely how the different sources o"
C14-1185,W13-1734,0,0.0396779,"orming features both in singleThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1962 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1962–1973, Dublin, Ireland, August 23-29 2014. and in cross-corpus settings (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Jarvis and Crossley, 2012; Brooke and Hirst, 2013b; Bykh et al., 2013; Gebre et al., 2013; Jarvis et al., 2013; Lynum, 2013), making them an essential component of any approach with state-of-the-art performance. At the same time, the question whether an NLI approach and its results capture general characteristics of language and language learning instead of only encoding the characteristics of a specific data set remains an essential concern. In the experiments in this paper, we thus include experiments on both a topic-balanced single-corpus and on a highly heterogeneous cross-corpus data set. The range of feature types used in NLI research raises a further question, namely how the different sources of information"
C14-1185,W13-1716,0,0.209404,"(2012) explored using non-lexicalized CFG production rules in a binary feature encoding on three corpora: ICLE, FCE (Yannakoudakis et al., 2011), and Lang-8 (Brooke and Hirst, 2013a). The authors conclude that including CFG features generally boosts the performance of the system. In the context of the First NLI Shared Task, in Bykh et al. (2013) we showed that non-lexicalized frequency-based CFG features contribute relevant information. Other recent work has focused on TSGs (Tetreault et al., 2012; Brooke and Hirst, 2013b; Swanson and Charniak, 2012; Swanson and Charniak, 2013; Swanson, 2013; Malmasi et al., 2013). Before extending syntactic modeling further, in this paper we want to systematically explore the range of options involving CFG rule features for NLI. We consider non-lexicalized and lexicalized CFG features, and different feature representations, from binary encodings to a normalized frequency encoding inspired by a variationist sociolinguistic perspective. Previous research in this domain often limited the use of lexicalized rules given that the lexicalization may lead to an unintended topic or domain dependence. Yet, NLI research has since established that lexical features, such as word-b"
C14-1185,P12-2038,0,0.145486,"res, reaching from character or word-based n-grams to different types of syntactic models have been employed in NLI. For example, Wong and Dras (2011) utilized character and part-of-speech (POS) n-grams as well as cross-sections of parse trees and Context-Free Grammar (CFG) features, i.e., local trees. Their approach with a binary representation of non-lexicalized rules (except for those rules lexicalized with function words and punctuation) outperformed a setup using only lexical features, such as n-grams, on data from the International Corpus of Learner English (ICLE; Granger et al., 2002). Swanson and Charniak (2012) used binary feature representations of CFG and Tree Substitution Grammar (TSG) rules replacing terminals (except for function words) by a special symbol. TSG outperformed CFG features in their settings. Among several options, Brooke and Hirst (2012) explored using non-lexicalized CFG production rules in a binary feature encoding on three corpora: ICLE, FCE (Yannakoudakis et al., 2011), and Lang-8 (Brooke and Hirst, 2013a). The authors conclude that including CFG features generally boosts the performance of the system. In the context of the First NLI Shared Task, in Bykh et al. (2013) we showe"
C14-1185,N13-1009,0,0.0814227,"s. Among several options, Brooke and Hirst (2012) explored using non-lexicalized CFG production rules in a binary feature encoding on three corpora: ICLE, FCE (Yannakoudakis et al., 2011), and Lang-8 (Brooke and Hirst, 2013a). The authors conclude that including CFG features generally boosts the performance of the system. In the context of the First NLI Shared Task, in Bykh et al. (2013) we showed that non-lexicalized frequency-based CFG features contribute relevant information. Other recent work has focused on TSGs (Tetreault et al., 2012; Brooke and Hirst, 2013b; Swanson and Charniak, 2012; Swanson and Charniak, 2013; Swanson, 2013; Malmasi et al., 2013). Before extending syntactic modeling further, in this paper we want to systematically explore the range of options involving CFG rule features for NLI. We consider non-lexicalized and lexicalized CFG features, and different feature representations, from binary encodings to a normalized frequency encoding inspired by a variationist sociolinguistic perspective. Previous research in this domain often limited the use of lexicalized rules given that the lexicalization may lead to an unintended topic or domain dependence. Yet, NLI research has since established"
C14-1185,W13-1719,0,0.0223364,"ooke and Hirst (2012) explored using non-lexicalized CFG production rules in a binary feature encoding on three corpora: ICLE, FCE (Yannakoudakis et al., 2011), and Lang-8 (Brooke and Hirst, 2013a). The authors conclude that including CFG features generally boosts the performance of the system. In the context of the First NLI Shared Task, in Bykh et al. (2013) we showed that non-lexicalized frequency-based CFG features contribute relevant information. Other recent work has focused on TSGs (Tetreault et al., 2012; Brooke and Hirst, 2013b; Swanson and Charniak, 2012; Swanson and Charniak, 2013; Swanson, 2013; Malmasi et al., 2013). Before extending syntactic modeling further, in this paper we want to systematically explore the range of options involving CFG rule features for NLI. We consider non-lexicalized and lexicalized CFG features, and different feature representations, from binary encodings to a normalized frequency encoding inspired by a variationist sociolinguistic perspective. Previous research in this domain often limited the use of lexicalized rules given that the lexicalization may lead to an unintended topic or domain dependence. Yet, NLI research has since established that lexical f"
C14-1185,C12-1158,0,0.329855,"on words) by a special symbol. TSG outperformed CFG features in their settings. Among several options, Brooke and Hirst (2012) explored using non-lexicalized CFG production rules in a binary feature encoding on three corpora: ICLE, FCE (Yannakoudakis et al., 2011), and Lang-8 (Brooke and Hirst, 2013a). The authors conclude that including CFG features generally boosts the performance of the system. In the context of the First NLI Shared Task, in Bykh et al. (2013) we showed that non-lexicalized frequency-based CFG features contribute relevant information. Other recent work has focused on TSGs (Tetreault et al., 2012; Brooke and Hirst, 2013b; Swanson and Charniak, 2012; Swanson and Charniak, 2013; Swanson, 2013; Malmasi et al., 2013). Before extending syntactic modeling further, in this paper we want to systematically explore the range of options involving CFG rule features for NLI. We consider non-lexicalized and lexicalized CFG features, and different feature representations, from binary encodings to a normalized frequency encoding inspired by a variationist sociolinguistic perspective. Previous research in this domain often limited the use of lexicalized rules given that the lexicalization may lead to"
C14-1185,W13-1706,0,0.795712,"ropose a technique to optimize and tune it. Combining the best performing syntactic features with four types of n-grams outperforms the best approach of the NLI Shared Task 2013. 1 Introduction and related work Native Language Identification (NLI) is the task of identifying the native language of a writer by analyzing texts written by this writer in a non-native language. NLI started to attract attention in computational linguistics with the work of Koppel et al. (2005). Since then, the interest has increased steadily, leading to the First NLI Shared Task in 2013, with 29 participating teams (Tetreault et al., 2013). The task of NLI is usually treated as a text classification problem with the L1s as classes. A wide range of features, reaching from character or word-based n-grams to different types of syntactic models have been employed in NLI. For example, Wong and Dras (2011) utilized character and part-of-speech (POS) n-grams as well as cross-sections of parse trees and Context-Free Grammar (CFG) features, i.e., local trees. Their approach with a binary representation of non-lexicalized rules (except for those rules lexicalized with function words and punctuation) outperformed a setup using only lexica"
C14-1185,D11-1148,0,0.0490764,"fying the native language of a writer by analyzing texts written by this writer in a non-native language. NLI started to attract attention in computational linguistics with the work of Koppel et al. (2005). Since then, the interest has increased steadily, leading to the First NLI Shared Task in 2013, with 29 participating teams (Tetreault et al., 2013). The task of NLI is usually treated as a text classification problem with the L1s as classes. A wide range of features, reaching from character or word-based n-grams to different types of syntactic models have been employed in NLI. For example, Wong and Dras (2011) utilized character and part-of-speech (POS) n-grams as well as cross-sections of parse trees and Context-Free Grammar (CFG) features, i.e., local trees. Their approach with a binary representation of non-lexicalized rules (except for those rules lexicalized with function words and punctuation) outperformed a setup using only lexical features, such as n-grams, on data from the International Corpus of Learner English (ICLE; Granger et al., 2002). Swanson and Charniak (2012) used binary feature representations of CFG and Tree Substitution Grammar (TSG) rules replacing terminals (except for funct"
C14-1185,P11-1019,0,0.204059,"se rules lexicalized with function words and punctuation) outperformed a setup using only lexical features, such as n-grams, on data from the International Corpus of Learner English (ICLE; Granger et al., 2002). Swanson and Charniak (2012) used binary feature representations of CFG and Tree Substitution Grammar (TSG) rules replacing terminals (except for function words) by a special symbol. TSG outperformed CFG features in their settings. Among several options, Brooke and Hirst (2012) explored using non-lexicalized CFG production rules in a binary feature encoding on three corpora: ICLE, FCE (Yannakoudakis et al., 2011), and Lang-8 (Brooke and Hirst, 2013a). The authors conclude that including CFG features generally boosts the performance of the system. In the context of the First NLI Shared Task, in Bykh et al. (2013) we showed that non-lexicalized frequency-based CFG features contribute relevant information. Other recent work has focused on TSGs (Tetreault et al., 2012; Brooke and Hirst, 2013b; Swanson and Charniak, 2012; Swanson and Charniak, 2013; Swanson, 2013; Malmasi et al., 2013). Before extending syntactic modeling further, in this paper we want to systematically explore the range of options involvi"
C16-1071,C10-3009,0,0.0190742,"Missing"
C16-1071,C10-1011,0,0.0180263,"with 11 L1-backgrounds (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu and Turkish) at three proficiency levels (low, medium, high). Each of the 11 L1s is represented by 1,100 essays (900 training, 100 development, 100 test). We use the union of the training and development sets for training and the standard test set for testing. In total for all L1s, we thus train on 11,000 and test on 1,100 essays. 4 Tools We utilized the MATE tools2 (Bj¨orkelund et al., 2010) for data preprocessing (tokenization, lemmatizing, POS-tagging) and the MATE dependency parser (Bohnet, 2010) to identify the arguments of a verb realized in a sentence, i.e., the subcat frame that was realized. For hierarchical clustering we employed WEKA (Hall et al., 2009). To process the resulting dendrograms we used the Libnewicktree3 tree parser. Finally, classification was carried out using L2-regularized Logistic Regression from the LIBLINEAR package (Fan et al., 2008) accessed through WEKA. 5 Features The hypothesis we are testing is whether writers with different L1s prefer different subcat variants. To systematically explore the potential benefits of feature grouping, we start with simple"
C16-1071,C12-1027,1,0.889661,"Missing"
C16-1071,C14-1185,1,0.943498,"es” (Labov, 1972), he found that the presence or absence of the consonant [r] in postvocalic position (e.g., fourth) correlates with the ranking of people in status (social stratification). Hence, under a variationist perspective, one observes which of the possible variants of a variable is chosen by a particular speaker (Tagliamonte, 2011). Recent research in the language learning context argues that a preference for particular variants can also be indicative of individual characteristics such as proficiency or L1-background (L¨udeling, 2011; Callies and Zaytseva, 2011; Meurers et al., 2014; Bykh and Meurers, 2014). Variationist features To obtain variationist features one has to implement the logic described in the previous paragraph. It requires choosing some language variables that can be realized by a particular set of variants. For our first explorations of the proposed technique, we chose verb lemmas as variables and the different subcategorization (subcat) patterns of that lemma as variants. Grouping technique As motivated above, we want to explore whether writers with a given L1 prefer certain subcat variants when realizing a particular verb lemma variable. In the training corpus, we can record"
C16-1071,W13-1726,1,0.897661,"Missing"
C16-1071,D14-1142,0,0.402272,"Missing"
C16-1071,W13-1714,0,0.225225,"Missing"
C16-1071,W15-0606,0,0.147409,"Missing"
C16-1071,D14-1144,0,0.0167672,"subject part of the verb subcat, and in addition we consider only those subjects, which are tagged as personal pronouns (prp) or nouns (nn). So, based on our training data, we extract features such as believe sbj+prp and believe sbj+nn, or study sbj+prp and study sbj+nn, etc. Then we run one vs. rest classifiers with L1 Chinese vs. the western L1s in our set, namely French, German, Italian and Spanish. The classifiers follow the logic of the [s/c] and [s/c, +bvm] settings discussed in section 6. To determine distinctive patterns, we used the weights assigned to the features by the classifier (Malmasi and Dras, 2014). First, we explored the general usage pattern for sbj+prp and sbj+nn variants, detached from particular verb lemmas. That was done by cutting off the dendrograms for the [s/c] settings at the root (cut-off 3.0), which results in having all of the considered verbs in a single cluster and hence, just the two variants, i.e., sbj+prp and sbj+nn, encoded by the relative frequency for each text. It turned out that both weights are negative, showing that there do not seem to be any pattern indicative for L1 Chinese compared to the 7 Best ensemble optimization parameters for all ensembles in this pap"
C16-1071,N13-1009,0,0.0464622,"Missing"
C16-1071,E14-4033,0,0.312099,"Missing"
C16-1071,W13-1706,0,0.0671222,"1 where f (v, V, D) is defined as above (1), and D = t is a given text. 5. Terminate evaluation (at the latest) after a particular cut-off r yielded one single group containing the whole variables set. Note that the same technique can also be used to group variants based on their frequency proportions for all variables using the k label-based data subsets – an option we here do not go into further to keep things clear and within the space constraints. 3 Data For the experiments described in this paper, we use the TOEFL11 corpus (Blanchard et al., 2013) introduced for the NLI Shared Task 2013 (Tetreault et al., 2013), which has become a common frame of reference for NLI research. It consists of essays written by English learners with 11 L1-backgrounds (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu and Turkish) at three proficiency levels (low, medium, high). Each of the 11 L1s is represented by 1,100 essays (900 training, 100 development, 100 test). We use the union of the training and development sets for training and the standard test set for testing. In total for all L1s, we thus train on 11,000 and test on 1,100 essays. 4 Tools We utilized the MATE tools2 (Bj¨orkel"
C18-1026,D12-1133,0,0.0175672,"son’s (2000) Dependency Locality Theory. Word frequency ratios based on Subtlex-DE (Brysbaert et al., 2011), dlexDB (Heister et al., 2011), Karlsruhe Children’s Texts (Lavalley et al., 2015) Approximation of age of active use based on Karlsruhe Children’s Texts. Table 3: Overview over complexity measures grouped by feature categories. In order to extract these measures, we employ an elaborate analysis pipeline which relies on a number of NLP tools and external linguistic resources. We use OpenNLP 1.6.0 for tokenization and sentence segmentation.7 This serves as input for the Mate tools 3.6.0 (Bohnet and Nivre, 2012), which perform a morphological analysis, lemmatization, POS tagging, and dependency parsing. We then use the JWord5 Our feature collection draws from varying perspectives on language complexity including SLA and human language processing research. While the confirmation or refutation of specific theories underlying these measures is an interesting research endeavor, our empirical questions focus on which of these features support the distinction of texts targeting different audiences. 6 We are working on integrating our German complexity analysis pipeline into CTAP (Chen and Meurers, 2016) to"
C18-1026,W16-4113,1,0.879033,"0 (Bohnet and Nivre, 2012), which perform a morphological analysis, lemmatization, POS tagging, and dependency parsing. We then use the JWord5 Our feature collection draws from varying perspectives on language complexity including SLA and human language processing research. While the confirmation or refutation of specific theories underlying these measures is an interesting research endeavor, our empirical questions focus on which of these features support the distinction of texts targeting different audiences. 6 We are working on integrating our German complexity analysis pipeline into CTAP (Chen and Meurers, 2016) to make it generally available and will include an online documentation for each feature. 7 http://opennlp.apache.org 307 Splitter 3.4.0 for compound analysis.8 The Mate POS tags are further used to inform the Stanford PCFG parser 3.6.0 (Rafferty and Manning, 2008) and the Berkeley parser 1.7.0 (Petrov and Klein, 2007), which we use for constituency and topological field parsing. For all tools, we use the German default models that were provided with them, except for the Berkeley parser, for which we use the topological field model by Ziai and Meurers (2018). With these annotations, we extrac"
C18-1026,C10-2032,0,0.587248,"s to information required for political and social participation, automatic readability assessment is of immediate social relevance. Accordingly, it has attracted considerable This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 303 Proceedings of the 27th International Conference on Computational Linguistics, pages 303–317 Santa Fe, New Mexico, USA, August 20-26, 2018. research interest over the last decades, particularly for the assessment of English (Crossley et al., 2011; Chen and Meurers, 2017; Feng et al., 2010). For German readability assessment, however, little progress has been made in recent years, despite a series of promising results published around the turn of the decade (Vor der Br¨uck et al., 2008; Hancke et al., 2012). In particular, German readability research has suffered from the lack of a shared reference corpus and sufficiently comparable corpora for cross-corpus testing of readability models: While for English research, the Common Core corpus consisting of examples from the English Language Arts Standards of the Common Core State Standards, and the WeeklyReader corpus of online news"
C18-1026,D12-1043,0,0.452727,"Missing"
C18-1026,C12-1065,1,0.835682,"e linguistische Bereiche informtiertem Modell deutscher Text-Lesbarkeit um das erste, f¨ur das robuste Ergebnisse in einer korpusu¨ bergreifenden Evaluation dokumentiert sind. Dar¨uber hinaus tragen wir mit unserer Arbeit zwei neue Datens¨atze zur Erforschung deutscher Text-Lesbarkeit bei, die auf Texten basieren, deren Eignung f¨ur ihre respektiven Zielgruppen extern durch wiederholte Rezeption validiert wurde: Wir haben aus Untertiteln deutscher Nachrichtenbeitr¨age das Tagesschau/Logo Korpus erstellt. Weiterhin haben wir das GEO/GEOlino Korpus betr¨achtlich erweitert, das urspr¨unglich von Hancke et al. (2012) erstellt wurde. 1 Introduction Readability assessment refers to the task of (automatically) linking a text to the appropriate target audience based on its complexity. A diverse spectrum of potential application domains has been identified for this task in the literature, ranging from the design and evaluation of education materials, to information retrieval, and text simplification. Given the increasing need for learning material adapted to different audiences and the barrier-free access to information required for political and social participation, automatic readability assessment is of imm"
C18-1026,N07-1051,0,0.0170888,"ic theories underlying these measures is an interesting research endeavor, our empirical questions focus on which of these features support the distinction of texts targeting different audiences. 6 We are working on integrating our German complexity analysis pipeline into CTAP (Chen and Meurers, 2016) to make it generally available and will include an online documentation for each feature. 7 http://opennlp.apache.org 307 Splitter 3.4.0 for compound analysis.8 The Mate POS tags are further used to inform the Stanford PCFG parser 3.6.0 (Rafferty and Manning, 2008) and the Berkeley parser 1.7.0 (Petrov and Klein, 2007), which we use for constituency and topological field parsing. For all tools, we use the German default models that were provided with them, except for the Berkeley parser, for which we use the topological field model by Ziai and Meurers (2018). With these annotations, we extract all instances of the linguistic constructs that we need to calculate the final 400 complexity ratios.9 5 Study 1: Which complexity measures are informative? 5.1 Set-Up We first want to determine the informativeness of each measure for distinguishing between adult and child target audience. For this, we calculate the i"
C18-1026,W08-1006,0,0.0307838,"ng research. While the confirmation or refutation of specific theories underlying these measures is an interesting research endeavor, our empirical questions focus on which of these features support the distinction of texts targeting different audiences. 6 We are working on integrating our German complexity analysis pipeline into CTAP (Chen and Meurers, 2016) to make it generally available and will include an online documentation for each feature. 7 http://opennlp.apache.org 307 Splitter 3.4.0 for compound analysis.8 The Mate POS tags are further used to inform the Stanford PCFG parser 3.6.0 (Rafferty and Manning, 2008) and the Berkeley parser 1.7.0 (Petrov and Klein, 2007), which we use for constituency and topological field parsing. For all tools, we use the German default models that were provided with them, except for the Berkeley parser, for which we use the topological field model by Ziai and Meurers (2018). With these annotations, we extract all instances of the linguistic constructs that we need to calculate the final 400 complexity ratios.9 5 Study 1: Which complexity measures are informative? 5.1 Set-Up We first want to determine the informativeness of each measure for distinguishing between adult"
C18-1026,W16-0534,0,0.0152117,"hat the typical aggregation of word frequencies across documents are less informative than richer representations including frequency standard deviations. In contrast to English, research on readability assessment for other languages, such as German, is more limited. There was a series of articles on this issue from the late 2000s to the early 2010s that demonstrated the benefits of broad linguistic modeling, in particular the use of morphological complexity measures for languages with rich morphological systems like German (Vor der Br¨uck et al., 2008; Hancke et al., 2012), but also Russian (Reynolds, 2016) or French (Franc¸ois and Fairon, 2012). The readability checker DeLite of Vor der Br¨uck et al. (2008) is one of the first more sophisticated approaches that went beyond using simple readability formulas for German. The tool employs morphological, lexical, syntactical, semantic, and discourse measures, which they trained on municipal administration texts rated for their readability by humans in an online readability study involving 500 texts and 300 participant, resulting in overall 3,000 ratings. However, due to the specific nature of the data, the robustness of the approach across genres is"
C18-1026,W12-2019,1,0.94968,"Chall and Dale, 1995), see for an overview DuBay (2004). While these formula are still used in some nonlinguistic studies (Woodmansey, 2010; Grootens-Wiegers et al., 2015; Esfahani et al., 2016), a decade ago research shifted towards using more elaborate statistical modeling approaches based on larger sets of linguistically more informed features. Automatic readability assessment has benefited from the use of Natural Language Processing tools for the assessment of syntactic, lexical, and discourse measures and from adapting complexity measures employed in Second Language Acquisition research (Vajjala and Meurers, 2012; Feng et al., 2010). There has also been extensive research on the relevance of cohesion 1 We are currently negotiating with the broadcasters of Tagesschau and Logo! and the publishers of GEO/GEOlino to make the data freely available to other researchers and will make it available from http://www.icall-research.de in that case. 2 The GEOlino magazine is advertised as targeting children between 8 and 14 years (cf. http://www.geo.de/ magazine/geolino-magazin, accessed 11.06.18, 15:49). Logo! does not specify the age of its target audience, but has been reported to be particularly popular with c"
C18-1026,E14-1031,1,0.859315,"s, despite a series of promising results published around the turn of the decade (Vor der Br¨uck et al., 2008; Hancke et al., 2012). In particular, German readability research has suffered from the lack of a shared reference corpus and sufficiently comparable corpora for cross-corpus testing of readability models: While for English research, the Common Core corpus consisting of examples from the English Language Arts Standards of the Common Core State Standards, and the WeeklyReader corpus of online news articles have been widely used in studies on English readability and text simplification (Vajjala and Meurers, 2014; Petersen and Ostendorf, 2009; Feng et al., 2010), there are no comparable resources for German. This is particularly problematic, as over-fitting is a potential issue for classification algorithms, especially given the limited size of the typical data sets. To address these issues, we first present two new data sets for German readability assessment in Section 3: a set of German news broadcast subtitles based on the primary German TV news outlet Tagesschau and the children’s counterpart Logo!, and a GEO/GEOlino corpus crawled from the educational GEO magazine’s web site, a source first ident"
C18-1026,N18-1011,1,0.824591,"exity analysis pipeline into CTAP (Chen and Meurers, 2016) to make it generally available and will include an online documentation for each feature. 7 http://opennlp.apache.org 307 Splitter 3.4.0 for compound analysis.8 The Mate POS tags are further used to inform the Stanford PCFG parser 3.6.0 (Rafferty and Manning, 2008) and the Berkeley parser 1.7.0 (Petrov and Klein, 2007), which we use for constituency and topological field parsing. For all tools, we use the German default models that were provided with them, except for the Berkeley parser, for which we use the topological field model by Ziai and Meurers (2018). With these annotations, we extract all instances of the linguistic constructs that we need to calculate the final 400 complexity ratios.9 5 Study 1: Which complexity measures are informative? 5.1 Set-Up We first want to determine the informativeness of each measure for distinguishing between adult and child target audience. For this, we calculate the information gain of each measure on both data sets using 10-folds cross-validation for training and testing. We then compare across both data sets i) the number of features that are informative, and ii) the 20 most informative measures that show"
de-smedt-etal-2014-clara,Y12-1015,0,\N,Missing
de-smedt-etal-2014-clara,W11-2153,0,\N,Missing
de-smedt-etal-2014-clara,W12-3903,0,\N,Missing
de-smedt-etal-2014-clara,W11-2605,0,\N,Missing
de-smedt-etal-2014-clara,W13-1728,0,\N,Missing
de-smedt-etal-2014-clara,R11-1041,1,\N,Missing
de-smedt-etal-2014-clara,W11-4647,0,\N,Missing
de-smedt-etal-2014-clara,W13-2907,1,\N,Missing
de-smedt-etal-2014-clara,W11-4604,1,\N,Missing
de-smedt-etal-2014-clara,P13-1054,1,\N,Missing
de-smedt-etal-2014-clara,Y12-1014,0,\N,Missing
de-smedt-etal-2014-clara,P11-3013,0,\N,Missing
de-smedt-etal-2014-clara,ramasamy-zabokrtsky-2012-prague,0,\N,Missing
de-smedt-etal-2014-clara,W13-2805,0,\N,Missing
de-smedt-etal-2014-clara,larasati-2012-identic,0,\N,Missing
de-smedt-etal-2014-clara,alonso-etal-2012-voting,1,\N,Missing
de-smedt-etal-2014-clara,W12-3410,0,\N,Missing
de-smedt-etal-2014-clara,drobac-etal-2014-heuristic,1,\N,Missing
de-smedt-etal-2014-clara,P13-2127,1,\N,Missing
de-smedt-etal-2014-clara,W11-4406,0,\N,Missing
de-smedt-etal-2014-clara,dione-2014-pruning,0,\N,Missing
de-smedt-etal-2014-clara,R11-2019,0,\N,Missing
de-smedt-etal-2014-clara,W12-6304,0,\N,Missing
de-smedt-etal-2014-clara,W14-1203,1,\N,Missing
de-smedt-etal-2014-clara,W12-5017,0,\N,Missing
de-smedt-etal-2014-clara,lis-2012-polish,0,\N,Missing
de-smedt-etal-2014-clara,W14-0808,0,\N,Missing
de-smedt-etal-2014-clara,schumann-2012-knowledge,0,\N,Missing
de-smedt-etal-2014-clara,C12-1065,1,\N,Missing
de-smedt-etal-2014-clara,dione-2012-morphological,0,\N,Missing
de-smedt-etal-2014-clara,escartin-2012-design,0,\N,Missing
de-smedt-etal-2014-clara,W12-2019,1,\N,Missing
de-smedt-etal-2014-clara,lenkiewicz-etal-2012-avatech,1,\N,Missing
de-smedt-etal-2014-clara,W11-3302,1,\N,Missing
de-smedt-etal-2014-clara,escartin-2014-chasing,0,\N,Missing
de-smedt-etal-2014-clara,W12-0503,0,\N,Missing
de-smedt-etal-2014-clara,W13-5411,1,\N,Missing
de-smedt-etal-2014-clara,gebre-etal-2012-towards,1,\N,Missing
E03-1068,W99-0606,0,0.0129456,"Missing"
E03-1068,brants-2000-inter,0,0.0923122,"Missing"
E03-1068,C00-1046,0,0.045323,"Missing"
E03-1068,C94-1103,0,0.0288718,"tment of Linguistics The Ohio State University dickinso@ling.osu.edu Abstract We propose a new method for detecting errors in ""gold-standard"" part-ofspeech annotation. The approach locates errors with high precision based on n-grams occurring in the corpus with multiple taggings. Two further techniques, closed-class analysis and finitestate tagging guide patterns, are discussed. The success of the three approaches is illustrated for the Wall Street Journal corpus as part of the Penn Treebank. 1 Introduction Part-of-speech (pos) annotated reference corpora, such as the British National Corpus (Leech et al., 1994), the Penn Treebank (Marcus et al., 1993), or the German Negra Treebank (Skut et al., 1997) play an important role for current work in computational linguistics. They provide training material for research on tagging algorithms and they serve as a gold standard for evaluating the performance of such tools. High quality, pos-annotated text is also relevant as input for syntactic processing, for practical applications such as information extraction, and for linguistic research making use of pos-based corpus queries. The gold-standard pos-annotation for such large reference corpora is generally o"
E03-1068,J93-2004,0,0.0280505,"Missing"
E03-1068,C02-1131,0,0.0379374,"Missing"
E03-1068,W96-0213,0,0.283799,"d, is independent of the particular language and tagset of the corpus, and requires no additional language resources such as lexica. We showed that an instance of this method based on identity of words in the variation contexts, so-called variation n-grams, successfully detects a variety of errors in the WSJ corpus. The usefulness of the notion of a variation ngram relies on a particular word to appear several times in a corpus, with different annotations. It thus works best for large corpora and hand-annotated or hand-corrected corpora, or corpora involving other sources of inconsistency. As Ratnaparkhi (1996) points out, interannotator bias creates inconsistencies which a completely automatically-tagged corpus does not have. And Baker (1997) makes the point that a human posteditor also decreases the internal consistency of the tagged data since he will spot a mistake made by an automatic tagger for some but not all of its occurrences. As a result, our variation n-gram approach is well suited for the gold-standard annotations generally resulting from a combination of automatic annotation and manual post-editing. A case in point is that we recently applied the variation n-gram algorithm to the BNC-s"
E03-1068,A97-1014,0,0.0172525,"ew method for detecting errors in ""gold-standard"" part-ofspeech annotation. The approach locates errors with high precision based on n-grams occurring in the corpus with multiple taggings. Two further techniques, closed-class analysis and finitestate tagging guide patterns, are discussed. The success of the three approaches is illustrated for the Wall Street Journal corpus as part of the Penn Treebank. 1 Introduction Part-of-speech (pos) annotated reference corpora, such as the British National Corpus (Leech et al., 1994), the Penn Treebank (Marcus et al., 1993), or the German Negra Treebank (Skut et al., 1997) play an important role for current work in computational linguistics. They provide training material for research on tagging algorithms and they serve as a gold standard for evaluating the performance of such tools. High quality, pos-annotated text is also relevant as input for syntactic processing, for practical applications such as information extraction, and for linguistic research making use of pos-based corpus queries. The gold-standard pos-annotation for such large reference corpora is generally obtained using an automatic tagger to produce a first annotation, followed by human post-edi"
E03-1068,E95-1029,0,0.1335,"Missing"
E03-1068,W02-1015,0,\N,Missing
E03-1068,W00-1907,0,\N,Missing
E14-1031,W10-1001,0,0.0118575,"ceived only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sentences by their reading level (Napoles and Dredze, 2010) and identifying sentential transformations needed for text simplification using text complexity features (Medero and Ostendorf, 2011). Some simplification approaches for non-English languages (Aluisio et al., 2010; ˇ Gasperin et al., 2009; Stajner et al., 2013) also touch on the use of readability assessment. In the present paper, we focus on the neglected connection between readability analysis and simplification. We show through a cross-corpus evaluation that a document level, regression-based readability model successfully identifies the differences between simplified vs. unsimplified sentences. This approach can be useful in various stages of simplification ranging from identifying simplification targets to the evaluation of simplification outcomes. 2 2.1 Wiki-SimpleWiki Sentence Aligned Corpus: Th"
E14-1031,I11-1053,0,0.0130211,"ix B of the Common Core Standards reading initiative of the U.S. education system (CCSSO, 2010). They are annotated by experts with grade bands that cover the grades 1 to 12. These texts serve as exemplars for the level of reading ability at a given grade level. This corpus was introduced as an evaluation corpus for readability models in the recent past (Sheehan et al., 2010; Nelson et al., 2012; Flor et al., 2013), so we used it to compare our model with other systems. texts, data-driven approaches, partly inspired by statistical machine translation, appeared (Specia, 2010; Zhu et al., 2010; Bach et al., 2011; Coster and Kauchak, 2011; Woodsend and Lapata, 2011). While simplification methods have evolved, understanding which parts of a text need to be simplified and methods for evaluating the simplified text so far received only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sentences by their reading level (Napole"
E14-1031,P13-3021,0,0.0142927,"ystems. texts, data-driven approaches, partly inspired by statistical machine translation, appeared (Specia, 2010; Zhu et al., 2010; Bach et al., 2011; Coster and Kauchak, 2011; Woodsend and Lapata, 2011). While simplification methods have evolved, understanding which parts of a text need to be simplified and methods for evaluating the simplified text so far received only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sentences by their reading level (Napoles and Dredze, 2010) and identifying sentential transformations needed for text simplification using text complexity features (Medero and Ostendorf, 2011). Some simplification approaches for non-English languages (Aluisio et al., 2010; ˇ Gasperin et al., 2009; Stajner et al., 2013) also touch on the use of readability assessment. In the present paper, we focus on the neglected connection between readability analysis and simplification. We show through a cross-corpus eval"
E14-1031,C96-2183,0,0.124897,"sic readers (Canning and Tait, 1999) to supporting the extraction of protein-protein interactions in the biomedical domain (Jonnalagadda and Gonzalez, 2009). A related field of research is automatic readability assessment, which can be useful for evaluating text simplification. It can also be relevant for intermediate simplification steps, such as the identification of target sentences for simplification. Yet, 1.1 Related Work Research into automatic text simplification essentially started with the idea of splitting long sentences into multiple shorter sentences to improve parsing efficiency (Chandrasekar et al., 1996; Chandrasekar and Srinivas, 1996). This was followed by rule-based approaches targeting human and machine uses (Carroll et al., 1999; Siddharthan, 2002, 2004). With the availability of a sentence-aligned corpus based on Wikipedia and SimpleWikipedia 288 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 288–297, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics the Appendix B of the Common Core Standards reading initiative of the U.S. education system (CCSSO, 2010). They are annotated by exper"
E14-1031,levy-andrew-2006-tregex,0,0.0127926,"clause, t-unit), measures of coordination and subordination (e.g., # of coordinate clauses per clause), the presence of particular syntactic structures (e.g., VPs per t-unit), the number of phrases of various categories (e.g., NP, VP, PP), the average lengths Common Core Standards Corpus: This corpus consists of 168 English texts available from 289 of phrases, the parse tree height, and the number of constituents per subtree. None of the syntactic features refer to specific words or lemmas. We used the BerkeleyParser (Petrov and Klein, 2007) for generating the parse trees and the Tregex tool (Levy and Andrew, 2006) to count the occurrences of the syntactic patterns. While the first two feature sets are based on our previous work, as far as we know the next two are used in readability assessment for the first time. with 26 linguistic and psychological attributes of about 1.5 million words.3 We used the measures of word familiarity, concreteness, imageability, meaningfulness, and age of acquisition from this database as our features, by encoding their average values per text. Kuperman et al. (2012) compiled a freely available database that includes Age of Acquisition (AoA) ratings for over 50,000 English"
E14-1031,P11-2117,0,0.0196888,"Core Standards reading initiative of the U.S. education system (CCSSO, 2010). They are annotated by experts with grade bands that cover the grades 1 to 12. These texts serve as exemplars for the level of reading ability at a given grade level. This corpus was introduced as an evaluation corpus for readability models in the recent past (Sheehan et al., 2010; Nelson et al., 2012; Flor et al., 2013), so we used it to compare our model with other systems. texts, data-driven approaches, partly inspired by statistical machine translation, appeared (Specia, 2010; Zhu et al., 2010; Bach et al., 2011; Coster and Kauchak, 2011; Woodsend and Lapata, 2011). While simplification methods have evolved, understanding which parts of a text need to be simplified and methods for evaluating the simplified text so far received only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sentences by their reading level (Napoles and Dredze, 2010) and id"
E14-1031,W10-0406,0,0.0267507,", 2011; Coster and Kauchak, 2011; Woodsend and Lapata, 2011). While simplification methods have evolved, understanding which parts of a text need to be simplified and methods for evaluating the simplified text so far received only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sentences by their reading level (Napoles and Dredze, 2010) and identifying sentential transformations needed for text simplification using text complexity features (Medero and Ostendorf, 2011). Some simplification approaches for non-English languages (Aluisio et al., 2010; ˇ Gasperin et al., 2009; Stajner et al., 2013) also touch on the use of readability assessment. In the present paper, we focus on the neglected connection between readability analysis and simplification. We show through a cross-corpus evaluation that a document level, regression-based readability model successfully identifies the differences between simplified vs. unsimplified sent"
E14-1031,W13-5608,0,0.119936,"Missing"
E14-1031,W13-1504,0,0.0485466,"th Conference of the European Chapter of the Association for Computational Linguistics, pages 288–297, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics the Appendix B of the Common Core Standards reading initiative of the U.S. education system (CCSSO, 2010). They are annotated by experts with grade bands that cover the grades 1 to 12. These texts serve as exemplars for the level of reading ability at a given grade level. This corpus was introduced as an evaluation corpus for readability models in the recent past (Sheehan et al., 2010; Nelson et al., 2012; Flor et al., 2013), so we used it to compare our model with other systems. texts, data-driven approaches, partly inspired by statistical machine translation, appeared (Specia, 2010; Zhu et al., 2010; Bach et al., 2011; Coster and Kauchak, 2011; Woodsend and Lapata, 2011). While simplification methods have evolved, understanding which parts of a text need to be simplified and methods for evaluating the simplified text so far received only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplif"
E14-1031,N07-1051,0,0.0158009,"ala and Meurers (2012): mean lengths of various production units (sentence, clause, t-unit), measures of coordination and subordination (e.g., # of coordinate clauses per clause), the presence of particular syntactic structures (e.g., VPs per t-unit), the number of phrases of various categories (e.g., NP, VP, PP), the average lengths Common Core Standards Corpus: This corpus consists of 168 English texts available from 289 of phrases, the parse tree height, and the number of constituents per subtree. None of the syntactic features refer to specific words or lemmas. We used the BerkeleyParser (Petrov and Klein, 2007) for generating the parse trees and the Tregex tool (Levy and Andrew, 2006) to count the occurrences of the syntactic patterns. While the first two feature sets are based on our previous work, as far as we know the next two are used in readability assessment for the first time. with 26 linguistic and psychological attributes of about 1.5 million words.3 We used the measures of word familiarity, concreteness, imageability, meaningfulness, and age of acquisition from this database as our features, by encoding their average values per text. Kuperman et al. (2012) compiled a freely available datab"
E14-1031,C12-1065,1,0.908654,"Missing"
E14-1031,W13-5634,0,0.029654,"approaches, partly inspired by statistical machine translation, appeared (Specia, 2010; Zhu et al., 2010; Bach et al., 2011; Coster and Kauchak, 2011; Woodsend and Lapata, 2011). While simplification methods have evolved, understanding which parts of a text need to be simplified and methods for evaluating the simplified text so far received only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sentences by their reading level (Napoles and Dredze, 2010) and identifying sentential transformations needed for text simplification using text complexity features (Medero and Ostendorf, 2011). Some simplification approaches for non-English languages (Aluisio et al., 2010; ˇ Gasperin et al., 2009; Stajner et al., 2013) also touch on the use of readability assessment. In the present paper, we focus on the neglected connection between readability analysis and simplification. We show through a cross-corpus evaluation that a document"
E14-1031,N03-1033,0,0.0144545,"morphological and psycholinguistic properties of words. The features can be broadly classified into four groups. Lexical richness and POS features: We adapted the lexical features from Vajjala and Meurers (2012). This includes measures of lexical richness from Second Language Acquisition (SLA) research and measures of lexical variation (noun, verb, adjective, adverb and modifier variation). In addition, this feature set also includes part-of-speech densities (e.g., the average # of nouns per sentence). The information needed to calculate these features was extracted using the Stanford Tagger (Toutanova et al., 2003). None of the lexical richness and POS features we used refer to specific words or lemmas. Corpora and Features Corpora We built and tested our document and sentence level readability models using three publicly available text corpora with reading level annotations. WeeBit Corpus: The WeeBit corpus (Vajjala and Meurers, 2012) consists of 3,125 articles belonging to five reading levels, with 625 articles per reading level. The texts compiled from the WeeklyReader and BBC Bitesize target English language learners from 7 to 16 years of age. We used this corpus to build our primary readability mod"
E14-1031,W12-2019,1,0.943599,"sentences. This approach can be useful in various stages of simplification ranging from identifying simplification targets to the evaluation of simplification outcomes. 2 2.1 Wiki-SimpleWiki Sentence Aligned Corpus: This corpus was created by Zhu et al. (2010) and consists of ∼100k aligned sentence pairs drawn from Wikipedia and Simple English Wikipedia. We removed all pairs of identical sentences, i.e., where the Wiki and the SimpleWiki versions are the same. We used this corpus to study reading level assessment at the sentence level. 2.2 Features We started with the feature set described in Vajjala and Meurers (2012) and added new features focusing on the morphological and psycholinguistic properties of words. The features can be broadly classified into four groups. Lexical richness and POS features: We adapted the lexical features from Vajjala and Meurers (2012). This includes measures of lexical richness from Second Language Acquisition (SLA) research and measures of lexical variation (noun, verb, adjective, adverb and modifier variation). In addition, this feature set also includes part-of-speech densities (e.g., the average # of nouns per sentence). The information needed to calculate these features w"
E14-1031,W13-2907,1,0.774525,"ssment. 3 Document-Level Readability Model In our first experiment, we tested the documentlevel readability model based on the 151 features using the WeeBit corpus. Under a regression perspective on readability, we evaluated the approach using Pearson Correlation and Root Mean Square Error (RMSE) in a 10-fold cross-validation setting. We used the SMO Regression implementation from WEKA (Hall et al., 2009) and achieved a Pearson correlation of 0.92 and an RMSE of 0.53. The document-level performance of our 151 feature model is virtually identical to that of the regression model we presented in Vajjala and Meurers (2013). But compared to our previous work, the Celex and psycholinguistic features we included here provide more lexical information that is meaningful to compute even for the sentencelevel analysis we turn to in the next section. To be able to compare our document-level results with other contemporary readability approaches, we need a common test corpus. Nelson et al. (2012) compared several state of the art readability assessment systems using five test sets and showed that the systems that went beyond traditional formulae and wordlists performed better Psycholinguistic features: The MRC Psycholin"
E14-1031,D11-1038,0,0.0274086,"tiative of the U.S. education system (CCSSO, 2010). They are annotated by experts with grade bands that cover the grades 1 to 12. These texts serve as exemplars for the level of reading ability at a given grade level. This corpus was introduced as an evaluation corpus for readability models in the recent past (Sheehan et al., 2010; Nelson et al., 2012; Flor et al., 2013), so we used it to compare our model with other systems. texts, data-driven approaches, partly inspired by statistical machine translation, appeared (Specia, 2010; Zhu et al., 2010; Bach et al., 2011; Coster and Kauchak, 2011; Woodsend and Lapata, 2011). While simplification methods have evolved, understanding which parts of a text need to be simplified and methods for evaluating the simplified text so far received only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sentences by their reading level (Napoles and Dredze, 2010) and identifying sentential transfo"
E14-1031,P12-1107,0,0.0505827,"Missing"
E14-1031,C10-1152,0,0.367203,"uistics the Appendix B of the Common Core Standards reading initiative of the U.S. education system (CCSSO, 2010). They are annotated by experts with grade bands that cover the grades 1 to 12. These texts serve as exemplars for the level of reading ability at a given grade level. This corpus was introduced as an evaluation corpus for readability models in the recent past (Sheehan et al., 2010; Nelson et al., 2012; Flor et al., 2013), so we used it to compare our model with other systems. texts, data-driven approaches, partly inspired by statistical machine translation, appeared (Specia, 2010; Zhu et al., 2010; Bach et al., 2011; Coster and Kauchak, 2011; Woodsend and Lapata, 2011). While simplification methods have evolved, understanding which parts of a text need to be simplified and methods for evaluating the simplified text so far received only little attention. The use of readability assessment for simplification has mostly been restricted to using traditional readability formulae for evaluating or generating simplified text (Zhu et al., 2010; Wubben et al., 2012; Klerke and Søgaard, 2013; Stymne et al., 2013). Some recent work briefly addresses issues such as classifying sentences by their re"
E14-1031,E99-1042,0,\N,Missing
J97-4003,W96-0303,0,0.0612124,"te encoding the finite-state automaton of Figure 16 is shown in Figure 18. 28 We now have a first complete encoding of the lexical rules and their interaction represented as covariation in lexical entries. The encoding consists of three types of definite clause predicates: 1. Lexical rule predicates representing the lexical rules; 2. Frame predicates specifying the frame for the lexical rule predicates; and 3. Interaction predicates encoding lexical rule interaction for the natural classes of lexical entries in the lexicon. The way these predicates interconnect is represented in Figure 19. 27 Briscoe and Copestake (1996) argue that semi-productivity of lexical rules, which can be understood as a generalization of exceptions to lexical rules, can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry. 28 In order to distinguish the different interaction predicates for the different classes of lexical entries, the compiler indexes the names of the interaction predicates. Since for expository reasons we will only discuss one kind of lexical entry in this paper, we will not show those indices in the examples given. 557 Computational Linguistics Volum"
J97-4003,J91-3003,0,0.0852205,"s belonging to the same word class. 33 The lexicon of the test grammar can be expanded out off-line since the recursive Complement Extraction Lexical Rule applies only to full verbs, i.e, lexical entries with a complement list of finite length. As a result, the grammar does not have an infinite lexicon. 563 Computational Linguistics Volume 23, Number 4 This means that the more lexical entries in a word class, the greater the saving in space. The covariation approach therefore is particularly attractive for grammars with a large lexicon. 7. Related Work The powerful mechanism of lexical rules (Carpenter 1991) has been used in many natural language processing systems. In this section we briefly discuss some of the more prominent approaches and compare them with the treatment proposed in this paper. 7.1 Off-line Expansion of Lexical Rules A common computational treatment of lexical rules adopted, for example, in the ALE system (Carpenter and Penn 1994) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time. While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations capture"
J97-4003,C90-3052,0,0.0270249,"e covariation encoding. 7.2 Lexical Rules as Unary Phrase Structure Rules Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31). A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990; Emele 1994). The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules. The encoding of lexical rules used in the covariation approach is related to the work of van Noord and Bouma (1994), who describe the hand-encoding of a single lexical rule as definite relations and show how these relations can be used to constrain a lexical entry. The covariation approach builds on this proposal and extends it in three ways: First, the approach shows how to detect and encode the interaction of a set of lexical rules. Second, it pr"
J97-4003,P85-1032,0,0.0478031,"Missing"
J97-4003,C94-2154,0,0.101851,"al entries serving as input to a lexical rule can be enforced that cannot be executed on the basis of the information present in the lexical entry alone, 6 and second, grammars including lexical rules that, under the MLR formalization, result in an infinite lexicon, can only 3 The terminology used in the literature varies. Types are also referred to as sorts, appropriateness conditions as feature declarations, and features as attributes. To avoid confusion, we will only use the terminology introduced in the text. 4 This interpretation of the signature is sometimes referred to as closed world (Gerdemann and King 1994; Gerdemann 1995). 5 An in-depth discussion including a comparison of both approaches is provided in Calcagno, Meurers, and Pollard (in preparation). 6 The Partial-VP Topicalization Lexical Rule proposed by Hinrichs and Nakazawa (1994, 10) is a linguistic example. The in-specification of this lexical rule makes use of an append relation to constrain the valence attribute of the auxiliaries serving as its input. In the lexicon, however, the complements of an auxiliary are uninstantiated because it raises the arguments of its verbal complement. 545 Computational Linguistics simple-word ---* LE1"
J97-4003,P95-1012,0,0.0325262,"Missing"
J97-4003,W97-1506,0,0.363371,"Missing"
J97-4003,P97-1001,1,0.889986,"Missing"
J97-4003,C96-1076,0,0.0113327,"and 2 of Figure 11 are given. We then only obtain seven of the clauses of Figure 22: those calling lex_rule_l or lex_rule_2, as well as the unit clauses for q_l, q_2, q3, and q_7. Applying constraint propagation to the extended lexical entry of Figure 17 yields the result shown in Figure 23. The information c o m m o n to all solutions to the interaction call is lifted up into the lexical entry and becomes available upon lexical lookup. 32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints (Maxwell and Kaplan 1989; Eisele and D6rre 1990; Griffith 1996) can be used to circumvent constraint propagation. Encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup. For analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficientprocessing. 561 Computational Linguistics Lct2[zB]j Volume 23, Number 4 - 1] c x_ t2Lz[](a'b)JJ Figure 23 An entry suitable for on-the-fly application (lexical rules 1 and 2 o"
J97-4003,C96-1092,0,0.0188819,"nslation of the lexical rule into a predicate is trivial. The result is displayed description language. 12 In order to focus on the computational aspects of the covariation approach, in this paper we will not go into a discussion of the full lexical rule specification language introduced in Meurers (1995). The reader interested in that language and its precise interpretation can find the relevant details in that paper. 13 A more detailed presentation can be found in Minnen (in preparation). 14 We use rather abstract lexical rules in the examples to be able to focus on the relevant aspects. 15 Hinrichs and Nakazawa (1996) show that the question of whether the application criterion of lexical rules should be a subsumption or a unification test is an important question deserving of more attention. We here assume unification as the application criterion, which formally corresponds to the conjunction of descriptions and their conversion to normal form (G6tz 1994). Computationally, a subsumption test could equally well be used in our compiler. 548 Meurers and Minnen input: ~ x i c o ~ Covariation Approach to HPSG Lexical Rules + I of translation lexical rules into definite relations ~ ~ ~ ~ ~ f ~~me determination o"
J97-4003,P92-1011,0,0.0526463,"Missing"
J97-4003,W89-0203,0,0.0244233,"s therefore assume that only the lexical rules 1 and 2 of Figure 11 are given. We then only obtain seven of the clauses of Figure 22: those calling lex_rule_l or lex_rule_2, as well as the unit clauses for q_l, q_2, q3, and q_7. Applying constraint propagation to the extended lexical entry of Figure 17 yields the result shown in Figure 23. The information c o m m o n to all solutions to the interaction call is lifted up into the lexical entry and becomes available upon lexical lookup. 32 In certain cases an extension of the constraint language with named disjunctions or contexted constraints (Maxwell and Kaplan 1989; Eisele and D6rre 1990; Griffith 1996) can be used to circumvent constraint propagation. Encoding the disjunctive possibilities for lexical rule application in this way, instead of with definite clause attachments, makes all relevant lexical information available at lexical lookup. For analyses proposing infinite lexica, though, a definite clause encoding of disjunctive possibilities is still necessary and constraint propagation is indispensable for efficientprocessing. 561 Computational Linguistics Lct2[zB]j Volume 23, Number 4 - 1] c x_ t2Lz[](a'b)JJ Figure 23 An entry suitable for on-the-f"
J97-4003,E95-1024,1,0.890971,"Missing"
J97-4003,C94-2131,0,0.0150888,"d their interaction. Finally, it automatically derives the frame specification for lexical rules such that, following standard HPSG practice, only the information changed in a lexical rule needs to be specified. 7.3 Alternative Ways to Express Lexical Generalizations Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information. In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; 564 Meurers and Minnen Covariation Approach to HPSG Lexical Rules Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995). The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy. These approaches seem to propose a completely different w a y to capture lexical generalizations. It is therefore interesting that the covariation lexical rule compiler produces a lexicon encoding that, basically, uses an underspecification representation: The resulting definite clause representation after constraint propagation represents the common information in the base lexical entry"
J97-4003,C94-1039,0,0.0452965,"Missing"
L16-1621,W08-0913,1,0.789035,"man. Keywords: information structure, focus, crowd sourcing, learner corpora, short-answer assessment 1. Introduction The information structure of a sentence is receiving significant interest in linguistics as the attention has shifted from individual sentences to the question how the information is packaged in sentences analyzed in context. Complementing the theoretical interest, identifying information structural concepts has also been shown to be relevant in practical computational linguistic tasks such as Short Answer Assessment: While some approaches have integrated aspects of givenness (Bailey and Meurers, 2008; Mohler et al., 2011), more recent work (Meurers et al., 2011; Hahn and Meurers, 2012) has argued for relying on focus as discussed in formal pragmatics (e.g., Krifka, 2007, p. 18). In this paper, we present a comprehensive focus annotation study based on the tasked-based corpus CREG (Ott et al., 2012), consisting of answers to explicitly given reading comprehension questions. We compare focus annotation by trained annotators with a crowd-sourcing setup making use of untrained native speakers. As a result of these annotation efforts, with this paper we provide both a substantial new corpus re"
L16-1621,W10-0713,0,0.0306269,"oach by reporting both substantial inter-annotator agreement and a substantial extrinsic improvement in automatically evaluating the meaning of answers if focus/background information is integrated into the system. Since manual focus annotation by experts is very timeconsuming for large data sets, both for annotator training and the annotation itself, a second component of our work on annotating authentic data explores the use of crowdsourcing for focus annotation. Crowd-sourcing as a way of collecting linguistically annotated data has been shown to work well for a number of tasks (cf., e.g., Finin et al., 2010; Tetreault et al., 2010; Zaidan and Callison-Burch, 2011)). We investigate how systematically the untrained crowd can identify a meaning-based linguistic notion like focus in authentic data and which characteristics of the data and context lead to consistent annotation results. 2. Data We base our work on the CREG corpus (Ott et al., 2012), a task-based corpus consisting of answers to reading comprehension questions written by American learners of German at the university level. The overall corpus includes 164 reading texts, 1,517 reading comprehension questions, 2,057 target answers provided"
L16-1621,W12-2039,1,0.839123,"er assessment 1. Introduction The information structure of a sentence is receiving significant interest in linguistics as the attention has shifted from individual sentences to the question how the information is packaged in sentences analyzed in context. Complementing the theoretical interest, identifying information structural concepts has also been shown to be relevant in practical computational linguistic tasks such as Short Answer Assessment: While some approaches have integrated aspects of givenness (Bailey and Meurers, 2008; Mohler et al., 2011), more recent work (Meurers et al., 2011; Hahn and Meurers, 2012) has argued for relying on focus as discussed in formal pragmatics (e.g., Krifka, 2007, p. 18). In this paper, we present a comprehensive focus annotation study based on the tasked-based corpus CREG (Ott et al., 2012), consisting of answers to explicitly given reading comprehension questions. We compare focus annotation by trained annotators with a crowd-sourcing setup making use of untrained native speakers. As a result of these annotation efforts, with this paper we provide both a substantial new corpus resource with gold standard focus annotation and conceptual insights into the nature of f"
L16-1621,W11-2401,1,0.935254,"er corpora, short-answer assessment 1. Introduction The information structure of a sentence is receiving significant interest in linguistics as the attention has shifted from individual sentences to the question how the information is packaged in sentences analyzed in context. Complementing the theoretical interest, identifying information structural concepts has also been shown to be relevant in practical computational linguistic tasks such as Short Answer Assessment: While some approaches have integrated aspects of givenness (Bailey and Meurers, 2008; Mohler et al., 2011), more recent work (Meurers et al., 2011; Hahn and Meurers, 2012) has argued for relying on focus as discussed in formal pragmatics (e.g., Krifka, 2007, p. 18). In this paper, we present a comprehensive focus annotation study based on the tasked-based corpus CREG (Ott et al., 2012), consisting of answers to explicitly given reading comprehension questions. We compare focus annotation by trained annotators with a crowd-sourcing setup making use of untrained native speakers. As a result of these annotation efforts, with this paper we provide both a substantial new corpus resource with gold standard focus annotation and conceptual insi"
L16-1621,P11-1076,0,0.0318387,"structure, focus, crowd sourcing, learner corpora, short-answer assessment 1. Introduction The information structure of a sentence is receiving significant interest in linguistics as the attention has shifted from individual sentences to the question how the information is packaged in sentences analyzed in context. Complementing the theoretical interest, identifying information structural concepts has also been shown to be relevant in practical computational linguistic tasks such as Short Answer Assessment: While some approaches have integrated aspects of givenness (Bailey and Meurers, 2008; Mohler et al., 2011), more recent work (Meurers et al., 2011; Hahn and Meurers, 2012) has argued for relying on focus as discussed in formal pragmatics (e.g., Krifka, 2007, p. 18). In this paper, we present a comprehensive focus annotation study based on the tasked-based corpus CREG (Ott et al., 2012), consisting of answers to explicitly given reading comprehension questions. We compare focus annotation by trained annotators with a crowd-sourcing setup making use of untrained native speakers. As a result of these annotation efforts, with this paper we provide both a substantial new corpus resource with gold stand"
L16-1621,C14-1145,0,0.0387083,"Missing"
L16-1621,ritz-etal-2008-annotation,0,0.43567,"Missing"
L16-1621,D08-1027,0,0.0453383,"Missing"
L16-1621,W10-1006,0,0.0240185,"th substantial inter-annotator agreement and a substantial extrinsic improvement in automatically evaluating the meaning of answers if focus/background information is integrated into the system. Since manual focus annotation by experts is very timeconsuming for large data sets, both for annotator training and the annotation itself, a second component of our work on annotating authentic data explores the use of crowdsourcing for focus annotation. Crowd-sourcing as a way of collecting linguistically annotated data has been shown to work well for a number of tasks (cf., e.g., Finin et al., 2010; Tetreault et al., 2010; Zaidan and Callison-Burch, 2011)). We investigate how systematically the untrained crowd can identify a meaning-based linguistic notion like focus in authentic data and which characteristics of the data and context lead to consistent annotation results. 2. Data We base our work on the CREG corpus (Ott et al., 2012), a task-based corpus consisting of answers to reading comprehension questions written by American learners of German at the university level. The overall corpus includes 164 reading texts, 1,517 reading comprehension questions, 2,057 target answers provided by the teachers, and 36"
L16-1621,P11-1122,0,0.0125417,"otator agreement and a substantial extrinsic improvement in automatically evaluating the meaning of answers if focus/background information is integrated into the system. Since manual focus annotation by experts is very timeconsuming for large data sets, both for annotator training and the annotation itself, a second component of our work on annotating authentic data explores the use of crowdsourcing for focus annotation. Crowd-sourcing as a way of collecting linguistically annotated data has been shown to work well for a number of tasks (cf., e.g., Finin et al., 2010; Tetreault et al., 2010; Zaidan and Callison-Burch, 2011)). We investigate how systematically the untrained crowd can identify a meaning-based linguistic notion like focus in authentic data and which characteristics of the data and context lead to consistent annotation results. 2. Data We base our work on the CREG corpus (Ott et al., 2012), a task-based corpus consisting of answers to reading comprehension questions written by American learners of German at the university level. The overall corpus includes 164 reading texts, 1,517 reading comprehension questions, 2,057 target answers provided by the teachers, and 36,335 learner answers. The CREG-5K"
L16-1621,W14-4922,1,0.568846,"(e.g., Ritz et al., 2008; Calhoun et al., 2010) . These approaches generally were only rewarded with limited success, as they have tried to identify focus in newspaper text or other data types where no explicit questions are available, making the task of determining the Question under Discussion (QUD, Roberts, 2012), and thus reliably annotating focus, particularly difficult. Yet, many of the natural tasks and authentic data in which focus annotation would be relevant actually do contain explicit task and context information of relevance to determining focus. Building on the work presented in Ziai and Meurers (2014), we show that reliable focus annotation in authentic data is feasible, even for somewhat ill-formed learner language, if one has access to explicit questions and explicitly takes them into account in an incremental annotation scheme. We demonstrate the effectiveness of the approach by reporting both substantial inter-annotator agreement and a substantial extrinsic improvement in automatically evaluating the meaning of answers if focus/background information is integrated into the system. Since manual focus annotation by experts is very timeconsuming for large data sets, both for annotator tra"
N18-1011,W12-1632,0,0.029923,"Missing"
N18-1011,S13-2045,0,0.0868919,"fferent settings involving the standard CoMiC system and a focusaugmented variant: i) using standard CoMiC with the givenness filter by itself as a baseline, and ii) augmenting standard CoMiC by additionally producing a focus version of each classification feature in Table 3. In each case, we used WEKA’s knearest-neighbor implementation for CoMiC, following positive results by Rudzewitz (2016). We use two test sets randomly selected from the CREG-5K data set (Ziai et al., 2016), one based on an ‘unseen answers‘ and one based on an ‘unseen questions‘ test scenario, based on the methodology of (Dzikovska et al., 2013): in ‘unseen answers’, the test set can contain answers to the same questions already part of the training set (but not the answers themselves), whereas in ‘unseen questions’ both questions and answers are new in the test set. In order to arrive at a fair and generalizable testing setup, we removed all answers from the CREG-5K training set that also occur in the CREG-ExpertFocus set used to train our focus detection classifier. This ensures that neither the focus classifier nor CoMiC have seen any of the test set answers before. The resulting smaller training set contains 1606 student answers,"
N18-1011,W05-0307,0,0.135472,"Missing"
N18-1011,foth-etal-2014-size,0,0.0362675,"Missing"
N18-1011,P09-1008,0,0.0182803,"hile discontinuous focus is possible, focus as operationalized in the scheme by Ziai and Meurers (2014) most often marks an adjacent group of words, a tendency that our word-based classifier did not always follow, as exemplified by the cases in Figures 1 and 2. Such groups very often correspond to a syntactic phrase, so constituent membership is likely indicative in predicting the focus status of an individual word. Similarly, the topological field (H¨ohle, 1986) identifying the major section of a sentence in relation to the clausal main verb is potentially relevant for a word’s focus status. Cheung and Penn (2009) present a parsing model that demonstrates good performance in determining both topological fields and phrase structure for German. The model is trained on the T¨uBa-D/Z treebank (Telljohann et al., 2004), whose rich syntactic model encodes topological fields as nodes in the syntax tree itself. Following Cheung and Penn (2009), we trained an updated version of their model using the current version of the Berkeley Parser (Petrov and Klein, 2007) and release 10 of the T¨uBa-D/Z.6 Based on the new parsing model, we integrated two new features into our focus detection model: Constituency-based Fea"
N18-1011,W97-0802,0,0.0611928,"Missing"
N18-1011,L18-1304,0,0.0446457,"Missing"
N18-1011,L16-1621,1,0.902535,"Missing"
N18-1011,E06-1050,0,0.0153331,"Missing"
N18-1011,riester-etal-2010-recursive,0,0.0441285,"Missing"
N18-1011,C02-1150,0,0.0399975,"on. We therefore conceptualized the task of automatic focus detection on a per-word level: for each word in an answer, as identified by the OpenNLP tokenizer and sentence segmenter2 , the classifier needs to decide whether it is an instance of focus or background. Besides the choice of Since our approach also makes use of question properties, it is also worth mentioning that there are a number of approaches on Answer Typing as a step in Question Answering (QA) approaches in order to constrain the search space of possible candidate answers and improve accuracy. While earlier approaches such as Li and Roth (2002) used a fixed set of answer types for classifying factoid 2 119 http://opennlp.apache.org matized form of the word as determined by TreeTagger (Schmid, 1994). classification algorithm, the crucial question naturally is the choice of linguistic features, which we turn to next. 4.1 Positional properties Where a word occurs in the answer or the question can be relevant for its information structural status. It has been observed since Halliday (1967) that given material tends to occur earlier in sentences (here: answers), while new or focused content tends to occur later. We encode this observatio"
N18-1011,W11-2401,1,0.920393,"milarity Match Setup It has been pointed out that evaluating the annotation of a theoretical linguistic notion only intrinsically is problematic because there is no nontheoretical grounding involved (Riezler, 2014). Therefore, besides a comparison to the gold standard, we also evaluated the resulting annotation in a larger computational task, the automatic meaning assessment of short answers to reading comprehension questions. Here the goal is to decide, given a question (Q) and a correct target answer (TA), whether the student answer (SA) actually answers the question or not. An example from Meurers et al. (2011) is shown in Figure 4. We used the freely available CoMiC system (Comparing Meaning in Context, Meurers et al. 2011) as a testbed for our experiment. CoMiC is an alignment-based system operating in three stages: 10. Type Match 11. Lemma Match 12. Synonym Match 13. Variety of Match (0-5) Description Percent of dependency heads aligned (relative to target) Percent of aligned target/student tokens Percent of aligned target/student chunks (as identified by OpenNLP3 ) Percent of aligned target/student dependency triples Percent of token alignments that were token-identical Percent of token alignmen"
N18-1011,J14-1009,0,0.0174179,"alignments (see Table 3), using a supervised machine learning setup Feature 1. Keyword Overlap Extrinsic Evaluation 2./3. Token Overlap 4./5. Chunk Overlap Complementing the intrinsic evaluation above, in this section we demonstrate how focus can be successfully used to improve performance in an authentic CL task, namely Short Answer Assessment (SAA). 8.1 6./7. Triple Overlap 8. Token Match 9. Similarity Match Setup It has been pointed out that evaluating the annotation of a theoretical linguistic notion only intrinsically is problematic because there is no nontheoretical grounding involved (Riezler, 2014). Therefore, besides a comparison to the gold standard, we also evaluated the resulting annotation in a larger computational task, the automatic meaning assessment of short answers to reading comprehension questions. Here the goal is to decide, given a question (Q) and a correct target answer (TA), whether the student answer (SA) actually answers the question or not. An example from Meurers et al. (2011) is shown in Figure 4. We used the freely available CoMiC system (Comparing Meaning in Context, Meurers et al. 2011) as a testbed for our experiment. CoMiC is an alignment-based system operatin"
N18-1011,ritz-etal-2008-annotation,0,0.0688111,"Missing"
N18-1011,nissim-etal-2004-annotation,0,0.0957259,"Missing"
N18-1011,W16-0527,0,0.0167235,"pothesis is that the alignment of focused elements in answers adds information about the quality of the answer with respect to the question, leading to a higher answer classification accuracy. We experimented with two different settings involving the standard CoMiC system and a focusaugmented variant: i) using standard CoMiC with the givenness filter by itself as a baseline, and ii) augmenting standard CoMiC by additionally producing a focus version of each classification feature in Table 3. In each case, we used WEKA’s knearest-neighbor implementation for CoMiC, following positive results by Rudzewitz (2016). We use two test sets randomly selected from the CREG-5K data set (Ziai et al., 2016), one based on an ‘unseen answers‘ and one based on an ‘unseen questions‘ test scenario, based on the methodology of (Dzikovska et al., 2013): in ‘unseen answers’, the test set can contain answers to the same questions already part of the training set (but not the answers themselves), whereas in ‘unseen questions’ both questions and answers are new in the test set. In order to arrive at a fair and generalizable testing setup, we removed all answers from the CREG-5K training set that also occur in the CREG-Exp"
N18-1011,telljohann-etal-2004-tuba,0,0.402871,"Missing"
N18-1011,S16-2026,1,0.860682,"Missing"
N18-1011,W14-4922,1,0.927736,"roaches to discourse and information structure has been developed (Kruijff-Korbayov´a and Steedman, 2003). Among these perspectives, the Focus-Background dichotomy provides a particularly valuable structuring of the information in a sentence in relation to the discourse. (1) is an example question-answer pair from Krifka and Musan (2012, p. 4) where the focus in the answer is marked by brackets. 117 Proceedings of NAACL-HLT 2018, pages 117–128 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics about the extent or size of the focused unit. More recently, Ziai and Meurers (2014) showed that for data collected in task contexts including explicit questions, such as answers to reading comprehension questions, reliable focus annotation is possible. In addition, an option for externally validating focus annotation was established by showing that such focus annotation improves the performance of Short Answer Assessment (SAA) systems. Focus enables the system to zoom in on the part of the answer addressing the question instead of considering all parts of the answer as equal. In this paper, we want to build on this strand of research and develop an approach for automatically"
P05-1040,E03-1068,1,0.932893,"pressions (e.g., Rayson et al., 2004), as well as for spoken language corpora or corpora with multiple layers of annotation which cross boundaries (e.g., Blache and Hirst, 2000). In this paper, we present an approach to the detection of errors in discontinuous structural annotation. We focus on syntactic annotation with potentially discontinuous constituents and show that the approach successfully deals with the discontinuous syntactic annotation found in the TIGER treebank (Brants et al., 2002). 2 The variation n-gram method Our approach builds on the variation n-gram algorithm introduced in Dickinson and Meurers (2003a,b). The basic idea behind that approach is that a string occurring more than once can occur with different labels in a corpus, which we refer to as variation. Variation is caused by one of two reasons: i) ambiguity: there is a type of string with multiple possible labels and different corpus occurrences of that string realize the different options, or ii) error: the tagging of a string is inconsistent across comparable occurrences. 1 The ordinary way of marking a constituent with brackets is inadequate for discontinuous constituents, so we instead boldface and underline the words belonging t"
P05-1040,J93-2004,0,0.0254548,"Missing"
P05-1040,A97-1014,0,0.0764029,"Missing"
P05-1040,W00-1907,0,\N,Missing
P05-1040,P98-2164,0,\N,Missing
P05-1040,C98-2159,0,\N,Missing
P05-1040,J01-2002,0,\N,Missing
P16-4002,P14-5010,0,0.00813453,"onstruction over the others and is independent of the normalization unit as it uses a ratio of the document length to the average document length in the collection. The final score of each document determines its place in the ranking and is calculated as: Text Extraction The Text Extractor makes use of the Boilerpipe library3 extracting plain text with the help of its DefaultExtractor. The choice is motivated by the high performance of the library as compared to other text extraction techniques (Kohlsch¨utter et al., 2010). Parsing Text parsing is facilitated by the Stanford CoreNLP library4 (Manning et al., 2014), which was chosen for its robust, performant and opensource implementation. Our initial prototype used the standard PCFG parser for constituent parsing, but its cubic time complexity was a significant issue when parsing texts with long sentences. We therefore switched to a shift-reduce implementation5 that scales linearly with sentence and parse length. While it resulted in a higher memory overhead due to its large language models, it allowed us to substantially improve the performance of our code. G(q, d) = P (k+1)×tft,d t∈q∩d tf +k×(1−b+b× |d |) t,d avdl × log Ndf+1 t where q is a FLAIR que"
P16-4002,W16-0521,1,0.881826,"Missing"
P16-4002,E14-1031,1,0.822737,"ghlighting the occurrences of words from customized vocabulary lists has also been implemented. In addition to the already available length and readability filters, we are working on the options to constrain the search space by including 9 The mean and the median are given for 81 targets because six grammatical constructions did not occur in the test set. 11 support for i) search restricted to specific web domains and data sets, such as Project Gutenberg10 or news pages, and ii) search through one’s own data set. We also plan to implement and test more sophisticated text readability formulas (Vajjala and Meurers, 2014) and extend our information retrieval algorithm. Finally, a pilot online user study targeting language teachers is the first step we are taking to empirically evaluate the efficacy of the tool. On the technical side, FLAIR was built from the ground up to be easily scalable and extensible. Our implementation taps the parallelizability of text parsing and distributes the task homogenously over any given hardware. While FLAIR presently supports the English language exclusively, its architecture enables us to add support for more languages and grammatical constructions with a minimal amount of wor"
P97-1001,C90-3052,0,\N,Missing
P97-1001,P95-1012,1,\N,Missing
S13-2102,P98-1013,0,0.0454028,"ucture, even for ill-formed sentences. CoSeC then aligns the LRS representations of the reference answer and the student answer to each other and also to the representation of the question. The alignment approach takes into account local criteria, namely the semantic similarity of pairs of elements that are linked by the alignment, as well as global criteria measuring the extent to which the alignment preserves structure at the levels of variables and the subterm structure of the semantic formulas. Local similarity of semantic expressions is estimated using WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998), PMI-IR (Turney, 2001) on the UkWaC (Baroni et al., 2009) as used in CoMiC, the Minimum Edit Distance (Levenshtein, 1966), and special parameters for comparing functional elements such as quantifiers and grammatical function labels. Based on the alignments, the system marks elements which are not linked to elements in the question or which are linked to the semantic contribution of an alternative in an alternative question as “focused”. This is intended as a first approximation of the concept of focus in the sense of Information Structure (von Heusinger, 1999; Kruijff-Korbayov´a and Steedman,"
S13-2102,P07-1033,0,0.0170652,"Missing"
S13-2102,S13-2045,0,0.158299,"Missing"
S13-2102,W12-2039,1,0.935151,"nally compared in realistic situations. In realistic situations, utterances are not necessarily well-formed or complete, there may be individual differences in situative and world knowledge among the speakers. This can complicate or even preclude a complete linguistic analysis, leading us to the following research question: Which linguistic representations can be used effectively and robustly for comparing the meaning of sentences and text fragments computationally? 1 http://purl.org/dm/projects/sfb833-a4 The second approach emerging from the research project is CoSeC (Hahn and Meurers, 2011; Hahn and Meurers, 2012), a semantics-based system for meaning comparison that was developed for German from the start and was ported to operate on English for this shared task. As a novel contribution in this paper, we present CoMeT (Comparing Meaning in T¨ubingen), a system that employs a meta-classifier for combining the output of CoMiC and CoSeC and three shallower bag approaches. In terms of the general context of our work, short answer assessment essentially comes in the two flavors of meaning comparison and grading, the first trying to determine whether or not two utterances convey the same meaning, the latter"
S13-2102,W11-2401,1,0.884527,"d from all of the student answers in the training data. In the meta-classifier, we tried to combine the benefits of the named sub-systems into one large system that eventually computed our submission to the SemEval 2013 Task 7 challenge. 2.1 CoMiC CoMiC (Comparing Meaning in Context) is an alignment-based system, i.e., it operates on a mapping of linguistic units found in a student answer to those given in a reference answer. CoMiC started off as a re-implementation of the Content Assessment Module (CAM) of Bailey and Meurers (2008). It exists in two flavors: CoMiC-DE for German, described in Meurers et al. (2011b), and CoMiC-EN for English, described in Meurers et al. (2011a). Both systems are positioned in the landscape of the short answer assessment field in Ziai et al. (2012). In this paper, we refer to CoMiC-EN simply as CoMiC. Sketched briefly, CoMiC operates in three stages: 1. Annotation uses various NLP modules to equip student answers and reference answers with linguistic abstractions of several types. 2. Alignment creates links between these linguistic abstractions from the reference answer to the student answer. 3. Classification uses summary statistics of these alignment links in machine"
S13-2102,C12-1158,0,0.0208754,"(Comparing Meaning in T¨ubingen) is a meta-classifier which builds on the predictions of our individual systems (feature stacking, see Wolpert, 1992). The rationale is that if systems are comple4 http://morphadorner.northwestern.edu 612 mentary, their combination will perform better (or at least as good) than any individual system on its own. The design is as follows: Each system produces predictions on the training set, using 10-fold cross-validation, and on the test set. In addition to the predicted class, each system was also made to output probabilities for each possible class (cf., e.g., Tetreault et al., 2012a). The class probabilities were then used as features in the meta classifier to train a model for the test data. In addition to the probabilities, we also used the question ID and module ID in the meta-classifier, in the hope that they would allow differentiation between scenarios. For example, an unseen question ID means that we are not testing on unseen answers and thus predictions from systems with more abstraction from the surface may be preferred. The class probabilities come from different sources, depending on the system. In the case of CoMiC, they are extracted directly from the decis"
S13-2102,W12-2022,1,0.907234,"that was developed for German from the start and was ported to operate on English for this shared task. As a novel contribution in this paper, we present CoMeT (Comparing Meaning in T¨ubingen), a system that employs a meta-classifier for combining the output of CoMiC and CoSeC and three shallower bag approaches. In terms of the general context of our work, short answer assessment essentially comes in the two flavors of meaning comparison and grading, the first trying to determine whether or not two utterances convey the same meaning, the latter aimed at grading the abilities of students (cf. Ziai et al., 2012). Short answer assessment is also closely related to the field of Recognizing Textual Entailment (RTE, Dagan et al., 2009), which this year is directly reflected by the fact that SemEval 2013 Task 7 is the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge. 608 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 608–616, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Turning to the organization of this paper, section 2 intr"
S13-2102,C98-1013,0,\N,Missing
S13-2102,W08-0913,1,\N,Missing
S16-2026,W12-1632,0,0.175768,"is used to predict patterns of prosodic prominence. Schwarzschild (1999) proposes to define Givenness in terms of i) may be transported by semantically similar words, ii) entailment rather than identity is at stake, and iii) so-called bridging cases seem to involve semantically related rather than semantically similar words. 209 Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 209–218, Berlin, Germany, August 11-12, 2016. Computational linguistic approaches to classifying Givenness (Hempelmann et al., 2005; Nissim, 2006; Rahman and Ng, 2011; Cahill and Riester, 2012) have concentrated on the information status of noun phrases, without taking into account other syntactic elements. Furthermore, they do not explicitly make use of similarity and relatedness between lexical units as we propose in this paper. Our approach thus explores a new avenue in computationally determining Givenness. Theoretical linguistic proposals spelling out Givenness are based on formal semantic formalisms and notions such as logical entailment, type shifting, and existential f-closure, which do not readily lend themselves to extending the computational linguistic approaches. As alre"
S16-2026,D15-1188,0,0.0189937,"., 2013) and GloVe (Pennington et al., 2014). While word2vec is a prediction-based approach that optimizes the probability of a word occurring in a certain context, GloVe is a counting approach based on co-occurrences of words. We compared the two on the lexical substitution task designed for GermEval 2015 (Miller et al., 2015). The task can be seen as related to recognizing Givenness: deciding what a good substitute for a word in context is requires similar mechanisms to deciding whether the meaning of a word is already present in previous utterances. For GloVe, we used the models trained by Dima (2015), which were also trained on a large German web corpus and were shown to perform well. However, results on the lexical substitution task put both of word2vec’s training approaches, continuous bagof-words (CBOW) and skip-gram, ahead of GloVe using the models previously mentioned, so we continued with word2vec. Finally, to select the optimal training algorithm for word2vec for our purpose, we again used the GermEval task as a benchmark. We explored both CBOW and skip-gram with negative sampling and hierarchical softmax, yielding four combinations. Among these, CBOW with hierarchical softmax sign"
S16-2026,D10-1113,0,0.0314686,"n got the job. b. I KNOW. They WANTed a New Yorker. The part of the formal definitions that is intended to capture the deaccenting of New Yorker in a context where John is known to be from that city simply refers to salience (Schwarzschild, 1999: “An utterance U counts as GIVEN iff it has a salient antecedent A . . . ”), which Schwarzschild readily admits is not actually modeled: “Exactly which propositions count as in the background for these purposes remains to be worked out”. While beyond the scope of our experiments, approaches computing semantic similarity in more local contexts, such as Dinu and Lapata (2010), may be able to provide an avenue for handling such narrowly contextualized notions of common ground in the evolving, dynamic discourse. A more straightforward case arises when such bridging examples involve semantic relatedness between expressions that are richly represented in corpora. For example, the fact that Giuliani was the mayor of New York and thus can be identified as semantically related to New Yorker in (4) is within reach of a distributional semantic approach. (4) a. Giuliani got the job. b. I KNOW. They WANTed a New Yorker. When exactly such bridging based on semantically relate"
S16-2026,P15-1010,0,0.0383204,"me.) I’d like to STRANgle the butcher. The linguistic approaches to Givenness do not formally tackle this since the lexical semantic specification and contextual disambiguation of butcher as a particular (undesirable type of) dentist is beyond their scope. The fact that butcher counts as GIVEN is not readily captured by a general distributional semantic approach either since it is dependent on the specific context and the top-down selection of the meaning of butcher as referring to people who brutally go about their job. Distributional semantic approaches distinguishing specific word senses (Iacobacci et al., 2015) could be applicable for extending the core approach worked out in this paper to cover such cases. Overall, at the conceptual level, a realization of Givenness in terms of distributional semantics can be seen as nicely complementing the theoretical linguistic approach in terms of the division of labor of formal and distributional factors. 3 Content Assessment: Baseline System and Gold Standard Data To be able to test the idea we conceptually motivated above, we chose short answer assessment as our experimental testbed. The content assessment of reading comprehension exercises is an authentic t"
S16-2026,W09-4613,0,0.0254542,"re 4: CREG example illustrating Semantic Relatedness 215 Q: Ist die Wohnung in einem Neubau oder einem Altbau? an old building is the flat in a new building or Identifying Givenness TA: Die Wohnung ist in einem Neubau. The flat is in a new building Alignment for Assessment SA: Die Wohnung ist in einem Neubau. The flat is in a new building Figure 5: CREG example illustrating overidentification by Givenness filter Semantic relatedness is not semantic similarity Second, it is difficult for distributional semantic approaches to distinguish semantic similarity from semantic relatedness (cf., e.g., Kolb, 2009). In the discussion of bridging in section 2 we saw that cases such as (4) could arguably benefit from the use of semantic relatedness to identify Givenness. Yet, allowing all semantic related material to count as GIVEN clearly overestimates what counts as GIVEN and can therefore be deaccented. As a result, our approach wrongly identifies some semantic relatedness cases as Givenness. Consider the semantically related words Vorstand (management board) and Aufsichtsrat (supervisory board) in the example shown in Figure 4. The Givenness filter ensures that the lexical material der (the), Vorstand"
S16-2026,W11-2401,1,0.839185,"l side, the characteristic problem of obtaining high inter-annotator agreement in focus annotation (Ritz et al., 2008; Calhoun et al., 2010) can be overcome through an incremental annotation process making reference to questions as part of an explicit task context (Ziai and Meurers, 2014; De Kuthy et al., 2016). In short answer assessment approaches determining whether a student response correctly answers a provided reading comprehension question, the practical value of excluding material that is mentioned in the question from evaluating the content of the answer has been clearly established (Meurers et al., 2011; Mohler et al., 2011). Yet these computational linguistic approaches only implement a very basic, completely surface-based perspective on Givenness: A word of the answer that appears as such in the question counts as GIVEN. Such a surface-based approach to Givenness fails to capture that the semantic notion of Givenness Givenness (Schwarzschild, 1999) is one of the central notions in the formal pragmatic literature discussing the organization of discourse. In this paper, we explore where distributional semantics can help address the gap between the linguistic insights into the formal pragmati"
S16-2026,P11-1076,0,0.143129,"stic problem of obtaining high inter-annotator agreement in focus annotation (Ritz et al., 2008; Calhoun et al., 2010) can be overcome through an incremental annotation process making reference to questions as part of an explicit task context (Ziai and Meurers, 2014; De Kuthy et al., 2016). In short answer assessment approaches determining whether a student response correctly answers a provided reading comprehension question, the practical value of excluding material that is mentioned in the question from evaluating the content of the answer has been clearly established (Meurers et al., 2011; Mohler et al., 2011). Yet these computational linguistic approaches only implement a very basic, completely surface-based perspective on Givenness: A word of the answer that appears as such in the question counts as GIVEN. Such a surface-based approach to Givenness fails to capture that the semantic notion of Givenness Givenness (Schwarzschild, 1999) is one of the central notions in the formal pragmatic literature discussing the organization of discourse. In this paper, we explore where distributional semantics can help address the gap between the linguistic insights into the formal pragmatic notion of Givenness"
S16-2026,W06-1612,0,0.00984996,"rically recoverable” and the notion is used to predict patterns of prosodic prominence. Schwarzschild (1999) proposes to define Givenness in terms of i) may be transported by semantically similar words, ii) entailment rather than identity is at stake, and iii) so-called bridging cases seem to involve semantically related rather than semantically similar words. 209 Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 209–218, Berlin, Germany, August 11-12, 2016. Computational linguistic approaches to classifying Givenness (Hempelmann et al., 2005; Nissim, 2006; Rahman and Ng, 2011; Cahill and Riester, 2012) have concentrated on the information status of noun phrases, without taking into account other syntactic elements. Furthermore, they do not explicitly make use of similarity and relatedness between lexical units as we propose in this paper. Our approach thus explores a new avenue in computationally determining Givenness. Theoretical linguistic proposals spelling out Givenness are based on formal semantic formalisms and notions such as logical entailment, type shifting, and existential f-closure, which do not readily lend themselves to extending"
S16-2026,D14-1162,0,0.081773,"since it is a large corpus that is freely available and it is already lemmatized, both of which have been argued to be desirable for word vector models. Further preprocessing consisted of excluding numbers and other undesired words such as foreign language material and words the POS tagger had labelled as non-words. The whole corpus was converted to lowercase to get rid of unwanted distinctions between multiple possible capitalizations. To select an implementation for our purpose, we compared two of the major word vector toolkits currently available, word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). While word2vec is a prediction-based approach that optimizes the probability of a word occurring in a certain context, GloVe is a counting approach based on co-occurrences of words. We compared the two on the lexical substitution task designed for GermEval 2015 (Miller et al., 2015). The task can be seen as related to recognizing Givenness: deciding what a good substitute for a word in context is requires similar mechanisms to deciding whether the meaning of a word is already present in previous utterances. For GloVe, we used the models trained by Dima (2015), which were also trained on a la"
S16-2026,D11-1099,0,0.0124601,"rable” and the notion is used to predict patterns of prosodic prominence. Schwarzschild (1999) proposes to define Givenness in terms of i) may be transported by semantically similar words, ii) entailment rather than identity is at stake, and iii) so-called bridging cases seem to involve semantically related rather than semantically similar words. 209 Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 209–218, Berlin, Germany, August 11-12, 2016. Computational linguistic approaches to classifying Givenness (Hempelmann et al., 2005; Nissim, 2006; Rahman and Ng, 2011; Cahill and Riester, 2012) have concentrated on the information status of noun phrases, without taking into account other syntactic elements. Furthermore, they do not explicitly make use of similarity and relatedness between lexical units as we propose in this paper. Our approach thus explores a new avenue in computationally determining Givenness. Theoretical linguistic proposals spelling out Givenness are based on formal semantic formalisms and notions such as logical entailment, type shifting, and existential f-closure, which do not readily lend themselves to extending the computational lin"
S16-2026,ritz-etal-2008-annotation,0,0.40428,"Missing"
S16-2026,W16-0527,0,0.153388,"to Givenness. 211 3.1 Baseline system CoMiC is an alignment-based Content Assessment system which assesses student answers by analyzing the quantity and quality of alignment links it finds between the student and the target answer. For content assessment, it extracts several numeric features based on the number and kind of alignments found between non-GIVEN answer parts. The only change we made to the baseline setup is to replace the TiMBL (Daelemans et al., 2007) implementation of k-nearest-neighbors with the WEKA package (Hall et al., 2009), setting k to 5 following the positive results of Rudzewitz (2016). The CoMiC system we use as baseline for our research employs a surface-based Givenness filter, only aligning tokens not found in the question. The surface-based Givenness filter thus ensures that parts of the answer already occurring in the question are not counted (or could be fed into separate features so that the machine learner making the final assessment can take their discourse status into account). 3.2 Gold-standard content assessment corpus The data we used for training and testing our extension of the CoMiC system are taken from the CREG corpus (Ott et al., 2012), a task-based corpu"
S16-2026,W14-4922,1,0.810407,"i,kdk,dm}@sfs.uni-tuebingen.de Abstract the entailment of the existential f-closure between previously mentioned material and the GIVEN expression, hereby also capturing the occurrence of synonyms and hyponyms as given. On the theoretical linguistic side, a foundational question is whether an approach to Information Structure should be grounded in terms of a GivenNew or a Focus-Background dichotomy, or whether the two are best seen as complementing each other. Computational linguistic research on short answer assessment points in the direction of both perspectives providing performance gains (Ziai and Meurers, 2014). On the empirical side, the characteristic problem of obtaining high inter-annotator agreement in focus annotation (Ritz et al., 2008; Calhoun et al., 2010) can be overcome through an incremental annotation process making reference to questions as part of an explicit task context (Ziai and Meurers, 2014; De Kuthy et al., 2016). In short answer assessment approaches determining whether a student response correctly answers a provided reading comprehension question, the practical value of excluding material that is mentioned in the question from evaluating the content of the answer has been clea"
S16-2026,L16-1621,1,\N,Missing
W02-0103,W01-1512,0,0.0266442,"more readable and compact grammars, which we believe to be of central importance in a teaching context. To illustrate this, we are currently porting the LinGO3 English Resource Grammar (ERG) from the LKB (on which the ERG was designed) to the TRALE system. Given the scope of our web-based training framework as including an integrated module on parsing, it is also relevant that the TRALE system itself can be relatively compact and transparent at the sourcecode level since it exploits its close affinity to the underlying Prolog on which it is implemented. This contrasts with the perspective of Copestake et al. (2001), who concede that the LKB is unsuitable for teaching parsing. 2 1 http://www.ilias.uni-koeln.de/ios/index-e.html Integration of linguistic and computational aspects 3 http://www-csli.stanford.edu/˜aac/lkb.html http://lingo.stanford.edu/csli/ 4.2 The use of hyperlinks Several different varieties of links are distinguished within the course material, giving a first-class representation to the transfer of knowledge between the linguistic, computational and mathematical sources that inform this interdisciplinary area. We intend to distinguish the following kinds of links: Conceptual/taxonomical:"
W02-0103,W97-1506,0,0.099023,"Missing"
W05-0103,W02-0105,0,0.062929,"sification tasks, such as language identification) • Writers’ aids (Spelling and grammar correction) • Machine translation (2 weeks) • Dialogue systems (2 weeks) • Computer-aided language learning • Social context of language technology use In contrast to the courses of which we are aware that offer computational linguistics to undergraduates, our Language and Computers is supposed to be accessible without prerequisites to students from every major (a requirement for GEC courses). For example, we cannot assume any linguistic background or language awareness. Like Lillian Lee’s Cornell course (Lee, 2002), the course cannot presume programming 17 • Reasoning about finite-state automata and regular expressions (in the contexts of web searching and of information management). Students reason about relationships between specific and general search terms. • Reasoning about more elaborate syntactic representations (such as context-free grammars) and semantic representations (such as predicate calculus), in order to better understand grammar checking and machine translation errors. • Reasoning about the interaction between components of natural language systems (in the contexts of machine translatio"
W08-0913,W05-0909,0,0.0166913,"the relationship between Clinton and Lewinsky. 3 Method The CAM design integrates multiple matching strategies at different levels of representation and various abstractions from the surface form to compare meanings across a range of response variations. The approach is related to the methods used in 2 We use the term concept to refer to an entity or a relation between entities in a representation of the meaning of a sentence. Thus, a response generally contains multiple concepts. 3 Note the incorrect presupposition in the cue provided by the instructor. machine translation evaluation (e.g., Banerjee and Lavie, 2005; Lin and Och, 2004), paraphrase recognition (e.g., Brockett and Dolan, 2005; Hatzivassiloglou et al., 1999), and automatic grading (e.g., Leacock, 2004; Mar´ın, 2004). To illustrate the general idea, consider the example from our corpus in Figure 2. Figure 2: Basic matching example We find one string identical match between the token was occurring in the target and the learner response. At the noun chunk level we can match home with his house. And finally, after pronoun resolution it is possible to match Bob Hope with he. The overall architecture of CAM is shown in Figure 3. Generally speakin"
W08-0913,I05-5001,0,0.0087146,"grates multiple matching strategies at different levels of representation and various abstractions from the surface form to compare meanings across a range of response variations. The approach is related to the methods used in 2 We use the term concept to refer to an entity or a relation between entities in a representation of the meaning of a sentence. Thus, a response generally contains multiple concepts. 3 Note the incorrect presupposition in the cue provided by the instructor. machine translation evaluation (e.g., Banerjee and Lavie, 2005; Lin and Och, 2004), paraphrase recognition (e.g., Brockett and Dolan, 2005; Hatzivassiloglou et al., 1999), and automatic grading (e.g., Leacock, 2004; Mar´ın, 2004). To illustrate the general idea, consider the example from our corpus in Figure 2. Figure 2: Basic matching example We find one string identical match between the token was occurring in the target and the learner response. At the noun chunk level we can match home with his house. And finally, after pronoun resolution it is possible to match Bob Hope with he. The overall architecture of CAM is shown in Figure 3. Generally speaking, CAM compares the learner response to a stored target response and decides"
W08-0913,W99-0411,0,0.042466,"Missing"
W08-0913,W99-0625,0,0.0109656,"trategies at different levels of representation and various abstractions from the surface form to compare meanings across a range of response variations. The approach is related to the methods used in 2 We use the term concept to refer to an entity or a relation between entities in a representation of the meaning of a sentence. Thus, a response generally contains multiple concepts. 3 Note the incorrect presupposition in the cue provided by the instructor. machine translation evaluation (e.g., Banerjee and Lavie, 2005; Lin and Och, 2004), paraphrase recognition (e.g., Brockett and Dolan, 2005; Hatzivassiloglou et al., 1999), and automatic grading (e.g., Leacock, 2004; Mar´ın, 2004). To illustrate the general idea, consider the example from our corpus in Figure 2. Figure 2: Basic matching example We find one string identical match between the token was occurring in the target and the learner response. At the noun chunk level we can match home with his house. And finally, after pronoun resolution it is possible to match Bob Hope with he. The overall architecture of CAM is shown in Figure 3. Generally speaking, CAM compares the learner response to a stored target response and decides whether the two responses are p"
W08-0913,P03-1054,0,0.00583468,"Missing"
W08-0913,P04-1077,0,0.0112133,"Clinton and Lewinsky. 3 Method The CAM design integrates multiple matching strategies at different levels of representation and various abstractions from the surface form to compare meanings across a range of response variations. The approach is related to the methods used in 2 We use the term concept to refer to an entity or a relation between entities in a representation of the meaning of a sentence. Thus, a response generally contains multiple concepts. 3 Note the incorrect presupposition in the cue provided by the instructor. machine translation evaluation (e.g., Banerjee and Lavie, 2005; Lin and Och, 2004), paraphrase recognition (e.g., Brockett and Dolan, 2005; Hatzivassiloglou et al., 1999), and automatic grading (e.g., Leacock, 2004; Mar´ın, 2004). To illustrate the general idea, consider the example from our corpus in Figure 2. Figure 2: Basic matching example We find one string identical match between the token was occurring in the target and the learner response. At the noun chunk level we can match home with his house. And finally, after pronoun resolution it is possible to match Bob Hope with he. The overall architecture of CAM is shown in Figure 3. Generally speaking, CAM compares the"
W08-0913,W07-0734,0,\N,Missing
W08-0913,I05-5003,0,\N,Missing
W08-1004,brants-plaehn-2000-interactive,0,0.037167,"Missing"
W08-1004,P03-1013,0,0.0557692,"arch on PCFG parsing using treebank training data present PARSEVAL measures in comparing the parsing performance for different languages and annotation schemes, reporting a number of striking differences. For example, Levy and Manning (2003), K¨ubler (2005), and K¨ubler et al. (2006) highlight the significant effect of language properties and annotation schemes for German and Chinese treebanks. In related work, parser enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of improvement, if any, for German (Dubey and Keller, 2003; Dubey, 2004; K¨ubler et al., 2006). Previous work has compared the similar Negra and Tiger corpora of German to the very different T¨uBa-D/Z corpus. K¨ubler et al. (2006) compares the Negra and T¨uBa-D/Z corpora of German using a PARSEVAL evaluation and an evaluation on core grammatical function labels that is included to address concerns about the PARSEVAL measure.1 Using the Stanford Parser (Klein and Manning, 2002), which employs a factored PCFG and dependency model, they claim that the model trained on T¨uBaD/Z consistently outperforms that trained on Negra in PARSEVAL and grammatical fu"
W08-1004,W06-1614,0,0.0357137,"Missing"
W08-1004,P03-1056,0,0.0336896,"008. 2008 Association for Computational Linguistics It also does not allow for evaluation of particular syntactic structures or provide meaningful information about where the parser is failing. In addition, and most directly relevant for this paper, PARSEVAL scores are difficult to compare across syntactic annotation schemes (Carroll et al., 2003). At the same time, previous research on PCFG parsing using treebank training data present PARSEVAL measures in comparing the parsing performance for different languages and annotation schemes, reporting a number of striking differences. For example, Levy and Manning (2003), K¨ubler (2005), and K¨ubler et al. (2006) highlight the significant effect of language properties and annotation schemes for German and Chinese treebanks. In related work, parser enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of improvement, if any, for German (Dubey and Keller, 2003; Dubey, 2004; K¨ubler et al., 2006). Previous work has compared the similar Negra and Tiger corpora of German to the very different T¨uBa-D/Z corpus. K¨ubler et al. (2006) compares the Negra and T¨uBa-D/Z corpora of Ge"
W08-1004,D07-1066,0,0.0803232,"Missing"
W08-1004,W07-1506,1,\N,Missing
W08-1004,P07-1032,0,\N,Missing
W10-0212,H05-1073,0,0.294015,"facial expressions and body language. In this way a virtual character would be able to simulate emotional perception and production of text in story telling scenarios. 3 Related Work Although EA is often referred to as a developing field, the amount of work carried out during the last decades is phenomenal. This section is not meant as a full overview of the related research as that scope is too great for the length of this paper. To contextualize the research presented in this paper we focus on the projects that inspired us and fostered the ideas. The work done by Alm (Alm and Sproat, 2005; Alm et al., 2005; Alm, 2008) is close to our project in its sprit and goals. Alm, (2008) aims at implementing affective text-to-speech system for storytelling scenarios. An EA system, detecting sentences with emotions expressed in written text is a crucial element for achieving this goal. The annotated corpus was composed of three sets of children’s stories written by Beatrix Potter, H. C. Andersen, and the Brothers Grimm. Like Liu et al. (2003), Alm (2008) uses several emotional categories, while most research in automatic EA works with pure polarities. The set of emotion categories used is essentially the l"
W10-0212,J08-4004,0,0.112385,"n emotional perception of text, it is important to research the nature of the emotion analysis performed by humans and examine whether they can reliably perform the task. To investigate these issues, we conducted an experiment to find out the strategies people use to annotate selected folk fairy tale texts for emotions. The participants had to choose from a set of fifteen emotion categories, a significantly larger 98 set than typically used in EA, and assign them to an unrestricted range of text. To explore whether human annotators can reliably perform a task, inter-annotator agreement (IAA) (Artstein and Poesio, 2008) is the relevant measure. This measure can be calculated between every two individual annotations in order to find pairs or even teams of annotators whose strategies seem to be consistent and coherent enough so that they can be used further as the gold-standard annotation suited to train a machine learning approach for automatic EA analysis. A resulting EA system, capable of simulating human emotional perception of text, would be useful for information retrieval and many other fields. There are two main aspects of the resulting annotations to be researched. First, how consistently can people p"
W10-0212,P98-1013,0,0.0267986,"s is relevant to our project since we are collecting lexicons of subjective clues and the mechanisms of contextual influence may prove to be of value for future automatic EA system training. Bethard et at. (2004) provide valuable information about corpus annotation for EA means and give accounts on the performance of various existing ML algorithms. They provide excellent analysis of automatic extraction of opinion proposition and their holders. For feature extraction, the authors employ such well-known resources as WordNet (Miller et al., 1990), PropBank (Kingsbury et al., 2002) and FrameNet (Baker et al., 1998). Several types of classification tasks involve evaluation on the level of documents. For example, detecting subjective sentences, expressions, and other opinionated items in documents representing certain press categories (Wiebe et al., 2004) and measuring strength of subjective clauses (Wilson et al., 2004). All these and many more helped us to decide upon our own strategies, provided many examples of corpus collection and annotation, feature extraction and ML techniques usage in ways specific for the EA task. 4 Having established the research context, we now turn to the questions we investi"
W10-0212,J04-3002,0,0.025975,"bout corpus annotation for EA means and give accounts on the performance of various existing ML algorithms. They provide excellent analysis of automatic extraction of opinion proposition and their holders. For feature extraction, the authors employ such well-known resources as WordNet (Miller et al., 1990), PropBank (Kingsbury et al., 2002) and FrameNet (Baker et al., 1998). Several types of classification tasks involve evaluation on the level of documents. For example, detecting subjective sentences, expressions, and other opinionated items in documents representing certain press categories (Wiebe et al., 2004) and measuring strength of subjective clauses (Wilson et al., 2004). All these and many more helped us to decide upon our own strategies, provided many examples of corpus collection and annotation, feature extraction and ML techniques usage in ways specific for the EA task. 4 Having established the research context, we now turn to the questions we investigate in this paper: the use of an enriched category set and the flexible annotation units, and their influence on annotation quality. We describe the experiment we conducted and its main results. Each participant performed several tasks for ea"
W10-0212,J09-3003,0,0.0110787,"negative surprise. The EA system described in Alm et al. (2005) is machine learning based, where the EA problem is defined as multi-class classification problem, with sentences as classification units. 99 Liu et al. (2003) have combined an emotion lexicon and handcrafted rules, which allowed them to create affect models and thus form a representation of the emotional affinity of a sentence. Their annotation scheme is also sentence-based. The EA system was tested on short user-composed text emails describing emotionally colored events. In the research on recognizing contextual polarity done by Wilson et al. (2009) a rich prior-polarity lexicon and dependency parsing technique were employed to detect and analyze subjectivity on phrasal level, taking into account all the power of context, captured through such features as negation, polarity modification and polarity shifters. The work presents auspicious results of high accuracy scores for classification between neutrality and polarized private states and between negative and positive subjective phrases. A detailed account of several ML algorithms performance tests is discussed in thought-provoking manner. This work encouraged us to build a lexicon of su"
W10-0212,C98-1013,0,\N,Missing
W10-1002,W04-1703,0,0.0623187,"Missing"
W10-1002,W08-0913,1,0.844779,"attern, we use Constraint Grammar rules (Karlsson et al., 1995) on top of POS tagging, which allow for straightforward formulation of local disambiguation rules such as: “If an -ing form immediately follows the preposition by, select the gerund reading.” Standard POS 3 Given the nature of the input enhancement using colors, the highlighting in the figure is only visible in a color printout. 4 The issue bears some resemblance to the task of identifying paraphrases (Androutsopoulos and Malakasiotis, 2009) or classes of learner answers which differ in form but are equivalent in terms of meaning (Bailey and Meurers, 2008). 13 Figure 3: Color activity for gerunds vs. to-infinitives, cf. http://purl.org/icall/werti-color-ex2 For the Click activity, the web page is shown with colored gerund and to-infinitival forms and the learner can click on the corresponding clue phrases. For the Practice activity, the learner is presented with a fill-in-the-black version of the web page, as in the screenshot in Figure 4. For each blank, the learner needs to enter the gerund or to-infinitival form of the base form shown in parentheses. Wh-questions Question formation in English, with its particular word order, constitutes a we"
W10-1002,W08-0910,0,0.0154314,"Missing"
W10-1002,ide-etal-2000-xces,0,0.0108103,"patterns and activities, cf. section 2.1. While the first prototype of the WERTi system2 presented at CALICO (Amaral, Metcalf and Meurers, 2006) and EUROCALL (Metcalf and Meurers, 2006) was implemented in Python, the current system is Java-based, with all NLP being integrated in the UIMA framework (Ferrucci and Lally, 2004). UIMA is an architecture for the management and analysis of unstructured information such as text, which is built on the idea of referential annotation and can be seen as an NLP analysis counterpart to current stand-off encoding standards for annotated corpora (cf., e.g., Ide et al. 2000). The input we are developing a Firefox plugin, leaving only the NLP up to the server. This increases compatibility with web pages using dynamically generated contents and special session handling. 2 http://purl.org/icall/werti-v1 can be monotonically enriched while passing from one NLP component to the next, using a flexible data repository common to all components (G¨otz and Suhre, 2004). Such annotation-based processing is particularly useful in the WERTi context, where keeping the original text intact is essential for displaying it in enhanced form. A second benefit of using the UIMA frame"
W10-1002,W08-0911,0,0.0425888,"e for future research concerns the starting point of the system, the step where learners search for a web page they are interested in and select it for presentation with input enhancement. Enhancing of patterns presupposes that the pages contain instances of the pattern. The less frequent the pattern, the less likely we are to find enough instances of it in web pages returned by the standard web search engines typically used by learners to find pages of interest to them. The issue is related to research on providing learners with texts at the right level of reading difficulty (Petersen, 2007; Miltsakaki and Troutt, 2008), but the focus for us is on ensuring that texts which include instances of the specific language pattern targeted by a given input enhancement are ranked high in the search results. Ott (2009) presents a search engine prototype which, in addition to the content-focused documentterm information and traditional readability measures, supports indexing based on a more general notion of a text model into which the patterns relevant to input enhancement can be integrated – an idea we are exploring further (Ott and Meurers, Submitted). Acknowledgments We benefited from the feedback we received at CA"
W10-1002,C10-2031,1,\N,Missing
W10-1002,W08-0909,0,\N,Missing
W10-1002,P06-4018,0,\N,Missing
W10-1002,W02-0109,0,\N,Missing
W11-1714,P07-1056,0,0.0298349,"of this huge amount, however, the data needs to be first Detmar Meurers University of Tübingen Wilhelmstr. 1923 72074 Tübingen Germany dm@sfs.unituebingen.de effectively processed so that it could be used in a helpful way. The automatic identification of sentiments would make possible the processing of large amounts of such opinionated data. The focus of this paper is sentiment classification at documentlevel, namely classification of product reviews in the categories positive polarity or negative polarity. Training and testing data for our experiments is the MultiDomain Sentiment Dataset (Blitzer et al., 2007), which consists of product reviews of different domains, downloaded from Amazon1. We explore the use of phrases occurring maximally in text as features for sentiment classification of product reviews. In contrast to many related works on sentiment classification of documents, we do not use general polarity lexicons, which contain predefined positive and negative words. Very often the same word or phrase could express something positive in one situation and something negative in another. We identify words and phrases, which are typically used in positive and negative documents of some specific"
W11-1714,W09-4102,1,0.600453,"ve and negative documents of some specific domains, based on the frequencies of the words and phrases in the domainspecific corpora. After that we use these phrases to classify new sentiment documents from the same type of documents, from which the phrases are extracted. 1 http://www.amazon.com/ 111 Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 111–117, c 24 June, 2011, Portland, Oregon, USA 2011 Association for Computational Linguistics 2 Phrase Extraction In order to extract distinctive phrases we use the approach of Burek and Gerdemann (2009), who try to identify phrases, which are distinctive for each of the four different categories of documents in their medical data. With distinctive they mean phrases, which occur predominantly in one category of the documents or another. The algorithm extracts phrases of any length. The idea is that if a phrase is distinctive for a particular category, it does not matter how long the phrase is. The algorithm looks for repeats of phrases of any length, and could also count different types of occurrences of phrases, e.g. maximal, left maximal, or right maximal. Considering such types of occurre"
W11-1714,W02-1011,0,0.0148949,"86 0.83 0.85 0.84 0.81 0.85 0.83 0.83 0.87 0.85 0.83 0.86 0.84 0.85 0.87 0.86 0.84 0.84 0.84 0.82 0.83 0.83 0.85 0.85 0.85 0.84 0.84 0.84 0.86 0.86 0.86 Phrases used 1746n. 1883 p. 1013n. 1053 p. 384 n. 432 p. 7572 n. 9821 p. All: 16378 n. 17951 p. Table 2: Domain camera&photos. In order to evaluate how well the results of the experiments are we performed several more experiments, in which the texts were represented with unigrams (1grams) and bigrams (2grams). Pang and Lee (2008) note that: whether higher order ngrams are useful features appears to be a matter of some debate. For example, Pang et al. (2002) report that unigrams outperform bigrams when classifying movie reviews by sentiment polarity, but Dave et al. (2003) find that in some settings, bigrams and trigrams yield better productreview polarity classification. Bekkerman and Allan (2004) review the results of different experiments on text categorization in which n gram approaches were used, and conclude that the use of bigrams for the representation of texts does not show general improvement (Burek and Gerdemann, 2009). It seems intuitive that when bigrams are used, we would have a better representation of the texts, because we would"
W11-1714,P02-1053,0,0.00816005,"on, Domain books. Features Precision Recall Fmeasure All phrases 0.86 0.86 0.86 1gram 0.85 0.85 0.85 2gram 0.83 0.83 0.83 4 Table 8: Comparison, Domain camera&photos. Tables 7 and 8 summarize the overall results using 116 Related Work Close to our work seems to be Funk et al. (2008). They classify product and company reviews into one of the 1star to 5star categories. The features to the learning algorithm (also SVM) are simple linguistic features of single tokens. They report best results with the combinations root & orthography, and only root. Another interesting related work is that of Turney (2002). He uses an unsupervised learning algorithm to classify a review as recommended or not recommended. The algorithm extracts phrases from a given review, and determines their pointwise mutual information with the words excellent and poor. Turney (2002) points out that the contexual information is very often necessary for the correct determination of the sentiment polarity of a certain word. 5 Fmeasure All phrases 1gram and 2gram models and a model based on distinctive phrases for the representation of the texts. For both domains the best results are achieved with the model based on phrases ("
W11-1714,J01-1001,0,0.0465495,"distinctive for a particular category, it does not matter how long the phrase is  as long as it helps for distinguishing one type of document from another, it should be extracted. In order to extract phrases in this way, the whole collection of documents is represented as one long string. Each phrase is then a substring of this string. It will be very expensive to compute statistics (i.e. tf and df) and to run the simulation process (see 2.2.1) for each substring in the text. The reason is that the amount of substrings might be huge  there are a total of N(N + 1) / 2 substrings in a corpus (Yamamoto and Church, 2001). Yamamoto and Church (2001) show how this problem can be overcome by grouping the substrings into equivalence classes and performing operations (i.e. computing statistics) on these classes instead of on the individual elements of the classes. They use for this the suffix array data structure. The number of the classes is at most 2N – 1. 2.2.3 Maximal Occurrence of a Phrase The suffix array data structure allows for easy manipulation of the strings. The algorithm extracts phrases if they repeat in text, and if the phrases occur maximally at least once in the text. If the phrase do not occur ma"
W11-2401,W08-0913,1,0.295531,"al-life challenge for the automatic analysis of meaning. Given a text and a question, the content assessment task is to determine whether the answer given to a reading comprehension question actually answers the question or not. Such reading comprehension exercises are a common activity in foreign language There is relatively little research on content assessment for reading comprehension tasks and it so far has focused exclusively on English, including both reading comprehension questions answered by native speakers (Leacock and Chodorow, 2003; Nielsen et al., 2009) and by language learners (Bailey and Meurers, 2008). The task is related to the increasingly popular strand of research on Recognizing Textual Entailment (RTE, Dagan et al., 2009) and the Answer Validation Exercise (AVE, Rodrigo et al., 2009), which both have also generally targeted English. 1 Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 1–9, c Edinburgh, Scotland, UK, July 30, 2011. 2011 Association for Computational Linguistics The RTE challenge abstracts away from concrete tasks to emphasize the generic semantic inference component and it has significantly advanced the field under this perspective. At"
W11-2401,W97-0802,0,0.653972,"Missing"
W11-2401,C02-1150,0,0.0172688,"Missing"
W11-2844,I08-1059,0,0.0722063,"Missing"
W11-2844,W08-1205,0,0.0351365,"Missing"
W11-2844,C10-2031,1,\N,Missing
W12-2019,W10-1001,0,0.137792,"and Ostendorf (2009) report on classification experiments with WeeklyReader data, considering statistical language models, traditional formulae, as well as certain basic parse tree features in building an SVMbased statistical model. Feng et al. (2010) and Feng (2010) went beyond lexical and syntactic features and studied the impact of several discourse-based features, comparing their performance on the WeeklyReader corpus. While the vast majority of approaches have targeted English texts, some work on other languages such as German, Portuguese, French and Italian (vor der Br¨uck et al., 2008; Aluisio et al., 2010; Francois and Watrin, 2011; Dell’Orletta et al., 2011) is starting to emerge. Parse-tree-based features have also been used to measure the complexity of spoken Swedish (Roll et al., 2007). The process of text comprehension and the effect of factors such as the coherence of texts have also been intensively studied (e.g., Crossley et al., 2007a; 2007b; Graesser et al., 2004) and measures to analyze the text under this perspective have been implemented in the CohMetrix project.4 The DARPA Machine Reading program created 3 http://www.weeklyreader.com http://www.bbc.co.uk/bitesize 4 164 http://rea"
W12-2019,P11-1073,0,0.0109279,"th texts at the appropriate level of difficulty throughout their schooling.6 Independent of the research on readability, the complexity of the texts produced by language learners has been extensively investigated in Second Language Acquisition (SLA) research (Housen and Kuiken, 2009). Recent approaches have automated and compared a number of such complexity measures for learner language, specifically in English as Second Language learner narratives (Lu, 2010; Lu, 2011b). So far, there is hardly any work on using such insights in computational linguistics, though, with the notable exception of Chen and Zechner (2011) using SLA features to evaluate spontaneous non-native speech. Given that graded corpora are also intended to be used by incremental age groups, we started to investigate whether the insights from SLA research can fruitfully be applied to readability classification. 3 Corpora We used a combined corpus of WeeklyReader and BBC-Bitesize to develop a statistical model that classifies texts into five grade levels, based on the age groups. WeeklyReader7 is an educational newspaper, with articles targeted at four grade levels (Level 2, Level 3, Level 4, and Senior), corresponding to children 5 The co"
W12-2019,N04-1025,0,0.889134,"equency counts, to estimate readability of texts (Dale and Chall, 1948; Chall and Dale, 1995; Stenner, 1 2 1996). Dubay (2006) provides a broad survey of traditional approaches to readability assessment. Although the features considered appear shallow in terms of linguistic modeling, they have been popular for many years and are widely used. More recently, the developments in computational linguistics made it possible to consider various lexical and syntactic features to automatically model readability. In some of the early works on statistical readability assessment, Si and Callan (2001) and Collins-Thompson and Callan (2004) reported the impact of using unigram language models to estimate the grade level of a given text. The models were built on a United States text book corpus. Heilman et al. (2007; 2008b; 2008a) extended this approach and worked towards retrieving relevant reading materials for language learners in the REAP3 project. They extended the above mentioned approach to include a set of manually and later automatically extracted grammatical features. Schwarm and Ostendorf (2005) and Petersen and Ostendorf (2009) report on classification experiments with WeeklyReader data, considering statistical langua"
W12-2019,W11-2308,0,0.317324,"Missing"
W12-2019,C10-2032,0,0.439796,"on a United States text book corpus. Heilman et al. (2007; 2008b; 2008a) extended this approach and worked towards retrieving relevant reading materials for language learners in the REAP3 project. They extended the above mentioned approach to include a set of manually and later automatically extracted grammatical features. Schwarm and Ostendorf (2005) and Petersen and Ostendorf (2009) report on classification experiments with WeeklyReader data, considering statistical language models, traditional formulae, as well as certain basic parse tree features in building an SVMbased statistical model. Feng et al. (2010) and Feng (2010) went beyond lexical and syntactic features and studied the impact of several discourse-based features, comparing their performance on the WeeklyReader corpus. While the vast majority of approaches have targeted English texts, some work on other languages such as German, Portuguese, French and Italian (vor der Br¨uck et al., 2008; Aluisio et al., 2010; Francois and Watrin, 2011; Dell’Orletta et al., 2011) is starting to emerge. Parse-tree-based features have also been used to measure the complexity of spoken Swedish (Roll et al., 2007). The process of text comprehension and the"
W12-2019,R11-1061,0,0.0248152,"eport on classification experiments with WeeklyReader data, considering statistical language models, traditional formulae, as well as certain basic parse tree features in building an SVMbased statistical model. Feng et al. (2010) and Feng (2010) went beyond lexical and syntactic features and studied the impact of several discourse-based features, comparing their performance on the WeeklyReader corpus. While the vast majority of approaches have targeted English texts, some work on other languages such as German, Portuguese, French and Italian (vor der Br¨uck et al., 2008; Aluisio et al., 2010; Francois and Watrin, 2011; Dell’Orletta et al., 2011) is starting to emerge. Parse-tree-based features have also been used to measure the complexity of spoken Swedish (Roll et al., 2007). The process of text comprehension and the effect of factors such as the coherence of texts have also been intensively studied (e.g., Crossley et al., 2007a; 2007b; Graesser et al., 2004) and measures to analyze the text under this perspective have been implemented in the CohMetrix project.4 The DARPA Machine Reading program created 3 http://www.weeklyreader.com http://www.bbc.co.uk/bitesize 4 164 http://reap.cs.cmu.edu http://cohmetr"
W12-2019,N07-1058,0,0.692258,"assessment. Although the features considered appear shallow in terms of linguistic modeling, they have been popular for many years and are widely used. More recently, the developments in computational linguistics made it possible to consider various lexical and syntactic features to automatically model readability. In some of the early works on statistical readability assessment, Si and Callan (2001) and Collins-Thompson and Callan (2004) reported the impact of using unigram language models to estimate the grade level of a given text. The models were built on a United States text book corpus. Heilman et al. (2007; 2008b; 2008a) extended this approach and worked towards retrieving relevant reading materials for language learners in the REAP3 project. They extended the above mentioned approach to include a set of manually and later automatically extracted grammatical features. Schwarm and Ostendorf (2005) and Petersen and Ostendorf (2009) report on classification experiments with WeeklyReader data, considering statistical language models, traditional formulae, as well as certain basic parse tree features in building an SVMbased statistical model. Feng et al. (2010) and Feng (2010) went beyond lexical an"
W12-2019,W08-0909,0,0.396312,"Missing"
W12-2019,W08-0910,0,0.0919857,"Missing"
W12-2019,C10-1062,0,0.0907879,"er et al., 2004) and measures to analyze the text under this perspective have been implemented in the CohMetrix project.4 The DARPA Machine Reading program created 3 http://www.weeklyreader.com http://www.bbc.co.uk/bitesize 4 164 http://reap.cs.cmu.edu http://cohmetrix.memphis.edu a corpus of general text readability containing various forms of human and machine generated texts (Strassel et al., 2010).5 The aim of this program is to transform natural language texts into a format suitable for automatic processing by machines and to filter out poorly written documents based on the text quality. Kate et al. (2010) used this data set to build a coarse grained model of text readability. While in this paper we focus on comparing computational linguistic approaches to readability assessment and improving the state of the art on a traditional and available data set, Nelson et al. (2012) compared several research and commercially available text difficulty assessment systems in support of the Common Core Standards’ goal of providing students with texts at the appropriate level of difficulty throughout their schooling.6 Independent of the research on readability, the complexity of the texts produced by languag"
W12-2019,levy-andrew-2006-tregex,0,0.0257288,"Clause (CN/C) – Complex Nominals per T-unit (CN/T) – Verb phrases per T-unit (VP/T) Other Syntactic features – Num. NPs per sentence (NumNP) – Num. VPs per sentence (NumVP) – Num. PPs per sentence (NumPP)) – Avg. length of a NP (NPSize) – Avg. length of a VP (VPSize) – Avg. length of a PP (PPSize) – Num. Dependent Clauses per sentence (NumDC) – Num. Complex-T units per sentence (NumCT) – Num. Co-ordinate Phrases per sentence (CoOrd) – Num. SBARs per sentence (NumSBAR) – Avg. Parse Tree Height (TreeHeight) Table 4: Syntactic features (S YN F EATURES) the trees using the Tregex pattern matcher (Levy and Andrew, 2006). More details about the patterns from the SLA literature and their definitions can be found in Lu (2010). We used the OpenNLP12 tagger to get POS tag information and calculate Lexical Richness features. We used the WEKA (Hall et al., 2009) toolkit for our classification experiments. We explored different classification algorithms such as Decision Trees, Support Vector Machines, and Logistic Regression. The Multi-Layer Perceptron (MLP)classifier performed best with various combinations of features, so we focus on reporting the results for that algorithm. 12 http://opennlp.apache.org Feature se"
W12-2019,N07-1051,0,0.0140513,"(NumSyll), and the average sentence length in words (MLS) have been used to derive formulae for readability in the past. We refer to them as Traditional Features below. We included MLS in the syntactic features and NumChar, and NumSyll in the Lexical features. We also included two popular readability formulae, Flesch-Kincaid score (Kincaid et al., 1975) and Coleman-Liau readability formula (Coleman and Liau, 1975), as additional features. The latter will be referred as Coleman below, and both formulas together as Traditional Formulae. 5 Experiments and Evaluation We used the Berkeley Parser (Petrov and Klein, 2007) with the standard model they provide for building syntactic parse trees and defined the patterns for extracting various syntactic features from 168 Syntactic features from SLA research (SLASYN) – Mean length of clause (MLC) – Mean length of a sentence (MLS) – Mean length of T-unit (MLT) – Num. of Clauses per Sentence (C/S) – Num. of T-Units per sentence (T/S) – Num. of Clauses per T-unit (C/T) – Num. of Complex-T-Units per T-unit (CT/T) – Dependent Clause to Clause Ratio (DC/C) – Dependent Clause to T-unit Ratio (DC/T) – Co-ordinate Phrases per Clause (CP/C) – Co-ordinate Phrases per T-unit ("
W12-2019,P05-1065,0,0.93318,"matically model readability. In some of the early works on statistical readability assessment, Si and Callan (2001) and Collins-Thompson and Callan (2004) reported the impact of using unigram language models to estimate the grade level of a given text. The models were built on a United States text book corpus. Heilman et al. (2007; 2008b; 2008a) extended this approach and worked towards retrieving relevant reading materials for language learners in the REAP3 project. They extended the above mentioned approach to include a set of manually and later automatically extracted grammatical features. Schwarm and Ostendorf (2005) and Petersen and Ostendorf (2009) report on classification experiments with WeeklyReader data, considering statistical language models, traditional formulae, as well as certain basic parse tree features in building an SVMbased statistical model. Feng et al. (2010) and Feng (2010) went beyond lexical and syntactic features and studied the impact of several discourse-based features, comparing their performance on the WeeklyReader corpus. While the vast majority of approaches have targeted English texts, some work on other languages such as German, Portuguese, French and Italian (vor der Br¨uck"
W12-2019,strassel-etal-2010-darpa,0,\N,Missing
W12-2022,C02-2023,0,0.0939708,"Missing"
W12-2022,W08-0913,1,0.796768,"ressions Text classification Paraphrase recognition Atenea (P´erez et al., 2005) Assessment, Automatic grading Assessment, automatic grading Automatic grading Logic-based System (Makatchev Meaning comparison Information extraction w/ handwritten patterns Information extraction w/ handwritten patterns N-gram overlap, Latent Semantic Analysis First-order logic, machine learning Alignment, machine learning Alignment of facets, machine learning CarmelTC (Ros´e et al., 2003) C-Rater (Leacock and Chodorow, 2003) IAT (Mitchell et al., 2003) Oxford (Pulman and Sukkarieh, 2005) and VanLehn, 2007) CAM (Bailey and Meurers, 2008), CoMiC-EN (Meurers et al., 2011a) Facets System (Nielsen et al., 2009) Meaning comparison Meaning comparison & tutoring systems Texas (Mohler et al., 2011) Automatic grading CoMiC-DE (Meurers et al., 2011b) Meaning comparison CoSeC-DE (Hahn and Meurers, Meaning comparison 2012) Graph alignment, semantic similarity Alignment, machine learning Alignment via Lexical-Resource Semantics Domain Foreign language teaching Physics Mathematics, Reading comp. Medical EN GCSE exams EN Computer science ES Physics EN Reading comp. in foreign language Elementary school science classes Computer science EN Re"
W12-2022,P09-1032,0,0.024039,"Missing"
W12-2022,J09-4005,0,0.0126746,"’s performance, several metrics need to be reported. Finally, an important point concerns the quality of gold standards. Given the relatively low interannotator agreement in the Texas corpus (r = 0.586, RM SE = 0.659) it seems fair to ask whether answers without perfect agreement should be used in training and testing systems at all. In the CREE and CREG corpora, answers with disagreement among the annotators have either been excluded from experiments or resolved by an additional judge. This approach is also supported by recent literature (cf., e.g., Beigman and Beigman Klebanov 2009; Beigman Klebanov and Beigman 2009). However, for the Texas corpus, Mohler et al. (2011) have opted to use the arithmetic mean of the two graders as gold standard. While mathematically a viable solution, it seems questionable whether the mean is reliable with only two graders, especially if they have not operated on the grounds of explicit guidelines. It would be interesting to see whether in this case, a system trained on more, singly annotated data would perform better than one on less, doubly annotated data, as argued for by Dligach et al. (2010). In any case, if many disagreements occur, one should ask the question whether"
W12-2022,W10-1808,0,0.0115084,"y recent literature (cf., e.g., Beigman and Beigman Klebanov 2009; Beigman Klebanov and Beigman 2009). However, for the Texas corpus, Mohler et al. (2011) have opted to use the arithmetic mean of the two graders as gold standard. While mathematically a viable solution, it seems questionable whether the mean is reliable with only two graders, especially if they have not operated on the grounds of explicit guidelines. It would be interesting to see whether in this case, a system trained on more, singly annotated data would perform better than one on less, doubly annotated data, as argued for by Dligach et al. (2010). In any case, if many disagreements occur, one should ask the question whether the annotation task is defined well enough and whether machines should really be expected to perform it consistently if humans have trouble doing so. 5 Conclusion We discussed several issues in the comparison of short answer evaluation systems. To that end, we gave an overview of the existing systems and picked two for a concrete comparison on the same data, the CoMiC-EN system (Meurers et al., 2011a) and the 198 Texas system (Mohler et al., 2011). In comparing the two, it was necessary to turn CoMiC-EN into a scor"
W12-2022,N12-1021,0,0.0571788,"Missing"
W12-2022,W12-2039,1,0.865909,"Missing"
W12-2022,W11-2401,1,0.779675,"with the automatic assessment of student answers to comprehension questions, from language learning contexts to computer science exams. They share the need to evaluate free-text answers but differ in task setting and grading/evaluation criteria, among others. This paper has the intention of fostering synergy between the different research strands. It discusses the different research strands, details the crucial differences, and explores under which circumstances systems can be compared given publicly available data. To that end, we present results with the CoMiC-EN Content Assessment system (Meurers et al., 2011a) on the dataset published by Mohler et al. (2011) and outline what was necessary to perform this comparison. We conclude with a general discussion on comparability and evaluation of short answer assessment systems. 1 2 The short answer assessment landscape 2.1 Introduction Short answer assessment systems compare students’ responses to questions with manually defined target responses or answer keys in order to judge the appropriateness of the responses, or in order to automatically assign a grade. A number of approaches have emerged in recent years, each of them with different aims and differ"
W12-2022,P11-1076,0,0.110374,"o comprehension questions, from language learning contexts to computer science exams. They share the need to evaluate free-text answers but differ in task setting and grading/evaluation criteria, among others. This paper has the intention of fostering synergy between the different research strands. It discusses the different research strands, details the crucial differences, and explores under which circumstances systems can be compared given publicly available data. To that end, we present results with the CoMiC-EN Content Assessment system (Meurers et al., 2011a) on the dataset published by Mohler et al. (2011) and outline what was necessary to perform this comparison. We conclude with a general discussion on comparability and evaluation of short answer assessment systems. 1 2 The short answer assessment landscape 2.1 Introduction Short answer assessment systems compare students’ responses to questions with manually defined target responses or answer keys in order to judge the appropriateness of the responses, or in order to automatically assign a grade. A number of approaches have emerged in recent years, each of them with different aims and different backgrounds. In this paper, we will draw a map"
W12-2022,W05-0202,0,0.494555,"a concrete comparison on the same data, the CoMiC-EN system (Meurers et al., 2011a) and the 198 Texas system (Mohler et al., 2011). In comparing the two, it was necessary to turn CoMiC-EN into a scoring system because the Texas corpus as the chosen gold standard contains numeric scores assigned by humans. Taking a step back from the concrete comparison, we gave a more general description of what is necessary to compare short answer evaluation systems. We observed that more datasets need to be publicly available in order for performance comparisons to have meaning, a point also made earlier by Pulman and Sukkarieh (2005). Moreover, we noted how datasets differ in similar aspects as systems do, such as task context and assessment scheme. We then criticized the use of correlation measures as evaluation metrics for short answer scoring. Finally, we discussed the importance of gold standard quality. We conclude that it is interesting and relevant to compare short answer evaluation systems even if the concrete task they tackle, such as grading or meaning comparison, is not the same. However, the availability and quality of the datasets will decide to what extent systems can sensibly be compared. For progress to be"
W12-2022,N03-2030,0,0.0782391,"Missing"
W12-2024,P03-1059,0,0.081658,"Missing"
W12-2024,J92-4003,0,0.746037,"oach from the Helping Our Own (HOO) 2011 Shared Task (Boyd and Meurers, 2011) to identify determiner and preposition errors in non-native English essays from the Cambridge Learner Corpus FCE Dataset (Yannakoudakis et al., 2011) as part of the HOO 2012 Shared Task. Our system focuses on three error categories: missing determiner, incorrect determiner, and incorrect preposition. Approximately two-thirds of the errors annotated in HOO 2012 training and test data fall into these three categories. To improve our approach, we developed a missing determiner detector and incorporated word clustering (Brown et al., 1992) into the n-gram prediction approach. 1 1. 2. 3. ... 9. rather a question on the scales falling In Elghafari, Meurers and Wunsch (2010), the queries are submitted to the Yahoo search engine and in Boyd and Meurers (2011), the search engine is replaced with the ACL Anthology Reference Corpus (ARC, Bird et al., 2008), which contains texts of the same genre as the HOO 2011 data. If no hits are found for any of the 7-gram queries, shorter overlapping n-grams are used to approximate the 7-gram query. For instance, a 7-gram may be approximated by two overlapping 6-grams: Introduction [rather a quest"
W12-2024,W11-2838,0,0.0265869,"2010), the queries are submitted to the Yahoo search engine and in Boyd and Meurers (2011), the search engine is replaced with the ACL Anthology Reference Corpus (ARC, Bird et al., 2008), which contains texts of the same genre as the HOO 2011 data. If no hits are found for any of the 7-gram queries, shorter overlapping n-grams are used to approximate the 7-gram query. For instance, a 7-gram may be approximated by two overlapping 6-grams: Introduction [rather a question of the scales falling] We extend our n-gram-based prediction approach (Boyd and Meurers, 2011) from the HOO 2011 Shared Task (Dale and Kilgarriff, 2011) for the HOO 2012 Shared Task. This approach is an extension of the preposition prediction approach presented in Elghafari, Meurers and Wunsch (2010), which uses a surface-based approach to predict prepositions in English using frequency information from web searches to choose the most likely preposition in a given context. For each preposition in the text, the prediction algorithm considers up to three words of context on each side of the preposition, building a 7-gram with a preposition slot in the middle: rather a question rather a question of the scales falling rather a question to the sca"
W12-2024,C10-2031,1,0.865799,"Missing"
W12-2024,I08-1059,0,0.0272806,"Missing"
W12-2024,han-etal-2004-detecting,0,0.0230675,"Missing"
W12-2024,W00-0708,0,0.0493859,"Missing"
W12-2024,P11-1093,0,0.0145664,"Corpus (Web1T5, Brants and Franz, 2006), which contains 1-gram to 5-gram counts for a web corpus with approximately 1 trillion tokens and 95 billion sentences. Compared to our earlier approach, using the Web1T5 corpus reduces the size of available context by going from 7-grams to 5-grams, but we are intentionally keeping the corpus resources and algorithm simple. We are particularly interested in exploring the space between surface forms and abstractions by incorporating information from word clustering, an issue which is independent from the choice of a more sophisticated learning algorithm. Rozovskaya and Roth (2011) compared a range of learning algorithms for the task of correcting errors made by non-native writers, including an averaged perceptron algorithm (Rizzolo and Roth, 2007) and an n-gram count-based approach (Bergsma et al., 209 2009), which is similar to our approach. They found that the count-based approach performs nearly as well as the averaged perceptron approach when trained with ten times as much data. Without access to a large multi-genre corpus even a tenth the size of the Web1T5 corpus, we chose to use Web1T5. Our longest queries thus are 5-grams with at least one word of context on ea"
W12-2024,W08-1205,0,0.196312,"Missing"
W12-2024,P11-1019,0,0.0205553,"nforming Determiner and Preposition Error Correction with Word Clusters Adriane Boyd Marion Zepf Detmar Meurers Seminar f¨ur Sprachwissenschaft Universit¨at T¨ubingen {adriane,mzepf,dm}@sfs.uni-tuebingen.de Abstract For each prediction task, a cohort of queries is constructed with each of the candidate prepositions in the slot to be predicted: We extend our n-gram-based data-driven prediction approach from the Helping Our Own (HOO) 2011 Shared Task (Boyd and Meurers, 2011) to identify determiner and preposition errors in non-native English essays from the Cambridge Learner Corpus FCE Dataset (Yannakoudakis et al., 2011) as part of the HOO 2012 Shared Task. Our system focuses on three error categories: missing determiner, incorrect determiner, and incorrect preposition. Approximately two-thirds of the errors annotated in HOO 2012 training and test data fall into these three categories. To improve our approach, we developed a missing determiner detector and incorporated word clustering (Brown et al., 1992) into the n-gram prediction approach. 1 1. 2. 3. ... 9. rather a question on the scales falling In Elghafari, Meurers and Wunsch (2010), the queries are submitted to the Yahoo search engine and in Boyd and Me"
W12-2039,W08-0913,1,0.281661,"question. Similarly, Nielsen et al. (2009) use manually annotated word-word relations or ”facets”. Pulman and Sukkarieh (2005) use machine learning to automatically find such patterns. Other systems evaluate the correctness of answers by comparing them to one or more manually annotated target answers. C-Rater (Leacock and Chodorow, 2003) and the system of Mohler et al. (2011) compare the syntactic parse to the parse of target answers. A comparison of a range of content assessment approaches can be found in Ziai et al. (2012). The work in this paper is most similar to a line of work started by Bailey and Meurers (2008), who present a system for automatically assessing answers to reading comprehension questions written by learners of English. The basic idea is to align the student answers to a target answer using a parallel approach with several levels on which words or chunks can be matched to each other. Classification is done by a machine learning component. The CoMiC-DE system for German is also based on this approach (Meurers et al., 2011). In terms of broader context, the task is related to the research on Recognizing Textual Entailment (RTE) (Dagan et al., 2006). In particular, alignment (e.g., MacCar"
W12-2039,H05-1049,0,0.0678875,"Missing"
W12-2039,W97-0802,0,0.5,"ed to the verbal predicate via grammatical function terms such as subj and obj predicating over a Davidsonian event variable, as in Figure 1.1 For formulas with generalized quantifiers, the quantifiers, the variables, the scopes and the restrictors are compared: sim(Q1 x1 (φ ◦ ψ), Q2 x2 (σ ◦ τ )) = sim(Q1 , Q2 ) · sim(x1 , x2 ) (3) ·sim(φ, σ) · sim(ψ, τ ) As a basic similarity, the Levenshtein distance normalized to the interval [0,1] (with 1 denoting identity and 0 total dissimilarity) is used. This accounts for the high frequency of spelling errors in learner language. Synonyms in GermaNet (Hamp and Feldweg, 1997) receive the score 1. For numbers, the (normalized) difference |n1 −n2 | max(n1 ,n2 ) is used. For certain pairs of dissimilar elements which belong to the same category, constant costs are defined. This encourages the system to align these elements, unless the structural factors, i.e., the quality of the unifier and the consistency with dominance constraints, discourage this. Such constants are defined for pairs of grammatical function terms. Other constants are defined for pairs of numerical terms and for pairs of terms encoding affirmative and negative natural language expressions and logic"
W12-2039,D08-1084,0,0.00818455,"Missing"
W12-2039,W11-2401,1,0.822958,"appropriate semantic formalisms can be useful for the evaluation of short answers. In this paper, we explore the use of Lexical Resource Semantics (Richter and Sailer, 2003), one of the underspecified semantic formalisms combining the capability of precisely representing semantic distinctions with the robustness and modularity needed to represent meaning in real-life applications. Specifically, we address the task of evaluating the meaning of answers to reading comprehension exercises. We will base our experiments on the freely available data set used for the evaluation of the CoMiCDE system (Meurers et al., 2011), which does not use semantic representations. The data consists of answers to reading comprehension exercise written by learners of German together with questions and corresponding target answers. 326 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 326–336, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics Related Work task of our system is to decide which answers correctly answer the given question and which do not. 3.1 General Setup Empirical challenge: CREG Our experiments are based on the freely available Corpus"
W12-2039,P11-1076,0,0.534403,"2 )   ∃e(haben(e) ∧ subj(e,x1 ) ∧ obj(e,x2 ) A x1(B C) zimmer(x1) dusche(x2) E 3 3.2 A There are several systems which assess the content of short answers. Mitchell et al. (2002) use handcrafted patterns which indicate correct answers to a question. Similarly, Nielsen et al. (2009) use manually annotated word-word relations or ”facets”. Pulman and Sukkarieh (2005) use machine learning to automatically find such patterns. Other systems evaluate the correctness of answers by comparing them to one or more manually annotated target answers. C-Rater (Leacock and Chodorow, 2003) and the system of Mohler et al. (2011) compare the syntactic parse to the parse of target answers. A comparison of a range of content assessment approaches can be found in Ziai et al. (2012). The work in this paper is most similar to a line of work started by Bailey and Meurers (2008), who present a system for automatically assessing answers to reading comprehension questions written by learners of English. The basic idea is to align the student answers to a target answer using a parallel approach with several levels on which words or chunks can be matched to each other. Classification is done by a machine learning component. The"
W12-2039,W05-0202,0,0.280607,"resentation of the sentence, and PARTS is a list containing the subterms of the representation.   INCONT haben(e)    EXCONT A     A, haben(e), ∀x (B → C),  1 * +     zimmer(x1 ), ∃x2 (D ∧ E), ¬ F,  PARTS   dusche(x2 ), subj(e,x1 ), obj(e,x2 )   ∃e(haben(e) ∧ subj(e,x1 ) ∧ obj(e,x2 ) A x1(B C) zimmer(x1) dusche(x2) E 3 3.2 A There are several systems which assess the content of short answers. Mitchell et al. (2002) use handcrafted patterns which indicate correct answers to a question. Similarly, Nielsen et al. (2009) use manually annotated word-word relations or ”facets”. Pulman and Sukkarieh (2005) use machine learning to automatically find such patterns. Other systems evaluate the correctness of answers by comparing them to one or more manually annotated target answers. C-Rater (Leacock and Chodorow, 2003) and the system of Mohler et al. (2011) compare the syntactic parse to the parse of target answers. A comparison of a range of content assessment approaches can be found in Ziai et al. (2012). The work in this paper is most similar to a line of work started by Bailey and Meurers (2008), who present a system for automatically assessing answers to reading comprehension questions written"
W12-2039,W12-2022,1,0.86573,"Missing"
W12-2039,W07-1401,0,\N,Missing
W13-1726,C10-1011,0,0.0421215,"ms was beneficial. This confirms our assumption that longer n-grams can be sufficiently common to be useful (Bykh and Meurers, 2012, p. 433). Thus we used the recurring OCPOS n-grams up to length five for the experiments in this paper. We again used a binary feature representation. Recurring word-based dependencies (rc. word dep.) Extending the perspective on recurring pieces of data to other data types, we explored a new feature: recurring word-based dependencies. A feature of this type consists of a head and all its immediate dependents. The dependencies were obtained using the MATE parser (Bohnet, 2010). The words in each n-tuple are recorded in lowercase and listed in the order in which they occur in the text; heads thus are not singled out in this encoding. For example, the sentence John gave Mary an interesting book yields the following two potential features (john, gave, mary, book) and (an, interesting, book). As with recurring n-grams we utilized only features occurring in at least two texts of the training set, and we used a binary feature representation. Recurring function-based dependencies (rc. func. dep.) The recurring function-based dependencies are a variant of the recurring wor"
W13-1726,C12-1027,1,0.838439,"The labels for the train and dev sets were given from the start, the labels for the test set were provided after the results were submitted. Introduction Native Language Identification (NLI) tackles the problem of determining the native language of an author based on a text the author has written in a second language. With Tomokiyo and Jones (2001), Jarvis et al. (2004), and Koppel et al. (2005) as first publications on NLI, the research focus in computational linguistics is relatively young. But with over a dozen new publications in the last two years, it is gaining significant momentum. In Bykh and Meurers (2012), we explored a datadriven approach using recurring n-grams with three ICLE: International Corpus of Learner English (Granger et al., 2009) The ICLEv2 corpus consists of 6085 essays written by English learners of 16 different L1 backgrounds. They are at a similar level of English proficiency, namely higher intermediate to advanced and of about the same age. For the crosscorpus tasks we used the essays for the seven L1s in the intersection with T11, i.e., Chinese (982 essays), French (311), German (431), Italian (391), Japanese (366), Spanish (248), and Turkish (276). FCE: First Certificate in"
W13-1726,de-marneffe-etal-2006-generating,0,0.0374728,"Missing"
W13-1726,levy-andrew-2006-tregex,0,0.0130428,"as shown to play a role in NLI (Tetreault et al., 2012), we implemented all the text complexity features from Vajjala and Meurers (2012), who used measures of learner language complexity from SLA research for readability classification. These features consist of lexical richness and syntactic complexity measures from SLA research (Lu, 2010; 2012) as well as other syntactic parse tree properties and traditionally used readability formulae. The parse trees were built using the Berkeley parser (Petrov and Klein, 2007) and the syntactic complexity measures were estimated using the Tregex package (Levy and Andrew, 2006). In addition, we included morphological and POS features from the CELEX Lexical Database (Baayen et al., 1995). The morphological properties of words in CELEX include information about the derivational, inflectional and compositional features of the words along with information about their morphological origins and complexity. POS properties of the words in CELEX describe the various attributes of a word depending on its parts of speech. 199 We included all the non-frequency based and nonword-string attributes from the English Morphology Lemma (EML) and English Syntax Lemma (ESL) files of the"
W13-1726,N07-1051,0,0.0606752,"sbj, gave, obj, obj) and (nmod, nmod, book). Complexity Given that the proficiency level of a learner was shown to play a role in NLI (Tetreault et al., 2012), we implemented all the text complexity features from Vajjala and Meurers (2012), who used measures of learner language complexity from SLA research for readability classification. These features consist of lexical richness and syntactic complexity measures from SLA research (Lu, 2010; 2012) as well as other syntactic parse tree properties and traditionally used readability formulae. The parse trees were built using the Berkeley parser (Petrov and Klein, 2007) and the syntactic complexity measures were estimated using the Tregex package (Levy and Andrew, 2006). In addition, we included morphological and POS features from the CELEX Lexical Database (Baayen et al., 1995). The morphological properties of words in CELEX include information about the derivational, inflectional and compositional features of the words along with information about their morphological origins and complexity. POS properties of the words in CELEX describe the various attributes of a word depending on its parts of speech. 199 We included all the non-frequency based and nonword"
W13-1726,C12-1158,0,0.430879,"an, interesting, book). As with recurring n-grams we utilized only features occurring in at least two texts of the training set, and we used a binary feature representation. Recurring function-based dependencies (rc. func. dep.) The recurring function-based dependencies are a variant of the recurring word-based dependencies described above, where each dependent is represented by its grammatical function. The above example sentence thus yields the two features (sbj, gave, obj, obj) and (nmod, nmod, book). Complexity Given that the proficiency level of a learner was shown to play a role in NLI (Tetreault et al., 2012), we implemented all the text complexity features from Vajjala and Meurers (2012), who used measures of learner language complexity from SLA research for readability classification. These features consist of lexical richness and syntactic complexity measures from SLA research (Lu, 2010; 2012) as well as other syntactic parse tree properties and traditionally used readability formulae. The parse trees were built using the Berkeley parser (Petrov and Klein, 2007) and the syntactic complexity measures were estimated using the Tregex package (Levy and Andrew, 2006). In addition, we included morpho"
W13-1726,W13-1706,0,0.346096,"Missing"
W13-1726,N01-1031,0,0.128202,"native language (L1) backgrounds (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu, Turkish), and from three different proficiency levels (low, medium, high). Each L1 is represented by a set of 1100 essays (train: 900, dev: 100, test: 100). The labels for the train and dev sets were given from the start, the labels for the test set were provided after the results were submitted. Introduction Native Language Identification (NLI) tackles the problem of determining the native language of an author based on a text the author has written in a second language. With Tomokiyo and Jones (2001), Jarvis et al. (2004), and Koppel et al. (2005) as first publications on NLI, the research focus in computational linguistics is relatively young. But with over a dozen new publications in the last two years, it is gaining significant momentum. In Bykh and Meurers (2012), we explored a datadriven approach using recurring n-grams with three ICLE: International Corpus of Learner English (Granger et al., 2009) The ICLEv2 corpus consists of 6085 essays written by English learners of 16 different L1 backgrounds. They are at a similar level of English proficiency, namely higher intermediate to adva"
W13-1726,W12-2019,1,0.83761,"curring in at least two texts of the training set, and we used a binary feature representation. Recurring function-based dependencies (rc. func. dep.) The recurring function-based dependencies are a variant of the recurring word-based dependencies described above, where each dependent is represented by its grammatical function. The above example sentence thus yields the two features (sbj, gave, obj, obj) and (nmod, nmod, book). Complexity Given that the proficiency level of a learner was shown to play a role in NLI (Tetreault et al., 2012), we implemented all the text complexity features from Vajjala and Meurers (2012), who used measures of learner language complexity from SLA research for readability classification. These features consist of lexical richness and syntactic complexity measures from SLA research (Lu, 2010; 2012) as well as other syntactic parse tree properties and traditionally used readability formulae. The parse trees were built using the Berkeley parser (Petrov and Klein, 2007) and the syntactic complexity measures were estimated using the Tregex package (Levy and Andrew, 2006). In addition, we included morphological and POS features from the CELEX Lexical Database (Baayen et al., 1995). T"
W13-1726,P11-1019,0,0.0339223,"datadriven approach using recurring n-grams with three ICLE: International Corpus of Learner English (Granger et al., 2009) The ICLEv2 corpus consists of 6085 essays written by English learners of 16 different L1 backgrounds. They are at a similar level of English proficiency, namely higher intermediate to advanced and of about the same age. For the crosscorpus tasks we used the essays for the seven L1s in the intersection with T11, i.e., Chinese (982 essays), French (311), German (431), Italian (391), Japanese (366), Spanish (248), and Turkish (276). FCE: First Certificate in English Corpus (Yannakoudakis et al., 2011) The FCE dataset consists of 1238 scripts produced by learners taking the First Certificate in English exam, assessing English at an 197 Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 197–206, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics Corpora upper-intermediate level. For the cross-corpus tasks, we used the essays by learners of the eight L1s in the intersection with T11, i.e., Chinese (66 essays), French (145), German (69), Italian (76), Japanese (81), Korean (84), Spanish (198), and Turkish (73)."
W13-2907,W08-0909,0,0.460569,"ese, we also added the traditionally used measures of average number of characters per word, average number of syllables per word, and two readability formulae, the Flesch-Kincaid score (Kincaid et al., 1975) and the Coleman-Liau score (Coleman and Liau, 1975). Finally, we included the percentage of words from the Academic Word List5 . It is a list created by Coxhead (2000) which consists of words that are more commonly found in academic texts. The Readability Model In computational linguistics, readability assessment is generally approached as a classification problem. To our knowledge, only Heilman et al. (2008) and Ma et al. (2012a) experimented with other kinds of statistical models. We approach readability assessment as a regression problem. This produces a model which provides a continuous estimate of the reading level, enabling us to see if there are documents that fall between two levels or above the maximal level found in the training data. We used the WEKA implementation of linear regression for this purpose. Since linear regression assumes that the data falls on an interval scale with evenly spaced reading levels, we used numeric values from 1–5 as reading levels instead of the original clas"
W13-2907,N04-1025,0,0.0755043,"Missing"
W13-2907,P11-2117,0,0.0993559,"Missing"
W13-2907,levy-andrew-2006-tregex,0,0.0982249,"that the data falls on an interval scale with evenly spaced reading levels, we used numeric values from 1–5 as reading levels instead of the original class names in the WeeBit corpus. Table 1 shows the mapping from WeeBit classes to numeric values, along with the age groups per class. WeeBit class Level 2 Level 3 Level 4 KS3 GCSE Syntactic features (S YN F EATURES) These features are adapted from the syntactic complexity measures used to analyze second language writing (Lu, 2010). They are calculated based on the parser output of the BerkeleyParser (Petrov and Klein, 2007), using the Tregex (Levy and Andrew, 2006) pattern matcher. They include: mean lengths of various production units (sentence, clause and t-unit); clauses per sentence and t-unit; t-units per sentence; complex-t units per t-unit and per sentence; dependent clauses per clause, t-unit and sentence; co-ordinate phrases per clause, t-unit and sentence; complex nominals per clause and t-unit; noun phrases, verb phrases and preposition phrases per sentence; average length of NP, VP and PP; verb phrases per t-unit; SBARs per sentence and average parse tree height. We refer to the feature subset containing all the traditionally used features ("
W13-2907,E09-1027,0,0.0368823,"bining language models with manually and automatically extracted grammatical features. The relation of text coherence and cohesion to readability is well explored in the CohMetrix project (McNamara et al., 2002). Ma et al. (2012a; 2012b) approached readability assessment as a ranking problem and also compared human versus automatic feature extraction for the task of labeling children’s literature. The WeeklyReader1 , an American educational newspaper with graded readers has been a popular source of data for readability classification research in the recent past. Petersen and Ostendorf (2009), Feng et al. (2009) and Feng (2010) used it to build readability models with a range of lexical, syntactic, language modeling and discourse features. In Vajjala and Meurers (2012) we created a larger corpus, WeeBit, by combining WeeklyReader with graded reading material from the BBCBitesize website.2 We adapted measures of lexical richness and syntactic complexity from Second Language Acquisition (SLA) research as features for readability classification and showed that such measures of proficiency can successfully be used as features for readability assessment. 2.2 used a word-acquisition model for readability p"
W13-2907,C12-1065,1,0.888352,"Missing"
W13-2907,N12-1063,0,0.126611,"Missing"
W13-2907,N07-1058,0,0.105872,"Missing"
W13-2907,W12-2208,0,0.0624575,"-9 2013. 2013 Association for Computational Linguistics classification problem and explore different types of features. Statistical language modeling has been a popular approach (Si and Callan, 2001; Collins-Thompson and Callan, 2004), with the hypothesis that the word usage patterns across grade levels are distinctive enough. Heilman et. al. (2007; 2008) extended this approach by combining language models with manually and automatically extracted grammatical features. The relation of text coherence and cohesion to readability is well explored in the CohMetrix project (McNamara et al., 2002). Ma et al. (2012a; 2012b) approached readability assessment as a ranking problem and also compared human versus automatic feature extraction for the task of labeling children’s literature. The WeeklyReader1 , an American educational newspaper with graded readers has been a popular source of data for readability classification research in the recent past. Petersen and Ostendorf (2009), Feng et al. (2009) and Feng (2010) used it to build readability models with a range of lexical, syntactic, language modeling and discourse features. In Vajjala and Meurers (2012) we created a larger corpus, WeeBit, by combining"
W13-2907,sato-etal-2008-automatic,0,0.0579403,"Missing"
W13-2907,W08-0911,0,0.0847707,"pus and then studied their applicability to real-world documents retrieved from the web as well as the applicability of those features across different web sources. Readability Assessment of Web Texts Despite the significant body of research on readability assessment, applying it to retrieve relevant texts from the web has elicited interest only in the recent past. While Benn¨ohr (2005) and Newbold et al. (2010) created new readability formulae for this purpose, Ott and Meurers (2010) and Tan et al. (2012) used existing readability formulae to filter search engine results. The READ-X project (Miltsakaki and Troutt, 2008; Miltsakaki, 2009) combined standard readability formulae with topic classification to retrieve relevant texts for users. The REAP Project3 supports the lexical acquisition of individual learners by retrieving texts that suit a given learner level. Kidwell et al. (2011) also 1 http://weeklyreader.com http://www.bbc.co.uk/bitesize 3 http://reap.cs.cmu.edu 2 4 60 http://goo.gl/aVy93 4 Lexical features (L EX F EATURES) The lexical features are motivated by the lexical richness measures used to estimate the quality of language learners’ oral narratives (Lu, 2012). We included several type-token r"
W13-2907,W12-2019,1,0.950228,"l explored in the CohMetrix project (McNamara et al., 2002). Ma et al. (2012a; 2012b) approached readability assessment as a ranking problem and also compared human versus automatic feature extraction for the task of labeling children’s literature. The WeeklyReader1 , an American educational newspaper with graded readers has been a popular source of data for readability classification research in the recent past. Petersen and Ostendorf (2009), Feng et al. (2009) and Feng (2010) used it to build readability models with a range of lexical, syntactic, language modeling and discourse features. In Vajjala and Meurers (2012) we created a larger corpus, WeeBit, by combining WeeklyReader with graded reading material from the BBCBitesize website.2 We adapted measures of lexical richness and syntactic complexity from Second Language Acquisition (SLA) research as features for readability classification and showed that such measures of proficiency can successfully be used as features for readability assessment. 2.2 used a word-acquisition model for readability prediction. Collins-Thompson et al. (2011) and Kim et al. (2012) employed word distribution based readability models for personalized search and for creating ent"
W13-2907,E09-2013,0,0.0178019,"plicability to real-world documents retrieved from the web as well as the applicability of those features across different web sources. Readability Assessment of Web Texts Despite the significant body of research on readability assessment, applying it to retrieve relevant texts from the web has elicited interest only in the recent past. While Benn¨ohr (2005) and Newbold et al. (2010) created new readability formulae for this purpose, Ott and Meurers (2010) and Tan et al. (2012) used existing readability formulae to filter search engine results. The READ-X project (Miltsakaki and Troutt, 2008; Miltsakaki, 2009) combined standard readability formulae with topic classification to retrieve relevant texts for users. The REAP Project3 supports the lexical acquisition of individual learners by retrieving texts that suit a given learner level. Kidwell et al. (2011) also 1 http://weeklyreader.com http://www.bbc.co.uk/bitesize 3 http://reap.cs.cmu.edu 2 4 60 http://goo.gl/aVy93 4 Lexical features (L EX F EATURES) The lexical features are motivated by the lexical richness measures used to estimate the quality of language learners’ oral narratives (Lu, 2012). We included several type-token ratio variants used"
W13-2907,C10-1152,0,0.0358693,"Missing"
W13-2907,W10-0406,0,0.2085,"Missing"
W13-2907,N07-1051,0,\N,Missing
W13-2907,P05-1065,0,\N,Missing
W13-2907,E09-1000,0,\N,Missing
W14-1203,P11-1073,0,0.0459722,"Missing"
W14-1203,daelemans-etal-2004-automatic,0,0.0364437,"Missing"
W14-1203,W14-0111,0,0.0190842,"onjunctions, interjections, and prepositions) and different verb forms (VBG, VBD, VBN, VBP in the Penn Treebank tagset; Santorini 1990) per document. The SLA-based lexical richness features we used are: type-token ratio and corrected typetoken ratio, lexical density, ratio of nouns, verbs, adjectives and adverbs to the number of lexical words in a document, as described in Lu (2012). The POS information required to extract these features was obtained using Stanford Tagger (Toutanova et al., 2003). The average number of senses for a non-function word was obtained by using the MIT WordNet API2 (Finlayson, 2014). Psycholinguistic features (PSYCH): This group of features includes an encoding of the average Age-of-acquisition (AoA) of words according to different norms as provided by Kuperman et al. (2012), including their own AoA rating obtained through crowd sourcing. It also includes measures of word familiarity, concreteness, imageability, meaningfulness and AoA as assigned in the MRC Psycholinguistic database3 (Wilson, 1988). For each feature, the value per text we computed is the average of the values for all the words in the text that had an entry in the database. While these measures were not d"
W14-1203,W13-1504,0,0.0283138,"Missing"
W14-1203,C12-1065,1,0.909256,"Missing"
W14-1203,S12-1066,0,0.0267105,"provided by Kuperman et al. (2012), including their own AoA rating obtained through crowd sourcing. It also includes measures of word familiarity, concreteness, imageability, meaningfulness and AoA as assigned in the MRC Psycholinguistic database3 (Wilson, 1988). For each feature, the value per text we computed is the average of the values for all the words in the text that had an entry in the database. While these measures were not developed with readability analysis in mind, we came across one paper using such features as measures of word difficulty in an approach to lexical simplification (Jauhar and Specia, 2012). Syntactic complexity features (SYNTAX): This group of features encodes the syntactic complexity of a text derived from the constituent structure of the sentences. Some of these features are 2 3 http://projects.csail.mit.edu/jwi 22 http://www.psych.rl.ac.uk/ Celex features (CELEX): The Celex lexical database (Baayen et al., 1995) for English consists of annotations for the morphological, syntactic, orthographic and phonological properties for more than 50k words and lemmas. We included all the morphological and syntactic properties that were encoded using character or numeric codes in our fea"
W14-1203,N03-1033,0,0.00412994,"of words belonging to different parts of speech (nouns, proper nouns, pronouns, determiners, adjectives, verbs, adverbs, conjunctions, interjections, and prepositions) and different verb forms (VBG, VBD, VBN, VBP in the Penn Treebank tagset; Santorini 1990) per document. The SLA-based lexical richness features we used are: type-token ratio and corrected typetoken ratio, lexical density, ratio of nouns, verbs, adjectives and adverbs to the number of lexical words in a document, as described in Lu (2012). The POS information required to extract these features was obtained using Stanford Tagger (Toutanova et al., 2003). The average number of senses for a non-function word was obtained by using the MIT WordNet API2 (Finlayson, 2014). Psycholinguistic features (PSYCH): This group of features includes an encoding of the average Age-of-acquisition (AoA) of words according to different norms as provided by Kuperman et al. (2012), including their own AoA rating obtained through crowd sourcing. It also includes measures of word familiarity, concreteness, imageability, meaningfulness and AoA as assigned in the MRC Psycholinguistic database3 (Wilson, 1988). For each feature, the value per text we computed is the ave"
W14-1203,W12-2019,1,0.851674,"nate phrases per clause and t-unit, # t-units/sentence), and specific syntactic structures (# complex nominals per clause and t-unit, # VP per t-unit). Other syntactic complexity features we made use of are the number of NPs, VPs, PPs, and SBARs per sentence and their average length (in terms of # words), the average parse tree height and the average number of constituents per sub-tree. All of these features were extracted using the Berkeley Parser (Petrov and Klein, 2007) and the Tregex pattern matcher (Levy and Andrew, 2006). While the selection of features for these two classes is based on Vajjala and Meurers (2012), for the following two sets of features, we explored further information available through psycholinguistic resources. the older age-groups tend to be longer (i.e., more words per text) and have longer sentences. While text length and sentence length seem to constitute informative features for predicting the age-group, we hypothesized that other linguistic properties of the language used may be at least as informative as those superficial (and easily manipulated) properties. Hence, we explored a broad linguistic feature set encoding various aspects of complexity. 3 Features The feature set we"
W14-1203,W13-2907,1,0.917702,"Missing"
W14-1203,E14-1031,1,0.798875,"iding more information for model building and testing. 5 http://reap.cs.cmu.edu https://texteval-pilot.ets.org/ TextEvaluator 6 27 In terms of the practical relevance of the results, one question that needs some attention is how well the features and trained models generalize across different type of TV programs or languages. While we have not yet investigated this for TV subtitles, in experiments investigating the cross-corpus performance of a model using the same feature set, we found that the approach performs well for a range of corpora composed of reading materials for language learners (Vajjala and Meurers, 2014b). The very high classification accuracies of the experiments we presented in the current paper thus seem to support the assumption that the approach can be useful in practice for automatically identifying TV programs for viewers of different age groups. Regarding the three class distinctions and the classifier setup we used in this paper, the approach can also be generalized to other scales and a regression setup (Vajjala and Meurers, 2013). feedback. This research was funded by LEAD Graduate School (GSC 1028, http://purl.org/ lead), a project of the Excellence Initiative of the German feder"
W14-1203,levy-andrew-2006-tregex,0,0.00969255,"t-unit, # dependent clauses per clause and t-unit), coordination in a sentence (# co-ordinate phrases per clause and t-unit, # t-units/sentence), and specific syntactic structures (# complex nominals per clause and t-unit, # VP per t-unit). Other syntactic complexity features we made use of are the number of NPs, VPs, PPs, and SBARs per sentence and their average length (in terms of # words), the average parse tree height and the average number of constituents per sub-tree. All of these features were extracted using the Berkeley Parser (Petrov and Klein, 2007) and the Tregex pattern matcher (Levy and Andrew, 2006). While the selection of features for these two classes is based on Vajjala and Meurers (2012), for the following two sets of features, we explored further information available through psycholinguistic resources. the older age-groups tend to be longer (i.e., more words per text) and have longer sentences. While text length and sentence length seem to constitute informative features for predicting the age-group, we hypothesized that other linguistic properties of the language used may be at least as informative as those superficial (and easily manipulated) properties. Hence, we explored a broa"
W14-1203,N07-1051,0,0.00885739,"a sentence (# clauses per t-unit, # complex t-units per t-unit, # dependent clauses per clause and t-unit), coordination in a sentence (# co-ordinate phrases per clause and t-unit, # t-units/sentence), and specific syntactic structures (# complex nominals per clause and t-unit, # VP per t-unit). Other syntactic complexity features we made use of are the number of NPs, VPs, PPs, and SBARs per sentence and their average length (in terms of # words), the average parse tree height and the average number of constituents per sub-tree. All of these features were extracted using the Berkeley Parser (Petrov and Klein, 2007) and the Tregex pattern matcher (Levy and Andrew, 2006). While the selection of features for these two classes is based on Vajjala and Meurers (2012), for the following two sets of features, we explored further information available through psycholinguistic resources. the older age-groups tend to be longer (i.e., more words per text) and have longer sentences. While text length and sentence length seem to constitute informative features for predicting the age-group, we hypothesized that other linguistic properties of the language used may be at least as informative as those superficial (and ea"
W14-1203,petukhova-etal-2012-sumat,0,\N,Missing
W14-3508,C10-1030,0,0.0133697,"table reading level for language learners. Sharoff et al. (2008) use a Principle Component Analysis to analyze the lexical and grammatical complexity of texts in a variety of languages, including 100 Russian. Similarly, Karpov et al. (2014) train a variety of machine-learning models on a small corpus of texts categorized by CEFR level, with promising results. Another set of studies has been dedicated to Russian Intelligent Tutoring Systems. The Boltun project2 has as one of its goals to develop NLP resources for ICALL, especially for analyzing learner language (Dickinson and Herring, 2008a,b; Dickinson, 2010). Another project, KLIOS, is a Learning Management System being developed specifically for Russian foreign language learning (Gorisev et al., 2013). KLIOS apparently makes use of the existing general-purpose tagger pymorphy23 and parser ABBYY Compreno4 , but not enough information is currently available to draw meaningful comparisons of the activities and analyses to our work on the Russian VIEW. Goal and Structure of the Paper The goal of this article is to explore the ability of authentic text ICALL systems to provide adaptive feedback to learners. In doing so, we also demonstrate some featu"
W14-3508,W08-0901,0,0.0216768,"entifying Russian texts at a suitable reading level for language learners. Sharoff et al. (2008) use a Principle Component Analysis to analyze the lexical and grammatical complexity of texts in a variety of languages, including 100 Russian. Similarly, Karpov et al. (2014) train a variety of machine-learning models on a small corpus of texts categorized by CEFR level, with promising results. Another set of studies has been dedicated to Russian Intelligent Tutoring Systems. The Boltun project2 has as one of its goals to develop NLP resources for ICALL, especially for analyzing learner language (Dickinson and Herring, 2008a,b; Dickinson, 2010). Another project, KLIOS, is a Learning Management System being developed specifically for Russian foreign language learning (Gorisev et al., 2013). KLIOS apparently makes use of the existing general-purpose tagger pymorphy23 and parser ABBYY Compreno4 , but not enough information is currently available to draw meaningful comparisons of the activities and analyses to our work on the Russian VIEW. Goal and Structure of the Paper The goal of this article is to explore the ability of authentic text ICALL systems to provide adaptive feedback to learners. In doing so, we also d"
W14-3508,W10-1002,1,0.885754,"ed grammatical structures. Various modes of enhancement have been suggested, such as font manipulation (e.g., bold, italic, color), capitalization, and other notations (e.g., underlining, circling). Such textual enhancements are intended to increase the likelihood that the learner will notice the target grammatical form in its grammatical and functional context of use. Visual Input Enhancement of the Web (VIEW) is an ATICALL system designed to automatically generate learning activities from user-selected texts on the web. A description of the system architecture can be found in Meurers et al. (2010). VIEW includes four activity types to guide the learner from recognition via practice to production. The highlight activity adds color to target wordforms. The click activity allows the learner to identify target wordforms in the text. The multiple-choice activity provides controlled practice, allowing the learner to choose the correct form from a multiple-choice list. The practice activity asks learners to type the wordforms themselves. The activities can be accessed as a web application on a webpage or through a toolbar provided as a Firefox web browser Add-on. Activities have previously be"
W14-3508,sharoff-etal-2008-designing,0,0.0120954,"e systematicity of the linguistic system allow us to generate distractors? 6. Feedback: What kind of feedback does the learner receive for (in)correct answers, under a perspective conceiving of feedback as scaffolding guiding the learner in their Zone of Proximal Development (Vygotsky, 1986)? Related work Although research in Russian NLP for language learning has not been as extensive as for English and some other languages, some significant inroads have been made. One string of research is concerned with methods for identifying Russian texts at a suitable reading level for language learners. Sharoff et al. (2008) use a Principle Component Analysis to analyze the lexical and grammatical complexity of texts in a variety of languages, including 100 Russian. Similarly, Karpov et al. (2014) train a variety of machine-learning models on a small corpus of texts categorized by CEFR level, with promising results. Another set of studies has been dedicated to Russian Intelligent Tutoring Systems. The Boltun project2 has as one of its goals to develop NLP resources for ICALL, especially for analyzing learner language (Dickinson and Herring, 2008a,b; Dickinson, 2010). Another project, KLIOS, is a Learning Manageme"
W14-4922,W08-0913,1,0.848276,"otion of information structure. Empirically, our work focuses on analyzing the responses to reading comprehension questions. In computational linguistics, automatic meaning assessment determining whether a response appropriately answers a given question about a given text has developed into an active field of research. Short Answer Assessment recently was also highlighted by the Joint Student Response Analysis and Textual Entailment Challenge (Dzikovska et al., 2013). Some research in this domain has pointed out the relevance of identifying which parts of a response are given by the question (Bailey and Meurers, 2008; Mohler et al., 2011), with recent work pointing out that the relevant notion here is that of focus as discussed in formal pragmatics (Meurers et al., 2011; Hahn and Meurers, 2012). Figure 1 provides an example of answer comparison for meaning assessment, where the focus (marked by square brackets) can effectively be used to zoom in on the information that is relevant for comparing a target answer (TA) with a student answer (SA) given a question (Q). Figure 1: Answer comparison with the help of focus This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page nu"
W14-4922,C00-1021,0,0.153573,"Missing"
W14-4922,S13-2045,0,0.0587861,"et al., 2011). 1 Introduction This paper discusses the interplay of linguistic and computational linguistic aspects in the analysis of focus as a core notion of information structure. Empirically, our work focuses on analyzing the responses to reading comprehension questions. In computational linguistics, automatic meaning assessment determining whether a response appropriately answers a given question about a given text has developed into an active field of research. Short Answer Assessment recently was also highlighted by the Joint Student Response Analysis and Textual Entailment Challenge (Dzikovska et al., 2013). Some research in this domain has pointed out the relevance of identifying which parts of a response are given by the question (Bailey and Meurers, 2008; Mohler et al., 2011), with recent work pointing out that the relevant notion here is that of focus as discussed in formal pragmatics (Meurers et al., 2011; Hahn and Meurers, 2012). Figure 1 provides an example of answer comparison for meaning assessment, where the focus (marked by square brackets) can effectively be used to zoom in on the information that is relevant for comparing a target answer (TA) with a student answer (SA) given a quest"
W14-4922,W12-2039,1,0.94998,"t determining whether a response appropriately answers a given question about a given text has developed into an active field of research. Short Answer Assessment recently was also highlighted by the Joint Student Response Analysis and Textual Entailment Challenge (Dzikovska et al., 2013). Some research in this domain has pointed out the relevance of identifying which parts of a response are given by the question (Bailey and Meurers, 2008; Mohler et al., 2011), with recent work pointing out that the relevant notion here is that of focus as discussed in formal pragmatics (Meurers et al., 2011; Hahn and Meurers, 2012). Figure 1 provides an example of answer comparison for meaning assessment, where the focus (marked by square brackets) can effectively be used to zoom in on the information that is relevant for comparing a target answer (TA) with a student answer (SA) given a question (Q). Figure 1: Answer comparison with the help of focus This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 159 LAW VIII - The 8th Linguistic Annotation Workshop, pages 1"
W14-4922,S13-1041,0,0.0715967,"e paper. 2 Data and Annotation Setup We base our work on the CREG corpus (Ott et al., 2012), a task-based corpus consisting of answers to reading comprehension questions written by learners of German at the university level. The overall corpus includes 164 reading texts, 1,517 reading comprehension questions, 2,057 target answers provided by the teachers, and 36,335 learner answers. We use the CREG-1032 data subset (Meurers et al., 2011) for the present annotation work in order to enable comparison to previously published results on that data set (Meurers et al., 2011; Hahn and Meurers, 2012; Horbach et al., 2013). The CREG-1032 data set consists of two sub-corpora, which correspond to the sites they were collected at, Kansas University (KU) and Ohio State University (OSU). For the present work, we limited ourselves to the OSU portion of the data because it contains longer answers and more answers per question. 160 The OSU subset consists of 422 student answers to 60 questions, for which 87 target answers are available. The student answers were produced by 175 intermediate learners of German in the US, who on average wrote about 15 tokens per answer. All student answers were rated by two annotators wit"
W14-4922,W11-2401,1,0.836691,"sk-based corpus providing more explicit context. The annotation study is based on the CREG corpus (Ott et al., 2012), which consists of answers to explicitly given reading comprehension questions. On the other hand, we operationalize focus annotation as an incremental process including several substeps which provide guidance, such as explicit answer typing. We evaluate the focus annotation both intrinsically by calculating agreement between annotators and extrinsically by showing that the focus information substantially improves the automatic meaning assessment of answers in the CoMiC system (Meurers et al., 2011). 1 Introduction This paper discusses the interplay of linguistic and computational linguistic aspects in the analysis of focus as a core notion of information structure. Empirically, our work focuses on analyzing the responses to reading comprehension questions. In computational linguistics, automatic meaning assessment determining whether a response appropriately answers a given question about a given text has developed into an active field of research. Short Answer Assessment recently was also highlighted by the Joint Student Response Analysis and Textual Entailment Challenge (Dzikovska et"
W14-4922,P11-1076,0,0.0356891,"ture. Empirically, our work focuses on analyzing the responses to reading comprehension questions. In computational linguistics, automatic meaning assessment determining whether a response appropriately answers a given question about a given text has developed into an active field of research. Short Answer Assessment recently was also highlighted by the Joint Student Response Analysis and Textual Entailment Challenge (Dzikovska et al., 2013). Some research in this domain has pointed out the relevance of identifying which parts of a response are given by the question (Bailey and Meurers, 2008; Mohler et al., 2011), with recent work pointing out that the relevant notion here is that of focus as discussed in formal pragmatics (Meurers et al., 2011; Hahn and Meurers, 2012). Figure 1 provides an example of answer comparison for meaning assessment, where the focus (marked by square brackets) can effectively be used to zoom in on the information that is relevant for comparing a target answer (TA) with a student answer (SA) given a question (Q). Figure 1: Answer comparison with the help of focus This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings"
W14-4922,J14-1009,0,0.0861778,"Annotator 1 correctly excluded “f¨ur” (‘for’) from the focus, only marking “die Bestellung” (‘the appointment’) given that “f¨ur” is only needed for reasons of well-formedness. Annotator 2 apparently thought that “f¨ur” makes a semantic difference here, but it is hard to construct a grammatical example with a different preposition that changes the meaning of the focused expression. 164 4.2 Extrinsic Evaluation It has been pointed out that evaluating an expert annotation of a theoretical linguistic notion only intrinsically is problematic because there is no non-theoretical grounding involved (Riezler, 2014). Therefore, besides calculating agreement measures, we also evaluated the resulting annotation in a larger computational task, the automatic meaning assessment of answers to reading comprehension questions. We used the CoMiC system (Comparing Meaning in Context, Meurers et al., 2011) as a testbed for our experiment. CoMiC is an alignment-based system operating in three stages: 1. Annotating linguistic units (words, chunks and dependencies) in student and target answer on various levels of abstraction 2. Finding alignments of linguistic units between student and target answer based on annotati"
W14-4922,ritz-etal-2008-annotation,0,0.202674,"Missing"
W16-0509,C10-2032,0,0.0281739,"ces provide frequency information in several forms motivated in van Heuven et al. (2014); we made use of the frequencies given on the Zipf scale (log10 of the frequency per billion words), as well as the CD values, for which each film or TV program counted as a context. 4.2 The WeeBit and Common Core Corpora The WeeBit corpus used in a number of readability and text simplification studies (Vajjala and Meurers, 2012; Vajjala and Meurers, 2013; Vajjala and Meurers, 2014) was collected from the educational magazine Weekly Reader used in earlier readability research (Petersen and Ostendorf, 2009; Feng et al., 2010) and the BBC-Bitesize website. As summarized in Table 1, it is a 789,926-word corpus of texts labeled with five grade reading levels. The Common Core corpus consists of exemplar texts from Appendix B of the English Language Arts Standards of the Common Core State Standards. The corpus we use for testing in our experiments is exactly the same as the one used by Nelson et al. (2012). They eliminated the lowest (K–1) level of the original six levels and removed repetition, dra87 Grade Level WR Level 2 WR Level 3 WR Level 4 BiteSize KS 3 BiteSize GCSE Age Group 7–8 8–9 9–10 11–14 14–16 # Articles"
W16-0509,C12-1065,1,0.90761,"Missing"
W16-0509,N07-1058,0,0.104325,"Missing"
W16-0509,P14-5010,0,0.00411829,"l. (2012). They eliminated the lowest (K–1) level of the original six levels and removed repetition, dra87 Grade Level WR Level 2 WR Level 3 WR Level 4 BiteSize KS 3 BiteSize GCSE Age Group 7–8 8–9 9–10 11–14 14–16 # Articles 616 616 616 616 616 # Words / Article 152.63 190.74 294.91 243.56 400.51 Table 1: Details of the WeeBit corpus mas, and texts intended for teacher to read aloud, resulting in 168 remaining passages at five levels. 4.3 Experimental Procedure The following basic procedure was followed for each of the experiments carried out: 1. Tokenize corpus texts with CoreNLP Tokenizer (Manning et al., 2014), which had also been used to compose the SUBTLEX frequency lists. 2. Characterize each text using frequency features. The nature of the features differs across the three studies, for which details are given in the following sections. 3. Train classification models on the WeeBit corpus i) in a 10-fold Cross-Validation (CV) setup or ii) using the full corpus when the Common Core data was used as test. The K-nearest neighbors algorithm of the R package class was used for model construction and testing. 4. Apply the trained model to the test folds or test corpus to assess model performance. 5. Re"
W16-0509,W12-2019,1,0.930466,"ental Setup Before turning to the three experiments carried out, let us introduce the resources and the general procedure used. As source of the frequency and CD1 information, we used the SUBTLEXus (Brysbaert and New, 2009) and the SUBTLEXuk (van Heuven et al., 2014) resources. We ran all experiments with two distinct frequency resources to be able to study the impact of the choice of resource. As corpus for exploring the approach and 10-fold cross validation 1 The CD measure we used is referred to as SUBTLCD in SUBTLEXus and as CD in SUBTLEXuk. testing we used the leveled text corpus WeeBit (Vajjala and Meurers, 2012). For independent crosscorpus testing, we trained on WeeBit and tested on the exemplar texts from Appendix B of the Common Core State Standards (CommonCore, 2010). For machine learning, we used the basic k-nearest neighbor algorithm implemented in the R package class given that in our initial exploration it turned out to perform on a par or better than other commonly used algorithms such as Support Vector Machine or Decision Trees. 4.1 The SUBTLEX Lists The SUBTLEXus (Brysbaert and New, 2009) contains 74,286 word forms with frequency values calculated from a 51-million-word corpus of subtitles"
W16-0509,W13-2907,1,0.855046,"rms with frequency values calculated from a 201.7-millionword corpus of subtitles from nine British TV channels broadcast between January 2010 and December 2012. The SUBTLEX resources provide frequency information in several forms motivated in van Heuven et al. (2014); we made use of the frequencies given on the Zipf scale (log10 of the frequency per billion words), as well as the CD values, for which each film or TV program counted as a context. 4.2 The WeeBit and Common Core Corpora The WeeBit corpus used in a number of readability and text simplification studies (Vajjala and Meurers, 2012; Vajjala and Meurers, 2013; Vajjala and Meurers, 2014) was collected from the educational magazine Weekly Reader used in earlier readability research (Petersen and Ostendorf, 2009; Feng et al., 2010) and the BBC-Bitesize website. As summarized in Table 1, it is a 789,926-word corpus of texts labeled with five grade reading levels. The Common Core corpus consists of exemplar texts from Appendix B of the English Language Arts Standards of the Common Core State Standards. The corpus we use for testing in our experiments is exactly the same as the one used by Nelson et al. (2012). They eliminated the lowest (K–1) level of"
W16-0521,P14-5010,0,0.00619044,"w.bing.com http://docs.oracle.com/javase/7/docs/ api/java/net/HttpURLConnection.html 4 https://code.google.com/p/boilerpipe/ It provides several algorithms for the extraction of the main textual content from different types of web pages. We tested the DefaultExtractor, the ArticleExtractor and the LargestContentExtractor on a development collection of 50 documents, which established DefaultExtractor as the best choice for our task; the other two options extracted too little text in some cases when the main content was divided into several parts. 3. The Parser module employs Stanford CoreNLP5 (Manning et al., 2014) to identify numerous linguistic forms using the syntactic category and dependency information obtained from it. We discuss this step further in the next section. Long sentences are quite frequent in web texts, so we employed the Stanford Shift-Reduce Parser, which is less sensitive to sentence length. The parser has also been reported to outperform the older Stanford constituency parsers. 4. The Ranker is responsible for reranking the top N results based on the statistical analysis of the data received from the previous modules. We chose the classical IR algorithm BM25 (Robertson and Walker,"
W16-0521,W10-1002,1,0.843519,"ance of form and content in the search for appropriate reading material. 1 http://purl.org/icall/flair 188 Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, pages 188–198, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics In terms of envisaged use cases, in the most straightforward case, FLAIR helps the teacher identify reading materials appropriate for a class or individual students in terms of form, content, and reading level. The system can also feed into platforms providing input enhancement such as WERTi (Meurers et al., 2010) or generating exercises from text such as Language MuseSM (Burstein et al., 2012), ensuring that the form targeted by enhancement or exercise generation is as richly represented as possible given the text base used. In scenarios putting more value on learner autonomy or data-driven learning, FLAIR makes it possible to distribute the specification of the form and content criteria between teacher and the learner: The teacher uses their pedagogical background in foreign language teaching and learning and their knowledge of the learner’s abilities and needs to configure FLAIR in a way that priori"
W16-0521,W13-2907,1,\N,Missing
W16-0521,D09-1019,0,\N,Missing
W16-0521,E14-1031,1,\N,Missing
W16-0521,W08-0911,0,\N,Missing
W16-1713,W10-0713,0,0.0294627,"nally, in computational linguistics it has been argued (Riezler, 2014) that annotation of theoretical linguistic notions by experts should be complemented by external grounding, either in the form of extrinsic evaluation, as reported above, or by using crowdsourcing: by formulating the annotation task in such a way that non-experts can understand it and carry it out, one ensures that the task does not depend on implicit knowledge shared only by a team of experts. In this paper, we explore the use of crowdsourcing – which has been shown to work well for a number of linguistic tasks (see, e.g., Finin et al. 2010; Tetreault et al. 2010; Zaidan and CallisonBurch 2011) – for focus annotation. We investigate how systematically the untrained crowd can identify a meaning-based linguistic notion like focus in authentic data and which characteristics of the data and context lead to consistent annotation results. Having established the general feasibility of non-expert focus annotation, we refine the crowdsourcing approach by taking into account the variability within the set of crowd judgements. The approach is based on the idea that sentences with little variation in the annotation provided by the crowd are"
W16-1713,L16-1342,0,0.0204007,"Missing"
W16-1713,ritz-etal-2008-annotation,0,0.0719515,"Missing"
W16-1713,W09-1904,0,0.0652364,"Missing"
W16-1713,D08-1027,0,0.522684,"Missing"
W16-1713,W11-2401,1,0.836925,"trinsically, by relating it to the expertbased gold-standard, and extrinsically, by integrating cost-based focus annotation data in a Short Answer Assessment system. tions written by American learners of German at the university level. The overall corpus includes 164 reading texts, 1,517 reading comprehension questions, 2,057 target answers provided by the teachers, and 36,335 learner answers. Each answer was rated by two annotators with respect to whether it is a correct (appropriate) answer or not. The CREG-5K subset used for the present annotation study is an extended version of CREG-1032 (Meurers et al., 2011), selected using the same criteria after the overall, four year corpus collection effort was completed. The criteria include balancedness (equal number of correct and incorrect answers), a minimum answer length of four tokens, and a language course level at the intermediate level or above. (3) provides an example of a question-answer pair from the CREG corpus. 2 • Question Form encodes the surface form of a question (e.g., WhPhrase, Yes/No or Alternative). (3) Q: Welches Thema wurde am 4. November nicht which topic was on the 4th November not diskutiert? discussed ‘Which topic was not discusse"
W16-1713,W10-1006,0,0.0225247,"onal linguistics it has been argued (Riezler, 2014) that annotation of theoretical linguistic notions by experts should be complemented by external grounding, either in the form of extrinsic evaluation, as reported above, or by using crowdsourcing: by formulating the annotation task in such a way that non-experts can understand it and carry it out, one ensures that the task does not depend on implicit knowledge shared only by a team of experts. In this paper, we explore the use of crowdsourcing – which has been shown to work well for a number of linguistic tasks (see, e.g., Finin et al. 2010; Tetreault et al. 2010; Zaidan and CallisonBurch 2011) – for focus annotation. We investigate how systematically the untrained crowd can identify a meaning-based linguistic notion like focus in authentic data and which characteristics of the data and context lead to consistent annotation results. Having established the general feasibility of non-expert focus annotation, we refine the crowdsourcing approach by taking into account the variability within the set of crowd judgements. The approach is based on the idea that sentences with little variation in the annotation provided by the crowd are more reliably annotate"
W16-1713,P11-1122,0,0.0723857,"Missing"
W16-1713,C14-1145,0,0.0406197,"Missing"
W16-1713,W14-4922,1,0.89661,"cates high quality, though high cost does not necessarily indicate low accuracy but increased variability. Overall, taking Consensus Cost into account improves both intrinsic and extrinsic evaluation measures. 1 Introduction This paper addresses the question of how to explore and evaluate the annotation of information structural concepts to support the analysis of authentic data. While the formal pragmatic concepts 110 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 110–119, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics More recently, Ziai and Meurers (2014) showed that reliable focus annotation is feasible, even for somewhat ill-formed learner language, if one has access to explicit questions and takes them into account in an incremental annotation scheme. They demonstrate the effectiveness of the approach by reporting both substantial inter-annotator agreement and a substantial extrinsic improvement resulting from integration of focus information into a Short Answer Assessment system. However, manual focus annotation by experts is time consuming, both for annotator training and the annotation itself. Additionally, in computational linguistics i"
W16-1713,J14-1009,0,0.117717,"le focus annotation is feasible, even for somewhat ill-formed learner language, if one has access to explicit questions and takes them into account in an incremental annotation scheme. They demonstrate the effectiveness of the approach by reporting both substantial inter-annotator agreement and a substantial extrinsic improvement resulting from integration of focus information into a Short Answer Assessment system. However, manual focus annotation by experts is time consuming, both for annotator training and the annotation itself. Additionally, in computational linguistics it has been argued (Riezler, 2014) that annotation of theoretical linguistic notions by experts should be complemented by external grounding, either in the form of extrinsic evaluation, as reported above, or by using crowdsourcing: by formulating the annotation task in such a way that non-experts can understand it and carry it out, one ensures that the task does not depend on implicit knowledge shared only by a team of experts. In this paper, we explore the use of crowdsourcing – which has been shown to work well for a number of linguistic tasks (see, e.g., Finin et al. 2010; Tetreault et al. 2010; Zaidan and CallisonBurch 201"
W16-4105,K15-1038,0,0.0506239,"large amounts of data on the actual reading performance of target population is difficult and time consuming. One way to tackle this is to develop a hybrid ARA model, which separately models text complexity and user’s language comprehension ability and link them through another model. In this paper, we describe one approach to integrate reader and text characteristics into a single model for automatic readability assessment. Eye-tracking was employed as a method to understand various NLP problems such as annotation task difficulty (Tomanek et al., 2010; Joshi et al., 2013; Joshi et al., 2014; Barrett and Søgaard, 2015), 38 Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity, pages 38–48, Osaka, Japan, December 11-17 2016. translation difficulty (Mishra et al., 2013), and studying reader eye movements using standard corpora (Mart´ınez-G´omez et al., 2012; Matthies and Søgaard, 2013). Cognitive psychologists have for a long time studied eye-movement patterns of readers to understand the cognitive processes in reading and comprehension, and what causes reading difficulty (Just and Carpenter, 1980; Rayner, 1998; Clifton Jr et al., 2007). Studying the eye movements of readers durin"
W16-4105,D15-1047,0,0.0262375,"ance, respectively. While the online models give us a better understanding of the cognitive correlates of reading with text complexity and language proficiency, modeling of the offline measures can be particularly relevant for incorporating user aspects into readability models. 1 Introduction Automatic Readability Assessment (ARA) has been an active area of research in computational linguistics over the past two decades, resulting in a wide range of supervised machine learning models that used both theory driven and data driven features (Petersen, 2007; Feng, 2010; Vajjala and Meurers, 2014b; Jiang et al., 2015, for example). Though the purpose of ARA is to predict text complexity, the eventual goal is ensure that the predictions reflect the comprehension difficulties in the reader. However, so far, ARA models primarily used training corpora that were based on judgements of teachers and other language experts, and not based on the actual reading performance of students, as was also recently criticized by education researchers (Valencia et al., 2014; Williamson et al., 2014; Cunningham and Mesmer, 2014). While this can be considered a shortcoming, obtaining large amounts of data on the actual reading"
W16-4105,N13-1088,0,0.0224832,"be considered a shortcoming, obtaining large amounts of data on the actual reading performance of target population is difficult and time consuming. One way to tackle this is to develop a hybrid ARA model, which separately models text complexity and user’s language comprehension ability and link them through another model. In this paper, we describe one approach to integrate reader and text characteristics into a single model for automatic readability assessment. Eye-tracking was employed as a method to understand various NLP problems such as annotation task difficulty (Tomanek et al., 2010; Joshi et al., 2013; Joshi et al., 2014; Barrett and Søgaard, 2015), 38 Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity, pages 38–48, Osaka, Japan, December 11-17 2016. translation difficulty (Mishra et al., 2013), and studying reader eye movements using standard corpora (Mart´ınez-G´omez et al., 2012; Matthies and Søgaard, 2013). Cognitive psychologists have for a long time studied eye-movement patterns of readers to understand the cognitive processes in reading and comprehension, and what causes reading difficulty (Just and Carpenter, 1980; Rayner, 1998; Clifton Jr et al., 20"
W16-4105,P14-2007,0,0.0173029,"rtcoming, obtaining large amounts of data on the actual reading performance of target population is difficult and time consuming. One way to tackle this is to develop a hybrid ARA model, which separately models text complexity and user’s language comprehension ability and link them through another model. In this paper, we describe one approach to integrate reader and text characteristics into a single model for automatic readability assessment. Eye-tracking was employed as a method to understand various NLP problems such as annotation task difficulty (Tomanek et al., 2010; Joshi et al., 2013; Joshi et al., 2014; Barrett and Søgaard, 2015), 38 Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity, pages 38–48, Osaka, Japan, December 11-17 2016. translation difficulty (Mishra et al., 2013), and studying reader eye movements using standard corpora (Mart´ınez-G´omez et al., 2012; Matthies and Søgaard, 2013). Cognitive psychologists have for a long time studied eye-movement patterns of readers to understand the cognitive processes in reading and comprehension, and what causes reading difficulty (Just and Carpenter, 1980; Rayner, 1998; Clifton Jr et al., 2007). Studying the ey"
W16-4105,C12-1107,0,0.0479197,"Missing"
W16-4105,D13-1075,0,0.0181971,"r, we describe one approach to integrate reader and text characteristics into a single model for automatic readability assessment. Eye-tracking was employed as a method to understand various NLP problems such as annotation task difficulty (Tomanek et al., 2010; Joshi et al., 2013; Joshi et al., 2014; Barrett and Søgaard, 2015), 38 Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity, pages 38–48, Osaka, Japan, December 11-17 2016. translation difficulty (Mishra et al., 2013), and studying reader eye movements using standard corpora (Mart´ınez-G´omez et al., 2012; Matthies and Søgaard, 2013). Cognitive psychologists have for a long time studied eye-movement patterns of readers to understand the cognitive processes in reading and comprehension, and what causes reading difficulty (Just and Carpenter, 1980; Rayner, 1998; Clifton Jr et al., 2007). Studying the eye movements of readers during reading considering both text and reader factors will give us a better understanding about the online link (during reading) between text complexity and reader proficiency. Asking readers to answer questions about the text will give us an understanding about the offline link (after reading) betwee"
W16-4105,P13-2062,0,0.0285305,"els text complexity and user’s language comprehension ability and link them through another model. In this paper, we describe one approach to integrate reader and text characteristics into a single model for automatic readability assessment. Eye-tracking was employed as a method to understand various NLP problems such as annotation task difficulty (Tomanek et al., 2010; Joshi et al., 2013; Joshi et al., 2014; Barrett and Søgaard, 2015), 38 Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity, pages 38–48, Osaka, Japan, December 11-17 2016. translation difficulty (Mishra et al., 2013), and studying reader eye movements using standard corpora (Mart´ınez-G´omez et al., 2012; Matthies and Søgaard, 2013). Cognitive psychologists have for a long time studied eye-movement patterns of readers to understand the cognitive processes in reading and comprehension, and what causes reading difficulty (Just and Carpenter, 1980; Rayner, 1998; Clifton Jr et al., 2007). Studying the eye movements of readers during reading considering both text and reader factors will give us a better understanding about the online link (during reading) between text complexity and reader proficiency. Asking"
W16-4105,D09-1034,0,0.0211103,"stions asked can be accessed in the Appendix of Vajjala (2015). 40 Flesch-Kincaid Grade Level (Kincaid et al., 1975) is a standard readability formula. VM refers to the readability score assigned by the model of Vajjala and Meurers (2014a), which is a regression model based on several lexical and syntactic features, and outputs a score between 1–6, with higher values indicating more difficult texts. Surprisal is a psycholinguistic measure of expected cognitive load during sentence processing, based on information theory. We took the average total surprisal for all sentences from Roark parser (Roark et al., 2009) as a measure of surprisal for each text. Though we modeled different notions of complexity, we only report about the models with the binary complexity from onestopenglish.com in this paper. Procedure: We employed Latin square design for the experiment, making sure each participant read all four texts, alternating between easy and difficult versions. No participant read the same text in two versions. They answered questions on paper after each text and the eye-tracker was re-calibrated for their next reading. Participants were randomly assigned to one of the four experimental conditions, which"
W16-4105,P10-1118,0,0.0317413,"2014). While this can be considered a shortcoming, obtaining large amounts of data on the actual reading performance of target population is difficult and time consuming. One way to tackle this is to develop a hybrid ARA model, which separately models text complexity and user’s language comprehension ability and link them through another model. In this paper, we describe one approach to integrate reader and text characteristics into a single model for automatic readability assessment. Eye-tracking was employed as a method to understand various NLP problems such as annotation task difficulty (Tomanek et al., 2010; Joshi et al., 2013; Joshi et al., 2014; Barrett and Søgaard, 2015), 38 Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity, pages 38–48, Osaka, Japan, December 11-17 2016. translation difficulty (Mishra et al., 2013), and studying reader eye movements using standard corpora (Mart´ınez-G´omez et al., 2012; Matthies and Søgaard, 2013). Cognitive psychologists have for a long time studied eye-movement patterns of readers to understand the cognitive processes in reading and comprehension, and what causes reading difficulty (Just and Carpenter, 1980; Rayner, 1998; C"
W16-4105,W14-1203,1,0.834705,"Easy 3 Difficult 3 Easy 4 Difficult 4 Easy Num. Sentences 12 15 11 14 11 13 12 14 NumWords 296 298 286 234 248 230 312 306 Flesch-Kincaid 14.75 10.09 11.00 6.30 11.10 7.74 13.70 11.08 VM 5.2 3.9 4.2 3.1 4.1 3.0 5.4 4.8 Surprisal 207.5 147.2 193.2 112.3 165.4 124.6 181.9 144.4 Table 1: Number of words in the texts used for the experiment 1 The texts in both versions, c-test and the questions asked can be accessed in the Appendix of Vajjala (2015). 40 Flesch-Kincaid Grade Level (Kincaid et al., 1975) is a standard readability formula. VM refers to the readability score assigned by the model of Vajjala and Meurers (2014a), which is a regression model based on several lexical and syntactic features, and outputs a score between 1–6, with higher values indicating more difficult texts. Surprisal is a psycholinguistic measure of expected cognitive load during sentence processing, based on information theory. We took the average total surprisal for all sentences from Roark parser (Roark et al., 2009) as a measure of surprisal for each text. Though we modeled different notions of complexity, we only report about the models with the binary complexity from onestopenglish.com in this paper. Procedure: We employed Lati"
W16-4113,C12-1065,1,0.867103,"er. Additional functionality, such as allowing users to add their own feature extractors and providing modules supporting machine learning to combine the collected evidence will be added in the near future. We are currently working on populating the system with complexity feature extractors implemented as UIMA AEs by either migrating existing analyzer code as well as reimplementing features reported on in other complexity studies. To validate and exemplify the approach, we plan to replicate the state-of-the-art linguistic complexity analyses for English (Vajjala and Meurers, 2014) and German (Hancke et al., 2012) using CTAP, making the components on which the analyses are based readily available. In making the tool freely available under a standard Creative Commons by-nc-sa licence, we would also like to call for contribution from other researchers. Interested parties are encouraged to join and contribute to the project at https://github.com/ctapweb. Only by making use of joint effort and expertise can we envisage a production level system that can support joint progress in the complexity research community, while at the same time making the analyses readily available to ordinary users seeking to anal"
W17-0302,W16-4113,1,0.890635,"complexity construct consists of a range of sub-constructs at all levels of linguistic modeling, such as lexical, morphological, syntactic, semantic, pragmatic and discourse (Lu, 2010; Lu, 2011; 1 Introduction The analysis of linguistic complexity is a prominent endeavor in Second Language Acquisition (SLA) where Natural Language Processing (NLP) technologies are increasingly applied in a way broadening the empirical foundation. Automatic complexity analysis tools such as CohMetrix (McNamara et al., 2014), the L2 Syntactic Complexity Analyzer (Lu, 2010), and the Common Text Analysis Platform (Chen and Meurers, 2016) support studies analyzing interlanguage development (Lu, 2011; Lu and Ai, 2015; Mazgutova and Kormos, 2015), performance evaluation (Yang et al., 2015; Taguchi et al., 2013), and readability assessment (Vajjala and Meurers, 2012; Nelson et al., 2012). In this paper, we introduce a new system called Syntactic Benchmark (SyB) that utilizes NLP to create syntactic complexity benchmarks This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0 Xiaobin Chen and Detmar Meurers 2017. Challenging learners in thei"
W17-0302,levy-andrew-2006-tregex,0,0.0491026,"gs of the Joint 6th Workshop on NLP for Computer Assisted Language Learning and 2nd Workshop on NLP for Research on Language Acquisition at NoDaLiDa 2017 12 Figure 3: The Search Result Window supporting selection of TL articles based on the learner production’s syntactic complexity level (and user-specified degree of challenge and overall target grade level) 3.2 NLP Processing 3.3 Each article in the Newsela TL reading corpus was processed with an NLP pipeline consisting of a sentence segmenter, a tokenizer and a parser from the Stanford CoreNLP Toolkit library (Manning et al., 2014). Tregex (Levy and Andrew, 2006), a utility for tree pattern matching, was used to extract syntactic units such as coordinate phrases, clauses, and T-units from the parse tree of a sentence. We used the Tregex patterns of Lu’s (2010) L2 Syntactic Complexity Analyzer and calculated the same set of 14 syntactic indices suggested in his study (p. 479, Table 1). This set of syntactic features have also been used in developmental syntactic complexity studies and proved to be valid and reliable (Larsen-Freeman, 1978; Ortega, 2003; Wolfe-Quintero et al., 1998). The SyB system currently uses a replication of Lu’s processing pipeline"
W17-0302,P14-5010,0,0.00419207,"e TL benchmark corpus Proceedings of the Joint 6th Workshop on NLP for Computer Assisted Language Learning and 2nd Workshop on NLP for Research on Language Acquisition at NoDaLiDa 2017 12 Figure 3: The Search Result Window supporting selection of TL articles based on the learner production’s syntactic complexity level (and user-specified degree of challenge and overall target grade level) 3.2 NLP Processing 3.3 Each article in the Newsela TL reading corpus was processed with an NLP pipeline consisting of a sentence segmenter, a tokenizer and a parser from the Stanford CoreNLP Toolkit library (Manning et al., 2014). Tregex (Levy and Andrew, 2006), a utility for tree pattern matching, was used to extract syntactic units such as coordinate phrases, clauses, and T-units from the parse tree of a sentence. We used the Tregex patterns of Lu’s (2010) L2 Syntactic Complexity Analyzer and calculated the same set of 14 syntactic indices suggested in his study (p. 479, Table 1). This set of syntactic features have also been used in developmental syntactic complexity studies and proved to be valid and reliable (Larsen-Freeman, 1978; Ortega, 2003; Wolfe-Quintero et al., 1998). The SyB system currently uses a replica"
W17-0302,W12-2019,1,0.905192,"inguistic complexity is a prominent endeavor in Second Language Acquisition (SLA) where Natural Language Processing (NLP) technologies are increasingly applied in a way broadening the empirical foundation. Automatic complexity analysis tools such as CohMetrix (McNamara et al., 2014), the L2 Syntactic Complexity Analyzer (Lu, 2010), and the Common Text Analysis Platform (Chen and Meurers, 2016) support studies analyzing interlanguage development (Lu, 2011; Lu and Ai, 2015; Mazgutova and Kormos, 2015), performance evaluation (Yang et al., 2015; Taguchi et al., 2013), and readability assessment (Vajjala and Meurers, 2012; Nelson et al., 2012). In this paper, we introduce a new system called Syntactic Benchmark (SyB) that utilizes NLP to create syntactic complexity benchmarks This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0 Xiaobin Chen and Detmar Meurers 2017. Challenging learners in their individual zone of proximal development using pedagogic developmental benchmarks of syntactic complexity. Proceedings of the Joint 6th Workshop on NLP for Computer Assisted Language Learning and 2nd Workshop on NLP for Research"
W17-0305,C92-3145,0,0.716117,"Missing"
W17-0305,W14-5201,0,0.0919423,"Missing"
W17-0305,W11-2401,1,0.873771,"Missing"
W17-0305,S13-2102,1,0.917363,"Missing"
W17-5038,W11-1401,0,0.126105,"te sentences in a text containing the target forms, apply basic constraints to filter out unsuitable sentences (such as those containing unresolvable pronouns), and then generate questions to the remaining ones. Once the target sentence has been selected, it can be used to generate questions targeting particular linguistic forms contained in the sentence. Heilman (2011) discusses the generation of factual, low-level questions suitable for beginner or intermediate students and gives a comprehensive overview of QG methods. Among the most prominent ones are: replacing the target form with a gap (Agarwal et al., 2011; Becker et al., 2012), applying transformation rules (Mitkov et al., 2006), filling templates (Curto et al., 2012), and generating all possible questions to a sentence and ranking them afterwards using a supervised learning algorithm (Heilman and Smith, 2009). Finally, QG is not an exception to the wave of neural networks, and Du et al. (2017) have recently approached automatic generation of reading comprehension questions on that basis. All of the mentioned QG systems either assess vocabulary or target reading comprehension, which contrasts with the focus of our work on functionally supporti"
W17-5038,C16-1107,0,0.120138,"Missing"
W17-5038,P17-1123,0,0.0317483,"ntence. Heilman (2011) discusses the generation of factual, low-level questions suitable for beginner or intermediate students and gives a comprehensive overview of QG methods. Among the most prominent ones are: replacing the target form with a gap (Agarwal et al., 2011; Becker et al., 2012), applying transformation rules (Mitkov et al., 2006), filling templates (Curto et al., 2012), and generating all possible questions to a sentence and ranking them afterwards using a supervised learning algorithm (Heilman and Smith, 2009). Finally, QG is not an exception to the wave of neural networks, and Du et al. (2017) have recently approached automatic generation of reading comprehension questions on that basis. All of the mentioned QG systems either assess vocabulary or target reading comprehension, which contrasts with the focus of our work on functionally supporting focus on form in language learning. Distractor generation is a separate complex task that has received some attention in the QG 3.2 Relevant SLA concepts Attention, input, and form-meaning mapping are key SLA concepts that are directly related to our work. We already saw in our introduction in section 1 that both meaning and form play import"
W17-5038,N12-1092,0,0.100117,"3 3.1 Background Automatic Question Generation A typical text-based Question Generation (QG) system consists of three components: target selection (sentences and words), generation of questions (and answers), and the generation of distractors, which is applicable for a multiple choice answer setup. Most of work on target selection follows a top-down perspective on the text: First, a set of suitable sentences is selected based on different criteria (e.g., Pino et al., 2008; Pil´an et al., 2013). Then the target words or linguistic forms are selected within the set of suitable sentences (e.g., Becker et al., 2012). Given our focus on input enhancement for language learning, we instead pursue a bottom-up approach: Given one or more target linguistic forms (e.g., the passive voice, or the present perfect tense), we automatically select all the candidate sentences in a text containing the target forms, apply basic constraints to filter out unsuitable sentences (such as those containing unresolvable pronouns), and then generate questions to the remaining ones. Once the target sentence has been selected, it can be used to generate questions targeting particular linguistic forms contained in the sentence. He"
W17-5038,H05-1103,0,0.334538,"nes. In the work presented in this paper, we primarily focus on the idea of functionally-driven input enhancement captured by the fourth type: questions drawing the learner’s attention to particular linguistic forms in the reading material and their interpretation. To contex335 community. It supports the provision of answers in a multiple-choice setup, and the choice of distractors is closely tied to what is intended to be assessed by the question. Traditionally, distractors are selected among words that are semantically related to the correct answer (Mitkov et al., 2006; Araki et al., 2016). Brown et al. (2005) select the distractors among the most frequent words that have the same part of speech as the correct answer. Pino and Eskenazi (2009) inform the distractor generation component by the wrong answers provided by the users of their system. Given that we do not focus on the multiple choice answer format here, distractor generation is not discussed further in this paper. tualize our approach, we first provide some background on automatic question generation and the SLA concepts grounding our proposal. 3 3.1 Background Automatic Question Generation A typical text-based Question Generation (QG) sys"
W17-5038,W16-0521,1,0.915584,"ut that is manipulated in particular ways to push learners to become dependent on form and structure to get meaning” (Lee and VanPatten, 1995). Structured input activities can be seen as an umbrella term for a wide range of language teaching techniques. They provide the learners with enriched input and prompt them to process and eventually produce the target linguistic forms. While in the original approach, the input enrichment and development of structured input activities is done manually, CL methods can support this process. We have developed a system for automatic input enrichment, FLAIR (Chinkina and Meurers, 2016), which supports retrieval of documents containing targeted linguistic features. The linguistic features covered by the system include the full set of grammatical constructions spelled out in the official English language curriculum of schools in Baden-W¨urttemberg (Germany). On this enriched input basis, automating the generation of questions as structured input activities is the logical next step. In the next section, we spell out the different types of questions that we are able to generate automatically and discuss the algorithms and challenges behind their generation. 4 The advisory group"
W17-5038,P14-5010,0,0.00384279,"ght in by all accounts turned a scrappy young internet startup into a highly profitable company that brought old-line advertising to a new medium. a. Who turned a scrappy young internet startup into a highly profitable company? Semel and the media executives he . Generation We generate form exposure questions to subjects, objects, and predicates. The main linguistic form we focus on is the grammatical tense, so our form exposure questions target verbs and verb phrases. 337 We use the Java implementation of Stanford CoreNLP 3.7.0 for part-of-speech tagging, parsing, and resolving coreferences (Manning et al., 2014). After extracting a sentence or a clause containing the target form, we perform the following steps: adjust and normalize the auxiliaries, resolve pronouns and other referential expressions, and detect quotation sources, if any. Then the algorithm proceeds to detect specific syntactic components of the sentence, to modify them if necessary, and finally transformation rules are used to turn a sentence into a question. Let us inspect the algorithm for generating questions to predicates. Challenges There is a two-stage process identifying the main syntactic components, POS- and dependency-based,"
W17-5038,W10-1002,1,0.848191,"notice linguistic forms and grammatical categories (Schmidt, 1990) and teaching can facilitate such noticing through so-called focus on form (Doughty and Williams, 1998). Focus on form is designed to draw the learner’s attention to relevant linguistic features of the language as they arise, while keeping the overriding focus on meaning (Long, 1991, pp. 45f). For written language, input enhancement (Sharwood Smith, 1993) has been proposed to make relevant forms more salient in the input, e.g., by coloring or font choices. Such visual input enhancement has also been automated using CL methods (Meurers et al., 2010), as part of a system also generating in-text exercises. One problem with form-based visual input enhancement is that coloring a form or otherwise In Foreign Language Teaching and Learning (FLTL), questions are systematically used to assess the learner’s understanding of a text. Computational linguistic (CL) approaches have been developed to generate such questions automatically given a text (e.g., Heilman, 2011). In this paper, we want to broaden the perspective on the different functions questions can play in FLTL and discuss how automatic question generation can support the different uses."
W17-5038,D10-1032,0,0.0280345,"aff or that cutting staff took place at all? Since the correct answers are known for each template, they can be hard-coded there. As the templates show, a target sentence should always contain a subject and a verb. The particle element is there for the case of particle verbs, and the object elements are optional. Challenges One limitation of the current implementation of grammar-concept questions is that without identifying the specific interpretation of a grammatical tense, we can only specify rather general templates, one or two per grammatical tense. The task of tense sense disambiguation (Reichart and Rappoport, 2010) is very relevant to our work and can facilitate the creation of more fine-grained templates. For example, in case of the past simple tense, one could also ask about the repetitive versus single occurrence of an action in the past; in case of the present perfect continuous tense, a question about the (in)completeness of an action would be plausible. 2. Did he play football in the past? (Yes) 3. Did he play once or often? (Often) One important application of grammar-concept questions is scaffolding feedback. The questions can incrementally guide the learner towards task completion by scaffoldin"
W18-0504,W16-0521,1,0.86441,"rs Universit¨at T¨ubingen Department of Linguistics, ICALL research group∗ LEAD Graduate School & Research Network {mchnkina,aoswal,dm}@sfs.uni-tuebingen.de Abstract provided either by a search engine or by the user. As a result, the most linguistically appropriate texts are prioritized and presented to the user. Input material at the appropriate level is crucial for language acquisition. Automating the search for such material can systematically and efficiently support teachers in their pedagogical practice. This is the goal of the computational linguistic task of automatic input enrichment (Chinkina and Meurers, 2016): It analyzes and re-ranks a collection of texts in order to prioritize those containing target linguistic forms. In the online study described in the paper, we collected 240 responses from English teachers in order to investigate whether they preferred automatic input enrichment over web search when selecting reading material for class. Participants demonstrated a general preference for the material provided by an automatic input enrichment system. It was also rated significantly higher than the texts retrieved by a standard web search engine with regard to the representation of linguistic fo"
W18-0504,N12-1092,0,0.0768262,"Missing"
W18-0504,D10-1032,0,0.0442427,"Missing"
W18-0504,W12-2019,1,0.767143,"tivities targeting certain linguistic forms. Either incidentally drawing learners’ attention to certain vocabulary and grammar or providing exercises targeting those, all of the aforementioned approaches rely on the existence of appropriate reading material with a rich representation of linguistic forms for effective language acquisition. The following section provides information on how language teachers can efficiently search for such material. 3 Automatic Input Enrichment for Language Teaching Automatic provision of reading material for language learners has been guided by text complexity (Vajjala and Meurers, 2012), lexical and grammatical properties (Brown and Eskenazi, 2004; Benn¨ohr, 2007), and the learner’s language proficiency (Collins-Thompson et al., 2011). 1 2 36 www.purl.org/icall/flair www.bing.com Figure 1: Comparison of the top results retrieved by a standard web search engine before and after automatic input enrichment. The 60 search results are plotted along the X axis, and the two target linguistic forms, regular and irregular verbs, are plotted on the Y axis. Microsoft Bing were compared to those provided by the automatic input enrichment system FLAIR. As FLAIR relies on Bing for retriev"
W18-0509,D15-1162,0,0.0128877,"in Section 3.2. The annotated information is used afterwards to enhance syllables and words of the text as can be seen in Appendix A, Figure 5. Further linguistic information for each enhanced word may be obtained individually, see Appendix A, Figure 6. Tool 3.1 System Description We developed COAST as a platform-independent web-based tool that is deployed with Apache on a server hosted on the Amazon Web Services (AWS).5 The front-end was developed with HTML, CSS, JavaScript, and AngularDart. 6 The backend was developed with Python using the frameworks Flask,7 and SQLAlchemy. 8 We use spaCy (Honnibal and Johnson, 2015) for natural language processing (NLP). 3.2 Features Crowd-Sourcing is one of COAST’s most innovative features. We exploit the crowd-knowledge for long-term improvements of our automatic syllabification and word stress analysis. Currently, the crowd is derived from COAST’s active users. To reliably identify not only syllable boundaries but also stress patterns is one of the biggest challenges in automatic syllable enhancement due to limitations of the available linguistic resources. This is especially true for languages other than English and 4 www.celeco.de/ www.aws.amazon.com/ 6 www.angulard"
W18-0509,W14-5709,0,0.0271886,"Missing"
W18-0509,W10-1002,1,0.791802,"anguage Acquisition (SLA). In this regard, insights form SLA research on input enhancement relate to reading and writing acquisition. The well-established Noticing Hypothesis (Schmidt, 1990) states that learning requires the exposure to salient linguistic constructions that may be recognized by the learner. To facilitate this recognition of relevant linguistic constructions, Input Enhancement (Smith, 1993) has been successfully used, in particular in terms of visual enhancement of texts (e.g. colors, font changes, capitalization, spacing), cf. (Rello and Baeza-Yates, 2017; Zorzi et al., 2012; Meurers et al., 2010). In response to this, we developed COAST.1 This paper presents COAST, a web-based application to easily and automatically enhance syllable structure, word stress, and spacing in texts, that was designed in close collaboration with learning therapists to ensure its practical relevance. Such syllable-enhanced texts are commonly used in learning therapy or private tuition to promote the recognition of syllables in order to improve reading and writing skills. In a state of the art solutions for automatic syllable enhancement, we put special emphasis on syllable stress and support specific marking"
W18-0509,N16-1078,0,0.0257912,"Missing"
W18-0513,W14-5201,0,0.0201823,"Missing"
W18-0513,P12-2071,0,0.109703,"Missing"
W18-0513,W17-0305,1,0.712378,"Missing"
W18-0513,P98-2196,0,0.0725406,"Missing"
W18-0513,P16-1174,0,0.0250391,"practical problem: in a class of 30 students, with substantial individual differences warranting individual feedback to students, it is highly challenging for a teacher to provide feedback in class or, in a timely fashion, on homework. ∗ http://icall-research.de 127 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 127–136 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics and obtain immediate feedback from the system. However, while for certain phenomena the feedback is quite explicit and accurate (Settles and Meeder, 2016, p. 1849), cases such as the one in Figure 1 are not handled appropriately. analyzing potentially ill-formed learner language. Since generation is done ahead of time, before learners actually interact with the system, we also avoid the performance bottleneck associated with creating and exploring the full search space at run time. The resulting system can be precise and fast in providing feedback on the grammar concepts in a curriculum underlying a given set of exercises. The paper is organized as follows: Section 2 discusses relevant related work before section 3 introduces our system and se"
W18-7109,W11-2308,0,0.0682146,"Missing"
W18-7109,C12-1065,1,0.835989,"adopted features from SLA research (Feng et al., 2010; Vajjala and Meurers, 2012). Measures of discourse and textual cohesion were also shown to be highly relevant for readability assessment (Crossley et al., 2008, 2011; Feng et al., 2009), as well as psycho-linguistic measures of language use (Chen and Meurers, 2017; Weiss and Meurers, 2018). While most work on readability assessment was conducted for English, the findings have also been corroborated for other languages such as French (Franc¸ois and Fairon, 2012), Italian (Dell’Orletta et al., 2011), and German (Vor der Br¨uck et al., 2008; Hancke et al., 2012; Weiss and Meurers, 2018). These data-driven machine learning approaches Proceedings of the 7th Workshop on NLP for Computer Assisted Language Learning at SLTC 2018 (NLP4CALL 2018) 81 2.3 Functional Illiteracy server client Two major studies have addressed the issue of functional illiteracy in Germany: The lea. - Literalit¨atsentwicklung von Arbeitskr¨aften study (“literacy development for workers”) and the leo. Level-One study.2 They defined degrees of (functional) illiteracy and severely low reading and writing abilities. They define functional illiteracy as reading and writing skills at wh"
W18-7109,levy-andrew-2006-tregex,0,0.0215568,"d on our NLP preprocessing pipeline. In total, 85 construction types are annotated on sentence-, phrase-, or token-level based on part-ofspeech (POS) annotations and constituency trees. On the sentence-level, we extract sentence types (e.g., simple or complex sentences) and question types (e.g., wh-questions). On the phrase-level, subordinate clause types (e.g., relative clauses) are extracted. On the word-level, we annotate properties of verbs, adjectives, nouns, negations, determiners, pronouns and prepositions. We use Tregex to identify patterns in parse trees based on regular expressions (Levy and Andrew, 2006). While FLAIR, too, makes use of Tregex patterns, we newly implemented all patterns to fit the German syntax and POS tags. We excluded constructions that are not relevant for German, such as long and short form adjective comparative conProceedings of the 7th Workshop on NLP for Computer Assisted Language Learning at SLTC 2018 (NLP4CALL 2018) 83 Figure 2: KANSAS’s interface: This view displays the search results for the query Demokratie (“democracy”). On the settings panel on the left, the user can assign weights to linguistic constructions and filter for Alpha readability levels. The preview p"
W18-7109,W10-1002,1,0.789645,"addition to other information retrieval systems that have been designed for the purpose of language acquisition, our work heavily draws on previous work on readability assessment in the context of SLA research, research on the accessibility of reading materials for users with cognitive disabilities, and specifically on German illiteracy research. 2.1 Related Systems The idea of retrieving and making use of authentic web texts for language learning purposes has been investigated in several research approaches. The ICALL systems VIEW and WERTi provide input enhancement techniques for websites (Meurers et al., 2010). They support visually enhancing selected linguistic constructions in order to make them more salient to the learner. Furthermore, they automatically generate fill-in-the-gap exercises for these constructions and embed them into the websites in real-time. Another productive line of research investigates the design of search engines for language learners. The REAP tutoring system (Brown and Eskenazi, 2004) helps selecting appropriate reading material from a digital library data base by matching texts against a student model focusing on vocabulary acquisition. It has also been ported to Portugu"
W18-7109,W12-2019,1,0.820375,"ic studies (Esfahani et al., 2016; Grootens-Wiegers et al., 2015) and in information retrieval systems (cf. Section 2.1). However, readability formulas are known to be highly limited and potentially unreliable as they only capture superficial text properties such as sentence and word length (Feng et al., 2009; Benjamin, 2012). Research on readability assessment thus has shifted towards broader linguistic modeling of syntactic, lexical, and discourse complexity based on elaborate Natural Language Processing (NLP) pipelines and successfully adopted features from SLA research (Feng et al., 2010; Vajjala and Meurers, 2012). Measures of discourse and textual cohesion were also shown to be highly relevant for readability assessment (Crossley et al., 2008, 2011; Feng et al., 2009), as well as psycho-linguistic measures of language use (Chen and Meurers, 2017; Weiss and Meurers, 2018). While most work on readability assessment was conducted for English, the findings have also been corroborated for other languages such as French (Franc¸ois and Fairon, 2012), Italian (Dell’Orletta et al., 2011), and German (Vor der Br¨uck et al., 2008; Hancke et al., 2012; Weiss and Meurers, 2018). These data-driven machine learning"
W18-7109,W13-2907,1,0.832075,"making the use of textbooks particularly questionable. In practice, adult literacy teachers depend on identifying appropriate materials for their classes online using standard content search engines like Google or Bing. However, identifying adequate reading material for readers with lower reading skills is a challenging task: Huenerfauth et al. (2009) and Feng (2009) point out that many texts that are accessible at low literacy levels actually target children and their content may thus be ill-suited for adult readers; texts of interest to adult readers often require higher levels of literacy. Vajjala and Meurers (2013) show that the reading level of web query results obtained using Bing is variable, but on average quite high. Web content specifically designed for readers with low reading skills is not necessarily suited for all learners either, due to the diversity of conditions that result in low literacy We present KANSAS, a search engine designed to retrieve reading materials for functional illiterates and learners of German as a Second Language. The system allows teachers to refine their searches for teaching material by selecting appropriate readability levels and (de)prioritizing linguistic constructi"
W18-7109,C18-1026,1,0.88092,"Missing"
W18-7109,R15-2005,0,0.169637,"(Eraslan et al., 2017; McCarthy and Swierenga, 2010). The article is structured as follows: First, we give some background on related work. In Section 3, we then describe our system’s technical implementation and general workflow. We put a special focus on its two main components: the algorithm for the identification of relevant linguistic constructions and the readability assessment algorithm. We then present the preliminary evaluation of these two algorithms from two pilot studies, which are currently being extended by follow up studies. We conclude with an outlook on future steps. skills (Yaneva, 2015). Our system is designed to support teachers in this challenging task of identifying appropriate material by combining content queries with the flexible (de)prioritization of relevant linguistic constructions and filtering results by readability levels. The system design is based on insights from Second Language Acquisition (SLA) research. Similar to SLA, the acquisition of reading and writing skills, even in the L1, does not happen implicitly through exposure but through explicit instruction. Thus, insights from SLA research are highly relevant for the context of literacy training. The import"
W18-7109,L16-1045,0,0.0662718,"ems. FLAIR integrates grammatical patterns specified in an official English L2 class curriculum into a content-based search engine. The system allows users to rerank search results by assigning weights to linguistic constructions. Furthermore, it visually enhances these constructions in a simple reading view and allows to filter texts for readability based on a readability formula. KANSAS adapts FLAIR to German and focuses primarily on the special needs of functional literacy training. 2.2 to readability modeling are not feasible for these populations due to a lack of (labeled) training data (Yaneva et al., 2016). Although there are corpusbased approaches to comparative readability assessment for low literacy readers (cf., e.g., Feng et al., 2009; Yaneva et al., 2016), eye-tracking studies are more common in research on readability assessment for these groups: Rello et al. investigate the effect of noun frequency and noun length (Rello et al., 2013a) and the effect of number representations (Rello et al., 2013b) on the readability and comprehensibility of texts for Spanish L1 readers with dyslexia. Eraslan et al. (2017) investigate general information extraction strategies of users with high functioni"
W18-7110,W18-0513,1,0.349802,"Missing"
W19-4404,D12-1043,0,0.0757301,"Missing"
W19-4404,C18-1026,1,0.70238,"2) employed in Second Language Acquisition (SLA) research to model different types of language per30 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 30–45 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics and Meurers, 2019). Complexity has also been investigated in relation to (academic) writing proficiency of native speakers (Crossley et al., 2011; McNamara et al., 2010). Research on languages other than English, remains rather limited, with some work on German, Russian, Swedish, Italian, and French (Weiss and Meurers, 2018; Reynolds, 2016; Pil´an et al., 2015; Dell’Orletta et al., 2014; Franc¸ois and Fairon, 2012). formance (McNamara et al., 2010; Vajjala and Meurers, 2012; Bult´e and Housen, 2014). We establish an automatically obtained measure of taskappropriate overall language complexity. With this, we identify texts of more and less appropriate language complexity, which we then manually assess for their accuracy. We use this to experimentally examine teaching experts’ grading behaviour and how it is influenced by accuracy and complexity. Our results show that while teachers seem to successfully identify l"
W19-4404,W19-4440,1,0.841679,"The vertical line marks the mean length. two main research strands on text complexity in our system: measures of the linguistic system and psycho-linguistic measures of language use and cognitive processing. An overview of all features can be found in Table 1. Our procedure is based on our implementation of a broad range of complexity features for German which we have successfully used for the assessment of German readability of media captions for adults and children (Weiss and Meurers, 2018), German L2 proficiency (Weiss, 2017; Weiss and Meurers, in press), and German L1 writing development (Weiss and Meurers, 2019). However, for the research presented here, we altered the segmenter for sentences and tokens. Due to the specific abbreviations for line and page references systematically used in our data, we found that a rule-based segmenter combined with a customized list of abbreviations typical for German Abitur essays outperformed the segmentation by OpenNLP (Bohnet and Nivre, 2012).4 As mentioned earlier, language complexity is an important component of the German curriculum for German arts and literacy (KMK, 2014b). While it lacks a full operationalization of language complexity, it names some example"
W19-4440,W16-4106,0,0.16055,"Missing"
W19-4440,C18-1026,1,0.606375,"r of words in the text as counted by the complexity analysis described in the previous subsection. The feature set measuring writing accuracy and an example feature is included as the last row in Table 1.4 Assessment of Writing Performance To assess writing performance in terms of complexity and accuracy, we operationalized these SLA concepts in terms of several features which we automatically computed or derived from the error annotation of the KCT corpus. 4.1 Complexity The analysis of complexity is based on our implementation of a broad range of complexity features for German (Weiss, 2017; Weiss and Meurers, 2018, in press). The features cover clausal and phrasal syntactic complexity, lexical complexity, discourse complexity, and morphological complexity. Complementing the measures of complexity of the linguistic system, we also compute two cognitively-motivated features: a characterization of language use based on word frequencies, and measures of human language processing (HLP). Table 1 summarizes the features designed to capture the elaborateness and variability in the respective domain, with more details provided in Weiss (2017) and Weiss and Meurers (in press). Overall, the studies in the current"
W19-4440,W19-4404,1,0.865553,"Missing"
W19-6305,W17-0302,1,0.919529,"l., 2009; Benjamin, 2012). In answer to this criticism, more advanced methods supporting broader linguistic modeling using Natural Language Processing (NLP) were established. For example, Vajjala and Meurers (2012) showed that measures of language complexity originally devised in Second Language Acquisition (SLA) research can successfully be adopted to the task of readability classification. An increasing amount of NLPbased research is being dedicated to the assessment of readability for different contexts, in particular for English (Feng et al., 2010; Crossley et al., 2011; Xia et al., 2016; Chen and Meurers, 2017b), with much less work on other languages, such as French, German, Italian, and Swedish (Franc¸ois and Fairon, 2012; Weiss and Meurers, 2018; Dell’Orletta et al., 2011; Pil´an et al., 2015). able for our education purposes, as it does not differentiate degrees of readability within the range of low literacy. In Weiss et al. (2018), we propose a rule-based classification approach for German following the analysis of texts for low literate readers in terms of the so-called Alpha Readability Levels introduced in the next section. As far as we are aware, this currently is the only automatic reada"
W19-6305,P16-4002,1,0.68387,"materials for low levels of literacy (henceforth: alpha sites), and a corpus of curated, high-quality literacy and basic education materials we are currently compiling. The corpus will come with a copyright allowing teachers to adjust and distribute the materials for their classes. We thus considerably extend the original KANSAS Suche system (Weiss et al., 2018), which only supported web search. Different from previous text retrieval systems for language learning, focusing on either web search or compiled text repositories (Heilman et al., 2010; Collins-Thompson et al., 2011; Walmsley, 2015; Chinkina et al., 2016), our approach instantiates a hybrid architecture in the spectrum of potential strategies (Chinkina and Meurers, 2016, Figure 4) by combining the strengths of focused, highquality text databases with large-scale, more or less parameterized web search. The remainder of the article is structured as follows: First, we briefly review research on readability and low literacy and compare previous approaches to text retrieval systems for education contexts (section 2). Then, we describe our system in section 3, before providing a quantitative and qualitative comparison of the three different search m"
W19-6305,W16-0521,1,0.933631,"nd basic education materials we are currently compiling. The corpus will come with a copyright allowing teachers to adjust and distribute the materials for their classes. We thus considerably extend the original KANSAS Suche system (Weiss et al., 2018), which only supported web search. Different from previous text retrieval systems for language learning, focusing on either web search or compiled text repositories (Heilman et al., 2010; Collins-Thompson et al., 2011; Walmsley, 2015; Chinkina et al., 2016), our approach instantiates a hybrid architecture in the spectrum of potential strategies (Chinkina and Meurers, 2016, Figure 4) by combining the strengths of focused, highquality text databases with large-scale, more or less parameterized web search. The remainder of the article is structured as follows: First, we briefly review research on readability and low literacy and compare previous approaches to text retrieval systems for education contexts (section 2). Then, we describe our system in section 3, before providing a quantitative and qualitative comparison of the three different search modes supported by our system in section 4. Section 5 closes with some final remarks on future work. 2 Related Work Te"
W19-6305,W11-2308,0,0.0704576,"Missing"
W19-6305,E09-1027,0,0.0426896,"nd ending with an overview of text retrieval approaches for language learning (section 2.3). 2.1 Readability Assessment Automatic readability assessment matches texts to readers with a certain literacy skill such that they can fulfill a predefined reading goal or task such as extracting information from a text. Early work on readability assessment started with readability formulas (Kincaid et al., 1975; Chall and Dale, 1995) which are still used in some studies (Grootens-Wiegers et al., 2015; Esfahani et al., 2016) despite having been widely criticized for being too simplistic and unreliable (Feng et al., 2009; Benjamin, 2012). In answer to this criticism, more advanced methods supporting broader linguistic modeling using Natural Language Processing (NLP) were established. For example, Vajjala and Meurers (2012) showed that measures of language complexity originally devised in Second Language Acquisition (SLA) research can successfully be adopted to the task of readability classification. An increasing amount of NLPbased research is being dedicated to the assessment of readability for different contexts, in particular for English (Feng et al., 2010; Crossley et al., 2011; Xia et al., 2016; Chen and"
W19-6305,C10-2032,0,0.0305143,"riticized for being too simplistic and unreliable (Feng et al., 2009; Benjamin, 2012). In answer to this criticism, more advanced methods supporting broader linguistic modeling using Natural Language Processing (NLP) were established. For example, Vajjala and Meurers (2012) showed that measures of language complexity originally devised in Second Language Acquisition (SLA) research can successfully be adopted to the task of readability classification. An increasing amount of NLPbased research is being dedicated to the assessment of readability for different contexts, in particular for English (Feng et al., 2010; Crossley et al., 2011; Xia et al., 2016; Chen and Meurers, 2017b), with much less work on other languages, such as French, German, Italian, and Swedish (Franc¸ois and Fairon, 2012; Weiss and Meurers, 2018; Dell’Orletta et al., 2011; Pil´an et al., 2015). able for our education purposes, as it does not differentiate degrees of readability within the range of low literacy. In Weiss et al. (2018), we propose a rule-based classification approach for German following the analysis of texts for low literate readers in terms of the so-called Alpha Readability Levels introduced in the next section. A"
W19-6305,D12-1043,0,0.321961,"Missing"
W19-6305,W08-0910,0,0.0846423,"Missing"
W19-6305,levy-andrew-2006-tregex,0,0.41872,"ncluding web search and corpus search components as well as the option to upload their own corpus. In the case of an unrestricted or filtered web search, the request is communicated to Microsoft Azure’s BING Web Search API (version 5.0)9 and further processed at runtime. The text content of each web page is then retrieved using the Boilerpipe Java API (Kohlschl¨utter et al., 2010).10 We remove links, meta information, and embedded advertisements. The NLP analysis is then performed using the Stanford CoreNLP API (Manning et al., 2014). We identify linguistic constructions with TregEx patterns (Levy and Andrew, 2006) we defined. The linguistic annotation is also used to extract all information for the readability classification. We use the algorithm developed for KANSAS Suche (Weiss et al., 2018), currently the only automatic approach we are aware of for determining readability levels for low literate readers in German. The resulting list of analyzed and readability-classified documents is then returned to the client side. The user can re-rank the results based on the (de-)prioritization of linguistic constructions, filter them by Alpha Readability Level, or use the system’s visualization to inspect the s"
W19-6305,P14-5010,0,0.0106215,"already been done offline) List of rankable documents Figure 1: System workflow including web search and corpus search components as well as the option to upload their own corpus. In the case of an unrestricted or filtered web search, the request is communicated to Microsoft Azure’s BING Web Search API (version 5.0)9 and further processed at runtime. The text content of each web page is then retrieved using the Boilerpipe Java API (Kohlschl¨utter et al., 2010).10 We remove links, meta information, and embedded advertisements. The NLP analysis is then performed using the Stanford CoreNLP API (Manning et al., 2014). We identify linguistic constructions with TregEx patterns (Levy and Andrew, 2006) we defined. The linguistic annotation is also used to extract all information for the readability classification. We use the algorithm developed for KANSAS Suche (Weiss et al., 2018), currently the only automatic approach we are aware of for determining readability levels for low literate readers in German. The resulting list of analyzed and readability-classified documents is then returned to the client side. The user can re-rank the results based on the (de-)prioritization of linguistic constructions, filter"
W19-6305,nilsson-borin-2002-living,0,0.0334811,"based web query and analyze the readability of the retrieved materials on the fly, often by using readability formulas as discussed in section 2.1. The readability level of the results is then displayed to the user as additional criterion for the text choice, the results are ranked according to the level, or a readability filter allows exclusion of hits with undesired readability levels. The majority of these systems are designed for English (Miltsakaki and Troutt, 2007; CollinsThompson et al., 2011; Chinkina et al., 2016), although there are some notable exceptions for a few other languages (Nilsson and Borin, 2002; Walmsley, 2015; Weiss et al., 2018). One of the main advantages of leveled web search engines is that they allow access to a broad bandwidth of texts that are always up-to-date. These are important features for the identification of interesting and relevant reading materials in educational contexts. Beyond the educational domain, leveled web search engines also contribute to web accessibility by allowing web users with low literacy skills to query web sites that are at a suitable reading level for their purposes. One example for such a system for literacy training is the original KANSAS Such"
W19-6305,W12-2019,1,0.916142,"racy skill such that they can fulfill a predefined reading goal or task such as extracting information from a text. Early work on readability assessment started with readability formulas (Kincaid et al., 1975; Chall and Dale, 1995) which are still used in some studies (Grootens-Wiegers et al., 2015; Esfahani et al., 2016) despite having been widely criticized for being too simplistic and unreliable (Feng et al., 2009; Benjamin, 2012). In answer to this criticism, more advanced methods supporting broader linguistic modeling using Natural Language Processing (NLP) were established. For example, Vajjala and Meurers (2012) showed that measures of language complexity originally devised in Second Language Acquisition (SLA) research can successfully be adopted to the task of readability classification. An increasing amount of NLPbased research is being dedicated to the assessment of readability for different contexts, in particular for English (Feng et al., 2010; Crossley et al., 2011; Xia et al., 2016; Chen and Meurers, 2017b), with much less work on other languages, such as French, German, Italian, and Swedish (Franc¸ois and Fairon, 2012; Weiss and Meurers, 2018; Dell’Orletta et al., 2011; Pil´an et al., 2015)."
W19-6305,W13-2907,1,0.892839,"Missing"
W19-6305,C18-1026,1,0.69389,"Processing (NLP) were established. For example, Vajjala and Meurers (2012) showed that measures of language complexity originally devised in Second Language Acquisition (SLA) research can successfully be adopted to the task of readability classification. An increasing amount of NLPbased research is being dedicated to the assessment of readability for different contexts, in particular for English (Feng et al., 2010; Crossley et al., 2011; Xia et al., 2016; Chen and Meurers, 2017b), with much less work on other languages, such as French, German, Italian, and Swedish (Franc¸ois and Fairon, 2012; Weiss and Meurers, 2018; Dell’Orletta et al., 2011; Pil´an et al., 2015). able for our education purposes, as it does not differentiate degrees of readability within the range of low literacy. In Weiss et al. (2018), we propose a rule-based classification approach for German following the analysis of texts for low literate readers in terms of the so-called Alpha Readability Levels introduced in the next section. As far as we are aware, this currently is the only automatic readability classification approach that differentiates degrees of readability at such low literacy levels. 2.2 Automatic approaches to readabilit"
W19-6305,W16-0502,0,0.0123151,"eliable (Feng et al., 2009; Benjamin, 2012). In answer to this criticism, more advanced methods supporting broader linguistic modeling using Natural Language Processing (NLP) were established. For example, Vajjala and Meurers (2012) showed that measures of language complexity originally devised in Second Language Acquisition (SLA) research can successfully be adopted to the task of readability classification. An increasing amount of NLPbased research is being dedicated to the assessment of readability for different contexts, in particular for English (Feng et al., 2010; Crossley et al., 2011; Xia et al., 2016; Chen and Meurers, 2017b), with much less work on other languages, such as French, German, Italian, and Swedish (Franc¸ois and Fairon, 2012; Weiss and Meurers, 2018; Dell’Orletta et al., 2011; Pil´an et al., 2015). able for our education purposes, as it does not differentiate degrees of readability within the range of low literacy. In Weiss et al. (2018), we propose a rule-based classification approach for German following the analysis of texts for low literate readers in terms of the so-called Alpha Readability Levels introduced in the next section. As far as we are aware, this currently is"
W19-6305,Q15-1021,0,0.0262099,"98) and ‘Guidelines for Easy-to-read Materials’ by Nomura et al. (2010). Yaneva (2015) applies this algorithm to web materials labeled as Easy-to-Read to investigate their compliance to the guidelines by Freyhoff et al. (1998). She shows that providers of Easy-to-Read materials overall adhere to the guidelines. This is an important finding since not all self-declared ‘simple’ reading materials on the web actually are suitable for readers with lower reading skills. For example, Simple Wikipedia was found to not be systematically simpler than Wikipedia (see, for ˇ example, Stajner et al., 2012, Xu et al., 2015, and Yaneva et al., 2016b), though Vajjala and Meurers (2014) illustrate that an analysis at the sentence level can identify relative complexity differences. While such research on the adherence of web materials to guidelines is an important contribution to the evaluation of web accessibility, it is less suitCharacterizing Low Literacy Skills According to recent large-scale studies, there is a high proportion of low literate readers in all age groups of the population in Germany (Schr¨oter and Bar-Kochva, 2019). For the German working age population (18–64 years), three major studies further"
W19-6305,R15-2005,0,0.0168708,"e lack of labeled training data for the highly heterogeneous group of adults with low literacy in their native language (Yaneva et al., 2016b). But there is research in this domain bringing in eye-tracking evidence to identify challenges and reading strategies for neuro-atypical readers with low literacy skills, such as people with dyslexia (Rello et al., 2013a,b) or ASD (Yaneva et al., 2016a; Eraslan et al., 2017). Two approaches should be mentioned that overcome the lack of available training data by implementing rules determined in previously developed guidelines for low literacy contexts. Yaneva (2015) presents a binary classification approach to determine the adherence of texts to Easy-to-read guidelines. Easy-to-read guidelines are designed to promote accessibility of reading materials for readers with cognitive disabilities such as ‘Make It Simple’ by Freyhoff et al. (1998) and ‘Guidelines for Easy-to-read Materials’ by Nomura et al. (2010). Yaneva (2015) applies this algorithm to web materials labeled as Easy-to-Read to investigate their compliance to the guidelines by Freyhoff et al. (1998). She shows that providers of Easy-to-Read materials overall adhere to the guidelines. This is an"
W19-6305,L16-1045,0,0.0766822,"propose a rule-based classification approach for German following the analysis of texts for low literate readers in terms of the so-called Alpha Readability Levels introduced in the next section. As far as we are aware, this currently is the only automatic readability classification approach that differentiates degrees of readability at such low literacy levels. 2.2 Automatic approaches to readability assessment at low literacy levels are less common, arguably also due to the lack of labeled training data for the highly heterogeneous group of adults with low literacy in their native language (Yaneva et al., 2016b). But there is research in this domain bringing in eye-tracking evidence to identify challenges and reading strategies for neuro-atypical readers with low literacy skills, such as people with dyslexia (Rello et al., 2013a,b) or ASD (Yaneva et al., 2016a; Eraslan et al., 2017). Two approaches should be mentioned that overcome the lack of available training data by implementing rules determined in previously developed guidelines for low literacy contexts. Yaneva (2015) presents a binary classification approach to determine the adherence of texts to Easy-to-read guidelines. Easy-to-read guideli"
W19-6310,P00-1037,0,0.495178,"ative Research Center 833 Department of Linguistics, ICALL Research Group∗ LEAD Graduate School & Research Network University of T¨ubingen Abstract This paper explores Short Answer Assessment (SAA) for the purpose of giving automatic meaning-oriented feedback in the context of a language tutoring system. In order to investigate the performance of standard SAA approaches on student responses arising in real-life foreign language teaching, we experimented with two different factors: 1) the incorporation of spelling normalization in the form of a task-dependent noisy channel model spell checker (Brill and Moore, 2000) and 2) training schemes, where we explored taskand item-based splits in addition to standard tenfold cross-validation. For evaluation purposes, we compiled a data set of 3,829 student answers across different comprehension task types collected in a German school setting with the English tutoring system FeedBook (Rudzewitz et al., 2017; Ziai et al., 2018) and had an expert score the answers with respect to appropriateness (correct vs. incorrect). Overall, results place the normalization-enhanced SAA system ahead of the standard version and a strong baseline derived from standard text similarit"
W19-6310,S13-2045,0,0.237914,"earch could make progress. 1 Introduction Short Answer Assessment (SAA) is the task of determining whether an answer to a question is correct or not with respect to meaning. The task is ∗ http://icall-research.de This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ also often called Automatic Short Answer Grading (ASAG) in cases where the outcome to determine is on an ordered scale (e.g., a numeric score). After a surge of attention (cf., e.g., Burrows et al., 2015) including shared tasks at SemEval (Dzikovska et al., 2013) and Kaggle1 , the field has quietened down somewhat, with a couple of recent exceptions (Riordan et al., 2017; Gomaa and Fahmy, 2019). However, SAA cannot be considered a solved problem. In particular, it is still unclear how well standard SAA approaches work in real-life educational contexts, for example when integrating language tutoring systems into a regular school setting. In such systems, the goal is to give immediate feedback on the language produced by the learner, e.g., to help students complete homework exercises in the system step by step. For meaning-oriented exercises, such as re"
W19-6310,W17-0305,1,0.87845,"Missing"
W19-6310,W18-0513,1,0.708234,"Missing"
W19-6310,W11-2401,1,0.68075,"nd possibly give helpful feedback on how to improve it in the direction of an acceptable answer. SAA can help with the evaluation part: if an answer is deemed correct, the feedback is positive, if not, further diagnosis can be carried out. The purpose of SAA in this context is thus to help the tutoring system decide whether the feedback to be given needs to be positive or negative. In this paper, we therefore report on SAA work in progress on authentic data from a language tutoring system for 7th grade English currently in use in German schools. We employ an alignment-based SAA system (CoMiC, Meurers et al., 2011a) shown to work well for several data sets where target answers are available (Meurers et al., 2011b; Ott et al., 2013), and use it to train a classifier mimicking a trained language teacher’s 1 https://www.kaggle.com/c/asap-sas judgments on whether a student response is acceptable or not. We investigate two main factors for SAA performance: 1) the impact of automatic spelling normalization on SAA using a noisy channel approach (Brill and Moore, 2000), and 2) the influence of using different training/test splits, namely ‘unseen answers’, ‘unseen items’ (questions), and ‘unseen tasks’, followi"
W19-6310,S13-2102,1,0.828595,"uation part: if an answer is deemed correct, the feedback is positive, if not, further diagnosis can be carried out. The purpose of SAA in this context is thus to help the tutoring system decide whether the feedback to be given needs to be positive or negative. In this paper, we therefore report on SAA work in progress on authentic data from a language tutoring system for 7th grade English currently in use in German schools. We employ an alignment-based SAA system (CoMiC, Meurers et al., 2011a) shown to work well for several data sets where target answers are available (Meurers et al., 2011b; Ott et al., 2013), and use it to train a classifier mimicking a trained language teacher’s 1 https://www.kaggle.com/c/asap-sas judgments on whether a student response is acceptable or not. We investigate two main factors for SAA performance: 1) the impact of automatic spelling normalization on SAA using a noisy channel approach (Brill and Moore, 2000), and 2) the influence of using different training/test splits, namely ‘unseen answers’, ‘unseen items’ (questions), and ‘unseen tasks’, following Dzikovska et al. (2013). Overall, results show that using spelling normalization yields superior performance for the"
W19-6310,W17-5017,0,0.0173841,"wer to a question is correct or not with respect to meaning. The task is ∗ http://icall-research.de This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ also often called Automatic Short Answer Grading (ASAG) in cases where the outcome to determine is on an ordered scale (e.g., a numeric score). After a surge of attention (cf., e.g., Burrows et al., 2015) including shared tasks at SemEval (Dzikovska et al., 2013) and Kaggle1 , the field has quietened down somewhat, with a couple of recent exceptions (Riordan et al., 2017; Gomaa and Fahmy, 2019). However, SAA cannot be considered a solved problem. In particular, it is still unclear how well standard SAA approaches work in real-life educational contexts, for example when integrating language tutoring systems into a regular school setting. In such systems, the goal is to give immediate feedback on the language produced by the learner, e.g., to help students complete homework exercises in the system step by step. For meaning-oriented exercises, such as reading and listening comprehension, this is especially challenging, since the system needs to evaluate the mean"
W19-6310,W18-0507,0,0.0228074,"r of the spelling problems in responses to this task were related to the Welsh proper names introduced by the reading text, such as ‘Gruffudd’ or ‘Llandysul’. These are very hard to spell for 7th grade English learners, but were successfully corrected by our spelling correction approach. Based on this information, we hypothesize that the effect of spelling correction is connected to the lexical material involved in the task rather than its more formal properties. In order to investigate this hypothesis, a systematic analysis of lexical complexity and/or complex word identification (cf., e.g., Yimam et al. 2018) within SAA could be a promising avenue to follow. 5 Conclusion We presented work in progress on Short Answer Assessment (SAA) on data from the FeedBook, an English language tutoring system we employed in a real-life school setting in Germany. The purpose of SAA in this context is to help the tutoring system decide whether the feedback to be given needs to be positive or negative. Figure 2: Reading task ‘2B1’ (abbreviated) To investigate the influence of spelling correction on SAA, we added a noisy channel spelling correction component to a standard SAA approach and found that it generally inc"
W19-6310,N18-1011,1,0.841042,"gap is about the same for ‘unseen items’ and ‘unseen answers’, but greater for ‘unseen tasks’. This suggests that the effect of spelling correction is more pronounced for out-ofdomain training scenarios, which may be due to the fact that the training basis for the spelling correction approach is disjunct from that of the SAA system, and thus does not suffer from generalization problems on this data set. Since these are the first results on this data set, we cannot directly compare them to any previous ones. Looking at recent related work on similar data, we can see that, e.g., the results of Ziai and Meurers (2018) on reading comprehension data in German are in the same ballpark, though slightly higher. We suspect this is the case because that data was more uniform, both with respect to task diversity and the resulting nature of the answers. 4.2.2 Results by Task In order to find out more about the effects of adding spelling correction to the CoMiC model, we analyzed the ‘unseen tasks’ results of ‘CoMiC’ and ‘CoMiC+SC’ on a per-task level. These results are listed in Table 3. The tasks are listed in the same order as in Table 1, namely by descendTask 2B1 3A3a 1CYP2b 1ET5 2CYP3 1B7b 2C5b 1AP37 1AP38 2ET3"
W19-6310,W18-7110,1,0.694634,"Missing"
