2020.acl-main.593,D15-1075,0,0.0320946,"natural language inference (NLI) tasks in English. NLI is a pairwise sentence classification task, where the goal is to predict whether a hypothesis sentence entails, contradicts or is neutral to a premise sentence (Dagan et al., 2005). Below we describe our datasets, our baselines, and our experimental setup. Datasets For text classification, we experiment with the AG news topic identification dataset (Zhang et al., 2015) and two sentiment analysis datasets: IMDB (Maas et al., 2011) and the binary Stanford sentiment treebank (SST; Socher et al., 2013).8 For NLI, we experiment with the SNLI (Bowman et al., 2015) and MultiNLI (MNLI; Williams et al., 2018) datasets. We use the standard train-development-test splits for all datasets except for MNLI, for which there is no public test set. As MNLI contains two validation sets (matched and mismatched), we use the matched validation set as our validation set and the mismatched validation set as our test set. See Table 1 for dataset statistics. Baselines We use two types of baselines: running BERT-large in the standard way, with a single output layer on top of the last layer, and three efficient baselines of increasing size (Figure 2). Each is a fine-tuned B"
2020.acl-main.593,2020.ngt-1.3,0,0.0718724,"Missing"
2020.acl-main.593,N10-1115,0,0.0467783,"uracy. In contrast, our model is overconfident in its prediction of some labels (business for AG, positive for SST), and underconfident in others (tech for AG, entailment for MNLI). These findings might indicate that while our method is designed to be globally calibrated, it is not necessarily calibrated for each label individually. Such observations relate to existing concerns regarding fairness when using calibrated classifiers (Pleiss et al., 2017). 7 Related Work Methods for making inference more efficient have received considerable attention in NLP over the years (Eisner and Satta, 1999; Goldberg and Elhadad, 2010, inter alia). As the field has converged on deep neural architecture solutions, most efforts focus on making models smaller (in terms of model parameters) in order to save space as well as potentially speed up inference. In model distillation (Hinton et al., 2014) a smaller model (the student) is trained to mimic the behavior or structure of the original, larger model (the teacher). The result is typically a student that is as accurate as the teacher, but smaller and faster (Kim and Rush, 2016; Jiao et al., 2019; Tang et al., 2019; Sanh et al., 2019). Pruning (LeCun et al., 1990) removes some"
2020.acl-main.593,N18-2017,1,0.889082,"Missing"
2020.acl-main.593,2020.emnlp-main.21,0,0.0574536,"r k is given as input a weighted sum of all the layers up to and including k, such that the weight is learned during fine-tuning (Peters et al., 2018).7 Calibration Classifiers’ confidence scores are not always reliable (Jiang et al., 2018). One way to mitigate this concern is to use calibration, which encourages the confidence level to correspond to the probability that the model is correct (DeGroot and Fienberg, 1983). In this paper we use temperature calibration, which is a simple technique that has been shown to work well in practice (Guo et al., 2017), in particular for BERT fine-tuning (Desai and Durrett, 2020). The method learns a single parameter, denoted temperature or T , and divides each of the logits {zi } by T before applying the softmax function: exp(zi /T ) pred = arg max P i j exp(zj /T ) We select T to maximize the log-likelihood of the development dataset. Note that temperature calibration is monotonic and thus does not influence predictions. It is only used in our model to make early-exit decisions. Discussion Our approach has several attractive properties. First, if mi is not sufficiently confident in its prediction, we reuse the computation and continue towards mi+1 without recomputin"
2020.acl-main.593,N19-1423,0,0.574981,"an early exit, avoiding the computation associated with successive (higher) layers (grayed out). Otherwise, the model continues to the next layer/classifier. The large increase in the size of artificial intelligence models often increases production costs (Amodei and Hernandez, 2018; Schwartz et al., 2019), and can also limit adoption on real-time devices. Compared to training, which is a one-time large investment, inference costs are incurred for every instance in production, and can thus add up ∗ No No Introduction 1 Prediction significantly. For instance, Microsoft reports that using BERT (Devlin et al., 2019) to process Bing queries requires more than 2,000 GPUs concurrently.2 We present a method to reduce the inference cost of today’s common models in NLP: fine-tuned contextual word representations. Our method exploits variation along two axes: models differ in size and cost, and instances vary in difficulty. Our method assesses the complexity of each test instance and matches it with the most efficient model in our “toolbelt.”3 As a result, some instances, which we refer to in this paper as “easy” or “simple,” can be solved by small models, leading to computational savings, while other instances"
2020.acl-main.593,D16-1139,0,0.0349806,"fficient have received considerable attention in NLP over the years (Eisner and Satta, 1999; Goldberg and Elhadad, 2010, inter alia). As the field has converged on deep neural architecture solutions, most efforts focus on making models smaller (in terms of model parameters) in order to save space as well as potentially speed up inference. In model distillation (Hinton et al., 2014) a smaller model (the student) is trained to mimic the behavior or structure of the original, larger model (the teacher). The result is typically a student that is as accurate as the teacher, but smaller and faster (Kim and Rush, 2016; Jiao et al., 2019; Tang et al., 2019; Sanh et al., 2019). Pruning (LeCun et al., 1990) removes some of the weights in the network, resulting in a smaller, potentially faster network. The basic pruning approach removes individual weights from the network (Swayamdipta et al., 2018; Gale et al., 2019). More sophisticated approaches induce structured sparsity, which removes full blocks (Michel et al., 2019; Voita et al., 2019; Dodge et al., 2019b). Liu et al. (2018) and Fan et al. (2020) pruned deep models by applying dropout to different layers, which allows dynamic control of 6647 Figure 5: In"
2020.acl-main.593,D19-1224,1,0.921999,"1, which is applied to each confidence score to decide whether to exit early. Lower thresholds result in earlier exits, with 0 implying the most efficient classifier is always used. A threshold of 1 always uses the most expensive and accurate classifier. 5 Results A better speed/accuracy tradeoff. Figure 3 presents our test results.10 The blue line shows our model, where each point corresponds to an increasingly large confidence threshold. The leftmost 9 Preliminary experiments with other configurations, including ones with more layers, led to similar results. 10 For increased reproduciblity (Dodge et al., 2019a), we also report validation results in Appendix B. 6643 Layer n Layer n Prediction Layer n Prediction No Layer l Layer l Layer l Layer k Layer k Layer k Is confident? Layer j Layer j Is confident? Layer i Layer i Layer i Layer 0 Layer 0 Layer 0 Input Input Input Yes Early exit prediction No Layer j Prediction (a) Efficient Baseline (b) Standard Baseline Yes Early exit prediction No Is confident? Yes Early exit prediction (c) Our approach Figure 2: Illustration of our baselines. (2a) Efficient baseline: adding a single output layer to an intermediate layer, while not processing the remaining"
2020.acl-main.593,D19-1110,1,0.923288,"1, which is applied to each confidence score to decide whether to exit early. Lower thresholds result in earlier exits, with 0 implying the most efficient classifier is always used. A threshold of 1 always uses the most expensive and accurate classifier. 5 Results A better speed/accuracy tradeoff. Figure 3 presents our test results.10 The blue line shows our model, where each point corresponds to an increasingly large confidence threshold. The leftmost 9 Preliminary experiments with other configurations, including ones with more layers, led to similar results. 10 For increased reproduciblity (Dodge et al., 2019a), we also report validation results in Appendix B. 6643 Layer n Layer n Prediction Layer n Prediction No Layer l Layer l Layer l Layer k Layer k Layer k Is confident? Layer j Layer j Is confident? Layer i Layer i Layer i Layer 0 Layer 0 Layer 0 Input Input Input Yes Early exit prediction No Layer j Prediction (a) Efficient Baseline (b) Standard Baseline Yes Early exit prediction No Is confident? Yes Early exit prediction (c) Our approach Figure 2: Illustration of our baselines. (2a) Efficient baseline: adding a single output layer to an intermediate layer, while not processing the remaining"
2020.acl-main.593,P99-1059,0,0.149133,"and solved with high accuracy. In contrast, our model is overconfident in its prediction of some labels (business for AG, positive for SST), and underconfident in others (tech for AG, entailment for MNLI). These findings might indicate that while our method is designed to be globally calibrated, it is not necessarily calibrated for each label individually. Such observations relate to existing concerns regarding fairness when using calibrated classifiers (Pleiss et al., 2017). 7 Related Work Methods for making inference more efficient have received considerable attention in NLP over the years (Eisner and Satta, 1999; Goldberg and Elhadad, 2010, inter alia). As the field has converged on deep neural architecture solutions, most efforts focus on making models smaller (in terms of model parameters) in order to save space as well as potentially speed up inference. In model distillation (Hinton et al., 2014) a smaller model (the student) is trained to mimic the behavior or structure of the original, larger model (the teacher). The result is typically a student that is as accurate as the teacher, but smaller and faster (Kim and Rush, 2016; Jiao et al., 2019; Tang et al., 2019; Sanh et al., 2019). Pruning (LeCu"
2020.acl-main.593,W18-2501,0,0.0401913,"Missing"
2020.acl-main.593,D18-1153,0,0.0765574,"riginal, larger model (the teacher). The result is typically a student that is as accurate as the teacher, but smaller and faster (Kim and Rush, 2016; Jiao et al., 2019; Tang et al., 2019; Sanh et al., 2019). Pruning (LeCun et al., 1990) removes some of the weights in the network, resulting in a smaller, potentially faster network. The basic pruning approach removes individual weights from the network (Swayamdipta et al., 2018; Gale et al., 2019). More sophisticated approaches induce structured sparsity, which removes full blocks (Michel et al., 2019; Voita et al., 2019; Dodge et al., 2019b). Liu et al. (2018) and Fan et al. (2020) pruned deep models by applying dropout to different layers, which allows dynamic control of 6647 Figure 5: Instances with different labels are predicted with different degrees of confidence. Figure 6: Comparing confidence levels and F1 scores of our most efficient classifier across datasets and labels. High confidence by the model is sometimes explained by “easy” classes that are predicted with high F1 (e.g., sports in AG). Other cases might stem from biases of the model which make it overconfident despite the label being harder than other labels (e.g., positive in SST)."
2020.acl-main.593,2020.acl-main.537,0,0.0823282,"y reducing training and/or inference time (Graves, 2016; Seo et al., 2018). Our method also puts less resources into some of the input, but does so at the document level rather than for individual tokens. A few concurrent works have explored similar ideas for dynamic early exits in the transformer model. Elbayad et al. (2020) and Dabre et al. (2020) introduced early stopping for sequence-tosequence tasks (e.g., machine translation). Bapna et al. (2020) modify the transformer architecture with “control symbols” which determine whether components are short-circuited to optimize budget. Finally, Liu et al. (2020) investigated several inference-time cost optimizations (including early stopping) in a multilingual setting. Several computer vision works explored similar ideas to the one in this paper. Wang et al. (2018) introduced a method for dynamically skipping convolutional layers. Bolukbasi et al. (2017) and Huang et al. (2018) learned early exit policies for computer vision architectures, observing substantial computational gains. 8 Conclusion We presented a method that improves the speed/accuracy tradeoff for inference using pretrained language models. Our method makes early exits for simple instan"
2020.acl-main.593,P11-1015,0,0.0160066,"ts, and the bottom set are NLI datasets. 4 Experiments To test our approach, we experiment with three text classification and two natural language inference (NLI) tasks in English. NLI is a pairwise sentence classification task, where the goal is to predict whether a hypothesis sentence entails, contradicts or is neutral to a premise sentence (Dagan et al., 2005). Below we describe our datasets, our baselines, and our experimental setup. Datasets For text classification, we experiment with the AG news topic identification dataset (Zhang et al., 2015) and two sentiment analysis datasets: IMDB (Maas et al., 2011) and the binary Stanford sentiment treebank (SST; Socher et al., 2013).8 For NLI, we experiment with the SNLI (Bowman et al., 2015) and MultiNLI (MNLI; Williams et al., 2018) datasets. We use the standard train-development-test splits for all datasets except for MNLI, for which there is no public test set. As MNLI contains two validation sets (matched and mismatched), we use the matched validation set as our validation set and the mismatched validation set as our test set. See Table 1 for dataset statistics. Baselines We use two types of baselines: running BERT-large in the standard way, with"
2020.acl-main.593,N18-1202,0,0.0415422,"s function, we sum the losses of all classification layers, such that lower layers are trained to both be useful as feature generators for the higher layers, and as input to their respective classifiers. This also means that every output layer is trained to perform well on all instances. Importantly, we do not perform early exits during training, but only during inference. To encourage monotonicity in performance of the different classifiers, each classifier at layer k is given as input a weighted sum of all the layers up to and including k, such that the weight is learned during fine-tuning (Peters et al., 2018).7 Calibration Classifiers’ confidence scores are not always reliable (Jiang et al., 2018). One way to mitigate this concern is to use calibration, which encourages the confidence level to correspond to the probability that the model is correct (DeGroot and Fienberg, 1983). In this paper we use temperature calibration, which is a simple technique that has been shown to work well in practice (Guo et al., 2017), in particular for BERT fine-tuning (Desai and Durrett, 2020). The method learns a single parameter, denoted temperature or T , and divides each of the logits {zi } by T before applying t"
2020.acl-main.593,S18-2023,0,0.0641465,"Missing"
2020.acl-main.593,P19-1580,0,0.0284801,"o mimic the behavior or structure of the original, larger model (the teacher). The result is typically a student that is as accurate as the teacher, but smaller and faster (Kim and Rush, 2016; Jiao et al., 2019; Tang et al., 2019; Sanh et al., 2019). Pruning (LeCun et al., 1990) removes some of the weights in the network, resulting in a smaller, potentially faster network. The basic pruning approach removes individual weights from the network (Swayamdipta et al., 2018; Gale et al., 2019). More sophisticated approaches induce structured sparsity, which removes full blocks (Michel et al., 2019; Voita et al., 2019; Dodge et al., 2019b). Liu et al. (2018) and Fan et al. (2020) pruned deep models by applying dropout to different layers, which allows dynamic control of 6647 Figure 5: Instances with different labels are predicted with different degrees of confidence. Figure 6: Comparing confidence levels and F1 scores of our most efficient classifier across datasets and labels. High confidence by the model is sometimes explained by “easy” classes that are predicted with high F1 (e.g., sports in AG). Other cases might stem from biases of the model which make it overconfident despite the label being harder t"
2020.acl-main.593,N18-1101,0,0.0507775,"n English. NLI is a pairwise sentence classification task, where the goal is to predict whether a hypothesis sentence entails, contradicts or is neutral to a premise sentence (Dagan et al., 2005). Below we describe our datasets, our baselines, and our experimental setup. Datasets For text classification, we experiment with the AG news topic identification dataset (Zhang et al., 2015) and two sentiment analysis datasets: IMDB (Maas et al., 2011) and the binary Stanford sentiment treebank (SST; Socher et al., 2013).8 For NLI, we experiment with the SNLI (Bowman et al., 2015) and MultiNLI (MNLI; Williams et al., 2018) datasets. We use the standard train-development-test splits for all datasets except for MNLI, for which there is no public test set. As MNLI contains two validation sets (matched and mismatched), we use the matched validation set as our validation set and the mismatched validation set as our test set. See Table 1 for dataset statistics. Baselines We use two types of baselines: running BERT-large in the standard way, with a single output layer on top of the last layer, and three efficient baselines of increasing size (Figure 2). Each is a fine-tuned BERT model with a single output layer after"
2020.acl-main.593,J81-4005,0,0.691775,"Missing"
2020.acl-main.593,D13-1170,0,0.0050123,"pproach, we experiment with three text classification and two natural language inference (NLI) tasks in English. NLI is a pairwise sentence classification task, where the goal is to predict whether a hypothesis sentence entails, contradicts or is neutral to a premise sentence (Dagan et al., 2005). Below we describe our datasets, our baselines, and our experimental setup. Datasets For text classification, we experiment with the AG news topic identification dataset (Zhang et al., 2015) and two sentiment analysis datasets: IMDB (Maas et al., 2011) and the binary Stanford sentiment treebank (SST; Socher et al., 2013).8 For NLI, we experiment with the SNLI (Bowman et al., 2015) and MultiNLI (MNLI; Williams et al., 2018) datasets. We use the standard train-development-test splits for all datasets except for MNLI, for which there is no public test set. As MNLI contains two validation sets (matched and mismatched), we use the matched validation set as our validation set and the mismatched validation set as our test set. See Table 1 for dataset statistics. Baselines We use two types of baselines: running BERT-large in the standard way, with a single output layer on top of the last layer, and three efficient ba"
2020.acl-main.626,P13-1023,0,0.017291,"annotations, exhibited in the Dense dataset (§2) as well as the output of the Fitzgerald et al. (2018) parser (§5). To that end, we ignore redundant true-positives, and collapse false-positive errors (see Appendix for details). 4 Dataset Quality Analysis Inter-Annotator Agreement (IAA) To estimate dataset consistency across different annotations, we measure F1 using our UA metric. 10 individual worker-vs-worker experiments yield 79.8 F1 agreement over 150 predicates, indicating high consistency across our annotators, in line with agreement rates in other structured semantic annotations, e.g. Abend and Rappoport (2013). Overall consistency of the dataset is assessed by measuring agreement between different consolidated annotations, obtained by disjoint triplets of workers, which achieves F1 of 84.1, averaged over 4 experiments, 35 predicates each. Notably, consolidation boosts agreement, indicating its necessity. For LA agreement, averaged F1 was 67.8; however, it is likely that the drop from UA is mainly due to falsely rejecting semantically equivalent questions under the S TRICT-M ATCH criterion, given that we found equal LA and UA scores in a manual evaluation of our dataset (see Table 4 below). Dataset"
2020.acl-main.626,P98-1013,0,0.0994746,"t an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded highquality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations. 1 Introduction Semantic Role Labeling (SRL) provides explicit annotation of predicate-argument relations. Common SRL schemes, particularly PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), rely on predefined role inventories and extensive predicate lexicons. Consequently, SRL annotation of new texts requires substantial efforts involving expert annotation, and possibly lexicon extension, limiting scalability. Aiming to address these limitations, QuestionAnswer driven Semantic Role Labeling (QA-SRL) (He et al., 2015) labels each predicate-argument relationship with a question-answer pair, where natural language questions represent semantic roles, and answers correspond to arguments (see Table 1). This approach follows the colloquial perception of semantic roles as answering que"
2020.acl-main.626,P19-1409,1,0.751829,"t set, including valuable implicit semantic arguments not manifested in syntactic structure (highlighted in Table 1). The importance of implicit arguments has been recognized in the literature (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), yet they are mostly overlooked by common SRL formalisms and tools. Overall, QA-SRL largely subsumes predicateargument information captured by traditional SRL schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achieve better implicit neural NLU models, as done recently by He et al. (2020), as well as an explicit semantic structure for downstream use, e.g. for producing Open Information Extraction propositions (Stanovsky and Dagan, 2016).1 1 Indeed, making direct use of QA-SRL role questions might seem more challenging than with categorical semantic roles, as in traditional SRL. In practice, however, when a model em"
2020.acl-main.626,N18-1076,0,0.0125319,"e.g., “Who” corresponding to the agent role). QA-SRL carries two attractive promises. First, using a question-answer format makes the annotation task intuitive and easily attainable by laymen, as it does not depend on linguistic resources (e.g. role lexicons), thus facilitating greater annotation scalability. Second, by relying on intuitive human comprehension, these annotations elicit a richer argument set, including valuable implicit semantic arguments not manifested in syntactic structure (highlighted in Table 1). The importance of implicit arguments has been recognized in the literature (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), yet they are mostly overlooked by common SRL formalisms and tools. Overall, QA-SRL largely subsumes predicateargument information captured by traditional SRL schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achie"
2020.acl-main.626,I17-1010,0,0.0131356,"nding to the agent role). QA-SRL carries two attractive promises. First, using a question-answer format makes the annotation task intuitive and easily attainable by laymen, as it does not depend on linguistic resources (e.g. role lexicons), thus facilitating greater annotation scalability. Second, by relying on intuitive human comprehension, these annotations elicit a richer argument set, including valuable implicit semantic arguments not manifested in syntactic structure (highlighted in Table 1). The importance of implicit arguments has been recognized in the literature (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), yet they are mostly overlooked by common SRL formalisms and tools. Overall, QA-SRL largely subsumes predicateargument information captured by traditional SRL schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achieve better implici"
2020.acl-main.626,P18-1191,1,0.943217,"20. 2020 Association for Computational Linguistics WH Why Why Who Around 47 people could be arrested, including the councillor. (1) Who might be arrested? 47 people |the councillor Perry called for the DAs resignation, and when she did not resign, cut funding to a program she ran. (2) Why was something cut by someone? she did not resign (3) Who cut something? Perry Table 1: QA-SRL examples. The bar (|) separates multiple answers. Implicit arguments are highlighted. Previous attempts to annotate QA-SRL initially involved trained annotators (He et al., 2015) but later resorted to crowdsourcing (Fitzgerald et al., 2018) for scalability. Naturally, employing crowd workers is challenging when annotating fairly demanding structures like SRL. As Fitzgerald et al. (2018) acknowledge, the main shortage of their large-scale dataset is limited recall, which we estimate to be in the lower 70s (see §4). Unfortunately, such low recall in gold standard datasets hinders proper research and evaluation, undermining the current viability of the QA-SRL paradigm. Aiming to enable future QA-SRL research, we present a generic controlled crowdsourcing annotation protocol and apply it to QA-SRL. Our process addresses worker quali"
2020.acl-main.626,D15-1076,1,0.934655,"rd will facilitate future replicable research of natural semantic annotations. 1 Introduction Semantic Role Labeling (SRL) provides explicit annotation of predicate-argument relations. Common SRL schemes, particularly PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), rely on predefined role inventories and extensive predicate lexicons. Consequently, SRL annotation of new texts requires substantial efforts involving expert annotation, and possibly lexicon extension, limiting scalability. Aiming to address these limitations, QuestionAnswer driven Semantic Role Labeling (QA-SRL) (He et al., 2015) labels each predicate-argument relationship with a question-answer pair, where natural language questions represent semantic roles, and answers correspond to arguments (see Table 1). This approach follows the colloquial perception of semantic roles as answering questions about the predicate (“Who did What to Whom, When, Where and How”, with, e.g., “Who” corresponding to the agent role). QA-SRL carries two attractive promises. First, using a question-answer format makes the annotation task intuitive and easily attainable by laymen, as it does not depend on linguistic resources (e.g. role lexic"
2020.acl-main.626,J05-1004,0,0.177821,"valuation. In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded highquality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations. 1 Introduction Semantic Role Labeling (SRL) provides explicit annotation of predicate-argument relations. Common SRL schemes, particularly PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), rely on predefined role inventories and extensive predicate lexicons. Consequently, SRL annotation of new texts requires substantial efforts involving expert annotation, and possibly lexicon extension, limiting scalability. Aiming to address these limitations, QuestionAnswer driven Semantic Role Labeling (QA-SRL) (He et al., 2015) labels each predicate-argument relationship with a question-answer pair, where natural language questions represent semantic roles, and answers correspond to arguments (see Table 1). This approach follows the colloquial perception"
2020.acl-main.626,D16-1252,1,0.870589,"L schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achieve better implicit neural NLU models, as done recently by He et al. (2020), as well as an explicit semantic structure for downstream use, e.g. for producing Open Information Extraction propositions (Stanovsky and Dagan, 2016).1 1 Indeed, making direct use of QA-SRL role questions might seem more challenging than with categorical semantic roles, as in traditional SRL. In practice, however, when a model embeds QA-SRL questions in context, we would expect 7008 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7008–7013 c July 5 - 10, 2020. 2020 Association for Computational Linguistics WH Why Why Who Around 47 people could be arrested, including the councillor. (1) Who might be arrested? 47 people |the councillor Perry called for the DAs resignation, and when she did not r"
2020.acl-main.626,P15-2115,0,0.0277604,"ehension, these annotations elicit a richer argument set, including valuable implicit semantic arguments not manifested in syntactic structure (highlighted in Table 1). The importance of implicit arguments has been recognized in the literature (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), yet they are mostly overlooked by common SRL formalisms and tools. Overall, QA-SRL largely subsumes predicateargument information captured by traditional SRL schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achieve better implicit neural NLU models, as done recently by He et al. (2020), as well as an explicit semantic structure for downstream use, e.g. for producing Open Information Extraction propositions (Stanovsky and Dagan, 2016).1 1 Indeed, making direct use of QA-SRL role questions might seem more challenging than with categorical semantic roles, as in t"
2020.acl-main.626,J12-4003,0,0.0287838,"t role). QA-SRL carries two attractive promises. First, using a question-answer format makes the annotation task intuitive and easily attainable by laymen, as it does not depend on linguistic resources (e.g. role lexicons), thus facilitating greater annotation scalability. Second, by relying on intuitive human comprehension, these annotations elicit a richer argument set, including valuable implicit semantic arguments not manifested in syntactic structure (highlighted in Table 1). The importance of implicit arguments has been recognized in the literature (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), yet they are mostly overlooked by common SRL formalisms and tools. Overall, QA-SRL largely subsumes predicateargument information captured by traditional SRL schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achieve better implicit neural NLU models, as"
2020.acl-main.626,2020.acl-main.772,0,0.0935617,"stly overlooked by common SRL formalisms and tools. Overall, QA-SRL largely subsumes predicateargument information captured by traditional SRL schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achieve better implicit neural NLU models, as done recently by He et al. (2020), as well as an explicit semantic structure for downstream use, e.g. for producing Open Information Extraction propositions (Stanovsky and Dagan, 2016).1 1 Indeed, making direct use of QA-SRL role questions might seem more challenging than with categorical semantic roles, as in traditional SRL. In practice, however, when a model embeds QA-SRL questions in context, we would expect 7008 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7008–7013 c July 5 - 10, 2020. 2020 Association for Computational Linguistics WH Why Why Who Around 47 people could b"
2020.acl-main.626,C98-1013,0,\N,Missing
2020.acl-main.738,W09-1901,0,0.554675,"ion P (y = ant(i)) over the set of all candidate antecedents Y(i) = {K previous mentions in the document} ∪ {}, where  is a dummy antecedent signifying that span i has no antecedent. This model does not require additional resources, such as syntactic dependencies or named entity recognition, and is thus well-suited for active learning scenarios for low-resource domains. Sample selection algorithm Previous approaches for the annotation of coreference resolution have used mostly pairwise selection, where pairs of mentions are shown to a human annotator who marks whether they are co-referring (Gasperin, 2009; Laws et al., 2012; Zhao and Ng, 2014; Sachan et al., 2015). To incorporate these binary annotations into their clustering coreference model, Sachan et al. (2015) introduced the notion of must-link and cannot-link penalties, which we describe and extend in Section 4. 3 Discrete Annotation In discrete annotation, as exemplified in Figure 1, we present the annotator with a document where the least certain span i (“Po-po”, in the example) and i’s model-predicted antecedent, A(i) (“locals”), are highlighted. Similarly to pairwise annotation, annotators are first asked whether i and A(i) are coref"
2020.acl-main.738,P19-1164,1,0.810293,"ocument, a span (yellow), and the span’s predicted antecedent (blue). In case the answer to the coreference question is negative (i.e., the spans are not coreferring), we present a follow-up question (“what is the first appearance of the entity?”), providing additional cost-effective signal. Our annotation interface can be seen in Figure 5 in the Appendix. Introduction Coreference resolution is the task of resolving anaphoric expressions to their antecedents (see Figure 1). It is often required in downstream applications such as question answering (Dasigi et al., 2019) or machine translation (Stanovsky et al., 2019). Exhaustively annotating coreference is an expensive process as it requires tracking coreference chains across long passages of text. In news stories, for example, important entities may be referenced many paragraphs after their introduction. Active learning is a technique which aims to reduce costs by annotating samples which will be most beneficial for the learning process, rather than fully labeling a large fixed training set. Active learning consists of two components: (1) a taskspecific learning algorithm, and (2) an iterative sample selection algorithm, which examines the performance of"
2020.acl-main.738,W14-1104,0,0.181252,"all candidate antecedents Y(i) = {K previous mentions in the document} ∪ {}, where  is a dummy antecedent signifying that span i has no antecedent. This model does not require additional resources, such as syntactic dependencies or named entity recognition, and is thus well-suited for active learning scenarios for low-resource domains. Sample selection algorithm Previous approaches for the annotation of coreference resolution have used mostly pairwise selection, where pairs of mentions are shown to a human annotator who marks whether they are co-referring (Gasperin, 2009; Laws et al., 2012; Zhao and Ng, 2014; Sachan et al., 2015). To incorporate these binary annotations into their clustering coreference model, Sachan et al. (2015) introduced the notion of must-link and cannot-link penalties, which we describe and extend in Section 4. 3 Discrete Annotation In discrete annotation, as exemplified in Figure 1, we present the annotator with a document where the least certain span i (“Po-po”, in the example) and i’s model-predicted antecedent, A(i) (“locals”), are highlighted. Similarly to pairwise annotation, annotators are first asked whether i and A(i) are coreferent. If they answer positively, we m"
2020.acl-main.738,W12-2409,0,0.32051,"Missing"
2020.acl-main.738,D17-1018,1,0.952354,"duct experiments across several sample selection algorithms using existing gold data for user labels and show that both of our contributions significantly improve performance on the CoNLL2012 dataset (Pradhan et al., 2012). Overall, our active learning method presents a superior alternative to pairwise annotation for coreference resolution, achieving better performing models for a given annotation budget. 2 Background Our work relies on two main components: a coreference resolution model and a sample selection algorithm. Coreference resolution model We use the span ranking model introduced by Lee et al. (2017), and later implemented in AllenNLP framework (Gardner et al., 2018). This model computes span embeddings for all possible spans i in a document, and uses them to compute a probability distribution P (y = ant(i)) over the set of all candidate antecedents Y(i) = {K previous mentions in the document} ∪ {}, where  is a dummy antecedent signifying that span i has no antecedent. This model does not require additional resources, such as syntactic dependencies or named entity recognition, and is thus well-suited for active learning scenarios for low-resource domains. Sample selection algorithm Prev"
2020.acl-main.738,W12-4501,0,0.518253,"del predictions 8320 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8320–8331 c July 5 - 10, 2020. 2020 Association for Computational Linguistics for all antecedents which belong to the same cluster. This avoids repeated labeling that would come with separately verifying every mention pair within the same cluster, as done in previous methods. We conduct experiments across several sample selection algorithms using existing gold data for user labels and show that both of our contributions significantly improve performance on the CoNLL2012 dataset (Pradhan et al., 2012). Overall, our active learning method presents a superior alternative to pairwise annotation for coreference resolution, achieving better performing models for a given annotation budget. 2 Background Our work relies on two main components: a coreference resolution model and a sample selection algorithm. Coreference resolution model We use the span ranking model introduced by Lee et al. (2017), and later implemented in AllenNLP framework (Gardner et al., 2018). This model computes span embeddings for all possible spans i in a document, and uses them to compute a probability distribution P (y ="
2020.acl-main.738,N13-1014,0,\N,Missing
2020.acl-main.738,W18-2501,1,\N,Missing
2020.emnlp-main.528,W05-0909,0,0.254165,"wide range of RC phenomena such as commonsense reasoning and understanding narrative over movie scripts. After collecting all annotations, we follow work on creating more robust evaluation sets (Kaushik et al., 2020; Gardner et al., 2020) and augment the test set of MOCHA by manually writing a small set of minimal pairs (Table 3). The set of minimal pairs serve as a harder evaluation set for probing metric robustness. Using MOCHA, we train a Learned Metric for Reading Comprehension which we abbreviate as LERC. We compare LERC against two sets of baselines: (1) existing metrics such as METEOR (Banerjee and Lavie, 2005) and BERTScore (Zhang et al., 2019); and (2) a sentence similarity model trained on STS-B (Cer et al., 2017). To ensure fair comparison, we evaluate LERC in an out-of-dataset setting: LERC is trained on all datasets except the one it is being evaluated on. On the test set, LERC outperforms baselines by as much as 36 Pearson correlation points and on the minimal pairs set, by as much as 26 accuracy points. Error analysis and minimal pair results indicate that there is substantial room to improve the robustness of LERC and its sensitivity to different linguistic phenomena. We hope that MOCHA and"
2020.emnlp-main.528,D18-1454,0,0.030811,"tion for doing this is that the number of generative QA datasets is quite small, which we attribute to the quality of evaluation metrics. The main focus of this work is in developing and evaluating metrics for generative RC. However, we wanted to see whether a learned metric could do well on span-selection datasets. We collected Collecting Candidates Candidates on all four generative datasets are generated using backtranslation (Sennrich et al., 2016) and using a fine-tuned GPT-2 model (Radford et al., 2019). We also generate candidates for NarrativeQA and MCScript using a trained MHPG model (Bauer et al., 2018). We tried using MHPG for CosmosQA and SocialIQA but candidates were of poor quality. Unique to NarrativeQA, each question has two references. We treat the second reference as a candidate to be annotated if it has low n-gram overlap with the first reference. We use a span-selection BERT-based model to generate candidates for Quoref and NAQANET (Dua et al., 2019) and NABERT2 models for DROP. Models are trained on the training sets of each constituent dataset and candidates are produced on instances from the validation set (and test set if available). We filtered out candidates that exactly matc"
2020.emnlp-main.528,W17-4755,0,0.0500144,"Missing"
2020.emnlp-main.528,W16-2302,0,0.0600135,"Missing"
2020.emnlp-main.528,S17-2001,0,0.0817922,"Missing"
2020.emnlp-main.528,D19-5817,1,0.872004,"is a challenging task in and of itself, which can lead to models that exploit spurious correlations (Jia and Liang, 2017; Min et al., 2019; Geva et al., 2019). Posing RC as a generation task addresses the aforementioned issues. Generative RC does not require distractors, circumventing biases that could be introduced by them, and allows arbitrary questions and answers. Unfortunately, existing metrics for evaluating text generation come with significant shortcomings. Many metrics score n-gram overlap, and it is well established that using token overlap as a measure of similarity has drawbacks (Chen et al., 2019; Edunov et al., 2019; Wang et al., 2020). Cur6521 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6521–6532, c November 16–20, 2020. 2020 Association for Computational Linguistics rent metrics also only consider the reference and are agnostic to the end-task being evaluated. Fig. 1 demonstrates that this is problematic for generative RC because scoring a candidate may require a metric to also consider the passage and the question. Without cheap and reliable evaluation, progress in generative reading comprehension has been extremely slow. To addres"
2020.emnlp-main.528,N19-1300,0,0.196703,"with a MSE loss. yˆi = W hi [CLS] lossi = (yi − yˆi )2 3.2 We pre-train BERT via 3-way classification to predict whether: a1 is the correct answer, a2 is the correct answer, or a1 and a2 are both correct. MultiRC has multiple correct answers per question and we create additional instances where both a1 and a2 are correct by duplicating the correct answer for all three datasets. Pre-Training the Learned Metric Learning the interactions between the input components can be difficult with only human judgement fine-tuning. To overcome this, we pre-train on four multiple-choice QA datasets: BoolQ (Clark et al., 2019a), MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), and MultiRC (Khashabi Experiments Training LERC: We use the PyTorch (Paszke et al., 2019), HuggingFace Transformers (Wolf et al., 2019), and AllenNLP (Gardner et al., 2017) libraries to implement LERC. We pre-train LERC before fine-tuning on MOCHA. We evaluate LERC in two settings, an out-of-dataset (OOD) setting and an all-datasets (AD) setting. In the OOD setting, we train and tune LERC on all datasets in MOCHA except the dataset we are evaluating on. This reflects the use case where we want to apply LERC to evaluate a new datase"
2020.emnlp-main.528,P19-1264,0,0.108636,"with a MSE loss. yˆi = W hi [CLS] lossi = (yi − yˆi )2 3.2 We pre-train BERT via 3-way classification to predict whether: a1 is the correct answer, a2 is the correct answer, or a1 and a2 are both correct. MultiRC has multiple correct answers per question and we create additional instances where both a1 and a2 are correct by duplicating the correct answer for all three datasets. Pre-Training the Learned Metric Learning the interactions between the input components can be difficult with only human judgement fine-tuning. To overcome this, we pre-train on four multiple-choice QA datasets: BoolQ (Clark et al., 2019a), MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), and MultiRC (Khashabi Experiments Training LERC: We use the PyTorch (Paszke et al., 2019), HuggingFace Transformers (Wolf et al., 2019), and AllenNLP (Gardner et al., 2017) libraries to implement LERC. We pre-train LERC before fine-tuning on MOCHA. We evaluate LERC in two settings, an out-of-dataset (OOD) setting and an all-datasets (AD) setting. In the OOD setting, we train and tune LERC on all datasets in MOCHA except the dataset we are evaluating on. This reflects the use case where we want to apply LERC to evaluate a new datase"
2020.emnlp-main.528,D19-1606,1,0.893089,"Missing"
2020.emnlp-main.528,N19-1423,0,0.0477598,"e human judgement scores, s1 and s2 , collected using the same interface in Fig. 2. The minimal pair is created so that c1 has a higher score (i.e. is a better answer) than c2 . Each minimal pair is designed to capture a particular linguistic phenomenon (see Table 3). Using this set of minimal pairs, we can study how often a metric prefers the better candidate. We create 200 minimal pairs (50 for each generative QA dataset), which we use for evaluation separately from the original test set. 3 A Learned Metric We provide details on LERC, our learned metric. LERC is initialized using BERT-base (Devlin et al., 2019) We define as input a tuple consisting of a passage, p, a question, q, a reference answer, a, and a candidate answer, ˆa. The input to BERT is 6525 Metric NarrativeQA Dev Test MCScript Dev Test CosmosQA Dev Test SocialIQA Dev Test DROP Dev Test Quoref Dev Test Avg. r Dev Test BLEU-1 METEOR ROUGE-L BERTScore 0.403 0.605 0.434 0.419 0.472 0.615 0.495 0.534 0.181 0.461 0.224 0.172 0.260 0.502 0.297 0.194 0.660 0.696 0.701 0.803 0.670 0.711 0.701 0.779 0.595 0.644 0.599 0.604 0.549 0.637 0.558 0.584 0.409 0.664 0.480 0.174 0.387 0.568 0.366 0.328 0.674 0.729 0.712 0.207 0.578 0.716 0.604 0.286 0.4"
2020.emnlp-main.528,N19-1246,1,0.915534,"t are diverse in their domains and answer types. This ensures that training and evaluation with MOCHA does not overfit to the characteristics of any constituent dataset. NarrativeQA (Kocisk´y et al., 2017) tests reasoning about events, entities, and their relations on movie scripts and book summaries. MCScript (Ostermann et al., 2018) tests reasoning on stories written for a child-level reader. CosmosQA (Huang et al., 2019) tests commonsense reasoning on blogs describing everyday events. SocialIQA (Sap et al., 2019) tests social reasoning with passages constructed from a knowledge base. DROP (Dua et al., 2019) tests predicate argument structure and numerical reasoning on Wikipedia articles concerning American football games, census results, and history. Quoref (Dasigi et al., 2019) tests coreferential reasoning on Wikipedia articles. NarrativeQA was created as a generative RC dataset. CosmosQA, MCScript, and SocialIQA were created as MC datasets which we re-purpose as generative datasets by using the correct choice as the reference. Our motivation for doing this is that the number of generative QA datasets is quite small, which we attribute to the quality of evaluation metrics. The main focus of th"
2020.emnlp-main.528,2020.emnlp-main.751,0,0.0731735,"Missing"
2020.emnlp-main.528,D19-1107,0,0.0223833,"e (MC) format. Both formats are easy to evaluate, ∗ Work done while at the Allen Institute for AI and the University of Washington. 1 The dataset, code, a leaderboard, and a demo are available at https://allennlp.org/mocha. but in return, have restrictions placed on the questions that can be asked or the answers that can be returned. Furthermore, both formats hinge on distractor spans/choices for learning to be effective. Ensuring high quality distractors is a challenging task in and of itself, which can lead to models that exploit spurious correlations (Jia and Liang, 2017; Min et al., 2019; Geva et al., 2019). Posing RC as a generation task addresses the aforementioned issues. Generative RC does not require distractors, circumventing biases that could be introduced by them, and allows arbitrary questions and answers. Unfortunately, existing metrics for evaluating text generation come with significant shortcomings. Many metrics score n-gram overlap, and it is well established that using token overlap as a measure of similarity has drawbacks (Chen et al., 2019; Edunov et al., 2019; Wang et al., 2020). Cur6521 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pag"
2020.emnlp-main.528,D19-1243,0,0.0380112,"Missing"
2020.emnlp-main.528,D17-1215,0,0.0326025,"sing a span-selection or multiple-choice (MC) format. Both formats are easy to evaluate, ∗ Work done while at the Allen Institute for AI and the University of Washington. 1 The dataset, code, a leaderboard, and a demo are available at https://allennlp.org/mocha. but in return, have restrictions placed on the questions that can be asked or the answers that can be returned. Furthermore, both formats hinge on distractor spans/choices for learning to be effective. Ensuring high quality distractors is a challenging task in and of itself, which can lead to models that exploit spurious correlations (Jia and Liang, 2017; Min et al., 2019; Geva et al., 2019). Posing RC as a generation task addresses the aforementioned issues. Generative RC does not require distractors, circumventing biases that could be introduced by them, and allows arbitrary questions and answers. Unfortunately, existing metrics for evaluating text generation come with significant shortcomings. Many metrics score n-gram overlap, and it is well established that using token overlap as a measure of similarity has drawbacks (Chen et al., 2019; Edunov et al., 2019; Wang et al., 2020). Cur6521 Proceedings of the 2020 Conference on Empirical Metho"
2020.emnlp-main.528,D17-1082,0,0.144271,"e. Human Judgement: 5 out of 5 LERC: 4.98 out of 5 BLEU-1: 0.07 ROUGE-L: 0.15 METEOR: 0.17 Figure 1: Generative reading comprehension example. Properly scoring the candidate requires access to the passage. Current metrics, such as BLEU, ROUGE and METEOR, are agnostic to the end-task while LERC is trained with the passage and question as input. As a result, LERC assigns a score that better reflects human judgement. Introduction Reading comprehension (RC) has seen significant progress in the last few years, with a number of question answering (QA) datasets being created (Rajpurkar et al., 2016; Lai et al., 2017; Talmor et al., 2018). However, a majority of datasets are presented using a span-selection or multiple-choice (MC) format. Both formats are easy to evaluate, ∗ Work done while at the Allen Institute for AI and the University of Washington. 1 The dataset, code, a leaderboard, and a demo are available at https://allennlp.org/mocha. but in return, have restrictions placed on the questions that can be asked or the answers that can be returned. Furthermore, both formats hinge on distractor spans/choices for learning to be effective. Ensuring high quality distractors is a challenging task in and o"
2020.emnlp-main.528,W04-1013,0,0.109362,"Missing"
2020.emnlp-main.528,W18-6450,0,0.0134631,"to question answering, where the passage and question should be assimilated. The final category consists of metrics learned end-to-end from human judgements (Cui et al., 2018; Sellam et al., 2020). These metrics are flexible in that they can be tuned to the specific evaluation setting but depend on a large corpus of human judgement scores to train on. We hope that the release of MOCHA pushes the development of QA metrics that fall into this category. MOCHA is directly inspired by the annual WMT Metrics Shared Task (Mach´acek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Participants submit automatic translations and human judgement scores are collected for the submitted translations. The annotations collected as part of the WMT Metrics Shared Task have made it easy to evaluate and create new translation metrics (Popovic, 2015; Ma et al., 2017; Shimanaka et al., 2018). In a similar vein, SummEval is a recently released dataset that evaluates a number of evaluation metrics for summarization (Fabbri et al., 2020). 6 Conclusion We present MOCHA, a dataset of human judgement scores for training and evaluating generative reading comprehension metrics. Usin"
2020.emnlp-main.528,W17-4768,0,0.0165145,"but depend on a large corpus of human judgement scores to train on. We hope that the release of MOCHA pushes the development of QA metrics that fall into this category. MOCHA is directly inspired by the annual WMT Metrics Shared Task (Mach´acek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Participants submit automatic translations and human judgement scores are collected for the submitted translations. The annotations collected as part of the WMT Metrics Shared Task have made it easy to evaluate and create new translation metrics (Popovic, 2015; Ma et al., 2017; Shimanaka et al., 2018). In a similar vein, SummEval is a recently released dataset that evaluates a number of evaluation metrics for summarization (Fabbri et al., 2020). 6 Conclusion We present MOCHA, a dataset of human judgement scores for training and evaluating generative reading comprehension metrics. Using MOCHA, we train a learned metric, LERC, that outperforms all existing metrics and is much more robust when evaluated on a set of minimal pairs. While we have demonstrated that LERC is a better metric for evaluating generative reading comprehension than any existing metric, considerab"
2020.emnlp-main.528,W19-5302,0,0.040145,"Missing"
2020.emnlp-main.528,W14-3336,0,0.0730042,"Missing"
2020.emnlp-main.528,P19-1416,1,0.819886,"or multiple-choice (MC) format. Both formats are easy to evaluate, ∗ Work done while at the Allen Institute for AI and the University of Washington. 1 The dataset, code, a leaderboard, and a demo are available at https://allennlp.org/mocha. but in return, have restrictions placed on the questions that can be asked or the answers that can be returned. Furthermore, both formats hinge on distractor spans/choices for learning to be effective. Ensuring high quality distractors is a challenging task in and of itself, which can lead to models that exploit spurious correlations (Jia and Liang, 2017; Min et al., 2019; Geva et al., 2019). Posing RC as a generation task addresses the aforementioned issues. Generative RC does not require distractors, circumventing biases that could be introduced by them, and allows arbitrary questions and answers. Unfortunately, existing metrics for evaluating text generation come with significant shortcomings. Many metrics score n-gram overlap, and it is well established that using token overlap as a measure of similarity has drawbacks (Chen et al., 2019; Edunov et al., 2019; Wang et al., 2020). Cur6521 Proceedings of the 2020 Conference on Empirical Methods in Natural Lang"
2020.emnlp-main.528,L18-1564,0,0.0490233,"lar a candidate is to a reference using the passage and the question. ing human judgement scores, and creating minimal pairs for evaluation. candidates on two span-based datasets, DROP and Quoref, to test this. 2.1 2.2 Datasets Candidates in MOCHA come from 6 constituent QA datasets that are diverse in their domains and answer types. This ensures that training and evaluation with MOCHA does not overfit to the characteristics of any constituent dataset. NarrativeQA (Kocisk´y et al., 2017) tests reasoning about events, entities, and their relations on movie scripts and book summaries. MCScript (Ostermann et al., 2018) tests reasoning on stories written for a child-level reader. CosmosQA (Huang et al., 2019) tests commonsense reasoning on blogs describing everyday events. SocialIQA (Sap et al., 2019) tests social reasoning with passages constructed from a knowledge base. DROP (Dua et al., 2019) tests predicate argument structure and numerical reasoning on Wikipedia articles concerning American football games, census results, and history. Quoref (Dasigi et al., 2019) tests coreferential reasoning on Wikipedia articles. NarrativeQA was created as a generative RC dataset. CosmosQA, MCScript, and SocialIQA were"
2020.emnlp-main.528,2001.mtsummit-papers.68,0,0.015855,"Missing"
2020.emnlp-main.528,W15-3049,0,0.0113584,"uation setting but depend on a large corpus of human judgement scores to train on. We hope that the release of MOCHA pushes the development of QA metrics that fall into this category. MOCHA is directly inspired by the annual WMT Metrics Shared Task (Mach´acek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Participants submit automatic translations and human judgement scores are collected for the submitted translations. The annotations collected as part of the WMT Metrics Shared Task have made it easy to evaluate and create new translation metrics (Popovic, 2015; Ma et al., 2017; Shimanaka et al., 2018). In a similar vein, SummEval is a recently released dataset that evaluates a number of evaluation metrics for summarization (Fabbri et al., 2020). 6 Conclusion We present MOCHA, a dataset of human judgement scores for training and evaluating generative reading comprehension metrics. Using MOCHA, we train a learned metric, LERC, that outperforms all existing metrics and is much more robust when evaluated on a set of minimal pairs. While we have demonstrated that LERC is a better metric for evaluating generative reading comprehension than any existing m"
2020.emnlp-main.528,N18-1023,0,0.0446668,"Missing"
2020.emnlp-main.528,D13-1020,0,0.0444038,"CLS] lossi = (yi − yˆi )2 3.2 We pre-train BERT via 3-way classification to predict whether: a1 is the correct answer, a2 is the correct answer, or a1 and a2 are both correct. MultiRC has multiple correct answers per question and we create additional instances where both a1 and a2 are correct by duplicating the correct answer for all three datasets. Pre-Training the Learned Metric Learning the interactions between the input components can be difficult with only human judgement fine-tuning. To overcome this, we pre-train on four multiple-choice QA datasets: BoolQ (Clark et al., 2019a), MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), and MultiRC (Khashabi Experiments Training LERC: We use the PyTorch (Paszke et al., 2019), HuggingFace Transformers (Wolf et al., 2019), and AllenNLP (Gardner et al., 2017) libraries to implement LERC. We pre-train LERC before fine-tuning on MOCHA. We evaluate LERC in two settings, an out-of-dataset (OOD) setting and an all-datasets (AD) setting. In the OOD setting, we train and tune LERC on all datasets in MOCHA except the dataset we are evaluating on. This reflects the use case where we want to apply LERC to evaluate a new dataset where we do not have human judgeme"
2020.emnlp-main.528,D19-1454,0,0.0562355,"Missing"
2020.emnlp-main.528,2020.acl-main.704,0,0.0246621,"of metrics that use some variant of n-gram matching (Papineni et al., 2001; Lin, 2004; Banerjee and Lavie, 2005). They are easy to implement, but lack flexibility by focusing only on token overlap. The second cateogry of metrics eschew some of the aforementioned issues by calculating a softer similarity score using embeddings of tokens (Clark et al., 2019b; Zhang et al., 2019). However, it is unclear how to tailor them to question answering, where the passage and question should be assimilated. The final category consists of metrics learned end-to-end from human judgements (Cui et al., 2018; Sellam et al., 2020). These metrics are flexible in that they can be tuned to the specific evaluation setting but depend on a large corpus of human judgement scores to train on. We hope that the release of MOCHA pushes the development of QA metrics that fall into this category. MOCHA is directly inspired by the annual WMT Metrics Shared Task (Mach´acek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Participants submit automatic translations and human judgement scores are collected for the submitted translations. The annotations collected as part of the WMT Metrics Sha"
2020.emnlp-main.528,P16-1009,0,0.0497328,"aset. CosmosQA, MCScript, and SocialIQA were created as MC datasets which we re-purpose as generative datasets by using the correct choice as the reference. Our motivation for doing this is that the number of generative QA datasets is quite small, which we attribute to the quality of evaluation metrics. The main focus of this work is in developing and evaluating metrics for generative RC. However, we wanted to see whether a learned metric could do well on span-selection datasets. We collected Collecting Candidates Candidates on all four generative datasets are generated using backtranslation (Sennrich et al., 2016) and using a fine-tuned GPT-2 model (Radford et al., 2019). We also generate candidates for NarrativeQA and MCScript using a trained MHPG model (Bauer et al., 2018). We tried using MHPG for CosmosQA and SocialIQA but candidates were of poor quality. Unique to NarrativeQA, each question has two references. We treat the second reference as a candidate to be annotated if it has low n-gram overlap with the first reference. We use a span-selection BERT-based model to generate candidates for Quoref and NAQANET (Dua et al., 2019) and NABERT2 models for DROP. Models are trained on the training sets of"
2020.emnlp-main.528,W18-6456,0,0.0135806,"arge corpus of human judgement scores to train on. We hope that the release of MOCHA pushes the development of QA metrics that fall into this category. MOCHA is directly inspired by the annual WMT Metrics Shared Task (Mach´acek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Participants submit automatic translations and human judgement scores are collected for the submitted translations. The annotations collected as part of the WMT Metrics Shared Task have made it easy to evaluate and create new translation metrics (Popovic, 2015; Ma et al., 2017; Shimanaka et al., 2018). In a similar vein, SummEval is a recently released dataset that evaluates a number of evaluation metrics for summarization (Fabbri et al., 2020). 6 Conclusion We present MOCHA, a dataset of human judgement scores for training and evaluating generative reading comprehension metrics. Using MOCHA, we train a learned metric, LERC, that outperforms all existing metrics and is much more robust when evaluated on a set of minimal pairs. While we have demonstrated that LERC is a better metric for evaluating generative reading comprehension than any existing metric, considerable work remains. Error an"
2020.emnlp-main.528,W15-3031,0,0.065281,"Missing"
2020.emnlp-main.528,2020.acl-main.450,0,0.0233687,"which can lead to models that exploit spurious correlations (Jia and Liang, 2017; Min et al., 2019; Geva et al., 2019). Posing RC as a generation task addresses the aforementioned issues. Generative RC does not require distractors, circumventing biases that could be introduced by them, and allows arbitrary questions and answers. Unfortunately, existing metrics for evaluating text generation come with significant shortcomings. Many metrics score n-gram overlap, and it is well established that using token overlap as a measure of similarity has drawbacks (Chen et al., 2019; Edunov et al., 2019; Wang et al., 2020). Cur6521 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6521–6532, c November 16–20, 2020. 2020 Association for Computational Linguistics rent metrics also only consider the reference and are agnostic to the end-task being evaluated. Fig. 1 demonstrates that this is problematic for generative RC because scoring a candidate may require a metric to also consider the passage and the question. Without cheap and reliable evaluation, progress in generative reading comprehension has been extremely slow. To address the need for better evaluation metrics"
2020.wmt-1.39,N13-1073,0,0.0998751,"Missing"
2020.wmt-1.39,P02-1040,0,0.110118,"the data regardless of context. In our example above, models would prefer to translate the doctor using masculine inflections, despite the context suggesting otherwise. In this work, we apply the WinoMT test suite on the submissions to the News shared task of WMT 2020. In addition to testing the phenomenon on a large number of models, we extend the WinoMT to the Polish and Czech languages, tackling unique language-specific challenges. We thoroughly analyze the extent of the phenomena for the tested languages and systems, as well as its correlation with the widely-used BLEU evaluation metric (Papineni et al., 2002), finding that systems with worse performance (in BLEU) make more errors for female professions than errors for male professions. On the other hand, better-performing systems (in BLEU) make more errors related to anti-stereotypical professions (e.g. female doctors, or male nurses). Similarly to the conclusions of Stanovsky et al. (2019), we find that all systems consistently perform better when the source texts exhibit stereotypical gender role assignments (e.g., male doctors, female nurses) versus non-stereotypical assignments (e.g., female doctors, male nurses), indicating that these models"
2020.wmt-1.39,P19-1164,1,0.919872,"ccur when translating from languages without grammatical noun genders, such as English or Turkish, to a language with gender inflections, such as Spanish, Polish, or Czech. In such cases, the translation model needs to assign gender inflection in the target language based on contextual cues in the source text. For example, when translating the English sentence “The doctor asked the nurse to help her in the operation” to Spanish, the model needs to produce the female inflected “doctora” based on the feminine English pronoun “her”. ∗ Part of work performed while at Charles University. Recently, Stanovsky et al. (2019) created a challenge set and an automatic evaluation metric, dubbed WinoMT, to examine whether popular MT models are capable of correctly capturing and translating such information from English into a diverse set of 8 target languages with grammatical gender. They found that all six tested systems, composed of four commercial and two academic models, consistently relied on gender role assignments in the data regardless of context. In our example above, models would prefer to translate the doctor using masculine inflections, despite the context suggesting otherwise. In this work, we apply the W"
2020.wmt-1.39,P14-5003,0,0.073023,"Missing"
2020.wmt-1.39,L16-1412,0,0.024373,"Missing"
2020.wmt-1.39,wolinski-2014-morfeusz,0,0.036363,"Missing"
2020.wmt-1.39,N18-2003,0,0.465423,"(e.g., female doctors, male nurses), indicating that these models rely on spurious correlations in their training data, rather than on more meaningful textual context. We hope that this evaluation will be used as a standard evaluation metric for MT as a means to track the improvement of this socially important aspect of translation. 357 Proceedings of the 5th Conference on Machine Translation (WMT), pages 357–364 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics 2 Background: WinoMT WinoMT was created as a concatenation of two coreference test suites: WinoGender (Zhao et al., 2018a) and WinoBias (Rudinger et al., 2018). Each instance in these datasets is a single English sentence, presenting two entities, identified by their profession (e.g., “teacher”, “janitor”, or “hairdresser”) and a single pronoun referring to one of them based on the context of the sentence. For example, in the sentence “The physician hired the secretary because he was overwhelmed with clients”, the marked pronoun refers to the physician. In contrast, in “The physician hired the secretary because he had good credentials” the pronoun likely refers to the secretary. Both datasets are created with a"
2021.eacl-main.187,E17-1051,0,0.0280162,"turn to the literature on abstract meaning representation (AMR; Banarescu et al., 2013) for established graph agreement metrics, which we adapt to our setting. Similarly to our PEG representation, the AMR formalism has predicate and argument nodes (lab operations and entities in our notation) and directed labeled edges which can form undirected cycles through reentrancies (nodes with multiple incoming edges).4 In Table 5 we report a graph Smatch score (Cai and Knight, 2013) widely used to quantify AMR’s graph structure agreement, as well as finer grained graph agreement metrics, adapted from Damonte et al. (2017). Smatch values are comparable to those obtained for AMR, where reported gold agreement 4 2194 Unfortunately, we cannot follow this analogy to train AMR models on our graphs, since, to the best of our knowledge, they are currently limited to single sentences, notwithstanding a promising recent initial exploration into multi-sentence AMR annotation (O’Gorman et al., 2018). Agreement Metric Smatch Argument identification Predicate identification Core roles Re-entrancies F1 Dataset Avg. #args/op #Ops. w/o core arg. #Ops. Pct. 84.99 89.72 86.68 80.52 73.12 WLP X-WLP Table 5: X-WLP inter-annotator"
2021.eacl-main.187,D19-1371,0,0.0540651,"ing. Our annotator pay was 13 USD / hour. The overall annotation budget for X-WLP was roughly 3,200 USD. 5 Models We present two approaches for PEG prediction. First, in §5.1 we design models for separate graph sub-component prediction, which are chained to form a pipeline PEG prediction model. Second, in §5.2 we present a model which directly predicts the entire PEG using a span-graph prediction approach. 5.1 Pipeline Model (P IPELINE) A full PEG representation as defined in §3 can be obtained by chaining the following models which predict its sub-components. In all of these, we use SciBERT (Beltagy et al., 2019) which was trained on scientific texts similar to our domain. 2195 Mention identification. Given a scientific protocol written in natural language, we begin by identifying all experiment-involved text spans mentioning lab operations (predicates) or entities and their traits (arguments), which are the building blocks for PEGs. We model this problem of mention identification as a sequence tagging problem. Specifically, we transfer span-level mention labels, which are annotated in the WLP corpus into token-level labels using the BIO tagging scheme, then fine-tune the SciBERT model for token class"
2021.eacl-main.187,D15-1114,0,0.0180507,"tions; linking our approach with their framework is a promising future direction. Structurally, PEGs are similar to abstract meaning representation (AMR; Banarescu et al. 2013), allowing us to use agreement and performance metrics developed for AMR. In contrast with the sentence-level AMR, a major challenge in this work is annotating and predicting procedure-level representations.8 Another line of research focuses on procedural text understanding for more general domains: simple scientific processes (Dalvi et al., 2018), open domain procedural texts (Tandon et al., 2020), and cooking recipes (Kiddon et al., 2015; Bosselut et al., 2018). These works represent process-level information and entity state changes, but typically feature shorter processes, simpler language and an open ontology, compared with our domain-specific terminology and grounded ontology. Our framework also provides a link to text-based game approaches to procedural text understanding. Tamari et al. (2019) modelled scientific procedures with text-based games but used only synthetic data. Our simulator enables leveraging recent advances on text-based games agents (e.g., (Adhikari et al., 2020)) towards natural language understanding."
2021.eacl-main.187,N18-2016,1,0.925152,"re 1 does not specify the swirling (mixing) speed or its duration. Our process execution graph (PEG) captures the predicate-argument structure of the protocol, allowing it to be more lenient than a programming language (for example, capturing that gently modifies swirl). Better suited to represent underspecified natural language, PEGs can serve as a convenient scaffold to support downstream tasks such as text-to-code assistants (Mehr et al., 2020). For example, by asking researchers to fill in missing required arguments for swirl. To annotate PEGs, we leverage the sentencelevel annotations of Kulkarni et al. (2018) (WLP henceforth). WLP, exemplified at the top of 2 2191 https://autoprotocol.org/specification Figure 1, collected sentence-level structures using the BRAT annotation tool (Stenetorp et al., 2012). For example, capturing that cells, culture tubes are arguments for add. However, WLP does not capture cross-sentence implicit relations such that culture tubes are an argument for incubate. These are abundant in lab protocols, require tracking entities across many sentences, and are not easy to annotate using BRAT (see discussion in §4). We vastly extend upon WLP annotations, aiming to capture the"
2021.eacl-main.187,N18-1144,0,0.127216,"identification. In conclusion, we make the following contributions: • We formalize a PEG representation for free-form, natural language lab protocols, providing a semantic scaffold between free-form scientific literature and low-level instruments instruction. • We develop a novel annotation interface for procedural text annotation using text-based games, and show that it is intuitive enough for wet-lab protocol annotation by non-experts. • We release X-WLP, a challenging corpus of 279 PEGs representing document-level lab protocols. This size is on par with similar corpora of procedural text (Dalvi et al., 2018; Mysore et al., 2019; Vaucher et al., 2020). • We develop two graph parsers: a pipeline model which chains predictions for graph subcomponents, and a joint-model of mention and relation detectors. 2 Background and Motivation Several formalisms for programmatic lab controller interfaces were developed in recent years (Yachie and Natsume, 2017; Lee and Miles, 2018). For instance, Autoprotocol defines 35 lab commands, including spin, incubate, and mix.2 While these define wet-lab experiments in a precise and unambiguous manner, they do not readily replace their natural language description in sc"
2021.eacl-main.187,N19-1308,0,0.0218954,"ps by issuing textual commands to the simulator. The commands are deterministically converted to our PEG representation. This interface takes much of the burden off annotators by keeping track of object traits and commonsense constraints. For example, when the annotator issues a transfer command for a container, the simulator moves all its contents as well. We find that in-house annotators were able to effectively use this interface on complex protocols, achieving good agreement. Finally, we use this data to explore several models, building upon recent advances in graph prediction algorithms (Luan et al., 2019; Wadden et al., 2019). We thoroughly analyze model performance and find that our data introduces interesting new challenges, such as complex coreference resolution and long-range, cross-sentence relation identification. In conclusion, we make the following contributions: • We formalize a PEG representation for free-form, natural language lab protocols, providing a semantic scaffold between free-form scientific literature and low-level instruments instruction. • We develop a novel annotation interface for procedural text annotation using text-based games, and show that it is intuitive enough f"
2021.eacl-main.187,W19-4007,0,0.21867,"conclusion, we make the following contributions: • We formalize a PEG representation for free-form, natural language lab protocols, providing a semantic scaffold between free-form scientific literature and low-level instruments instruction. • We develop a novel annotation interface for procedural text annotation using text-based games, and show that it is intuitive enough for wet-lab protocol annotation by non-experts. • We release X-WLP, a challenging corpus of 279 PEGs representing document-level lab protocols. This size is on par with similar corpora of procedural text (Dalvi et al., 2018; Mysore et al., 2019; Vaucher et al., 2020). • We develop two graph parsers: a pipeline model which chains predictions for graph subcomponents, and a joint-model of mention and relation detectors. 2 Background and Motivation Several formalisms for programmatic lab controller interfaces were developed in recent years (Yachie and Natsume, 2017; Lee and Miles, 2018). For instance, Autoprotocol defines 35 lab commands, including spin, incubate, and mix.2 While these define wet-lab experiments in a precise and unambiguous manner, they do not readily replace their natural language description in scientific publications"
2021.eacl-main.187,C18-1313,0,0.065185,"Missing"
2021.eacl-main.187,W19-2609,1,0.827688,"representations.8 Another line of research focuses on procedural text understanding for more general domains: simple scientific processes (Dalvi et al., 2018), open domain procedural texts (Tandon et al., 2020), and cooking recipes (Kiddon et al., 2015; Bosselut et al., 2018). These works represent process-level information and entity state changes, but typically feature shorter processes, simpler language and an open ontology, compared with our domain-specific terminology and grounded ontology. Our framework also provides a link to text-based game approaches to procedural text understanding. Tamari et al. (2019) modelled scientific procedures with text-based games but used only synthetic data. Our simulator enables leveraging recent advances on text-based games agents (e.g., (Adhikari et al., 2020)) towards natural language understanding. 8 Conclusion We developed a novel meaning representation and simulation-based annotation interface, enabling the collection of process-level annotations of experimental procedures, as well as two parsers (pipeline and joint modelling) trained on this data. Our dataset and experiments present several directions for future work, including the modelling of challenging"
2021.eacl-main.187,2020.emnlp-main.520,0,0.0178966,"g, relying on a human-inthe-loop for corrections; linking our approach with their framework is a promising future direction. Structurally, PEGs are similar to abstract meaning representation (AMR; Banarescu et al. 2013), allowing us to use agreement and performance metrics developed for AMR. In contrast with the sentence-level AMR, a major challenge in this work is annotating and predicting procedure-level representations.8 Another line of research focuses on procedural text understanding for more general domains: simple scientific processes (Dalvi et al., 2018), open domain procedural texts (Tandon et al., 2020), and cooking recipes (Kiddon et al., 2015; Bosselut et al., 2018). These works represent process-level information and entity state changes, but typically feature shorter processes, simpler language and an open ontology, compared with our domain-specific terminology and grounded ontology. Our framework also provides a link to text-based game approaches to procedural text understanding. Tamari et al. (2019) modelled scientific procedures with text-based games but used only synthetic data. Our simulator enables leveraging recent advances on text-based games agents (e.g., (Adhikari et al., 2020)"
2021.eacl-main.187,D19-1585,0,0.30608,"al commands to the simulator. The commands are deterministically converted to our PEG representation. This interface takes much of the burden off annotators by keeping track of object traits and commonsense constraints. For example, when the annotator issues a transfer command for a container, the simulator moves all its contents as well. We find that in-house annotators were able to effectively use this interface on complex protocols, achieving good agreement. Finally, we use this data to explore several models, building upon recent advances in graph prediction algorithms (Luan et al., 2019; Wadden et al., 2019). We thoroughly analyze model performance and find that our data introduces interesting new challenges, such as complex coreference resolution and long-range, cross-sentence relation identification. In conclusion, we make the following contributions: • We formalize a PEG representation for free-form, natural language lab protocols, providing a semantic scaffold between free-form scientific literature and low-level instruments instruction. • We develop a novel annotation interface for procedural text annotation using text-based games, and show that it is intuitive enough for wet-lab protocol an"
2021.eacl-main.187,E12-2021,0,0.146098,"Missing"
2021.emnlp-main.384,P18-1031,0,0.0478842,"in Figure 1. The Open Richly Annotated Cuneiform Corpus (Oracc)1 is one of the major Akkadian transcription collections, culminating in approximately 1 http://oracc.org 2.3M transcribed signs from 10K tablets. As further evidenced in Figure 1, many of the signs in the tablets were eroded over time and some parts were broken or lost, forcing editors to “ﬁll in the gaps” where possible, based on the context of the surrounding words. In this paper, we identify that the task of masked language modeling, used ubiquitously in recent years for pretraining other downstream tasks (Peters et al., 2018; Howard and Ruder, 2018; Liu et al., 2019) lends itself directly to missing sign prediction in the transliterated texts. We experiment with various adaptations of BERT-based models (Devlin et al., 2019) trained and tested on Oracc, combined with a greedy decoding scheme to extend the prediction from single tokens to multiple words. We speciﬁcally focus on the effect multilingual pretraining has on downstream performance, which was recently shown beneﬁcial for low-resource settings (Chau et al., 2020). 4682 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4682–4691 c Novem"
2021.emnlp-main.384,W19-1409,0,0.0478047,"Missing"
2021.emnlp-main.384,2020.findings-emnlp.118,0,0.036949,"Missing"
2021.emnlp-main.384,2020.acl-main.536,0,0.252096,"introduce the Akkadian language and the Open Richly Annotated Cuneiform 4683 2.2 Multilingual Masked Language Modeling In masked language modeling (MLM), a model is asked to predict masked parts in a text given their surrounding context. Recent years have seen large gains for almost all NLP tasks by using the token representations learned during MLM as a starting point for downstream applications. In particular, recent work has noticed that joint training on various languages greatly helps downstream applications, especially where labeled data is sparse (Pires et al., 2019; Chau et al., 2020; Conneau et al., 2020). In this work we identify that the MLM objective directly corresponds to the task of ﬁlling in gaps in Akkadian texts and train several MLM variants on it. In the following sections, we will especially examine the effect of multilingual pretraining on our task. 3 Task Deﬁnition Intuitively, our task, as demonstrated in Figure 2, is to predict missing tokens or signs given their context in transliterated Akkadian documents. Human experts achieve this when compiling Oracc by considering not only the surrounding context in the tablet, but also its wider, external context, such as its corpus, or"
2021.emnlp-main.384,N19-1423,0,0.151529,"ibed signs from 10K tablets. As further evidenced in Figure 1, many of the signs in the tablets were eroded over time and some parts were broken or lost, forcing editors to “ﬁll in the gaps” where possible, based on the context of the surrounding words. In this paper, we identify that the task of masked language modeling, used ubiquitously in recent years for pretraining other downstream tasks (Peters et al., 2018; Howard and Ruder, 2018; Liu et al., 2019) lends itself directly to missing sign prediction in the transliterated texts. We experiment with various adaptations of BERT-based models (Devlin et al., 2019) trained and tested on Oracc, combined with a greedy decoding scheme to extend the prediction from single tokens to multiple words. We speciﬁcally focus on the effect multilingual pretraining has on downstream performance, which was recently shown beneﬁcial for low-resource settings (Chau et al., 2020). 4682 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4682–4691 c November 7–11, 2021. 2021 Association for Computational Linguistics In an automatic evaluation, we ﬁnd that a combination of large-scale multilingual pretraining with Akkadian ﬁnetunin"
2021.emnlp-main.384,2021.ccl-1.108,0,0.0424962,"Missing"
2021.emnlp-main.384,2020.tlt-1.11,0,0.0391474,"Missing"
2021.emnlp-main.384,N18-1202,0,0.0141638,"right of the tablet in Figure 1. The Open Richly Annotated Cuneiform Corpus (Oracc)1 is one of the major Akkadian transcription collections, culminating in approximately 1 http://oracc.org 2.3M transcribed signs from 10K tablets. As further evidenced in Figure 1, many of the signs in the tablets were eroded over time and some parts were broken or lost, forcing editors to “ﬁll in the gaps” where possible, based on the context of the surrounding words. In this paper, we identify that the task of masked language modeling, used ubiquitously in recent years for pretraining other downstream tasks (Peters et al., 2018; Howard and Ruder, 2018; Liu et al., 2019) lends itself directly to missing sign prediction in the transliterated texts. We experiment with various adaptations of BERT-based models (Devlin et al., 2019) trained and tested on Oracc, combined with a greedy decoding scheme to extend the prediction from single tokens to multiple words. We speciﬁcally focus on the effect multilingual pretraining has on downstream performance, which was recently shown beneﬁcial for low-resource settings (Chau et al., 2020). 4682 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,"
2021.emnlp-main.384,P19-1493,0,0.2043,"kadian texts. In this section, we will introduce the Akkadian language and the Open Richly Annotated Cuneiform 4683 2.2 Multilingual Masked Language Modeling In masked language modeling (MLM), a model is asked to predict masked parts in a text given their surrounding context. Recent years have seen large gains for almost all NLP tasks by using the token representations learned during MLM as a starting point for downstream applications. In particular, recent work has noticed that joint training on various languages greatly helps downstream applications, especially where labeled data is sparse (Pires et al., 2019; Chau et al., 2020; Conneau et al., 2020). In this work we identify that the MLM objective directly corresponds to the task of ﬁlling in gaps in Akkadian texts and train several MLM variants on it. In the following sections, we will especially examine the effect of multilingual pretraining on our task. 3 Task Deﬁnition Intuitively, our task, as demonstrated in Figure 2, is to predict missing tokens or signs given their context in transliterated Akkadian documents. Human experts achieve this when compiling Oracc by considering not only the surrounding context in the tablet, but also its wider,"
2021.emnlp-main.384,2020.lrec-1.479,0,0.0752446,"Missing"
2021.emnlp-main.384,D19-1077,0,0.0610652,"iven input from Oracc with missing signs (red ‘x’s). We experiment with different language models and pretraining data. 4684 and iteratively predict each of the tokens composing it. 4.2 Masked Language Models We experimented with monolingual and multilingual versions of BERT. First, we pretrained from scratch a monolingual BERT model with a reduced number of parameters (750K) following conclusions from Kaplan et al. (2020). Second, following recent research suggesting that pretraining on similar languages is beneﬁcial for many NLP tasks, including in low-resource settings (Pires et al., 2019; Wu and Dredze, 2019; Chau et al., 2020; Conneau et al., 2020), we ﬁnetuned a pretrained multilingual BERT (M-BERT) model (Devlin et al., 2019).3 M-BERT was trained on the 104 most common languages of Wikipedia, including Hebrew and Arabic - Semitic languages that are typologically similar to Akkadian. To adapt M-BERT to Akkadian, we assign its 99 available free tokens, optimizing for maximum likelihood by the WordPiece tokenization algorithm (Schuster and Nakajima, 2012; Wu et al., 2016). 4.3 Decoding: From Tokens to Signs While the MLM task is designed to predict single tokens, in our setting, multiple signs an"
2021.findings-acl.453,P19-1409,1,0.924762,"s and sets baseline results over predicted mentions. Our model is also simpler and substantially more efficient than existing CD coreference systems. Taken together, our work seeks to bridge the gap between WD and CD coreference, driving further research of the latter in realistic settings. 2 Background Cross-document coreference Previous works on CD coreference resolution learn a pairwise scorer between mentions and use a clustering approach to form the coreference clusters (Cybulska and Vossen, 2015; Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Bugert et al., 2020). Barhom et al. (2019) proposed to jointly learn entity and event coreference resolution, leveraging predicate-argument structures. Their model forms the coreference clusters incrementally, while alternating between event and entity coreference. Based on this work, Meged et al. (2020) improved results on event coreference by leverag5100 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5100–5107 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: A high-level diagram of our model for cross-document coreference resolution. (1) Extract and score all possible spa"
2021.findings-acl.453,P10-1143,0,0.293584,"Missing"
2021.findings-acl.453,2021.starsem-1.13,1,0.761257,"Missing"
2021.findings-acl.453,D17-1226,0,0.224187,"Missing"
2021.findings-acl.453,cybulska-vossen-2014-using,0,0.252298,". Our model achieves competitive results for event and entity coreference resolution on gold mentions. More importantly, we set first baseline results, on the standard ECB+ dataset, for CD coreference resolution over predicted mentions. Further, our model is simpler and more efficient than recent CD coreference resolution systems, while not using any external resources.1 1 Introduction Cross-document (CD) coreference resolution consists of identifying textual mentions across multiple documents that refer to the same concept. For example, consider the following sentences from the ECB+ dataset (Cybulska and Vossen, 2014), where colors represent coreference clusters (for brevity, we omit some clusters): 1. Thieves pulled off a two million euro jewellery heist in central Paris on Monday after smashing their car through the store’s front window. 2. Four men drove a 4x4 through the front window of the store on Rue de Castiglione, before making off with the jewellery and watches. Despite its importance for downstream tasks, CD coreference resolution has been lagging behind the 1 https://github.com/ariecattan/coref impressive strides made in the scope of a single document (Lee et al., 2017; Joshi et al., 2019, 2020"
2021.findings-acl.453,W15-0801,0,0.0132712,"erative clustering that was shown useful in CD models. Our model achieves competitive results on ECB+ over gold mentions and sets baseline results over predicted mentions. Our model is also simpler and substantially more efficient than existing CD coreference systems. Taken together, our work seeks to bridge the gap between WD and CD coreference, driving further research of the latter in realistic settings. 2 Background Cross-document coreference Previous works on CD coreference resolution learn a pairwise scorer between mentions and use a clustering approach to form the coreference clusters (Cybulska and Vossen, 2015; Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Bugert et al., 2020). Barhom et al. (2019) proposed to jointly learn entity and event coreference resolution, leveraging predicate-argument structures. Their model forms the coreference clusters incrementally, while alternating between event and entity coreference. Based on this work, Meged et al. (2020) improved results on event coreference by leverag5100 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5100–5107 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: A"
2021.findings-acl.453,2020.tacl-1.5,1,0.905254,"Missing"
2021.findings-acl.453,D19-1588,1,0.93481,"bulska and Vossen, 2014), where colors represent coreference clusters (for brevity, we omit some clusters): 1. Thieves pulled off a two million euro jewellery heist in central Paris on Monday after smashing their car through the store’s front window. 2. Four men drove a 4x4 through the front window of the store on Rue de Castiglione, before making off with the jewellery and watches. Despite its importance for downstream tasks, CD coreference resolution has been lagging behind the 1 https://github.com/ariecattan/coref impressive strides made in the scope of a single document (Lee et al., 2017; Joshi et al., 2019, 2020; Wu et al., 2020). Further, state-of-the-art models exhibit several shortcomings, such as operating on gold mentions or relying on external resources such as SRL or a paraphrase dataset (Shwartz et al., 2017), preventing them from being applied on realistic settings. To address these limitations, we develop the first end-to-end CD coreference model building upon a prominent within-document (WD) coreference model (Lee et al., 2017) which we extend with recent advances in transformer-based encoders. We address the inherently non-linear nature of the CD setting by combining the WD corefere"
2021.findings-acl.453,S18-2001,0,0.379007,"es competitive results on ECB+ over gold mentions and sets baseline results over predicted mentions. Our model is also simpler and substantially more efficient than existing CD coreference systems. Taken together, our work seeks to bridge the gap between WD and CD coreference, driving further research of the latter in realistic settings. 2 Background Cross-document coreference Previous works on CD coreference resolution learn a pairwise scorer between mentions and use a clustering approach to form the coreference clusters (Cybulska and Vossen, 2015; Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Bugert et al., 2020). Barhom et al. (2019) proposed to jointly learn entity and event coreference resolution, leveraging predicate-argument structures. Their model forms the coreference clusters incrementally, while alternating between event and entity coreference. Based on this work, Meged et al. (2020) improved results on event coreference by leverag5100 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5100–5107 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: A high-level diagram of our model for cross-document coreference resolu"
2021.findings-acl.453,D12-1045,0,0.28704,"omerative clustering can effectively find cross-document coreference clusters. 4 4.1 Experiments Experimental setup Following most recent work, we conduct our experiments ECB+ (Cybulska and Vossen, 2014), which is the largest dataset that includes both WD and CD coreference annotation (see Appendix A.2). We use the document clustering of Barhom et al. (2019) for pre-processing and apply our coreference model separately on each predicted document cluster. Following Barhom et al. (2019), we present the model’s performance on both event and entity coreference resolution. In addition, inspired by Lee et al. (2012), we train our model to perform event and entity coreference jointly, which we term “ALL”. This represents a useful scenario when we are interested in finding all the coreference links in a set of documents, without having to distinguish event and entity mentions. Addressing CD coreference with ALL is challenging because (1) the search space is larger than when treating separately event and entity coreference and (2) models need to make subtle distinctions between event and entity mentions that are lexically similar but do not corefer. For example, the entity voters do not corefer with the eve"
2021.findings-acl.453,D17-1018,0,0.322866,"e ECB+ dataset (Cybulska and Vossen, 2014), where colors represent coreference clusters (for brevity, we omit some clusters): 1. Thieves pulled off a two million euro jewellery heist in central Paris on Monday after smashing their car through the store’s front window. 2. Four men drove a 4x4 through the front window of the store on Rue de Castiglione, before making off with the jewellery and watches. Despite its importance for downstream tasks, CD coreference resolution has been lagging behind the 1 https://github.com/ariecattan/coref impressive strides made in the scope of a single document (Lee et al., 2017; Joshi et al., 2019, 2020; Wu et al., 2020). Further, state-of-the-art models exhibit several shortcomings, such as operating on gold mentions or relying on external resources such as SRL or a paraphrase dataset (Shwartz et al., 2017), preventing them from being applied on realistic settings. To address these limitations, we develop the first end-to-end CD coreference model building upon a prominent within-document (WD) coreference model (Lee et al., 2017) which we extend with recent advances in transformer-based encoders. We address the inherently non-linear nature of the CD setting by combi"
2021.findings-acl.453,2021.ccl-1.108,0,0.0839573,"Missing"
2021.findings-acl.453,2020.findings-emnlp.440,1,0.861846,"tter in realistic settings. 2 Background Cross-document coreference Previous works on CD coreference resolution learn a pairwise scorer between mentions and use a clustering approach to form the coreference clusters (Cybulska and Vossen, 2015; Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Bugert et al., 2020). Barhom et al. (2019) proposed to jointly learn entity and event coreference resolution, leveraging predicate-argument structures. Their model forms the coreference clusters incrementally, while alternating between event and entity coreference. Based on this work, Meged et al. (2020) improved results on event coreference by leverag5100 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5100–5107 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: A high-level diagram of our model for cross-document coreference resolution. (1) Extract and score all possible spans, (2) keep top spans according to sm (i), (3) score all pairs s(i, j), and (4) cluster spans using agglomerative clustering. ing a paraphrase resource (Chirps; Shwartz et al., 2017) as distant supervision. Parallel to our work, recent approaches propose to fine"
2021.findings-acl.453,Q15-1037,0,0.344054,"shown useful in CD models. Our model achieves competitive results on ECB+ over gold mentions and sets baseline results over predicted mentions. Our model is also simpler and substantially more efficient than existing CD coreference systems. Taken together, our work seeks to bridge the gap between WD and CD coreference, driving further research of the latter in realistic settings. 2 Background Cross-document coreference Previous works on CD coreference resolution learn a pairwise scorer between mentions and use a clustering approach to form the coreference clusters (Cybulska and Vossen, 2015; Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Bugert et al., 2020). Barhom et al. (2019) proposed to jointly learn entity and event coreference resolution, leveraging predicate-argument structures. Their model forms the coreference clusters incrementally, while alternating between event and entity coreference. Based on this work, Meged et al. (2020) improved results on event coreference by leverag5100 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5100–5107 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: A high-level diagram"
2021.findings-acl.453,2020.coling-main.275,0,0.678153,"ag5100 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5100–5107 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: A high-level diagram of our model for cross-document coreference resolution. (1) Extract and score all possible spans, (2) keep top spans according to sm (i), (3) score all pairs s(i, j), and (4) cluster spans using agglomerative clustering. ing a paraphrase resource (Chirps; Shwartz et al., 2017) as distant supervision. Parallel to our work, recent approaches propose to fine-tune BERT on the pairwise coreference scorer (Zeng et al., 2020), where the state-of-the-art on ECB+ is achieved using a cross-document language model (CDLM) on pairs of full documents (Caciularu et al., 2021). Instead of applying BERT for all mentions pairs which is quadratically costly, our work separately encodes each (predicted) mention. All above models suffer from several drawbacks. First, they use only gold mentions and treat entities and events separately.2 Second, pairwise scores are recomputed after each merging step, which is resource and time consuming. Finally, they rely on additional resources, such as semantic role labeling, a within-documen"
2021.findings-acl.453,W12-4501,0,0.085386,"gi , gj , gi ◦ gj ]) s(i, j) = sm (i) + sm (j) + sa (i, j) 3 Model The overall structure of our model is shown in Figure 1. The major obstacle in applying the e2e-coref model directly in the CD setting is its reliance on textual ordering – it forms coreference chains by linking each mention to an antecedent span appearing before it in the document. This linear clustering method cannot be used in the multipledocument setting since there is no inherent ordering between the documents. Additionally, ECB+ (the main benchmark for CD coreference resolution) is relatively small compared to OntoNotes (Pradhan et al., 2012), making it hard to jointly optimize mention detection and coreference decision. These challenges have implications in all stages of model development, as elaborated below. Pre-training To address the small scale of the dataset, we pre-train the mention scorer sm (·) on the gold mention spans, as ECB+ includes singleton annotation. This enables generating good candidate spans from the first epoch, and as we show in Section 4.3, it substantially improves performance. Training Instead of comparing a mention only to its previous spans in the text, our pairwise scorer sa (i, j) compares a mention"
2021.findings-acl.453,S17-1019,1,0.924892,"car through the store’s front window. 2. Four men drove a 4x4 through the front window of the store on Rue de Castiglione, before making off with the jewellery and watches. Despite its importance for downstream tasks, CD coreference resolution has been lagging behind the 1 https://github.com/ariecattan/coref impressive strides made in the scope of a single document (Lee et al., 2017; Joshi et al., 2019, 2020; Wu et al., 2020). Further, state-of-the-art models exhibit several shortcomings, such as operating on gold mentions or relying on external resources such as SRL or a paraphrase dataset (Shwartz et al., 2017), preventing them from being applied on realistic settings. To address these limitations, we develop the first end-to-end CD coreference model building upon a prominent within-document (WD) coreference model (Lee et al., 2017) which we extend with recent advances in transformer-based encoders. We address the inherently non-linear nature of the CD setting by combining the WD coreference model with agglomerative clustering that was shown useful in CD models. Our model achieves competitive results on ECB+ over gold mentions and sets baseline results over predicted mentions. Our model is also simp"
2021.findings-acl.453,2020.acl-main.622,0,0.0308084,"where colors represent coreference clusters (for brevity, we omit some clusters): 1. Thieves pulled off a two million euro jewellery heist in central Paris on Monday after smashing their car through the store’s front window. 2. Four men drove a 4x4 through the front window of the store on Rue de Castiglione, before making off with the jewellery and watches. Despite its importance for downstream tasks, CD coreference resolution has been lagging behind the 1 https://github.com/ariecattan/coref impressive strides made in the scope of a single document (Lee et al., 2017; Joshi et al., 2019, 2020; Wu et al., 2020). Further, state-of-the-art models exhibit several shortcomings, such as operating on gold mentions or relying on external resources such as SRL or a paraphrase dataset (Shwartz et al., 2017), preventing them from being applied on realistic settings. To address these limitations, we develop the first end-to-end CD coreference model building upon a prominent within-document (WD) coreference model (Lee et al., 2017) which we extend with recent advances in transformer-based encoders. We address the inherently non-linear nature of the CD setting by combining the WD coreference model with agglomera"
2021.findings-emnlp.211,W18-2501,0,0.0121772,"models: mBART50_m2m (Tang et al., 2020; Liu et al., 2020), m2m_100_418M (Fan et al., 2020), and Opus-MT (Tiedemann and Thottingal, 2020), representing the state-of-the-art for publicly available neural machine translations models. We translated BUG from English to a set of eight diverse target languages with grammatical gender: Arabic, Czech, German, Spanish, Hebrew, Italian, Russian and Ukrainian, using tools developed in previous work to infer the translated gender based on morphological inflections (Stanovsky et al., 2019; Kocmi et al., 2020).6 Coreference resolution. We use the AllenNLP (Gardner et al., 2018) implementation of SpanBERT (Joshi et al., 2020). SpanBERT introduces contextual span representation to the the e2e-coreference model (Lee et al., 2018; Joshi et al., 2019) to achieve state-of-the-art results on the English portion of the popular CoNLL-2012 shared task coreference benchmark (Pradhan et al., 2012). 4.2 Metrics For each tested model we compute three metrics, following Zhao et al. (2018) and Stanovsky et al. 5 https://github.com/UKPLab/EasyNMT We used the implementation provided by github.com/ gabrielStanovsky/mt_gender 6 Accuracy: Denotes the F1 score of the gender prediction. F"
2021.findings-emnlp.211,2020.findings-emnlp.180,0,0.0500971,"Missing"
2021.findings-emnlp.211,2020.emnlp-main.209,0,0.0475616,"Missing"
2021.findings-emnlp.211,2020.tacl-1.5,0,0.0129046,"., 2020), m2m_100_418M (Fan et al., 2020), and Opus-MT (Tiedemann and Thottingal, 2020), representing the state-of-the-art for publicly available neural machine translations models. We translated BUG from English to a set of eight diverse target languages with grammatical gender: Arabic, Czech, German, Spanish, Hebrew, Italian, Russian and Ukrainian, using tools developed in previous work to infer the translated gender based on morphological inflections (Stanovsky et al., 2019; Kocmi et al., 2020).6 Coreference resolution. We use the AllenNLP (Gardner et al., 2018) implementation of SpanBERT (Joshi et al., 2020). SpanBERT introduces contextual span representation to the the e2e-coreference model (Lee et al., 2018; Joshi et al., 2019) to achieve state-of-the-art results on the English portion of the popular CoNLL-2012 shared task coreference benchmark (Pradhan et al., 2012). 4.2 Metrics For each tested model we compute three metrics, following Zhao et al. (2018) and Stanovsky et al. 5 https://github.com/UKPLab/EasyNMT We used the implementation provided by github.com/ gabrielStanovsky/mt_gender 6 Accuracy: Denotes the F1 score of the gender prediction. For machine translation, this indicates the perce"
2021.findings-emnlp.211,S18-2005,0,0.0477867,"Missing"
2021.findings-emnlp.211,2020.wmt-1.39,1,0.772473,"translation. We used EasyNMT5 to evaluate three machine translation models: mBART50_m2m (Tang et al., 2020; Liu et al., 2020), m2m_100_418M (Fan et al., 2020), and Opus-MT (Tiedemann and Thottingal, 2020), representing the state-of-the-art for publicly available neural machine translations models. We translated BUG from English to a set of eight diverse target languages with grammatical gender: Arabic, Czech, German, Spanish, Hebrew, Italian, Russian and Ukrainian, using tools developed in previous work to infer the translated gender based on morphological inflections (Stanovsky et al., 2019; Kocmi et al., 2020).6 Coreference resolution. We use the AllenNLP (Gardner et al., 2018) implementation of SpanBERT (Joshi et al., 2020). SpanBERT introduces contextual span representation to the the e2e-coreference model (Lee et al., 2018; Joshi et al., 2019) to achieve state-of-the-art results on the English portion of the popular CoNLL-2012 shared task coreference benchmark (Pradhan et al., 2012). 4.2 Metrics For each tested model we compute three metrics, following Zhao et al. (2018) and Stanovsky et al. 5 https://github.com/UKPLab/EasyNMT We used the implementation provided by github.com/ gabrielStanovsky/m"
2021.findings-emnlp.211,N18-2108,0,0.03544,"Missing"
2021.findings-emnlp.211,2020.tacl-1.47,0,0.0176558,"diverse target languages on the BUG dataset. Acc represents the overall accuracy (F1) of gender translation. ∆G is the difference in accuracy between masculine and feminine entities. ∆S is the difference in performance between stereotypical and antistereotypical gender role assignments. Positive ∆G and ∆S values indicate that the translations are gender biased. (2019), while adapting the terminology suggested recently by Mehrabi et al. (2021). many users. 4.1 Experimental Setup Machine translation. We used EasyNMT5 to evaluate three machine translation models: mBART50_m2m (Tang et al., 2020; Liu et al., 2020), m2m_100_418M (Fan et al., 2020), and Opus-MT (Tiedemann and Thottingal, 2020), representing the state-of-the-art for publicly available neural machine translations models. We translated BUG from English to a set of eight diverse target languages with grammatical gender: Arabic, Czech, German, Spanish, Hebrew, Italian, Russian and Ukrainian, using tools developed in previous work to infer the translated gender based on morphological inflections (Stanovsky et al., 2019; Kocmi et al., 2020).6 Coreference resolution. We use the AllenNLP (Gardner et al., 2018) implementation of SpanBERT (Joshi et"
2021.findings-emnlp.211,W12-4501,0,0.0105755,"mmatical gender: Arabic, Czech, German, Spanish, Hebrew, Italian, Russian and Ukrainian, using tools developed in previous work to infer the translated gender based on morphological inflections (Stanovsky et al., 2019; Kocmi et al., 2020).6 Coreference resolution. We use the AllenNLP (Gardner et al., 2018) implementation of SpanBERT (Joshi et al., 2020). SpanBERT introduces contextual span representation to the the e2e-coreference model (Lee et al., 2018; Joshi et al., 2019) to achieve state-of-the-art results on the English portion of the popular CoNLL-2012 shared task coreference benchmark (Pradhan et al., 2012). 4.2 Metrics For each tested model we compute three metrics, following Zhao et al. (2018) and Stanovsky et al. 5 https://github.com/UKPLab/EasyNMT We used the implementation provided by github.com/ gabrielStanovsky/mt_gender 6 Accuracy: Denotes the F1 score of the gender prediction. For machine translation, this indicates the percentage of instances in which a correct grammatical gender inflection was produced in the target language. For example translating a female doctor as doctor-a in Spanish. For coreference resolution accuracy refers to the portion of instances where the entity’s anteced"
2021.findings-emnlp.211,W17-1609,0,0.0690244,"Missing"
2021.findings-emnlp.211,N18-2002,0,0.0381227,"Missing"
2021.findings-emnlp.211,2020.gebnlp-1.4,0,0.049101,"Missing"
2021.findings-emnlp.211,2020.acl-demos.3,0,0.274451,"available at github.com/ SLAB-NLP/BUG. We hope they will spur future research into gender bias evaluation mitigation techniques in realistic settings. 1 Figure 1: We propose a semi-automatic method to vastly extend synthetic, small diagnostic datasets. We start with the texts of Winogender (Rudinger et al., 2018) and WinoBias (Zhao et al., 2018), specifically designed to to be challenging for coreference and machine translation (top), extract syntactic patterns focusing on the salient entities in the artificial sentences (middle), and query real-world datasets for matching texts, using SPIKE (Shlain et al., 2020). The result is a large collection of diverse real-world texts exhibiting similar challenging properties which lends itself to both finetuning and testing (bottom). Introduction Gender bias in machine learning occurs when supervised models predict based on spurious societal correlations in their training data. This may result in harmful behaviour when it occurs in models deployed in real-world applications (Caliskan et al., 2017; Buolamwini and Gebru, 2018; Bender et al., 2021).1 Recent work has quantified bias mostly using carefully designed templates, following the Winograd schema (Levesque"
2021.findings-emnlp.211,P19-1164,1,0.923376,"he Winograd schema (Levesque et al., 2012). Zhao et al. 1 We acknowledge that gender identity is non-binary. Throughout this work we refer to grammatical gender, which has categorical inflections in the discussed languages (e.g., masculine and feminine pronouns in English). (2018) and Rudinger et al. (2018) probed for gender bias in coreference resolution with templates portraying two human entities and a single pronoun. For example, given the sentence “the doctor asked the nurse to help her because she was busy”, models often erroneously cluster “her” with “nurse”, rather than with “doctor”. Stanovsky et al. (2019) used the same data to evaluate gender bias in machine translation. When translating this sentence to a language with grammatical gender, models tend to inflect nouns based on stereotypes, e.g., in Spanish, preferring the masculine inflection over the correct feminine inflection (“doctor-a”). While these experiments are useful for quanti2470 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2470–2480 November 7–11, 2021. ©2021 Association for Computational Linguistics fying gender bias in a controlled environment, we identify two shortcomings with this approach. Firs"
2021.findings-emnlp.211,2020.eamt-1.61,0,0.0110157,"overall accuracy (F1) of gender translation. ∆G is the difference in accuracy between masculine and feminine entities. ∆S is the difference in performance between stereotypical and antistereotypical gender role assignments. Positive ∆G and ∆S values indicate that the translations are gender biased. (2019), while adapting the terminology suggested recently by Mehrabi et al. (2021). many users. 4.1 Experimental Setup Machine translation. We used EasyNMT5 to evaluate three machine translation models: mBART50_m2m (Tang et al., 2020; Liu et al., 2020), m2m_100_418M (Fan et al., 2020), and Opus-MT (Tiedemann and Thottingal, 2020), representing the state-of-the-art for publicly available neural machine translations models. We translated BUG from English to a set of eight diverse target languages with grammatical gender: Arabic, Czech, German, Spanish, Hebrew, Italian, Russian and Ukrainian, using tools developed in previous work to infer the translated gender based on morphological inflections (Stanovsky et al., 2019; Kocmi et al., 2020).6 Coreference resolution. We use the AllenNLP (Gardner et al., 2018) implementation of SpanBERT (Joshi et al., 2020). SpanBERT introduces contextual span representation to the the e2e-"
2021.findings-emnlp.211,2020.nlpcovid19-acl.1,0,0.0237487,"ur dataset ensures at least a single human entity (marked by their profession) and a gendered pronoun, marked in bold. The sentences marked in blue are classified as anti-stereotypical while the sentences marked in orange are classified as stereotypical, and the sentence marked in green classified is neutral. The figure depicts 7 templates out of the 14 we designed. See the Appendix for a complete list. indexes large-scale corpora and retrieves matching instances given a lexical-syntactic pattern. We queried corpora from three domains: Wikipedia, PubMed abstracts, and Covid19 research papers (Wang et al., 2020). The examples in Figure 2 highlight the diversity of the approach, while they all adhere to one of the predefined patterns, they vary widely in vocabulary and in syntactic construction, often introducing complex phenomena, such as coordination or adverbial phrases. 2.2 Marking Entities and Gender Roles professions, taken from the U.S. census. Following, to mark each instance as either stereotypical or anti-stereotypical, we we follow Zhao et al. (2018) and Rudinger et al. (2018) and use the United States 2015 census’ gender distribution per occupation.4 For instance, the first example Figure"
2021.findings-emnlp.211,Q18-1042,0,0.0413119,"Missing"
2021.findings-emnlp.211,N18-2003,0,0.111122,"sented with natural inputs, which may be especially harmful when deployed in commercial systems. Finally, we show that our dataset lends itself to finetuning a coreference resolution model, finding it mitigates bias on a held out set. Our dataset and models are publicly available at github.com/ SLAB-NLP/BUG. We hope they will spur future research into gender bias evaluation mitigation techniques in realistic settings. 1 Figure 1: We propose a semi-automatic method to vastly extend synthetic, small diagnostic datasets. We start with the texts of Winogender (Rudinger et al., 2018) and WinoBias (Zhao et al., 2018), specifically designed to to be challenging for coreference and machine translation (top), extract syntactic patterns focusing on the salient entities in the artificial sentences (middle), and query real-world datasets for matching texts, using SPIKE (Shlain et al., 2020). The result is a large collection of diverse real-world texts exhibiting similar challenging properties which lends itself to both finetuning and testing (bottom). Introduction Gender bias in machine learning occurs when supervised models predict based on spurious societal correlations in their training data. This may result"
2021.findings-emnlp.259,2021.naacl-main.9,1,0.759325,"sk, observing that vanilla BERTstyle often masks ungrounded words like “umm” or “yeah”. We share the same motivation to mask highly visual words. 6.3 Challenges in VQA generalization Visual understanding Language and vision tasks inherently demand deep understanding of both the text and the image. However, many works show that models can succeed on VQA datasets using strong language priors, and by relying on superficial cues, and there are still challenges to overcome for tasks with more compositional structure (Jabri et al., 2016; Zhang et al., 2016; Goyal et al., 2017; Agarwal et al., 2020; Bitton et al., 2021; Dancette et al., 2021). Balanced datasets such as VQA 2.0 (Goyal et al., 2017) and GQA (Hudson and Manning, 2019) have been presented to address these challenges. Novel models with richer visual representations (Zhang et al., 2021) were also presented, and some works tried to encourage the model to look at the “correct” image regions (Liu et al., 2021; Yang et al., 2020). Bias Yang et al. (2021) and Hendricks et al. (2018) have shown that attention-based visionlanguage models suffer from bias that misleads the attention module to focus on spurious correlations in training data, and leads to"
2021.findings-emnlp.259,2020.tacl-1.5,0,0.0636908,"Missing"
2021.findings-emnlp.259,D19-1514,0,0.0944787,"ngs might be that the model is evaluated mostly on retrieving objects, and had we tested it on other classes, its performance would have substantially decreased. To test this hypothesis, we inspect the same model’s performance on questions with answers from different semantic types. To do so, we experiment with the GQA dataset, which includes partitioning of the answers into different semantic types, including Objects, Relations (subject or object of a described relation, e.g., “what is the girl wearing?&quot;), and Attributes (the properties or position of an object). Many works (Lu et al., 2019; Tan and Bansal, 2019; Chen et al., 2020) assume that a VLP model should include an MLM component that is capable of predicting every masked token, including objects, properties, but also stop words and punctuation. Does a model that uses our Objects strategy, and The results for the semantic type partition are masks only objects, learn to complete words from presented in Table 3. Comparing between the other classes? If not, can such a pre-training strat- models trained with Objects and Baseline MLM egy be effective? masking strategies, the Objects masking strategy To examine this questions, we extend the experi-"
2021.naacl-main.9,P19-1164,1,0.793014,"s from the scene graph. Bottom: relations among the objects in the scene graph. First line at the top is the original QA pair, while the following 3 lines show our pertubated questions: replacing a single element in the question (a fence) with other options (a wall, men, an elephant), leading to a change in the output label. For each QA pair, the LXMERT predicted output is shown. Introduction the out-of-domain performance of these models is often severely deteriorated (Jia and Liang, 2017; Ribeiro et al., 2018; Gururangan et al., 2018; Geva et al., 2019; McCoy et al., 2019; Feng et al., 2019; Stanovsky et al., 2019). Recently, Kaushik et al. (2019) and Gardner et al. (2020) introduced the contrast sets approach to probe out-of-domain generalization. Contrast sets are constructed via minimal modifications to test inputs, such that their label is modified. For example, in Fig. 1, replacing “a fence” with “a wall”, changes the answer NLP benchmarks typically evaluate in-distribution generalization, where test sets are drawn i.i.d from a distribution similar to the training set. Recent works showed that high performance on test sets sampled in this manner is often achieved by exploiting systematic gaps, anno"
2021.naacl-main.9,2020.emnlp-main.158,0,0.0161646,"his is due to model architecture or dataset design. Bogin et al. (2020) claim that both of these models are prone to fail on compositional generalization because they do not decompose the problem into smaller sub-tasks. Our results support this claim. On the other hand, it is possible that a different dataset could prevent these models from finding shortcuts. Is there a dataset that can prevent all shortcuts? Our automatic method for creating contrast sets allows us to ask those questions, while we believe that future work in better training mechanisms, as suggested in Bogin et al. (2020) and Jin et al. (2020), could help in making more robust models. We proposed an automatic method for creating contrast sets for VQA datasets that use annotated scene graphs. We created contrast sets for the GQA dataset, which is designed to be compositional, balanced, and robust against statistical biases. We observed a large performance drop between the original and augmented sets. As our contrast sets Mor Geva, Yoav Goldberg, and Jonathan Berant. 2019. Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets. In Proceedings of the 2019 Conference on"
2021.naacl-main.9,D19-1514,0,0.122313,"Missing"
2021.naacl-main.9,D18-1009,1,0.746347,"Missing"
2021.naacl-main.9,2020.blackboxnlp-1.12,0,0.0341525,"t 94 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 94–105 June 6–11, 2021. ©2021 Association for Computational Linguistics 2 from “Yes” to “No”. Since such perturbations introduce minimal additional semantic complexity, robust models are expected to perform similarly on the test and contrast sets. However, a range of NLP models severely degrade in performance on contrast sets, hinting that they do not generalize well (Gardner et al., 2020). Except two recent exceptions for textual datasets (Li et al., 2020; Rosenman et al., 2020), contrast sets have so far been built manually, requiring extensive human effort and expertise. Automatic Contrast Set Construction To construct automatic contrast sets for GQA we first identify a large subset of questions requiring specific reasoning skills (§2.1). Using the scene graph representation, we perturb each question in a manner which changes its gold answer (§2.2). Finally, we validate the automatic process via crowdsourcing (§2.3). 2.1 Identifying Recurring Patterns in GQA The questions in the GQA dataset present a diverse set of modelling challenges, as e"
2021.naacl-main.9,P19-1472,0,0.0438219,"Missing"
2021.nllp-1.4,W19-2204,0,0.0253964,". Sometimes this sentence also contains the conditioned punishment, for ex This annotation step resulted in 132 sentences annotated positively with either actual imprison ment or probation, while the remaining 13K sen tences were marked negatively, either automati cally or by human experts. This annotation aver aged 1.26 sentence marked positive for conveying the punishment per case, thus matching our intu ition that the punishment in each decision tends to 3 The high variance in decisions’ length in the legal domain is due in part to the difﬁculty in segmenting legal texts, as noted by Sanchez (2019). 38 All Annotated Cases Sentences Words 1043 100 181K 13K 3M 210K Sentence Length (words) Average length Min Max 16.5[±15.4] 0 433 25[±16.5] 3 321 Table 1: Statistics of our annotated data, referring to the full corpus as well as to the annotated subset. In both cases, each legal decision contains many sentences and the length of the decisions varies considerably. Hebrew example English translation Comments Punishment  עתרה התובעת,אשר על כן להטלת עונש מאסר בפועל על  למאסר,הנאשם לתקופה ממושכת על תנאי ולפיצוי משמעותי .למתלוננת Therefore, the prosecution requested to impose a punis"
2021.nllp-1.4,W19-2208,0,0.0627481,"Missing"
2021.starsem-1.13,P98-1012,0,0.749714,"Missing"
2021.starsem-1.13,P19-1409,1,0.929146,"reference introduces additional unique challenges. Most notably, lexical similarity is often not a good indicator when identifying cross-document links, as documents are authored independently. As shown in Table 1, the same event can be referenced using different expressions (“nominated”, “approached”), while two different events can be referenced using the same expression (“name”). Despite these challenges, reported state-of-the-art results on the 1 https://github.com/ariecattan/coref popular CD coreference ECB+ benchmark (Cybulska and Vossen, 2014) are relatively high, reaching up to 80 F1 (Barhom et al., 2019; Meged et al., 2020). In this paper, we show that CD coreference models achieve these numbers using overly-permissive evaluation protocols, namely assuming gold entity and event mentions are given, rewarding singletons and bypassing the lexical ambiguity challenge. Accordingly, we present more realistic evaluation principles which better reflect model performance in real-world scenarios. First, following well established standards in WD coreference resolution (Pradhan et al., 2012), we propose that CD coreference models should be also evaluated on predicted mentions. While recent models unrea"
2021.starsem-1.13,J14-2004,0,0.138225,"orld event (e.g., the nomination of Sanjay Gupta), and topics, which in turn consist of two lexically similar subtopics. Full ECB+ details are presented in Appendix A. The ECB+ evaluation protocol largely follows that of CoNLL-2012, perhaps the most popular WD benchmark (Pradhan et al., 2012), with two major distinctions. First, barring a few notable exceptions (Yang et al., 2015; Choubey and Huang, 2017),2 most recent CD models have unrealistically assumed that gold entity and event mentions are given as part of the input, reducing the task to finding coreference links between gold mentions (Bejan and Harabagiu, 2014; Cybulska and Vossen, 2015; Kenyon-Dean et al., 2018; Barhom et al., 2019; Meged et al., 2020). Second, while singletons are omitted on CoNLL-2012, they are exhaustively annotated in ECB+. In the following section, we present a more realistic evaluation framework for CD coreference, taking into account the interacting distinctions of ECB+. 3 Realistic Evaluation Principles In this paper, we suggest that CD coreference models should perform and be evaluated on predicted mentions. To achieve this, in Section 3.1, we will introduce the singleton effect on coreference evaluation and propose to de"
2021.starsem-1.13,P10-1143,0,0.301535,"e propose that models report performance also at the topic level. Finally, we show empirically that both of these evaluation practices artificially inflate results. An end-to end model that outperforms state-of-the-art results on previous evaluation settings drops by 33 F1 points when using our proposed evaluation scheme, pointing at weaknesses that future modelling work could explore. 2 Background In this work, we will examine the evaluation of CD coreference on the popular ECB+ corpus (Cybulska and Vossen, 2014), constructed as an augmentation of the EECB and ECB datasets (Lee et al., 2012; Bejan and Harabagiu, 2010). As exemplified in Table 1, ECB+ groups its annotated documents into subtopics, consisting of different reports of the same real-world event (e.g., the nomination of Sanjay Gupta), and topics, which in turn consist of two lexically similar subtopics. Full ECB+ details are presented in Appendix A. The ECB+ evaluation protocol largely follows that of CoNLL-2012, perhaps the most popular WD benchmark (Pradhan et al., 2012), with two major distinctions. First, barring a few notable exceptions (Yang et al., 2015; Choubey and Huang, 2017),2 most recent CD models have unrealistically assumed that go"
2021.starsem-1.13,2021.findings-acl.453,1,0.690881,"original evaluation setup of the ECB+ corpus (Bejan and Harabagiu, 2014). 4 clusion, and subtopic clustering) artificially inflates the results (§4.2). As recent CD coreference models are designed to perform on gold mentions (§2), we cannot use them to set baseline results on predicted mentions. We therefore develop a simple and efficient end-to-end model for CD coreference resolution by combining the successful single document e2e-coref (Lee et al., 2017) with common CD modeling approaches. 4.1 Model We briefly describe the general architecture of our model, further details are explained in (Cattan et al., 2021) and Appendix C. Given a set of documents, our model operates in four sequential steps: (1) following Lee et al. (2017), we encode all possible spans up to a length n with the concatenation of four vectors: the output representations of the span boundary (first and last) tokens, an attentionweighted sum of token representations in the span, and a feature vector denoting the span length (2) we train a mention detector on the ECB+ mentions, and keep further spans with a positive score,5 (3) we generate positive and negative coreference pairs on the predicted mentions and train a pairwise scorer,"
2021.starsem-1.13,D19-1588,1,0.874627,"Missing"
2021.starsem-1.13,S18-2001,0,0.270421,"topics, which in turn consist of two lexically similar subtopics. Full ECB+ details are presented in Appendix A. The ECB+ evaluation protocol largely follows that of CoNLL-2012, perhaps the most popular WD benchmark (Pradhan et al., 2012), with two major distinctions. First, barring a few notable exceptions (Yang et al., 2015; Choubey and Huang, 2017),2 most recent CD models have unrealistically assumed that gold entity and event mentions are given as part of the input, reducing the task to finding coreference links between gold mentions (Bejan and Harabagiu, 2014; Cybulska and Vossen, 2015; Kenyon-Dean et al., 2018; Barhom et al., 2019; Meged et al., 2020). Second, while singletons are omitted on CoNLL-2012, they are exhaustively annotated in ECB+. In the following section, we present a more realistic evaluation framework for CD coreference, taking into account the interacting distinctions of ECB+. 3 Realistic Evaluation Principles In this paper, we suggest that CD coreference models should perform and be evaluated on predicted mentions. To achieve this, in Section 3.1, we will introduce the singleton effect on coreference evaluation and propose to decouple the evaluation of mention prediction from core"
2021.starsem-1.13,D12-1045,0,0.178213,"To address this, we propose that models report performance also at the topic level. Finally, we show empirically that both of these evaluation practices artificially inflate results. An end-to end model that outperforms state-of-the-art results on previous evaluation settings drops by 33 F1 points when using our proposed evaluation scheme, pointing at weaknesses that future modelling work could explore. 2 Background In this work, we will examine the evaluation of CD coreference on the popular ECB+ corpus (Cybulska and Vossen, 2014), constructed as an augmentation of the EECB and ECB datasets (Lee et al., 2012; Bejan and Harabagiu, 2010). As exemplified in Table 1, ECB+ groups its annotated documents into subtopics, consisting of different reports of the same real-world event (e.g., the nomination of Sanjay Gupta), and topics, which in turn consist of two lexically similar subtopics. Full ECB+ details are presented in Appendix A. The ECB+ evaluation protocol largely follows that of CoNLL-2012, perhaps the most popular WD benchmark (Pradhan et al., 2012), with two major distinctions. First, barring a few notable exceptions (Yang et al., 2015; Choubey and Huang, 2017),2 most recent CD models have unr"
2021.starsem-1.13,D17-1018,0,0.0501395,"recommend that models report results also at the topic level (when document clustering is not applied). This will conform to ECB+’s purpose and follows the original evaluation setup of the ECB+ corpus (Bejan and Harabagiu, 2014). 4 clusion, and subtopic clustering) artificially inflates the results (§4.2). As recent CD coreference models are designed to perform on gold mentions (§2), we cannot use them to set baseline results on predicted mentions. We therefore develop a simple and efficient end-to-end model for CD coreference resolution by combining the successful single document e2e-coref (Lee et al., 2017) with common CD modeling approaches. 4.1 Model We briefly describe the general architecture of our model, further details are explained in (Cattan et al., 2021) and Appendix C. Given a set of documents, our model operates in four sequential steps: (1) following Lee et al. (2017), we encode all possible spans up to a length n with the concatenation of four vectors: the output representations of the span boundary (first and last) tokens, an attentionweighted sum of token representations in the span, and a feature vector denoting the span length (2) we train a mention detector on the ECB+ mention"
2021.starsem-1.13,2021.ccl-1.108,0,0.0850703,"Missing"
2021.starsem-1.13,H05-1004,0,0.113638,"tons are already evaluated under the mention detection evaluation. We note also that even when omitting singletons, coreference metrics still penalize models for making coreference errors involving singletons (as S2 is penalized for linking “announcement” to a cluster). We further show empirically (§4.2) that when evaluating using gold mentions, the singleton effect is amplified and harms the validity of the current CD evaluation protocol. Evidently, a dummy baseline that predicts no coreference links and puts each input gold mention in a singleton cluster achieves non-negligible performance (Luo, 2005), while state-of-the-art results are artificially inflated. 3.2 Confronting Lexical Ambiguity As mentioned previously, the same event can be described in documents from different topics, while documents in the same topic may describe different events (e.g. different nominations as surgeon general, as shown in Table 1). Such settings pose a lexical ambiguity problem, where models encounter identical or lexically-similar words that should be assigned to different coreference clusters. Accordingly, while topical document clustering is useful for CD coreference resolution in general, it does not s"
2021.starsem-1.13,D09-1101,0,0.0515912,"and (2) using coreference metrics also on singleton prediction. With respect to (1), S2 achieves higher results according to all evaluation metrics. In (2), we see the opposite, the results of S1 are significantly higher than S2 w.r.t B3 (+18.4), CEAF-e (+45.1), and LEA (+19), but not w.r.t MUC, a link-based metric. Indeed, these evaluation metrics reward S1 in both recall and precision for all predicted singletons, while penalizing S2 for the wrong and missing singleton spans. Since singletons are abundant in natural text, they contribute greatly to the overall score. However, as observed by Rahman and Ng (2009), a model’s ability to identify that these singletons do not belong to any coreference cluster is already captured in the evaluation metrics, and additional penalty is not desired. In Appendix B, we introduce the aforementioned evaluation metrics for coreference resolution (MUC, B3 , CEAF and LEA) and explain how singletons affect them. To address the singleton effect, we suggest decoupling the evaluation of the two coreference substasks, mention detection and coreference linking, allowing to better analyze coreference results and to compare systems more appropriately.4 Mention detection is ty"
2021.starsem-1.13,P09-1074,0,0.059586,"ttings pose a lexical ambiguity problem, where models encounter identical or lexically-similar words that should be assigned to different coreference clusters. Accordingly, while topical document clustering is useful for CD coreference resolution in general, it does not solve the ambiguity problem and models still need to make subtle disambiguation distinctions (e.g nomination of Sanjay Gupta vs. nomination of Regina Benjamin). Aiming at simulating this chal145 4 This also makes possible to compare coreference results across datasets that include/omit singletons, addressing an issue raised by Stoyanov et al. (2009). B3 MUC Subtopic Clustering Topic Level CEAF e LEA CoNLL R P F1 R P F1 R P F1 R P F1 F1 Singleton baseline+ Singleton baseline− Barhom et al. (2019)+ Barhom et al. (2019)− Meged et al. (2020)+ Meged et al. (2020)− 0 0 78.1 78.1 78.8 78.8 0 0 84.0 84.0 84.7 84.7 0 0 80.9 80.9 81.6 81.6 45.2 0 76.8 61.2 75.9 60.4 100 0 86.1 73.5 85.9 73.8 62.3 0 81.2 66.8 80.6 66.4 86.7 0 79.6 63.2 81.1 65.5 39.2 0 73.3 48.9 74.8 49.5 54.0 0 76.3 55.2 77.8 56.4 35.0 0 64.6 58.4 64.7 57.2 35.0 0 72.3 71.2 73.4 71.2 35.0 0 68.3 64.2 68.8 63.4 38.8 0 79.5 67.6 80.0 68.1 Our model – Gold+ Our model – Gold− Our mode"
2021.starsem-1.13,M95-1005,0,0.860963,"Missing"
C16-1272,S12-1051,0,0.158401,"rase detection (Dolan et al., 2004), for example, tries to identify whether two texts express the same information. It cannot, however, capture cases where there is only partial information overlap. One paradigm that addresses this issue is textual entailment (Dagan et al., 2006), which asks whether one text can be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for these cases by measuring “how much” similar two sentences are. In this paper, we consider an additional paradigm for modeling the semantic overlap between two texts. The task of sentence intersection, as defined in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which"
C16-1272,J05-3002,0,0.134834,"Missing"
C16-1272,P99-1071,0,0.315577,"Missing"
C16-1272,W04-1016,0,0.0794161,"Missing"
C16-1272,C04-1051,0,0.749669,"e it is difficult for non-experts to annotate. We analyze 200 pairs of similar sentences and identify several underlying properties of sentence intersection. We leverage these insights to design an algorithm that decomposes the sentence intersection task into several simpler annotation tasks, facilitating the construction of a high quality dataset via crowdsourcing. We implement this approach and provide an annotated dataset of 1,764 sentence intersections. 1 Introduction Various paradigms exist for comparing the meanings of two texts and modeling their semantic overlap. Paraphrase detection (Dolan et al., 2004), for example, tries to identify whether two texts express the same information. It cannot, however, capture cases where there is only partial information overlap. One paradigm that addresses this issue is textual entailment (Dagan et al., 2006), which asks whether one text can be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for th"
C16-1272,D08-1019,0,0.068328,"Missing"
C16-1272,P08-2049,0,0.0504803,"Missing"
C16-1272,E14-1057,0,0.0558176,"Missing"
C16-1272,P16-2041,1,0.833208,"sist in defining the underlying structure of extractive intersection. Subtree entailment in context models inference between rich lexical-syntactic patterns (subtrees of syntactic dependency trees), while considering internal context (instantiated arguments) and external context (substitution in a complete sentence). As such, it generalizes over previous ideas presented separately in prior art; subtrees were used in TEASE (Szpektor et al., 2015) and PPDB (Pavlick et al., 2015), internal context was considered in contextsensitive relation inference (Zeichner et al., 2012; Melamud et al., 2013; Levy and Dagan, 2016), and external context is studied in the lexical substitution task (McCarthy and Navigli, 2007; Biemann, 2013; Kremer et al., 2014). Subtree entailment in context is the first notion that combines all these traits. In addition to these advantages, we also introduce a new mechanism for handling changes in arity, in case one subtree contains more/less arguments than another. The Substitution Function To define subtree entailment in context, we must first define an auxiliary operation – subtree substitution. The substitution function σ is given a sentence tree s, a subtree within that sentence t,"
C16-1272,P13-2080,1,0.857695,"d in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which generalizes over several previous approaches. In particular, it is expressive enough to capture partial semantic overlaps that are not modeled by textual entailment or even partial textual entailment (Nielsen et al., 2009; Levy et al., 2013). Furthermore, rather than quantifying the amount of shared information as in semantic text similarity, sentence intersection captures what this shared information is. Although sentence intersection has existed for over a decade, it has received little attention due to a lack of annotated data. Previous annotation attempts have either used experts, which did not scale, or crowdsourcing, which yielded unreliable annotations (McKeown et al., 2010). We also observe that annotating sentence intersection is difficult for non-experts. We hypothesize that this difficulty stems from the task’s require"
C16-1272,W04-1013,0,0.0536885,"ria – output set and extractiveness – we define extractive sentence intersection as the set of all sentences that each contains all the information common to the input sentences, while being composed only of words that appeared in the input sentences and placeholders.2 The entire set of extractive intersections allows for more accurate automatic evaluation, since the evaluation mechanism does not need to overcome issues in lexical variability; instead, it can simply select the most similar expert-crafted sentence from the set using simple metrics such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004). Having multiple possible solutions is not a foreign concept to NLP, and is widely used in translation and summarization. 2 While this paper discusses intersections between two input sentences, one can theoretically extend this setting to multiple input sentences by consecutively applying the intersection operation. For example, if we have three sentences s1 , s2 , s3 , we could first find the intersection between s1 and s2 , and then for each sentence s0 in s1 ∩ s2 , intersect S that with s3 . We would essentially take the union of the latter set of intersection sets, i.e. s1 ∩ s2 ∩ s3 = (s1"
C16-1272,W05-1612,0,0.420831,"et al., 2006), which asks whether one text can be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for these cases by measuring “how much” similar two sentences are. In this paper, we consider an additional paradigm for modeling the semantic overlap between two texts. The task of sentence intersection, as defined in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which generalizes over several previous approaches. In particular, it is expressive enough to capture partial semantic overlaps that are not modeled by textual entailment or even partial textual entailment (Nielsen et al., 2009; Levy et al., 2013). Furthermore, rather than q"
C16-1272,S07-1009,0,0.0447691,"in context models inference between rich lexical-syntactic patterns (subtrees of syntactic dependency trees), while considering internal context (instantiated arguments) and external context (substitution in a complete sentence). As such, it generalizes over previous ideas presented separately in prior art; subtrees were used in TEASE (Szpektor et al., 2015) and PPDB (Pavlick et al., 2015), internal context was considered in contextsensitive relation inference (Zeichner et al., 2012; Melamud et al., 2013; Levy and Dagan, 2016), and external context is studied in the lexical substitution task (McCarthy and Navigli, 2007; Biemann, 2013; Kremer et al., 2014). Subtree entailment in context is the first notion that combines all these traits. In addition to these advantages, we also introduce a new mechanism for handling changes in arity, in case one subtree contains more/less arguments than another. The Substitution Function To define subtree entailment in context, we must first define an auxiliary operation – subtree substitution. The substitution function σ is given a sentence tree s, a subtree within that sentence t, and another subtree t0 , which is not necessarily part of s. It creates a new sentence s0 by"
C16-1272,N10-1044,0,0.285035,"s whether one text can be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for these cases by measuring “how much” similar two sentences are. In this paper, we consider an additional paradigm for modeling the semantic overlap between two texts. The task of sentence intersection, as defined in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which generalizes over several previous approaches. In particular, it is expressive enough to capture partial semantic overlaps that are not modeled by textual entailment or even partial textual entailment (Nielsen et al., 2009; Levy et al., 2013). Furthermore, rather than quantifying the amount"
C16-1272,P13-1131,1,0.858605,"context, which will assist in defining the underlying structure of extractive intersection. Subtree entailment in context models inference between rich lexical-syntactic patterns (subtrees of syntactic dependency trees), while considering internal context (instantiated arguments) and external context (substitution in a complete sentence). As such, it generalizes over previous ideas presented separately in prior art; subtrees were used in TEASE (Szpektor et al., 2015) and PPDB (Pavlick et al., 2015), internal context was considered in contextsensitive relation inference (Zeichner et al., 2012; Melamud et al., 2013; Levy and Dagan, 2016), and external context is studied in the lexical substitution task (McCarthy and Navigli, 2007; Biemann, 2013; Kremer et al., 2014). Subtree entailment in context is the first notion that combines all these traits. In addition to these advantages, we also introduce a new mechanism for handling changes in arity, in case one subtree contains more/less arguments than another. The Substitution Function To define subtree entailment in context, we must first define an auxiliary operation – subtree substitution. The substitution function σ is given a sentence tree s, a subtree"
C16-1272,N04-1019,0,0.0377941,"Missing"
C16-1272,P02-1040,0,0.0954815,"scope. Combining both these criteria – output set and extractiveness – we define extractive sentence intersection as the set of all sentences that each contains all the information common to the input sentences, while being composed only of words that appeared in the input sentences and placeholders.2 The entire set of extractive intersections allows for more accurate automatic evaluation, since the evaluation mechanism does not need to overcome issues in lexical variability; instead, it can simply select the most similar expert-crafted sentence from the set using simple metrics such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004). Having multiple possible solutions is not a foreign concept to NLP, and is widely used in translation and summarization. 2 While this paper discusses intersections between two input sentences, one can theoretically extend this setting to multiple input sentences by consecutively applying the intersection operation. For example, if we have three sentences s1 , s2 , s3 , we could first find the intersection between s1 and s2 , and then for each sentence s0 in s1 ∩ s2 , intersect S that with s3 . We would essentially take the union of the latter set of intersection sets, i."
C16-1272,P15-2070,0,0.0224273,"Missing"
C16-1272,P13-1051,0,0.0304221,"Missing"
C16-1272,W11-1606,0,0.0173189,"be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for these cases by measuring “how much” similar two sentences are. In this paper, we consider an additional paradigm for modeling the semantic overlap between two texts. The task of sentence intersection, as defined in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which generalizes over several previous approaches. In particular, it is expressive enough to capture partial semantic overlaps that are not modeled by textual entailment or even partial textual entailment (Nielsen et al., 2009; Levy et al., 2013). Furthermore, rather than quantifying the amount of shared information as in"
C16-1272,I13-1198,0,0.0305508,"Missing"
C16-1272,P12-2031,1,0.836148,"subtree entailment in context, which will assist in defining the underlying structure of extractive intersection. Subtree entailment in context models inference between rich lexical-syntactic patterns (subtrees of syntactic dependency trees), while considering internal context (instantiated arguments) and external context (substitution in a complete sentence). As such, it generalizes over previous ideas presented separately in prior art; subtrees were used in TEASE (Szpektor et al., 2015) and PPDB (Pavlick et al., 2015), internal context was considered in contextsensitive relation inference (Zeichner et al., 2012; Melamud et al., 2013; Levy and Dagan, 2016), and external context is studied in the lexical substitution task (McCarthy and Navigli, 2007; Biemann, 2013; Kremer et al., 2014). Subtree entailment in context is the first notion that combines all these traits. In addition to these advantages, we also introduce a new mechanism for handling changes in arity, in case one subtree contains more/less arguments than another. The Substitution Function To define subtree entailment in context, we must first define an auxiliary operation – subtree substitution. The substitution function σ is given a sente"
C16-1272,N07-1051,0,\N,Missing
D16-1086,P15-1034,0,0.306749,"(Open IE) is to extract coherent propositions from a sentence, each represented as a tuple of a relation phrase and one or more argument phrases (e.g., born in (Barack Obama; Hawaii)). Open IE has been shown to be useful for a wide range of semantic tasks, including question answering (Fader et al., 2014), summarization (Christensen et al., 2013) and text comprehension (Stanovsky et al., 2015), and has consequently drawn consistent attention over the last years (Banko et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015). Although similar applications of Open IE in other languages are obvious, most previous work focused 1 Source code and online demo available at https://github.com/UKPLab/props-de In this paper, we study whether an existing set of rules to extract Open IE tuples from English dependency parses can be ported to another language. We use German, a relatively close language, and the PropS system (Stanovsky et al., 2016) as examples in our analysis. Instead of creating rule sets from scratch, such a transfer approach would simplify the rule creation, making it possible to build Open IE systems for o"
D16-1086,Q13-1034,0,0.0531775,"Missing"
D16-1086,N13-1136,0,0.0459881,"Missing"
D16-1086,W08-1301,0,0.0581554,"Missing"
D16-1086,D11-1142,0,0.111728,"ly usable in downstream applications.1 1 Introduction The goal of Open Information Extraction (Open IE) is to extract coherent propositions from a sentence, each represented as a tuple of a relation phrase and one or more argument phrases (e.g., born in (Barack Obama; Hawaii)). Open IE has been shown to be useful for a wide range of semantic tasks, including question answering (Fader et al., 2014), summarization (Christensen et al., 2013) and text comprehension (Stanovsky et al., 2015), and has consequently drawn consistent attention over the last years (Banko et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015). Although similar applications of Open IE in other languages are obvious, most previous work focused 1 Source code and online demo available at https://github.com/UKPLab/props-de In this paper, we study whether an existing set of rules to extract Open IE tuples from English dependency parses can be ported to another language. We use German, a relatively close language, and the PropS system (Stanovsky et al., 2016) as examples in our analysis. Instead of creating rule sets from scratch, such a trans"
D16-1086,L16-1146,1,0.879747,"Missing"
D16-1086,D12-1048,0,0.116203,"roduction The goal of Open Information Extraction (Open IE) is to extract coherent propositions from a sentence, each represented as a tuple of a relation phrase and one or more argument phrases (e.g., born in (Barack Obama; Hawaii)). Open IE has been shown to be useful for a wide range of semantic tasks, including question answering (Fader et al., 2014), summarization (Christensen et al., 2013) and text comprehension (Stanovsky et al., 2015), and has consequently drawn consistent attention over the last years (Banko et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015). Although similar applications of Open IE in other languages are obvious, most previous work focused 1 Source code and online demo available at https://github.com/UKPLab/props-de In this paper, we study whether an existing set of rules to extract Open IE tuples from English dependency parses can be ported to another language. We use German, a relatively close language, and the PropS system (Stanovsky et al., 2016) as examples in our analysis. Instead of creating rule sets from scratch, such a transfer approach would simplify the rule creation"
D16-1086,D13-1043,0,0.470644,"Missing"
D16-1086,moriceau-tannier-2014-french,0,0.0589965,"Missing"
D16-1086,L16-1262,0,0.0678305,"Missing"
D16-1086,seeker-kuhn-2012-making,0,0.0610596,"Missing"
D16-1086,P15-2050,1,0.887842,"Missing"
D16-1086,D15-1063,0,0.0553119,"Missing"
D16-1086,P10-1013,0,0.140857,"r English and readily usable in downstream applications.1 1 Introduction The goal of Open Information Extraction (Open IE) is to extract coherent propositions from a sentence, each represented as a tuple of a relation phrase and one or more argument phrases (e.g., born in (Barack Obama; Hawaii)). Open IE has been shown to be useful for a wide range of semantic tasks, including question answering (Fader et al., 2014), summarization (Christensen et al., 2013) and text comprehension (Stanovsky et al., 2015), and has consequently drawn consistent attention over the last years (Banko et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015). Although similar applications of Open IE in other languages are obvious, most previous work focused 1 Source code and online demo available at https://github.com/UKPLab/props-de In this paper, we study whether an existing set of rules to extract Open IE tuples from English dependency parses can be ported to another language. We use German, a relatively close language, and the PropS system (Stanovsky et al., 2016) as examples in our analysis. Instead of creating rule sets from s"
D16-1252,W12-3010,0,0.0379506,"Missing"
D16-1252,P15-1034,0,0.409347,"Christensen et al., 2013; Balasubramanian et al., 2013). In parallel, many Open IE extractors were developed. TextRunner (Banko et al., 2007) and WOE (Wu and Weld, 2010) take a self-supervised approach over automatically produced dependency parses. Perhaps more dominant is the rule based approach taken by ReVerb (Fader et al., 2011), OLLIE (Mausam et al., 2012), KrakeN (Akbik and L¨oser, 2012) and ClausIE (Del Corro and Gemulla, 2013). Two recent systems take a semantically-oriented approach. Open IE-42 uses semantic role labeling to extract tuples, while Stanford Open Information Extraction (Angeli et al., 2015) uses natural logic inference to arrive at shorter, more salient, arguments. Recently, Stanovsky et al. (2016b) presented PropS, a proposition oriented representation, obtained via conversion rules from dependency trees. Performing Open IE extraction over PropS structures is straightforward – follow the clearly marked predicated nodes to their direct arguments. Contrary to the vast interest in Open IE, its task formulation has been largely overlooked. There are currently no common guidelines defining a valid extraction, which consequently hinders the creation of an evaluation benchmark for the"
D16-1252,P98-1013,0,0.0522963,"ent objectives. Particularly, SRL identifies argument role labels, which is not addressed in Open IE. Yet, the two tasks overlap as they both need to recover predicate-argument structures in sentences. We now examine the above Open IE requirements and suggest that while they are only partly embedded within SRL structures, they can be fully recovered from QA-SRL. Asserted (matrix) propositions appear in SRL as non-embedded predicates (e.g., succeeded in the “Sam succeeded to convince John”). However, SRL’s predicates are grounded to a lexicon such as PropBank (Palmer et al., 2005) or FrameNet (Baker et al., 1998), which violates the completeness and open lexicon principle. Further, in contrast to the minimal propositions principle, arguments in SRL annotations are inclusive, each marked as full subtrees in a syntactic parse. Yet, QA-SRL seems to bridge this gap between traditional SRL structures and Open IE requirements. Its predicate vocabulary is open, and its question-answer format solicits minimal propositions, as was found in a recent study by (Stanovsky et al., 2016a). This correlation suggests that the QASRL methodology is in fact also an attractive means for soliciting Open IE extractions from"
D16-1252,D13-1178,0,0.0509507,"ance against previous baselines, alleviating the current need for ad-hoc evaluation. 2 Background 2.1 Open IE Open Information Extraction (Open IE) was introduced as an open variant of traditional Information Extraction (Etzioni et al., 2008). As mentioned in the Introduction, its primary goal is to extract coherent propositions from a sentence, each comprising of a relation phrase and two or more argument phrases (e.g., (Barack Obama, born in, Hawaii)). Since its inception, Open IE has gained consistent attention, mostly used as a component within larger frameworks (Christensen et al., 2013; Balasubramanian et al., 2013). In parallel, many Open IE extractors were developed. TextRunner (Banko et al., 2007) and WOE (Wu and Weld, 2010) take a self-supervised approach over automatically produced dependency parses. Perhaps more dominant is the rule based approach taken by ReVerb (Fader et al., 2011), OLLIE (Mausam et al., 2012), KrakeN (Akbik and L¨oser, 2012) and ClausIE (Del Corro and Gemulla, 2013). Two recent systems take a semantically-oriented approach. Open IE-42 uses semantic role labeling to extract tuples, while Stanford Open Information Extraction (Angeli et al., 2015) uses natural logic inference to ar"
D16-1252,W05-0620,0,0.217559,"Missing"
D16-1252,N13-1136,0,0.0112423,"sily compare their performance against previous baselines, alleviating the current need for ad-hoc evaluation. 2 Background 2.1 Open IE Open Information Extraction (Open IE) was introduced as an open variant of traditional Information Extraction (Etzioni et al., 2008). As mentioned in the Introduction, its primary goal is to extract coherent propositions from a sentence, each comprising of a relation phrase and two or more argument phrases (e.g., (Barack Obama, born in, Hawaii)). Since its inception, Open IE has gained consistent attention, mostly used as a component within larger frameworks (Christensen et al., 2013; Balasubramanian et al., 2013). In parallel, many Open IE extractors were developed. TextRunner (Banko et al., 2007) and WOE (Wu and Weld, 2010) take a self-supervised approach over automatically produced dependency parses. Perhaps more dominant is the rule based approach taken by ReVerb (Fader et al., 2011), OLLIE (Mausam et al., 2012), KrakeN (Akbik and L¨oser, 2012) and ClausIE (Del Corro and Gemulla, 2013). Two recent systems take a semantically-oriented approach. Open IE-42 uses semantic role labeling to extract tuples, while Stanford Open Information Extraction (Angeli et al., 2015) use"
D16-1252,D11-1142,0,0.182854,"defined that an Open IE extractor should “produce one triple for every relation stated explicitly in the text, but is not required to infer implicit facts”. For example, given the sentence “John managed to open the door” an Open IE extractor should produce the tuple (John; managed to open; the door) but is not required to produce the extraction (John; opened; the door). 1 Publicly available at http://www.cs.biu.ac.il/ nlp/resources/downloads Following this initial presentation of the task, Open IE has gained substantial and consistent attention. Many automatic extractors were created (e.g., (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013)) and were put to use in various downstream applications. In spite of this wide attention, Open IE’s formal definition is lacking. There are no clear guidelines as to what constitutes a valid proposition to be extracted, and subsequently there is no large scale benchmark annotation. Open IE evaluations therefore usually consist of a post-hoc manual evaluation of a small output sample. This evaluation practice lacks in several respects: (1) Most works provide a precision oriented metric, whereas recall is often not measured, (2) the numbers are"
D16-1252,D15-1076,0,0.169036,"ice lacks in several respects: (1) Most works provide a precision oriented metric, whereas recall is often not measured, (2) the numbers are not comparable across systems, as they use different guidelines and datasets, and (3) the experiments are hard to replicate. In this work, we aim to contribute to the standardization of Open IE evaluation by providing a large gold benchmark corpus. For that end, we first identify consensual guiding principles across prominent Open IE systems, resulting in a clearer formulation of the Open IE task. Following, we find that the recent formulation of QA-SRL (He et al., 2015) in fact subsumes these requirements for Open IE. This enables us to automatically convert the annotations of QA-SRL to a high-quality Open IE corpus of more than 10K extractions, 13 times larger than the previous largest Open IE annotation. Finally, we automatically evaluate the performance of various Open IE systems against our corpus, using a soft matching criterion. This is the first 2300 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2300–2305, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics time such a com"
D16-1252,D12-1048,0,0.259461,"n IE extractor should “produce one triple for every relation stated explicitly in the text, but is not required to infer implicit facts”. For example, given the sentence “John managed to open the door” an Open IE extractor should produce the tuple (John; managed to open; the door) but is not required to produce the extraction (John; opened; the door). 1 Publicly available at http://www.cs.biu.ac.il/ nlp/resources/downloads Following this initial presentation of the task, Open IE has gained substantial and consistent attention. Many automatic extractors were created (e.g., (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013)) and were put to use in various downstream applications. In spite of this wide attention, Open IE’s formal definition is lacking. There are no clear guidelines as to what constitutes a valid proposition to be extracted, and subsequently there is no large scale benchmark annotation. Open IE evaluations therefore usually consist of a post-hoc manual evaluation of a small output sample. This evaluation practice lacks in several respects: (1) Most works provide a precision oriented metric, whereas recall is often not measured, (2) the numbers are not comparable acros"
D16-1252,J05-1004,0,0.0369733,"n IE have been defined with different objectives. Particularly, SRL identifies argument role labels, which is not addressed in Open IE. Yet, the two tasks overlap as they both need to recover predicate-argument structures in sentences. We now examine the above Open IE requirements and suggest that while they are only partly embedded within SRL structures, they can be fully recovered from QA-SRL. Asserted (matrix) propositions appear in SRL as non-embedded predicates (e.g., succeeded in the “Sam succeeded to convince John”). However, SRL’s predicates are grounded to a lexicon such as PropBank (Palmer et al., 2005) or FrameNet (Baker et al., 1998), which violates the completeness and open lexicon principle. Further, in contrast to the minimal propositions principle, arguments in SRL annotations are inclusive, each marked as full subtrees in a syntactic parse. Yet, QA-SRL seems to bridge this gap between traditional SRL structures and Open IE requirements. Its predicate vocabulary is open, and its question-answer format solicits minimal propositions, as was found in a recent study by (Stanovsky et al., 2016a). This correlation suggests that the QASRL methodology is in fact also an attractive means for so"
D16-1252,P15-2050,1,0.818442,"Missing"
D16-1252,P10-1013,0,0.0163434,"n Extraction (Open IE) was introduced as an open variant of traditional Information Extraction (Etzioni et al., 2008). As mentioned in the Introduction, its primary goal is to extract coherent propositions from a sentence, each comprising of a relation phrase and two or more argument phrases (e.g., (Barack Obama, born in, Hawaii)). Since its inception, Open IE has gained consistent attention, mostly used as a component within larger frameworks (Christensen et al., 2013; Balasubramanian et al., 2013). In parallel, many Open IE extractors were developed. TextRunner (Banko et al., 2007) and WOE (Wu and Weld, 2010) take a self-supervised approach over automatically produced dependency parses. Perhaps more dominant is the rule based approach taken by ReVerb (Fader et al., 2011), OLLIE (Mausam et al., 2012), KrakeN (Akbik and L¨oser, 2012) and ClausIE (Del Corro and Gemulla, 2013). Two recent systems take a semantically-oriented approach. Open IE-42 uses semantic role labeling to extract tuples, while Stanford Open Information Extraction (Angeli et al., 2015) uses natural logic inference to arrive at shorter, more salient, arguments. Recently, Stanovsky et al. (2016b) presented PropS, a proposition orient"
D16-1252,C98-1013,0,\N,Missing
D16-1252,P16-2077,1,\N,Missing
D18-1182,D14-1110,0,0.0195316,"tered contexts of a target word like “bank.” For instance, “the bank had an ATM” and “I got money from the bank” might fall into one cluster, while “I fished at the river bank” might fall into a second cluster. Second, they annotated each instance of the target word with its sense cluster and then learned a standard one-to-one vector mapping from the annotated corpus. For instance, they would learn vector representations using the sentences “the bank-1 had an ATM,” “I got money from the bank-1,” and “I fished at the river bank2.” Other researchers (Neelakantan et al., 2014; Tian et al., 2014; Chen et al., 2014; Li and JurafBackground: ELMo Recently, Peters et al. (2018) introduced the concept of Embeddings from Language Models (ELMo). ELMo dynamically represents each word based on the context with which it appears, achieved by representing a word in a sentence using its representation from a pretrained bidirectional Language Model (biLM), encoded using a bi-directional RNN (Schuster and Paliwal, 1997). Subsequently, the same word may get different representations in different contexts. For example, the representation of “bank” may differ between “She fished by the river bank” and “She deposited her"
D18-1182,N16-1163,0,0.0117651,"r, cobalt azure, sky seal, horse, whale manatee, dolphin tweet, snore, chirp cluck, quack iceberg, serrano, romaine butterhead, bibb green, garden, democratic republican, libertarian sky, 2015; Bartunov et al., 2016) focused on modifying the vector learning model itself, typically the skip-gram model (Mikolov et al., 2013b), to directly learn multiple embeddings for each word. Additional work focused on using the technique of retrofitting (Faruqui et al., 2014) to adapt pretrained word vectors into sense vectors using auxiliary resources like WordNet (Jauhar et al., 2015) or parallel corpora (Ettinger et al., 2016). Other work (Guo et al., 2014; Suster et al., 2016; Upadhyay et al., 2017) used parallel corpora as the main signal for learning sense vectors. Table 4: Selection of errors made by the w2v.googlenews solver on A NOMIAC OMMON D EV, indicating a tendency to choose polysemous words as the odd-man-out. The incorrect selection is in bold, while the correct answer is in italics. we investigate this further. Background: sense vectors There is a significant literature on how to learn a one-to-many mapping from words to vector representations. An early paradigm (Reisinger and Mooney, 2010; Huang et al"
D18-1182,W16-2506,0,0.0831728,"nington et al., 2014). These widely-used representations often significantly improve performance in downstream tasks, as they are able to leverage large amounts of unstructured data. However, most of the popular collections of word embeddings assign only one vector to each word, thus shifting the burden of word disambiguation to deeper, task-specific layers, which commonly rely on data of much smaller scales. While there has been a significant body of work around sense embeddings (i.e., embedding senses, instead of lexical units), evaluating such representations remains the subject of debate (Faruqui et al., 2016; Gladkova and Drozd, 2016). In this work, we propose a new evaluation task called Odd-Man-Out. The goal of an Odd-ManOut puzzle is simple. Given a set of words1 like cherry, orange, apple, grass, grape, the objective is to identify the word that does not belong (here, the answer is grass, because it is not a fruit). While there are often multiple relationships among the words, we will show that non-experts typically agree on the odd-man-out, and are able to generate hard puzzles on a large scale, using a novel crowdsourcing protocol. Following the creation of this large test set, we conduct a"
D18-1182,W18-2501,0,0.0679167,"Missing"
D18-1182,W16-2507,0,0.120576,"These widely-used representations often significantly improve performance in downstream tasks, as they are able to leverage large amounts of unstructured data. However, most of the popular collections of word embeddings assign only one vector to each word, thus shifting the burden of word disambiguation to deeper, task-specific layers, which commonly rely on data of much smaller scales. While there has been a significant body of work around sense embeddings (i.e., embedding senses, instead of lexical units), evaluating such representations remains the subject of debate (Faruqui et al., 2016; Gladkova and Drozd, 2016). In this work, we propose a new evaluation task called Odd-Man-Out. The goal of an Odd-ManOut puzzle is simple. Given a set of words1 like cherry, orange, apple, grass, grape, the objective is to identify the word that does not belong (here, the answer is grass, because it is not a fruit). While there are often multiple relationships among the words, we will show that non-experts typically agree on the odd-man-out, and are able to generate hard puzzles on a large scale, using a novel crowdsourcing protocol. Following the creation of this large test set, we conduct a thorough analysis of the a"
D18-1182,C14-1048,0,0.0229797,"ale manatee, dolphin tweet, snore, chirp cluck, quack iceberg, serrano, romaine butterhead, bibb green, garden, democratic republican, libertarian sky, 2015; Bartunov et al., 2016) focused on modifying the vector learning model itself, typically the skip-gram model (Mikolov et al., 2013b), to directly learn multiple embeddings for each word. Additional work focused on using the technique of retrofitting (Faruqui et al., 2014) to adapt pretrained word vectors into sense vectors using auxiliary resources like WordNet (Jauhar et al., 2015) or parallel corpora (Ettinger et al., 2016). Other work (Guo et al., 2014; Suster et al., 2016; Upadhyay et al., 2017) used parallel corpora as the main signal for learning sense vectors. Table 4: Selection of errors made by the w2v.googlenews solver on A NOMIAC OMMON D EV, indicating a tendency to choose polysemous words as the odd-man-out. The incorrect selection is in bold, while the correct answer is in italics. we investigate this further. Background: sense vectors There is a significant literature on how to learn a one-to-many mapping from words to vector representations. An early paradigm (Reisinger and Mooney, 2010; Huang et al., 2012; Liu et al., 2015; Wu"
D18-1182,P12-1092,0,0.327942,"Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics We make all our code and models publicly available.2 2 Existing Evaluation Methods In this section, we briefly survey some existing evaluation methods for lexical resources, and discuss their pros and cons. 2.1 Word Similarity The word similarity task (Rubenstein and Goodenough, 1965) has been a dominant approach to assess word vector quality. In this task, systems are required to score the similarity of two words on a numeric scale, sometimes without context and sometimes in a sentential context (Huang et al., 2012). For instance, the pair (tiger, mammal) has a similarity score of 6.85 (out of 10) on the WordSim dataset (Finkelstein et al., 2001). A common criticism (Faruqui et al., 2016; Gladkova and Drozd, 2016) of the word similarity task is that “similarity” is subjective, and conflates several different potential relationships between words. For instance, (Faruqui et al., 2016) questions why the pair (cup, coffee) should be considered more similar than (car, train), as it is according to the WordSim dataset. Odd-man-out puzzles naturally disambiguate the nebulous concept of “similarity”, because eac"
D18-1182,N15-1070,0,0.0154368,"school, canoe, flock herd, pack navy, amber, cobalt azure, sky seal, horse, whale manatee, dolphin tweet, snore, chirp cluck, quack iceberg, serrano, romaine butterhead, bibb green, garden, democratic republican, libertarian sky, 2015; Bartunov et al., 2016) focused on modifying the vector learning model itself, typically the skip-gram model (Mikolov et al., 2013b), to directly learn multiple embeddings for each word. Additional work focused on using the technique of retrofitting (Faruqui et al., 2014) to adapt pretrained word vectors into sense vectors using auxiliary resources like WordNet (Jauhar et al., 2015) or parallel corpora (Ettinger et al., 2016). Other work (Guo et al., 2014; Suster et al., 2016; Upadhyay et al., 2017) used parallel corpora as the main signal for learning sense vectors. Table 4: Selection of errors made by the w2v.googlenews solver on A NOMIAC OMMON D EV, indicating a tendency to choose polysemous words as the odd-man-out. The incorrect selection is in bold, while the correct answer is in italics. we investigate this further. Background: sense vectors There is a significant literature on how to learn a one-to-many mapping from words to vector representations. An early parad"
D18-1182,S12-1047,0,0.0348268,"a et al., 2016; Sun et al., 2017a). The task is to identify whether a particular bag of words is “relevant” to a target word. For instance, “tie” is considered relevant to the bag “winner, score, tied, completion, identical, results, sports,” but irrelevant to the bag “domestic, hog, pig, culinary, eaten, cooked, fat”. This task has the attractiveness of being a simple binary evaluation, but demands only that a model can identify a broad sense of relatedness, not the ability to pinpoint specific relationships. Analogies Analogies like “king is to queen as man is to...” (Mikolov et al., 2013b; Jurgens et al., 2012) are related to the odd-man-out task. Analogies, however, are best suited to particular relationships, such as hypernym → hyponym (which are the subject of extensive research, e.g., (Shwartz et al., 2016)), while odd-man-out puzzles can capture a broader range of associations (see for instance, the auto-racing puzzle from the previous section). Analogies are also more subject to ambiguity, since the premise can involve only two words. For instance, the puzzle “cherry is to strawberry as grass is to...” could refer to the fact that cherry and strawberry are both fruits, both red, or both red 2"
D18-1182,E14-1057,0,0.0368612,"Missing"
D18-1182,D15-1200,0,0.0403916,"Missing"
D18-1182,S10-1011,0,0.0272875,"irrelevant is the context of a puzzle like car, train, checkered flag, racetrack, pit stop. 2.2 fruits. Odd-man-out puzzles provide a simple way of reducing ambiguity: adding more choices to the puzzle. 2.3 Word Sense Disambiguation and Induction Word sense disambiguation (Navigli, 2009) is a popular way to evaluate polysemous word representations. The common criticism is that systems are rewarded based on their ability to classify words according to a fixed inventory of senses, whose granularity is regarded by some as too coarse and others as too fine. An alternative is word sense induction (Manandhar et al., 2010), which allows systems to cluster word senses without an agreed-upon sense inventory. However, there is not an obvious evaluation metric. The two metrics used in SemEval 2010 Task 14 (Manandhar et al., 2010) yielded highly divergent system rankings. 2.4 Word Context Relevance A recent evaluation method is Word Context Relevance (Arora et al., 2016; Sun et al., 2017a). The task is to identify whether a particular bag of words is “relevant” to a target word. For instance, “tie” is considered relevant to the bag “winner, score, tied, completion, identical, results, sports,” but irrelevant to the"
D18-1182,S07-1009,0,0.023792,"such as hypernym → hyponym (which are the subject of extensive research, e.g., (Shwartz et al., 2016)), while odd-man-out puzzles can capture a broader range of associations (see for instance, the auto-racing puzzle from the previous section). Analogies are also more subject to ambiguity, since the premise can involve only two words. For instance, the puzzle “cherry is to strawberry as grass is to...” could refer to the fact that cherry and strawberry are both fruits, both red, or both red 2 https://github.com/gabrielStanovsky/ odd-man-out 2.5 Lexical Substitution Lexical substitution tasks (McCarthy and Navigli, 2007; Biemann, 2013; Kremer et al., 2014), in which the task is to determine whether one word can replace another word in a particular context, are effective, but are restricted in the kind of relationships they can test (mainly synonymy). 3 Odd Man Out Datasets In this section, we describe the creation of several Odd-Man-Out datasets. We begin by describing a small-scale, curated annotation, then show how to scale the annotation using crowdsourcing techniques. In the subsequent sections, we use these datasets to explore the properties of a wide array of lexical resources. 1534 3.1 Expert annotati"
D18-1182,P16-1226,0,0.0385234,"Missing"
D18-1182,N16-1160,0,0.0314195,"Missing"
D18-1182,C14-1016,0,0.0233488,"h. First, they clustered contexts of a target word like “bank.” For instance, “the bank had an ATM” and “I got money from the bank” might fall into one cluster, while “I fished at the river bank” might fall into a second cluster. Second, they annotated each instance of the target word with its sense cluster and then learned a standard one-to-one vector mapping from the annotated corpus. For instance, they would learn vector representations using the sentences “the bank-1 had an ATM,” “I got money from the bank-1,” and “I fished at the river bank2.” Other researchers (Neelakantan et al., 2014; Tian et al., 2014; Chen et al., 2014; Li and JurafBackground: ELMo Recently, Peters et al. (2018) introduced the concept of Embeddings from Language Models (ELMo). ELMo dynamically represents each word based on the context with which it appears, achieved by representing a word in a sentence using its representation from a pretrained bidirectional Language Model (biLM), encoded using a bi-directional RNN (Schuster and Paliwal, 1997). Subsequently, the same word may get different representations in different contexts. For example, the representation of “bank” may differ between “She fished by the river bank” and"
D18-1182,W17-2613,0,0.013512,"p cluck, quack iceberg, serrano, romaine butterhead, bibb green, garden, democratic republican, libertarian sky, 2015; Bartunov et al., 2016) focused on modifying the vector learning model itself, typically the skip-gram model (Mikolov et al., 2013b), to directly learn multiple embeddings for each word. Additional work focused on using the technique of retrofitting (Faruqui et al., 2014) to adapt pretrained word vectors into sense vectors using auxiliary resources like WordNet (Jauhar et al., 2015) or parallel corpora (Ettinger et al., 2016). Other work (Guo et al., 2014; Suster et al., 2016; Upadhyay et al., 2017) used parallel corpora as the main signal for learning sense vectors. Table 4: Selection of errors made by the w2v.googlenews solver on A NOMIAC OMMON D EV, indicating a tendency to choose polysemous words as the odd-man-out. The incorrect selection is in bold, while the correct answer is in italics. we investigate this further. Background: sense vectors There is a significant literature on how to learn a one-to-many mapping from words to vector representations. An early paradigm (Reisinger and Mooney, 2010; Huang et al., 2012; Liu et al., 2015; Wu and Giles, 2015) took a 2-pass approach. Firs"
D18-1182,H92-1116,0,0.348788,"e specificity of the vertex labeled “element” is 16 . • Given an odd-man-out puzzle w1 , ..., wn , the explanation of word wk is the vertex v of highest specificity such that: (i) for each word wj such that j =/ k, there exists some descendent v ′ of v where L(v ′ ) = wj , (ii) there does not exist a descendent v ′ of v such that L(v ′ ) = wk . For instance, the explanation of helium with respect to the puzzle helium, mercury, lead, silver, gold is the node labeled “metallic element.” Taxonomy-Based Solvers In this section, we show how to create odd-manout solvers for taxonomies like WordNet (Miller, 1992). Define a taxonomy as a triple (V, E, L), where (V, E) is a directed acyclic graph, and L maps each vertex V to a string. A simple example is shown in Figure 1, where the each vertex v is labeled with L(v). We create an odd-man-out solver from a taxonomy as follows: • If some word does not have an explanation, or if there is no word whose explanation is uniquely most specific, then the solver abstains from answering. Otherwise, the solver returns the word with the most specific explanation. • The specificity of a vertex v is defined as the reciprocal of the number of its descendants. 6 https:"
D18-1182,D14-1113,0,0.0278605,"015) took a 2-pass approach. First, they clustered contexts of a target word like “bank.” For instance, “the bank had an ATM” and “I got money from the bank” might fall into one cluster, while “I fished at the river bank” might fall into a second cluster. Second, they annotated each instance of the target word with its sense cluster and then learned a standard one-to-one vector mapping from the annotated corpus. For instance, they would learn vector representations using the sentences “the bank-1 had an ATM,” “I got money from the bank-1,” and “I fished at the river bank2.” Other researchers (Neelakantan et al., 2014; Tian et al., 2014; Chen et al., 2014; Li and JurafBackground: ELMo Recently, Peters et al. (2018) introduced the concept of Embeddings from Language Models (ELMo). ELMo dynamically represents each word based on the context with which it appears, achieved by representing a word in a sentence using its representation from a pretrained bidirectional Language Model (biLM), encoded using a bi-directional RNN (Schuster and Paliwal, 1997). Subsequently, the same word may get different representations in different contexts. For example, the representation of “bank” may differ between “She fished by"
D18-1182,D14-1162,0,0.0953173,"surpasses all other representations on all OddMan-Out collections. 1 Introduction Correctly disambiguating the sense of a polysemous word (e.g., “spring is a beautiful season” versus “John was ready to spring into action”) is a crucial part of various NLP tasks, such as translation, question answering, or textual entailment. The state-of-the-art, and the de-facto common practice for essentially all of these tasks, involves neural networks (see (Goldberg, 2015) for a recent survey), which are commonly initialized with pretrained word vectors, such as Word2Vec (Mikolov et al., 2013a) or GloVe (Pennington et al., 2014). These widely-used representations often significantly improve performance in downstream tasks, as they are able to leverage large amounts of unstructured data. However, most of the popular collections of word embeddings assign only one vector to each word, thus shifting the burden of word disambiguation to deeper, task-specific layers, which commonly rely on data of much smaller scales. While there has been a significant body of work around sense embeddings (i.e., embedding senses, instead of lexical units), evaluating such representations remains the subject of debate (Faruqui et al., 2016;"
D18-1182,N18-1202,0,0.343659,"d embeddings towards more exploratory evaluations that would aim not for generic scores, but for identification of strengths and weaknesses of embeddings”, and conduct rigorous analysis of each representation. Overall, we find that all lexical resources are prone to miss associations which are intuitive for humans, leaving ample room for future improvement. Moreover, we show empirical evidence that lexical resources that do not account for polysemy are handicapped by this weakness. Finally, we propose a new sense embedding technique, which leverages the recent introduction of ELMo embeddings (Peters et al., 2018) by performing unsupervised clustering over a large unstructured corpus. We show that this new resource surpasses all other baselines on various Odd-ManOut datasets. 1 In this paper, we use the term “word” loosely to also include the multi-word expressions like “fire engine” and “magnifying glass.” 1533 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1533–1542 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics We make all our code and models publicly available.2 2 Existing Evaluation Methods In this s"
D18-1182,N10-1013,0,0.0452278,"arallel corpora (Ettinger et al., 2016). Other work (Guo et al., 2014; Suster et al., 2016; Upadhyay et al., 2017) used parallel corpora as the main signal for learning sense vectors. Table 4: Selection of errors made by the w2v.googlenews solver on A NOMIAC OMMON D EV, indicating a tendency to choose polysemous words as the odd-man-out. The incorrect selection is in bold, while the correct answer is in italics. we investigate this further. Background: sense vectors There is a significant literature on how to learn a one-to-many mapping from words to vector representations. An early paradigm (Reisinger and Mooney, 2010; Huang et al., 2012; Liu et al., 2015; Wu and Giles, 2015) took a 2-pass approach. First, they clustered contexts of a target word like “bank.” For instance, “the bank had an ATM” and “I got money from the bank” might fall into one cluster, while “I fished at the river bank” might fall into a second cluster. Second, they annotated each instance of the target word with its sense cluster and then learned a standard one-to-one vector mapping from the annotated corpus. For instance, they would learn vector representations using the sentences “the bank-1 had an ATM,” “I got money from the bank-1,”"
D18-1263,P17-2021,0,0.133168,"). We deviate from current graph-parsing approaches to SDP (Peng et al., 2017a) by treating the different semantic formalisms as foreign target dialects, while having English a as a common source language (Section 3). Subsequently, we devise a neural MT sequence-to-sequence framework that is suited for the task. In order to apply sequence-to-sequence models for structured prediction, a linearization function is required to interpret the model’s sequential input and output. Initial work on structured prediction sequence-to-sequence modeling has focused on tree structures (Vinyals et al., 2015; Aharoni and Goldberg, 2017), as these are quite easy to linearize using the bracketed representation (as employed in the Penn TreeBank (Marcus et al., 1993)). Following, various efforts were made to port the attractiveness of sequence-to-sequence modeling to the more general graph structure of semantic representations, such as AMR or MRS (Peng et al., 2017b; Barzdins and Gosko, 2016; Konstas et al., 2017; Buys and Blunsom, 2017). However, to the best of our knowledge, all such current methods actually sidestep the challenge of graph linearization – they reduce the input graph to a tree using lossy heuristics, which are"
D18-1263,W13-2322,0,0.123921,"ral sequence-to-sequence framework which can effectively recover our graph linearizations, performing almost on-par with previous SDP state-of-the-art while requiring less parallel training annotations. Beyond SDP, our linearization technique opens the door to integration of graph-based semantic representations as features in neural models for downstream applications. 1 Introduction Many sentence-level representations were developed with the goal of capturing the sentence’s proposition structure and making it accessible for downstream applications (Montague, 1973; Carreras and M`arquez, 2005; Banarescu et al., 2013; Abend and Rappoport, 2013). See Abend and Rappoport (2017), for a recent survey. While syntactic grammars (Marcus et al., 1993; Nivre, 2005) induce a rooted tree structure over the sentence by connecting verbal predicates to their arguments, these semantic representations often take the form of the more general labeled graph structure, and aim to capture a wider notion of propositions (e.g, nominalizations, adjectivals, or appositives). In particular, we will focus on the three graph-based semantic representations collected in the Broad-Coverage Semantic Dependency Parsing SemEval shared tas"
D18-1263,S16-1176,0,0.122982,"or structured prediction, a linearization function is required to interpret the model’s sequential input and output. Initial work on structured prediction sequence-to-sequence modeling has focused on tree structures (Vinyals et al., 2015; Aharoni and Goldberg, 2017), as these are quite easy to linearize using the bracketed representation (as employed in the Penn TreeBank (Marcus et al., 1993)). Following, various efforts were made to port the attractiveness of sequence-to-sequence modeling to the more general graph structure of semantic representations, such as AMR or MRS (Peng et al., 2017b; Barzdins and Gosko, 2016; Konstas et al., 2017; Buys and Blunsom, 2017). However, to the best of our knowledge, all such current methods actually sidestep the challenge of graph linearization – they reduce the input graph to a tree using lossy heuristics, which are specifi1 DM is automatically derived from Minimal Recursion Semantics (MRS) (Copestake et al., 1999). 2412 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2412–2421 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics cally tailored for their target representation."
D18-1263,P17-1112,0,0.29393,"structure, and aim to capture a wider notion of propositions (e.g, nominalizations, adjectivals, or appositives). In particular, we will focus on the three graph-based semantic representations collected in the Broad-Coverage Semantic Dependency Parsing SemEval shared task (SDP) (Oepen et al., 2015): (1) DELPH-IN Bi∗ Work performed while at Bar-Ilan University. Lexical Dependencies (DM) (Flickinger, 2000),1 (2) Enju Predicate-Argument Structures (PAS) (Miyao et al., 2014), and (3) Prague Semantic Dependencies (PSD) (Hajic et al., 2012). These annotations have garnered recent attention (e.g., (Buys and Blunsom, 2017; Peng et al., 2017a)), and were consistently annotated in parallel on over more than 30K sentences of the Wall Street Journal corpus (Charniak et al., 2000). In this work we take a novel approach to graph parsing, casting sentence-level semantic parsing as a multilingual machine-translation task (MT). We deviate from current graph-parsing approaches to SDP (Peng et al., 2017a) by treating the different semantic formalisms as foreign target dialects, while having English a as a common source language (Section 3). Subsequently, we devise a neural MT sequence-to-sequence framework that is suited"
D18-1263,W05-0620,0,0.0770807,"Missing"
D18-1263,P81-1022,0,0.150131,"Missing"
D18-1263,N10-1115,0,0.0221714,"leading to better (hopefully more semantic) abstractions. To the best of our knowledge, the human annotation order is not available for the SDP annotations, and there is no clear a priori optimal ordering. We therefore experiment with several visiting orders, as described in Table 2. Notably, Sentence order is equivalent to the ordering used by Vinyals et al. (2015) for syntactic linearization, while Closest words orders child nodes from short to longer range-dependencies (commonly associated with syntactic versus semantic relations), and Smaller first is motivated by the easy-first approach (Goldberg and Elhadad, 2010), first encoding paths which are shorter (and easier to memorize), before longer, more complicated sequences. In Section 6 we evaluate the effect of these variations on the SDP parsing task. 5 ARG1 Model We start by describing our model architecture, inspired by recent MT architectures, while allowing for different types of inputs, namely English sentences and linearized graphs. Following, we present our methods for training and testing, and specific hyper-parameter configuration and implementation details. (b) Our linearization scheme for the sentence in 2a. Each node is represented by its re"
D18-1263,hajic-etal-2012-announcing,0,0.235626,"Missing"
D18-1263,Q17-1024,0,0.0823583,"Missing"
D18-1263,Q16-1023,0,0.0495816,"Missing"
D18-1263,P17-4012,0,0.0423017,"ds Smaller-first DM PAS PSD 86.1 87.2 87.5 87.9 87.7 90.3 89.8 90.9 78.4 79.9 79.7 80.3 following decoding steps. Due to its better performance, we will report only the smaller-first’s performance in all following evaluations. Avg. 84.1 85.8 85.8 86.2 Table 3: Evaluation of different DFS orderings, in labeled F1 score, across the different tasks. epochs, in about 12 hours on a GPU (NVIDIA GeForce GTX 1080 Ti), in batches of 50 sentences. All of these sentences belong to the same task, which is chosen at random before each batch. Finally, our models were developed using the OpenNMT-py library (Klein et al., 2017), and are made available.7 6 Evaluation We perform several evaluations, testing the impact of alternative configurations, including the different DFS traversal orders and MTL versus single-task approach, as well as our model’s performance against current state-of-the-art on each of the PRIMARY tasks. 6.1 Results The results of our different analyses are reported in Tables 3-6, as elaborated below. For all evaluations, we use the in-domain test partition of the SDP corpus, containing 1, 410 sentences. Following Peng et al. (2017a) we report performance using labeled F1 scores as well as average"
D18-1263,P17-1014,0,0.354057,"a linearization function is required to interpret the model’s sequential input and output. Initial work on structured prediction sequence-to-sequence modeling has focused on tree structures (Vinyals et al., 2015; Aharoni and Goldberg, 2017), as these are quite easy to linearize using the bracketed representation (as employed in the Penn TreeBank (Marcus et al., 1993)). Following, various efforts were made to port the attractiveness of sequence-to-sequence modeling to the more general graph structure of semantic representations, such as AMR or MRS (Peng et al., 2017b; Barzdins and Gosko, 2016; Konstas et al., 2017; Buys and Blunsom, 2017). However, to the best of our knowledge, all such current methods actually sidestep the challenge of graph linearization – they reduce the input graph to a tree using lossy heuristics, which are specifi1 DM is automatically derived from Minimal Recursion Semantics (MRS) (Copestake et al., 1999). 2412 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2412–2421 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics cally tailored for their target representation. In contrast, we design"
D18-1263,D16-1180,0,0.0525922,"Missing"
D18-1263,L16-1262,0,0.0203834,"Missing"
D18-1263,S15-2153,0,0.707411,"Missing"
D18-1263,S14-2008,0,0.29444,"Missing"
D18-1263,P17-1186,0,0.50423,"apture a wider notion of propositions (e.g, nominalizations, adjectivals, or appositives). In particular, we will focus on the three graph-based semantic representations collected in the Broad-Coverage Semantic Dependency Parsing SemEval shared task (SDP) (Oepen et al., 2015): (1) DELPH-IN Bi∗ Work performed while at Bar-Ilan University. Lexical Dependencies (DM) (Flickinger, 2000),1 (2) Enju Predicate-Argument Structures (PAS) (Miyao et al., 2014), and (3) Prague Semantic Dependencies (PSD) (Hajic et al., 2012). These annotations have garnered recent attention (e.g., (Buys and Blunsom, 2017; Peng et al., 2017a)), and were consistently annotated in parallel on over more than 30K sentences of the Wall Street Journal corpus (Charniak et al., 2000). In this work we take a novel approach to graph parsing, casting sentence-level semantic parsing as a multilingual machine-translation task (MT). We deviate from current graph-parsing approaches to SDP (Peng et al., 2017a) by treating the different semantic formalisms as foreign target dialects, while having English a as a common source language (Section 3). Subsequently, we devise a neural MT sequence-to-sequence framework that is suited for the task. In o"
D18-1263,E17-1035,0,0.36486,"apture a wider notion of propositions (e.g, nominalizations, adjectivals, or appositives). In particular, we will focus on the three graph-based semantic representations collected in the Broad-Coverage Semantic Dependency Parsing SemEval shared task (SDP) (Oepen et al., 2015): (1) DELPH-IN Bi∗ Work performed while at Bar-Ilan University. Lexical Dependencies (DM) (Flickinger, 2000),1 (2) Enju Predicate-Argument Structures (PAS) (Miyao et al., 2014), and (3) Prague Semantic Dependencies (PSD) (Hajic et al., 2012). These annotations have garnered recent attention (e.g., (Buys and Blunsom, 2017; Peng et al., 2017a)), and were consistently annotated in parallel on over more than 30K sentences of the Wall Street Journal corpus (Charniak et al., 2000). In this work we take a novel approach to graph parsing, casting sentence-level semantic parsing as a multilingual machine-translation task (MT). We deviate from current graph-parsing approaches to SDP (Peng et al., 2017a) by treating the different semantic formalisms as foreign target dialects, while having English a as a common source language (Section 3). Subsequently, we devise a neural MT sequence-to-sequence framework that is suited for the task. In o"
D18-1263,D14-1162,0,0.0964516,"ferent relation. To overcome this we artificially increase the softmax probabilities (dashed edges in Figure 3) so that they reflect the DFS path decoded up until that point. Specifically, we override the predicted word according to the previous index, and backtrack “up” the corresponding edge. 5.3 Implementation Details All of our hyper-parameters were tuned on a held out partition of 1000 sentences in the training set. In particular, we use 3 hidden layers for both of the encoders, and 2 hidden layers for the decoder. English word embeddings were fixed with 300-dimensional GloVe embeddings (Pennington et al., 2014), while the graph elements, which consist of a lexicon of roughly 400 tokens across three representations, were randomly initialized. We trained the model until convergence, roughly 20 2418 Random Sentence order Closest words Smaller-first DM PAS PSD 86.1 87.2 87.5 87.9 87.7 90.3 89.8 90.9 78.4 79.9 79.7 80.3 following decoding steps. Due to its better performance, we will report only the smaller-first’s performance in all following evaluations. Avg. 84.1 85.8 85.8 86.2 Table 3: Evaluation of different DFS orderings, in labeled F1 score, across the different tasks. epochs, in about 12 hours on"
D18-1263,P16-2038,0,0.100388,"Missing"
D18-1263,I17-1003,0,0.0204456,"graph decoder. Interestingly, we show that training on the auxiliary inter-representation translation tasks greatly improves the performance on the original SDP tasks, without requiring any additional manual annotation effort (Section 6). Our contributions are two-fold. First, we show that novel sequence-to-sequence models are able to effectively capture and recover general graph structures, making them a viable and easily extensible approach towards the SDP task. Second, beyond SDP, as the inclusion of syntactic linearization was shown beneficial in various tasks (Aharoni and Goldberg, 2017; Le et al., 2017) so does our approach prompt easy integration of graphbased representations as complementary semantic signal in various downstream applications. 2 Background We begin this section by presenting the corpus we use to train and test our model (the SDP corpus) and the current state-of-the-art in predicting semantic dependencies. Then, we discuss previous work on sequence-to-sequence models for tree prediction, which this work extends to general graph structures. Finally, we briefly describe the multilingual translation approach, which we borrow and adapt to the semantic parsing task. #Train senten"
D18-1263,J93-2004,0,0.0643227,"DP state-of-the-art while requiring less parallel training annotations. Beyond SDP, our linearization technique opens the door to integration of graph-based semantic representations as features in neural models for downstream applications. 1 Introduction Many sentence-level representations were developed with the goal of capturing the sentence’s proposition structure and making it accessible for downstream applications (Montague, 1973; Carreras and M`arquez, 2005; Banarescu et al., 2013; Abend and Rappoport, 2013). See Abend and Rappoport (2017), for a recent survey. While syntactic grammars (Marcus et al., 1993; Nivre, 2005) induce a rooted tree structure over the sentence by connecting verbal predicates to their arguments, these semantic representations often take the form of the more general labeled graph structure, and aim to capture a wider notion of propositions (e.g, nominalizations, adjectivals, or appositives). In particular, we will focus on the three graph-based semantic representations collected in the Broad-Coverage Semantic Dependency Parsing SemEval shared task (SDP) (Oepen et al., 2015): (1) DELPH-IN Bi∗ Work performed while at Bar-Ilan University. Lexical Dependencies (DM) (Flickinge"
D18-1263,S14-2056,0,0.274258,"ntence by connecting verbal predicates to their arguments, these semantic representations often take the form of the more general labeled graph structure, and aim to capture a wider notion of propositions (e.g, nominalizations, adjectivals, or appositives). In particular, we will focus on the three graph-based semantic representations collected in the Broad-Coverage Semantic Dependency Parsing SemEval shared task (SDP) (Oepen et al., 2015): (1) DELPH-IN Bi∗ Work performed while at Bar-Ilan University. Lexical Dependencies (DM) (Flickinger, 2000),1 (2) Enju Predicate-Argument Structures (PAS) (Miyao et al., 2014), and (3) Prague Semantic Dependencies (PSD) (Hajic et al., 2012). These annotations have garnered recent attention (e.g., (Buys and Blunsom, 2017; Peng et al., 2017a)), and were consistently annotated in parallel on over more than 30K sentences of the Wall Street Journal corpus (Charniak et al., 2000). In this work we take a novel approach to graph parsing, casting sentence-level semantic parsing as a multilingual machine-translation task (MT). We deviate from current graph-parsing approaches to SDP (Peng et al., 2017a) by treating the different semantic formalisms as foreign target dialects,"
D19-5549,P07-2027,0,0.0727729,"Missing"
D19-5549,2005.mtsummit-papers.11,0,0.265903,"Missing"
D19-5549,W09-3944,0,0.0826417,"Missing"
D19-5549,C18-1003,0,0.0544177,"Missing"
D19-5549,N19-1423,0,\N,Missing
D19-5817,W05-0909,0,0.426678,". In this work, we study all mentioned metrics in the context of question answering. BLEU is a precision-based metric developed for evaluating machine translation (Papineni et al., 2001). BLEU scores a candidate by computing the number of n-grams in the candidate that also appear in a reference. n is varied from 1 up to a specified N and the scores for varying n are aggregated with a geometric mean. In this work, we look at BLEU-1 and BLEU-4, where N = 1 and N = 4 respectively. METEOR is an F-measure metric developed for evaluating machine translation which operates on unigrams (i.e. tokens) (Banerjee and Lavie, 2005). METEOR first creates an alignment by attempting to map each token in a candidate to a token in a reference (and vice versa). A token is aligned to another token if they are the same, are synonyms, or their stems match. The alignment is aggregated into precision and recall values, which are combined into an F-measure score in which more weight is given to recall. ROUGE is an F-measure metric designed for evaluating translation and summarization (Lin, 2004). There are a number of variants of ROUGE however in this work we focus on ROUGE-L. ROUGE-L is computed based on the longest common subsequ"
D19-5817,D18-1454,0,0.0486278,"of ROPES is F1 . A unique characteristic of ROPES is that questions generally present two possible answer choices, one of which is incorrect (Table 1). Because incorrect and correct answers often have some n-gram overlap, we believe F1 will struggle to accurately assign scores (Figure 1b). 4 Models We describe the models used to generate predictions for our datasets. These models have publicly available code and have reasonable performance compared to the current state-of-the-art models. Multi-hop Point Generator For NarrativeQA and SemEval, we use a multi-hop pointer generator (MHPG) model (Bauer et al., 2018)3 . MHPG represents its input using ELMo embeddings. The embeddings are then fed into a sequence of BiDAF (Seo et al., 2017) cells, where the output of one BiDAF cell is fed as the input into another BiDAF cell. This allows multi-hop reasoning over the context. The output layer consists of a generative decoder with a copying mechanism. We evaluate MHPG’s predictions using BLEU-1, BLEU-4, ROUGE-L, METEOR, SMS, BERTScore and Conditional BERTScore. BERT For ROPES, we finetune BERT as a span based QA model following the procedure used for 3 https://github.com/yicheng-w/ CommonSenseMultiHopQA https"
D19-5817,C04-1046,0,0.246488,"Missing"
D19-5817,P19-1264,0,0.0264628,"e (LCS), which searches for the longest co-occurring set of tokens common to both reference and candidate. An advantage of ROUGEL is that no predefined n-gram size is required. F1 While the previously mentioned metrics have been adapted for evaluating generative question answering, F1 has been generally reserved for evaluating span-based question answering (Rajpurkar et al., 2016). It is computed over tokens in the candidate and reference. Sentence Mover’s Similarity (SMS) is a recent metric based on earth mover’s distance for evaluated multi-sentence texts such as machinegenerated summaries (Clark et al., 2019) .1 SMS 120 1 https://github.com/eaclark07/sms first computes an embedding for each sentence in a document as an average its ELMo word representations (Peters et al., 2018). A linear program is then solved to obtain the distance of “moving” a candidate document’s sentences to match a reference document. SMS has shown better results over ROUGE-L in evaluating generated summaries and student essays. BERTScore is recent metric for evaluating translation (Zhang et al., 2019).2 BERTScore first obtains BERT representations of each word in the candidate and reference by feeding the candidate and refe"
D19-5817,N19-1423,0,0.033544,"d what batteries it required: two AA-sized batteries . . . Why did they throw away the old batteries? They were no longer useful ROPES 11,202 . . . A catalyst is a chemical that speeds up chemical reactions . . . [Mark] conducts two tests, test A and test B, on an organism. In test A he reduces catalysts from the organism, but in test B he induces catalysts in the organism ... Which test would see reactions taking place slower, test A or test B? test A Table 1: Examples for the datasets we use in our study. The # of QA Pairs column refers to the number of QA pairs in the training sets. SQuAD (Devlin et al., 2019). We evaluate BERT’s predictions using F1 , SMS, BERTScore, and Conditional BERTScore. 5 5.1 Evaluating QA Metrics using Human Judgements 5.3 Collecting Human Judgements After training our models on the three datasets, we extract 500, 500, and 300 data points from the validation sets of NarrativeQA, ROPES, and SemEval, respectively, along with the model predictions. When extracting data points to label, we filter out data points where the predicted answer exactly matches the gold answer. This filtering step is done as we are interested on how well metrics do when it cannot resort to exact stri"
D19-5817,N19-1246,1,0.798824,". Figure 1: Examples where existing n-gram based metrics fail to align with human judgements. Human judgements are between 1 and 5. (a) illustrates that because existing metrics do not use the context, they fail to capture coreferences. (b) illustrates that changing a single token can make a prediction incorrect while F1 assigns a non-zero score. Introduction Question answering (QA) has emerged as a burgeoning research field driven by the availability of large datasets. These datasets are built to test a variety of reading comprehension skills such as multihop (Welbl et al., 2017), numerical (Dua et al., 2019), and commonsense (Talmor et al., 2018) reasoning. A key component of a QA dataset is the evaluation metric associated with it, which aims to automatically approximate human accuracy judgments of a predicted answer against a gold answer. The metrics used to evaluate QA datasets have a number of ramifications. The first is that they drive research focus. Models that rank higher on a leaderboard according to a metric will receive 119 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 119–124 c Hong Kong, China, November 4, 2019. 2019 Association for Computational"
D19-5817,W04-1013,0,0.0728504,"ively. METEOR is an F-measure metric developed for evaluating machine translation which operates on unigrams (i.e. tokens) (Banerjee and Lavie, 2005). METEOR first creates an alignment by attempting to map each token in a candidate to a token in a reference (and vice versa). A token is aligned to another token if they are the same, are synonyms, or their stems match. The alignment is aggregated into precision and recall values, which are combined into an F-measure score in which more weight is given to recall. ROUGE is an F-measure metric designed for evaluating translation and summarization (Lin, 2004). There are a number of variants of ROUGE however in this work we focus on ROUGE-L. ROUGE-L is computed based on the longest common subsequence (LCS), which searches for the longest co-occurring set of tokens common to both reference and candidate. An advantage of ROUGEL is that no predefined n-gram size is required. F1 While the previously mentioned metrics have been adapted for evaluating generative question answering, F1 has been generally reserved for evaluating span-based question answering (Rajpurkar et al., 2016). It is computed over tokens in the candidate and reference. Sentence Mover"
D19-5817,D19-5808,1,0.902171,"t is crucial that the metrics we use are able to assign scores that accurately reflect human judgements. Despite the value of metrics as drivers of research, a comprehensive study of QA metrics across a number of datasets has yet to be completed. This is important as present metrics are based on n-gram matching, which have a number of shortcomings (Figure 1). In this work, we survey the landscape of evaluation metrics for QA and study how well current metrics approximate (i.e. correlate with) human judgements. We conduct our study on three datasets: NarrativeQA (Kocisk´y et al., 2017), ROPES (Lin et al., 2019), and SemEval-2018 Task 11 (Ostermann et al., 2018). For the generative NarrativeQA dataset, we find that existing metrics provide reasonable correlation with human accuracy judgements while still leaving considerable room for improvement. We also study the span-based ROPES dataset, finding that it presents an interesting case where F1 struggles due to the high overlap in right and wrong answers. Finally, we convert the multiple-choice SemEval-2018 Task 11 dataset into a generative QA dataset. This produces a more difficult generative QA dataset compared to NarrativeQA as answers in SemEval ar"
D19-5817,D18-1429,0,0.0405241,"/question/answer triples during its pretraining. Finetuning a BERT model on QA datasets can potentially yield a better BERTScore-based metric. 6 Related Work N-gram based metrics such as BLEU and METEOR were originally developed and tested for evaluation of machine translation. These metrics have grown to become popular choices in evaluating all forms of natural language generation, including image captioning, question answering, and dialog systems. As these metrics continue to be used, there have been a number of papers that try to assess how suitable these metrics are for different domains. Nema and Khapra (2018) show that for question generation, n-gram metrics assign scores that correlate poorly to the notion of answerability (i.e., is a generated question answerable). Yang et al. (2018) study the effect of using BLEU and ROUGE in evaluating QA, focusing on yes-no and entity questions on the Chinese DuReader dataset (He et al., 2017). For these types of questions, changing a single word from a gold answer can lead to an incorrect answer. In these cases, BLEU and ROUGE assign scores that do not necessarily reflect the correctness of an answer. Our work is continuation of this line of work in assessin"
D19-5817,S18-1119,0,0.147813,"to assign scores that accurately reflect human judgements. Despite the value of metrics as drivers of research, a comprehensive study of QA metrics across a number of datasets has yet to be completed. This is important as present metrics are based on n-gram matching, which have a number of shortcomings (Figure 1). In this work, we survey the landscape of evaluation metrics for QA and study how well current metrics approximate (i.e. correlate with) human judgements. We conduct our study on three datasets: NarrativeQA (Kocisk´y et al., 2017), ROPES (Lin et al., 2019), and SemEval-2018 Task 11 (Ostermann et al., 2018). For the generative NarrativeQA dataset, we find that existing metrics provide reasonable correlation with human accuracy judgements while still leaving considerable room for improvement. We also study the span-based ROPES dataset, finding that it presents an interesting case where F1 struggles due to the high overlap in right and wrong answers. Finally, we convert the multiple-choice SemEval-2018 Task 11 dataset into a generative QA dataset. This produces a more difficult generative QA dataset compared to NarrativeQA as answers in SemEval are often more free-form in nature and have less over"
D19-5817,2001.mtsummit-papers.68,0,0.0372419,"ghtly improves results when evaluating generative QA, though not to an extant that is statistically significant. Overall, our results indicate that studying the evaluation of QA is an underresearched area with substantial room for further experimentation. 2 Metrics We provide a summary of popular n-gram based metrics, as well as sentence mover’s similarity, BERTScore, and an extension of BERTScore which we call conditional BERTScore. In this work, we study all mentioned metrics in the context of question answering. BLEU is a precision-based metric developed for evaluating machine translation (Papineni et al., 2001). BLEU scores a candidate by computing the number of n-grams in the candidate that also appear in a reference. n is varied from 1 up to a specified N and the scores for varying n are aggregated with a geometric mean. In this work, we look at BLEU-1 and BLEU-4, where N = 1 and N = 4 respectively. METEOR is an F-measure metric developed for evaluating machine translation which operates on unigrams (i.e. tokens) (Banerjee and Lavie, 2005). METEOR first creates an alignment by attempting to map each token in a candidate to a token in a reference (and vice versa). A token is aligned to another toke"
D19-5817,N18-1202,1,0.468177,"ired. F1 While the previously mentioned metrics have been adapted for evaluating generative question answering, F1 has been generally reserved for evaluating span-based question answering (Rajpurkar et al., 2016). It is computed over tokens in the candidate and reference. Sentence Mover’s Similarity (SMS) is a recent metric based on earth mover’s distance for evaluated multi-sentence texts such as machinegenerated summaries (Clark et al., 2019) .1 SMS 120 1 https://github.com/eaclark07/sms first computes an embedding for each sentence in a document as an average its ELMo word representations (Peters et al., 2018). A linear program is then solved to obtain the distance of “moving” a candidate document’s sentences to match a reference document. SMS has shown better results over ROUGE-L in evaluating generated summaries and student essays. BERTScore is recent metric for evaluating translation (Zhang et al., 2019).2 BERTScore first obtains BERT representations of each word in the candidate and reference by feeding the candidate and reference through a BERT model separately. An alignment is then computed between candidate and reference words by computing pairwise cosine similarity. This alignment is then a"
D19-5817,W18-2611,0,0.0400521,"as BLEU and METEOR were originally developed and tested for evaluation of machine translation. These metrics have grown to become popular choices in evaluating all forms of natural language generation, including image captioning, question answering, and dialog systems. As these metrics continue to be used, there have been a number of papers that try to assess how suitable these metrics are for different domains. Nema and Khapra (2018) show that for question generation, n-gram metrics assign scores that correlate poorly to the notion of answerability (i.e., is a generated question answerable). Yang et al. (2018) study the effect of using BLEU and ROUGE in evaluating QA, focusing on yes-no and entity questions on the Chinese DuReader dataset (He et al., 2017). For these types of questions, changing a single word from a gold answer can lead to an incorrect answer. In these cases, BLEU and ROUGE assign scores that do not necessarily reflect the correctness of an answer. Our work is continuation of this line of work in assessing the quality of current metrics for use in evaluating question answering across a number of datasets. Because of the inherent limitations of n-gram metrics, recent work has focuse"
E17-1014,P16-1096,0,0.0261968,"in both scenarios - As can be seen from the ablation test in Table 3, in both supervised and annotator development settings, our pretrained embeddings improve performance by at least 13 points 5 https://github.com/allenai/ openie-standalone 148 1 7 To the best of our knowledge, there has been no previous work attempting to recognize in-context adverse drug reaction mentions on the CADEC corpus. There are, however, several papers which addressed the same task on a different corpus, and others who have used the CADEC corpus for orthogonal tasks. In this section we survey two such recent papers. Limsopatham and Collier (2016) have used CADEC for the normalization of medical concepts. They take as input an out-of-context ADR (e.g., “I couldn’t sleep all night” or “head explodes”) and predict its normalized form (e.g., “insomnia” or “headache”, respectively), based on a predefined vocabulary. They use an RNN model and report accuracy of 79.98. This task can be seen a subsequent task to ours. The ADR spans we output can serve as an input for ADR normalization, giving medical experts a consolidated summary of the reported adverse events. Iqbal et al. (2015) share our motivation to identify ADR mentions in the context"
E17-1014,J93-2004,0,0.0690224,"ngful and frequently occurring in our setting (see details in Section 6). Figure 1 shows the complete architecture of our model, including the RNN transducer LSTM and the pretrained word embeddings augmented with DBpedia entity embeddings. The loss from the network propagates back to the word embeddings, allowing them to assimilate task-specific information during training. The motivation for using external knowledge bases in our case stems from the relatively small size of the CADEC corpus (see Table 1), in comparison with other neural models training corpora. For example, The Penn Treebank (Marcus et al., 1993) which is often used for training dependency parsing algorithms, consists of roughly 7M tokens, versus only about 95K tokens in CADEC. 145 5 Human in the Loop From our experience, real world applications often do not have a pre-existing rich gold standard corpus from which they can efficiently train high quality models. This lack creates a serious impediment to entering and exploring the opportunities for text analytics in such domains, due to the high cost of producing the requisite semantic assets. The medical domain is one where this is especially true. Even within a particular specialty, e"
E17-1014,W95-0107,0,0.0681022,"m a much larger (often unsupervised) corpus. We experiment with initializing our word embeddings from both out of domain (and out of the box) word embeddings from Google (Mikolov et al., 2013), as well as with purpose trained embeddings utilizing predicate-argument structure from Open-IE (Etzioni et al., 2008) (following (Stanovsky et al., 2015)) from the Blekko medical corpus (a 2GB corpus of web pages categorized as “medical domain” by the Blekko search engine4 ). “IO stoppedO takingO AmbienO afterO threeO weeksO – itO gaveO meO aO terribleB headacheI ” This formulation, termed BIO tagging (Ramshaw and Marcus, 1995; Sang and Veenstra, 1999), is equivalent to noun phrase (NP) chunking annotation convention with a single type of NP. While Ratinov and Roth (2009) have shown that a more elaborate tagging scheme (BILOU)3 improved performance in their experiments in Named Entity Recognition, those experiments are out of the scope for this work. 4 Augmenting RNNs with DBpedia Despite the original motivation for knowledge graph embedding, few efforts were made to use 3 BILOU uses tags for: Beginning, Inside, Last and Unit length chunks. 4 144 https://en.wikipedia.org/wiki/Blekko Text Snippet “I’ve tried Klonopi"
E17-1014,W09-1119,0,0.0505519,"beddings from Google (Mikolov et al., 2013), as well as with purpose trained embeddings utilizing predicate-argument structure from Open-IE (Etzioni et al., 2008) (following (Stanovsky et al., 2015)) from the Blekko medical corpus (a 2GB corpus of web pages categorized as “medical domain” by the Blekko search engine4 ). “IO stoppedO takingO AmbienO afterO threeO weeksO – itO gaveO meO aO terribleB headacheI ” This formulation, termed BIO tagging (Ramshaw and Marcus, 1995; Sang and Veenstra, 1999), is equivalent to noun phrase (NP) chunking annotation convention with a single type of NP. While Ratinov and Roth (2009) have shown that a more elaborate tagging scheme (BILOU)3 improved performance in their experiments in Named Entity Recognition, those experiments are out of the scope for this work. 4 Augmenting RNNs with DBpedia Despite the original motivation for knowledge graph embedding, few efforts were made to use 3 BILOU uses tags for: Beginning, Inside, Last and Unit length chunks. 4 144 https://en.wikipedia.org/wiki/Blekko Text Snippet “I’ve tried Klonopin which gave me nightmarish side effects, Lexapro which made me gain 30 lbs and that gave me more anxiety and borderline depression, Effexor which m"
E17-1014,E99-1023,0,0.114878,"upervised) corpus. We experiment with initializing our word embeddings from both out of domain (and out of the box) word embeddings from Google (Mikolov et al., 2013), as well as with purpose trained embeddings utilizing predicate-argument structure from Open-IE (Etzioni et al., 2008) (following (Stanovsky et al., 2015)) from the Blekko medical corpus (a 2GB corpus of web pages categorized as “medical domain” by the Blekko search engine4 ). “IO stoppedO takingO AmbienO afterO threeO weeksO – itO gaveO meO aO terribleB headacheI ” This formulation, termed BIO tagging (Ramshaw and Marcus, 1995; Sang and Veenstra, 1999), is equivalent to noun phrase (NP) chunking annotation convention with a single type of NP. While Ratinov and Roth (2009) have shown that a more elaborate tagging scheme (BILOU)3 improved performance in their experiments in Named Entity Recognition, those experiments are out of the scope for this work. 4 Augmenting RNNs with DBpedia Despite the original motivation for knowledge graph embedding, few efforts were made to use 3 BILOU uses tags for: Beginning, Inside, Last and Unit length chunks. 4 144 https://en.wikipedia.org/wiki/Blekko Text Snippet “I’ve tried Klonopin which gave me nightmaris"
E17-1014,P15-2050,1,0.685573,"Missing"
E17-1014,E12-2021,0,0.057079,"Missing"
K19-1042,P13-1023,0,0.0601246,"Missing"
K19-1042,W13-2322,0,0.0513238,"Missing"
K19-1042,D14-1159,1,0.825604,"novsky2,4 Vivek Srikumar3 Yichu Zhou3 Jonathan Berant1,2 1 Tel-Aviv University, 2 Allen Institute for AI 3 The University of Utah, 4 University of Washington {omri.koshorek,joberant}@cs.tau.ac.il gabis@allenai.org, {flyaway,svivek}@cs.utah.edu Abstract Banarescu et al., 2013), universal conceptual cognitive annotation (UCCA; Abend and Rappoport, 2013), question-answer driven SRL (QA-SRL; He et al., 2015), and universal dependencies (Nivre et al., 2016), as well as domain-specific semantic representations for particular users in fields such as biology (Kim et al., 2009; N´edellec et al., 2013; Berant et al., 2014) and material science (Mysore et al., 2017; Kim et al., 2019). Currently, the dominant paradigm for building models that predict such representations is supervised learning, which requires annotating thousands of sentences with their correct structured representation, usually by experts. This arduous data collection is the main bottleneck for building parsers for different users in new domains. Past work has proposed directions for accelerating data collection and improving data efficiency through multi-task learning across different representations (Stanovsky and Dagan, 2018; Hershcovich et a"
K19-1042,P18-1174,0,0.302926,"collection and improving data efficiency through multi-task learning across different representations (Stanovsky and Dagan, 2018; Hershcovich et al., 2018), or having non-experts annotate sentences in natural language (He et al., 2015, 2016). One of the classic and natural solutions for reducing annotation costs is to use active learning, an iterative procedure for selecting unlabeled examples which are most likely to improve the performance of a model, and annotating them (Settles, 2009). Recently, learning to actively-learn (LTAL) has been proposed (Fang et al., 2017; Bachman et al., 2017; Liu et al., 2018), where the procedure for selecting unlabeled examples is trained using methods from reinforcement and imitation learning. In recent work by Liu et al. (2018), given a labeled dataset from some domain, active learning is simulated on this dataset, and a policy is trained to iteratively select the subset of examples that maximizes performance on a development set. Then, this policy is used on a target domain to select unlabeled examples for annotation. If the learned One of the goals of natural language understanding is to develop models that map sentences into meaning representations. However,"
K19-1042,D19-1003,0,0.0858022,"OM S MALL M ODEL O RACLE S MALL M ODEL 110 45.2 43.3 46.6 44.8 44.2 45.2 30.0 40.9 45.1 44.9 51.9 53.8 150 47.2 49.2 48.8 47.4 48.3 47.5 38.2 44.6 48.6 48.8 54.8 56.6 210 50.5 52.9 51.4 52.1 52.5 51.9 41.0 50.1 52.4 51.4 57.3 58.9 290 53.1 56.8 55.8 55.9 55.5 55.1 51.5 54.1 55.6 53.9 59.5 60.3 370 55.8 57.8 57.6 — 58.0 56.7 53.7 — 57.1 57.0 61.4 61.5 510 58.5 60.3 58.6 — 59.8 58.7 57.2 — 59.8 59.2 62.6 63.3 tecture modifications do not expose an advantage of the oracle policy compared to the random one. We did not examine a simpler linear model for span detection, in light of recent findings (Lowell et al., 2019) that it is important to test LTAL with state-of-the-art models, as performance is tied to the specific model being trained. Myopicity We hypothesized that greedily selecting an example that maximizes performance in a specific iteration might be suboptimal in the long run. Because non-greedy selection strategies are computationaly intractable, we perform the following two experiments. First, we examine E PSILON -G REEDY- P, where in each iteration the oracle policy selects the set Cj that maximizes target performance with probability 1 − p and randomly chooses a set with probability p. This is"
K19-1042,D17-1063,0,0.0315033,"proposed directions for accelerating data collection and improving data efficiency through multi-task learning across different representations (Stanovsky and Dagan, 2018; Hershcovich et al., 2018), or having non-experts annotate sentences in natural language (He et al., 2015, 2016). One of the classic and natural solutions for reducing annotation costs is to use active learning, an iterative procedure for selecting unlabeled examples which are most likely to improve the performance of a model, and annotating them (Settles, 2009). Recently, learning to actively-learn (LTAL) has been proposed (Fang et al., 2017; Bachman et al., 2017; Liu et al., 2018), where the procedure for selecting unlabeled examples is trained using methods from reinforcement and imitation learning. In recent work by Liu et al. (2018), given a labeled dataset from some domain, active learning is simulated on this dataset, and a policy is trained to iteratively select the subset of examples that maximizes performance on a development set. Then, this policy is used on a target domain to select unlabeled examples for annotation. If the learned One of the goals of natural language understanding is to develop models that map sentenc"
K19-1042,W18-2501,0,0.0604564,"Missing"
K19-1042,W13-2001,0,0.0473772,"Missing"
K19-1042,D15-1076,0,0.133676,"the dominant paradigm for building models that predict such representations is supervised learning, which requires annotating thousands of sentences with their correct structured representation, usually by experts. This arduous data collection is the main bottleneck for building parsers for different users in new domains. Past work has proposed directions for accelerating data collection and improving data efficiency through multi-task learning across different representations (Stanovsky and Dagan, 2018; Hershcovich et al., 2018), or having non-experts annotate sentences in natural language (He et al., 2015, 2016). One of the classic and natural solutions for reducing annotation costs is to use active learning, an iterative procedure for selecting unlabeled examples which are most likely to improve the performance of a model, and annotating them (Settles, 2009). Recently, learning to actively-learn (LTAL) has been proposed (Fang et al., 2017; Bachman et al., 2017; Liu et al., 2018), where the procedure for selecting unlabeled examples is trained using methods from reinforcement and imitation learning. In recent work by Liu et al. (2018), given a labeled dataset from some domain, active learning"
K19-1042,L16-1262,0,0.0301266,"Missing"
K19-1042,D16-1258,0,0.0481928,"Missing"
K19-1042,P18-1035,0,0.0219761,"nt et al., 2014) and material science (Mysore et al., 2017; Kim et al., 2019). Currently, the dominant paradigm for building models that predict such representations is supervised learning, which requires annotating thousands of sentences with their correct structured representation, usually by experts. This arduous data collection is the main bottleneck for building parsers for different users in new domains. Past work has proposed directions for accelerating data collection and improving data efficiency through multi-task learning across different representations (Stanovsky and Dagan, 2018; Hershcovich et al., 2018), or having non-experts annotate sentences in natural language (He et al., 2015, 2016). One of the classic and natural solutions for reducing annotation costs is to use active learning, an iterative procedure for selecting unlabeled examples which are most likely to improve the performance of a model, and annotating them (Settles, 2009). Recently, learning to actively-learn (LTAL) has been proposed (Fang et al., 2017; Bachman et al., 2017; Liu et al., 2018), where the procedure for selecting unlabeled examples is trained using methods from reinforcement and imitation learning. In recent work b"
K19-1042,S14-2008,0,0.0250035,"ation strongly affects the examples selected by the oracle. We conclude that the current applicability of LTAL for improving data efficiency in learning semantic meaning representations is limited. 1 Introduction The task of mapping a natural language sentence into a semantic representation, that is, a structure that represents its meaning, is one of the core goals of natural language processing. This goal has led to the creation of many general-purpose formalisms for representing the structure of language, such as semantic role labeling (SRL; Palmer et al., 2005), semantic dependencies (SDP; Oepen et al., 2014), abstract meaning representation (AMR; 452 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 452–462 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics 2 policy generalizes well, we can reduce the cost of learning semantic representations. Liu et al. (2018) and Vu et al. (2019) have shown that such learned policies significantly reduce annotation costs on both text classification and named entity recognition (NER). Learning to Actively Learn Classic pool-based active learning (Settles, 2009) assumes access to a small label"
K19-1042,J05-1004,0,0.160826,"hile in our setup the stochastic nature of optimization strongly affects the examples selected by the oracle. We conclude that the current applicability of LTAL for improving data efficiency in learning semantic meaning representations is limited. 1 Introduction The task of mapping a natural language sentence into a semantic representation, that is, a structure that represents its meaning, is one of the core goals of natural language processing. This goal has led to the creation of many general-purpose formalisms for representing the structure of language, such as semantic role labeling (SRL; Palmer et al., 2005), semantic dependencies (SDP; Oepen et al., 2014), abstract meaning representation (AMR; 452 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 452–462 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics 2 policy generalizes well, we can reduce the cost of learning semantic representations. Liu et al. (2018) and Vu et al. (2019) have shown that such learned policies significantly reduce annotation costs on both text classification and named entity recognition (NER). Learning to Actively Learn Classic pool-based active learnin"
K19-1042,W09-1401,0,0.0514371,"Representations Omri Koshorek1 Gabriel Stanovsky2,4 Vivek Srikumar3 Yichu Zhou3 Jonathan Berant1,2 1 Tel-Aviv University, 2 Allen Institute for AI 3 The University of Utah, 4 University of Washington {omri.koshorek,joberant}@cs.tau.ac.il gabis@allenai.org, {flyaway,svivek}@cs.utah.edu Abstract Banarescu et al., 2013), universal conceptual cognitive annotation (UCCA; Abend and Rappoport, 2013), question-answer driven SRL (QA-SRL; He et al., 2015), and universal dependencies (Nivre et al., 2016), as well as domain-specific semantic representations for particular users in fields such as biology (Kim et al., 2009; N´edellec et al., 2013; Berant et al., 2014) and material science (Mysore et al., 2017; Kim et al., 2019). Currently, the dominant paradigm for building models that predict such representations is supervised learning, which requires annotating thousands of sentences with their correct structured representation, usually by experts. This arduous data collection is the main bottleneck for building parsers for different users in new domains. Past work has proposed directions for accelerating data collection and improving data efficiency through multi-task learning across different representation"
K19-1042,D14-1162,0,0.0826899,"oder, producing a representation hi for every token. Each span xi∶j is represented by concatenating the respective hidden states: sij = [hi ; hj ]. A fully connected network consumes the span representation sij , and predicts a probability whether the span is an argument or not. To accelerate training, we reduce the number of parameters to 488K by freezing the token embeddings, reducing the number of layers in the encoder, and by shrinking the dimension of both the hidden representations and the binary predicate indicator embedding. Following FitzGerald et al. (2018), we use GLoVe embeddings (Pennington et al., 2014). AllenNLP (Gardner et al., 2018), and experiment with two variants: (1) NER-M ULTILANG: A BiLSTM CRF model (20K parameters) with 40 dimensional multi-lingual word embeddings (Ammar et al., 2016), and (2) NER-L INEAR: A linear CRF model which was originally used by Liu et al. (2018). 5.3 Results Span Detection: Table 2 shows F1 score (the official metric) of the QA-SRL span detector models for different sizes of Slab for BASE O RACLE and the other baselines. Figure 2 (left) shows the relative improvement of the baselines over R AN DOM . We observe that the maximal improvement of BASE O RACLE o"
K19-1042,W03-0419,0,0.169446,"Missing"
K19-1042,D18-1318,0,0.0202569,"representation models. In a large empirical study, Lowell et al. (2019) have recently shown other limitations in active learning. They investigate the performance of active learning across NLP tasks and model architectures, and demonstrate that it does not achieve consistent gains over supervised learning, mostly because the collected samples are beneficial to a specific model architecture, and does not yield better results than random selection when switching to a new architecture. There has been little research regarding active learning of semantic representations. Among the relevant work, Siddhant and Lipton (2018) have shown that uncertainty estimation using dropout and Bayes-By-Backprop (Blundell et al., 2015) achieves good results on the SRL formulation. The improvements in performance due to LTAL approaches on various tasks (Konyushkova et al., 2017; Bachman et al., 2017; Fang et al., 2017; Liu et al., 2018) has raised the question whether learned policies can be applied also to the field of learning semantic representations. 8 Acknowledgements We thank Julian Michael and Oz Anani for their useful comments and feedback. This research was supported by The U.S-Israel Binational Science Foundation gran"
K19-1042,P19-1401,0,0.0204549,"the core goals of natural language processing. This goal has led to the creation of many general-purpose formalisms for representing the structure of language, such as semantic role labeling (SRL; Palmer et al., 2005), semantic dependencies (SDP; Oepen et al., 2014), abstract meaning representation (AMR; 452 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 452–462 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics 2 policy generalizes well, we can reduce the cost of learning semantic representations. Liu et al. (2018) and Vu et al. (2019) have shown that such learned policies significantly reduce annotation costs on both text classification and named entity recognition (NER). Learning to Actively Learn Classic pool-based active learning (Settles, 2009) assumes access to a small labeled dataset Slab and a large pool of unlabeled examples Sunlab for a target task. In each iteration, a heuristic is used to select L unlabeled examples, which are sent to annotation and added to Slab . An example heuristic is uncertainty sampling (Lewis and Gale, 1994), which at each iteration chooses examples that the current model is the least con"
K19-1042,D18-1342,0,0.0483898,"Missing"
K19-1042,P18-1191,0,\N,Missing
N18-1081,D11-1142,0,0.960397,"corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader et al., 2014), and knowledge base population (Angeli et al., 2015). However, perhaps due to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling (Zhou and Xu, 2015; He et al., 2017). We also introduce a method to calculate extraction confidence, allowing us to effectively trade off precision and recall (Section 4)."
N18-1081,N03-1013,0,0.11271,"Missing"
N18-1081,W12-3010,0,0.210272,"Missing"
N18-1081,P17-1044,1,0.941233,"ue to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling (Zhou and Xu, 2015; He et al., 2017). We also introduce a method to calculate extraction confidence, allowing us to effectively trade off precision and recall (Section 4). ∗ Work performed while at Bar-Ilan University. Our code and models are made publicly available at https://github.com/gabrielStanovsky/ supervised-oie 1 Experiments demonstrate that our approach out885 Proceedings of NAACL-HLT 2018, pages 885–895 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2.2 performs state-of-the-art Open IE systems on several benchmarks (Section 6), including three that were collected independen"
N18-1081,P15-1034,0,0.374367,"o the QAMR corpus (Michael et al., 2018), an open variant of QA-SRL that covers a wider range of predicate-argument structures (Section 5). The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader et al., 2014), and knowledge base population (Angeli et al., 2015). However, perhaps due to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling"
N18-1081,D15-1076,1,0.951313,"partly because it causes antibiotic resistance) Figure 1: Open IE extractions from an example sentence. Each proposition is composed of a tuple with a single predicate position (in bold), and an ordered list of arguments, separated by semicolons. We build on recent work that studies other natural-language driven representations of predicate argument structure, which can be annotated by non-experts. Recently, Stanovsky and Dagan (2016) created the first labeled corpus for evaluation of Open IE by an automatic translation from question-answer driven semantic role labeling (QA-SRL) annotations (He et al., 2015). We extend these techniques and apply them to the QAMR corpus (Michael et al., 2018), an open variant of QA-SRL that covers a wider range of predicate-argument structures (Section 5). The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader e"
N18-1081,J93-2004,0,0.0607468,"imilar to other cases in NLP, we would like to allow some variability in the predicted tuples. For example, for the sentence The sheriff standing against the wall spoke in a very soft voice we would want to treat both (The Sheriff; spoke; in a soft voice) and (The sheriff standing against the wall; spoke; in a very soft voice) as acceptable extractions. To that end, we follow He et al. (2015) which judge an argument as correct if and only if it includes the syntactic head of the gold argument (and similarly for predicates). For OIE2016, we use the available Penn Treebank gold syntactic trees (Marcus et al., 1993), while for the other test sets, we use predicted trees instead. While this metric may sometimes be too lenient, it does allow a more balanced and fair comparison between systems which can make different, but equally valid, span boundary decisions. 6.3 Performance Analysis In our analysis, we find that RnnOIE generalizes to unseen predicates, produces more and shorter arguments on average than are in the gold extractions, and, like all of the systems we tested, struggles with nominal predicates. Unseen predicates We split the propositions in the gold and predicted OIE2016 test set into two par"
N18-1081,P11-1062,1,0.79897,"e labeling (QA-SRL) annotations (He et al., 2015). We extend these techniques and apply them to the QAMR corpus (Michael et al., 2018), an open variant of QA-SRL that covers a wider range of predicate-argument structures (Section 5). The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader et al., 2014), and knowledge base population (Angeli et al., 2015). However, perhaps due to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for"
N18-1081,D12-1048,0,0.899671,"and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader et al., 2014), and knowledge base population (Angeli et al., 2015). However, perhaps due to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling (Zhou and Xu, 2015; He et al., 2017). We also introduce a method to calculate extraction confidence, allowing us to effectively trade off precision and recall (Section 4). ∗ Work performed whil"
N18-1081,D16-1006,0,0.270204,"tions asserted by a given input sentence are extracted (see Figure 1 for examples). The broadness of this definition, along with the lack of a standard benchmark dataset for the task, prompted the development of various Open IE systems tackling different facets of the task. While most Open IE systems aim to extract the common case of verbal binary propositions (i.e, subject-verb-object tuples), some systems specialize in other syntactic constructions, including noun-mediated relations (Yahya et al., 2014; Pal and Mausam, 2016), n-ary relations (Akbik and L¨oser, 2012), or nested propositions (Bhutani et al., 2016). Many different modeling approaches have also been developed for Open IE. Some of the early systems made use of distant supervision (Banko et al., 2007; Wu and Weld, 2010), while the current best systems use rule-based techniques to extract predicate-argument structures as a post-processing step over an intermediate representation. ReVerb (Fader et al., 2011) extracts Open IE propositions from part of speech tags, OLLIE (Mausam et al., 2012), ClausIE (Del Corro and Gemulla, 2013) and PropS (Stanovsky et al., 2016) postprocess dependency trees, and Open IE42 extracts tuples from Semantic Role"
N18-1081,N18-2089,1,0.915945,"rom an example sentence. Each proposition is composed of a tuple with a single predicate position (in bold), and an ordered list of arguments, separated by semicolons. We build on recent work that studies other natural-language driven representations of predicate argument structure, which can be annotated by non-experts. Recently, Stanovsky and Dagan (2016) created the first labeled corpus for evaluation of Open IE by an automatic translation from question-answer driven semantic role labeling (QA-SRL) annotations (He et al., 2015). We extend these techniques and apply them to the QAMR corpus (Michael et al., 2018), an open variant of QA-SRL that covers a wider range of predicate-argument structures (Section 5). The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader et al., 2014), and knowledge base population (Angeli et al., 2015). However, perhaps d"
N18-1081,D13-1043,0,0.0752202,"extraction confidence, allowing us to effectively trade off precision and recall (Section 4). ∗ Work performed while at Bar-Ilan University. Our code and models are made publicly available at https://github.com/gabrielStanovsky/ supervised-oie 1 Experiments demonstrate that our approach out885 Proceedings of NAACL-HLT 2018, pages 885–895 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2.2 performs state-of-the-art Open IE systems on several benchmarks (Section 6), including three that were collected independently of our work (Xu et al., 2013; de S´a Mesquita et al., 2013; Schneider et al., 2017). This shows that for Open IE, careful data curation and model design can push the state of the art using supervised learning. 2 Recent work addressed the lack of labeled reference Open IE datasets for comparatively evaluating extractors. Stanovsky and Dagan (2016) created a large Open IE corpus (OIE2016) for verbal predicates by automatic conversion from QA-SRL (He et al., 2015), a variant of traditional SRL that labels arguments of verbs with simple, template-based natural language questions. Schneider et al. (2017) aggregated datasets annotated independently in prev"
N18-1081,W16-1307,0,0.223307,") was to extend traditional (closed) information extraction, such that all of the propositions asserted by a given input sentence are extracted (see Figure 1 for examples). The broadness of this definition, along with the lack of a standard benchmark dataset for the task, prompted the development of various Open IE systems tackling different facets of the task. While most Open IE systems aim to extract the common case of verbal binary propositions (i.e, subject-verb-object tuples), some systems specialize in other syntactic constructions, including noun-mediated relations (Yahya et al., 2014; Pal and Mausam, 2016), n-ary relations (Akbik and L¨oser, 2012), or nested propositions (Bhutani et al., 2016). Many different modeling approaches have also been developed for Open IE. Some of the early systems made use of distant supervision (Banko et al., 2007; Wu and Weld, 2010), while the current best systems use rule-based techniques to extract predicate-argument structures as a post-processing step over an intermediate representation. ReVerb (Fader et al., 2011) extracts Open IE propositions from part of speech tags, OLLIE (Mausam et al., 2012), ClausIE (Del Corro and Gemulla, 2013) and PropS (Stanovsky et a"
N18-1081,D14-1162,0,0.0820803,"Missing"
N18-1081,W95-0107,0,0.240007,"ed below the dashed lines, where subscripts indicate the associated BIO label. Demonstrating: (a) the encoding of a multi-word predicate, (b) several arguments collapsed into the same A0 argument position, (c) argument position deviating from the sentence ordering. (w1 , . . . , wn ), a tuple consists of (x1 , . . . , xm ), where each xi is a contiguous subspan of S. One of the xi is distinguished as the predicate (marked in bold in Figure 1), while the other spans are considered its arguments. Following this definition, we reformulate Open IE as a sequence labeling task, using a custom BIO3 (Ramshaw and Marcus, 1995; Sang and Veenstra, 1999) scheme adapted from recent deep SRL models (He et al., 2017). In our formulation, the set of Open IE tuples for a sentence S are grouped by predicate head-word p, as shown in Table 2. For instance, example (b) lists two tuples for the predicate head “born”, which is underlined in the sentence. Grouping tuples this way allows us to run the model once for each predicate head, and accumulate the predictions across predicates to produce the final set of extractions. Open IE tuples deviate from SRL predicateargument structures in two major respects. First, while SRL gener"
N18-1081,E99-1023,0,0.376532,"where subscripts indicate the associated BIO label. Demonstrating: (a) the encoding of a multi-word predicate, (b) several arguments collapsed into the same A0 argument position, (c) argument position deviating from the sentence ordering. (w1 , . . . , wn ), a tuple consists of (x1 , . . . , xm ), where each xi is a contiguous subspan of S. One of the xi is distinguished as the predicate (marked in bold in Figure 1), while the other spans are considered its arguments. Following this definition, we reformulate Open IE as a sequence labeling task, using a custom BIO3 (Ramshaw and Marcus, 1995; Sang and Veenstra, 1999) scheme adapted from recent deep SRL models (He et al., 2017). In our formulation, the set of Open IE tuples for a sentence S are grouped by predicate head-word p, as shown in Table 2. For instance, example (b) lists two tuples for the predicate head “born”, which is underlined in the sentence. Grouping tuples this way allows us to run the model once for each predicate head, and accumulate the predictions across predicates to produce the final set of extractions. Open IE tuples deviate from SRL predicateargument structures in two major respects. First, while SRL generally deals with single-wor"
N18-1081,W17-5402,0,0.151396,"allowing us to effectively trade off precision and recall (Section 4). ∗ Work performed while at Bar-Ilan University. Our code and models are made publicly available at https://github.com/gabrielStanovsky/ supervised-oie 1 Experiments demonstrate that our approach out885 Proceedings of NAACL-HLT 2018, pages 885–895 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2.2 performs state-of-the-art Open IE systems on several benchmarks (Section 6), including three that were collected independently of our work (Xu et al., 2013; de S´a Mesquita et al., 2013; Schneider et al., 2017). This shows that for Open IE, careful data curation and model design can push the state of the art using supervised learning. 2 Recent work addressed the lack of labeled reference Open IE datasets for comparatively evaluating extractors. Stanovsky and Dagan (2016) created a large Open IE corpus (OIE2016) for verbal predicates by automatic conversion from QA-SRL (He et al., 2015), a variant of traditional SRL that labels arguments of verbs with simple, template-based natural language questions. Schneider et al. (2017) aggregated datasets annotated independently in previous Open IE efforts (WEB"
N18-1081,D16-1252,1,0.932243,"the-art in Open IE on benchmark datasets. 1 (mercury filling; particularly prevalent; in the USA) (mercury filling; causes; antibiotic resistance) (mercury filling; was banned; in the EU; partly because it causes antibiotic resistance) Figure 1: Open IE extractions from an example sentence. Each proposition is composed of a tuple with a single predicate position (in bold), and an ordered list of arguments, separated by semicolons. We build on recent work that studies other natural-language driven representations of predicate argument structure, which can be annotated by non-experts. Recently, Stanovsky and Dagan (2016) created the first labeled corpus for evaluation of Open IE by an automatic translation from question-answer driven semantic role labeling (QA-SRL) annotations (He et al., 2015). We extend these techniques and apply them to the QAMR corpus (Michael et al., 2018), an open variant of QA-SRL that covers a wider range of predicate-argument structures (Section 5). The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic p"
N18-1081,P10-1013,0,0.735933,"Section 5). The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader et al., 2014), and knowledge base population (Angeli et al., 2015). However, perhaps due to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling (Zhou and Xu, 2015; He et al., 2017). We also introduce a method to calculate extraction confidence, allowing us to effectivel"
N18-1081,N13-1107,0,0.0293875,"ce a method to calculate extraction confidence, allowing us to effectively trade off precision and recall (Section 4). ∗ Work performed while at Bar-Ilan University. Our code and models are made publicly available at https://github.com/gabrielStanovsky/ supervised-oie 1 Experiments demonstrate that our approach out885 Proceedings of NAACL-HLT 2018, pages 885–895 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2.2 performs state-of-the-art Open IE systems on several benchmarks (Section 6), including three that were collected independently of our work (Xu et al., 2013; de S´a Mesquita et al., 2013; Schneider et al., 2017). This shows that for Open IE, careful data curation and model design can push the state of the art using supervised learning. 2 Recent work addressed the lack of labeled reference Open IE datasets for comparatively evaluating extractors. Stanovsky and Dagan (2016) created a large Open IE corpus (OIE2016) for verbal predicates by automatic conversion from QA-SRL (He et al., 2015), a variant of traditional SRL that labels arguments of verbs with simple, template-based natural language questions. Schneider et al. (2017) aggregated datasets a"
N18-1081,D14-1038,0,0.0579622,"(Banko et al., 2007) was to extend traditional (closed) information extraction, such that all of the propositions asserted by a given input sentence are extracted (see Figure 1 for examples). The broadness of this definition, along with the lack of a standard benchmark dataset for the task, prompted the development of various Open IE systems tackling different facets of the task. While most Open IE systems aim to extract the common case of verbal binary propositions (i.e, subject-verb-object tuples), some systems specialize in other syntactic constructions, including noun-mediated relations (Yahya et al., 2014; Pal and Mausam, 2016), n-ary relations (Akbik and L¨oser, 2012), or nested propositions (Bhutani et al., 2016). Many different modeling approaches have also been developed for Open IE. Some of the early systems made use of distant supervision (Banko et al., 2007; Wu and Weld, 2010), while the current best systems use rule-based techniques to extract predicate-argument structures as a post-processing step over an intermediate representation. ReVerb (Fader et al., 2011) extracts Open IE propositions from part of speech tags, OLLIE (Mausam et al., 2012), ClausIE (Del Corro and Gemulla, 2013) an"
N18-1081,P15-1109,0,0.0829896,"However, perhaps due to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling (Zhou and Xu, 2015; He et al., 2017). We also introduce a method to calculate extraction confidence, allowing us to effectively trade off precision and recall (Section 4). ∗ Work performed while at Bar-Ilan University. Our code and models are made publicly available at https://github.com/gabrielStanovsky/ supervised-oie 1 Experiments demonstrate that our approach out885 Proceedings of NAACL-HLT 2018, pages 885–895 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2.2 performs state-of-the-art Open IE systems on several benchmarks (Section 6), including three that were co"
N18-2089,W04-2705,0,0.491097,"here is no need for a carefully curated ontology and the labels are highly interpretable. However, we differ from QA-SRL in focusing on all words in the sentence rather than just verbs, and allowing free form questions instead of using templates. Introduction Predicate-argument relationships form a key part of sentential meaning representations, and support answering basic questions such as who did what to whom. Resources for predicate-argument structure are well-developed for verbs (e.g. PropBank (Palmer et al., 2005)) and there have been efforts to study other parts of speech (e.g. NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998)) and introduce whole-sentence structures (e.g. AMR (Banarescu et al., 2013)). However, highly skilled and trained annotators are re1 The QAMR formulation provides a new way of thinking about predicate-argument structure. Any form of sentence meaning—from a vector of real numbers to a logical form—should support the challenge of determining which questions are answerable by the sentence, and what the answers are. A QAMR sidesteps intermediate formal representations by surfacing those questions and an∗ Work performed while at Bar-Ilan University. github.com/uwn"
N18-2089,P13-1023,0,0.0755469,"Missing"
N18-2089,P98-1013,0,0.843785,"ated ontology and the labels are highly interpretable. However, we differ from QA-SRL in focusing on all words in the sentence rather than just verbs, and allowing free form questions instead of using templates. Introduction Predicate-argument relationships form a key part of sentential meaning representations, and support answering basic questions such as who did what to whom. Resources for predicate-argument structure are well-developed for verbs (e.g. PropBank (Palmer et al., 2005)) and there have been efforts to study other parts of speech (e.g. NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998)) and introduce whole-sentence structures (e.g. AMR (Banarescu et al., 2013)). However, highly skilled and trained annotators are re1 The QAMR formulation provides a new way of thinking about predicate-argument structure. Any form of sentence meaning—from a vector of real numbers to a logical form—should support the challenge of determining which questions are answerable by the sentence, and what the answers are. A QAMR sidesteps intermediate formal representations by surfacing those questions and an∗ Work performed while at Bar-Ilan University. github.com/uwnlp/qamr 560 Proceedings of NAACL-H"
N18-2089,J05-1004,0,0.608029,"(He et al., 2015), each question-answer pair corresponds to a predicateargument relationship. There is no need for a carefully curated ontology and the labels are highly interpretable. However, we differ from QA-SRL in focusing on all words in the sentence rather than just verbs, and allowing free form questions instead of using templates. Introduction Predicate-argument relationships form a key part of sentential meaning representations, and support answering basic questions such as who did what to whom. Resources for predicate-argument structure are well-developed for verbs (e.g. PropBank (Palmer et al., 2005)) and there have been efforts to study other parts of speech (e.g. NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998)) and introduce whole-sentence structures (e.g. AMR (Banarescu et al., 2013)). However, highly skilled and trained annotators are re1 The QAMR formulation provides a new way of thinking about predicate-argument structure. Any form of sentence meaning—from a vector of real numbers to a logical form—should support the challenge of determining which questions are answerable by the sentence, and what the answers are. A QAMR sidesteps intermediate formal representations"
N18-2089,W13-2322,0,0.308993,"r from QA-SRL in focusing on all words in the sentence rather than just verbs, and allowing free form questions instead of using templates. Introduction Predicate-argument relationships form a key part of sentential meaning representations, and support answering basic questions such as who did what to whom. Resources for predicate-argument structure are well-developed for verbs (e.g. PropBank (Palmer et al., 2005)) and there have been efforts to study other parts of speech (e.g. NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998)) and introduce whole-sentence structures (e.g. AMR (Banarescu et al., 2013)). However, highly skilled and trained annotators are re1 The QAMR formulation provides a new way of thinking about predicate-argument structure. Any form of sentence meaning—from a vector of real numbers to a logical form—should support the challenge of determining which questions are answerable by the sentence, and what the answers are. A QAMR sidesteps intermediate formal representations by surfacing those questions and an∗ Work performed while at Bar-Ilan University. github.com/uwnlp/qamr 560 Proceedings of NAACL-HLT 2018, pages 560–568 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Asso"
N18-2089,D16-1252,1,0.904315,"Missing"
N18-2089,J93-2004,0,0.0727984,"Missing"
N19-1246,D13-1160,0,0.349224,"nding. Many existing algebra word problem datasets also contain similar phenomena to what is in DROP (Koncel-Kedziorski et al., 2015; Kushman et al., 2014; Hosseini et al., 2014; Clark et al., 2016; Ling et al., 2017). Our dataset is different in that it uses much longer contexts, is more open domain, and requires deeper paragraph understanding. Semantic parsing The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013a, inter alia). It is this literature that we modeled our questions on, particularly looking at the questions in the WikiTableQuestions dataset (Pasupat and Liang, 2015). If we had a structured, tabular representation of the content of our paragraphs, DROP would be largely the same as WikiTableQuestions, with similar (possibly even simpler) question semantics. Our novelty is that we are the first to combine these complex questions with paragraph understanding, with the aim of encouraging systems that can produce comprehensive structural analyses of paragraphs, either explicitly or implicitly."
N19-1246,N18-2017,0,0.0831929,"Missing"
N19-1246,P17-1044,0,0.0139727,"edicates and assigns predicate-specific argument roles.6 To adhere to KDG’s structured representation format, we convert each of these representations into a table, where rows are predicateargument structures and columns correspond to different argument roles. Logical form language Our logical form language identifies five basic elements in the table representation: predicate-argument structures (i.e., table rows), relations (column-headers), strings, num6 We used the AllenNLP implementations of state-of-theart models for all of these representations (Gardner et al., 2017; Dozat et al., 2017; He et al., 2017; Stanovsky et al., 2018). 2372 did the the w er e happ ened goa ls did did did eld tim s tions win intercep the e th the r te af pl ay er d di inhere w were did ys da s is asse p of es more there team there which h whic g lon ce en er as w re we e th di was di er en ce s int po s wn did do ch tou t percen s wa e th of e th sc or ed were people in from did the ed or sc e th s wa rst not en wh w ho the yards long er did aft er pa ss ed the it wa s the kick ed had the caught who threw rst the second happen ed the ed or sc the rst e or m the the be tw ee n event th e ed pen hap is did yea r th e"
N19-1246,D14-1058,0,0.0307579,"complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.1 One could argue that we are adding numerical reasoning as an “additional complexity”, and this is true; however, it is only simple reasoning that is relatively well-understood in the semantic parsing literature, and we use it as a necessary means to force more comprehensive passage understanding. Many existing algebra word problem datasets also contain similar phenomena to what is in DROP (Koncel-Kedziorski et al., 2015; Kushman et al., 2014; Hosseini et al., 2014; Clark et al., 2016; Ling et al., 2017). Our dataset is different in that it uses much longer contexts, is more open domain, and requires deeper paragraph understanding. Semantic parsing The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013a, inter alia). It is this literature that we modeled our questions on, particularly looking at the questions in the WikiTableQuestions dataset (Pasupat and L"
N19-1246,D15-1075,0,0.0729969,"-pretrained-bert 10 For the black-box BERT model, we convert DROP to SQuAD format by using the first match as the gold span. 2373 (6 v.s. 7) due to the GPU memory limit. We adopt the ELMo representations trained on 5.5B corpus for the QANet+ELMo baseline and the large uncased BERT model for the BERT baseline. The hyper-parameters for our NAQANet model (§6) are the same as for the QANet baseline. 5.3 Heuristic Baselines A recent line of work (Gururangan et al., 2018; Kaushik and Lipton, 2018) has identified that popular crowdsourced NLP datasets (such as SQuAD (Rajpurkar et al., 2016) or SNLI (Bowman et al., 2015)) are prone to have artifacts and annotation biases which can be exploited by supervised algorithms that learn to pick up these artifacts as signal instead of more meaningful semantic features. We estimate artifacts by training the QANet model described in Section 5.2 on a version of DROP where either the question or the paragraph input representation vectors are zeroed out (question-only and paragraph-only, respectively). Consequently, the resulting models can then only predict answer spans from either the question or the paragraph. In addition, we devise a baseline that estimates the answer"
N19-1246,P17-1147,0,0.11449,"Missing"
N19-1246,W05-0620,0,0.13903,"Missing"
N19-1246,D18-1546,0,0.0390583,"ion, while the score for BERT is based on an open-source implementation from Hugging Face: https://github.com/huggingface/pytorch-pretrained-bert 10 For the black-box BERT model, we convert DROP to SQuAD format by using the first match as the gold span. 2373 (6 v.s. 7) due to the GPU memory limit. We adopt the ELMo representations trained on 5.5B corpus for the QANet+ELMo baseline and the large uncased BERT model for the BERT baseline. The hyper-parameters for our NAQANet model (§6) are the same as for the QANet baseline. 5.3 Heuristic Baselines A recent line of work (Gururangan et al., 2018; Kaushik and Lipton, 2018) has identified that popular crowdsourced NLP datasets (such as SQuAD (Rajpurkar et al., 2016) or SNLI (Bowman et al., 2015)) are prone to have artifacts and annotation biases which can be exploited by supervised algorithms that learn to pick up these artifacts as signal instead of more meaningful semantic features. We estimate artifacts by training the QANet model described in Section 5.2 on a version of DROP where either the question or the paragraph input representation vectors are zeroed out (question-only and paragraph-only, respectively). Consequently, the resulting models can then only"
N19-1246,P16-1223,0,0.146227,"Missing"
N19-1246,P18-1078,1,0.903373,"Missing"
N19-1246,N19-1423,0,0.270861,"Missing"
N19-1246,K17-3002,0,0.0174839,"es for polysemous predicates and assigns predicate-specific argument roles.6 To adhere to KDG’s structured representation format, we convert each of these representations into a table, where rows are predicateargument structures and columns correspond to different argument roles. Logical form language Our logical form language identifies five basic elements in the table representation: predicate-argument structures (i.e., table rows), relations (column-headers), strings, num6 We used the AllenNLP implementations of state-of-theart models for all of these representations (Gardner et al., 2017; Dozat et al., 2017; He et al., 2017; Stanovsky et al., 2018). 2372 did the the w er e happ ened goa ls did did did eld tim s tions win intercep the e th the r te af pl ay er d di inhere w were did ys da s is asse p of es more there team there which h whic g lon ce en er as w re we e th di was di er en ce s int po s wn did do ch tou t percen s wa e th of e th sc or ed were people in from did the ed or sc e th s wa rst not en wh w ho the yards long er did aft er pa ss ed the it wa s the kick ed had the caught who threw rst the second happen ed the ed or sc the rst e or m the the be tw ee n event th e ed pen hap i"
N19-1246,N18-1023,0,0.0470213,"w in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.1 One could argue that we"
N19-1246,Q18-1023,0,0.0584383,"Missing"
N19-1246,D17-1160,1,0.897774,"ing Semantic parsing has been used to translate natural language utterances into formal executable languages (e.g., SQL) that can perform discrete operations against a structured knowledge representation, such as knowledge graphs or tabular databases (Zettlemoyer and Collins, 2005; Berant et al., 2013b; Yin and Neubig, 2017; Chen and Mooney, 2011, inter alia). Since many of DROP’s questions require similar discrete reasoning, it is appealing to port some of the successful work in semantic parsing to the DROP dataset. Specifically, we use the grammar-constrained semantic parsing model built by Krishnamurthy et al. (2017) (KDG) for the W IKI TABLE Q UESTIONS tabular dataset (Pasupat and Liang, 2015). Sentence representation schemes We experimented with three paradigms to represent paragraphs as structured contexts: (1) Stanford dependencies (de Marneffe and Manning, 2008, Syn Dep); which capture word-level syntactic relations, (2) Open Information Extraction (Banko et al., 2007, Open IE), a shallow semantic representation which directly links predicates and arguments; and (3) Semantic Role Labeling (Carreras and M`arquez, 2005, SRL), which disambiguates senses for polysemous predicates and assigns predicate-sp"
N19-1246,P14-1026,0,0.0365246,"e of these additional complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.1 One could argue that we are adding numerical reasoning as an “additional complexity”, and this is true; however, it is only simple reasoning that is relatively well-understood in the semantic parsing literature, and we use it as a necessary means to force more comprehensive passage understanding. Many existing algebra word problem datasets also contain similar phenomena to what is in DROP (Koncel-Kedziorski et al., 2015; Kushman et al., 2014; Hosseini et al., 2014; Clark et al., 2016; Ling et al., 2017). Our dataset is different in that it uses much longer contexts, is more open domain, and requires deeper paragraph understanding. Semantic parsing The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013a, inter alia). It is this literature that we modeled our questions on, particularly looking at the questions in the WikiTableQuestions"
N19-1246,P17-1003,0,0.0395215,"; Zellers et al., 2018; Zhang et al., 2019; Zellers et al., 2019). In our case, instead of using an adversarial baseline to filter automatically generated examples, we use it in a crowdsourcing task, to teach crowd workers to avoid easy questions, raising the difficulty level of the questions they provide. Neural symbolic reasoning DROP is designed to encourage research on methods that combine neural methods with discrete, symbolic reasoning. We present one such model in Section 6. Other related work along these lines has been done by Reed and de Freitas (2016), Neelakantan et al. (2016), and Liang et al. (2017). 3 DROP Data Collection In this section, we describe our annotation protocol, which consists of three phases. First, we automatically extract passages from Wikipedia which are expected to be amenable to complex questions. Second, we crowdsource question-answer pairs on these passages, eliciting questions which require 2370 discrete reasoning. Finally, we validate the development and test portions of DROP to ensure their quality and report inter-annotator agreement. Passage extraction We searched Wikipedia for passages that had a narrative sequence of events, particularly with a high proportio"
N19-1246,P17-1015,0,0.0311548,"s of text paired with independent questions, with only linguistic facility required to answer the questions.1 One could argue that we are adding numerical reasoning as an “additional complexity”, and this is true; however, it is only simple reasoning that is relatively well-understood in the semantic parsing literature, and we use it as a necessary means to force more comprehensive passage understanding. Many existing algebra word problem datasets also contain similar phenomena to what is in DROP (Koncel-Kedziorski et al., 2015; Kushman et al., 2014; Hosseini et al., 2014; Clark et al., 2016; Ling et al., 2017). Our dataset is different in that it uses much longer contexts, is more open domain, and requires deeper paragraph understanding. Semantic parsing The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013a, inter alia). It is this literature that we modeled our questions on, particularly looking at the questions in the WikiTableQuestions dataset (Pasupat and Liang, 2015). If we had a structured, tab"
N19-1246,W08-1301,0,0.0643822,"Missing"
N19-1246,D18-1260,0,0.0265468,"nswering datasets With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved,"
N19-1246,K18-1007,0,0.0469015,"Missing"
N19-1246,N18-1144,0,0.0225428,"uestion Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single"
N19-1246,L18-1564,0,0.0210105,"aset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single passages of text paired w"
N19-1246,D18-1258,0,0.0257774,"ng conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.1 One could argue that we are adding numerical reasoning as an “additional complexity”, and this is true;"
N19-1246,P16-1144,0,0.0594744,"Missing"
N19-1246,P15-1142,0,0.251793,"et al., 2014; Clark et al., 2016; Ling et al., 2017). Our dataset is different in that it uses much longer contexts, is more open domain, and requires deeper paragraph understanding. Semantic parsing The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013a, inter alia). It is this literature that we modeled our questions on, particularly looking at the questions in the WikiTableQuestions dataset (Pasupat and Liang, 2015). If we had a structured, tabular representation of the content of our paragraphs, DROP would be largely the same as WikiTableQuestions, with similar (possibly even simpler) question semantics. Our novelty is that we are the first to combine these complex questions with paragraph understanding, with the aim of encouraging systems that can produce comprehensive structural analyses of paragraphs, either explicitly or implicitly. Adversarial dataset construction We continue 1 Some questions in our dataset require limited sports domain knowledge to answer; we expect that there are enough such ques"
N19-1246,D14-1162,0,0.0816178,"Missing"
N19-1246,N18-1202,1,0.603282,"Missing"
N19-1246,P18-2124,0,0.0476756,"t can be found at https://allennlp.org/drop. 2 Related Work Question answering datasets With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned"
N19-1246,Q19-1016,0,0.0743529,"Missing"
N19-1246,N18-1081,1,0.786086,"gns predicate-specific argument roles.6 To adhere to KDG’s structured representation format, we convert each of these representations into a table, where rows are predicateargument structures and columns correspond to different argument roles. Logical form language Our logical form language identifies five basic elements in the table representation: predicate-argument structures (i.e., table rows), relations (column-headers), strings, num6 We used the AllenNLP implementations of state-of-theart models for all of these representations (Gardner et al., 2017; Dozat et al., 2017; He et al., 2017; Stanovsky et al., 2018). 2372 did the the w er e happ ened goa ls did did did eld tim s tions win intercep the e th the r te af pl ay er d di inhere w were did ys da s is asse p of es more there team there which h whic g lon ce en er as w re we e th di was di er en ce s int po s wn did do ch tou t percen s wa e th of e th sc or ed were people in from did the ed or sc e th s wa rst not en wh w ho the yards long er did aft er pa ss ed the it wa s the kick ed had the caught who threw rst the second happen ed the ed or sc the rst e or m the the be tw ee n event th e ed pen hap is did yea r th e how many yea rs at wh wa"
N19-1246,N18-1140,0,0.0196887,"e (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.1 One could argue that we are adding numerical reasoning as an “additional complexity”, and this is true; however, it is only simple r"
N19-1246,N18-1059,0,0.0558538,"st baseline system. The dataset, code for the baseline systems, and a leaderboard with a hidden test set can be found at https://allennlp.org/drop. 2 Related Work Question answering datasets With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud"
N19-1246,Q18-1021,0,0.0624305,"these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.1"
N19-1246,D18-1259,0,0.0643269,"ncrease over the best baseline system. The dataset, code for the baseline systems, and a leaderboard with a hidden test set can be found at https://allennlp.org/drop. 2 Related Work Question answering datasets With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefi"
N19-1246,P17-1041,0,0.0121247,"d overlap. When an answer has multiple spans, we first perform a one-to-one alignment greedily based on bag-of-word overlap on the set of spans and then compute average F1 over each span. When there are multiple annotated answers, both metrics take a max over all gold answers. 5.1 Semantic Parsing Semantic parsing has been used to translate natural language utterances into formal executable languages (e.g., SQL) that can perform discrete operations against a structured knowledge representation, such as knowledge graphs or tabular databases (Zettlemoyer and Collins, 2005; Berant et al., 2013b; Yin and Neubig, 2017; Chen and Mooney, 2011, inter alia). Since many of DROP’s questions require similar discrete reasoning, it is appealing to port some of the successful work in semantic parsing to the DROP dataset. Specifically, we use the grammar-constrained semantic parsing model built by Krishnamurthy et al. (2017) (KDG) for the W IKI TABLE Q UESTIONS tabular dataset (Pasupat and Liang, 2015). Sentence representation schemes We experimented with three paradigms to represent paragraphs as structured contexts: (1) Stanford dependencies (de Marneffe and Manning, 2008, Syn Dep); which capture word-level syntact"
N19-1246,D18-1009,0,0.0711059,"Missing"
N19-1246,P18-1156,0,0.0679837,"s, and a leaderboard with a hidden test set can be found at https://allennlp.org/drop. 2 Related Work Question answering datasets With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these"
P15-2050,P12-1015,0,0.0345862,"Missing"
P15-2050,N13-1136,1,0.584267,"ook advantage) are grouped in a single predicate slot. Additionally, arguments are truncated in cases such as prepositional phrases and reduced relative clauses. The resulting structure can be understood as an extension of shallow syntactic chunking (Abney, 1992), where chunks are labeled as either predicates or arguments, and are then interlinked to form a complete proposition. It is not clear apriory whether the differences manifested in Open IE’s structure could be beneficial as intermediate structures for downstream applications. Although a few end tasks have made use of Open IE’s output (Christensen et al., 2013; Balasubramanian et al., 2013), there has been no systematic comparison against other structures. In the following sections, we quantitatively study and analyze the value of Open IE structures against the more common intermediate structures – lexical, dependency and SRL, for three downstream NLP tasks. 3 (a) Lexical matching of a 5 words window (marked with a box). Current window yields a score of 4 - words contributing to the score are marked in bold. (b) Dependency matching yields a score of 3. Contributing triplets are marked in bold. S: refused0.1 : A0 : John A1 : to visit a Vegas casino"
P15-2050,N13-1090,0,0.0553311,"sk Text comprehension tasks extrinsically test natural language understanding through question answer304 Target refused Lexical Dependency SRL Open IE John to visit Vegas nsubj John xcomp visit A0 A1 A1 A1 0 1 1 2 John to visit Vegas systems take three input words (A:A∗ , B:?) and output a word B ∗ , such that the relation between B and B ∗ is closest to the relation between A and A∗ . For instance, queen is the desired answer for the triple (man:king, woman:?). Some recent state-of-the-art approaches to these two tasks derive a similarity score via arithmetic computations on word embeddings (Mikolov et al., 2013b). While original training of word embeddings used lexical contexts (n-grams), recently Levy and Goldberg (2014) generalized this to arbitrary contexts, such as dependency paths. We use their software1 and recompute the word embeddings using contexts from our four structures: lexical context, dependency paths, SRL’s semantic relations, and Open IE’s surrounding tuple elements. Table 1 shows the different contexts for a sample word. John to visit Vegas Table 1: Some of the different contexts for the target word “refused” in the sentence ”John refused to visit Vegas”. SRL and Open IE contexts a"
P15-2050,W08-1301,0,0.0428588,"Missing"
P15-2050,D11-1142,0,0.780242,"Missing"
P15-2050,D13-1020,0,0.00596479,"ch as dependency paths. We use their software1 and recompute the word embeddings using contexts from our four structures: lexical context, dependency paths, SRL’s semantic relations, and Open IE’s surrounding tuple elements. Table 1 shows the different contexts for a sample word. John to visit Vegas Table 1: Some of the different contexts for the target word “refused” in the sentence ”John refused to visit Vegas”. SRL and Open IE contexts are preceded by their element (predicate or argument) index. See figure 1 for the different representations of this sentence. ing. We use the MCTest corpus (Richardson et al., 2013), which is composed of short stories followed by multiple choice questions. The MCTest task does not require extensive world knowledge, which makes it ideal for testing underlying sentence representations, as performance will mostly depend on accuracy and informativeness of the extracted structures. We adapt the unsupervised lexical matching algorithm from the original MCTest paper. It counts lexical matches between an assertion obtained from a candidate answer (CA) and a sliding window over the story. The selected answer is the one for which the maximum number of matches are found. Our adapta"
P15-2050,D07-1002,0,0.0221474,"equence or the bag of words, (2) Stanford dependency parse trees (De Marneffe and Manning, 2008), which draw syntactic relations between words, and (3) Semantic role labeling (SRL), which extracts frames linking predicates with their semantic arguments (Carreras and M`arquez, 2005). For instance, a QA application can evaluate a question and a candidate answer by examining their lexical overlap (P´erez-Couti˜no et al., 2006), by using short dependency paths as features to compare their syntactic relationships (Liang et al., 2013), or by using SRL to compare their predicate-argument structures (Shen and Lapata, 2007). In a seemingly independent research direction, Open Information Extraction (Open IE) extracts coherent propositions from a sentence, each comprising a relation phrase and two or more argument 2 Intermediate Structures In this section we review how intermediate structures differ from each other, in terms of their imposed structure, predicate and argument boundaries, and the type of relations that they introduce. We include Open IE in this analysis, along with lexical, dependency and SRL representations, and highlight its unique properties. As we show in Section 4, these differences have an im"
P15-2050,J15-4004,0,0.0052185,"Missing"
P15-2050,P14-2050,0,0.0651708,"et refused Lexical Dependency SRL Open IE John to visit Vegas nsubj John xcomp visit A0 A1 A1 A1 0 1 1 2 John to visit Vegas systems take three input words (A:A∗ , B:?) and output a word B ∗ , such that the relation between B and B ∗ is closest to the relation between A and A∗ . For instance, queen is the desired answer for the triple (man:king, woman:?). Some recent state-of-the-art approaches to these two tasks derive a similarity score via arithmetic computations on word embeddings (Mikolov et al., 2013b). While original training of word embeddings used lexical contexts (n-grams), recently Levy and Goldberg (2014) generalized this to arbitrary contexts, such as dependency paths. We use their software1 and recompute the word embeddings using contexts from our four structures: lexical context, dependency paths, SRL’s semantic relations, and Open IE’s surrounding tuple elements. Table 1 shows the different contexts for a sample word. John to visit Vegas Table 1: Some of the different contexts for the target word “refused” in the sentence ”John refused to visit Vegas”. SRL and Open IE contexts are preceded by their element (predicate or argument) index. See figure 1 for the different representations of thi"
P15-2050,P10-1040,0,0.0288292,"). lar to the word similarity tasks. To our knowledge, Open IE results on both analogy datasets surpass the state of the art. An example (from the Microsoft test set) which supports the observation regarding Open IE embeddings space is (gentlest:gentler, loudest:?), for which only Open IE answers correctly as louder, while lexical respond with higher-pitched (domain similar to loudest), and dependency with thinnest (functionally similar to loudest). Our Open-IE embeddings are freely available6 and we note that these can serve as plug-in features for other NLP applications, as demonstrated in (Turian et al., 2010). 5 References Steven P Abney. 1992. Parsing by chunks. Principlebased parsing, pages 257–278. Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 19–27. Association for Computational Linguistics. Conclusions Niranjan Balasubramanian, Stephen Mausam, and Oren Etzioni. 2013. coherent event schemas at sca"
P15-2050,J13-2005,0,0.00683734,"(1) Lexical representations, in which features are extracted from the original word sequence or the bag of words, (2) Stanford dependency parse trees (De Marneffe and Manning, 2008), which draw syntactic relations between words, and (3) Semantic role labeling (SRL), which extracts frames linking predicates with their semantic arguments (Carreras and M`arquez, 2005). For instance, a QA application can evaluate a question and a candidate answer by examining their lexical overlap (P´erez-Couti˜no et al., 2006), by using short dependency paths as features to compare their syntactic relationships (Liang et al., 2013), or by using SRL to compare their predicate-argument structures (Shen and Lapata, 2007). In a seemingly independent research direction, Open Information Extraction (Open IE) extracts coherent propositions from a sentence, each comprising a relation phrase and two or more argument 2 Intermediate Structures In this section we review how intermediate structures differ from each other, in terms of their imposed structure, predicate and argument boundaries, and the type of relations that they introduce. We include Open IE in this analysis, along with lexical, dependency and SRL representations, an"
P15-2050,W13-3512,0,0.00940111,"Missing"
P15-2050,D12-1048,1,0.598339,"Missing"
P15-2050,W11-0906,0,\N,Missing
P15-2050,W05-0620,0,\N,Missing
P15-2050,P11-1060,0,\N,Missing
P15-2050,N09-1003,0,\N,Missing
P15-2050,D13-1178,1,\N,Missing
P16-1119,P15-1034,0,0.0265843,"he previous examples were of relative clauses, Figure 1 demonstrates this distinction in various other syntactic constructions. Identifying and removing non-restrictive modifiers yields shorter NP arguments, which proved beneficial in many NLP tasks. In the context of abstractive summarization (Ganesan et al., 2010) or sentence compression (Knight and Marcu, 2002), non-restrictive modifiers can be removed to shorten sentences, while restrictive modification should be preserved. Further, recent work in information extraction showed that shorter arguments can be beneficial for downstream tasks. Angeli et al. (2015) built an Open-IE system which focuses on shorter argument spans, and demonstrated its usefulness in a state-of-the-art Knowledge Base Population system. Stanovsky et al. (2015) compared the performance of several off-the-shelf analyzers in different semantic tasks. Most relevant to this work is the comparison between Open-IE and Semantic Role Labeling (Carreras and M`arquez, 2005). Specifically, they suggest that SRL’s longer arguments introduce noise which hurts performance for downstream tasks. Finally, in question answering, omitting nonrestrictive modification can assist in providing more"
P16-1119,P98-1013,0,0.234061,"Missing"
P16-1119,W13-2322,0,0.0413898,"Missing"
P16-1119,W05-0620,0,0.142981,"Missing"
P16-1119,W08-1301,0,0.0636439,"Missing"
P16-1119,W14-5601,0,0.38984,"relevant to this work is the comparison between Open-IE and Semantic Role Labeling (Carreras and M`arquez, 2005). Specifically, they suggest that SRL’s longer arguments introduce noise which hurts performance for downstream tasks. Finally, in question answering, omitting nonrestrictive modification can assist in providing more concise answers, or in matching between multiple answer occurrences. Despite these benefits, there is currently no consistent large scale annotation of restrictiveness, which hinders the development of automatic tools for its classification. In prior art in this field, Dornescu et al. (2014) used trained annotators to mark restrictiveness in a large corpus. Although they reached good agreement levels in restrictiveness annotation, their corpus suffered from inconsistencies, since it conflated restrictiveness annotation with inconsistent modifier span annotation. The contributions of this work are twofold. Primarily, we propose a novel crowdsroucing anno1256 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1256–1265, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tation methodology which decouples"
P16-1119,P05-1045,0,0.0842967,"Missing"
P16-1119,C10-1039,0,0.0110114,"sident Obama who just came into the room” (Huddleston et al., 2002; Fabb, 1990; Umbach, 2006). The distinction between the two types is semantic in nature and relies heavily on the context of the NP. Evidently, many syntactic constructions can appear in both restrictive and non-restrictive uses. While the previous examples were of relative clauses, Figure 1 demonstrates this distinction in various other syntactic constructions. Identifying and removing non-restrictive modifiers yields shorter NP arguments, which proved beneficial in many NLP tasks. In the context of abstractive summarization (Ganesan et al., 2010) or sentence compression (Knight and Marcu, 2002), non-restrictive modifiers can be removed to shorten sentences, while restrictive modification should be preserved. Further, recent work in information extraction showed that shorter arguments can be beneficial for downstream tasks. Angeli et al. (2015) built an Open-IE system which focuses on shorter argument spans, and demonstrated its usefulness in a state-of-the-art Knowledge Base Population system. Stanovsky et al. (2015) compared the performance of several off-the-shelf analyzers in different semantic tasks. Most relevant to this work is"
P16-1119,D15-1076,0,0.2681,"me of the harder, more context-dependent, cases (most notably, prepositional and adjectival modifiers), our system provides an applicable means for identifying nonrestrictive modification in a realistic NLP setting. 2 Background In this section we cover relevant literature from several domains. In Section 2.1 we discuss the established linguistic distinction between restrictive and non-restrictive modification. Following, in Section 2.2 we discuss previous NLP work on annotating and identifying this distinction. Finally, in Section 2.3 we briefly describe the recent QASRL annotation paradigm (He et al., 2015), which we utilize in Section 3 as part of our annotation scheme. 2.1 Non-Restrictive Modification Throughout the paper we follow Huddleston et al.’s (2002) well-known distinction between two types of NP modifiers: (1) Restrictive modifiers, for which the content of the modifier is an integral part of the meaning of the containing NP, and, in contrast, (2) Non-restrictive modifiers, that present a separate, parenthetical unit of information about the NP. While some syntactic modifiers (such as determiners or genitives) are always restrictive, others are known to appear in both restrictive as w"
P16-1119,J07-3004,0,0.105647,"Missing"
P16-1119,P10-1022,0,0.018824,"Uses a CRF model to classify each modifier as either restrictive or nonrestrictive, based on the features listed in Table 2, 8 While linguistic literature generally regards appositives as non-restrictive, some of the appositions marked in the dependency conversion are in fact misclassified coordinations, which explains why some of them were marked as restrictive. and elaborated below.9 5.1 Baselines We begin by replicating the algorithms in the two prior works discussed in Section 2.2. This allows us to test their performance consistently against our new human annotated dataset. Replicating (Honnibal et al., 2010) They annotated a modifier as restrictive if and only if it was preceded with a comma. We re-implement this baseline and classify all of the modifiers in the test set according to this simple property. Replicating (Dornescu et al., 2014) Their best performing ML-based algorithm10 uses the supervised CRFsuite classifier (Okazaki, 2007) over “standard features used in chunking, such as word form, lemma and part of speech tags”. Replicating their baseline, we extract the list of features detailed in Table 2 (in the row labeled “chunking features”). 5.2 Our Classifier In addition to Dornescu et al"
P16-1119,W08-2123,0,0.0184913,"on-Answer pairs (discussed in Section 2.3), and keep their train / dev / test split into 744 / 249 / 248 sentences, respectively. This conveniently allows us to link between argument NPs and their corresponding argument role question needed for our annotation process, as described in previous section. This dataset is composed of 1241 sentences from the CoNLL 2009 English dataset (Hajiˇc et al., 2009), which consists of newswire text annotated by the Penn TreeBank (Marcus et al., 1993), PropBank (Palmer et al., 2005), and NomBank (Meyers et al., 2004), and converted into dependency grammar by (Johansson and Nugues, 2008). As mentioned in Section 3.2, each of our annotation instances is composed of a sentence s, a verbal predicate v, a noun phrase p, and a modifier m. We extract each such possible tuple from the set of sentences in the following automatic manner: 1. Identify a verb v in the gold dependency tree. 2. Follow its outgoing dependency arcs to a noun phrase argument p (a dependent of v with a nominal part of speech). 3. Find m, a modifying clause of p which might be non-restrictive, according to the rules described in Table 1, under the “Identified By” column. This filters out modifiers which are alw"
P16-1119,J93-2004,0,0.0580913,"ementary, respectively). In the sentence marked (RC1), the highlighted relative clause is restrictive, distinguishing the necklace being referred to from other necklaces, while in sentence (RC2), the relative clause does not pick an entity from a larger set, but instead presents separate information about an already specified definite entity. 2.2 Non-Restrictive Modification in NLP Syntactic and semantic annotations generally avoid the distinction between restrictive and nonrestrictive modification (referred here as “restrictiveness” annotation). The syntactic annotation of the Penn TreeBank (Marcus et al., 1993) and its common conversion to dependency trees (e.g., (de Marneffe and Manning, 2008)) do not differentiate the cases discussed above, providing the same syntactic structure for the semantically different instances. See Figure 2 for an example. Furthermore, prominent semantic annotations, 1257 such as PropBank (Palmer et al., 2005), AMR (Banarescu et al., 2013), CCG (Hockenmaier and Steedman, 2007), or FrameNet (Baker et al., 1998), also avoid this distinction. For example, PropBank does not differentiate between such modifiers, treating both types of modification as an integral part of an arg"
P16-1119,W04-2705,0,0.0341668,"We use the dataset which He et al. (2015) annotated with Question-Answer pairs (discussed in Section 2.3), and keep their train / dev / test split into 744 / 249 / 248 sentences, respectively. This conveniently allows us to link between argument NPs and their corresponding argument role question needed for our annotation process, as described in previous section. This dataset is composed of 1241 sentences from the CoNLL 2009 English dataset (Hajiˇc et al., 2009), which consists of newswire text annotated by the Penn TreeBank (Marcus et al., 1993), PropBank (Palmer et al., 2005), and NomBank (Meyers et al., 2004), and converted into dependency grammar by (Johansson and Nugues, 2008). As mentioned in Section 3.2, each of our annotation instances is composed of a sentence s, a verbal predicate v, a noun phrase p, and a modifier m. We extract each such possible tuple from the set of sentences in the following automatic manner: 1. Identify a verb v in the gold dependency tree. 2. Follow its outgoing dependency arcs to a noun phrase argument p (a dependent of v with a nominal part of speech). 3. Find m, a modifying clause of p which might be non-restrictive, according to the rules described in Table 1, und"
P16-1119,J05-1004,0,0.684328,"ite entity. 2.2 Non-Restrictive Modification in NLP Syntactic and semantic annotations generally avoid the distinction between restrictive and nonrestrictive modification (referred here as “restrictiveness” annotation). The syntactic annotation of the Penn TreeBank (Marcus et al., 1993) and its common conversion to dependency trees (e.g., (de Marneffe and Manning, 2008)) do not differentiate the cases discussed above, providing the same syntactic structure for the semantically different instances. See Figure 2 for an example. Furthermore, prominent semantic annotations, 1257 such as PropBank (Palmer et al., 2005), AMR (Banarescu et al., 2013), CCG (Hockenmaier and Steedman, 2007), or FrameNet (Baker et al., 1998), also avoid this distinction. For example, PropBank does not differentiate between such modifiers, treating both types of modification as an integral part of an argument NP. Two recent works have focused on automatically identifying non-restrictive modifications. Honnibal et al. (2010) added simple automated restrictiveness annotations to NP-modifiers in the CCGbank (Hockenmaier and Steedman, 2007). Following a writing style and grammar rule, a modifier was judged as non-restrictive if and on"
P16-1119,P15-2050,1,0.812718,"Missing"
P16-1119,C98-1013,0,\N,Missing
P16-1119,W09-1201,0,\N,Missing
P16-2077,D11-1142,0,0.416453,"es and their corresponding arguments. Surprisingly, there are no accepted NLP-standards which specify what the “right” span of an argument should be. Semantic representations typically take an inclusive (or maximal) approach: PropBank annotation (Palmer et al., 2005), for example, marks arguments as full constituency subtrees. From an application perspective, this maximal approach ensures that all arguments are indeed embedded within the annotated span, yet it is often not trivial how to accurately recover them. In contrast to this maximal-span approach, Open-IE systems (Etzioni et al., 2008; Fader et al., 2011) put emphasis on extracting readable standalone propositions, typically producing shorter arguments (see examples in Section 2.1). Several recent works have exploited this property, using 1 http://knowitall.github.io/openie Open IE-4 is based on ClearNLPs SRL, allowing for a direct comparison. 2 474 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 474–478, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics a target predicate3 was marked, and are requested to annotate argument role questions (from a restricted gramm"
P16-2077,D15-1076,0,0.431959,"in textual similarity, analogies, and reading comprehension tasks.2 While Open-IE extractors do provide a reduction of argument span, they lack consistency and principled rigor – there is no clear definition for the desired argument span, which is defined defacto by the different implementations. This lack of a common system-independent definition, let alone an annotation methodology, hinders the creation of gold standard argument-span annotation. In this work we propose a concrete argument span reduction criterion and an accompanying annotation procedure, based on the recent QASRL paradigm (He et al., 2015). We show that this criterion can be consistently annotated with high agreement, and that it is intuitive enough to be obtained through crowd-sourcing. As future work, we intend to apply the reduction criterion to other types of predicates (e.g., nomiProminent semantic annotations take an inclusive approach to argument span annotation, marking arguments as full constituency subtrees. Some works, however, showed that identifying a reduced argument span can be beneficial for various semantic tasks. While certain practical methods do extract reduced argument spans, such as in Open-IE , these solu"
P16-2077,D12-1048,0,0.0618862,"(i.e., “Obama”). Notably, different implementations of Open-IE provide an applicable generic way to reduce argument span. Since there are no common guidelines for this task, each Open-IE extractor produces different argument spans. We cover briefly some of the main differences in a few prominent Open-IE systems. ReVerb (Fader et al., 2011) uses part-of-speechbased regular expressions to decide whether a word should be included within an argument span. For example, they move certain light verb compliments and prepositions from the argument to the predicate slot (e.g., “gave a talk at”). OLLIE (Mausam et al., 2012) learns lexical-syntactic patterns and splits extractions across certain prepositions. For example, given “I flew from Paris to Berlin”, OLLIE yields (I; flew; from Paris) and (I; flew; to Berlin). More recently, (Angeli et al., 2015) used natural logic to remove non-integral parts of arguments (e.g., removing the underlined non-restrictive prepositional phrase in “Heinz Fischer of Austria”). 2.2 3 Argument Reduction In this section, we propose annotation criteria and process for obtaining minimal argument spans. Given an original, non-reduced argument, we aim to reduce it to a set of (one or"
P16-2077,J05-1004,0,0.253685,"ystem-dependent, with no commonly accepted standards. In this paper we propose a generic argument reduction criterion, along with an annotation procedure, and show that it can be consistently and intuitively annotated using the recent QA-SRL paradigm. 1 Introduction Representations of predicate-argument structure need to determine the span of predicates and their corresponding arguments. Surprisingly, there are no accepted NLP-standards which specify what the “right” span of an argument should be. Semantic representations typically take an inclusive (or maximal) approach: PropBank annotation (Palmer et al., 2005), for example, marks arguments as full constituency subtrees. From an application perspective, this maximal approach ensures that all arguments are indeed embedded within the annotated span, yet it is often not trivial how to accurately recover them. In contrast to this maximal-span approach, Open-IE systems (Etzioni et al., 2008; Fader et al., 2011) put emphasis on extracting readable standalone propositions, typically producing shorter arguments (see examples in Section 2.1). Several recent works have exploited this property, using 1 http://knowitall.github.io/openie Open IE-4 is based on Cl"
P16-2077,P15-2050,1,0.85204,"Missing"
P16-2077,P15-1034,0,0.0642189,"cover briefly some of the main differences in a few prominent Open-IE systems. ReVerb (Fader et al., 2011) uses part-of-speechbased regular expressions to decide whether a word should be included within an argument span. For example, they move certain light verb compliments and prepositions from the argument to the predicate slot (e.g., “gave a talk at”). OLLIE (Mausam et al., 2012) learns lexical-syntactic patterns and splits extractions across certain prepositions. For example, given “I flew from Paris to Berlin”, OLLIE yields (I; flew; from Paris) and (I; flew; to Berlin). More recently, (Angeli et al., 2015) used natural logic to remove non-integral parts of arguments (e.g., removing the underlined non-restrictive prepositional phrase in “Heinz Fischer of Austria”). 2.2 3 Argument Reduction In this section, we propose annotation criteria and process for obtaining minimal argument spans. Given an original, non-reduced argument, we aim to reduce it to a set of (one or more) smaller arguments, which jointly specify the same answer to the argument’s role question. Formally, given a non-reduced argument a = {w1 , ..., wn }, along with its role question Q(a) with respect to predicate p in sentence s, w"
P17-2056,L16-1699,0,0.106669,"Missing"
P17-2056,W13-2322,0,0.00583018,"e-Kohler (2016), these are the classes containing the majority of the verb types. 4 https://spacy.io Note that in our case this ranges between 0 (perfect performance) and 6 (worst performance). 6 UW used the IBM CPLEX Optimizer 5 355 library7 ). All hyperparameters were tuned on the development set. Dataset All-factual UW feat.† AMR Rule-based Supervised Semantic representation approach In addition to the rule-based and supervised approaches, we experimented with a semantic abstraction of the sentence. For that end, we extracted features inspired by the UW system on the popular AMR formalism (Banarescu et al., 2013) using a SoA parser (Pust et al., 2015). Our hope was that this would improve performance by focusing on the more semantically-significant portions of the predicate-argument structure. In particular, we extracted the following features from the predicted AMR structures: immediate parents, grandparents and siblings of the target node, lemma and POS tag of the target and preceding token in the sentence, and a Boolean feature based on the AMR polarity role (indicating semantic negation). MEANTIME MAE r MAE .80 .81 .66 .75 .59 0 .66 .66 .62 .71 .78 .51 .64 .72 .42 0 .71 .58 .63 .66 .31 .56 .44 .35"
P17-2056,W13-0501,0,0.0192302,"ves (dishonest), future tense (will, won’t), and more. Looking at the numerous and varied possibilities language offers to express all the different shades of modality, it is clear that factuality does not assume any fixed set of discrete values either. Instead, the underlying linguistic system forms a continuous spectrum ranging from factual to counterfactual (Saur´ı and Pustejovsky, 2009). While linguistic theory assigns a spectrum of factuality values, recent years have seen many practical efforts to capture the notion of factuality in a consistent annotation (Saur´ı and Pustejovsky, 2009; Nissim et al., 2013; Lee et al., 2015; OGorman et al., 2016; Minard et al., 2016; Ghia et al., 2016). Each of these make certain deci1 https://github.com/gabrielStanovsky/ unified-factuality 353 Corpus FactBank MEANTIME† UW #Tokens/Sentences 77231 / 3839 9743 / 631 106371 / 4234 Factuality Values Original Our mapping Factual (CT+/-) +3.0 / -3.0 Probable (PR+/-) +2.0 / -2.0 Possible (PS+/-) +1.0 / -1.0 Unknown (Uu/CTu ) 0.0 Fact / Counterfact +3.0 / -3.0 Possibility (uncertain) +1.5 / -1.5 Possibility (future) +0.5 / -0.5 [-3.0, 3.0] Type Annotators Perspective Discrete Experts Author’s and discourse-internal sou"
P17-2056,P16-1077,1,0.890518,"uality value. advcl advmod acomp nsubj nsubj ccomp nsubj dobj Don was dishonest when he said he paid taxes mod prop of mod subj comp subj dobj Don was dishonest when he said he paid taxes Figure 2: Dependency tree (top, obtained with spaCy) versus PropS representation (bottom, obtained via the online demo). Note that PropS posits dishonest as the head of said, while the dependency tree obstructs this relation. 5 Evaluation Extending TruthTeller’s lexicon We extended the TruthTeller lexicon of single-word predicates by integrating a large resource of modality markers. Following the approach of Eckle-Kohler (2016), we first induced the modality status of English adjectives and nouns from the subcategorization frames of their German counterparts listed in a large valency lexicon (using the “IMSLex German Lexicon” (Fitschen, 2004) and Google Translate for obtaining the translations2 ). We focused on four modality classes (the classes whfactual and wh/if-factual indicating factuality, and the two classes future-orientation and non-factual, indicating uncertainty)3 and semi-automatically mapped them to the signatures used in TruthTeller. We performed the same kind of mapping for the modality classes of Eng"
P17-2056,W16-5706,0,0.00694973,"n’t), and more. Looking at the numerous and varied possibilities language offers to express all the different shades of modality, it is clear that factuality does not assume any fixed set of discrete values either. Instead, the underlying linguistic system forms a continuous spectrum ranging from factual to counterfactual (Saur´ı and Pustejovsky, 2009). While linguistic theory assigns a spectrum of factuality values, recent years have seen many practical efforts to capture the notion of factuality in a consistent annotation (Saur´ı and Pustejovsky, 2009; Nissim et al., 2013; Lee et al., 2015; OGorman et al., 2016; Minard et al., 2016; Ghia et al., 2016). Each of these make certain deci1 https://github.com/gabrielStanovsky/ unified-factuality 353 Corpus FactBank MEANTIME† UW #Tokens/Sentences 77231 / 3839 9743 / 631 106371 / 4234 Factuality Values Original Our mapping Factual (CT+/-) +3.0 / -3.0 Probable (PR+/-) +2.0 / -2.0 Possible (PS+/-) +1.0 / -1.0 Unknown (Uu/CTu ) 0.0 Fact / Counterfact +3.0 / -3.0 Possibility (uncertain) +1.5 / -1.5 Possibility (future) +0.5 / -0.5 [-3.0, 3.0] Type Annotators Perspective Discrete Experts Author’s and discourse-internal sources Discrete Experts Author’s Continuou"
P17-2056,D15-1136,0,0.0405415,"ining the majority of the verb types. 4 https://spacy.io Note that in our case this ranges between 0 (perfect performance) and 6 (worst performance). 6 UW used the IBM CPLEX Optimizer 5 355 library7 ). All hyperparameters were tuned on the development set. Dataset All-factual UW feat.† AMR Rule-based Supervised Semantic representation approach In addition to the rule-based and supervised approaches, we experimented with a semantic abstraction of the sentence. For that end, we extracted features inspired by the UW system on the popular AMR formalism (Banarescu et al., 2013) using a SoA parser (Pust et al., 2015). Our hope was that this would improve performance by focusing on the more semantically-significant portions of the predicate-argument structure. In particular, we extracted the following features from the predicted AMR structures: immediate parents, grandparents and siblings of the target node, lemma and POS tag of the target and preceding token in the sentence, and a Boolean feature based on the AMR polarity role (indicating semantic negation). MEANTIME MAE r MAE .80 .81 .66 .75 .59 0 .66 .66 .62 .71 .78 .51 .64 .72 .42 0 .71 .58 .63 .66 .31 .56 .44 .35 .34 r 0 .33 .30 .23 .47 is due to its"
P17-2056,J12-2002,0,0.0450076,"Missing"
P17-2056,S12-1020,0,0.315353,"d into rule-based systems which examine deep linguistic features, and machine learning algorithms which generally extract more shallow features. The De Facto factuality profiler (Saur´ı and Pustejovsky, 2012) and TruthTeller algorithms (Lotan et al., 2013) take the rule-based approach and assign a discrete annotation of factuality (following the values assigned by FactBank) using a deterministic rule-based topdown approach on dependency trees, changing the factuality assessment when encountering factuality affecting predicates or modality and negation cues (following implicative signatures by Karttunen (2012)). In addition to a factuality assessment, TruthTeller assigns three values per predicate in the sentence: (1) implicative signature from a hand-coded lexicon indicating how this predicate changes the factuality of its embedded clause, in positive and negative contexts, (2) clause truth, marking the factuality assessment of the entire clause, and (3) negation and uncertainty, indicating whether this predicate is affected by negation or modality. Both of these algorithms rely on a hand-written lexicon of predicates, indicating how they modify the factuality status of their embedded predicates ("
P17-2056,D15-1189,0,0.276512,"se, while hypothetical or negated ones should be left out. Similarly, for argumentation analysis and question answering, factuality can play a major role in backing a specific claim or supporting evidence for an answer to a question at hand. Recent research efforts have approached the factuality task from two complementing directions: automatic prediction and large scale annotation. Previous attempts for automatic factuality prediction either took a rule-based, deep syntactic approach (Lotan et al., 2013; Saur´ı and Pustejovsky, 2012) or a machine learning approach over more shallow features (Lee et al., 2015). In terms of annotation, each effort was largely carried out independently of the others, picking up different factuality flavors and different annotation scales. In correlation, the proposed algorithms have targeted a single annotated resource which they aim to recover. Subsequently, this separation between annotated corpora has prevented a comparison across datasets. Further, the models are nonportable, inhibiting advancements in one dataset to carry over to any of the other annotations. Our contribution in this work is twofold. First, we suggest that the task can benefit from a unified rep"
P17-2056,N13-1091,1,0.814565,"e, in knowledge base population, only propositions marked as factual should be admitted into the knowledge base, while hypothetical or negated ones should be left out. Similarly, for argumentation analysis and question answering, factuality can play a major role in backing a specific claim or supporting evidence for an answer to a question at hand. Recent research efforts have approached the factuality task from two complementing directions: automatic prediction and large scale annotation. Previous attempts for automatic factuality prediction either took a rule-based, deep syntactic approach (Lotan et al., 2013; Saur´ı and Pustejovsky, 2012) or a machine learning approach over more shallow features (Lee et al., 2015). In terms of annotation, each effort was largely carried out independently of the others, picking up different factuality flavors and different annotation scales. In correlation, the proposed algorithms have targeted a single annotated resource which they aim to recover. Subsequently, this separation between annotated corpora has prevented a comparison across datasets. Further, the models are nonportable, inhibiting advancements in one dataset to carry over to any of the other annotatio"
P17-2056,W09-3714,0,0.0913577,"Missing"
P19-1164,P06-1084,0,0.0264317,"erms of alphabet, word order, or grammar), while still allowing for highly accurate automatic morphological analysis. These languages belong to four different families: (1) Romance languages: Spanish, French, and Italian, all of which have gendered noun-determiner agreement and spaCy morphological analysis support (Honnibal and Montani, 2017). (2) Slavic languages (Cyrillic alphabet): Russian and Ukrainian, for which we use the morphological analyzer developed by Korobov (2015). (3) Semitic languages: Hebrew and Arabic, each with a unique alphabet. For Hebrew, we use the analyzer developed by Adler and Elhadad (2006), while gender inflection in Arabic can be easily identified via the ta marbuta character, which uniquely indicates feminine inflection. (4) Germanic languages: German, for which we Results Our main findings are presented in Tables 2 and 3. For each tested MT system and target language we compute three metrics with respect to their ability to convey the correct gender in the target language. Ultimately, our analyses indicate that all tested MT systems are indeed gender biased. First, the overall system Accuracy is calculated by the percentage of instances in which the translation preserved the"
P19-1164,D17-1042,0,0.0262651,"Missing"
P19-1164,N18-1118,0,0.0150505,"h or “soldat” (soldier) in French, which do not have female inflections. See Table 5 for detailed examples. 4 Discussion Related work This work is most related to several recent efforts which evaluate MT through the use of challenge sets. Similarly to our use WinoMT, these works evaluate MT systems (either manually or automatically) on test sets which are specially created to exhibit certain linguistic phenomena, thus going beyond the traditional BLEU metric (Papineni et al., 2002). These include challenge sets for language-specific idiosyncrasies (Isabelle et al., 2017), discourse phenomena (Bawden et al., 2018), pronoun translation (M¨uller et al., 2018; Webster et al., 2018), or coreference and multiword expressions (Burchardt et al., 2017). Limitations and future work While our work presents the first large-scale evaluation of gender bias in MT, it still suffers from certain limitations which could be addressed in follow up work. First, like some of the challenge sets discussed above, WinoMT is composed of synthetic English sourceside examples. On the one hand, this allows for a controlled experiment environment, while, on the other hand, this might introduce some artificial biases in our data and"
P19-1164,D17-1263,0,0.0335542,"ders, for example “sastre” (tailor) in Spanish or “soldat” (soldier) in French, which do not have female inflections. See Table 5 for detailed examples. 4 Discussion Related work This work is most related to several recent efforts which evaluate MT through the use of challenge sets. Similarly to our use WinoMT, these works evaluate MT systems (either manually or automatically) on test sets which are specially created to exhibit certain linguistic phenomena, thus going beyond the traditional BLEU metric (Papineni et al., 2002). These include challenge sets for language-specific idiosyncrasies (Isabelle et al., 2017), discourse phenomena (Bawden et al., 2018), pronoun translation (M¨uller et al., 2018; Webster et al., 2018), or coreference and multiword expressions (Burchardt et al., 2017). Limitations and future work While our work presents the first large-scale evaluation of gender bias in MT, it still suffers from certain limitations which could be addressed in follow up work. First, like some of the challenge sets discussed above, WinoMT is composed of synthetic English sourceside examples. On the one hand, this allows for a controlled experiment environment, while, on the other hand, this might intro"
P19-1164,W18-6307,0,0.0594926,"Missing"
P19-1164,N13-1073,1,0.665237,"3,888 instances, and is equally balanced between male and female genders, as well as between stereotypical and nonstereotypical gender-role assignments (e.g., a female doctor versus a female nurse). Additional dataset statistics are presented in Table 1. We use WinoMT to estimate the gender-bias of an MT model, M , in target-language L by performing following steps (exemplified in Figure 1): (1) Translate all of the sentences in WinoMT into L using M , thus forming a bilingual corpus of English and the target language L. (2) Align between the source and target translations, using fast align (Dyer et al., 2013), trained on the automatic translations from from step (1). Winogender Experimental Setup MT systems We test six widely used MT models, representing the state of the art in both commercial and academic research: (1) Google Translate,1 (2) Microsoft Translator,2 (3) Amazon Translate,3 (4) SYSTRAN,4 (5) the model of Ott et al. (2018), which recently achieved the best performance on English-to-French translation on the WMT’14 test set, and (6) the model of Edunov et al. (2018), the WMT’18 winner on English-toGerman translation. We query the online API for the first four commercial MT systems, whi"
P19-1164,D18-1045,0,0.016978,"ingual corpus of English and the target language L. (2) Align between the source and target translations, using fast align (Dyer et al., 2013), trained on the automatic translations from from step (1). Winogender Experimental Setup MT systems We test six widely used MT models, representing the state of the art in both commercial and academic research: (1) Google Translate,1 (2) Microsoft Translator,2 (3) Amazon Translate,3 (4) SYSTRAN,4 (5) the model of Ott et al. (2018), which recently achieved the best performance on English-to-French translation on the WMT’14 test set, and (6) the model of Edunov et al. (2018), the WMT’18 winner on English-toGerman translation. We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pretrained models provided by the Fairseq toolkit.5 1680 1 https://translate.google.com https://www.bing.com/translator 3 https://aws.amazon.com/translate 4 http://www.systransoft.com 5 https://github.com/pytorch/fairseq 2 Google Translate Amazon Translate∗ Microsoft Translator SYSTRAN Acc ∆G ∆S Acc ∆G ∆S Acc ∆G ∆S Acc ∆G ∆S ES FR IT 53.1 63.6 39.6 23.4 6.4 32.9 21.3 26.7 21.5 47.3 44.7 39.8 36.8 36.4 39.8 23.2 29.7 17.0 59.4"
P19-1164,D18-1002,0,0.0218372,"e languages, all annotated with ground truth entity gender. Second, similar to any medium size test set, it is clear that WinoMT serves only as a proxy estimation for the phenomenon of gender bias, and would probably be easy to overfit. A larger annotated corpus can perhaps provide a better signal for training. Finally, even though in Section 3.3 we show a very rudimentary debiasing scheme which relies on oracle coreference system, it is clear that this is not applicable in a real-world scenario. While recent research has shown that getting rid of such biases may prove to be very challenging (Elazar and Goldberg, 2018; Gonen and Goldberg, 2019), we hope that this work will serve as a first step for developing more gender-balanced MT models. 5 Conclusions We presented the first large-scale multilingual quantitative evidence for gender bias in MT, showing that on eight diverse target languages, all four tested popular commercial systems and two recent state-of-the-art academic MT models are significantly prone to translate based on gender stereotypes rather than more meaningful context. Our data and code are publicly available at https://github.com/ gabrielStanovsky/mt_gender. Acknowledgments We would like t"
P19-1164,W19-3821,0,0.233151,"Missing"
P19-1164,W19-3621,0,0.0489826,"with ground truth entity gender. Second, similar to any medium size test set, it is clear that WinoMT serves only as a proxy estimation for the phenomenon of gender bias, and would probably be easy to overfit. A larger annotated corpus can perhaps provide a better signal for training. Finally, even though in Section 3.3 we show a very rudimentary debiasing scheme which relies on oracle coreference system, it is clear that this is not applicable in a real-world scenario. While recent research has shown that getting rid of such biases may prove to be very challenging (Elazar and Goldberg, 2018; Gonen and Goldberg, 2019), we hope that this work will serve as a first step for developing more gender-balanced MT models. 5 Conclusions We presented the first large-scale multilingual quantitative evidence for gender bias in MT, showing that on eight diverse target languages, all four tested popular commercial systems and two recent state-of-the-art academic MT models are significantly prone to translate based on gender stereotypes rather than more meaningful context. Our data and code are publicly available at https://github.com/ gabrielStanovsky/mt_gender. Acknowledgments We would like to thank Mark Yatskar, Iz Be"
P19-1164,W18-6301,0,0.0342982,"Missing"
P19-1164,P02-1040,0,0.113146,"emale inflections for professions which were stereotypically associated with one of the genders, for example “sastre” (tailor) in Spanish or “soldat” (soldier) in French, which do not have female inflections. See Table 5 for detailed examples. 4 Discussion Related work This work is most related to several recent efforts which evaluate MT through the use of challenge sets. Similarly to our use WinoMT, these works evaluate MT systems (either manually or automatically) on test sets which are specially created to exhibit certain linguistic phenomena, thus going beyond the traditional BLEU metric (Papineni et al., 2002). These include challenge sets for language-specific idiosyncrasies (Isabelle et al., 2017), discourse phenomena (Bawden et al., 2018), pronoun translation (M¨uller et al., 2018; Webster et al., 2018), or coreference and multiword expressions (Burchardt et al., 2017). Limitations and future work While our work presents the first large-scale evaluation of gender bias in MT, it still suffers from certain limitations which could be addressed in follow up work. First, like some of the challenge sets discussed above, WinoMT is composed of synthetic English sourceside examples. On the one hand, this"
P19-1164,W17-1609,0,0.15079,"Missing"
P19-1164,N18-2002,0,0.232322,"Missing"
P19-1164,Q18-1042,0,0.0532662,"ctions. See Table 5 for detailed examples. 4 Discussion Related work This work is most related to several recent efforts which evaluate MT through the use of challenge sets. Similarly to our use WinoMT, these works evaluate MT systems (either manually or automatically) on test sets which are specially created to exhibit certain linguistic phenomena, thus going beyond the traditional BLEU metric (Papineni et al., 2002). These include challenge sets for language-specific idiosyncrasies (Isabelle et al., 2017), discourse phenomena (Bawden et al., 2018), pronoun translation (M¨uller et al., 2018; Webster et al., 2018), or coreference and multiword expressions (Burchardt et al., 2017). Limitations and future work While our work presents the first large-scale evaluation of gender bias in MT, it still suffers from certain limitations which could be addressed in follow up work. First, like some of the challenge sets discussed above, WinoMT is composed of synthetic English sourceside examples. On the one hand, this allows for a controlled experiment environment, while, on the other hand, this might introduce some artificial biases in our data and evaluation. Ideally, WinoMT could be augmented with natural “in t"
P19-1164,D17-1323,0,0.163501,"Missing"
P19-1164,N18-2003,0,0.250997,"Missing"
S17-1019,W99-0201,0,0.0232231,"Missing"
S17-1019,N03-1003,0,0.0539901,"tude from existing resources, it complements them with nonconsecutive predicates (e.g. take [a]0 from [a]1 ) and paraphrases which are highly context specific. The resource and the source code are available at http://github.com/vered1986/ Chirps.2 As of the end of May 2017, it contains 456,221 predicate pairs in 1,239,463 different contexts. Our resource is ever-growing and is expected to contain around 2 million predicate paraphrases within a year. Until it reaches a large enough size, we will release a daily update, and at a later stage, we plan to release a periodic update. 2 et al., 2002; Barzilay and Lee, 2003). The assumption is that multiple news articles describing the same event use various lexical choices, providing a good source for paraphrases. Heuristics are applied to recognize that two news articles discuss the same event, such as lexical overlap and same publish date (Shinyama and Sekine, 2006). Given such a pair of articles, it is likely that predicates connecting the same arguments will be paraphrases, as in the following example: 1. GOP lawmakers introduce new health care plan 2. GOP lawmakers unveil new health care plan Zhang and Weld (2013) and Zhang et al. (2015) introduced methods"
S17-1019,N06-1039,0,0.165942,"p-ranked predicate paraphrases. Introduction their accuracy is limited. Specifically, the first approach may extract antonyms, that also have similar argument distribution (e.g. [a]0 raise to [a]1 / [a]0 fall to [a]1 ) while the second may conflate multiple senses of the foreign phrase. A third approach was proposed to harvest paraphrases from multiple mentions of the same event in news articles.1 This approach assumes that various redundant reports make different lexical choices to describe the same event. Although there has been some work following this approach (e.g. Shinyama et al., 2002; Shinyama and Sekine, 2006; Roth and Frank, 2012; Zhang and Weld, 2013), it was less exhaustively investigated and did not result in creating paraphrase resources. In this paper we present a novel unsupervised method for ever-growing extraction of lexicallydivergent predicate paraphrase pairs from news tweets. We apply our methodology to create a resource of predicate paraphrases, exemplified in Table 1. Analysis of the resource obtained after ten Recognizing that various textual descriptions across multiple texts refer to the same event or action can benefit NLP applications such as recognizing textual entailment (Dag"
S17-1019,P01-1008,0,0.222672,"extension to the distributional hypothesis (Harris, 1954). DIRT (Lin and Pantel, 2001) is a resource of 10 million paraphrases, in which the similarity between predicate pairs is estimated by the geometric mean of the similarities of their argument slots. Berant (2012) constructed an entailment graph of distributionally similar predicates by enforcing transitivity constraints and applying global optimization, releasing 52 million directional entailment rules (e.g. [a]0 shoot [a]1 → [a]0 kill [a]1 ). A second notable source for extracting paraphrases is multiple translations of the same text (Barzilay and McKeown, 2001). The Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015) is a huge collection of paraphrases extracted from bilingual parallel corpora. Paraphrases are scored heuristically, and the database is available for download in six increasingly large sizes according to scores (the smallest size being the most accurate). In addition to lexical paraphrases, PPDB also consists of 140 million syntactic paraphrases, some of which include predicates with non-terminals as arguments. 2.2 3 We present a methodology to automatically collect binary verbal predicate paraphrases from Twit"
S17-1019,P16-1119,1,0.83296,"rce release consists of two files: We extract propositions from news tweets using PropS (Stanovsky et al., 2016), which simplifies dependency trees by conveniently marking a wide range of predicates (e.g, verbal, adjectival, nonlexical) and positioning them as direct heads of their corresponding arguments. Specifically, we run PropS over dependency trees predicted by spaCy6 and extract predicate types (as in Table 1) composed of verbal predicates, datives, prepositions, and auxiliaries. Finally, we employ a pre-trained argument reduction model to remove non-restrictive argument modifications (Stanovsky and Dagan, 2016). This is essential for our subsequent alignment step, as it is likely that short and concise phrases will tend to match more frequently in comparison to longer, more specific arguments. Figure 1 exemplifies some of the phenomena handled by this process, along with the automatically predicted output. 3.3 1. Instances: the specific contexts in which the predicates are paraphrases (as in Table 2). In practice, to comply with Twitter policy, we release predicate paraphrase pair types along with their arguments and tweet IDs, and provide a script for downloading the full texts. 2. Types: predicate"
S17-1019,P10-1124,1,0.765007,"er “when did the US Supreme Court approve samesex marriage?” given the text “In June 2015, the Supreme Court ruled for same-sex marriage”, approve and ruled for should be identified as describing the same action. To that end, much effort has been devoted to identifying predicate paraphrases, some of which resulted in releasing resources of predicate entailment or paraphrases. Two main approaches were proposed for that matter; the first leverages the similarity in argument distribution across a large corpus between two predicates (e.g. [a]0 buy [a]1 / [a]0 acquire [a]1 ) (Lin and Pantel, 2001; Berant et al., 2010). The second approach exploits bilingual parallel corpora, extracting as paraphrases pairs of texts that were translated identically to foreign languages (Ganitkevitch et al., 2013). While these methods have produced exhaustive resources which are broadly used by applications, 1 This corresponds to instances of event coreference (Bagga and Baldwin, 1999). 155 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 155–160, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics weeks of acquisition shows that the set of pa"
S17-1019,P13-2080,1,0.798824,"006; Roth and Frank, 2012; Zhang and Weld, 2013), it was less exhaustively investigated and did not result in creating paraphrase resources. In this paper we present a novel unsupervised method for ever-growing extraction of lexicallydivergent predicate paraphrase pairs from news tweets. We apply our methodology to create a resource of predicate paraphrases, exemplified in Table 1. Analysis of the resource obtained after ten Recognizing that various textual descriptions across multiple texts refer to the same event or action can benefit NLP applications such as recognizing textual entailment (Dagan et al., 2013) and question answering. For example, to answer “when did the US Supreme Court approve samesex marriage?” given the text “In June 2015, the Supreme Court ruled for same-sex marriage”, approve and ruled for should be identified as describing the same action. To that end, much effort has been devoted to identifying predicate paraphrases, some of which resulted in releasing resources of predicate entailment or paraphrases. Two main approaches were proposed for that matter; the first leverages the similarity in argument distribution across a large corpus between two predicates (e.g. [a]0 buy [a]1"
S17-1019,P07-1058,1,0.737065,"Missing"
S17-1019,Q14-1034,0,0.0797018,"ments will be paraphrases, as in the following example: 1. GOP lawmakers introduce new health care plan 2. GOP lawmakers unveil new health care plan Zhang and Weld (2013) and Zhang et al. (2015) introduced methods that leverage parallel news streams to cluster predicates by meaning, using temporal constraints. Since this approach acquires paraphrases from descriptions of the same event, it is potentially more accurate than methods that acquire paraphrases from the entire corpus or translation phrase table. However, there is currently no paraphrase resource acquired in this approach.3 Finally, Xu et al. (2014) developed a supervised model to collect sentential paraphrases from Twitter. They used Twitter’s trending topic service, and considered two tweets from the same topic as paraphrases if they shared a single anchor word. Background 2.1 Existing Paraphrase Resources A prominent approach to acquire predicate paraphrases is to compare the distribution of their arguments across a corpus, as an extension to the distributional hypothesis (Harris, 1954). DIRT (Lin and Pantel, 2001) is a resource of 10 million paraphrases, in which the similarity between predicate pairs is estimated by the geometric me"
S17-1019,P12-2031,1,0.861583,"erant (Berant, 2012), a resource of predicate entailments, and PPDB (Pavlick et al., 2015), a resource of paraphrases, both described in Section 2. We expect our resource to be more accurate than resources which are based on the distributional approach (Berant, 2012; Lin and Pantel, 2001). In addition, in comparison to PPDB, we specialize on binary verbal predicates, and apply an additional phase of proposition extraction, handling various phenomena such as non-consecutive particles and minimality of arguments. Berant (2012) evaluated their resource against a dataset of predicate entailments (Zeichner et al., 2012), using a recall-precision curve to show the performance obtained with a range of thresholds on the resource score. This kind of evaluation is less suitable for our resource; first, predicate entailment is directional, causing paraphrases with the wrong entailment direction to be labeled negative in the dataset. Second, since our resource is still relatively small, it is unlikely to have sufficient coverage of the dataset at that point. We therefore 6 Conclusion We presented a new unsupervised method to acquire fairly accurate predicate paraphrases from news tweets discussing the same event. W"
S17-1019,P15-2070,0,0.0588367,"Missing"
S17-1019,Q15-1009,0,0.0777347,"2 et al., 2002; Barzilay and Lee, 2003). The assumption is that multiple news articles describing the same event use various lexical choices, providing a good source for paraphrases. Heuristics are applied to recognize that two news articles discuss the same event, such as lexical overlap and same publish date (Shinyama and Sekine, 2006). Given such a pair of articles, it is likely that predicates connecting the same arguments will be paraphrases, as in the following example: 1. GOP lawmakers introduce new health care plan 2. GOP lawmakers unveil new health care plan Zhang and Weld (2013) and Zhang et al. (2015) introduced methods that leverage parallel news streams to cluster predicates by meaning, using temporal constraints. Since this approach acquires paraphrases from descriptions of the same event, it is potentially more accurate than methods that acquire paraphrases from the entire corpus or translation phrase table. However, there is currently no paraphrase resource acquired in this approach.3 Finally, Xu et al. (2014) developed a supervised model to collect sentential paraphrases from Twitter. They used Twitter’s trending topic service, and considered two tweets from the same topic as paraphr"
S17-1019,D13-1183,0,0.507084,"r accuracy is limited. Specifically, the first approach may extract antonyms, that also have similar argument distribution (e.g. [a]0 raise to [a]1 / [a]0 fall to [a]1 ) while the second may conflate multiple senses of the foreign phrase. A third approach was proposed to harvest paraphrases from multiple mentions of the same event in news articles.1 This approach assumes that various redundant reports make different lexical choices to describe the same event. Although there has been some work following this approach (e.g. Shinyama et al., 2002; Shinyama and Sekine, 2006; Roth and Frank, 2012; Zhang and Weld, 2013), it was less exhaustively investigated and did not result in creating paraphrase resources. In this paper we present a novel unsupervised method for ever-growing extraction of lexicallydivergent predicate paraphrase pairs from news tweets. We apply our methodology to create a resource of predicate paraphrases, exemplified in Table 1. Analysis of the resource obtained after ten Recognizing that various textual descriptions across multiple texts refer to the same event or action can benefit NLP applications such as recognizing textual entailment (Dagan et al., 2013) and question answering. For"
S17-1019,S12-1030,0,\N,Missing
S19-2153,P16-1202,0,0.0154978,"sible values of the third side y? (A) 0 < y < x (B) 0 < y < 2x (C) 4 < y < 2x Overview Over the past four years, there has been a surge of interest in math question answering. Research groups from around the globe have published papers on the topic, including MIT (Kushman et al., 2014), University of Washington (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015), National Institute of Informatics, Japan (Matsuzaki et al., 2014), University of Illinois (Roy and Roth, 2015), Microsoft Research (Shi et al., 2015; Upadhyay and Chang, 2016), Baidu (Zhou et al., 2015), Arizona State University (Mitra and Baral, 2016), KU Leuven (Dries et al., 2017), Carnegie Mellon University (Sachan et al., 2017), Tencent (Wang et al., 2017), and DeepMind (Ling et al., 2017). Math question answering has several attractive properties that have rekindled this interest: Figure 1: Example math questions from different genres. correct answer (11) has a subtle relationship to the other quantities mentioned in the text (3 and 15). There is no obvious shortcut (like word association metrics on a bag-of-words representation of the question) to guessing 11. 3. Math questions exhibit interesting semantic phenomena like cross-senten"
S19-2153,D15-1202,0,0.585751,"y: The lengths of two sides of a triangle are (x − 2) and (x + 2), where x > 2. Which of the following ranges includes all and only the possible values of the third side y? (A) 0 < y < x (B) 0 < y < 2x (C) 4 < y < 2x Overview Over the past four years, there has been a surge of interest in math question answering. Research groups from around the globe have published papers on the topic, including MIT (Kushman et al., 2014), University of Washington (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015), National Institute of Informatics, Japan (Matsuzaki et al., 2014), University of Illinois (Roy and Roth, 2015), Microsoft Research (Shi et al., 2015; Upadhyay and Chang, 2016), Baidu (Zhou et al., 2015), Arizona State University (Mitra and Baral, 2016), KU Leuven (Dries et al., 2017), Carnegie Mellon University (Sachan et al., 2017), Tencent (Wang et al., 2017), and DeepMind (Ling et al., 2017). Math question answering has several attractive properties that have rekindled this interest: Figure 1: Example math questions from different genres. correct answer (11) has a subtle relationship to the other quantities mentioned in the text (3 and 15). There is no obvious shortcut (like word association metric"
S19-2153,D17-1081,0,0.323397,"ew Over the past four years, there has been a surge of interest in math question answering. Research groups from around the globe have published papers on the topic, including MIT (Kushman et al., 2014), University of Washington (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015), National Institute of Informatics, Japan (Matsuzaki et al., 2014), University of Illinois (Roy and Roth, 2015), Microsoft Research (Shi et al., 2015; Upadhyay and Chang, 2016), Baidu (Zhou et al., 2015), Arizona State University (Mitra and Baral, 2016), KU Leuven (Dries et al., 2017), Carnegie Mellon University (Sachan et al., 2017), Tencent (Wang et al., 2017), and DeepMind (Ling et al., 2017). Math question answering has several attractive properties that have rekindled this interest: Figure 1: Example math questions from different genres. correct answer (11) has a subtle relationship to the other quantities mentioned in the text (3 and 15). There is no obvious shortcut (like word association metrics on a bag-of-words representation of the question) to guessing 11. 3. Math questions exhibit interesting semantic phenomena like cross-sentence coreference and indirect coreference (e.g. in the geometry question from Figure"
S19-2153,D15-1171,1,0.884394,"Missing"
S19-2153,D17-1083,1,0.849523,"Missing"
S19-2153,D15-1135,0,0.137105,"are (x − 2) and (x + 2), where x > 2. Which of the following ranges includes all and only the possible values of the third side y? (A) 0 < y < x (B) 0 < y < 2x (C) 4 < y < 2x Overview Over the past four years, there has been a surge of interest in math question answering. Research groups from around the globe have published papers on the topic, including MIT (Kushman et al., 2014), University of Washington (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015), National Institute of Informatics, Japan (Matsuzaki et al., 2014), University of Illinois (Roy and Roth, 2015), Microsoft Research (Shi et al., 2015; Upadhyay and Chang, 2016), Baidu (Zhou et al., 2015), Arizona State University (Mitra and Baral, 2016), KU Leuven (Dries et al., 2017), Carnegie Mellon University (Sachan et al., 2017), Tencent (Wang et al., 2017), and DeepMind (Ling et al., 2017). Math question answering has several attractive properties that have rekindled this interest: Figure 1: Example math questions from different genres. correct answer (11) has a subtle relationship to the other quantities mentioned in the text (3 and 15). There is no obvious shortcut (like word association metrics on a bag-of-words representation of"
S19-2153,D14-1058,1,0.915493,"ketball tournament involving 8 teams, each team played 4 games with each of the other teams. How many games were played at this tournament? Geometry: The lengths of two sides of a triangle are (x − 2) and (x + 2), where x > 2. Which of the following ranges includes all and only the possible values of the third side y? (A) 0 < y < x (B) 0 < y < 2x (C) 4 < y < 2x Overview Over the past four years, there has been a surge of interest in math question answering. Research groups from around the globe have published papers on the topic, including MIT (Kushman et al., 2014), University of Washington (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015), National Institute of Informatics, Japan (Matsuzaki et al., 2014), University of Illinois (Roy and Roth, 2015), Microsoft Research (Shi et al., 2015; Upadhyay and Chang, 2016), Baidu (Zhou et al., 2015), Arizona State University (Mitra and Baral, 2016), KU Leuven (Dries et al., 2017), Carnegie Mellon University (Sachan et al., 2017), Tencent (Wang et al., 2017), and DeepMind (Ling et al., 2017). Math question answering has several attractive properties that have rekindled this interest: Figure 1: Example math questions from different genres. correct answer (1"
S19-2153,D17-1088,0,0.0895999,"ere has been a surge of interest in math question answering. Research groups from around the globe have published papers on the topic, including MIT (Kushman et al., 2014), University of Washington (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015), National Institute of Informatics, Japan (Matsuzaki et al., 2014), University of Illinois (Roy and Roth, 2015), Microsoft Research (Shi et al., 2015; Upadhyay and Chang, 2016), Baidu (Zhou et al., 2015), Arizona State University (Mitra and Baral, 2016), KU Leuven (Dries et al., 2017), Carnegie Mellon University (Sachan et al., 2017), Tencent (Wang et al., 2017), and DeepMind (Ling et al., 2017). Math question answering has several attractive properties that have rekindled this interest: Figure 1: Example math questions from different genres. correct answer (11) has a subtle relationship to the other quantities mentioned in the text (3 and 15). There is no obvious shortcut (like word association metrics on a bag-of-words representation of the question) to guessing 11. 3. Math questions exhibit interesting semantic phenomena like cross-sentence coreference and indirect coreference (e.g. in the geometry question from Figure 1, “the third side” refers t"
S19-2153,D15-1096,0,0.0142085,"following ranges includes all and only the possible values of the third side y? (A) 0 < y < x (B) 0 < y < 2x (C) 4 < y < 2x Overview Over the past four years, there has been a surge of interest in math question answering. Research groups from around the globe have published papers on the topic, including MIT (Kushman et al., 2014), University of Washington (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015), National Institute of Informatics, Japan (Matsuzaki et al., 2014), University of Illinois (Roy and Roth, 2015), Microsoft Research (Shi et al., 2015; Upadhyay and Chang, 2016), Baidu (Zhou et al., 2015), Arizona State University (Mitra and Baral, 2016), KU Leuven (Dries et al., 2017), Carnegie Mellon University (Sachan et al., 2017), Tencent (Wang et al., 2017), and DeepMind (Ling et al., 2017). Math question answering has several attractive properties that have rekindled this interest: Figure 1: Example math questions from different genres. correct answer (11) has a subtle relationship to the other quantities mentioned in the text (3 and 15). There is no obvious shortcut (like word association metrics on a bag-of-words representation of the question) to guessing 11. 3. Math questions exhibi"
S19-2153,N16-1136,1,0.815245,"Missing"
S19-2153,P14-1026,0,0.219149,"ing baseline. 1 Open-vocabulary algebra: At a basketball tournament involving 8 teams, each team played 4 games with each of the other teams. How many games were played at this tournament? Geometry: The lengths of two sides of a triangle are (x − 2) and (x + 2), where x > 2. Which of the following ranges includes all and only the possible values of the third side y? (A) 0 < y < x (B) 0 < y < 2x (C) 4 < y < 2x Overview Over the past four years, there has been a surge of interest in math question answering. Research groups from around the globe have published papers on the topic, including MIT (Kushman et al., 2014), University of Washington (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015), National Institute of Informatics, Japan (Matsuzaki et al., 2014), University of Illinois (Roy and Roth, 2015), Microsoft Research (Shi et al., 2015; Upadhyay and Chang, 2016), Baidu (Zhou et al., 2015), Arizona State University (Mitra and Baral, 2016), KU Leuven (Dries et al., 2017), Carnegie Mellon University (Sachan et al., 2017), Tencent (Wang et al., 2017), and DeepMind (Ling et al., 2017). Math question answering has several attractive properties that have rekindled this interest: Figure 1: Example math q"
S19-2153,P17-1015,0,0.0625938,"n math question answering. Research groups from around the globe have published papers on the topic, including MIT (Kushman et al., 2014), University of Washington (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015), National Institute of Informatics, Japan (Matsuzaki et al., 2014), University of Illinois (Roy and Roth, 2015), Microsoft Research (Shi et al., 2015; Upadhyay and Chang, 2016), Baidu (Zhou et al., 2015), Arizona State University (Mitra and Baral, 2016), KU Leuven (Dries et al., 2017), Carnegie Mellon University (Sachan et al., 2017), Tencent (Wang et al., 2017), and DeepMind (Ling et al., 2017). Math question answering has several attractive properties that have rekindled this interest: Figure 1: Example math questions from different genres. correct answer (11) has a subtle relationship to the other quantities mentioned in the text (3 and 15). There is no obvious shortcut (like word association metrics on a bag-of-words representation of the question) to guessing 11. 3. Math questions exhibit interesting semantic phenomena like cross-sentence coreference and indirect coreference (e.g. in the geometry question from Figure 1, “the third side” refers to a triangle introduced in a previ"
S19-2153,S19-2227,0,0.0273382,"Missing"
W14-2413,P14-2120,1,0.819635,"he truth status of propositions and author commitment. In the current version infinitive constructions are treated as nested propositions, similar to their representation in syntactic parse trees. Providing a consistent, useful and transparent representation for infinitive constructions is a challenging direction for future research. Other extensions of the proposed representation are also possible. One appealing direction is going beyond the sentence level and representing discourse level relations, including implied propositions and predicate - argument relationships expressed by discourse (Stern and Dagan, 2014; Ruppenhofer et al., 2010; Gerber and Chai, 2012). Such an extension may prove useful as an intermediary representation for parsers of semantic formalisms targeted at the discourse level (such as DRT). Compared to proposition-based semantic representations, we do not attempt to assign framespecific thematic roles, nor do we attempt to disambiguate or interpret word meanings. We restrict ourselves to representing predicates by their (lemmatized) surface forms, and labeling arguments based on a “syntactic” role inventory, similar to the label-sets available in dependency representations. This d"
W14-2413,Q13-1005,0,0.0299228,"oduced by appositive constructions. Other benefits over dependency-trees are explicit marking of logical relations between propositions, explicit marking of multiword predicate such as light-verbs, and a consistent representation for syntacticallydifferent but semantically-similar structures. The representation is meant to serve as a useful input layer for semanticoriented applications, as well as to provide a better starting point for further levels of semantic analysis such as semantic-rolelabeling and semantic-parsing. 1 Introduction Parsers for semantic formalisms (such as Neodavidsonian (Artzi and Zettlemoyer, 2013) and DRT (Kamp, 1988)) take unstructured natural language text as input, and output a complete semantic representation, aiming to capture the meaning conveyed by the text. We suggest that this task may be effectively separated into a sequential combination of two different tasks. The first of these tasks is syntactic abstraction over phenomena such as expression of tense, negation, modality, and passive versus active voice, which are all either expressed or implied from syntactic structure. The second task is semantic interpretation 66 Proceedings of the ACL 2014 Workshop on Semantic Parsing,"
W14-2413,P98-1013,0,0.127419,"ck of space, include propositional modifiers (e.g., relative clause modifiers), propositional arguments (such as ”John asserted that he will go home”), conditionals, and the canonicalization of passive and active voice. 4 Relation to Other Representations Our proposed representation is intended to serve as a bridging layer between purely syntactic representations such as dependency trees, and semantic oriented applications. In particular, we explicitly represent many semantic relations expressed in a sentence that are not captured by contemporary proposition-directed semantic representations (Baker et al., 1998; Kingsbury and Palmer, 2003; Meyers et al., 2004; Carreras and M`arquez, 2005). Compared to dependency-based representations such as Stanford-dependency trees (De Marneffe 1 A case of conjunctions requiring special treatment is introduced by reciprocals, in which the entities roles are exchangeable. For example: “John and Mary bet against each other on future rates” (adaption of wsj 0117). 2 Care needs to be taken to distinguish from cases such as “going to Italy” in which “going to” is not followed by a verbal predicate. 68 5 and Manning, 2008b), we abstract away over many syntactic details"
W14-2413,W08-2222,0,0.0793849,"Missing"
W14-2413,W05-0620,0,0.168162,"Missing"
W14-2413,W08-1301,0,0.404567,"Missing"
W14-2413,J12-4003,0,0.0156683,"ment. In the current version infinitive constructions are treated as nested propositions, similar to their representation in syntactic parse trees. Providing a consistent, useful and transparent representation for infinitive constructions is a challenging direction for future research. Other extensions of the proposed representation are also possible. One appealing direction is going beyond the sentence level and representing discourse level relations, including implied propositions and predicate - argument relationships expressed by discourse (Stern and Dagan, 2014; Ruppenhofer et al., 2010; Gerber and Chai, 2012). Such an extension may prove useful as an intermediary representation for parsers of semantic formalisms targeted at the discourse level (such as DRT). Compared to proposition-based semantic representations, we do not attempt to assign framespecific thematic roles, nor do we attempt to disambiguate or interpret word meanings. We restrict ourselves to representing predicates by their (lemmatized) surface forms, and labeling arguments based on a “syntactic” role inventory, similar to the label-sets available in dependency representations. This design choice makes our representation much easier"
W14-2413,P11-1060,0,0.0468222,"Missing"
W14-2413,J93-2004,0,0.0457685,"aptures both explicit and implicit propositions, while staying relatively close to the syntactic level. We believe that this kind of representation will serve not only as an advantageous input for semantically-centered applications, such as question answering, summarization and information extraction, but also serve as a rich representation layer that can be used as input for systems aiming to provide a finer level of semantic analysis, such as semantic-parsers. We are currently at the beginning of our investigation. In the near future we plan to semiautomatically annotate the Penn Tree Bank (Marcus et al., 1993) with these structures, as well as to provide software for deriving (some of) the implicit and explicit annotations from automatically produced parse-trees. We believe such resources will be of immediate use to semantic-oriented applications. In the longer term, we plan to investigate dedicated algorithms for automatically producing such representation from raw text. The architecture we describe can easily accommodate additional layers of abstraction, by encoding these layers as features of propositions, predicates or arguments. Such layers can include the marking of named entities, the truth"
W14-2413,W04-2705,0,0.322908,"presentation of copular sentences) 2. Pierre Vinken will join the board as a nonexecutive director Nov. 29. Adjectives, as in the sentence “you emphasized the high prevalence of mental illness” (wsj 0105). Here an adjective is used to describe a definite subject and introduces another proposition, namely the high prevalence of mental illness. Nominalizations, for instance in the sentence “Googles acquisition of Waze occurred yesterday”, introduce the implicit proposition that “Google acquired Waze”. Such propositions were studied and annotated in the NOMLEX (Macleod et al., 1998) and NOMBANK (Meyers et al., 2004) resources. It remains an open issue how to represent or distinguish cases in which nominalization introduce an underspecified proposition. For example, consider “dancing” in “I read a book about dancing”. Possessives, such as “John’s book” introduce the proposition that John has a book. Similarly, examples such as “John’s Failure” combine a possessive construction with nominalization and introduce the proposition that John has failed. Conjunctions - for example in “They operate ships and banks.” (wsj 0083), introduce several propositions in one sentence: 1. They operate ships 2. They operate"
W14-2413,S10-1008,0,0.0268506,"ositions and author commitment. In the current version infinitive constructions are treated as nested propositions, similar to their representation in syntactic parse trees. Providing a consistent, useful and transparent representation for infinitive constructions is a challenging direction for future research. Other extensions of the proposed representation are also possible. One appealing direction is going beyond the sentence level and representing discourse level relations, including implied propositions and predicate - argument relationships expressed by discourse (Stern and Dagan, 2014; Ruppenhofer et al., 2010; Gerber and Chai, 2012). Such an extension may prove useful as an intermediary representation for parsers of semantic formalisms targeted at the discourse level (such as DRT). Compared to proposition-based semantic representations, we do not attempt to assign framespecific thematic roles, nor do we attempt to disambiguate or interpret word meanings. We restrict ourselves to representing predicates by their (lemmatized) surface forms, and labeling arguments based on a “syntactic” role inventory, similar to the label-sets available in dependency representations. This design choice makes our rep"
W14-2413,C98-1013,0,\N,Missing
W14-4504,P12-3014,1,0.824939,"rom those documents as fine-grained propositions (2) they represent the semantic relations between those propositions. These semantic relations can be leveraged to create high-quality summarizations. For example, the paraphrase (mutual entailment) relation prevents redundancy. Links of a temporal or causal nature can also dictate the order in which each proposition is presented. A recent method of summarizing text with entailment graphs (Gupta et al., 2014) demonstrates the appeal and feasibility of this application. Faceted Search Faceted search allows a user to interactively navigate a PKG. Adler et al. (2012) demonstrate this concept on a limited proposition graph. When searching for “headache” in their demo, the user can drill-down to find possible causes or remedies, and even focus on subcategories of those; for example, finding the foods which relieve headaches. As opposed to the structured query application, retrieval is not fully automated, but rather interactive. It thus allows users to explore and discover new information they might not have considered a-priori. 4 Discussion In this position paper we outlined a framework for information discovery that leverages and extends Open IE, while ad"
W14-4504,W13-2322,0,0.0170123,"drawbacks. The proposed framework enriches Open IE by representing natural language in a traversable graph, composed of propositions and their semantic interrelations – A Propositional Knowledge Graph (PKG). The resulting structure provides a representation in two levels: locally, at sentence level, by representing the syntactic proposition structure embedded in a single sentence, and globally, at inter-proposition level, where relations are drawn between propositions from discourse, or from various sources. At the sentence level, PKG can be compared to Abstract Meaning Representation (AMR) (Banarescu et al., 2013), which maps a sentence onto a hierarchical structure of propositions (predicate-argument relations) - a “meaning representation”. AMR uses Propbank (Kingsbury and Palmer, 2003) for predicates’ meaning representation, where possible, and ungrounded natural language, where no respective 22 Propbank lexicon entry exists. While AMR relies on a deep semantic interpretation, our sentence level representation is more conservative (and thus, hopefully, more feasible) and can be obtained by syntactic interpretation. At inter-proposition level, PKG can be compared with traditional Knowledge Graphs (suc"
W14-4504,W08-1301,0,0.0604435,"Missing"
W14-4504,S14-1010,0,0.0197595,"rmation from multiple documents on the same topic. PKGs can be a natural platform leveraged by summarization because: (1) they would contain the information from those documents as fine-grained propositions (2) they represent the semantic relations between those propositions. These semantic relations can be leveraged to create high-quality summarizations. For example, the paraphrase (mutual entailment) relation prevents redundancy. Links of a temporal or causal nature can also dictate the order in which each proposition is presented. A recent method of summarizing text with entailment graphs (Gupta et al., 2014) demonstrates the appeal and feasibility of this application. Faceted Search Faceted search allows a user to interactively navigate a PKG. Adler et al. (2012) demonstrate this concept on a limited proposition graph. When searching for “headache” in their demo, the user can drill-down to find possible causes or remedies, and even focus on subcategories of those; for example, finding the foods which relieve headaches. As opposed to the structured query application, retrieval is not fully automated, but rather interactive. It thus allows users to explore and discover new information they might no"
W14-4504,W14-1610,1,0.832308,"texts. However, in a PKG, they are marked as paraphrases (mutually entailing), and both entail an additional proposition from a third source: “Curiosity is a lab”. If one were to query all the 21 propositions that entail “Curiosity is a lab” – e.g. in response to the query “What is Curiosity?” – all three propositions would be retrieved, even though their surface forms may have “functions as” instead of “is” or “laboratory” instead of “lab”. We have recently taken some first steps in this direction, investigating algorithms for constructing entailment edges over sets of related propositions (Levy et al., 2014). Even between simple propositions, recognizing entailment is challenging. We are currently working on new methods that will leverage structured and unstructured data to recognize entailment for Open IE propositions. There are additional relations, besides entailment, that should desirably be represented in PKGs as well. Two such examples are temporal relations (depicted in Figure 1) and causality. Investigating and adapting methods for recognizing and utilizing these relations is intended for future work. 3 Applications An appealing application of knowledge graphs is question answering (QA)."
W14-4504,D12-1048,0,0.0391103,"Open IE systems retrieve only propositions in which both predicates and arguments are instantiated in succession in the surface form. For such propositions, these systems produce independent tuples (typically a (subject, verb, object) triplet) consisting of a predicate and a list of its arguments, all expressed in natural language, in the same way they originally appeared in the sentence. This methodology lacks the ability to represent cases in which propositions are inherently embedded, such as conditionals and propositional arguments (e.g. “Senator Kennedy asked congress to pass the bill”). Mausam et al. (2012) introduced a context analysis layer, extending this 20 representation with an additional field per tuple, which intends to represent the factuality of the extraction, accounting specifically for cases of conditionals and attribution. For instance, the assertion “If he wins five key states, Romney will be elected President” will be represented as ((Romney; will be elected; President) ClausalModifier if; he wins five key states). While these methods capture some of the propositions conveyed by text, they fail to retrieve other propositions expressed by more sophisticated syntactic constructs. C"
W14-4504,W14-2413,1,0.823008,"te (namely “acquired”) does not appear in the surface form. Implicit propositions might be introduced in many other linguistic constructs, such as: appositions (“The company, Random House, doesn’t report its earnings.” implies that Random House is a company), adjectives (“Tall John walked home” implies that John is tall), and possessives (“John’s book is on the table” implies that John has a book). We intend to syntactically identify these implicit propositions, and make them explicit in our representation. For further analysis of syntax-driven proposition representation, see our recent work (Stanovsky et al., 2014). We believe that this extension of Open IE representation is feasibly extractable from syntactic parse trees, and are currently working on automatic conversion from Stanford dependencies (de Marneffe and Manning, 2008) to interconnected propositions as described. 2.2 Consolidating Information across Propositions While Open IE is indeed much more scalable than supervised approaches, it does not consolidate natural language expressions, which leads to either insufficient or redundant information when accessing a repository of Open IE extractions. As an illustrating example, querying the Univers"
W17-0902,J14-2004,0,0.0128507,"etween predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-ali"
W17-0902,araki-etal-2014-detecting,0,0.0144858,"tracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between pre"
W17-0902,D08-1031,1,0.548822,"reras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (O"
W17-0902,P10-1124,1,0.91175,"ic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+"
W17-0902,J12-1003,1,0.927138,"rbal performance (accuracy of 0.73 vs. 0.25). Finally, argument identification was hard mainly because of inconsistencies in verbal versus nominal predicate-argument structure in dependency trees.4 The low performance in predicate coreference compared to entity coreference can be explained by the higher variability of predicate terms. The argument co-reference task becomes easy given gold predicate-argument structures, as most arguments are singletons (i.e. composed of a single element). Finally, while the performance of the predicate entailment component reflects the current stateof-the-art (Berant et al., 2012; Han and Sun, 2016), the performance on entity entailment is much worse than the current state-of-the-art in this task as measured on common lexical inference test sets. We conjecture that this stems from the nature of the entities in our dataset, consisting of both named entities and common nouns, many of which are multi-word expressions, whereas most work in entity entailment is focused on single word common nouns. Furthermore, it is worth noting that our annotations are of naturally occurring texts, and represent lexical entailment in real world coreference chains, as opposed to synthetica"
W17-0902,W05-0620,0,0.162639,"Missing"
W17-0902,W99-0201,0,0.0935993,"on Extraction Open IE (Open Information Extraction) (Etzioni et al., 2008) is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference"
W17-0902,W09-4303,0,0.0115663,") is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining en"
W17-0902,W13-2322,0,0.107609,"le texts, and in specifying how such representation can be created based on entity and event coreference and lexical entailment. An accompanying contribution is our annotated dataset, which can be used to analyze the involved phenomena and their interactions, and as a development and test set for automated generation of OKR structures. We further note that while this paper focuses on creating an open representation, by consolidating Open IE propositions, future work may investigate the consolidation of other semantic sentence representations, for example AMR (Abstract Meaning Representation) (Banarescu et al., 2013), while exploiting similar principles to those proposed here. ated (e.g., Fader et al. (2011); Del Corro and Gemulla (2013)). Open IE’s extractions were also shown to be effective as intermediate sentencelevel representation in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al.,"
W17-0902,P15-1136,0,0.0225471,"Missing"
W17-0902,E12-1004,0,0.0217269,"tractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art"
W17-0902,cybulska-vossen-2014-using,0,0.0126286,"achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can h"
W17-0902,P99-1071,0,0.536493,"on. We can expect the use of OKR structures in MDS to shift the research efforts in this task to other components, e.g. generation, and eventually contribute to improving state of the art on this task. Similarly, an algorithm creating the ECKG structure can benefit from building upon a consolidated structure such as OKR, rather than working directly on free text. systematic solution, and the burden of integrating information across multiple texts is delegated to downstream applications, leading to partial solutions which are geared to specific applications. Multi-Document Summarization (MDS) (Barzilay et al., 1999) is a task whose goal is to produce a concise summary from a set of related text documents, such that it includes the most important information in a non-redundant manner. While extractive summarization selects salient sentences from the document collection, abstractive summarization generates new sentences, and is considered a more promising yet more difficult task. A recent approach for abstractive summarization generates a graphical representation of the input documents by: (1) parsing each sentence/document into a meaning representation structure; and (2) merging the structures into a sing"
W17-0902,P13-2080,1,0.82203,"ment annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can help in consolidating textual information. In discourse representation theory (DRT), a proposition applies to all co-referring entities (Kamp et al., 2011). In recognizing textual entailment (Dagan et al., 2013), lexical substitution of co-referring elements is useful (Stern and Dagan, 2012). For example, in Figure 1, sentence (1) together with the coreference relation between plane and jet entail that “Turkey forces down Syrian jet.” 2.3 Proposed Representation Our Open Knowledge Representation (OKR) aims to capture the consolidated information expressed jointly in a set of texts. In some analogy to structured knowledge bases, we would like the elements of our representation to correspond to entities in the described scenario and to statements (propositions) that relate them. Still, in the spirit of"
W17-0902,bejan-harabagiu-2008-linguistic,0,0.0326074,"1; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align"
W17-0902,duclaye-etal-2002-using,0,0.0512409,"implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 20"
W17-0902,D12-1045,0,0.0269834,"2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing t"
W17-0902,D11-1142,0,0.0614886,"ference and lexical entailment. An accompanying contribution is our annotated dataset, which can be used to analyze the involved phenomena and their interactions, and as a development and test set for automated generation of OKR structures. We further note that while this paper focuses on creating an open representation, by consolidating Open IE propositions, future work may investigate the consolidation of other semantic sentence representations, for example AMR (Abstract Meaning Representation) (Banarescu et al., 2013), while exploiting similar principles to those proposed here. ated (e.g., Fader et al. (2011); Del Corro and Gemulla (2013)). Open IE’s extractions were also shown to be effective as intermediate sentencelevel representation in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments a"
W17-0902,D14-1168,0,0.0154703,"selects salient sentences from the document collection, abstractive summarization generates new sentences, and is considered a more promising yet more difficult task. A recent approach for abstractive summarization generates a graphical representation of the input documents by: (1) parsing each sentence/document into a meaning representation structure; and (2) merging the structures into a single structure that represents the entire summary, e.g. by identifying coreferring items. In that sense, this approach is similar to OKR. However, current methods applying this approach are still limited. Gerani et al. (2014) parse each document to discourse tree representation (Joty et al., 2013), aggregating them based on entity coreference. Yet, they work with a limited set of (discourse) relations, and rely on coreference only between entities, which was detected manually. Similarly, Liu et al. (2015) parse each input sentence into an individual AMR graph (Banarescu et al., 2013), and merge those into a single graph through identical concepts. This work extends the AMR formalism of canonicalized representation per entity or event to multiple sentences. However, they only focus on certain types of named entitie"
W17-0902,C16-1273,0,0.0306302,"Missing"
W17-0902,liu-etal-2014-supervised,0,0.0231139,"Missing"
W17-0902,D15-1076,0,0.0606389,"on in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use"
W17-0902,N15-1114,0,0.026058,"ng each sentence/document into a meaning representation structure; and (2) merging the structures into a single structure that represents the entire summary, e.g. by identifying coreferring items. In that sense, this approach is similar to OKR. However, current methods applying this approach are still limited. Gerani et al. (2014) parse each document to discourse tree representation (Joty et al., 2013), aggregating them based on entity coreference. Yet, they work with a limited set of (discourse) relations, and rely on coreference only between entities, which was detected manually. Similarly, Liu et al. (2015) parse each input sentence into an individual AMR graph (Banarescu et al., 2013), and merge those into a single graph through identical concepts. This work extends the AMR formalism of canonicalized representation per entity or event to multiple sentences. However, they only focus on certain types of named entities, and collapse two entities based on their names rather than on coreference. 7 Conclusions In this paper we advocate the development of representation frameworks for the consolidated information expressed in a set of texts. The key ingredients of our approach are the extraction of pr"
W17-0902,C92-2082,0,0.446376,"wo candidate sentences for the summary differ only in terms that hold a lexical inference relation (e.g. “the plane landed in Ankara” and “the plane landed in Turkey”). Recognizing the inference direction, e.g. that Ankara is more specific than Turkey, can help in selecting the desired granularity level of the description. There has been consistent attention to recognizing lexical inference between terms. Some methods aim to recognize a general lexical inference relation (e.g. (Kotlerman et al., 2010; Turney and Mohammad, 2015)), others focus on a specific semantic relation, mostly hypernymy (Hearst, 1992; Snow et al., 2005; Santus et al., 2014; Shwartz et al., 2016), while recent methods classify a pair of 3.1 Entities To represent entities, we first annotate the text by entity mentions and coreference. Following the typical notion for these tasks, an entity mention corresponds to a word or multi-word expression that refers to an object or concept in the described scenario (in the broader sense of “entity”). Ac14 Original texts: (1) Turkey forces down Syrian plane. (2) Syrian jet grounded by Turkey carried munitions from Moscow. (3) Intercepted Syrian plane carried ammunition from Russia. Ent"
W17-0902,W97-1311,0,0.0722333,"rior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (Open Information Extraction) (Etzioni et al., 2008) is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to"
W17-0902,N15-1050,1,0.790932,"event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored"
W17-0902,P13-1048,0,0.0297432,"zation generates new sentences, and is considered a more promising yet more difficult task. A recent approach for abstractive summarization generates a graphical representation of the input documents by: (1) parsing each sentence/document into a meaning representation structure; and (2) merging the structures into a single structure that represents the entire summary, e.g. by identifying coreferring items. In that sense, this approach is similar to OKR. However, current methods applying this approach are still limited. Gerani et al. (2014) parse each document to discourse tree representation (Joty et al., 2013), aggregating them based on entity coreference. Yet, they work with a limited set of (discourse) relations, and rely on coreference only between entities, which was detected manually. Similarly, Liu et al. (2015) parse each input sentence into an individual AMR graph (Banarescu et al., 2013), and merge those into a single graph through identical concepts. This work extends the AMR formalism of canonicalized representation per entity or event to multiple sentences. However, they only focus on certain types of named entities, and collapse two entities based on their names rather than on corefere"
W17-0902,W16-5304,1,0.885624,"Missing"
W17-0902,K15-1018,1,0.833569,"m for improvement. These bottle-necks are bound to hinder the performance of a pipeline end-to-end system. Future research into OKR should first target these areas; either as a pipeline or in a joint learning framework. 3 For Argument Mention detection we attach the components (entities and propositions) as arguments of predicates when the components are syntactically dependent on them. Argument Coreference is simply predicted by marking coreference if and only if the arguments are both mentions of the same entity co-reference chain. For Entity Entailment purposes we used knowledge resources (Shwartz et al., 2015) and a pretrained model for HypeNET (Shwartz et al., 2016) to obtain a score for all pairs of Wikipedia common words (unigrams, bigrams, and trigrams). A threshold for the binary entailment decision was then calibrated on a held out development set. Finally, for Predicate Entailment we used the entailment rules extracted by Berant et al. (2012). 5.1 Results and Error Analysis Using the same metrics used for measuring interannotator agreement, we evaluated how well the presented models were able to recover the different facets of the OKR gold annotations. The performance on the different subtas"
W17-0902,P15-1146,0,0.0257952,"Missing"
W17-0902,P16-1226,1,0.873666,"Missing"
W17-0902,K15-1002,1,0.834714,"ile the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (Open Information Extraction) (Etzioni et al.,"
W17-0902,D16-1038,1,0.86973,"Missing"
W17-0902,P11-1080,0,0.0191905,"ddle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (Open Information Extraction) (Etzioni et al., 2008) is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankar"
W17-0902,S12-1030,0,0.0281997,"ms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee e"
W17-0902,J01-4004,0,0.174444,"s like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolutio"
W17-0902,P16-1119,1,0.813405,"cly available tools and simple baselines which approximate the current state-of-the-art in each of these subtasks. For brevity sake, in the rest of the section we briefly describe each of these baselines. For a more detailed technical description see the OKR repository (https://github.com/vered1986/ OKR). For Entity Mention extraction we use the spaCy NER model2 in addition to annotating all of the nouns and adjectives as entities. For Proposition Mention detection we use Open IE propositions extracted from PropS (Stanovsky et al., 2016), where non-restrictive arguments were reduced following Stanovsky and Dagan (2016). For ProposiFor entity, predicate, and argument co-reference we calculated coreference resolution metrics: the link-based MUC (Vilain et al., 1995), the mentionbased B 3 (Bagga and Baldwin, 1998), the entitybased CEAF, and the widely adopted CoNLL F1 measure which is an average of the three. For entity and proposition entailment we compute the F1 score over the annotated directed edges in each entailment graph, as is common for entailment agreement metrics (Berant et al., 2010). We macro-averaged these scores to obtain an overall agreement on the 5 events annotated by both annotators. The agr"
W17-0902,E14-4008,0,0.0143178,"mmary differ only in terms that hold a lexical inference relation (e.g. “the plane landed in Ankara” and “the plane landed in Turkey”). Recognizing the inference direction, e.g. that Ankara is more specific than Turkey, can help in selecting the desired granularity level of the description. There has been consistent attention to recognizing lexical inference between terms. Some methods aim to recognize a general lexical inference relation (e.g. (Kotlerman et al., 2010; Turney and Mohammad, 2015)), others focus on a specific semantic relation, mostly hypernymy (Hearst, 1992; Snow et al., 2005; Santus et al., 2014; Shwartz et al., 2016), while recent methods classify a pair of 3.1 Entities To represent entities, we first annotate the text by entity mentions and coreference. Following the typical notion for these tasks, an entity mention corresponds to a word or multi-word expression that refers to an object or concept in the described scenario (in the broader sense of “entity”). Ac14 Original texts: (1) Turkey forces down Syrian plane. (2) Syrian jet grounded by Turkey carried munitions from Moscow. (3) Intercepted Syrian plane carried ammunition from Russia. Entities: E1 = {Turkey}, E2 = {Syrian}, E3"
W17-0902,P15-2050,1,0.81398,"test set for automated generation of OKR structures. We further note that while this paper focuses on creating an open representation, by consolidating Open IE propositions, future work may investigate the consolidation of other semantic sentence representations, for example AMR (Abstract Meaning Representation) (Banarescu et al., 2013), while exploiting similar principles to those proposed here. ated (e.g., Fader et al. (2011); Del Corro and Gemulla (2013)). Open IE’s extractions were also shown to be effective as intermediate sentencelevel representation in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are rep"
W17-0902,D10-1106,0,0.0195202,"ub.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity c"
W17-0902,P12-3013,1,0.834165,"ss document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can help in consolidating textual information. In discourse representation theory (DRT), a proposition applies to all co-referring entities (Kamp et al., 2011). In recognizing textual entailment (Dagan et al., 2013), lexical substitution of co-referring elements is useful (Stern and Dagan, 2012). For example, in Figure 1, sentence (1) together with the coreference relation between plane and jet entail that “Turkey forces down Syrian jet.” 2.3 Proposed Representation Our Open Knowledge Representation (OKR) aims to capture the consolidated information expressed jointly in a set of texts. In some analogy to structured knowledge bases, we would like the elements of our representation to correspond to entities in the described scenario and to statements (propositions) that relate them. Still, in the spirit of Open IE, we would like the representation to be open, while relying only on the"
W17-0902,W04-3206,1,0.412965,"ailable at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions,"
W17-0902,C16-1183,1,0.835662,"on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can help in consolidating textual"
W17-0902,M95-1005,0,0.294299,"ction we briefly describe each of these baselines. For a more detailed technical description see the OKR repository (https://github.com/vered1986/ OKR). For Entity Mention extraction we use the spaCy NER model2 in addition to annotating all of the nouns and adjectives as entities. For Proposition Mention detection we use Open IE propositions extracted from PropS (Stanovsky et al., 2016), where non-restrictive arguments were reduced following Stanovsky and Dagan (2016). For ProposiFor entity, predicate, and argument co-reference we calculated coreference resolution metrics: the link-based MUC (Vilain et al., 1995), the mentionbased B 3 (Bagga and Baldwin, 1998), the entitybased CEAF, and the widely adopted CoNLL F1 measure which is an average of the three. For entity and proposition entailment we compute the F1 score over the annotated directed edges in each entailment graph, as is common for entailment agreement metrics (Berant et al., 2010). We macro-averaged these scores to obtain an overall agreement on the 5 events annotated by both annotators. The agreement scores for the two annotators are shown in Table 1, and overall show high levels of agreement. A qualitative analysis of the more common disa"
W17-0902,C14-1212,0,0.0125443,"t Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task"
W17-0902,P13-2012,0,0.0402841,"Missing"
W17-0902,N15-1002,0,0.0522403,"Missing"
W17-0902,P02-1014,0,\N,Missing
W17-0902,P15-1034,0,\N,Missing
