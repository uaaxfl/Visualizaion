2021.emnlp-main.92,Label Verbalization and Entailment for Effective Zero and Few-Shot Relation Extraction,2021,-1,-1,5,0,6128,oscar sainz,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Relation extraction systems require large amounts of labeled examples which are costly to annotate. In this work we reformulate relation extraction as an entailment task, with simple, hand-made, verbalizations of relations produced in less than 15 min per relation. The system relies on a pretrained textual entailment engine which is run as-is (no training examples, zero-shot) or further fine-tuned on labeled examples (few-shot or fully trained). In our experiments on TACRED we attain 63{\%} F1 zero-shot, 69{\%} with 16 examples per relation (17{\%} points better than the best supervised system on the same conditions), and only 4 points short to the state-of-the-art (which uses 20 times more training data). We also show that the performance can be improved significantly with larger entailment models, up to 12 points in zero-shot, allowing to report the best results to date on TACRED when fully trained. The analysis shows that our few-shot systems are specially effective when discriminating between relations, and that the performance difference in low data regimes comes mainly from identifying no-relation cases."
2021.acl-long.506,Beyond Offline Mapping: Learning Cross-lingual Word Embeddings through Context Anchoring,2021,-1,-1,5,0,13428,aitor ormazabal,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Recent research on cross-lingual word embeddings has been dominated by unsupervised mapping approaches that align monolingual embeddings. Such methods critically rely on those embeddings having a similar structure, but it was recently shown that the separate training in different languages causes departures from this assumption. In this paper, we propose an alternative approach that does not have this limitation, while requiring a weak seed dictionary (e.g., a list of identical words) as the only form of supervision. Rather than aligning two fixed embedding spaces, our method works by fixing the target language embeddings, and learning a new set of embeddings for the source language that are aligned with them. To that end, we use an extension of skip-gram that leverages translated context words as anchor points, and incorporates self-learning and iterative restarts to reduce the dependency on the initial dictionary. Our approach outperforms conventional mapping methods on bilingual lexicon induction, and obtains competitive results in the downstream XNLI task."
2020.nlpcovid19-2.15,Automatic Evaluation vs. User Preference in Neural Textual {Q}uestion{A}nswering over {COVID}-19 Scientific Literature,2020,-1,-1,5,1,16264,arantxa otegi,Proceedings of the 1st Workshop on {NLP} for {COVID}-19 (Part 2) at {EMNLP} 2020,0,"We present a Question Answering (QA) system that won one of the tasks of the Kaggle CORD-19 Challenge, according to the qualitative evaluation of experts. The system is a combination of an Information Retrieval module and a reading comprehension module that finds the answers in the retrieved passages. In this paper we present a quantitative and qualitative analysis of the system. The quantitative evaluation using manually annotated datasets contradicted some of our design choices, e.g. the fact that using QuAC for fine-tuning provided better answers over just using SQuAD. We analyzed this mismatch with an additional A/B test which showed that the system using QuAC was indeed preferred by users, confirming our intuition. Our analysis puts in question the suitability of automatic metrics and its correlation to user preferences. We also show that automatic metrics are highly dependent on the characteristics of the gold standard, such as the average length of the answers."
2020.lrec-1.55,Conversational Question Answering in Low Resource Scenarios: A Dataset and Case Study for {B}asque,2020,-1,-1,5,1,16264,arantxa otegi,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Conversational Question Answering (CQA) systems meet user information needs by having conversations with them, where answers to the questions are retrieved from text. There exist a variety of datasets for English, with tens of thousands of training examples, and pre-trained language models have allowed to obtain impressive results. The goal of our research is to test the performance of CQA systems under low-resource conditions which are common for most non-English languages: small amounts of native annotations and other limitations linked to low resource languages, like lack of crowdworkers or smaller wikipedias. We focus on the Basque language, and present the first non-English CQA dataset and results. Our experiments show that it is possible to obtain good results with low amounts of native data thanks to cross-lingual transfer, with quality comparable to those obtained for English. We also discovered that dialogue history models are not directly transferable to another language, calling for further research. The dataset is publicly available."
2020.lrec-1.588,Give your Text Representation Models some Love: the Case for {B}asque,2020,22,0,7,0,7239,rodrigo agerri,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Word embeddings and pre-trained language models allow to build rich representations of text and have enabled improvements across most NLP tasks. Unfortunately they are very expensive to train, and many small companies and research groups tend to use models that have been pre-trained and made available by third parties, rather than building their own. This is suboptimal as, for many languages, the models have been trained on smaller (or lower quality) corpora. In addition, monolingual pre-trained models for non-English languages are not always available. At best, models for those languages are included in multilingual versions, where each language shares the quota of substrings and parameters with the rest of the languages. This is particularly true for smaller languages such as Basque. In this paper we show that a number of monolingual models (FastText word embeddings, FLAIR and BERT language models) trained with larger Basque corpora produce much better results than publicly available versions in downstream NLP tasks, including topic classification, sentiment classification, PoS tagging and NER. This work sets a new state-of-the-art in those tasks for Basque. All benchmarks and models used in this work are publicly available."
2020.emnlp-main.326,Spot The Bot: A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems,2020,-1,-1,8,0,3133,jan deriu,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"The lack of time efficient and reliable evalu-ation methods is hampering the development of conversational dialogue systems (chat bots). Evaluations that require humans to converse with chat bots are time and cost intensive, put high cognitive demands on the human judges, and tend to yield low quality results. In this work, we introduce Spot The Bot, a cost-efficient and robust evaluation framework that replaces human-bot conversations with conversations between bots. Human judges then only annotate for each entity in a conversation whether they think it is human or not (assuming there are humans participants in these conversations). These annotations then allow us to rank chat bots regarding their ability to mimic conversational behaviour of humans. Since we expect that all bots are eventually recognized as such, we incorporate a metric that measures which chat bot is able to uphold human-like be-havior the longest, i.e.Survival Analysis. This metric has the ability to correlate a bot{'}s performance to certain of its characteristics (e.g.fluency or sensibleness), yielding interpretable results. The comparably low cost of our frame-work allows for frequent evaluations of chatbots during their evaluation cycle. We empirically validate our claims by applying Spot The Bot to three domains, evaluating several state-of-the-art chat bots, and drawing comparisonsto related work. The framework is released asa ready-to-use tool."
2020.emnlp-main.618,Translation Artifacts in Cross-lingual Transfer Learning,2020,41,2,3,1,10622,mikel artetxe,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively."
2020.coling-main.230,Improving Conversational Question Answering Systems after Deployment using Feedback-Weighted Learning,2020,-1,-1,5,0,16265,jon campos,Proceedings of the 28th International Conference on Computational Linguistics,0,"The interaction of conversational systems with users poses an exciting opportunity for improving them after deployment, but little evidence has been provided of its feasibility. In most applications, users are not able to provide the correct answer to the system, but they are able to provide binary (correct, incorrect) feedback. In this paper we propose feedback-weighted learning based on importance sampling to improve upon an initial supervised system using binary user feedback. We perform simulated experiments on document classification (for development) and Conversational Question Answering datasets like QuAC and DoQA, where binary user feedback is derived from gold annotations. The results show that our method is able to improve over the initial supervised system, getting close to a fully-supervised system that has access to the same labeled examples in in-domain experiments (QuAC), and even matching in out-of-domain experiments (DoQA). Our work opens the prospect to exploit interactions with real users and improve conversational systems after deployment."
2020.acl-srw.34,Unsupervised Multilingual Sentence Embeddings for Parallel Corpus Mining,2020,-1,-1,4,0,13979,ivana kvapilikova,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"Existing models of multilingual sentence embeddings require large parallel data resources which are not available for low-resource languages. We propose a novel unsupervised method to derive multilingual sentence embeddings relying only on monolingual data. We first produce a synthetic parallel corpus using unsupervised machine translation, and use it to fine-tune a pretrained cross-lingual masked language model (XLM) to derive the multilingual sentence representations. The quality of the representations is evaluated on two parallel corpus mining tasks with improvements of up to 22 F1 points over vanilla XLM. In addition, we observe that a single synthetic bilingual corpus is able to improve results for other language pairs."
2020.acl-main.84,A Methodology for Creating Question Answering Corpora Using Inverse Data Annotation,2020,31,0,8,0,3133,jan deriu,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data. For this, we introduce an intermediate representation that is based on the logical query plan in a database, called Operation Trees (OT). This representation allows us to invert the annotation process without loosing flexibility in the types of queries that we generate. Furthermore, it allows for fine-grained alignment of the tokens to the operations. Thus, we randomly generate OTs from a context free grammar and annotators just have to write the appropriate question and assign the tokens. We compare our corpus OTTA (Operation Trees and Token Assignment), a large semantic parsing corpus for evaluating natural language interfaces to databases, to Spider and LC-QuaD 2.0 and show that our methodology more than triples the annotation speed while maintaining the complexity of the queries. Finally, we train a state-of-the-art semantic parsing model on our data and show that our dataset is a challenging dataset and that the token alignment can be leveraged to significantly increase the performance."
2020.acl-main.652,{D}o{QA} - Accessing Domain-Specific {FAQ}s via Conversational {QA},2020,17,0,6,0,16265,jon campos,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"The goal of this work is to build conversational Question Answering (QA) interfaces for the large body of domain-specific information available in FAQ sites. We present DoQA, a dataset with 2,437 dialogues and 10,917 QA pairs. The dialogues are collected from three Stack Exchange sites using the Wizard of Oz method with crowdsourcing. Compared to previous work, DoQA comprises well-defined information needs, leading to more coherent and natural conversations with less factoid questions and is multi-domain. In addition, we introduce a more realistic information retrieval (IR) scenario where the system needs to find the answer in any of the FAQ documents. The results of an existing, strong, system show that, thanks to transfer learning from a Wikipedia QA dataset and fine tuning on a single FAQ domain, it is possible to build high quality conversational QA systems for FAQs without in-domain training data. The good results carry over into the more challenging IR scenario. In both cases, there is still ample room for improvement, as indicated by the higher human upperbound."
2020.acl-main.658,A Call for More Rigor in Unsupervised Cross-lingual Learning,2020,76,0,5,1,10622,mikel artetxe,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world{'}s languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models."
P19-1019,An Effective Approach to Unsupervised Machine Translation,2019,30,3,3,1,10622,mikel artetxe,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"While machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure. Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fine-tuned through on-the-fly back-translation. Together, we obtain large improvements over the previous state-of-the-art in unsupervised machine translation. For instance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points more than the previous best unsupervised system, and 0.5 points more than the (supervised) shared task winner back in 2014."
P19-1492,Analyzing the Limitations of Cross-lingual Word Embedding Mappings,2019,0,10,5,0,13428,aitor ormazabal,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Recent research in cross-lingual word embeddings has almost exclusively focused on offline methods, which independently train word embeddings in different languages and map them to a shared space through linear transformations. While several authors have questioned the underlying isomorphism assumption, which states that word embeddings in different languages have approximately the same structure, it is not clear whether this is an inherent limitation of mapping approaches or a more general issue when learning cross-lingual embeddings. So as to answer this question, we experiment with parallel corpora, which allows us to compare offline mapping to an extension of skip-gram that jointly learns both embedding spaces. We observe that, under these ideal conditions, joint learning yields to more isomorphic embeddings, is less sensitive to hubness, and obtains stronger results in bilingual lexicon induction. We thus conclude that current mapping methods do have strong limitations, calling for further research to jointly learn cross-lingual embeddings with a weaker cross-lingual signal."
P19-1494,Bilingual Lexicon Induction through Unsupervised Machine Translation,2019,0,5,3,1,10622,mikel artetxe,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"A recent research line has obtained strong results on bilingual lexicon induction by aligning independently trained word embeddings in two languages and using the resulting cross-lingual embeddings to induce word translation pairs through nearest neighbor or related retrieval methods. In this paper, we propose an alternative approach to this problem that builds on the recent work on unsupervised machine translation. This way, instead of directly inducing a bilingual lexicon from cross-lingual embeddings, we use them to build a phrase-table, combine it with a language model, and use the resulting machine translation system to generate a synthetic parallel corpus, from which we extract the bilingual lexicon using statistical word alignment techniques. As such, our method can work with any word embedding and cross-lingual mapping technique, and it does not require any additional resource besides the monolingual corpus used to train the embeddings. When evaluated on the exact same cross-lingual embeddings, our proposed method obtains an average improvement of 6 accuracy points over nearest neighbor and 4 points over CSLS retrieval, establishing a new state-of-the-art in the standard MUSE dataset."
P19-1574,Probing for Semantic Classes: Diagnosing the Meaning Content of Word Embeddings,2019,52,0,4,0,10941,yadollah yaghoobzadeh,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Word embeddings typically represent different meanings of a word in a single conflated vector. Empirical analysis of embeddings of ambiguous words is currently limited by the small size of manually annotated resources and by the fact that word senses are treated as unrelated individual concepts. We present a large dataset based on manual Wikipedia annotations and word senses, where word senses from different words are related by semantic classes. This is the basis for novel diagnostic tests for an embedding{'}s content: we probe word embeddings for semantic classes and analyze the embedding space by classifying embeddings into semantic classes. Our main findings are: (i) Information about a sense is generally represented well in a single-vector embedding {--} if the sense is frequent. (ii) A classifier can accurately predict whether a word is single-sense or multi-sense, based only on its embedding. (iii) Although rare senses are not well represented in single-vector embeddings, this does not have negative impact on an NLP application whose performance depends on frequent senses."
W18-2505,The risk of sub-optimal use of Open Source {NLP} Software: {UKB} is inadvertently state-of-the-art in knowledge-based {WSD},2018,5,0,1,1,8824,eneko agirre,Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS}),0,"UKB is an open source collection of programs for performing, among other tasks, Knowledge-Based Word Sense Disambiguation (WSD). Since it was released in 2009 it has been often used out-of-the-box in sub-optimal settings. We show that nine years later it is the state-of-the-art on knowledge-based WSD. This case shows the pitfalls of releasing open source NLP software without optimal default settings and precise instructions for reproducibility."
P18-1073,A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings,2018,12,4,3,1,10622,mikel artetxe,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at \url{https://github.com/artetxem/vecmap}."
K18-1017,Learning Text Representations for 500{K} Classification Tasks on Named Entity Disambiguation,2018,0,3,3,1,8823,ander barrena,Proceedings of the 22nd Conference on Computational Natural Language Learning,0,"Named Entity Disambiguation algorithms typically learn a single model for all target entities. In this paper we present a word expert model and train separate deep learning models for each target entity string, yielding 500K classification tasks. This gives us the opportunity to benchmark popular text representation alternatives on this massive dataset. In order to face scarce training data we propose a simple data-augmentation technique and transfer-learning. We show that bag-of-word-embeddings are better than LSTMs for tasks with scarce training data, while the situation is reversed when having larger amounts. Transferring a LSTM which is learned on all datasets is the most effective context representation option for the word experts in all frequency bands. The experiments show that our system trained on out-of-domain Wikipedia data surpass comparable NED systems which have been trained on in-domain training data."
K18-1028,Uncovering Divergent Linguistic Information in Word Embeddings with Lessons for Intrinsic and Extrinsic Evaluation,2018,25,2,4,1,10622,mikel artetxe,Proceedings of the 22nd Conference on Computational Natural Language Learning,0,"Following the recent success of word embeddings, it has been argued that there is no such thing as an ideal representation for words, as different models tend to capture divergent and often mutually incompatible aspects like semantics/syntax and similarity/relatedness. In this paper, we show that each embedding model captures more information than directly apparent. A linear transformation that adjusts the similarity order of the model without any external resource can tailor it to achieve better results in those aspects, providing a new perspective on how embeddings encode divergent linguistic information. In addition, we explore the relation between intrinsic and extrinsic evaluation, as the effect of our transformations in downstream tasks is higher for unsupervised systems than for supervised ones."
D18-1399,Unsupervised Statistical Machine Translation,2018,0,50,3,1,10622,mikel artetxe,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"While modern machine translation has relied on large parallel corpora, a recent line of work has managed to train Neural Machine Translation (NMT) systems from monolingual corpora only (Artetxe et al., 2018c; Lample et al., 2018). Despite the potential of this approach for low-resource settings, existing systems are far behind their supervised counterparts, limiting their practical interest. In this paper, we propose an alternative approach based on phrase-based Statistical Machine Translation (SMT) that significantly closes the gap with supervised systems. Our method profits from the modular architecture of SMT: we first induce a phrase table from monolingual corpora through cross-lingual embedding mappings, combine it with an n-gram language model, and fine-tune hyperparameters through an unsupervised MERT variant. In addition, iterative backtranslation improves results further, yielding, for instance, 14.08 and 26.22 BLEU points in WMT 2014 English-German and English-French, respectively, an improvement of more than 7-10 BLEU points over previous unsupervised systems, and closing the gap with supervised SMT (Moses trained on Europarl) down to 2-5 BLEU points. Our implementation is available at \url{https://github.com/artetxem/monoses}."
S17-2001,{S}em{E}val-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation,2017,51,219,3,0,9653,daniel cer,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in \textit{all language tracks}. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the \textit{STS Benchmark} is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017)."
P17-1042,Learning bilingual word embeddings with (almost) no bilingual data,2017,18,136,3,1,10622,mikel artetxe,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Most methods to learn bilingual word embeddings rely on large parallel corpora, which is difficult to obtain for most language pairs. This has motivated an active research line to relax this requirement, with methods that use document-aligned corpora or bilingual dictionaries of a few thousand words instead. In this work, we further reduce the need of bilingual resources using a very simple self-learning approach that can be combined with any dictionary-based mapping technique. Our method exploits the structural similarity of embedding spaces, and works with as little bilingual evidence as a 25 word dictionary or even an automatically generated list of numerals, obtaining results comparable to those of systems that use richer resources."
W16-6405,Adding syntactic structure to bilingual terminology for improved domain adaptation,2016,3,1,7,1,10622,mikel artetxe,Proceedings of the 2nd Deep Machine Translation Workshop,0,None
W16-2332,{SMT} and Hybrid systems of the {QTL}eap project in the {WMT}16 {IT}-task,2016,26,3,3,0,17856,rosa gaudio,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper presents the description of 12 systems submitted to the WMT16 IT-task, covering six different languages, namely Basque, Bulgarian, Dutch, Czech, Portuguese and Spanish. All these systems were developed under the scope of the QTLeap project, presenting a common strategy. For each language two different systems were submitted, namely a phrasebased MT system built using Moses, and a system exploiting deep language engineering approaches, that in all the languages but Bulgarian was implemented using TectoMT. For 4 of the 6 languages, the TectoMT-based system performs better than the Moses-based one."
S16-1081,"{S}em{E}val-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation",2016,29,110,1,1,8824,eneko agirre,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"Comunicacio presentada al 10th International Workshop on Semantic Evaluation (SemEval-2016), celebrat els dies 16 i 17 de juny de 2016 a San Diego, California."
S16-1082,{S}em{E}val-2016 Task 2: Interpretable Semantic Textual Similarity,2016,11,18,1,1,8824,eneko agirre,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"Comunicacio presentada al 10th International Workshop on Semantic Evaluation (SemEval-2016), celebrat els dies 16 i 17 de juny de 2016 a San Diego, California."
S16-1119,i{UBC} at {S}em{E}val-2016 Task 2: {RNN}s and {LSTM}s for interpretable {STS},2016,5,3,2,0,30364,inigo lopezgazpio,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
P16-1179,Alleviating Poor Context with Background Knowledge for Named Entity Disambiguation,2016,15,2,3,1,8823,ander barrena,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
L16-1064,Evaluating Translation Quality and {CLIR} Performance of Query Sessions,2016,8,0,2,0,5340,xabier saralegi,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper presents the evaluation of the translation quality and Cross-Lingual Information Retrieval (CLIR) performance when using session information as the context of queries. The hypothesis is that previous queries provide context that helps to solve ambiguous translations in the current query. We tested several strategies on the TREC 2010 Session track dataset, which includes query reformulations grouped by generalization, specification, and drifting types. We study the Basque to English direction, evaluating both the translation quality and CLIR performance, with positive results in both cases. The results show that the quality of translation improved, reducing error rate by 12{\%} (HTER) when using session information, which improved CLIR results 5{\%} (nDCG). We also provide an analysis of the improvements across the three kinds of sessions: generalization, specification, and drifting. Translation quality improved in all three types (generalization, specification, and drifting), and CLIR improved for generalization and specification sessions, preserving the performance in drifting sessions."
L16-1139,A comparison of Named-Entity Disambiguation and Word Sense Disambiguation,2016,8,7,4,0,9386,angel chang,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Named Entity Disambiguation (NED) is the task of linking a named-entity mention to an instance in a knowledge-base, typically Wikipedia-derived resources like DBpedia. This task is closely related to word-sense disambiguation (WSD), where the mention of an open-class word is linked to a concept in a knowledge-base, typically WordNet. This paper analyzes the relation between two annotated datasets on NED and WSD, highlighting the commonalities and differences. We detail the methods to construct a NED system following the WSD word-expert approach, where we need a dictionary and one classifier is built for each target entity mention string. Constructing a dictionary for NED proved challenging, and although similarity and ambiguity are higher for NED, the results are also higher due to the larger number of training data, and the more crisp and skewed meaning differences."
L16-1268,Addressing the {MFS} Bias in {WSD} systems,2016,13,4,3,0,17438,marten postma,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Word Sense Disambiguation (WSD) systems tend to have a strong bias towards assigning the Most Frequent Sense (MFS), which results in high performance on the MFS but in a very low performance on the less frequent senses. We addressed the MFS bias in WSD systems by combining the output from a WSD system with a set of mostly static features to create a MFS classifier to decide when to and not to choose the MFS. The output from this MFS classifier, which is based on the Random Forest algorithm, is then used to modify the output from the original WSD system. We applied our classifier to one of the state-of-the-art supervised WSD systems, i.e. IMS, and to of the best state-of-the-art unsupervised WSD systems, i.e. UKB. Our main finding is that we are able to improve the system output in terms of choosing between the MFS and the less frequent senses. When we apply the MFS classifier to fine-grained WSD, we observe an improvement on the less frequent sense cases, whereas we maintain the overall recall."
L16-1441,Word Sense-Aware Machine Translation: Including Senses as Contextual Features for Improved Translation Models,2016,16,9,3,0,24253,steven neale,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Although it is commonly assumed that word sense disambiguation (WSD) should help to improve lexical choice and improve the quality of machine translation systems, how to successfully integrate word senses into such systems remains an unanswered question. Some successful approaches have involved reformulating either WSD or the word senses it produces, but work on using traditional word senses to improve machine translation have met with limited success. In this paper, we build upon previous work that experimented on including word senses as contextual features in maxent-based translation models. Training on a large, open-domain corpus (Europarl), we demonstrate that this aproach yields significant improvements in machine translation from English to Portuguese."
L16-1483,{QTL}eap {WSD}/{NED} Corpora: Semantic Annotation of Parallel Corpora in Six Languages,2016,9,5,7,1,16264,arantxa otegi,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This work presents parallel corpora automatically annotated with several NLP tools, including lemma and part-of-speech tagging, named-entity recognition and classification, named-entity disambiguation, word-sense disambiguation, and coreference. The corpora comprise both the well-known Europarl corpus and a domain-specific question-answer troubleshooting corpus on the IT domain. English is common in all parallel corpora, with translations in five languages, namely, Basque, Bulgarian, Czech, Portuguese and Spanish. We describe the annotated corpora and the tools used for annotation, as well as annotation statistics for each language. These new resources are freely available and will help research on semantic processing for machine translation and cross-lingual transfer."
D16-1250,Learning principled bilingual mappings of word embeddings while preserving monolingual invariance,2016,10,116,3,1,10622,mikel artetxe,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-1293,Improving Translation Selection with Supersenses,2016,30,0,4,0,35787,haiqing tang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Selecting appropriate translations for source words with multiple meanings still remains a challenge for statistical machine translation (SMT). One reason for this is that most SMT systems are not good at detecting the proper sense for a polysemic word when it appears in different contexts. In this paper, we adopt a supersense tagging method to annotate source words with coarse-grained ontological concepts. In order to enable the system to choose an appropriate translation for a word or phrase according to the annotated supersense of the word or phrase, we propose two translation models with supersense knowledge: a maximum entropy based model and a supersense embedding model. The effectiveness of our proposed models is validated on a large-scale English-to-Spanish translation task. Results indicate that our method can significantly improve translation quality via correctly conveying the meaning of the source language to the target language."
W15-5707,Deep-syntax {T}ecto{MT} for {E}nglish-{S}panish {MT},2015,-1,-1,6,0.305553,8822,gorka labaka,Proceedings of the 1st Deep Machine Translation Workshop,0,None
W15-2711,Predicting word sense annotation agreement,2015,20,2,4,0,20634,hector alonso,"Proceedings of the First Workshop on Linking Computational Models of Lexical, Sentential and Discourse-level Semantics",0,"High agreement is a common objective when annotating data for word senses. However, a number of factors make perfect agreement impossible, e.g. the limitations of sense inventories, the difficulty of the examples or the interpretation preferences of the annotators. Estimating potential agreement is thus a relevant task to supplement the evaluation of sense annotations. In this article we propose two methods to predict agreement on wordannotation instances. We experiment with a continuous representation and a threeway discretization of observed agreement. In spite of the difficulty of the task, we find that different levels of agreement can be identifiedxe2x80x94in particular, low-agreement examples are easier to identify."
W15-1007,Analyzing {E}nglish-{S}panish Named-Entity enhanced Machine Translation,2015,6,1,2,1,10622,mikel artetxe,"Proceedings of the Ninth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"Translation of named-entities (NEs) is an issue in SMT. In this paper we analyze the errors when translating NEs with a SMT system from English to Spanish. We train on Europarl and test on News Commentary, focusing on entities correctly recognized by an automatic NE recognition system. The automatic systems translate around 85% NEs correctly, leaving a small margin for improving performance. In addition, we implement a purpose-build NE translator and integrate it in the SMT system, yielding a small but significant improvement in BLEU score. Our analysis shows that, contrary to similar systems translating from Chinese to English, there was no improvement in NE translation, prompting further work."
W15-0114,Crowdsourced Word Sense Annotations and Difficult Words and Examples,2015,10,2,2,0.572765,8821,oier lacalle,Proceedings of the 11th International Conference on Computational Semantics,0,"Word Sense Disambiguation has been stuck for many years. The recent availability of crowdsourced data with large numbers of sense annotations per example facilitates the exploration of new perspectives. Previous work has shown that words with uniform sense distribution have lower accuracy. In this paper we show that the agreement between annotators has a stronger correlation with performance, and that it can also be used to detect problematic examples. In particular, we show that, for many words, such examples are not useful for training, and that they are more difficult to disambiguate. The manual analysis seems to indicate that most of the problematic examples correspond to occurrences of subtle sense distinctions where the context is not enough to discern which is the sense that should be applied."
S15-2032,{UBC}: Cubes for {E}nglish Semantic Textual Similarity and Supervised Approaches for Interpretable {STS},2015,13,8,1,1,8824,eneko agirre,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"In Semantic Textual Similarity, systems rate the degree of semantic equivalence on a graded scale from 0 to 5, with 5 being the most similar. For the English subtask, we present a system which relies on several resources for token-to-token and phrase-to-phrase similarity to build a data-structure which holds all the information, and then combine the information to get a similarity score. We also participated in the pilot on Interpretable STS, where we apply a pipeline which first aligns tokens, then chunks, and finally uses supervised systems to label and score each chunk alignment."
S15-2045,"{S}em{E}val-2015 Task 2: Semantic Textual Similarity, {E}nglish, {S}panish and Pilot on Interpretability",2015,15,106,1,1,8824,eneko agirre,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"In semantic textual similarity (STS), systems rate the degree of semantic equivalence between two text snippets. This year, the participants were challenged with new datasets in English and Spanish. The annotations for both subtasks leveraged crowdsourcing. The English subtask attracted 29 teams with 74 system runs, and the Spanish subtask engaged 7 teams participating with 16 system runs. In addition, this year we ran a pilot task on interpretable STS, where the systems needed to add an explanatory layer, that is, they had to align the chunks in the sentence pair, explicitly annotating the kind of relation and the score of the chunk pair. The train and test data were manually annotated by an expert, and included headline and image sentence pairs from previous years. 7 teams participated with 29 runs."
S15-2132,{S}em{E}val-2015 Task 4: {T}ime{L}ine: Cross-Document Event Ordering,2015,14,25,3,0,5690,annelyse minard,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes the outcomes of the TimeLine task (Cross-Document Event Ordering), that was organised within the Time and Space track of SemEval-2015. Given a set of documents and a set of target entities, the task consisted of building a timeline for each entity, by detecting, anchoring in time and ordering the events involving that entity. The TimeLine task goes a step further than previous evaluation challenges by requiring participant systems to perform both event coreference and temporal relation extraction across documents. Four teams submitted the output of their systems to the four proposed subtracks for a total of 13 runs, the best of which obtained an F1-score of 7.85 in the main track (timeline creation from raw text)."
S15-1007,A Methodology for Word Sense Disambiguation at 90{\\%} based on large-scale {C}rowd{S}ourcing,2015,16,2,2,0.572765,8821,oier lacalle,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"Word Sense Disambiguation has been stuck for many years. In this paper we explore the use of large-scale crowdsourcing to cluster senses that are often confused by non-expert annotators. We show that we can increase performance at will: our in-domain experiment involving 45 highly polysemous nouns, verbs and adjective (9.8 senses on average), yields an average accuracy of 92.6 using a supervised classifier for an average polysemy of 6.1. Our proposal has the advantage of being cost-effective and being able to produce different levels of granularity. Our analysis shows that the error reduction with respect to finegrained senses is higher, and manual inspection show that the clusters are sensible when compared to those of OntoNotes and WordNet Supersenses."
S15-1011,Combining Mention Context and Hyperlinks from {W}ikipedia for Named Entity Disambiguation,2015,8,9,3,1,8823,ander barrena,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"Named entity disambiguation is the task of linking entity mentions to their intended referent, as represented in a Knowledge Base, usually derived from Wikipedia. In this paper, we combine local mention context and global hyperlink structure from Wikipedia in a probabilistic framework. We test our method in eight datasets, improving the state-of-the-art results in five. Our results show that the two models of context, namely, words in the context and hyperlink pathways to other entities in the context, are complementary. Our results are not tuned to any of the datasets, showing that it is robust to out-of-domain scenarios, and that further improvements are possible."
P15-2045,Improving distant supervision using inference learning,2015,23,3,2,0,13906,roland roller,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Distant supervision is a widely applied approach to automatic training of relation extraction systems and has the advantage that it can generate large amounts of labelled data with minimal effort. However, this data may contain errors and consequently systems trained using distant supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data."
N15-1066,Diamonds in the Rough: Event Extraction from Imperfect Microblog Data,2015,15,7,2,0,26503,ander intxaurrondo,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We introduce a distantly supervised event extraction approach that extracts complex event templates from microblogs. We show that this near real-time data source is more challenging than news because it contains information that is both approximate (e.g., with values that are close but different from the gold truth) and ambiguous (due to the brevity of the texts), impacting both the evaluation and extraction methods. For the former, we propose a novel, xe2x80x9csoftxe2x80x9d, F1 metric that incorporates similarity between extracted fillers and the gold truth, giving partial credit to different but similar values. With respect to extraction methodology, we propose two extensions to the distant supervision paradigm: to address approximate information, we allow positive training examples to be generated from information that is similar but not identical to gold values; to address ambiguity, we aggregate contexts across tweets discussing the same event. We evaluate our contributions on the complex domain of earthquakes, with events with up to 20 arguments. Our results indicate that, despite their simplicity, our contributions yield a statistically-significant improvement of 33% (relative) over a strong distantly-supervised system. The dataset containing the knowledge base, relevant tweets and manual annotations is publicly available."
N15-1165,Random Walks and Neural Network Language Models on Knowledge Bases,2015,20,26,3,0,37695,josu goikoetxea,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Random walks over large knowledge bases like WordNet have been successfully used in word similarity, relatedness and disambiguation tasks. Unfortunately, those algorithms are relatively slow for large repositories, with significant memory footprints. In this paper we present a novel algorithm which encodes the structure of a knowledge base in a continuous vector space, combining random walks and neural net language models in order to produce novel word representations. Evaluation in word relatedness and similarity datasets yields equal or better results than those of a random walk algorithm, using a dense representation (300 dimensions instead of 117K). Furthermore, the word representations are complementary to those of the random walk algorithm and to corpus-based continuous representations, improving the stateof-the-art in the similarity dataset. Our technique opens up exciting opportunities to combine distributional and knowledge-based word representations."
W14-4704,Exploring the use of word embeddings and random walks on {W}ikipedia for the {C}og{A}lex shared task,2014,18,0,2,0,37695,josu goikoetxea,Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex),0,"In our participation on the task we wanted to test three different kinds of relatedness algorithms: one based on embeddings induced from corpora, another based on random walks on WordNet and a last one based on random walks based on Wikipedia. All three of them perform similarly in noun relatedness datasets like WordSim353, close to the highest reported values. Although the task definition gave examples of nouns, the train and test data were based on the Edinburgh Association Thesaurus, and around 50% of the target words were not nouns. The corpus-based algorithm performed much better than the other methods in the training dataset, and was thus submitted for the test."
S14-2010,{S}em{E}val-2014 Task 10: Multilingual Semantic Textual Similarity,2014,25,158,1,1,8824,eneko agirre,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"In Semantic Textual Similarity, systems rate the degree of semantic equivalence between two text snippets. This year, the participants were challenged with new data sets for English, as well as the introduction of Spanish, as a new language in which to assess semantic similarity. For the English subtask, we exposed the systems to a diversity of testing scenarios, by preparing additional OntoNotesWordNet sense mappings and news headlines, as well as introducing new genres, including image descriptions, DEFT discussion forums, DEFT newswire, and tweet-newswire headline mappings. For Spanish, since, to our knowledge, this is the first time that official evaluations are conducted, we used well-formed text, by featuring sentences extracted from encyclopedic content and newswire. The annotations for both tasks leveraged crowdsourcing. The Spanish subtask engaged 9 teams participating with 22 system runs, and the English subtask attracted 15 teams with 38 system runs."
P14-2106,On {W}ord{N}et Semantic Classes and Dependency Parsing,2014,24,4,2,0,15673,kepa bengoetxea,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper presents experiments with WordNet semantic classes to improve dependency parsing. We study the effect of semantic classes in three dependency parsers, using two types of constituencyto-dependency conversions of the English Penn Treebank. Overall, we can say that the improvements are small and not significant using automatic POS tags, contrary to previously published results using gold POS tags (Agirre et al., 2011). In addition, we explore parser combinations, showing that the semantically enhanced parsers yield a small significant gain only on the more semantically oriented LTH treebank conversion."
J14-1003,Random Walks for Knowledge-Based Word Sense Disambiguation,2014,68,104,1,1,8824,eneko agirre,Computational Linguistics,0,"Word Sense Disambiguation WSD systems automatically choose the intended meaning of a word in context. In this article we present a WSD algorithm based on random walks over large Lexical Knowledge Bases LKB. We show that our algorithm performs better than other graph-based methods when run on a graph built from WordNet and eXtended WordNet. Our algorithm and LKB combination compares favorably to other knowledge-based approaches in the literature that use similar knowledge on a variety of English data sets and a data set on Spanish. We include a detailed analysis of the factors that affect the algorithm. The algorithm and the LKBs used are publicly available, and the results easily reproducible."
C14-1213,{``}One Entity per Discourse{''} and {``}One Entity per Collocation{''} Improve Named-Entity Disambiguation,2014,17,3,2,1,8823,ander barrena,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"The xe2x80x9cone sense per discoursexe2x80x9d (OSPD) and xe2x80x9cone sense per collocationxe2x80x9d (OSPC) hypotheses have been very influential in Word Sense Disambiguation. The goal of this paper is twofold: (i) to explore whether these hypotheses hold for entities, that is, whether several mentions in the same discourse (or the same collocation) tend to refer to the same entity or not, and (ii) test their impact in Named-Entity Disambiguation (NED). Our experiments show consistent results on different collections and three state-of-the-art NED system. OSPD hypothesis holds in around 96%-98% of documents whereas OSPC hypothesis holds in 91%-98% of collocations. Furthermore, a simple NED post-processing in which the majority entity is promoted, produces a gain in performance in all cases, reaching up to 8 absolute points of improvement in F-measure. These results show that NED systems would benefit of considering these hypotheses into their implementation."
W13-3801,Text Understanding using Knowledge-Bases and Random Walks,2013,0,0,1,1,8824,eneko agirre,Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora,0,None
W13-2701,Generating Paths through Cultural Heritage Collections,2013,28,0,6,0.833333,40905,samuel fernando,"Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"Cultural heritage collections usually organise sets of items into exhibitions or guided tours. These items are often accompanied by text that describes the theme and topic of the exhibition and provides background context and details of connections with other items. The PATHS project brings the idea of guided tours to digital library collections where a tool to create virtual paths are used to assist with navigation and provide guides on particular subjects and topics. In this paper we characterise and analyse paths of items created by users of our online system. The analysis highlights that most users spend time selecting items relevant to their chosen topic, but few users took time to add background information to the paths. In order to address this, we conducted preliminary investigations to test whether Wikipedia can be used to automatically add background text for sequences of items. In the future we would like to explore the automatic creation of full paths."
S13-1004,*{SEM} 2013 shared task: Semantic Textual Similarity,2013,8,157,1,1,8824,eneko agirre,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,"In Semantic Textual Similarity (STS), systems rate the degree of semantic equivalence, on a graded scale from 0 to 5, with 5 being the most similar. This year we set up two tasks: (i) a core task (CORE), and (ii) a typed-similarity task (TYPED). CORE is similar in set up to SemEval STS 2012 task with pairs of sentences from sources related to those of 2012, yet different in genre from the 2012 set, namely, this year we included newswire headlines, machine translation evaluation datasets and multiple lexical resource glossed sets. TYPED, on the other hand, is novel and tries to characterize why two items are deemed similar, using cultural heritage items which are described with metadata such as title, author or description. Several types of similarity have been defined, including similar author, similar time period or similar location. The annotation for both tasks leverages crowdsourcing, with relative high interannotator correlation, ranging from 62% to 87%. The CORE task attracted 34 participants with 89 runs, and the TYPED task attracted 6 teams with 14 runs."
S13-1018,{UBC}{\\_}{UOS}-{TYPED}: Regression for typed-similarity,2013,12,4,1,1,8824,eneko agirre,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,We approach the typed-similarity task using a range of heuristics that rely on information from the appropriate metadata fields for each type of similarity. In addition we train a linear regressor for each type of similarity. The results indicate that the linear regression is key for good performance. Our best system was ranked third in the task.
P13-4026,{PATHS}: A System for Accessing Cultural Heritage Collections,2013,22,9,1,1,8824,eneko agirre,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"This paper describes a system for navigating large collections of information about cultural heritage which is applied to Europeana, the European Library. Europeana contains over 20 million artefacts with meta-data in a wide range of European languages. The system currently provides access to Europeana content with meta-data in English and Spanish. The paper describes how Natural Language Processing is used to enrich and organise this meta-data to assist navigation through Europeana and shows how this information is used within the system."
J13-3006,Selectional Preferences for Semantic Role Classification,2013,57,22,2,1,37296,benat zapirain,Computational Linguistics,0,"This paper focuses on a well-known open issue in Semantic Role Classification (SRC) research: the limited influence and sparseness of lexical features. We mitigate this problem using models that integrate automatically learned selectional preferences (SP). We explore a range of models based on WordNet and distributional-similarity SPs. Furthermore, we demonstrate that the SRC task is better modeled by SP models centered on both verbs and prepositions, rather than verbs alone. Our experiments with SP-based models in isolation indicate that they outperform a lexical baseline with 20 F1 points in domain and almost 40 F1 points out of domain. Furthermore, we show that a state-of-the-art SRC system extended with features based on selectional preferences performs significantly better, both in domain (17% error reduction) and out of domain (13% error reduction). Finally, we show that in an end-to-end semantic role labeling system we obtain small but statistically significant improvements, even though our modifie..."
W12-1013,Enabling the Discovery of Digital Cultural Heritage Objects through {W}ikipedia,2012,26,8,5,0,40908,mark hall,"Proceedings of the 6th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"Over the past years large digital cultural heritage collections have become increasingly available. While these provide adequate search functionality for the expert user, this may not offer the best support for non-expert or novice users. In this paper we propose a novel mechanism for introducing new users to the items in a collection by allowing them to browse Wikipedia articles, which are augmented with items from the cultural heritage collection. Using Europeana as a case-study we demonstrate the effectiveness of our approach for encouraging users to spend longer exploring items in Europeana compared with the existing search provision."
S12-1051,{S}em{E}val-2012 Task 6: A Pilot on Semantic Textual Similarity,2012,10,355,1,1,8824,eneko agirre,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts. This paper presents the results of the STS pilot task in Semeval. The training data contained 2000 sentence pairs from previously existing paraphrase datasets and machine translation evaluation resources. The test data also comprised 2000 sentences pairs for those datasets, plus two surprise datasets with 400 pairs from a different machine translation evaluation corpus and 750 pairs from a lexical resource mapping exercise. The similarity of pairs of sentences was rated on a 0-5 scale (low to high similarity) by human judges using Amazon Mechanical Turk, with high Pearson correlation scores, around 90%. 35 teams participated in the task, submitting 88 runs. The best results scored a Pearson correlation >80%, well above a simple lexical baseline that only scored a 31% correlation. This pilot task opens an exciting way ahead, although there are still open issues, specially the evaluation metric."
S12-1091,{SRIUBC}: Simple Similarity Features for Semantic Textual Similarity,2012,16,1,2,1,31138,eric yeh,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"We describe the systems submitted by SRI International and the University of the Basque Country for the Semantic Textual Similarity (STS) SemEval-2012 task. Our systems focused on using a simple set of features, featuring a mix of semantic similarity resources, lexical match heuristics, and part of speech (POS) information. We also incorporate precision focused scores over lexical and POS information derived from the BLEU measure, and lexical and POS features computed over split-bigrams from the ROUGE-S measure. These were used to train support vector regressors over the pairs in the training data. From the three systems we submitted, two performed well in the overall ranking, with split-bigrams improving performance over pairs drawn from the MSR Research Video Description Corpus. Our third system maintained three separate regressors, each trained specifically for the STS dataset they were drawn from. It used a multinomial classifier to predict which dataset regressor would be most appropriate to score a given pair, and used it to score that pair. This system underperformed, primarily due to errors in the dataset predictor."
agirre-etal-2012-matching,Matching Cultural Heritage items to {W}ikipedia,2012,10,14,1,1,8824,eneko agirre,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Digitised Cultural Heritage (CH) items usually have short descriptions and lack rich contextual information. Wikipedia articles, on the contrary, include in-depth descriptions and links to related articles, which motivate the enrichment of CH items with information from Wikipedia. In this paper we explore the feasibility of finding matching articles in Wikipedia for a given Cultural Heritage item. We manually annotated a random sample of items from Europeana, and performed a qualitative and quantitative study of the issues and problems that arise, showing that each kind of CH item is different and needs a nuanced definition of what ``matching article'' means. In addition, we test a well-known wikification (aka entity linking) algorithm on the task. Our results indicate that a substantial number of items can be effectively linked to their corresponding Wikipedia article."
C12-1007,Contribution of Complex Lexical Information to Solve Syntactic Ambiguity in {B}asque,2012,25,5,2,0,43730,aitziber atutxa,Proceedings of {COLING} 2012,0,"In this study, we explore the impact of complex lexical information to solve syntactic ambiguity, including verbal subcategorization in the form of verbal transitivity and verb-noun-case or verb-noun-case-auxiliary relations. The information was obtained from different sources, including a subcategorization dictionary extracted from a Basque corpus, the web as a corpus, an English corpus and a Basque dictionary. Functional ambiguity between subject and object is a widespread problem in Basque, where 22% of subjects and objects are ambiguous, and this ambiguity surfaces in 33% of the sentences. This problem is comparable to PP attachment ambiguities in other languages. Our results show that, using complex lexical information, our results are better than a state-of-the-art statistical parser, obtaining a statistically significant error reduction of 20%. The disambiguation system is independent on the actual parsing algorithm used. The analysis revealed that the most relevant information are the case carried by the noun and the transitivity of the verb. TITLE AND ABSTRACT IN BASQUE Informazio lexikal konplexuaren ekarpena euskarazko anbiguotasun sintaktikoen ebazpenean Lan honetan informazio lexikal konplexua erabiltzearen garrantzia aztertzen dugu euskarazko anbiguotasun sintaktikoen ebazpenean. Aditzen iragankortasuna erakusten duen azpikategorizazioaren ekarpena aztertu dugu, baita aditz-izen-kasu eta aditz-izen-kasu-laguntzaile erlazioena ere. Informazio horiek hainbat iturritatik jaso ditugu: euskarazko corpus batetik, webetik berau corpus gisa hartuta, ingelesezko corpus batetik eta euskarazko hiztegi batetik. Subjektu eta objektuaren arteko anbiguotasun funtzionala maiz aurkitzen dugu euskarazko testuetan; subjektua edo objektua bereiztea kasuen %22an ambiguoa da, eta hori gertatzen da perpausen %33an. Horrela, arazo horren garrantzi handia konparagarria da beste hizkuntza batzuek duten PP attachment arazoarenarekin. Gure sistemaren emaitzak hobeak dira artearen egoerako analizatzaile sintaktiko estatistiko batenak baino, estatistikoki esanguratsua den %20ko errore-murrizketa lortzen baitu. Analisi sintaktikoa egiteko edozein algoritmorekin erabil daiteke desanbiguazio-sistema hau."
C12-1054,Comparing Taxonomies for Organising Collections of Documents,2012,27,9,3,0.833333,40905,samuel fernando,Proceedings of {COLING} 2012,0,"There is a demand for taxonomies to organise large collections of documents into categories for browsing and exploration. This paper examines four existing taxonomies that have been manually created, along with two methods for deriving taxonomies automatically from data items. We use these taxonomies to organise items from a large online cultural heritage collection. We then present two human evaluations of the taxonomies. The first measures the cohesion of the taxonomies to determine how well they group together similar items under the same concept node. The second analyses the concept relations in the taxonomies. The results show that the manual taxonomies have high quality well defined relations. However the novel automatic method is found to generate very high cohesion."
P11-2123,Improving Dependency Parsing with Semantic Classes,2011,25,32,1,1,8824,eneko agirre,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"This paper presents the introduction of WordNet semantic classes in a dependency parser, obtaining improvements on the full Penn Treebank for the first time. We tried different combinations of some basic semantic classes and word sense disambiguation algorithms. Our experiments show that selecting the adequate combination of semantic features on development data is key for success. Given the basic nature of the semantic classes and word sense disambiguation algorithms used, we think there is ample room for future improvements."
I11-1175,Query Expansion for {IR} using Knowledge-Based Relatedness,2011,18,8,3,1,16264,arantxa otegi,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"The limitations of keyword-only approaches to information retrieval were recognized since the early days, specially in cases where different but closely-related words are used in the query and the relevant document. Query expansion techniques like pseudo-relevance feedback rely on the target document set in order to bridge the gap between those words, but they might suffer from topic drift. This paper explores the use of knowledge-based semantic relatedness in order to bridge the gap between query and documents. We performed query expansion, with positive effects over some language modeling baselines."
W10-3301,{KYOTO}: an open platform for mining facts,2010,13,14,3,0.359361,5469,piek vossen,Proceedings of the 6th Workshop on {O}ntologies and {L}exical {R}esources,0,"This document describes an open text-mining system that was developed for the Asian-European project KYOTO. The KYOTO system uses an open text representation format and a central ontology to enable extraction of knowledge and facts from large volumes of text in many different languages. We implemented a semantic tagging approach that performs off-line reasoning. Mining of facts and knowledge is achieved through a flexible pattern matching module that can work in much the same way for different languages, can handle efficiently large volumes of documents and is not restricted to a specific domain. We applied the system to an English database on estuaries."
S10-1013,{S}em{E}val-2010 Task 17: All-Words Word Sense Disambiguation on a Specific Domain,2010,17,46,1,1,8824,eneko agirre,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"Domain portability and adaptation of NLP components and Word Sense Disambiguation systems present new challenges. The difficulties found by supervised systems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledge-based WSD systems. Unfortunately, all existing evaluation datasets for specific domains are lexical-sample corpora. This task presented all-words datasets on the environment domain for WSD in four languages (Chinese, Dutch, English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain."
S10-1093,{K}yoto: An Integrated System for Specific Domain {WSD},2010,10,2,2,0.75,13429,aitor soroa,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"This document describes the preliminary release of the integrated Kyoto system for specific domain WSD. The system uses concept miners (Tybots) to extract domain-related terms and produces a domain-related thesaurus, followed by knowledge-based WSD based on wordnet graphs (UKB). The resulting system can be applied to any language with a lexical knowledge base, and is based on publicly available software and resources. Our participation in Semeval task #17 focused on producing running systems for all languages in the task, and we attained good results in all except Chinese. Due to the pressure of the time-constraints in the competition, the system is still under development, and we expect results to improve in the near future."
N10-1058,Improving Semantic Role Classification with Selectional Preferences,2010,10,14,2,1,37296,benat zapirain,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"This work incorporates Selectional Preferences (SP) into a Semantic Role (SR) Classification system. We learn separate selectional preferences for noun phrases and prepositional phrases and we integrate them in a state-of-the-art SR classification system both in the form of features and individual class predictors. We show that the inclusion of the refined SPs yields statistically significant improvements on both in domain and out of domain data (14.07% and 11.67% error reduction, respectively). The key factor for success is the combination of several SP methods with the original classification model using meta-classification."
agirre-etal-2010-exploring,Exploring Knowledge Bases for Similarity,2010,28,35,1,1,8824,eneko agirre,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Graph-based similarity over WordNet has been previously shown to perform very well on word similarity. This paper presents a study of the performance of such a graph-based algorithm when using different relations and versions of Wordnet. The graph algorithm is based on Personalized PageRank, a random-walk based algorithm which computes the probability of a random-walk initiated in the target word to reach any synset following the relations in WordNet (Haveliwala, 2002). Similarity is computed as the cosine of the probability distributions for each word over WordNet. The best combination of relations includes all relations in WordNet 3.0, included disambiguated glosses, and automatically disambiguated topic signatures called KnowNets. All relations are part of the official release of WordNet, except KnowNets, which have been derived automatically. The results over the WordSim 353 dataset show that using the adequate relations the performance improves over previously published WordNet-based results on the WordSim353 dataset (Finkelstein et al., 2002). The similarity software and some graphs used in this paper are publicly available at http://ixa2.si.ehu.es/ukb."
C10-2002,Document Expansion Based on {W}ord{N}et for Robust {IR},2010,23,25,1,1,8824,eneko agirre,Coling 2010: Posters,0,"The use of semantic information to improve IR is a long-standing goal. This paper presents a novel Document Expansion method based on a WordNet-based system to find related concepts and words. Expansion words are indexed separately, and when combined with the regular index, they improve the results in three datasets over a state-of-the-art IR engine. Considering that many IR systems are not robust in the sense that they need careful fine-tuning and optimization of their parameters, we explored some parameter settings. The results show that our method is specially effective for realistic, non-optimal settings, adding robustness to the IR engine. We also explored the effect of document length, and show that our method is specially successful with shorter documents."
C10-1005,Plagiarism Detection across Distant Language Pairs,2010,27,45,3,0,15265,alberto barroncedeno,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Plagiarism, the unacknowledged reuse of text, does not end at language boundaries. Cross-language plagiarism occurs if a text is translated from a fragment written in a different language and no proper citation is provided. Regardless of the change of language, the contents and, in particular, the ideas remain the same. Whereas different methods for the detection of monolingual plagiarism have been developed, less attention has been paid to the cross-language case.n n In this paper we compare two recently proposed cross-language plagiarism detection methods (CL-CNG, based on character n-grams and CL-ASA, based on statistical translation), to a novel approach to this problem, based on machine translation and monolingual similarity analysis (TMA). We explore the effectiveness of the three approaches for less related languages. CL-CNG shows not be appropriate for this kind of language pairs, whereas TMA performs better than the previously proposed models."
W09-3206,{W}iki{W}alk: Random walks on {W}ikipedia for Semantic Relatedness,2009,17,131,4,1,31138,eric yeh,Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing ({T}ext{G}raphs-4),0,"Computing semantic relatedness of natural language texts is a key component of tasks such as information retrieval and summarization, and often depends on knowledge of a broad range of real-world concepts and relationships. We address this knowledge integration issue by computing semantic relatedness using personalized PageRank (random walks) on a graph derived from Wikipedia. This paper evaluates methods for building the graph, including link selection strategies, and two methods for representing input texts as distributions over the graph nodes: one based on a dictionary lookup, the other based on Explicit Semantic Analysis. We evaluate our techniques on standard word relatedness and text similarity datasets, finding that they capture similarity information complementary to existing Wikipedia-based relatedness measures, resulting in small improvements on a state-of-the-art measure."
W09-2420,{S}em{E}val-2010 Task 17: All-words Word Sense Disambiguation on a Specific Domain,2009,30,20,1,1,8824,eneko agirre,Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions ({SEW}-2009),0,"Domain portability and adaptation of NLP components and Word Sense Disambiguation systems present new challenges. The difficulties found by supervised systems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledgebased WSD systems. Unfortunately, all existing evaluation datasets for specific domains are lexical-sample corpora. With this paper we want to motivate the creation of an allwords test dataset for WSD on the environment domain in several languages, and present the overall design of this SemEval task."
R09-1080,A Study on Linking {W}ikipedia Categories to {W}ordnet Synsets using Text Similarity,2009,18,16,3,0,9426,antonio toral,Proceedings of the International Conference {RANLP}-2009,0,"This paper studies the application of text similarity methods to disambiguate ambiguous links between WordNet nouns and Wikipedia categories. The methods range from word overlap between glosses, random projections, WordNetbased similarity, and a full-fledged textual entailment system. Both unsupervised and supervised combinations have been tried. The goldstandard with disambiguated links is publicly available. The results range from 64.7% for the first sense heuristic, 68% for an unsupervised combination, and up to 77.74% for a supervised combination."
P09-2019,Generalizing over Lexical Features: Selectional Preferences for Semantic Role Classification,2009,9,21,2,1,37296,benat zapirain,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"This paper explores methods to alleviate the effect of lexical sparseness in the classification of verbal arguments. We show how automatically generated selectional preferences are able to generalize and perform better than lexical features in a large dataset for semantic role classification. The best results are obtained with a novel second-order distributional similarity measure, and the positive effect is specially relevant for out-of-domain data. Our findings suggest that selectional preferences have potential for improving a full system for Semantic Role Labeling."
N09-1003,A Study on Similarity and Relatedness Using Distributional and {W}ord{N}et-based Approaches,2009,29,570,1,1,8824,eneko agirre,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"This paper presents and compares WordNet-based and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses."
E09-1005,Personalizing {P}age{R}ank for Word Sense Disambiguation,2009,21,409,1,1,8824,eneko agirre,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"In this paper we propose a new graph-based method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation. Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets. We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet. In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster."
E09-1006,Supervised Domain Adaption for {WSD},2009,23,24,1,1,8824,eneko agirre,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"The lack of positive results on supervised domain adaptation for WSD have cast some doubts on the utility of hand-tagging general corpora and thus developing generic supervised WSD systems. In this paper we show for the first time that our WSD system trained on a general source corpus (Bnc) and the target corpus, obtains up to 22% error reduction when compared to a system trained on the target corpus alone. In addition, we show that as little as 40% of the target corpus (when supplemented with the source corpus) is sufficient to obtain the same results as training on the full target data. The key for success is the use of unlabeled data with svd, a combination of kernels and svm."
2009.eamt-1.9,Use of Rich Linguistic Information to Translate Prepositions and Grammar Cases to {B}asque,2009,18,5,1,1,8824,eneko agirre,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,"This paper presents three successful techniques to translate prepositions heading verbal complements by means of rich linguistic information, in the context of a rule-based Machine Translation system for an agglutinative language with scarce resources. This information comes in the form of lexicalized syntactic dependency triples, verb subcategorization and manually coded selection rules based on lexical, syntactic and semantic information. The first two resources have been automatically extracted from monolingual corpora. The results obtained using a new evaluation methodology show that all proposed techniques improve precision over the baselines, including a translation dictionary compiled from an aligned corpus, and a state-of-the-art statistical Machine Translation system. The results also show that linguistic information in all three techniques are complementary, and that a combination of them obtains the best F-score results overall."
P08-1037,Improving Parsing and {PP} Attachment Performance with Sense Information,2008,27,67,1,1,8824,eneko agirre,Proceedings of ACL-08: HLT,1,"To date, parsers have made limited use of semantic information, but there is evidence to suggest that semantic features can enhance parse disambiguation. This paper shows that semantic classes help to obtain significant improvement in both parsing and PP attachment tasks. We devise a gold-standard sense- and parse tree-annotated dataset based on the intersection of the Penn Treebank and SemCor, and experiment with different approaches to both semantic representation and disambiguation. For the Bikel parser, we achieved a maximal error reduction rate over the baseline parser of 6.9% and 20.5%, for parsing and PP-attachment respectively, using an unsupervised WSD strategy. This demonstrates that word sense information can indeed enhance the performance of syntactic disambiguation."
P08-1063,Robustness and Generalization of Role Sets: {P}rop{B}ank vs. {V}erb{N}et,2008,10,17,2,1,37296,benat zapirain,Proceedings of ACL-08: HLT,1,"This paper presents an empirical study on the robustness and generalization of two alternative role sets for semantic role labeling: PropBank numbered roles and VerbNet thematic roles. By testing a statexe2x80x93ofxe2x80x93thexe2x80x93art SRL system with the two alternative role annotations, we show that the PropBank role set is more robust to the lack of verbxe2x80x93specific semantic information and generalizes better to infrequent and unseen predicates. Keeping in mind that thematic roles are better for application needs, we also tested the best way to generate VerbNet annotation. We conclude that tagging first PropBank roles and mapping into VerbNet roles is as effective as training and tagging directly on VerbNet, and more robust for domain shifts."
agirre-soroa-2008-using,Using the Multilingual Central Repository for Graph-Based Word Sense Disambiguation,2008,13,23,1,1,8824,eneko agirre,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents the results of a graph-based method for performing knowledge-based Word Sense Disambiguation (WSD). The technique exploits the structural properties of the graph underlying the chosen knowledge base. The method is general, in the sense that it is not tied to any particular knowledge base, but in this work we have applied it to the Multilingual Central Repository (MCR). The evaluation has been performed on the Senseval-3 all-words task. The main contributions of the paper are twofold: (1) We have evaluated the separate and combined performance of each type of relation in the MCR, and thus indirectly validated the contents of the MCR and their potential for WSD. (2) We obtain state-of-the-art results, and in fact yield the best results that can be obtained using publicly available data."
vossen-etal-2008-kyoto,"{KYOTO}: a System for Mining, Structuring and Distributing Knowledge across Languages and Cultures",2008,32,44,2,0.359361,5469,piek vossen,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We outline work performed within the framework of a current EC project. The goal is to construct a language-independent information system for a specific domain (environment/ecology/biodiversity) anchored in a language-independent ontology that is linked to wordnets in seven languages. For each language, information extraction and identification of lexicalized concepts with ontological entries is carried out by text miners (ÂKybotsÂ). The mapping of language-specific lexemes to the ontology allows for crosslinguistic identification and translation of equivalent terms. The infrastructure developed within this project enables long-range knowledge sharing and transfer across many languages and cultures, addressing the need for global and uniform transition of knowledge beyond the specific domains addressed here."
pociello-etal-2008-wnterm,{WNTERM}: Enriching the {MCR} with a Terminological Dictionary,2008,15,3,3,0,48081,eli pociello,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In this paper we describe the methodology and the first steps for the creation of WNTERM (from WordNet and Terminology), a specialized lexicon produced from the merger of the EuroWordNet-based Multilingual Central Repository (MCR) and the Basic Encyclopaedic Dictionary of Science and Technology (BDST). As an example, the ecology domain has been used. The final result is a multilingual (Basque and English) light-weight domain ontology, including taxonomic and other semantic relations among its concepts, which is tightly connected to other wordnets."
C08-1003,On Robustness and Domain Adaptation using {SVD} for Word Sense Disambiguation,2008,22,23,1,1,8824,eneko agirre,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"In this paper we explore robustness and domain adaptation issues for Word Sense Disambiguation (WSD) using Singular Value Decomposition (SVD) and unlabeled data. We focus on the semi-supervised domain adaptation scenario, where we train on the source corpus and test on the target corpus, and try to improve results using unlabeled data. Our method yields up to 16.3% error reduction compared to state-of-the-art systems, being the first to report successful semi-supervised domain adaptation. Surprisingly the improvement comes from the use of unlabeled data from the source corpus, and not from the target corpora, meaning that we get robustness rather than domain adaptation. In addition, we study the behavior of our system on the target domain."
S07-1001,{S}em{E}val-2007 Task 01: Evaluating {WSD} on Cross-Language Information Retrieval,2007,17,19,1,1,8824,eneko agirre,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"This paper presents a first attempt of an application-driven evaluation exercise of WSD. We used a CLIR testbed from the Cross Lingual Evaluation Forum. The expansion, indexing and retrieval strategies where fixed by the organizers. The participants had to return both the topics and documents tagged with WordNet 1.6 word senses. The organization provided training data in the form of a pre-processed Semcor which could be readily used by participants. The task had two participants, and the organizer also provide an in-house WSD system for comparison."
S07-1002,{S}em{E}val-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems,2007,12,127,1,1,8824,eneko agirre,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledge-based systems. In total there were 6 participating systems. We reused the SemEval-2007 English lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping). We provide a comparison to the results of the systems participating in the lexical sample subtask of task 17."
S07-1074,{UBC}-{ALM}: Combining k-{NN} with {SVD} for {WSD},2007,6,37,1,1,8824,eneko agirre,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"This work describes the University of the Basque Country system (UBC-ALM) for lexical sample and all-words WSD subtasks of SemEval-2007 task 17, where it performed in the second and fifth positions respectively. The system is based on a combination of k-Nearest Neighbor classifiers, with each classifier learning from a distinct set of features: local features (syntactic, collocations features), topical features (bag-of-words, domain information) and latent features learned from a reduced space using Singular Value Decomposition."
S07-1075,{UBC}-{AS}: A Graph Based Unsupervised System for Induction and Classification,2007,10,13,1,1,8824,eneko agirre,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"This paper describes a graph-based unsupervised system for induction and classification. The system performs a two stage graph based clustering where a cooccurrence graph is first clustered to compute similarities against contexts. The context similarity matrix is pruned and the resulting associated graph is clustered again by means of a random-walk type algorithm. The system relies on a set of parameters that have been tuned to fit the corpus data. The system has participated in tasks 2 and 13 of the SemEval-2007 competition, on word sense induction and Web people search, respectively, with mixed results."
S07-1076,{UBC}-{UMB}: Combining unsupervised and supervised systems for all-words {WSD},2007,5,0,3,0.895522,13902,david martinez,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"This paper describes the joint submission of two systems to the all-words WSD sub-task of SemEval-2007 task 17. The main goal of this work was to build a competitive unsupervised system by combining heterogeneous algorithms. As a secondary goal, we explored the integration of unsupervised predictions into a supervised system by different means."
S07-1077,{UBC}-{UPC}: Sequential {SRL} Using Selectional Preferences. An approach with Maximum Entropy {M}arkov Models,2007,8,7,2,1,37296,benat zapirain,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"We present a sequential Semantic Role Labeling system that describes the tagging problem as a Maximum Entropy Markov Model. The system uses full syntactic information to select BIO-tokens from input data, and classifies them sequentially using state-of-the-art features, with the addition of Selectional Preference features. The system presented achieves competitive performance in the CoNLL-2005 shared task dataset and it ranks first in the SRL subtask of the Semeval-2007 task 17."
W06-3814,Evaluating and optimizing the parameters of an unsupervised graph-based {WSD} algorithm,2006,11,38,1,1,8824,eneko agirre,Proceedings of {T}ext{G}raphs: the First Workshop on Graph Based Methods for Natural Language Processing,0,"Veronis (2004) has recently proposed an innovative unsupervised algorithm for word sense disambiguation based on small-world graphs called HyperLex. This paper explores two sides of the algorithm. First, we extend Veronis' work by optimizing the free parameters (on a set of words which is different to the target set). Second, given that the empirical comparison among unsupervised systems (and with respect to supervised systems) is seldom made, we used hand-tagged corpora to map the induced senses to a standard lexicon (WordNet) and a publicly available gold standard (Senseval 3 English Lexical Sample). Our results for nouns show that thanks to the optimization of parameters and the mapping method, HyperLex obtains results close to supervised systems using the same kind of bag-of-words features. Given the information loss inherent in any mapping step and the fact that the parameters were tuned for another set of words, these are very interesting results."
W06-1669,Two graph-based algorithms for state-of-the-art {WSD},2006,19,75,1,1,8824,eneko agirre,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"This paper explores the use of two graph algorithms for unsupervised induction and tagging of nominal word senses based on corpora. Our main contribution is the optimization of the free parameters of those algorithms and its evaluation against publicly available gold standards. We present a thorough evaluation comprising supervised and unsupervised modes, and both lexical-sample and all-words tasks. The results show that, in spite of the information loss inherent to mapping the induced senses to the gold-standard, the optimization of parameters based on a small sample of nouns carries over to all nouns, performing close to supervised systems in the lexical sample task and yielding the second-best WSD systems for the Senseval-3 all-words task."
U06-1008,Word Relatives in Context for Word Sense Disambiguation,2006,20,16,2,0.943396,13902,david martinez,Proceedings of the Australasian Language Technology Workshop 2006,0,"The current situation for Word Sense Disambiguation (WSD) is somewhat stuck due to lack of training data. We present in this paper a novel disambiguation algorithm that improves previous systems based on acquisition of examples by incorporating local context information. With a basic configuration, our method is able to obtain state-of-the-art performance. We complemented this work by evaluating other well-known methods in the same dataset, and analysing the comparative results per word. We observed that each algorithm performed better for different types of words, and each of them failed for some particular words. We proposed then a simple unsupervised voting scheme that improved significantly over single systems, achieving the best unsupervised performance on both the Senseval 2 and Senseval 3 lexical sample datasets."
agirre-etal-2006-preliminary,A Preliminary Study for Building the {B}asque {P}rop{B}ank,2006,15,10,1,1,8824,eneko agirre,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper presents a methodology for adding a layer of semantic annotation to a syntactically annotated corpus of Basque (EPEC), in terms of semantic roles. The proposal we make here is the combination of three resources: the model used in the PropBank project (Palmer et al., 2005), an in-house database with syntactic/semantic subcategorization frames for Basque verbs (Aldezabal, 2004) and the Basque dependency treebank (Aduriz et al., 2003). In order to validate the methodology and to confirm whether the PropBank model is suitable for Basque and our treebank design, we have built lexical entries and labelled all argument and adjuncts occurring in our treebank for 3 Basque verbs. The result of this study has been very positive, and has produced a methodology adapted to the characteristics of the language and the Basque dependency treebank. Another goal of this study was to study whether semi-automatic tagging was possible. The idea is to present the human taggers a pre-tagged version of the corpus. We have seen that many arguments could be automatically tagged with high precision, given only the verbal entries for the verbs and a handful of examples."
agirre-etal-2006-methodology,A methodology for the joint development of the {B}asque {W}ord{N}et and Semcor,2006,17,8,1,1,8824,eneko agirre,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes the methodology adopted to jointly develop the Basque WordNet and a hand annotated corpora (the Basque Semcor). This joint development allows for better motivated sense distinctions, and a tighter coupling between both resources. The methodology involves edition, tagging and refereeing tasks. We are currently half way through the nominal part of the 300.000 word corpus (roughly equivalent to a 500.000 word corpus for English). We present a detailed description of the task, including the main criteria for difficult cases in the edition of the senses and the tagging of the corpus, with special mention to multiword entries. Finally we give a detailed picture of the current figures, as well as an analysis of the agreement rates."
W04-3204,Unsupervised {WSD} based on Automatically Retrieved Examples: The Importance of Bias,2004,17,56,1,1,8824,eneko agirre,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,"This paper explores the large-scale acquisition of sense-tagged examples for Word Sense Disambiguation (WSD). We have applied the xe2x80x9cWordNet monosemous relativesxe2x80x9d method to construct automatically a web corpus that we have used to train disambiguation systems. The corpus-building process has highlighted important factors, such as the distribution of senses (bias). The corpus has been used to train WSD algorithms that include supervised methods (combining automatic and manuallytagged examples), minimally supervised (requiring sense bias information from hand-tagged corpora), and fully unsupervised. These methods were tested on the Senseval-2 lexical sample test set, and compared successfully to other systems with minimum or no supervision."
W04-0801,The {B}asque lexical-sample task,2004,4,4,1,1,8824,eneko agirre,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,"In this paper we describe the Senseval 3 Basque lexical sample task. The task comprised 40 words (15 nouns, 15 verbs and 10 adjectives) selected from the Basque WordNet. 10 of the words were chosen in coordination with other lexical-sample tasks. The examples were taken from newspapers, an in-house balanced corpus and Internet texts. We additionally included a large set of untagged examples, and a lemmatised version of the data including lemma, PoS and case information. The method used to hand-tag the examples produced an inter-tagger agreement of 78.2% before arbitration. The eight competing systems attained results well above the most frequent baseline and the best system from Swarthmore College scored 70.4% recall."
W04-0813,The {B}asque Country University system: {E}nglish and {B}asque tasks,2004,10,24,1,1,8824,eneko agirre,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,"Our group participated in the Basque and English lexical sample tasks in Senseval-3. A language-specific feature set was defined for Basque. Four different learning algorithms were applied, and also a method that combined their outputs. Before submission, the performance of the methods was tested for each task on the Senseval-3 training data using cross validation. Finally, two systems were submitted for each language: the best single algorithm and the best ensemble."
W04-0861,The {``}Meaning{''} system on the {E}nglish all-words task,2004,0,4,3,0,49095,luis villarejo,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,None
atserias-etal-2004-cross,Cross-Language Acquisition of Semantic Models for Verbal Predicates,2004,11,2,4,0.714286,36866,jordi atserias,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper presents a semantic-driven methodology for the automatic acquisition of verbal models. Our approach relies strongly on the semantic generalizations allowed by already existing resources (e.g. Domain labels, Named Entity categories, concepts in the SUMO ontology, etc). Several experiments have been carried out using comparable corpora in four languages (Italian, Spanish, Basque and English) and two domains (FINANCE and SPORT) showing that the semantic patterns acquired can be general enough to be ported from one language to the other language."
martinez-agirre-2004-effect,The Effect of Bias on an Automatically-built Word Sense Corpus,2004,9,6,2,1,13902,david martinez,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The goal of this paper is to explore the large-scale automatic acquisition of sense-tagged examples to be used for Word Sense Disambiguation (WSD). We have applied the xe2x80x9cmonosemous relativesxe2x80x9d method on the Web in order to build such a resource for all nouns in WordNet. The analysis of some parameters revealed that the distribution of the word senses (bias) in the training and test corpus is a determinant factor. Provided there is a method to approximate the bias for each word sense, the results we obtained for English are comparable to the use of hand-tagged data (Semcor), which is a very interesting perspective for lesser studied languages."
agirre-etal-2004-exploring,Exploring Portability of Syntactic Information from {E}nglish to {B}asque,2004,5,4,1,1,8824,eneko agirre,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper explores a crosslingual approach to the PP attachment problem. We built a large dependency database for English based on an automatic parse of the BNC, and Reuters (sports and finances sections). The Basque attachment decisions are taken based on the occurrence frequency of the translations of the Basque (verb-noun) pairs in the English syntactic database. The results show that with this simple technique it is possible to transfer syntactic information from a language like English in order to make PP attachment decisions in another language, in this case Basque. 1 Authors listed in alphabetical order. Introduction & Motivation This work is comprised in a broader endeavor in the context of the MEANING project (Rigau et al., 2002), with the goal of exploring the possibility of porting linguistic knowledge acquired in one language to another. This portability issue could be especially relevant for minority languages with few resources like Basque. Hence the main motivation underlying this experiment is to explore ways to overcome the limitations originated by the lack of resources. If we were able to transfer some of the linguistic knowledge available for English to other languages we would effectively reduce some of the restrictions in these languages (small corpora, lack of hand annotated corpora, etc.). Cross-language information transfer is not something new, however most of the work done relies on the usage of parallel corpora (Hwa et al 2002), which are difficult to find, specially for lesser studied languages. This is one of the reasons that lead us to consider the usage of comparable corpora, since it is easier to obtain. Another noteworthy aspect is the pair of languages selected for the experiment: English and Basque. Hypothetically, these two languages are linguistically distant enough to make this work extensible to any other language pair. The following could be a short characterization of the most relevant differences between the two languages: ??English is a head initial language with an SVO word order, while Basque is a head final free word order language. ??English does not show strong morphology, while Basque does. ??English is not a pro-drop language, and Basque is a three-way pro-drop language. ??English and Basque do not belong to the same typological family. We chose the PP attachment problem in order to explore the portability issue. This problem is especially hard for free word order languages like Basque. Our current partial parser makes attachment decisions based on certain rules and heuristics. Our experiment has been devised to transfer attachment information coming from English parsed data making the attachment decisions for Basque based on this transferred information. The basic idea behind the system presented here is that verbs show certain preferences on the nouns they appear with. Therefore, if we have a sentence with two verbs, and some noun phrases, one of the verbs will show higher preference for some of the noun phrases while the other verb will show higher preference for the others. We will make one assumption beyond this basic idea, the assumption being that these preferences happen and to some extent can be transferred cross-linguistically (Agirre et al. 2003). Note that this is a preliminary work so at this point we aim to keep the system as simple as possible. Thus, higher co-occurrence of the verb and a noun will be taken to be higher preference of that verb over that noun. The results obtained suggest that cross language transferring of knowledge acquired from comparable corpora, is worth pursuing. Even employing a very simple machinery, results seem very promising. Outline of the method Our starting point was the Basque parser described in (Aldezabal et al 2000). This parser uses a unification grammar to build syntactic structures. Having a sentence it chunks it into phrases, finds the head of each phrase and then applying certain rules and heuristics tries to link those heads to the different verbs belonging to the sentence. To test our attachment system, we selected sentences with two verbs, and used the Basque parser to obtain information about the chunks in the sentences. The attachment information provided by the parser is discarded, maintaining only the chunking information. The heads of the noun groups are extracted, and a set of all possible syntactically dependent (verb-noun) pairs are constructed. The goal was to select for each noun which verb should it be attached to from the two possibilities. The method works as follows. We first obtain from the Basque sentence the verbs and surrounding heads. We translate them into English using a bilingual dictionary, and for each (verb-noun) Basque pair we search all possible translation combinations in the dependency database built from an automatically parsed English corpus. Take for example this Basque sentence, Lendakariak hautezkundeak irabazi zituen botoen %60 lortuz inbersoreen artean. The president won the election obtaining 60% of the votes among the investors. The verbs and heads obtained by the Basque parser/chunker are the following: NP-ergative(lendakaria) NP-absolutive(elections) PPabsolutive(boto) PP-distributive(Inbertsore) V1(irabazi) V2(lortu) We translate all the nouns and verbs. NP-ergative(lendakaria): President, chairman (ncsubj) NP-absolutive(elections): poll, election Ppabsolutive(boto): vote,vow PP-distributive(Inbertsore): investor, shareholder V1(irabazi): to win, to earn, to gain V2(lortu): to get, to obtain, to attain All possible English noun-verb pairs are created with the corresponding English relation or preposition for each Basque case, for example for lendakari-irabazi vs. lendakari lortu: win-President-ncsubj get-President-ncsubj earn-President-ncsubj obtain-President-ncsubj gain-President-ncsubj vs. attain-President-ncsubj win-President-ncsubj get-President-ncsubj earn-President-ncsubj obtain-President-ncsubj gain-President-ncsubj attain-President-ncsubj Note that we only search for the English verb and noun translations occurring in a direct syntactic dependency (moreover, we search for an English syntactic dependency equivalent to the Basque one). We collect and add the frequencies of all translated English pairs for each (verbnoun) Basque pair. In order to select the correct attachment for each noun, the mutual information of the two (verb-noun) pairs are compared. This way we normalize over the amount of translations, and also over the occurrences of the English translations in the target corpus. P(any-EVT, any-ENT) MI(BV,BN)= log P(any-EVT)*P(any-ENT) P(any-EVT, any-ENT) corresponds to the probability of finding any translation of the Basque verb with any translation of the Basque noun in the English corpus. P(any-EVT) corresponds to the probability of finding any translation of the Basque verb in the English corpus, and P(any-ENT) corresponds to the probability of finding any translation of the Basque noun in the English corpus. A higher Mutual information value (maintaining the same syntactic relation in both languages) is taken as an indicative of a stronger preference between the head and one of the verbs, the one that will be selected. As mentioned above, we intended to keep the same syntactic relation across both languages when searching. For that, we employed the information provided by the Basque morphological case attached to each noun as an indicative of this relation. There is an equivalence between Basque morphological cases and English prepositions. This equivalence is not one-to-one, thus each Basque case will have several English prepositions as possible translations, and the opposite. Bilingual dictionaries do not contain such information, so we used the equivalence table described in (Lersundi et al 2002). In this equivalence table all possibilities are listed, even low frequency and rare ones. The RASP parser does not incorporate exhaustive information about multiwords, and therefore we included a heuristic method to search for them. So for example, the Basque verb bilatu is translated as xe2x80x9clook upxe2x80x9d in English. In xe2x80x9clook up in the dictionaryxe2x80x9d, we would like to have a dependency between xe2x80x9clook upxe2x80x9d and dictionary. The parser will find that dictionary is a dependent of look , through the preposition in, and up will appear as a particle of look in another relation. The heuristic applied consists of searching for the pair look-dictionary related through the preposition in, and checking that up also appears as a particle of look in the same sentence. Still, certain multiwords need more complex processing. For instance the Basque verb garestitu is a result of an incorporation process and it is translated as xe2x80x9cto make more expensivexe2x80x9d. At this point we are not treating such multiword translations, and they would return a 0 frequency on the search."
agirre-de-lacalle-2004-publicly,Publicly Available Topic Signatures for all {W}ord{N}et Nominal Senses,2004,10,55,1,1,8824,eneko agirre,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Topic signatures are context vectors built for word senses and concepts. They can be automatically acquired from the web for any concept hierarchy using the xe2x80x9cmonosemous relativexe2x80x9d method. Topic signatures have been shown to be useful in Word Sense Disambiguation, for modeling similarity between word senses, classifying new terms in hierarchies and also building hierarchical clusters of word senses for a given word. In this work we present a publicly available resource which comprises both automatically extracted examples for all WordNet 1.6 noun senses and topic signatures built based on those examples. We gathered around 700 sentences per each noun in WordNet. When the monosemous relatives are used to build a sense corpus for polysemous words, they comprise an average of around 3,500 sentences per word sense. The size of the topic signatures thus constructed is of around 4,500 words per word sense."
W02-1304,{MEANING}: a Roadmap to Knowledge Technologies,2002,30,30,3,0.930759,6129,german rigau,{COLING}-02: A Roadmap for Computational Linguistics,0,"Knowledge Technologies need to extract knowledge from existing texts, which calls for advanced Human Language Technologies (HLT). Progress is being made in Natural Language Processing but there is still a long way towards Natural Language Understanding. An important step towards this goal is the development of technologies and resources that deal with concepts rather than words. The MEANING project argues that we need to solve two complementary and intermediate tasks to enable the next generation of intelligent open domain HLT application systems: Word Sense Disambiguation and large-scale enrichment of Lexical Knowledge Bases. Innovations in this area will lead to HLT with deeper understanding of texts, and immediate progress in real applications of Knowledge Technologies."
W02-0801,A Multilingual Approach to Disambiguate Prepositions and Case Suffixes,2002,10,3,1,1,8824,eneko agirre,Proceedings of the {ACL}-02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,0,"This paper presents preliminary experiments in the use of translation equivalences to disambiguate prepositions or case suffixes. The core of the method is to find translations of the occurrence of the target preposition or case suffix, and assign the intersection of their set of interpretations. Given a table with prepositions and their possible interpretations, the method is fully automatic. We have tested this method on the occurrences of the Basque instrumental case -z in the definitions of a Basque dictionary, looking for the translations in the definitions from 3 Spanish and 3 English dictionaries. The results have been that we are able to disambiguate with 94.5% accuracy 2.3% of those occurrences (up to 91). The ambiguity is reduced from 7 readings down to 3.1. The results are very encouraging given the simple techniques used, and show great potential for improvement."
C02-1112,Syntactic Features for High Precision Word Sense Disambiguation,2002,15,26,2,1,13902,david martinez,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"This paper explores the contribution of a broad range of syntactic features to WSD: grammatical relations coded as the presence of adjuncts/arguments in isolation or as subcategorization frames, and instantiated grammatical relations between words. We have tested the performance of syntactic features using two different ML algorithms (Decision Lists and AdaBoost) on the Senseval-2 data. Adding syntactic features to a basic set of traditional features improves performance, especially for AdaBoost. In addition, several methods to build arbitrarily high accuracy WSD systems are also tried, showing that syntactic features allow for a precision of 86% and a coverage of 26% or 95% precision and 8% coverage."
W01-0703,Learning class-to-class selectional preferences,2001,6,68,1,1,8824,eneko agirre,Proceedings of the {ACL} 2001 Workshop on Computational Natural Language Learning ({C}on{LL}),0,"Selectional preference learning methods have usually focused on word-to-class relations, e.g., a verb selects as its subject a given nominal class. This papers extends previous statistical models to class-to-class preferences, and presents a model that learns selectional preferences for classes of verbs. The motivation is twofold: different senses of a verb may have different preferences, and some classes of verbs can share preferences. The model is tested on a word sense disambiguation task which uses subject-verb and object-verb relationships extracted from a small sense-disambiguated corpus."
S01-1002,The {B}asque Task: Did Systems Perform in the Upperbound?,2001,0,2,1,1,8824,eneko agirre,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"In this paper we describe the Senseval 2 Basque lexical-sample task. The task comprised 40 words (15 nouns, 15 verbs and 10 adjectives) selected from Euskal Hiztegia, the main Basque dictionary. Most examples were taken from the Egunkaria newspaper. The method used to hand-tag the examples produced low inter-tagger agreement (75%) before arbitration. The four competing systems attained results well above the most frequent baseline and the best system scored 75% precision at 100% coverage. The paper includes an analysis of the tagging procedure used, as well as the performance of the competing systems. In particular, we argue that inter-tagger agreement is not a real upperbound for the Basque WSD task."
S01-1028,Decision Lists for {E}nglish and {B}asque,2001,5,6,2,1,13902,david martinez,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"In this paper we describe the systems we developed for the English (lexical and all-words) and Basque tasks. They were all supervised systems based on Yarowsky's Decision Lists. We used Semcor for training in the English all-words task. We defined different feature sets for each language. For Basque, in order to extract all the information from the text, we defined features that have not been used before in the literature, using a morphological analyzer. We also implemented systems that selected automatically good features and were able to obtain a prefixed precision (85%) at the cost of coverage. The systems that used all the features were identified as BCU-ehu-dlist-all and the systems that selected some features as BCU-ehu-dlist-best."
W00-1702,Exploring Automatic Word Sense Disambiguation with Decision Lists and the Web,2000,18,89,1,1,8824,eneko agirre,Proceedings of the {COLING}-2000 Workshop on Semantic Annotation and Intelligent Content,0,"The most effective paradigm for word sense disambiguation, supervised learning, seems to be stuck because of the knowledge acquisition bottleneck. In this paper we take an in-depth study of the performance of decision lists on two publicly available corpora and an additional corpus automatically acquired from the Web, using the fine-grained highly polysemous senses in WordNet. Decision lists are shown a versatile state-of-the-art technique. The experiments reveal, among other facts, that SemCor can be an acceptable (0.7 precision for polysemous words) starting point for an all-words system. The results on the DSO corpus show that for some highly polysemous words 0.7 precision seems to be the current state-of-the-art limit. On the other hand, independently constructed hand-tagged corpora are not mutually useful, and a corpus automatically acquired from the Web is shown to fail."
W00-1326,One Sense per Collocation and Genre/Topic Variations,2000,16,41,2,1,13902,david martinez,2000 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,0,"This paper revisits the one sense per collocation hypothesis using fine-grained sense distinctions and two different corpora. We show that the hypothesis is weaker for fine-grained sense distinctions (70% vs. 99% reported earlier on 2-way ambiguities). We also show that one sense per collocation does hold across corpora, but that collocations vary from one corpus to the other, following genre and topic variations. This explains the low results when performing word sense disambiguation across corpora. In fact, we demonstrate that when two independent corpora share a related genre/topic, the word sense disambiguation results would be better. Future work on word sense disambiguation will have to take into account genre and topic as important parameters on their models."
P98-2181,Building Accurate Semantic Taxonomies from Monolingual {MRD}s,1998,13,30,3,1,6129,german rigau,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"This paper presents a method that conbines a set of unsupervised algorithms in order to accurately build large taxonomies from any machine-readable dictionary (MRD). Our aim is to profit from conventional MRDs, with no explicit semantic coding. We propose a system that 1) performs fully automatic extraction of taxonomic links from MRD entries and 2) ranks the extracted relations in a way that selective manual refinement is allowed. Tested accuracy can reach around 100% depending on the degree of coverage selected, showing that taxonomy building is not limited to structured dictionaries such as LDOCE."
P98-1003,Towards a Single Proposal in Spelling Correction,1998,7,19,1,1,8824,eneko agirre,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"The study presented here relies on the integrated use of different kinds of knowledge in order to improve first-guess accuracy in non-word context-sensitive correction for general unrestricted text. State of the art spelling correction systems, e.g. ispell, apart from detecting spelling errors, also assist the user by offering a set of candidate corrections that are close to the misspelled word. Based on the correction proposals of ispell, we built several guessers, which were combined in different ways. Firstly, we evaluated all possibilities and selected the best ones in a corpus with artificially generated typing errors. Secondly, the best combinations were tested on texts with genuine spelling errors. The results for the latter suggest that we can expect automatic non-word correction for all the errors in a free running text with 80% precision and a single proposal 98% of the times (1.02 proposals on average)."
C98-2176,Building Accurate Semantic Taxonomies Monolingual {MRD}s,1998,13,30,3,1,6129,german rigau,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"This paper presents a method that conbines a set of unsupervised algorithms in order to accurately build large taxonomies from any machine-readable dictionary (MRD). Our aim is to profit from conventional MRDs, with no explicit semantic coding. We propose a system that 1) performs fully automatic extraction of taxonomic links from MRD entries and 2) ranks the extracted relations in a way that selective manual refinement is allowed. Tested accuracy can reach around 100% depending on the degree of coverage selected, showing that taxonomy building is not limited to structured dictionaries such as LDOCE."
C98-1003,Towards a single proposal in spelling correction,1998,7,19,1,1,8824,eneko agirre,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"The study presented here relies on the integrated use of different kinds of knowledge in order to improve first-guess accuracy in non-word context-sensitive correction for general unrestricted text. State of the art spelling correction systems, e.g. ispell, apart from detecting spelling errors, also assist the user by offering a set of candidate corrections that are close to the misspelled word. Based on the correction proposals of ispell, we built several guessers, which were combined in different ways. Firstly, we evaluated all possibilities and selected the best ones in a corpus with artificially generated typing errors. Secondly, the best combinations were tested on texts with genuine spelling errors. The results for the latter suggest that we can expect automatic non-word correction for all the errors in a free running text with 80% precision and a single proposal 98% of the times (1.02 proposals on average)."
P97-1007,Combining Unsupervised Lexical Knowledge Methods for Word Sense Disambiguation,1997,23,62,3,1,6129,german rigau,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"This paper presents a method to combine a set of unsupervised algorithms that can accurately disambiguate word senses in a large, completely untagged corpus. Although most of the techniques for word sense resolution have been presented as stand-alone, it is our belief that full-fledged lexical ambiguity resolution should combine several information sources and techniques. The set of techniques have been applied in a combined way to disambiguate the genus terms of two machine-readable dictionaries (MRD), enabling us to construct complete taxonomies for Spanish and French. Texted accuracy is above 80% overall and 95% for two-way ambiguous genus terms, showing that texonomy building is not limited to structured dictionaries such as LDOCE."
C96-1005,Word Sense Disambiguation using Conceptual Density,1996,17,372,1,1,8824,eneko agirre,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"This paper present a method for the resolution of lexical ambiguity of nouns and its automatic evaluation over the Brown Corpus. The method relies on the use of the wide-coverage noun taxonomy of WordNet and the notion of conceptual distance among concepts, captured by a Conceptual Density formula developed for this purpose. This fully automatic method requires no hand coding of lexical entries, hand tagging of text nor any kind of training process. The results of the experiments have been automatically evaluted against SemCor, the sense-tagged version of the Brown Corpus."
