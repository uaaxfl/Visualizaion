Y18-1006,Towards an Automatic Text Comprehension for the {A}rabic Question-Answering: Semantic and Logical Representation of Texts,2018,0,0,2,0,27467,wided bakari,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
S18-1081,{LIS} at {S}em{E}val-2018 Task 2: Mixing Word Embeddings and Bag of Features for Multilingual Emoji Prediction,2018,0,0,3,1,5289,gael guibon,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"In this paper we present the system submitted to the SemEval2018 task2 : Multilingual Emoji Prediction. Our system approaches both languages as being equal by first; considering word embeddings associated to automatically computed features of different types, then by applying bagging algorithm RandomForest to predict the emoji of a tweet."
2018.jeptalnrecital-deft.1,{DEFT}2018 : recherche d{'}information et analyse de sentiments dans des tweets concernant les transports en {{\\^I}}le de {F}rance ({DEFT}2018 : Information Retrieval and Sentiment Analysis in Tweets about Public Transportation in {{\\^I}}le de {F}rance Region ),2018,-1,-1,3,0,5615,patrick paroubek,"Actes de la Conf{\\'e}rence TALN. Volume 2 - D{\\'e}monstrations, articles des Rencontres Jeunes Chercheurs, ateliers DeFT",0,"Cet article pr{\'e}sente l{'}{\'e}dition 2018 de la campagne d{'}{\'e}valuation DEFT (D{\'e}fi Fouille de Textes). A partir d{'}un corpus de tweets, quatre t{\^a}ches ont {\'e}t{\'e} propos{\'e}es : identifier les tweets sur la th{\'e}matique des transports, puis parmi ces derniers, identifier la polarit{\'e} (n{\'e}gatif, neutre, positif, mixte), identifier les marqueurs de sentiment et la cible, et enfin, annoter compl{\`e}tement chaque tweet en source et cible des sentiments exprim{\'e}s. Douze {\'e}quipes ont particip{\'e}, majoritairement sur les deux premi{\`e}res t{\^a}ches. Sur l{'}identification de la th{\'e}matique des transports, la micro F-mesure varie de 0,827 {\`a} 0,908. Sur l{'}identification de la polarit{\'e} globale, la micro F-mesure varie de 0,381 {\`a} 0,823."
2018.jeptalnrecital-court.38,De l{'}usage r{\\'e}el des emojis {\\`a} une pr{\\'e}diction de leurs cat{\\'e}gories (From Emoji Usage to Emoji-Category Prediction),2018,-1,-1,3,1,5289,gael guibon,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"L{'}utilisation des emojis dans les messageries sociales n{'}a eu de cesse d{'}augmenter ces derni{\`e}res ann{\'e}es. Plusieurs travaux r{\'e}cents ont port{\'e} sur la pr{\'e}diction d{'}emojis afin d{'}{\'e}pargner {\`a} l{'}utillisateur le parcours de librairies d{'}emojis de plus en plus cons{\'e}quentes. Nous proposons une m{\'e}thode permettant de r{\'e}cup{\'e}rer automatiquement les cat{\'e}gories d{'}emojis {\`a} partir de leur contexte d{'}utilisation afin d{'}am{\'e}liorer la pr{\'e}diction finale. Pour ce faire nous utilisons des plongements lexicaux en consid{\'e}rant les emojis comme des mots pr{\'e}sents dans des tweets. Nous appliquons ensuite un regroupement automatique restreint aux emojis visages afin de v{\'e}rifier l{'}ad{\'e}quation des r{\'e}sultats avec la th{\'e}orie d{'}Ekman. L{'}approche est reproductible et applicable sur tous types d{'}emojis, ou lorsqu{'}il est n{\'e}cessaire de pr{\'e}dire de nombreuses classes."
S17-2120,{LSIS} at {S}em{E}val-2017 Task 4: Using Adapted Sentiment Similarity Seed Words For {E}nglish and {A}rabic Tweet Polarity Classification,2017,0,5,3,1,30981,amal htait,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"We present, in this paper, our contribution in SemEval2017 task 4 : {``}Sentiment Analysis in Twitter{''}, subtask A: {``}Message Polarity Classification{''}, for English and Arabic languages. Our system is based on a list of sentiment seed words adapted for tweets. The sentiment relations between seed words and other terms are captured by cosine similarity between the word embedding representations (word2vec). These seed words are extracted from datasets of annotated tweets available online. Our tests, using these seed words, show significant improvement in results compared to the use of Turney and Littman{'}s (2003) seed words, on polarity classification of tweet messages."
2017.jeptalnrecital-demo.7,Une plateforme de recommandation automatique d{'}emojis (An emoji recommandation platform),2017,-1,-1,3,1,5289,gael guibon,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 3 - D{\\'e}monstrations,0,Nous pr{\'e}sentons une interface de recommandation d{'}emojis porteurs de sentiments qui utilise un mod{\`e}le de pr{\'e}diction appris sur des messages informels priv{\'e}s. Chacun {\'e}tant associ{\'e} {\`a} deux scores de polarit{\'e} pr{\'e}dits. Cette interface permet permet {\'e}galement d{'}enregistrer les choix de l{'}utilisateur pour confirmer ou infirmer la recommandation.
S16-1076,{LSIS} at {S}em{E}val-2016 Task 7: Using Web Search Engines for {E}nglish and {A}rabic Unsupervised Sentiment Intensity Prediction,2016,12,9,3,1,30981,amal htait,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"In this paper, we present our contribution in SemEval2016 task7 1 : Determining Sentiment Intensity of English and Arabic Phrases, where we use web search engines for English and Arabic unsupervised sentiment intensity prediction. Our work is based, first, on a group of classic sentiment lexicons (e.g. Sen-timent140 Lexicon, SentiWordNet). Second, on web search engines' ability to find the co-occurrence of sentences with predefined negative and positive words. The use of web search engines (e.g. Google Search API) enhance the results on phrases built from opposite polarity terms."
L16-1576,Bilbo-Val: Automatic Identification of Bibliographical Zone in Papers,2016,0,0,3,1,30981,amal htait,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper, we present the automatic annotation of bibliographical references{'} zone in papers and articles of XML/TEI format. Our work is applied through two phases: first, we use machine learning technology to classify bibliographical and non-bibliographical paragraphs in papers, by means of a model that was initially created to differentiate between the footnotes containing or not containing bibliographical references. The previous description is one of BILBO{'}s features, which is an open source software for automatic annotation of bibliographic reference. Also, we suggest some methods to minimize the margin of error. Second, we propose an algorithm to find the largest list of bibliographical references in the article. The improvement applied on our model results an increase in the model{'}s efficiency with an Accuracy equal to 85.89. And by testing our work, we are able to achieve 72.23{\%} as an average for the percentage of success in detecting bibliographical references{'} zone."
S15-2095,{L}sislif: Feature Extraction and Label Weighting for Sentiment Analysis in {T}witter,2015,16,23,2,1,32337,hussam hamdan,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes our sentiment analysis systems which have been built for SemEval2015 Task 10 Subtask B and E. For subtask B, a Logistic Regression classifier has been trained after extracting several groups of features including lexical, syntactic, lexiconbased, Z score and semantic features. A weighting schema has been adapted for positive and negative labels in order to take into account the unbalanced distribution of tweets between the positive and negative classes. This system is ranked third over 40 participants, it achieves average F1 64.27 on Twitter data set 2015 just 0.57% less than the first system. We also present our participation in Subtask E in which our system has got the second rank with Kendall metric but the first one with Spearman for ranking twitter terms according to their association with the positive sentiment."
S15-2128,{L}sislif: {CRF} and Logistic Regression for Opinion Target Extraction and Sentiment Polarity Analysis,2015,19,20,2,1,32337,hussam hamdan,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes our contribution in Opinion Target Extraction OTE and Sentiment Polarity sub tasks of SemEval 2015 ABSA task. A CRF model with IOB notation has been adopted for OTE with several groups of features including syntactic, lexical, semantic, sentiment lexicon features. Our submission for OTE is ranked fifth over twenty submissions. A Logistic Regression model with a weighting schema of positive and negative labels have been used for sentiment polarity; several groups of features (lexical, syntactic, semantic, lexicon and Z score) are extracted. Our submission for Sentiment Polarity is ranked third over ten submissions on the restaurant data set, third over thirteen on the laptops data set, but the first over eleven on the hotel data set that is out-of-domain set."
S14-2104,Supervised Methods for Aspect-Based Sentiment Analysis,2014,15,4,2,1,32337,hussam hamdan,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"In this paper, we present our contribution in SemEval2014 ABSA task, some supervised methods for Aspect-Based Sentiment Analysis of restaurant and laptop reviews are proposed, implemented and evaluated. We focus on determining the aspect terms existing in each sentence, finding out their polarities, detecting the categories of the sentence and the polarity of each category. The evaluation results of our proposed methods exhibit a significant improvement in terms of accuracy and f-measure over all four subtasks regarding to the baseline proposed by SemEval organisers."
S14-2113,The Impact of Z{\\_}score on {T}witter Sentiment Analysis,2014,22,3,2,1,32337,hussam hamdan,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"Twitter has become more and more an important resource of user-generated data. Sentiment Analysis in Twitter is interesting for many applications and objectives. In this paper, we propose to exploit some features which can be useful for this task; the main contribution is the use of Z-scores as features for sentiment classification in addition to pre-polarity and POS tags features. Our experiments have been evaluated using the test data provided by SemEval 2013 and 2014. The evaluation demonstrates that Z_scores features can significantly improve the prediction performance."
benkoussas-etal-2014-collection,A Collection of Scholarly Book Reviews from the Platforms of electronic sources in Humanities and Social Sciences {O}pen{E}dition.org,2014,10,0,3,0,39895,chahinez benkoussas,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we present our contribution for the automatic construction of the Scholarly Book Reviews corpora from two different sources, the OpenEdition platform which is dedicated to electronic resources in the humanities and social sciences, and the Web. The main target is the collect of reviews in order to provide automatic links between each review and its potential book in the future. For these purposes, we propose different document representations and we apply some supervised approaches for binary genre classification before evaluating their impact."
F14-2021,Impact of the nature and size of the training set on performance in the automatic detection of named entities (Impact de la nature et de la taille des corpus d{'}apprentissage sur les performances dans la d{\\'e}tection automatique des entit{\\'e}s nomm{\\'e}es) [in {F}rench],2014,0,0,3,0,40012,anais ollagnier,Proceedings of TALN 2014 (Volume 2: Short Papers),0,None
S13-2075,"Experiments with {DB}pedia, {W}ord{N}et and {S}enti{W}ord{N}et as resources for sentiment analysis in micro-blogging",2013,19,25,3,1,32337,hussam hamdan,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"Sentiment Analysis in Twitter has become an important task due to the huge user-generated content published over such media. Such analysis could be useful for many domains such as Marketing, Finance, Politics, and Social. We propose to use many features in order to improve a trained classifier of Twitter messages; these features extend the feature vector of uni-gram model by the concepts extracted from DBpedia, the verb groups and the similar adjectives extracted from WordNet, the Sentifeatures extracted using SentiWordNet and some useful domain specific features. We also built a dictionary for emotion icons, abbreviation and slang words in tweets which is useful before extending the tweets with different features. Adding these features has improved the f-measure accuracy 2% with SVM and 4% with NaiveBayes."
P13-2027,Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?,2013,17,7,3,1,41401,romain deveaud,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. More, the semantic coherence of the topics has never been considered in this field. We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. We perform a first experimental evaluation using two major TREC test collections. Results show that retrieval performances tend to be better when using topics with higher semantic coherence."
kim-etal-2012-annotated,Annotated Bibliographical Reference Corpora in Digital Humanities,2012,10,4,2,0,28771,youngmin kim,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper, we present new bibliographical reference corpora in digital humanities (DH) that have been developed under a research project, Robust and Language Independent Machine Learning Approaches for Automatic Annotation of Bibliographical References in DH Books supported by Google Digital Humanities Research Awards. The main target is the bibliographical references in the articles of Revues.org site, an oldest French online journal platform in DH field. Since the final object is to provide automatic links between related references and articles, the automatic recognition of reference fields like author and title is essential. These fields are therefore manually annotated using a set of carefully defined tags. After providing a full description of three corpora, which are separately constructed according to the difficulty level of annotation, we briefly introduce our experimental results on the first two corpora. A popular machine learning technique, Conditional Random Field (CRF) is used to build a model, which automatically annotates the fields of new references. In the experiments, we first establish a standard for defining features and labels adapted to our DH reference data. Then we show our new methodology against less structured references gives a meaningful result."
F12-2043,Combinaison de ressources g{\\'e}n{\\'e}rales pour une contextualisation implicite de requ{\\^e}tes (Query Contextualization and Reformulation by Combining External Corpora) [in {F}rench],2012,-1,-1,2,1,41401,romain deveaud,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
2011.jeptalnrecital-demonstration.18,Restad : un logiciel d{'}indexation et de stockage relationnel de contenus {XML} (Restad: an indexing and relational storing software for {XML} content),2011,-1,-1,3,0,44940,yoann moreau,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. D{\\'e}monstrations,0,
2011.jeptalnrecital-court.11,Ajout d{'}informations contextuelles pour la recherche de passages au sein de Wikip{\\'e}dia (Integrating contextual information for passage retrieval in {W}ikipedia),2011,-1,-1,3,1,41401,romain deveaud,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"La recherche de passages consiste {\`a} extraire uniquement des passages pertinents par rapport {\`a} une requ{\^e}te utilisateur plut{\^o}t qu{'}un ensemble de documents entiers. Cette r{\'e}cup{\'e}ration de passages est souvent handicap{\'e}e par le manque d{'}informations compl{\'e}mentaires concernant le contexte de la recherche initi{\'e}e par l{'}utilisateur. Des {\'e}tudes montrent que l{'}ajout d{'}informations contextuelles par l{'}utilisateur peut am{\'e}liorer les performances des syst{\`e}mes de recherche de passages. Nous confirmons ces observations dans cet article, et nous introduisons {\'e}galement une m{\'e}thode d{'}enrichissement de la requ{\^e}te {\`a} partir d{'}informations contextuelles issues de documents encyclop{\'e}diques. Nous menons des exp{\'e}rimentations en utilisant la collection et les m{\'e}thodes d{'}{\'e}valuation propos{\'e}es par la campagne INEX. Les r{\'e}sultats obtenus montrent que l{'}ajout d{'}informations contextuelles permet d{'}am{\'e}liorer significativement les performances de notre syst{\`e}me de recherche de passages. Nous observons {\'e}galement que notre approche automatique obtient les meilleurs r{\'e}sultats parmi les diff{\'e}rentes approches que nous {\'e}valuons."
2011.jeptalnrecital-court.29,Mesure non-supervis{\\'e}e du degr{\\'e} d{'}appartenance d{'}une entit{\\'e} {\\`a} un type (An unsupervised measure of the degree of belonging of an entity to a type),2011,-1,-1,2,0,44952,ludovic bonnefoy,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"La recherche d{'}entit{\'e}s nomm{\'e}es a {\'e}t{\'e} le sujet de nombreux travaux. Cependant, la construction des ressources n{\'e}cessaires {\`a} de tels syst{\`e}mes reste un probl{\`e}me majeur. Dans ce papier, nous proposons une m{\'e}thode compl{\'e}mentaire aux outils capables de reconna{\^\i}tre des entit{\'e}s de types larges, dont l{'}objectif est de d{\'e}terminer si une entit{\'e} est d{'}un type donn{\'e}, et ce de mani{\`e}re non-supervis{\'e}e et quel que soit le type. Nous proposons pour cela une approche bas{\'e}e sur la comparaison de mod{\`e}les de langage estim{\'e}s {\`a} partir du Web. L{'}int{\'e}r{\^e}t de notre approche est valid{\'e} par une {\'e}valuation sur 100 entit{\'e}s et 273 types diff{\'e}rents."
sitbon-etal-2008-evaluation,Evaluation of Lexical Resources and Semantic Networks on a Corpus of Mental Associations,2008,8,3,2,0.6,34164,laurianne sitbon,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"When a user cannot find a word, he may think of semantically related words that could be used into an automatic process to help him. This paper presents an evaluation of lexical resources and semantic networks for modelling mental associations. A corpus of associations has been constructed for its evaluation. It is composed of 20 low frequency target words each associated 5 times by 20 users. In the experiments we look for the target word in propositions made from the associated words thanks to 5 different resources. The results show that even if each resource has a useful specificity, the global recall is low. An experiment to extract common semantic features of several associations showed that we cannot expect to see the target word below a rank of 20 propositions."
sitbon-etal-2008-evaluating,Evaluating Robustness Of A {QA} System Through A Corpus Of Real-Life Questions,2008,7,1,2,0.6,34164,laurianne sitbon,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents the sequential evaluation of the question answering system SQuaLIA. This system is based on the same sequential process as most statistical question answering systems, involving 4 main steps from question analysis to answer extraction.The evaluation is based on a corpus made from 20 questions taken in the set of an evaluation campaign and which were well answered by SQuaLIA. Each of the 20 questions has been typed by 17 native participants, non natives and dyslexics. They were vocally instructed the target of each question. Each of the 4 analysis steps of the system involves a loss of accuracy, until an average of 60 of right answers at the end of the process. The main cause of this loss seems to be the orthographic mistakes users make on nouns."
2008.jeptalnrecital-long.7,Quelles combinaisons de scores et de crit{\\`e}res num{\\'e}riques pour un syst{\\`e}me de Questions/R{\\'e}ponses ?,2008,-1,-1,2,1,45918,laurent gillard,Actes de la 15{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans cet article, nous pr{\'e}sentons une discussion sur la combinaison de diff{\'e}rents scores et crit{\`e}res num{\'e}riques pour la s{\'e}lection finale d{'}une r{\'e}ponse dans la partie en charge des questions factuelles du syst{\`e}me de Questions/R{\'e}ponses d{\'e}velopp{\'e} au LIA. Ces scores et crit{\`e}res num{\'e}riques sont d{\'e}riv{\'e}s de ceux obtenus en sortie de deux composants cruciaux pour notre syst{\`e}me : celui de s{\'e}lection des passages susceptibles de contenir une r{\'e}ponse et celui d{'}extraction et de s{\'e}lection d{'}une r{\'e}ponse. Ils sont {\'e}tudi{\'e}s au regard de leur expressivit{\'e}. Des comparaisons sont faites avec des approches de s{\'e}lection de passages mettant en oeuvre des scores conventionnels en recherche d{'}information. Parall{\`e}lement, l{'}influence de la taille des contextes (en nombre de phrases) est {\'e}valu{\'e}e. Cela permet de mettre en {\'e}vidence que le choix de passages constitu{\'e}s de trois phrases autour d{'}une r{\'e}ponse candidate, avec une s{\'e}lection des r{\'e}ponses bas{\'e}e sur une combinaison entre un score de passage de type Lucene ou Cosine et d{'}un score de compacit{\'e} appara{\^\i}t comme un compromis int{\'e}ressant."
2008.jeptalnrecital-court.15,"Rep{\\'e}rage de citations, classification des styles de discours rapport{\\'e} et identification des constituants citationnels en {\\'e}crits journalistiques",2008,-1,-1,4,0,46598,fabien poulard,Actes de la 15{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Dans le contexte de la recherche de plagiat, le rep{\'e}rage de citations et de ses constituants est primordial puisqu{'}il peut amener {\`a} {\'e}valuer le caract{\`e}re licite ou illicite d{'}une reprise (source cit{\'e}e ou non). Nous proposons ici une comparaison de m{\'e}thodes automatiques pour le rep{\'e}rage de ces informations et rapportons une {\'e}valuation quantitative de celles-ci. Un corpus d{'}{\'e}crits journalistiques fran{\c{c}}ais a {\'e}t{\'e} manuellement annot{\'e} pour nous servir de base d{'}apprentissage et de test."
2007.jeptalnrecital-poster.8,Analyse des {\\'e}checs d{'}une approche pour traiter les questions d{\\'e}finitoires soumises {\\`a} un syst{\\`e}me de questions/r{\\'e}ponses,2007,-1,-1,2,1,45918,laurent gillard,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Cet article revient sur le type particulier des questions d{\'e}finitoires {\'e}tudi{\'e}es dans le cadre des campagnes d{'}{\'e}valuation des syst{\`e}mes de Questions/R{\'e}ponses. Nous pr{\'e}sentons l{'}approche d{\'e}velopp{\'e}e suite {\`a} notre participation {\`a} la campagne EQueR et son {\'e}valuation lors de QA@CLEF 2006. La r{\'e}ponse propos{\'e}e est la plus repr{\'e}sentative des expressions pr{\'e}sentes en apposition avec l{'}objet {\`a} d{\'e}finir, sa s{\'e}lection est faite depuis des indices d{\'e}riv{\'e}s de ces appositions. Environ 80{\%} de bonnes r{\'e}ponses sont trouv{\'e}es sur les questions d{\'e}finitoires des volets francophones de CLEF. Les cas d{'}erreurs rencontr{\'e}s sont analys{\'e}s et discut{\'e}s en d{\'e}tail."
2007.jeptalnrecital-poster.26,Traitements phrastiques phon{\\'e}tiques pour la r{\\'e}{\\'e}criture de phrases dysorthographi{\\'e}es,2007,-1,-1,2,0.5,34164,laurianne sitbon,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Cet article d{\'e}crit une m{\'e}thode qui combine des hypoth{\`e}ses graph{\'e}miques et phon{\'e}tiques au niveau de la phrase, {\`a} l{'}aide d{'}une r{\'e}pr{\'e}sentation en automates {\`a} {\'e}tats finis et d{'}un mod{\`e}le de langage, pour la r{\'e}{\'e}criture de phrases tap{\'e}es au clavier par des dysorthographiques. La particularit{\'e} des {\'e}crits dysorthographi{\'e}s qui emp{\^e}che les correcteurs orthographiques d{'}{\^e}tre efficaces pour cette t{\^a}che est une segmentation en mots parfois incorrecte. La r{\'e}{\'e}criture diff{\`e}re de la correction en ce sens que les phrases r{\'e}{\'e}crites ne sont pas {\`a} destination de l{'}utilisateur mais d{'}un syst{\`e}me automatique, tel qu{'}un moteur de recherche. De ce fait l{'}{\'e}valuation est conduite sur des versions filtr{\'e}es et lemmatis{\'e}es des phrases. Le taux d{'}erreurs mots moyen passe de 51 {\%} {\`a} 20 {\%} avec notre m{\'e}thode, et est de 0 {\%} sur 43 {\%} des phrases test{\'e}es."
sitbon-bellot-2006-tools,Tools and methods for objective or contextual evaluation of topic segmentation,2006,12,2,2,0.5,34164,laurianne sitbon,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this paper we discuss the way of evaluating topic segmentation, from mathematical measures on variously constructed reference corpus to contextual evaluation depending on different topic segmentation usages. We present an overview of the different ways of building reference corpora and of mathematically evaluating segmentation methods, and then we focus on three tasks which may involve a topic segmentation: text extraction, information retrieval and document presentation. We have developed two graphical interfaces, one for an intrinsic comparison, and the other one dedicated to an evaluation in an information retrieval context. These tools will be very soon distributed under GPL licences on the Technolangue project web page."
2006.jeptalnrecital-long.13,"Questions Bool{\\'e}ennes : Oui ou Non, des Questions et des R{\\'e}ponses",2006,-1,-1,2,1,45918,laurent gillard,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans cet article, nous pr{\'e}sentons une approche afin de traiter les questions bool{\'e}ennes, c{'}est-{\`a}-dire des questions dont la r{\'e}ponse peut {\^e}tre un Oui ou un Non, cela, dans le cadre d{'}un syst{\`e}me de Questions-R{\'e}ponses. En effet, la campagne Technolangue-EQueR, premi{\`e}re campagne francophone de Questions-R{\'e}ponses (QR) utilisant des questions et un corpus en fran{\c{c}}ais, a {\'e}galement {\'e}t{\'e} la premi{\`e}re campagne QR {\`a} introduire une {\'e}valuation pour ce type de questions. Nous d{\'e}taillons, parall{\`e}lement {\`a} notre approche, des pistes de r{\'e}flexion sur les aspects sous-jacents {\`a} ces questions bool{\'e}ennes, notamment au travers d{'}une analyse des r{\'e}sultats obtenus par notre syst{\`e}me dans un contexte similaire {\`a} celui de notre participation {\`a} la campagne officielle."
2006.jeptalnrecital-long.31,Vers une pr{\\'e}diction automatique de la difficult{\\'e} d{'}une question en langue naturelle,2006,-1,-1,4,0.5,34164,laurianne sitbon,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous proposons et testons deux m{\'e}thodes de pr{\'e}diction de la capacit{\'e} d{'}un syst{\`e}me {\`a} r{\'e}pondre {\`a} une question factuelle. Une telle pr{\'e}diciton permet de d{\'e}terminer si l{'}on doit initier un dialogue afin de pr{\'e}ciser ou de reformuler la question pos{\'e}e par l{'}utilisateur. La premi{\`e}re approche que nous proposons est une adaptation d{'}une m{\'e}thode de pr{\'e}diction dans le domaine de la recherche documentaire, bas{\'e}e soit sur des machines {\`a} vecteurs supports (SVM) soit sur des arbres de d{\'e}cision, avec des crit{\`e}res tels que le contenu des questions ou des documents, et des mesures de coh{\'e}sion entre les documents ou passages de documents d{'}o{\`u} sont extraits les r{\'e}ponses. L{'}autre approche vise {\`a} utiliser le type de r{\'e}ponse attendue pour d{\'e}cider de la capacit{\'e} du syst{\`e}me {\`a} r{\'e}pondre. Les deux approches ont {\'e}t{\'e} test{\'e}es sur les donn{\'e}es de la campagne Technolangue EQUER des syst{\`e}mes de questions-r{\'e}ponses en fran{\c{c}}ais. L{'}approche {\`a} base de SVM est celle qui obtient les meilleurs r{\'e}sultats. Elle permet de distinguer au mieux les questions faciles, celles auxquelles notre syst{\`e}me apporte une bonne r{\'e}ponse, des questions difficiles, celles rest{\'e}es sans r{\'e}ponses ou auxquelles le syst{\`e}me a r{\'e}pondu de mani{\`e}re incorrecte. A l{'}oppos{\'e} on montre que pour notre syst{\`e}me, le type de r{\'e}ponse attendue (personnes, quantit{\'e}s, lieux...) n{'}est pas un facteur d{\'e}terminant pour la difficult{\'e} d{'}une question."
2005.jeptalnrecital-court.21,Segmentation th{\\'e}matique par cha{\\^\\i}nes lexicales pond{\\'e}r{\\'e}es,2005,-1,-1,2,1,34164,laurianne sitbon,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Cet article propose une m{\'e}thode innovante et efficace pour segmenter un texte en parties th{\'e}matiquement coh{\'e}rentes, en utilisant des cha{\^\i}nes lexicales pond{\'e}r{\'e}es. Les cha{\^\i}nes lexicales sont construites en fonction de hiatus variables, ou bien sans hiatus, ou encore pond{\'e}r{\'e}es en fonction de la densit{\'e} des occurrences du terme dans la cha{\^\i}ne. D{'}autre part, nous avons constat{\'e} que la prise en compte du rep{\'e}rage d{'}entit{\'e}s nomm{\'e}es dans la cha{\^\i}ne de traitement, du moins sans r{\'e}solution des anaphores, n{'}am{\'e}liore pas significativement les performances. Enfin, la qualit{\'e} de la segmentation propos{\'e}e est stable sur diff{\'e}rentes th{\'e}matiques, ce qui montre une ind{\'e}pendance par rapport au type de document."
2004.jeptalnrecital-long.1,Evaluation de m{\\'e}thodes de segmentation th{\\'e}matique lin{\\'e}aire non supervis{\\'e}es apr{\\`e}s adaptation au fran{\\c{c}}ais,2004,-1,-1,2,1,34164,laurianne sitbon,Actes de la 11{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous proposons une {\'e}valuation de diff{\'e}rentes m{\'e}thodes et outils de segmentation th{\'e}matique de textes. Nous pr{\'e}sentons les outils de segmentation lin{\'e}aire et non supervis{\'e}e DotPlotting, Segmenter, C99, TextTiling, ainsi qu{'}une mani{\`e}re de les adapter et de les tester sur des documents fran{\c{c}}ais. Les r{\'e}sultats des tests montrent des diff{\'e}rences en performance notables selon les sujets abord{\'e}s dans les documents, et selon que le nombre de segments {\`a} trouver est fix{\'e} au pr{\'e}alable par l{'}utilisateur. Ces travaux font partie du projet Technolangue AGILE-OURAL."
2003.jeptalnrecital-poster.11,Bases de connaissances pour asseoir la cr{\\'e}dibilit{\\'e} des r{\\'e}ponses d{'}un syst{\\`e}me de {Q}/{R},2003,-1,-1,2,0,45918,laurent gillard,Actes de la 10{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,Cet article pr{\'e}sente un prototype de Question/R{\'e}ponse (Q/R) impliquant un ensemble de bases de connaissances (BC) dont l{'}objectif est d{'}apporter un cr{\'e}dit suppl{\'e}mentaire aux r{\'e}ponses candidates trouv{\'e}es. Ces BC et leur influence sur la strat{\'e}gie d{'}ordonnancement mise en Âuvre sont d{\'e}crites dans le cadre de la participation du syst{\`e}me {\`a} la campagne Q/R de TREC-2002.
