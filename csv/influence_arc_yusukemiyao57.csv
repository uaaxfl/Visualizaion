2009.iwslt-evaluation.15,P08-1023,0,0.0260161,"a2 are word alignments between a source language phrase f1m and a target language phrase en1 ; p(·) and l(·) are phrasal translation probabilities and lexical weights, respectively; wp stands for the word penalty. In order to be used in CKY-style decoding [10], a rule in the form of (1) can be easily transformed into an end-to-end Hiero-style [10] translation rule: (2) And the corresponding synchronous PCFG production takes the form of: X →∑i wi ∗log(pi ) f1m , en1 . (3) It has been proved that the end-to-end phrase table significantly influence the translation result of syntax-based systems [11, 12]. Note that the phrase table mentioned here 2 http://www.statmt.org/moses/ is not constrained to be linguistic phrases, i.e., a phrase inside the table is not necessarily covered by a subtree. This makes the phrase table more flexible to be used than in the tree/forest-to-string systems [11, 12] where additional approaches have to be employed in order to make use of nonlinguistic phrases [13, 14]. Return back to Figure 1, HPSG trees attached with PASs are generated by parsing the target language sentences using Enju. Then, we extract HPSG and PAS based translation rules from the word-aligned a"
2009.iwslt-evaluation.15,P07-1089,0,0.0155645,"hronous PCFG production takes the form of: X →∑i wi ∗log(pi ) f1m , en1 . (3) It has been proved that the end-to-end phrase table significantly influence the translation result of syntax-based systems [11, 12]. Note that the phrase table mentioned here 2 http://www.statmt.org/moses/ is not constrained to be linguistic phrases, i.e., a phrase inside the table is not necessarily covered by a subtree. This makes the phrase table more flexible to be used than in the tree/forest-to-string systems [11, 12] where additional approaches have to be employed in order to make use of nonlinguistic phrases [13, 14]. Return back to Figure 1, HPSG trees attached with PASs are generated by parsing the target language sentences using Enju. Then, we extract HPSG and PAS based translation rules from the word-aligned and target-language-parsed parallel corpora. The HPSG-based xRs (tree-to-string, [15]) translation rules (binarized inversely [16]) are extracted using the GHKM minimal-rule extraction algorithm of [1]. In order to trace the lexical level translation information (similar to PASbased rules (Section 3 and Figure 2)), we remember the endto-end alignments in an HPSG-based translation rule. The ideas a"
2009.iwslt-evaluation.15,N04-1035,0,0.47118,"by Head-driven Phrase Structure Grammar and Predicate-Argument Structures. We report the results of our system on both the development and test sets. 1. Introduction How can we integrate deep syntactic information into current statistical machine translation (SMT) systems to further improve the accuracy and fluency? How to deal with global reordering problem for a language pair that do not share isomorphic syntactic structures? These remain to be essential issues faced by current SMT research community. In this paper, we manage to answer these questions in terms of string-to-tree translation [1, 2, 3]. English, the target language in our case study, is the most popularly researched language with plenty resources and syntactic parsers. In contrast to commit to Probabilistic Context-Free Grammar (PCFG) which only generates shallow trees of English [1, 2, 3], we propose the use of deep parse trees and semantic dependencies described respectively by Head-driven Phrase Structure Grammar (HPSG) [4, 5] and PredicateArgument Structures (PASs). We illustrate two major characteristics that an HPSG tree (used by us) differs from a PCFG tree. First, a node in an HPSG tree is represented by a typed fea"
2009.iwslt-evaluation.15,P06-1121,0,0.0665645,"by Head-driven Phrase Structure Grammar and Predicate-Argument Structures. We report the results of our system on both the development and test sets. 1. Introduction How can we integrate deep syntactic information into current statistical machine translation (SMT) systems to further improve the accuracy and fluency? How to deal with global reordering problem for a language pair that do not share isomorphic syntactic structures? These remain to be essential issues faced by current SMT research community. In this paper, we manage to answer these questions in terms of string-to-tree translation [1, 2, 3]. English, the target language in our case study, is the most popularly researched language with plenty resources and syntactic parsers. In contrast to commit to Probabilistic Context-Free Grammar (PCFG) which only generates shallow trees of English [1, 2, 3], we propose the use of deep parse trees and semantic dependencies described respectively by Head-driven Phrase Structure Grammar (HPSG) [4, 5] and PredicateArgument Structures (PASs). We illustrate two major characteristics that an HPSG tree (used by us) differs from a PCFG tree. First, a node in an HPSG tree is represented by a typed fea"
2009.iwslt-evaluation.15,N09-1025,0,0.211268,"by Head-driven Phrase Structure Grammar and Predicate-Argument Structures. We report the results of our system on both the development and test sets. 1. Introduction How can we integrate deep syntactic information into current statistical machine translation (SMT) systems to further improve the accuracy and fluency? How to deal with global reordering problem for a language pair that do not share isomorphic syntactic structures? These remain to be essential issues faced by current SMT research community. In this paper, we manage to answer these questions in terms of string-to-tree translation [1, 2, 3]. English, the target language in our case study, is the most popularly researched language with plenty resources and syntactic parsers. In contrast to commit to Probabilistic Context-Free Grammar (PCFG) which only generates shallow trees of English [1, 2, 3], we propose the use of deep parse trees and semantic dependencies described respectively by Head-driven Phrase Structure Grammar (HPSG) [4, 5] and PredicateArgument Structures (PASs). We illustrate two major characteristics that an HPSG tree (used by us) differs from a PCFG tree. First, a node in an HPSG tree is represented by a typed fea"
2009.iwslt-evaluation.15,J08-1002,1,0.797575,"e pointer to semantic arguments Table 1: Examples of syntactic/semantic features extracted from HPSG signs that are included in the output of Enju (top and bottom stands for features of phrasal and lexical nodes, respectively). ing translation. The idea proposed in this paper can be considered as a natural integration of syntactic information and semantic dependency information for assisting string-to-tree translation. We call the integration natural here, because the HPSG tree and PAS of an English sentence are generated synchronously by using a state-of-the-art HPSG parser on English, Enju1 [6]. Note that the information available in the output of Enju is a fairly crude approximation of the TFS used in the full HPSG grammar [4, 5] due to practicable considerations [6]. Although the information taken from Enju’s output is much more than the commonly used PCFG parser, the HPSG-based translation rule is still extracted from an approximation of the full HPSG grammar. 2. System Outline 2.1. Parameter Estimation The diagram of parameter estimation in our system is shown in Figure 1, which is similar to most syntax-based SMT sys1 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html Proceed"
2009.iwslt-evaluation.15,J03-1002,0,0.00225715,"ction and estimation, GHKM [1] Phrase-training (Moses [9]) PAS-based translation rules Phrase translation table HPSG-based translation rules Binarizing (Section 4) To Hiero-style (Section 2.1) α1 SRILM [17] 5-gram LM α2 α3 Binarizing (Section 4) α4 Minimum Error Rate Training on the development sets (Z-mert [18]) Figure 1: The parameter estimation and rule combination diagram of our system. tems [1, 7]. Given bilingual parallel corpora, we first tokenize the source and target sentences (e.g., word segmentation of Chinese; punctuation segmentation and lowercase of English). Then, we use GIZA++ [8] and grow-diag-finaland balancing strategy (dealing with unaligned source/target words) [9] on the tokenized parallel corpora to obtain a phrase-aligned parallel corpora. A phrase translation table (PTT) is estimated from the phrase-aligned parallel corpora. We implement the step of phrase table extraction employing the Moses toolkit2 [9]. Recall that the Moses-style phrase translation rule takes the following form: Here, a1 and a2 are word alignments between a source language phrase f1m and a target language phrase en1 ; p(·) and l(·) are phrasal translation probabilities and lexical weights,"
2009.iwslt-evaluation.15,P07-2045,0,0.00823132,"The diagram of parameter estimation in our system is shown in Figure 1, which is similar to most syntax-based SMT sys1 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html Proceedings of IWSLT 2009, Tokyo - Japan Original parallel corpora Target language parsing (Enju [6]) Lexical analyzing Phrase-aligned and target-language-parsed parallel corpora Tokenized parallel corpora Predicate-Argument Structure based translation rule extraction and estimation (Section 3) GIZA++ & balancing Phrase-aligned parallel corpora HPSG translation rule extraction and estimation, GHKM [1] Phrase-training (Moses [9]) PAS-based translation rules Phrase translation table HPSG-based translation rules Binarizing (Section 4) To Hiero-style (Section 2.1) α1 SRILM [17] 5-gram LM α2 α3 Binarizing (Section 4) α4 Minimum Error Rate Training on the development sets (Z-mert [18]) Figure 1: The parameter estimation and rule combination diagram of our system. tems [1, 7]. Given bilingual parallel corpora, we first tokenize the source and target sentences (e.g., word segmentation of Chinese; punctuation segmentation and lowercase of English). Then, we use GIZA++ [8] and grow-diag-finaland balancing strategy (dealing wi"
2009.iwslt-evaluation.15,J07-2003,0,0.634756,"9] on the tokenized parallel corpora to obtain a phrase-aligned parallel corpora. A phrase translation table (PTT) is estimated from the phrase-aligned parallel corpora. We implement the step of phrase table extraction employing the Moses toolkit2 [9]. Recall that the Moses-style phrase translation rule takes the following form: Here, a1 and a2 are word alignments between a source language phrase f1m and a target language phrase en1 ; p(·) and l(·) are phrasal translation probabilities and lexical weights, respectively; wp stands for the word penalty. In order to be used in CKY-style decoding [10], a rule in the form of (1) can be easily transformed into an end-to-end Hiero-style [10] translation rule: (2) And the corresponding synchronous PCFG production takes the form of: X →∑i wi ∗log(pi ) f1m , en1 . (3) It has been proved that the end-to-end phrase table significantly influence the translation result of syntax-based systems [11, 12]. Note that the phrase table mentioned here 2 http://www.statmt.org/moses/ is not constrained to be linguistic phrases, i.e., a phrase inside the table is not necessarily covered by a subtree. This makes the phrase table more flexible to be used than in"
2009.iwslt-evaluation.15,P09-1020,0,0.0220831,"hronous PCFG production takes the form of: X →∑i wi ∗log(pi ) f1m , en1 . (3) It has been proved that the end-to-end phrase table significantly influence the translation result of syntax-based systems [11, 12]. Note that the phrase table mentioned here 2 http://www.statmt.org/moses/ is not constrained to be linguistic phrases, i.e., a phrase inside the table is not necessarily covered by a subtree. This makes the phrase table more flexible to be used than in the tree/forest-to-string systems [11, 12] where additional approaches have to be employed in order to make use of nonlinguistic phrases [13, 14]. Return back to Figure 1, HPSG trees attached with PASs are generated by parsing the target language sentences using Enju. Then, we extract HPSG and PAS based translation rules from the word-aligned and target-language-parsed parallel corpora. The HPSG-based xRs (tree-to-string, [15]) translation rules (binarized inversely [16]) are extracted using the GHKM minimal-rule extraction algorithm of [1]. In order to trace the lexical level translation information (similar to PASbased rules (Section 3 and Figure 2)), we remember the endto-end alignments in an HPSG-based translation rule. The ideas a"
2009.iwslt-evaluation.15,N04-1014,0,0.0309107,"t constrained to be linguistic phrases, i.e., a phrase inside the table is not necessarily covered by a subtree. This makes the phrase table more flexible to be used than in the tree/forest-to-string systems [11, 12] where additional approaches have to be employed in order to make use of nonlinguistic phrases [13, 14]. Return back to Figure 1, HPSG trees attached with PASs are generated by parsing the target language sentences using Enju. Then, we extract HPSG and PAS based translation rules from the word-aligned and target-language-parsed parallel corpora. The HPSG-based xRs (tree-to-string, [15]) translation rules (binarized inversely [16]) are extracted using the GHKM minimal-rule extraction algorithm of [1]. In order to trace the lexical level translation information (similar to PASbased rules (Section 3 and Figure 2)), we remember the endto-end alignments in an HPSG-based translation rule. The ideas are described in [1, 11] in detail. In order to use the dependency structure, we describe a linear-time algorithm based on minimum covering trees to extract PAS-based translation rules (Section 3). SRI Language Modeling (SRILM) toolkit3 [17] is employed to train a 5-gram language model"
2009.iwslt-evaluation.15,N06-1033,0,0.449598,"a phrase inside the table is not necessarily covered by a subtree. This makes the phrase table more flexible to be used than in the tree/forest-to-string systems [11, 12] where additional approaches have to be employed in order to make use of nonlinguistic phrases [13, 14]. Return back to Figure 1, HPSG trees attached with PASs are generated by parsing the target language sentences using Enju. Then, we extract HPSG and PAS based translation rules from the word-aligned and target-language-parsed parallel corpora. The HPSG-based xRs (tree-to-string, [15]) translation rules (binarized inversely [16]) are extracted using the GHKM minimal-rule extraction algorithm of [1]. In order to trace the lexical level translation information (similar to PASbased rules (Section 3 and Figure 2)), we remember the endto-end alignments in an HPSG-based translation rule. The ideas are described in [1, 11] in detail. In order to use the dependency structure, we describe a linear-time algorithm based on minimum covering trees to extract PAS-based translation rules (Section 3). SRI Language Modeling (SRILM) toolkit3 [17] is employed to train a 5-gram language model (LM) on the tokenized target language side w"
2009.iwslt-evaluation.15,P03-1021,0,0.00667254,"lexical level translation information (similar to PASbased rules (Section 3 and Figure 2)), we remember the endto-end alignments in an HPSG-based translation rule. The ideas are described in [1, 11] in detail. In order to use the dependency structure, we describe a linear-time algorithm based on minimum covering trees to extract PAS-based translation rules (Section 3). SRI Language Modeling (SRILM) toolkit3 [17] is employed to train a 5-gram language model (LM) on the tokenized target language side with Kneser-Ney smoothing. Toolkit Z-mert4 [18] is used for Minimum-Error Rate Training (MERT) [19]. 3 http://www.speech.sri.com/projects/srilm/ 4 http://www.cs.jhu.edu/ - 100 - ozaidan/zmert/ Proceedings of IWSLT 2009, Tokyo - Japan 2.2. Rule Combination Since the PTT, the HPSG-based rules, and the PAS-based rules are independently extracted and estimated, the distribution overlapping among them is inevitable. As shown in Figure 1, we use Z-mert to tune their weights on the development sets. The optimal derivation is computed by:   3 ∑  ∑ αi wj 4 d∗ = arg max log pji i (d) + log pα . LM (d)  d∈D  i=1 ji Here, pj1 , pj2 , and pj3 represent the feature subsets from PPT, binarized PAS-b"
2009.iwslt-evaluation.15,W05-1506,0,0.0324405,"3], we define a PASR to be an xRs rule and binarize it in an inverse way [16]. 3.2.1. Definition of PASR 2.3. Decoding We use a CKY-style algorithm with beam-pruning and cubepruning [10] to decode Chinese sentences. For efficient decoding with integrated N-gram LMs, we binarize all translation rules into rules that contain at most two variables and can be incrementally scored by LM [16]. For each source language sentence f , the output of the chart-parsing algorithm is expressed as a hyper-graph representing a set of derivations. Given a hyper-graph for f , we use the Algorithm 3 described in [20] to extract its k-best derivations. Since different derivations may lead to the same target language string e, we further adopt Algorithm 3’s modification (i.e., keep a hash-table to maintain the unique target sentences [21]) to efficiently generate the unique k-best translations. 3. PAS-based Translation Rule Extraction In this section, we first express an example of an HPSG tree attached with PASs, and then describe the data structure and an extraction algorithm of PAS-based translation rules (short as PASR, hereafter). A PASR is a 4-tuple ⟨S, T, A, n⟩, which describes the alignment A betwee"
2009.iwslt-evaluation.15,2006.amta-papers.8,0,0.0203907,"e included in the three subsets. In addition, number of phrases and words are contained in pj1 , and number of rules are included in pj2 and pj3 . three elements in the source and target language sides. In particular, we observe that the “head” of this rule is ignored whose arguments can be generalized into variables. Note that PASs are not only attached to verbs in a sentence, but also to all other words in the sentence. The corresponding PASRs are illustrated in Figure 2 as well. Even apparently similar in data structure, we argue our PASRs are still different from the traditional xRs rules [1, 11, 21], since the knowledge of semantic dependencies are further explicitly employed. We give the formal definitions and a lineartime rule extraction algorithm in the following subsections. 3.2. Definitions Using a strategy similar to most string-to-tree systems [3], we define a PASR to be an xRs rule and binarize it in an inverse way [16]. 3.2.1. Definition of PASR 2.3. Decoding We use a CKY-style algorithm with beam-pruning and cubepruning [10] to decode Chinese sentences. For efficient decoding with integrated N-gram LMs, we binarize all translation rules into rules that contain at most two varia"
2009.iwslt-evaluation.15,P02-1040,0,0.0763328,"Missing"
2020.coling-main.213,D16-1162,0,0.255693,"2018; Aoki et al., 2019). These models generate ﬂuent sentences, but we often observed problematic generated sentences in terms of correctness. As shown in Fig. 1, the word gain is possibly generated, although the word drop or rebound is expected. The terms that express the ﬂuctuation of stock prices are crucial because such errors could reverse the meaning of the sentence in the worst case. Similar issues have been seen in other generation tasks, such as machine translation or summarization. The known solutions are, for example, the use of alignments between input and output (Sennrich, 2017; Arthur et al., 2016) or copy mechanisms (See et al., 2017). However, they cannot be directly applied to our task because ours treat sequences of numerical values as an input. In this paper, we consider how to alleviate such errors by using contrastive examples, which are identical to the correct examples except for a single word: Nikkei gained vs. Nikkei dropped. Learning with such examples provides models direct signals on the words that are not to be generated in addition to those to be generated. We propose a learning framework to examine how to use such examples from the viewpoint of loss functions and rules"
2020.coling-main.213,D19-1310,0,0.0828,"loss functions, and 3) the use of the examples produced by some speciﬁc rules further improves performance. Human evaluation also supports the effectiveness of using contrastive examples. 1 Introduction We address the task of generating market comments from stock prices as illustrated in Fig. 1. This can be seen as a data-to-text generation task. Recently, neural data-to-text generation has been studied in a wide range of domains such as biography (Lebret et al., 2016; Liu et al., 2018), sports recap (Wiseman et al., 2017; Puduppully et al., 2019a; Puduppully et al., 2019b; Iso et al., 2019; Gong et al., 2019), and market comments (Murakami et al., 2017; Aoki et al., 2018; Aoki et al., 2019). These models generate ﬂuent sentences, but we often observed problematic generated sentences in terms of correctness. As shown in Fig. 1, the word gain is possibly generated, although the word drop or rebound is expected. The terms that express the ﬂuctuation of stock prices are crucial because such errors could reverse the meaning of the sentence in the worst case. Similar issues have been seen in other generation tasks, such as machine translation or summarization. The known solutions are, for example, the u"
2020.coling-main.213,D18-1150,0,0.124047,"t. In this paper, we consider how to alleviate such errors by using contrastive examples, which are identical to the correct examples except for a single word: Nikkei gained vs. Nikkei dropped. Learning with such examples provides models direct signals on the words that are not to be generated in addition to those to be generated. We propose a learning framework to examine how to use such examples from the viewpoint of loss functions and rules to create contrastive examples. Recent studies show the effectiveness of learning methods that exploit explicit negative examples in language modeling. Huang et al. (2018) introduced a margin loss to penalize sentences in a beam, assuming that the generated sentences are imperfect. Noji and Takamura (2020) used synthesized ungrammatical sentences in addition to the originals to improve the syntactic ability of language models. ∗ The ﬁrst and second authors equally contributed to this work. This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 2352 Proceedings of the 28th International Conference on Computational Linguistics, pages 2352–2362 Barcelona, Spain (Online),"
2020.coling-main.213,D16-1128,0,0.151849,"grading the ﬂuency, 2) the choice of the loss function is an important factor because the performances of different metrics depend on the types of loss functions, and 3) the use of the examples produced by some speciﬁc rules further improves performance. Human evaluation also supports the effectiveness of using contrastive examples. 1 Introduction We address the task of generating market comments from stock prices as illustrated in Fig. 1. This can be seen as a data-to-text generation task. Recently, neural data-to-text generation has been studied in a wide range of domains such as biography (Lebret et al., 2016; Liu et al., 2018), sports recap (Wiseman et al., 2017; Puduppully et al., 2019a; Puduppully et al., 2019b; Iso et al., 2019; Gong et al., 2019), and market comments (Murakami et al., 2017; Aoki et al., 2018; Aoki et al., 2019). These models generate ﬂuent sentences, but we often observed problematic generated sentences in terms of correctness. As shown in Fig. 1, the word gain is possibly generated, although the word drop or rebound is expected. The terms that express the ﬂuctuation of stock prices are crucial because such errors could reverse the meaning of the sentence in the worst case. S"
2020.coling-main.213,2020.acl-main.309,1,0.923847,"es except for a single word: Nikkei gained vs. Nikkei dropped. Learning with such examples provides models direct signals on the words that are not to be generated in addition to those to be generated. We propose a learning framework to examine how to use such examples from the viewpoint of loss functions and rules to create contrastive examples. Recent studies show the effectiveness of learning methods that exploit explicit negative examples in language modeling. Huang et al. (2018) introduced a margin loss to penalize sentences in a beam, assuming that the generated sentences are imperfect. Noji and Takamura (2020) used synthesized ungrammatical sentences in addition to the originals to improve the syntactic ability of language models. ∗ The ﬁrst and second authors equally contributed to this work. This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 2352 Proceedings of the 28th International Conference on Computational Linguistics, pages 2352–2362 Barcelona, Spain (Online), December 8-13, 2020 ・ ・ ・ ・ ・・・ ・・ ・ Stock Prices[yen] 19,600 19,500 19,400 19,300 Previous day Today Time Gold: Nikkei suddenly drops…"
2020.coling-main.213,P02-1040,0,0.107919,"583 2,592 12.77 Test 1,951 1,615 2,634 12.69 Table 2: The statistics of the dataset. every epoch, then selected the best model on validation dataset. We used Adam (Kingma and Ba, 2015) optimizer with the initial learning rate 0.001. Each index was converted to a 32-dimensional vector. The dimensions for the hidden layer in the encoder and the decoder were set to 256. We used 128 for the dimensions of word embeddigns. We report the averaged values of three trials with different random seeds for automatic evaluation. 3.3 Automatic Evaluation Since we aim to improve correctness, only using BLEU (Papineni et al., 2002) is not sufﬁcient. It is ideal to evaluate the effect of the use of contrastive pairs from various perspectives. We propose four metrics to capture how well models exploit contrastive examples and generate crucial terms. 3.3.1 Accuracy We expect the trained model should correctly distinguish the difference between reference sentences and their contrastive sentences i.e., assign a higher probability to the reference sentences than its contrastive sentences as a direct effect of the learning with the losses that take into account contrastive examples. Therefore, following the work by Sennrich et"
2020.coling-main.213,P19-1195,0,0.0898084,"r because the performances of different metrics depend on the types of loss functions, and 3) the use of the examples produced by some speciﬁc rules further improves performance. Human evaluation also supports the effectiveness of using contrastive examples. 1 Introduction We address the task of generating market comments from stock prices as illustrated in Fig. 1. This can be seen as a data-to-text generation task. Recently, neural data-to-text generation has been studied in a wide range of domains such as biography (Lebret et al., 2016; Liu et al., 2018), sports recap (Wiseman et al., 2017; Puduppully et al., 2019a; Puduppully et al., 2019b; Iso et al., 2019; Gong et al., 2019), and market comments (Murakami et al., 2017; Aoki et al., 2018; Aoki et al., 2019). These models generate ﬂuent sentences, but we often observed problematic generated sentences in terms of correctness. As shown in Fig. 1, the word gain is possibly generated, although the word drop or rebound is expected. The terms that express the ﬂuctuation of stock prices are crucial because such errors could reverse the meaning of the sentence in the worst case. Similar issues have been seen in other generation tasks, such as machine translat"
2020.coling-main.213,P17-1099,0,0.332476,"nerate ﬂuent sentences, but we often observed problematic generated sentences in terms of correctness. As shown in Fig. 1, the word gain is possibly generated, although the word drop or rebound is expected. The terms that express the ﬂuctuation of stock prices are crucial because such errors could reverse the meaning of the sentence in the worst case. Similar issues have been seen in other generation tasks, such as machine translation or summarization. The known solutions are, for example, the use of alignments between input and output (Sennrich, 2017; Arthur et al., 2016) or copy mechanisms (See et al., 2017). However, they cannot be directly applied to our task because ours treat sequences of numerical values as an input. In this paper, we consider how to alleviate such errors by using contrastive examples, which are identical to the correct examples except for a single word: Nikkei gained vs. Nikkei dropped. Learning with such examples provides models direct signals on the words that are not to be generated in addition to those to be generated. We propose a learning framework to examine how to use such examples from the viewpoint of loss functions and rules to create contrastive examples. Recent"
2020.coling-main.213,E17-2060,0,0.111184,"7; Aoki et al., 2018; Aoki et al., 2019). These models generate ﬂuent sentences, but we often observed problematic generated sentences in terms of correctness. As shown in Fig. 1, the word gain is possibly generated, although the word drop or rebound is expected. The terms that express the ﬂuctuation of stock prices are crucial because such errors could reverse the meaning of the sentence in the worst case. Similar issues have been seen in other generation tasks, such as machine translation or summarization. The known solutions are, for example, the use of alignments between input and output (Sennrich, 2017; Arthur et al., 2016) or copy mechanisms (See et al., 2017). However, they cannot be directly applied to our task because ours treat sequences of numerical values as an input. In this paper, we consider how to alleviate such errors by using contrastive examples, which are identical to the correct examples except for a single word: Nikkei gained vs. Nikkei dropped. Learning with such examples provides models direct signals on the words that are not to be generated in addition to those to be generated. We propose a learning framework to examine how to use such examples from the viewpoint of los"
2020.coling-main.465,C16-1236,0,0.0187955,"suggests that our progress in solving SimpleQuestions dataset does not indicate the success of more general simple question answering. We discuss a possible future direction toward this goal. 1 Introduction Simple factoid question answering over a knowledge base is an important task in natural language understanding. Although it only deals with factoid questions about a single entity and a predicate, they cover much of the real user queries (Dai et al., 2016), and also, accurate mapping of these is a critical subproblem in semantic parsing-based complex query generation (Berant et al., 2013; Bao et al., 2016; Reddy et al., 2016; Trivedi et al., 2017). SimpleQuestions (Bordes et al., 2015) is the largest and most popular dataset on this task. It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates the success of the task itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al."
2020.coling-main.465,D13-1160,0,0.284366,"nion of the datasets, suggests that our progress in solving SimpleQuestions dataset does not indicate the success of more general simple question answering. We discuss a possible future direction toward this goal. 1 Introduction Simple factoid question answering over a knowledge base is an important task in natural language understanding. Although it only deals with factoid questions about a single entity and a predicate, they cover much of the real user queries (Dai et al., 2016), and also, accurate mapping of these is a critical subproblem in semantic parsing-based complex query generation (Berant et al., 2013; Bao et al., 2016; Reddy et al., 2016; Trivedi et al., 2017). SimpleQuestions (Bordes et al., 2015) is the largest and most popular dataset on this task. It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates the success of the task itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestion"
2020.coling-main.465,P13-1042,0,0.0875409,"st and most popular dataset on this task. It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates the success of the task itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data. Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017; Naik et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) while little effort has been made on question answering over a knowledge base, though, in practice, it would be critical because a practical system has to be rob"
2020.coling-main.465,P16-1076,0,0.0200008,"ess of existing systems using different datasets. Our analysis, including shifting of training and test datasets and training on a union of the datasets, suggests that our progress in solving SimpleQuestions dataset does not indicate the success of more general simple question answering. We discuss a possible future direction toward this goal. 1 Introduction Simple factoid question answering over a knowledge base is an important task in natural language understanding. Although it only deals with factoid questions about a single entity and a predicate, they cover much of the real user queries (Dai et al., 2016), and also, accurate mapping of these is a critical subproblem in semantic parsing-based complex query generation (Berant et al., 2013; Bao et al., 2016; Reddy et al., 2016; Trivedi et al., 2017). SimpleQuestions (Bordes et al., 2015) is the largest and most popular dataset on this task. It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates"
2020.coling-main.465,N19-1423,0,0.0276074,"ble. FreebaseQA can be seen as an attempt toward this goal, but we found that this dataset has several issues. Another direction is to invent a model or a learning mechanism that can generalize robustly from biased datasets. Our data union can be seen as a simple approach toward this end, but we found that current models do not exploit useful information beyond each target dataset. More sophisticated approaches, such as distributionally robust optimization (Delage and Ye, 2010; Oren et al., 2019), may help. Another promising way is relying on strong pretrained language models, including BERT (Devlin et al., 2019). We have not included BERTbased models in this paper, because its application on SimpleQuestion has not outperformed a simpler baseline so far (Lukovnikov et al., 2019), and it is also nontrivial to integrate BERT with knowledge graph embeddings, which is necessary for KEQA-based approach and is currently actively studied (Peters et al., 2019; Weijie et al., 2020). The integration of such approaches, along with robustness evaluation as done in this paper, will be of practical importance toward robust question answering not specific to a single dataset. Acknowledgements This paper is based on"
2020.coling-main.465,D17-1215,0,0.0334288,"itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data. Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017; Naik et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) while little effort has been made on question answering over a knowledge base, though, in practice, it would be critical because a practical system has to be robust on real user queries, which may be outliers in the training data. Our experiments suggest that, while SimpleQuestions is the largest, the examples are too simple and the success on it does not indicate progress in factoid question answering in general. For example, we show that, under the same training data size, the system’s accuracy on SimpleQuestions gets about 10 po"
2020.coling-main.465,N19-1028,0,0.126841,"gued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates the success of the task itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data. Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017; Naik et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) while little effort has been made on question answering over a knowledge base, though, in practice, it would be critical because a practical system has to be robust on real user queries, which may be outliers in the train"
2020.coling-main.465,P17-1147,0,0.0188656,"edi et al., 2017; Talmor and Berant, 2018). However, we will see in Section 4.3 that it also tends to introduce certain biases, which affect models’ generalization. The authors also define a subset of Freebase called FB2M that covers 2M entities and 5K predicates, including all entities appearing in WebQuestions, and create all questions from this subset. FreebaseQA (Jiang et al., 2019) This is the latest dataset aiming at more difficult factoid questions than SimpleQuestions while maintaining the scale of data size. Specifically, the questions in this dataset are first sampled from TriviaQA (Joshi et al., 2017) and then filtered by heuristics to collect factoid questions answerable on Freebase. Although the authors argue that their procedure reliably eliminates non-factoid questions, we find several problems in this dataset, which we describe in Section 4.2. 2.1 Preprocessing Apart from the difference in construction methods, the four datasets additionally differ based on (1) whether they contain non-factoid questions and (2) the assumed subset of Freebase. Because we aim 1 https://github.com/ad-freiburg/aqqu Cai and Yates (2013) only mention that questions are written by two native English speakers"
2020.coling-main.465,P19-1334,0,0.0224764,"uate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data. Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017; Naik et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) while little effort has been made on question answering over a knowledge base, though, in practice, it would be critical because a practical system has to be robust on real user queries, which may be outliers in the training data. Our experiments suggest that, while SimpleQuestions is the largest, the examples are too simple and the success on it does not indicate progress in factoid question answering in general. For example, we show that, under the same training data size, the system’s accuracy on SimpleQuestions gets about 10 points higher than that on WebQuestions."
2020.coling-main.465,N18-2047,0,0.0758464,". Although it only deals with factoid questions about a single entity and a predicate, they cover much of the real user queries (Dai et al., 2016), and also, accurate mapping of these is a critical subproblem in semantic parsing-based complex query generation (Berant et al., 2013; Bao et al., 2016; Reddy et al., 2016; Trivedi et al., 2017). SimpleQuestions (Bordes et al., 2015) is the largest and most popular dataset on this task. It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates the success of the task itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that ar"
2020.coling-main.465,C18-1198,0,0.0236031,"o this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data. Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017; Naik et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) while little effort has been made on question answering over a knowledge base, though, in practice, it would be critical because a practical system has to be robust on real user queries, which may be outliers in the training data. Our experiments suggest that, while SimpleQuestions is the largest, the examples are too simple and the success on it does not indicate progress in factoid question answering in general. For example, we show that, under the same training data size, the system’s accuracy on SimpleQuestions gets about 10 points higher than th"
2020.coling-main.465,D19-1432,0,0.0167117,"is to improve the dataset quality. We need to create a dataset, that is real and challenging, while still being scalable. FreebaseQA can be seen as an attempt toward this goal, but we found that this dataset has several issues. Another direction is to invent a model or a learning mechanism that can generalize robustly from biased datasets. Our data union can be seen as a simple approach toward this end, but we found that current models do not exploit useful information beyond each target dataset. More sophisticated approaches, such as distributionally robust optimization (Delage and Ye, 2010; Oren et al., 2019), may help. Another promising way is relying on strong pretrained language models, including BERT (Devlin et al., 2019). We have not included BERTbased models in this paper, because its application on SimpleQuestion has not outperformed a simpler baseline so far (Lukovnikov et al., 2019), and it is also nontrivial to integrate BERT with knowledge graph embeddings, which is necessary for KEQA-based approach and is currently actively studied (Peters et al., 2019; Weijie et al., 2020). The integration of such approaches, along with robustness evaluation as done in this paper, will be of practical"
2020.coling-main.465,D19-1005,0,0.0237469,"nformation beyond each target dataset. More sophisticated approaches, such as distributionally robust optimization (Delage and Ye, 2010; Oren et al., 2019), may help. Another promising way is relying on strong pretrained language models, including BERT (Devlin et al., 2019). We have not included BERTbased models in this paper, because its application on SimpleQuestion has not outperformed a simpler baseline so far (Lukovnikov et al., 2019), and it is also nontrivial to integrate BERT with knowledge graph embeddings, which is necessary for KEQA-based approach and is currently actively studied (Peters et al., 2019; Weijie et al., 2020). The integration of such approaches, along with robustness evaluation as done in this paper, will be of practical importance toward robust question answering not specific to a single dataset. Acknowledgements This paper is based on results obtained from projects JPNP20006 and JPNP15009, commissioned by the New Energy and Industrial Technology Development Organization (NEDO), and also with the support of RIKEN–AIST Joint Research Fund (Feasibility study). For experiments, computational resource of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of A"
2020.coling-main.465,D18-1051,0,0.423246,"in natural language understanding. Although it only deals with factoid questions about a single entity and a predicate, they cover much of the real user queries (Dai et al., 2016), and also, accurate mapping of these is a critical subproblem in semantic parsing-based complex query generation (Berant et al., 2013; Bao et al., 2016; Reddy et al., 2016; Trivedi et al., 2017). SimpleQuestions (Bordes et al., 2015) is the largest and most popular dataset on this task. It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates the success of the task itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset a"
2020.coling-main.465,Q16-1010,0,0.0522411,"Missing"
2020.coling-main.465,2020.acl-main.442,0,0.0151219,"f four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data. Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017; Naik et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) while little effort has been made on question answering over a knowledge base, though, in practice, it would be critical because a practical system has to be robust on real user queries, which may be outliers in the training data. Our experiments suggest that, while SimpleQuestions is the largest, the examples are too simple and the success on it does not indicate progress in factoid question answering in general. For example, we show that, under the same training data size, the system’s accuracy on SimpleQuestions gets about 10 points higher than that on WebQuestions. Although the simplicity"
2020.coling-main.465,N18-1059,0,0.0156238,"ents, containing over 100,000 questions answerable by a single fact. Contrary to WebQuestions, each question in this dataset is created from a sampled fact in Freebase, which is then verbalized and paraphrased by a crowd worker. Possibly due to this procedure starting from a KB fact, we find that, as in Free917, this dataset also tends to verbalize a predicate with directly related terms, such as “What type of music . . .?” for music.artist.genre.2 This approach eases the collection of a lot of data and is popular in data creation for semantic parsing (Wang et al., 2015; Trivedi et al., 2017; Talmor and Berant, 2018). However, we will see in Section 4.3 that it also tends to introduce certain biases, which affect models’ generalization. The authors also define a subset of Freebase called FB2M that covers 2M entities and 5K predicates, including all entities appearing in WebQuestions, and create all questions from this subset. FreebaseQA (Jiang et al., 2019) This is the latest dataset aiming at more difficult factoid questions than SimpleQuestions while maintaining the scale of data size. Specifically, the questions in this dataset are first sampled from TriviaQA (Joshi et al., 2017) and then filtered by h"
2020.coling-main.465,P19-1485,0,0.0793845,"a careful comparison and manual analysis using the standardized datasets. Given our analysis, we suggest two possible future directions. One is to invent a clever novel data creation method that would be scalable while avoiding bias as much as possible. In this respect, we point out that a recent attempt by FreebaseQA (Jiang et al., 2019) is not successful, and that significant bias still exists. Another is to exploit useful information from the large dataset of SimpleQuestions in a better way. In the last analysis, we demonstrate that a simple approach of training on a union of the datasets (Talmor and Berant, 2019) is not satisfactory toward this end, calling for a more sophisticated method of exploiting useful features across datasets effectively. 2 Datasets We use four QA datasets over a knowledge base (KB) as our target datasets. These datasets were selected because they share a common KB (Freebase), and a large portion of each dataset comprises of factoid questions, which are the main focus of this paper. A factoid question asks a single fact, or a triple (subject, predicate, object) on a KB, where the object corresponds to the answer. For example, “Which country is Albert Bolender from?” correspond"
2020.coling-main.465,D17-1307,0,0.0191326,"itself is not available. 5 https://github.com/castorini/BuboQA 5323 architectures (Bordes et al., 2015; Yin et al., 2016). Specifically, for entity linking, a trained LSTM first detects the entity spans, which are then heuristically mapped to the candidate KB entities and scored with the Levenshtein distance to the canonical entity label. Relation prediction is performed independently by another classifier on top of a different LSTM. Finally, the best combination of (ˆ e, rˆ) is found according to a weighted sum of these two module scores.6 This is an extension of an even simpler baseline of Ture and Jojic (2017), and a similar approach is employed in Petrochuk and Zettlemoyer (2018). Note that this system treats relation prediction as classification among the predicates appearing in the training data. This means that it cannot solve zero-shot relation prediction, which occurs to some extent especially in the dataset transfer experiment (Section 4.3). On the other hand, the other three systems theoretically can handle them, as described in the following. Hierarchical Residual BiLSTM (HR-BiLSTM) (Yu et al., 2017) On this system (and the next, KBQA-Adapter), relation prediction is performed differently,"
2020.coling-main.465,P15-1129,0,0.013098,"his is the largest dataset in our experiments, containing over 100,000 questions answerable by a single fact. Contrary to WebQuestions, each question in this dataset is created from a sampled fact in Freebase, which is then verbalized and paraphrased by a crowd worker. Possibly due to this procedure starting from a KB fact, we find that, as in Free917, this dataset also tends to verbalize a predicate with directly related terms, such as “What type of music . . .?” for music.artist.genre.2 This approach eases the collection of a lot of data and is popular in data creation for semantic parsing (Wang et al., 2015; Trivedi et al., 2017; Talmor and Berant, 2018). However, we will see in Section 4.3 that it also tends to introduce certain biases, which affect models’ generalization. The authors also define a subset of Freebase called FB2M that covers 2M entities and 5K predicates, including all entities appearing in WebQuestions, and create all questions from this subset. FreebaseQA (Jiang et al., 2019) This is the latest dataset aiming at more difficult factoid questions than SimpleQuestions while maintaining the scale of data size. Specifically, the questions in this dataset are first sampled from Triv"
2020.coling-main.465,P19-1616,0,0.420991,"vedi et al., 2017). SimpleQuestions (Bordes et al., 2015) is the largest and most popular dataset on this task. It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates the success of the task itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data. Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017; Naik et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) while little effort has been made on question answering over a knowledge base, though,"
2020.coling-main.465,P15-1128,0,0.029467,"iction are modeled with simple classifiers. Despite its simplicity, this approach outperforms several more complex 3 The reason for the decrease in the first step for FreebaseQA is that it contains two-hop questions involving a mediator node in Freebase, which we exclude from the target. 4 When searching for open software, we often found that many systems along with a paper are not self-contained; in particular, they often are missing an entity linking module. This is especially the case for systems targeting WebQustions, for which many systems rely on the outputs of the entity linker used in Yih et al. (2015) and found in https: //github.com/scottyih/STAGG, while the entity linker itself is not available. 5 https://github.com/castorini/BuboQA 5323 architectures (Bordes et al., 2015; Yin et al., 2016). Specifically, for entity linking, a trained LSTM first detects the entity spans, which are then heuristically mapped to the candidate KB entities and scored with the Levenshtein distance to the canonical entity label. Relation prediction is performed independently by another classifier on top of a different LSTM. Finally, the best combination of (ˆ e, rˆ) is found according to a weighted sum of these"
2020.coling-main.465,P16-2033,0,0.261633,"ataset on this task. It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates the success of the task itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data. Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017; Naik et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) while little effort has been made on question answering over a knowledge base, though, in practice, it would be critical because a practical system has to be robust on real user q"
2020.coling-main.465,C16-1164,0,0.0491652,"Missing"
2020.coling-main.465,P17-1053,0,0.0602595,"et al., 2016; Trivedi et al., 2017). SimpleQuestions (Bordes et al., 2015) is the largest and most popular dataset on this task. It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018; Mohammed et al., 2018). In this paper, we present a thorough empirical analysis to assess whether the success of one particular dataset indicates the success of the task itself in general. To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018; Yu et al., 2017; Wu et al., 2019; Huang et al., 2019), across four different datasets (Cai and Yates, 2013; Yih et al., 2016; Bordes et al., 2015; Jiang et al., 2019), under different conditions. One of our research goals is to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data. Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017; Naik et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) while little effort has been made on question answering over a knowled"
2020.inlg-1.21,D10-1049,0,0.05664,"Missing"
2020.inlg-1.21,W13-2127,0,0.0411847,"Missing"
2020.inlg-1.21,E17-1060,0,0.0340918,"Missing"
2020.inlg-1.21,J05-1002,0,0.162505,"Missing"
2020.inlg-1.21,P16-1154,0,0.224027,"del. To switch the copy mode and noncopy mode, we add a special token “<PRICE>”, which is inserted before every numerical value in the training data and indicates that the next token is a price value. Utilizing “<PRICE>”, we define each conditional probability of generating target word ? ? at time ? as: 3.2 where ? gen (? ? |·) and ? copy (? ? |·) are obtained by the generation mode and the copy mode, respectively. This method is inspired by Pointer-generator network introduced by See et al. (2017). These two probabilities are defined as: Decoder with Copy Mechanism We adapt a copy mechanism (Gu et al., 2016) in our decoder to generate numerical values by attending to the input. Recall that in the current task, the values in the output text usually do not appear in the input data (Section 2.2); rather, they can be obtained by applying an arithmetic operation to the certain value in the data. We generate a numerical value by an extension of a copy mechanism, wherein a value is generated by applying one of the operations in Table 1 to a data point ? ?,1 ˜ , which is the first value (latest price) of x ?˜ . Denoting an arithmetic operation as ? ∈ {? ? 1 , · · · , ? ? 12 }, the value is identified by"
2020.inlg-1.21,P82-1020,0,0.677071,"Missing"
2020.inlg-1.21,N12-1093,0,0.0483852,"Missing"
2020.inlg-1.21,P83-1022,0,0.625173,"Missing"
2020.inlg-1.21,D16-1128,0,0.0381251,"Missing"
2020.inlg-1.21,C18-1089,0,0.0730793,"Missing"
2020.inlg-1.21,P09-1011,0,0.108843,"Missing"
2020.inlg-1.21,N16-1086,0,0.0177022,"er the movement expression in the comment matches the movement with the latest stock price (X) or not (×). have been increased providing opportunities to treat various types of large-scale data, so that they are interested in automatically learning a correspondence relationship from data to text and generating a description of this relationship. Therefore, recent works have focused on generating text from data with neural networks, that can solve the above sub-tasks in one through. Especially the models, which utilize an encoder-decoder model (Sutskever et al., 2014) have proven to be useful (Mei et al., 2016; Lebret et al., 2016). While text generation by neural network can describe the text fluently, they do not describe the exact entities or numbers. Therefore, a copy mechanism (Vinyals et al., 2015; Gu et al., 2016), which provides a way to directly copy words from the input, has been utilized. By these neural networks, the works such as conditional language generation based on tables (Yang However, Joulin and Mikolov (2015) and Neelakantan et al. (2016) indicate that current neural models have difficulties in learning arithmetic operations such as addition and comparisons by neural program in"
2020.inlg-1.21,D18-1422,0,0.0151064,"as been utilized. By these neural networks, the works such as conditional language generation based on tables (Yang However, Joulin and Mikolov (2015) and Neelakantan et al. (2016) indicate that current neural models have difficulties in learning arithmetic operations such as addition and comparisons by neural program inductions. Thus, there have been some methods to prepare the numerical values with arithmetic operations in advance. Murakami et al. (2017) post-process the price by extending the copy mechanism and replacing numerical values with defined arithmetic operations after generation. Nie et al. (2018) utilizes information from pre-computed operations on raw data to consider incorporating the facts that can be inferred from the input data to guide the generation process. Our model prepares numerical values with defined arithmetic operations as Murakami et al. (2017) for copy and that copy target is guided by encoded input. 155 6 Conclusion In this paper, we have proposed an encoder-decoder model with multi-timestep data and a copy mechanism for generating the market comment from data with the noisy alignments. Both BLEU scores and our proposal evaluation showed the accuracy of sentence gene"
2020.inlg-1.21,P17-1099,0,0.0385523,"values in the text with this copy mechanism. To do this, we exclude numerical values from the vocabulary of the model. To switch the copy mode and noncopy mode, we add a special token “<PRICE>”, which is inserted before every numerical value in the training data and indicates that the next token is a price value. Utilizing “<PRICE>”, we define each conditional probability of generating target word ? ? at time ? as: 3.2 where ? gen (? ? |·) and ? copy (? ? |·) are obtained by the generation mode and the copy mode, respectively. This method is inspired by Pointer-generator network introduced by See et al. (2017). These two probabilities are defined as: Decoder with Copy Mechanism We adapt a copy mechanism (Gu et al., 2016) in our decoder to generate numerical values by attending to the input. Recall that in the current task, the values in the output text usually do not appear in the input data (Section 2.2); rather, they can be obtained by applying an arithmetic operation to the certain value in the data. We generate a numerical value by an extension of a copy mechanism, wherein a value is generated by applying one of the operations in Table 1 to a data point ? ?,1 ˜ , which is the first value (lates"
2020.inlg-1.21,P02-1040,0,0.107462,"m vectors, we set ? = 7 for xlong and ? = 62 for x ?˜ , following Murakami et al. (2017), changing the range of ? by setting ? ∈ [0, 6]. The embedding sizes of a word, five-minute tag f ?˜ , article-tag a, and time tag t are 128, 80, 64, and 64, respectively. We trained the models for 150 epochs with the mini-batch size of 100, using Adam (Kingma and Ba, 2015) optimizer with the initial learning rate 1 × 10−4 , and saved the parameters every epoch, selecting the model with the highest BLEU score on the validation dataset. 4.3 Evaluation Metrics We conduct two types of evaluation: one is BLEU (Papineni et al., 2002) to measure the matching degree between the market comments written by humans as references and the output comments generated by the models, and the other is a new evaluation metric that we created. The new metric uses the matching between the market price movement in the data and the movement expressions in ???? the comments. Using (x? , w? , w??? ?? ), which are the ?-th sample of the input data, the market comment written by humans, and the output comment generated by the models, we define the following 3https://hosted.datascope.reuters.com/ DataScope/ (Latest ? move > 0) (Latest ? move < 0"
2020.inlg-1.21,D17-1197,0,0.0345052,"Missing"
2020.inlg-1.21,P19-1195,0,0.0371637,"Missing"
2020.inlg-1.21,W18-6557,0,0.0142069,"s to treat large-scale data. movement on three points (the closing prices of the Hence, there is an increasing demand to automati- last two days and the latest price). This is valid for cally generate a text from large and complex data. In the prices at (I) and (II), but does not hold at (III) recent studies, neural network-based models have because the latest price (17039.22 yen) is lower than achieved significant progress on the data-to-text the last closing price (17041.45 yen). In addition, which is a text generation task from input data the expression “gains 88 yen” is only valid at the (Puzikov and Gurevych, 2018; Liu et al., 2018; Iso opening time (I) and is not valid at (II). To deal with et al., 2019). these inconsistencies, the models have to be aware One important issue in constructing a dataset of these possible mismatch of data and text due for data-to-text is to obtain the correct alignment to the delay, but a simple encoder-decoder-based 148 Proceedings of The 13th International Conference on Natural Language Generation, pages 148–157, c Dublin, Ireland, 15-18 December, 2020. 2020 Association for Computational Linguistics Delivery time Price movement Comment (bold text: movement expression) ("
2020.lrec-1.225,S12-1051,0,0.0477468,"Missing"
2020.lrec-1.225,S13-1004,0,0.0437349,"Missing"
2020.lrec-1.225,S14-2010,0,0.0372592,"Missing"
2020.lrec-1.225,S15-2045,0,0.0375724,"Missing"
2020.lrec-1.225,S16-1081,0,0.045039,"Missing"
2020.lrec-1.225,W16-2502,0,0.0123388,"can be explained by the accuracies of intrinsic evaluation with causal relations, as Chiu et al. (2016) assumed. The causal diagram shown in Figure 2 represents this hypothesis. Following the previous studies (Chiu et al., 2016; Rogers et al., 2018; Wang et al., 2019), we introduce the structure of datasets for intrinsic and extrinsic evaluation to our causal diagram. For intrinsic evaluation, we employ the BATS dataset (Gladkova et al., 2016). We do not use word similarity datasets, because of the ambiguous definition of similarity and the problem of inter-annotator agreement on the dataset (Batchkarov et al., 2016). The BATS dataset consists of four linguistic categories containing ten subcategories, such as inflectional morphology, derivational morphology, lexicography knowledge, and encyclopedia knowledge. Table 1 lists more details of the BATS dataset. Following Gladkova et al. (2016), we assume that each linguistic category is one latent variable that reflects the accuracies of its ten subcategories for the measurement model in our causal diagrams. By binding subcategories with one latent variable, we can reduce the number of parameters in the PLS-PM model, which allows us to fit the model with fewe"
2020.lrec-1.225,W16-2508,0,0.0161526,"is hard for correlation analysis, such as verifying the existence of causal relations between intrinsic and extrinsic evaluation, the explanatory power of intrinsic evaluation for extrinsic evaluation, and the effectiveness of hyperparameters on intrinsic and extrinsic evaluation. As a result, we have proven part of a causal hypothesis in previous studies, namely, that the accuracies of intrinsic evaluation can explain the accuracies of extrinsic evaluation. In addition, our PLS-PM models have provided novel findings, such as the structural problem of inflection knowledge in the BATS dataset. Camacho-Collados and Navigli (2016) argued that previous studies on relations between intrinsic and extrinsic evaluation have salient limitations in terms of generality. We believe that our contribution is to employ a statistical methodology to investigate causal relations between intrinsic and extrinsic evaluation, in order to prove them with more generality. In future work, we hope to apply PLS-PM analysis to other vector representations, such as contextualized word representations (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2018; Radford et al., 2019) for more general insight about word embedding. 1830 6. Bibl"
2020.lrec-1.225,S17-2001,0,0.0310112,"Missing"
2020.lrec-1.225,W16-2501,0,0.0981154,"sing (NLP) tasks, yet it is still unclear why and how it contributes to achieving high accuracy on NLP tasks. A series of extrinsic experiments has proven the effectiveness of word embedding on downstream NLP tasks such as syntactic analysis and semantic textual similarity (Nayak et al., 2016; Conneau and Kiela, 2018). On the other hand, intrinsic evaluation of word embedding, as in word similarity (Bruni et al., 2014; Hill et al., 2015) and word analogy tasks (Mikolov et al., 2013; Gladkova et al., 2016), has been proposed for assessing what linguistic knowledge it encodes. Previous studies (Chiu et al., 2016; Rogers et al., 2018; Wang et al., 2019) tried to prove this intuition by using correlation analysis. Correlation analysis does not, however, assume any causal hypothesis, therefore it is hard to extract any cause-effect relationships from those studies. Hence, we investigate causal relations between the accuracies of intrinsic and extrinsic evaluation, by applying partial least squares path modeling (PLS-PM) (Wold, 1982), a method of structural equation modeling. PLS-PM is a widely accepted method in social science disciplines for analyzing causal relations among observed and latent variable"
2020.lrec-1.225,L18-1269,0,0.118774,"xt window, and for validating the effectiveness of intrinsic evaluation. Keywords: Word Embedding, Intrinsic Evaluation, Extrinsic Evaluation, Structural Equation Modeling, Partial Least Squares Path Modeling 1. Introduction Word embedding is an indispensable tool for a variety of natural language processing (NLP) tasks, yet it is still unclear why and how it contributes to achieving high accuracy on NLP tasks. A series of extrinsic experiments has proven the effectiveness of word embedding on downstream NLP tasks such as syntactic analysis and semantic textual similarity (Nayak et al., 2016; Conneau and Kiela, 2018). On the other hand, intrinsic evaluation of word embedding, as in word similarity (Bruni et al., 2014; Hill et al., 2015) and word analogy tasks (Mikolov et al., 2013; Gladkova et al., 2016), has been proposed for assessing what linguistic knowledge it encodes. Previous studies (Chiu et al., 2016; Rogers et al., 2018; Wang et al., 2019) tried to prove this intuition by using correlation analysis. Correlation analysis does not, however, assume any causal hypothesis, therefore it is hard to extract any cause-effect relationships from those studies. Hence, we investigate causal relations between"
2020.lrec-1.225,C04-1051,0,0.0435845,"Missing"
2020.lrec-1.225,N13-1092,0,0.105993,"Missing"
2020.lrec-1.225,N16-2002,0,0.179339,"assess a PLS-PM result, researchers use various reliability indexes. First, the design of the measurement model can 3.1. Experimental setup Design for casual diagrams incorporating intrinsic and extrinsic evaluation In this paper, we examine causal hypotheses between intrinsic and extrinsic evaluation of word embedding with the PLS-PM methodology. We thus aim to fit PLS-PM models, following causal hypotheses of previous studies. Our causal hypothesis is that the accuracies of extrinsic evaluation can be explained by the accuracies of intrinsic evaluation with causal relations, as Chiu et al. (2016) assumed. The causal diagram shown in Figure 2 represents this hypothesis. Following the previous studies (Chiu et al., 2016; Rogers et al., 2018; Wang et al., 2019), we introduce the structure of datasets for intrinsic and extrinsic evaluation to our causal diagram. For intrinsic evaluation, we employ the BATS dataset (Gladkova et al., 2016). We do not use word similarity datasets, because of the ambiguous definition of similarity and the problem of inter-annotator agreement on the dataset (Batchkarov et al., 2016). The BATS dataset consists of four linguistic categories containing ten subcat"
2020.lrec-1.225,J15-4004,0,0.125069,"ic Evaluation, Structural Equation Modeling, Partial Least Squares Path Modeling 1. Introduction Word embedding is an indispensable tool for a variety of natural language processing (NLP) tasks, yet it is still unclear why and how it contributes to achieving high accuracy on NLP tasks. A series of extrinsic experiments has proven the effectiveness of word embedding on downstream NLP tasks such as syntactic analysis and semantic textual similarity (Nayak et al., 2016; Conneau and Kiela, 2018). On the other hand, intrinsic evaluation of word embedding, as in word similarity (Bruni et al., 2014; Hill et al., 2015) and word analogy tasks (Mikolov et al., 2013; Gladkova et al., 2016), has been proposed for assessing what linguistic knowledge it encodes. Previous studies (Chiu et al., 2016; Rogers et al., 2018; Wang et al., 2019) tried to prove this intuition by using correlation analysis. Correlation analysis does not, however, assume any causal hypothesis, therefore it is hard to extract any cause-effect relationships from those studies. Hence, we investigate causal relations between the accuracies of intrinsic and extrinsic evaluation, by applying partial least squares path modeling (PLS-PM) (Wold, 198"
2020.lrec-1.225,Q15-1016,0,0.596045,"esents a latent variable, and an edge arrow represents a causal relationship. between intrinsic and extrinsic evaluation, to explain and predict the performance of extrinsic evaluation by intrinsic evaluation. In fact, it is hard to interpret correlation results alone without assuming any causal hypothesis for their existence (Pearl, 2009; Koller and Friedman, 2009). Moreover, with correlation analysis, it is not easy to disentangle the effects of external factors, such as the training algorithm, corpus, and hyperparameters of word embedding, which highly affect the quality of word embedding (Levy et al., 2015; Lai et al., 2016). To examine causal relations among various observed and latent variables involving analysis of word embedding, we thus require a more general framework for statistical analysis than what correlation analysis provides. 2.2. Structural equation modeling Structural equation modeling, which was first invented by Wright (1921), provides a convenient framework for statistical analysis that includes several traditional multivariate procedures, such as factor analysis, regression analysis, and canonical correlation analysis. In the social science field, structural equation modeling"
2020.lrec-1.225,S14-2001,0,0.0488812,"Missing"
2020.lrec-1.225,N13-1090,0,0.193358,", Partial Least Squares Path Modeling 1. Introduction Word embedding is an indispensable tool for a variety of natural language processing (NLP) tasks, yet it is still unclear why and how it contributes to achieving high accuracy on NLP tasks. A series of extrinsic experiments has proven the effectiveness of word embedding on downstream NLP tasks such as syntactic analysis and semantic textual similarity (Nayak et al., 2016; Conneau and Kiela, 2018). On the other hand, intrinsic evaluation of word embedding, as in word similarity (Bruni et al., 2014; Hill et al., 2015) and word analogy tasks (Mikolov et al., 2013; Gladkova et al., 2016), has been proposed for assessing what linguistic knowledge it encodes. Previous studies (Chiu et al., 2016; Rogers et al., 2018; Wang et al., 2019) tried to prove this intuition by using correlation analysis. Correlation analysis does not, however, assume any causal hypothesis, therefore it is hard to extract any cause-effect relationships from those studies. Hence, we investigate causal relations between the accuracies of intrinsic and extrinsic evaluation, by applying partial least squares path modeling (PLS-PM) (Wold, 1982), a method of structural equation modeling."
2020.lrec-1.225,W16-2504,0,0.216833,"dimension, and context window, and for validating the effectiveness of intrinsic evaluation. Keywords: Word Embedding, Intrinsic Evaluation, Extrinsic Evaluation, Structural Equation Modeling, Partial Least Squares Path Modeling 1. Introduction Word embedding is an indispensable tool for a variety of natural language processing (NLP) tasks, yet it is still unclear why and how it contributes to achieving high accuracy on NLP tasks. A series of extrinsic experiments has proven the effectiveness of word embedding on downstream NLP tasks such as syntactic analysis and semantic textual similarity (Nayak et al., 2016; Conneau and Kiela, 2018). On the other hand, intrinsic evaluation of word embedding, as in word similarity (Bruni et al., 2014; Hill et al., 2015) and word analogy tasks (Mikolov et al., 2013; Gladkova et al., 2016), has been proposed for assessing what linguistic knowledge it encodes. Previous studies (Chiu et al., 2016; Rogers et al., 2018; Wang et al., 2019) tried to prove this intuition by using correlation analysis. Correlation analysis does not, however, assume any causal hypothesis, therefore it is hard to extract any cause-effect relationships from those studies. Hence, we investigat"
2020.lrec-1.225,N18-1202,0,0.0783587,"Missing"
2020.lrec-1.225,C18-1228,0,0.137583,"et it is still unclear why and how it contributes to achieving high accuracy on NLP tasks. A series of extrinsic experiments has proven the effectiveness of word embedding on downstream NLP tasks such as syntactic analysis and semantic textual similarity (Nayak et al., 2016; Conneau and Kiela, 2018). On the other hand, intrinsic evaluation of word embedding, as in word similarity (Bruni et al., 2014; Hill et al., 2015) and word analogy tasks (Mikolov et al., 2013; Gladkova et al., 2016), has been proposed for assessing what linguistic knowledge it encodes. Previous studies (Chiu et al., 2016; Rogers et al., 2018; Wang et al., 2019) tried to prove this intuition by using correlation analysis. Correlation analysis does not, however, assume any causal hypothesis, therefore it is hard to extract any cause-effect relationships from those studies. Hence, we investigate causal relations between the accuracies of intrinsic and extrinsic evaluation, by applying partial least squares path modeling (PLS-PM) (Wold, 1982), a method of structural equation modeling. PLS-PM is a widely accepted method in social science disciplines for analyzing causal relations among observed and latent variables (Henseler et al., 2"
2020.lrec-1.225,W00-0726,0,0.280738,"past noun+less, un+adj., adj.+ly, over+adj./ved, adj.+ness, re+verb, verb+able, verb+er, verb+ation, verb+ment hypernyms (animals), hypernyms (miscellaneous), hyponyms (miscellaneous), meronyms (substance), meronyms (member), meronyms (part-whole), synonyms (intensity), synonyms (exact), antonyms (gradable), antonyms (binary) geography (capitals), geography (country:language), geography (uk city:county), people (nationalities), people (occupation), animals (the young), animals (sounds), animals (shelter), other (thing:color), other (male:female) POS-tagging (Toutanova et al., 2003), Chunking (Sang and Buchholz, 2000) Named Entity Recognition (Sang and Erik, 2002), Sentiment Classification (Socher et al., 2013), Question Classification (Li and Roth, 2006), Natural Language Inference (Ganitkevitch et al., 2013) Movie Review, Product Review, Subjectivity Status, Opinion-polarity (Wang and Manning, 2012), Binary Sentiment Analysis, Fine-grained Sentiment Analysis (Socher et al., 2013), Question Classification (Li and Roth, 2006) Natural Language Inference (Marelli et al., 2014) STS 2012 (Agirre et al., 2012), STS 2013 (Agirre et al., 2013), STS 2014 (Agirre et al., 2014), STS 2015 (Agirre et al., 2015), STS 2"
2020.lrec-1.225,W02-2024,0,0.104816,"j.+ness, re+verb, verb+able, verb+er, verb+ation, verb+ment hypernyms (animals), hypernyms (miscellaneous), hyponyms (miscellaneous), meronyms (substance), meronyms (member), meronyms (part-whole), synonyms (intensity), synonyms (exact), antonyms (gradable), antonyms (binary) geography (capitals), geography (country:language), geography (uk city:county), people (nationalities), people (occupation), animals (the young), animals (sounds), animals (shelter), other (thing:color), other (male:female) POS-tagging (Toutanova et al., 2003), Chunking (Sang and Buchholz, 2000) Named Entity Recognition (Sang and Erik, 2002), Sentiment Classification (Socher et al., 2013), Question Classification (Li and Roth, 2006), Natural Language Inference (Ganitkevitch et al., 2013) Movie Review, Product Review, Subjectivity Status, Opinion-polarity (Wang and Manning, 2012), Binary Sentiment Analysis, Fine-grained Sentiment Analysis (Socher et al., 2013), Question Classification (Li and Roth, 2006) Natural Language Inference (Marelli et al., 2014) STS 2012 (Agirre et al., 2012), STS 2013 (Agirre et al., 2013), STS 2014 (Agirre et al., 2014), STS 2015 (Agirre et al., 2015), STS 2016 (Agirre et al., 2016), STS Benchmark (Cer e"
2020.lrec-1.225,D15-1036,0,0.0218976,"shows correlation disagreement in the evaluation results. This indicates that the existing intrinsic evaluation for inflectional morphology may not reflect the structure of linguistic knowledge encoded in word embedding. We further investigate the impacts of hyperparameters for training word embedding on intrinsic and extrinsic evaluation, by incorporating hyperparameter categories into causal diagrams. 2. 2.1. Statistical methodology for testing causal hypotheses Background The relationship between intrinsic and extrinsic evaluation on word embedding has been studied by various researchers (Schnabel et al., 2015; Chiu et al., 2016; Rogers et al., 2018; Wang et al., 2019). They mainly conducted only simple correlation analysis, which only measures the correlation between two sets of observed variables. What NLP researchers want to reveal, however, are causal relationships 1823 x11 noun+less (life:lifeless) x12 un+adj. (able:unable) x21 y1 Derivational Morphology y2 NLP tasks for Syntactic Properties x1n verb+ment (pay:payment) POS-Tagging x22 Chunking Figure 1: Sample of a causal diagram for the relationship between intrinsic and extrinsic evaluation. A rectangle represents an observed variable, a cir"
2020.lrec-1.225,D13-1170,0,0.00307343,", verb+ment hypernyms (animals), hypernyms (miscellaneous), hyponyms (miscellaneous), meronyms (substance), meronyms (member), meronyms (part-whole), synonyms (intensity), synonyms (exact), antonyms (gradable), antonyms (binary) geography (capitals), geography (country:language), geography (uk city:county), people (nationalities), people (occupation), animals (the young), animals (sounds), animals (shelter), other (thing:color), other (male:female) POS-tagging (Toutanova et al., 2003), Chunking (Sang and Buchholz, 2000) Named Entity Recognition (Sang and Erik, 2002), Sentiment Classification (Socher et al., 2013), Question Classification (Li and Roth, 2006), Natural Language Inference (Ganitkevitch et al., 2013) Movie Review, Product Review, Subjectivity Status, Opinion-polarity (Wang and Manning, 2012), Binary Sentiment Analysis, Fine-grained Sentiment Analysis (Socher et al., 2013), Question Classification (Li and Roth, 2006) Natural Language Inference (Marelli et al., 2014) STS 2012 (Agirre et al., 2012), STS 2013 (Agirre et al., 2013), STS 2014 (Agirre et al., 2014), STS 2015 (Agirre et al., 2015), STS 2016 (Agirre et al., 2016), STS Benchmark (Cer et al., 2017), SICK-R (Marelli et al., 2014) Para"
2020.lrec-1.225,N03-1033,0,0.036504,"le:3ps.sg, participle:past, 3ps.sg:past noun+less, un+adj., adj.+ly, over+adj./ved, adj.+ness, re+verb, verb+able, verb+er, verb+ation, verb+ment hypernyms (animals), hypernyms (miscellaneous), hyponyms (miscellaneous), meronyms (substance), meronyms (member), meronyms (part-whole), synonyms (intensity), synonyms (exact), antonyms (gradable), antonyms (binary) geography (capitals), geography (country:language), geography (uk city:county), people (nationalities), people (occupation), animals (the young), animals (sounds), animals (shelter), other (thing:color), other (male:female) POS-tagging (Toutanova et al., 2003), Chunking (Sang and Buchholz, 2000) Named Entity Recognition (Sang and Erik, 2002), Sentiment Classification (Socher et al., 2013), Question Classification (Li and Roth, 2006), Natural Language Inference (Ganitkevitch et al., 2013) Movie Review, Product Review, Subjectivity Status, Opinion-polarity (Wang and Manning, 2012), Binary Sentiment Analysis, Fine-grained Sentiment Analysis (Socher et al., 2013), Question Classification (Li and Roth, 2006) Natural Language Inference (Marelli et al., 2014) STS 2012 (Agirre et al., 2012), STS 2013 (Agirre et al., 2013), STS 2014 (Agirre et al., 2014), S"
2020.lrec-1.225,P12-2018,0,0.0696214,"Missing"
2020.rail-1.5,K18-2013,0,0.0154266,"ces. For non-projective sentences, it employs the arc-standard system of Nivre (Nivre, 2014). To handle non-projective sentences, it has an extra transition called “swap” that reorders two words. It uses neural network classifiers to predict correct transitions. For the purpose of improving parsing accuracy, it adds search-based oracles. It also includes optional beam search decoding, similar to that of Zhang and Nivre (Zhang and Nivre, 2011). 3.2. UUParser 3.4. jPTDP Turku Parser Turku is a neural parsing pipeline for segmentation, morphological tagging, dependency parsing and lemmatization (Kanerva et al., 2018). For sentence segmentation and tokenization, the system relies on the output of UDPipe. The pipeline allows pre-trained embeddings to be included in the training. The tagging is done using the system of Dozat et al. (Dozat et al., 2017) which applies a time-distributed affine classifier to the tokens within a sentence. Tokens are first embedded with a word encoder. The encoder sums up a learned token embedding, a pre-trained token embedding, and a token embedding encoded from the sequence of its characters using a unidirectional LSTM. Next, a bidirectional LSTM reads the sequence of embedded"
2020.rail-1.5,Q16-1023,0,0.339704,"to neural networks. In this line of research, language is represented in the form of non-linear features. The approach is inspired by the the way computation works in the brain (Goldberg, 2017). It is applied in areas such as machine translation, computer vision, and speech recognition. With regards to parsing, the wave of neural network parsers was started in 2014 by Chen and Manning (Chen and Manning, 2014), who presented a fast and accurate transition-based parser using neural networks. Since then other parsing models have employed various techniques such as stack LSTM (Dyer et al., 2015; Kiperwasser and Goldberg, 2016), global normalization (Andor et al., 2016), biaffine attention (Dozat and Manning, 2017) or recurrent neural network grammars (Dyer et al., 2016; Kuncoro et al., 2017). Due to the existence of a treebank for different languages and the shared task of CoNLL 2017 (Zeman et al., 2017) and 2018 (Zeman et al., 2018), large improvements in dependency parsing using neural networks have been reported. For instance, the neural graph-based parser 2. Background A parsing system may use a model which is learned from a treebank to predict the grammatical structure for new sentences. This method of parser"
2020.rail-1.5,E17-1117,0,0.0203506,"(Goldberg, 2017). It is applied in areas such as machine translation, computer vision, and speech recognition. With regards to parsing, the wave of neural network parsers was started in 2014 by Chen and Manning (Chen and Manning, 2014), who presented a fast and accurate transition-based parser using neural networks. Since then other parsing models have employed various techniques such as stack LSTM (Dyer et al., 2015; Kiperwasser and Goldberg, 2016), global normalization (Andor et al., 2016), biaffine attention (Dozat and Manning, 2017) or recurrent neural network grammars (Dyer et al., 2016; Kuncoro et al., 2017). Due to the existence of a treebank for different languages and the shared task of CoNLL 2017 (Zeman et al., 2017) and 2018 (Zeman et al., 2018), large improvements in dependency parsing using neural networks have been reported. For instance, the neural graph-based parser 2. Background A parsing system may use a model which is learned from a treebank to predict the grammatical structure for new sentences. This method of parser development is called datadriven parsing. The goal of data-driven dependency parsing is to learn to accurately predict dependency graphs from the treebank. Following th"
2020.rail-1.5,P05-1012,0,0.0742337,"sing systems we use to developed the parser. Section 4 presents our comparison and the results we obtained. The final section, Section 5, summarizes and points out the future directions of the research. Dependency parsing is the task of analyzing the dependency structure of a given input sentence automatically (K¨ubler et al., 2009). It requires a series of decisions to form the syntactic structure in the light of dependency relations. Nowadays, dependency grammar is gaining popularity because of its capability to handle predicate-argument structures that are needed in other NLP applications (Mcdonald et al., 2005). In addition, dependency grammar is recommended for languages that have free word order (K¨ubler et al., 2009; Tsarfaty et al., 2010). However, while dependency parsing is adaptable to many languages, it performs less well with morphologically rich languages like Arabic, Basque, and Greek (Dehdari et al., 2011). It is confirmed in (Habash, 2010), that languages like Arabic, Hebrew, and Amharic present a special challenges for the design of a dependency grammar due to their complex morphology and agreement. Starting from the mid 20th century, research in NLP has shifted to neural networks. In"
2020.rail-1.5,K18-2008,0,0.0175955,"tokens in a sentence to create a context-sensitive token representations. These representations are then transformed with ReLU layers separately for each affine tag classification layer (namely UPOS and XPOS). These two classification layers are trained jointly by summing their cross-entropy losses. Lemmatization is another pipeline in the Turku parser in which the researchers develop their own lemmatization component. The system considers lemmatization as a sequence-to-sequence translation problem. They consider jPTDP is a joint model for part-of-speech (POS) tagging and dependency parsing (Nguyen and Verspoor, 2018). It was released in two versions; for our experiment, we used the latest version, jPTDP v2.0. This model is based on the BIST graph-based dependency parser of Kiperwasser and Goldberg (Kiperwasser and Goldberg, 2016). Given word tokens in an input sentence, the tagging component uses a BiLSTM to learn latent feature vectors representing the tokens. Then the tagging component feeds these feature vectors into a multi-layer perceptron (MLP) with one hidden layer to predict POS tags. The parsing component uses another BiLSTM to learn a set of latent feature representations which are based on both"
2020.rail-1.5,J08-4003,0,0.0406725,"d sentences (5,245 tokens or 10,010 words). The sentences were collected from grammar books, biographies, news, and fictional and religious texts. The researchers made an effort to include different types of 25 sentences. The data is included in the UD website1 . 3. 3.3. UUParser (version 2.3) is a pipeline system for dependency parsing that consists of three components (de Lhoneux et al., 2017). The first component performs joint word and sentence segmentation, the second predicts POS tags and morphological features, and the third predicts dependency relation from the words and the POS tags (Nivre, 2008). The word and sentence segmentation is jointly modeled as character-level sequence labeling, employing bidirectional recurrent neural networks (BiRNN) together with CRF (de Lhoneux et al., 2017). The predictions of POS tags and morphological features are accomplished using a Meta-BiLSTM model with contextsensitive token encoding. This method is adopted from the work of Bohnet et al. (Bohnet et al., 2018). The method applies BiLSTM to modeling both words and characters at the sentence level, giving the model access to the sentence context. The character and word models are combined in the Meta"
2020.rail-1.5,D18-1291,0,0.0254441,"Missing"
2020.rail-1.5,L16-1680,0,0.0274463,"Missing"
2020.rail-1.5,P18-1246,0,0.014598,"The first component performs joint word and sentence segmentation, the second predicts POS tags and morphological features, and the third predicts dependency relation from the words and the POS tags (Nivre, 2008). The word and sentence segmentation is jointly modeled as character-level sequence labeling, employing bidirectional recurrent neural networks (BiRNN) together with CRF (de Lhoneux et al., 2017). The predictions of POS tags and morphological features are accomplished using a Meta-BiLSTM model with contextsensitive token encoding. This method is adopted from the work of Bohnet et al. (Bohnet et al., 2018). The method applies BiLSTM to modeling both words and characters at the sentence level, giving the model access to the sentence context. The character and word models are combined in the Meta-BiLSTMs. In the Meta-BiLSTM, they concatenate the output, for each word, (of its context sensitive character and word-based embedding) and pass to another BiLSTM to create an additional combined context sensitive encoding. This is followed by a final MLP, whose output is passed onto a linear layer for POS tag prediction. The third component is dependency parsing, in which a greedy transition-based parser"
2020.rail-1.5,K18-2005,0,0.0270183,"t, we obtain an 83.79 LAS score using the UDPipe system. Better accuracy is achieved when the neural parsing system uses external resources like word embedding. Using such resources, the LAS score for UDPipe improves to 85.26. Our experiment shows that the neural networks can learn dependency relations better from limited data while segmentation and POS tagging require much data. Keywords: Dependency Parsing, Neural Network, Amharic 1. Introduction of Dozat et al. (Dozat et al., 2017) won the CoNLL2017 UD Shared Task. In the CoNLL2018 UD Shared Task, the winning system was that of Che et al. (Che et al., 2018). These systems have improved the neural network approach to parsing through the application of optimization functions and external resources such as word embedding. Nowadays, the state-of-the-art in parsing is neural networks incorporating word embedding. In this paper, we present our experiment on developing a dependency parser for Amharic using the state-of-the-art method. The remaining sections are structured as follows. Section 2 gives a brief background about the process of developing the Amharic treebank and describes the treebank we used for training the neural network models. Section"
2020.rail-1.5,K18-2001,0,0.0213055,"Missing"
2020.rail-1.5,D14-1082,0,0.0609103,"ike Arabic, Hebrew, and Amharic present a special challenges for the design of a dependency grammar due to their complex morphology and agreement. Starting from the mid 20th century, research in NLP has shifted to neural networks. In this line of research, language is represented in the form of non-linear features. The approach is inspired by the the way computation works in the brain (Goldberg, 2017). It is applied in areas such as machine translation, computer vision, and speech recognition. With regards to parsing, the wave of neural network parsers was started in 2014 by Chen and Manning (Chen and Manning, 2014), who presented a fast and accurate transition-based parser using neural networks. Since then other parsing models have employed various techniques such as stack LSTM (Dyer et al., 2015; Kiperwasser and Goldberg, 2016), global normalization (Andor et al., 2016), biaffine attention (Dozat and Manning, 2017) or recurrent neural network grammars (Dyer et al., 2016; Kuncoro et al., 2017). Due to the existence of a treebank for different languages and the shared task of CoNLL 2017 (Zeman et al., 2017) and 2018 (Zeman et al., 2018), large improvements in dependency parsing using neural networks have"
2020.rail-1.5,K17-3022,0,0.0347662,"Missing"
2020.rail-1.5,W11-3802,0,0.0678224,"Missing"
2020.rail-1.5,P15-1033,0,0.01397,"in NLP has shifted to neural networks. In this line of research, language is represented in the form of non-linear features. The approach is inspired by the the way computation works in the brain (Goldberg, 2017). It is applied in areas such as machine translation, computer vision, and speech recognition. With regards to parsing, the wave of neural network parsers was started in 2014 by Chen and Manning (Chen and Manning, 2014), who presented a fast and accurate transition-based parser using neural networks. Since then other parsing models have employed various techniques such as stack LSTM (Dyer et al., 2015; Kiperwasser and Goldberg, 2016), global normalization (Andor et al., 2016), biaffine attention (Dozat and Manning, 2017) or recurrent neural network grammars (Dyer et al., 2016; Kuncoro et al., 2017). Due to the existence of a treebank for different languages and the shared task of CoNLL 2017 (Zeman et al., 2017) and 2018 (Zeman et al., 2018), large improvements in dependency parsing using neural networks have been reported. For instance, the neural graph-based parser 2. Background A parsing system may use a model which is learned from a treebank to predict the grammatical structure for new"
2020.rail-1.5,N16-1024,0,0.0286271,"works in the brain (Goldberg, 2017). It is applied in areas such as machine translation, computer vision, and speech recognition. With regards to parsing, the wave of neural network parsers was started in 2014 by Chen and Manning (Chen and Manning, 2014), who presented a fast and accurate transition-based parser using neural networks. Since then other parsing models have employed various techniques such as stack LSTM (Dyer et al., 2015; Kiperwasser and Goldberg, 2016), global normalization (Andor et al., 2016), biaffine attention (Dozat and Manning, 2017) or recurrent neural network grammars (Dyer et al., 2016; Kuncoro et al., 2017). Due to the existence of a treebank for different languages and the shared task of CoNLL 2017 (Zeman et al., 2017) and 2018 (Zeman et al., 2018), large improvements in dependency parsing using neural networks have been reported. For instance, the neural graph-based parser 2. Background A parsing system may use a model which is learned from a treebank to predict the grammatical structure for new sentences. This method of parser development is called datadriven parsing. The goal of data-driven dependency parsing is to learn to accurately predict dependency graphs from the"
2020.rail-1.5,L18-1550,0,0.0160627,"from the system seeing gold lemmas or the input data. 4.2. Parsing Model Enhanced with External Resources We carried out another experiment in which models can be enhanced by external resources. One way of enhancing a model is to use a pre-trained word embedding. For this purpose, we used the trained model for Amharic using fasttext7 . The data for training the model is from Wikipedia and Common Crawl8 . The models were trained using continuous bag of words (CBOW) with position-weights, in dimension 300 and considered character of n-grams of length 5 with a window of size 5 and 10 negatives (Grave et al., 2018). In this comparison, we have included the Turku parser as it requires a pre-trained word embedding. Table 2 presents the results when each model is enhanced with word embedding. Before we discuss the comparison, we describe the experimental set up we followed. The standard practice of preparing data is to divide the data into training, development and test set, usually 80 percent for training, and 10 percent each for development and testing. However, the data set we have is too small to be divided into such proportions. Instead, we carry out ten-fold cross-validation (Zeman et al., 2017), ran"
2020.rail-1.5,P14-5003,0,0.0451555,"Missing"
2020.rail-1.5,W10-1401,0,0.0259598,"Missing"
2020.rail-1.5,K17-3001,0,0.0319144,"Missing"
2020.rail-1.5,P11-2033,0,0.123537,"Missing"
2020.sdp-1.16,E06-1002,0,0.00895393,"description for an output vector. In our grounding, instead of directly associating each math word to a text description, we put an intermediate procedure: making groups each of which consists of math words referring to an identical mathematical concept. For instance in Figure 1, the first and the third y belong to a group because they both refer to the same function, while the second one is in another group because it refers to a vector. It is notable that the grounding is similar to some established tasks, namely coreference resolution (Sukthanker et al., 2020) and named entity recognition (Bunescu and Pasca, 2006). In this work, we checked the feasibility of the proposing task direction for the grounding. For this purpose, we made a long annotated scientific paper in which all formulae are annotated with math word spans and text descriptions of the corresponding mathematical concepts. The math words in the annotated paper which refer to the same mathematical concept are tied together in a group. We did the annotation for an entire paper rather than small fragments of texts to disclose the flexibility of math word usage. Through the analysis of our annotated paper, we revealed that the meanings of math"
2020.sdp-1.16,N18-1028,0,0.0278746,"the natural logarithm of the value ?”. Though their purpose is close to ours, we annotated not only descriptions but also a few pieces of additional information, i.e., affix types and group information (what concept the word refer to). In the terms of linguistics, these two can be regarded respectively as word spans and coreference information. Additionally, we did the annotation for a longer document than their target papers with coherency. We were especially interested in longer documents so that we can analyze how diverse meanings of mathematical concepts can be. The variable typing task (Stathopoulos et al., 2018) is also closely relevant to our goal. Their task is simply associating mathematical type (technical terms referring to mathematical concepts) to each variable in STEM documents. For example, for a sentence 139 Let ? be a parabolic subgroup of GL(?) with Levi decomposition ? = ? ?, where ? is the unipotent radical. (Stathopoulos et al., 2018) they assigned the “parabolic subgroup” and “unipotent radical” respectively to variables ? and ? as their mathematical types. Based on arXMLiv, they introduced their own dataset, which includes 33,524 labeled variables in 7,803 sentences. Their work reThe"
2020.signlang-1.3,bono-etal-2014-colloquial,1,0.792519,"ons for utterance units: one is to develop corpus linguistics research for both signed and spoken corpora; the other is to build an informatics system that includes, but is not limited to, a machine translation system for sign languages. Keywords: utterance unit, annotation, sign language dialogue 1. directions, to identify utterance unit. The method is based on classic observations in a research field of Conversation Analysis (CA) and Interaction Studies for spoken social interactions. Introduction This paper describes a method for annotating the Japanese Sign Language (JSL) dialogue corpus (Bono et al., 2014)1. Some linguists, including Deaf researchers who are interested in collecting sign language dialogue, began collecting data in April 2011. When we started, the general purpose of the project was to increase awareness of sign language as a distinct language in Japan. However, the academic aspects of the study recently became clear through interdisciplinary collaboration with engineering researchers, i.e., for natural language processing and image processing. In this paper, we introduce a preliminary result of our annotation process and annotated data, while explaining the concept of a ‘utteran"
2020.signlang-1.3,den-etal-2010-two,0,0.0204516,"y of translated texts (e.g. JSL to Japanese). As widely known, there are some functional and grammatical utterance-final particles in Japanese, such as ne (ね), yo (よ), yone (よね) etc., they are possibly a signal of identifying interactional boundary. On the other hands, there is no functional and grammatical manual signs in JSL. In case of sign languages, these kinds of utterance final elements are spread in multimodal way, such as facial expressions and body postures. Utterance Unit The concept of utterance unit was already provided to segmenting and annotating spontaneous Japanese dialogues (Den et al. 2010; Maruyama et al., in print). They propose a way of annotating utterance unit in two levels by emerging four linguistic and phonetic schemes, interpausal units, intonation-units, clause-units and pragmatic units. In this paper, we define the concept of utterance unit for segmenting and annotating JSL dialogue data. We utilize JSL signer’s native sense which is related to not only grammatical features but also multimodal features, such as mouth movements, non-manual movements, and gaze 2.3 Turn Constructional Units (TCUs) in CA First of all, we had to introduce a classic concept of interaction-"
2021.alvr-1.3,D14-1179,0,0.00826614,"Missing"
2021.alvr-1.3,W14-3348,0,0.107592,"trees as intermediate representations can better help the model learn how to apply the specified syntactic information to the captions and the intermediate representations can give users an intuitive impression on which part of the captions’ syntactic structures is controlled. Finally, we propose a syntactic dependencybased evaluation metric which evaluates whether the generated captions have been controlled in terms of syntactic structures. Our metric is computed based on the overlap of syntactic dependencies which is different from existing metrics like BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2018) which rely on the overlap of ngrams or semantic graphs. Empirical results show that image captions generated by our model are effectively controlled in terms of specified words and their syntactic structures. Introduction Controllable image captioning emerges as a popular research topic in recent years. Existing works attempt to enhance models’ controllability and captions’ diversity by controlling the attributes of image captions such as style (Mathews et al., 2016), sentiments (Gan et al., 2017), contents (D"
2021.alvr-1.3,Q14-1006,0,0.0513033,"n Generator The caption generator takes the syntactic dependency tree generated in the first step as input and encodes it with the syntactic dependency tree encoder into syntactic dependency tree features. The caption generator combines it with image features extracted in the first step and use the combined features to initialize the LSTM decoder (Hochreiter and Schmidhuber, 1997) to generate the caption. 4 Experiment Preparing Datasets with Partial Dependency Trees For evaluation, we apply two methods to create partial dependency trees for on Microsoft COCO (Chen et al., 2015) and Flickr30k (Young et al., 2014). The first method extracts partial dependency trees from reference captions. We parsing reference captions to syntactic dependency trees using Spacy 2 and then randomly sample subsets from each syntactic dependency tree. Sampled partial dependency trees are then paired with corresponding reference captions. The dataset created by this procedure is denoted as testgold in Section 5. The other method creates partial dependency trees from images in two steps: (1) we first train a syntactic dependency classifier to predict syntactic dependencies for an input image. (2) Predicted syntactic dependen"
2021.alvr-1.3,P82-1020,0,0.769779,"Missing"
2021.alvr-1.3,W04-1013,0,0.135425,"ns can better help the model learn how to apply the specified syntactic information to the captions and the intermediate representations can give users an intuitive impression on which part of the captions’ syntactic structures is controlled. Finally, we propose a syntactic dependencybased evaluation metric which evaluates whether the generated captions have been controlled in terms of syntactic structures. Our metric is computed based on the overlap of syntactic dependencies which is different from existing metrics like BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2018) which rely on the overlap of ngrams or semantic graphs. Empirical results show that image captions generated by our model are effectively controlled in terms of specified words and their syntactic structures. Introduction Controllable image captioning emerges as a popular research topic in recent years. Existing works attempt to enhance models’ controllability and captions’ diversity by controlling the attributes of image captions such as style (Mathews et al., 2016), sentiments (Gan et al., 2017), contents (Dai et al., 2018; Co"
2021.alvr-1.3,P02-1040,0,0.111372,"generating syntactic dependency trees as intermediate representations can better help the model learn how to apply the specified syntactic information to the captions and the intermediate representations can give users an intuitive impression on which part of the captions’ syntactic structures is controlled. Finally, we propose a syntactic dependencybased evaluation metric which evaluates whether the generated captions have been controlled in terms of syntactic structures. Our metric is computed based on the overlap of syntactic dependencies which is different from existing metrics like BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2018) which rely on the overlap of ngrams or semantic graphs. Empirical results show that image captions generated by our model are effectively controlled in terms of specified words and their syntactic structures. Introduction Controllable image captioning emerges as a popular research topic in recent years. Existing works attempt to enhance models’ controllability and captions’ diversity by controlling the attributes of image captions such as style (Mathews et al., 2016), sentim"
2021.alvr-1.3,P15-1150,0,0.0213654,"Missing"
2021.argmining-1.11,P16-2089,0,0.070244,"onclusion. Reasonableness is about acceptability in the dialectical context. Our method can serve in the dialectical context because we can model the validity of an argument and its counterargument together in the same Bayesian network. On the other hand, we do not focus on effectiveness which relates to an emotional appeal and the style of an argument. The previous models for quantitatively assessing the quality of an argument, no matter which dimension of quality is being addressed, predict a score with either feature-based machine learning (Persing et al., 2010; Persing and Ng, 2013, 2015; Ghosh et al., 2016; Wachsmuth et al., 2016; Wachsmuth and Stein, 2017; Ke et al., 2019; Wachsmuth and Werner, 2020) or neural networks (Ke et al., 2018; Lauscher et al., 2020). In contrast, our method uses a probabilistic model to calculate the validity. In argumentation theory, the probabilistic model for argument assessment is not mature. Baroni et al. (2018) provide a comprehensive survey of formal argumentation. In chapter 2 of the book, Prakken states ‘systematic studies of the combination of argumentation with probability were sparse.’ Our method is based on Bayesian networks. The use of Bayesian networks"
2021.argmining-1.11,P19-1390,0,0.0166552,"text. Our method can serve in the dialectical context because we can model the validity of an argument and its counterargument together in the same Bayesian network. On the other hand, we do not focus on effectiveness which relates to an emotional appeal and the style of an argument. The previous models for quantitatively assessing the quality of an argument, no matter which dimension of quality is being addressed, predict a score with either feature-based machine learning (Persing et al., 2010; Persing and Ng, 2013, 2015; Ghosh et al., 2016; Wachsmuth et al., 2016; Wachsmuth and Stein, 2017; Ke et al., 2019; Wachsmuth and Werner, 2020) or neural networks (Ke et al., 2018; Lauscher et al., 2020). In contrast, our method uses a probabilistic model to calculate the validity. In argumentation theory, the probabilistic model for argument assessment is not mature. Baroni et al. (2018) provide a comprehensive survey of formal argumentation. In chapter 2 of the book, Prakken states ‘systematic studies of the combination of argumentation with probability were sparse.’ Our method is based on Bayesian networks. The use of Bayesian networks in argumentation has been studied through practice rather than theo"
2021.argmining-1.11,W19-4012,0,0.0606834,"Missing"
2021.argmining-1.11,D10-1023,0,0.0219996,"is twofold. and the relevance of a premise to a conclusion. Reasonableness is about acceptability in the dialectical context. Our method can serve in the dialectical context because we can model the validity of an argument and its counterargument together in the same Bayesian network. On the other hand, we do not focus on effectiveness which relates to an emotional appeal and the style of an argument. The previous models for quantitatively assessing the quality of an argument, no matter which dimension of quality is being addressed, predict a score with either feature-based machine learning (Persing et al., 2010; Persing and Ng, 2013, 2015; Ghosh et al., 2016; Wachsmuth et al., 2016; Wachsmuth and Stein, 2017; Ke et al., 2019; Wachsmuth and Werner, 2020) or neural networks (Ke et al., 2018; Lauscher et al., 2020). In contrast, our method uses a probabilistic model to calculate the validity. In argumentation theory, the probabilistic model for argument assessment is not mature. Baroni et al. (2018) provide a comprehensive survey of formal argumentation. In chapter 2 of the book, Prakken states ‘systematic studies of the combination of argumentation with probability were sparse.’ Our method is based on"
2021.argmining-1.11,P13-1026,0,0.0154949,"elevance of a premise to a conclusion. Reasonableness is about acceptability in the dialectical context. Our method can serve in the dialectical context because we can model the validity of an argument and its counterargument together in the same Bayesian network. On the other hand, we do not focus on effectiveness which relates to an emotional appeal and the style of an argument. The previous models for quantitatively assessing the quality of an argument, no matter which dimension of quality is being addressed, predict a score with either feature-based machine learning (Persing et al., 2010; Persing and Ng, 2013, 2015; Ghosh et al., 2016; Wachsmuth et al., 2016; Wachsmuth and Stein, 2017; Ke et al., 2019; Wachsmuth and Werner, 2020) or neural networks (Ke et al., 2018; Lauscher et al., 2020). In contrast, our method uses a probabilistic model to calculate the validity. In argumentation theory, the probabilistic model for argument assessment is not mature. Baroni et al. (2018) provide a comprehensive survey of formal argumentation. In chapter 2 of the book, Prakken states ‘systematic studies of the combination of argumentation with probability were sparse.’ Our method is based on Bayesian networks. Th"
2021.argmining-1.11,P15-1053,0,0.0451476,"Missing"
2021.argmining-1.11,reed-etal-2008-language,0,0.307052,"rpose of argument evaluation. Stab and Gurevych (2014) annotate argument components (major claim, claim, and premises) and relations (support and attack) in persuasive essays. Ghosh et al. (2016) apply the same annotation as Stab and Gurevych (2014) to TOEFL essays associated with holistic scores (high/medium/low). Our dataset consists of pairs of a short argumentative text and its corresponding Bayesian network for modeling the validity of arguments. The major challenge in dataset creation is the difficulty of annotating argumentation schemes. Among existing datasets, the AraucariaDB corpus (Reed et al., 2008) is the largest as far as we know and includes approximately 660 manually annotated arguments (Feng and Hirst, 2011). Lawrence et al. (2019) state that annotated corpora of argumentation schemes are scarce, small, and unrepresentative. They provide an annotation tool to address the issue. We adopt a different approach inspired by Wang et al. (2015), in which they create training data for a semantic parser by generating canonical utterances from logical forms and paraphrasing them using crowdsourcing to reduce annotation effort. We employ a procedure to generate an argument text from a pre-buil"
2021.argmining-1.11,C14-1142,0,0.0286265,"networks 113 and argumentation schemes are compatible, argumentation schemes are used for assisting manual network construction by humans. It does not aim to automatically construct a network from the argumentative text. We combine Bayesian networks with predicate logic towards automatic network construction using natural language processing. In addition, we also prepare fragments for the argumentation schemes that are not covered in the legal domain, such as practical reasoning. 2.3 Datasets A few datasets annotate argument structures in argument texts for the purpose of argument evaluation. Stab and Gurevych (2014) annotate argument components (major claim, claim, and premises) and relations (support and attack) in persuasive essays. Ghosh et al. (2016) apply the same annotation as Stab and Gurevych (2014) to TOEFL essays associated with holistic scores (high/medium/low). Our dataset consists of pairs of a short argumentative text and its corresponding Bayesian network for modeling the validity of arguments. The major challenge in dataset creation is the difficulty of annotating argumentation schemes. Among existing datasets, the AraucariaDB corpus (Reed et al., 2008) is the largest as far as we know an"
2021.argmining-1.11,J17-3005,0,0.0177745,"ans_to(X, Y) bring_about(Y) Idiom Combination an argumentative text Idiom Selection Should the Death Penalty Be Allowed? analyze We should introduce the death penalty. The death penalty deters crime. ... means_to(DP, DC) bring_about(DP) Idiom Instantioation X ← DP: the death penalty Y ← DC: deterring crime bring_about(DC) Figure 2: Overall Process of Our Methodology patterns that appear in the actual argument we analyze. Our idiom covers 25 out of 60 argumentation schemes in the Walton et al. (2008)’s list. Regarding the design of idioms, unlike annotation schemes of argument mining datasets (Stab and Gurevych, 2017, 2014; Peldszus and Stede, 2015), the direction of an edge is not necessarily from a premise to a conclusion. The reason is that an idiom is designed to express the reasoning structure behind the justification of an argument. Although the justifications described by argumentation schemes often involve logical leaps, we can sometimes find a deductive relationship among premises and a conclusion. For example, practical reasoning concludes ‘I ought to carry out this action A’ from the two premises, ‘I have a goal G’ and ‘carrying out this action A is a means to realize G.’ That justification is"
2021.argmining-1.11,D18-1402,0,0.0142709,"ext and annotating a network. In this work, we adopt the procedure of generating an argument text from a pre-built network. 4.1 Dataset Creation Procedure Dataset creation is done in the following steps. Create a Bayesian network from a wide range of discussions on a particular topic. ProCon.org is a website that collects a wide range of opinions for and against controversial issues, mainly in the United States. We select six topics (the death penalty, gun control, minimum wage, legal abortion, school uniforms, and nuclear power) that are also selected in the existing argument mining dataset (Stab et al., 2018). We divide each topic into several subtopics. For each subtopic, we summarize the main points of discussion on the site and create a Bayesian network by applying network idioms. For example, for the subtopic on the death penalty, ’Does the death penalty deter crime?’, we create a network like Fig. 1. We also create a text corresponding to the network we have created. This procedure is carried out by the author. Decompose the built network into network fragments. We decompose the built Bayesian network, which represents the entire argument of each subtopic, into network fragments so that one f"
2021.argmining-1.11,C16-1158,0,0.0135538,"eness is about acceptability in the dialectical context. Our method can serve in the dialectical context because we can model the validity of an argument and its counterargument together in the same Bayesian network. On the other hand, we do not focus on effectiveness which relates to an emotional appeal and the style of an argument. The previous models for quantitatively assessing the quality of an argument, no matter which dimension of quality is being addressed, predict a score with either feature-based machine learning (Persing et al., 2010; Persing and Ng, 2013, 2015; Ghosh et al., 2016; Wachsmuth et al., 2016; Wachsmuth and Stein, 2017; Ke et al., 2019; Wachsmuth and Werner, 2020) or neural networks (Ke et al., 2018; Lauscher et al., 2020). In contrast, our method uses a probabilistic model to calculate the validity. In argumentation theory, the probabilistic model for argument assessment is not mature. Baroni et al. (2018) provide a comprehensive survey of formal argumentation. In chapter 2 of the book, Prakken states ‘systematic studies of the combination of argumentation with probability were sparse.’ Our method is based on Bayesian networks. The use of Bayesian networks in argumentation has be"
2021.argmining-1.11,E17-1017,0,0.0165751,"a conclusion is plausible if all premises are true. • We create a dataset consisting of pairs of In the legal domain, argumentation schemes help an argumentative text and a corresponding to construct a Bayesian network though network Bayesian network while reducing an annotaconstruction is manual. In the analysis of court tion effort. (Sec. 4) cases, it has been attempted to calculate the validity of hypotheses based on evidence or testimony 2 Related Work using Bayesian networks (Fenton et al., 2013; Vlek 2.1 Argument Validity Assessment et al., 2014). Since the arbitrariness of the network Wachsmuth et al. (2017) divided argumentation construction procedure is an issue, Timmer (2017) quality into three main dimensions, Cogency, Ef- transforms an argumentation scheme that can be fectiveness, and Reasonableness, through compre- used in court cases into a fragment of a Bayesian hensive survey. Our method mainly focuses on co- network which provides a typical network structure. gency which relates to the acceptability of premises While the research shows that Bayesian networks 113 and argumentation schemes are compatible, argumentation schemes are used for assisting manual network construction by humans."
2021.argmining-1.11,2020.coling-main.592,0,0.015714,"can serve in the dialectical context because we can model the validity of an argument and its counterargument together in the same Bayesian network. On the other hand, we do not focus on effectiveness which relates to an emotional appeal and the style of an argument. The previous models for quantitatively assessing the quality of an argument, no matter which dimension of quality is being addressed, predict a score with either feature-based machine learning (Persing et al., 2010; Persing and Ng, 2013, 2015; Ghosh et al., 2016; Wachsmuth et al., 2016; Wachsmuth and Stein, 2017; Ke et al., 2019; Wachsmuth and Werner, 2020) or neural networks (Ke et al., 2018; Lauscher et al., 2020). In contrast, our method uses a probabilistic model to calculate the validity. In argumentation theory, the probabilistic model for argument assessment is not mature. Baroni et al. (2018) provide a comprehensive survey of formal argumentation. In chapter 2 of the book, Prakken states ‘systematic studies of the combination of argumentation with probability were sparse.’ Our method is based on Bayesian networks. The use of Bayesian networks in argumentation has been studied through practice rather than theory, as in the case of the leg"
2021.argmining-1.11,P15-1129,0,0.0337822,"Missing"
2021.inlg-1.11,2021.eacl-main.125,1,0.751031,"g (Kim and Choi, 2020) and data-totext (Taniguchi et al., 2019). Various methods for encoding video frames have been actively studied (Dosovitskiy et al., 2021); commentaries often include comments that focus on the positional relation between cars, which requires a more ﬁne-grained understanding of video frames. The performance of current vision encoders still needs to be evaluated. Data-totext is the task of converting structured data into natural language, which has been applied to the domain of ﬁnance (Murakami et al., 2017; Aoki et al., 2018, 2021; Uehara et al., 2020), weather forecast (Murakami et al., 2021), a summary of sports matches (Puduppully and Lapata, 2021; Iso et al., 2019) and live sports commentary (Taniguchi et al., 2019). The inputs used for existing studies are time-sequence numerical data (Murakami et al., 2017), tables (Puduppully and Lapata, 2021; Gardent et al., 2017) or simulated images (Murakami et al., 2021). These models focus on neural networkbased approaches; however, data-to-text tasks have been studied for a long time (see a survey paper (Gatt and Krahmer, 2018) for details). 3 Dataset We describe the procedure used to create our dataset. We then show its statistics and"
2021.inlg-1.11,P17-1017,0,0.0201372,"tary in real-time, we need to solve at least two tasks: timing identiﬁcation and utterance generation tasks. However, existing studies focus on the latter, where the timings are given, for example, minute-by-minute updates (Kubo et al., 2013). Unlike baseball, the timing identiﬁcation task for race commentary is not trivial because a race cannot be segmented simply. Datasets play important roles in studies on generation. Existing datasets for generation tasks contain data in a single modality, such as, videos (Zhou et al., 2018; Krishna et al.) or structured data (Puduppully and Lapata, 2021; Gardent et al., 2017). We propose a new largescale dataset that contains transcribed commentaries aligned with videos and structured numerical data. Our setting can be considered as a combination of two different research topics: video captioning (Kim and Choi, 2020) and data-totext (Taniguchi et al., 2019). Various methods for encoding video frames have been actively studied (Dosovitskiy et al., 2021); commentaries often include comments that focus on the positional relation between cars, which requires a more ﬁne-grained understanding of video frames. The performance of current vision encoders still needs to be"
2021.inlg-1.11,P02-1040,0,0.119124,"en state of the LSTM in the decoder side is 230, which is the sum of the size of the encoded images, textual information and structured data. The size of the character embeddings in the decoder is set to 100. We use separate vocabularies for the textual input and the target text. We use Adam (Kingma and Ba, 2015) with several initial learning rates ranging from 10−3 to 10−5 for optimizing parameters. We continue the training iterations until the loss in the validation dataset does not decrease for 10 epochs. We conduct the utterance generation experiments for the gold timestamps. We use BLEU (Papineni et al., 2002) to evaluate the baseline models for this task. The scores are shown in Table 5. The model based only on telemetry data worked well. Adding textual information improved BLEU score if the learning rate is set to lower values i.e., 10−4 or 10−5 . However, we obtained a very low BLEU score when we used only vision-based input. Adding vision information to struct+text model degraded the score if the learning rate is set to 10−3 or 10−4 . Even with a smaller learning rate, 10−5 , vision information did not signiﬁcantly improve the performance. 5.2 Timing Identiﬁcation 6 Discussion the average gap b"
2021.inlg-1.11,P82-1020,0,0.691425,"Missing"
C02-1100,P90-1021,0,0.144802,"nciple consists of only one element. When we lose just one element in the head feature principle, a large amount of information in the daughter’s substructure is not propagated to its mother. As Copestake (1993) mentioned, another problem in Carpenter’s default unification is that the time complexity for finding the optimal answer of default unification is exponential because we have to verify the unifiability of the power set of constraints in a default feature structure. Here, we propose ideal lenient default unifica2 Background Default unification has been investigated by many researchers (Bouma, 1990; Russell et al., 1991; Copestake, 1993; Carpenter, 1993; Lascarides and Copestake, 1999) in the context of developing lexical semantics. Here, we first explain the definition given by Carpenter (1993) because his definition is both concise and comprehensive. 2.1 Carpenter’s Default Unification Carpenter proposed two types of default unification, credulous default unification and skeptical default unification. (Credulousn Default¯ Unification) o &lt; ¯ 0 is maximal such that F tc G = F t G0 ¯ G v G 0 F t G is defined (Skeptical Default Unification) &lt; &lt; F ts G = (F tc G) F F is called a strict fea"
C02-1100,P00-1058,0,0.0182108,"n, i.e., the extracted rules are not frequently triggered because they can be applied to feature structures that are exactly equivalent to their daughter’s part. By collecting a number of such rules,3 a grammar becomes wide-coverage with some overgeneration. They can be regarded as exceptions in a grammar, which are difficult to be captured only by propagating information from daughters to a mother. This approach can be regarded as a kind of explanation-based learning (Samuelsson and Rayner, 1991). The explanation-based learning method is recently attracting researcher’s attention (Xia, 1999; Chiang, 2000) because their parsers are comparative to the state-of-the-art parsers in terms of precision and recall. In the context of unificationbased grammars, Neumann (1994) has developed a parser running with an HPSG grammar learned by explanation-based learning. It should be also noted that Kiyono and Tsujii (1993) exemplified the grammar extraction approach using offline parsing in the 3 Although the size of the grammar becomes very large, the extracted rules can be found by a hash algorithm very efficiently. This tractability helps to use this approach in practical applications. # of sentences Avg."
C02-1100,C92-2072,0,0.0428875,"cessing, thus, efficient and wide coverage parsing has been extensively pursued in natural language literature. This study aims at robust processing within the Head-driven Phrase Structure Grammar (HPSG) to extend the coverage of manually-developed HPSG grammars. The meaning of ‘robust processing’ is not limited to robust processing for ill-formed sentences found in a spoken language, but includes robust processing for sentences which are well-formed but beyond the grammar writer’s expectation. Studies of robust parsing within unification-based grammars have been explored by many researchers (Douglas and Dale, 1992; Imaichi and Matsumoto, 1995). They classified the errors found in analyzing ill-formed sentences into several categories to make them tractable, e.g., constraint violation, missing or extra elements, etc. In this paper, we focus on recovery from the constraint violation errors, which is a violation of feature values. All errors in agreement fall into this category. Since many of the grammatical components in HPSG are written as constraints represented by feature structures, many of the errors are expected to be recovered by the recovery of constraint violation errors. This paper proposes two"
C02-1100,E93-1027,1,0.777179,"grammar, which are difficult to be captured only by propagating information from daughters to a mother. This approach can be regarded as a kind of explanation-based learning (Samuelsson and Rayner, 1991). The explanation-based learning method is recently attracting researcher’s attention (Xia, 1999; Chiang, 2000) because their parsers are comparative to the state-of-the-art parsers in terms of precision and recall. In the context of unificationbased grammars, Neumann (1994) has developed a parser running with an HPSG grammar learned by explanation-based learning. It should be also noted that Kiyono and Tsujii (1993) exemplified the grammar extraction approach using offline parsing in the 3 Although the size of the grammar becomes very large, the extracted rules can be found by a hash algorithm very efficiently. This tractability helps to use this approach in practical applications. # of sentences Avg. length of sentences Training Corpus 5,903 23.59 Test Set A 1,480 23.93 Test Set B 100 6.63    Table 1: Corpus size and average length of sentences                                            Figure 4: The average"
C02-1100,J99-1002,0,0.181571,"the head feature principle, a large amount of information in the daughter’s substructure is not propagated to its mother. As Copestake (1993) mentioned, another problem in Carpenter’s default unification is that the time complexity for finding the optimal answer of default unification is exponential because we have to verify the unifiability of the power set of constraints in a default feature structure. Here, we propose ideal lenient default unifica2 Background Default unification has been investigated by many researchers (Bouma, 1990; Russell et al., 1991; Copestake, 1993; Carpenter, 1993; Lascarides and Copestake, 1999) in the context of developing lexical semantics. Here, we first explain the definition given by Carpenter (1993) because his definition is both concise and comprehensive. 2.1 Carpenter’s Default Unification Carpenter proposed two types of default unification, credulous default unification and skeptical default unification. (Credulousn Default¯ Unification) o &lt; ¯ 0 is maximal such that F tc G = F t G0 ¯ G v G 0 F t G is defined (Skeptical Default Unification) &lt; &lt; F ts G = (F tc G) F F is called a strict feature structure, whose information must not be lost, and G is called a default feature str"
C02-1100,J93-2004,0,0.0240333,"Missing"
C02-1100,P91-1028,0,0.0775035,"Missing"
C02-1100,W98-0141,1,0.8832,"Missing"
C04-1204,J97-4005,0,0.0510543,"ment labels in a predicate-argument structure are basically defined in a left-to-right order of syntactic realizations, while if we had a cue for a movement in the Penn Treebank, arguments are put in its canonical position in a predicate-argument structure. 3.2 Disambiguation model By grammar extraction, we are able to obtain a large lexicon together with complete derivation trees of HPSG, i.e, an HPSG treebank. The HPSG treebank can then be used as training data for the machine learning of the disambiguation model. Following recent research about disambiguation models on linguistic grammars (Abney, 1997; Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2003; Miyao et al., 2003; Malouf and van Noord, 2004), we apply a log-linear model or maximum entropy model (Berger et al., 1996) on HPSG derivations. We represent an HPSG sign as a tu, where is a lexical sign of the ple head word, is a part-of-speech, and is a symbol representing the structure of the sign (mostly corresponding to nonterminal symbols of the Penn Treebank). Given an HPSG schema and the distance between the head words of the head/nonhead daughter constituents, each (binary) branching of an HPSG derivation is represe"
C04-1204,P98-1013,0,0.0442569,"Missing"
C04-1204,J96-1002,0,0.00497721,"Missing"
C04-1204,A00-2018,0,0.420965,"with existing studies on the task of identifying PropBank annotations. 1 Introduction Recently, deep linguistic analysis has successfully been applied to real-world texts. Several parsers have been implemented in various grammar formalisms and empirical evaluation has been reported: LFG (Riezler et al., 2002; Cahill et al., 2002; Burke et al., 2004), LTAG (Chiang, 2000), CCG (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003), and HPSG (Miyao et al., 2003; Malouf and van Noord, 2004). However, their accuracy was still below the state-of-theart PCFG parsers (Collins, 1999; Charniak, 2000) in terms of the PARSEVAL score. Since deep parsers can output deeper representation of the structure of a sentence, such as predicate argument structures, several studies reported the accuracy of predicateargument relations using a treebank developed for each formalism. However, resources used for the evaluation were not available for other formalisms, and the results cannot be compared with each other. In this paper, we employ PropBank (Kingsbury and Palmer, 2002) for the evaluation of the accuracy of HPSG parsing. In the PropBank, semantic arguments of a predicate and their semantic roles a"
C04-1204,W03-1006,0,0.277134,"of HPSG parsing. In the PropBank, semantic arguments of a predicate and their semantic roles are manually annotated. Since the PropBank has been developed independently of any grammar formalisms, the results are comparable with other published results using the same test data. Interestingly, several studies suggested that the identification of PropBank annotations would require linguistically-motivated features that can be Jun’ichi Tsujii Department of Computer Science University of Tokyo CREST, JST tsujii@is.s.u-tokyo.ac.jp obtained by deep linguistic analysis (Gildea and Hockenmaier, 2003; Chen and Rambow, 2003). They employed a CCG (Steedman, 2000) or LTAG (Schabes et al., 1988) parser to acquire syntactic/semantic structures, which would be passed to statistical classifier as features. That is, they used deep analysis as a preprocessor to obtain useful features for training a probabilistic model or statistical classifier of a semantic argument identifier. These results imply the superiority of deep linguistic analysis for this task. Although the statistical approach seems a reasonable way for developing an accurate identifier of PropBank annotations, this study aims at establishing a method of dire"
C04-1204,2000.iwpt-1.9,0,0.0774359,"linguistic analysis of real-world text was impossible. Their success owed much to a consistent effort to maintain a wide-coverage LFG grammar, as well as varS ARG0-choose NP-1 they VP VP did n’t VP have S NP ARG0-choose VP *-1 to VP choose REL-choose ARG1-choose NP this particular moment Figure 1: Annotation of the PropBank ious techniques for robust parsing. However, the manual development of widecoverage linguistic grammars is still a difficult task. Recent progress in deep linguistic analysis has mainly depended on the acquisition of lexicalized grammars from annotated corpora (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000; Hockenmaier and Steedman, 2002a; Cahill et al., 2002; Frank et al., 2003; Miyao et al., 2004). This approach not only allows for the low-cost development of wide-coverage grammars, but also provides the training data for statistical modeling as a byproduct. Thus, we now have a basis for integrating statistical language modeling with deep linguistic analysis. To date, accurate parsers have been developed for LTAG (Chiang, 2000), CCG (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003), and LFG (Cahill et al., 2002; Burke et al., 2004). Those studies have open"
C04-1204,P00-1058,0,0.0827633,"th PropBank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations. 1 Introduction Recently, deep linguistic analysis has successfully been applied to real-world texts. Several parsers have been implemented in various grammar formalisms and empirical evaluation has been reported: LFG (Riezler et al., 2002; Cahill et al., 2002; Burke et al., 2004), LTAG (Chiang, 2000), CCG (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003), and HPSG (Miyao et al., 2003; Malouf and van Noord, 2004). However, their accuracy was still below the state-of-theart PCFG parsers (Collins, 1999; Charniak, 2000) in terms of the PARSEVAL score. Since deep parsers can output deeper representation of the structure of a sentence, such as predicate argument structures, several studies reported the accuracy of predicateargument relations using a treebank developed for each formalism. However, resources used for the evaluation were not available for other formalisms, a"
C04-1204,W03-1013,0,0.0325772,"ly defined in a left-to-right order of syntactic realizations, while if we had a cue for a movement in the Penn Treebank, arguments are put in its canonical position in a predicate-argument structure. 3.2 Disambiguation model By grammar extraction, we are able to obtain a large lexicon together with complete derivation trees of HPSG, i.e, an HPSG treebank. The HPSG treebank can then be used as training data for the machine learning of the disambiguation model. Following recent research about disambiguation models on linguistic grammars (Abney, 1997; Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2003; Miyao et al., 2003; Malouf and van Noord, 2004), we apply a log-linear model or maximum entropy model (Berger et al., 1996) on HPSG derivations. We represent an HPSG sign as a tu, where is a lexical sign of the ple head word, is a part-of-speech, and is a symbol representing the structure of the sign (mostly corresponding to nonterminal symbols of the Penn Treebank). Given an HPSG schema and the distance between the head words of the head/nonhead daughter constituents, each (binary) branching of an HPSG derivation is represented as a tuple , where"
C04-1204,P02-1042,0,0.109748,"from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations. 1 Introduction Recently, deep linguistic analysis has successfully been applied to real-world texts. Several parsers have been implemented in various grammar formalisms and empirical evaluation has been reported: LFG (Riezler et al., 2002; Cahill et al., 2002; Burke et al., 2004), LTAG (Chiang, 2000), CCG (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003), and HPSG (Miyao et al., 2003; Malouf and van Noord, 2004). However, their accuracy was still below the state-of-theart PCFG parsers (Collins, 1999; Charniak, 2000) in terms of the PARSEVAL score. Since deep parsers can output deeper representation of the structure of a sentence, such as predicate argument structures, several studies reported the accuracy of predicateargument relations using a treebank developed for each formalism. However, resources used for the evaluation were not available for other formalisms, and the results cannot be compared with each other. In this"
C04-1204,P02-1036,0,0.0638059,"Missing"
C04-1204,W03-1008,0,0.272538,"the evaluation of the accuracy of HPSG parsing. In the PropBank, semantic arguments of a predicate and their semantic roles are manually annotated. Since the PropBank has been developed independently of any grammar formalisms, the results are comparable with other published results using the same test data. Interestingly, several studies suggested that the identification of PropBank annotations would require linguistically-motivated features that can be Jun’ichi Tsujii Department of Computer Science University of Tokyo CREST, JST tsujii@is.s.u-tokyo.ac.jp obtained by deep linguistic analysis (Gildea and Hockenmaier, 2003; Chen and Rambow, 2003). They employed a CCG (Steedman, 2000) or LTAG (Schabes et al., 1988) parser to acquire syntactic/semantic structures, which would be passed to statistical classifier as features. That is, they used deep analysis as a preprocessor to obtain useful features for training a probabilistic model or statistical classifier of a semantic argument identifier. These results imply the superiority of deep linguistic analysis for this task. Although the statistical approach seems a reasonable way for developing an accurate identifier of PropBank annotations, this study aims at estab"
C04-1204,J02-3001,0,0.0540139,"argument (i.e., subject) and “this particular moment” for the 1st argument (i.e., object). Existing studies applied statistical classifiers to the identification of the PropBank or FrameNet annotations. Similar to many methods of applying machine learning to NLP tasks, they first formulated the task as identifying in a sentence each argument of a given predicate. Then, parameters of the identifier were learned from the annotated corpus. Features of a statistical model were defined as a pattern on a partial structure of the syntactic tree output by an automatic parser (Gildea and Palmer, 2002; Gildea and Jurafsky, 2002). Several studies proposed the use of deep linguistic features, such as predicate-argument relations output by a CCG parser (Gildea and Hockenmaier, 2003) and derivation trees output by an LTAG parser (Chen and Rambow, 2003). Both studies reported that the identification accuracy improved by introducing such deep linguistic features. Although deep analysis has not outperformed PCFG parsers in terms of the accuracy of surface structure, these results are implicitly supporting the necessity of deep linguistic analysis for the recognition of semantic relations. However, these results do not direc"
C04-1204,P02-1031,0,0.0161748,"ents: “they” for the 0th argument (i.e., subject) and “this particular moment” for the 1st argument (i.e., object). Existing studies applied statistical classifiers to the identification of the PropBank or FrameNet annotations. Similar to many methods of applying machine learning to NLP tasks, they first formulated the task as identifying in a sentence each argument of a given predicate. Then, parameters of the identifier were learned from the annotated corpus. Features of a statistical model were defined as a pattern on a partial structure of the syntactic tree output by an automatic parser (Gildea and Palmer, 2002; Gildea and Jurafsky, 2002). Several studies proposed the use of deep linguistic features, such as predicate-argument relations output by a CCG parser (Gildea and Hockenmaier, 2003) and derivation trees output by an LTAG parser (Chen and Rambow, 2003). Both studies reported that the identification accuracy improved by introducing such deep linguistic features. Although deep analysis has not outperformed PCFG parsers in terms of the accuracy of surface structure, these results are implicitly supporting the necessity of deep linguistic analysis for the recognition of semantic relations. However"
C04-1204,hockenmaier-steedman-2002-acquiring,0,0.127828,"ons, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations. 1 Introduction Recently, deep linguistic analysis has successfully been applied to real-world texts. Several parsers have been implemented in various grammar formalisms and empirical evaluation has been reported: LFG (Riezler et al., 2002; Cahill et al., 2002; Burke et al., 2004), LTAG (Chiang, 2000), CCG (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003), and HPSG (Miyao et al., 2003; Malouf and van Noord, 2004). However, their accuracy was still below the state-of-theart PCFG parsers (Collins, 1999; Charniak, 2000) in terms of the PARSEVAL score. Since deep parsers can output deeper representation of the structure of a sentence, such as predicate argument structures, several studies reported the accuracy of predicateargument relations using a treebank developed for each formalism. However, resources used for the evaluation were not available for other formalisms, and the results cannot be compared wit"
C04-1204,P02-1043,0,0.194943,"ons, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations. 1 Introduction Recently, deep linguistic analysis has successfully been applied to real-world texts. Several parsers have been implemented in various grammar formalisms and empirical evaluation has been reported: LFG (Riezler et al., 2002; Cahill et al., 2002; Burke et al., 2004), LTAG (Chiang, 2000), CCG (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003), and HPSG (Miyao et al., 2003; Malouf and van Noord, 2004). However, their accuracy was still below the state-of-theart PCFG parsers (Collins, 1999; Charniak, 2000) in terms of the PARSEVAL score. Since deep parsers can output deeper representation of the structure of a sentence, such as predicate argument structures, several studies reported the accuracy of predicateargument relations using a treebank developed for each formalism. However, resources used for the evaluation were not available for other formalisms, and the results cannot be compared wit"
C04-1204,P03-1046,0,0.437634,"representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations. 1 Introduction Recently, deep linguistic analysis has successfully been applied to real-world texts. Several parsers have been implemented in various grammar formalisms and empirical evaluation has been reported: LFG (Riezler et al., 2002; Cahill et al., 2002; Burke et al., 2004), LTAG (Chiang, 2000), CCG (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003), and HPSG (Miyao et al., 2003; Malouf and van Noord, 2004). However, their accuracy was still below the state-of-theart PCFG parsers (Collins, 1999; Charniak, 2000) in terms of the PARSEVAL score. Since deep parsers can output deeper representation of the structure of a sentence, such as predicate argument structures, several studies reported the accuracy of predicateargument relations using a treebank developed for each formalism. However, resources used for the evaluation were not available for other formalisms, and the results cannot be compared with each other. In this paper, we employ Pr"
C04-1204,P99-1069,0,0.0303224,"n a predicate-argument structure are basically defined in a left-to-right order of syntactic realizations, while if we had a cue for a movement in the Penn Treebank, arguments are put in its canonical position in a predicate-argument structure. 3.2 Disambiguation model By grammar extraction, we are able to obtain a large lexicon together with complete derivation trees of HPSG, i.e, an HPSG treebank. The HPSG treebank can then be used as training data for the machine learning of the disambiguation model. Following recent research about disambiguation models on linguistic grammars (Abney, 1997; Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2003; Miyao et al., 2003; Malouf and van Noord, 2004), we apply a log-linear model or maximum entropy model (Berger et al., 1996) on HPSG derivations. We represent an HPSG sign as a tu, where is a lexical sign of the ple head word, is a part-of-speech, and is a symbol representing the structure of the sign (mostly corresponding to nonterminal symbols of the Penn Treebank). Given an HPSG schema and the distance between the head words of the head/nonhead daughter constituents, each (binary) branching of an HPSG derivation is represented as a tuple , wher"
C04-1204,kingsbury-palmer-2002-treebank,0,0.512225,"SG (Miyao et al., 2003; Malouf and van Noord, 2004). However, their accuracy was still below the state-of-theart PCFG parsers (Collins, 1999; Charniak, 2000) in terms of the PARSEVAL score. Since deep parsers can output deeper representation of the structure of a sentence, such as predicate argument structures, several studies reported the accuracy of predicateargument relations using a treebank developed for each formalism. However, resources used for the evaluation were not available for other formalisms, and the results cannot be compared with each other. In this paper, we employ PropBank (Kingsbury and Palmer, 2002) for the evaluation of the accuracy of HPSG parsing. In the PropBank, semantic arguments of a predicate and their semantic roles are manually annotated. Since the PropBank has been developed independently of any grammar formalisms, the results are comparable with other published results using the same test data. Interestingly, several studies suggested that the identification of PropBank annotations would require linguistically-motivated features that can be Jun’ichi Tsujii Department of Computer Science University of Tokyo CREST, JST tsujii@is.s.u-tokyo.ac.jp obtained by deep linguistic analy"
C04-1204,H94-1020,0,0.0617959,"ances in deep linguistic analysis and the development of semantically annotated corpora. Section 3 describes the details of the implementation of an HPSG parser evaluated in this study. Section 4 discusses a problem in adopting PropBank for the performance evaluation of deep linguistic parsers and proposes its solution. Section 5 reports empirical evaluation of the accuracy of the HPSG parser. 2 Deep linguistic analysis and semantically annotated corpora Riezler et al. (2002) reported the successful application of a hand-crafted LFG (Bresnan, 1982) grammar to the parsing of the Penn Treebank (Marcus et al., 1994) by exploiting various techniques for robust parsing. The study was impressive because most researchers had believed that deep linguistic analysis of real-world text was impossible. Their success owed much to a consistent effort to maintain a wide-coverage LFG grammar, as well as varS ARG0-choose NP-1 they VP VP did n’t VP have S NP ARG0-choose VP *-1 to VP choose REL-choose ARG1-choose NP this particular moment Figure 1: Annotation of the PropBank ious techniques for robust parsing. However, the manual development of widecoverage linguistic grammars is still a difficult task. Recent progress"
C04-1204,P02-1035,0,0.468797,"ent relations. We could directly compare the output of HPSG parsing with PropBank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations. 1 Introduction Recently, deep linguistic analysis has successfully been applied to real-world texts. Several parsers have been implemented in various grammar formalisms and empirical evaluation has been reported: LFG (Riezler et al., 2002; Cahill et al., 2002; Burke et al., 2004), LTAG (Chiang, 2000), CCG (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003), and HPSG (Miyao et al., 2003; Malouf and van Noord, 2004). However, their accuracy was still below the state-of-theart PCFG parsers (Collins, 1999; Charniak, 2000) in terms of the PARSEVAL score. Since deep parsers can output deeper representation of the structure of a sentence, such as predicate argument structures, several studies reported the accuracy of predicateargument relations using a treebank developed for each formalism. However, resources use"
C04-1204,C88-2121,0,0.0261415,"Missing"
C04-1204,J03-4003,0,\N,Missing
C04-1204,C98-1013,0,\N,Missing
C08-2011,W02-1006,0,0.0377747,"y for an observation sequence x is calculated by p(y|x) = destroy man Sense number A sense number is the number of a sense of a word in W ORD N ET. Since senses of a word are ordered according to frequency, the sense number can act as a powerful feature for WSD, which offers a preference for frequent senses, and especially as a back-off feature, which enables our model to output the first sense when no other feature is available for that word. 2.3 ROOT &lt;ROOT> hX 1 exp λj fj (e, x, y) Z(x) e∈E,j i X + µk gk (v, x, y) (1) 3.3 Vertex features Most of the vertex features we use are those used by Lee and Ng (2002). All these features are combined with each of the four sense labels sn (v), and incorporated as gk in Equation (1). v∈V,k where E and V are the sets of edges and vertices, fj and gk are the feature vectors for an edge and a vertex, λj and µk are the weight vectors for them, and Z(x) is the normalization function. For a detailed description of TCRFs, see Tang et al. (2006). • Word form, lemma, and part of speech. • Word forms, lemmas, and parts of speech of the head and dependents in a dependency tree. 44 Development Brown-1 Brown-2 S ENSEVAL-3 #sentences 470 10,712 8,956 300 #words 5,178 100,"
C08-2011,W04-0838,0,0.0302444,"Inter-word sense dependencies Since the all-words task requires us to disambiguate all content words, it seems reasonable to assume that we could perform better WSD by considering the sense dependencies among words, and optimizing word senses over the whole sentence. Specifically, we base our model on the assumption that there are strong sense dependencies between a head word and its dependents in a dependency tree; therefore, we employ the dependency tree structures for modeling the sense dependencies. There have been a few WSD systems that incorporate the inter-word sense dependencies (e.g. Mihalcea and Faruque (2004)). However, to the extent of our knowledge, their effectiveness has not explicitly examined thus far for supervised WSD. Introduction Word sense disambiguation (WSD) is one of the fundamental underlying problems in computational linguistics. The task of WSD is to determine the appropriate sense for each polysemous word within a given text. Traditionally, there are two task settings for WSD: the lexical sample task, in which only one targeted word is disambiguated given its context, and the all-words task, in which all content words within a text are disambiguated. Whilst most of the WSD resear"
C08-2011,S07-1090,0,0.0290068,"Missing"
C08-2011,D07-1111,1,0.851866,"Missing"
C08-2011,W06-1670,0,0.0130526,"ORD N ET, with which each noun or verb synset is associated. Since 43 Coling 2008: Companion volume – Posters and Demonstrations, pages 43–46 Manchester, August 2008 ROOT they are originally introduced for ease of lexicographers’ work, their classification is fairly general, but not too abstract, and is hence expected to act as good coarse-grained semantic categories. The numbers of the supersenses are 26 and 15 for nouns and verbs. The effectiveness of the use of supersenses and other coarse-grained tagsets for WSD has been recently shown by several researchers (e.g. Kohomban and Lee (2005), Ciaramita and Altun (2006), and Mihalcea et al. (2007)). &lt;SBJ> &lt;ROOT> the destroy &lt;OBJ> confidence &lt;NMOD> &lt;NMOD> in &lt;PMOD> &lt;SBJ> &lt;OBJ> man confidence &lt;NMOD> : in bank bank Figure 1: An example sentence described as a dependency tree structure. 3 WSD Model using Tree-structured CRFs 3.1 Overview Let us consider the following sentence. (i) The man destroys confidence in banks. In the beginning, we parse a given sentence by using a dependency parser. The left-hand side of Figure 1 shows the dependency tree for Sentence (i) in the CoNLL-X dependency format. Next, we convert the outputted tree into a tree of content words,"
C08-2011,W04-0811,0,0.0948917,"Missing"
C08-2011,W04-0827,0,0.0766079,"Missing"
C08-2011,S07-1057,0,0.0315553,"Missing"
C08-2011,P05-1005,0,0.0163155,"cographers’ file ID in W ORD N ET, with which each noun or verb synset is associated. Since 43 Coling 2008: Companion volume – Posters and Demonstrations, pages 43–46 Manchester, August 2008 ROOT they are originally introduced for ease of lexicographers’ work, their classification is fairly general, but not too abstract, and is hence expected to act as good coarse-grained semantic categories. The numbers of the supersenses are 26 and 15 for nouns and verbs. The effectiveness of the use of supersenses and other coarse-grained tagsets for WSD has been recently shown by several researchers (e.g. Kohomban and Lee (2005), Ciaramita and Altun (2006), and Mihalcea et al. (2007)). &lt;SBJ> &lt;ROOT> the destroy &lt;OBJ> confidence &lt;NMOD> &lt;NMOD> in &lt;PMOD> &lt;SBJ> &lt;OBJ> man confidence &lt;NMOD> : in bank bank Figure 1: An example sentence described as a dependency tree structure. 3 WSD Model using Tree-structured CRFs 3.1 Overview Let us consider the following sentence. (i) The man destroys confidence in banks. In the beginning, we parse a given sentence by using a dependency parser. The left-hand side of Figure 1 shows the dependency tree for Sentence (i) in the CoNLL-X dependency format. Next, we convert the outputted tree in"
C08-2016,J96-1002,0,0.0941699,"ell et al., 1999). The junction tree algorithm is a generic algorithm for exact inference on any graphical model, and it allows for efficient inference on sparse graphs. The method converts a graph into a junction tree, which is a tree of cliques in the original graph. When we have a junction tree for each document, we can efficiently perform belief propagation in order to compute argmax in Equation (1), or the marginal probabilities of cliques and labels, necessary for the parameter estimation of machine learning classifiers, including perceptrons (Collins, 2002), and maximum entropy models (Berger et al., 1996). The computational complexity of the inference on junction trees is proportional to the exponential of the tree width, which is the maximum number of labels in a clique, minus one. An essential idea of this method is that a graphical model is constructed for each document. Even when features are defined on all pairs of labels, active features for a specific document are limited. When combined with feature selection, this method greatly increases the sparsity of the resulting graphs, which is key to efficiency. A weakness of this method comes from the assumption of feature sparseness. We are f"
C08-2016,W02-1001,0,0.397861,"ce, such as the junction tree algorithm (Cowell et al., 1999). The junction tree algorithm is a generic algorithm for exact inference on any graphical model, and it allows for efficient inference on sparse graphs. The method converts a graph into a junction tree, which is a tree of cliques in the original graph. When we have a junction tree for each document, we can efficiently perform belief propagation in order to compute argmax in Equation (1), or the marginal probabilities of cliques and labels, necessary for the parameter estimation of machine learning classifiers, including perceptrons (Collins, 2002), and maximum entropy models (Berger et al., 1996). The computational complexity of the inference on junction trees is proportional to the exponential of the tree width, which is the maximum number of labels in a clique, minus one. An essential idea of this method is that a graphical model is constructed for each document. Even when features are defined on all pairs of labels, active features for a specific document are limited. When combined with feature selection, this method greatly increases the sparsity of the resulting graphs, which is key to efficiency. A weakness of this method comes f"
C08-2016,W07-1017,0,0.0261428,"Missing"
C08-2016,W07-1013,0,0.187409,"of the decision for other labels, this method cannot be sensitive to label correlations, or the tendency of label cooccurrences. A recent research effort has been devoted to the modeling of label correlations. While a number of approaches have been proposed for dealing with label correlations (see Tsoumakas and c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 63 Coling 2008: Companion volume – Posters and Demonstrations, pages 63–66 Manchester, August 2008 Katakis (2007) for the comprehensive survey), the intuitively-appealing method is to incorporate features on two labels into the model (Ghamrawi and McCallum, 2005). The following label correlation feature indicates a cooccurrence of two labels and a word: ( fl,l0 ,w (x, y) = 3 cmc2007 reuters10 reuters90 # train 978 6,490 7,770 # test 976 2,545 3,019 # labels 45 10 90 card. 1.23 1.10 1.24 Table 1: Statistics of evaluation data sets l, l0 cmc2007 reuters10 reuters90 cx (w) if ∈ y, 0 otherwise. κ 1,000 5,000 5,000 ν 10 20 80 c 0 5 5 Table 2: Parameters for evaluation data sets A Method for Exact Inference A"
C10-1089,W08-0601,0,0.0955005,"heir method might remove important information for a given target relation. For example, they might accidentally simplify a noun phrase that is needed to extract the relation. Still, they improved overall PPI extraction recall using such simplifications. To remove unnecessary information from a sentence, some works have addressed sentence simplification by iteratively removing unnecessary phrases. Most of this work is not task-specific; it is intended to compress all information in a target sentence into a few words (Dorr et al., 2003; Vanderwende et al., 2007). Among them, Vickrey and Koller (2008) applied sentence simplification to semantic role labeling. With retaining all arguments of a verb, Vickrey simplified the sentence by removing some information outside of the verb and arguments. 3 Entity-Focused Sentence Simplification We simplify a target sentence using simple rules applicable to the output of a deep parser called Mogura (Matsuzaki et al., 2007), to remove noisy information for relation extraction. Our method relies on the deep parser; the rules depend on the Head-driven Phrase Structure Grammar (HPSG) used by Mogura, and all the rules are written for the parser Enju XML out"
C10-1089,H05-1091,0,0.0707068,"sed as an example relation extraction problem. A dozen simple rules are defined on output from a deep parser. Each rule specifically examines the entities in one target interaction pair. These simple rules were tested using several PPI corpora. The PPI extraction performance was improved on all the PPI corpora. Recently, machine-learning methods, boosted by NLP techniques, have proved to be effective for RE. These methods are usually intended to highlight or select the relation-related regions in parsed sentences using feature vectors or kernels. The shortest paths between a pair of entities (Bunescu and Mooney, 2005) or pair-enclosed trees (Zhang et al., 2006) are widely used as focus regions. These regions are useful, but they can include unnecessary sub-paths such as appositions, which cause noisy features. 1 Introduction Relation extraction (RE) is the task of finding a relevant semantic relation between two given target entities in a sentence (Sarawagi, 2008). Some example relation types are person–organization relations (Doddington et al., 2004), protein– protein interactions (PPI), and disease–gene associations (DGA) (Chun et al., 2006). Among the possible RE tasks, we chose the PPI extraction probl"
C10-1089,W09-1304,0,0.0196783,"es sufficient information to determine the value of the relation in these examples. Relation-related mentions remained for most of the simplification error cases. There were only five critical errors, which changed the truth-value of the relation, out of 46 errors in 241 pairs shown in Table 8. Please note that some rules can be dangerous for other relation extraction tasks. For example, the sentence clause rule could remove modality information (negation, speculation, etc.) modifying the clause, but there are few such cases in the PPI corpora (see Table 8). Also, the task of hedge detection (Morante and Daelemans, 2009) can be solved separately, in the original sentences, after the interacting pairs have been found. For example, in the BioNLP shared task challenge and the BioInfer corpus, interaction detection and modality are treated as two different tasks. Once other NLP tasks, like static relation (Pyysalo et 794 al., 2009) or coreference resolution, become good enough, they can supplement or even substitute some of the proposed rules. There are different difficulties in the BioInfer and AIMed corpora. BioInfer includes more complicated sentences and problems than the other corpora do, because 1) the appo"
C10-1089,W09-1301,1,0.461091,"Missing"
C10-1089,doddington-etal-2004-automatic,0,0.0213557,"ded to highlight or select the relation-related regions in parsed sentences using feature vectors or kernels. The shortest paths between a pair of entities (Bunescu and Mooney, 2005) or pair-enclosed trees (Zhang et al., 2006) are widely used as focus regions. These regions are useful, but they can include unnecessary sub-paths such as appositions, which cause noisy features. 1 Introduction Relation extraction (RE) is the task of finding a relevant semantic relation between two given target entities in a sentence (Sarawagi, 2008). Some example relation types are person–organization relations (Doddington et al., 2004), protein– protein interactions (PPI), and disease–gene associations (DGA) (Chun et al., 2006). Among the possible RE tasks, we chose the PPI extraction problem. PPI extraction is a major RE task; In this paper, we propose a method to remove information that is deemed unnecessary for RE. Instead of selecting the whole region between a target pair, the target sentence is simplified into simpler, pair-related, sentences using general, task-independent, rules. By addressing particularly the target entities, the rules do not affect important relation-related expressions between the target entities"
C10-1089,W03-0501,0,0.00953737,"k grammar parser by simplifying the target sentence in a general manner, so their method might remove important information for a given target relation. For example, they might accidentally simplify a noun phrase that is needed to extract the relation. Still, they improved overall PPI extraction recall using such simplifications. To remove unnecessary information from a sentence, some works have addressed sentence simplification by iteratively removing unnecessary phrases. Most of this work is not task-specific; it is intended to compress all information in a target sentence into a few words (Dorr et al., 2003; Vanderwende et al., 2007). Among them, Vickrey and Koller (2008) applied sentence simplification to semantic role labeling. With retaining all arguments of a verb, Vickrey simplified the sentence by removing some information outside of the verb and arguments. 3 Entity-Focused Sentence Simplification We simplify a target sentence using simple rules applicable to the output of a deep parser called Mogura (Matsuzaki et al., 2007), to remove noisy information for relation extraction. Our method relies on the deep parser; the rules depend on the Head-driven Phrase Structure Grammar (HPSG) used by"
C10-1089,P08-1040,0,0.189425,"eneral manner, so their method might remove important information for a given target relation. For example, they might accidentally simplify a noun phrase that is needed to extract the relation. Still, they improved overall PPI extraction recall using such simplifications. To remove unnecessary information from a sentence, some works have addressed sentence simplification by iteratively removing unnecessary phrases. Most of this work is not task-specific; it is intended to compress all information in a target sentence into a few words (Dorr et al., 2003; Vanderwende et al., 2007). Among them, Vickrey and Koller (2008) applied sentence simplification to semantic role labeling. With retaining all arguments of a verb, Vickrey simplified the sentence by removing some information outside of the verb and arguments. 3 Entity-Focused Sentence Simplification We simplify a target sentence using simple rules applicable to the output of a deep parser called Mogura (Matsuzaki et al., 2007), to remove noisy information for relation extraction. Our method relies on the deep parser; the rules depend on the Head-driven Phrase Structure Grammar (HPSG) used by Mogura, and all the rules are written for the parser Enju XML out"
C10-1089,P06-1104,0,0.166397,"ozen simple rules are defined on output from a deep parser. Each rule specifically examines the entities in one target interaction pair. These simple rules were tested using several PPI corpora. The PPI extraction performance was improved on all the PPI corpora. Recently, machine-learning methods, boosted by NLP techniques, have proved to be effective for RE. These methods are usually intended to highlight or select the relation-related regions in parsed sentences using feature vectors or kernels. The shortest paths between a pair of entities (Bunescu and Mooney, 2005) or pair-enclosed trees (Zhang et al., 2006) are widely used as focus regions. These regions are useful, but they can include unnecessary sub-paths such as appositions, which cause noisy features. 1 Introduction Relation extraction (RE) is the task of finding a relevant semantic relation between two given target entities in a sentence (Sarawagi, 2008). Some example relation types are person–organization relations (Doddington et al., 2004), protein– protein interactions (PPI), and disease–gene associations (DGA) (Chun et al., 2006). Among the possible RE tasks, we chose the PPI extraction problem. PPI extraction is a major RE task; In th"
C10-2162,P00-1058,0,0.0298208,"d particular attention to a different grammar framework, i.e. HPSG, with the analysis of more Chinese constructions, such as the serial verb construction. In addition, in our on-going deep parsing work, we use the developed Chinese HPSG grammar, i.e. the lexical entries, to train a full-fledged HPSG parser directly. Additionally, there are some works that induce lexicalized grammar from corpora for other languages. For example, by using the Penn Treebank, Miyao et al. (2005) automatically extracted a large HPSG lexicon, Xia (1999), Chen and Shanker (2000), Hockenmaier and Steedman (2002), and Chiang (2000) invented LTAG/CCG specific procedures for lexical entry extraction. From the German Tiger corpus, Cramer and Zhang (2009) constructed a German HPSG grammar; Hockenmaier (2006) created a German CCGbank; and Rehbei and Genabith (2009) acquired LFG resources. In addition, Schluter and Genabith (2009) automatically obtained widecoverage LFG resources from a French Treebank. Our work implements a similar idea to these works, but we apply different grammar design and annotation rules, which are specific to Chinese. Furthermore, we obtained a comparative result to state-of-the-art works for English."
C10-2162,P04-1014,0,0.103169,"Missing"
C10-2162,W09-2605,0,0.0464518,"ons, such as the serial verb construction. In addition, in our on-going deep parsing work, we use the developed Chinese HPSG grammar, i.e. the lexical entries, to train a full-fledged HPSG parser directly. Additionally, there are some works that induce lexicalized grammar from corpora for other languages. For example, by using the Penn Treebank, Miyao et al. (2005) automatically extracted a large HPSG lexicon, Xia (1999), Chen and Shanker (2000), Hockenmaier and Steedman (2002), and Chiang (2000) invented LTAG/CCG specific procedures for lexical entry extraction. From the German Tiger corpus, Cramer and Zhang (2009) constructed a German HPSG grammar; Hockenmaier (2006) created a German CCGbank; and Rehbei and Genabith (2009) acquired LFG resources. In addition, Schluter and Genabith (2009) automatically obtained widecoverage LFG resources from a French Treebank. Our work implements a similar idea to these works, but we apply different grammar design and annotation rules, which are specific to Chinese. Furthermore, we obtained a comparative result to state-of-the-art works for English. There are some researchers who worked on Chinese HPSG grammar development manually. Zhang (2004) implemented a Chinese HP"
C10-2162,hockenmaier-steedman-2002-acquiring,0,0.475048,"cted adjuncts. But in our grammar, we only deal with extracted arguments, and the gap in a relative clause (as indicated in the dash-boxed part in Figure 12). When the extracted phrase is an adjunct of the relative clause, we simply view the clause as a modifier of the extracted phrase. shown in Figure 13, we obtain a lexical entry for the word ‘写/write’ as shown in Figure 14. 3.1.2 Rules for Correcting Inconsistency There are some inconsistencies in the annotation of the CTB, which presents difficulties for performing the derivation tree annotation. Therefore, we define 49 rules, as done in (Hockenmaier and Steedman, 2002) for English, to mitigate inconsistencies before annotation (refer to Table 3). 3.1.3 Rules for Assisting Annotation We also define 48 rules (refer to Table 2), which are similar to the rules used in (Miyao, 2006) for English, to help the derivation tree annotation. For example, 12 pattern rules are defined to assign the schemas to corresponding constituents. Figure 13. HPSG derivation tree for Figure 8. Rule Type Rules for correcting inconsistent annotation Rule Description Rule # Fix tree annotation 37 Fix phrase tag annotation 5 Fix functional tag annotation 5 Fix POS tag annotation 2 Slash"
C10-2162,J08-1002,1,0.92028,"Missing"
C10-2162,schluter-van-genabith-2008-treebank,0,0.0354136,"Missing"
C10-2162,Y09-2048,1,0.594404,"Missing"
C10-2162,C02-1145,0,0.0248821,"work, we paid particular attention to a different grammar framework, i.e. HPSG, with the analysis of more Chinese constructions, such as the serial verb construction. In addition, in our on-going deep parsing work, we use the developed Chinese HPSG grammar, i.e. the lexical entries, to train a full-fledged HPSG parser directly. Additionally, there are some works that induce lexicalized grammar from corpora for other languages. For example, by using the Penn Treebank, Miyao et al. (2005) automatically extracted a large HPSG lexicon, Xia (1999), Chen and Shanker (2000), Hockenmaier and Steedman (2002), and Chiang (2000) invented LTAG/CCG specific procedures for lexical entry extraction. From the German Tiger corpus, Cramer and Zhang (2009) constructed a German HPSG grammar; Hockenmaier (2006) created a German CCGbank; and Rehbei and Genabith (2009) acquired LFG resources. In addition, Schluter and Genabith (2009) automatically obtained widecoverage LFG resources from a French Treebank. Our work implements a similar idea to these works, but we apply different grammar design and annotation rules, which are specific to Chinese. Furthermore, we obtained a comparative result to state-of-the-art"
C10-2162,N04-1013,0,\N,Missing
C10-2162,W02-1502,0,\N,Missing
C10-2162,P06-1064,0,\N,Missing
C10-2162,W08-1700,0,\N,Missing
C12-1084,W94-0322,0,0.348877,"tem as a subsystem. By aggregating the answers and confidence values from a factoid-style question answering system we can determine the correctness of the entire proposition or the substitutions that make the proposition false. We evaluated the system on multiple-choice questions from a university admission test on world history, and found it to be highly accurate. Keywords: question answering, facts validation, yes/no question, question inversion. Proceedings of COLING 2012: Technical Papers, pages 1377–1392, COLING 2012, Mumbai, December 2012. 1377 1 Introduction Yes/no question answering (Green and Carberry, 1994; Hirschberg, 1984) can be equated to the task of determining the correctness of a given proposition. The target of such a mechanism is not limited to explicit interrogative questions since any general declarative sentences can in principle be validated. For example, consider the following two propositions, where (1) is true and (2) false1 : (1) Chirac was the president of France in 2000. ∗ (2) Chirac was the president of Germany in 2000. As suggested by this example, a false proposition can often be produced by replacing a part of a true proposition. During a conversation, a human with knowle"
C12-1084,P84-1012,0,0.5707,"regating the answers and confidence values from a factoid-style question answering system we can determine the correctness of the entire proposition or the substitutions that make the proposition false. We evaluated the system on multiple-choice questions from a university admission test on world history, and found it to be highly accurate. Keywords: question answering, facts validation, yes/no question, question inversion. Proceedings of COLING 2012: Technical Papers, pages 1377–1392, COLING 2012, Mumbai, December 2012. 1377 1 Introduction Yes/no question answering (Green and Carberry, 1994; Hirschberg, 1984) can be equated to the task of determining the correctness of a given proposition. The target of such a mechanism is not limited to explicit interrogative questions since any general declarative sentences can in principle be validated. For example, consider the following two propositions, where (1) is true and (2) false1 : (1) Chirac was the president of France in 2000. ∗ (2) Chirac was the president of Germany in 2000. As suggested by this example, a false proposition can often be produced by replacing a part of a true proposition. During a conversation, a human with knowledge of the facts mi"
C12-1084,D07-1073,0,0.0161426,"e to be a common noun which is a hypernym of the pivot term. A thesaurus such as WordNet can be used, but choosing the right level of abstraction can be a problem. An effective approach to making this selection involves mining the corpus to determine co-occurrence counts with each possible hypernym (Prager et al., 2001). A simpler solution, which we adopt here as a guideline, is to select the type from the first sentence of Wikipedia articles about the subject term, from the observation that many opening sentences are of the form “X is Y” where X is the title of the article and Y is its type (Kazama and Torisawa, 2007). In the 1382 example above, “calligrapher” and “dynasty” are assigned as types of “Yen Chen-ching” and “Sung dynasty”, respectively. The pivot term is then replaced with a noun phrase consisting of “this” and the type. When the type is Person and the pivot term is the subject of the head predicate of the proposition, just “he” or “she” is used. From (5), two factoid questions (5a) and (5b) are generated, the substitutions denoted by italic. (5a) He was the best known chirographer in the Sung dynasty. (5b) Yen Chen-ching was the best known chirographer in this dynasty. For each question there"
C12-1084,H01-1006,1,0.396818,"yed when there are no proper nouns. For example, two pivot terms (underlined) are found in proposition (5), which is the second row in Table 1. (5) Yen Chen-ching was the best known chirographer in the Sung dynasty. Next, a type is determined for each pivot term. For this algorithm, we define a type to be a common noun which is a hypernym of the pivot term. A thesaurus such as WordNet can be used, but choosing the right level of abstraction can be a problem. An effective approach to making this selection involves mining the corpus to determine co-occurrence counts with each possible hypernym (Prager et al., 2001). A simpler solution, which we adopt here as a guideline, is to select the type from the first sentence of Wikipedia articles about the subject term, from the observation that many opening sentences are of the form “X is Y” where X is the title of the article and Y is its type (Kazama and Torisawa, 2007). In the 1382 example above, “calligrapher” and “dynasty” are assigned as types of “Yen Chen-ching” and “Sung dynasty”, respectively. The pivot term is then replaced with a noun phrase consisting of “this” and the type. When the type is Person and the pivot term is the subject of the head predi"
C12-1084,P06-1135,1,0.678326,"n answering (Ravichandran and Hovy, 2002; Bian et al., 2008) and definition question answering (Xu et al., 2003; Cui et al., 2005). Even though there are only two possible answers, yes or no, such questions can be quite hard to answer. Search technologies cannot be applied easily because in many cases we can not rely on the existence of explicit negative evidence in the information source, e.g. “Chirac was not a president of Germany.” This paper tackles yes/no question answering by exploiting an existing system for factoidstyle question answering. The key idea, inspired by question inversion (Prager et al., 2006), involves generation of factoid questions by replacing some parts of a given proposition with abstract (i.e. ungrounded) expressions. For example, to determine the correctness of (1) and (2), two question sentences are generated for each proposition, with abstracted parts in italics and anticipated answers in brackets2 . (1a) He was the president of France in 2000. [Chirac] (1b) Chirac was the president of this country in 2000. (2a) He was the president of Germany in 2000. ∗ [France] [Chirac] (2b) Chirac was the president of this country in 2000. ∗ [Germany] DeepQA (Ferrucci, 2012), the facto"
C12-1084,P02-1006,0,0.027771,"false proposition can often be produced by replacing a part of a true proposition. During a conversation, a human with knowledge of the facts might not only say that the utterance (2) is wrong, but respond with something like “not Germany, but France.” Therefore we believe this kind of validation system we are proposing here may have application outside of a strict question-answering framework. In spite of the importance of the process, yes/no-style question answering has not been intensively studied, compared to other type of question answering, such as the factoid-style question answering (Ravichandran and Hovy, 2002; Bian et al., 2008) and definition question answering (Xu et al., 2003; Cui et al., 2005). Even though there are only two possible answers, yes or no, such questions can be quite hard to answer. Search technologies cannot be applied easily because in many cases we can not rely on the existence of explicit negative evidence in the information source, e.g. “Chirac was not a president of Germany.” This paper tackles yes/no question answering by exploiting an existing system for factoidstyle question answering. The key idea, inspired by question inversion (Prager et al., 2006), involves generatio"
C12-1084,P06-1085,0,\N,Missing
C14-1202,W06-2922,0,0.084989,"Missing"
C14-1202,J13-1002,0,0.0324426,"Missing"
C14-1202,W06-2920,0,0.447444,"and, August 23-29 2014. difficult to comprehend is from psycholinguistic studies mainly on English. The second motivation is to verify whether a parser with the developed system can be viable for crosslinguistical study of human language processing. There is evidence that a human parser cannot store elements of a small constant number, such as three or four (Cowan, 2001). If our system confirms to such a severe constraint, we may claim its cognitive plausibility across languages. We will pursue these questions using the multilingual dependency treebanks from the CoNLL-X and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). In short, our contributions of this paper can be sketched as follows: 1. We formulate a transition system for dependency parsing with a left-corner strategy. 2. We characterize our transition system with its memory cost by simulating oracle transitions along with other transition systems on the CoNLL multilingual treebanks. This is the first empirical study of required memory for left-corner parsing in a crosslinguistical setting. 2 Memory cost of Parsing Algorithms In this work, we focus on the memory cost for dependency parsing transition systems. While there have been"
C14-1202,D11-1114,0,0.0328401,"Missing"
C14-1202,J13-4008,0,0.128312,"ner parser for constituency with shift and reduce actions. In fact, they used the same kind of actions as our transition system: shift, insert, predict, and composition. Though they did not mentioned explicitly, we showed how to construct a left-corner parsing algorithm with these actions by decomposing the push-down recognizer of Resnik (1992). These are examples of broad-coverage parsing models with cognitively plausibility, which has recently received considerable attention in interdisciplinary research on psycholinguistics and computational linguistics (Schuler et al., 2010; Keller, 2010; Demberg et al., 2013). Differently from previous models, our target is dependency. A dependency-based cognitively plausible model is attractive, especially from a crosslinguistical viewpoint. Keller (2010) argued that current models only work for English, or German in few exceptions, and the importance of crosslinguistically valid models of human language processing. There has been some attempts to use a transition system for studying human language processing (Boston and Hale, 2007; Boston et al., 2008), so it is interesting to compare automatic parsing behaviors with various transition systems to human processin"
C14-1202,P07-1024,0,0.0168435,"between our approach and theirs: We need a dummy node to represent a subtree corresponding to that in Resnik’s algorithm, while they introduced it to confirm that every dependency tree on a sentence prefix is fully connected. This difference leads to a technical difference; a subtree of their parser can contain more than one dummy node, while we restrict each subtree to containing only one dummy node on a right spine. Our experiments in section 4 can be considered as a study on functional biases existing in language or language evolution (Jaeger and Tily, 2011). In computational linguistics, Gildea and Temperley (2007; 2010) examined the bias on general sentences called dependency length minimization (DLM), which 2147 argues that grammar should favor the dependency structures that reduce the sum of dependency arc lengths. They reordered English and German treebank sentences with various criteria: original, random with projectivity, and optimal that minimizes the sum of dependency lengths. They observed that the word order of English fits very well to the optimal ordering, while German does not. We examined the universality of the bias to reduce memory cost for left-corner parsing. Although we cannot compar"
C14-1202,Q13-1033,0,0.0290643,"Missing"
C14-1202,P12-1069,0,0.0118955,"is on the language universality of the claim in a crosslingual setting. Two different but relevant motivations exist for this analysis. The first is to answer the following scientific question: Is the claim that people tend to avoid generating center-embedded sentences language universal? This is unclear since the observation that a center-embedded sentence is This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 The top-down parser of Hayashi et al. (2012) is an exception, but its processing is not incremental. 2140 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2140–2150, Dublin, Ireland, August 23-29 2014. difficult to comprehend is from psycholinguistic studies mainly on English. The second motivation is to verify whether a parser with the developed system can be viable for crosslinguistical study of human language processing. There is evidence that a human parser cannot store elements of a small constant number, such as three or four (Cowan, 2001). If our system confirms t"
C14-1202,W13-3518,0,0.0115325,"e analyze the underlying differences between these two operations. We argue that the difference lies in the form of the recognized constituency tree: The former R IGHT-P REDs at step 4, which means that it recognizes a constituency of the form ((a b) c), while the latter recognizes (a (b c)) due to its R IGHT-C OMP operation. Therefore, the spurious ambiguity of our system is caused by the ambiguity of converting a dependency tree to a constituency tree. Recently, some transition systems have exploited similar ambiguities using dynamic oracles (Goldberg and Nivre, 2013; Sartorio et al., 2013; Honnibal et al., 2013). The same type of analysis might be possible for our system, but we leave it for future work; here we only present a static oracle and discuss its properties. Since our system performs shift and reduce actions interchangeably, we need two functions to define the oracle. Let c = (σ|σ2 |σ1 , β, A). The next shift action is determined as follows: • I NSERT: if σ1 = hσ10 |i|x(λ)i and (i, j) ∈ Ag and j has no dependents in β (if i exists) or ∃k ∈ λ; (j, k) ∈ Ag (otherwise). • S HIFT: otherwise. The next reduce action is determined as follows: 3 We use small caps to refer to a specific action, e.g."
C14-1202,P10-1110,0,0.0679416,"6 Conclusion We have pointed out that the memory cost on current transition systems for dependency parsing do not coincide with observations in people, proposing a system with a left-corner strategy. Our crosslinguistical analysis confirms the universality of the claim that people avoid generating center-embedded sentences, which also suggests that it is worthy for crosslinguistical studies of human language processing. As a next stage, we are seeking to train a parser model as in other transition systems with a discriminative framework such as a structured perceptron (Zhang and Nivre, 2011; Huang and Sagae, 2010). A parser with our transition system might also be attractive for the problem of grammar induction, where recovering dependency trees are a central problem (Klein and Manning, 2004), and where some linguisˇ tic biases have been exploited, such as reducibility (Mareˇcek and Zabokrtsk´ y, 2012) or acoustic cues (Pate and Goldwater, 2013). Recently, Cohen et al. (2011) showed how to interpret shift-reduce actions as a generative model; combining their idea and our transition system might enable the model to exploit memory biases that exist in natural sentences. Finally, dependency grammars are s"
C14-1202,P98-1101,0,0.696807,"d memory between original and random sentences for many languages. This result indicates that our system can parse with less memory for only naturally occurring sentences. For Chinese and Hungarian, the differences are subtle. However, the differences are also small for the other systems, which implies that these corpora have some biases on graphs reducing the differences. 5 Related Work and Discussion To the best of our knowledge, parsing with a left-corner strategy has only been studied for constituency. Roark (2001) proposed a top-down parser for a CFG with a left-corner grammar transform (Johnson, 1998), which is essentially the same as left-corner parsing but enables several extensions in a unified framework. Roark et al. (2009) studied the psychological plausibility of Roark’s parser, observing that it fits well to human reading time data. Another model with a left-corner strategy is Schuler et al. (2010): they observed that the transformed grammar of English requires only limited memory, proposing a finite state approximation with a hierarchical hidden Markov model. This parser was later extended by van Schijndel and Schuler (2013), which defined a left-corner parser for constituency with"
C14-1202,P10-2012,0,0.0885859,"tion-based dependency parsing is appealing not only from an engineering perspective due to its efficiency, but also from a scientific perspective: These parsers process a sentence incrementally similar to a human parser, which have motivated several studies concerning their cognitive plausibility (Nivre, 2004; Boston and Hale, 2007; Boston et al., 2008). A cognitively plausible dependency parser is attractive for many reasons, one of the most important being that dependency treebanks are available in many languages, so it is suitable for crosslinguistical studies of human language processing (Keller, 2010). However, current transition systems based on shift-reduce actions fully or partially employ a bottom-up strategy1 , which is problematic from a psycholinguistical point of view: Bottom-up or top-down strategies are known to fail in predicting the difficulty for certain sentences, such as center-embedding, which people have troubles in comprehending (Abney and Johnson, 1991). We propose a transition system for dependency parsing with a left-corner strategy. For constituency parsing, unlike other strategies, the arc-eager left-corner strategy is known to correctly predict processing difficulti"
C14-1202,P10-2035,0,0.0198859,"ost for some rightbranching structures, such as in Figures 1(b–c), and some center-embedded structures. Therefore, for the arc-eager system, it is complicated to discuss the required order of memory cost. We summarize these results in Table 1. Our goal is to develop an algorithm with the properties of the last column, requiring non-constant memory for only center-embedded structures. Other systems All systems where stack elements cannot be connected have the same problem as the arc-standard system because of their bottom-up constructions, including the hybrid system of Kuhlmann et al. (2011). Kitagawa and Tanaka-Ishii (2010) and Sartorio et al. (2013) present an interesting variant, which attaches a node to another node that may not be the head of a subtree on the stack. We can use the same reasoning for the arc-eager system for these systems: they sometimes do not incur costs for center-embedded structures, while they incur a non-constant cost for some right-branching structures. 3 Left-corner Dependency Parsing We now discuss the construction of our transition system with the left-corner strategy. Resnik (1992) proposed a push-down recognizer for a CFG. In the following, we instead characterize his algorithm by"
C14-1202,P04-1061,0,0.0860943,"eft-corner strategy. Our crosslinguistical analysis confirms the universality of the claim that people avoid generating center-embedded sentences, which also suggests that it is worthy for crosslinguistical studies of human language processing. As a next stage, we are seeking to train a parser model as in other transition systems with a discriminative framework such as a structured perceptron (Zhang and Nivre, 2011; Huang and Sagae, 2010). A parser with our transition system might also be attractive for the problem of grammar induction, where recovering dependency trees are a central problem (Klein and Manning, 2004), and where some linguisˇ tic biases have been exploited, such as reducibility (Mareˇcek and Zabokrtsk´ y, 2012) or acoustic cues (Pate and Goldwater, 2013). Recently, Cohen et al. (2011) showed how to interpret shift-reduce actions as a generative model; combining their idea and our transition system might enable the model to exploit memory biases that exist in natural sentences. Finally, dependency grammars are suitable for treating non-projective structures. Extensions for transition systems have been proposed to handle non-projective structures with additional actions (Attardi, 2006; Nivre"
C14-1202,P14-2130,0,0.064328,"nguistical viewpoint. Keller (2010) argued that current models only work for English, or German in few exceptions, and the importance of crosslinguistically valid models of human language processing. There has been some attempts to use a transition system for studying human language processing (Boston and Hale, 2007; Boston et al., 2008), so it is interesting to compare automatic parsing behaviors with various transition systems to human processing. We introduced a dummy node for representing a subtree with an unknown head or dependent. Recently, Menzel and colleagues (Beuck and Menzel, 2013; Kohn and Menzel, 2014) have also studied dependency parsing with a dummy node. While conceptually similar, the aim of introducing a dummy node is different between our approach and theirs: We need a dummy node to represent a subtree corresponding to that in Resnik’s algorithm, while they introduced it to confirm that every dependency tree on a sentence prefix is fully connected. This difference leads to a technical difference; a subtree of their parser can contain more than one dummy node, while we restrict each subtree to containing only one dummy node on a right spine. Our experiments in section 4 can be consider"
C14-1202,P11-1068,0,0.0393279,"Missing"
C14-1202,D12-1028,0,0.327315,"Missing"
C14-1202,W03-3017,0,0.125667,"Missing"
C14-1202,W04-0308,0,0.257324,"ith those of other transition systems on treebanks of 18 typologically diverse languages. A crosslinguistical analysis confirms the universality of the claim that a parser with our system requires less memory for parsing naturally occurring sentences. 1 Introduction It is sometimes argued that transition-based dependency parsing is appealing not only from an engineering perspective due to its efficiency, but also from a scientific perspective: These parsers process a sentence incrementally similar to a human parser, which have motivated several studies concerning their cognitive plausibility (Nivre, 2004; Boston and Hale, 2007; Boston et al., 2008). A cognitively plausible dependency parser is attractive for many reasons, one of the most important being that dependency treebanks are available in many languages, so it is suitable for crosslinguistical studies of human language processing (Keller, 2010). However, current transition systems based on shift-reduce actions fully or partially employ a bottom-up strategy1 , which is problematic from a psycholinguistical point of view: Bottom-up or top-down strategies are known to fail in predicting the difficulty for certain sentences, such as center"
C14-1202,J08-4003,0,0.601446,"right-branching structure, it reaches 7 after reading b, which means that a and b are connected by a subtree. On the other hand, for the center-embedded structure, it reaches 6 after reading b, but a and b cannot be connected at this point, requiring extra memory. 2.2 Transition-based Dependency Parsing Next, we summarize the issues with current transition systems for dependency parsing with regards to their memory cost. A transition-based dependency parser processes a sentence on a transition system, which is defined as a set of configurations and a set of transitions between configurations (Nivre, 2008). Each configuration has a stack preserving constructed subtrees on which we define the memory cost as a function for each system. 2 We should distinguish between two types of characterizations of parsing: strategy and algorithm. A parsing strategy is an abstract notion that defines “a way of enumerating the nodes and arcs of parse trees” (Abney and Johnson, 1991), while a parsing algorithm defines the implementation of that strategy, typically with push-down automata (Johnson-Laird, 1983; Resnik, 1992). A parsing strategy is useful for characterizing the properties of each parser, and we conc"
C14-1202,P09-1040,0,0.0608623,"Missing"
C14-1202,Q13-1006,0,0.0173916,"so suggests that it is worthy for crosslinguistical studies of human language processing. As a next stage, we are seeking to train a parser model as in other transition systems with a discriminative framework such as a structured perceptron (Zhang and Nivre, 2011; Huang and Sagae, 2010). A parser with our transition system might also be attractive for the problem of grammar induction, where recovering dependency trees are a central problem (Klein and Manning, 2004), and where some linguisˇ tic biases have been exploited, such as reducibility (Mareˇcek and Zabokrtsk´ y, 2012) or acoustic cues (Pate and Goldwater, 2013). Recently, Cohen et al. (2011) showed how to interpret shift-reduce actions as a generative model; combining their idea and our transition system might enable the model to exploit memory biases that exist in natural sentences. Finally, dependency grammars are suitable for treating non-projective structures. Extensions for transition systems have been proposed to handle non-projective structures with additional actions (Attardi, 2006; Nivre, 2009). Although our system cannot handle non-projective structures, a similar extension might be possible, which would enable a left-corner analysis for n"
C14-1202,C92-1032,0,0.380964,"a transition system for dependency parsing with a left-corner strategy. 2. We characterize our transition system with its memory cost by simulating oracle transitions along with other transition systems on the CoNLL multilingual treebanks. This is the first empirical study of required memory for left-corner parsing in a crosslinguistical setting. 2 Memory cost of Parsing Algorithms In this work, we focus on the memory cost for dependency parsing transition systems. While there have been many studies concerning the memory cost for an algorithm in constituency parsing (Abney and Johnson, 1991; Resnik, 1992), the same kind of study is rare in dependency parsing. This section discusses the memory cost for the current dependency parsing transition systems. Before that, we first review the known results in constituency parsing regarding memory cost. 2.1 Center-Embedding and the Left-Corner Strategy 10X 13 11 3 2X 7 3 2X 11 5X 1a 9X 12d 1a 6X The structures on the right side are called left-branching, 6 13 11 7 10 9 right-branching, and center-embedding, respectively. Peo9X 4b 5X 12d 8c 2X 5 13 6 8 3 10 ple have difficulty when parsing center-embedded struc8c 12d 4b 7c 1a 4b tures, while no difficult"
C14-1202,D09-1034,0,0.0208517,"memory for only naturally occurring sentences. For Chinese and Hungarian, the differences are subtle. However, the differences are also small for the other systems, which implies that these corpora have some biases on graphs reducing the differences. 5 Related Work and Discussion To the best of our knowledge, parsing with a left-corner strategy has only been studied for constituency. Roark (2001) proposed a top-down parser for a CFG with a left-corner grammar transform (Johnson, 1998), which is essentially the same as left-corner parsing but enables several extensions in a unified framework. Roark et al. (2009) studied the psychological plausibility of Roark’s parser, observing that it fits well to human reading time data. Another model with a left-corner strategy is Schuler et al. (2010): they observed that the transformed grammar of English requires only limited memory, proposing a finite state approximation with a hierarchical hidden Markov model. This parser was later extended by van Schijndel and Schuler (2013), which defined a left-corner parser for constituency with shift and reduce actions. In fact, they used the same kind of actions as our transition system: shift, insert, predict, and comp"
C14-1202,P13-1014,0,0.135174,"s, such as in Figures 1(b–c), and some center-embedded structures. Therefore, for the arc-eager system, it is complicated to discuss the required order of memory cost. We summarize these results in Table 1. Our goal is to develop an algorithm with the properties of the last column, requiring non-constant memory for only center-embedded structures. Other systems All systems where stack elements cannot be connected have the same problem as the arc-standard system because of their bottom-up constructions, including the hybrid system of Kuhlmann et al. (2011). Kitagawa and Tanaka-Ishii (2010) and Sartorio et al. (2013) present an interesting variant, which attaches a node to another node that may not be the head of a subtree on the stack. We can use the same reasoning for the arc-eager system for these systems: they sometimes do not incur costs for center-embedded structures, while they incur a non-constant cost for some right-branching structures. 3 Left-corner Dependency Parsing We now discuss the construction of our transition system with the left-corner strategy. Resnik (1992) proposed a push-down recognizer for a CFG. In the following, we instead characterize his algorithm by inference rules, which are"
C14-1202,J10-1001,0,0.424996,"ition system, we characterize it by looking into the following question: Is it true that naturally occurring sentences can be parsed on this system with a lower memory overhead? This should be true under the assumptions that 1) people avoid generating a sentence that causes difficulty for them, and 2) center-embedding is a kind of such structure. Specifically, we focus on analyzing the oracle transitions of the system, i.e., parser actions to recover the gold dependency tree for a sentence. In English, it is known that left-corner transformed treebank sentences can be parsed with less memory (Schuler et al., 2010), but our focus in this paper is on the language universality of the claim in a crosslingual setting. Two different but relevant motivations exist for this analysis. The first is to answer the following scientific question: Is the claim that people tend to avoid generating center-embedded sentences language universal? This is unclear since the observation that a center-embedded sentence is This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/"
C14-1202,N13-1010,0,0.193156,"Missing"
C14-1202,P11-2033,0,0.0353327,"uction for these cases. 6 Conclusion We have pointed out that the memory cost on current transition systems for dependency parsing do not coincide with observations in people, proposing a system with a left-corner strategy. Our crosslinguistical analysis confirms the universality of the claim that people avoid generating center-embedded sentences, which also suggests that it is worthy for crosslinguistical studies of human language processing. As a next stage, we are seeking to train a parser model as in other transition systems with a discriminative framework such as a structured perceptron (Zhang and Nivre, 2011; Huang and Sagae, 2010). A parser with our transition system might also be attractive for the problem of grammar induction, where recovering dependency trees are a central problem (Klein and Manning, 2004), and where some linguisˇ tic biases have been exploited, such as reducibility (Mareˇcek and Zabokrtsk´ y, 2012) or acoustic cues (Pate and Goldwater, 2013). Recently, Cohen et al. (2011) showed how to interpret shift-reduce actions as a generative model; combining their idea and our transition system might enable the model to exploit memory biases that exist in natural sentences. Finally, d"
C14-1202,C98-1098,0,\N,Missing
C14-1202,P13-1051,0,\N,Missing
C14-1202,D07-1096,0,\N,Missing
C16-1005,P11-1020,0,0.284446,"Missing"
C16-1005,W14-3348,0,0.0282314,"Missing"
C16-1005,W04-1013,0,0.0294467,"Missing"
C16-1005,D15-1166,0,0.0855569,"Missing"
C16-1005,P02-1040,0,0.095693,"Missing"
C16-1005,N15-1173,0,0.224094,"Missing"
C18-1260,E17-1118,0,0.0217294,"iệt. 1 Introduction Syntactic parsing plays a crucial role in improving the quality of natural language processing (NLP) applications and speech processing. Parsing has been broadly studied for languages such as English and Chinese, which leads to the development of many parsing methods. For example, Klein and Manning (2003), Matsuzaki et al. (2005), and Petrov and Klein (2007) improved probabilistic context-free grammar (PCFG) parsing by enriching contextual information, Finkel et al. (2008) and Hall et al. (2014) employed feature-based conditional random fields (CRFs), Zhu et al. (2013) and Coavoux and Crabbé (2017) employed shift-reduce algorithms, while Durrett and Klein (2015) and Dyer et al. (2016) used neural networks. The parsing problem has not been studied thoroughly enough for Vietnamese. Since a Vietnamese Treebank was built by Nguyen et al. (2009), a few researchers have adapted available constituent parsers ∗ This work was carried out while the first author was a PhD student at The Graduate University for Advanced Studies (SOKENDAI), Japan. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/ 3075 P"
C18-1260,P15-1030,0,0.147882,"oving the quality of natural language processing (NLP) applications and speech processing. Parsing has been broadly studied for languages such as English and Chinese, which leads to the development of many parsing methods. For example, Klein and Manning (2003), Matsuzaki et al. (2005), and Petrov and Klein (2007) improved probabilistic context-free grammar (PCFG) parsing by enriching contextual information, Finkel et al. (2008) and Hall et al. (2014) employed feature-based conditional random fields (CRFs), Zhu et al. (2013) and Coavoux and Crabbé (2017) employed shift-reduce algorithms, while Durrett and Klein (2015) and Dyer et al. (2016) used neural networks. The parsing problem has not been studied thoroughly enough for Vietnamese. Since a Vietnamese Treebank was built by Nguyen et al. (2009), a few researchers have adapted available constituent parsers ∗ This work was carried out while the first author was a PhD student at The Graduate University for Advanced Studies (SOKENDAI), Japan. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/ 3075 Proceedings of the 27th International Conference on Computational"
C18-1260,N16-1024,0,0.163661,"language processing (NLP) applications and speech processing. Parsing has been broadly studied for languages such as English and Chinese, which leads to the development of many parsing methods. For example, Klein and Manning (2003), Matsuzaki et al. (2005), and Petrov and Klein (2007) improved probabilistic context-free grammar (PCFG) parsing by enriching contextual information, Finkel et al. (2008) and Hall et al. (2014) employed feature-based conditional random fields (CRFs), Zhu et al. (2013) and Coavoux and Crabbé (2017) employed shift-reduce algorithms, while Durrett and Klein (2015) and Dyer et al. (2016) used neural networks. The parsing problem has not been studied thoroughly enough for Vietnamese. Since a Vietnamese Treebank was built by Nguyen et al. (2009), a few researchers have adapted available constituent parsers ∗ This work was carried out while the first author was a PhD student at The Graduate University for Advanced Studies (SOKENDAI), Japan. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/ 3075 Proceedings of the 27th International Conference on Computational Linguistics, pages 3075"
D09-1013,D07-1024,0,0.440421,"ume that the feature space is same, and that the labels may be different in only some examples, while most of DA methods assume that the labels are the same, and that the feature space is different. Among the methods, we use adaptive SVM (aSVM) (Yang et al., 2007), singular value decomposition (SVD) based alternating structure optimization (SVDASO) (Ando et al., 2005), and transfer AdaBoost (TrAdaBoost) (Dai et al., 2007) to compare with SVM-CW. We do not use semi-supervised learning (SSL) methods, because it would be considerably costly to generate enough clean unlabeled data needed for SSL (Erkan et al., 2007). aSVM is seen as a promising DA method among several modifications of SVM including SVM-CW. aSVM tries to find a model that is close to the one made from other classification problems. SVDASO is one of the most successful SSL, DA, or multi-task learning methods in NLP. The method tries to find an additional useful feature space by solving auxiliary problems that are close to the target problem. With well-designed auxiliary problems, the method has been applied to text classification, text chunking, and word sense disambiguation (Ando, 2006). The method was reported to perform better than or c"
D09-1013,W06-2911,0,0.0115377,"enough clean unlabeled data needed for SSL (Erkan et al., 2007). aSVM is seen as a promising DA method among several modifications of SVM including SVM-CW. aSVM tries to find a model that is close to the one made from other classification problems. SVDASO is one of the most successful SSL, DA, or multi-task learning methods in NLP. The method tries to find an additional useful feature space by solving auxiliary problems that are close to the target problem. With well-designed auxiliary problems, the method has been applied to text classification, text chunking, and word sense disambiguation (Ando, 2006). The method was reported to perform better than or comparable to the best state-of-the-art systems in all of these tasks. TrAdaBoost was proposed as an ITL method. In training, the method reduces the effect of incompatible examples by decreasing their weights, and thereby tries to use useful examples from source corpora. The method has been applied to text classification, and the reported performance was better than SVM and transductive SVM (Dai et al., 2007). Related Works While sentence-based, pair-wise PPI extraction was initially tackled by using simple methods based on co-occurrences, la"
D09-1013,P08-1006,1,0.760868,"es (PAS) from Enju, and by using the dependency trees from KSDEP. 3.1.1 3.1 Feature Vector Bag-of-Words (BOW) Features The BOW feature includes the lemma form of a word, its relative position to the target pair of proteins (Before, Middle, After), and its frequency in the target sentence. BOW features form the BOW kernel in the original kernel method. BOW features for the pair in Figure 2 are shown in Figure 4. We propose a feature vector with three types of features, corresponding to the three different kernels, which were each combined with the two parsers: the Enju 2.3.0, and KSDEP beta 1 (Miyao et al., 2008); this feature vector is used because the kernels with these parsers were shown to be effective for PPI extraction by Miwa et al. (2008), and because it is important to start from a good performance single corpus system. Both parsers were retrained using the GENIA Treebank corpus provided by Kim et al. (2003). By using our linear feature vector, we can perform calculations faster by using fast linear classifiers like L2-SVM, and we also obtain a more accurate extraction, than by using the original kernel method. Figure 3 summarizes the way in which the feature vector is constructed. The system"
D09-1013,W02-1001,0,\N,Missing
D09-1121,D07-1112,0,0.246361,"Missing"
D09-1121,gimenez-marquez-2008-towards,0,0.113426,"Missing"
D09-1121,H94-1020,0,0.03626,": three wrong outputs for “ARG2” of “on” (Error 1) and “ARG1” of “our” (Error 2) and “work” (Error 3), excess relation “ARG1” of “force” (Error 4), and missing relation “ARG1” for “today” (Error 5). By correcting each of the errors 1, 2, 3 and 4, all of these errors are corrected together, and therefore classified into the same cooccurring error group. Although error 5 cannot participate in the group, correcting error 5 can correct all of the errors in the group, and therefore an We applied our approaches to parsing errors given by the HPSG parser Enju, which was trained on the Penn Treebank (Marcus et al., 1994) section 2-21. We first examined each approach, and then explored the combination of the approaches. 4.1 Evaluation of descriptive approach We examined our descriptive approach. We first parsed sentences in the Penn Treebank section 22 with Enju, and then observed the errors. Based on the observation, we next described the patterns as shown in Section 3. After that, we parsed section 0 and then applied the patterns to the errors. Table 3 summarizes the extracted errors. As the table shows, with the 14 error patterns, we successfully matched 1,671 locations in error outputs and covered 2,078 of"
D09-1121,D07-1013,0,0.105428,"ptured as parameters for model training, or policies for re-ranking the parse candidates. The combination of our approaches would give us interesting clues for planning effective strategies for improving the parser. Our challenges for combining the two approaches are now in the preliminary stage and there would be many possibilities for further detailed analysis. 5 Related work Although there have been many researches which analyzed errors on their own systems in the part of the experiments, there have been few researches which focused mainly on error analysis itself. In the field of parsing, McDonald and Nivre (2007) compared parsing errors between graphbased and transition-based parsers. They observed the accuracy transitions from various points of view, and the obtained statistical data suggested that error propagation seemed to occur in the graph structures of parsing outputs. Our research proceeded for one step in this point, and attempted to reveal the way of the propagations. In examining the combination of the two types of parsing, McDonald and Nivre (2007) utilized similar approaches to our empirical analysis. They allowed a parser to give only structures given by the parsers. They implemented the"
D09-1121,P05-1011,1,0.933823,"ous combinations of error patterns as organized error phenomena on the basis of linguistic knowledge, and then extract such combinations from given errors. In our empirical approach, on the other and, we re-parse a sentence under the condition where a target error is corrected, and errors which are additionally corrected are regarded as dependent errors. By capturing dependencies among parsing errors through systematic approaches, we can effectively collect errors which are related to the same linguistic properties. In the experiments, we applied both of our approaches to an HPSG parser Enju (Miyao and Tsujii, 2005; Ninomiya et al., 2006), and then evaluated the obtained error classes. After examining the individual approaches, we explored the combination of them. 2 Parser and its evaluation A parser is a system which interprets structures of given sentences from some grammatical or in some cases semantical viewpoints, and interpreted structures are utilized as essential information for various natural language tasks such as information extraction, machine translation, and so on. In most cases, an output structure of a parser is based on a certain grammatical framework such as CFG, CCG (Steedman, 2000),"
D09-1121,W06-1619,1,0.906675,"Missing"
D09-1138,C08-1002,0,0.0336512,"Missing"
D09-1138,P98-1013,0,0.0632726,"ailability of a lexicon, such as in word sense disambiguation (WSD) (McCarthy et al., 2004), and in token-level verb class disambiguation (Lapata and Brew, 2004; Girju et Jun’ichi Tsujii University of Tokyo University of Manchester National Center for Text Mining Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan tsujii@is.s.u-tokyo.ac.jp al., 2005; Li and Brew, 2007; Abend et al., 2008). In other words, those methods are heavily dependent on the availability of a semantic lexicon. Therefore, recent research efforts have invested in developing semantic resources, such as WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998), and VerbNet (Kipper et al., 2000; Kipper-Schuler, 2005), which greatly advanced research in semantic processing. However, the construction of such resources is expensive, and it is unrealistic to presuppose the availability of full-coverage lexicons; this is the case because unknown words always appear in real texts, and word-semantics associations may vary (Abend et al., 2008). This paper explores a method for the supervised learning of a probabilistic model for the VerbNet lexicon. We target the automatic classification of arbitrary verbs, including polysemous verbs, into all VerbNet class"
D09-1138,C96-1055,0,0.105412,"Missing"
D09-1138,E03-1040,0,0.0250868,"resenting the statistics of syntactic frames, are extracted from the unannotated corpora. Additionally, as the classes represent semantic commonalities, semantically inspired features, like distributionally similar words, are used. These features can be considered as a generalized representation of verbs, and we expect that the obtained probabilistic model predicts VerbNet classes of the unknown words. Our model is evaluated in two tasks of typelevel verb classification: one is the classification of monosemous verbs into a small subset of the classes, which was studied in some previous works (Joanis and Stevenson, 2003; Joanis et al., 2008). The other task is the classification of all verbs into the full set of VerbNet classes, which has not yet 1328 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1328–1337, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP been attempted. In the experiments, training instances are obtained from VerbNet and/or SemLink (Loper et al., 2007), while features are extracted from the British National Corpus or from Wall Street Journal. We empirically compare several settings for model learning by varying the set of features, the source"
D09-1138,kipper-etal-2006-extending,0,0.0299393,"Missing"
D09-1138,W04-2606,0,0.0592579,"Missing"
D09-1138,P03-1009,0,0.337213,"Missing"
D09-1138,W02-0907,0,0.0231614,"Missing"
D09-1138,J04-1003,0,0.214269,"Missing"
D09-1138,P98-2127,0,0.219608,"Missing"
D09-1138,P04-1036,0,0.052023,"Missing"
D09-1138,J01-3003,0,0.031791,"om the British National Corpus or from Wall Street Journal. We empirically compare several settings for model learning by varying the set of features, the source domain and the size of a corpus for feature extraction, and the use of the token-level statistics obtained from a manually disambiguated corpus. We also provide the analysis of the remaining errors, which will lead us to further improve the supervised learning of a probabilistic semantic lexicon. Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008). However, their focus has been limited to a small subset of verb classes, and a limited number of monosemous verbs. The main contributions of the present work are: i) to provide empirical results for the automatic classification of all verbs, including polysemous ones, into all VerbNet classes, and ii) to empirically explore the effective settings for the supervised learning of a probabilistic lexicon of verb semantic classes. 2 2.1 43 Emission 43.1 Light Emission beam, glow, sparkle, . . . 43.2 Sound Emission blare"
D09-1138,J08-1002,1,0.86468,"Missing"
D09-1138,J05-1004,0,0.167846,"Missing"
D09-1138,C00-2108,0,0.0699652,"Missing"
D09-1138,E03-1037,0,0.0471195,"Missing"
D09-1138,W03-0410,0,0.247258,"pus or from Wall Street Journal. We empirically compare several settings for model learning by varying the set of features, the source domain and the size of a corpus for feature extraction, and the use of the token-level statistics obtained from a manually disambiguated corpus. We also provide the analysis of the remaining errors, which will lead us to further improve the supervised learning of a probabilistic semantic lexicon. Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008). However, their focus has been limited to a small subset of verb classes, and a limited number of monosemous verbs. The main contributions of the present work are: i) to provide empirical results for the automatic classification of all verbs, including polysemous ones, into all VerbNet classes, and ii) to empirically explore the effective settings for the supervised learning of a probabilistic lexicon of verb semantic classes. 2 2.1 43 Emission 43.1 Light Emission beam, glow, sparkle, . . . 43.2 Sound Emission blare, chime, jangle, . . . ... 4"
D09-1138,E99-1007,0,0.0172543,"e features are extracted from the British National Corpus or from Wall Street Journal. We empirically compare several settings for model learning by varying the set of features, the source domain and the size of a corpus for feature extraction, and the use of the token-level statistics obtained from a manually disambiguated corpus. We also provide the analysis of the remaining errors, which will lead us to further improve the supervised learning of a probabilistic semantic lexicon. Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008). However, their focus has been limited to a small subset of verb classes, and a limited number of monosemous verbs. The main contributions of the present work are: i) to provide empirical results for the automatic classification of all verbs, including polysemous ones, into all VerbNet classes, and ii) to empirically explore the effective settings for the supervised learning of a probabilistic lexicon of verb semantic classes. 2 2.1 43 Emission 43.1 Light Emission beam, glow, sparkle, . ."
D09-1138,W99-0503,0,0.00941077,"oper et al., 2007), while features are extracted from the British National Corpus or from Wall Street Journal. We empirically compare several settings for model learning by varying the set of features, the source domain and the size of a corpus for feature extraction, and the use of the token-level statistics obtained from a manually disambiguated corpus. We also provide the analysis of the remaining errors, which will lead us to further improve the supervised learning of a probabilistic semantic lexicon. Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008). However, their focus has been limited to a small subset of verb classes, and a limited number of monosemous verbs. The main contributions of the present work are: i) to provide empirical results for the automatic classification of all verbs, including polysemous ones, into all VerbNet classes, and ii) to empirically explore the effective settings for the supervised learning of a probabilistic lexicon of verb semantic classes. 2 2.1 43 Emission 43.1 Light Emissio"
D09-1138,J93-2004,0,\N,Missing
D09-1138,C98-1013,0,\N,Missing
D09-1138,C98-2122,0,\N,Missing
D13-1118,P11-1087,0,0.0215445,"(zS = k 0 |z−S ) and decomposed as: p(zS = k 0 |z−S ) = Y j (n−S +αk0 )nj (S) jk0 , P (Nj−S + k αk )nj (S) (13) where nj (S) is the number of word tokens connected with S in document j. H IERARCHICAL We skip tables on restaurants of k = 0, because these tables are all from other topics and we cannot construct a block. The effects of λ can be ignored because these are shared by all topics. S WITCHING In the S WITCHING, p(zS = k 0 |z−S ) cannot be calculated in a closed form because p(lji |hji ) in (9) would be changed dynamically when adding customers. This problem is the same one addressed by Blunsom and Cohn (2011), and we follow the same approximation in which, when we calculate the probability, we fractionally add tables and customers recursively. 4.3 Inference of Hyperparameters We also place a prior on each hyperparameter and sample value from the posterior distribution for every iteration. As in Teh (2006a), we set different values of a and b for each depth of PYP, but share across all topics and sample values with an auxiliary variable method. We also set different value of γ for each depth, on which we place Gamma(1, 1). We make the topic prior α asymmetric: α = βα0 ; β ∼ Gamma(1, 1), α0 ∼ Dir(1)"
D13-1118,P09-2085,0,0.0172038,"ght be difficult under the token-based sampler. The table-based sampler moves those different n-grams having common suffixes jointly into another topic. Figure 3 shows a transition of state by the tablebased sampler and Algorithm 4.2 depicts a highlevel description of one iteration. First, we select a table in a restaurant, which is shown with a dotted line in the figure. Next, we descend the tree to collect the tables connected to the selected table, which are pointed by arrows. Because this connection cannot be preserved in common data structures for a restaurant described in Teh (2006a) or Blunsom et al. (2009), we select the child tables randomly. This is correct because customers in CRP are exchange1185 for all table in all restaurants do Remove a customer from the parent restaurant. Construct a block of seating arrangement S by descending the tree recursively. Sample topic assignment zS ∼ p(zS |S, S−S , z−S ). Move S to sampled topic, and add a customer to the parent restaurant of the first selected table. end for able, so we can restore the parent-child relations arbitrarily. We continue this process recursively until reaching the leaf nodes, obtaining a block of seating arrangement S. After cal"
D13-1118,H91-1057,0,0.507272,"fficiently search for higher probability space even with higher order n-grams. In terms of modeling assumption, we found it is effective to assign a topic to only some parts of a document. 1 Introduction N -gram language model is still ubiquitous in NLP, but due to its simplicity it fails to capture some important aspects of language, such as difference of word usage in different situations, sentence level syntactic correctness, and so on. Toward language model that can consider such a more global context, many extensions have been proposed from lexical pattern adaptation, e.g., adding cache (Jelinek et al., 1991) or topic information (Gildea and Hofmann, 1999; Wallach, 2006), to grammaticality aware models (Pauls and Klein, 2012). Topic language models are important for use in e.g., unsupervised language model adaptation: we want a language model that can adapt to the domain or topic of the current situation (e.g., a document in SMT or a conversation in ASR) automatically and select the appropriate words using both topic and syntactic context. Wallach (2006) is one such model, which generate each word based on local context and global topic information to capture the difference of lexical usage among"
D13-1118,D12-1020,0,0.0190113,"ram topic models have focused mainly on information retrieval. Wang et 1186 Corpus Brown NIPS BNC min. appear 4 4 10 # types 19,759 22,705 33,071 training set # docs # tokens 470 1,157,225 1500 5,088,786 6,162 12,783,130 test set # docs # tokens 30 70,795 50 167,730 100 202,994 Table 1: Corpus statistics after the pre-processing: We replace words appearing less than min.appear times in training + test documents, or appearing only in a test set with an unknown token. All numbers are replaced with #, while punctuations are remained. al. (2007) is a topic model on automatically segmented chunks. Lindsey et al. (2012) extended this model with the hierarchical Pitman-Yor prior. They also used switching variables, but for a different purpose: to determine the segmenting points. They treat these variables completely independently, while our model employs a hierarchical prior to share statistical strength among similar contexts. Our primary interest is language model adaptation, which has been studied mainly in the area of speech processing. Conventionally, this adaptation has relied on a heuristic combination of two separately trained models: an n-gram model p(w|h) and a topic model p(w|d). The unigram rescal"
D13-1118,P12-1101,0,0.0250708,"nd it is effective to assign a topic to only some parts of a document. 1 Introduction N -gram language model is still ubiquitous in NLP, but due to its simplicity it fails to capture some important aspects of language, such as difference of word usage in different situations, sentence level syntactic correctness, and so on. Toward language model that can consider such a more global context, many extensions have been proposed from lexical pattern adaptation, e.g., adding cache (Jelinek et al., 1991) or topic information (Gildea and Hofmann, 1999; Wallach, 2006), to grammaticality aware models (Pauls and Klein, 2012). Topic language models are important for use in e.g., unsupervised language model adaptation: we want a language model that can adapt to the domain or topic of the current situation (e.g., a document in SMT or a conversation in ASR) automatically and select the appropriate words using both topic and syntactic context. Wallach (2006) is one such model, which generate each word based on local context and global topic information to capture the difference of lexical usage among different topics. However, Wallach’s experiments were limited to bigrams, a toy setting for language models, and experi"
D13-1118,P06-1124,0,0.614867,"odel is as follows. First, for each topic k ∈ 1, · · · , K, where K is the number of topics, the model generates an n-gram language model Gkh .2 These n-gram models are generated by the PYP, so Gkh ∼ PYP(a, b, Gkh0 ) holds. The model then generate a document collection. For each document j ∈ 1, · · · , D, it generates a K1 This is the model called prior 2 in Wallach (2006); it consistently outperformed the other prior. Wallach used the Dirichlet language model as each topic, but we only explore the model with HPYLM because its superiority to the Dirichlet language model has been well studied (Teh, 2006b). 2 We sometimes denote Gkh to represent a language model of topic k, not a specific multinomial for some context h, depending on the context. dimensional topic distribution θj by a Dirichlet distribution Dir(α) where α = (α1 , α2 , · · · , αK ) is a prior. Finally, for each word position i ∈ 1, · · · , Nj where Nj is the number of words in document j, ith word’s topic assignment zji is chosen according z to θj , then a word type wji is generated from Ghjiji where hji is the last n − 1 words preceding wji . We can summarize this process as follows: 1. Generate topics: For each h ∈ φ, {W }, ·"
D14-1143,C12-1049,1,0.369758,"is essential to support them when they are reading. Educational experts have been continuously studying methods for measuring the size of a learner’s vocabulary, i.e., the number of words ∗ The main body of this work was done when the first author was a Ph.D. candidate in the University of Tokyo and the paper was later greatly revised when the first author was a JSPS (Japan Society for the Promotion of Science) research fellow (PD) at National Institute of Informatics. See http: //yoehara.com/ for details. the learner knows, over the decades (Meara and Buxton, 1987; Laufer and Nation, 1999). Ehara et al. (2012) formalized a more fine-grained measurement task called vocabulary prediction. The goal of this task is to predict whether a learner knows a given word based on only a relatively small portion of his/her vocabulary. This vocabulary prediction task can be further used for predicting the readability of texts. By predicting vocabulary unknown to readers and showing the meaning of those specific words to readers, Ehara et al. (2013) showed that the number of documents that learners can read increases. Word sampling is essential for vocabulary prediction. Because of the large size of language vocab"
D15-1244,D14-1059,0,0.111137,"Missing"
D15-1244,P84-1044,0,0.391243,"06) are also an instance of this class. For example, Some student manages to come is formalized as (3) ∃x(student(x) ∧ manage(x, come(x))) where manage is a veridical predicate taking a proposition as the second argument; it licenses an inference to ∃x(student(x) ∧ come(x)). Attitude verbs A wide range of propositional attitude verbs such as believe and hope are similar to modals in that they do not license an inference from attitude contexts to actual contexts. But factives like know and remember are an exception; they are veridical.4 A first-order translation can be given along the lines of Hintikka (1962). (4) is translated as (5). (4) know(john, ∃x(student(x) ∧ come(x))) (5) ∀w1 (Rjohn w0 w1 → ∃x(student(w1 , x) ∧ come(w1 , x))) 4 Factive predicates show the important inference patterns known as presupposition projection (van der Sandt, 1992), which are beyond the scope of this paper. 2057 Inference pattern Existential import Axiom Conservativity ∀F ∀G (most(F, G) Monotonicity (right-upward) ∀F ∀G ∀H (most(F, G) Veridicality ∀P (true(P ) → P ) Section Quantifiers Plurals Adjectives Comparatives Verbs Attitudes Total ∀F ∀G (most(F, G) → ∃x(F x ∧ Gx)) → most(F, λx.(F x ∧ Gx))) → (∀x(Gx → Hx) →"
D15-1244,C04-1180,0,0.789098,"rned with generalized quantifiers and intensional operators, and outperforms the state-of-the-art firstorder inference system. 1 Introduction Entailment relations are of central importance in the enterprise of both formal and computational semantics. Traditionally, formal semanticists have concentrated on a relatively small set of linguistic inferences. However, since the emergence of statistical parsers based on sophisticated syntactic theories (Clark and Curran, 2007), an open domain system has been developed that supports certain degree of robust semantic interpretation with wide coverage (Bos et al., 2004). It is then reasonable to expect that a state-of-the-art formal semantics provides an accurate computational basis of natural language inferences. However, there are still obstacles in the way of achieving this goal. One is that the statistical parsers on which semantic interpretations rely do not necessarily reflect the best syntactic analysis as assumed in the formal semantics literature (Honnibal et al., 2010). Another persistent problem is the gap between the logics employed in the two com3 CREST, JST Saitama, Japan munities; while it is generally assumed among formal semanticists that ad"
D15-1244,P85-1008,0,0.180444,"ture (Honnibal et al., 2010). Another persistent problem is the gap between the logics employed in the two com3 CREST, JST Saitama, Japan munities; while it is generally assumed among formal semanticists that adequate semantic representations for natural language demand higher-order logic or type theory (Carpenter, 1997), the dominant view in computational linguistics is that inferences based on higher-order logic are hopelessly inefficient for practical applications (Bos, 2009a). Accordingly, it is claimed that some approximation of higher-order representations in terms of first-order logic (Hobbs, 1985), or a more efficient “natural logic” system based on surface structures is needed. However, it is often not a trivial task to give an approximation of rich higher-order information within a first-order language (Pulman, 2007). Moreover, the coverage of existing natural logic systems is limited to single-premise inferences (MacCartney and Manning, 2008). In this paper, we first present an improved compositional semantics that fills the gap between the parser syntax and a composition derivation. We then develop an inference system that is capable of higher-order inferences in natural languages."
D15-1244,W08-2222,0,0.546466,"ory : S NP semantics : λQ.Q(λx .True)(λx .E(x )) Examples most might true manage believe Figure 1: Schematic lexical entry (semantic template) for intransitive verbs. E is a position in which a particular lexical item appears. Semantic Types (E → Prop) → (E → Prop) → Prop Prop → Prop Prop → Prop Prop → E → Prop Prop → E → Prop Table 1: A classification of key linguistic elements having higher-order denotations. category : NP /N semantics : λF λGλH .∀x (Fx ∧ Gx → Hx ) surf : every Figure 2: The lexical entry for determiner every puts are mapped onto semantic representations in a standard way (Bos, 2008), using λ-calculus as an interface between syntax and semantics. The strategy we use to build a semantic lexicon is similar to that of Bos et al. (2004). A lexical entry for each open word class consists of a syntactic category in CCG (possibly with syntactic features) and a semantic representation encoded as a λ-term. Fig. 1 gives an example.1 For a limited number of closed words such as logical or functional expressions, a λ-term is directly assigned to a surface form (see Fig. 2). The output formula is obtained by combining each λ-term in accordance with meaning composition rules and then b"
D15-1244,J07-3004,0,0.0112043,"al Grammar, CCG). The system is designed to handle multi-premise inferences as well as singlepremise ones. We test our system on the FraCaS test suite (Cooper et al., 1994), which is suitable for evaluating the linguistic coverage of an inference system. The experiments show that our higher-order system outperforms the state-of-the-art first-order system with respect to the speed and accuracy of making logical inferences. 2 CCG and Compositional Semantics As an initial step of compositional semantics, we use the C&C parser (Clark and Curran, 2007), a statistical CCG parser trained on CCGbank (Hockenmaier and Steedman, 2007). Parser out2055 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2055–2061, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. category : S NP semantics : λQ.Q(λx .True)(λx .E(x )) Examples most might true manage believe Figure 1: Schematic lexical entry (semantic template) for intransitive verbs. E is a position in which a particular lexical item appears. Semantic Types (E → Prop) → (E → Prop) → Prop Prop → Prop Prop → Prop Prop → E → Prop Prop → E → Prop Table 1: A classification of key linguistic elements"
D15-1244,P10-1022,0,0.0905554,"d on sophisticated syntactic theories (Clark and Curran, 2007), an open domain system has been developed that supports certain degree of robust semantic interpretation with wide coverage (Bos et al., 2004). It is then reasonable to expect that a state-of-the-art formal semantics provides an accurate computational basis of natural language inferences. However, there are still obstacles in the way of achieving this goal. One is that the statistical parsers on which semantic interpretations rely do not necessarily reflect the best syntactic analysis as assumed in the formal semantics literature (Honnibal et al., 2010). Another persistent problem is the gap between the logics employed in the two com3 CREST, JST Saitama, Japan munities; while it is generally assumed among formal semanticists that adequate semantic representations for natural language demand higher-order logic or type theory (Carpenter, 1997), the dominant view in computational linguistics is that inferences based on higher-order logic are hopelessly inefficient for practical applications (Bos, 2009a). Accordingly, it is claimed that some approximation of higher-order representations in terms of first-order logic (Hobbs, 1985), or a more effi"
D15-1244,2014.lilt-9.7,0,0.0599933,"s is a proportional generalized quantifier like most and half of (Barwise and Cooper, 1981). Model-theoretically, they denote relations between sets. We represent them as a two-place higher-order predicate taking firstorder predicates as arguments. For instance, Most students work is represented as follows. (1) most(λx.student(x), λx.work(x)) Here, most is a higher-order predicate in the sense that it takes first-order predicates λx.student(x) and λx.work(x) as arguments. We take the entailment patterns governing most as axioms, along the same lines of natural logic and monotonicity calculus (Icard and Moss, 2014), where determiners are taken as primitive two-place operators. Standard quantifiers like every and some could also be treated as binary operators in the same way as the binary most in (1). But we choose to adopt the first-order decomposition in such cases (see Fig. 2 for the lexical entry of every). Modals Modal auxiliary expressions like might, must and can are represented as unary sentential operators. For instance, the sentence Some student might come is represented as: (2) ∃x(student(x) ∧ might(come(x))). An important inference role of such a modal operator is to distinguish modal context"
D15-1244,W07-1431,0,0.175903,"utomated, by feeding to its interactive mode a set of predefined tactics combined with user-defined proof-search tactics. Table 2 shows the axioms we implemented. Modals and non-veridical predicates (by which we mean predicates that are neither veridical nor antiveridical) do not have particular axioms, with the consequence that actual and hypothetical contexts are correctly distinguished. 4 Experiments We evaluated our system on the FraCaS test suite (Cooper et al., 1994), a set of entailment problems that is designed to evaluate theories of formal semantics.5 We used the version provided by MacCartney and Manning (2007). The whole data set is divided into nine sections, each devoted to linguistically challenging problems. Of these, we used six sections, excluding three sections (nominal anaphora, ellipsis and temporal reference) that 5 Our system will be publicly available https://github.com/mynlp/ccg2lambda. at involve a task of resolving context-dependency, a task beyond the scope of this paper. Each problem consists of one or more premises, followed by a hypothesis. There are three types of answer: yes (the premise set entails the hypothesis), no (the premise set entails the negation of the hypothesis), a"
D15-1244,C08-1066,0,0.486346,"iew in computational linguistics is that inferences based on higher-order logic are hopelessly inefficient for practical applications (Bos, 2009a). Accordingly, it is claimed that some approximation of higher-order representations in terms of first-order logic (Hobbs, 1985), or a more efficient “natural logic” system based on surface structures is needed. However, it is often not a trivial task to give an approximation of rich higher-order information within a first-order language (Pulman, 2007). Moreover, the coverage of existing natural logic systems is limited to single-premise inferences (MacCartney and Manning, 2008). In this paper, we first present an improved compositional semantics that fills the gap between the parser syntax and a composition derivation. We then develop an inference system that is capable of higher-order inferences in natural languages. We combine a state-of-the-art higher-order proof system (Coq) with a wide-coverage parser based on a modern syntactic theory (Combinatory Categorial Grammar, CCG). The system is designed to handle multi-premise inferences as well as singlepremise ones. We test our system on the FraCaS test suite (Cooper et al., 1994), which is suitable for evaluating t"
D15-1244,J07-4004,0,0.0435106,"on a reasonably-sized semantic lexicon and a manageable number of non-first-order axioms enables efficient logical inferences, including those concerned with generalized quantifiers and intensional operators, and outperforms the state-of-the-art firstorder inference system. 1 Introduction Entailment relations are of central importance in the enterprise of both formal and computational semantics. Traditionally, formal semanticists have concentrated on a relatively small set of linguistic inferences. However, since the emergence of statistical parsers based on sophisticated syntactic theories (Clark and Curran, 2007), an open domain system has been developed that supports certain degree of robust semantic interpretation with wide coverage (Bos et al., 2004). It is then reasonable to expect that a state-of-the-art formal semantics provides an accurate computational basis of natural language inferences. However, there are still obstacles in the way of achieving this goal. One is that the statistical parsers on which semantic interpretations rely do not necessarily reflect the best syntactic analysis as assumed in the formal semantics literature (Honnibal et al., 2010). Another persistent problem is the gap"
D15-1244,W06-3907,0,0.113004,"Veridical and anti-veridical predicates A sentential operator O is veridical if O(A) entails A, and anti-veridical if O(A) entails ¬A. While modal auxiliary verbs like might are neither veridical nor anti-veridical, there is a class of expressions licensing these patterns of inference. Typical examples are adjectives taking an embedded proposition, such as true/correct and false/incorrect. Note that sentences like Everything/what he said is false involve a quantification over propositions, which is problematic for the first-order approach. The so-called implicative verbs like manage and fail (Nairn et al., 2006) are also an instance of this class. For example, Some student manages to come is formalized as (3) ∃x(student(x) ∧ manage(x, come(x))) where manage is a veridical predicate taking a proposition as the second argument; it licenses an inference to ∃x(student(x) ∧ come(x)). Attitude verbs A wide range of propositional attitude verbs such as believe and hope are similar to modals in that they do not license an inference from attitude contexts to actual contexts. But factives like know and remember are an exception; they are veridical.4 A first-order translation can be given along the lines of Hin"
D15-1244,P14-1008,1,0.797423,"Missing"
D15-1244,Q13-1015,0,\N,Missing
D16-1002,D13-1160,0,0.0435073,"s (e.g. “how”, “many”, “WRB”, etc.) and produces a tree fragment with terminals (“COUNT”, x1 , x2 ) and non-terminals (“ID”) with a specific structure. Rules r2 and r3 only consume but do not produce symbols (other than variables). The rhs of rules are target tree fragments that connect to each other at the frontier nodes (those with variables). Rules r4 and r5 are terminal rules, where r4 produces the predicate Team and rule r5 produces the entity Uefa and a disambiguating predicate League that has no lexical support on the source side, similarly to the role that bridging predicates play in Berant et al. (2013). Given a corpus of source and target tree pairs, the learning stage aims to obtain rules such as r1 − r5 in Figure 1 and their associated probabilities or scores. We discuss our novel approach to rule extraction in Section 5. For the assignment of rule scores, we adopt the latent variable averaged structured perceptron, a discriminative procedure similar to Tsochantaridis et al. (2005) and Cohn and Lapata (2009). Here, we instantiate feature values f for every rule, and reward the weights w of rules that participate in a derivation (latent variable) that transforms a training source tree into"
D16-1002,J93-2003,0,0.0955461,"cher (1970), and have been greatly developed recently (Knight and Graehl, 2005). Jones et al. (2012) used tree transducers to semantically parse narrow-domain questions into Prolog queries for GeoQuery (Wong and Mooney, 2006), a small database of 700 geographical facts. Rules were exhaustively enumerated, which was possible given the small size of the database and low variability of questions. Another strategy is that of Li et al. (2013), where they used a variant of GHKM to induce tree transducers that parse into λ-SCFG. Wordto-node alignments could be reliably estimated with the IBM models (Brown et al., 1993) given, again, the small vocabulary and database size of GeoQuery. In such small-scale tasks, our rule extraction and back-off scheme offers no obvious advantage. However, when doing QA over larger and more realistic KBs (and other tasks with similar characteristics), exhaustive enumeration of rules or reliable estimations of alignments are not possible, which prevents the application of tree transducers. Thus, it is on the latter type of tasks where we focus our contribution. A similar problem has been considered in the tree 1 The entity lexicon was released by the authors of F REE 917, and t"
D16-1002,P13-1042,0,0.0160231,"s. Our rule extraction algorithm and back-off scheme are general, in the sense that they can be applied to any tree transformation task. However, in this paper, we extrinsically evaluate the quality of the extracted rules in a QA task, where the objective is to transform syntactic trees of questions into constituent trees that represent Sparql queries on Freebase, a large Knowledge Base. Implementing all components of a QA system at a sufficient level is out of the scope of this paper; for that reason, in order to evaluate our contribution in isolation, we use the F REE 917 corpus released by Cai and Yates (2013), for which an entity and predicate lexicon is available1 . We show that a tree-to-tree transducer induced using our rule extraction and back-off scheme is accurate and generalizes well, which was not previously achieved with tree transducers in semantic parsing tasks such as QA over large KBs. 2 Related Work Tree transducers were first proposed by Rounds (1970) and Thatcher (1970), and have been greatly developed recently (Knight and Graehl, 2005). Jones et al. (2012) used tree transducers to semantically parse narrow-domain questions into Prolog queries for GeoQuery (Wong and Mooney, 2006),"
D16-1002,P03-2041,0,0.0648123,"s of finding the minimum edit cost and its corresponding edit script2 that transforms a source into a target tree. The problem was first solved by Tai (1979), and later Zhang and Shasha (1989) proposed a simpler and faster dynamic programming algorithm that operates in polynomial time, and that has inspired multiple variations (Bille, 2005). However, we need edit operations that involve tree fragments (e.g., noun phrases or parts of verb phrases), rather than single nodes, when searching for the best mappings. We address this problem by searching for non-isomorphic tree mappings, in line with Eisner (2003), except that our rule extraction algorithm is guided by an ensemble of cost functions over pairs of tree fragments. This algorithm is capable of extracting rules more robustly than GHKM by permitting misalignments in a controlled manner. Finding a tree mapping solves simultaneously the alignment and the rule extraction problem. There is a wide array of tree transducers with different expressive capabilities (Knight and Graehl, 2005). We consider extended3 root-to-frontier4 linear5 transducers (Maletti et al., 2009), possibly with deleting6 operations. In this paper, we syntactically parse the"
D16-1002,N04-1035,0,0.0974168,"d Yates, 2013), but an automatically generated predicate lexicon. Instead, our system and the second baseline use manually created entity and predicate lexicons, where the latter was created by selecting all words from every question that relate to the target predicate. For example, for the question “what olympics has egypt participated in”, we created an entry that maps the discontinuous phrase “olympics participated in” to the predicate OlympicsParticipatedIn. The second baseline is a tree-to-tree transducer whose rules are extracted using a straightforward adaptation of the GHKM algorithm (Galley et al., 2004) for pairs of trees. Word-to-concept alignments are extracted using three different strategies: i) ghkm-g uses the IBM models (Brown et al., 1993) as implemented in GIZA++ (Och and Ney, 2003), ii) ghkm-m maps KB concepts (target leaves) to as many source words as present in the entity/predicate lexicons, and iii) ghkm-c maps KB concepts as in ii) but only retaining the longest contiguous sequence of source words (or right-most sequence if there is a tie). Bridging predicates are assumed when a KB concept does not align (according to the lexicon) to any source word. Finally, rule state names ar"
D16-1002,W05-0602,0,0.0203909,"ver pairs of tree fragments. This algorithm is capable of extracting rules more robustly than GHKM by permitting misalignments in a controlled manner. Finding a tree mapping solves simultaneously the alignment and the rule extraction problem. There is a wide array of tree transducers with different expressive capabilities (Knight and Graehl, 2005). We consider extended3 root-to-frontier4 linear5 transducers (Maletti et al., 2009), possibly with deleting6 operations. In this paper, we syntactically parse the natural language question and transform it into a meaning representation, similarly to Ge and Mooney (2005). But instead of using Prolog formulae or λ-SCFG, we use constituent representations of λ−DCS expressions (Liang, 2013), which is a formal language convenient to represent Sparql queries where variables are eliminated by making existential quantifications implicit (see example in Figure 1). Another challenge is to construct transducers with sufficient rule coverage, which would require billions of lexical rules that map question phrases to database entities or relations. Even if those rules were available, estimating their rule probabilities would be difficult given the small data sets of ques"
D16-1002,N04-1014,0,0.269265,"e transducers apply to general tree transformation problems, but for illustrative purposes, we use the tree pair s and t in Figure 1 (from F REE 917) as a running example. s is the syntactic constituent tree of the question “how many teams participate in the uefa”, whereas t is a constituent tree of an executable meaning representation in the λ−DCS formalism: count(Team.League.Uefa) Its corresponding lambda expression is count(λx.∃a.Team(x, a) ∧ League(a, Uefa)) which can be converted into a Sparql KB query: SELECT COUNT(?x) WHERE { ?a Team ?x . ?a League Uefa . } Following the terminology of Graehl and Knight (2004), we define a tree-to-tree transducer as a 5tuple (Q, Σ, ∆, qstart , R) where Q is the set of states, Σ and ∆ are the sets of symbols of the input and output languages, qstart is the initial state, and R is the set of rules. For convenience, define TΣ as the set of trees with symbols in Σ, TΣ (A) the set of trees with symbols in Σ ∪ A where symbols in A only appear in the leaves, X as the set of variables {x1 , . . . , xn }, and A.B for the cross-product of two sets A and B. A rule r ∈ R has the form s q.ti → to , where q ∈ Q is a state, ti ∈ TΣ (X ) is the left-hand-side (lhs) tree pattern (o"
D16-1002,P07-1019,0,0.0175399,"ed and added to the priority queue indexed by pc , to propagate upwards in the hierarchy of solutions the decision of not combining disjoint subpaths. Finally, G ENERATE D ISJOINT returns the n-best pairs of disjoint subpaths of minimum cost (p, p0 ) that accumulated in the priority queue P for path ps . 5.3.3 Figure 2: Hypergraph with 2-best pairs of disjoint subpaths for Other Considerations The n-best source and target pairs of disjoint subpaths are stored at every pair of source and target paths (ps , pt ) (lines 2-10), forming a hypergraph, as in Figure 2. Then, with a hypergraph search (Huang and Chiang, 2007) we can retrieve at least n-best sequences of rules (derivations) that transform the source into the target tree (line 11). To maintain diversity of partial disjoint subpaths, we divide P into a matrix of buckets with as many rows and columns as the number of non-variable terminals of the source and target tree patterns, trading memory for more effective search (Koehn, 2015). This operation is implicit in lines 24, 30 and 35. 18 6 6.1 Experiments Experiment Settings Data The training data is a corpus of questions annotated with their logical forms that can be executed on Freebase to obtain a p"
D16-1002,P12-1051,0,0.0672533,"ff states at which some cost functions generate right-hand-sides of previously unseen lefthand-sides, thus creating transducer rules “on-the-fly”. We test the generalization power of our induced tree transducers on a QA task over a large Knowledge Base, obtaining a reasonable syntactic accuracy and effectively overcoming the typical lack of rule coverage. 1 Introduction Tree transducers are general and solid theoretical models that have been applied to a variety of NLP tasks, such as machine translation (Knight and Graehl, 2005), text summarization (Cohn and Lapata, 2009), question answering (Jones et al., 2012), paraphrasing and textual entailment (Wu, 2005). One strategy to obtain transducer rules is by exhaustive enumeration; however, this method is ineffective when there is a high structural language variability and we wish to have an expressive model. Another strategy is to heuristically extract rules from a corpus of tree/string pairs and word-alignments, as Our main contribution is an algorithm that formulates the rule extraction as a cost minimization problem, where the search for the best rules is guided by an ensemble of cost functions over pairs of tree fragments. In GHKM, a tree fragment"
D16-1002,P03-1054,0,0.0731422,"Missing"
D16-1002,J13-2005,0,0.014439,"in a controlled manner. Finding a tree mapping solves simultaneously the alignment and the rule extraction problem. There is a wide array of tree transducers with different expressive capabilities (Knight and Graehl, 2005). We consider extended3 root-to-frontier4 linear5 transducers (Maletti et al., 2009), possibly with deleting6 operations. In this paper, we syntactically parse the natural language question and transform it into a meaning representation, similarly to Ge and Mooney (2005). But instead of using Prolog formulae or λ-SCFG, we use constituent representations of λ−DCS expressions (Liang, 2013), which is a formal language convenient to represent Sparql queries where variables are eliminated by making existential quantifications implicit (see example in Figure 1). Another challenge is to construct transducers with sufficient rule coverage, which would require billions of lexical rules that map question phrases to database entities or relations. Even if those rules were available, estimating their rule probabilities would be difficult given the small data sets of ques2 Sequence of edit operations. lhs may have depth larger than 1. 4 Top-down transformations. 5 lhs variables appear at"
D16-1002,Q15-1023,0,0.0156373,"ical rules that map question phrases to database entities or relations. Even if those rules were available, estimating their rule probabilities would be difficult given the small data sets of ques2 Sequence of edit operations. lhs may have depth larger than 1. 4 Top-down transformations. 5 lhs variables appear at most once in the rhs. 6 Some variables on the lhs may not appear in the rhs. 3 tions paired with their logical representations. We solve the problem by constructing lexical rules “onthe-fly” at the decoding stage, similarly to the candidate generation stage of entity linking systems (Ling et al., 2015). Rule weights are also predicted on-thefly given rule features and model parameters similar to Cohn and Lapata (2009). 3 Background Tree transducers apply to general tree transformation problems, but for illustrative purposes, we use the tree pair s and t in Figure 1 (from F REE 917) as a running example. s is the syntactic constituent tree of the question “how many teams participate in the uefa”, whereas t is a constituent tree of an executable meaning representation in the λ−DCS formalism: count(Team.League.Uefa) Its corresponding lambda expression is count(λx.∃a.Team(x, a) ∧ League(a, Uefa"
D16-1002,P09-1063,0,0.0607215,"Missing"
D16-1002,J03-1002,0,0.0137904,"Missing"
D16-1002,C69-0101,0,0.334202,"rge Knowledge Base. Implementing all components of a QA system at a sufficient level is out of the scope of this paper; for that reason, in order to evaluate our contribution in isolation, we use the F REE 917 corpus released by Cai and Yates (2013), for which an entity and predicate lexicon is available1 . We show that a tree-to-tree transducer induced using our rule extraction and back-off scheme is accurate and generalizes well, which was not previously achieved with tree transducers in semantic parsing tasks such as QA over large KBs. 2 Related Work Tree transducers were first proposed by Rounds (1970) and Thatcher (1970), and have been greatly developed recently (Knight and Graehl, 2005). Jones et al. (2012) used tree transducers to semantically parse narrow-domain questions into Prolog queries for GeoQuery (Wong and Mooney, 2006), a small database of 700 geographical facts. Rules were exhaustively enumerated, which was possible given the small size of the database and low variability of questions. Another strategy is that of Li et al. (2013), where they used a variant of GHKM to induce tree transducers that parse into λ-SCFG. Wordto-node alignments could be reliably estimated with the IBM"
D16-1002,P15-1129,0,0.0228827,"Missing"
D16-1002,N06-1056,0,0.0248302,"by Cai and Yates (2013), for which an entity and predicate lexicon is available1 . We show that a tree-to-tree transducer induced using our rule extraction and back-off scheme is accurate and generalizes well, which was not previously achieved with tree transducers in semantic parsing tasks such as QA over large KBs. 2 Related Work Tree transducers were first proposed by Rounds (1970) and Thatcher (1970), and have been greatly developed recently (Knight and Graehl, 2005). Jones et al. (2012) used tree transducers to semantically parse narrow-domain questions into Prolog queries for GeoQuery (Wong and Mooney, 2006), a small database of 700 geographical facts. Rules were exhaustively enumerated, which was possible given the small size of the database and low variability of questions. Another strategy is that of Li et al. (2013), where they used a variant of GHKM to induce tree transducers that parse into λ-SCFG. Wordto-node alignments could be reliably estimated with the IBM models (Brown et al., 1993) given, again, the small vocabulary and database size of GeoQuery. In such small-scale tasks, our rule extraction and back-off scheme offers no obvious advantage. However, when doing QA over larger and more"
D16-1002,W05-1205,0,0.015619,"-sides of previously unseen lefthand-sides, thus creating transducer rules “on-the-fly”. We test the generalization power of our induced tree transducers on a QA task over a large Knowledge Base, obtaining a reasonable syntactic accuracy and effectively overcoming the typical lack of rule coverage. 1 Introduction Tree transducers are general and solid theoretical models that have been applied to a variety of NLP tasks, such as machine translation (Knight and Graehl, 2005), text summarization (Cohn and Lapata, 2009), question answering (Jones et al., 2012), paraphrasing and textual entailment (Wu, 2005). One strategy to obtain transducer rules is by exhaustive enumeration; however, this method is ineffective when there is a high structural language variability and we wish to have an expressive model. Another strategy is to heuristically extract rules from a corpus of tree/string pairs and word-alignments, as Our main contribution is an algorithm that formulates the rule extraction as a cost minimization problem, where the search for the best rules is guided by an ensemble of cost functions over pairs of tree fragments. In GHKM, a tree fragment and a sequence of words are extracted together i"
D16-1004,N10-1083,0,0.377162,"Missing"
D16-1004,Q13-1007,0,0.303546,"by calculating the valence from indices in the rule. For example, after L-P RED, wh does not take any right dependents so θS (stop|wh , →, h = j), where j is the right span index of X[wh ], is multiplied. Improvement Though we omit the details, we can improve the time complexity of the above grammar from O(n6 ) to O(n4 ) applying the technique similar to Eisner and Satta (1999) without changing the binarization mechanism mentioned above. We implemented this improved grammar. 5 Experimental setup A sound evaluation metric in grammar induction is known as an open problem (Schwartz et al., 2011; Bisk and Hockenmaier, 2013), which essentially arises from the ambiguity in the notion of head. For example, Universal dependencies (UD) is the recent standard in annotation and prefers content words to be heads, but as shown below this is very different from the conventional style, e.g., the one in CoNLL shared tasks (Johansson and Nugues, 2007): nsbj UD Ivan C O NLL is the cop det best sbj prd nmod amod nmod The problem is that both trees are correct under some linguistic theories but the standard metric, unlabeled attachment score (UAS), only takes into account the annotation of the current gold data. Our goal in thi"
D16-1004,P15-2143,0,0.0194069,"describe below. For D EP, we use δ = 1.ξ to denote the relaxed maximum depth allowing span length up to ξ (Eq. 4). L EN is the previously explored structural bias (Smith and Eisner, 2006), which penalizes longer dependencies by modifying each attachment score: Depth bound δ Length bias γ 50 20 0 0 1 1.2 1.3 1.4 2 0.1 0.2 0.3 0.4 0.5 Parameters (upper=δ; bottom=γ) Figure 8: UAS for various settings on (UD) WSJ. Hyperparameters Selecting hyperparameters in multilingual grammar induction is difficult; some works tune values for each language based on the development set (Smith and Eisner, 2006; Bisk et al., 2015), but this violates the assumption of unsupervised learning. We instead follow many works ˇ (Mareˇcek and Zabokrtsk´ y, 2012; Naseem et al., 2010) and select the values with the English data. For this, we use the WSJ data, which we obtain in UD style from the Stanford CoreNLP (ver. 3.6.0).9 6 Experiments WSJ Figure 8 shows the result on WSJ. Both D EP and L EN have one parameter: the maximum depth δ, and γ (Eq. 5), and the figure shows the sensitivity on them. Note that x-axis = 0 represents F UNC. For L EN, we can see the optimal parameter γ is 0.1, and degrades the performance when increasin"
D16-1004,P99-1059,0,0.269584,"ned based on the structure of the binarized CFG parse. Parameterization We can encode DMV parameters into each rule. A new arc is introduced by one of {L/R}-{P RED /C OMP}, and the stop probabilities can be assigned appropriately in each rule by calculating the valence from indices in the rule. For example, after L-P RED, wh does not take any right dependents so θS (stop|wh , →, h = j), where j is the right span index of X[wh ], is multiplied. Improvement Though we omit the details, we can improve the time complexity of the above grammar from O(n6 ) to O(n4 ) applying the technique similar to Eisner and Satta (1999) without changing the binarization mechanism mentioned above. We implemented this improved grammar. 5 Experimental setup A sound evaluation metric in grammar induction is known as an open problem (Schwartz et al., 2011; Bisk and Hockenmaier, 2013), which essentially arises from the ambiguity in the notion of head. For example, Universal dependencies (UD) is the recent standard in annotation and prefers content words to be heads, but as shown below this is very different from the conventional style, e.g., the one in CoNLL shared tasks (Johansson and Nugues, 2007): nsbj UD Ivan C O NLL is the co"
D16-1004,N12-1069,0,0.676724,"e the models try to recover the syntactic structure of language without access to the syntactically annotated data, e.g., from raw or partof-speech tagged text only. In these settings, finding better syntactic regularities universal across languages is essential, as they work as a small cue to the correct linguistic structures. A preference exploited in many previous works is favoring shorter dependencies, which has been encoded in various ways, e.g., initialization of EM (Klein and Manning, 2004), or model parameters (Smith and Eisner, 2006), and this has been the key to success of learning (Gimpel and Smith, 2012). Center-embedding is difficult to process and is known as a rare syntactic construction across languages. In this paper we describe a method to incorporate this assumption into the grammar induction tasks by restricting the search space of a model to trees with limited centerembedding. The key idea is the tabulation of left-corner parsing, which captures the degree of center-embedding of a parse via its stack depth. We apply the technique to learning of famous generative model, the dependency model with valence (Klein and Manning, 2004). Cross-linguistic experiments on Universal Dependencies"
D16-1004,N09-1012,1,0.930549,"Missing"
D16-1004,W07-2416,0,0.0222205,") applying the technique similar to Eisner and Satta (1999) without changing the binarization mechanism mentioned above. We implemented this improved grammar. 5 Experimental setup A sound evaluation metric in grammar induction is known as an open problem (Schwartz et al., 2011; Bisk and Hockenmaier, 2013), which essentially arises from the ambiguity in the notion of head. For example, Universal dependencies (UD) is the recent standard in annotation and prefers content words to be heads, but as shown below this is very different from the conventional style, e.g., the one in CoNLL shared tasks (Johansson and Nugues, 2007): nsbj UD Ivan C O NLL is the cop det best sbj prd nmod amod nmod The problem is that both trees are correct under some linguistic theories but the standard metric, unlabeled attachment score (UAS), only takes into account the annotation of the current gold data. Our goal in this experiment is to assess the effect of our structural constraints. To this end, we try to eliminate such arbitrariness in our evaluation as much as possible in the following way: • We experiment on UD, in which every treebank follows the consistent UD style annotation. • We restrict the model to explore only trees that"
D16-1004,P98-1101,1,0.715457,"e largest stack depth d∗ during parsing this tree is: d∗ = λ + 1. Schuler et al. (2010) found that on English treebanks larger stack depth such as 3 or 4 rarely occurs while Noji and Miyao (2014) validated the language universality of this observation through crosslinguistic experiments. These suggest we may utilize LC parsing as a tool for exploiting universal syntactic biases as we discuss in Section 3. Historical notes Rosenkrantz and Lewis (1970) first presented the idea of LC parsing as a grammar transform. This is arc-standard, and has no relevance to center-embedding; Resnik (1992) and Johnson (1998) formulated an arc-eager variant by extending this algorithm. The presented algorithm here is the same as Schuler et al. (2010), and is slightly different from Johnson (1998). The difference is in the start and end conditions: while 2 Schuler et al. (2010) skip this subtlety by only concerning stack depth after P RED or C OMP. We do not take this approach since ours allows a flexible extension described in Section 3. our parser begins with an empty symbol, Johnson’s parser begins with the predicted start symbol, and finishes with an empty symbol. 3 Learning with structural constraints Now we d"
D16-1004,P07-1022,1,0.729021,"2) this one-to-many mapping prevents the insideoutside algorithm to work correctly (Eisner, 2000). As a concrete example, Figures 5(a) and 5(c) show two CFG parses corresponding to the dependency tree dogsx rany fast. We approach this problem by first providing a grammar transform, which generates all valid LC transformed parses (e.g., Figures 5(b) and 5(d)) and then restricting the grammar 3 Another approach might be just applying the technique in Section 3 to some PCFG that encodes DMV, e.g., Headden III et al. (2009). The problem with this approach, in particular with split-head grammars (Johnson, 2007), is that the calculated stack depth no longer reflects the degree of center-embedding in the original parse correctly. As we discuss later, instead, we can speed up inference by applying head-splitting after obtaining the LC transformed grammar. 4 Technical details including the chart algorithm for splithead grammars can be found in the Ph.D. thesis of the first author (Noji, 2016). 37 Naive method Let us begin with the grammar below, which suffers from the spurious ambiguity: S HIFT: X[wh ]d → wh S CAN : X[wh ]d → X[wh /wp ]d wp L-P RED : X[wp /wp ]d → X[wh ]d (whx wp ); R-P RED : X[wh /wp ]"
D16-1004,P04-1061,0,0.839343,"du.au Abstract 2012; Bisk and Hockenmaier, 2013) or weaklysupervised (Garrette et al., 2015) grammar induction tasks, where the models try to recover the syntactic structure of language without access to the syntactically annotated data, e.g., from raw or partof-speech tagged text only. In these settings, finding better syntactic regularities universal across languages is essential, as they work as a small cue to the correct linguistic structures. A preference exploited in many previous works is favoring shorter dependencies, which has been encoded in various ways, e.g., initialization of EM (Klein and Manning, 2004), or model parameters (Smith and Eisner, 2006), and this has been the key to success of learning (Gimpel and Smith, 2012). Center-embedding is difficult to process and is known as a rare syntactic construction across languages. In this paper we describe a method to incorporate this assumption into the grammar induction tasks by restricting the search space of a model to trees with limited centerembedding. The key idea is the tabulation of left-corner parsing, which captures the degree of center-embedding of a parse via its stack depth. We apply the technique to learning of famous generative mo"
D16-1004,D12-1028,0,0.157762,"Missing"
D16-1004,D10-1120,1,0.881222,"t, comparing the effect of several biases including the one against longer dependencies. Our main empirical finding is that though two biases, avoiding center-embedding and favoring shorter dependencies, are conceptually similar (both favor simpler grammars), often they capture different aspects of syntax, leading to different grammars. In particular our bias cooperates well with additional small syntactic cue such as the one that the sentence root tends to be a verb or a noun, with which our models compete with the strong baseline relying on a larger number of hand crafted rules on POS tags (Naseem et al., 2010). Our contributions are: the idea to utilize leftcorner parsing for a tool to constrain the models of syntax (Section 3), the formulation of this idea for DMV (Section 4), and cross-linguistic experiments across 25 languages to evaluate the universality of the proposed approach (Sections 5 and 6). 2 Left-corner parsing We first describe (arc-eager) left-corner (LC) parsing as a push-down automaton (PDA), and then reformulate it as a grammar transform. In previous work this algorithm has been called right-corner parsing (e.g., Schuler et al. (2010)); we avoid this term and instead treat it as a"
D16-1004,C14-1202,1,0.895681,"areˇcek and Zabokrtsk´ y, In this paper, we explore the utility for another universal syntactic bias that has not yet been exploited in grammar induction: a bias against centerembedding. Center-embedding is a syntactic construction on which a clause is embedded into another one. An example is “The reporter [who the senator [who Mary met] attacked] ignored the president.”, where “who Mary met” is embedded in a larger relative clause. These constructions are known to cause memory overflow (Miller and Chomsky, 1963; Gibson, 2000), and also are rarely observed crosslinguistically (Karlsson, 2007; Noji and Miyao, 2014). Our learning method exploits this universal property of language. Intuitively during learning our models explore the restricted search space, which excludes linguistically implausible trees, i.e., those with deeper levels of center-embedding. We describe how these constraints can be imposed in EM with the inside-outside algorithm. The central 33 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 33–43, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics S HIFT S CAN P RED C OMP a σ d−1 7− → σ d−1 |Ad a d−1 σ |B/Ad 7−"
D16-1004,C92-1032,0,0.36042,"Missing"
D16-1004,P80-1024,0,0.811412,"fication is necessary since C OMP for a single token occurs for building purely right-branching structures.2 Formally, then, given a tree with degree λ of center-embedding the largest stack depth d∗ during parsing this tree is: d∗ = λ + 1. Schuler et al. (2010) found that on English treebanks larger stack depth such as 3 or 4 rarely occurs while Noji and Miyao (2014) validated the language universality of this observation through crosslinguistic experiments. These suggest we may utilize LC parsing as a tool for exploiting universal syntactic biases as we discuss in Section 3. Historical notes Rosenkrantz and Lewis (1970) first presented the idea of LC parsing as a grammar transform. This is arc-standard, and has no relevance to center-embedding; Resnik (1992) and Johnson (1998) formulated an arc-eager variant by extending this algorithm. The presented algorithm here is the same as Schuler et al. (2010), and is slightly different from Johnson (1998). The difference is in the start and end conditions: while 2 Schuler et al. (2010) skip this subtlety by only concerning stack depth after P RED or C OMP. We do not take this approach since ours allows a flexible extension described in Section 3. our parser begins w"
D16-1004,J10-1001,0,0.519299,"arger number of hand crafted rules on POS tags (Naseem et al., 2010). Our contributions are: the idea to utilize leftcorner parsing for a tool to constrain the models of syntax (Section 3), the formulation of this idea for DMV (Section 4), and cross-linguistic experiments across 25 languages to evaluate the universality of the proposed approach (Sections 5 and 6). 2 Left-corner parsing We first describe (arc-eager) left-corner (LC) parsing as a push-down automaton (PDA), and then reformulate it as a grammar transform. In previous work this algorithm has been called right-corner parsing (e.g., Schuler et al. (2010)); we avoid this term and instead treat it as a variant of LC parsing following more recent studies, e.g., van Schijndel 34 D D i j A B j+1 C OMP k ===⇒ i B j C A j+1 k Figure 2: C OMP combines two subtrees on the top of the stack. i, j, k are indices of spans. and Schuler (2013). The central motivation for this technique is to detect center-embedding in a parse efficiently. We describe this mechanism after providing the algorithm itself. We then give historical notes on LC parsing at the end of this section. PDA Let us assume a CFG is given, and it is in CNF. We formulate LC parsing as a set"
D16-1004,P11-1067,0,0.0146579,"opriately in each rule by calculating the valence from indices in the rule. For example, after L-P RED, wh does not take any right dependents so θS (stop|wh , →, h = j), where j is the right span index of X[wh ], is multiplied. Improvement Though we omit the details, we can improve the time complexity of the above grammar from O(n6 ) to O(n4 ) applying the technique similar to Eisner and Satta (1999) without changing the binarization mechanism mentioned above. We implemented this improved grammar. 5 Experimental setup A sound evaluation metric in grammar induction is known as an open problem (Schwartz et al., 2011; Bisk and Hockenmaier, 2013), which essentially arises from the ambiguity in the notion of head. For example, Universal dependencies (UD) is the recent standard in annotation and prefers content words to be heads, but as shown below this is very different from the conventional style, e.g., the one in CoNLL shared tasks (Johansson and Nugues, 2007): nsbj UD Ivan C O NLL is the cop det best sbj prd nmod amod nmod The problem is that both trees are correct under some linguistic theories but the standard metric, unlabeled attachment score (UAS), only takes into account the annotation of the curre"
D16-1004,silveira-etal-2014-gold,0,0.0503099,"Missing"
D16-1004,P06-1072,0,0.829822,") or weaklysupervised (Garrette et al., 2015) grammar induction tasks, where the models try to recover the syntactic structure of language without access to the syntactically annotated data, e.g., from raw or partof-speech tagged text only. In these settings, finding better syntactic regularities universal across languages is essential, as they work as a small cue to the correct linguistic structures. A preference exploited in many previous works is favoring shorter dependencies, which has been encoded in various ways, e.g., initialization of EM (Klein and Manning, 2004), or model parameters (Smith and Eisner, 2006), and this has been the key to success of learning (Gimpel and Smith, 2012). Center-embedding is difficult to process and is known as a rare syntactic construction across languages. In this paper we describe a method to incorporate this assumption into the grammar induction tasks by restricting the search space of a model to trees with limited centerembedding. The key idea is the tabulation of left-corner parsing, which captures the degree of center-embedding of a parse via its stack depth. We apply the technique to learning of famous generative model, the dependency model with valence (Klein"
D16-1004,N13-1010,0,0.0355593,"Missing"
D16-1004,C98-1098,1,\N,Missing
D16-1242,D15-1296,0,0.570323,"e general-purpose theorem provers, there is room for developing an automated reasoning system specialized for natural language inference. In general, a higher-order representation makes the logical structure of a sentence more explicit than a first-order encoding does and hence can simplify the process of proof search (Miller and Nadathur, 1986). Recently, based on the evaluation on the FraCaS dataset (Cooper et al., 1994), Mineshima et al. (2015) showed that a higher-order inference system outperformed the Boxer/Nutcracker’s firstorder system (Bos, 2008) in both speed and accuracy. Likewise, Abzianidze (2015) developed a higher-order prover based on natural logic tableau system and showed that it achieved high accuracy comparable to state-of-the-art results on the SICK dataset (Marelli et al., 2014). There are three main steps in our pipeline. The focus of this paper is on the last two components. 1. Syntactic parsing Input sentences are mapped onto CCG trees. We use a Japanese CCG parser Jigg (Noji and Miyao, 2016)1 , a statistical parser based on Japanese CCGbank (Uematsu et al., 2015). 2. Semantic parsing CCG derivation trees are compositionally mapped onto semantic representations in HOL. The"
D16-1242,S13-1002,0,0.108184,"t focus on a variety of complex linguistic phenomena, including those that are difficult to represent in the standard first-order logic. 1 Introduction Logic-based semantic representations have played an important role in the study of semantic parsing and inference. For English, several methods have been proposed to map outputs of parsers based on syntactic theories like CCG (Steedman, 2000) onto logical formulas (Bos, 2015). Output formulas have been used in various tasks, including Question Answering (Lewis and Steedman, 2013) and Recognizing Textual Entailment (RTE) (Bos and Markert, 2005; Beltagy et al., 2013; Bjerva et al., 2014). Syntactic and semantic parsing for Japanese, by contrast, has been dominated by chunk-based dependency parsing and semantic role labelling (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2011; Hayashibe et al., 2011). Recently, the method of inducing wide-coverage CCG resources for English (Hockenmaier and Steedman, 2007) has been applied to Japanese and a robust CCG parser based on it has been developed (Uematsu et al., 2015). However, building a method to map CCG trees in Japanese onto logical formulas is not a trivial task, Daisuke Bekki1 bekki@is.ocha.ac.jp Natio"
D16-1242,S14-2114,0,0.0879134,"Missing"
D16-1242,H05-1079,0,0.43454,"inference problems that focus on a variety of complex linguistic phenomena, including those that are difficult to represent in the standard first-order logic. 1 Introduction Logic-based semantic representations have played an important role in the study of semantic parsing and inference. For English, several methods have been proposed to map outputs of parsers based on syntactic theories like CCG (Steedman, 2000) onto logical formulas (Bos, 2015). Output formulas have been used in various tasks, including Question Answering (Lewis and Steedman, 2013) and Recognizing Textual Entailment (RTE) (Bos and Markert, 2005; Beltagy et al., 2013; Bjerva et al., 2014). Syntactic and semantic parsing for Japanese, by contrast, has been dominated by chunk-based dependency parsing and semantic role labelling (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2011; Hayashibe et al., 2011). Recently, the method of inducing wide-coverage CCG resources for English (Hockenmaier and Steedman, 2007) has been applied to Japanese and a robust CCG parser based on it has been developed (Uematsu et al., 2015). However, building a method to map CCG trees in Japanese onto logical formulas is not a trivial task, Daisuke Bekki1 bek"
D16-1242,W08-2222,0,0.766979,"guage (Montague, 1974). Although HOL does not have general-purpose theorem provers, there is room for developing an automated reasoning system specialized for natural language inference. In general, a higher-order representation makes the logical structure of a sentence more explicit than a first-order encoding does and hence can simplify the process of proof search (Miller and Nadathur, 1986). Recently, based on the evaluation on the FraCaS dataset (Cooper et al., 1994), Mineshima et al. (2015) showed that a higher-order inference system outperformed the Boxer/Nutcracker’s firstorder system (Bos, 2008) in both speed and accuracy. Likewise, Abzianidze (2015) developed a higher-order prover based on natural logic tableau system and showed that it achieved high accuracy comparable to state-of-the-art results on the SICK dataset (Marelli et al., 2014). There are three main steps in our pipeline. The focus of this paper is on the last two components. 1. Syntactic parsing Input sentences are mapped onto CCG trees. We use a Japanese CCG parser Jigg (Noji and Miyao, 2016)1 , a statistical parser based on Japanese CCGbank (Uematsu et al., 2015). 2. Semantic parsing CCG derivation trees are compositi"
D16-1242,W15-1841,0,0.0164941,"esentations and performs automated inference in higher-order logic. The system is evaluated on a textual entailment dataset. It is shown that the system solves inference problems that focus on a variety of complex linguistic phenomena, including those that are difficult to represent in the standard first-order logic. 1 Introduction Logic-based semantic representations have played an important role in the study of semantic parsing and inference. For English, several methods have been proposed to map outputs of parsers based on syntactic theories like CCG (Steedman, 2000) onto logical formulas (Bos, 2015). Output formulas have been used in various tasks, including Question Answering (Lewis and Steedman, 2013) and Recognizing Textual Entailment (RTE) (Bos and Markert, 2005; Beltagy et al., 2013; Bjerva et al., 2014). Syntactic and semantic parsing for Japanese, by contrast, has been dominated by chunk-based dependency parsing and semantic role labelling (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2011; Hayashibe et al., 2011). Recently, the method of inducing wide-coverage CCG resources for English (Hockenmaier and Steedman, 2007) has been applied to Japanese and a robust CCG parser base"
D16-1242,J07-4004,0,0.322433,"cl. ecei.tohoku.ac.jp/rite2/doku.php). 6 2240 FraCaS. M15 refers to the accuracy of Mineshima et al. (2015) on the corresponding sections of FraCaS. possible, our system with gold parses outperforms it for all sections. Out of the 523 problems, 417 are Japanese translations of the FraCaS problems. Table 4 shows a comparison between the performance of our system on this subset of the JSeM problems and the performance of the RTE system for English in Mineshima et al. (2015) on the corresponding problems in the FraCaS dataset. Mineshima et al. (2015) used system parses of the English C&C parser (Clark and Curran, 2007). The total accuracy of our system is comparable to that of Mineshima et al. (2015). Most errors we found are due to syntactic parse errors caused by the CCG parser, where no correct syntactic parses were found in n-best responses. Comparison between gold parses and system parses shows that correct syntactic disambiguation improves performance. 5 Conclusion To our knowledge, this study provides the first semantic parsing system based on CCG that compositionally maps real texts in Japanese onto logical forms. We have also demonstrated the capacity of HOL for textual entailment. The evaluation o"
D16-1242,I11-1023,0,0.0135898,"tic parsing and inference. For English, several methods have been proposed to map outputs of parsers based on syntactic theories like CCG (Steedman, 2000) onto logical formulas (Bos, 2015). Output formulas have been used in various tasks, including Question Answering (Lewis and Steedman, 2013) and Recognizing Textual Entailment (RTE) (Bos and Markert, 2005; Beltagy et al., 2013; Bjerva et al., 2014). Syntactic and semantic parsing for Japanese, by contrast, has been dominated by chunk-based dependency parsing and semantic role labelling (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2011; Hayashibe et al., 2011). Recently, the method of inducing wide-coverage CCG resources for English (Hockenmaier and Steedman, 2007) has been applied to Japanese and a robust CCG parser based on it has been developed (Uematsu et al., 2015). However, building a method to map CCG trees in Japanese onto logical formulas is not a trivial task, Daisuke Bekki1 bekki@is.ocha.ac.jp National Institute of Informatics The Graduate University for Advanced Studies Tokyo, Japan 3 mainly due to the differences in syntactic structures between English and Japanese (Section 3). There are two primary contributions of this paper. First,"
D16-1242,P85-1008,0,0.741917,"the correct meaning for VP modifiers, the semantic type of a verb is raised so that the verb takes a modifier as argument but not vice versa. Figures 2 and 3 give example derivations. VP modifiers such as slowly license an inference from John walked slowly to John walked, an inference correctly captured by the formula in (2). In English and Japanese, however, there are intensional VP modifiers that do not license this inference pattern. Thus, the sentence John almost walked does not entail John walked (Dowty, 1979). While it is not easy to provide a desirable analysis in first-order language (Hobbs, 1985), HOL gives a perspicuous representation: (3) ∃v(almost(walk, v) ∧ (Nom(v) = john) ∧ Past(v)) Here, almost is a higher-order predicate having the semantic type (Ev ⇒ Prop) ⇒ Ev ⇒ Prop. The meaning assignment to VP modifiers of category S/S in Table 1 is for extensional modifiers; an intensional modifier is assigned the representation λSK.S(λJv.K(Base(J), v)) in the lexical entry, which results in a representation as in (3). 3.3 Semantic composition for NPs ‘John walked slowly’ (2) ∃v(walk(v) ∧ (Nom(v) = john) ∧ slow(v) ∧ Past(v)) In this approach, verbs are analyzed as 1-place predicates over"
D16-1242,J07-3004,0,0.134853,"s based on syntactic theories like CCG (Steedman, 2000) onto logical formulas (Bos, 2015). Output formulas have been used in various tasks, including Question Answering (Lewis and Steedman, 2013) and Recognizing Textual Entailment (RTE) (Bos and Markert, 2005; Beltagy et al., 2013; Bjerva et al., 2014). Syntactic and semantic parsing for Japanese, by contrast, has been dominated by chunk-based dependency parsing and semantic role labelling (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2011; Hayashibe et al., 2011). Recently, the method of inducing wide-coverage CCG resources for English (Hockenmaier and Steedman, 2007) has been applied to Japanese and a robust CCG parser based on it has been developed (Uematsu et al., 2015). However, building a method to map CCG trees in Japanese onto logical formulas is not a trivial task, Daisuke Bekki1 bekki@is.ocha.ac.jp National Institute of Informatics The Graduate University for Advanced Studies Tokyo, Japan 3 mainly due to the differences in syntactic structures between English and Japanese (Section 3). There are two primary contributions of this paper. First, based on an in-depth analysis of the syntax-semantics interface in Japanese, we present the first system th"
D16-1242,I11-1051,0,0.0204822,"ant role in the study of semantic parsing and inference. For English, several methods have been proposed to map outputs of parsers based on syntactic theories like CCG (Steedman, 2000) onto logical formulas (Bos, 2015). Output formulas have been used in various tasks, including Question Answering (Lewis and Steedman, 2013) and Recognizing Textual Entailment (RTE) (Bos and Markert, 2005; Beltagy et al., 2013; Bjerva et al., 2014). Syntactic and semantic parsing for Japanese, by contrast, has been dominated by chunk-based dependency parsing and semantic role labelling (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2011; Hayashibe et al., 2011). Recently, the method of inducing wide-coverage CCG resources for English (Hockenmaier and Steedman, 2007) has been applied to Japanese and a robust CCG parser based on it has been developed (Uematsu et al., 2015). However, building a method to map CCG trees in Japanese onto logical formulas is not a trivial task, Daisuke Bekki1 bekki@is.ocha.ac.jp National Institute of Informatics The Graduate University for Advanced Studies Tokyo, Japan 3 mainly due to the differences in syntactic structures between English and Japanese (Section 3). There are two primary contributio"
D16-1242,W02-2016,0,0.0415891,"ions have played an important role in the study of semantic parsing and inference. For English, several methods have been proposed to map outputs of parsers based on syntactic theories like CCG (Steedman, 2000) onto logical formulas (Bos, 2015). Output formulas have been used in various tasks, including Question Answering (Lewis and Steedman, 2013) and Recognizing Textual Entailment (RTE) (Bos and Markert, 2005; Beltagy et al., 2013; Bjerva et al., 2014). Syntactic and semantic parsing for Japanese, by contrast, has been dominated by chunk-based dependency parsing and semantic role labelling (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2011; Hayashibe et al., 2011). Recently, the method of inducing wide-coverage CCG resources for English (Hockenmaier and Steedman, 2007) has been applied to Japanese and a robust CCG parser based on it has been developed (Uematsu et al., 2015). However, building a method to map CCG trees in Japanese onto logical formulas is not a trivial task, Daisuke Bekki1 bekki@is.ocha.ac.jp National Institute of Informatics The Graduate University for Advanced Studies Tokyo, Japan 3 mainly due to the differences in syntactic structures between English and Japanese (Section 3). The"
D16-1242,W07-1431,0,0.0374715,"s of this paper. First, based on an in-depth analysis of the syntax-semantics interface in Japanese, we present the first system that compositionally derives semantic representations for a wide-coverage Japanese CCG parser. Output representations are formulas in higher-order logic (HOL) combined with NeoDavidsonian Event Semantics (Parsons, 1990). Second, we demonstrate the capacity of HOL for textual entailment. We evaluate the system on a Japanese textual entailment dataset (Kawazoe et al., 2015), a dataset constructed in a similar way to the FraCaS dataset for English (Cooper et al., 1994; MacCartney and Manning, 2007). Although it is usually thought that HOL is unfeasible for practical applications, the results show that the entire system is able to perform efficient logical inference on complex linguistic phenomena such as generalized quantifiers and intensional modifiers — phenomena that pose challenges to the standard first-order-logic-based approaches. 2 Background and system overview This section provides a brief overview of the entire system as applied to RTE, a task of determining whether a given text (T ) entails, contradicts, or is just consistent with, a given hypothesis (H). In logic-based appro"
D16-1242,C08-1066,0,0.0741623,"Missing"
D16-1242,marelli-etal-2014-sick,0,0.0672121,"logical structure of a sentence more explicit than a first-order encoding does and hence can simplify the process of proof search (Miller and Nadathur, 1986). Recently, based on the evaluation on the FraCaS dataset (Cooper et al., 1994), Mineshima et al. (2015) showed that a higher-order inference system outperformed the Boxer/Nutcracker’s firstorder system (Bos, 2008) in both speed and accuracy. Likewise, Abzianidze (2015) developed a higher-order prover based on natural logic tableau system and showed that it achieved high accuracy comparable to state-of-the-art results on the SICK dataset (Marelli et al., 2014). There are three main steps in our pipeline. The focus of this paper is on the last two components. 1. Syntactic parsing Input sentences are mapped onto CCG trees. We use a Japanese CCG parser Jigg (Noji and Miyao, 2016)1 , a statistical parser based on Japanese CCGbank (Uematsu et al., 2015). 2. Semantic parsing CCG derivation trees are compositionally mapped onto semantic representations in HOL. The compositional mapping is implemented via simply typed λ-calculus in the standard way (Bos, 2008; Mart´ınez-G´omez et al., 2016). 3. Logical inference Theorem proving in HOL is performed to check"
D16-1242,P16-4015,1,0.724535,"Missing"
D16-1242,P86-1037,0,0.869953,"e represented in the standard FOL; a typical example is a generalized quantifier such as most (Barwise and Cooper, 1981). Accordingly, it has been standard in formal semantics of natural language to use HOL as a representation language (Montague, 1974). Although HOL does not have general-purpose theorem provers, there is room for developing an automated reasoning system specialized for natural language inference. In general, a higher-order representation makes the logical structure of a sentence more explicit than a first-order encoding does and hence can simplify the process of proof search (Miller and Nadathur, 1986). Recently, based on the evaluation on the FraCaS dataset (Cooper et al., 1994), Mineshima et al. (2015) showed that a higher-order inference system outperformed the Boxer/Nutcracker’s firstorder system (Bos, 2008) in both speed and accuracy. Likewise, Abzianidze (2015) developed a higher-order prover based on natural logic tableau system and showed that it achieved high accuracy comparable to state-of-the-art results on the SICK dataset (Marelli et al., 2014). There are three main steps in our pipeline. The focus of this paper is on the last two components. 1. Syntactic parsing Input sentence"
D16-1242,D15-1244,1,0.737489,"Missing"
D16-1242,P16-4018,1,0.71355,"r et al., 1994), Mineshima et al. (2015) showed that a higher-order inference system outperformed the Boxer/Nutcracker’s firstorder system (Bos, 2008) in both speed and accuracy. Likewise, Abzianidze (2015) developed a higher-order prover based on natural logic tableau system and showed that it achieved high accuracy comparable to state-of-the-art results on the SICK dataset (Marelli et al., 2014). There are three main steps in our pipeline. The focus of this paper is on the last two components. 1. Syntactic parsing Input sentences are mapped onto CCG trees. We use a Japanese CCG parser Jigg (Noji and Miyao, 2016)1 , a statistical parser based on Japanese CCGbank (Uematsu et al., 2015). 2. Semantic parsing CCG derivation trees are compositionally mapped onto semantic representations in HOL. The compositional mapping is implemented via simply typed λ-calculus in the standard way (Bos, 2008; Mart´ınez-G´omez et al., 2016). 3. Logical inference Theorem proving in HOL is performed to check for entailment and contradiction. Axioms and proof-search procedures are largely language-independent, so we use the higherorder inference system of Mineshima et al. (2015)2 and adapt it for our purpose. 1 2 https://gith"
D16-1242,Q13-1015,0,\N,Missing
E03-1047,2000.iwpt-1.9,0,0.0223572,"us heuristic and statistical methods are applicable. Consistency between the grammar rules and the obtained lexical entries is assured independently of the methods of annotation. Lastly, the validity of the grammar theories is evaluated on real-world texts. A degree of low coverage by a linguistically motivated grammar does not necessarily reflect inadequacy of the grammar theories; a lack of appropriate lexical entries may also be responsible. The analysis of obtained grammars gives us grounds for discussing the pros and cons of the theories. The studies on the extraction of LTAG (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000) and CCG (Hockenmaier and Steedman, 2002) represent the first attempts at the acquisition of linguistically motivated grammars from annotated corpora. Those studies are limited to specific formalisms, and can be interpreted as instances of our approach as described in Section 3. This paper does not describe any concrete algorithms for grammar acquisition that depend on specific grammar formalisms. The contribution of our work is to formally state the conditions required for the acquisition of lexicalized grammars and to demon127 strate that it can be applied to lexicalized gramm"
E03-1047,P00-1058,0,0.0287483,"ethods are applicable. Consistency between the grammar rules and the obtained lexical entries is assured independently of the methods of annotation. Lastly, the validity of the grammar theories is evaluated on real-world texts. A degree of low coverage by a linguistically motivated grammar does not necessarily reflect inadequacy of the grammar theories; a lack of appropriate lexical entries may also be responsible. The analysis of obtained grammars gives us grounds for discussing the pros and cons of the theories. The studies on the extraction of LTAG (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000) and CCG (Hockenmaier and Steedman, 2002) represent the first attempts at the acquisition of linguistically motivated grammars from annotated corpora. Those studies are limited to specific formalisms, and can be interpreted as instances of our approach as described in Section 3. This paper does not describe any concrete algorithms for grammar acquisition that depend on specific grammar formalisms. The contribution of our work is to formally state the conditions required for the acquisition of lexicalized grammars and to demon127 strate that it can be applied to lexicalized grammars other than"
E03-1047,P02-1043,0,0.105742,"Consistency between the grammar rules and the obtained lexical entries is assured independently of the methods of annotation. Lastly, the validity of the grammar theories is evaluated on real-world texts. A degree of low coverage by a linguistically motivated grammar does not necessarily reflect inadequacy of the grammar theories; a lack of appropriate lexical entries may also be responsible. The analysis of obtained grammars gives us grounds for discussing the pros and cons of the theories. The studies on the extraction of LTAG (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000) and CCG (Hockenmaier and Steedman, 2002) represent the first attempts at the acquisition of linguistically motivated grammars from annotated corpora. Those studies are limited to specific formalisms, and can be interpreted as instances of our approach as described in Section 3. This paper does not describe any concrete algorithms for grammar acquisition that depend on specific grammar formalisms. The contribution of our work is to formally state the conditions required for the acquisition of lexicalized grammars and to demon127 strate that it can be applied to lexicalized grammars other than LTAG and CCG, such as HPSG. 2 Lexicalized"
E03-1047,H94-1020,0,0.177212,"Missing"
E03-1047,C88-2121,0,0.806242,"T (Japan Science and Technology Corporation) Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN {yusuke, ninomi , tsuj ii}@is s .u-tokyo ac jp Abstract This paper presents a formalization of automatic grammar acquisition that is based on lexicalized grammar formalisms (e.g. LTAG and HPSG). We state the conditions for the consistent acquisition of a unique lexicalized grammar from an annotated corpus. 1 Introduction Linguistically motivated and computationally oriented grammar theories take the form of lexicalized grammar formalisms; examples include Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988), Combinatory Categorial Grammar (Steedman, 2000), and Head-driven Phrase Structure Grammar (HPSG) (Sag and Wasow, 1999). They have been a great success in terms of linguistic analysis and efficiency in the parsing of real-world texts. However, such grammars have not generally been considered suitable for the syntactic analysis within practical NLP systems because considerable effort is required to develop and maintain lexicalized grammars that are both robust and provide broad coverage. One novel approach to grammar development is based on the automatic acquisition of lexicalized grammars fro"
E12-1070,2001.mtsummit-papers.26,0,0.0352842,"h set. Dorr (1997) created an LCS-based lexical resource as an interlingual representation for machine translation. This framework was also used for text generation (Habash et al., 2003). However, the problem of multiple-role assignment was not completely solved on the resource. As a comparison of different semantic structures, Dorr (2001) and Hajiˇcov´a and Kuˇcerov´a (2002) analyzed the connection between LCS and PropBank roles, and showed that the mapping between LCS and PropBank roles was many to many correspondence and roles can map only by comparing a whole argument structure of a verb. Habash and Dorr (2001) tried to map LCS structures into thematic roles by using their thematic hierarchy. 3 Multiple role expression using lexical conceptual structure Lexical conceptual structure is an approach to describe a generalized structure of an event or state represented by a verb. A meaning of a verb is represented as a structure composed of several primitive predicates. For example, the LCS structure for the verb “throw” is shown in Figure 1 and includes the predicates cause, affect, go, from, fromward, toward, locate, in, and at. The arguments of primitive predicates are filled by core arguments of the"
E12-1070,hajicova-kucerova-2002-argument,0,0.0906746,"Missing"
E12-1070,kawahara-kurohashi-2006-case,0,0.19127,"Missing"
E12-1070,kingsbury-palmer-2002-treebank,0,0.221764,"role annotation which solves these problems by extending the theory of lexical conceptual structure (LCS). By comparing our framework with that of existing resources, including VerbNet and FrameNet, we demonstrate that our extended LCS framework can give a formal definition of semantic role labels, and that multiple roles of arguments can be represented strictly and naturally. 1 Introduction Recent developments of large semantic resources have accelerated empirical research on semantic processing (M`arquez et al., 2008). Specifically, corpora with semantic role annotations, such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Ruppenhofer et al., 2006), are indispensable resources for semantic role labeling. However, there are two topics we have to carefully take into consideration regarding role assignment frameworks: (1) clarity of semantic role meanings and (2) the constraint that a single semantic role is assigned to each syntactic argument. While these resources are undoubtedly invaluable for empirical research on semantic process[John] Agent Source Arg0 Agent Agent threw [a ball] Patient Theme Arg1 Theme Theme [from the window] . Source/Path Arg2 Source Source Table 1: Examples of single role as"
E12-1070,J08-2001,0,0.0617362,"Missing"
E12-1070,N07-1069,0,0.0318867,"cal research on semantic process[John] Agent Source Arg0 Agent Agent threw [a ball] Patient Theme Arg1 Theme Theme [from the window] . Source/Path Arg2 Source Source Table 1: Examples of single role assignments with existing resources. ing, current usage of semantic labels for SRL systems is questionable from a theoretical viewpoint. For example, most of the works on SRL have used PropBank’s numerical role labels (Arg0 to Arg5). However, the meanings of these numbers depend on each verb in principle and PropBank does not expect semantic consistency, namely on Arg2 to Arg5. Moreover, Yi et al. (2007) explicitly showed that Arg2 to Arg5 are semantically inconsistent. The reason why such labels have been used in SRL systems is that verb-specific roles generally have a small number of instances and are not suitable for learning. However, it is necessary to avoid using inconsistent labels since those labels confuse machine learners and can be a cause of low accuracy in automatic processing. In addition, clarity of the definition of roles are particularly important for users to rationally know how to use each role in their applications. For this reasons, well-organized and generalized labels g"
E17-1067,D15-1296,0,0.787045,"mizu University 3 National Institute of Informatics and JST, PRESTO 4 The Graduate University for Advanced Studies (SOKENDAI) Tokyo, Japan Abstract mantic (Bos et al., 2004), where logical formulas that represent the text fragments are constructed and used in a formal proof system. And yet others are hybrid systems (Beltagy et al., 2013), where a combination of statistical features and logical formulas are used to judge entailment relations. In this paper, we adopt a strategy based on logics, encouraged by the high-performance that these systems achieve in linguistically challenging datasets (Abzianidze, 2015; Mineshima et al., 2015). An important advantage of these systems (including ours) is that they are unsupervised, thus no training data is necessary and no parameters need to be adjusted. Under the perspective of these logic-based systems, there are mainly two associated challenges when solving RTE problems. The first challenge is to model the logics of the language with the purpose to represent the semantics of text fragments accurately. To this end, we follow the standard practice in formal semantics where the meaning of sentences is represented using logical formulas. The second challenge"
E17-1067,S13-1002,0,0.368894,"ction of Lexical Knowledge for Recognising Textual Entailment Pascual Mart´ınez-G´omez1 pascual.mg@aist.go.jp Koji Mineshima2 mineshima.koji@ocha.ac.jp Daisuke Bekki1,2,3 bekki@is.ocha.ac.jp Yusuke Miyao1,3,4 yusuke@nii.ac.jp 1 Artificial Intelligence Research Center, AIST 2 Ochanomizu University 3 National Institute of Informatics and JST, PRESTO 4 The Graduate University for Advanced Studies (SOKENDAI) Tokyo, Japan Abstract mantic (Bos et al., 2004), where logical formulas that represent the text fragments are constructed and used in a formal proof system. And yet others are hybrid systems (Beltagy et al., 2013), where a combination of statistical features and logical formulas are used to judge entailment relations. In this paper, we adopt a strategy based on logics, encouraged by the high-performance that these systems achieve in linguistically challenging datasets (Abzianidze, 2015; Mineshima et al., 2015). An important advantage of these systems (including ours) is that they are unsupervised, thus no training data is necessary and no parameters need to be adjusted. Under the perspective of these logic-based systems, there are mainly two associated challenges when solving RTE problems. The first ch"
E17-1067,J16-4007,0,0.361894,"nt problem. Moreover, Nutcracker is not a purely logical system in that it uses a proofapproximation method with model-builders. By contrast, our system is purely logic-based, in that it solely relies on proof constructions based on natural deduction system to make entailment judgements. In addition, as we will see below, a goal-directed proof construction procedure in our system is naturally combined with on-demand axiom injection, as opposed to simply selecting any two arbitrary phrases from T and H that display Perhaps the most similar strategies to ours are those of Tian et al. (2014) and Beltagy et al. (2016), where the authors produce on-thefly knowledge when the hypothesis H cannot be proved. In the work of Tian et al. (2014), propositions between T and H are aligned using logical clues; then, dependency paths are extracted between these propositions and WordNet or word vectors are used to assess the similarity between paths. However, the expressive power of their underlying representation system in Dependencybased Compositional Semantics (DCS) is rather limited and much weaker than the full first-order logic (Liang et al., 2013). Several extensions have been proposed (Tian et al., 2014; Dong et"
E17-1067,Y14-1067,1,0.860632,"(2016), where the authors produce on-thefly knowledge when the hypothesis H cannot be proved. In the work of Tian et al. (2014), propositions between T and H are aligned using logical clues; then, dependency paths are extracted between these propositions and WordNet or word vectors are used to assess the similarity between paths. However, the expressive power of their underlying representation system in Dependencybased Compositional Semantics (DCS) is rather limited and much weaker than the full first-order logic (Liang et al., 2013). Several extensions have been proposed (Tian et al., 2014; Dong et al., 2014), yet these DCS-based inference systems are a non-standard axiomatic system with many axioms and tend to be ad hoc. Whereas their semantic representations are specific to their logic framework, ours are well-understood, logically transparent representations that are generic to most stateof-the-art theorem provers using first-order logic. Beltagy et al. (2016) use a Modified Robinson Resolution strategy to align clauses and literals between T and H. These alignments also constrain how the unaligned fragments of T and H may correspond to each other, reducing the 711 ple formula (2) but to a form"
E17-1067,S14-2114,0,0.393359,"Missing"
E17-1067,N13-1092,0,0.155939,"Missing"
E17-1067,S14-2131,0,0.0585618,"Missing"
E17-1067,H05-1079,0,0.212184,"al-logic-based tableuax prover. However, his logical representations are based on a nonstandard natural logic, which requires the definition of new inference rules for each logical word (e.g. every, some, no) and for which generic theorem provers are not reusable. Regarding the introduction of linguistic knowledge, the author uses only WordNet. However, during the learning phase, he adds missing knowledge manually (e.g. note is a hyponym of paper), whereas we restrict our results to those automatically generated. Related Work Our work on recognising textual entailment is primarily inspired by Bos and Markert (2005), where first-order logic interpretations of sentences are used to prove entailment relations with theorem provers and model builders. These semantic interpretations were composed using Boxer (Bos et al., 2004) from derivations of a Combinatory Categorial Grammar (CCG) (Steedman, 2000) automatically obtained by C&C, a wide-coverage CCG parser (Clark and Curran, 2007). This system was later extended into Nutcracker (Bjerva et al., 2014), where WordNet (Miller, 1995) and relations from Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) are used to introduce external linguistic resources to a"
E17-1067,C04-1180,0,0.0492373,"r which generic theorem provers are not reusable. Regarding the introduction of linguistic knowledge, the author uses only WordNet. However, during the learning phase, he adds missing knowledge manually (e.g. note is a hyponym of paper), whereas we restrict our results to those automatically generated. Related Work Our work on recognising textual entailment is primarily inspired by Bos and Markert (2005), where first-order logic interpretations of sentences are used to prove entailment relations with theorem provers and model builders. These semantic interpretations were composed using Boxer (Bos et al., 2004) from derivations of a Combinatory Categorial Grammar (CCG) (Steedman, 2000) automatically obtained by C&C, a wide-coverage CCG parser (Clark and Curran, 2007). This system was later extended into Nutcracker (Bjerva et al., 2014), where WordNet (Miller, 1995) and relations from Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) are used to introduce external linguistic resources to account for lexical divergences (Pavlick et al., 2015). Pavlick et al. (2015) study the characteristics of linguistic relations that may signal entailment or contradiction at subsentential level. However, they i"
E17-1067,S14-2055,0,0.267184,"oach is effective and precise, producing a system that outperforms other logicbased systems and is competitive with state-of-the-art statistical methods. 1 Introduction Recognising Textual Entailment (RTE) is a challenging NLP application where the objective is to judge whether a text fragment H logically follows from another text fragment T (Dagan et al., 2013). Advances in RTE have potentially positive implications in other areas such as fact checking, question-answering or information retrieval. Solutions to the RTE problem span a wide array of methods. Some methods are purely statistical (Lai and Hockenmaier, 2014; Zhao et al., 2014), where a classifying function is estimated using lexical or syntactic features. Other methods are purely se710 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 710–720, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics any linguistic relation. of a logical proof, detects unprovable sub-goals, and inserts axioms when necessary if a lexical relation is found in an external linguistic resource. These linguistic axioms encode lexical relations between speci"
E17-1067,W08-2222,0,0.21491,"given text (T ) entails a given hypothesis (H). In logic-based approaches, T and H are mapped onto logical formulas; whether T entails H is then determined by checking whether T → H is a theorem in a logical system, possibly with the help of a knowledge base. To obtain logical formulas for input sentences, we use the framework of Combinatory Categorial Grammar (CCG) (Steedman, 2000), a lexicalized grammar formalism that provides a transparent interface between syntax and semantics. We follow the standard method of building compositional semantics in CCG-based systems (Blackburn and Bos, 2005; Bos, 2008), where each syntactic category is schematically assigned a meaning representation formally specified as a λ-term. By combining the meanings of constituent words that appear in a CCG derivation tree, we can obtain a logical formula that serves as a semantic representation of an input sentence. For semantic representations, we adopt NeoDavidsonian Event Semantics (Parsons, 1990; Bos, 2008; Jurafsky and Martin, 2009). For instance, the sentence in (1) is mapped not to a sim4 4.1 Methodology Preliminaries: proving strategy We adopt natural deduction (Prawitz, 1965) as a proof calculus. Here, a ty"
E17-1067,D14-1107,0,0.220312,"na¨ıve and the SPSA methods were all timed-out after 100 seconds, at which the entailment judgement “unknown” was produced. When a syntactic parse error occurs, our systems tend to judge the entailment relation as “unknown”. To gain robustness and following Abzianidze (2015), we use a multiparsing strategy (unless stated otherwise), that is, we use both C&C and EasyCCG parsers, and output any of their judgements if they are different Experimental setup We parsed the tokenized sentences of the premises and hypotheses using the wide-coverage CCG parsers C&C (Clark and Curran, 2007) and EasyCCG (Lewis and Steedman, 2014). CCG derivation trees (parses) were converted into logical semantic representations using ccg2lambda (Mart´ınez-G´omez et al., 2016) and our first-order Neo-Davidsonian event semantics. The validation of our version of semantic templates was carried out exclusively on the trial split of the SICK dataset. We used Coq (Cast´eran and Bertot, 2004), an interactive natural deduction (Coquand and Huet, 1988) theorem prover that we run fully automatically with a number of built-in theorem-proving routines called tactics, which include first-order 2 https://www.cis.upenn.edu/˜treebank/ tokenization.h"
E17-1067,W04-3205,0,0.309738,"an semantic templates are opensourced and publicly available at https:// github.com/mynlp/ccg2lambda. (1) In the example above, the sub-goal sleep(v1 ) is a candidate sub-goal to form an axiom, and its list of possible relations is Rsleep = {nap}. 4.3 On-demand axiom construction Given a candidate sub-goal p0j (θj0 ), Rj is a list of possible predicates that may semantically subsume or exclude the meaning of p0j . At this point, we only need to classify each pi ∈ Rj as subsuming (entailing) p0j , excluding (contradicting) it, or unrelated. In this work, we choose to use WordNet and VerbOcean (Chklovski and Pantel, 2004) as sources of external linguistic knowledge for their high precision. However, one could use other databases, ontologies or statistical classifiers, but we leave those considerations out of the scope of this paper. There are two possible types of axioms that can be created: either entailing axioms ∀θ.pi (θi ) → p0j (θj0 ), or contradiction axioms ∀θ.pi (θi ) → ¬p0j (θj0 ), where θ = θj0 ∪ θi is the union of variable names occurring in θj0 and θi . Entailing axioms are created when synonymy (e.g. house → home), 5 5.1 Experiments Dataset We use the SemEval-2014 version of the SICK dataset (Mare"
E17-1067,J13-2005,0,0.0146573,"t similar strategies to ours are those of Tian et al. (2014) and Beltagy et al. (2016), where the authors produce on-thefly knowledge when the hypothesis H cannot be proved. In the work of Tian et al. (2014), propositions between T and H are aligned using logical clues; then, dependency paths are extracted between these propositions and WordNet or word vectors are used to assess the similarity between paths. However, the expressive power of their underlying representation system in Dependencybased Compositional Semantics (DCS) is rather limited and much weaker than the full first-order logic (Liang et al., 2013). Several extensions have been proposed (Tian et al., 2014; Dong et al., 2014), yet these DCS-based inference systems are a non-standard axiomatic system with many axioms and tend to be ad hoc. Whereas their semantic representations are specific to their logic framework, ours are well-understood, logically transparent representations that are generic to most stateof-the-art theorem provers using first-order logic. Beltagy et al. (2016) use a Modified Robinson Resolution strategy to align clauses and literals between T and H. These alignments also constrain how the unaligned fragments of T and"
E17-1067,J07-4004,0,0.741316,"earning phase, he adds missing knowledge manually (e.g. note is a hyponym of paper), whereas we restrict our results to those automatically generated. Related Work Our work on recognising textual entailment is primarily inspired by Bos and Markert (2005), where first-order logic interpretations of sentences are used to prove entailment relations with theorem provers and model builders. These semantic interpretations were composed using Boxer (Bos et al., 2004) from derivations of a Combinatory Categorial Grammar (CCG) (Steedman, 2000) automatically obtained by C&C, a wide-coverage CCG parser (Clark and Curran, 2007). This system was later extended into Nutcracker (Bjerva et al., 2014), where WordNet (Miller, 1995) and relations from Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) are used to introduce external linguistic resources to account for lexical divergences (Pavlick et al., 2015). Pavlick et al. (2015) study the characteristics of linguistic relations that may signal entailment or contradiction at subsentential level. However, they ignore the logical context in which these linguistic relations occur in the entailment problem. Moreover, Nutcracker is not a purely logical system in that it u"
E17-1067,marelli-etal-2014-sick,0,0.203561,"004) as sources of external linguistic knowledge for their high precision. However, one could use other databases, ontologies or statistical classifiers, but we leave those considerations out of the scope of this paper. There are two possible types of axioms that can be created: either entailing axioms ∀θ.pi (θi ) → p0j (θj0 ), or contradiction axioms ∀θ.pi (θi ) → ¬p0j (θj0 ), where θ = θj0 ∪ θi is the union of variable names occurring in θj0 and θi . Entailing axioms are created when synonymy (e.g. house → home), 5 5.1 Experiments Dataset We use the SemEval-2014 version of the SICK dataset (Marelli et al., 2014), which is a dataset of English single-premise textual entailment problems annotated with three relations: entailing (yes), contradicting (no) or unrelated (unknown). The SICK dataset was originally developed to test approaches of compositional distributional semantics and it includes a variety of lexical, syntactic and semantic phenomena at the sentential level. With respect to FraCaS (Cooper et al., 1994), it contains less linguistically challenging problems but there is a higher need of lexical knowledge, 1 To maximise coverage, we consider all possible senses for a given predicate (word)."
E17-1067,P16-4015,1,0.792727,"Missing"
E17-1067,D15-1244,1,0.781361,"Missing"
E17-1067,D16-1242,1,0.889008,"Missing"
E17-1067,P15-1146,0,0.0645042,"Missing"
E17-1067,S14-2093,0,0.0227471,"al split of the SICK dataset. We used Coq (Cast´eran and Bertot, 2004), an interactive natural deduction (Coquand and Huet, 1988) theorem prover that we run fully automatically with a number of built-in theorem-proving routines called tactics, which include first-order 2 https://www.cis.upenn.edu/˜treebank/ tokenization.html 715 from “unknown”3 . Out of more than 20 participating teams in SemEval 2014, we compare our system to the following representative state-of-the-art systems: Illinois-LH (Lai and Hockenmaier, 2014), ECNU (Zhao et al., 2014), UNAL-NLP (Jim´enez et al., 2014), SemantiKLUE (Proisl et al., 2014) are systems that build statistical classifiers on shallow features such as word alignments, syntactic structures and distributional similarities. These systems are the top performing systems in SemEval-2014. The Meaning Factory (Bjerva et al., 2014) is a hybrid system that combines logic semantic representations derived from CCG trees, with model builders and a statistical classifier, whereas LangPro (Abzianidze, 2015) is a purely logic system that composes Lambda Logical Forms of Natural Logic from CCG derivations. Nutcracker is a first-order logic system, where the effectiveness of introduc"
E17-1067,P14-1008,1,0.860108,"s occur in the entailment problem. Moreover, Nutcracker is not a purely logical system in that it uses a proofapproximation method with model-builders. By contrast, our system is purely logic-based, in that it solely relies on proof constructions based on natural deduction system to make entailment judgements. In addition, as we will see below, a goal-directed proof construction procedure in our system is naturally combined with on-demand axiom injection, as opposed to simply selecting any two arbitrary phrases from T and H that display Perhaps the most similar strategies to ours are those of Tian et al. (2014) and Beltagy et al. (2016), where the authors produce on-thefly knowledge when the hypothesis H cannot be proved. In the work of Tian et al. (2014), propositions between T and H are aligned using logical clues; then, dependency paths are extracted between these propositions and WordNet or word vectors are used to assess the similarity between paths. However, the expressive power of their underlying representation system in Dependencybased Compositional Semantics (DCS) is rather limited and much weaker than the full first-order logic (Liang et al., 2013). Several extensions have been proposed ("
E17-1067,S14-2042,0,0.149524,"se, producing a system that outperforms other logicbased systems and is competitive with state-of-the-art statistical methods. 1 Introduction Recognising Textual Entailment (RTE) is a challenging NLP application where the objective is to judge whether a text fragment H logically follows from another text fragment T (Dagan et al., 2013). Advances in RTE have potentially positive implications in other areas such as fact checking, question-answering or information retrieval. Solutions to the RTE problem span a wide array of methods. Some methods are purely statistical (Lai and Hockenmaier, 2014; Zhao et al., 2014), where a classifying function is estimated using lexical or syntactic features. Other methods are purely se710 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 710–720, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics any linguistic relation. of a logical proof, detects unprovable sub-goals, and inserts axioms when necessary if a lexical relation is found in an external linguistic resource. These linguistic axioms encode lexical relations between specific segments of the"
E17-1067,P11-1060,0,\N,Missing
fujita-etal-2014-overview,I13-1192,1,\N,Missing
fujita-etal-2014-overview,C12-1084,1,\N,Missing
fujita-etal-2014-overview,W12-3212,0,\N,Missing
grissom-ii-miyao-2012-annotating,W09-2510,0,\N,Missing
I05-1018,P02-1036,0,0.0239988,"Missing"
I05-1018,P05-1011,1,0.642017,"Missing"
I05-1018,J96-1002,0,0.004459,"Missing"
I05-1018,A00-2021,0,0.447388,"Missing"
I05-1018,P04-1014,0,0.0421718,"Missing"
I05-1018,H94-1020,0,\N,Missing
I05-1018,P99-1069,0,\N,Missing
I08-2122,I05-1018,1,0.788734,"se. In order to observe such differences, we need to integrate available combinations of tools into a workflow and to compare the combinatorial results. Although generic frameworks like UIMA (Unstructured Information Management Architecture) provide interoperability to solve this problem, the solution they provide is only partial. In order for truly interoperable toolkits to become a reality, we also need 1 Introduction Recently, an increasing number of TM/NLP tools such as part-of-speech (POS) taggers (Tsuruoka et al., 2005), named entity recognizers (NERs) (Settles, 2005) syntactic parsers (Hara et al., 2005) and relation or event extractors (ERs) have been developed. Nevertheless, it is still very difficult to integrate independently developed tools into an aggregated application that achieves a specific task. The difficulties are caused not only by differences in programming platforms and different input/output data formats, but also by the lack of higher level interoperability among modules developed by different groups. 859 uima.jcas.cas.TOP tcas.uima.Annotation -begin: int -end: int SyntacticAnnotation POS SemanticAnnotation UnknownPOS PennPOS -posType: String Token Sentence Phrase NamedEntit"
I08-2122,W04-1213,0,0.0372378,"Missing"
I08-2122,J93-2004,0,0.0293416,"type systems have to be related through a sharable type system, which our platform defines. Such a shared type system can bridge modules with different type systems, though the bridging module may lose some information during the translation process. Whether such a sharable type system can be defined or not is dependent on the nature of each problem. For example, a sharable type system for POS tags in English can be defined rather easily, since most of POS-related modules (such as POS taggers, shallow parsers, etc.) more or less follow the well established types defined by the Penn Treebank (Marcus et al., 1993) tag set. Figure 1 shows a part of our sharable type system. We deliberately define a highly organized type hierarchy as described above. Secondly we should consider that the type system may be used to compare a similar sort of tools. Types should be defined in a distinct and 861 hierarchical manner. For example, both tokenizers and POS taggers output an object of type Token, but their roles are different when we assume a cascaded pipeline. We defined Token as a supertvpe, POSToken as subtypes of Token. Each tool should have an individual type to make clear which tool generated which instance,"
I08-2122,E06-1015,0,0.0352696,"Missing"
I08-2122,J96-1002,0,0.0129233,"Missing"
I11-1084,W06-1615,0,0.0623828,"me parsing system can be applied to a novel domain. However, there are some cases where we cannot achieve such high parsing accuracy as parsing 2 Related work Since domain adaptation is an extensive research area in parsing research (Nivre et al., 2007), many ideas have been proposed, including un- or 749 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 749–757, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP and imperatives. In the experiments, we also used QuestionBank for comparison. semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). The main focus of these works is on adapting parsing models trained with a specific genre of text (in most cases the Penn Treebank WSJ) to other genres of text, such as biomedical research papers and broadcast news. The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression. However, parsing imperatives and questions involves a significantly different problem; even when"
I11-1084,W07-2208,1,0.7578,"et al., 2006)3 , respectively. Although the publicly available implementation of each parser also has the option to restrict the output to a projective dependency tree, we used the non-projective versions because the dependency structures converted from the question sentences in the Brown Corpus included many non-projective dependencies. We used the pennconverter (Johansson and Nugues, 2007) 4 to convert a PTB-style treebank into dependency trees 5 . To evaluate the output from each of the parsers, we used the labeled attachment accuracy excluding punctuation. 3.2 HPSG parser The Enju parser (Ninomiya et al., 2007)6 is a deep parser based on the HPSG (Head Driven Phrase Structure Grammar) formalism. It produces an analysis of a sentence including the syntactic structure (i.e., parse tree) and the semantic structure represented as a set of predicateargument dependencies. We used the toolkit distributed with the Enju parser to train the parser with a PTB-style treebank. The toolkit initially converts a PTB-style treebank into an HPSG treebank and then trains the parser on this. The HPSG treebank converted from the test section was used as the gold-standard in the evaluation. As evaluation metrics for the"
I11-1084,J07-4004,0,0.025712,"accuracy of state-of-the-art parsers on questions, and proposed a supervised parser adaptation by manually creating a treebank of questions.1 The question sentences are annotated with phrase structure trees in the Penn Treebank scheme, although function tags and empty categories are omitted. QuestionBank was used for the supervised training of an LFG parser, resulting in a significant improvement in parsing accuracy. Rimell and Clark (2008) also worked on the problem of question parsing in the context of domain adaptation, and proposed a supervised method for the adaptation of the C&C parser (Clark and Curran, 2007). In this work, question sentences were collected from TREC 9-12 competitions and annotated with POS and CCG lexical categories. The authors reported a significant improvement in CCG parsing without phrase structure annotations. Our work further extends Judge et al. (2006) and Rimell and Clark (2008), while covering a wider range of sentence constructions. Although QuestionBank and the resource of Rimell and Clark (2008) claim to be corpora of questions, they are biased because the sentences come from QA queries. For example, such queries rarely include yes/no questions or tag questions. For o"
I11-1084,P81-1022,0,0.773475,"Missing"
I11-1084,W05-1102,0,0.0323997,"re some cases where we cannot achieve such high parsing accuracy as parsing 2 Related work Since domain adaptation is an extensive research area in parsing research (Nivre et al., 2007), many ideas have been proposed, including un- or 749 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 749–757, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP and imperatives. In the experiments, we also used QuestionBank for comparison. semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). The main focus of these works is on adapting parsing models trained with a specific genre of text (in most cases the Penn Treebank WSJ) to other genres of text, such as biomedical research papers and broadcast news. The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression. However, parsing imperatives and questions involves a significantly different problem; even when all words in a sentence are known, the sentence has a very different str"
I11-1084,W07-2202,1,0.899758,"adaptation is an extensive research area in parsing research (Nivre et al., 2007), many ideas have been proposed, including un- or 749 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 749–757, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP and imperatives. In the experiments, we also used QuestionBank for comparison. semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). The main focus of these works is on adapting parsing models trained with a specific genre of text (in most cases the Penn Treebank WSJ) to other genres of text, such as biomedical research papers and broadcast news. The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression. However, parsing imperatives and questions involves a significantly different problem; even when all words in a sentence are known, the sentence has a very different structure from declarative sentences. Compared to domain adaptation, structural types of sentences h"
I11-1084,D08-1050,0,0.0186762,"ptation, structural types of sentences have received little attention to date. A notable exception is the work on QuestionBank (Judge et al., 2006). This work highlighted the low accuracy of state-of-the-art parsers on questions, and proposed a supervised parser adaptation by manually creating a treebank of questions.1 The question sentences are annotated with phrase structure trees in the Penn Treebank scheme, although function tags and empty categories are omitted. QuestionBank was used for the supervised training of an LFG parser, resulting in a significant improvement in parsing accuracy. Rimell and Clark (2008) also worked on the problem of question parsing in the context of domain adaptation, and proposed a supervised method for the adaptation of the C&C parser (Clark and Curran, 2007). In this work, question sentences were collected from TREC 9-12 competitions and annotated with POS and CCG lexical categories. The authors reported a significant improvement in CCG parsing without phrase structure annotations. Our work further extends Judge et al. (2006) and Rimell and Clark (2008), while covering a wider range of sentence constructions. Although QuestionBank and the resource of Rimell and Clark (20"
I11-1084,N03-1027,0,0.0233071,"stems, and therefore the same parsing system can be applied to a novel domain. However, there are some cases where we cannot achieve such high parsing accuracy as parsing 2 Related work Since domain adaptation is an extensive research area in parsing research (Nivre et al., 2007), many ideas have been proposed, including un- or 749 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 749–757, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP and imperatives. In the experiments, we also used QuestionBank for comparison. semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). The main focus of these works is on adapting parsing models trained with a specific genre of text (in most cases the Penn Treebank WSJ) to other genres of text, such as biomedical research papers and broadcast news. The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression. However, parsing imperatives and questions involves a significantly differe"
I11-1084,W07-2416,0,0.0948282,"and Malt parsers The MST and Malt parsers are dependency parsers that produce non-projective dependency trees, using the spanning tree algorithm (McDonald et al., 2005a; McDonald et al., 2005b)2 and transitionbased algorithm (Nivre et al., 2006)3 , respectively. Although the publicly available implementation of each parser also has the option to restrict the output to a projective dependency tree, we used the non-projective versions because the dependency structures converted from the question sentences in the Brown Corpus included many non-projective dependencies. We used the pennconverter (Johansson and Nugues, 2007) 4 to convert a PTB-style treebank into dependency trees 5 . To evaluate the output from each of the parsers, we used the labeled attachment accuracy excluding punctuation. 3.2 HPSG parser The Enju parser (Ninomiya et al., 2007)6 is a deep parser based on the HPSG (Head Driven Phrase Structure Grammar) formalism. It produces an analysis of a sentence including the syntactic structure (i.e., parse tree) and the semantic structure represented as a set of predicateargument dependencies. We used the toolkit distributed with the Enju parser to train the parser with a PTB-style treebank. The toolkit"
I11-1084,P06-1063,0,0.244348,"Missing"
I11-1084,E03-1008,0,0.085386,"be applied to a novel domain. However, there are some cases where we cannot achieve such high parsing accuracy as parsing 2 Related work Since domain adaptation is an extensive research area in parsing research (Nivre et al., 2007), many ideas have been proposed, including un- or 749 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 749–757, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP and imperatives. In the experiments, we also used QuestionBank for comparison. semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). The main focus of these works is on adapting parsing models trained with a specific genre of text (in most cases the Penn Treebank WSJ) to other genres of text, such as biomedical research papers and broadcast news. The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression. However, parsing imperatives and questions involves a significantly different problem; even when all words in a sentence"
I11-1084,P06-1043,0,0.0275101,"omain. However, there are some cases where we cannot achieve such high parsing accuracy as parsing 2 Related work Since domain adaptation is an extensive research area in parsing research (Nivre et al., 2007), many ideas have been proposed, including un- or 749 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 749–757, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP and imperatives. In the experiments, we also used QuestionBank for comparison. semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). The main focus of these works is on adapting parsing models trained with a specific genre of text (in most cases the Penn Treebank WSJ) to other genres of text, such as biomedical research papers and broadcast news. The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression. However, parsing imperatives and questions involves a significantly different problem; even when all words in a sentence are known, the sentenc"
I11-1084,W06-2902,0,0.0245006,"2 Related work Since domain adaptation is an extensive research area in parsing research (Nivre et al., 2007), many ideas have been proposed, including un- or 749 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 749–757, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP and imperatives. In the experiments, we also used QuestionBank for comparison. semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). The main focus of these works is on adapting parsing models trained with a specific genre of text (in most cases the Penn Treebank WSJ) to other genres of text, such as biomedical research papers and broadcast news. The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression. However, parsing imperatives and questions involves a significantly different problem; even when all words in a sentence are known, the sentence has a very different structure from declarative sentences. Compared to domain adaptation, structural"
I11-1084,N10-1004,0,0.0265445,"not achieve such high parsing accuracy as parsing 2 Related work Since domain adaptation is an extensive research area in parsing research (Nivre et al., 2007), many ideas have been proposed, including un- or 749 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 749–757, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP and imperatives. In the experiments, we also used QuestionBank for comparison. semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). The main focus of these works is on adapting parsing models trained with a specific genre of text (in most cases the Penn Treebank WSJ) to other genres of text, such as biomedical research papers and broadcast news. The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression. However, parsing imperatives and questions involves a significantly different problem; even when all words in a sentence are known, the sentence has a very different structure from declarative"
I11-1084,P05-1012,0,0.0476,"come from QA queries. For example, such queries rarely include yes/no questions or tag questions. For our study, sentences were collected from the Brown Corpus, which includes a wider range of types of questions 3 Target Parsers and POS tagger We examined the performance of two dependency parsers and a deep parser on the target text sets. All parsers assumed that the input was already POS-tagged. We used the tagger in Tsuruoka et al. (2005). 3.1 MST and Malt parsers The MST and Malt parsers are dependency parsers that produce non-projective dependency trees, using the spanning tree algorithm (McDonald et al., 2005a; McDonald et al., 2005b)2 and transitionbased algorithm (Nivre et al., 2006)3 , respectively. Although the publicly available implementation of each parser also has the option to restrict the output to a projective dependency tree, we used the non-projective versions because the dependency structures converted from the question sentences in the Brown Corpus included many non-projective dependencies. We used the pennconverter (Johansson and Nugues, 2007) 4 to convert a PTB-style treebank into dependency trees 5 . To evaluate the output from each of the parsers, we used the labeled attachment"
I11-1084,H05-1066,0,0.0651665,"Missing"
I11-1084,P05-1011,1,0.816729,"iate. As a result, we extracted 750 imperative sentences and 1,241 question sentences from 24,243 sentences. Examples of extracted sentences are shown in Figure 1. Table 1 gives the statistics of the extracted sentences, which show that each genre contains top-level / embedded imperative and question sentences to some extent.7 As described below, we also used QuestionBank in the experiments. The advantage, however, of using the Brown treebank is that it includes annotations of function tags and empty categories, and therefore, we can apply the Penn Treebank-to-HPSG conversion program of Enju (Miyao and Tsujii, 2005), which relies on function tags and empty categories. Hence, we show experimental results for Enju only with the Brown data. It should also be noted that, a constituencyto-dependency converter, pennconverter (Johansson and Nugues, 2007), provides a more accurate conversion when function tags and empty categories are available (see footnote 4). Imperatives - Let &apos;s face it ! ! - Let this generation have theirs . - Believe me . - Make up your mind to pool your resources and get the most out of your remaining years of life . - Believe me ! ! - Find out what you like to do most and really give it"
I11-1084,nivre-etal-2006-maltparser,0,\N,Missing
I11-1084,D07-1096,0,\N,Missing
I11-1136,P04-1015,0,0.226468,".t ◦ s0 .t (b) s0 .w ◦ d s0 .t ◦ d s0 .w ◦ s0 .vl s1 .w ◦ s1 .vr s1 .w ◦ s1 .vl s0 .lc.w s0 .lc.t s1 .lc.w s1 .lc.t s1 .rc2 .w s1 .rc2 .t s0 .t ◦ s0 .lc.t ◦ s0 .lc2 .t s1 .t ◦ s1 .lc.t ◦ s1 .lc2 .t (c) j s2 .t s1 .w s1 .t s0 .w s0 .t (d) d s0 .vl s0 .lc.w s1 .rc.w s0 .lc2 .w s1 .rc2 .w s0 .lc2 .t s1 .rc2 .t To deal with conflicts between more than one of these actions, each action is associated with a score, and the score of a parser state is the total score of the actions that have been applied. To train the model, we adopt the averaged perceptron algorithm (Collins, 2002) with early update (Collins and Roark, 2004), following Huang and Sagae (2010). With the early update, whenever the gold action sequence falls off from the beam, the parameters are immediately updated with the rest of the sentence neglected. 2.2.2 Merging equivalent states Dynamic programming is enabled by merging equivalent states: if two states produce the same feature vector, they are merged into one state. Formally, a parser state (or configuration) ψ is described by h`, i, j, Si, where ` is the current step, [i . . . j] is the span of the top tree s0 in the stack S = (sd−1 , . . . , s0 ), where d is the depth of the stack. The equi"
I11-1136,W02-1001,0,0.181731,"plicable to any languages for which a projective shift-reduce parser works well. 2 Baseline Models First of all, we describe our baseline POS tagger and dependency parsers. These models will later be combined into pipelined models, which are then used as the baseline models in Section 4. 2.1 Baseline POS Tagger We build a baseline POS tagger, which uses the same POS-tagging features as those used in the state-of-the-art joint word segmentation and POS tagging model for Chinese (Zhang and Clark, 2008a). The list of features are shown in Table 1. We train the model with the averaged perceptron (Collins, 2002), and the decoding is performed using the Viterbi algorithm with beam search. Following Zhang and Clark (2008a), we use a tag dictionary and closed-set tags, which lead to improvement in both speed and accuracy. During training, the model stores all word–tag pairs into a tag dictionary, and for each word occurring more 2.2 Baseline Parsers For the baseline parsers for experiments, we build two dependency parsers: a reimplementation of the parser by Huang and Sagae (2010) (hereinafter Parser-HS), which is a shift-reduce dependency parser enhanced with dynamic programming (DP) using graph-struct"
I11-1136,D07-1098,0,0.0991258,"first-/secondwhere Φ order delayed features generated by action a be4 Experiment ing applied to ψ. When a S HIFT (t) action is performed, the model fills in the argument in the de4.1 Experimental Settings layed features with the newly-assigned tag t, as well as adding new delayed features it generates: We evaluate the performance of our joint parsers ~ 1 (ψ, SH (t)) + T (t, d~2 ), Φ ~ 2 (ψ, SH (t))i, and baseline models on the Chinese Penn Treehd~1 , d~2 i ← hΦ bank (CTB) 5 dataset. We use the standard split where T (t, d~2 ) is the resulting feature vector afof CTB-5 described in Duan et al. (2007) and the ter tag t is filled in to the first argument of the head-finding rules in Zhang and Clark (2008b). features in d~2 . Note that action SH (t) also adds We iteratively train each of the models and d~0 = T (t, d~1 ) to its (non-delayed) feature vector. choose the best model, in terms of the tagging acNote that the above formulation with the decuracy (for tagger) or word-level dependency aclayed features is equivalent to the model with full curacy (for parsers and joint parsers) on the devellook-ahead features if the exact decoding is peropment set, to use in the final evaluation. When fo"
I11-1136,P10-1110,0,0.220606,"ith underspecified POS tags of look-ahead words, we overcome this issue by introducing so-called delayed features. Our joint approach achieved substantial improvements over the pipeline and baseline systems in both POS tagging and dependency parsing task, achieving the new state-of-the-art performance on this joint task. 1    Introduction The tasks of part-of-speech (POS) tagging and dependency parsing have been widely investigated since the early stages of NLP research. Among mainstream approaches to dependency parsing, an incremental parsing framework is commonly used (e.g. Nivre (2008); Huang and Sagae (2010)), mainly because it achieves state-of-the-art accuracy while retaining linear-time computational complexity, and is also considered to reflect how humans process natural language sentences (Frazier and Rayner, 1982). However, although some of the Chinese POS tags require long-range syntactic information in order to be disambiguated, to the extent of our knowledge, none of the previous approaches have addressed the joint modeling of these two tasks in an incremental framework. Also, since POS tagging is a preliminary step for dependency parsing, the traditional pipeline approach may suffer fro"
I11-1136,P08-1102,0,0.0699744,"Missing"
I11-1136,C08-1049,0,0.0608786,"Missing"
I11-1136,P03-1054,0,0.00400021,"years, joint segmentation and tagging have been widely investigated (e.g. Zhang and Clark (2010); Kruengkrai et al. (2009); Zhang and Clark (2008a); Jiang et al. (2008a); Jiang et al. (2008b)). Particularly, our framework of using a single perceptron to solve the joint problem is motivated by Zhang and Clark (2008a). Also, our joint parsing framework is an extension of Huang and Sagae (2010)’s framework, which is described in detail in Section 2.2. In constituency parsing, the parsing naturally involves the POS tagging since the non-terminal symbols are commonly associated with POS tags (e.g. Klein and Manning (2003)). Rush et al. (2010) proposed to use dual composition to combine a constituency parser and a trigram POS tagger, showing the effectiveness of taking advantage of these two systems. In dependency parsing, Lee et al. (2011) recently proposed a discriminative graphical model that solves morphological disambiguation and dependency parsing jointly. However, their main focus was to capture interaction between morphology and syntax in morphologically-rich, highlyinflected languages (such as Latin and Ancient Greek), which are unlike Chinese. More recently, Li et al. (2011) proposed the first joint m"
I11-1136,P09-1058,0,0.14012,"Missing"
I11-1136,P11-1089,0,0.0529511,"g a single perceptron to solve the joint problem is motivated by Zhang and Clark (2008a). Also, our joint parsing framework is an extension of Huang and Sagae (2010)’s framework, which is described in detail in Section 2.2. In constituency parsing, the parsing naturally involves the POS tagging since the non-terminal symbols are commonly associated with POS tags (e.g. Klein and Manning (2003)). Rush et al. (2010) proposed to use dual composition to combine a constituency parser and a trigram POS tagger, showing the effectiveness of taking advantage of these two systems. In dependency parsing, Lee et al. (2011) recently proposed a discriminative graphical model that solves morphological disambiguation and dependency parsing jointly. However, their main focus was to capture interaction between morphology and syntax in morphologically-rich, highlyinflected languages (such as Latin and Ancient Greek), which are unlike Chinese. More recently, Li et al. (2011) proposed the first joint model for Chinese POS tagging and dependency parsing in a graph-based parsing framework, which is one of our baseline systems. On the other hand, our work is the first incremental approach to this joint task. 6 Conclusion I"
I11-1136,D11-1109,0,0.41298,"ith POS tags (e.g. Klein and Manning (2003)). Rush et al. (2010) proposed to use dual composition to combine a constituency parser and a trigram POS tagger, showing the effectiveness of taking advantage of these two systems. In dependency parsing, Lee et al. (2011) recently proposed a discriminative graphical model that solves morphological disambiguation and dependency parsing jointly. However, their main focus was to capture interaction between morphology and syntax in morphologically-rich, highlyinflected languages (such as Latin and Ancient Greek), which are unlike Chinese. More recently, Li et al. (2011) proposed the first joint model for Chinese POS tagging and dependency parsing in a graph-based parsing framework, which is one of our baseline systems. On the other hand, our work is the first incremental approach to this joint task. 6 Conclusion In this paper, we have presented the first joint approach that successfully solves POS tagging and dependency parsing on an incremental framework. The proposed joint models outperform the pipeline models in terms of both tagging and dependency parsing accuracies, and our best model achieved the new state-of-the-art performance on this joint task, whi"
I11-1136,J08-4003,0,0.136623,"difficulties with underspecified POS tags of look-ahead words, we overcome this issue by introducing so-called delayed features. Our joint approach achieved substantial improvements over the pipeline and baseline systems in both POS tagging and dependency parsing task, achieving the new state-of-the-art performance on this joint task. 1    Introduction The tasks of part-of-speech (POS) tagging and dependency parsing have been widely investigated since the early stages of NLP research. Among mainstream approaches to dependency parsing, an incremental parsing framework is commonly used (e.g. Nivre (2008); Huang and Sagae (2010)), mainly because it achieves state-of-the-art accuracy while retaining linear-time computational complexity, and is also considered to reflect how humans process natural language sentences (Frazier and Rayner, 1982). However, although some of the Chinese POS tags require long-range syntactic information in order to be disambiguated, to the extent of our knowledge, none of the previous approaches have addressed the joint modeling of these two tasks in an incremental framework. Also, since POS tagging is a preliminary step for dependency parsing, the traditional pipeline"
I11-1136,D10-1001,0,0.0463127,"nd tagging have been widely investigated (e.g. Zhang and Clark (2010); Kruengkrai et al. (2009); Zhang and Clark (2008a); Jiang et al. (2008a); Jiang et al. (2008b)). Particularly, our framework of using a single perceptron to solve the joint problem is motivated by Zhang and Clark (2008a). Also, our joint parsing framework is an extension of Huang and Sagae (2010)’s framework, which is described in detail in Section 2.2. In constituency parsing, the parsing naturally involves the POS tagging since the non-terminal symbols are commonly associated with POS tags (e.g. Klein and Manning (2003)). Rush et al. (2010) proposed to use dual composition to combine a constituency parser and a trigram POS tagger, showing the effectiveness of taking advantage of these two systems. In dependency parsing, Lee et al. (2011) recently proposed a discriminative graphical model that solves morphological disambiguation and dependency parsing jointly. However, their main focus was to capture interaction between morphology and syntax in morphologically-rich, highlyinflected languages (such as Latin and Ancient Greek), which are unlike Chinese. More recently, Li et al. (2011) proposed the first joint model for Chinese POS"
I11-1136,J95-2002,0,0.0248697,"ociated with dependency labels and head information of stack elements, are not included since our framework is based on unlabeled dependencies and the arc-standard strategy. The additional features for Parser-ZN− require the features in Table 2 (d) to be added into the set of kernel features. 2.2.4 Beam search with DP In the shift-reduce parsing with dynamic programming, we cannot simply apply beam search as in a non-DP shift-reduce parsing, because each state does not have a unique score any more. To decide the ordering of states within the beam, the concept of prefix score and inside score (Stolcke, 1995) is adopted. The prefix score ξ is the total score of the best action sequence from the initial state to the current state, while the inside score η 1218 (a) q0 .t q0 .w ◦ q0 .t s0 .t ◦ q0 .t ◦ q1 .t s0 .w ◦ q0 .t ◦ q1 .t (b) t ◦ s0 .w t ◦ s0 .w ◦ q0 .w t ◦ B(s0 .w) ◦ q0 .w t ◦ s0 .t ◦ s0 .rc.t t ◦ s0 .w ◦ s0 .t ◦ s0 .rc.t (c) j s2 .t q0 .w s1 .w s1 .t s0 .w s0 .t is the score of the tree on the top of the stack. With these scores and a set of predictor states Π(ψ) of state ψ, the full description of state ψ takes the form ψ : h`, i, j, S; ξ, η, Πi. The calculation of the prefix and inside sco"
I11-1136,P11-1139,0,0.048531,"Missing"
I11-1136,P08-1101,0,0.443622,"sion on the results and error analysis. Although we specifically focus on Chinese in this work, our joint model is applicable to any languages for which a projective shift-reduce parser works well. 2 Baseline Models First of all, we describe our baseline POS tagger and dependency parsers. These models will later be combined into pipelined models, which are then used as the baseline models in Section 4. 2.1 Baseline POS Tagger We build a baseline POS tagger, which uses the same POS-tagging features as those used in the state-of-the-art joint word segmentation and POS tagging model for Chinese (Zhang and Clark, 2008a). The list of features are shown in Table 1. We train the model with the averaged perceptron (Collins, 2002), and the decoding is performed using the Viterbi algorithm with beam search. Following Zhang and Clark (2008a), we use a tag dictionary and closed-set tags, which lead to improvement in both speed and accuracy. During training, the model stores all word–tag pairs into a tag dictionary, and for each word occurring more 2.2 Baseline Parsers For the baseline parsers for experiments, we build two dependency parsers: a reimplementation of the parser by Huang and Sagae (2010) (hereinafter P"
I11-1136,D08-1059,0,0.701122,"sion on the results and error analysis. Although we specifically focus on Chinese in this work, our joint model is applicable to any languages for which a projective shift-reduce parser works well. 2 Baseline Models First of all, we describe our baseline POS tagger and dependency parsers. These models will later be combined into pipelined models, which are then used as the baseline models in Section 4. 2.1 Baseline POS Tagger We build a baseline POS tagger, which uses the same POS-tagging features as those used in the state-of-the-art joint word segmentation and POS tagging model for Chinese (Zhang and Clark, 2008a). The list of features are shown in Table 1. We train the model with the averaged perceptron (Collins, 2002), and the decoding is performed using the Viterbi algorithm with beam search. Following Zhang and Clark (2008a), we use a tag dictionary and closed-set tags, which lead to improvement in both speed and accuracy. During training, the model stores all word–tag pairs into a tag dictionary, and for each word occurring more 2.2 Baseline Parsers For the baseline parsers for experiments, we build two dependency parsers: a reimplementation of the parser by Huang and Sagae (2010) (hereinafter P"
I11-1136,D10-1082,0,0.130141,"Missing"
I11-1136,P11-2033,0,0.280377,"se a tag dictionary and closed-set tags, which lead to improvement in both speed and accuracy. During training, the model stores all word–tag pairs into a tag dictionary, and for each word occurring more 2.2 Baseline Parsers For the baseline parsers for experiments, we build two dependency parsers: a reimplementation of the parser by Huang and Sagae (2010) (hereinafter Parser-HS), which is a shift-reduce dependency parser enhanced with dynamic programming (DP) using graph-structured stack (GSS; Tomita (1991)), and our extension of Parser-HS by incorporating a richer set of features taken from Zhang and Nivre (2011) (hereinafter Parser-ZN), which is originally a non-DP arc-eager dependency parser and achieves the current state-of-theart performance for Chinese dependency parsing. In this section, we briefly describe these models since the features and DP formalism serve as a basis for the joint models described in Section 3. 2.2.1 Shift-reduce parsing Shift-reduce dependency parsing algorithms incrementally process an input sentence from left to right. In the framework known as “arc-standard” (Nivre, 2008), the parser performs one of the following three actions at each step: • S HIFT (SH): move the first"
I11-1136,P08-1000,0,\N,Missing
I13-1090,P11-1121,0,0.0150227,"s and Colomb, 2010), although neither of them contains any misspellings or grammatical errors. (1a) The outsourcing of high-tech work to Asia by corporations means the loss of jobs for many middle-class American workers. 753 International Joint Conference on Natural Language Processing, pages 753–759, Nagoya, Japan, 14-18 October 2013. Preserved inarticulations (i.e., annotation scheme design, see Section 2). Our corpus construction had several substantial advantages in comparison to the existing corpora such as the NUCLE (Dahlmeier and Ng, 2011), NICT JLE (Izumi et al., 2004) and KJ corpora (Nagata et al., 2011). First, the proofreading process is separated from the annotation process. By doing this, both the writer and the proofreader were unaware of the construction of the corpus, so it could capture real articulations and corrections to these. Second, the alignment-based annotation scheme was employed in annotations to capture all types of articulation correction. This allowed us to annotate discontinuous paraphasing patterns, which were not neatly handled in other corpora. Third, paraphrases were captured, and were proved to be an important type of articulation correction for advanced learners. T"
I13-1090,J08-4005,0,0.1915,"2.1. The other subtypes are Duplicate, Spelling, Typo, and Unaligned, which will be explained in the following. • Duplicate: A duplicate alignment connects words that appear once in the original sentence, but more than once in the proofread sentence, or vice versa. This tag captures the correction for articulations like the word learning in the example in Figure 3(B). • Spelling: A spelling alignment is used for misspellings, e.g., occured→occurred1 . This also includes the use of hyphens, e.g., state of Annotation scheme design We extended the alignment-based paraphrase annotation scheme of Cohn et al. (2008) by categorizing the alignments into more fine-grained types (see Figure 2) to capture all types of inarticulation corrections. Figure 3 outlines example annotations to illustrate our annotation scheme. The alignments at the top level, are divided up into four broad types: Preserved, Metadata, Inarticulation Bi-alignment and Inarticulation Mono-alignment. The Preserved type of alignments is the most trivial type that connects words with the same surface and function, e.g., the, efficiency, various, methodologies in Figure 3(A). Still, there are many cases where two words have the same surface"
I13-1090,P11-1094,0,0.0140239,"was then proofread by English native experts (i.e., proofreading). After that, we preprocessed the documents to convert them into a predefined format (i.e., preprocessing, see Section 3). Annotators with linguistic backgrounds were asked to strictly follow our annotation scheme, which had been designed to capture all types of Introduction Detection and correction of misspellings and grammatical errors have been recognized as key techniques for writing assistance, and have extensively been studied in natural language processing (NLP) (Whitelaw et al., 2009; Gamon, 2010; Tetreault et al., 2010; Park and Levy, 2011). However, correcting misspellings and grammatical errors, which can be performed by normal English native speakers, does not satisfy all the requirements of professional writing (Futagi, 2010). The core of the proofreading process, in reality, is paraphrasing inarticulations, which can only be done by expert proofreaders. Considering the two paraphrased sentences (1a) and (1b) below, we can see that sentence (1b) is likely to be considered better by most people (Williams and Colomb, 2010), although neither of them contains any misspellings or grammatical errors. (1a) The outsourcing of high-t"
I13-1090,P11-1092,0,0.012545,"sentence (1b) is likely to be considered better by most people (Williams and Colomb, 2010), although neither of them contains any misspellings or grammatical errors. (1a) The outsourcing of high-tech work to Asia by corporations means the loss of jobs for many middle-class American workers. 753 International Joint Conference on Natural Language Processing, pages 753–759, Nagoya, Japan, 14-18 October 2013. Preserved inarticulations (i.e., annotation scheme design, see Section 2). Our corpus construction had several substantial advantages in comparison to the existing corpora such as the NUCLE (Dahlmeier and Ng, 2011), NICT JLE (Izumi et al., 2004) and KJ corpora (Nagata et al., 2011). First, the proofreading process is separated from the annotation process. By doing this, both the writer and the proofreader were unaware of the construction of the corpus, so it could capture real articulations and corrections to these. Second, the alignment-based annotation scheme was employed in annotations to capture all types of articulation correction. This allowed us to annotate discontinuous paraphasing patterns, which were not neatly handled in other corpora. Third, paraphrases were captured, and were proved to be a"
I13-1090,P10-2065,0,0.0222343,"ee Section 3), and this was then proofread by English native experts (i.e., proofreading). After that, we preprocessed the documents to convert them into a predefined format (i.e., preprocessing, see Section 3). Annotators with linguistic backgrounds were asked to strictly follow our annotation scheme, which had been designed to capture all types of Introduction Detection and correction of misspellings and grammatical errors have been recognized as key techniques for writing assistance, and have extensively been studied in natural language processing (NLP) (Whitelaw et al., 2009; Gamon, 2010; Tetreault et al., 2010; Park and Levy, 2011). However, correcting misspellings and grammatical errors, which can be performed by normal English native speakers, does not satisfy all the requirements of professional writing (Futagi, 2010). The core of the proofreading process, in reality, is paraphrasing inarticulations, which can only be done by expert proofreaders. Considering the two paraphrased sentences (1a) and (1b) below, we can see that sentence (1b) is likely to be considered better by most people (Williams and Colomb, 2010), although neither of them contains any misspellings or grammatical errors. (1a) The"
I13-1090,D09-1093,0,0.0123826,"collected (i.e., data collection, see Section 3), and this was then proofread by English native experts (i.e., proofreading). After that, we preprocessed the documents to convert them into a predefined format (i.e., preprocessing, see Section 3). Annotators with linguistic backgrounds were asked to strictly follow our annotation scheme, which had been designed to capture all types of Introduction Detection and correction of misspellings and grammatical errors have been recognized as key techniques for writing assistance, and have extensively been studied in natural language processing (NLP) (Whitelaw et al., 2009; Gamon, 2010; Tetreault et al., 2010; Park and Levy, 2011). However, correcting misspellings and grammatical errors, which can be performed by normal English native speakers, does not satisfy all the requirements of professional writing (Futagi, 2010). The core of the proofreading process, in reality, is paraphrasing inarticulations, which can only be done by expert proofreaders. Considering the two paraphrased sentences (1a) and (1b) below, we can see that sentence (1b) is likely to be considered better by most people (Williams and Colomb, 2010), although neither of them contains any misspel"
I13-1090,N10-1019,0,0.0134199,"collection, see Section 3), and this was then proofread by English native experts (i.e., proofreading). After that, we preprocessed the documents to convert them into a predefined format (i.e., preprocessing, see Section 3). Annotators with linguistic backgrounds were asked to strictly follow our annotation scheme, which had been designed to capture all types of Introduction Detection and correction of misspellings and grammatical errors have been recognized as key techniques for writing assistance, and have extensively been studied in natural language processing (NLP) (Whitelaw et al., 2009; Gamon, 2010; Tetreault et al., 2010; Park and Levy, 2011). However, correcting misspellings and grammatical errors, which can be performed by normal English native speakers, does not satisfy all the requirements of professional writing (Futagi, 2010). The core of the proofreading process, in reality, is paraphrasing inarticulations, which can only be done by expert proofreaders. Considering the two paraphrased sentences (1a) and (1b) below, we can see that sentence (1b) is likely to be considered better by most people (Williams and Colomb, 2010), although neither of them contains any misspellings or gram"
I13-1090,P08-4006,0,0.0769197,"Missing"
I13-1090,W10-0405,0,0.0455454,"Missing"
I13-1090,izumi-etal-2004-overview,0,\N,Missing
I13-1090,W11-2838,0,\N,Missing
I13-1090,W10-4236,0,\N,Missing
I13-1147,P05-1066,0,0.15332,"2 Related Work We propose a new rule-based pre-ordering method for Japanese-to-English statistical machine translation that employs heuristic rules in two-stages. This two-stage framework contributes to experimental results that our method outperforms conventional rule-based methods in BLEU and RIBES. 1 Introduction Reordering is an important strategy in statistical machine translation (SMT) to achieve high quality translation. While many reordering methods often fail in long distance reordering due to computational complexity, a promising technology called pre-ordering (Xia and McCord, 2004; Collins et al., 2005) has been successful for distant English-to-Japanese translation (Isozaki et al., 2010b). However, this strong effectiveness has not been shown for Japanese-to-English translation. In this paper, we propose a novel rule-based pre-ordering method for the Japanese-to-English translation. The method utilizes simple heuristic rules in two-stages1 : the inter-chunk and intrachunk levels. Thus the method can achieve more accurate reorderings in Japanese. The translation experiments in patent domain showed that our method outperformed conventional rule-based methods, especially on the word reordering"
I13-1147,W08-0509,0,0.0229865,"million sentence pairs for training, 500 sentence pairs for development, and 2,000 sentence pairs for testing. The Japanese sentences are tokenized by MeCab 0.99445 . In addition, we employed two parser configurations for Japanese parsing: (1) the KNP configuration used KNP 4.016 (Sasano and Kurohashi, 2011) for both dependency and PAS analyzer; (2) the CaboCha+SynCha configuration used CaboCha 0.657 (Kudo and Matsumoto, 2002) for dependency analysis and SynCha 0.38 (Iida and Poesio, 2011) for PAS analysis. For the common SMT system, we used SRILM 1.7.0 (Stolcke et al., 2011), MGIZA++ 0.7.3 (Gao and Vogel, 2008), Moses 0.91 (Koehn et al., 2007)9 , and two popular evaluation metrics for Japanese-to-English: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). For the pre-ordering methods, we implemented rule-based methods proposed by (Komachi et al., 2006) and (Katz-Brown and Collins, 2008). In addition, an implementation10 of statistical method proposed by Neubig et al. (2012) are used11 . We did not use any pre-ordering in the baseline. 4.2 Experimental Results Table 1 shows the experimental results for Japanese-to-English patent document translations that compare the following pre-orderi"
I13-1147,P11-1081,0,0.0681856,"used the NTCIR9 PatentMT Test Collection Japanese-to-English Machine Translation Data3 package that contains approximately 3.2 million sentence pairs for training, 500 sentence pairs for development, and 2,000 sentence pairs for testing. The Japanese sentences are tokenized by MeCab 0.99445 . In addition, we employed two parser configurations for Japanese parsing: (1) the KNP configuration used KNP 4.016 (Sasano and Kurohashi, 2011) for both dependency and PAS analyzer; (2) the CaboCha+SynCha configuration used CaboCha 0.657 (Kudo and Matsumoto, 2002) for dependency analysis and SynCha 0.38 (Iida and Poesio, 2011) for PAS analysis. For the common SMT system, we used SRILM 1.7.0 (Stolcke et al., 2011), MGIZA++ 0.7.3 (Gao and Vogel, 2008), Moses 0.91 (Koehn et al., 2007)9 , and two popular evaluation metrics for Japanese-to-English: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). For the pre-ordering methods, we implemented rule-based methods proposed by (Komachi et al., 2006) and (Katz-Brown and Collins, 2008). In addition, an implementation10 of statistical method proposed by Neubig et al. (2012) are used11 . We did not use any pre-ordering in the baseline. 4.2 Experimental Results Tabl"
I13-1147,D10-1092,1,0.935952,"Missing"
I13-1147,W10-1736,1,0.956068,"statistical machine translation that employs heuristic rules in two-stages. This two-stage framework contributes to experimental results that our method outperforms conventional rule-based methods in BLEU and RIBES. 1 Introduction Reordering is an important strategy in statistical machine translation (SMT) to achieve high quality translation. While many reordering methods often fail in long distance reordering due to computational complexity, a promising technology called pre-ordering (Xia and McCord, 2004; Collins et al., 2005) has been successful for distant English-to-Japanese translation (Isozaki et al., 2010b). However, this strong effectiveness has not been shown for Japanese-to-English translation. In this paper, we propose a novel rule-based pre-ordering method for the Japanese-to-English translation. The method utilizes simple heuristic rules in two-stages1 : the inter-chunk and intrachunk levels. Thus the method can achieve more accurate reorderings in Japanese. The translation experiments in patent domain showed that our method outperformed conventional rule-based methods, especially on the word reorderings. Our claims in this paper are summarized as follows: 1. The inter-chunk pre-ordering"
I13-1147,P03-1021,0,0.0888984,"en-PatentMT.html 4 http://mecab.googlecode.com/svn/ trunk/mecab/doc/index.html 5 Since (unlike English) the Japanese language does not utilize spaces to delineate word boundaries, MeCab was used to perform the required Japanese tokenization. 6 http://nlp.ist.i.kyoto-u.ac.jp/EN/ index.php?KNP 7 http://code.google.com/p/cabocha/ 8 http://www.cl.cs.titech.ac.jp/~ryu-i/ syncha/ Baseline Proposed BLEU 15.03 16.12 RIBES 62.71 69.30 Table 4: Results within a News Domain. 9 the following configurations are used in the system: 6gram for language modeling, msd-bidirectional-fe for reordering, and MERT (Och, 2003) for tuning. After reviewing our preliminary findings, distortion limits were set to 20 for the baseline and (Komachi et al., 2006), and 10 for others. 10 http://www.phontron.com/lader/ 11 Only 10,000 sampled lines were used for training due to its computational complexity: During the training process, it consumed 120 GB of memory space for almost entire month. 1064 improvement in RIBES by adding Rule 2 to Rule 1-2 and Rule 1-3, even thought this combination yields better translations for native speakers. This phenomenon can be explained by the characteristic difference between BLEU and RIBES."
I13-1147,P02-1040,0,0.094191,"e sentences are tokenized by MeCab 0.99445 . In addition, we employed two parser configurations for Japanese parsing: (1) the KNP configuration used KNP 4.016 (Sasano and Kurohashi, 2011) for both dependency and PAS analyzer; (2) the CaboCha+SynCha configuration used CaboCha 0.657 (Kudo and Matsumoto, 2002) for dependency analysis and SynCha 0.38 (Iida and Poesio, 2011) for PAS analysis. For the common SMT system, we used SRILM 1.7.0 (Stolcke et al., 2011), MGIZA++ 0.7.3 (Gao and Vogel, 2008), Moses 0.91 (Koehn et al., 2007)9 , and two popular evaluation metrics for Japanese-to-English: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). For the pre-ordering methods, we implemented rule-based methods proposed by (Komachi et al., 2006) and (Katz-Brown and Collins, 2008). In addition, an implementation10 of statistical method proposed by Neubig et al. (2012) are used11 . We did not use any pre-ordering in the baseline. 4.2 Experimental Results Table 1 shows the experimental results for Japanese-to-English patent document translations that compare the following pre-ordering methods: the baseline (no pre-ordering), Komachi et al. (2006), Katz-Brown and Collins (2008),Neubig et al. (2012), our pr"
I13-1147,I11-1085,0,0.0287568,"ents 4.1 Experimental Setup In order to compare pre-ordering methods, we conducted Japanese-to-English translation experiments on a fixed data set and SMT system. For the common data set, we used the NTCIR9 PatentMT Test Collection Japanese-to-English Machine Translation Data3 package that contains approximately 3.2 million sentence pairs for training, 500 sentence pairs for development, and 2,000 sentence pairs for testing. The Japanese sentences are tokenized by MeCab 0.99445 . In addition, we employed two parser configurations for Japanese parsing: (1) the KNP configuration used KNP 4.016 (Sasano and Kurohashi, 2011) for both dependency and PAS analyzer; (2) the CaboCha+SynCha configuration used CaboCha 0.657 (Kudo and Matsumoto, 2002) for dependency analysis and SynCha 0.38 (Iida and Poesio, 2011) for PAS analysis. For the common SMT system, we used SRILM 1.7.0 (Stolcke et al., 2011), MGIZA++ 0.7.3 (Gao and Vogel, 2008), Moses 0.91 (Koehn et al., 2007)9 , and two popular evaluation metrics for Japanese-to-English: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). For the pre-ordering methods, we implemented rule-based methods proposed by (Komachi et al., 2006) and (Katz-Brown and Collins, 2"
I13-1147,P03-1010,0,0.132275,"Missing"
I13-1147,C04-1073,0,0.317275,"ab.ntt.co.jp Abstract 2 Related Work We propose a new rule-based pre-ordering method for Japanese-to-English statistical machine translation that employs heuristic rules in two-stages. This two-stage framework contributes to experimental results that our method outperforms conventional rule-based methods in BLEU and RIBES. 1 Introduction Reordering is an important strategy in statistical machine translation (SMT) to achieve high quality translation. While many reordering methods often fail in long distance reordering due to computational complexity, a promising technology called pre-ordering (Xia and McCord, 2004; Collins et al., 2005) has been successful for distant English-to-Japanese translation (Isozaki et al., 2010b). However, this strong effectiveness has not been shown for Japanese-to-English translation. In this paper, we propose a novel rule-based pre-ordering method for the Japanese-to-English translation. The method utilizes simple heuristic rules in two-stages1 : the inter-chunk and intrachunk levels. Thus the method can achieve more accurate reorderings in Japanese. The translation experiments in patent domain showed that our method outperformed conventional rule-based methods, especially"
I13-1147,P07-2045,0,0.00751715,"ng, 500 sentence pairs for development, and 2,000 sentence pairs for testing. The Japanese sentences are tokenized by MeCab 0.99445 . In addition, we employed two parser configurations for Japanese parsing: (1) the KNP configuration used KNP 4.016 (Sasano and Kurohashi, 2011) for both dependency and PAS analyzer; (2) the CaboCha+SynCha configuration used CaboCha 0.657 (Kudo and Matsumoto, 2002) for dependency analysis and SynCha 0.38 (Iida and Poesio, 2011) for PAS analysis. For the common SMT system, we used SRILM 1.7.0 (Stolcke et al., 2011), MGIZA++ 0.7.3 (Gao and Vogel, 2008), Moses 0.91 (Koehn et al., 2007)9 , and two popular evaluation metrics for Japanese-to-English: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). For the pre-ordering methods, we implemented rule-based methods proposed by (Komachi et al., 2006) and (Katz-Brown and Collins, 2008). In addition, an implementation10 of statistical method proposed by Neubig et al. (2012) are used11 . We did not use any pre-ordering in the baseline. 4.2 Experimental Results Table 1 shows the experimental results for Japanese-to-English patent document translations that compare the following pre-ordering methods: the baseline (no pre-"
I13-1147,2006.iwslt-evaluation.11,1,0.39014,"ng that relies on PAS analysis contributes to improvements in translation quality. 2. The intra-chunk pre-ordering which converts postpositional phrases into prepositional phrases further improves translation quality. 3. Thus, our two-stage framework is more effective than other pre-ordering methods. Japanese-to-English is challenging because the grammatical forms of the two languages are totally dissimilar. For instance, English is a head-initial language, and utilizes subject-verb-object (SVO) word orders, while Japanese is a pure head-final language, and utilizes subject-object-verb (SOV). Komachi et al. (2006) proposed a rule-based pre-ordering method to convert SOV into SVO via a PAS analyzer. This method pre-orders interchunk level word orders in a single-stage, via the PAS analyzer which produced dependency trees and tagged each S, O, and V label. Then SOV sequences are converted into SVO. However, since the non-labeled words are left untouched, the effectiveness of this method is limited to simple SOV labeled matrix sentences without multiple clauses. Katz-Brown and Collins (2008) proposed a twostage rule-based pre-ordering method. In the first stage, SOV sequences are converted into SVO via th"
I13-1147,W02-2016,0,0.24869,"nts on a fixed data set and SMT system. For the common data set, we used the NTCIR9 PatentMT Test Collection Japanese-to-English Machine Translation Data3 package that contains approximately 3.2 million sentence pairs for training, 500 sentence pairs for development, and 2,000 sentence pairs for testing. The Japanese sentences are tokenized by MeCab 0.99445 . In addition, we employed two parser configurations for Japanese parsing: (1) the KNP configuration used KNP 4.016 (Sasano and Kurohashi, 2011) for both dependency and PAS analyzer; (2) the CaboCha+SynCha configuration used CaboCha 0.657 (Kudo and Matsumoto, 2002) for dependency analysis and SynCha 0.38 (Iida and Poesio, 2011) for PAS analysis. For the common SMT system, we used SRILM 1.7.0 (Stolcke et al., 2011), MGIZA++ 0.7.3 (Gao and Vogel, 2008), Moses 0.91 (Koehn et al., 2007)9 , and two popular evaluation metrics for Japanese-to-English: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). For the pre-ordering methods, we implemented rule-based methods proposed by (Komachi et al., 2006) and (Katz-Brown and Collins, 2008). In addition, an implementation10 of statistical method proposed by Neubig et al. (2012) are used11 . We did not use"
I13-1147,D12-1077,0,0.131501,"is method pre-orders interchunk level word orders in a single-stage, via the PAS analyzer which produced dependency trees and tagged each S, O, and V label. Then SOV sequences are converted into SVO. However, since the non-labeled words are left untouched, the effectiveness of this method is limited to simple SOV labeled matrix sentences without multiple clauses. Katz-Brown and Collins (2008) proposed a twostage rule-based pre-ordering method. In the first stage, SOV sequences are converted into SVO via the dependency analyzer. In the second stage, each chunk word order is naively reversed2 . Neubig et al. (2012) proposed a statistical model that was capable of learning how to pre-order word sequences from human annotated or automatically generated alignment data. However, this method has very large computational complexity to model long distance reordering. 3 Two-stage Pre-ordering Method Here, we describe a new pre-ordering method which employs heuristic rules in two-stages. In the first stage, we reorganize and extend the rules described in (Komachi et al., 2006; Katz-Brown and Collins, 2008). In the second stage, we propose a new rule to consider chunk internal word orders. More precisely, we appl"
I13-1192,N06-2015,0,0.0511828,"Missing"
I13-1192,P10-2013,0,0.0226515,"lly developed for assessing human English ability, rather than specifically developed for NLP system evaluation. Therefore, it is expected that various aspects of human natural language understanding appear in solving such questions. 6 Related Work Recent advancement of empirical NLP owes much to language resources, such as annotated corpora and lexicons. Language resources to date have been developed specifically for focused NLP tasks, such as syntactic/semantic parsing, coreference resolution, and word sense disambiguation (Marcus et al., 1993; Kingsbury and Palmer, 2002; Hovy et al., 2006; Ide et al., 2010; Tateisi et al., 2005; Kawahara et al., 2002; Iida et al., 2007). Another type of corpora has been developed for evaluating NLP applications, such as machine translation and question answering, which are often provided in application-oriented evaluation campaigns (Voorhees and Buckland, 2012; Kando 6 et al., 2011; Catarci et al., 2012). In other words, the development of language resources is initiated by the demand for NLP tasks/applications. However, the resources presented in this paper are motivated in an opposite way. We start from texts that involve problem solving by humans, i.e., univ"
I13-1192,W07-1522,0,0.0163168,"specifically developed for NLP system evaluation. Therefore, it is expected that various aspects of human natural language understanding appear in solving such questions. 6 Related Work Recent advancement of empirical NLP owes much to language resources, such as annotated corpora and lexicons. Language resources to date have been developed specifically for focused NLP tasks, such as syntactic/semantic parsing, coreference resolution, and word sense disambiguation (Marcus et al., 1993; Kingsbury and Palmer, 2002; Hovy et al., 2006; Ide et al., 2010; Tateisi et al., 2005; Kawahara et al., 2002; Iida et al., 2007). Another type of corpora has been developed for evaluating NLP applications, such as machine translation and question answering, which are often provided in application-oriented evaluation campaigns (Voorhees and Buckland, 2012; Kando 6 et al., 2011; Catarci et al., 2012). In other words, the development of language resources is initiated by the demand for NLP tasks/applications. However, the resources presented in this paper are motivated in an opposite way. We start from texts that involve problem solving by humans, i.e., university entrance examinations, and by analyzing them we can identi"
I13-1192,W12-3212,0,0.148057,"Missing"
I13-1192,I05-2038,0,0.0288364,"Missing"
I13-1192,C12-1084,1,0.31893,"Missing"
I13-1192,kawahara-etal-2002-construction,0,0.0503858,"h ability, rather than specifically developed for NLP system evaluation. Therefore, it is expected that various aspects of human natural language understanding appear in solving such questions. 6 Related Work Recent advancement of empirical NLP owes much to language resources, such as annotated corpora and lexicons. Language resources to date have been developed specifically for focused NLP tasks, such as syntactic/semantic parsing, coreference resolution, and word sense disambiguation (Marcus et al., 1993; Kingsbury and Palmer, 2002; Hovy et al., 2006; Ide et al., 2010; Tateisi et al., 2005; Kawahara et al., 2002; Iida et al., 2007). Another type of corpora has been developed for evaluating NLP applications, such as machine translation and question answering, which are often provided in application-oriented evaluation campaigns (Voorhees and Buckland, 2012; Kando 6 et al., 2011; Catarci et al., 2012). In other words, the development of language resources is initiated by the demand for NLP tasks/applications. However, the resources presented in this paper are motivated in an opposite way. We start from texts that involve problem solving by humans, i.e., university entrance examinations, and by analyzin"
I13-1192,kingsbury-palmer-2002-treebank,0,0.0686929,"f this pilot task is that questions are originally developed for assessing human English ability, rather than specifically developed for NLP system evaluation. Therefore, it is expected that various aspects of human natural language understanding appear in solving such questions. 6 Related Work Recent advancement of empirical NLP owes much to language resources, such as annotated corpora and lexicons. Language resources to date have been developed specifically for focused NLP tasks, such as syntactic/semantic parsing, coreference resolution, and word sense disambiguation (Marcus et al., 1993; Kingsbury and Palmer, 2002; Hovy et al., 2006; Ide et al., 2010; Tateisi et al., 2005; Kawahara et al., 2002; Iida et al., 2007). Another type of corpora has been developed for evaluating NLP applications, such as machine translation and question answering, which are often provided in application-oriented evaluation campaigns (Voorhees and Buckland, 2012; Kando 6 et al., 2011; Catarci et al., 2012). In other words, the development of language resources is initiated by the demand for NLP tasks/applications. However, the resources presented in this paper are motivated in an opposite way. We start from texts that involve"
I13-1192,J93-2004,0,0.0477862,"examinations. • Classifies questions from the NLP point of view, and discusses research issues involved. • Introduces present use cases of our resources to show their effectiveness. The resources introduced in this paper are made available for research purposes. As we will see below, this resource involves a variety of research issues in NLP and related AI technologies, and thus collaborative research based on such open resources is indispensable. 2 Motivation Current NLP corpora can be classified into two types. One is to focus on specific fundamental NLP technologies, such as Penn Treebank (Marcus et al., 1993) developed for parsing research. The other is application-oriented data sets, meaning that corpora are used for evaluating specific NLP applications, such as machine translation and question answering (Voorhees and Buckland, 2012; Kando et al., 2011). However, despite significant advancement achieved by these resources, it is still unclear how far current NLP technologies have approached human intelligence, in particular, about the ability of generic problem solving. In the current NLP, research topics are inherently determined when corpora are developed, and there is no room for investigating"
J08-1002,J97-4005,0,0.00941329,"ammars (Johnson et al. 1999; Riezler et al. 2000, 2002; Geman and Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen 2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models. This is because these grammar formalisms exploit feature structures to represent linguistic constraints. Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997). The maximum entropy model is a reasonable choice for credible probabilistic models. It also allows various overlapping features to be incorporated, and we can expect higher accuracy in disambiguation. A maximum entropy model gives a probabilistic distribution that maximizes the likelihood of training data under given feature functions. Given training data E = {x, y}, a maximum entropy model gives conditional probability p(y|x) as follows. Deﬁnition 1 (Maximum entropy model) A maximum entropy model is deﬁned as the solution of the following optimization problem. pM (y|x) = argmax p    −"
J08-1002,W03-0403,0,0.217533,", statistical modeling of these grammars is attracting considerable attention. This is because natural language processing applications usually require disambiguated or ranked parse results, and statistical modeling of syntactic/semantic preference is one of the most promising methods for disambiguation. The focus of this article is the problem of probabilistic modeling of wide-coverage HPSG parsing. Although previous studies have proposed maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) of HPSG-style parse trees (Oepen, Toutanova, et al. 2002b; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004), the straightforward application of maximum entropy models to wide-coverage HPSG parsing is infeasible because estimation of maximum entropy models is computationally expensive, especially when targeting wide-coverage parsing. In general, complete structures, such as transition sequences in Markov models and parse trees, have an exponential number of ambiguities. This causes an exponential explosion when estimating the parameters of maximum entropy models. We therefore require solutions to make model estimation tractable. This article ﬁrst proposes feature forest m"
J08-1002,J96-1002,0,0.0442627,"Missing"
J08-1002,I05-1015,0,0.0106983,"ure forest models to probabilistic HPSG parsing. Section 5 presents an empirical evaluation of probabilistic HPSG parsing, and Section 6 introduces research related to our proposals. Section 7 concludes. 2. Problem Maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now becoming the de facto standard approach for disambiguation models for lexicalized or 36 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing feature structure grammars (Johnson et al. 1999; Riezler et al. 2000, 2002; Geman and Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen 2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models. This is because these grammar formalisms exploit feature structures to represent linguistic constraints. Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997). The maximum entropy model is a reasonable choice for credible probabilistic models. It also allows various overlapping features to be incor"
J08-1002,A00-2018,0,0.0198204,"reebank derived from Sections 02–21 of the Wall Street Journal portion of the Penn Treebank, that is, the same set used for lexicon extraction. For training of the disambiguation models, we eliminated sentences of 40 words or more and sentences for which the parser could not produce the correct parses. The resulting training set consists of 33,604 sentences (when n = 10 and = 0.95; see Section 5.4 for details). The treebanks derived from Sections 22 and 23 were used as the development and ﬁnal test sets, respectively. Following previous studies on parsing with PCFG-based models (Collins 1997; Charniak 2000), accuracy is measured for sentences of less than 40 words and for those with less than 100 words. Table 5 shows the speciﬁcations of the test data. The measure for evaluating parsing accuracy is precision/recall of predicate– argument dependencies output by the parser. A predicate–argument dependency is deﬁned as a tuple wh , wn , π, ρ, where wh is the head word of the predicate, wn is the head word of the argument, π is the type of the predicate (e.g., adjective, intransitive verb), and ρ is an argument label (MODARG, ARG1, . . ., ARG4). For example, He tried running has three dependencies"
J08-1002,P05-1022,0,0.0543457,"hether dynamic programming or sampling can deliver a better balance of estimation efﬁciency and accuracy. The answer will differ in different problems. When most effective features can be represented locally in tractablesize feature forests, dynamic programming methods including ours are suitable. However, when global context features are indispensable for high accuracy, sampling methods might be better. We should also investigate compromise solutions such as dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) and reranking techniques (Collins 2000; Charniak and Johnson 2005). There is no analytical way of predicting the best solution, and it must be investigated experimentally for each target task. 7. Conclusion A dynamic programming algorithm was presented for maximum entropy modeling and shown to provide a solution to the parameter estimation of probabilistic models of complete structures without the independence assumption. We ﬁrst deﬁned the notion of a feature forest, which is a packed representation of an exponential number of trees of features. When training data is represented with feature forests, model parameters are estimated at a tractable cost withou"
J08-1002,W03-1013,0,0.191187,"o, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing. The methods we propose here were applied to an English HPSG parser, Enju (Tsujii Laboratory 2004). We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994). The content of this article is an extended version of our earlier work reported in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The major contribution of this article is a strict mathematical deﬁnition of the feature forest model and the parameter estimation algorithm, which are substantially reﬁned and extended from Miyao and Tsujii (2002). Another contribution is that this article thoroughly discusses the relationships between the feature forest model and its application to HPSG parsing. We also provide an extensive empirical evaluation of the resulting HPSG parsing approach using real-world text. Section 2 discusses a problem of conventional probabilistic models for lexicaliz"
J08-1002,C04-1041,0,0.034581,"ning, transitive verb, ARG2 running, he, intransitive verb, ARG1 Labeled precision/recall (LP/LR) is the ratio of tuples correctly identiﬁed by the parser, and unlabeled precision/recall (UP/UR) is the ratio of wh and wn correctly identiﬁed regardless of π and ρ. F-score is the harmonic mean of LP and LR. Sentence accuracy is the exact match accuracy of complete predicate–argument relations in a sentence. These measures correspond to those used in other studies measuring the accuracy of predicate–argument dependencies in CCG parsing (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003; Clark and Curran 2004b) and LFG parsing (Burke et al. 2004), although exact ﬁgures cannot be compared directly because the deﬁnitions of dependencies are different. All predicate–argument dependencies in a sentence are the target of evaluation except quotation marks and periods. The accuracy is measured by parsing test sentences with gold-standard part-of-speech tags from the Penn Treebank unless otherwise noted. The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its hyper-parameter was tuned for each model to maximize F-score for the development set. The algorithm for parameter estimation w"
J08-1002,P04-1014,0,0.029243,"Missing"
J08-1002,P02-1042,0,0.0160518,"Missing"
J08-1002,P97-1003,0,0.0325988,"was the HPSG treebank derived from Sections 02–21 of the Wall Street Journal portion of the Penn Treebank, that is, the same set used for lexicon extraction. For training of the disambiguation models, we eliminated sentences of 40 words or more and sentences for which the parser could not produce the correct parses. The resulting training set consists of 33,604 sentences (when n = 10 and = 0.95; see Section 5.4 for details). The treebanks derived from Sections 22 and 23 were used as the development and ﬁnal test sets, respectively. Following previous studies on parsing with PCFG-based models (Collins 1997; Charniak 2000), accuracy is measured for sentences of less than 40 words and for those with less than 100 words. Table 5 shows the speciﬁcations of the test data. The measure for evaluating parsing accuracy is precision/recall of predicate– argument dependencies output by the parser. A predicate–argument dependency is deﬁned as a tuple wh , wn , π, ρ, where wh is the head word of the predicate, wn is the head word of the argument, π is the type of the predicate (e.g., adjective, intransitive verb), and ρ is an argument label (MODARG, ARG1, . . ., ARG4). For example, He tried running has th"
J08-1002,J03-4003,0,0.0849956,"Missing"
J08-1002,1995.tmi-1.2,0,0.0621836,"or the wh-extraction of the object of love (left) and for the control construction of try (right). The ﬁrst condition is satisﬁed because both lexical entries refer to CONT|HOOK of argument signs in SUBJ, COMPS, and SLASH. None of the lexical entries directly access ARGX of the arguments. The second condition is also satisﬁed because the values of CONT|HOOK of all of the argument signs are percolated to ARGX of the mother. In addition, the elements in CONT|RELS are percolated to the mother by the Semantic Principle. Compositional semantics usually satisﬁes the above conditions, including MRS (Copestake et al. 1995, 2006). The composition of MRS refers to HOOK, and no internal structures of daughters. The Semantic Principle of MRS also assures that all semantic relations in RELS are percolated to the mother. When these conditions are satisﬁed, semantics may include any constraints, such as selectional restrictions, although the grammar we used in the experiments does not include semantic restrictions to constrain parse forests. Under these conditions, local structures of predicate–argument structures are encoded into a conjunctive node when the values of all of its arguments have been instantiated. We i"
J08-1002,P02-1036,0,0.40534,"o, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing. The methods we propose here were applied to an English HPSG parser, Enju (Tsujii Laboratory 2004). We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994). The content of this article is an extended version of our earlier work reported in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The major contribution of this article is a strict mathematical deﬁnition of the feature forest model and the parameter estimation algorithm, which are substantially reﬁned and extended from Miyao and Tsujii (2002). Another contribution is that this article thoroughly discusses the relationships between the feature forest model and its application to HPSG parsing. We also provide an extensive empirical evaluation of the resulting HPSG parsing approach using real-world text. Section 2 discusses a problem of conventional probabilistic models for lexicaliz"
J08-1002,W01-0521,0,0.0472805,"Missing"
J08-1002,P03-1046,0,0.0161053,"arsing tried, running, transitive verb, ARG2 running, he, intransitive verb, ARG1 Labeled precision/recall (LP/LR) is the ratio of tuples correctly identiﬁed by the parser, and unlabeled precision/recall (UP/UR) is the ratio of wh and wn correctly identiﬁed regardless of π and ρ. F-score is the harmonic mean of LP and LR. Sentence accuracy is the exact match accuracy of complete predicate–argument relations in a sentence. These measures correspond to those used in other studies measuring the accuracy of predicate–argument dependencies in CCG parsing (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003; Clark and Curran 2004b) and LFG parsing (Burke et al. 2004), although exact ﬁgures cannot be compared directly because the deﬁnitions of dependencies are different. All predicate–argument dependencies in a sentence are the target of evaluation except quotation marks and periods. The accuracy is measured by parsing test sentences with gold-standard part-of-speech tags from the Penn Treebank unless otherwise noted. The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its hyper-parameter was tuned for each model to maximize F-score for the development set. The algorithm for"
J08-1002,hockenmaier-steedman-2002-acquiring,0,0.00870125,"s is possible when they are represented by feature forests. This article also describes methods for representing HPSG syntactic structures and predicate–argument structures with feature forests. Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing. The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed. 1. Introduction Following the successful development of wide-coverage lexicalized grammars (Riezler et al. 2000; Hockenmaier and Steedman 2002; Burke et al. 2004; Miyao, Ninomiya, and ∗ Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan. E-mail: yusuke@is.s.u-tokyo.ac.jp. ∗∗ Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan. E-mail: tsujii@is.s.u-tokyo.ac.jp. Submission received: 11 June 2006; revised submission received: 2 March 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 1 Tsujii 2005), statistical modeling of these grammars is attracting conside"
J08-1002,P99-1069,0,0.0207855,"d grammars. Section 3 proposes feature forest models for solving this problem. Section 4 describes the application of feature forest models to probabilistic HPSG parsing. Section 5 presents an empirical evaluation of probabilistic HPSG parsing, and Section 6 introduces research related to our proposals. Section 7 concludes. 2. Problem Maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now becoming the de facto standard approach for disambiguation models for lexicalized or 36 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing feature structure grammars (Johnson et al. 1999; Riezler et al. 2000, 2002; Geman and Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen 2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models. This is because these grammar formalisms exploit feature structures to represent linguistic constraints. Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997). The maximum e"
J08-1002,A00-2021,0,0.0119105,"experimentally in Section 5.4. We have several ways to integrate p¯ with the estimated model p(t|T(w)). In the experiments, we will empirically compare the following methods in terms of accuracy and estimation time. Filtering only: The unigram probability p¯ is used only for ﬁltering in training. Product: The probability is deﬁned as the product of p¯ and the estimated model p. Reference distribution: p¯ is used as a reference distribution of p. Feature function: log p¯ is used as a feature function of p. This method has been shown to be a generalization of the reference distribution method (Johnson and Riezler 2000). 4.5 Features Feature functions in maximum entropy models are designed to capture the characteristics of em , el , er . In this article, we investigate combinations of the atomic features listed Figure 19 Filtering of lexical entries for saw. 57 Computational Linguistics Volume 34, Number 1 Table 1 Templates for atomic features. RULE DIST COMMA SPAN SYM WORD POS LE ARG name of the applied schema distance between the head words of the daughters whether a comma exists between daughters and/or inside of daughter phrases number of words dominated by the phrase symbol of the phrasal category (e."
J08-1002,N04-1013,0,0.0192178,"Missing"
J08-1002,W02-2018,0,0.0626834,"local ambiguities in parse trees potentially cause exponential growth in the number of structures assigned to sub-sequences of words, resulting in billions of structures for whole sentences. For example, when we apply rewriting rule S → NP VP, and the left NP and the right VP, respectively, have n and m ambiguous subtrees, the result of the rule application generates n × m trees. This is problematic because the complexity of parameter estimation is proportional to the size of Y(x). The cost of the parameter estimation algorithms is bound by the computation of model expectation, µi , given as (Malouf 2002):   ˜ p(x) µi = x∈X y∈Y(x)   ˜ p(x) = x∈X y∈Y(x) fi (x, y)p(y|x)    fi (x, y) 1 exp  λj fj (x, y) Z(x) (1) j As shown in this deﬁnition, the computation of model expectation requires the summation over Y(x) for every x in the training data. The complexity of the overall estimation algorithm is O(|Y˜ ||F˜ ||E |), where |Y˜ |and |F˜ |are the average numbers of y and activated features for an event, respectively, and |E |is the number of events. When Y(x) grows exponentially, the parameter estimation becomes intractable. In PCFGs, the problem of computing probabilities of parse trees is"
J08-1002,H94-1020,0,0.0157438,"sing. We describe methods for representing HPSG parse trees and predicate–argument structures using feature forests (Miyao, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing. The methods we propose here were applied to an English HPSG parser, Enju (Tsujii Laboratory 2004). We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994). The content of this article is an extended version of our earlier work reported in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The major contribution of this article is a strict mathematical deﬁnition of the feature forest model and the parameter estimation algorithm, which are substantially reﬁned and extended from Miyao and Tsujii (2002). Another contribution is that this article thoroughly discusses the relationships between the feature forest model and its application to HPSG parsing. We also provide an extensive empirical evaluation of the resulting HPSG"
J08-1002,W03-0430,0,0.0168907,"ecause feature forests can represent Markov chains. In an analogy, CRFs correspond to HMMs, whereas feature forest models correspond to PCFGs. 71 Computational Linguistics Volume 34, Number 1 Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also regarded as instances of feature forest models. This fact implies that our algorithm is applicable to not only parsing but also to other tasks. CRFs are now widely used for sequence-based tasks, such as parts-of-speech tagging and named entity recognition, and have been shown to achieve the best performance in various tasks (McCallum and Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al. 2003; Sha and Pereira 2003; Peng and McCallum 2004; Roark et al. 2004; Settles 2004; Sutton, Rohanimanesh, and McCallum 2004). These results suggest that the method proposed in the present article will achieve high accuracy when applied to various statistical models with tree structures. Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for extending feature forest models. The purpose of dynamic CRFs is to incorporate feature functions that are not re"
J08-1002,P06-1128,1,0.50409,"Missing"
J08-1002,W03-0401,1,0.836753,"Missing"
J08-1002,P05-1011,1,0.550494,"o, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing. The methods we propose here were applied to an English HPSG parser, Enju (Tsujii Laboratory 2004). We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994). The content of this article is an extended version of our earlier work reported in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The major contribution of this article is a strict mathematical deﬁnition of the feature forest model and the parameter estimation algorithm, which are substantially reﬁned and extended from Miyao and Tsujii (2002). Another contribution is that this article thoroughly discusses the relationships between the feature forest model and its application to HPSG parsing. We also provide an extensive empirical evaluation of the resulting HPSG parsing approach using real-world text. Section 2 discusses a problem of conventional probabilistic models for lexicaliz"
J08-1002,W05-1510,1,0.463395,"Missing"
J08-1002,W05-1511,1,0.72648,"test sentences with gold-standard part-of-speech tags from the Penn Treebank unless otherwise noted. The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its hyper-parameter was tuned for each model to maximize F-score for the development set. The algorithm for parameter estimation was the limited-memory BFGS method (Nocedal 1980; Nocedal and Wright 1999). The parser was implemented in C++ with the LiLFeS library (Makino et al. 2002), and various speed-up techniques for HPSG parsing were used such as quick check and iterative beam search (Tsuruoka, Miyao, and Tsujii 2004; Ninomiya et al. 2005). Other efﬁcient parsing techniques, including global thresholding, hybrid parsing with a chunk parser, and large constituent inhibition, were not used. The results obtained using these techniques are given in Ninomiya et al. A limit on the number of constituents was set for time-out; the parser stopped parsing when the number of constituents created during parsing exceeded 50,000. In such a case, the parser output nothing, and the recall was computed as zero. Features occurring more than twice were included in the probabilistic models. A method of ﬁltering lexical entries was applied to the p"
J08-1002,A00-2022,0,0.0617176,"Missing"
J08-1002,C02-2025,0,0.0131052,"Missing"
J08-1002,C00-1085,0,0.16404,"le approach to avoid this problem is to develop a fully restrictive grammar that never causes an exponential explosion, although the development of such a grammar requires considerable effort and it cannot be acquired from treebanks using existing approaches. We think that exponential explosion is inevitable, particularly with the large-scale wide-coverage grammars required to analyze real-world texts. In such cases, these methods of model estimation are intractable. Another approach to estimating log-linear models for HPSG was to extract a small informative sample from the original set T(w) (Osborne 2000). The method was successfully applied to Dutch HPSG parsing (Malouf and van Noord 2004). A possible problem with this method is in the approximation of exponentially many parse trees by a polynomial-size sample. However, their method has an advantage in that any features on parse results can be incorporated into a model, whereas our method forces feature functions to be deﬁned locally on conjunctive nodes. We will discuss the trade-off between the approximation solution and the locality of feature functions in Section 6.3. Non-probabilistic statistical classiﬁers have also been applied to disa"
J08-1002,N04-1042,0,0.00894058,"ure forest models correspond to PCFGs. 71 Computational Linguistics Volume 34, Number 1 Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also regarded as instances of feature forest models. This fact implies that our algorithm is applicable to not only parsing but also to other tasks. CRFs are now widely used for sequence-based tasks, such as parts-of-speech tagging and named entity recognition, and have been shown to achieve the best performance in various tasks (McCallum and Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al. 2003; Sha and Pereira 2003; Peng and McCallum 2004; Roark et al. 2004; Settles 2004; Sutton, Rohanimanesh, and McCallum 2004). These results suggest that the method proposed in the present article will achieve high accuracy when applied to various statistical models with tree structures. Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for extending feature forest models. The purpose of dynamic CRFs is to incorporate feature functions that are not represented locally, and the solution is to apply a variational method, which is an algorithm of numerical c"
J08-1002,P02-1035,0,0.0100621,"Missing"
J08-1002,P00-1061,0,0.176824,"of any data structures is possible when they are represented by feature forests. This article also describes methods for representing HPSG syntactic structures and predicate–argument structures with feature forests. Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing. The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed. 1. Introduction Following the successful development of wide-coverage lexicalized grammars (Riezler et al. 2000; Hockenmaier and Steedman 2002; Burke et al. 2004; Miyao, Ninomiya, and ∗ Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan. E-mail: yusuke@is.s.u-tokyo.ac.jp. ∗∗ Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan. E-mail: tsujii@is.s.u-tokyo.ac.jp. Submission received: 11 June 2006; revised submission received: 2 March 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 1 Tsujii 2005), statistical modeling of these"
J08-1002,W04-3223,0,0.0738302,"aused by errors of argument/modiﬁer distinction in to-inﬁnitive clauses. A signiﬁcant portion of the errors discussed above cannot be resolved by the features we investigated in this study, and the design of other features will be necessary for improving parsing accuracy. 6. Discussion 6.1 Probabilistic Modeling of Complete Structures The model described in this article was ﬁrst published in Miyao and Tsujii (2002), and has been applied to probabilistic models for parsing with lexicalized grammars. Applications to CCG parsing (Clark and Curran 2003, 2004b) and LFG parsing (Kaplan et al. 2004; Riezler and Vasserman 2004) demonstrated that feature forest models attained higher accuracy than other models. These researchers applied feature forests to representations of the packed parse results of LFG and the dependency/derivation structures of CCG. Their work demonstrated the applicability and effectiveness of feature forest models in parsing with wide-coverage lexicalized grammars. Feature forest models were also shown to be effective for wide-coverage sentence realization (Nakanishi, Miyao, and Tsujii 2005). This work demonstrated that feature forest models are generic enough to be applied to natural language"
J08-1002,P04-1007,0,0.0238318,"pond to PCFGs. 71 Computational Linguistics Volume 34, Number 1 Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also regarded as instances of feature forest models. This fact implies that our algorithm is applicable to not only parsing but also to other tasks. CRFs are now widely used for sequence-based tasks, such as parts-of-speech tagging and named entity recognition, and have been shown to achieve the best performance in various tasks (McCallum and Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al. 2003; Sha and Pereira 2003; Peng and McCallum 2004; Roark et al. 2004; Settles 2004; Sutton, Rohanimanesh, and McCallum 2004). These results suggest that the method proposed in the present article will achieve high accuracy when applied to various statistical models with tree structures. Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for extending feature forest models. The purpose of dynamic CRFs is to incorporate feature functions that are not represented locally, and the solution is to apply a variational method, which is an algorithm of numerical computation, to obta"
J08-1002,H92-1019,0,0.506127,"Missing"
J08-1002,W04-1221,0,0.00766165,"omputational Linguistics Volume 34, Number 1 Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also regarded as instances of feature forest models. This fact implies that our algorithm is applicable to not only parsing but also to other tasks. CRFs are now widely used for sequence-based tasks, such as parts-of-speech tagging and named entity recognition, and have been shown to achieve the best performance in various tasks (McCallum and Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al. 2003; Sha and Pereira 2003; Peng and McCallum 2004; Roark et al. 2004; Settles 2004; Sutton, Rohanimanesh, and McCallum 2004). These results suggest that the method proposed in the present article will achieve high accuracy when applied to various statistical models with tree structures. Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for extending feature forest models. The purpose of dynamic CRFs is to incorporate feature functions that are not represented locally, and the solution is to apply a variational method, which is an algorithm of numerical computation, to obtain approximate"
J08-1002,N03-1028,0,0.592524,"o, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing. The methods we propose here were applied to an English HPSG parser, Enju (Tsujii Laboratory 2004). We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994). The content of this article is an extended version of our earlier work reported in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The major contribution of this article is a strict mathematical deﬁnition of the feature forest model and the parameter estimation algorithm, which are substantially reﬁned and extended from Miyao and Tsujii (2002). Another contribution is that this article thoroughly discusses the relationships between the feature forest model and its application to HPSG parsing. We also provide an extensive empirical evaluation of the resulting HPSG parsing approach using real-world text. Section 2 discusses a problem of conventional probabilistic models for lexicaliz"
J08-1002,P85-1018,0,0.56965,"uare boxes (ci ) are conjunctive nodes, and di disjunctive nodes. A solid arrow represents a disjunctive daughter function, and a dotted line expresses a conjunctive daughter function. Formally, a chart E, Er , α is mapped into a feature forest C, D, R, γ, δ as follows.6 r r r C = {em , el , er |em ∈ E ∧ (el , er ) ∈ α(em )} ∪ {w|w ∈ w} D=E R = {em , el , er |em ∈ Er ∧ em , el , er  ∈ C} 4 For simplicity, only binary trees are considered. Extension to unary and n-ary (n > 2) trees is trivial. 5 We assume that CONT and DTRS (a feature used to represent daughter signs) are restricted (Shieber 1985), and we will discuss a method for encoding CONT in a feature forest in Section 4.3. We also assume that parse trees are packed according to equivalence relations rather than subsumption relations (Oepen and Carroll 2000). We cannot simply map parse forests packed under subsumption into feature forests, because they over-generate possible unpacked trees. 6 For ease of explanation, the deﬁnition of the root node is different from the original deﬁnition given in Section 3. In this section, we deﬁne R as a set of conjunctive nodes rather than a single node r. The deﬁnition here is translated into"
J08-1002,W04-3201,0,0.0180033,"slight, trivial extension of PCFG. As described herein, however, feature forests can represent structures beyond CFG parse trees. Furthermore, because feature forests are a generalized representation of ambiguous structures, each node in a feature forest need not correspond to a node in a PCFG parse forest. That is, a node in a feature forest may represent any linguistic entity, including a fragment of a syntactic structure, a semantic relation, or other sentence-level information. The idea of feature forest models could be applied to non-probabilistic machine learning methods. Taskar et al. (2004) proposed a dynamic programming algorithm for the learning of large-margin classiﬁers including support vector machines (Vapnik 1995), and presented its application to disambiguation in CFG parsing. Their algorithm resembles feature forest models; an optimization function is computed by a dynamic programing algorithm without unpacking packed forest structures. From the discussion in this article, it is evident that if the main part of an update formula is represented 72 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing with (the exponential of) linear combinations, a method"
J08-1002,W02-2030,0,0.363425,"me 34, Number 1 Tsujii 2005), statistical modeling of these grammars is attracting considerable attention. This is because natural language processing applications usually require disambiguated or ranked parse results, and statistical modeling of syntactic/semantic preference is one of the most promising methods for disambiguation. The focus of this article is the problem of probabilistic modeling of wide-coverage HPSG parsing. Although previous studies have proposed maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) of HPSG-style parse trees (Oepen, Toutanova, et al. 2002b; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004), the straightforward application of maximum entropy models to wide-coverage HPSG parsing is infeasible because estimation of maximum entropy models is computationally expensive, especially when targeting wide-coverage parsing. In general, complete structures, such as transition sequences in Markov models and parse trees, have an exponential number of ambiguities. This causes an exponential explosion when estimating the parameters of maximum entropy models. We therefore require solutions to make model estimation tractable. This article ﬁr"
J08-1002,W04-3222,0,0.0246333,"Missing"
J08-1002,H05-1059,1,0.722295,"Missing"
J08-1002,P87-1015,0,0.142407,"Missing"
J08-1002,W06-1634,1,\N,Missing
J08-1002,W07-1204,0,\N,Missing
L16-1243,W09-3035,0,0.689291,"s and discussed with our linguistics colleagues to create a new POS tag set for Vietnamese with 33 tags which are shown in Table 3. Using our POS tags, we can recognize the role of a word in a phrase or sentence. For example, the demonstrative pronouns modifying head words of noun phases are annotated with the Pd label, and personal pronouns that are head words of noun phrases are annotated with the Pp label. 4.3. Policy for annotation of part-of-speech In our POS tagging guidelines, the words are tagged on the basis of the following criteria: Building part-of-speech tag set In previous work, Nguyen et al. (2009) classified the words on the basis of their combination ability and syntactic function. They created a POS tag set for Vietnamese including a total of 17 tags (except the tags for unknown words and the punctuation). However, this tag set cannot cover all the combination abilities as well as the syntactic functions of the Vietnamese words. For example, they used the 6 Việc is a special classifier noun that is understood as -ion, -ment, -ing, -ity, -ness, or so on when it comes before verbs or adjectives. An expression of the special classifier noun việc and a verb or adjective is understood as"
L16-1243,W12-5005,1,0.809598,"es, are important resources for researchers in natural language processing (NLP). Treebanks provide important syntactic information in order to improve the quality of NLP tools. To strengthen the automatic processing of the Vietnamese language, Nguyen et al. (2009) have built a Vietnamese treebank, named VLSP treebank, containing 10,000 sentences. However, the quality of the VLSP treebank, including the quality of the annotation scheme, the annotation guidelines, and the annotation process, is not satisfactory and is a possible source for the low performance of Vietnamese language processing (Nguyen et al., 2012; Nguyen et al., 2013). We have been building a new Vietnamese treebank with 3,000 texts (about 40,000 sentences) covering 14 topics collected from a Vietnamese online newspaper, Thanhnien news1 . Our treebank is annotated with three layers: word segmentation (WS), part-of-speech (POS) tagging, and bracketing as showed in Figure 12 . We have found that ensuring the annotation consistency and accuracy is one of the most important considerations in the annotation of a treebank. This requires clear and complete annotation guidelines. The guidelines contain the annotation scheme, consistent princi"
L16-1243,W13-2303,1,0.841275,"ources for researchers in natural language processing (NLP). Treebanks provide important syntactic information in order to improve the quality of NLP tools. To strengthen the automatic processing of the Vietnamese language, Nguyen et al. (2009) have built a Vietnamese treebank, named VLSP treebank, containing 10,000 sentences. However, the quality of the VLSP treebank, including the quality of the annotation scheme, the annotation guidelines, and the annotation process, is not satisfactory and is a possible source for the low performance of Vietnamese language processing (Nguyen et al., 2012; Nguyen et al., 2013). We have been building a new Vietnamese treebank with 3,000 texts (about 40,000 sentences) covering 14 topics collected from a Vietnamese online newspaper, Thanhnien news1 . Our treebank is annotated with three layers: word segmentation (WS), part-of-speech (POS) tagging, and bracketing as showed in Figure 12 . We have found that ensuring the annotation consistency and accuracy is one of the most important considerations in the annotation of a treebank. This requires clear and complete annotation guidelines. The guidelines contain the annotation scheme, consistent principles to annotate lingu"
L16-1243,xia-etal-2000-developing,0,0.177004,"biguity of annotating a sentence in Vietnamese. 2. Characteristics of Vietnamese language and methodology for guideline preparation Unlike Western languages, in which blank spaces denote word delimiters, in Vietnamese, blank spaces play the roles of not only word delimiters but also syllable delimiters (Diep, 2005; SCSSV, 1983) that cause difficulties in defining words. In addition, unlike English and Japanese, Vietnamese is not an inflectional language for which morphological forms can provide useful clues for word segmentation and POS tagging. While similar problems also occur with Chinese (Xia et al., 2000), annotating Vietnamese words may be more difficult, because the modern Vietnamese writing system is based on Latin characters, which represent the pronunciation but not the meaning of words, resulting in many homonyms. Difficulties in Vietnamese occur in not only determining words as mentioned above but also bracketing phrases. One of the reasons is that there are many expressions having the same POS sequence but different phrase types in Vietnamese. Other difficulties are caused by the fact that word order in Vietnamese is very flexible. Moreover, there is little consensus in community about"
L16-1261,W08-1301,0,0.258594,"Missing"
L16-1261,de-marneffe-etal-2014-universal,0,0.14179,"Missing"
L16-1261,den-etal-2008-proper,0,0.0441224,"Missing"
L16-1261,N06-1023,0,0.0301,"ersal part-of-speech (POS) tags (UPOS) (Petrov et al., 2012). In our research, we attempt to port the UD annotation scheme to the Japanese language. The traditional annotation schemes for the Japanese language have been uniquely developed and are markedly different from other schemes, such as Penn Treebank-style annotation. Japanese syntactic parsing trees are usually represented as unlabeled dependency structures between bunsetsu chunks (base phrase units), as found in the Kyoto University Text Corpus (Kurohashi and Nagao, 2003) and the outputs of syntactic parsers (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2006). Therefore, we must devise a method to construct word-based dependency structures that match the characteristics of the Japanese language (Uchimoto and Den, 2008; Mori et al., 2014; Tanaka and Nagata, 2015) and are able to derive the syntactic information required to assign relation types to dependencies. We describe the conversion from the Japanese POS tagset to the UPOS tagset, the adaptation of the UD annotation for Japanese syntax, and the attempt to build a UD corpus by converting the existing resources. We also address the remaining issues that may emerge when applying the UD scheme to"
L16-1261,W02-2016,1,0.600528,"al., 2014) and Google universal part-of-speech (POS) tags (UPOS) (Petrov et al., 2012). In our research, we attempt to port the UD annotation scheme to the Japanese language. The traditional annotation schemes for the Japanese language have been uniquely developed and are markedly different from other schemes, such as Penn Treebank-style annotation. Japanese syntactic parsing trees are usually represented as unlabeled dependency structures between bunsetsu chunks (base phrase units), as found in the Kyoto University Text Corpus (Kurohashi and Nagao, 2003) and the outputs of syntactic parsers (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2006). Therefore, we must devise a method to construct word-based dependency structures that match the characteristics of the Japanese language (Uchimoto and Den, 2008; Mori et al., 2014; Tanaka and Nagata, 2015) and are able to derive the syntactic information required to assign relation types to dependencies. We describe the conversion from the Japanese POS tagset to the UPOS tagset, the adaptation of the UD annotation for Japanese syntax, and the attempt to build a UD corpus by converting the existing resources. We also address the remaining issues that may emerge"
L16-1261,W04-3230,1,0.799406,"Missing"
L16-1261,maekawa-etal-2000-spontaneous,0,0.265949,"Missing"
L16-1261,mori-etal-2014-japanese,1,0.826748,"apanese language have been uniquely developed and are markedly different from other schemes, such as Penn Treebank-style annotation. Japanese syntactic parsing trees are usually represented as unlabeled dependency structures between bunsetsu chunks (base phrase units), as found in the Kyoto University Text Corpus (Kurohashi and Nagao, 2003) and the outputs of syntactic parsers (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2006). Therefore, we must devise a method to construct word-based dependency structures that match the characteristics of the Japanese language (Uchimoto and Den, 2008; Mori et al., 2014; Tanaka and Nagata, 2015) and are able to derive the syntactic information required to assign relation types to dependencies. We describe the conversion from the Japanese POS tagset to the UPOS tagset, the adaptation of the UD annotation for Japanese syntax, and the attempt to build a UD corpus by converting the existing resources. We also address the remaining issues that may emerge when applying the UD scheme to other languages. Word unit The definition of a word unit is indispensable in UD annotation, which is not a trivial question for Japanese, since a sentence is not segmented into word"
L16-1261,P11-2093,1,0.783083,"e adaptation of the UD annotation for Japanese syntax, and the attempt to build a UD corpus by converting the existing resources. We also address the remaining issues that may emerge when applying the UD scheme to other languages. Word unit The definition of a word unit is indispensable in UD annotation, which is not a trivial question for Japanese, since a sentence is not segmented into words or morphemes by white space in its orthography. Thus, we have several word unit standards that can be found in corpus annotation schemata or in the outputs of morphological analyzers (Kudo et al., 2004; Neubig et al., 2011). NINJAL1 proposed several word unit standards for Japanese corpus linguistics, such as the minimum word unit (Maekawa et al., 2000). Since 2002, the Institute has maintained a morphological information annotated lexicon, UniDic (Den et al., 2008), and has proposed three types of word unit standards: Short Unit Word (SUW): SUW is a minimal language unit that has a morphological function. SUW almost always corresponds to an entry in traditional Japanese dictionaries. Middle Unit Word (MUW): MUW is based on the rightbranching compound word construction and on phonological constructions, such as"
L16-1261,petrov-etal-2012-universal,0,0.0821258,". Keywords: typed dependencies, Short Unit Word, multiword expression, UniDic 1. Introduction 2. The Universal Dependencies (UD) project has been developing cross-linguistically consistent treebank annotation for various languages in recent years. The goal of the project is to facilitate multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective (Nivre, 2015). The annotation scheme is based on (universal) Stanford dependencies (de Marneffe and Manning, 2008; de Marneffe et al., 2014) and Google universal part-of-speech (POS) tags (UPOS) (Petrov et al., 2012). In our research, we attempt to port the UD annotation scheme to the Japanese language. The traditional annotation schemes for the Japanese language have been uniquely developed and are markedly different from other schemes, such as Penn Treebank-style annotation. Japanese syntactic parsing trees are usually represented as unlabeled dependency structures between bunsetsu chunks (base phrase units), as found in the Kyoto University Text Corpus (Kurohashi and Nagao, 2003) and the outputs of syntactic parsers (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2006). Therefore, we must devise a m"
L16-1261,W13-4913,1,0.86233,"(10) 太郎 . NOUN . Taro . cop は . ADP . -TOPIC . 学生 . . NOUN . student . だ . AUX . COPULA . ‘Taro is a student.’ 5. Corpus It is reasonable to obtain Japanese UD corpora by converting existent linguistic resources; however, a direct conversion from the major Japanese corpora such as the Kyoto University Text Corpus (Kurohashi and Nagao, 2003) is not simple since they lack syntactic information (unlabeled) and the structure is not suitable to recover constituents (bunsetsu chunk-based dependency trees). Therefore, we first constructed conversion rules for use with Japanese constituent treebank (Tanaka and Nagata, 2013) 1655 for the Mainichi Shimbun Newspaper. The treebank was initially built by converting the Kyoto University Text Corpus and was manually annotated. The treebank has clause level annotations with syntactic function labels, e.g., syntactic role and clause type, and coordination construction, which are required for UD annotation. The treebank is composed of complete binary trees, and can be easily converted to dependency tree by adapting the head percolation rules and dependency type rules for each partial tree. The UD corpus is composed of 10,000 sentences, and it contains 267,631 tokens. The"
L16-1261,P15-2039,1,0.532985,"ve been uniquely developed and are markedly different from other schemes, such as Penn Treebank-style annotation. Japanese syntactic parsing trees are usually represented as unlabeled dependency structures between bunsetsu chunks (base phrase units), as found in the Kyoto University Text Corpus (Kurohashi and Nagao, 2003) and the outputs of syntactic parsers (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2006). Therefore, we must devise a method to construct word-based dependency structures that match the characteristics of the Japanese language (Uchimoto and Den, 2008; Mori et al., 2014; Tanaka and Nagata, 2015) and are able to derive the syntactic information required to assign relation types to dependencies. We describe the conversion from the Japanese POS tagset to the UPOS tagset, the adaptation of the UD annotation for Japanese syntax, and the attempt to build a UD corpus by converting the existing resources. We also address the remaining issues that may emerge when applying the UD scheme to other languages. Word unit The definition of a word unit is indispensable in UD annotation, which is not a trivial question for Japanese, since a sentence is not segmented into words or morphemes by white sp"
L16-1261,uchimoto-den-2008-word,0,0.0247262,"tation schemes for the Japanese language have been uniquely developed and are markedly different from other schemes, such as Penn Treebank-style annotation. Japanese syntactic parsing trees are usually represented as unlabeled dependency structures between bunsetsu chunks (base phrase units), as found in the Kyoto University Text Corpus (Kurohashi and Nagao, 2003) and the outputs of syntactic parsers (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2006). Therefore, we must devise a method to construct word-based dependency structures that match the characteristics of the Japanese language (Uchimoto and Den, 2008; Mori et al., 2014; Tanaka and Nagata, 2015) and are able to derive the syntactic information required to assign relation types to dependencies. We describe the conversion from the Japanese POS tagset to the UPOS tagset, the adaptation of the UD annotation for Japanese syntax, and the attempt to build a UD corpus by converting the existing resources. We also address the remaining issues that may emerge when applying the UD scheme to other languages. Word unit The definition of a word unit is indispensable in UD annotation, which is not a trivial question for Japanese, since a sentence is not"
L16-1607,anick-etal-2014-identification,0,0.0203449,"time mobile devices, Mac lower bound, cost, sentence Asia, space, between human, Eugene Charniak CRF, algorithm qualitative, new five, two-fold, several can, cannot, need to it, they Miyao and Tsujii 2008, [1] English, natural language NLP, biomedicine ERLA, universities F=0.98 Table 1: Entity tags, definitions and examples: names in monospaced font denotes the class in IAO (algorithms, materials, tools, and data used in invention), EFFECT (effects of a technology that can be expressed as a pair comprising an attribute and a value), and ATTRIBUTE and VALUE (attribute and value in the effect). Anick et al. (2014) extracted technology terms defined as Artifact (object created as a result of some process), Process/Technique (method for creation) or Field (a discipline or a scientific area relating to creation) using a corpus in which mentions of entities playing these roles are labeled. Roth and Klein (2015) extracted terms that denote an ACTION, ACTOR, OBJECT, and PROPERTY, using an annotated dataset in which entity mentions are labeled based on the ontology defined by Roth et al. (2014). In their ontology concepts are classified according to roles that things can play in a particular operation, such a"
L16-1607,I11-1001,0,0.225982,"based analysis such as information extraction (IE) concerning the methodological aspects of research papers and patents for analyzing technical trends and discovering emerging research fields. Their focus is on determining how things such as systems and data are developed and used. Consequently, in the annotated corpora used for establishing the systems for these purposes, things described in a document are labeled and classified according to their role in a certain context, such as application domain, method, and product. Some studies attach role-based labels to entity mentions. For example, Gupta and Manning (2011), in establishing a method for identifying the technical trends from abstracts in the ACL anthology 2 , extracted the FOCUS (main contribution of the article), DOMAIN (application domain), and TECHNIQUE (a method or tool used to achieve the FOCUS). The corpus used for the study attaches these labels directly to mentions of the corresponding entities. Similarly, Fukuda et al. (2012) annotated and classified entities in patent documents as TECHNOLOGY 2 3836 https://aclweb.org/anthology/ Type THING OCCURRENT PROCESS TIME CONTINUANT ARTIFACT DATA-ITEM LOCATION PERSON PLAN QUALITY QUANTITY MODALITY"
L16-1607,S10-1004,0,0.0371118,"DOMAIN (an area of study); and FORMULA (a mathematical formula). In addition, we defined the following compound or “ambiguous” types to handle systematic ambiguity in 3 Annotated Data Our dataset was constructed from 400 abstracts of research papers (250 abstracts from the ACL anthology and 150 from the ACM digital library 3 ). In the ACL subset, 150 abstracts were randomly selected from the entire set and the remaining 100 were randomly selected from the set used by Gupta and Manning (2011). The abstracts in the ACM subset were randomly selected from the set used for the SEMEVAL-2010 task 5 (Kim et al., 2010). Errors in text resulting from PDF conversion were manually corrected. Annotation was performed by a single annotator (the second author). A screenshot of the brat system (Stenetorp et al., 2012) is given in Figure 1 as an annotation example. In 1959 sentences in the ACL set, 14887 entities and 13310 relations were identified. In the 1213 sentences in the ACM set, the numbers of identified entities and relations were 12463 and 11201, respectively. The distributions of entity and relation types, in proportion, in the two domains are shown in Figures 2 and 3. The results shown in Figure 2 indic"
L16-1607,P03-1054,0,0.00501384,"riginal annotation on the same part of the abstract shown in Figure 1, converted to standoff format for displaying in brat. Their annotation is sparser than ours (Figure 1), annotating only terms related to the topic of the paper as a whole. For extraction, they used heuristic rules based on trigger words and Stanford dependencies such as “A term is FOCUS if it is the direct object of the verb present” as seed rules. Then, the rule set was enhanced by iteratively adding the head words of extracted phrases as the triggers. The abstracts were tokenized using the Stanford parser (version 3.4.1) (Klein and Manning 2003), and the tokens are labeled with binary labels for inclusion in GuptaManning terms for each topic class (FOCUS, DOMAIN, and TECHNIQUE). Then, the support vector classifier from the python scikit-learn 0.17 package (Pedregosa et al., 2011) with a linear kernel was used to predict the labels. We tested several combinations of the features from the Stanford parser and our annotation. The features from the Stanford parser were parts of speech (P in Table 5) and the triplet of type, direction (head or argument), and the part of speech of the token it depends/depended on, for each dependency involv"
L16-1607,J08-1002,1,0.854996,"Missing"
L16-1607,D14-1200,0,0.0358364,"Missing"
L16-1607,W09-3716,0,0.0330807,"otactic component (...). The three components mentioned are entities of different types, i.e., lexicon is a dataset (DATA-ITEM), and the others are program functions (PLAN). The current convention uses IS-A relation to relate components and lexicon etc., which is impossible without violating Rule-IS. This suggests that we need to define a new relation for role-playing and a new type or types for “role” words such as components. We also found that ambiguity and metonymic constructions cause annotation difficulty. These violations suggest a need for a type-coercion mechanism, such as dot-types (Pustejovsky et al. 2009). For example, when a process uses parameters, the names of the parameters can denote “the invocation of the process with the parameters” (e. g. RM pairs extracted can perform the mapping, where RM pairs extracted denotes a process using the pairs as parameters) and the name of data structure is used for both the data structure itself and the content of the data (Bigrams and trigrams are commonly used in statistical natural language processing). They lead to the annotation of APPLY-TO relation between DATAITEM and PROCESS, which violates Rule-APP. Another type of the problem is the ambiguity b"
L16-1607,W04-2401,0,0.0369304,"Missing"
L16-1607,W15-0403,0,0.0213946,"gs, definitions and examples: names in monospaced font denotes the class in IAO (algorithms, materials, tools, and data used in invention), EFFECT (effects of a technology that can be expressed as a pair comprising an attribute and a value), and ATTRIBUTE and VALUE (attribute and value in the effect). Anick et al. (2014) extracted technology terms defined as Artifact (object created as a result of some process), Process/Technique (method for creation) or Field (a discipline or a scientific area relating to creation) using a corpus in which mentions of entities playing these roles are labeled. Roth and Klein (2015) extracted terms that denote an ACTION, ACTOR, OBJECT, and PROPERTY, using an annotated dataset in which entity mentions are labeled based on the ontology defined by Roth et al. (2014). In their ontology concepts are classified according to roles that things can play in a particular operation, such as a participant, actor, object, and property. Another type of approach to capturing the structure of entity roles is to annotate the relationship between entities to label the entities as “things in a certain context” and “how they are related to other things in the same context”. Kameda et al. (20"
L16-1607,W14-2410,0,0.0296543,"ressed as a pair comprising an attribute and a value), and ATTRIBUTE and VALUE (attribute and value in the effect). Anick et al. (2014) extracted technology terms defined as Artifact (object created as a result of some process), Process/Technique (method for creation) or Field (a discipline or a scientific area relating to creation) using a corpus in which mentions of entities playing these roles are labeled. Roth and Klein (2015) extracted terms that denote an ACTION, ACTOR, OBJECT, and PROPERTY, using an annotated dataset in which entity mentions are labeled based on the ontology defined by Roth et al. (2014). In their ontology concepts are classified according to roles that things can play in a particular operation, such as a participant, actor, object, and property. Another type of approach to capturing the structure of entity roles is to annotate the relationship between entities to label the entities as “things in a certain context” and “how they are related to other things in the same context”. Kameda et al. (2013), using Related Work sections from the proceedings of the Association for the Advancement of Artificial Intelligence (AAAI2010), identified the papertopic relation along with the me"
L16-1607,D07-1111,0,0.0605503,"Missing"
L16-1607,E12-2021,1,0.768208,"taset was constructed from 400 abstracts of research papers (250 abstracts from the ACL anthology and 150 from the ACM digital library 3 ). In the ACL subset, 150 abstracts were randomly selected from the entire set and the remaining 100 were randomly selected from the set used by Gupta and Manning (2011). The abstracts in the ACM subset were randomly selected from the set used for the SEMEVAL-2010 task 5 (Kim et al., 2010). Errors in text resulting from PDF conversion were manually corrected. Annotation was performed by a single annotator (the second author). A screenshot of the brat system (Stenetorp et al., 2012) is given in Figure 1 as an annotation example. In 1959 sentences in the ACL set, 14887 entities and 13310 relations were identified. In the 1213 sentences in the ACM set, the numbers of identified entities and relations were 12463 and 11201, respectively. The distributions of entity and relation types, in proportion, in the two domains are shown in Figures 2 and 3. The results shown in Figure 2 indicate that software (PLAN, PLAN-OR-PROCESS) is more frequently discussed than hardware (ARTIFACT) in both the general computer science/technology domain (ACM) and the natural language processing sub"
L16-1607,W13-2318,1,0.777851,"Missing"
L16-1607,tateisi-etal-2014-annotation,1,0.890874,"gence (AAAI2010), identified the papertopic relation along with the method-purpose relation among concepts described in the paper in order to construct a network representing the methods developed in one study and used by others and to evaluate the influence of the research. Nassour-Kassis et al. (2015) identified the mentions of tasks and attributes and linked them with one of 6 types (Means-End, Instance-of, Consists-of, Associated-with, Contributes-to, and Compares-to) of relations, using ten articles on summarization for building a conceptual map in the natural language processing domain. Tateisi et al. (2014) developed a corpus on research articles from Journal of Information Processing Society of Japan (IPSJ Journal) where relationship among OBJECTS (named entities), MEASURE (judgment and evaluation, including numbers), and TERM (general technical concepts other than OBJECT and MEASURE) are identified and labeled with one of 16 types such as Apply-to (method-purpose), Evaluate (evaluation objectevaluation result), and Attribute (object-attribute), and developed a prototype of a keyword-based search system in which results can be filtered according to the relations involving the keyword. Those wor"
L16-1630,D11-1031,0,0.0192561,"linguistic properties of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where subst"
L16-1630,W13-2322,0,0.201241,"ns as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer t"
L16-1630,W08-2222,0,0.0201912,"and sources, comprising gold-standard annotations as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks and to encourage broader use of this standardized collec"
L16-1630,Q15-1040,1,0.86691,"banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional manual annotation was perf"
L16-1630,J93-2004,0,0.0550061,"ies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional manual annotation was performed). CCD: Combinatory Categorial Grammar Dependencies Hockenmaier & Steedman (2007) construct CCGbank from a combination of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Journal (WSJ) Corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bi-lexical dependency triples. The latter “includ"
L16-1630,S14-2082,0,0.0694129,"information. We envision that general availability of a standardized and comprehensive set of semantic dependency graphs and associated tools will stimulate more research in this sub-area of semantic parsing. To date, reported ‘parsing success’ 7 To seek to relate these different approaches to the encoding of lexical valency, one can multiply out the DM frame identifiers with verb lemmata, which yields a count of some 4,600 distinct combinations, i.e. slightly less than the set of observed sense distinctions in PSD. measures in terms of dependency F1 range between the high seventies for PSD (Martins & Almeida, 2014) and high eighties to low nineties for CCD, DM, and PAS (Du et al., 2015; Miyao et al., 2014). Such variation may in principle be owed to differences in the number and complexity of linguistic distinctions made, to homogeneity and consistency of training and test data, and of course to the cumulative effort that has gone into pushing the state of the art on individual target representations. A deeper understanding of these parameters, as well as of contentful vs. superficial linguistic differences across frameworks, will be a prerequisite to judging the relative suitability of different resour"
L16-1630,J07-4004,0,0.0382917,"bank from a combination of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Journal (WSJ) Corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bi-lexical dependency triples. The latter “include most semantically relevant non-anaphoric local and longrange dependencies” and are suggested by the CCGbank creators as a proxy for predicate–argument structure. While these have mainly been used for contrastive parser evaluation (Clark & Curran, 2007; Fowler & Penn, 2010; inter alios), recent parsing work as mentioned above views each set of triples as a directed graph and parses directly into these target representations. Our CCD graphs combine the CCGbank dependency triples with information gleaned from the CCG syntactic derivations, notably the part of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannot"
L16-1630,copestake-flickinger-2000-open,1,0.723309,"al trees), and with corresponding ‘companion’ syntactic analyses from a broad variety of frameworks and sources, comprising gold-standard annotations as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic propert"
L16-1630,S14-2056,1,0.955729,"s). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional manual annotation was performed). CCD: Combinatory Categorial Grammar Dependencies Hockenmaier & Steedman (2007) construct CCGbank from a combination of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Jo"
L16-1630,P15-1149,0,0.063476,"s of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional"
L16-1630,flickinger-etal-2014-towards,1,0.859802,"ing ‘companion’ syntactic analyses from a broad variety of frameworks and sources, comprising gold-standard annotations as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks a"
L16-1630,P10-1035,0,0.0195635,"n of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Journal (WSJ) Corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bi-lexical dependency triples. The latter “include most semantically relevant non-anaphoric local and longrange dependencies” and are suggested by the CCGbank creators as a proxy for predicate–argument structure. While these have mainly been used for contrastive parser evaluation (Clark & Curran, 2007; Fowler & Penn, 2010; inter alios), recent parsing work as mentioned above views each set of triples as a directed graph and parses directly into these target representations. Our CCD graphs combine the CCGbank dependency triples with information gleaned from the CCG syntactic derivations, notably the part of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannotation, dubbed DeepBan"
L16-1630,hajic-etal-2012-announcing,1,0.917423,"Missing"
L16-1630,S15-2153,1,0.925074,"Missing"
L16-1630,S14-2008,1,0.928327,"Missing"
L16-1630,oepen-lonning-2006-discriminant,1,0.921801,"rivations, notably the part of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannotation, dubbed DeepBank2 , of Sections 00–21 of the WSJ Corpus with syntactico-semantic analyses from the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). Native ERG semantics take the form of underspecified logical forms, which Oepen et al. (2002); Oepen & Lønning (2006); and Ivanova et al. (2012) map onto the DM bi-lexical semantic dependencies in a twostep conversion pipeline.3 For this target representation, top nodes designate the highest-scoping (non-quantificational) predicate in the graph, e.g. the scopal adverb almost in Figure 1 below. PAS: Enju Predicate–Argument Structures The Enju Treebank4 is derived from automatic HPSG-style reannotation of the PTB (Miyao, 2006). Our PAS graphs stem from the Enju Treebank, without contentful conversion, and from the application of the same basic techniques to the Penn Chinese Treebank (CTB; Xue et al., 2005). To"
L16-1630,J07-3004,0,0.0404021,"hon tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Tree"
L16-1630,W12-3602,1,0.888177,"of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannotation, dubbed DeepBank2 , of Sections 00–21 of the WSJ Corpus with syntactico-semantic analyses from the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). Native ERG semantics take the form of underspecified logical forms, which Oepen et al. (2002); Oepen & Lønning (2006); and Ivanova et al. (2012) map onto the DM bi-lexical semantic dependencies in a twostep conversion pipeline.3 For this target representation, top nodes designate the highest-scoping (non-quantificational) predicate in the graph, e.g. the scopal adverb almost in Figure 1 below. PAS: Enju Predicate–Argument Structures The Enju Treebank4 is derived from automatic HPSG-style reannotation of the PTB (Miyao, 2006). Our PAS graphs stem from the Enju Treebank, without contentful conversion, and from the application of the same basic techniques to the Penn Chinese Treebank (CTB; Xue et al., 2005). Top nodes in this representat"
L18-1287,W16-5406,1,0.730532,"D Japanese-GSD, UD JapanesePUD, and UD Japanese-Modern (Omura et al., 2017). Table 1 presents the current status of UD Japanese resources. Below, we describe these resources briefly. UD Japanese-BCCWJ is UD data based on the ‘Balanced Corpus of Contemporary Written Japanese’ (hereafter BCCWJ) (Maekawa et al., 2014). The BCCWJ defines 1 million word-scale core data samples in which the morphological information is manually annotated with three layers of word delimitations: Short Unit Word (SUW), Long Unit Word (LUW), and bunsetsu. The BCCWJ has several syntactic annotations. The BCCWJ-DepPara (Asahara and Matsumoto, 2016) is a bunsetsu-based syntactic dependency and coordinate structure annotation. The BCCWJ-PAS (Ueda et al., 2015) is a predicate-argument relation annotation with the NAIST Text Corpus annotation schema (Iida et al., 2007). We maintain conversion rules based on these annotations. UD Japanese-KTC (Tanaka et al., 2016) is based on the NTT Japanese Phrase Structure Treebank (Tanaka and Nagata, 2013) which contains the same original text as the Kyoto Text Corpus (KTC) (Kurohashi and Nagao, 2003). KTC is a bunsetsu, namely base phrase, based dependency treebank with its own word delimitation schema"
L18-1287,W07-1522,1,0.718422,"ced Corpus of Contemporary Written Japanese’ (hereafter BCCWJ) (Maekawa et al., 2014). The BCCWJ defines 1 million word-scale core data samples in which the morphological information is manually annotated with three layers of word delimitations: Short Unit Word (SUW), Long Unit Word (LUW), and bunsetsu. The BCCWJ has several syntactic annotations. The BCCWJ-DepPara (Asahara and Matsumoto, 2016) is a bunsetsu-based syntactic dependency and coordinate structure annotation. The BCCWJ-PAS (Ueda et al., 2015) is a predicate-argument relation annotation with the NAIST Text Corpus annotation schema (Iida et al., 2007). We maintain conversion rules based on these annotations. UD Japanese-KTC (Tanaka et al., 2016) is based on the NTT Japanese Phrase Structure Treebank (Tanaka and Nagata, 2013) which contains the same original text as the Kyoto Text Corpus (KTC) (Kurohashi and Nagao, 2003). KTC is a bunsetsu, namely base phrase, based dependency treebank with its own word delimitation schema and POS tagset. The NTT Japanese Phrase Structure Treebank is a phrase structure-based treebank. The word delimitation and POS are adapted to the UniDic SUW standard. The data is still in version 1.0 schema as of February"
L18-1287,C00-1060,1,0.238943,"version 1.0 schema as of February 2018. We are now modifying UD Japanese KTC from version 1.0 schema to version 2.0. UD Japanese-GSD (formerly known as UD Japanese) consists of sentences from Wikipedia. The version 2.0 of this annotated corpus was provided for the CoNLL 2017 Shared Task (Zeman et al., 2017). In the release of version 2.0, the sentences have been automatically split into words by IBM’s word segmenter. The segmentation errors were removed by adding lexicons specific to the data. In addition, the dependencies are automatically resolved using the bunsetsu-level dependency parser (Kanayama et al., 2000) with the attachment rules for functional words defined in UD Japanese (Tanaka et al., 2016). Complex sentences with parenthesis were removed to avoid parsing errors. In the version 2.1 released in November 2017, manual annotations were merged with the semi-automatic annotations to reduce remaining errors. UD Japanese-PUD was created in the same manner as UD Japanese-GSD, with the goal of maintaining consistency with UD Japanese-GSD. Since it is a parallel corpus with other languages, no sentences were removed from the corpus, including the ones containing parenthesis. UD Japanese-Modern (Omur"
L18-1287,P13-2017,0,0.1339,"Missing"
L18-1287,W13-4913,1,0.80844,"information is manually annotated with three layers of word delimitations: Short Unit Word (SUW), Long Unit Word (LUW), and bunsetsu. The BCCWJ has several syntactic annotations. The BCCWJ-DepPara (Asahara and Matsumoto, 2016) is a bunsetsu-based syntactic dependency and coordinate structure annotation. The BCCWJ-PAS (Ueda et al., 2015) is a predicate-argument relation annotation with the NAIST Text Corpus annotation schema (Iida et al., 2007). We maintain conversion rules based on these annotations. UD Japanese-KTC (Tanaka et al., 2016) is based on the NTT Japanese Phrase Structure Treebank (Tanaka and Nagata, 2013) which contains the same original text as the Kyoto Text Corpus (KTC) (Kurohashi and Nagao, 2003). KTC is a bunsetsu, namely base phrase, based dependency treebank with its own word delimitation schema and POS tagset. The NTT Japanese Phrase Structure Treebank is a phrase structure-based treebank. The word delimitation and POS are adapted to the UniDic SUW standard. The data is still in version 1.0 schema as of February 2018. We are now modifying UD Japanese KTC from version 1.0 schema to version 2.0. UD Japanese-GSD (formerly known as UD Japanese) consists of sentences from Wikipedia. The ver"
L18-1287,uchimoto-den-2008-word,0,0.0154352,"d on UniDic word boundary definition. The definition contains three layers: SUW, LUW, and bunsetsu. SUW can be produced by the morphological analyser MeCab.1 with UniDic2 LUW and bunsetsu can be produced by the pre-trained chunker Comainu.3 NINJAL4 defined five sorts of word unit definitions by operationalism. The most fine-grained unit is NINJAL Minimum Unit Word. SUW (Short Unit Word: 短単位) is constructively defined by the NINJAL Minimum Unit Word (最小単位). MUW (Middle Unit Word: 中単位) is a basic unit where a sound may change at the beginning or the ending of a word and/or an accent may change (Uchimoto and Den, 2008). The Middle Unit Word defines voiced compound (“rendaku”) (van de Weijer et al., 2005). 1825 1 taku910.github.io/mecab/ unidic.ninjal.ac.jp/ 3 osdn.net/projects/comainu/ 4 National Institute for Japanese Language and Linguistics. 2 Short Unit Word (SUW) advcl root iobj name punct iobj name 中国 . PROPN . China . . ・ . PUNCT . . . 北京 . PROPN . Beijing . . punct case 大 . NOUN . univ. . . obj aux に . ADP . -IOBJ . . 留学 . VERB . . . compound し . AUX . . study . abroad . 、 . PUNCT . . . 帰国 . VERB . return . to Japan . case 後 . NOUN . after . . case に . ADP . -IOBJ . . 双子 . NOUN . twins . . を. ADP ."
L18-1287,K17-3001,1,0.858266,"Missing"
L18-1350,de-marneffe-etal-2014-universal,0,0.045659,"Missing"
L18-1350,W09-0715,0,0.0115523,"nts. However, in the segmentation task, since both definite and case markers co-occur, we segment them separately. Morphemes to be considered as clitics are listed in Binyam, Miyao, and Baye (2016). Following this, we developed a manually segmented data of 2, 300 sentences or 50,520 tokens out of which we selected only 1000 sentences, 12, 039 tokens for the manual annotation of POS tagging, morphological information, and dependency relations. 4. Parts of speech annotation There have been some works on POS tagging in Amharic (Gamback B., 2012; Martha, Solomon, and Besacier, 2011; Binyam, 2010; Gambäck, Olsson, Argaw, and Asker, 2009; Sisay, 2005). However, the work of Demeke and Getachew (2006), known as the Walta Information Center corpus (WIC), has received much attention among Amharic NLP researchers and has been used for different applications. They propose a 31 tag-set for the manual annotation of a news corpus of 210,000 tokens. The tag-set is based on orthographic words. As a result, they propose a compound tag-set for those words which attach preposi2217 tion and/or conjunctions. Since these elements are attached to different lexical categories like nouns, verbs, adjectives, etc, the number of tag-sets has increa"
L18-1350,W09-3819,0,0.0382846,"here they serve as a gold standard for these tools. Treebanks have been developed for well-resourced languages in different frameworks such as Phrase Structure, HPSG, and Dependency. However, there are no treebanks for Amharic in any form. In this study, an attempt will be done to create treebanks for Amharic. Apart from developing this resource, the research contributes to the general problem of parsing Morphologically-rich Languages (MRL). In such languages, a dependency relation exists not only between the orthographic words (space-delimited tokens) but also relations within a word itself (Goldberg, Elhadad, and Gurion, 2009). Because of this, clitics attached to orthographic words need to be segmented for proper syntactic analysis. However, automatic segmentation of the prefix and the suffix clitics from the orthographic word in Amharic is problematic due to morpheme co-occurrence restriction, assimilation and ambiguity of the clitics (cf. Section 3 and 4). In this paper, first we discuss clitic segmentation then we describe the creation of the treebanks which are annotated for POS tag, morphological information and dependency relation. 2. Background Universal Dependencies (UD) project is a collaborative effort"
L18-1350,L16-1262,0,0.0297788,"him the big black book.” In the above examples, the definite marker (-u) and the case marker (-n) are attached to the head noun in (1), but to the adjective in (2) and (3). When the noun phrase expands both markers are attached to the left most element. The noun phrases in (2) and (3) get their definite features from other elements within the phrase. That is why we consider these features as phrasal elements. However, in the segmentation task, since both definite and case markers co-occur, we segment them separately. Morphemes to be considered as clitics are listed in Binyam, Miyao, and Baye (2016). Following this, we developed a manually segmented data of 2, 300 sentences or 50,520 tokens out of which we selected only 1000 sentences, 12, 039 tokens for the manual annotation of POS tagging, morphological information, and dependency relations. 4. Parts of speech annotation There have been some works on POS tagging in Amharic (Gamback B., 2012; Martha, Solomon, and Besacier, 2011; Binyam, 2010; Gambäck, Olsson, Argaw, and Asker, 2009; Sisay, 2005). However, the work of Demeke and Getachew (2006), known as the Walta Information Center corpus (WIC), has received much attention among Amharic"
L18-1350,petrov-etal-2012-universal,0,0.0445868,"tics from the orthographic word in Amharic is problematic due to morpheme co-occurrence restriction, assimilation and ambiguity of the clitics (cf. Section 3 and 4). In this paper, first we discuss clitic segmentation then we describe the creation of the treebanks which are annotated for POS tag, morphological information and dependency relation. 2. Background Universal Dependencies (UD) project is a collaborative effort to ensure consistent annotations across many languages. This project has benefited from earlier efforts including universal annotation of Google Universal part-ofspeech tags (Petrov, Das, and Mcdonald, 2012), morphosyntactic features (Zeman, 2008; Zeman et al., 2012) and Stanford Dependencies (de Marneffe et al., 2014; de Marneffe and Manning, 2008). The objective of UD, as stated in Nivre (2015) is to encourage multilingual parser improvement, cross-lingual learning, and parsing research from a language typology point of view. Even if UD proposes consistent ways of annotations across languages, it does not compromise the unique features of each language. The framework allows language-specific features to be included in annotations. In this paper, we discuss the language-specific features for Am"
L18-1350,W05-0707,0,0.0132076,"ince both definite and case markers co-occur, we segment them separately. Morphemes to be considered as clitics are listed in Binyam, Miyao, and Baye (2016). Following this, we developed a manually segmented data of 2, 300 sentences or 50,520 tokens out of which we selected only 1000 sentences, 12, 039 tokens for the manual annotation of POS tagging, morphological information, and dependency relations. 4. Parts of speech annotation There have been some works on POS tagging in Amharic (Gamback B., 2012; Martha, Solomon, and Besacier, 2011; Binyam, 2010; Gambäck, Olsson, Argaw, and Asker, 2009; Sisay, 2005). However, the work of Demeke and Getachew (2006), known as the Walta Information Center corpus (WIC), has received much attention among Amharic NLP researchers and has been used for different applications. They propose a 31 tag-set for the manual annotation of a news corpus of 210,000 tokens. The tag-set is based on orthographic words. As a result, they propose a compound tag-set for those words which attach preposi2217 tion and/or conjunctions. Since these elements are attached to different lexical categories like nouns, verbs, adjectives, etc, the number of tag-sets has increased. This in r"
L18-1350,zeman-2008-reusable,0,0.247233,"e to morpheme co-occurrence restriction, assimilation and ambiguity of the clitics (cf. Section 3 and 4). In this paper, first we discuss clitic segmentation then we describe the creation of the treebanks which are annotated for POS tag, morphological information and dependency relation. 2. Background Universal Dependencies (UD) project is a collaborative effort to ensure consistent annotations across many languages. This project has benefited from earlier efforts including universal annotation of Google Universal part-ofspeech tags (Petrov, Das, and Mcdonald, 2012), morphosyntactic features (Zeman, 2008; Zeman et al., 2012) and Stanford Dependencies (de Marneffe et al., 2014; de Marneffe and Manning, 2008). The objective of UD, as stated in Nivre (2015) is to encourage multilingual parser improvement, cross-lingual learning, and parsing research from a language typology point of view. Even if UD proposes consistent ways of annotations across languages, it does not compromise the unique features of each language. The framework allows language-specific features to be included in annotations. In this paper, we discuss the language-specific features for Amharic. UD (v2.0) was released on March 0"
L18-1350,zeman-etal-2012-hamledt,0,0.0514493,"Missing"
matsubayashi-etal-2012-building,kingsbury-palmer-2002-treebank,0,\N,Missing
matsubayashi-etal-2012-building,W07-1522,0,\N,Missing
matsubayashi-etal-2012-building,E12-1070,1,\N,Missing
matsubayashi-etal-2012-building,kawahara-kurohashi-2006-case,0,\N,Missing
N18-1166,W06-1623,0,0.0229177,"rmation extraction research in the NLP community. The original TimeBank only annotated relations judged to be salient by annotators and resulted in sparse annotations. Subsequent TempEval-1,2,3 competitions (Verhagen et al., 2009, 2010; UzZaman et al., 2012) mostly relied on TimeBank, but also aimed to improve coverage by annotating relations between all events and time expressions in the same sentence. However, most missing relations between mentions in different sentences are not considered. In order to solve the sparsity issue, researchers started the work towards denser annotation schema. Bramsen et al. (2006) annotated multi-sentence segments of text to build directed acyclic graphs. Kolomiyets et al. (2012) annotated temporal dependency structures, though they only focused on relations between pairs of events. Do et al. (2012) produced the densest annotation and the annotator was required to annotate pairs “as many as possible”. Cassidy et al. (2014) proposed a compulsory mechanism to force annotators to label every pair in a given sentence window. They performed the annotation (TimeBankDense) on a subset (36 documents) of TimeBank, which achieved a denser corpus with 6.3 TLINKs per event and tim"
N18-1166,P14-2082,0,0.253247,"el temporal relations. For solving this, many dense annotation schemata are proposed to force annotators to annotate more or even complete graph pairs. However, dense annotation is time-consuming and unstable human judgments 1 2 https://catalog.ldc.upenn.edu/LDC2006T08 http://www.timeml.org/ 1833 Proceedings of NAACL-HLT 2018, pages 1833–1843 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics on “salient” pairs are not improved at all. As a consequence, a high proportion of “vague” or “nolink” pairs appears in these dense corpora such as TimeBank-Dense (Cassidy et al., 2014). In this work, we propose a new approach to obtain temporal relations from time anchors, i.e. absolute time value, of all mentions. We assume that a temporal relation can be induced by comparing the relative temporal order of two time anchors (e.g. YYYY-MM-DD) in a time axis. We use pre-defined rules (Section 3) to generate temporal order (TORDER) relations (e.g. BEFORE, AFTER, SAME DAY, etc.) by taking two annotated time anchors as input. This proposal requires the annotation of time anchors, of which the annotation effort is linear with the number of mentions. This is the first work to obta"
N18-1166,Q14-1022,0,0.0621459,"es the issue that hand-labeling all dense TLINKs is extremely time-consuming and the unclear definition of “salient” is not improved at all. 2.2 Temporal Relation Classification The majority of the temporal relation classifiers focus on exploiting a variety of features to improve the performance in TimeBank. Laokulrat et al. (2013) extracted lexical and morphological features derived from WordNet synsets. Mani et al. (2006); D’Souza and Ng (2013) incorporated semantic relations between verbs from VerbOcean. Recently, more researchers move on to diverse approaches on the TimeBank-Dense corpus. Chambers et al. (2014) proposed a multi-sieve classifier composed of several rule-based and machine learning based sieves ranked by their precision. Mirza and Tonelli (2016) started to mine the value of low-dimensional word embeddings by concatenating them with traditional sparse feature 1834 vectors to improve their classifier. Inspired by the success of the deep learning work in the similar task: relation extraction, Cheng and Miyao (2017) proposed the shortest dependency path based Bi-directional Long short-term memory (Hochreiter and Schmidhuber, 1997) (Bi-LSTM) to achieve state-of-the-art performance in the Ti"
N18-1166,chang-manning-2012-sutime,0,0.0405976,"(Allen, 1990) defines ‘starts’, ‘finish’ relations, which are not included in our current defini1836 tion. We can easily extend our definition by detecting whether two time anchors have the equivalent begin or end points. Our inducing proposal takes human annotated time expressions and normalized values as inputs to generate TORDER relations as the training data of the next processes (e.g. classification). In the case of processing raw texts, we can perform detection and normalization of time expressions by using existing temporal taggers, e.g. HeidelTime (Str¨otgen and Gertz, 2015), SUTime (Chang and Manning, 2012), etc. 4 Comparison of TORDERs and TLINKs Fairly evaluating the TORDER’s capability of encoding temporal order information compared to the existing data is difficult but necessary work. This section provides empirical statistics of TORDER and TLINK annotations, and compare the performance of automatic recognition. Additionally, we evaluate these two frameworks in a downstream task performance in Section 5. 4.1 Correspondences and Differences Our new TORDERs are formally similar to the conventional TLINKs, as both state a temporal relation between two mentions. BEFORE and AFTER represent that o"
N18-1166,P17-2001,1,0.819618,"006); D’Souza and Ng (2013) incorporated semantic relations between verbs from VerbOcean. Recently, more researchers move on to diverse approaches on the TimeBank-Dense corpus. Chambers et al. (2014) proposed a multi-sieve classifier composed of several rule-based and machine learning based sieves ranked by their precision. Mirza and Tonelli (2016) started to mine the value of low-dimensional word embeddings by concatenating them with traditional sparse feature 1834 vectors to improve their classifier. Inspired by the success of the deep learning work in the similar task: relation extraction, Cheng and Miyao (2017) proposed the shortest dependency path based Bi-directional Long short-term memory (Hochreiter and Schmidhuber, 1997) (Bi-LSTM) to achieve state-of-the-art performance in the TimeBank-Dense corpus, which is adopted for the experiments in this paper. There are two reasons to use this classifier: 1) intersentence temporal relations are well treated. 2) only word, part-of-speech and dependency relation embeddings are required as input. 2.3 Time Anchor Annotation A related task: Cross-Document Event Ordering (Minard et al., 2015) aims to order the events involving a target entity in a timeline giv"
N18-1166,D12-1062,0,0.0237635,"9, 2010; UzZaman et al., 2012) mostly relied on TimeBank, but also aimed to improve coverage by annotating relations between all events and time expressions in the same sentence. However, most missing relations between mentions in different sentences are not considered. In order to solve the sparsity issue, researchers started the work towards denser annotation schema. Bramsen et al. (2006) annotated multi-sentence segments of text to build directed acyclic graphs. Kolomiyets et al. (2012) annotated temporal dependency structures, though they only focused on relations between pairs of events. Do et al. (2012) produced the densest annotation and the annotator was required to annotate pairs “as many as possible”. Cassidy et al. (2014) proposed a compulsory mechanism to force annotators to label every pair in a given sentence window. They performed the annotation (TimeBankDense) on a subset (36 documents) of TimeBank, which achieved a denser corpus with 6.3 TLINKs per event and time expression, comparing to 0.7 in the original TimeBank corpus. However, it raises the issue that hand-labeling all dense TLINKs is extremely time-consuming and the unclear definition of “salient” is not improved at all. 2."
N18-1166,N13-1112,0,0.0640384,"Missing"
N18-1166,P12-1010,0,0.021663,"udged to be salient by annotators and resulted in sparse annotations. Subsequent TempEval-1,2,3 competitions (Verhagen et al., 2009, 2010; UzZaman et al., 2012) mostly relied on TimeBank, but also aimed to improve coverage by annotating relations between all events and time expressions in the same sentence. However, most missing relations between mentions in different sentences are not considered. In order to solve the sparsity issue, researchers started the work towards denser annotation schema. Bramsen et al. (2006) annotated multi-sentence segments of text to build directed acyclic graphs. Kolomiyets et al. (2012) annotated temporal dependency structures, though they only focused on relations between pairs of events. Do et al. (2012) produced the densest annotation and the annotator was required to annotate pairs “as many as possible”. Cassidy et al. (2014) proposed a compulsory mechanism to force annotators to label every pair in a given sentence window. They performed the annotation (TimeBankDense) on a subset (36 documents) of TimeBank, which achieved a denser corpus with 6.3 TLINKs per event and time expression, comparing to 0.7 in the original TimeBank corpus. However, it raises the issue that han"
N18-1166,S13-2015,0,0.019125,"annotators to label every pair in a given sentence window. They performed the annotation (TimeBankDense) on a subset (36 documents) of TimeBank, which achieved a denser corpus with 6.3 TLINKs per event and time expression, comparing to 0.7 in the original TimeBank corpus. However, it raises the issue that hand-labeling all dense TLINKs is extremely time-consuming and the unclear definition of “salient” is not improved at all. 2.2 Temporal Relation Classification The majority of the temporal relation classifiers focus on exploiting a variety of features to improve the performance in TimeBank. Laokulrat et al. (2013) extracted lexical and morphological features derived from WordNet synsets. Mani et al. (2006); D’Souza and Ng (2013) incorporated semantic relations between verbs from VerbOcean. Recently, more researchers move on to diverse approaches on the TimeBank-Dense corpus. Chambers et al. (2014) proposed a multi-sieve classifier composed of several rule-based and machine learning based sieves ranked by their precision. Mirza and Tonelli (2016) started to mine the value of low-dimensional word embeddings by concatenating them with traditional sparse feature 1834 vectors to improve their classifier. In"
N18-1166,S15-2134,0,0.0226272,"e relations easily, and increases informativeness of temporal relations. We compare the empirical statistics and automatic recognition results with our data against a previous temporal relation corpus. We also reveal that our data contributes to a significant improvement of the downstream time anchor prediction task, demonstrating 14.1 point increase in overall accuracy. 1 Introduction Temporal information extraction is becoming an active research field in natural language processing (NLP) due to the rapidly growing need for NLP applications such as timeline generation and question answering (Llorens et al., 2015; Meng et al., 2017). It has great potential to create many practical applications. For example, SemEval2015 Task 4 (Minard et al., 2015) collects news articles about a target entity and the task required participants automatically ordering the events involving that entity in a timeline. The timeline representation of news can help people more easily comprehend a mass of information. This work aims to contribute to such timeline applications by extracting temporal information in specific domains like news articles. TimeBank1 (Pustejovsky et al., 2003) is the first widely used corpus with tempo"
N18-1166,P06-1095,0,0.0692724,"Dense) on a subset (36 documents) of TimeBank, which achieved a denser corpus with 6.3 TLINKs per event and time expression, comparing to 0.7 in the original TimeBank corpus. However, it raises the issue that hand-labeling all dense TLINKs is extremely time-consuming and the unclear definition of “salient” is not improved at all. 2.2 Temporal Relation Classification The majority of the temporal relation classifiers focus on exploiting a variety of features to improve the performance in TimeBank. Laokulrat et al. (2013) extracted lexical and morphological features derived from WordNet synsets. Mani et al. (2006); D’Souza and Ng (2013) incorporated semantic relations between verbs from VerbOcean. Recently, more researchers move on to diverse approaches on the TimeBank-Dense corpus. Chambers et al. (2014) proposed a multi-sieve classifier composed of several rule-based and machine learning based sieves ranked by their precision. Mirza and Tonelli (2016) started to mine the value of low-dimensional word embeddings by concatenating them with traditional sparse feature 1834 vectors to improve their classifier. Inspired by the success of the deep learning work in the similar task: relation extraction, Chen"
N18-1166,D17-1092,0,0.0161606,"d increases informativeness of temporal relations. We compare the empirical statistics and automatic recognition results with our data against a previous temporal relation corpus. We also reveal that our data contributes to a significant improvement of the downstream time anchor prediction task, demonstrating 14.1 point increase in overall accuracy. 1 Introduction Temporal information extraction is becoming an active research field in natural language processing (NLP) due to the rapidly growing need for NLP applications such as timeline generation and question answering (Llorens et al., 2015; Meng et al., 2017). It has great potential to create many practical applications. For example, SemEval2015 Task 4 (Minard et al., 2015) collects news articles about a target entity and the task required participants automatically ordering the events involving that entity in a timeline. The timeline representation of news can help people more easily comprehend a mass of information. This work aims to contribute to such timeline applications by extracting temporal information in specific domains like news articles. TimeBank1 (Pustejovsky et al., 2003) is the first widely used corpus with temporal information anno"
N18-1166,C16-1265,0,0.0160741,"oral Relation Classification The majority of the temporal relation classifiers focus on exploiting a variety of features to improve the performance in TimeBank. Laokulrat et al. (2013) extracted lexical and morphological features derived from WordNet synsets. Mani et al. (2006); D’Souza and Ng (2013) incorporated semantic relations between verbs from VerbOcean. Recently, more researchers move on to diverse approaches on the TimeBank-Dense corpus. Chambers et al. (2014) proposed a multi-sieve classifier composed of several rule-based and machine learning based sieves ranked by their precision. Mirza and Tonelli (2016) started to mine the value of low-dimensional word embeddings by concatenating them with traditional sparse feature 1834 vectors to improve their classifier. Inspired by the success of the deep learning work in the similar task: relation extraction, Cheng and Miyao (2017) proposed the shortest dependency path based Bi-directional Long short-term memory (Hochreiter and Schmidhuber, 1997) (Bi-LSTM) to achieve state-of-the-art performance in the TimeBank-Dense corpus, which is adopted for the experiments in this paper. There are two reasons to use this classifier: 1) intersentence temporal relati"
N18-1166,P16-1207,0,0.0815089,"In our first evaluation (Section 4), we compare the correspondence and difference between the new TORDERs and conventional TLINKs. The comparison of empirical statistics shows the new data is label balanced, contains informative relations and reduces “vague” relations. Besides, the classification performance suggests the new data achieve reasonable accuracy, although accuracy numbers are not directly comparable. Many text processing tasks are often requiring to know time anchors when events occurred in a timeline. In Section 5, we evaluate the data in a downstream time anchor prediction task (Reimers et al., 2016) by using the temporal relation recognizers separately trained with TORDERs or TLINKs. The main results show that the recognizer trained with our TORDERs significantly outperforms the recognizer trained with the TLINKs by 14.1 point exact match accuracy. 2 2.1 Background Temporal Relation Annotation TimeBank started a wave of data-driven temporal information extraction research in the NLP community. The original TimeBank only annotated relations judged to be salient by annotators and resulted in sparse annotations. Subsequent TempEval-1,2,3 competitions (Verhagen et al., 2009, 2010; UzZaman et"
N18-1166,D15-1063,0,0.0568309,"Missing"
P03-2033,W97-1506,0,0.0736449,"Missing"
P03-2033,W02-1508,0,0.0189577,"l-purpose grammar through using language intuition encoded in syntactically tagged corpora in XML format. Second, it records data of grammar defects to allow developers to have a whole picture of parsing errors found in the target corpora to save debugging time and effort by prioritizing them. 2 What Is the Ideal Grammar Debugging? There are already other grammar developing tools, such as a grammar writer of XTAG (Paroubek et al., 1992), ALEP (Schmidt et al., 1996), ConTroll (G¨otz and Meurers, 1997), a tool by Nara Institute of Science and Technology (Miyata et al., 1999), and [incr tsdb()] (Oepen et al., 2002). But these tools have following problems; they largely depend on human debuggers’ language intuition, they do not help users to handle large amount of parsing results effectively, and they let human debuggers correct the bugs one after another manually and locally. To cope with these shortcomings, willex proposes an alternative method for more efficient debugging process. The workflow of the conventional grammar developing tools and willex are different in the following ways. With the conventional tools, human debuggers must check each sentence to find out grammar defects and modify them one"
P03-2033,A92-1030,0,0.0317506,"n effort. Hence, we have developed willex that helps to improve the general-purpose grammars. Willex has two major functions. First, it reduces a human workload to improve the general-purpose grammar through using language intuition encoded in syntactically tagged corpora in XML format. Second, it records data of grammar defects to allow developers to have a whole picture of parsing errors found in the target corpora to save debugging time and effort by prioritizing them. 2 What Is the Ideal Grammar Debugging? There are already other grammar developing tools, such as a grammar writer of XTAG (Paroubek et al., 1992), ALEP (Schmidt et al., 1996), ConTroll (G¨otz and Meurers, 1997), a tool by Nara Institute of Science and Technology (Miyata et al., 1999), and [incr tsdb()] (Oepen et al., 2002). But these tools have following problems; they largely depend on human debuggers’ language intuition, they do not help users to handle large amount of parsing results effectively, and they let human debuggers correct the bugs one after another manually and locally. To cope with these shortcomings, willex proposes an alternative method for more efficient debugging process. The workflow of the conventional grammar deve"
P03-2033,C96-1049,0,0.0133803,"oped willex that helps to improve the general-purpose grammars. Willex has two major functions. First, it reduces a human workload to improve the general-purpose grammar through using language intuition encoded in syntactically tagged corpora in XML format. Second, it records data of grammar defects to allow developers to have a whole picture of parsing errors found in the target corpora to save debugging time and effort by prioritizing them. 2 What Is the Ideal Grammar Debugging? There are already other grammar developing tools, such as a grammar writer of XTAG (Paroubek et al., 1992), ALEP (Schmidt et al., 1996), ConTroll (G¨otz and Meurers, 1997), a tool by Nara Institute of Science and Technology (Miyata et al., 1999), and [incr tsdb()] (Oepen et al., 2002). But these tools have following problems; they largely depend on human debuggers’ language intuition, they do not help users to handle large amount of parsing results effectively, and they let human debuggers correct the bugs one after another manually and locally. To cope with these shortcomings, willex proposes an alternative method for more efficient debugging process. The workflow of the conventional grammar developing tools and willex are d"
P03-2033,W98-0141,1,0.819893,"ons. First, it decreases ambiguity of the parsing results by comparing them to an annotated corpus and removing wrong partial results both automatically and manually. Second, willex accumulates parsing errors as data for the developers to clarify the defects of the grammar statistically. We applied willex to a large-scale HPSG-style grammar as an example. 1 Introduction There is an increasing need for syntactical parsers for practical usages, such as information extraction. For example, Yakushiji et al. (2001) extracted argument structures from biomedical papers using a parser based on XHPSG (Tateisi et al., 1998), which is a large-scale HPSG. Although large-scale and general-purpose grammars have been developed, they have a problem of limited coverage. The limits are derived from deficiencies of grammars themselves. For example, XHPSG cannot treat coordinations of verbs (ex. “Molybdate slowed but did not prevent the conversion.”) nor reduced relatives (ex. “Rb mutants derived from patients with retinoblastoma.”). Finding these grammar defects and modifying them require tremendous human effort. Hence, we have developed willex that helps to improve the general-purpose grammars. Willex has two major func"
P03-2033,W98-1118,0,\N,Missing
P03-2033,C00-2102,0,\N,Missing
P03-2033,P02-1060,0,\N,Missing
P04-3017,P98-2132,1,0.723841,"y of Medicine’s MEDLINE database. We deﬁned topical nouns as the names tagged as protein, peptide, amino acid, DNA, RNA, or nucleic acid. We chose PASs which take one or more topical nouns as an argument or arguments, and substrings matched by POS patterns which include topical nouns. All names tagged in the corpus were replaced by their head nouns in order to reduce complexity of sentences and thus reduce the task of the parser and the POS pattern matcher. 4.1 Implementation of PAS method We implemented PAS method on LiLFeS, a uniﬁcation-based programming system for typed feature structures (Makino et al., 1998; Miyao et al., 2000). The selection in Step 2 described in Section 3 is realized by matching PASs with nine PAS templates. Four of the templates are illustrated in Figure 3. 4.2 POS Pattern Method We constructed a POS pattern matcher with a partial verb chunking function according to (Hatzivassiloglou and Weng, 2002). Because the original matcher has problems in recall (its verb group detector has low coverage) and precision (it does not consider other words to detect relations between verb groups and topical nouns), we implemented 2 (a) may be selected if the anaphora (“it”) is resolved. But"
P04-3017,P03-1029,0,0.0362329,"Missing"
P04-3017,C98-2128,1,\N,Missing
P05-1010,A00-2018,0,\N,Missing
P05-1010,J98-4004,0,\N,Missing
P05-1010,W96-0214,0,\N,Missing
P05-1010,C02-1126,0,\N,Missing
P05-1010,J03-4003,0,\N,Missing
P05-1010,P03-1054,0,\N,Missing
P05-1010,N03-1014,0,\N,Missing
P05-1010,P04-1014,0,\N,Missing
P05-1010,W03-3021,0,\N,Missing
P05-1010,W04-3327,0,\N,Missing
P05-1010,P96-1024,0,\N,Missing
P05-1011,J97-4005,0,0.235291,"02; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004b; Kaplan et al., 2004). Previous studies on probabilistic models for HPSG (Toutanova and Manning, 2002; Baldridge and Osborne, 2003; Malouf and van Noord, 2004) also adopted log-linear models. HPSG exploits feature structures to represent linguistic constraints. Such constraints are known 83 Proceedings of the 43rd Annual Meeting of the ACL, pages 83–90, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997). Log-linear models are required for credible probabilistic models and are also beneficial for incorporating various overlapping features. This study follows previous studies on the probabilistic models for HPSG. The probability,  , of producing the parse result  from a given sentence  is defined as           ´ µ                      where    is a reference distribution (usually assumed to be a uniform distribution), and   is a set of parse candidates assigned to . The feature function    represents the characteristic"
P05-1011,W02-2018,0,0.0802697,"ls for HPSG. The probability,  , of producing the parse result  from a given sentence  is defined as           ´ µ                      where    is a reference distribution (usually assumed to be a uniform distribution), and   is a set of parse candidates assigned to . The feature function    represents the characteristics of  and , while the corresponding model parameter    is its weight. Model parameters that maximize the loglikelihood of the training data are computed using a numerical optimization method (Malouf, 2002). Estimation of the above model requires a set of pairs    , where  is the correct parse for sentence . While  is provided by a treebank,   is computed by parsing each  in the treebank. Previous studies assumed   could be enumerated; however, the assumption is impractical because the size of   is exponentially related to the length of . The problem of exponential explosion is inevitable in the wide-coverage parsing of real-world texts because many parse candidates are produced to support various constructions in long sentences. Figure 1: Chart for parsing “he saw a girl wi"
P05-1011,H94-1020,0,0.0768769,"et al., 2004). A large treebank can be used as training and test data for statistical models. Therefore, we now have the basis for the development and the evaluation of statistical disambiguation models for wide-coverage HPSG parsing. Jun’ichi Tsujii Department of Computer Science University of Tokyo Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan CREST, JST tsujii@is.s.u-tokyo.ac.jp The aim of this paper is to report the development of log-linear models for the disambiguation in widecoverage HPSG parsing, and their empirical evaluation through the parsing of the Wall Street Journal of Penn Treebank II (Marcus et al., 1994). This is challenging because the estimation of log-linear models is computationally expensive, and we require solutions to make the model estimation tractable. We apply two techniques for reducing the training cost. One is the estimation on a packed representation of HPSG parse trees (Section 3). The other is the filtering of parse candidates according to a preliminary probability distribution (Section 4). To our knowledge, this work provides the first results of extensive experiments of parsing Penn Treebank with a probabilistic HPSG. The results from the Wall Street Journal are significant"
P05-1011,W03-0403,0,0.0962518,"t because the complexity of the sentences is different from that of short sentences. Experiments of the parsing of realworld sentences can properly evaluate the effectiveness and possibility of parsing models for HPSG. 2 Disambiguation models for HPSG Discriminative log-linear models are now becoming a de facto standard for probabilistic disambiguation models for deep parsing (Johnson et al., 1999; Riezler et al., 2002; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004b; Kaplan et al., 2004). Previous studies on probabilistic models for HPSG (Toutanova and Manning, 2002; Baldridge and Osborne, 2003; Malouf and van Noord, 2004) also adopted log-linear models. HPSG exploits feature structures to represent linguistic constraints. Such constraints are known 83 Proceedings of the 43rd Annual Meeting of the ACL, pages 83–90, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997). Log-linear models are required for credible probabilistic models and are also beneficial for incorporating various overlapping features. This study follows previous studies on the probabilistic"
P05-1011,C04-1041,0,0.577901,"tensive experiments of parsing Penn Treebank with a probabilistic HPSG. The results from the Wall Street Journal are significant because the complexity of the sentences is different from that of short sentences. Experiments of the parsing of realworld sentences can properly evaluate the effectiveness and possibility of parsing models for HPSG. 2 Disambiguation models for HPSG Discriminative log-linear models are now becoming a de facto standard for probabilistic disambiguation models for deep parsing (Johnson et al., 1999; Riezler et al., 2002; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004b; Kaplan et al., 2004). Previous studies on probabilistic models for HPSG (Toutanova and Manning, 2002; Baldridge and Osborne, 2003; Malouf and van Noord, 2004) also adopted log-linear models. HPSG exploits feature structures to represent linguistic constraints. Such constraints are known 83 Proceedings of the 43rd Annual Meeting of the ACL, pages 83–90, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997). Log-linear models are required for credible probabilistic mod"
P05-1011,P04-1014,0,0.728437,"tensive experiments of parsing Penn Treebank with a probabilistic HPSG. The results from the Wall Street Journal are significant because the complexity of the sentences is different from that of short sentences. Experiments of the parsing of realworld sentences can properly evaluate the effectiveness and possibility of parsing models for HPSG. 2 Disambiguation models for HPSG Discriminative log-linear models are now becoming a de facto standard for probabilistic disambiguation models for deep parsing (Johnson et al., 1999; Riezler et al., 2002; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004b; Kaplan et al., 2004). Previous studies on probabilistic models for HPSG (Toutanova and Manning, 2002; Baldridge and Osborne, 2003; Malouf and van Noord, 2004) also adopted log-linear models. HPSG exploits feature structures to represent linguistic constraints. Such constraints are known 83 Proceedings of the 43rd Annual Meeting of the ACL, pages 83–90, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997). Log-linear models are required for credible probabilistic mod"
P05-1011,C02-2025,0,0.045094,"of log-linear models requires high computational cost, especially with widecoverage grammars. Using techniques to reduce the estimation cost, we trained the models using 20 sections of Penn Treebank. A series of experiments empirically evaluated the estimation techniques, and also examined the performance of the disambiguation models on the parsing of real-world sentences. 1 Introduction Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) has been studied extensively from both linguistic and computational points of view. However, despite research on HPSG processing efficiency (Oepen et al., 2002a), the application of HPSG parsing is still limited to specific domains and short sentences (Oepen et al., 2002b; Toutanova and Manning, 2002). Scaling up HPSG parsing to assess real-world texts is an emerging research field with both theoretical and practical applications. Recently, a wide-coverage grammar and a large treebank have become available for English HPSG (Miyao et al., 2004). A large treebank can be used as training and test data for statistical models. Therefore, we now have the basis for the development and the evaluation of statistical disambiguation models for wide-coverage HP"
P05-1011,C00-1085,0,0.0745644,"rst assigned to each word a small number of supertags, which correspond to lexical entries in our case, and parsed supertagged sentences. Since they did not mention the probabilities of supertags, their method corresponds to our “filtering only” method. However, they also applied the same supertagger in a parsing stage, and this seemed to be crucial for high accuracy. This means that they estimated the probability of producing a parse tree from a supertagged sentence. Another approach to estimating log-linear models for HPSG is to extract a small informative sample from the original set   (Osborne, 2000). Malouf and van Noord (2004) successfully applied this method to German HPSG. The problem with this method was in the approximation of exponentially many parse trees by a polynomial-size sample. However, their method has the advantage that any features on a parse tree can be incorporated into the model. The trade-off between approximation and locality of features is an outstanding problem. Other discriminative classifiers were applied to the disambiguation in HPSG parsing (Baldridge and Osborne, 2003; Toutanova et al., 2004). The problem of exponential explosion is also inevitable for 7 Discu"
P05-1011,P02-1036,0,0.530025,"ledge, this work provides the first results of extensive experiments of parsing Penn Treebank with a probabilistic HPSG. The results from the Wall Street Journal are significant because the complexity of the sentences is different from that of short sentences. Experiments of the parsing of realworld sentences can properly evaluate the effectiveness and possibility of parsing models for HPSG. 2 Disambiguation models for HPSG Discriminative log-linear models are now becoming a de facto standard for probabilistic disambiguation models for deep parsing (Johnson et al., 1999; Riezler et al., 2002; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004b; Kaplan et al., 2004). Previous studies on probabilistic models for HPSG (Toutanova and Manning, 2002; Baldridge and Osborne, 2003; Malouf and van Noord, 2004) also adopted log-linear models. HPSG exploits feature structures to represent linguistic constraints. Such constraints are known 83 Proceedings of the 43rd Annual Meeting of the ACL, pages 83–90, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997). Log-linear mo"
P05-1011,P02-1035,0,0.153978,"ection 4). To our knowledge, this work provides the first results of extensive experiments of parsing Penn Treebank with a probabilistic HPSG. The results from the Wall Street Journal are significant because the complexity of the sentences is different from that of short sentences. Experiments of the parsing of realworld sentences can properly evaluate the effectiveness and possibility of parsing models for HPSG. 2 Disambiguation models for HPSG Discriminative log-linear models are now becoming a de facto standard for probabilistic disambiguation models for deep parsing (Johnson et al., 1999; Riezler et al., 2002; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004b; Kaplan et al., 2004). Previous studies on probabilistic models for HPSG (Toutanova and Manning, 2002; Baldridge and Osborne, 2003; Malouf and van Noord, 2004) also adopted log-linear models. HPSG exploits feature structures to represent linguistic constraints. Such constraints are known 83 Proceedings of the 43rd Annual Meeting of the ACL, pages 83–90, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Ab"
P05-1011,A00-2021,0,0.233003,"ween the parsing cost and the accuracy will be examined experimentally. We have several ways to integrate with the estimated model   . In the experiments, we will empirically compare the following methods in terms of accuracy and estimation time. Filtering only The unigram probability only for filtering. is used Product The probability is defined as the product of and the estimated model . Reference distribution tribution of . is used as a reference disFeature function  is used as a feature function of . This method was shown to be a generalization of the reference distribution method (Johnson and Riezler, 2000). 5 Features Feature functions in the log-linear models are designed to capture the characteristics of      . In this paper, we investigate combinations of the atomic features listed in Table 1. The following combinations are used for representing the characteristics of the binary/unary schema applications. binary   RULE,DIST,COMMA  SPAN  SYM  WORD  POS  LE   SPAN   SYM   WORD   POS   LE  unary  RULE,SYM,WORD,POS,LE  In addition, the following is for expressing the condition of the root node of the parse tree. root  SYM,WORD,POS,LE  86 Figure 4: Example feat"
P05-1011,W04-3201,0,0.037174,"Missing"
P05-1011,P99-1069,0,0.344409,"bility distribution (Section 4). To our knowledge, this work provides the first results of extensive experiments of parsing Penn Treebank with a probabilistic HPSG. The results from the Wall Street Journal are significant because the complexity of the sentences is different from that of short sentences. Experiments of the parsing of realworld sentences can properly evaluate the effectiveness and possibility of parsing models for HPSG. 2 Disambiguation models for HPSG Discriminative log-linear models are now becoming a de facto standard for probabilistic disambiguation models for deep parsing (Johnson et al., 1999; Riezler et al., 2002; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004b; Kaplan et al., 2004). Previous studies on probabilistic models for HPSG (Toutanova and Manning, 2002; Baldridge and Osborne, 2003; Malouf and van Noord, 2004) also adopted log-linear models. HPSG exploits feature structures to represent linguistic constraints. Such constraints are known 83 Proceedings of the 43rd Annual Meeting of the ACL, pages 83–90, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics to introduce inconsistencies in probabilistic models estimated using simple"
P05-1011,W02-2030,0,0.211656,"on cost, we trained the models using 20 sections of Penn Treebank. A series of experiments empirically evaluated the estimation techniques, and also examined the performance of the disambiguation models on the parsing of real-world sentences. 1 Introduction Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) has been studied extensively from both linguistic and computational points of view. However, despite research on HPSG processing efficiency (Oepen et al., 2002a), the application of HPSG parsing is still limited to specific domains and short sentences (Oepen et al., 2002b; Toutanova and Manning, 2002). Scaling up HPSG parsing to assess real-world texts is an emerging research field with both theoretical and practical applications. Recently, a wide-coverage grammar and a large treebank have become available for English HPSG (Miyao et al., 2004). A large treebank can be used as training and test data for statistical models. Therefore, we now have the basis for the development and the evaluation of statistical disambiguation models for wide-coverage HPSG parsing. Jun’ichi Tsujii Department of Computer Science University of Tokyo Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan CREST, JST tsujii@is.s.u-to"
P05-1011,N04-1013,0,0.367443,"rsing Penn Treebank with a probabilistic HPSG. The results from the Wall Street Journal are significant because the complexity of the sentences is different from that of short sentences. Experiments of the parsing of realworld sentences can properly evaluate the effectiveness and possibility of parsing models for HPSG. 2 Disambiguation models for HPSG Discriminative log-linear models are now becoming a de facto standard for probabilistic disambiguation models for deep parsing (Johnson et al., 1999; Riezler et al., 2002; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004b; Kaplan et al., 2004). Previous studies on probabilistic models for HPSG (Toutanova and Manning, 2002; Baldridge and Osborne, 2003; Malouf and van Noord, 2004) also adopted log-linear models. HPSG exploits feature structures to represent linguistic constraints. Such constraints are known 83 Proceedings of the 43rd Annual Meeting of the ACL, pages 83–90, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997). Log-linear models are required for credible probabilistic models and are also benefi"
P05-1011,A00-2018,0,\N,Missing
P05-1011,J03-4003,0,\N,Missing
P06-1059,W04-1221,0,0.154531,"Missing"
P06-1059,M95-1002,0,0.046288,"Missing"
P06-1059,W05-1514,1,0.831367,"Missing"
P06-1059,A97-1029,0,0.0419464,"Missing"
P06-1059,W04-1219,0,0.184969,"Missing"
P06-1059,W04-1217,0,0.0945932,"Missing"
P06-1059,P05-1045,0,0.0374246,"Missing"
P06-1059,W04-1213,0,0.505862,"Missing"
P06-1059,I05-1057,0,0.131398,"Missing"
P06-1128,P05-1022,0,0.00458241,"atabases. For biomedical terms other than genes/gene products, the Unified Medical Language System (UMLS) meta-thesaurus (Lindberg et al., 1993) is a large database that contains various names of biomedical and health-related concepts. Ontology databases provide mappings between textual expressions and entities in the real world. For example, Table 1 indicates that CRP, MGC88244, and PTX1 denote the same gene conceptually. Hence, these resources enable us to canonicalize variations of textual expressions of ontological entities. 2.2 Parsing technologies Recently, state-of-the-art CFG parsers (Charniak and Johnson, 2005) can compute phrase structures of natural sentences at fairly high accuracy. These parsers have been used in various NLP tasks including IE and text mining. In addition, parsers that compute deeper analyses, such as predicate argument structures, have become available for 1018 the processing of real-world sentences (Miyao and Tsujii, 2005). Predicate argument structures are canonicalized representations of sentence meanings, and express the semantic relations of words explicitly. Figure 1 shows an output of an HPSG parser (Miyao and Tsujii, 2005) for the sentence “A normal serum CRP measuremen"
P06-1128,I05-1018,1,0.746453,"aphies of articles, about half of which have abstracts. Research on IE and text mining in biomedical science has focused mainly on MEDLINE. In the present paper, we target all articles indexed in MEDLINE at the end of 2004 (14,785,094 articles). The following sections explain in detail off-/on-line processing for the text retrieval system for MEDLINE. 3.1 Off-line processing: HPSG parsing and term recognition We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures. Because our target is biomedical texts, we re-trained a parser (Hara et al., 2005) with the GENIA treebank (Tateisi et al., 2005), and also applied a bidirectional part-ofspeech tagger (Tsuruoka and Tsujii, 2005) trained with the GENIA treebank as a preprocessor. Because parsing speed is still unrealistic for parsing the entire MEDLINE on a single machine, we used two geographically separated computer clusters having 170 nodes (340 Xeon CPUs). These clusters are separately administered and not dedicated for use in the present study. In order to effectively use such an environment, GXP (Taura, 2004) was used to connect these clusters and distribute the load among them. Our p"
P06-1128,W04-3102,0,0.0566302,"nd: Resources and Tools for Semantic Annotations The proposed system for the retrieval of relational concepts is a product of recent developments in NLP resources and tools. In this section, ontology databases, deep parsers, and search algorithms for structured data are introduced. 2.1 Ontology databases Ontology databases are collections of words and phrases in specific domains. Such databases have been constructed extensively for the systematic management of domain knowledge by organizing textual expressions of ontological entities that are detached from actual sentences. For example, GENA (Koike and Takagi, 2004) is a database of genes and gene products that is semi-automatically collected from well-known databases, including HUGO, OMIM, Genatlas, Locuslink, GDB, MGI, FlyBase, WormBase, Figure 1: An output of HPSG parsing Figure 2: A predicate argument structure CYGD, and SGD. Table 1 shows an example of a GENA entry. “Symbol” and “Name” denote short forms and nomenclatures of genes, respectively. “Species” represents the organism species in which this gene is observed. “Synonym” is a list of synonyms and name variations. “Product” gives a list of products of this gene, such as proteins coded by this"
P06-1128,P05-1011,1,0.829945,"advance with semantic structures and are stored in a structured database. User requests are converted on the fly into patterns of these semantic annotations, and texts are retrieved by matching these patterns with the pre-computed semantic annotations. The accurate retrieval of relational concepts is attained because we can precisely describe relational concepts using semantic annotations. In addition, real-time retrieval is possible because semantic annotations are computed in advance. This framework has been implemented for a text retrieval system for MEDLINE. We first apply a deep parser (Miyao and Tsujii, 2005) and a dictionary-based term recognizer (Tsuruoka and Tsujii, 2004) to MEDLINE and obtain annotations of predicate argument structures and ontological identifiers of genes, gene products, diseases, and events. We then provide a search engine for these annotated sentences. User requests are converted into queries of region algebra (Clarke et al., 1995) extended with variables (Masuda et al., 2006) on these annotations. A search engine for the extended region algebra efficiently finds sentences having semantic annotations that match the input queries. In this paper, we evaluate this system with"
P06-1128,I05-2038,1,0.581796,"abstracts. Research on IE and text mining in biomedical science has focused mainly on MEDLINE. In the present paper, we target all articles indexed in MEDLINE at the end of 2004 (14,785,094 articles). The following sections explain in detail off-/on-line processing for the text retrieval system for MEDLINE. 3.1 Off-line processing: HPSG parsing and term recognition We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures. Because our target is biomedical texts, we re-trained a parser (Hara et al., 2005) with the GENIA treebank (Tateisi et al., 2005), and also applied a bidirectional part-ofspeech tagger (Tsuruoka and Tsujii, 2005) trained with the GENIA treebank as a preprocessor. Because parsing speed is still unrealistic for parsing the entire MEDLINE on a single machine, we used two geographically separated computer clusters having 170 nodes (340 Xeon CPUs). These clusters are separately administered and not dedicated for use in the present study. In order to effectively use such an environment, GXP (Taura, 2004) was used to connect these clusters and distribute the load among them. Our processes were given the lowest priority so that"
P06-1128,H05-1059,1,0.143897,"ainly on MEDLINE. In the present paper, we target all articles indexed in MEDLINE at the end of 2004 (14,785,094 articles). The following sections explain in detail off-/on-line processing for the text retrieval system for MEDLINE. 3.1 Off-line processing: HPSG parsing and term recognition We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures. Because our target is biomedical texts, we re-trained a parser (Hara et al., 2005) with the GENIA treebank (Tateisi et al., 2005), and also applied a bidirectional part-ofspeech tagger (Tsuruoka and Tsujii, 2005) trained with the GENIA treebank as a preprocessor. Because parsing speed is still unrealistic for parsing the entire MEDLINE on a single machine, we used two geographically separated computer clusters having 170 nodes (340 Xeon CPUs). These clusters are separately administered and not dedicated for use in the present study. In order to effectively use such an environment, GXP (Taura, 2004) was used to connect these clusters and distribute the load among them. Our processes were given the lowest priority so that our task would not disturb other users. We finished parsing the entire MEDLINE in"
P06-2091,copestake-flickinger-2000-open,0,0.0261048,"Missing"
P06-2091,hockenmaier-steedman-2002-acquiring,0,0.0768649,"Missing"
P06-2091,C04-1180,0,\N,Missing
P06-2091,H94-1020,0,\N,Missing
P06-2109,J96-1002,0,0.00824673,"th of the compressed sentence; that is, they divide the log probability by the length of the compressed sentence. Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. However their model also has the same problem. McDonald (McDonald, 2006) independently proposed a new machine learning approach. He does not trim input parse trees but uses rich features about syntactic trees and improved performance. 2.2 count (joint(rl , rs )) . count(rs ) Maximum Entropy Model The maximum entropy model (Berger et al., 1996) estimates a probability distribution from training data. The model creates the most “uniform” distribution within the constraints given by users. The distribution with the maximum entropy is considered the most uniform. Given two finite sets of event variables, X and Y, we estimate their joint probability distribution, P (x, y). An output, y (∈ Y), is produced, and Finally, new subtrees grow from new daughter nodes in each expanded node. In Figure 1, (E (G g) (H h)) grows from E. The PCFG scores, Pcfg , of these subtrees are calculated. Then, each probability is assumed to be independent of t"
P06-2109,P05-1022,0,0.0297862,"Missing"
P06-2109,W03-0501,0,0.0742534,"hat our method produced more grammatical and informative compressed sentences than other methods. 1 Introduction In most automatic summarization approaches, text is summarized by extracting sentences from a given document without modifying the sentences themselves. Although these methods have been significantly improved to extract good sentences as summaries, they are not intended to shorten sentences; i.e., the output often has redundant words or phrases. These methods cannot be used to make a shorter sentence from an input sentence or for other applications such as generating headline news (Dorr et al., 2003) or messages for the small screens of mobile devices. We need to compress sentences to obtain short and useful summaries. 850 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 850–857, c Sydney, July 2006. 2006 Association for Computational Linguistics 2 Background 2.1 A The Noisy-Channel Model for Sentence Compression B D Knight and Marcu proposed a sentence compression method using a noisy-channel model (Knight and Marcu, 2000). This model assumes that a long sentence was originally a short one and that the longer sentence was generated because some unnecessary words"
P06-2109,A00-2023,0,0.0139546,"tree contains rs = (B → D F ). They assume that rs was expanded into rl , and count the node pairs as joint events. The expansion probability of two rules is given by: Pexpand (rl |rs ) = d A C E F G H g h c f B C D F d f c Figure 1: Examples of original and compressed parse trees. subtrees: P (l|s) = Y Pexpand (rl |rs ) · (rl ,rs )∈R Y Pcfg (r), r∈R0 where R is the set of rule pairs, and R 0 is the set of generation rules in new subtrees. To compress an input sentence, they create a tree with the highest score of all possible trees. They pack all possible trees in a shared-forest structure (Langkilde, 2000). The forest structure is represented by an AND-OR tree, and it contains many tree structures. The forest representation saves memory and makes calculation faster because the trees share sub structures, and this can reduce the total number of calculations. They normalize each log probability using the length of the compressed sentence; that is, they divide the log probability by the length of the compressed sentence. Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. However their mode"
P06-2109,W04-1013,0,0.0103954,"om the tree of Figure 2, (S (S · · · ) (, , ) (NP I) (VP said) (. .)), a new tree, (S (S · · · ) (. .)), is extracted. However, the rule (S → S .) is ungrammatical. 4 Experiment 4.1 Evaluation Method We evaluated each sentence compression method using word F -measures, bigram F -measures, and B LEU scores (Papineni et al., 2002). B LEU scores are usually used for evaluating machine translation quality. A B LEU score is defined as the weighted geometric average of n-gram precisions with length penalties. We used from unigram to 4-gram precisions and uniform weights for the B LEU scores. ROUGE (Lin, 2004) is a set of recall-based criteria that is mainly used for evaluating summarization tasks. ROUGE -N uses average N-gram recall, and ROUGE -1 is word recall. ROUGE -L uses the length of the longest common subsequence (LCS) of the original and summarized sentences. In our model, the length of the LCS is equal to the number of common words, and ROUGE -L is equal to the unigram F -measure because words are not rearranged. ROUGE -L and ROUGE -1 are supposed to be appropriate for the headline gener853 ation task (Lin, 2004). This is not our task, but it is the most similar task in his paper. We also"
P06-2109,E06-1038,0,0.118005,"y an AND-OR tree, and it contains many tree structures. The forest representation saves memory and makes calculation faster because the trees share sub structures, and this can reduce the total number of calculations. They normalize each log probability using the length of the compressed sentence; that is, they divide the log probability by the length of the compressed sentence. Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. However their model also has the same problem. McDonald (McDonald, 2006) independently proposed a new machine learning approach. He does not trim input parse trees but uses rich features about syntactic trees and improved performance. 2.2 count (joint(rl , rs )) . count(rs ) Maximum Entropy Model The maximum entropy model (Berger et al., 1996) estimates a probability distribution from training data. The model creates the most “uniform” distribution within the constraints given by users. The distribution with the maximum entropy is considered the most uniform. Given two finite sets of event variables, X and Y, we estimate their joint probability distribution, P (x,"
P06-2109,P02-1040,0,0.0729168,"VP VP . never think so . never think so Figure 2: Example of parse tree pair that cannot be matched. A A B D d C E F G H g h c f B C D E d G c g Figure 3: Example of bottom-up method. Note that this “tree” is not guaranteed to be a grammatical “parse tree” by the CFG grammar. For example, from the tree of Figure 2, (S (S · · · ) (, , ) (NP I) (VP said) (. .)), a new tree, (S (S · · · ) (. .)), is extracted. However, the rule (S → S .) is ungrammatical. 4 Experiment 4.1 Evaluation Method We evaluated each sentence compression method using word F -measures, bigram F -measures, and B LEU scores (Papineni et al., 2002). B LEU scores are usually used for evaluating machine translation quality. A B LEU score is defined as the weighted geometric average of n-gram precisions with length penalties. We used from unigram to 4-gram precisions and uniform weights for the B LEU scores. ROUGE (Lin, 2004) is a set of recall-based criteria that is mainly used for evaluating summarization tasks. ROUGE -N uses average N-gram recall, and ROUGE -1 is word recall. ROUGE -L uses the length of the longest common subsequence (LCS) of the original and summarized sentences. In our model, the length of the LCS is equal to the numb"
P06-2109,P05-1036,0,0.550148,"n input sentence, they create a tree with the highest score of all possible trees. They pack all possible trees in a shared-forest structure (Langkilde, 2000). The forest structure is represented by an AND-OR tree, and it contains many tree structures. The forest representation saves memory and makes calculation faster because the trees share sub structures, and this can reduce the total number of calculations. They normalize each log probability using the length of the compressed sentence; that is, they divide the log probability by the length of the compressed sentence. Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. However their model also has the same problem. McDonald (McDonald, 2006) independently proposed a new machine learning approach. He does not trim input parse trees but uses rich features about syntactic trees and improved performance. 2.2 count (joint(rl , rs )) . count(rs ) Maximum Entropy Model The maximum entropy model (Berger et al., 1996) estimates a probability distribution from training data. The model creates the most “uniform” distribution within the constraints given by u"
P06-2109,W04-1015,0,0.0632249,"Missing"
P06-4005,P05-1011,1,0.777319,"Missing"
P06-4005,I05-1018,1,0.813084,"eather conditions forced them to scrub Monday’s scheduled return.” 3 MEDIE: a search engine for MEDLINE Figure 2 shows the top page of the MEDIE. MEDIE is an intelligent search engine for the accurate retrieval of relational concepts from MEDLINE 2 (Miyao et al., 2006). Prior to retrieval, all sentences are annotated with predicate argument structures and ontological identifiers by applying Enju and a term recognizer. 3.1 Automatically Annotated Corpus First, we applied a POS analyzer and then Enju. The POS analyzer and HPSG parser are trained by using the GENIA corpus (Tsuruoka et al., 2005; Hara et al., 2005), which comprises around 2,000 MEDLINE abstracts annotated with POS and Penn Treebank style syntactic parse trees (Tateisi et al., 2005). The HPSG parser generates parse trees in a stand-off format that can be converted to XML by combining it with the original text. We also annotated technical terms of genes and diseases in our developed corpus. Technical terms are annotated simply by exact matching of dictio2 Functions of MEDIE 4 Info-PubMed: a GUI-based MEDLINE search tool Info-PubMed is a MEDLINE search tool with GUI, helping users to find information about biomedical entities such as genes"
P06-4005,W05-1511,1,0.867094,"Missing"
P06-4005,I05-2038,1,0.900999,"e predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MOD, ARG1, ..., ARG4), and wa is the head word of the argument. Precision/recall is the ratio of tuples correctly identified by the parser. The lexicon of the grammar was extracted from Sections 02-21 of Penn Treebank (39,832 sentences). In the table, ‘HPSG-PTB’ means that the statistical model was trained on Penn Treebank. ‘HPSG-GENIA’ means that the statistical model was trained on both Penn Treebank and GENIA treebank as described in (Hara et al., 2005). The GENIA treebank (Tateisi et al., 2005) consists of 500 abstracts (4,446 sentences) extracted from MEDLINE. Figure 1 shows a part of the parse tree and feaRecently, biomedical researchers have been facing the vast repository of research papers, e.g. MEDLINE. These researchers are eager to search biomedical correlations such as protein-protein or gene-disease associations. The use of natural language processing technology is expected to reduce their burden, and various attempts of information extraction using NLP has been being made (Blaschke and Valencia, 2002; Hao et al., 2005; Chun et al., 2006). However, the framework of traditi"
P06-4005,W04-3102,0,0.0153543,"of Informatics, Kogakuin University ¶ Information Technology Center, University of Tokyo † 1 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/ 17 Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 17–20, c Sydney, July 2006. 2006 Association for Computational Linguistics nary entries and the terms separated by space, tab, period, comma, hat, colon, semi-colon, brackets, square brackets and slash in MEDLINE. The entire dictionary was generated by applying the automatic generation method of name variations (Tsuruoka and Tsujii, 2004) to the GENA dictionary for the gene names (Koike and Takagi, 2004) and the UMLS (Unified Medical Language System) meta-thesaurus for the disease names (Lindberg et al., 1993). It was generated by applying the name-variation generation method, and we obtained 4,467,855 entries of a gene and disease dictionary. 3.2 MEDIE provides three types of search, semantic search, keyword search, GCL search. GCL search provides us the most fundamental and powerful functions in which users can specify the boolean relations, linear order relation and structural relations with variables. Trained users can enjoy all functions in MEDIE by the GCL search, but it is not easy for"
P06-4005,P06-1128,1,\N,Missing
P07-1079,J99-2004,0,0.09211,"minutes, since two dependency parsers are used sequentially. 5 Related work There are other approaches that combine shallow processing with deep parsing (Crysmann et al., 2002; Frank et al., 2003; Daum et al., 2003) to improve parsing efficiency. Typically, shallow parsing is used to create robust minimal recursion semantics, which are used as constraints to limit ambiguity during parsing. Our approach, in contrast, uses syntactic dependencies to achieve a significant improvement in the accuracy of wide-coverage HPSG parsing. Additionally, our approach is in many ways similar to supertagging (Bangalore and Joshi, 1999), which uses sequence labeling techniques as an efficient way to pre-compute parsing constraints (specifically, the assignment of lexical entries to input words). 6 Conclusion We have presented a novel framework for taking advantage of the strengths of a shallow parsing approach and a deep parsing approach. We have shown that by constraining the application of rules in HPSG parsing according to results from a dependency parser, we can significantly improve the accuracy of deep parsing by using shallow syntactic analyses. To illustrate how this framework allows for improvements in the accuracy"
P07-1079,W06-2920,0,0.0263757,"uistically sophisticated formalism. In addition, improvements in dependency parsing accuracy are converted directly into improvements in HPSG parsing accuracy. From the point of view of dependency parsing, the application of HPSG rules to structures generated by a surface dependency model provides a principled and linguistically motivated way to identify deep syntactic phenomena, such as long-distance dependencies, raising and control. Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar We begin by describing our dependency and (LFG) (Bresnan, 1982), Lexicalized Tree Adjoin- HPSG parsing approaches in section 2."
P07-1079,E03-1052,0,0.0224732,"Missing"
P07-1079,C96-1058,0,0.019801,"directly into improvements in HPSG parsing accuracy. From the point of view of dependency parsing, the application of HPSG rules to structures generated by a surface dependency model provides a principled and linguistically motivated way to identify deep syntactic phenomena, such as long-distance dependencies, raising and control. Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar We begin by describing our dependency and (LFG) (Bresnan, 1982), Lexicalized Tree Adjoin- HPSG parsing approaches in section 2. In section ing Grammar (LTAG) (Schabes et al., 1988), Head- 3, we present our framework for HPS"
P07-1079,P03-1014,0,0.0366287,"Missing"
P07-1079,P06-1088,0,0.0377423,"Missing"
P07-1079,W02-2018,0,0.00702209,"s to lexical/phrasal structures, where L = hl1 , . . . , ln i are lexical entries and 625 p(li |W ) is the supertagging probability, i.e., the probability of assignining the lexical entry li to wi (Ninomiya et al., 2006). The probability p(T |L, W ) is a maximum entropy model on HPSG parse trees, where Z is a normalization factor, and feature functions fi (T ) represent syntactic characteristics, such as head words, lengths of phrases, and applied schemas. Given the HPSG treebank as training data, the model parameters λi are estimated so as to maximize the log-likelihood of the training data (Malouf, 2002). 3 HPSG parsing with dependency constraints While a number of fairly straightforward models can be applied successfully to dependency parsing, designing and training HPSG parsing models has been regarded as a significantly more complex task. Although it seems intuitive that a more sophisticated linguistic formalism should be more difficult to parameterize properly, we argue that the difference in complexity between HPSG and dependency structures can be seen as incremental, and that the use of accurate and efficient techniques to determine the surface dependency structure of a sentence provide"
P07-1079,J93-2004,0,0.0326234,"encies that roughly correspond to deep syntax. The second step is to perform HPSG parsing, as described in section 2.2, but using the shallow dependency tree to constrain the application of HPSG rules. We now discuss these two steps in more detail. 3.1 Determining shallow dependencies in HPSG structures using dependency parsing In order to apply a data-driven dependency approach to the task of identifying the shallow dependency tree in HPSG structures, we first need a corpus of such dependency trees to serve as training data. We created a dependency training corpus based on the Penn Treebank (Marcus et al., 1993), or more specifically on the HPSG Treebank generated from the Penn Treebank (see section 2.2). For each HPSG structure in the HPSG Treebank, a dependency tree is extracted in two steps. First, the HPSG tree is converted into a CFG-style tree, simply by removing long-distance dependency links between nodes. A dependency tree is then extracted from the resulting lexicalized CFG-style tree, as is commonly done for converting constituent trees into dependency trees after the application of a headpercolation table (Collins, 1999). Once a dependency training corpus is available, it is used to train"
P07-1079,H05-1066,0,0.208417,"a more complex and linguistically sophisticated formalism. In addition, improvements in dependency parsing accuracy are converted directly into improvements in HPSG parsing accuracy. From the point of view of dependency parsing, the application of HPSG rules to structures generated by a surface dependency model provides a principled and linguistically motivated way to identify deep syntactic phenomena, such as long-distance dependencies, raising and control. Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar We begin by describing our dependency and (LFG) (Bresnan, 1982), Lexicalized Tree Adjoin- HPSG parsin"
P07-1079,P05-1011,1,0.918834,"accurate, but also efficient. The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz, 2004) has been shown to offer state-of-the-art accuracy (Nivre et al., 2006) with high efficiency due to a greedy search strategy. Our approach is based on Nivre and Scholz’s approach, using support vector machines for classification of shift/reduce actions. 2.2 Wide-coverage HPSG parsing Figure 2: Extracting HPSG lexical entries from the Penn Treebank we finally obtain an HPSG parse tree that covers the entire sentence. In this paper, we use an HPSG parser developed by Miyao and Tsujii (2005). This parser has a widecoverage HPSG lexicon which is extracted from the Penn Treebank. Figure 2 illustrates their method for extraction of HPSG lexical entries. First, given a parse tree from the Penn Treebank (top), HPSGstyle constraints are added and an HPSG-style parse tree is obtained (middle). Lexical entries are then extracted from the terminal nodes of the HPSG parse tree (bottom). This way, in addition to a widecoverage lexicon, we also obtain an HPSG treebank, which can be used as training data for disambiguation models. The disambiguation model of this parser is based on a maximum"
P07-1079,W06-1619,1,0.853739,"xical entries express subcategorization frames and predicate argument structures. Parsing proceeds by app(T |W ) = p(T |L, W )p(L|W ) plying schemas to lexical entries. In this example, ! X Y the Head-Complement Schema is applied to the lex1 = exp λ f (T ) p(lj |W ), i i ical entries of “tried” and “running”. We then obtain Z i j a phrasal structure for “tried running”. By repeatedly applying schemas to lexical/phrasal structures, where L = hl1 , . . . , ln i are lexical entries and 625 p(li |W ) is the supertagging probability, i.e., the probability of assignining the lexical entry li to wi (Ninomiya et al., 2006). The probability p(T |L, W ) is a maximum entropy model on HPSG parse trees, where Z is a normalization factor, and feature functions fi (T ) represent syntactic characteristics, such as head words, lengths of phrases, and applied schemas. Given the HPSG treebank as training data, the model parameters λi are estimated so as to maximize the log-likelihood of the training data (Malouf, 2002). 3 HPSG parsing with dependency constraints While a number of fairly straightforward models can be applied successfully to dependency parsing, designing and training HPSG parsing models has been regarded as"
P07-1079,C04-1010,0,0.410319,"dapting these models to a more complex and linguistically sophisticated formalism. In addition, improvements in dependency parsing accuracy are converted directly into improvements in HPSG parsing accuracy. From the point of view of dependency parsing, the application of HPSG rules to structures generated by a surface dependency model provides a principled and linguistically motivated way to identify deep syntactic phenomena, such as long-distance dependencies, raising and control. Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar We begin by describing our dependency and (LFG) (Bresnan, 1982), Lexicalized T"
P07-1079,W06-2933,0,0.00677943,"Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics Figure 1: HPSG parsing evaluate this framework empirically. Sections 5 and 6 discuss related work and conclusions. 2 2.1 Fast dependency parsing and wide-coverage HPSG parsing Data-driven dependency parsing Because we use dependency parsing as a step in deep parsing, it is important that we choose a parsing approach that is not only accurate, but also efficient. The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz, 2004) has been shown to offer state-of-the-art accuracy (Nivre et al., 2006) with high efficiency due to a greedy search strategy. Our approach is based on Nivre and Scholz’s approach, using support vector machines for classification of shift/reduce actions. 2.2 Wide-coverage HPSG parsing Figure 2: Extracting HPSG lexical entries from the Penn Treebank we finally obtain an HPSG parse tree that covers the entire sentence. In this paper, we use an HPSG parser developed by Miyao and Tsujii (2005). This parser has a widecoverage HPSG lexicon which is extracted from the Penn Treebank. Figure 2 illustrates their method for extraction of HPSG lexical entries. First, given a"
P07-1079,W05-1513,1,0.770179,"onverted into a CFG-style tree, simply by removing long-distance dependency links between nodes. A dependency tree is then extracted from the resulting lexicalized CFG-style tree, as is commonly done for converting constituent trees into dependency trees after the application of a headpercolation table (Collins, 1999). Once a dependency training corpus is available, it is used to train a dependency parser as described in section 2.1. This is done by training a classifier to determine parser actions based on local features that represent the current state of the parser (Nivre and Scholz, 2004; Sagae and Lavie, 2005). Training data for the classifier is obtained by applying the parsing algorithm over the training sentences (for which the correct dependency structures are known) and recording the appropriate parser actions that result in the formation of the correct dependency trees, coupled with the features that represent the state of the parser mentioned in section 2.1. An evaluation of the resulting dependency parser and its efficacy in aiding HPSG parsing is presented in section 4. 3.2 Parsing with dependency constraints Given a set of dependencies, the bottom-up process of HPSG parsing can be constra"
P07-1079,N06-2033,1,0.641306,"ser combination has been shown to be a powerful way to obtain very high accuracy in dependency parsing (Sagae and Lavie, 2006). Using dependency constraints allows us to improve HPSG parsing accuracy simply by using an existing parser combination approach. As a first step, we train two additional parsers with the dependencies extracted from the HPSG Treebank. The first uses the same shiftreduce framework described in section 2.1, but it process the input from right to left (RL). This has been found to work well in previous work on depenˇ dency parser combination (Zeman and Zabokrtsk´ y, 2005; Sagae and Lavie, 2006). The second parser is MSTParser, the large-margin maximum spanning tree parser described in (McDonald et al., 2005)3 . We examine the use of two combination schemes: one using two parsers, and one using three parsers. The first combination approach is to keep only dependencies for which there is agreement between the two parsers. In other words, dependencies that are proposed by one parser but not the other are simply discarded. Using the left-to-right shift-reduce parser and MSTParser, we find that this results in very high precision of surface dependencies on the development data. In the se"
P07-1079,C88-2121,0,0.104986,"ral language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar We begin by describing our dependency and (LFG) (Bresnan, 1982), Lexicalized Tree Adjoin- HPSG parsing approaches in section 2. In section ing Grammar (LTAG) (Schabes et al., 1988), Head- 3, we present our framework for HPSG parsing with driven Phrase Structure Grammar (HPSG) (Pollard shallow dependency constraints, and in section 4 we 624 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 624–631, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics Figure 1: HPSG parsing evaluate this framework empirically. Sections 5 and 6 discuss related work and conclusions. 2 2.1 Fast dependency parsing and wide-coverage HPSG parsing Data-driven dependency parsing Because we use dependency parsing as a step"
P07-1079,W05-1518,0,0.0398449,"Missing"
P07-1079,J96-1002,0,\N,Missing
P07-1079,P02-1056,0,\N,Missing
P08-1006,P06-2006,0,0.0284334,"on, our system cannot be compared directly to the results reported by Erkan et al. (2007) and Katrenko and Adriaans (2006), while Sætre et al. (2007) presented better results than theirs in the same evaluation criterion. 5 Related Work Though the evaluation of syntactic parsers has been a major concern in the parsing community, and a couple of works have recently presented the comparison of parsers based on different frameworks, their methods were based on the comparison of the parsing accuracy in terms of a certain intermediate parse representation (Ringger et al., 2004; Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007; Miyao et al., 2007; Clegg and Shepherd, 2007; Pyysalo et al., 2007b; Pyysalo et al., 2007a; Sagae et al., 2008). Such evaluation requires gold standard data in an intermediate representation. However, it has been argued that the conversion of parsing results into an intermediate representation is difficult and far from perfect. The relationship between parsing accuracy and task accuracy has been obscure for many years. Quirk and Corston-Oliver (2006) investigated the impact of parsing accuracy on statistical MT. However, this work was only concerned with a single depe"
P08-1006,P04-1056,0,0.02889,"gher dependency accuracy when trained only with GENIA. We therefore only input GENIA as the training data for the retraining of dependency parsers. For the other parsers, we input the concatenation of WSJ and GENIA for the retraining, while the reranker of RERANK was not retrained due to its cost. Since the parsers other than NO-RERANK and RERANK require an external POS tagger, a WSJ-trained POS tagger is used with WSJtrained parsers, and geniatagger (Tsuruoka et al., 2005) is used with GENIA-retrained parsers. 4 Experiments 4.1 Experiment settings In the following experiments, we used AImed (Bunescu and Mooney, 2004), which is a popular corpus for the evaluation of PPI extraction systems. The corpus consists of 225 biomedical paper abstracts (1970 sentences), which are sentence-split, tokenized, and annotated with proteins and PPIs. We use gold protein annotations given in the corpus. Multi-word protein names are concatenated and treated as single words. The accuracy is measured by abstract-wise 10-fold cross validation and the one-answer-per-occurrence criterion (Giuliano et al., 2006). A threshold for SVMs is moved to adjust the balance of precision and recall, and the maximum f-scores are reported for"
P08-1006,P05-1022,0,0.0663263,"parsing) using five different parse representations. We run a PPI system with several combinations of parser and parse representation, and examine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus,"
P08-1006,P04-1014,0,0.498927,"s show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to compare parsers based on different frameworks, because parse representations are often framework-specific and di"
P08-1006,P07-1032,0,0.0427625,"mpared directly to the results reported by Erkan et al. (2007) and Katrenko and Adriaans (2006), while Sætre et al. (2007) presented better results than theirs in the same evaluation criterion. 5 Related Work Though the evaluation of syntactic parsers has been a major concern in the parsing community, and a couple of works have recently presented the comparison of parsers based on different frameworks, their methods were based on the comparison of the parsing accuracy in terms of a certain intermediate parse representation (Ringger et al., 2004; Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007; Miyao et al., 2007; Clegg and Shepherd, 2007; Pyysalo et al., 2007b; Pyysalo et al., 2007a; Sagae et al., 2008). Such evaluation requires gold standard data in an intermediate representation. However, it has been argued that the conversion of parsing results into an intermediate representation is difficult and far from perfect. The relationship between parsing accuracy and task accuracy has been obscure for many years. Quirk and Corston-Oliver (2006) investigated the impact of parsing accuracy on statistical MT. However, this work was only concerned with a single dependency parser, and did n"
P08-1006,P02-1034,0,0.0398489,"ng classifiers (Katrenko and Adriaans, 2006; Erkan et al., 2007; Sætre et al., 2007). For the protein pair IL-8 and CXCR1 in Figure 4, a dependency parser outputs a dependency tree shown in Figure 1. From this dependency tree, we can extract a dependency path shown in Figure 5, which appears to be a strong clue in knowing that these proteins are mentioned as interacting. (dep_path (SBJ (ENTITY1 recognizes)) (rOBJ (recognizes ENTITY2))) Figure 6: Tree representation of a dependency path We follow the PPI extraction method of Sætre et al. (2007), which is based on SVMs with SubSet Tree Kernels (Collins and Duffy, 2002; Moschitti, 2006), while using different parsers and parse representations. Two types of features are incorporated in the classifier. The first is bag-of-words features, which are regarded as a strong baseline for IE systems. Lemmas of words before, between and after the pair of target proteins are included, and the linear kernel is used for these features. These features are commonly included in all of the models. Filtering by a stop-word list is not applied because this setting made the scores higher than Sætre et al. (2007)’s setting. The other type of feature is syntactic features. For de"
P08-1006,P97-1003,0,0.0850946,"ank. Penn Treebank-style phrase structure trees without function tags and empty nodes. This is the default output format for phrase structure parsers. We also create this representation by converting ENJU’s output by tree structure matching, although this conversion is not perfect because forms of PTB and ENJU’s output are not necessarily compatible. PTB Dependency trees of syntactic heads (Figure 8). This representation is obtained by converting PTB trees. We first determine lexical heads of nonterminal nodes by using Bikel’s implementation of Collins’ head detection algorithm9 (Bikel, 2004; Collins, 1997). We then convert lexicalized trees into dependencies between lexical heads. HD The Stanford dependency format (Figure 9). This format was originally proposed for extracting dependency relations useful for practical applications (de Marneffe et al., 2006). A program to convert PTB is attached to the Stanford parser. Although the concept looks similar to CoNLL, this representaSD 8 http://nlp.cs.lth.se/pennconverter/ http://www.cis.upenn.edu/˜dbikel/software. html 9 Figure 9: Stanford dependencies tion does not necessarily form a tree structure, and is designed to express more fine-grained relat"
P08-1006,de-marneffe-etal-2006-generating,0,0.148922,"Missing"
P08-1006,C96-1058,0,0.0205152,"for the sentence “IL-8 recognizes and activates CXCR1.” An advantage of dependency parsing is that dependency trees are a reasonable approximation of the semantics of sentences, and are readily usable in NLP applications. Furthermore, the efficiency of popular approaches to dependency parsing compare favorable with those of phrase structure parsing or deep parsing. While a number of approaches have been proposed for dependency parsing, this paper focuses on two typical methods. McDonald and Pereira (2006)’s dependency 1 parser, based on the Eisner algorithm for projective dependency parsing (Eisner, 1996) with the secondorder factorization. MST 1 http://sourceforge.net/projects/mstparser 47 Figure 2: Penn Treebank-style phrase structure tree Sagae and Tsujii (2007)’s dependency parser,2 based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005). KSDEP 2.2 Phrase structure parsing Owing largely to the Penn Treebank, the mainstream of data-driven parsing research has been dedicated to the phrase structure parsing. These parsers output Penn Treebank-style phrase structure trees, although function tags and empty categories are stri"
P08-1006,D07-1024,0,0.532677,"e for NLP researchers in choosing an appropriate parser for their purposes. In this paper, we present a comparative evaluation of syntactic parsers and their output representations based on different frameworks: dependency parsing, phrase structure parsing, and deep parsing. Our approach to parser evaluation is to measure accuracy improvement in the task of identifying protein-protein interaction (PPI) information in biomedical papers, by incorporating the output of different parsers as statistical features in a machine learning classifier (Yakushiji et al., 2005; Katrenko and Adriaans, 2006; Erkan et al., 2007; Sætre et al., 2007). PPI identification is a reasonable task for parser evaluation, because it is a typical information extraction (IE) application, and because recent studies have shown the effectiveness of syntactic parsing in this task. Since our evaluation method is applicable to any parser output, and is grounded in a real application, it allows for a fair comparison of syntactic parsers based on different frameworks. Parser evaluation in PPI extraction also illuminates domain portability. Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) portion"
P08-1006,W01-0521,0,0.0556271,"parsing in this task. Since our evaluation method is applicable to any parser output, and is grounded in a real application, it allows for a fair comparison of syntactic parsers based on different frameworks. Parser evaluation in PPI extraction also illuminates domain portability. Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) portion of the Penn Treebank, and high accuracy has been reported for WSJ text; however, these parsers rely on lexical information to attain high accuracy, and it has been criticized that these parsers may overfit to WSJ text (Gildea, 2001; 46 Proceedings of ACL-08: HLT, pages 46–54, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics Klein and Manning, 2003). Another issue for discussion is the portability of training methods. When training data in the target domain is available, as is the case with the GENIA Treebank (Kim et al., 2003) for biomedical papers, a parser can be retrained to adapt to the target domain, and larger accuracy improvements are expected, if the training method is sufficiently general. We will examine these two aspects of domain portability by comparing the original parsers w"
P08-1006,E06-1051,0,0.19788,"used with GENIA-retrained parsers. 4 Experiments 4.1 Experiment settings In the following experiments, we used AImed (Bunescu and Mooney, 2004), which is a popular corpus for the evaluation of PPI extraction systems. The corpus consists of 225 biomedical paper abstracts (1970 sentences), which are sentence-split, tokenized, and annotated with proteins and PPIs. We use gold protein annotations given in the corpus. Multi-word protein names are concatenated and treated as single words. The accuracy is measured by abstract-wise 10-fold cross validation and the one-answer-per-occurrence criterion (Giuliano et al., 2006). A threshold for SVMs is moved to adjust the balance of precision and recall, and the maximum f-scores are reported for each setting. 4.2 Comparison of accuracy improvements Tables 1 and 2 show the accuracy obtained by using the output of each parser in each parse representation. The row “baseline” indicates the accuracy obtained with bag-of-words features. Table 3 shows the time for parsing the entire AImed corpus, and Table 4 shows the time required for 10-fold cross validation with GENIA-retrained parsers. When using the original WSJ-trained parsers (Table 1), all parsers achieved almost t"
P08-1006,W07-2202,1,0.375843,"en used in parser evaluation and applications. PAS is a graph structure that represents syntactic/semantic relations among words (Figure 3). The concept is therefore similar to CoNLL dependencies, though PAS expresses deeper relations, and may include reentrant structures. In this work, we chose the two versions of the Enju parser (Miyao and Tsujii, 2008). The HPSG parser that consists of an HPSG grammar extracted from the Penn Treebank, and a maximum entropy model trained with an HPSG treebank derived from the Penn Treebank.7 ENJU The HPSG parser adapted to biomedical texts, by the method of Hara et al. (2007). Because this parser is trained with both WSJ and GENIA, we compare it parsers that are retrained with GENIA (see section 3.3). ENJU-GENIA 3 Evaluation Methodology In our approach to parser evaluation, we measure the accuracy of a PPI extraction system, in which 6 http://nlp.stanford.edu/software/lex-parser. shtml 7 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/ 48 the parser output is embedded as statistical features of a machine learning classifier. We run a classifier with features of every possible combination of a parser and a parse representation, by applying conversions between representat"
P08-1006,W07-2416,0,0.333541,"ntences, and are readily usable in NLP applications. Furthermore, the efficiency of popular approaches to dependency parsing compare favorable with those of phrase structure parsing or deep parsing. While a number of approaches have been proposed for dependency parsing, this paper focuses on two typical methods. McDonald and Pereira (2006)’s dependency 1 parser, based on the Eisner algorithm for projective dependency parsing (Eisner, 1996) with the secondorder factorization. MST 1 http://sourceforge.net/projects/mstparser 47 Figure 2: Penn Treebank-style phrase structure tree Sagae and Tsujii (2007)’s dependency parser,2 based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005). KSDEP 2.2 Phrase structure parsing Owing largely to the Penn Treebank, the mainstream of data-driven parsing research has been dedicated to the phrase structure parsing. These parsers output Penn Treebank-style phrase structure trees, although function tags and empty categories are stripped off (Figure 2). While most of the state-of-the-art parsers are based on probabilistic CFGs, the parameterization of the probabilistic model of each parser var"
P08-1006,N04-1013,0,0.601287,"uracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to compare parsers based on different frameworks, because parse representations are often f"
P08-1006,P03-1054,0,0.166071,"ructure parsing, or deep parsing) using five different parse representations. We run a PPI system with several combinations of parser and parse representation, and examine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of"
P08-1006,P05-1010,1,0.242557,"parser/ http://bllip.cs.brown.edu/resources.shtml 4 We set n = 50 in this paper. 5 http://nlp.cs.berkeley.edu/Main.html#Parsing This study demonstrates that IL-8 recognizes and activates CXCR1, CXCR2, and the Duffy antigen by distinct mechanisms. The molar ratio of serum retinol-binding protein (RBP) to transthyretin (TTR) is not useful to assess vitamin A status during infection in hospitalised children. Figure 3: Predicate argument structure timized automatically by assigning latent variables to each nonterminal node and estimating the parameters of the latent variables by the EM algorithm (Matsuzaki et al., 2005). Stanford’s unlexicalized parser (Klein and Manning, 2003).6 Unlike NO-RERANK, probabilities are not parameterized on lexical heads. Figure 4: Sentences including protein names SBJ OBJ ENTITY1(IL-8) −→ recognizes ←− ENTITY2(CXCR1) Figure 5: Dependency path STANFORD 2.3 Deep parsing Recent research developments have allowed for efficient and robust deep parsing of real-world texts (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). While deep parsers compute theory-specific syntactic/semantic structures, predicate argument structures (PAS) are often used in parser evaluation"
P08-1006,E06-1011,0,0.0491874,"inations of parser and parse representation, and examine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to"
P08-1006,J08-1002,1,0.858485,"f accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to compare parsers based on different frameworks, because parse representations are often framework-specific and differ from parser to parse"
P08-1006,E06-1015,0,0.0153065,"and Adriaans, 2006; Erkan et al., 2007; Sætre et al., 2007). For the protein pair IL-8 and CXCR1 in Figure 4, a dependency parser outputs a dependency tree shown in Figure 1. From this dependency tree, we can extract a dependency path shown in Figure 5, which appears to be a strong clue in knowing that these proteins are mentioned as interacting. (dep_path (SBJ (ENTITY1 recognizes)) (rOBJ (recognizes ENTITY2))) Figure 6: Tree representation of a dependency path We follow the PPI extraction method of Sætre et al. (2007), which is based on SVMs with SubSet Tree Kernels (Collins and Duffy, 2002; Moschitti, 2006), while using different parsers and parse representations. Two types of features are incorporated in the classifier. The first is bag-of-words features, which are regarded as a strong baseline for IE systems. Lemmas of words before, between and after the pair of target proteins are included, and the linear kernel is used for these features. These features are commonly included in all of the models. Filtering by a stop-word list is not applied because this setting made the scores higher than Sætre et al. (2007)’s setting. The other type of feature is syntactic features. For dependency-based par"
P08-1006,P05-1013,0,0.273658,"representation, and examine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to compare parsers based on"
P08-1006,N07-1051,0,0.0565978,"t parse representations. We run a PPI system with several combinations of parser and parse representation, and examine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treeban"
P08-1006,W07-1004,0,0.29972,"). This format was originally proposed for extracting dependency relations useful for practical applications (de Marneffe et al., 2006). A program to convert PTB is attached to the Stanford parser. Although the concept looks similar to CoNLL, this representaSD 8 http://nlp.cs.lth.se/pennconverter/ http://www.cis.upenn.edu/˜dbikel/software. html 9 Figure 9: Stanford dependencies tion does not necessarily form a tree structure, and is designed to express more fine-grained relations such as apposition. Research groups for biomedical NLP recently adopted this representation for corpus annotation (Pyysalo et al., 2007a) and parser evaluation (Clegg and Shepherd, 2007; Pyysalo et al., 2007b). Predicate-argument structures. This is the default output format for ENJU and ENJU-GENIA. PAS Although only CoNLL is available for dependency parsers, we can create four representations for the phrase structure parsers, and five for the deep parsers. Dotted arrows in Figure 7 indicate imperfect conversion, in which the conversion inherently introduces errors, and may decrease the accuracy. We should therefore take caution when comparing the results obtained by imperfect conversion. We also measure the accuracy obtained"
P08-1006,W06-1608,0,0.0654162,"n of the parsing accuracy in terms of a certain intermediate parse representation (Ringger et al., 2004; Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007; Miyao et al., 2007; Clegg and Shepherd, 2007; Pyysalo et al., 2007b; Pyysalo et al., 2007a; Sagae et al., 2008). Such evaluation requires gold standard data in an intermediate representation. However, it has been argued that the conversion of parsing results into an intermediate representation is difficult and far from perfect. The relationship between parsing accuracy and task accuracy has been obscure for many years. Quirk and Corston-Oliver (2006) investigated the impact of parsing accuracy on statistical MT. However, this work was only concerned with a single dependency parser, and did not focus on parsers based on different frameworks. 6 Conclusion and Future Work We have presented our attempts to evaluate syntactic parsers and their representations that are based on different frameworks; dependency parsing, phrase structure parsing, or deep parsing. The basic idea is to measure the accuracy improvements of the PPI extraction task by incorporating the parser output as statistical features of a machine learning classifier. Experiments"
P08-1006,ringger-etal-2004-using,0,0.102376,"owever, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to compare parsers based on different frameworks, because parse representations are often framework-specific and differ from parser to parser (Ringger et al., 2004). The lack of such comparisons is a serious obstacle for NLP researchers in choosing an appropriate parser for their purposes. In this paper, we present a comparative evaluation of syntactic parsers and their output representations based on different frameworks: dependency parsing, phrase structure parsing, and deep parsing. Our approach to parser evaluation is to measure accuracy improvement in the task of identifying protein-protein interaction (PPI) information in biomedical papers, by incorporating the output of different parsers as statistical features in a machine learning classifier (Ya"
P08-1006,D07-1111,1,0.727784,"ine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to compare parsers based on different frameworks, bec"
P08-1006,1993.iwpt-1.22,0,0.0459583,"e on par with each other, while parsing speed differs significantly. We also found that accuracy improvements vary when parsers are retrained with domainspecific data, indicating the importance of domain adaptation and the differences in the portability of parser training methods. Although we restricted ourselves to parsers trainable with Penn Treebank-style treebanks, our methodology can be applied to any English parsers. Candidates include RASP (Briscoe and Carroll, 53 2006), the C&C parser (Clark and Curran, 2004), the XLE parser (Kaplan et al., 2004), MINIPAR (Lin, 1998), and Link Parser (Sleator and Temperley, 1993; Pyysalo et al., 2006), but the domain adaptation of these parsers is not straightforward. It is also possible to evaluate unsupervised parsers, which is attractive since evaluation of such parsers with goldstandard data is extremely problematic. A major drawback of our methodology is that the evaluation is indirect and the results depend on a selected task and its settings. This indicates that different results might be obtained with other tasks. Hence, we cannot conclude the superiority of parsers/representations only with our results. In order to obtain general ideas on parser performance,"
P08-1006,A00-2018,0,\N,Missing
P08-1006,J93-2004,0,\N,Missing
P08-1006,J04-4004,0,\N,Missing
P10-5001,J08-1003,1,0.889847,"Missing"
P10-5001,J08-1002,1,0.800767,"Missing"
P10-5001,J07-3004,1,0.850258,"Missing"
P12-1046,P10-1112,0,0.121454,"as been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al., 2010). TSG is a natural extension of CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, but is still inferior to state-of-the-art parsers (e.g., the Berkeley parser (Petrov et al., 2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CF"
P12-1046,D10-1117,0,0.113633,"nparametric Bayesian model. The PYP produces power-law distributions, which have been shown to be well-suited for such uses as language modeling (Teh, 2006b), and TSG induction (Cohn et al., 2010). One major issue as regards modeling an SR-TSG is that the space of the grammar rules will be very sparse since SR-TSG allows for arbitrarily large tree fragments and also an arbitrarily large set of symbol subcategories. To address the sparseness problem, we employ a hierarchical PYP to encode a backoff scheme from the SRTSG rules to simpler CFG rules, inspired by recent work on dependency parsing (Blunsom and Cohn, 2010). Our model consists of a three-level hierarchy. Table 1 shows an example of the SR-TSG rule and its backoff tree fragments as an illustration of this threelevel hierarchy. The topmost level of our model is a distribution over the SR-TSG rules as follows. e |xk Gxk ∼ Gxk  ∼ PYP dxk , θxk , P sr-tsg (· |xk ) , where xk is a refined root symbol of an elementary tree e, while x is a raw nonterminal symbol in the corpus and k = 0, 1, . . . is an index of the symbol subcategory. Suppose x is NP and its symbol subcategory is 0, then xk is NP0 . The PYP has three parameters: (dxk , θxk , P sr-tsg )."
P12-1046,P05-1022,0,0.838813,"ten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, but is still inferior to state-of-the-art parsers (e.g., the Berkeley parser (Petrov et al., 2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement"
P12-1046,N10-1081,0,0.163802,"However, an adaptor grammar differs from ours in that all its rules are complete: all leaf nodes must be terminal symbols, while our model permits nonterminal symbols as leaf nodes. Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as (a) (b) (c) Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of (a). The refinement annotation is hyphenated with a nonterminal symbol. morphology analysis, word segmentation (Johnson and Goldwater, 2009), and dependency grammar induction (Cohen et al., 2010), rather than constituent syntax parsing. An all-fragments grammar (Bansal and Klein, 2010) is another variant of TSG that aims to utilize all possible subtrees as rules. It maps a TSG to an implicit representation to make the grammar tractable and practical for large-scale parsing. The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task. As mentioned in the introduction, our model focuses on the automatic learning of a TSG and symbol refinement without heuristics. 3 Symbol-Refined Tr"
P12-1046,J03-4003,0,0.043631,"2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments. As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010), large 440 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tree fragments and symbol refinement work complementarily for syntactic parsing. F"
P12-1046,D09-1076,0,0.042041,"Missing"
P12-1046,N09-2064,0,0.139748,"Missing"
P12-1046,N04-1035,0,0.0974858,"Missing"
P12-1046,P08-1067,0,0.305949,"Missing"
P12-1046,N09-1036,0,0.151665,"ol refinement, and is thus closely related to our SR-TSG model. However, an adaptor grammar differs from ours in that all its rules are complete: all leaf nodes must be terminal symbols, while our model permits nonterminal symbols as leaf nodes. Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as (a) (b) (c) Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of (a). The refinement annotation is hyphenated with a nonterminal symbol. morphology analysis, word segmentation (Johnson and Goldwater, 2009), and dependency grammar induction (Cohen et al., 2010), rather than constituent syntax parsing. An all-fragments grammar (Bansal and Klein, 2010) is another variant of TSG that aims to utilize all possible subtrees as rules. It maps a TSG to an implicit representation to make the grammar tractable and practical for large-scale parsing. The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task. As mentioned in the introduction, our model focuses on the automatic learning of a TSG and sy"
P12-1046,N07-1018,0,0.296046,"a process of forming a parse tree. It starts with a root symbol and rewrites (substi441 p (e |t ) ∝ p (t |e ) p (e) . where p (t |e ) is either equal to 1 (when t and e are consistent) or 0 (otherwise). Therefore, the task of TSG induction from parse trees turns out to consist of modeling the prior distribution p (e). Recent work on TSG induction defines p (e) as a nonparametric Bayesian model such as the Dirichlet Process (Ferguson, 1973) or the Pitman-Yor Process to encourage sparse and compact grammars. Several studies have combined TSG induction and symbol refinement. An adaptor grammar (Johnson et al., 2007a) is a sort of nonparametric Bayesian TSG model with symbol refinement, and is thus closely related to our SR-TSG model. However, an adaptor grammar differs from ours in that all its rules are complete: all leaf nodes must be terminal symbols, while our model permits nonterminal symbols as leaf nodes. Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as (a) (b) (c) Figure 1: (a) Example parse tree. (b) Example TSG derivation of (a). (c) Example SR-TSG derivation of (a). The refinement annotation is hyphenated with a no"
P12-1046,J98-4004,0,0.640761,"Petrov et al., 2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments. As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010), large 440 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tree fragments and symbol refinement work complementarily for synta"
P12-1046,P03-1054,0,0.254122,"ion Syntactic parsing has played a central role in natural language processing. The resulting syntactic analysis can be used for various applications such as machine translation (Galley et al., 2004; DeNeefe and Knight, 2009), sentence compression (Cohn and Lapata, 2009; Yamangil and Shieber, 2010), and question answering (Wang et al., 2007). Probabilistic context-free grammar (PCFG) underlies many statistical parsers, however, it is well known that the PCFG rules extracted from treebank data via maximum likelihood estimation do not perform well due to unrealistic context freedom assumptions (Klein and Manning, 2003). In recent years, there has been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al., 2010). TSG is a natural extension of CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 201"
P12-1046,J93-2004,0,0.058271,"ation probabilities according to our model. This heuristics is helpful for finding large tree fragments and learning compact grammars. 4.3 Hyperparameter Estimation We treat hyperparameters {d, θ} as random variables and update their values for every MCMC iteration. We place a prior on the hyperparameters as follows: d ∼ Beta (1, 1), θ ∼ Gamma (1, 1). The values of d and θ are optimized with the auxiliary variable technique (Teh, 2006a). 5 Experiment Model 5.1 Settings CFG 5.1.1 Data Preparation We ran experiments on the Wall Street Journal (WSJ) portion of the English Penn Treebank data set (Marcus et al., 1993), using a standard data split (sections 2–21 for training, 22 for development and 23 for testing). We also used section 2 as a small training set for evaluating the performance of our model under low-resource conditions. Henceforth, we distinguish the small training set (section 2) from the full training set (sections 2-21). The treebank data is right-binarized (Matsuzaki et al., 2005) to construct grammars with only unary and binary productions. We replace lexical words with count ≤ 5 in the training data with one of 50 unknown words using lexical features, following (Petrov et al., 2006). We"
P12-1046,P05-1010,1,0.934163,"harniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments. As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010), large 440 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tree fragments and symbol refinement work complementarily for syntactic parsing. For example, Bansal and Kl"
P12-1046,P06-1055,0,0.730889,"CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, but is still inferior to state-of-the-art parsers (e.g., the Berkeley parser (Petrov et al., 2006) and the Charniak parser (Charniak and Johnson, 2005)). One major drawback of TSG is that the context freedom assumptions still remain at substitution sites, that is, TSG tree fragments are generated that are conditionally independent of all others given root nonterminal symbols. Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Colli"
P12-1046,N10-1003,0,0.14792,"Missing"
P12-1046,P09-2012,0,0.2292,"anslation (Galley et al., 2004; DeNeefe and Knight, 2009), sentence compression (Cohn and Lapata, 2009; Yamangil and Shieber, 2010), and question answering (Wang et al., 2007). Probabilistic context-free grammar (PCFG) underlies many statistical parsers, however, it is well known that the PCFG rules extracted from treebank data via maximum likelihood estimation do not perform well due to unrealistic context freedom assumptions (Klein and Manning, 2003). In recent years, there has been an increasing interest in tree substitution grammar (TSG) as an alternative to CFG for modeling syntax trees (Post and Gildea, 2009; Tenenbaum et al., 2009; Cohn et al., 2010). TSG is a natural extension of CFG in which nonterminal symbols can be rewritten (substituted) with arbitrarily large tree fragments. These tree fragments have great advantages over tiny CFG rules since they can capture non-local contexts explicitly such as predicate-argument structures, idioms and grammatical agreements (Cohn et al., 2010). Previous work on TSG parsing (Cohn et al., 2010; Post and Gildea, 2009; Bansal and Klein, 2010) has consistently shown that a probabilistic TSG (PTSG) parser is significantly more accurate than a PCFG parser, bu"
P12-1046,P06-1124,0,0.186641,"R-TSG derivations from a corpus of parse trees in an unsupervised fashion. That is, we wish to infer the symbol subcategories of every node and substitution site (i.e., nodes where substitution occurs) from parse trees. Extracted rules and their probabilities can be used to parse new raw sentences. 442 3.1 Probabilistic Model We define a probabilistic model of an SR-TSG based on the Pitman-Yor Process (PYP) (Pitman and Yor, 1997), namely a sort of nonparametric Bayesian model. The PYP produces power-law distributions, which have been shown to be well-suited for such uses as language modeling (Teh, 2006b), and TSG induction (Cohn et al., 2010). One major issue as regards modeling an SR-TSG is that the space of the grammar rules will be very sparse since SR-TSG allows for arbitrarily large tree fragments and also an arbitrarily large set of symbol subcategories. To address the sparseness problem, we employ a hierarchical PYP to encode a backoff scheme from the SRTSG rules to simpler CFG rules, inspired by recent work on dependency parsing (Blunsom and Cohn, 2010). Our model consists of a three-level hierarchy. Table 1 shows an example of the SR-TSG rule and its backoff tree fragments as an il"
P12-1046,D07-1003,0,0.0199049,"Missing"
P12-1046,P10-1096,0,0.0399252,"Missing"
P12-1046,D09-1161,0,0.157163,"Missing"
P12-1046,D07-1058,0,0.0141628,"urthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its backoff model, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998; Collins, 2003; Matsuzaki et al., 2005). Symbol refinement is a successful approach for weakening context freedom assumptions by dividing coarse treebank symbols (e.g. NP and VP) into subcategories, rather than extracting large tree fragments. As shown in several studies on TSG parsing (Zuidema, 2007; Bansal and Klein, 2010), large 440 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 440–448, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tree fragments and symbol refinement work complementarily for syntactic parsing. For example, Bansal and Klein (2010) have reported that deterministic symbol refinement with heuristics helps improve the accuracy of a TSG parser. In this paper, we propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. SR-TSG is an extension of the conventio"
P12-1110,D07-1022,0,0.0208608,"Missing"
P12-1110,P04-1015,0,0.256701,"ark (2010), the POS tag is assigned to the word when its first character is shifted, and the word–tag pairs observed in the training data and the closed-set tags (Xia, 2000) are used to prune (1) All subtrees spanning M consecutive characters unlikely derivations. Because 33 tags are defined in have the same index 2M − 1. the CTB tag set (Xia, 2000), our model exploits a (2) All terminal states have the same step index 2N total of 36 actions. (including the root arc), where N is the number To train the model, we use the averaged percepof characters in the sentence. tron with the early update (Collins and Roark, 2004). (3) Every action increases the index. In our joint model, the early update is invoked by Note that the number of shifted characters is also mistakes in any of word segmentation, POS tagging, necessary to meet condition (3). Otherwise, it allows or dependency parsing. an unlimited number of SH(t) actions without incrementing the step index. Figure 1 portrays how the 3.2 Alignment of States states are aligned using the proposed scheme, where When dependency parsing is integrated into the task a subtree is denoted as a rectangle with its partial of joint word segmentation and POS tagging, it is"
P12-1110,D07-1098,0,0.0272609,"st setting we found is σp = 0.5: this result suggests that we probably should resolve remaining errors by preferentially using the local n-gram based features at the early stage of training. Otherwise, the premature incorporation of the non-local syntactic dependencies might engender overfitting to the training data. 4 4.1 Experiment Experimental Settings We use the Chinese Penn Treebank ver. 5.1, 6.0, and 7.0 (hereinafter CTB-5, CTB-6, and CTB-7) for evaluation. These corpora are split into training, development, and test sets, according to previous works. For CTB-5, we refer to the split by Duan et al. (2007) as CTB-5d, and to the split by Jiang et al. (2008) as CTB-5j. We also prepare a dataset for cross validation: the dataset CTB-5c consists of sentences from CTB-5 excluding the development and test sets of CTB-5d and CTB-5j. We split CTB5c into five sets (CTB-5c-n), and alternatively use four of these as the training set and the rest as the test set. CTB-6 is split according to the official split described in the documentation, and CTB-7 is split according to Wang et al. (2011). The statistics of these splits are shown in Table 2. As external dictionaries, we use the HowNet Word List3 , consis"
P12-1110,P08-1043,0,0.0320765,"Missing"
P12-1110,I11-1136,1,0.801089,"their model is practically ten times as fast as their original model. To incorporate the word-level features into the character-based decoder, the features are decomposed into substring-level features, which are effective for incomplete words to have comparable scores to complete words in the beam. Because we found that even an incremental approach with beam search is intractable if we perform the wordbased decoding, we take a character-based approach to produce our joint model. The incremental framework of our model is based on the joint POS tagging and dependency parsing model for Chinese (Hatori et al., 2011), which is an extension of the shift-reduce dependency parser with dynamic programming (Huang and Sagae, 2010). They specifically modified the shift action so that it assigns the POS tag when a word is shifted onto the stack. However, because they regarded word segmentation as given, their model did not consider the interaction between segmentation and POS tagging. 3 Model 3.1 Incremental Joint Segmentation, POS Tagging, and Dependency Parsing Based on the joint POS tagging and dependency parsing model by Hatori et al. (2011), we build our joint model to solve word segmentation, POS tagging, a"
P12-1110,P10-1110,0,0.192797,"ese characters causes numerous oversegmentation errors for OOV words. Based on these observations, we aim at building a joint model that simultaneously processes word segmentation, POS tagging, and dependency parsing, trying to capture global interaction among 1045 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1045–1053, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics these three tasks. To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming (Huang and Sagae, 2010), and propose an efficient method of character-based decoding over candidate structures. Two major challenges exist in formalizing the joint segmentation and dependency parsing task in the character-based incremental framework. First, we must address the problem of how to align comparable states effectively in the beam. Because the number of dependency arcs varies depending on how words are segmented, we devise a step alignment scheme using the number of character-based arcs, which enables effective joint decoding for the three tasks. Second, although the feature set is fundamentally a combina"
P12-1110,P08-1102,0,0.0101226,"sts that we probably should resolve remaining errors by preferentially using the local n-gram based features at the early stage of training. Otherwise, the premature incorporation of the non-local syntactic dependencies might engender overfitting to the training data. 4 4.1 Experiment Experimental Settings We use the Chinese Penn Treebank ver. 5.1, 6.0, and 7.0 (hereinafter CTB-5, CTB-6, and CTB-7) for evaluation. These corpora are split into training, development, and test sets, according to previous works. For CTB-5, we refer to the split by Duan et al. (2007) as CTB-5d, and to the split by Jiang et al. (2008) as CTB-5j. We also prepare a dataset for cross validation: the dataset CTB-5c consists of sentences from CTB-5 excluding the development and test sets of CTB-5d and CTB-5j. We split CTB5c into five sets (CTB-5c-n), and alternatively use four of these as the training set and the rest as the test set. CTB-6 is split according to the official split described in the documentation, and CTB-7 is split according to Wang et al. (2011). The statistics of these splits are shown in Table 2. As external dictionaries, we use the HowNet Word List3 , consisting of 91,015 words, and page names from the Chine"
P12-1110,P09-1058,0,0.0291533,"to peace-operation-related groups. 1 Introduction In processing natural languages that do not include delimiters (e.g. spaces) between words, word segmentation is the crucial first step that is necessary to perform virtually all NLP tasks. Furthermore, the word-level information is often augmented with the POS tags, which, along with segmentation, form the basic foundation of statistical NLP. Because the tasks of word segmentation and POS tagging have strong interactions, many studies have been devoted to the task of joint word segmentation and POS tagging for languages such as Chinese (e.g. Kruengkrai et al. (2009)). This is because some of the segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a sequence of POS tags. The joint approach to word segmentation and POS tagging has been reported to improve word segmentation and POS tagging accuracies by more than âS the only difference is the existence of the last word ; however, whether or not this word exists changes the whole syntactic structure and segmentation of the sentence. This is an example in which word segmentation cannot be handled properly without considering long-range syntactic"
P12-1110,D11-1109,0,0.170218,"Missing"
P12-1110,W03-1025,0,0.220404,"Missing"
P12-1110,P94-1010,0,0.471525,"Missing"
P12-1110,P11-1139,0,0.0620577,"Missing"
P12-1110,I11-1035,0,0.116556,"are split into training, development, and test sets, according to previous works. For CTB-5, we refer to the split by Duan et al. (2007) as CTB-5d, and to the split by Jiang et al. (2008) as CTB-5j. We also prepare a dataset for cross validation: the dataset CTB-5c consists of sentences from CTB-5 excluding the development and test sets of CTB-5d and CTB-5j. We split CTB5c into five sets (CTB-5c-n), and alternatively use four of these as the training set and the rest as the test set. CTB-6 is split according to the official split described in the documentation, and CTB-7 is split according to Wang et al. (2011). The statistics of these splits are shown in Table 2. As external dictionaries, we use the HowNet Word List3 , consisting of 91,015 words, and page names from the Chinese Wikipedia4 as of Oct 26, 2011, consisting of 709,352 words. These dictionaries only consist of word forms with no frequency or POS information. We use standard measures of word-level precision, recall, and F1 score, for evaluating each task. The output of dependencies cannot be correct unless the syntactic head and dependent of the dependency relation are both segmented correctly. Following the standard setting in dependency"
P12-1110,P08-1101,0,0.0702245,"an existing morphological analyzer. In addition, the lattice does not include word segmentation ambiguities crossing boundaries of space-delimited tokens. In contrast, because the Chinese language does not have spaces between words, we fundamentally need to consider the lattice structure of the whole sentence. Therefore, we place no restriction on the segmentation possibilities to consider, and we assess the full potential of the joint segmentation and dependency parsing model. Among the many recent works on joint segmentation and POS tagging for Chinese, the linear-time incremental models by Zhang and Clark (2008) and Zhang and Clark (2010) largely inspired our model. Zhang and Clark (2008) proposed an incremental joint segmentation and POS tagging model, with an effective feature set for Chinese. However, it requires to computationally expensive multiple beams to compare words of different lengths using beam search. More recently, Zhang and Clark (2010) proposed an efficient character-based decoder for their word-based model. In their new model, a single beam suffices for decoding; hence, they reported that their model is practically ten times as fast as their original model. To incorporate the word-l"
P12-1110,D10-1082,0,0.120486,"haracter-based decoding over candidate structures. Two major challenges exist in formalizing the joint segmentation and dependency parsing task in the character-based incremental framework. First, we must address the problem of how to align comparable states effectively in the beam. Because the number of dependency arcs varies depending on how words are segmented, we devise a step alignment scheme using the number of character-based arcs, which enables effective joint decoding for the three tasks. Second, although the feature set is fundamentally a combination of those used in previous works (Zhang and Clark, 2010; Huang and Sagae, 2010), to integrate them in a single incremental framework is not straightforward. Because we must perform decisions of three kinds (segmentation, tagging, and parsing) in an incremental framework, we must adjust which features are to be activated when, and how they are combined with which action labels. We have also found that we must balance the learning rate between features for segmentation and tagging decisions, and those for dependency parsing. We perform experiments using the Chinese Treebank (CTB) corpora, demonstrating that the accuracies of the three tasks can be i"
P12-1110,P11-2033,0,0.0886955,"Missing"
P12-1110,J96-3004,0,\N,Missing
P13-1103,C04-1180,0,0.155143,"dependency parsing is applied and then semantic role labeling is performed on the dependencies (Sasano and Kurohashi, 2011; Kawahara and Kurohashi, 2011; Kudo and Matsumoto, 2002; Iida and Poesio, 2011; Hayashibe et al., 2011). This dominance is mainly because chunkbased dependency analysis looks most appropriate for Japanese syntax due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present our work on inducing wide-coverage Japanese resources based on combinatory categorial grammar (CCG) (Steedman, 2001). Our work is basically an extension of a seminal work on CCGbank (Hockenmaier and Steedman, 2007), in which the phrase structure trees of the Penn Treebank (PTB) (Marcus et al., 1993) are converted into CCG derivations and a wide-coverage CCG lexicon is then extracted from these derivations. As CCGbank has enabled a variety of outstanding works o"
P13-1103,P05-2013,0,0.658358,"Missing"
P13-1103,J07-4004,0,0.109849,"dependency-based pipeline in which chunk-based dependency parsing is applied and then semantic role labeling is performed on the dependencies (Sasano and Kurohashi, 2011; Kawahara and Kurohashi, 2011; Kudo and Matsumoto, 2002; Iida and Poesio, 2011; Hayashibe et al., 2011). This dominance is mainly because chunkbased dependency analysis looks most appropriate for Japanese syntax due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present our work on inducing wide-coverage Japanese resources based on combinatory categorial grammar (CCG) (Steedman, 2001). Our work is basically an extension of a seminal work on CCGbank (Hockenmaier and Steedman, 2007), in which the phrase structure trees of the Penn Treebank (PTB) (Marcus et al., 1993) are converted into CCG derivations and a wide-coverage CCG lexicon is then extracted from these derivations. As CCGbank has"
P13-1103,hanaoka-etal-2010-japanese,1,0.904346,"Missing"
P13-1103,I11-1023,0,0.0377645,"ces are dependency-based. Our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexicon and the accuracy of parsing. 1 Introduction Syntactic parsing for Japanese has been dominated by a dependency-based pipeline in which chunk-based dependency parsing is applied and then semantic role labeling is performed on the dependencies (Sasano and Kurohashi, 2011; Kawahara and Kurohashi, 2011; Kudo and Matsumoto, 2002; Iida and Poesio, 2011; Hayashibe et al., 2011). This dominance is mainly because chunkbased dependency analysis looks most appropriate for Japanese syntax due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present our work on inducing wide-coverage Japanese resources based on combinatory"
P13-1103,J07-3004,0,0.654,"due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present our work on inducing wide-coverage Japanese resources based on combinatory categorial grammar (CCG) (Steedman, 2001). Our work is basically an extension of a seminal work on CCGbank (Hockenmaier and Steedman, 2007), in which the phrase structure trees of the Penn Treebank (PTB) (Marcus et al., 1993) are converted into CCG derivations and a wide-coverage CCG lexicon is then extracted from these derivations. As CCGbank has enabled a variety of outstanding works on wide-coverage deep parsing for English, our resources are expected to significantly contribute to Japanese deep parsing. The application of the CCGbank method to Japanese is not trivial, as resources like PTB are not available in Japanese. The widely used resources for parsing research are the Kyoto corpus (Kawahara et al., 2002) and the NAIST t"
P13-1103,P06-1064,0,0.109969,"treebank of Japanese. We largely extended their work by exploiting the standard chunk-based Japanese corpora and demonstrated the first results for Japanese deep parsing with grammar induced from large corpora. Corpus-based acquisition of wide-coverage CCG resources has enjoyed great success for English (Hockenmaier and Steedman, 2007). In that method, PTB was converted into CCG-based derivations from which a wide-coverage CCG lexicon was extracted. CCGbank has been used for the development of wide-coverage CCG parsers (Clark and Curran, 2007). The same methodology has been applied to German (Hockenmaier, 2006), Italian (Bos et al., 2009), and Turkish (C¸akıcı, 2005). Their treebanks are annotated with dependencies of words, the conversion of which into phrase structures is not a big concern. A notable contribution of the present work is a method for inducing CCG grammars from chunk-based dependency structures, which is not obvious, as we discuss later in this paper. CCG parsing provides not only predicate argument relations but also CCG derivations, which can be used for various semantic processing tasks (Bos et al., 2004; Bos, 2007). Our work constitutes a starting point for such deep linguistic p"
P13-1103,P11-1081,0,0.0302296,"panese syntactic resources are dependency-based. Our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexicon and the accuracy of parsing. 1 Introduction Syntactic parsing for Japanese has been dominated by a dependency-based pipeline in which chunk-based dependency parsing is applied and then semantic role labeling is performed on the dependencies (Sasano and Kurohashi, 2011; Kawahara and Kurohashi, 2011; Kudo and Matsumoto, 2002; Iida and Poesio, 2011; Hayashibe et al., 2011). This dominance is mainly because chunkbased dependency analysis looks most appropriate for Japanese syntax due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present our work on inducing wide-coverage Japanese resour"
P13-1103,W07-1522,0,0.0261019,"he phrase structure trees of the Penn Treebank (PTB) (Marcus et al., 1993) are converted into CCG derivations and a wide-coverage CCG lexicon is then extracted from these derivations. As CCGbank has enabled a variety of outstanding works on wide-coverage deep parsing for English, our resources are expected to significantly contribute to Japanese deep parsing. The application of the CCGbank method to Japanese is not trivial, as resources like PTB are not available in Japanese. The widely used resources for parsing research are the Kyoto corpus (Kawahara et al., 2002) and the NAIST text corpus (Iida et al., 2007), both of which are based on the dependency structures of chunks. Moreover, the relation between chunk-based dependency structures and CCG derivations is not obvious. In this work, we propose a method to integrate multiple dependency-based corpora into phrase structure trees augmented with predicate argument relations. We can then convert the phrase structure trees into CCG derivations. In the following, we describe the details of the integration method as well as Japanese-specific issues in the conversion into CCG derivations. The method is empirically evaluated in terms of the quality of the"
P13-1103,I11-1051,0,0.0253575,"ese have not been widely studied, mainly because most Japanese syntactic resources are dependency-based. Our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexicon and the accuracy of parsing. 1 Introduction Syntactic parsing for Japanese has been dominated by a dependency-based pipeline in which chunk-based dependency parsing is applied and then semantic role labeling is performed on the dependencies (Sasano and Kurohashi, 2011; Kawahara and Kurohashi, 2011; Kudo and Matsumoto, 2002; Iida and Poesio, 2011; Hayashibe et al., 2011). This dominance is mainly because chunkbased dependency analysis looks most appropriate for Japanese syntax due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present o"
P13-1103,kawahara-etal-2002-construction,0,0.0285141,"Gbank (Hockenmaier and Steedman, 2007), in which the phrase structure trees of the Penn Treebank (PTB) (Marcus et al., 1993) are converted into CCG derivations and a wide-coverage CCG lexicon is then extracted from these derivations. As CCGbank has enabled a variety of outstanding works on wide-coverage deep parsing for English, our resources are expected to significantly contribute to Japanese deep parsing. The application of the CCGbank method to Japanese is not trivial, as resources like PTB are not available in Japanese. The widely used resources for parsing research are the Kyoto corpus (Kawahara et al., 2002) and the NAIST text corpus (Iida et al., 2007), both of which are based on the dependency structures of chunks. Moreover, the relation between chunk-based dependency structures and CCG derivations is not obvious. In this work, we propose a method to integrate multiple dependency-based corpora into phrase structure trees augmented with predicate argument relations. We can then convert the phrase structure trees into CCG derivations. In the following, we describe the details of the integration method as well as Japanese-specific issues in the conversion into CCG derivations. The method is empiri"
P13-1103,W02-2016,0,0.789644,"ed, mainly because most Japanese syntactic resources are dependency-based. Our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexicon and the accuracy of parsing. 1 Introduction Syntactic parsing for Japanese has been dominated by a dependency-based pipeline in which chunk-based dependency parsing is applied and then semantic role labeling is performed on the dependencies (Sasano and Kurohashi, 2011; Kawahara and Kurohashi, 2011; Kudo and Matsumoto, 2002; Iida and Poesio, 2011; Hayashibe et al., 2011). This dominance is mainly because chunkbased dependency analysis looks most appropriate for Japanese syntax due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present our work on inducing wide-c"
P13-1103,J93-2004,0,0.0433915,"However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like English. In this paper, we present our work on inducing wide-coverage Japanese resources based on combinatory categorial grammar (CCG) (Steedman, 2001). Our work is basically an extension of a seminal work on CCGbank (Hockenmaier and Steedman, 2007), in which the phrase structure trees of the Penn Treebank (PTB) (Marcus et al., 1993) are converted into CCG derivations and a wide-coverage CCG lexicon is then extracted from these derivations. As CCGbank has enabled a variety of outstanding works on wide-coverage deep parsing for English, our resources are expected to significantly contribute to Japanese deep parsing. The application of the CCGbank method to Japanese is not trivial, as resources like PTB are not available in Japanese. The widely used resources for parsing research are the Kyoto corpus (Kawahara et al., 2002) and the NAIST text corpus (Iida et al., 2007), both of which are based on the dependency structures o"
P13-1103,J08-1002,1,0.869855,"Missing"
P13-1103,J05-1004,0,0.0607126,"3 Step 2-2, 2-3 NPni せ CAUSE ＜ に DAT た PAST ＜ T1 交渉 negotiation VP T/T S NPni PP Aux Noun Step 2-1 S ＜ Scont＼NPni Svo_s＼NPni に DAT Svo_s＼NPni 参加 participation Sbase＼Scont Scont＼Svo_s た PAST Svo_s＼Svo_s せ CAUSE さ do Figure 9: A phrase structure into a CCG derivation. In the figure, the annotation given in the two corpora is shown inside the dotted box at the bottom. We converted the predicate-argument annotations given as labeled word-to-word dependencies into the relations between the predicate words and their argument phrases. The results are thus similar to the annotation style of PropBank (Palmer et al., 2005). In the NAIST corpus, each pred-arg relation is labeled with the argument-type (ga/o/ni) and a flag indicating that the relation is mediated by either a syntactic dependency or a zero anaphora. For a relation of a predicate wp and its argument wa in the NAIST corpus, the boundary of the argument phrase is determined as follows: 1. If wa precedes wp and the relation is mediated by a syntactic dep., select the maximum PP that is formed by attaching one or more postpositions to the NP headed by wa . 2. If wp precedes wa or the relation is mediated by a zero anaphora, select the maximum NP headed"
P13-1103,I11-1085,0,0.0137283,"e languages, those for Japanese have not been widely studied, mainly because most Japanese syntactic resources are dependency-based. Our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexicon and the accuracy of parsing. 1 Introduction Syntactic parsing for Japanese has been dominated by a dependency-based pipeline in which chunk-based dependency parsing is applied and then semantic role labeling is performed on the dependencies (Sasano and Kurohashi, 2011; Kawahara and Kurohashi, 2011; Kudo and Matsumoto, 2002; Iida and Poesio, 2011; Hayashibe et al., 2011). This dominance is mainly because chunkbased dependency analysis looks most appropriate for Japanese syntax due to its morphosyntactic typology, which includes agglutination and scrambling (Bekki, 2010). However, it is also true that this type of analysis has prevented us from deeper syntactic analysis such as deep parsing (Clark and Curran, 2007) and logical inference (Bos et al., 2004; Bos, 2007), both of which have been surpassing shallow parsing-based approaches in languages like Englis"
P13-1103,P09-2013,0,0.033705,"Missing"
P13-1103,W02-1210,0,0.191891,"Missing"
P13-1103,P07-1031,0,0.0702006,"se corpora―namely, the Kyoto corpus, the NAIST text corpus, and the JP corpus ―into a phrase structure treebank, which is similar in spirit to PTB. Our approach is to convert the dependency structures of the Kyoto corpus into phrase structures and then augment them with syntactic/semantic roles from the other two corpora. The conversion involves two steps: 1) recognizing the chunk-internal structures, and (2) converting inter-chunk dependencies into phrase structures. For 1), we don’t have any explicit information in the Kyoto corpus although, in principle, each chunk has internal structures (Vadas and Curran, 2007; Yamada et al., 2010). The lack of a chunk-internal structure makes the dependencyto-constituency conversion more complex than a similar procedure by Bos et al. (2009) that converts an Italian dependency treebank into constituency trees since their dependency trees are annotated down to the level of each word. For the current implementation, we abandon the idea of identifying exact structures and instead basically rely on the following generic rules (Fig. 6): Nominal chunks Compound nouns are first formed as a right-branching phrase and post-positions are then attached to it. Verbal chunks Ve"
P13-1103,P05-2024,0,0.0296169,"ng is fairly limited. Formal theories of Japanese syntax were presented by Gunji (1987) based on Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003) and by Komagata (1999) based on CCG, although their implementations in real-world parsing have not been very successful. JACY (Siegel 1 In fact, the NAIST text corpus includes additional texts, but in this work we only use the news text section. 1044 and Bender, 2002) is a large-scale Japanese grammar based on HPSG, but its semantics is tightly embedded in the grammar and it is not as easy to systematically switch them as it is in CCG. Yoshida (2005) proposed methods for extracting a wide-coverage lexicon based on HPSG from a phrase structure treebank of Japanese. We largely extended their work by exploiting the standard chunk-based Japanese corpora and demonstrated the first results for Japanese deep parsing with grammar induced from large corpora. Corpus-based acquisition of wide-coverage CCG resources has enjoyed great success for English (Hockenmaier and Steedman, 2007). In that method, PTB was converted into CCG-based derivations from which a wide-coverage CCG lexicon was extracted. CCGbank has been used for the development of wide-c"
P13-2049,bentivogli-etal-2010-building,0,0.0136188,"scope of t2 did not overlap with that of t1 even if t2 was converted and TE would be unchanged. An example in case that we allowed to convert t2 is shown below. Boldfaced types in Table 4 shows that it becomes easy to compare t1 with t2 by converting to t2. Y (32) N (29) Total (61) Monothematic pairs Y N Total 116 – 116 96 29 125 212 29 241 Table 5: Distribution of monothematic pairs with respect to original Y/N pairs When the methodology was applied to 61 pairs, a total of 241 and an average of 3.95 monothematic pairs were derived. The average was slightly greater than the 2.98 reported in (Bentivogli et al., 2010). For pairs originally labeled ‘Y’ and ‘N’, an average of 3.62 and 3.31 monothematic pairs were derived, respectively. Both average values were slightly higher than the values of 3.03 and 2.80 reported in (Bentivogli et al., 2010). On the basis of the small differences between the average values in our study and those in (Bentivogli et al., 2010), we are justified in saying that our methodology is valid. Table 6 3 shows the distribution of BSRs in t1t2 pairs in an existing study and the present study. We can see from Table 6 that Corference was seen more frequently in Bentivogli’s study than i"
P13-2049,P10-1122,0,0.0722695,"Missing"
P14-1008,E09-1025,0,0.0537802,"Missing"
P14-1008,W09-0208,0,0.0205201,"Missing"
P14-1008,D12-1058,0,0.0257949,"Missing"
P14-1008,S13-1002,0,0.025353,"Selection operators are implemented as markers assigned to abstract denotations, with specially designed axioms. For example superlatives satisfy the following property: A ⊂ B & shighest (B) ⊂ A ⇒ shighest (B) = shighest (A). New rules can be added if necessary. T/H Abstract denotations DCS trees Language resources Inference On-the-fly knowledge Yes/No Axioms Figure 4: RTE system tic expressions, and apply distributional similarity to judge their validity. In this way, our framework combines distributional and logical semantics, which is also the main subject of Lewis and Steedman (2013) and Beltagy et al. (2013). As follows, our full system (Figure 4) additionally invokes linguistic knowledge on-the-fly: Coreference We use Stanford CoreNLP to resolve coreferences (Raghunathan et al., 2010), whereas coreference is implemented as a special type of selection. If a node σ in a DCS tree T belongs to a mention cluster m, we take the abstract denotation [[Tσ ]] and make a selection sm ([[Tσ ]]), which is regarded as the abstract denotation of that mention. Then all selections of the same mention cluster are declared to be equal. 3 Parsing Coreference 4. If H is not proven, compare DCS trees of T and H, and"
P14-1008,H05-1049,0,0.10417,"Missing"
P14-1008,D13-1160,0,0.0654843,"Missing"
P14-1008,N10-1145,0,0.0314724,"Missing"
P14-1008,H05-1079,0,0.0660109,"s blamed for deaths. H: A storm has caused loss of life. The germ OBJ(blame) and germ ARG(death) in DCS tree of T are joined by the underscored path. Two paths are aligned if the joined germs are aligned, and we impose constraints on aligned germs to inhibit meaningless alignments, as described below. However, this method does not work for realworld datasets such as PASCAL RTE (Dagan et al., 2006), because of the knowledge bottleneck: it is often the case that the lack of sufficient linguistic knowledge causes failure of inference, thus the system outputs “no entailment” for almost all pairs (Bos and Markert, 2005). The transparent syntax-to-semantics interface of DCS enables us to back off to NLP techniques during inference for catching up the lack of knowledge. We extract fragments of DCS trees as paraphrase candidates, translate them back to linguis3.2 Aligning germs by logical clues Two germs are aligned if they are both at leaf nodes (e.g. ARG(death) in T and ARG(life) in H, Figure 5), or they already have part of their meanings in common, by some logical clues. 83 T : OBJ ARG Debby ARG ARG storm ARG MOD blame IOBJ ARG death H : storm, Debby, [[ Whatandis tropical is blamed for death ]] cause ARG S"
P14-1008,D13-1161,0,0.0450047,"Missing"
P14-1008,P13-1042,0,0.0158794,"Missing"
P14-1008,Q13-1015,0,0.132113,"here sf is a selection marker. Selection operators are implemented as markers assigned to abstract denotations, with specially designed axioms. For example superlatives satisfy the following property: A ⊂ B & shighest (B) ⊂ A ⇒ shighest (B) = shighest (A). New rules can be added if necessary. T/H Abstract denotations DCS trees Language resources Inference On-the-fly knowledge Yes/No Axioms Figure 4: RTE system tic expressions, and apply distributional similarity to judge their validity. In this way, our framework combines distributional and logical semantics, which is also the main subject of Lewis and Steedman (2013) and Beltagy et al. (2013). As follows, our full system (Figure 4) additionally invokes linguistic knowledge on-the-fly: Coreference We use Stanford CoreNLP to resolve coreferences (Raghunathan et al., 2010), whereas coreference is implemented as a special type of selection. If a node σ in a DCS tree T belongs to a mention cluster m, we take the abstract denotation [[Tσ ]] and make a selection sm ([[Tσ ]]), which is regarded as the abstract denotation of that mention. Then all selections of the same mention cluster are declared to be equal. 3 Parsing Coreference 4. If H is not proven, compare"
P14-1008,P11-1060,0,0.132622,"efined as: n  [[T ]] = w(σ) ∩ ( ιri (πri0 ([[Tτi ]])) × WRσ i ), Non-emptiness A 6= ∅: the set A is not empty. Subsumption A ⊂ B: set A is subsumed by B.3 i=1 Roughly speaking, the relations correspond to the logical concepts satisfiability and entailment. 4 Negation and disjointness (“k”) are explained in §2.5. http://nlp.stanford.edu/software/ corenlp.shtml 6 In (Liang et al., 2011) DCS trees are learned from QA pairs and database entries. We obtain DCS trees from dependency trees, to bypass the need of a concrete database. 7 The definition differs slightly from the original Liang et al. (2011), mainly for the sake of simplicity and clarity. 5 2 If A and B has the same dimension, q⊂ (A, B) is either ∅ or {∗} (0-dimension point set), depending on if A ⊂ B. 3 Using division operator, subsumption can be represented by non-emptiness, since for sets A, B of the same dimension, q⊂ (A, B) 6= ∅ ⇔ A ⊂ B. 81 πOBJ (F6 ) = dog ∩ F7 T F6 6= ∅ T dog ⊂ πOBJ (F2 ) Axiom 4 dog ⊂ animal Axiom 8 dog ⊂ F3 dog ∩ F7 6= ∅ dog ∩ F7 ⊂ F3 ∩ F7 πOBJ (F4 ) = F3 ∩ F7 F3 ∩ F7 6= ∅ Axiom 6 Axiom 4 F4 6= ∅ Figure 3: An example of proof using abstract denotations 1. 2. 3. 4. W 6= ∅ A∩B ⊂A r Br × q⊂ (A, B) ⊂ A πR (A"
P14-1008,W07-1431,0,0.250248,"), we output “yes” if H is proven, or try to prove the negation of H if H is not proven. To negate H, we use the root negation as described in §2.5. If the negation of H is proven, we output “no”, otherwise we output “unknown”. The result is shown in Table 4. Since our system uses an off-the-shelf dependency parser, and semantic representations are obtained from simple rule-based conversion from dependency trees, there will be only one (right or wrong) interpretation in face of ambiguous sentences. Still, our system outperforms Lewis and Steedman (2013)’s probabilistic CCG-parser. Compared to MacCartney and Manning (2007) and MacCartney and Manning (2008), our system does not need a pretrained alignment model, and it improves by making multi-sentence inferences. To sum up, the result shows that DCS is good at handling universal quantifiers and negations. Most errors are due to wrongly generated DCS trees (e.g. wrongly assigned semantic roles) or unimplemented quantifier triggers (e.g. “neither”) or generalized quantifiers (e.g. “at least a few”). These could be addressed by future work. Experiments In this section, we evaluate our system on FraCaS (§4.2) and PASCAL RTE datasets (§4.3). 4.1 Language Resources T"
P14-1008,C08-1066,0,0.42732,"gation of H if H is not proven. To negate H, we use the root negation as described in §2.5. If the negation of H is proven, we output “no”, otherwise we output “unknown”. The result is shown in Table 4. Since our system uses an off-the-shelf dependency parser, and semantic representations are obtained from simple rule-based conversion from dependency trees, there will be only one (right or wrong) interpretation in face of ambiguous sentences. Still, our system outperforms Lewis and Steedman (2013)’s probabilistic CCG-parser. Compared to MacCartney and Manning (2007) and MacCartney and Manning (2008), our system does not need a pretrained alignment model, and it improves by making multi-sentence inferences. To sum up, the result shows that DCS is good at handling universal quantifiers and negations. Most errors are due to wrongly generated DCS trees (e.g. wrongly assigned semantic roles) or unimplemented quantifier triggers (e.g. “neither”) or generalized quantifiers (e.g. “at least a few”). These could be addressed by future work. Experiments In this section, we evaluate our system on FraCaS (§4.2) and PASCAL RTE datasets (§4.3). 4.1 Language Resources The lexical knowledge we use are sy"
P14-1008,P13-1131,0,0.0199019,"Missing"
P14-1008,W04-3206,0,0.0533051,"Missing"
P14-1008,N13-1090,0,0.0608526,"Missing"
P14-1008,P07-1058,0,0.0264103,"is proven, or try to prove the negation of H if H is not proven. To negate H, we use the root negation as described in §2.5. If the negation of H is proven, we output “no”, otherwise we output “unknown”. The result is shown in Table 4. Since our system uses an off-the-shelf dependency parser, and semantic representations are obtained from simple rule-based conversion from dependency trees, there will be only one (right or wrong) interpretation in face of ambiguous sentences. Still, our system outperforms Lewis and Steedman (2013)’s probabilistic CCG-parser. Compared to MacCartney and Manning (2007) and MacCartney and Manning (2008), our system does not need a pretrained alignment model, and it improves by making multi-sentence inferences. To sum up, the result shows that DCS is good at handling universal quantifiers and negations. Most errors are due to wrongly generated DCS trees (e.g. wrongly assigned semantic roles) or unimplemented quantifier triggers (e.g. “neither”) or generalized quantifiers (e.g. “at least a few”). These could be addressed by future work. Experiments In this section, we evaluate our system on FraCaS (§4.2) and PASCAL RTE datasets (§4.3). 4.1 Language Resources T"
P14-1008,H05-1047,0,0.0901183,"Missing"
P14-1008,N07-1071,0,0.0267031,"Missing"
P14-1008,P06-2105,0,0.0401589,"Missing"
P14-1008,P13-1092,0,0.0114834,"Missing"
P14-1008,P10-1040,0,0.00435916,"Missing"
P14-1008,D10-1048,0,0.00528686,"& shighest (B) ⊂ A ⇒ shighest (B) = shighest (A). New rules can be added if necessary. T/H Abstract denotations DCS trees Language resources Inference On-the-fly knowledge Yes/No Axioms Figure 4: RTE system tic expressions, and apply distributional similarity to judge their validity. In this way, our framework combines distributional and logical semantics, which is also the main subject of Lewis and Steedman (2013) and Beltagy et al. (2013). As follows, our full system (Figure 4) additionally invokes linguistic knowledge on-the-fly: Coreference We use Stanford CoreNLP to resolve coreferences (Raghunathan et al., 2010), whereas coreference is implemented as a special type of selection. If a node σ in a DCS tree T belongs to a mention cluster m, we take the abstract denotation [[Tσ ]] and make a selection sm ([[Tσ ]]), which is regarded as the abstract denotation of that mention. Then all selections of the same mention cluster are declared to be equal. 3 Parsing Coreference 4. If H is not proven, compare DCS trees of T and H, and generate path alignments. 5. Aligned paths are evaluated by a similarity score to estimate their likelihood of being paraphrases. Path alignments with scores higher than a threshold"
P14-1008,C10-1131,0,0.050771,"Missing"
P14-1008,D12-1018,0,0.00909485,"Missing"
P14-1008,N06-1005,0,0.0594277,"Missing"
P14-1008,P13-1045,0,0.00800173,"n ⊂ πSUBJ (die) dog ⊂ animal all criminals commit criminal ⊂ πSUBJ (commit∩ a crime (WSUBJ × crimeOBJ )) rise k fall no dogs are hurt dog k πOBJ (hurt) Abstract denotations and statements are convenient for representing semantics of various types of expressions and linguistic knowledge. Some examples are shown in Table 2.4 2.4 Based on abstract denotations, we briefly describe our process to apply DCS to textual inference. Table 2: Abstract denotations and statements 2.4.1 Natural language to DCS trees To obtain DCS trees from natural language, we use Stanford CoreNLP5 for dependency parsing (Socher et al., 2013), and convert Stanford dependencies to DCS trees by pattern matching on POS tags and dependency labels.6 Currently we use the following semantic roles: ARG, SUBJ, OBJ, IOBJ, TIME and MOD. The semantic role MOD is used for any restrictive modifiers. Determiners such as “all”, “every” and “each” trigger quantifiers, as shown in Figure 2. • Content words: a content word (e.g. read) defines a set representing the word (e.g. read = {(x, y) |read(x, y)}). In addition we introduce following functions: • ×: the Cartesian product of two sets. • ∩: the intersection of two sets. • πr : projection onto do"
P14-1008,R11-1063,0,0.0483574,"Missing"
P14-1008,P12-1030,0,0.0343029,"Missing"
P14-1008,P03-1029,0,0.0284226,"Missing"
P14-1008,N13-1007,0,\N,Missing
P15-1148,J92-4003,0,0.0720282,"riment Table 6 compares our parsing system with those of previous studies. When we look at closed settings, where no external resource other than the training Penn Treebank is used, our system outperforms all other systems including the Berkeley parser (Petrov and Klein, 2007) and the Stanford parser (Socher et al., 2013) in terms of F1. The parsing systems with external features or reranking outperform our system. However, it should be noted that our system could also be improved by external features. For example, the feature of type-level distributional similarity, such as Brown clustering (Brown et al., 1992), can be incorporated with our system without changing the theoretical runtime. 7 Related Work and Discussion Though the framework is shift-reduce, we can notice that our system is strikingly similar to the CKY-based discriminative parser (Hall et al., 2014) because our features basically come from two nodes on the stack and their spans. From this viewpoint, it is interesting to see that our system outperforms theirs by a large margin (Figure 6). Identifying the source of this performance change is beyond the scope of this paper, but we believe this is an important question for future parsing"
P15-1148,P05-1022,0,0.083237,"87.9 90.4 90.4 91.1 Sent./s. 3.7 2.2 93.4 8.4 13.6 90.1 90.3 89.0 90.3 90.7 89.5 90.2 90.5 89.3 6.1 3.3 0.7 91.2 92.2 91.1 91.8 92.6 91.5 91.5 92.4 91.3 2.1 1.2 47.6 Table 6: The final results for section 23 of the Penn Treebank. The systems with † are reported by authors running on different hardware. We divide baseline state-of-the-art systems into three categories: shift-reduce systems (Sagae and Lavie, 2005; Sagae and Lavie, 2006; Zhu et al., 2013), other chart-based systems (Petrov and Klein, 2007; Socher et al., 2013), and the systems with external semi supervised features or reranking (Charniak and Johnson, 2005; McClosky et al., 2006; Zhu et al., 2013). 2013). The score with the non-DP beam size = 16 and Z&C (89.1 F1) is the same as that reported in their paper (the features are the same). Final Experiment Table 6 compares our parsing system with those of previous studies. When we look at closed settings, where no external resource other than the training Penn Treebank is used, our system outperforms all other systems including the Berkeley parser (Petrov and Klein, 2007) and the Stanford parser (Socher et al., 2013) in terms of F1. The parsing systems with external features or reranking outperform"
P15-1148,N06-1022,0,0.028628,"xample of our feature projection. θGP is a weight vector with the GP, which collapses every c. θLF is with the LF, which collapses all elements in Table 4. s1 .c s1 .len s1 .ft s1 .shape s1 .fw s1 .rule s1 .bt s0 .rule s1 .bw Table 4: List of feature elements ignored in the LF. Grammar Projection (GP) Our first projection borrows the idea from the filter projection of Klein and Manning (2003a), in which the grammar symbols (nonterminals) are collapsed into a single label X. Our projection, however, does not collapse all the labels into X; instead, we utilize constituent labels in level 2 from Charniak et al. (2006), in which labels that tend to be head, such as S or VP are collapsed into HP and others are collapsed into MP. θG in Table 3 is an example of how feature weights are relaxed with this projection. Here we show each feature as a tuple including action name (a). Let πGP be a feature projection function: e.g., ((a ◦ s1 .c ◦ s1 .ft) = (S H ◦ VP ◦ NN)) 7→πGP ((a ◦ s1 .c ◦ s1 .ft) = (S H ◦ HP ◦ NN)). Formally, for k-th feature, the weight θGP (k) is determined by minimizing over the features collapsed by πGP : θGP (k) = min 1≤k0 ≤K:πGP (gk0 )=gk θ(k 0 ), where gk is the value of the k-th feature. Le"
P15-1148,P04-1015,0,0.379174,". It does not use probability and 2 Many existing constituent parsers use two kinds of reduce actions for selecting the direction of its head child while we do not distinguish these two. In our English experiments, we found no ambiguity for head selection in our binarized grammar (See Section 4). the local score is just φ(ai , pi−1 ) = θ |f (ai , pi−1 ). In practice, this global model is much stronger than the local MaxEnt model. However, training this model without any approximation is hard, and the common practice is to rely on well-known heuristics such as an early update with beam search (Collins and Roark, 2004). We are not aware of any previous study that succeeded in training a structured perceptron for parsing without approximation. We will show how this becomes possible in Section 3. 2.2 Previous Best-First Shift-Reduce Parsing The basic idea behind best-first search (BFS) for shift-reduce parsing is assuming each parser state as a node on a graph and then searching for the minimal cost path from a start state (node) to the final state. This is the idea of Sagae and Lavie (2006), and it was later refined by Zhao et al. (2013). BFS gives a priority to each state, and a state with the highest prior"
P15-1148,W02-1001,0,0.705438,"achieve BFS with the structured perceptron, and the second is how to apply that BFS to constituent parsing. Interestingly, the solution to the first problem makes the second problem relatively trivial. 3.1 Superiority of Structured Perceptron We must design each priority of a state to satisfy the superiority condition. φ(ai , pi−1 ) = θ |f (ai , pi−1 ) is the usual local score employed in structured perceptrons (Huang and Sagae, 2010) but we cannot use it as a local cost for two reasons. First, in our system, the best parse should have the lowest cost; it is opposite in the ordinary setting (Collins, 2002). We can resolve this conflict by changing the direction of structured perceptron training so that the best parse has the lowest score.4 Second, each φ(ai , pi−1 ) can take a negative value but the cost should always be positive. This is in contrast to the MaxEnt model in which the negative log probability is always positive. Our strategy is to add a constant offset δ to every local cost. If δ is large enough so that every score is positive, the superiority condition is satisfied.5 Unary Merging Though this technique solves the problem with the structured perceptron for a simpler shift-reduce"
P15-1148,P14-1022,0,0.141731,", the current state-of-the-art in shift-reduce parsing (Zhu et al., 2013). As we will see, this model change makes search quite hard, which motivates us to invent new feature templates as well as to improve the search algorithm. In existing parsers, features are commonly exploited from the parsing history, such as the top k elements on the stack. However, such features are expensive in terms of search efficiency. Instead of relying on features primarily from the stack, our features mostly come from the span of the top few nodes, an idea inspired by the recent empirical success in CRF parsing (Hall et al., 2014). We show that these span features also fit quite well in the shift-reduce system and lead to state-of-the-art accuracy. We further improve search with new A* heuristics that make optimal search for shift-reduce parsers with a structured perceptron tractable for the first time. The primary contribution of this paper is to demonstrate the effectiveness and the practicality of optimal search for shift-reduce parsing, especially when combined with appropriate features and efficient search. In English Penn Treebank experiments, our parser achieves an F1 score of 91.1 on test set at a speed of 13.6"
P15-1148,P10-1110,0,0.600225,"ith its exponential operation (2). Unfortunately, this is not the case in our global structured perceptron because the score of each action is just the sum of the feature weights. Resolving this search difficulty is the central problem of this paper; we illustrate this problem in Section 4 and resolve it in Section 5. 1535 2.3 Hypergraph Search of Zhao et al. (2013) The worst time complexity of BFS in Sagae and Lavie (2006) is exponential. For dependency parsing, Zhao et al. (2013) reduce it to polynomial by converting the search graph into a hypergraph by using the state merging technique of Huang and Sagae (2010). This hypergraph search is the basis of our parser, so we will briefly review it here. The algorithm is closely related to agendabased best-first parsing algorithms for PCFGs (Klein and Manning, 2001; Pauls and Klein, 2009). As in those algorithms, it maintains two data structures: a chart C that preserves processed states as well as a priority queue (agenda) Q. The difference is in the basic items processed in C and Q. In PCFG parsing, they are spans. Each span abstracts many derivations on that span and the chart maps a span to the best (lowest cost) derivation found so far. In shift-reduce"
P15-1148,W01-1812,0,0.0810909,"ch difficulty is the central problem of this paper; we illustrate this problem in Section 4 and resolve it in Section 5. 1535 2.3 Hypergraph Search of Zhao et al. (2013) The worst time complexity of BFS in Sagae and Lavie (2006) is exponential. For dependency parsing, Zhao et al. (2013) reduce it to polynomial by converting the search graph into a hypergraph by using the state merging technique of Huang and Sagae (2010). This hypergraph search is the basis of our parser, so we will briefly review it here. The algorithm is closely related to agendabased best-first parsing algorithms for PCFGs (Klein and Manning, 2001; Pauls and Klein, 2009). As in those algorithms, it maintains two data structures: a chart C that preserves processed states as well as a priority queue (agenda) Q. The difference is in the basic items processed in C and Q. In PCFG parsing, they are spans. Each span abstracts many derivations on that span and the chart maps a span to the best (lowest cost) derivation found so far. In shift-reduce parsing, the basic items are not spans but states, i.e., partial representations of the stack.3 We denote p = hi, j, sd ...s0 i where si is the i-th top subtree on the stack and s0 spans i to j. We e"
P15-1148,N03-1016,0,0.541403,"e by memoriz1539 3 ing the previously applied rule for each span (subtree). This is a bit costly, because it means we have to preserve labels of the left and right children for each node, which lead to an additional |G|2 factor of complexity. However, we will see that this problem can be alleviated by our heuristic cost functions in A* search described below. 5.2 a SH SH SH ◦ ◦ ◦ ◦ SH SH SH ◦ ◦ ◦ A* Search We now explain our A* search, another key technique for speeding up our search. To our knowledge, this is the first work to successfully apply A* search to shift-reduce parsing. A* parsing (Klein and Manning, 2003a) modifies the calculation of priority σ(pi ) for state pi . In BFS, it is basically the prefix cost, the sum of every local cost (Section 3.1), which we denote as βpi : X βpi = (φ(aj , pj−1 ) + δ). 1≤j≤i In A* parsing, σ(pi ) = βpi + h(pi ) where h(pi ) is a heuristic cost. βpi corresponds to the Viterbi inside cost of PCFG parsing (Klein and Manning, 2003a) while h(pi ) is the Viterbi outside cost, an approximation of the cost for the future best path (action sequence) from pi . h(pi ) must be a lower bound of the true Viterbi outside cost. In PCFG parsing, this is often achieved with a tec"
P15-1148,P03-1054,0,0.639901,"e by memoriz1539 3 ing the previously applied rule for each span (subtree). This is a bit costly, because it means we have to preserve labels of the left and right children for each node, which lead to an additional |G|2 factor of complexity. However, we will see that this problem can be alleviated by our heuristic cost functions in A* search described below. 5.2 a SH SH SH ◦ ◦ ◦ ◦ SH SH SH ◦ ◦ ◦ A* Search We now explain our A* search, another key technique for speeding up our search. To our knowledge, this is the first work to successfully apply A* search to shift-reduce parsing. A* parsing (Klein and Manning, 2003a) modifies the calculation of priority σ(pi ) for state pi . In BFS, it is basically the prefix cost, the sum of every local cost (Section 3.1), which we denote as βpi : X βpi = (φ(aj , pj−1 ) + δ). 1≤j≤i In A* parsing, σ(pi ) = βpi + h(pi ) where h(pi ) is a heuristic cost. βpi corresponds to the Viterbi inside cost of PCFG parsing (Klein and Manning, 2003a) while h(pi ) is the Viterbi outside cost, an approximation of the cost for the future best path (action sequence) from pi . h(pi ) must be a lower bound of the true Viterbi outside cost. In PCFG parsing, this is often achieved with a tec"
P15-1148,P11-1068,0,0.106279,"Missing"
P15-1148,J93-2004,0,0.0571696,"ituent Parsing This section evaluates the empirical performance of our best-first constituent parser that we built in the previous section. As mentioned in Section 2.2, the previous empirical success of best-first shiftreduce parsers might be due to the sparsity property of the MaxEnt model, which may not hold true in the structured perceptron. We investigate the validity of this assumption by comparing two systems, a locally trained MaxEnt model and a globally trained structured perceptron. Setting We follow the standard practice and train each model on section 2-21 of the WSJ Penn Treebank (Marcus et al., 1993), which is binarized using the algorithm in Zhang and Clark (2009) with the head rule of Collins (1999). We report the F1 scores for the development set of section 22. The Stanford POS tagger is used for part-ofspeech tagging.6 We used the EVALB program to evaluate parsing performance.7 Every experiment reported here was performed on hardware 6 7 http://nlp.stanford.edu/software/tagger.shtml http://nlp.cs.nyu.edu/evalb 160 120 Perceptron MaxEnt 80 40 0 0 20 40 60 Sentence length Figure 2: Comparison of the average number of the processed states of the structured perceptron with those of the Ma"
P15-1148,P06-1043,0,0.0583644,"3.7 2.2 93.4 8.4 13.6 90.1 90.3 89.0 90.3 90.7 89.5 90.2 90.5 89.3 6.1 3.3 0.7 91.2 92.2 91.1 91.8 92.6 91.5 91.5 92.4 91.3 2.1 1.2 47.6 Table 6: The final results for section 23 of the Penn Treebank. The systems with † are reported by authors running on different hardware. We divide baseline state-of-the-art systems into three categories: shift-reduce systems (Sagae and Lavie, 2005; Sagae and Lavie, 2006; Zhu et al., 2013), other chart-based systems (Petrov and Klein, 2007; Socher et al., 2013), and the systems with external semi supervised features or reranking (Charniak and Johnson, 2005; McClosky et al., 2006; Zhu et al., 2013). 2013). The score with the non-DP beam size = 16 and Z&C (89.1 F1) is the same as that reported in their paper (the features are the same). Final Experiment Table 6 compares our parsing system with those of previous studies. When we look at closed settings, where no external resource other than the training Penn Treebank is used, our system outperforms all other systems including the Berkeley parser (Petrov and Klein, 2007) and the Stanford parser (Socher et al., 2013) in terms of F1. The parsing systems with external features or reranking outperform our system. However, it"
P15-1148,N15-1108,0,0.666889,"ny unary chains as long as those in the training corpus by collapsing a chain into one rule. We preprocess the corpus in this way along with binarization (See Section 4). Note that this system is quite similar to the transition system for dependency parsing. The only changes are that we have several varieties of shift and reduce actions. This modification also makes it easy to apply an algorithm developed for dependency parsing to constituent parsing, such as dynamic programming with beam search (Huang and Sagae, 2010), which has not been applied into constituent parsing until quite recently (Mi and Huang, 2015) (See Section 7). Algorithm 1 BFS for Constituent Parsing; Only differences from Zhao et al. (2013) 1: procedure S HIFT(x, Q) 2: T RYA DD(sh(x), Q) 3: for y ∈ shu(x) do 4: T RYA DD(y, Q) 5: procedure R EDUCE(A, B, Q) 6: for (x, y) ∈ A × B do 7: for z ∈ re(x, y) ∪ reu(x, y) do 8: T RYA DD(z, Q) 3.2 BFS with Dynamic Programming Now applying BFS of Zhao et al. (2013) for dependency parsing into constituent parsing is not hard. Figure 1 shows the deductive system of dynamic programming, which is much similar to that in dependency parsing. One important change is that we include a cost for a shift"
P15-1148,J08-4003,0,0.400812,"l Joint Conference on Natural Language Processing, pages 1534–1544, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics which changes the current state into the new one. For example, S HIFT pops the front word from the queue and pushes it onto the stack, while R E DUCE (X) combines the top two elements on the stack into their parent.2 For example, if the top two elements on the stack are DT and NN, R E DUCE (NP) combines these by applying the CFG rule NP → DT NN. Unary Action The actions above are essentially the same as those in shift-reduce dependency parsing (Nivre, 2008), but a special action for constituent parsing U NARY(X) complicates the system and search. For example, if the top element on the stack is NN, U NARY(NP) changes it to NP by applying the rule NP → NN. In particular, this causes inconsistency in the numbers of actions between derivations (Zhu et al., 2013), which makes it hard to apply the existing best first search for dependency grammar to our system. We revisit this problem in Section 3.1. Model The model of a shift-reduce parser gives a score to each derivation, i.e., an action sequence a = (a1 , · · · , a|a |), in which each ai is a shift"
P15-1148,N09-1063,0,0.253517,"ral problem of this paper; we illustrate this problem in Section 4 and resolve it in Section 5. 1535 2.3 Hypergraph Search of Zhao et al. (2013) The worst time complexity of BFS in Sagae and Lavie (2006) is exponential. For dependency parsing, Zhao et al. (2013) reduce it to polynomial by converting the search graph into a hypergraph by using the state merging technique of Huang and Sagae (2010). This hypergraph search is the basis of our parser, so we will briefly review it here. The algorithm is closely related to agendabased best-first parsing algorithms for PCFGs (Klein and Manning, 2001; Pauls and Klein, 2009). As in those algorithms, it maintains two data structures: a chart C that preserves processed states as well as a priority queue (agenda) Q. The difference is in the basic items processed in C and Q. In PCFG parsing, they are spans. Each span abstracts many derivations on that span and the chart maps a span to the best (lowest cost) derivation found so far. In shift-reduce parsing, the basic items are not spans but states, i.e., partial representations of the stack.3 We denote p = hi, j, sd ...s0 i where si is the i-th top subtree on the stack and s0 spans i to j. We extract features from sd"
P15-1148,P06-2089,0,0.212153,"mation is hard, and the common practice is to rely on well-known heuristics such as an early update with beam search (Collins and Roark, 2004). We are not aware of any previous study that succeeded in training a structured perceptron for parsing without approximation. We will show how this becomes possible in Section 3. 2.2 Previous Best-First Shift-Reduce Parsing The basic idea behind best-first search (BFS) for shift-reduce parsing is assuming each parser state as a node on a graph and then searching for the minimal cost path from a start state (node) to the final state. This is the idea of Sagae and Lavie (2006), and it was later refined by Zhao et al. (2013). BFS gives a priority to each state, and a state with the highest priority (lowest cost) is always processed first. BFS guarantees that the first found goal is the best (optimality) if the superiority condition is satisfied: a state never has a lower cost than the costs of its previous states. Though the found parse is guaranteed to be optimal, in practice, current BFS-based systems are not stronger than other systems with approximate search (Zhu et al., 2013; Wang and Xue, 2014) since all existing systems are based on the MaxEnt model. With thi"
P15-1148,P13-1045,0,0.167441,"ky (2006) Zhu (2013) +semi LR 86.0 88.1 90.2 90.2 90.9 LP 86.1 87.8 90.7 90.6 91.2 F1 86.0 87.9 90.4 90.4 91.1 Sent./s. 3.7 2.2 93.4 8.4 13.6 90.1 90.3 89.0 90.3 90.7 89.5 90.2 90.5 89.3 6.1 3.3 0.7 91.2 92.2 91.1 91.8 92.6 91.5 91.5 92.4 91.3 2.1 1.2 47.6 Table 6: The final results for section 23 of the Penn Treebank. The systems with † are reported by authors running on different hardware. We divide baseline state-of-the-art systems into three categories: shift-reduce systems (Sagae and Lavie, 2005; Sagae and Lavie, 2006; Zhu et al., 2013), other chart-based systems (Petrov and Klein, 2007; Socher et al., 2013), and the systems with external semi supervised features or reranking (Charniak and Johnson, 2005; McClosky et al., 2006; Zhu et al., 2013). 2013). The score with the non-DP beam size = 16 and Z&C (89.1 F1) is the same as that reported in their paper (the features are the same). Final Experiment Table 6 compares our parsing system with those of previous studies. When we look at closed settings, where no external resource other than the training Penn Treebank is used, our system outperforms all other systems including the Berkeley parser (Petrov and Klein, 2007) and the Stanford parser (Socher"
P15-1148,P14-1069,0,0.171081,"that facilitate state merging of dynamic programming and A* search. Our system achieves 91.1 F1 on a standard English experiment, a level which cannot be reached by other beam-based systems even with large beam sizes.1 1 Introduction A parsing system comprises two components: a scoring model for a tree and a search algorithm. In shift-reduce parsing, the focus of most previous studies has been the former, typically by enriching feature templates, while the search quality has often been taken less seriously. For example, the current state-of-the-art parsers for constituency (Zhu et al., 2013; Wang and Xue, 2014) and dependency (Bohnet et al., 2013) both employ beam search with a constant beam size, which may suffer from severe search errors. This is contrary to ordinary PCFG parsing which, while it often uses some approximations, has nearly optimal quality (Petrov and Klein, 2007). In this paper, we instead investigate the question of whether we can obtain a practical shift-reduce parser with state-of-the-art accuracy by focusing on optimal search quality like PCFG parsing. We base our system on best-first search for shiftreduce parsing formulated in Zhao et al. (2013), but it differs from their appr"
P15-1148,W09-3825,0,0.555539,"er is to demonstrate the effectiveness and the practicality of optimal search for shift-reduce parsing, especially when combined with appropriate features and efficient search. In English Penn Treebank experiments, our parser achieves an F1 score of 91.1 on test set at a speed of 13.6 sentences per second. This score is in excess of that of a beam-based system with larger beam size and same speed. 2 2.1 Background and Related Work Shift-Reduce Constituent Parsing We first introduce the shift-reduce algorithm for constituent structures. For space reasons, our exposition is rather informal; See Zhang and Clark (2009) for details. A shift-reduce parser parses a sentence through transitions between states, each of which consists of two data structures of a stack and a queue. The stack preserves intermediate parse results, while the queue saves unprocessed tokens. At each step, a parser selects an action, 1534 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1534–1544, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics which changes the current state into the"
P15-1148,D13-1071,0,0.340465,"onstituency (Zhu et al., 2013; Wang and Xue, 2014) and dependency (Bohnet et al., 2013) both employ beam search with a constant beam size, which may suffer from severe search errors. This is contrary to ordinary PCFG parsing which, while it often uses some approximations, has nearly optimal quality (Petrov and Klein, 2007). In this paper, we instead investigate the question of whether we can obtain a practical shift-reduce parser with state-of-the-art accuracy by focusing on optimal search quality like PCFG parsing. We base our system on best-first search for shiftreduce parsing formulated in Zhao et al. (2013), but it differs from their approach in two points. First, we focus on constituent parsing while they use dependency grammar. Second, and more crucially, they use a locally trained MaxEnt model, which is simple but not strong, while we explore 1 The open source software of our system is available at https://github.com/mynlp/optsr. a structured perceptron, the current state-of-the-art in shift-reduce parsing (Zhu et al., 2013). As we will see, this model change makes search quite hard, which motivates us to invent new feature templates as well as to improve the search algorithm. In existing par"
P15-1148,P13-1043,0,0.317926,"feature templates that facilitate state merging of dynamic programming and A* search. Our system achieves 91.1 F1 on a standard English experiment, a level which cannot be reached by other beam-based systems even with large beam sizes.1 1 Introduction A parsing system comprises two components: a scoring model for a tree and a search algorithm. In shift-reduce parsing, the focus of most previous studies has been the former, typically by enriching feature templates, while the search quality has often been taken less seriously. For example, the current state-of-the-art parsers for constituency (Zhu et al., 2013; Wang and Xue, 2014) and dependency (Bohnet et al., 2013) both employ beam search with a constant beam size, which may suffer from severe search errors. This is contrary to ordinary PCFG parsing which, while it often uses some approximations, has nearly optimal quality (Petrov and Klein, 2007). In this paper, we instead investigate the question of whether we can obtain a practical shift-reduce parser with state-of-the-art accuracy by focusing on optimal search quality like PCFG parsing. We base our system on best-first search for shiftreduce parsing formulated in Zhao et al. (2013), but it di"
P15-1148,N07-1051,0,\N,Missing
P15-1148,J03-4003,0,\N,Missing
P15-1148,Q13-1034,0,\N,Missing
P15-2023,W10-1749,0,0.0237746,"r to train such a classifier, we need an oracle label, W or M , for each node. Since we cannot rely on manual label annotation, we define a procedure to obtain oracle labels from word alignments. The principal idea is that we determine an oracle label of each node v(i, p, j) so that it maximizes Kendall’s τ under v(i, p, j). This is intuitively a straightforward idea, because our objective is to find a monotonic order, which indicates maximization of Kendall’s τ . In the context of statistical machine translation, Kendall’s τ is used as an evaluation metric for monotonicity of word orderings (Birch and Osborne, 2010; Isozaki et al., 2010a; Talbot et al., 2011). Given an integer list x = x1 , . . . , xn , τ (x) −c(a(p + 1, j) · a(i, p)), where · indicates a concatenation of vectors. Then, a node that has s(v(i, p, j)) < 0 is assigned W , and a node that has s(v(i, p, j)) > 0 is assigned M . All the nodes scored as s = 0 are excluded from the training data, because they are noisy and ambiguous in terms of binary classification. 2.3 Proof of Independency over Constituency The question then arises: Can oracle labels achieve the best reordering in total? We see this 2 We used median values to approximate this"
P15-2023,P05-1066,0,0.109678,"dering method, and is comparable with, or superior to, state-of-the-art methods that rely on language-specific heuristics. Our contributions are summarized as follows: Introduction Current statistical machine translation systems suffer from major accuracy degradation in distant languages, primarily because they utilize exceptionally dissimilar word orders. One promising solution to this problem is preordering, in which source sentences are reordered to resemble the target language word orders, after which statistical machine translation is applied to reordered sentences (Xia and McCord, 2004; Collins et al., 2005). This is particularly effective for distant language pairs such as English and Japanese (Isozaki et al., 2010b). Among such preordering, one of the simplest and straightforward model is a discriminative preordering model (Li et al., 2007), which classifies whether children of each constituent node should be reordered, given binary trees.1 This simple model has, however, difficulty to find oracle labels. Yang et al. (2012) proposed a method to approximate oracle labels along dependency trees. The present paper proposes a new procedure to find oracle labels. The main idea is simple: we • We def"
P15-2023,W08-0509,0,0.0154767,"BLEU ∆ RIBES ∆ ∆ +3.43 70.22 78.07 +7.85 30.51 34.13 +3.62 68.90 76.76 +7.86 29.99 33.14 +3.15 +2.99 +0.49 +2.84 +3.16 Table 5: Comparison with previous systems in Japanese-to-English translation, of which scores are retrieved from their papers. Boldfaces indicate the highest scores and differences. 8 training sets, used the first 1000 sentences in NTCIR-8 development set, and then fetched both the NTCIR-9 and NTCIR-10 testing sets. The machine translation experiments pipelined Moses 3 (Koehn et al., 2007) with lexicalized reordering, SRILM 1.7.0 (Stolcke et al., 2011) in 6-gram order, MGIZA (Gao and Vogel, 2008), and RIBES (Isozaki et al., 2010a) and BLEU (Papineni et al., 2002) for evaluation. Binary constituent parsing in Japanese used Haruniwa (Fang et al., 2014), Berkeley parser 1.7 (Petrov and Klein, 2007), Comainu 0.7.0 (Kozawa et al., 2014), MeCab 0.996 (Kudo et al., 2004), and Unidic 2.1.2. We explore two types of word alignment data for training our preordering model. The first data (Giza) is created by running an unsupervised aligner Giza (Och and Ney, 2003) on the training data (3 million sentences). The second data (Nile) is developed by training a supervised aligner Nile (Riesa et al., 2"
P15-2023,P12-2061,0,0.705339,"to our oracle labels, hence c(a) and τ (a) of entire sentence.3 Essentially, our decisions on each node are equivalent to sorting a list consists of left and right points, while the order of the points inside of left and right lists are left untouched. We determine oracle labels for a given constituent tree by computing s(v(i, p, j)) for every v(i, p, j) independently. 3 Experiment 3.1 Experimental Settings We perform experiments over the NTCIR patent corpus (Goto et al., 2011) that consists of more than 3 million sentences in English and Japanese. Following conventional literature settings (Goto et al., 2012; Hayashi et al., 2013), we used all 3 million sentences from the NTCIR-7 and NTCIR3 Oracle labels guarantee τ (a) ≥ 0, but not τ (a) = 1, because parsed trees will not correspond to word alignments. 141 test9 BLEU Reordering Methods DL RIBES ∆ Moses Proposed preordering 20 10 69.88 77.97 +8.09 Moses (Hoshino et al., 2013) Preordering (Hoshino et al., 2013) Moses (Goto et al., 2012) Moses-chart (Goto et al., 2012) Postordering (Goto et al., 2012) Moses (Hayashi et al., 2013) Postordering (Hayashi et al., 2013) 20 10 20 68.08 72.37 68.28 70.64 75.48 69.31 76.46 20 0 +4.29 +2.36 +7.20 +7.15 30.1"
P15-2023,D13-1139,1,0.949755,"ls, hence c(a) and τ (a) of entire sentence.3 Essentially, our decisions on each node are equivalent to sorting a list consists of left and right points, while the order of the points inside of left and right lists are left untouched. We determine oracle labels for a given constituent tree by computing s(v(i, p, j)) for every v(i, p, j) independently. 3 Experiment 3.1 Experimental Settings We perform experiments over the NTCIR patent corpus (Goto et al., 2011) that consists of more than 3 million sentences in English and Japanese. Following conventional literature settings (Goto et al., 2012; Hayashi et al., 2013), we used all 3 million sentences from the NTCIR-7 and NTCIR3 Oracle labels guarantee τ (a) ≥ 0, but not τ (a) = 1, because parsed trees will not correspond to word alignments. 141 test9 BLEU Reordering Methods DL RIBES ∆ Moses Proposed preordering 20 10 69.88 77.97 +8.09 Moses (Hoshino et al., 2013) Preordering (Hoshino et al., 2013) Moses (Goto et al., 2012) Moses-chart (Goto et al., 2012) Postordering (Goto et al., 2012) Moses (Hayashi et al., 2013) Postordering (Hayashi et al., 2013) 20 10 20 68.08 72.37 68.28 70.64 75.48 69.31 76.46 20 0 +4.29 +2.36 +7.20 +7.15 30.12 33.55 27.57 30.56 30."
P15-2023,I13-1147,1,0.859223,"ng s(v(i, p, j)) for every v(i, p, j) independently. 3 Experiment 3.1 Experimental Settings We perform experiments over the NTCIR patent corpus (Goto et al., 2011) that consists of more than 3 million sentences in English and Japanese. Following conventional literature settings (Goto et al., 2012; Hayashi et al., 2013), we used all 3 million sentences from the NTCIR-7 and NTCIR3 Oracle labels guarantee τ (a) ≥ 0, but not τ (a) = 1, because parsed trees will not correspond to word alignments. 141 test9 BLEU Reordering Methods DL RIBES ∆ Moses Proposed preordering 20 10 69.88 77.97 +8.09 Moses (Hoshino et al., 2013) Preordering (Hoshino et al., 2013) Moses (Goto et al., 2012) Moses-chart (Goto et al., 2012) Postordering (Goto et al., 2012) Moses (Hayashi et al., 2013) Postordering (Hayashi et al., 2013) 20 10 20 68.08 72.37 68.28 70.64 75.48 69.31 76.46 20 0 +4.29 +2.36 +7.20 +7.15 30.12 33.55 27.57 30.56 30.20 30.69 33.04 29.43 32.59 test10 BLEU ∆ RIBES ∆ ∆ +3.43 70.22 78.07 +7.85 30.51 34.13 +3.62 68.90 76.76 +7.86 29.99 33.14 +3.15 +2.99 +0.49 +2.84 +3.16 Table 5: Comparison with previous systems in Japanese-to-English translation, of which scores are retrieved from their papers. Boldfaces indicate th"
P15-2023,D10-1092,1,0.885359,"Missing"
P15-2023,W10-1736,1,0.961659,"heuristics. Our contributions are summarized as follows: Introduction Current statistical machine translation systems suffer from major accuracy degradation in distant languages, primarily because they utilize exceptionally dissimilar word orders. One promising solution to this problem is preordering, in which source sentences are reordered to resemble the target language word orders, after which statistical machine translation is applied to reordered sentences (Xia and McCord, 2004; Collins et al., 2005). This is particularly effective for distant language pairs such as English and Japanese (Isozaki et al., 2010b). Among such preordering, one of the simplest and straightforward model is a discriminative preordering model (Li et al., 2007), which classifies whether children of each constituent node should be reordered, given binary trees.1 This simple model has, however, difficulty to find oracle labels. Yang et al. (2012) proposed a method to approximate oracle labels along dependency trees. The present paper proposes a new procedure to find oracle labels. The main idea is simple: we • We define a method for obtaining oracle labels in discriminative preordering as the maximization of Kendall’s τ . •"
P15-2023,N07-1051,0,0.0573074,"which scores are retrieved from their papers. Boldfaces indicate the highest scores and differences. 8 training sets, used the first 1000 sentences in NTCIR-8 development set, and then fetched both the NTCIR-9 and NTCIR-10 testing sets. The machine translation experiments pipelined Moses 3 (Koehn et al., 2007) with lexicalized reordering, SRILM 1.7.0 (Stolcke et al., 2011) in 6-gram order, MGIZA (Gao and Vogel, 2008), and RIBES (Isozaki et al., 2010a) and BLEU (Papineni et al., 2002) for evaluation. Binary constituent parsing in Japanese used Haruniwa (Fang et al., 2014), Berkeley parser 1.7 (Petrov and Klein, 2007), Comainu 0.7.0 (Kozawa et al., 2014), MeCab 0.996 (Kudo et al., 2004), and Unidic 2.1.2. We explore two types of word alignment data for training our preordering model. The first data (Giza) is created by running an unsupervised aligner Giza (Och and Ney, 2003) on the training data (3 million sentences). The second data (Nile) is developed by training a supervised aligner Nile (Riesa et al., 2011) with manually annotated 8,000 sentences, then applied the trained alignment model to remaining training data. In the evaluation on manually annotated 1,000 sentences4 , Giza achieved F1 50.1 score,"
P15-2023,D11-1046,0,0.173982,"Missing"
P15-2023,W04-3250,0,0.55929,"Missing"
P15-2023,2011.mtsummit-papers.36,1,0.933737,"Missing"
P15-2023,W11-2102,0,0.013815,"label, W or M , for each node. Since we cannot rely on manual label annotation, we define a procedure to obtain oracle labels from word alignments. The principal idea is that we determine an oracle label of each node v(i, p, j) so that it maximizes Kendall’s τ under v(i, p, j). This is intuitively a straightforward idea, because our objective is to find a monotonic order, which indicates maximization of Kendall’s τ . In the context of statistical machine translation, Kendall’s τ is used as an evaluation metric for monotonicity of word orderings (Birch and Osborne, 2010; Isozaki et al., 2010a; Talbot et al., 2011). Given an integer list x = x1 , . . . , xn , τ (x) −c(a(p + 1, j) · a(i, p)), where · indicates a concatenation of vectors. Then, a node that has s(v(i, p, j)) < 0 is assigned W , and a node that has s(v(i, p, j)) > 0 is assigned M . All the nodes scored as s = 0 are excluded from the training data, because they are noisy and ambiguous in terms of binary classification. 2.3 Proof of Independency over Constituency The question then arises: Can oracle labels achieve the best reordering in total? We see this 2 We used median values to approximate this y-th word in the target sentence for simplic"
P15-2023,W04-3230,0,0.165103,"Missing"
P15-2023,C04-1073,0,0.176348,"rms a rule-based preordering method, and is comparable with, or superior to, state-of-the-art methods that rely on language-specific heuristics. Our contributions are summarized as follows: Introduction Current statistical machine translation systems suffer from major accuracy degradation in distant languages, primarily because they utilize exceptionally dissimilar word orders. One promising solution to this problem is preordering, in which source sentences are reordered to resemble the target language word orders, after which statistical machine translation is applied to reordered sentences (Xia and McCord, 2004; Collins et al., 2005). This is particularly effective for distant language pairs such as English and Japanese (Isozaki et al., 2010b). Among such preordering, one of the simplest and straightforward model is a discriminative preordering model (Li et al., 2007), which classifies whether children of each constituent node should be reordered, given binary trees.1 This simple model has, however, difficulty to find oracle labels. Yang et al. (2012) proposed a method to approximate oracle labels along dependency trees. The present paper proposes a new procedure to find oracle labels. The main idea"
P15-2023,P14-2091,0,0.0664321,"bstantial gain in RIBES, we attained a rather comparable gain in BLEU. The investigation of our translation suggests that insufficient generation of English articles caused a significant degradation in the BLEU score. Previous systems listed in Table 5 incorporated article generation and demonstrated its positive effect (Goto et al., 2012; Hayashi et al., 2013). While we achieved state-ofthe-art accuracy without language-specific techniques, it is also a promising direction to integrate our preordering method with language-specific techniques such as article generation and subject generation (Kudo et al., 2014). 3.2 Result Table 4 shows the performance of our method, which indicates that our preordering significantly improved translation accuracy in both RIBES and BLEU scores, from the baseline result attained by Moses without preordering. In particular, the preordering model trained with the Giza data revealed a substantial improvement, while the use of the Nile data further improves accuracy. This suggests that our method is particularly effective when high-accuracy word alignments are given. In 4 5 We could not find a comparable report using tree-based machine translation systems apart from Moses"
P15-2023,P12-1096,0,0.368513,"sentences are reordered to resemble the target language word orders, after which statistical machine translation is applied to reordered sentences (Xia and McCord, 2004; Collins et al., 2005). This is particularly effective for distant language pairs such as English and Japanese (Isozaki et al., 2010b). Among such preordering, one of the simplest and straightforward model is a discriminative preordering model (Li et al., 2007), which classifies whether children of each constituent node should be reordered, given binary trees.1 This simple model has, however, difficulty to find oracle labels. Yang et al. (2012) proposed a method to approximate oracle labels along dependency trees. The present paper proposes a new procedure to find oracle labels. The main idea is simple: we • We define a method for obtaining oracle labels in discriminative preordering as the maximization of Kendall’s τ . • We give a theoretical background to Kendall’s τ based reordering for binary constituent trees. • We achieve state-of-the-art accuracy in Japanese-to-English translation with a simple method without language-specific heuristics. 1 It is also possible to use n-ary trees (Li et al., 2007; Yang et al., 2012), but we ke"
P15-2023,D13-1049,0,0.0380846,"suke Mori, Toshiaki Nakazawa, Graham Neubig, Hiroshi Noji, and anonymous reviewers for their insightful comments. Li et al. (2007) proposed a simple discriminative preordering model as described in Section 2.1. They employed heuristics that utilize Giza to align their training sentences, then sort source words to resemble target word indices. After that, sorted source sentences without overlaps are used to train the model. They gained BLEU +1.54 improvement in Chinese-to-English evaluation. Our proposal follows their model, while we do not rely on their heuristics for preparing training data. Lerner and Petrov (2013) proposed another discriminative preordering model along dependency trees, which classifies whether the parent of each node should be the head in target language. They reported BLEU +3.7 improvement in English-to-Japanese translation. Hoshino et al. (2013) proposed a similar but rule-based method for Japanese-to-English dependency preordering. Yang et al. (2012) proposed a method to produce oracle reordering in the discriminative preordering model along dependency trees. Their idea behind is to minimize word alignment crossing recursively, which is essentially the same reordering objective as"
P15-2023,P07-1091,0,0.642315,"or accuracy degradation in distant languages, primarily because they utilize exceptionally dissimilar word orders. One promising solution to this problem is preordering, in which source sentences are reordered to resemble the target language word orders, after which statistical machine translation is applied to reordered sentences (Xia and McCord, 2004; Collins et al., 2005). This is particularly effective for distant language pairs such as English and Japanese (Isozaki et al., 2010b). Among such preordering, one of the simplest and straightforward model is a discriminative preordering model (Li et al., 2007), which classifies whether children of each constituent node should be reordered, given binary trees.1 This simple model has, however, difficulty to find oracle labels. Yang et al. (2012) proposed a method to approximate oracle labels along dependency trees. The present paper proposes a new procedure to find oracle labels. The main idea is simple: we • We define a method for obtaining oracle labels in discriminative preordering as the maximization of Kendall’s τ . • We give a theoretical background to Kendall’s τ based reordering for binary constituent trees. • We achieve state-of-the-art accu"
P15-2023,P14-2024,0,0.0127024,"hows the performance of our method, which indicates that our preordering significantly improved translation accuracy in both RIBES and BLEU scores, from the baseline result attained by Moses without preordering. In particular, the preordering model trained with the Giza data revealed a substantial improvement, while the use of the Nile data further improves accuracy. This suggests that our method is particularly effective when high-accuracy word alignments are given. In 4 5 We could not find a comparable report using tree-based machine translation systems apart from Moses-chart; nevertheless, Neubig and Duh (2014) reported that their forestto-string system on the same corpus, which is unfortunately evaluated on the different testing data (test7), showed RIBES +6.19 (75.94) and BLEU +2.93 (33.70) improvements. Although not directly comparable, our method achieves a comparable or superior improvement. This testing data is excluded from latter experiments. 142 4 Related Work Acknowledgments We would like to thank Kevin Duh, Atsushi Fujita, Taku Kudo, Shinsuke Mori, Toshiaki Nakazawa, Graham Neubig, Hiroshi Noji, and anonymous reviewers for their insightful comments. Li et al. (2007) proposed a simple disc"
P15-2023,J03-1002,0,0.00843586,"ments pipelined Moses 3 (Koehn et al., 2007) with lexicalized reordering, SRILM 1.7.0 (Stolcke et al., 2011) in 6-gram order, MGIZA (Gao and Vogel, 2008), and RIBES (Isozaki et al., 2010a) and BLEU (Papineni et al., 2002) for evaluation. Binary constituent parsing in Japanese used Haruniwa (Fang et al., 2014), Berkeley parser 1.7 (Petrov and Klein, 2007), Comainu 0.7.0 (Kozawa et al., 2014), MeCab 0.996 (Kudo et al., 2004), and Unidic 2.1.2. We explore two types of word alignment data for training our preordering model. The first data (Giza) is created by running an unsupervised aligner Giza (Och and Ney, 2003) on the training data (3 million sentences). The second data (Nile) is developed by training a supervised aligner Nile (Riesa et al., 2011) with manually annotated 8,000 sentences, then applied the trained alignment model to remaining training data. In the evaluation on manually annotated 1,000 sentences4 , Giza achieved F1 50.1 score, while Nile achieved F1 86.9 score, for word alignment task. addition, we achieved modest improvements even with DL=0 (no distortion allowed), which indicates the monotonicity of our reordered sentences. Table 5 shows a comparison of the proposed method with a ru"
P15-2023,P02-1040,0,0.092262,"6.76 +7.86 29.99 33.14 +3.15 +2.99 +0.49 +2.84 +3.16 Table 5: Comparison with previous systems in Japanese-to-English translation, of which scores are retrieved from their papers. Boldfaces indicate the highest scores and differences. 8 training sets, used the first 1000 sentences in NTCIR-8 development set, and then fetched both the NTCIR-9 and NTCIR-10 testing sets. The machine translation experiments pipelined Moses 3 (Koehn et al., 2007) with lexicalized reordering, SRILM 1.7.0 (Stolcke et al., 2011) in 6-gram order, MGIZA (Gao and Vogel, 2008), and RIBES (Isozaki et al., 2010a) and BLEU (Papineni et al., 2002) for evaluation. Binary constituent parsing in Japanese used Haruniwa (Fang et al., 2014), Berkeley parser 1.7 (Petrov and Klein, 2007), Comainu 0.7.0 (Kozawa et al., 2014), MeCab 0.996 (Kudo et al., 2004), and Unidic 2.1.2. We explore two types of word alignment data for training our preordering model. The first data (Giza) is created by running an unsupervised aligner Giza (Och and Ney, 2003) on the training data (3 million sentences). The second data (Nile) is developed by training a supervised aligner Nile (Riesa et al., 2011) with manually annotated 8,000 sentences, then applied the train"
P15-2023,P07-2045,0,\N,Missing
P15-2046,D11-1096,0,0.0298796,"e 2(a) shows the unlexicalized dependency tree for our example sentence. Our lexicalized tree representation is derived from the unlexicalized representation by attaching words as terminal nodes. In order to reduce the number of nodes, we collapse the relation and entity tags with their corresponding POS tags. Figure 2(b) shows the resulting tree for the example sentence. 4.3 Lexicalized Tree Kernel Since simply adding words to lexicalize a tree kernel leads to sparsity problems, a type of smoothing must be applied. Bloehdorn and Moschitti (2007) measure the similarity of words using WordNet. Croce et al. (2011) employ word vectors created by Singular Value Decomposition (Golub and Kahan., 1965) from a word co-occurrence matrix. Plank and Moschitti (2013) use word vectors created by Brown clustering algorithm (Brown et al., 1992), which is another smoothed word representation that represents words as binary vectors. Srivastava et al. (2013) use word embeddings of Collobert and Weston (2008), but their tree kernel does not incorporate POS tags or dependency labels. We propose using word embeddings created by a neural network model (Collobert and Weston, 2008), in which words are represented by n-dimen"
P15-2046,P14-5010,0,0.00392353,"consists of three modules: entity extraction, relation candidate extraction, and tree 279 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 279–284, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 1: Our Open IE system structure. kernel filtering. The system structure is outlined in Figure 1. We identify named entities, parse sentences, and convert constituency trees into dependency structures using the Stanford tools (Manning et al., 2014). Entities within a fixed token distance (set to 20 according to development results) are extracted as pairs {&lt; E1 , E2 >}. We then identify relation candidates R for each entity pair in a sentence, using dependency paths. Finally, the candidate triples {&lt; E1 , R, E2 >} are paired with their corresponding tree structures, and provided as input to the SVM tree kernel. Our Open IE system outputs the triples that are classified as positive. In the following sections, we describe the components of the system in more detail. 3 The relation candidates are manually annotated as correct/incorrect in t"
P15-2046,D12-1048,0,0.0381022,"the results of the candidate extraction module. For semantic tasks, the design of input structures to tree kernels is as important as the design of the tree kernels themselves. In this section, we introduce our tree structure, describe the prior basic tree kernel, and finally present our lexicalized tree kernel function. Relation Candidates 4.1 Tree Structure Relation candidates are words that may represent a relation between two entities. We consider only lemmatized nouns, verbs and adjectives that are within two dependency links from either of the entities. Following Wu and Weld (2010) and Mausam et al. (2012), we use dependency patterns rather than POS patterns, which allows us to identify relation candidates which are farther away from entities in terms of token distance. We extract the first two content words along the dependency path between E1 and E2 . In the following example, the path is E1 → encounter → build → E2 , and the two relation word candidates between “Mr. Wathen” and “Plant Security Service” are encounter and build, of which the latter is the correct one. In order to formulate the input for tree kernel models, we need to convert the dependency path to a tree-like structure with un"
P15-2046,D13-1043,0,0.0168662,"(n1 , n2 ) = 0 if the productions (contextfree rules) of n1 and n2 are different. 2. Otherwise, ∆(n1 , n2 ) = 1 if n1 and n2 are matching pre-terminals (POS tags). 3. Otherwise, ∏ ∆(n1 , n2 ) = j (1 + ∆(c(n1 , j), c(n2 , j)), where c(n, j) is the jth child of n. 281 5 Smoothing none (Xu13) none Brown (PM13) Brown (PM13) Brown (PM13) embedding embedding embedding Experiments Here we evaluate alternative tree kernel configurations, and compare our Open IE system to previous work. We perform experiments on three datasets (Table 1): the Penn Treebank set (Xu et al., 2013), the New York Times set (Mesquita et al., 2013), and the ClueWeb set which we created for this project from a large collection of web pages.1 The models are trained on the Penn Treebank training set and tested on the three test sets, of which the Penn Treebank set is in-domain, and the other two sets are out-of-domain. For word embedding and Brown clustering representations, we use the data provided by Turian et al. (2010). The SVM parameters, as well as the Brown cluster size and code length, are tuned on the development set. Set Penn Treebank New York Times ClueWeb train 750 — — dev 100 300 450 Lexical info none all words relation only a"
P15-2046,P13-1147,0,0.0837377,"zed representation by attaching words as terminal nodes. In order to reduce the number of nodes, we collapse the relation and entity tags with their corresponding POS tags. Figure 2(b) shows the resulting tree for the example sentence. 4.3 Lexicalized Tree Kernel Since simply adding words to lexicalize a tree kernel leads to sparsity problems, a type of smoothing must be applied. Bloehdorn and Moschitti (2007) measure the similarity of words using WordNet. Croce et al. (2011) employ word vectors created by Singular Value Decomposition (Golub and Kahan., 1965) from a word co-occurrence matrix. Plank and Moschitti (2013) use word vectors created by Brown clustering algorithm (Brown et al., 1992), which is another smoothed word representation that represents words as binary vectors. Srivastava et al. (2013) use word embeddings of Collobert and Weston (2008), but their tree kernel does not incorporate POS tags or dependency labels. We propose using word embeddings created by a neural network model (Collobert and Weston, 2008), in which words are represented by n-dimensional real valued vectors. Each dimension represents a latent feature of the word that reflects its semantic and syntactic properties. Next, we d"
P15-2046,N13-1008,0,0.186228,"Missing"
P15-2046,D13-1144,0,0.018378,"the resulting tree for the example sentence. 4.3 Lexicalized Tree Kernel Since simply adding words to lexicalize a tree kernel leads to sparsity problems, a type of smoothing must be applied. Bloehdorn and Moschitti (2007) measure the similarity of words using WordNet. Croce et al. (2011) employ word vectors created by Singular Value Decomposition (Golub and Kahan., 1965) from a word co-occurrence matrix. Plank and Moschitti (2013) use word vectors created by Brown clustering algorithm (Brown et al., 1992), which is another smoothed word representation that represents words as binary vectors. Srivastava et al. (2013) use word embeddings of Collobert and Weston (2008), but their tree kernel does not incorporate POS tags or dependency labels. We propose using word embeddings created by a neural network model (Collobert and Weston, 2008), in which words are represented by n-dimensional real valued vectors. Each dimension represents a latent feature of the word that reflects its semantic and syntactic properties. Next, we describe how we embed these vectors into tree kernels. Our lexicalized tree kernel model is the same as SST, except in the following case: if n1 and n2 are matching pre-terminals (POS tags),"
P15-2046,P10-1040,0,0.0377711,"we evaluate alternative tree kernel configurations, and compare our Open IE system to previous work. We perform experiments on three datasets (Table 1): the Penn Treebank set (Xu et al., 2013), the New York Times set (Mesquita et al., 2013), and the ClueWeb set which we created for this project from a large collection of web pages.1 The models are trained on the Penn Treebank training set and tested on the three test sets, of which the Penn Treebank set is in-domain, and the other two sets are out-of-domain. For word embedding and Brown clustering representations, we use the data provided by Turian et al. (2010). The SVM parameters, as well as the Brown cluster size and code length, are tuned on the development set. Set Penn Treebank New York Times ClueWeb train 750 — — dev 100 300 450 Lexical info none all words relation only all words excl. entities relation only all words excl. entities P 85.7 89.8 88.7 84.5 86.2 93.9 93.8 95.9 R 72.7 66.7 71.2 74.2 75.8 69.7 68.2 71.2 F1 78.7 76.5 79.0 79.0 80.7 80.0 79.0 81.7 Table 2: The results of relation extraction with alternative smoothing and lexicalization techniques on the Penn Treebank set (with our relation candidate extraction and tree structure). la"
P15-2046,P10-1013,0,0.0249908,"elation candidates from the results of the candidate extraction module. For semantic tasks, the design of input structures to tree kernels is as important as the design of the tree kernels themselves. In this section, we introduce our tree structure, describe the prior basic tree kernel, and finally present our lexicalized tree kernel function. Relation Candidates 4.1 Tree Structure Relation candidates are words that may represent a relation between two entities. We consider only lemmatized nouns, verbs and adjectives that are within two dependency links from either of the entities. Following Wu and Weld (2010) and Mausam et al. (2012), we use dependency patterns rather than POS patterns, which allows us to identify relation candidates which are farther away from entities in terms of token distance. We extract the first two content words along the dependency path between E1 and E2 . In the following example, the path is E1 → encounter → build → E2 , and the two relation word candidates between “Mr. Wathen” and “Plant Security Service” are encounter and build, of which the latter is the correct one. In order to formulate the input for tree kernel models, we need to convert the dependency path to a tr"
P15-2046,N13-1107,1,0.921102,"path to a tree-like structure with unlabelled edges. The target dependency path is the shortest path that includes the triple and other content words along the path. Consider the following example, which is a simplified representation of the sentence “Georgia-Pacific Corp.’s unsolicited $3.9 billion bid for Great Northern Nekoosa Corp. was hailed by Wall Street.” The candidate triple identified by the relation candidate extraction module is &lt;Georgia-Pacific Corp., bid, Great Northern Nekoosa Corp.>. Our unlexicalized tree representation model is similar to the unlexicalized representations of Xu et al. (2013), except that instead of using the POS tag of the path’s head word as the root, we create an abstract Root node. We preserve the dependency labels, POS tags, and entity information as tree nodes: (a) the top dependency labels are inIf there are no content words on the dependency path between the two entities, we instead consider words that are directly linked to either of them. In the following example, the only relation candidate is the word battle, which is directly linked to “Edelman.” 280 (a) An un-lexicalized dependency tree. (b) A lexicalized dependency tree. Figure 2: An unlexicalized t"
P15-2046,J92-4003,0,\N,Missing
P15-2046,P02-1034,0,\N,Missing
P16-4015,D15-1296,0,0.184905,"to specify semantic patterns. Templates for English and Japanese accompany our software, and they are easy to understand, use and extend to cover other linguistic phenomena or languages. We also provide scripts to use our semantic representations in a textual entailment task, and a visualization tool to display semantically augmented CCG trees in HTML. 1 4 Introduction 2 We are motivated by NLP problems that benefit from any degree of computer language understanding or semantic parsing. Two prominent examples are Textual Entailment and QuestionAnswering, where the most successful approaches (Abzianidze, 2015; Berant et al., 2013) require symbolic representations of the semantics of sentences. We are inspired by the theoretical developments in the formal semantics literature, where higher-order logical (HOL) formulas are used to derive meaning representations (MR); despite what is typically believed in the NLP community, Mineshima et al. (2015) demonstrated that HOL can be used effectively at a reasonable speed. In this paper, we describe ccg2lambda, our system to obtain MRs given derivations (trees) of a Combinatory Categorial Grammar (CCG) (Steedman, 2000). In order to obtain the MRs, our system"
P16-4015,D14-1059,0,0.0675814,"Missing"
P16-4015,W07-1431,0,0.0422328,"e or tea. premise2 : Some woman did not order coffee. conclusion: Some woman ordered tea. r u l e =” l e x ” /&gt; &lt;s p a n . . . /&gt; &lt;/ c c g&gt; &lt;s e m a n t i c s&gt; &lt;s p a n i d =” s 1 ” c h i l d =” s 2 ” sem=”y . t e a ( y ) ” t y p e =” t e a : E n t i t y −&gt; P r o p ” /&gt; &lt;s p a n . . . /&gt; &lt;/ s e m a n t i c s&gt; &lt;/ s e n t e n c e&gt; &lt;/ s e n t e n c e s&gt; &lt;/ r o o t&gt; Figure 3: XML output of the semantic composition. Span nodes of the semantics tag contain logical semantic representations of that constituent. Contrarily to other textual entailment systems based on logics (Angeli and Manning, 2014; MacCartney and Manning, 2007), we do not assume single-premise problems, which makes our system more general. The MRs of the problem above are: element characteristics as XML node attributes. For example, the base and surface forms, and the POS tag of a token are all represented as XML attributes in a &lt;token&gt; tag. p1 : ∀x.(woman(x) → ∃y.((tea(y) ∨ coffee(y)) ∧ order(x, y))) p2 : ∃x.(woman(x) ∧ ¬∃y.(coffee(y) ∧ order(x, y))) Our semantic composition produces the &lt;semantics&gt; tag, which has as many children nodes (&lt;span&gt;) as the CCG tree, the same span identifiers and structure. However, semantic spans also have a “sem” attr"
P16-4015,P14-5010,0,0.00385199,"rrently supports the C&C parser (Clark and Curran, 2004) for English, and Jigg (Noji and Miyao, 2016) for Japanese. The second stage is the semantic composition, where MRs are constructed compositionally over CCG trees using lambda calculus, thus allowing higher-order logics if necessary. To this end, our system is guided by the compositional rules of the CCG tree and the semantic templates provided by the user. In Section 4 we describe in detail how these semantic templates are specified and how they control the semantic outputs. The output of this stage is a Stanford CoreNLP-style XML file (Manning et al., 2014) where each sentence has three XML nodes: &lt;tokens&gt;, &lt;ccg&gt; and &lt;semantics&gt;. Thus, sentence semantics can simply be read off the root node of the CCG tree. In the case of recognizing textual entailment, the third stage is the theorem construction, definition of predicate types, and execution with a logic prover. This stage is not essential to our system, but it is added to this paper to show the usefulness of our semantic representations in an NLP task. 3 Semantic Composition 4.1 Semantic templates Semantic templates are defined declaratively in a YAML4 file, typically by a formal semanticist af"
P16-4015,D13-1160,0,0.0363053,"ic patterns. Templates for English and Japanese accompany our software, and they are easy to understand, use and extend to cover other linguistic phenomena or languages. We also provide scripts to use our semantic representations in a textual entailment task, and a visualization tool to display semantically augmented CCG trees in HTML. 1 4 Introduction 2 We are motivated by NLP problems that benefit from any degree of computer language understanding or semantic parsing. Two prominent examples are Textual Entailment and QuestionAnswering, where the most successful approaches (Abzianidze, 2015; Berant et al., 2013) require symbolic representations of the semantics of sentences. We are inspired by the theoretical developments in the formal semantics literature, where higher-order logical (HOL) formulas are used to derive meaning representations (MR); despite what is typically believed in the NLP community, Mineshima et al. (2015) demonstrated that HOL can be used effectively at a reasonable speed. In this paper, we describe ccg2lambda, our system to obtain MRs given derivations (trees) of a Combinatory Categorial Grammar (CCG) (Steedman, 2000). In order to obtain the MRs, our system is guided by the comb"
P16-4015,D15-1244,1,0.755024,"Missing"
P16-4015,C04-1180,0,0.632755,"ogical (HOL) formulas are used to derive meaning representations (MR); despite what is typically believed in the NLP community, Mineshima et al. (2015) demonstrated that HOL can be used effectively at a reasonable speed. In this paper, we describe ccg2lambda, our system to obtain MRs given derivations (trees) of a Combinatory Categorial Grammar (CCG) (Steedman, 2000). In order to obtain the MRs, our system is guided by the combinatory characteristics of CCG derivations and a list of manually designed semantic templates. The linguistic intuRelated Work The most similar system to ours is Boxer (Bos et al., 2004), which outputs first order formulas given CCG trees. Our system can additionally produce higher-order formulas, which are more expressive and potentially accurate (Mineshima et al., 2015). There are three prominent textbook systems for computational semantics, that of Bird et al. (2009), Blackburn and Bos (2005) and van Eijck and Unger (2010). These three systems, together with the Lambda Calculator2 (Champollion et al., 2007) are excellent educational resources that are very accessible to beginner linguists in general, and semanticists in particular. The development of ccg2lambda is inspired"
P16-4015,P16-4018,1,0.809689,"nction application rules, and the type-shift rule in C&C, respectively. These rules and the syntactic categories guide the semantic composition, provided with semantic templates that describe the specific semantics. System Overview Although our main system contribution is a semantic parser, we use the problem of textual entailment as an end-to-end task. Figure 1 schematically shows the several components of our system. The first stage is to parse sentences into CCG trees (see Figure 2 for an example). Our system currently supports the C&C parser (Clark and Curran, 2004) for English, and Jigg (Noji and Miyao, 2016) for Japanese. The second stage is the semantic composition, where MRs are constructed compositionally over CCG trees using lambda calculus, thus allowing higher-order logics if necessary. To this end, our system is guided by the compositional rules of the CCG tree and the semantic templates provided by the user. In Section 4 we describe in detail how these semantic templates are specified and how they control the semantic outputs. The output of this stage is a Stanford CoreNLP-style XML file (Manning et al., 2014) where each sentence has three XML nodes: &lt;tokens&gt;, &lt;ccg&gt; and &lt;semantics&gt;. Thus,"
P16-4015,P15-1087,0,0.030623,"ust 7-12, 2016. 2016 Association for Computational Linguistics 4 lus. However, these systems are mainly developed for educational purposes and are not connected to fully fledged parsers, hence not immediately usable as a component of larger NLP systems. We have developed ccg2lambda to process trees that are produced by wide-coverage CCG parsers (e.g. C&C and Jigg3 ). Other semantic parsers such as those developed by Bos et al. (2004), Abzianidze (2015) and Lewis and Steedman (2013) also connect to wide-coverage CCG parsers, but they do not emphasize easy accessibility or extensibility. NL2KR (Vo et al., 2015) is an interactive system with powerful generalization capabilities, but it does not allow fine-grained lexicon specifications (only CCG categories) and does not output machine readable semantics. Instead, ccg2lambda produces XML machine-readable MRs, which make our system easy to integrate in larger logic or statistical NLP systems. 3 ccg2lambda receives CCG trees and outputs (possibly higher-order) logic formulas. To that end, we use i) the combinatory characteristics of CCG trees to guide the semantic compositions, and ii) a list of semantic templates to assign a precise meaning to CCG cons"
P16-4015,P04-1014,0,0.0727602,"ymbols &lt;, &gt; and lex stand for left and right function application rules, and the type-shift rule in C&C, respectively. These rules and the syntactic categories guide the semantic composition, provided with semantic templates that describe the specific semantics. System Overview Although our main system contribution is a semantic parser, we use the problem of textual entailment as an end-to-end task. Figure 1 schematically shows the several components of our system. The first stage is to parse sentences into CCG trees (see Figure 2 for an example). Our system currently supports the C&C parser (Clark and Curran, 2004) for English, and Jigg (Noji and Miyao, 2016) for Japanese. The second stage is the semantic composition, where MRs are constructed compositionally over CCG trees using lambda calculus, thus allowing higher-order logics if necessary. To this end, our system is guided by the compositional rules of the CCG tree and the semantic templates provided by the user. In Section 4 we describe in detail how these semantic templates are specified and how they control the semantic outputs. The output of this stage is a Stanford CoreNLP-style XML file (Manning et al., 2014) where each sentence has three XML"
P16-4018,W04-3230,0,0.010467,"ies to support // Java programming. List<Node> sentences = jigg.util.XMLUtil.findAllSub( annotation, ""sentence""); Node firstSentence = sentences.get(0); List<Node> tokens = jigg.util.XMLUtil.findAllSub( firstSentence, ""token""); System.out.print(""POS tags on the first sentence: ""); for (Node token: tokens) { String pos = XMLUtil.find(token, ""@pos"").toString(); System.out.print(pos + "" ""); } Figure 5: Jigg also supports Java programming. cally, Jigg has been started as a pipeline framework focusing on Japanese language processing. Jigg thus supports many Japanese processing tools such as MeCab (Kudo et al., 2004), a famous morphological analyzer, as well as a Japanese CCG parser based on the Japanese CCGBank (Uematsu et al., 2013). For English, currently the core tool is Stanford CoreNLP. Here we present an interesting application to integrate Berkeley parser into the full pipeline of Stanford CoreNLP: Figure 4: A programmatic usage from Scala. Properties As in Stanford CoreNLP, these arguments can be customized through a Java properties file. For example, the following properties file customizes the behavior of corenlp besides the parser: $ cat sample.properties annotators: corenlp[tokenize,ssplit],b"
P16-4018,P14-5010,0,0.00810274,"the common API of Jigg (Scala XML object), which requires typically several dozes of lines of code. • For researchers or tool developers of downstream tasks, supporting the full pipeline from an input text in their software is boring and time consuming. For example, two famous dependency parsing systems, MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), both assume that an input sentence is already tokenized and assigned POS tags, and encoded in a specific format, such as the CoNLL format. The software design of Jigg is highly inspired by the success of Stanford CoreNLP (Manning et al., 2014), which is now the most widely used NLP toolkit supporting pipeline processing from raw texts. One characteristic of Stanford CoreNLP is 103 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics—System Demonstrations, pages 103–108, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics $ cat sample.txt This is a cat. That is a dog. $ echo sample.txt |java -cp ""*""  jigg.pipeline.Pipeline -annotators ""corenlp[tokenize,ssplit],berkeleyparser"" -berkeleyparser.grFileName ./eng_sm6.gr > sample.xml its simplicity of API, which allows"
P16-4018,H05-1066,0,0.018006,"ipeline by choosing a tool at each step on a command-line interface. Jigg is written in Scala, and can easily be extended with JVM languages including Java. A new tool can be incorporated into this framework by writing a wrapper of that to follow the common API of Jigg (Scala XML object), which requires typically several dozes of lines of code. • For researchers or tool developers of downstream tasks, supporting the full pipeline from an input text in their software is boring and time consuming. For example, two famous dependency parsing systems, MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), both assume that an input sentence is already tokenized and assigned POS tags, and encoded in a specific format, such as the CoNLL format. The software design of Jigg is highly inspired by the success of Stanford CoreNLP (Manning et al., 2014), which is now the most widely used NLP toolkit supporting pipeline processing from raw texts. One characteristic of Stanford CoreNLP is 103 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics—System Demonstrations, pages 103–108, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics $ c"
P16-4018,nivre-etal-2006-maltparser,0,0.0292164,"gg, a user can easily construct a pipeline by choosing a tool at each step on a command-line interface. Jigg is written in Scala, and can easily be extended with JVM languages including Java. A new tool can be incorporated into this framework by writing a wrapper of that to follow the common API of Jigg (Scala XML object), which requires typically several dozes of lines of code. • For researchers or tool developers of downstream tasks, supporting the full pipeline from an input text in their software is boring and time consuming. For example, two famous dependency parsing systems, MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), both assume that an input sentence is already tokenized and assigned POS tags, and encoded in a specific format, such as the CoNLL format. The software design of Jigg is highly inspired by the success of Stanford CoreNLP (Manning et al., 2014), which is now the most widely used NLP toolkit supporting pipeline processing from raw texts. One characteristic of Stanford CoreNLP is 103 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics—System Demonstrations, pages 103–108, c Berlin, Germany, August 7-12, 2016. 2016 Associa"
P16-4018,N07-1051,0,0.0108667,"gg and Stanford CoreNLP is the focused NLP components. Stanford CoreNLP is basically a collection of NLP tools developed by the Stanford NLP group, e.g., Stanford POS tagger (Toutanova et al., 2003) and Stanford parser (Socher et al., 2013). Jigg, on the other hand, is an integration framework of various NLP tools developed by various groups. This means that adding a new component in Jigg is easier than Stanford CoreNLP. Also as indicated in Figure 1, Jigg provides a wrapper to Stanford CoreNLP itself, so a user can enjoy combination of Stanford CoreNLP and other tools, e.g., Berkeley parser (Petrov and Klein, 2007) (see Section 2). This difference essentially comes from the underlying object annotated on each step, which is CoreMap object in Stanford CoreNLP, and Scala XML object in Jigg, which gives more flexibility as we describe later (Section 5). Before that, in the following, we first describes the concrete usage (Section 2), the core software design (Section 3), and a way to add a new component (Section 4). The code is open-source under the Apache License Version 2.0. Followings are the pointers to the related websites: Figure 2: A command-line usage to run the Berkeley parser on sentences tokeniz"
P16-4018,P13-1045,0,0.0104923,"behavior can be customized with a Java properties file. On the other hand, it focuses just on processing of a single document on a single machine, and does not provide the solution to more complex scenarios such as distributed processing or visualization, which UIMA and related projects (Ferrucci and Lally, 2004; Kano et al., 2011) may provide. The largest difference between Jigg and Stanford CoreNLP is the focused NLP components. Stanford CoreNLP is basically a collection of NLP tools developed by the Stanford NLP group, e.g., Stanford POS tagger (Toutanova et al., 2003) and Stanford parser (Socher et al., 2013). Jigg, on the other hand, is an integration framework of various NLP tools developed by various groups. This means that adding a new component in Jigg is easier than Stanford CoreNLP. Also as indicated in Figure 1, Jigg provides a wrapper to Stanford CoreNLP itself, so a user can enjoy combination of Stanford CoreNLP and other tools, e.g., Berkeley parser (Petrov and Klein, 2007) (see Section 2). This difference essentially comes from the underlying object annotated on each step, which is CoreMap object in Stanford CoreNLP, and Scala XML object in Jigg, which gives more flexibility as we desc"
P16-4018,N03-1033,0,0.0249011,"e of Jigg is command-line interface, and the behavior can be customized with a Java properties file. On the other hand, it focuses just on processing of a single document on a single machine, and does not provide the solution to more complex scenarios such as distributed processing or visualization, which UIMA and related projects (Ferrucci and Lally, 2004; Kano et al., 2011) may provide. The largest difference between Jigg and Stanford CoreNLP is the focused NLP components. Stanford CoreNLP is basically a collection of NLP tools developed by the Stanford NLP group, e.g., Stanford POS tagger (Toutanova et al., 2003) and Stanford parser (Socher et al., 2013). Jigg, on the other hand, is an integration framework of various NLP tools developed by various groups. This means that adding a new component in Jigg is easier than Stanford CoreNLP. Also as indicated in Figure 1, Jigg provides a wrapper to Stanford CoreNLP itself, so a user can enjoy combination of Stanford CoreNLP and other tools, e.g., Berkeley parser (Petrov and Klein, 2007) (see Section 2). This difference essentially comes from the underlying object annotated on each step, which is CoreMap object in Stanford CoreNLP, and Scala XML object in Jig"
P16-4018,P13-1103,1,0.623426,"e firstSentence = sentences.get(0); List<Node> tokens = jigg.util.XMLUtil.findAllSub( firstSentence, ""token""); System.out.print(""POS tags on the first sentence: ""); for (Node token: tokens) { String pos = XMLUtil.find(token, ""@pos"").toString(); System.out.print(pos + "" ""); } Figure 5: Jigg also supports Java programming. cally, Jigg has been started as a pipeline framework focusing on Japanese language processing. Jigg thus supports many Japanese processing tools such as MeCab (Kudo et al., 2004), a famous morphological analyzer, as well as a Japanese CCG parser based on the Japanese CCGBank (Uematsu et al., 2013). For English, currently the core tool is Stanford CoreNLP. Here we present an interesting application to integrate Berkeley parser into the full pipeline of Stanford CoreNLP: Figure 4: A programmatic usage from Scala. Properties As in Stanford CoreNLP, these arguments can be customized through a Java properties file. For example, the following properties file customizes the behavior of corenlp besides the parser: $ cat sample.properties annotators: corenlp[tokenize,ssplit],berkeleyparser berkeleyparser.grFileName: ./eng_sm6.gr corenlp.tokenize.whitespace: true corenlp.ssplit.eolonly: true -an"
P16-4018,N13-1071,0,\N,Missing
P17-1126,D10-1049,0,0.0239258,"Nikkei Stock Average. Automatic evaluation with BLEU score (Papineni et al., 2002) and F-score of time-dependent expressions reveals that our model outperforms a baseline encoder-decoder model significantly. Furthermore, human assessment and error analysis prove that our best model generates characteristic expressions discussed above almost perfectly, approaching the fluency and the informativeness of human-generated market comments. 2 Related Work The task of generating descriptions from timeseries or structured data has been tackled in various domains such as weather forecasts (Belz, 2007; Angeli et al., 2010), healthcare (Portet et al., 2009; Banaee et al., 2013b), and sports (Liang et al., 2009). Traditionally, many studies used handcrafted rules (Goldberg et al., 1994; Dale et al., 2003; Reiter et al., 2005). On the other hand, interest has recently been growing in automatically learning a correspondence relationship from data to text and generating a description of this relationship since large-scale data in diversified formats have become easy to acquire. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generation (Vinyals et al., 20"
P17-1126,D14-1179,0,0.00917524,"Missing"
P17-1126,P16-1154,0,0.0137433,"t al., 2014) for generating a description from time-series or structured data to solve the subtasks jointly in a single framework, and this model has been proven to be useful (Mei et al., 2016b; Lebret et al., 2016). However, the task of generating a description from numerical time-series data presents difficulties such as the second and third problems mentioned in Section 1. For the second problem, the model needs to be fed with information on delivery time. Also, the model needs arithmetic operations such as subtraction for the third problem because even if we simply apply a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) to the model, it cannot derive a calculated value such as (3), (5), or (6) in Figure 1 from input. Thus, in this work, we tackle these problems and develop a model on the basis of the encoder-decoder model that can mention a specific numerical value by referring to the input data or producing a processed value with mathematical calculation and mention time-dependent expressions by incorporating the information on delivery time into its decoder. There has also been some work on generating market comments. Kukich (1983) developed a system consisting of rule-based compone"
P17-1126,W13-2127,0,0.125326,"core (Papineni et al., 2002) and F-score of time-dependent expressions reveals that our model outperforms a baseline encoder-decoder model significantly. Furthermore, human assessment and error analysis prove that our best model generates characteristic expressions discussed above almost perfectly, approaching the fluency and the informativeness of human-generated market comments. 2 Related Work The task of generating descriptions from timeseries or structured data has been tackled in various domains such as weather forecasts (Belz, 2007; Angeli et al., 2010), healthcare (Portet et al., 2009; Banaee et al., 2013b), and sports (Liang et al., 2009). Traditionally, many studies used handcrafted rules (Goldberg et al., 1994; Dale et al., 2003; Reiter et al., 2005). On the other hand, interest has recently been growing in automatically learning a correspondence relationship from data to text and generating a description of this relationship since large-scale data in diversified formats have become easy to acquire. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generation (Vinyals et al., 2015) and weather forecast generation (Mei et al., 2016b"
P17-1126,P16-1014,0,0.0611281,"generating a description from time-series or structured data to solve the subtasks jointly in a single framework, and this model has been proven to be useful (Mei et al., 2016b; Lebret et al., 2016). However, the task of generating a description from numerical time-series data presents difficulties such as the second and third problems mentioned in Section 1. For the second problem, the model needs to be fed with information on delivery time. Also, the model needs arithmetic operations such as subtraction for the third problem because even if we simply apply a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) to the model, it cannot derive a calculated value such as (3), (5), or (6) in Figure 1 from input. Thus, in this work, we tackle these problems and develop a model on the basis of the encoder-decoder model that can mention a specific numerical value by referring to the input data or producing a processed value with mathematical calculation and mention time-dependent expressions by incorporating the information on delivery time into its decoder. There has also been some work on generating market comments. Kukich (1983) developed a system consisting of rule-based components for generating stock"
P17-1126,H05-1042,0,0.0145202,"been growing in automatically learning a correspondence relationship from data to text and generating a description of this relationship since large-scale data in diversified formats have become easy to acquire. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generation (Vinyals et al., 2015) and weather forecast generation (Mei et al., 2016b). The task, called data-to-text or concept-to-text, is generally divided into two subtasks: content selection and surface realization. Whereas previous studies tackled the subtasks separately (Barzilay and Lapata, 2005; Wong and Mooney, 2007; Lu et al., 2009), recent work has focused on solving them jointly using a single framework (Chen and Mooney, 2008; Kim and Mooney, 2010; Angeli et al., 2010; Konstas and Lapata, 2012, 2013). More recently, there has been some work on an encoder-decoder model (Sutskever et al., 2014) for generating a description from time-series or structured data to solve the subtasks jointly in a single framework, and this model has been proven to be useful (Mei et al., 2016b; Lebret et al., 2016). However, the task of generating a description from numerical time-series data presents"
P17-1126,N07-1021,0,0.0358973,"ments on the Nikkei Stock Average. Automatic evaluation with BLEU score (Papineni et al., 2002) and F-score of time-dependent expressions reveals that our model outperforms a baseline encoder-decoder model significantly. Furthermore, human assessment and error analysis prove that our best model generates characteristic expressions discussed above almost perfectly, approaching the fluency and the informativeness of human-generated market comments. 2 Related Work The task of generating descriptions from timeseries or structured data has been tackled in various domains such as weather forecasts (Belz, 2007; Angeli et al., 2010), healthcare (Portet et al., 2009; Banaee et al., 2013b), and sports (Liang et al., 2009). Traditionally, many studies used handcrafted rules (Goldberg et al., 1994; Dale et al., 2003; Reiter et al., 2005). On the other hand, interest has recently been growing in automatically learning a correspondence relationship from data to text and generating a description of this relationship since large-scale data in diversified formats have become easy to acquire. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generati"
P17-1126,W04-3250,0,0.0108369,"ch size of 100. The dimensions of word embeddings, time embeddings, and hidden states for both the encoder and decoder are set to 128, 64, and 256, respectively. For CNN, we used a single convolutional layer and set the filter size to 3. In the experiments, we conducted three types of evaluation: two for automatic evaluation, and one for human evaluation. For one automatic evaluation, we used BLEU (Papineni et al., 2002) to measure the matching degree between the market comments written by humans as references and output comments generated by our model. We applied paired bootstrap resampling (Koehn, 2004) for a significance test. For the other automatic evaluation metric, we calculate F-measures for time-dependent expressions, using market comments written by humans as references, to investigate whether our model can correctly output timedependent expressions such as “open with” and describe how the price changes compared with the previous period referring to the series of preceding prices such as “continual fall”. Specifically, we calculate F-measures for 13 expressions shown in Figure 3. For the human evaluation, we recruited a specialist in financial engineering as a judge to evaluate the q"
P17-1126,N12-1093,0,0.0138468,"re. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generation (Vinyals et al., 2015) and weather forecast generation (Mei et al., 2016b). The task, called data-to-text or concept-to-text, is generally divided into two subtasks: content selection and surface realization. Whereas previous studies tackled the subtasks separately (Barzilay and Lapata, 2005; Wong and Mooney, 2007; Lu et al., 2009), recent work has focused on solving them jointly using a single framework (Chen and Mooney, 2008; Kim and Mooney, 2010; Angeli et al., 2010; Konstas and Lapata, 2012, 2013). More recently, there has been some work on an encoder-decoder model (Sutskever et al., 2014) for generating a description from time-series or structured data to solve the subtasks jointly in a single framework, and this model has been proven to be useful (Mei et al., 2016b; Lebret et al., 2016). However, the task of generating a description from numerical time-series data presents difficulties such as the second and third problems mentioned in Section 1. For the second problem, the model needs to be fed with information on delivery time. Also, the model needs arithmetic operations suc"
P17-1126,D13-1157,0,0.00981197,"Missing"
P17-1126,N16-1086,0,0.0282149,"naee et al., 2013b), and sports (Liang et al., 2009). Traditionally, many studies used handcrafted rules (Goldberg et al., 1994; Dale et al., 2003; Reiter et al., 2005). On the other hand, interest has recently been growing in automatically learning a correspondence relationship from data to text and generating a description of this relationship since large-scale data in diversified formats have become easy to acquire. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generation (Vinyals et al., 2015) and weather forecast generation (Mei et al., 2016b). The task, called data-to-text or concept-to-text, is generally divided into two subtasks: content selection and surface realization. Whereas previous studies tackled the subtasks separately (Barzilay and Lapata, 2005; Wong and Mooney, 2007; Lu et al., 2009), recent work has focused on solving them jointly using a single framework (Chen and Mooney, 2008; Kim and Mooney, 2010; Angeli et al., 2010; Konstas and Lapata, 2012, 2013). More recently, there has been some work on an encoder-decoder model (Sutskever et al., 2014) for generating a description from time-series or structured data to sol"
P17-1126,P83-1022,0,0.456216,"because even if we simply apply a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) to the model, it cannot derive a calculated value such as (3), (5), or (6) in Figure 1 from input. Thus, in this work, we tackle these problems and develop a model on the basis of the encoder-decoder model that can mention a specific numerical value by referring to the input data or producing a processed value with mathematical calculation and mention time-dependent expressions by incorporating the information on delivery time into its decoder. There has also been some work on generating market comments. Kukich (1983) developed a system consisting of rule-based components for generating stock reports from a database of daily stock quotes. Although she used several components individually and had to define a number of rules for the generation, our encoder-decoder model can 1375 3 preprocessing 12167.29 12278.83 ... 12451.66 12461.36 lshort hshort encoder xlong 12116.57 12120.94 ... 12145.70 12150.49 concatenation encoder llong hlong (2) Incorporating Time Embedding &lt;s&gt; Generating Market Comments To generate market comments on stock prices, we introduce an encoder-decoder model. Encoderdecoder models have be"
P17-1126,P02-1040,0,0.12253,"network, or a convolutional network is adopted as a basic encoder. In the decoding phase, we feed our model with the delivery time of the market comment to generate the expressions depending on time of day to address the second problem. To address the third problem regarding with numerical values mentioned in the generated text, we allow our model to choose an arithmetic operation such as subtraction or rounding instead of generating a word. The proposed methods are evaluated on the task of generating Japanese market comments on the Nikkei Stock Average. Automatic evaluation with BLEU score (Papineni et al., 2002) and F-score of time-dependent expressions reveals that our model outperforms a baseline encoder-decoder model significantly. Furthermore, human assessment and error analysis prove that our best model generates characteristic expressions discussed above almost perfectly, approaching the fluency and the informativeness of human-generated market comments. 2 Related Work The task of generating descriptions from timeseries or structured data has been tackled in various domains such as weather forecasts (Belz, 2007; Angeli et al., 2010), healthcare (Portet et al., 2009; Banaee et al., 2013b), and s"
P17-1126,D16-1128,0,0.0231934,"Missing"
P17-1126,P16-1094,0,0.0224068,"in accordance with price history or the time they are observed. For instance, when the market opens, comments usually mention how much the stock price has increased or decreased compared with the closing price of the previous trading day, as in (1) and (3) in Figure 1. Our model creates vectors called time embedding vectors T on the basis of the time when the comment is delivered (e.g., 9:00 a.m. or 3:00 p.m.). Then a time embedding vector is added to each hidden state s j in decoding so that words are generated depending on time. This mechanism is inspired by speaker embedding introduced by Li et al. (2016). They use an encoder-decoder model for a conversational agent that inherits the characteristics of a speaker, such as his/her manner of speaking. They encode speaker-specific information (e.g., dialect, age, and gender) into speaker embedding vectors and used them in decoding. 3.3 Estimation of Arithmetic Operations Text generation systems based on language models such as RNNLM often generate erroneous words for named entities; that is, they often mention a similar but incorrect entity, e.g., Nissan for Toyota. To overcome this problem, Gulcehre et al. (2016) developed a text generation metho"
P17-1126,P09-1011,0,0.0101461,"Missing"
P17-1126,D09-1042,0,0.0248115,"dence relationship from data to text and generating a description of this relationship since large-scale data in diversified formats have become easy to acquire. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generation (Vinyals et al., 2015) and weather forecast generation (Mei et al., 2016b). The task, called data-to-text or concept-to-text, is generally divided into two subtasks: content selection and surface realization. Whereas previous studies tackled the subtasks separately (Barzilay and Lapata, 2005; Wong and Mooney, 2007; Lu et al., 2009), recent work has focused on solving them jointly using a single framework (Chen and Mooney, 2008; Kim and Mooney, 2010; Angeli et al., 2010; Konstas and Lapata, 2012, 2013). More recently, there has been some work on an encoder-decoder model (Sutskever et al., 2014) for generating a description from time-series or structured data to solve the subtasks jointly in a single framework, and this model has been proven to be useful (Mei et al., 2016b; Lebret et al., 2016). However, the task of generating a description from numerical time-series data presents difficulties such as the second and third"
P17-1126,D15-1044,0,0.0436072,"mponents individually and had to define a number of rules for the generation, our encoder-decoder model can 1375 3 preprocessing 12167.29 12278.83 ... 12451.66 12461.36 lshort hshort encoder xlong 12116.57 12120.94 ... 12145.70 12150.49 concatenation encoder llong hlong (2) Incorporating Time Embedding &lt;s&gt; Generating Market Comments To generate market comments on stock prices, we introduce an encoder-decoder model. Encoderdecoder models have been widely used and proven useful in various tasks of natural language generation such as machine translation (Cho et al., 2014) and text summarization (Rush et al., 2015). Our task is similar to these tasks in that the system takes sequential data and generates text. Therefore, it is natural to use an encoder-decoder model in modeling stock prices. Figure 2 illustrates our model. In describing time-series data, the model is expected to capture various types of change and important values in the given sequence, such as absolute or relative changes and maximum or minimum value, in different time-scales. Moreover, it is necessary to generate time-dependent comments and numerical values that require arithmetic operations for derivation, such as “The closing price"
P17-1126,N07-1022,0,0.0206477,"ly learning a correspondence relationship from data to text and generating a description of this relationship since large-scale data in diversified formats have become easy to acquire. In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generation (Vinyals et al., 2015) and weather forecast generation (Mei et al., 2016b). The task, called data-to-text or concept-to-text, is generally divided into two subtasks: content selection and surface realization. Whereas previous studies tackled the subtasks separately (Barzilay and Lapata, 2005; Wong and Mooney, 2007; Lu et al., 2009), recent work has focused on solving them jointly using a single framework (Chen and Mooney, 2008; Kim and Mooney, 2010; Angeli et al., 2010; Konstas and Lapata, 2012, 2013). More recently, there has been some work on an encoder-decoder model (Sutskever et al., 2014) for generating a description from time-series or structured data to solve the subtasks jointly in a single framework, and this model has been proven to be useful (Mei et al., 2016b; Lebret et al., 2016). However, the task of generating a description from numerical time-series data presents difficulties such as th"
P17-2001,P06-1095,0,0.798735,", our model achieves comparable performance, without using external knowledge and manually annotated attributes of entities (class, tense, polarity, etc.). 1 (i) There was no hint of trouble in the last conversation between controllers and TWA pilot Steven Snyder. (ii) In Washington today, the Federal Aviation Administration released air traffic control tapes. (iii) The U.S. Navy has 27 ships in the maritime barricade of Iraq. Marcu and Echihabi (2002) propose an approach considering word-based pairs as useful features. The following researchers (Laokulrat et al., 2013; Chambers et al., 2014; Mani et al., 2006; D’Souza and Ng, 2013) focus on extracting lexical, syntactic or semantic information from various external knowledge bases such as: WordNet (Miller, 1995) and VerbOcean (Chklovski and Pantel, 2004). However, these feature based methods rely on hand-crafted efforts and external resources. In addition, these works require the features of entity attributes (class, tense, polarity, etc.), which are manually annotated to achieve high performance. Consequently, they are hard to obtain in practical application scenarios. In relation extraction, there is an explosion of the works done with the depen"
P17-2001,H05-1091,0,0.515356,"ion from various external knowledge bases such as: WordNet (Miller, 1995) and VerbOcean (Chklovski and Pantel, 2004). However, these feature based methods rely on hand-crafted efforts and external resources. In addition, these works require the features of entity attributes (class, tense, polarity, etc.), which are manually annotated to achieve high performance. Consequently, they are hard to obtain in practical application scenarios. In relation extraction, there is an explosion of the works done with the dependency path (DP) based methods, which employ various models along dependency paths (Bunescu and Mooney, 2005; Plank and Moschitti, 2013). In recent years, the DP-based neural networks (Socher et al., 2011; Xu et al., 2015a,b) show state-of-the-art performance, with less requirements on explicit features. Intuitively, the DP-based approaches have the potential to classify temporal relations. Both relation extraction and temporal relation classification require the identification of relationIntroduction Recently, the need for extracting temporal information from text is motivated rapidly by many NLP tasks such as: question answering (QA), information extraction (IE), etc. Along with the TimeBank1 (Pus"
P17-2001,P02-1047,0,0.0403696,"s (DP). We make a “common root” assumption to extend DP representations of cross-sentence links. In the final comparison to two stateof-the-art systems on TimeBank-Dense, our model achieves comparable performance, without using external knowledge and manually annotated attributes of entities (class, tense, polarity, etc.). 1 (i) There was no hint of trouble in the last conversation between controllers and TWA pilot Steven Snyder. (ii) In Washington today, the Federal Aviation Administration released air traffic control tapes. (iii) The U.S. Navy has 27 ships in the maritime barricade of Iraq. Marcu and Echihabi (2002) propose an approach considering word-based pairs as useful features. The following researchers (Laokulrat et al., 2013; Chambers et al., 2014; Mani et al., 2006; D’Souza and Ng, 2013) focus on extracting lexical, syntactic or semantic information from various external knowledge bases such as: WordNet (Miller, 1995) and VerbOcean (Chklovski and Pantel, 2004). However, these feature based methods rely on hand-crafted efforts and external resources. In addition, these works require the features of entity attributes (class, tense, polarity, etc.), which are manually annotated to achieve high perf"
P17-2001,P14-2082,0,0.231598,"Missing"
P17-2001,Q14-1022,0,0.536202,"stems on TimeBank-Dense, our model achieves comparable performance, without using external knowledge and manually annotated attributes of entities (class, tense, polarity, etc.). 1 (i) There was no hint of trouble in the last conversation between controllers and TWA pilot Steven Snyder. (ii) In Washington today, the Federal Aviation Administration released air traffic control tapes. (iii) The U.S. Navy has 27 ships in the maritime barricade of Iraq. Marcu and Echihabi (2002) propose an approach considering word-based pairs as useful features. The following researchers (Laokulrat et al., 2013; Chambers et al., 2014; Mani et al., 2006; D’Souza and Ng, 2013) focus on extracting lexical, syntactic or semantic information from various external knowledge bases such as: WordNet (Miller, 1995) and VerbOcean (Chklovski and Pantel, 2004). However, these feature based methods rely on hand-crafted efforts and external resources. In addition, these works require the features of entity attributes (class, tense, polarity, etc.), which are manually annotated to achieve high performance. Consequently, they are hard to obtain in practical application scenarios. In relation extraction, there is an explosion of the works"
P17-2001,C16-1265,0,0.126673,"imensions) with word, POS embeddings as input (without any dependency information) is provided. The significant out-performance of our proposed model over the baseline indicates the effectiveness of the dependency path information and our Bi-LSTM in classifying temporal links. As a hybrid system, ‘CAEVO’ (Chambers et al., 2014) includes hand-crafted rules for their E-T and E-D classifiers. For instance, the temporal prepositions in, on, over, during, and within indicate ‘IN INCLUDED’ relations. Their system is superior in E-T and E-D. ’Miza’ takes the pure featureOverall Performance Recently, Mirza and Tonelli (2016) report state-ofthe-art performance on TimeBank-Dense. They show the new attempt to mine the value of lowdimensions word embeddings by concatenating them with sparse traditional features. Their traditional features include entity attributes, temporal signals, semantic information of WordNet, etc., which means it’s a hard setting for challenging their performance. In Table 2 and 3, ‘Mirza’ denotes their system. Table 2 shows the detailed comparison to 4 based methods and performs slightly better in EE and overall, compared to ‘CAEVO’. Our system shows the highest scores in E-E and overall among"
P17-2001,D15-1206,0,0.0682665,"ver, these feature based methods rely on hand-crafted efforts and external resources. In addition, these works require the features of entity attributes (class, tense, polarity, etc.), which are manually annotated to achieve high performance. Consequently, they are hard to obtain in practical application scenarios. In relation extraction, there is an explosion of the works done with the dependency path (DP) based methods, which employ various models along dependency paths (Bunescu and Mooney, 2005; Plank and Moschitti, 2013). In recent years, the DP-based neural networks (Socher et al., 2011; Xu et al., 2015a,b) show state-of-the-art performance, with less requirements on explicit features. Intuitively, the DP-based approaches have the potential to classify temporal relations. Both relation extraction and temporal relation classification require the identification of relationIntroduction Recently, the need for extracting temporal information from text is motivated rapidly by many NLP tasks such as: question answering (QA), information extraction (IE), etc. Along with the TimeBank1 (Pustejovsky et al., 2003) and other temporal information annotated corpora, a series of temporal evaluation challeng"
P17-2001,P13-1147,0,0.173719,"knowledge bases such as: WordNet (Miller, 1995) and VerbOcean (Chklovski and Pantel, 2004). However, these feature based methods rely on hand-crafted efforts and external resources. In addition, these works require the features of entity attributes (class, tense, polarity, etc.), which are manually annotated to achieve high performance. Consequently, they are hard to obtain in practical application scenarios. In relation extraction, there is an explosion of the works done with the dependency path (DP) based methods, which employ various models along dependency paths (Bunescu and Mooney, 2005; Plank and Moschitti, 2013). In recent years, the DP-based neural networks (Socher et al., 2011; Xu et al., 2015a,b) show state-of-the-art performance, with less requirements on explicit features. Intuitively, the DP-based approaches have the potential to classify temporal relations. Both relation extraction and temporal relation classification require the identification of relationIntroduction Recently, the need for extracting temporal information from text is motivated rapidly by many NLP tasks such as: question answering (QA), information extraction (IE), etc. Along with the TimeBank1 (Pustejovsky et al., 2003) and o"
P17-2001,D11-1014,0,0.145352,"Missing"
P19-1202,H05-1042,0,0.183494,"Missing"
P19-1202,P16-1154,0,0.0600758,"ed data including sports commentary (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecast (Liang et al., 2009; Mei et al., 2016), biographical text from infobox in Wikipedia (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018) and market comments from stock prices (Murakami et al., 2017; Aoki et al., 2018). Neural generation methods have become the mainstream approach for data-to-text generation. The encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014) with the attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) has successfully applied to data-to-text tasks. However, neural generation methods sometimes yield fluent but inadequate descriptions (Tu et al., 2017). In data-to-text generation, descriptions inconsistent to the input data are problematic. Recently, Wiseman et al. (2017) introduced the ROTOW IRE dataset, which contains multisentence summaries of basketball games with boxscore (Table 1). This dataset requires the selection of a salient subset of data records for generating descriptions. They also proposed automatic evaluation metrics for measuring the informativeness"
P19-1202,P16-1014,0,0.0614873,"sports commentary (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecast (Liang et al., 2009; Mei et al., 2016), biographical text from infobox in Wikipedia (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018) and market comments from stock prices (Murakami et al., 2017; Aoki et al., 2018). Neural generation methods have become the mainstream approach for data-to-text generation. The encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014) with the attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) has successfully applied to data-to-text tasks. However, neural generation methods sometimes yield fluent but inadequate descriptions (Tu et al., 2017). In data-to-text generation, descriptions inconsistent to the input data are problematic. Recently, Wiseman et al. (2017) introduced the ROTOW IRE dataset, which contains multisentence summaries of basketball games with boxscore (Table 1). This dataset requires the selection of a salient subset of data records for generating descriptions. They also proposed automatic evaluation metrics for measuring the informativeness of generated summaries."
P19-1202,D18-1130,0,0.0233321,"that the both consider a sequence of data records as content planning. However, our proposal differs from theirs in that ours uses a recurrent neural network for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salient entity and appropr"
P19-1202,D17-1195,0,0.0211576,"rrent neural network for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salient entity and appropriate attribute in each timestep, updates their states, and generates coherent summaries from the selected data record. 3 Data Through c"
P19-1202,D16-1032,0,0.014764,"that ours uses a recurrent neural network for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salient entity and appropriate attribute in each timestep, updates their states, and generates coherent summaries from the selected data record."
P19-1202,N16-1099,1,0.856363,"ea is similar to ours in that the both consider a sequence of data records as content planning. However, our proposal differs from theirs in that ours uses a recurrent neural network for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salien"
P19-1202,D16-1128,0,0.141635,"Missing"
P19-1202,N18-1204,0,0.0431553,"Missing"
P19-1202,P09-1011,0,0.277927,"Missing"
P19-1202,D15-1166,0,0.0235551,"riptions from structured or non-structured data including sports commentary (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecast (Liang et al., 2009; Mei et al., 2016), biographical text from infobox in Wikipedia (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018) and market comments from stock prices (Murakami et al., 2017; Aoki et al., 2018). Neural generation methods have become the mainstream approach for data-to-text generation. The encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014) with the attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) has successfully applied to data-to-text tasks. However, neural generation methods sometimes yield fluent but inadequate descriptions (Tu et al., 2017). In data-to-text generation, descriptions inconsistent to the input data are problematic. Recently, Wiseman et al. (2017) introduced the ROTOW IRE dataset, which contains multisentence summaries of basketball games with boxscore (Table 1). This dataset requires the selection of a salient subset of data records for generating descriptions. They also proposed automatic evaluation metric"
P19-1202,N16-1086,0,0.161392,"Missing"
P19-1202,P02-1040,0,0.103895,"rmation to each of the two stages, we can clearly see which part of the model to which the writer information contributes to. For Puduppully et al. (2019) model, we attach the writer information in the following three ways: 1. concatenating writer embedding w with the input vector for LSTM in the content planning decoder (stage 1); 2. concatenating writer embedding w with the input vector for LSTM in the text generator (stage 2); 3. using both 1 and 2 above. For more details about each decoding stage, readers can refer to Puduppully et al. (2019). 5.3 As evaluation metrics, we use BLEU score (Papineni et al., 2002) and the extractive metrics proposed by Wiseman et al. (2017), i.e., relation generation (RG), content selection (CS), and content ordering (CO) as evaluation metrics. The extractive metrics measure how well the relations extracted from the generated summary match the correct relations6 : 6 5 Our code is available from https://github.com/ aistairc/sports-reporter Evaluation metrics The model for extracting relation tuples was trained on tuples made from the entity (e.g., team name, city name, player name) and attribute value (e.g., “Lakers”, “92”) ex2107 - RG: the ratio of the correct relation"
P19-1202,Q17-1007,0,0.0173455,"phical text from infobox in Wikipedia (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018) and market comments from stock prices (Murakami et al., 2017; Aoki et al., 2018). Neural generation methods have become the mainstream approach for data-to-text generation. The encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014) with the attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) has successfully applied to data-to-text tasks. However, neural generation methods sometimes yield fluent but inadequate descriptions (Tu et al., 2017). In data-to-text generation, descriptions inconsistent to the input data are problematic. Recently, Wiseman et al. (2017) introduced the ROTOW IRE dataset, which contains multisentence summaries of basketball games with boxscore (Table 1). This dataset requires the selection of a salient subset of data records for generating descriptions. They also proposed automatic evaluation metrics for measuring the informativeness of generated summaries. Puduppully et al. (2019) proposed a two-stage method that first predicts the sequence of data records to be mentioned and then generates a summary condi"
P19-1202,D17-1197,0,0.0124483,"ork for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salient entity and appropriate attribute in each timestep, updates their states, and generates coherent summaries from the selected data record. 3 Data Through careful examination,"
P19-1202,P98-2209,0,0.875231,"Missing"
P99-1075,1997.iwpt-1.5,0,0.0220683,"ased on feature structure unification. This method automatically extracts equivalent parts of feature structures and collapses them into a single packed feature structure. A packed feature structure can be processed more efficiently because we can avoid redundant repetition of unification of the equivalent parts of original feature structures. There have been many studies on efficient 1In this paper we consider typed feature structures described in (Carpenter, 1992). 579 unification of disjunctive feature structures (Kasper and Rounds, 1986; Hasida, 1986; DSrre and Eisele, 1990; Nakano, 1991; Blache, 1997; Blache, 1998). All of them suppose that disjunctive feature structures should be given by grammar writers or lexicographers. However, it is not practical to specify all ambiguity using only manually-tailored disjunctive feature structures in grammar development. Where disjunctive feature structures cannot be given explicitly those algorithms lose their advantages. Hence, an automatic conversion method, such as the packing method described hereafter, is required for further optimization of those systems. In addition, this packing method converts general feature structures to a suitable form f"
P99-1075,P98-1019,0,0.357486,"e structure unification. This method automatically extracts equivalent parts of feature structures and collapses them into a single packed feature structure. A packed feature structure can be processed more efficiently because we can avoid redundant repetition of unification of the equivalent parts of original feature structures. There have been many studies on efficient 1In this paper we consider typed feature structures described in (Carpenter, 1992). 579 unification of disjunctive feature structures (Kasper and Rounds, 1986; Hasida, 1986; DSrre and Eisele, 1990; Nakano, 1991; Blache, 1997; Blache, 1998). All of them suppose that disjunctive feature structures should be given by grammar writers or lexicographers. However, it is not practical to specify all ambiguity using only manually-tailored disjunctive feature structures in grammar development. Where disjunctive feature structures cannot be given explicitly those algorithms lose their advantages. Hence, an automatic conversion method, such as the packing method described hereafter, is required for further optimization of those systems. In addition, this packing method converts general feature structures to a suitable form for a simple and"
P99-1075,C90-2018,0,0.0317246,"atic optimization method for parsers based on feature structure unification. This method automatically extracts equivalent parts of feature structures and collapses them into a single packed feature structure. A packed feature structure can be processed more efficiently because we can avoid redundant repetition of unification of the equivalent parts of original feature structures. There have been many studies on efficient 1In this paper we consider typed feature structures described in (Carpenter, 1992). 579 unification of disjunctive feature structures (Kasper and Rounds, 1986; Hasida, 1986; DSrre and Eisele, 1990; Nakano, 1991; Blache, 1997; Blache, 1998). All of them suppose that disjunctive feature structures should be given by grammar writers or lexicographers. However, it is not practical to specify all ambiguity using only manually-tailored disjunctive feature structures in grammar development. Where disjunctive feature structures cannot be given explicitly those algorithms lose their advantages. Hence, an automatic conversion method, such as the packing method described hereafter, is required for further optimization of those systems. In addition, this packing method converts general feature str"
P99-1075,C96-1076,0,0.0282091,"xicographers. However, it is not practical to specify all ambiguity using only manually-tailored disjunctive feature structures in grammar development. Where disjunctive feature structures cannot be given explicitly those algorithms lose their advantages. Hence, an automatic conversion method, such as the packing method described hereafter, is required for further optimization of those systems. In addition, this packing method converts general feature structures to a suitable form for a simple and efficient unification algorithm which is also described in this paper. Griffith (Griffith, 1995; Griffith, 1996) points out the same problem and proposes a compilation method for feature structures called modularization. However, modularization is very time-consuming, and is not suitable for optimizing feature structures produced during parsing. An earlier paper of myself (Miyao et al., 1998) also discusses the same problem and proposes another packing method. However, that method can pack only pre-specified parts of input feature structures, and this characteristic limits the overall efficient gain. The new method in this paper can pack any kind of feature structures as far as possible, and is more gen"
P99-1075,C86-1018,0,0.0318678,"ch is an automatic optimization method for parsers based on feature structure unification. This method automatically extracts equivalent parts of feature structures and collapses them into a single packed feature structure. A packed feature structure can be processed more efficiently because we can avoid redundant repetition of unification of the equivalent parts of original feature structures. There have been many studies on efficient 1In this paper we consider typed feature structures described in (Carpenter, 1992). 579 unification of disjunctive feature structures (Kasper and Rounds, 1986; Hasida, 1986; DSrre and Eisele, 1990; Nakano, 1991; Blache, 1997; Blache, 1998). All of them suppose that disjunctive feature structures should be given by grammar writers or lexicographers. However, it is not practical to specify all ambiguity using only manually-tailored disjunctive feature structures in grammar development. Where disjunctive feature structures cannot be given explicitly those algorithms lose their advantages. Hence, an automatic conversion method, such as the packing method described hereafter, is required for further optimization of those systems. In addition, this packing method conv"
P99-1075,P86-1038,0,0.0745812,"g feature structures, which is an automatic optimization method for parsers based on feature structure unification. This method automatically extracts equivalent parts of feature structures and collapses them into a single packed feature structure. A packed feature structure can be processed more efficiently because we can avoid redundant repetition of unification of the equivalent parts of original feature structures. There have been many studies on efficient 1In this paper we consider typed feature structures described in (Carpenter, 1992). 579 unification of disjunctive feature structures (Kasper and Rounds, 1986; Hasida, 1986; DSrre and Eisele, 1990; Nakano, 1991; Blache, 1997; Blache, 1998). All of them suppose that disjunctive feature structures should be given by grammar writers or lexicographers. However, it is not practical to specify all ambiguity using only manually-tailored disjunctive feature structures in grammar development. Where disjunctive feature structures cannot be given explicitly those algorithms lose their advantages. Hence, an automatic conversion method, such as the packing method described hereafter, is required for further optimization of those systems. In addition, this packi"
P99-1075,P98-2132,0,0.0509788,"Missing"
P99-1075,W98-0127,1,0.838885,"version method, such as the packing method described hereafter, is required for further optimization of those systems. In addition, this packing method converts general feature structures to a suitable form for a simple and efficient unification algorithm which is also described in this paper. Griffith (Griffith, 1995; Griffith, 1996) points out the same problem and proposes a compilation method for feature structures called modularization. However, modularization is very time-consuming, and is not suitable for optimizing feature structures produced during parsing. An earlier paper of myself (Miyao et al., 1998) also discusses the same problem and proposes another packing method. However, that method can pack only pre-specified parts of input feature structures, and this characteristic limits the overall efficient gain. The new method in this paper can pack any kind of feature structures as far as possible, and is more general than the previous method. 2 D a t a Structure and A l g o r i t h m s This section describes the data structure of packed feature structures, and the algorithms for packing and unification of packed feature structures. Through of this section, I will refer to examples from the"
P99-1075,P91-1040,0,0.0196827,"for parsers based on feature structure unification. This method automatically extracts equivalent parts of feature structures and collapses them into a single packed feature structure. A packed feature structure can be processed more efficiently because we can avoid redundant repetition of unification of the equivalent parts of original feature structures. There have been many studies on efficient 1In this paper we consider typed feature structures described in (Carpenter, 1992). 579 unification of disjunctive feature structures (Kasper and Rounds, 1986; Hasida, 1986; DSrre and Eisele, 1990; Nakano, 1991; Blache, 1997; Blache, 1998). All of them suppose that disjunctive feature structures should be given by grammar writers or lexicographers. However, it is not practical to specify all ambiguity using only manually-tailored disjunctive feature structures in grammar development. Where disjunctive feature structures cannot be given explicitly those algorithms lose their advantages. Hence, an automatic conversion method, such as the packing method described hereafter, is required for further optimization of those systems. In addition, this packing method converts general feature structures to a s"
P99-1075,W98-0141,1,0.81954,"Missing"
P99-1075,C96-2160,0,0.0702745,"Missing"
P99-1075,C98-1019,0,\N,Missing
P99-1075,C98-2128,0,\N,Missing
S14-2008,D12-1133,0,0.0149346,"ained or otherwise derived from WSJ Section 21. This restriction implies that typical off-the-shelf syntactic parsers had to be re-trained, as many datadriven parsers for English include this section of the PTB in their default training data. To simplify participation in the open track, the organizers prepared ready-to-use ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in two formats, viz. PTB-style phrase structure trees obtained from the parser of Petrov et al. (2006) and Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). 6 Submissions and Results From 36 teams who had registered for the task, test runs were submitted for nine systems. Each team submitted one or two test runs per track. In total, there were ten runs submitted to the closed track and nine runs to the open track. Three teams submitted to both the closed and the open track. The main results are summarized and ranked in Table 4. The ranking is based on the average LF score across all three target representations, which is given in the LF column. In cases where a team submitted two runs to a track, only the highestranked score is included in the t"
S14-2008,W06-2920,0,0.101128,"em in comparison to other sub-tasks in computational language analysis, introduce the semantic dependency target representations used, reflect on high-level commonalities and differences between these representations, and summarize the task setup, participating systems, and main results. 1 Background and Motivation Syntactic dependency parsing has seen great advances in the past decade, in part owing to relatively broad consensus on target representations, and in part reflecting the successful execution of a series of shared tasks at the annual Conference for Natural Language Learning (CoNLL; Buchholz & Marsi, 2006; Nivre et al., 2007; inter alios). From this very active research area accurate and efficient syntactic parsers have developed for a wide range of natural languages. However, the predominant data structure in dependency parsing to date are trees, in the formal sense that every node in the dependency graph is reachable from a distinguished root node by exactly one directed path. (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans, and rice. Semantically, technique arguably is dependent on the determiner (the quantificational locus), the modifier simil"
S14-2008,de-marneffe-etal-2006-generating,0,0.0702807,"Missing"
S14-2008,oepen-lonning-2006-discriminant,1,0.758648,"antic dependency graphs originate in a manual re-annotation of Sections 00– 21 of the WSJ Corpus with syntactico-semantic analyses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czec"
S14-2008,W09-1201,1,0.855031,"Missing"
S14-2008,J05-1004,0,0.129381,"ependency graphs for Example (1). uous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002). In much previous work, however, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena— for example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in o"
S14-2008,P06-1055,0,0.0124328,"e of the gold-standard syntactic or semantic analyses of the SDP 2014 test data, i.e. were directly or indirectly trained or otherwise derived from WSJ Section 21. This restriction implies that typical off-the-shelf syntactic parsers had to be re-trained, as many datadriven parsers for English include this section of the PTB in their default training data. To simplify participation in the open track, the organizers prepared ready-to-use ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in two formats, viz. PTB-style phrase structure trees obtained from the parser of Petrov et al. (2006) and Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). 6 Submissions and Results From 36 teams who had registered for the task, test runs were submitted for nine systems. Each team submitted one or two test runs per track. In total, there were ten runs submitted to the closed track and nine runs to the open track. Three teams submitted to both the closed and the open track. The main results are summarized and ranked in Table 4. The ranking is based on the average LF score across all three target representations, which is given i"
S14-2008,hajic-etal-2012-announcing,1,0.772691,"Missing"
S14-2008,W12-3602,1,0.885947,"yses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czech translations. Similarly to other treebanks in the Prague family, there are two layers of syntactic annotation: analytical ("
S14-2008,C08-1095,0,0.478753,".27 to 75.89 and the corresponding scores across systems are 88.64 for PAS, 84.95 for DM, and 67.52 for PCEDT. While these scores are consistently higher than in the closed track, the differences are small. In fact, for each of the three teams that submitted to both tracks (Alpage, Potsdam, and Priberam) improvements due to the use of additional resources in the open track do not exceed two points LF. 7 dencies), while the others apply post-processing to recover non-tree structures. The second strategy is to use a parsing algorithm that can directly generate graph structures (in the spirit of Sagae & Tsujii, 2008; Titov et al., 2009). In many cases such algorithms generate restricted types of graph structures, but these restrictions appear feasible for our target representations. The last approach is more machine learning–oriented; they apply classifiers or scoring methods (e.g. edge-factored scores), and find the highest-scoring structures by some decoding method. It is difficult to tell which approach is the best; actually, the top three systems in the closed and open tracks selected very different approaches. A possible conclusion is that exploiting existing systems or techniques for dependency par"
S14-2008,P10-5006,0,0.0693315,"ple inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Another difference to common interpretations of SRL is that the SDP 2014 task definition does not encompass predicate disambiguation, a design decision in part owed to our goal to focus on parsing-oriented, i.e. structural, analysis, and in part to lacking consensus on sense inventories for all content words. Finally, a third closely related area of much current interest is often dubbed ‘semantic parsing’, which Kate and Wong (2010) define as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” In contrast to most work in this tradition, our SDP target representations aim to be task- and domainindependent, though at least part of this generality comes at the expense of ‘completeness’ in the above sense; i.e. there are aspects of sentence meaning that arguably remain implicit. 2 Target Representations We use three distinct target representations for semantic dependencies. As is evident in our running example (Figure"
S14-2008,J93-2004,0,0.0590496,"t of multiple predicates (i.e. have more than one incoming arc), and it will often be desirable to leave nodes corresponding to semantically vacuous word classes unattached (with no incoming arcs). Thus, Task 8 at SemEval 2014, Broad-Coverage Semantic Dependency Parsing (SDP 2014),1 seeks to stimulate the dependency parsing community to move towards more general graph processing, to thus enable a more direct analysis of Who did What to Whom? For English, there exist several independent annotations of sentence meaning over the venerable Wall Street Journal (WSJ) text of the Penn Treebank (PTB; Marcus et al., 1993). These resources constitute parallel semantic annotations over the same common text, but to date they have not been related to each other and, in fact, have hardly been applied for training and testing of datadriven parsers. In this task, we have used three different such target representations for bi-lexical semantic dependencies, as demonstrated in Figure 1 below for the WSJ sentence: Task 8 at SemEval 2014 defines BroadCoverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate–argument relationships for all content words, i.e. the semantic structure"
S14-2008,P07-1031,0,0.0115637,"ersely, in PCEDT the last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected to the graph.7 A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure 2. Much noun phrase–internal structure is not made explicit in the PTB, and the Enju Treebank from which our PAS representation derives predates the bracketing work of Vadas and Curran (2007). In the four-way nominal compounding example of Figure 2, thus, PAS arrives at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of the compound either; PCEDT, on the other hand, annotates both the actual compound-internal bracketing and the assignment of roles, e.g. making stock the PAT(ient) of investment. In this spirit, the PCEDT annotations could be directly paraphrased along the lines of plans by employees for investment in stocks. In a middle position between the other two, DM disambiguates the bracketing but, by design, merely ass"
S14-2008,meyers-etal-2004-annotating,0,0.0465249,"Example (1). uous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002). In much previous work, however, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena— for example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Another"
S14-2008,S14-2056,1,0.893424,"pBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czech translations. Similarly to other treebanks in the Prague family, there are two layers of syntactic annotation: analytical (a-trees) and tectogrammatical (t-trees). PCEDT bi-lexical dependencies in this task have been extracted from the t-trees. The specifics of the PCE"
S14-2008,C10-1011,0,\N,Missing
S14-2008,S14-2080,0,\N,Missing
S14-2008,S14-2082,0,\N,Missing
S14-2008,J02-3001,0,\N,Missing
S14-2008,W15-0128,1,\N,Missing
S14-2008,D07-1096,0,\N,Missing
S14-2008,cinkova-2006-propbank,0,\N,Missing
S14-2056,D11-1037,1,0.860866,"ing data is positively biased towards our ensemble members.13 But even with this caveat, it seems fair to observe that the ERG and Enju parsers both are very competitive for the DM and PAS target representations, respectively, specifically so when judged in exact match scores. A possible explanation for these results lies in the depth of grammatical information available to these parsers, where DM or PAS semantic dependency graphs are merely a simpliefied view on the complete underlying HPSG analyses. These parsers have performed well in earlier contrastive evaluation too (Miyao et al., 2007; Bender et al., 2011; Ivanova et al., 2013; inter alios). Results for the Treex English parsing scenario, on the other hand, show that this ensemble member is not fine-tuned for the PCEDT target representation; due to the reasons mentioned above, its performance even falls behind the shared task baseline. As is evident from the comparison of labeled vs. unlabeled F1 scores, (a) the PCEDT parser is comparatively stronger at recovering semantic dependency structure than at assigning labels, and (b) about the same appears to be the case for the best-performing Priberam system (on this target representation). by the"
S14-2056,hajic-etal-2012-announcing,0,0.189879,"Missing"
S14-2056,A00-2022,1,0.412002,"er most commonly used with the ERG, called PET (Callmeier, 2002),1 constructs a complete, This work is licenced under a Creative Commons Attribution 4.0 International License; page numbers and the proceedings footer are added by the organizers. http:// creativecommons.org/licenses/by/4.0/ 1 The SDP test data was parsed using the 1212 release of the ERG, using PET and converter versions from what 335 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 335–340, Dublin, Ireland, August 23-24, 2014. subsumption-based parse forest of partial HPSG derivations (Oepen and Carroll, 2000), and then extracts from the forest n-best lists (in globally correct rank order) of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–20 of DeepBank and otherwise used the default, non-pruning development configuration, which is optimized for accuracy. In this setup, ERG parsing on average takes close to ten seconds per sentence. conversion steps are by design lossy, DM semantic dependency graphs present a true subset of the information encoded in the full, original MRS. 3 PAS: The Enju Par"
S14-2056,J93-2004,0,0.0466436,"plied to the parser outputs as were used in originally reducing the gold-standard MRSs from DeepBank into the SDP bi-lexical semantic dependency graphs. The three target representations for Task 8 at SemEval 2014, Broad-Coverage Semantic Dependency Parsing (SDP; Oepen et al., 2014), are rooted in language engineering efforts that have been under continuous development for at least the past decade. The gold-standard semantic dependency graphs used for training and testing in the Task result from largely manual annotation, in part re-purposing and adapting resources like the Penn Treebank (PTB; Marcus et al., 1993), PropBank (Palmer et al., 2005), and others. But the groups who prepared the SDP target data have also worked in parallel on automated parsing systems for these representations. Thus, for each of the target representations, there is a pre-existing parser, often developed in parallel to the creation of the target dependency graphs, viz. (a) for the DM representation, the parser of the hand-engineered LinGO English Resource Grammar (ERG; Flickinger, 2000); (b) for PAS, the Enju parsing system (Miyao, 2006), with its probabilistic HPSG acquired through linguistic projection of the PTB; and (c) f"
S14-2056,S14-2082,0,0.0922872,"rees can contain generated nodes, which represent elided words and do not correspond to any surface to5 Results and Reflections Seeing as our ‘in-house’ parsers are not directly trained on the semantic dependency graphs provided for the Task, but rather are built from additional linguistic resources, we submitted results from the parsing pipelines sketched in Sections 2 to 4 above to the open SDP track. Table 1 summarizes parser performance in terms of labeled and unlabeled F1 (LF and UF)12 and fullsentence exact match (LM and UM), comparing to the best-performing submission (dubbed Priberam; Martins and Almeida, 2014) to this track. Judging by the official SDP evaluation metric, average labeled F1 over the three representations, our ensemble ranked last among six participating 10 The system was able to output the following functors (ordered in the descending order of their frequency in the system output): RSTR, PAT, ACT, CONJ.member, APP, MANN, LOC, TWHEN, DISJ.member, BEN, RHEM, PREC, ACMP, MEANS, ADVS.member, CPR, EXT, DIR3, CAUS, COND, TSIN, REG, DIR2, CNCS, and TTILL. 11 In the SDP context, the target representation derived from the PCEDT is called by the same name as the original treebank; but note th"
S14-2056,S14-2008,1,0.87444,"Missing"
S14-2056,H05-1066,0,0.0955956,"Missing"
S14-2056,J05-1004,0,0.0249374,"ere used in originally reducing the gold-standard MRSs from DeepBank into the SDP bi-lexical semantic dependency graphs. The three target representations for Task 8 at SemEval 2014, Broad-Coverage Semantic Dependency Parsing (SDP; Oepen et al., 2014), are rooted in language engineering efforts that have been under continuous development for at least the past decade. The gold-standard semantic dependency graphs used for training and testing in the Task result from largely manual annotation, in part re-purposing and adapting resources like the Penn Treebank (PTB; Marcus et al., 1993), PropBank (Palmer et al., 2005), and others. But the groups who prepared the SDP target data have also worked in parallel on automated parsing systems for these representations. Thus, for each of the target representations, there is a pre-existing parser, often developed in parallel to the creation of the target dependency graphs, viz. (a) for the DM representation, the parser of the hand-engineered LinGO English Resource Grammar (ERG; Flickinger, 2000); (b) for PAS, the Enju parsing system (Miyao, 2006), with its probabilistic HPSG acquired through linguistic projection of the PTB; and (c) for PCEDT, the scenario for Engli"
S14-2056,W07-2207,1,0.819624,"ge numbers and the proceedings footer are added by the organizers. http:// creativecommons.org/licenses/by/4.0/ 1 The SDP test data was parsed using the 1212 release of the ERG, using PET and converter versions from what 335 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 335–340, Dublin, Ireland, August 23-24, 2014. subsumption-based parse forest of partial HPSG derivations (Oepen and Carroll, 2000), and then extracts from the forest n-best lists (in globally correct rank order) of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–20 of DeepBank and otherwise used the default, non-pruning development configuration, which is optimized for accuracy. In this setup, ERG parsing on average takes close to ten seconds per sentence. conversion steps are by design lossy, DM semantic dependency graphs present a true subset of the information encoded in the full, original MRS. 3 PAS: The Enju Parsing System Enju Predicate–Argument Structures (PAS) are derived from the automatic HPSG-style annotation of the PTB, which was primarily used for the development of the Enj"
S14-2056,J08-1002,1,0.798501,"used for the development of the Enju parsing system4 (Miyao, 2006). A notable feature of this parser is that the grammar is not developed by hand; instead, the Enju HPSG-style treebank is first developed, and the grammar (or, more precisely, the vast majority of lexical entries) is automatically extracted from the treebank (Miyao et al., 2004). In this ‘projection’ step, PTB annotations such as empty categories and coindexation are used for deriving the semantic representations that correspond to HPSG derivations. Its probabilistic model for disambiguation is also trained using this treebank (Miyao and Tsujii, 2008).5 The PAS data set is an extraction of predicate– argument structures from the Enju HPSG treebank. The Enju parser outputs results in ‘readyto-use’ formats like phrase structure trees and predicate–argument structures, as full HPSG analyses are not friendly to users who are not familiar with the HPSG theory. The gold-standard PAS target data in the Task was developed using this function; the conversion program from full HPSG analyses to predicate–argument structures was applied to the Enju Treebank. Predicate–argument structures (PAS) represent word-to-word semantic dependencies, such as sema"
S14-2056,W12-3602,1,\N,Missing
S14-2056,D07-1096,0,\N,Missing
S14-2056,oepen-lonning-2006-discriminant,1,\N,Missing
S14-2056,W13-5707,1,\N,Missing
S15-2153,D12-1133,0,0.036307,"semantic dependencies distributed for the task. Systems in the open track, on the other hand, could use additional resources, such as a syntactic parser, for example—provided that they make sure to not use any tools or resources that encompass knowledge of the gold-standard syntactic or semantic analyses of the SDP 2015 test data.11 To simplify participation in the open track, the organizers prepared ready-touse ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in the form of Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). Finally, to more directly gauge the the contributions of syntactic structure on the semantic dependency parsing problem, an idealized gold track was introduced in SDP 2015. For this track, gold-standard syntactic companion files were provided in a varity of formats, viz. (a) Stanford Basic dependencies, derived from the PTB, (b) HPSG syntactic dependencies in the form called DM by Ivanova et al. (2012), derived from DeepBank, and (c) HPSG syntactic dependencies derived from the Enju Treebank. 6 Submissions and Results From almost 40 teams who had registered for the task, twelve teams obtaine"
S15-2153,cinkova-2006-propbank,1,0.772792,".6993 .5743 .6719 − .5630 .5675 .5490 − Table 2: Pairwise F1 similarities, including punctuation (upper right diagonals) or not (lower left). Frame or sense distinctions are a new property in SDP 2015 and currently are only available for the English DM and PSD data. Table 1 reveals a stark difference in granularity: DM limits itself to argument structure distinctions that are grammaticized, e.g. causative vs. inchoative contrasts or differences in the arity or coarse semantic typing of argument frames; PSD, on the other hand, draws on the much richer sense inventory of the EngValLex database (Cinková, 2006). Accordingly, the two target representations represent quite different challenges for the predicate disambiguation sub-task of SDP 2015. Finally, in Table 2 we seek to quantify pairwise structural similarity between the three representations in terms of unlabeled dependency F1 (dubbed UF in Section 5 below). We provide four variants of this metric, (a) taking into account the directionality of edges or not and (b) including edges involving punctuation marks or not. On this view, DM and PAS are structurally much closer to each other than either of the two is to PSD, even more so when discardin"
S15-2153,de-marneffe-etal-2006-generating,0,0.0580061,"Missing"
S15-2153,S14-2080,0,0.42538,"ably because the additional dependency parser they used was trained on data from the target domain. 7 Overview of Approaches Table 5 shows a summary of the tracks in which each submitted system participated, and Table 6 shows an overview of approaches and additionally used resources. All the teams except In-House submitted results for cross-lingual data (Czech and Chinese). Teams except Lisbon also tackled with predicate disambiguation. Only Turku participated in the Gold track. The submitted teams explored a variety of approaches. Riga and Peking relied on the graph-to-tree transformation of Du et al. (2014) as a basis. This method converts semantic dependency graphs into tree structures. Training data of semantic dependency 12 Please see the task web page at the address indicated above for full labeled and unlabeled scores. Team In-House Lisbon Minsk Peking Riga Turku Closed X X X X Open X X X Cross-Lingual Predicate Disambiguation Gold X X X X X X X X X X X Table 5: Summary of tracks in which submitted systems participated Team Approach Resources In-House Lisbon Minsk Peking Riga Turku grammar-based parsing (Miyao et al., 2014) graph parsing with dual decomposition (Martins & Almeida, 2014) tra"
S15-2153,hajic-etal-2012-announcing,1,0.91158,"Missing"
S15-2153,W12-3602,1,0.93906,"ctionality of edges or not and (b) including edges involving punctuation marks or not. On this view, DM and PAS are structurally much closer to each other than either of the two is to PSD, even more so when discarding punctuation. While relaxing the comparison to ignore edge directionality also increases similarity scores for this pair, the effect is much more pronounced when comparing either to PSD. This suggests that directionality of semantic dependencies is a major source of diversion between DM and PAS on the one hand, and PSD on the other hand. Linguistic Comparison Among other aspects, Ivanova et al. (2012) categorize a range of syntactic and semantic dependency annotation schemes according to the role that functional elements take. In Figure 1 and the discussion of Table 1 above, we already observed that PAS differs from the other representations in integrating into the graph auxiliaries, the infinitival marker, the case-marking preposition introducing the argument of apply (to), and most punctuation marks;9 while these (and other functional elements, e.g. complementizers) are analyzed as semantically vacuous in DM and PSD, they function as predicates in PAS, though do not always serve as ‘loca"
S15-2153,P10-5006,0,0.0554855,"R-arg ACT-arg PAT-arg RSTR EXT RSTR CONJ.m APPS.m ADDR-arg APPS.m CONJ.m A similar technique is almost impossible to apply to other crops , such as cotton , soybeans _ _ _ ev-w218f2 _ _ _ ev-w119f2 _ _ _ _ _ _ _ _ _ CONJ.m and _ rice . _ _ (c) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PSD). Figure 1: Sample semantic dependency graphs for Example (1). sentence’ semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Finally, a third related area of much interest is often dubbed ‘semantic parsing’, which Kate and Wong (2010) define as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” In contrast to much work in this tradition, our SDP target representations aim to be task- and domain-independent. 2 Target Representations We use three distinct target representations for semantic dependencies. As is evident in our running example (Figure 1), showing what are called the DM, PAS, and PSD semantic dependencies, there are contentful differences among these annotations, and there is of course not one obvious (o"
S15-2153,J93-2004,0,0.0679778,"stitute of Informatics, Tokyo Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics • Stanford University, Center for the Study of Language and Information ♠ ◦ sdp-organizers@emmtee.net Abstract more general graph processing, to thus enable a more direct analysis of Who did What to Whom? Extending the very similar predecessor task SDP 2014 (Oepen et al., 2014), we make use of three distinct, parallel semantic annotations over the same common texts, viz. the venerable Wall Street Journal (WSJ) and Brown segments of the Penn Treebank (PTB; Marcus et al., 1993) for English, as well as comparable resources for Chinese and Czech. Figure 1 below shows example target representations, bi-lexical semantic dependency graphs in all cases, for the WSJ sentence: Task 18 at SemEval 2015 defines BroadCoverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate–argument relationships for all content words, i.e. the semantic structure constituting the relational core of sentence meaning. In this task description, we position the problem in comparison to other language analysis sub-tasks, introduce and compare the semantic de"
S15-2153,S14-2082,0,0.0634842,"ormation of Du et al. (2014) as a basis. This method converts semantic dependency graphs into tree structures. Training data of semantic dependency 12 Please see the task web page at the address indicated above for full labeled and unlabeled scores. Team In-House Lisbon Minsk Peking Riga Turku Closed X X X X Open X X X Cross-Lingual Predicate Disambiguation Gold X X X X X X X X X X X Table 5: Summary of tracks in which submitted systems participated Team Approach Resources In-House Lisbon Minsk Peking Riga Turku grammar-based parsing (Miyao et al., 2014) graph parsing with dual decomposition (Martins & Almeida, 2014) transition-based dependency graph parsing in the spirit of Titov et al. (2009) (Du et al., 2014) extended with weighted tree approximation, parser ensemble (Du et al., 2014)’s graph-to-tree transformation, Mate, C6.0, parser ensemble sequence labeling for argument detection for each predicate, SVM classifiers for top node recognition and sense prediction ERG & Enju companion — — — companion Table 6: Overview of approaches and additional resources used (if any). graphs are converted into tree structures, and wellestablished parsing methods for tree structures are applied to converted structure"
S15-2153,meyers-etal-2004-annotating,0,0.0555434,"the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node re-entrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. Besides its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002).2 However, we require parsers to identify ‘full2 In much previous SRL work, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena—for example negation 915 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 915–926, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics top BV ARG3 ARG2 ARG1 ARG1 ARG1 ARG1 ARG1 mwe ARG2 conj _and_c A similar technique is almost impossible to apply to other crops , such as cotton, soybeans and rice . q:i-h-h a_to:e-i n:x _ a:e-h a_for:e-h-i _ v_to:e-i-p-i _ a:e-i n:x _ p:e-u-i p:e-u-i n:x n:x _ n:"
S15-2153,S14-2056,1,0.858363,"ndencies, there are contentful differences among these annotations, and there is of course not one obvious (or even objective) truth. Advancing in-depth comparison of representations and underlying design decisions, in fact, is among the moand other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—often remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inference-based techniques. 916 tivations for the SDP task series. Please see Oepen et al. (2014) and Miyao et al. (2014) for additional background. DM: DELPH-IN MRS-Derived Bi-Lexical Dependencies These semantic dependency graphs originate in a manual re-annotation, dubbed DeepBank, of Sections 00–21 of the WSJ Corpus and of selected parts of the Brown Corpus with syntacticosemantic analyses of the LinGO English Resource Grammar (Flickinger, 2000; Flickinger et al., 2012). For this target representation, top nodes designate the highest-scoping (non-quantifier) predicate in the graph, e.g. the (scopal) adverb almost in Figure 1.3 PAS: Enju Predicate–Argument Structures The Enju Treebank and parser4 are derived f"
S15-2153,S14-2008,1,0.363094,"Missing"
S15-2153,J05-1004,0,0.359612,"s preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node re-entrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. Besides its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002).2 However, we require parsers to identify ‘full2 In much previous SRL work, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena—for example negation 915 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 915–926, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics top BV ARG3 ARG2 ARG1 ARG1 ARG1 ARG1 ARG1 mwe ARG2 conj _and_c A similar technique is almost impossible to apply to other crops , such as cotton, soybeans and rice . q:i-h-h a_to:e-i n:x _ a:e-h a_for:e-h-i _ v_to:e-i-p-i _ a:e-i n:x _ p:e-u-"
S15-2153,P07-1031,0,0.0187644,"ersely, in PSD the last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected to the graph.10 A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure 2. Much noun phrase– internal structure is not made explicit in the PTB, and the Enju Treebank from which our PAS representation derives predates the bracketing work of Vadas and Curran (2007). In the four-way nominal compounding example of Figure 2, thus, PAS arrives at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of the compound either; PSD, on the other hand, annotates both the actual compound-internal bracketing and the assignment of roles, e.g. making stock the PAT(ient) of investment. In this spirit, the PSD annotations could be directly paraphrased along the lines of plans by employees for investment in stocks. In a middle position between the other two, DM disambiguates the bracketing but, by design, merely assigns"
tateisi-etal-2008-genia,W03-2401,0,\N,Missing
tateisi-etal-2008-genia,de-marneffe-etal-2006-generating,0,\N,Missing
tateisi-etal-2008-genia,N04-1013,0,\N,Missing
tateisi-etal-2008-genia,J93-2004,0,\N,Missing
tateisi-etal-2008-genia,E03-1025,0,\N,Missing
tateisi-etal-2008-genia,W07-1004,0,\N,Missing
tateisi-etal-2008-genia,W07-2202,1,\N,Missing
tateisi-etal-2008-genia,W04-3111,0,\N,Missing
tateisi-etal-2008-genia,P06-4020,0,\N,Missing
tateisi-etal-2008-genia,P07-1032,0,\N,Missing
tateisi-etal-2008-genia,P06-2006,0,\N,Missing
tateisi-etal-2008-genia,ohta-etal-2006-linguistic,1,\N,Missing
tateisi-etal-2014-annotation,liakata-etal-2010-corpora,0,\N,Missing
tateisi-etal-2014-annotation,brants-2000-inter,0,\N,Missing
tateisi-etal-2014-annotation,D11-1025,0,\N,Missing
tateisi-etal-2014-annotation,P11-4002,0,\N,Missing
tateisi-etal-2014-annotation,varga-etal-2012-unsupervised,0,\N,Missing
tateisi-etal-2014-annotation,mori-etal-2014-flow,0,\N,Missing
tateisi-etal-2014-annotation,I11-1001,0,\N,Missing
tateisi-etal-2014-annotation,W13-2001,0,\N,Missing
tateisi-etal-2014-annotation,E12-2021,0,\N,Missing
tateisi-etal-2014-annotation,W13-2318,1,\N,Missing
W01-1510,W00-2006,0,0.022452,"to LTAG derivation trees. All modules other than the last one are related to the conversion process from LTAG into HPSG, and the last one enables to obtain LTAG analysis from the obtained HPSG analysis. Tateisi et al. also translated LTAG into HPSG (Tateisi et al., 1998). However, their method depended on translator’s intuitive analysis of the original grammar. Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars. However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity. Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages. Section 2 reviews the source and the target grammar formalisms of the conversion algorithm. Section 3 describes the conversion algorithm which the core module in the RenTAL system uses. Section"
W01-1510,2000.iwpt-1.9,0,0.035923,"Missing"
W01-1510,P00-1058,0,0.0138769,"r (Yoshinaga and Miyao, 2001). Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications. Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch. Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environ1 In this paper, we use the term LTAG to refer to FBLTAG, if not confusing. LTAG-based application RenTAL System HPSG-based application LTAG Resources Tree converter HPSG Resources Grammar: Elementary tree templates Lexicon Type hierarchy extractor Lexicon converter LTAG parsers anchor * Grammar: Lexical entry templates Initial tree foot node Auxiliary tree α2 S substitution node α1 NP Lexicon VP VP V N V can We run NP β1 VP * HPSG parsers Figure 2: Elementary trees Derivation trees Derivation translator Parse trees F"
W01-1510,C00-1060,1,0.921494,"nd Miyao, 2001). Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications. Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch. Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environ1 In this paper, we use the term LTAG to refer to FBLTAG, if not confusing. LTAG-based application RenTAL System HPSG-based application LTAG Resources Tree converter HPSG Resources Grammar: Elementary tree templates Lexicon Type hierarchy extractor Lexicon converter LTAG parsers anchor * Grammar: Lexical entry templates Initial tree foot node Auxiliary tree α2 S substitution node α1 NP Lexicon VP VP V N V can We run NP β1 VP * HPSG parsers Figure 2: Elementary trees Derivation trees Derivation translator Parse trees Figure 1: The RenTAL Syst"
W01-1510,W98-0134,0,0.0195913,"matically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar (Yoshinaga and Miyao, 2001). Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications. Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch. Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environ1 In this paper, we use the term LTAG to refer to FBLTAG, if not confusing. LTAG-based application RenTAL System HPSG-based application LTAG Resources Tree converter HPSG Resources Grammar: Elementary tree templates Lexicon Type hierarchy extractor Lexicon converter LTAG parsers anchor * Grammar: Lexical entry templates Initial tree foot node Auxiliary tree α2 S substitution node α1 NP Lexicon VP VP V N V can We run NP β1 VP * HPSG parsers Figure 2"
W01-1510,W00-1605,0,0.0466259,"Missing"
W01-1510,C88-2121,0,0.469209,"Missing"
W01-1510,W98-0141,1,0.83789,"rarchy extractor module extracts the symbols of the node, features, and feature values from the LTAG elementary tree templates and lexicon, and construct the type hierarchy from them. The lexicon converter module converts LTAG elementary tree templates into HPSG lexical entries. The derivation translator module takes HPSG parse trees, and map them to LTAG derivation trees. All modules other than the last one are related to the conversion process from LTAG into HPSG, and the last one enables to obtain LTAG analysis from the obtained HPSG analysis. Tateisi et al. also translated LTAG into HPSG (Tateisi et al., 1998). However, their method depended on translator’s intuitive analysis of the original grammar. Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars. However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions"
W01-1510,P95-1013,0,0.83404,"trees, and map them to LTAG derivation trees. All modules other than the last one are related to the conversion process from LTAG into HPSG, and the last one enables to obtain LTAG analysis from the obtained HPSG analysis. Tateisi et al. also translated LTAG into HPSG (Tateisi et al., 1998). However, their method depended on translator’s intuitive analysis of the original grammar. Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars. However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity. Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages. Section 2 reviews the source and the target grammar formalisms of the conversion algorithm. Section 3 describes the conversion algorithm which the core module in the Re"
W01-1510,P98-2144,1,0.85278,"ure 6 shows a rule application to “can run” and “we”. There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts (Flickinger et al., 2000). Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000). In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994). Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000). S anchor * foot node substitution node trunk NP VP V S* think think: Sym: V Sym : VP Arg: Leaf : S Dir : right Foot?: + , Sym : S Leaf : NP Dir : left Foot?: _ Figure 8: A conversion from a canonical elementary tree into an HPSG lexical entry  h mother Sym : Arg :  1 i 2 X*X2XXX 3 4 Arg : 4 5j Sym : 3 Arg : h i substitution node 2 Sym : 1 Leaf : 3 Dir : lef t Foot? : 2 + 3 5 trunk node Figure 9: Left substitution rule 3 Grammar conversion The grammar conversion from LTAG to HPSG (Yoshinaga and Miyao"
W01-1510,W98-0131,0,0.0541757,"Missing"
W01-1510,C88-2147,0,0.0507767,"rammar conversion from an FB-LTAG grammar to a strongly equivalent HPSG-style grammar. The system is applied to the latest version of the XTAG English grammar. Experimental results show that the obtained HPSG-style grammar successfully worked with an HPSG parser, and achieved a drastic speed-up against an LTAG parser. This system enables to share not only grammars and lexicons but also parsing techniques. 1 Introduction This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar (FB-LTAG1 ) (Vijay-Shanker, 1987; Vijay-Shanker and Joshi, 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) by a method of grammar conversion. The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar (Yoshinaga and Miyao, 2001). Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications. Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch. Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly"
W01-1510,J93-2004,0,\N,Missing
W01-1510,C98-2139,1,\N,Missing
W01-1510,C98-2128,1,\N,Missing
W01-1510,P98-2132,1,\N,Missing
W02-2227,P95-1013,0,0.0217347,"nature of strong equivalence (Yoshinaga et al., 2001b; Yoshinaga et al., 2001a), applications which contribute much to the developments of the two formalisms. In the past decades, LTAG and HPSG have received considerable attention as approaches to the formalization of natural languages in the field of computational linguistics. Discussion of the correspondences between the two formalisms has accompanied their development; that is, their linguistic relationships and differences have been investigated (Abeill´e, 1993; Kasper, 1998), as has conversion between two grammars in the two formalisms (Kasper et al., 1995; Tateisi et al., 1998; Becker and Lopez, 2000). These ongoing efforts have contributed greatly to the development of the two formalisms. Following this direction, in our earlier work (Yoshinaga and Miyao, 2001), we provided a method for converting grammars from LTAG to HPSG-style, which is the notion that we defined according to the computational device that underlies HPSG. We used the grammar conversion to obtain an HPSG-style grammar from LTAG (The XTAG Research Group, 2001), and then empirically showed strong equivalence between the LTAG and the obtained HPSG-style grammar for the sentence"
W02-2227,W00-2027,0,0.0223548,"rcinkiewicz, 1994). We exploited the nature of strong equivalence between the LTAG and the HPSG-style grammars to provide some applications such as sharing of existing resources between the two grammar formalisms (Yoshinaga et al., 2001b), a comparison of performance between parsers based on the two different formalisms (Yoshinaga et al., 2001a), and linguistic correspondence between the HPSG-style grammar and HPSG. As the most important result for the LTAG community, through the experiments of parsing within the above sentences, we showed that the empirical time complexity of an LTAG parser (Sarkar, 2000) is higher than that of an HPSG parser (Torisawa et al., 2000). This result is contrary to the general expectations from the viewpoint of the theoretical bound of worst time complexity, which is worth exploring further. However, the lack of the formal proof of strong equivalence restricts scope of the applications of our grammar conversion to grammars which are empirically attested the strong equivalence, and this prevents the applications from maximizing their true potential. In this paper we give a formal proof of strong equivalence between any LTAG and an HPSG-style grammar converted from"
W02-2227,C88-2121,0,0.21412,"Missing"
W02-2227,J95-4002,0,0.0747168,"Missing"
W02-2227,W01-1510,1,0.881133,"Missing"
W02-2227,J93-2004,0,\N,Missing
W02-2227,W98-0141,1,\N,Missing
W02-2232,2000.iwpt-1.9,0,0.265486,"n are in real-world texts, extracted grammars are practical for natural language processing. However, automatically extracted grammars are not systematically arranged according to syntactic classes their anchors belong to, like the XTAG grammar. Because of this, automatically extracted grammars tend to be strongly dependent on the corpus. This limitation can be a critical disadvantage of such extracted grammars when the grammars are used for various applications. Then, we want to arrange an extracted grammar according to the syntactic classes ofwords, without loosing the benefit for the cost. Chen and Vijay-Shanker (2000) proposed the solution to the issue. To improve the coverage of an extracted LTAG grammar, they classified the extracted elementary trees according to the tree families in the XTAG English grammar. First, the method searches for a tree farnily that contains an elementary tree template of extracted elementary tree et. Next, the method collects other possible tree templates in the tree family and makes elementary trees with the anchor of et and the tree templates. By using tree families, the method can add only proper elementary trees that correspond to the syntactic class of anchors. Chen and V"
W02-2232,C88-2121,0,0.20761,"Missing"
W02-2232,J93-2004,0,\N,Missing
W02-2232,P00-1058,0,\N,Missing
W03-0401,J97-4005,0,0.833904,"guation models of lexicalized grammars should be totally different from that of LPCFG, because the grammars define the relation of syntax and semantics, and can restrict the possible structure of parsing results. Parsing results cannot simply be decomposed into primitive dependencies, because the complete structure is determined by solving the syntactic constraints of a complete sentence. For example, when we apply a unification-based grammar, LPCFG-like modeling results in an inconsistent probability model because the model assigns probabilities to parsing results not allowed by the grammar (Abney, 1997). We have only two ways of adhering to LPCFG models: preserve the consistency of probability models by abandoning improvements to the lexicalized grammars using complex constraints (Chiang, 2000), or ignore the inconsistency in probability models (Clark et al., 2002). This paper provides a new model of syntactic disambiguation in which lexicalized grammars can restrict the possible structures of parsing results. Our modeling aims at providing grounds for i) producing a consistent probabilistic model of lexicalized grammars, as well as ii) evaluating the contributions of syntactic and semantic"
W03-0401,J96-1002,0,0.00877629,"Missing"
W03-0401,2000.iwpt-1.9,0,0.665026,"e dependencies of two words, our model selects the most probable parsing result from a set of candidates allowed by a lexicalized grammar. Since parsing results given by the lexicalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition of parsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependenc"
W03-0401,hockenmaier-steedman-2002-acquiring,0,0.100344,"ur model selects the most probable parsing result from a set of candidates allowed by a lexicalized grammar. Since parsing results given by the lexicalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition of parsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have pro"
W03-0401,P02-1043,0,0.0485315,"ur model selects the most probable parsing result from a set of candidates allowed by a lexicalized grammar. Since parsing results given by the lexicalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition of parsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have pro"
W03-0401,P99-1069,0,0.644014,"002; Geman and Johnson, 2002), which allows parameters to be efficiently estimated. The algorithm enables probabilistic modeling of complete structures, such as transition sequences in Markov models and parse trees, without dividing them into independent sub-events. The algorithm avoids exponential explosion by representing a probabilistic event by a packed representation of a feature space. If a complete structure is represented with a feature forest of a tractable size, the parameters can be efficiently estimated by dynamic programming. A series of studies on parsing with wide-coverage LFG (Johnson et al., 1999; Riezler et al., 2000; Riezler et al., 2002) have had a similar motivation to ours. Their models have also been based on a discriminative model to select a parsing result from all candidates given by the grammar. A significant difference is that we apply maximum entropy estimation for feature forests to avoid the inherent problem with estimation: the exponential explosion of parsing results given by the grammar. They assumed that parsing results would be suppressed to a reasonable number through using heuristic rules, or by carefully implementing a fully restrictive and wide-coverage grammar,"
W03-0401,C94-1024,0,0.229074,"tactic categories. Formally, p(A|w) = p(c|w)p(A|c). The first probability in the above formula is the probability of syntactic categories, i.e., the probability of selecting a sequence of syntactic categories in a sentence. Since syntactic categories in lexicalized grammars determine the syntactic constraints of words, this expresses the syntactic preference of each word in a sentence. Note that our objective is not only to improve parsing accuracy but also to investigate the relation between syntax and semantics. We have not adopted the local contexts of words as in the supertaggers in LTAG (Joshi and Srinivas, 1994) because they partially include the semantic preferences of a sentence. The probability is purely unigram to select the probable syntactic category for each word. The probability is then given by the product of probabilities to select a syntactic category for each word from a set of candidate categories allowed by the lexicon.  p(ci |wi ) p(c|w) = i The second describes the probability of semantics, which expresses the semantic preferences of relating the words in a sentence. Note that the semantics probability is dependent on the syntactic categories determined by the syntax probability, bec"
W03-0401,H94-1020,0,0.0568261,"probabilities to parsing results not allowed by the grammar. • Since the syntax and semantics probabilities are separate, we can improve them individually. For example, the syntax model can be improved by smoothing using the syntactic classes of words, while the semantics model should be able to be improved by using semantic classes. In addition, the model can be a starting point that allows the theory of syntax and semantics to be evaluated through consulting an extensive corpus. We evaluated the validity of our model through experiments on a disambiguation task of parsing the Penn Treebank (Marcus et al., 1994) with an automatically acquired LTAG grammar. To assess the contribution of the syntax and semantics probabilities to the accuracy of parsing and to evaluate the validity of applying maximum entropy estimation for feature forests, we compared three models trained with the same training set and the same set of features. Following the experimental results, we concluded that i) a parser with the syntax probability only achieved high accuracy with the lexicalized grammar, ii) the incorporation of preferences for lexical association through the semantics probability resulted in significant improvem"
W03-0401,P00-1061,0,0.109313,", 2002), which allows parameters to be efficiently estimated. The algorithm enables probabilistic modeling of complete structures, such as transition sequences in Markov models and parse trees, without dividing them into independent sub-events. The algorithm avoids exponential explosion by representing a probabilistic event by a packed representation of a feature space. If a complete structure is represented with a feature forest of a tractable size, the parameters can be efficiently estimated by dynamic programming. A series of studies on parsing with wide-coverage LFG (Johnson et al., 1999; Riezler et al., 2000; Riezler et al., 2002) have had a similar motivation to ours. Their models have also been based on a discriminative model to select a parsing result from all candidates given by the grammar. A significant difference is that we apply maximum entropy estimation for feature forests to avoid the inherent problem with estimation: the exponential explosion of parsing results given by the grammar. They assumed that parsing results would be suppressed to a reasonable number through using heuristic rules, or by carefully implementing a fully restrictive and wide-coverage grammar, which requires a cons"
W03-0401,P00-1058,0,0.490694,"icalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition of parsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have proved that the incorporation of lexical associations (i.e., dependencies of words) significantly improves the accuracy of parsing, t"
W03-0401,P02-1035,0,0.100317,"parameters to be efficiently estimated. The algorithm enables probabilistic modeling of complete structures, such as transition sequences in Markov models and parse trees, without dividing them into independent sub-events. The algorithm avoids exponential explosion by representing a probabilistic event by a packed representation of a feature space. If a complete structure is represented with a feature forest of a tractable size, the parameters can be efficiently estimated by dynamic programming. A series of studies on parsing with wide-coverage LFG (Johnson et al., 1999; Riezler et al., 2000; Riezler et al., 2002) have had a similar motivation to ours. Their models have also been based on a discriminative model to select a parsing result from all candidates given by the grammar. A significant difference is that we apply maximum entropy estimation for feature forests to avoid the inherent problem with estimation: the exponential explosion of parsing results given by the grammar. They assumed that parsing results would be suppressed to a reasonable number through using heuristic rules, or by carefully implementing a fully restrictive and wide-coverage grammar, which requires a considerable amount of effo"
W03-0401,P02-1042,0,0.638011,"be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition of parsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have proved that the incorporation of lexical associations (i.e., dependencies of words) significantly improves the accuracy of parsing, this idea has been naively in"
W03-0401,P96-1025,0,0.448362,"ethod of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition of parsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have proved that the incorporation of lexical associations (i.e., dependencies of words) significantly improves the accuracy of parsing, this idea has been naively inherited in the recent studies on disambiguation models of lexicalized grammars. Jun’ichi Tsujii Department of Computer Science, University of Tokyo CREST, JST (Japan Science and Technology Corporatio"
W03-0401,C88-2121,0,0.233619,"Missing"
W03-0401,P97-1003,0,0.781556,"ing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition of parsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have proved that the incorporation of lexical associations (i.e., dependencies of words) significantly improves the accuracy of parsing, this idea has been naively inherited in the recent studies on disambiguation models of lexicalized grammars. Jun’ichi Tsujii Department of Computer Science, University of Tokyo CREST, JST (Japan Science and Technology Corporation) tsujii@is.s."
W03-0401,P02-1036,0,0.565105,"imum entropy models (Berger et al., 1996) and support vector machines (Vapnik, 1995) provide grounds for this type of modeling, because it allows various dependent features to be incorporated into the model without the independence assumption. The above approach, however, has a serious deficiency: a lexicalized grammar assigns exponentially many parsing results because of local ambiguities in a sentence, which is problematic in estimating the parameters of a probability model. To cope with this, we adopted an algorithm of maximum entropy estimation for feature forests (Miyao and Tsujii, 2002; Geman and Johnson, 2002), which allows parameters to be efficiently estimated. The algorithm enables probabilistic modeling of complete structures, such as transition sequences in Markov models and parse trees, without dividing them into independent sub-events. The algorithm avoids exponential explosion by representing a probabilistic event by a packed representation of a feature space. If a complete structure is represented with a feature forest of a tractable size, the parameters can be efficiently estimated by dynamic programming. A series of studies on parsing with wide-coverage LFG (Johnson et al., 1999; Riezler"
W03-0416,C02-1114,0,\N,Missing
W03-0416,J92-4003,0,\N,Missing
W03-0416,J98-1004,0,\N,Missing
W03-0416,P98-2124,0,\N,Missing
W03-0416,C98-2119,0,\N,Missing
W05-1510,C00-1007,0,0.486249,"niques exported from parsing to generation worked well while the effects were slightly different in detail. The Nitrogen system (Langkilde and Knight, 1998; Langkilde, 2000) maps semantic relations to a packed forest containing all realizations and selects the best one with a bigram model. Our method extends their approach in that we can utilize syntactic features in the disambiguation model in addition to the bigram. From the perspective of using a lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches. The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al., 1988)) for generating a word lattice containing realizations and selects the best one using a trigram model. White and Baldridge (2003) developed a chart generator for CCG (Combinatory Categorial Grammar (Steedman, 2000)) and proposed several techniques for efficient generation such as best-first search, beam thresholding and chunking the input logical forms (White, 2004). Although some of the techniques look effective, the models to rank candidates are still limited to simple language models. Carroll et al. (1999) developed a cha"
W05-1510,2001.mtsummit-papers.68,0,0.021304,"Missing"
W05-1510,C88-2121,0,0.0611593,"re slightly different in detail. The Nitrogen system (Langkilde and Knight, 1998; Langkilde, 2000) maps semantic relations to a packed forest containing all realizations and selects the best one with a bigram model. Our method extends their approach in that we can utilize syntactic features in the disambiguation model in addition to the bigram. From the perspective of using a lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches. The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al., 1988)) for generating a word lattice containing realizations and selects the best one using a trigram model. White and Baldridge (2003) developed a chart generator for CCG (Combinatory Categorial Grammar (Steedman, 2000)) and proposed several techniques for efficient generation such as best-first search, beam thresholding and chunking the input logical forms (White, 2004). Although some of the techniques look effective, the models to rank candidates are still limited to simple language models. Carroll et al. (1999) developed a chart generator using HPSG. After the generator outputs all the sentence"
W05-1510,J96-1002,0,0.0114489,"onstruct the final output, but slow down the generation because they can be combined with the rest of the input to construct grammatically correct phrases or sentences. Carroll et al. (1999) and White (2004) proposed different algorithms to address the same problem. We adopted Kay’s simple solution in the current ongoing work, but logical form chunking proposed by White is also applicable to our system. 2.3 Probabilistic models for generation with HPSG Some existing studies on probabilistic models for HPSG parsing (Malouf and van Noord, 2004; Miyao and Tsujii, 2005) adopted log-linear models (Berger et al., 1996). Since log-linear models allow us to 1 To introduce an edge with no semantic relations as mentioned in the previous section, we need to combine the edges with edges having no relations. e v er b : e  H E AD  SUBCAT   he bought the book e n o u n : y  H E AD  SUBCAT   bought the book v er b : e  N P : y  H E AD  N O N L O C |SL ASH  E q u iv alen t class y n o u n : y  H E AD SUBCAT    he bought e v er b : e   H E AD SUBCAT N P : x N P : y     v er b : e  H E AD   N P :x  SUBCAT  N O N L O C |SL ASH N P : y  the book z b u y e  x   y  past   n o"
W05-1510,W02-2030,0,0.153481,"Missing"
W05-1510,P96-1027,0,0.530597,"er of solutions including the methods of estimating loglinear models using packed forests of parse trees and pruning improbable candidates during parsing. The aim of this paper is to apply these techniques to generation. Since parsing and generation both output the best probable tree under some constraints, we expect that techniques that work effectively in parsing are also beneficial for generation. First, we enabled estimation of log-linear models with less cost by representing a set of generation trees in a packed forest. The forest representation was obtained by adopting chart generation (Kay, 1996; Car93 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 93–102, c Vancouver, October 2005. 2005 Association for Computational Linguistics roll et al., 1999) where ambiguous candidates are packed into an equivalence class and mapping a chart into a forest in the same way as parsing. Second, we reduced the search space in runtime by adopting iterative beam search (Tsuruoka and Tsujii, 2004) that efficiently pruned improbable candidates. We evaluated the generator on the Penn Treebank (Marcus et al., 1993), which is highly reliable corpus consisting of real-w"
W05-1510,P98-1116,0,0.1183,"e compared the performance of several disambiguation models following an existing study (Velldal and Oepen, 2005) and examined how the performance changed according to the size of training data, the feature set, and the beam width. Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. The results indicated that the techniques exported from parsing to generation worked well while the effects were slightly different in detail. The Nitrogen system (Langkilde and Knight, 1998; Langkilde, 2000) maps semantic relations to a packed forest containing all realizations and selects the best one with a bigram model. Our method extends their approach in that we can utilize syntactic features in the disambiguation model in addition to the bigram. From the perspective of using a lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches. The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al., 1988)) for generating a word lattice containing realizations and"
W05-1510,W02-2103,0,0.842009,"Missing"
W05-1510,A00-2023,0,0.881498,"f several disambiguation models following an existing study (Velldal and Oepen, 2005) and examined how the performance changed according to the size of training data, the feature set, and the beam width. Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. The results indicated that the techniques exported from parsing to generation worked well while the effects were slightly different in detail. The Nitrogen system (Langkilde and Knight, 1998; Langkilde, 2000) maps semantic relations to a packed forest containing all realizations and selects the best one with a bigram model. Our method extends their approach in that we can utilize syntactic features in the disambiguation model in addition to the bigram. From the perspective of using a lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches. The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al., 1988)) for generating a word lattice containing realizations and selects the best"
W05-1510,J93-2004,0,0.035825,"forest representation was obtained by adopting chart generation (Kay, 1996; Car93 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 93–102, c Vancouver, October 2005. 2005 Association for Computational Linguistics roll et al., 1999) where ambiguous candidates are packed into an equivalence class and mapping a chart into a forest in the same way as parsing. Second, we reduced the search space in runtime by adopting iterative beam search (Tsuruoka and Tsujii, 2004) that efficiently pruned improbable candidates. We evaluated the generator on the Penn Treebank (Marcus et al., 1993), which is highly reliable corpus consisting of real-world texts. Through a series of experiments, we compared the performance of several disambiguation models following an existing study (Velldal and Oepen, 2005) and examined how the performance changed according to the size of training data, the feature set, and the beam width. Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. The results indicated that the techniques exported from parsing"
W05-1510,P05-1011,1,0.548976,"h is more fluent and easier to understand than others. In principle, we need to enumerate all alternative realizations in order to estimate a log-linear model for generation. It therefore requires high computational cost to estimate a probabilistic model for a wide-coverage grammar because there are considerable ambiguities and the alternative realizations are hard to enumerate explicitly. Moreover, even after the model has been estimated, to explore all possible candidates in runtime is also expensive. The same problems also arise with HPSG parsing, and recent studies (Tsuruoka et al., 2004; Miyao and Tsujii, 2005; Ninomiya et al., 2005) proposed a number of solutions including the methods of estimating loglinear models using packed forests of parse trees and pruning improbable candidates during parsing. The aim of this paper is to apply these techniques to generation. Since parsing and generation both output the best probable tree under some constraints, we expect that techniques that work effectively in parsing are also beneficial for generation. First, we enabled estimation of log-linear models with less cost by representing a set of generation trees in a packed forest. The forest representation was"
W05-1510,2005.mtsummit-papers.15,0,0.240419,"ssociation for Computational Linguistics roll et al., 1999) where ambiguous candidates are packed into an equivalence class and mapping a chart into a forest in the same way as parsing. Second, we reduced the search space in runtime by adopting iterative beam search (Tsuruoka and Tsujii, 2004) that efficiently pruned improbable candidates. We evaluated the generator on the Penn Treebank (Marcus et al., 1993), which is highly reliable corpus consisting of real-world texts. Through a series of experiments, we compared the performance of several disambiguation models following an existing study (Velldal and Oepen, 2005) and examined how the performance changed according to the size of training data, the feature set, and the beam width. Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. The results indicated that the techniques exported from parsing to generation worked well while the effects were slightly different in detail. The Nitrogen system (Langkilde and Knight, 1998; Langkilde, 2000) maps semantic relations to a packed forest containing all realizatio"
W05-1510,W03-2316,0,0.124088,"a packed forest containing all realizations and selects the best one with a bigram model. Our method extends their approach in that we can utilize syntactic features in the disambiguation model in addition to the bigram. From the perspective of using a lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches. The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al., 1988)) for generating a word lattice containing realizations and selects the best one using a trigram model. White and Baldridge (2003) developed a chart generator for CCG (Combinatory Categorial Grammar (Steedman, 2000)) and proposed several techniques for efficient generation such as best-first search, beam thresholding and chunking the input logical forms (White, 2004). Although some of the techniques look effective, the models to rank candidates are still limited to simple language models. Carroll et al. (1999) developed a chart generator using HPSG. After the generator outputs all the sentences the grammar allows, the ranking mod94  R EL  INDEX   A R G 1  A R G 2 T ENS E bu y  e  x   y  p a s t  t h e"
W05-1510,P02-1040,0,\N,Missing
W05-1510,C98-1112,0,\N,Missing
W05-1510,W05-1511,1,\N,Missing
W05-1511,E03-1052,0,0.140946,"-tokyo.ac.jp Yoshimasa Tsuruoka CREST, JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Yusuke Miyao Department of Computer Science The University of Tokyo yusuke@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Abstract Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank. We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank. We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid parsing with a CFG chunk parser. The cont"
W05-1511,P03-1014,0,0.137203,"asa Tsuruoka CREST, JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Yusuke Miyao Department of Computer Science The University of Tokyo yusuke@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Abstract Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank. We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank. We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid parsing with a CFG chunk parser. The contributions of the lar"
W05-1511,C04-1185,0,0.103326,"JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Yusuke Miyao Department of Computer Science The University of Tokyo yusuke@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Abstract Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank. We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank. We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid parsing with a CFG chunk parser. The contributions of the large constituent"
W05-1511,P02-1036,0,0.582034,"ity is guaranteed. The interesting point of this approach is that, once the exhaustive parsing is completed, the probabilities of non-local dependencies, which cannot be computed during parsing, are computed after making a packed parse forest. Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed (Kasper et al., 1996; Briscoe and Carroll, 1993; Kiefer et al., 2002), and the most probable parse is found by PCFG parsing. This model is based on PCFG and not probabilistic unification-based grammar parsing. Geman and Johnson (Geman and Johnson, 2002) proposed a dynamic programming algorithm for finding the most probable parse in a packed parse forest generated by unification-based grammars without expanding the forest. However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing. In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG parsing for a large-scale corpora, the Penn treebank. We show how techniques developed for efficient deep parsing can improve the efficiency of probabilistic parsing. These techniques were evaluated in exp"
W05-1511,W97-0302,0,0.13169,"plications, a number of studies have focused on improving the parsing efficiency of unificationbased grammars (Oepen et al., 2002). Although significant improvements in efficiency have been made, parsing speed is still not high enough for practical applications. Introduction We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic head-driven phrase structure grammar (HPSG) parsing for the Penn treebank. We first applied beam thresholding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997), and iterative parsing (Tsuruoka and Tsujii, 2005b). The recent introduction of probabilistic models of wide-coverage unification-based grammars (Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) has opened up the novel possibility of increasing parsing speed by guiding the search path using probabilities. That is, since we often require only the most probable parse result, we can compute partial parse results that are likely to contribute to the final parse result. This approach has been extensively studied in the field of probabilistic 103 Proceedings of the Ninth Int"
W05-1511,J97-4005,0,0.0385754,"ion provides the phrasal sign of the mother. The sign of the larger constituent is obtained by repeatedly applying schemata to lexical/phrasal signs. Finally, the parse result is output as a phrasal sign that dominates the sentence. Given set W of words and set F of feature structures, an HPSG is formulated as a tuple, G = hL, Ri, where L = {l = hw, F i|w ∈ W, F ∈ F} is a set of lexical entries, and R is a set of schemata, i.e., r ∈ R is a partial function: F × F → F. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = hw1 , . . . , wn i is ! X 1 exp λi fi (T ) p(T |w) = Zw i Zw = X T0 exp X i ! λi fi (T 0 ) , where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T . Intuitively, the probability is defined as the normalized prod"
W05-1511,P03-1046,0,0.0915903,"Tsujii (2005a). Table 1 shows the abbreviations used in presenting the results. We measured the accuracy of the predicateargument relations output by the parser. A predicate-argument relation is defined as a tuple hσ, wh , a, wa i, where σ is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Precision/recall is the ratio of tuples correctly identified by the parser. This evaluation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 2003; Clark and Curran, 2004; Miyao and Tsujii, 2005). The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU. Section 22 of the Treebank was used as the development set, and performance was evaluated using sentences of less than 40 words in Section 23 (2,164 sentences, 20.3 words/sentence). The performance of each parsing technique was analyzed using the sentences in Section 24 of less than 15 words (305 sentences) and less than 40 words (1145 sentences). Table 2 shows the parsing performance using all"
W05-1511,J96-1002,0,0.0108747,"set F of feature structures, an HPSG is formulated as a tuple, G = hL, Ri, where L = {l = hw, F i|w ∈ W, F ∈ F} is a set of lexical entries, and R is a set of schemata, i.e., r ∈ R is a partial function: F × F → F. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = hw1 , . . . , wn i is ! X 1 exp λi fi (T ) p(T |w) = Zw i Zw = X T0 exp X i ! λi fi (T 0 ) , where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T . Intuitively, the probability is defined as the normalized product of the weights exp(λi ) when a characteristic corresponding to fi appears in parse result T . Model parameters λi are estimated using numerical optimization methods (Malouf, 2002) so as to maximize the log-likelihood of the training data. However, the above model"
W05-1511,P99-1069,0,0.0875204,"the phrasal sign of the mother. The sign of the larger constituent is obtained by repeatedly applying schemata to lexical/phrasal signs. Finally, the parse result is output as a phrasal sign that dominates the sentence. Given set W of words and set F of feature structures, an HPSG is formulated as a tuple, G = hL, Ri, where L = {l = hw, F i|w ∈ W, F ∈ F} is a set of lexical entries, and R is a set of schemata, i.e., r ∈ R is a partial function: F × F → F. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = hw1 , . . . , wn i is ! X 1 exp λi fi (T ) p(T |w) = Zw i Zw = X T0 exp X i ! λi fi (T 0 ) , where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T . Intuitively, the probability is defined as the normalized product of the weights exp"
W05-1511,J93-1002,0,0.0695129,"hout using probabilities and then select the highest probability parse. The behavior of their algorithms is like that of the Viterbi algorithm for PCFG parsing, so the correct parse with the highest probability is guaranteed. The interesting point of this approach is that, once the exhaustive parsing is completed, the probabilities of non-local dependencies, which cannot be computed during parsing, are computed after making a packed parse forest. Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed (Kasper et al., 1996; Briscoe and Carroll, 1993; Kiefer et al., 2002), and the most probable parse is found by PCFG parsing. This model is based on PCFG and not probabilistic unification-based grammar parsing. Geman and Johnson (Geman and Johnson, 2002) proposed a dynamic programming algorithm for finding the most probable parse in a packed parse forest generated by unification-based grammars without expanding the forest. However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing. In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG"
W05-1511,J98-2004,0,0.0287342,"on-terminal symbols, and prevent the HPSG parser from generating edges that cross brackets. 4 4.1 Beam thresholding for HPSG parsing Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein 106 and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, extending the CYK algorithm"
W05-1511,A00-2018,0,0.0825924,"edges that cross brackets. 4 4.1 Beam thresholding for HPSG parsing Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein 106 and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, extending the CYK algorithm to output the Viterbi parse is straightforward (N"
W05-1511,N04-1013,0,0.39086,"artment of Computer Science The University of Tokyo ninomi@is.s.u-tokyo.ac.jp Yoshimasa Tsuruoka CREST, JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Yusuke Miyao Department of Computer Science The University of Tokyo yusuke@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Abstract Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank. We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank. We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent in"
W05-1511,W89-0206,0,0.237937,"from the Penn treebank. The principal idea of using the chunk parser is to use the bracket information, i.e., parse trees without non-terminal symbols, and prevent the HPSG parser from generating edges that cross brackets. 4 4.1 Beam thresholding for HPSG parsing Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein 106 and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in"
W05-1511,P99-1061,0,0.0644723,"rse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 105 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of these techniques. Quick check Quick check filters out non-unifiable feature structures (Malouf et al., 2000). Suppose we have two non-unifiable feature structures. They are destructively unified by traversing and modifying them, and then finally they are found to be not unifiable in the middle of the unification process. Quick check quickly judges their un"
W05-1511,P04-1014,0,0.182952,"hat is, only a nonterminal symbol of a mother is considered in further processing by ignoring the structure of its daughters. With this assumption, we can compute the figures of merit (FOMs) of partial parse results. This assumption restricts the possibility of feature functions that represent non-local dependencies expressed in a parse result. Since unification-based grammars can express semantic relations, such as predicate-argument relations, in their structure, the assumption unjustifiably restricts the flexibility of probabilistic modeling. However, previous research (Miyao et al., 2003; Clark and Curran, 2004; Kaplan et al., 2004) showed that predicate-argument relations can be represented under the assumption of feature locality. We thus assumed the locality of feature functions and exploited it for the efficient search of probable parse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 105 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinat"
W05-1511,C02-1075,0,0.0132895,"d then select the highest probability parse. The behavior of their algorithms is like that of the Viterbi algorithm for PCFG parsing, so the correct parse with the highest probability is guaranteed. The interesting point of this approach is that, once the exhaustive parsing is completed, the probabilities of non-local dependencies, which cannot be computed during parsing, are computed after making a packed parse forest. Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed (Kasper et al., 1996; Briscoe and Carroll, 1993; Kiefer et al., 2002), and the most probable parse is found by PCFG parsing. This model is based on PCFG and not probabilistic unification-based grammar parsing. Geman and Johnson (Geman and Johnson, 2002) proposed a dynamic programming algorithm for finding the most probable parse in a packed parse forest generated by unification-based grammars without expanding the forest. However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing. In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG parsing for a large-s"
W05-1511,N03-1016,0,0.0639364,"Missing"
W05-1511,W02-2018,0,0.0412473,"f unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = hw1 , . . . , wn i is ! X 1 exp λi fi (T ) p(T |w) = Zw i Zw = X T0 exp X i ! λi fi (T 0 ) , where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T . Intuitively, the probability is defined as the normalized product of the weights exp(λi ) when a characteristic corresponding to fi appears in parse result T . Model parameters λi are estimated using numerical optimization methods (Malouf, 2002) so as to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T |w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w). They assumed that features are functions"
W05-1511,J93-4001,0,0.0395362,"it for the efficient search of probable parse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 105 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of these techniques. Quick check Quick check filters out non-unifiable feature structures (Malouf et al., 2000). Suppose we have two non-unifiable feature structures. They are destructively unified by traversing and modifying them, and then finally they are found to be not unifiable in the middle of the unification proces"
W05-1511,P03-1026,0,0.0211687,"ving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 105 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of these techniques. Quick check Quick check filters out non-unifiable feature structures (Malouf et al., 2000). Suppose we have two non-unifiable feature structures. They are destructively unified by traversing and modifying them, and then finally they are found to be not unifiable in the middle of the unification process. Quick check quickly judges their unifiability by peeping the values of the given paths. If one of the path values is not unif"
W05-1511,P00-1061,0,0.132155,"e mother. The sign of the larger constituent is obtained by repeatedly applying schemata to lexical/phrasal signs. Finally, the parse result is output as a phrasal sign that dominates the sentence. Given set W of words and set F of feature structures, an HPSG is formulated as a tuple, G = hL, Ri, where L = {l = hw, F i|w ∈ W, F ∈ F} is a set of lexical entries, and R is a set of schemata, i.e., r ∈ R is a partial function: F × F → F. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = hw1 , . . . , wn i is ! X 1 exp λi fi (T ) p(T |w) = Zw i Zw = X T0 exp X i ! λi fi (T 0 ) , where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T . Intuitively, the probability is defined as the normalized product of the weights exp(λi ) when a character"
W05-1511,J01-2004,0,0.0203056,"s brackets. 4 4.1 Beam thresholding for HPSG parsing Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein 106 and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, extending the CYK algorithm to output the Viterbi parse is straightforward (Ney, 1991; Jur"
W05-1511,P80-1024,0,0.762838,"Missing"
W05-1511,C88-2121,0,0.072195,"Missing"
W05-1511,P05-1011,1,0.829882,"is still not high enough for practical applications. Introduction We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic head-driven phrase structure grammar (HPSG) parsing for the Penn treebank. We first applied beam thresholding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997), and iterative parsing (Tsuruoka and Tsujii, 2005b). The recent introduction of probabilistic models of wide-coverage unification-based grammars (Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) has opened up the novel possibility of increasing parsing speed by guiding the search path using probabilities. That is, since we often require only the most probable parse result, we can compute partial parse results that are likely to contribute to the final parse result. This approach has been extensively studied in the field of probabilistic 103 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 103–114, c Vancouver, October 2005. 2005 Association for Computational Linguistics CFG (PCFG) parsing, such as Viterbi parsing and beam thresholding. While many"
W05-1511,J00-1003,0,0.0194491,"on. The grammar for the chunk parser is automatically extracted from the CFG treebank translated from the HPSG treebank, which is generated during grammar extraction from the Penn treebank. The principal idea of using the chunk parser is to use the bracket information, i.e., parse trees without non-terminal symbols, and prevent the HPSG parser from generating edges that cross brackets. 4 4.1 Beam thresholding for HPSG parsing Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein 106 and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorit"
W05-1511,W05-1514,1,0.885649,"Missing"
W05-1511,J97-3004,0,0.449191,"Missing"
W05-1511,J93-2004,0,\N,Missing
W05-1511,J03-4003,0,\N,Missing
W06-1619,J97-4005,0,0.142664,"verb SUBJ &lt; 1 &gt; COMPS &lt; &gt; come Spring/NN HEAD verb 2 SUBJ &lt; 1 &gt; COMPS &lt;&gt; has/VBZ come/VBN flex= &lt;spring, NN, Figure 1: HPSG parsing. Ã X T0 exp &gt; (2005) also introduced a preliminary probabilistic model p0 (T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage. We have ! X 1 exp λu fu (T ) phpsg (T |w) = Zw u Ã X HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; Figure 2: Example of features. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = hw1 , . . . , wn i is Zw = HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; (Previous probabilistic HPSG) Ã ! X 1 exp λu fu (T ) phpsg0 (T |w) = p0 (T |w) Zw u ! λu fu (T 0 ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree"
W06-1619,J99-2004,0,0.501786,"studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. Supertagging was, in the first place, a technique to reduce the cost of parsing with lexicalized grammars; ambiguity in assigning lexical entries to words is reduced by the light-weight process of supertagging before the heavy process of parsing. Bangalore and Joshi (1999) claimed that if words can be assigned correct supertags, syntactic parsing is almost trivial. Wha"
W06-1619,J96-1002,0,0.016666,"arsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage. We have ! X 1 exp λu fu (T ) phpsg (T |w) = Zw u Ã X HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; Figure 2: Example of features. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = hw1 , . . . , wn i is Zw = HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; (Previous probabilistic HPSG) Ã ! X 1 exp λu fu (T ) phpsg0 (T |w) = p0 (T |w) Zw u ! λu fu (T 0 ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T , and Zw is the sum over the set of all possible parse trees for the sentence. Intuitively, the probability is defined as the normalized product of the weights exp(λu ) when a characteristic corresponding to fu appears in parse result T . The model parameters,"
W06-1619,P05-1022,0,0.134381,"Missing"
W06-1619,C04-1041,0,0.527929,"ment of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics University of Manchester Yusuke Miyao Department of Computer Science University of Tokyo Jun’ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, University of Manchester SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract niak and Johnson, 2005) or over complex phrase structures of head-driven phrase structure grammar (HPSG) or combinatory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which wa"
W06-1619,P05-1011,1,0.474032,"a Tsuruoka School of Informatics University of Manchester Yusuke Miyao Department of Computer Science University of Tokyo Jun’ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, University of Manchester SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract niak and Johnson, 2005) or over complex phrase structures of head-driven phrase structure grammar (HPSG) or combinatory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which was originally developed for lexicalized tree adjoining"
W06-1619,P04-1014,0,0.410288,"ment of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics University of Manchester Yusuke Miyao Department of Computer Science University of Tokyo Jun’ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, University of Manchester SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract niak and Johnson, 2005) or over complex phrase structures of head-driven phrase structure grammar (HPSG) or combinatory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which wa"
W06-1619,E03-1071,0,0.0114331,"k and Curran, 2004b). The CCG supertagger uses a maximum entropy classifier and is similar to our model. We evaluated the performance of our probabilistic model as a supertagger. The accuracy of the resulting supertagger on our development set (Section 22) is given in Table 5 and Table 6. The test sentences were automatically POS-tagged. Results of other supertaggers for automatically exWhen compared with other supertag sets of automatically extracted lexicalized grammars, the (effective) size of our supertag set, 1,361 lexical entries, is between the CCG supertag set (398 categories) used by Curran and Clark (2003) and the LTAG supertag set (2920 elementary trees) used by Shen and Joshi (2003). The relative order based on the sizes of the tag sets exactly matches the order based on the accuracies of corresponding supertaggers. 161 ambiguation of phrase structures. We have not yet investigated whether our results can be reproduced with other lexicalized grammars. Our results might hold only for HPSG because HPSG has strict feature constraints and has lexical entries with rich syntactic information such as wh-movement. 5.2 Efficacy of extremely lexicalized models The implemented parsers of models 1 and 2"
W06-1619,W04-3308,0,0.07953,"ar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. Supertagging was, in the first place, a technique to reduce the cost of parsing with lexicalized grammars; ambiguity in assigning lexical entries to words is reduced by the light-weight process of supertagging before the heavy process of parsing. Bangalore and Jos"
W06-1619,P02-1036,0,0.0599348,"he weights exp(λu ) when a characteristic corresponding to fu appears in parse result T . The model parameters, λu , are estimated using numerical optimization methods (Malouf, 2002) to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T |w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w). Miyao and Tsujii Zw = X Ã 0 p0 (T |w) exp ! 0 λu fu (T ) u T0 p0 (T |w) = X n Y p(li |wi ), i=1 where li is a lexical entry assigned to word wi in T and p(li |wi ) is the probability of selecting lexical entry li for wi . In the experiments, we compared our model with the probabilistic HPSG model of Miyao and Tsujii (2005). The features used in their model are combinations of the feature templates listed in Table 1. The feature templates fbinary and funary are defined for constituent"
W06-1619,W05-1511,1,0.805598,"← α + ∆α; β ← β + ∆β; κ ← κ + ∆κ; δ ← δ + ∆δ; θ ← θ + ∆θ; 4.2 We evaluated the speed and accuracy of parsing with extremely lexicalized models by using Enju 2.1, the HPSG grammar for English (Miyao et al., 2005; Miyao and Tsujii, 2005). The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank (Marcus et al., 1994) (39,832 sentences). The grammar consisted of 3,797 lexical entries for 10,536 words1 . The probabilistic models were trained using the same portion of the treebank. We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al., 2005) and other techFigure 3: Pseudo-code of iterative parsing for HPSG. Zw = X exp Ã X l0 ! 0 λu fu (l , w, i) , u where Zw is the sum over all possible lexical entries for the word wi . The feature templates used in our model are listed in Table 2 and are word trigrams and POS 5-grams. 4 Evaluation Experiments 1 An HPSG treebank is automatically generated from the Penn Treebank. Those lexical entries were generated by applying lexical rules to observed lexical entries in the HPSG treebank (Nakanishi et al., 2004). The lexicon, however, included many lexical entries that do not appear in the HPSG"
W06-1619,W97-0302,0,0.1716,", i). The FOM of a newly created partial parse, F , is computed by summing the values of ρ of the daughters and an additional FOM of F if the model is the previous model or model 3. The FOM for models 1 and 2 is computed by only summing the values of ρ of the daughters; i.e., weights exp(λu ) in the figure are assigned zero. The terms κ and δ are the thresholds of the number of phrasal signs in the chart cell and the beam width for signs in the chart cell. The terms α and β are the thresholds of the number and the beam width of lexical entries, and θ is the beam width for global thresholding (Goodman, 1997). Table 2: Features for the probabilities of lexical entry selection. procedure Parsing(hw1 , . . . , wn i, hL, Ri, α, β, κ, δ, θ) for i = 1 to n foreachP F 0 ∈ {F |hwi , F i ∈ L} p= λu fu (F 0 ) u π[i − 1, i] ← π[i − 1, i] ∪ {F 0 } if (p &gt; ρ[i − 1, i, F 0 ]) then ρ[i − 1, i, F 0 ] ← p LocalThresholding(i − 1, i,α, β) for d = 1 to n for i = 0 to n − d j =i+d for k = i + 1 to j − 1 foreach Fs ∈ φ[i, k], Ft ∈ φ[k, j], r ∈ R if F = r(Fs , Ft ) has succeeded P λu fu (F ) p = ρ[i, k, Fs ] + ρ[k, j, Ft ] + u π[i, j] ← π[i, j] ∪ {F } if (p &gt; ρ[i, j, F ]) then ρ[i, j, F ] ← p LocalThresholding(i, j,κ,"
W06-1619,P03-1046,0,0.045596,"f the parser. A predicate-argument relation is defined as a tuple hσ, wh , a, wa i, where σ is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Labeled precision (LP)/labeled recall (LR) is the ratio of tuples correctly identified by the parser3 . Unlabeled precision (UP)/unlabeled recall (UR) is the ratio of tuples without the predicate type and the argument label. This evaluation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 2003; Clark and Curran, 2004b; Miyao and Tsujii, 2005). The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU. Section 22 of the Treebank was used as the development set, and the performance was evaluated using sentences of ≤ 40 and 100 words in Section 23. The performance of each parsing technique was analyzed using the sentences in Section 24 of ≤ 100 words. Table 3 details the numbers and average lengths of the tested sentences of ≤ 40 and 100 words in Sections 23 and 24, and the total numbers of sentences in Sections 23 and 24. The parsing performance for Section 23 is sho"
W06-1619,P99-1069,0,0.0141744,"1 &gt; COMPS &lt; &gt; come Spring/NN HEAD verb 2 SUBJ &lt; 1 &gt; COMPS &lt;&gt; has/VBZ come/VBN flex= &lt;spring, NN, Figure 1: HPSG parsing. Ã X T0 exp &gt; (2005) also introduced a preliminary probabilistic model p0 (T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage. We have ! X 1 exp λu fu (T ) phpsg (T |w) = Zw u Ã X HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; Figure 2: Example of features. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = hw1 , . . . , wn i is Zw = HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; (Previous probabilistic HPSG) Ã ! X 1 exp λu fu (T ) phpsg0 (T |w) = p0 (T |w) Zw u ! λu fu (T 0 ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T , and Zw is the sum"
W06-1619,P00-1061,0,0.0728146,"ing/NN HEAD verb 2 SUBJ &lt; 1 &gt; COMPS &lt;&gt; has/VBZ come/VBN flex= &lt;spring, NN, Figure 1: HPSG parsing. Ã X T0 exp &gt; (2005) also introduced a preliminary probabilistic model p0 (T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage. We have ! X 1 exp λu fu (T ) phpsg (T |w) = Zw u Ã X HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; Figure 2: Example of features. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = hw1 , . . . , wn i is Zw = HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; (Previous probabilistic HPSG) Ã ! X 1 exp λu fu (T ) phpsg0 (T |w) = p0 (T |w) Zw u ! λu fu (T 0 ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T , and Zw is the sum over the set of all po"
W06-1619,N04-1013,0,0.0264089,"e/VBN flex= &lt;spring, NN, Figure 1: HPSG parsing. Ã X T0 exp &gt; (2005) also introduced a preliminary probabilistic model p0 (T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage. We have ! X 1 exp λu fu (T ) phpsg (T |w) = Zw u Ã X HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; Figure 2: Example of features. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = hw1 , . . . , wn i is Zw = HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; (Previous probabilistic HPSG) Ã ! X 1 exp λu fu (T ) phpsg0 (T |w) = p0 (T |w) Zw u ! λu fu (T 0 ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T , and Zw is the sum over the set of all possible parse trees for the sentence. Intuitively,"
W06-1619,P03-1064,0,0.184888,"similar to our model. We evaluated the performance of our probabilistic model as a supertagger. The accuracy of the resulting supertagger on our development set (Section 22) is given in Table 5 and Table 6. The test sentences were automatically POS-tagged. Results of other supertaggers for automatically exWhen compared with other supertag sets of automatically extracted lexicalized grammars, the (effective) size of our supertag set, 1,361 lexical entries, is between the CCG supertag set (398 categories) used by Curran and Clark (2003) and the LTAG supertag set (2920 elementary trees) used by Shen and Joshi (2003). The relative order based on the sizes of the tag sets exactly matches the order based on the accuracies of corresponding supertaggers. 161 ambiguation of phrase structures. We have not yet investigated whether our results can be reproduced with other lexicalized grammars. Our results might hold only for HPSG because HPSG has strict feature constraints and has lexical entries with rich syntactic information such as wh-movement. 5.2 Efficacy of extremely lexicalized models The implemented parsers of models 1 and 2 were around four times faster than the previous model without a loss of accuracy"
W06-1619,P03-1054,0,0.0219404,"hrasestructure-based model. The hybrid model is not only significantly faster but also significantly more accurate by two points of precision and recall compared to the previous model. 1 Introduction For the last decade, accurate and wide-coverage parsing for real-world text has been intensively and extensively pursued. In most of state-of-theart parsers, probabilistic events are defined over phrase structures because phrase structures are supposed to dominate syntactic configurations of sentences. For example, probabilities were defined over grammar rules in probabilistic CFG (Collins, 1999; Klein and Manning, 2003; Char155 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 155–163, c Sydney, July 2006. 2006 Association for Computational Linguistics probabilistic model is defined as the probability of unigram supertagging. So, the hybrid model can be regarded as an extension of supertagging from unigram to n-gram. The hybrid model can also be regarded as a variant of the statistical CDG parser (Wang, 2003; Wang and Harper, 2004), in which the parse tree probabilities are defined as the product of the supertagging probabilities and the dependency pr"
W06-1619,H05-1059,1,0.811846,"nd the performance was evaluated using sentences of ≤ 40 and 100 words in Section 23. The performance of each parsing technique was analyzed using the sentences in Section 24 of ≤ 100 words. Table 3 details the numbers and average lengths of the tested sentences of ≤ 40 and 100 words in Sections 23 and 24, and the total numbers of sentences in Sections 23 and 24. The parsing performance for Section 23 is shown in Table 4. The upper half of the table shows the performance using the correct POSs in the Penn Treebank, and the lower half shows the performance using the POSs given by a POS tagger (Tsuruoka and Tsujii, 2005). The left and right sides of the table show the performances for the sentences of ≤ 40 and ≤ 100 words. Our models significantly increased not only the parsing speed but also the parsing accuracy. Model 3 was around three to four times faster and had around two points higher precision and recall than the previous model. Surprisingly, model 1, which used only lexical information, was very fast and as accurate as the previous model. Model 2 also improved the accuracy slightly without information of phrase structures. When the automatic POS tagger was introduced, both precision and recall droppe"
W06-1619,W04-0307,0,0.294366,"natory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. Supertagging was, in the first place, a technique to reduce the cost of parsing with lexicalized grammars; ambiguity in assigning lexical entries to words is reduced by the light-weight process of supertagging before the heavy process of pa"
W06-1619,W02-2018,0,0.00906575,"iven sentence w = hw1 , . . . , wn i is Zw = HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; (Previous probabilistic HPSG) Ã ! X 1 exp λu fu (T ) phpsg0 (T |w) = p0 (T |w) Zw u ! λu fu (T 0 ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T , and Zw is the sum over the set of all possible parse trees for the sentence. Intuitively, the probability is defined as the normalized product of the weights exp(λu ) when a characteristic corresponding to fu appears in parse result T . The model parameters, λu , are estimated using numerical optimization methods (Malouf, 2002) to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T |w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w). Miyao and Tsujii Zw = X Ã 0 p0 (T |w) exp ! 0"
W06-1619,J93-2004,0,\N,Missing
W06-1619,J03-4003,0,\N,Missing
W06-1634,H94-1020,0,0.10923,"Missing"
W06-1634,P03-1029,0,0.0540281,"ons for information like protein-protein interactions. Full parsing is more effective for acquiring generalized data from long-length words than shallow parsing. The sentences at left in Figure 1 exemplify the advantages of full parsing. The gerund “activating” in the last sentence takes a non-local semantic subject “ENTITY1”, and shallow parsing cannot recognize this relation because “ENTITY1” and “activating” are in different phrases. Full parsing, on the other hand, can identify both the subject of the whole sentence and the semantic subject of “activating” have been shared. 3 Related Work Sudo et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005) acquired substructures derived from dependency trees as extraction patterns for IE in general domains. Their approaches were similar to our approach using PASs derived from full parsing. However, one problem with their systems is that they could not treat non-local dependencies such as semantic subjects of gerund constructions (discussed in Section 2), and thus rules acquired from the constructions were partial. Bunescu and Mooney (2006) also learned extraction patterns for protein-protein interactions by SVM with a generalized subseq"
W06-1634,P04-1056,0,0.125732,"Missing"
W06-1634,H05-1091,0,0.0138968,"s. Full parsing is more effective for acquiring generalized data from long-length words than shallow parsing. The sentences at left in Figure 1 exemplify the advantages of full parsing. The gerund “activating” in the last sentence takes a non-local semantic subject “ENTITY1”, and shallow parsing cannot recognize this relation because “ENTITY1” and “activating” are in different phrases. Full parsing, on the other hand, can identify both the subject of the whole sentence and the semantic subject of “activating” have been shared. 3 Related Work Sudo et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005) acquired substructures derived from dependency trees as extraction patterns for IE in general domains. Their approaches were similar to our approach using PASs derived from full parsing. However, one problem with their systems is that they could not treat non-local dependencies such as semantic subjects of gerund constructions (discussed in Section 2), and thus rules acquired from the constructions were partial. Bunescu and Mooney (2006) also learned extraction patterns for protein-protein interactions by SVM with a generalized subsequence kernel. Their patterns are sequences of words, POSs,"
W06-1634,P05-1052,0,0.0403215,"within the high-recall range. Compared to theirs, one of our problems is that our method could not handle attributives. One example is “binding property of ENTITY1 to ENTITY2”. We could not obtain “binding” because the smallest set of PASs connecting “ENTITY1” and “ENTITY2” includes only the PASs of “property”, “of” and “to”. To handle these attributives, we need distinguish necessary attributives from those that are general4 by semantic analysis or bootstrapping. Another approach to improve our method is to include local information in sentences, such as surface words between protein names. Zhao and Grishman (2005) reported that adding local information to deep syntactic information improved IE results. This approach is also applicable to IE in other domains, where related entities are in a short 5.3.2 Lack of Necessary Patterns and Learning of Inappropriate Patterns There are two different reasons causing the problems with the lack of necessary patterns and the learning of inappropriate patterns: (1) the training corpus was not sufﬁciently large to saturate IE accuracy and (2) our method of pattern construction was too limited. Effect of Training Corpus Size To investigate whether the training corpus w"
W06-1634,P04-1054,0,0.0565752,"like protein-protein interactions. Full parsing is more effective for acquiring generalized data from long-length words than shallow parsing. The sentences at left in Figure 1 exemplify the advantages of full parsing. The gerund “activating” in the last sentence takes a non-local semantic subject “ENTITY1”, and shallow parsing cannot recognize this relation because “ENTITY1” and “activating” are in different phrases. Full parsing, on the other hand, can identify both the subject of the whole sentence and the semantic subject of “activating” have been shared. 3 Related Work Sudo et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005) acquired substructures derived from dependency trees as extraction patterns for IE in general domains. Their approaches were similar to our approach using PASs derived from full parsing. However, one problem with their systems is that they could not treat non-local dependencies such as semantic subjects of gerund constructions (discussed in Section 2), and thus rules acquired from the constructions were partial. Bunescu and Mooney (2006) also learned extraction patterns for protein-protein interactions by SVM with a generalized subsequence kernel. Their patterns"
W06-1634,P05-1053,0,0.0780532,"Missing"
W07-2202,A00-2021,0,0.0726437,"Missing"
W07-2202,I05-1006,0,0.0756674,"Missing"
W07-2202,H94-1020,0,0.181523,"Missing"
W07-2202,J99-2004,0,0.068113,"Missing"
W07-2202,J96-1002,0,0.00405831,"Missing"
W07-2202,W06-1615,0,0.0552113,"Missing"
W07-2202,P06-1012,0,0.0317632,"Missing"
W07-2202,C04-1041,0,0.0205609,"Missing"
W07-2202,P04-1014,0,0.0114937,"Missing"
W07-2202,N06-1019,0,0.0615363,"Missing"
W07-2202,W05-1102,0,0.112719,"Missing"
W07-2202,I05-1018,1,0.493404,"Missing"
W07-2202,P06-1043,0,0.0634495,"Missing"
W07-2202,P05-1011,1,0.486667,"Missing"
W07-2202,W06-1619,1,0.793255,"Missing"
W07-2202,W04-1203,0,0.0172303,"Missing"
W07-2202,N03-1027,0,0.104549,"Missing"
W07-2202,E03-1008,0,0.0935117,"Missing"
W07-2202,W06-2902,0,0.126602,"Missing"
W07-2208,P06-1041,0,0.0124517,"ciency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed. Wang and Harper (2004) also demonstrated"
W07-2208,J97-4005,0,0.510494,"ar (LFG) (Bresnan, 1982). They are preferred because they give precise and in-depth analyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007"
W07-2208,J99-2004,0,0.0748299,"n Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed. Wang and Harper (2004) also demonstrated the effects of supertagging with a statistical constraint dependency grammar (CDG) parser by showing accura"
W07-2208,J96-1002,0,0.0413835,"grammar (CCG) (Steedman, 2000) and lexical function grammar (LFG) (Bresnan, 1982). They are preferred because they give precise and in-depth analyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 6"
W07-2208,C04-1041,0,0.456892,"es such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a proc"
W07-2208,P04-1014,0,0.211016,"es such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a proc"
W07-2208,P06-1037,0,0.0803821,"are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed. Wang and Harper (2004) also demonstrated the effects of supertagging with a statistical constraint dependency grammar (CDG) parser by showing accuracy as high as the state-of-the-art parsers, and Foth et al. (2006) and Foth and Menzel (2006) reported that accuracy was significantly improved by incorporating the supertagging probabilities into manually tuned Weighted CDG. Ninomiya et al. (2006) showed the parsing model using only supertagging probabilities could achieve accuracy as high as the probabilistic model for phrase structures. This means that syntactic structures are almost determined by supertags as is claimed by Bangalore and Joshi (1999). However, supertaggers themselves were heuristically used as an external tagger. They filter out unlikely lexical entries just to help parsing (Clark and Cur"
W07-2208,P02-1036,0,0.316566,"in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexic"
W07-2208,W97-0302,0,0.0999642,"and Tsujii (2005)’s model, but ‘our model 1’ achieved 0.56 points higher F-score, and ‘our model 2’ achieved 0.8 points higher F-score. When the automatic POS tagger was introduced, Fscore dropped by around 2.4 points for all models. We also compared our model with Matsuzaki et al. (2007)’s model. Matsuzaki et al. (2007) proThe terms κ and δ are the thresholds of the number of phrasal signs in the chart cell and the beam width for signs in the chart cell. The terms α and β are the thresholds of the number and the beam width of lexical entries, and θ is the beam width for global thresholding (Goodman, 1997). The terms with suffixes 0 are the initial values. The parser iterates parsing until it succeeds to generate a parse tree. The parameters increase for each iteration by the terms prefixed by ∆, and parsing finishes when the parameters reach the terms with suffixes last. Details of the parameters are written in (Ninomiya et al., 2005). The beam thresholding parameters for ‘our model 2’ are α0 = 18, ∆α = 6, αlast = 42, β0 = 9.0, ∆β = 3.0, βlast = 21.0, δ0 = 18, ∆δ = 6, δlast = 42, κ0 = 9.0, ∆κ = 3.0, κlast = 21.0. In ‘our model 2’, the global thresholding was not used. 66 posed a technique for"
W07-2208,P03-1046,0,0.0277048,"f the parser. A predicate-argument relation is defined as a tuple hσ, wh , a, wa i, where σ is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Labeled precision (LP)/labeled recall (LR) is the ratio of tuples correctly identified by the parser2 . Unlabeled precision (UP)/unlabeled recall (UR) is the ratio of tuples without the predicate type and the argument label. This evaluation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 2003; Clark The HPSG treebank is used for training the probabilistic model for lexical entry selection, and hence, those lexical entries that do not appear in the treebank are rarely selected by the probabilistic model. The ‘effective’ tag set size, therefore, is around 1,361, the number of lexical entries without those never-seen lexical entries. 2 When parsing fails, precision and recall are evaluated, although nothing is output by the parser; i.e., recall decreases greatly. 65 and Curran, 2004b; Miyao and Tsujii, 2005). The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU."
W07-2208,A00-2021,0,0.0236621,"ence w = hw1 , . . . , wn i is to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w). Miyao and Tsujii (2005) also introduced a preliminary probabilistic model p0 (T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution (Jelinek, 1998; Johnson and Riezler, 2000) of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage (Miyao and Tsujii, 2005), or a probabilistic model can be augmented by several distributions estimated from the larger and simpler corpus (Johnson and Riezler, 2000). In (Miyao and Tsujii, 2005), p0 (T |w) is defined as the product of probabilities of selecting lexical entries with word and POS unigram features: (Miyao and Tsujii (2005)’s model) 1 puniref (T |w) = p0 (T |w) exp Zw Zw = (Probabilistic HPSG) Zw = T0 Ã 0 p0 (T |w) exp T0 phpsg (T |w) = X X"
W07-2208,P99-1069,0,0.311858,"snan, 1982). They are preferred because they give precise and in-depth analyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Comput"
W07-2208,N04-1013,0,0.253832,"nalyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wa"
W07-2208,W05-1511,1,0.920749,"y a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged wi"
W07-2208,W06-1619,1,0.158093,"scriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed"
W07-2208,P00-1061,0,0.388867,"preferred because they give precise and in-depth analyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ra"
W07-2208,W02-2018,0,0.014737,"SG) Zw = T0 Ã 0 p0 (T |w) exp T0 phpsg (T |w) = X X 1 exp Zw Ã exp Ã X X ! p0 (T |w) = λu fu (T ) X Ã X u ! λu fu (T ) ! 0 λu fu (T ) u n Y p(li |wi ), i=1 u ! 0 λu fu (T ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T , and Zw is the sum over the set of all possible parse trees for the sentence. Intuitively, the probability is defined as the normalized product of the weights exp(λu ) when a characteristic corresponding to fu appears in parse result T . The model parameters, λu , are estimated using numerical optimization methods (Malouf, 2002) to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T |w) for all parse candidates assigned 62 where li is a lexical entry assigned to word wi in T and p(li |wi ) is the probability of selecting lexical entry li for wi . In the experiments, we compared our model with other two types of probabilistic models using a supertagger (Ninomiya et al., 2006). The first one is the simplest probabilistic model, which is defined with only the probabilities of lexical entry selection. It is defined si"
W07-2208,H05-1059,1,0.820103,"Missing"
W07-2208,W04-0307,0,0.13942,"04; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result"
W07-2208,P05-1011,1,0.268471,"g linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr"
W07-2208,W04-3308,0,0.0211161,"005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improveme"
W07-2208,J93-2004,0,\N,Missing
W08-0504,J96-1002,0,0.0067758,"Missing"
W08-0504,D07-1024,0,0.016794,"OBJ This study demonstrates that IL-8 recognizes and activates CXCR1, CXCR2, and the Duffy antigen by distinct mechanisms. ROOT SBJ COORD CC ROOT IL-8 recognizes and activates CXCR1 The molar ratio of serum retinol-binding protein (RBP) to transthyretin (TTR) is not useful to assess vitamin A status during infection in hospitalized children. Figure 1: A dependency tree Figure 2: Example sentences with protein names SBJ is a positive example, and &lt;RBP, TTR&gt; is a negative example. Following recent work on using dependency parsing in systems that identify protein interactions in biomedical text (Erkan et al., 2007; Sætre et al., 2007; Katrenko and Adriaans, 2006), we have built a system for PPI extraction that uses dependency relations as features. As exemplified, for the protein pair IL-8 and CXCR1 in the first sentence of Figure 2, a dependency parser outputs a dependency tree shown in Figure 1. From this dependency tree, we can extract a dependency path between IL-8 and CXCR1 (Figure 3), which appears to be a strong clue in knowing that these proteins are mentioned as interacting. The system we use in this paper is similar to the one described in Sætre et al. (2007), except that it uses syntactic de"
W08-0504,W07-2202,1,0.851396,"of the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994) during development and training. While this claim is supported by convincing evaluations that show that parsers trained on the WSJ Penn Treebank alone perform poorly on biomedical text in terms of accuracy of dependencies or bracketing of phrase structure, the benefits of using domainspecific data in terms of practical system performance have not been quantified. These expected benefits drive the development of domain-specific resources, such as the GENIA treebank (Tateisi et al., 2005), and parser domain adaption (Hara et al., 2007), which are of clear importance in parsing research, but of largely unconfirmed impact on practical systems. Quirk and Corston-Oliver (2006) examine a similar issue, the relationship between parser accuracy and overall system accuracy in syntaxinformed machine translation. Their research is similar to the work presented here, but they focused on the use of varying amounts of out-ofdomain training data for the parser, measuring how a translation system for technical text performed when its syntactic parser was trained with varying amounts of Wall Street Journal text. Our work, in contrast, inve"
W08-0504,W04-3111,0,0.0291568,"Missing"
W08-0504,I05-1006,0,0.120707,"Missing"
W08-0504,P08-1006,1,0.923319,"n section 2 we discuss our motivation and related efforts. Section 3 describes the system for identification of protein-protein interactions used in our experiments, and in section 4 describes the syntactic parser that provides the analyses for the PPI system, and the data used to train the parser. We describe our experiments, results and analysis in section 5, and conclude in section 6. 2 Motivation and related work While recent work has addressed questions relating to the use of different parsers or different types of syntactic representations in the PPI extraction task (Sætre et al., 2007, Miyao et al., 2008), little concrete evidence has been provided for potential benefits of improved parsers or additional resources for training syntactic parsers. In fact, although there is increasing interest in parser evaluation in the biomedical domain in terms of precision/recall of brackets and dependency accuracy (Clegg and Shepherd, 2007; Pyysalo et al., 2007; Sagae et al., 2008), the relationship between these evaluation metrics and the performance of practical information extraction systems remains unclear. In the parsing community, relatively small accuracy gains are often reported as success stories,"
W08-0504,W07-1004,0,0.0158531,"s in section 5, and conclude in section 6. 2 Motivation and related work While recent work has addressed questions relating to the use of different parsers or different types of syntactic representations in the PPI extraction task (Sætre et al., 2007, Miyao et al., 2008), little concrete evidence has been provided for potential benefits of improved parsers or additional resources for training syntactic parsers. In fact, although there is increasing interest in parser evaluation in the biomedical domain in terms of precision/recall of brackets and dependency accuracy (Clegg and Shepherd, 2007; Pyysalo et al., 2007; Sagae et al., 2008), the relationship between these evaluation metrics and the performance of practical information extraction systems remains unclear. In the parsing community, relatively small accuracy gains are often reported as success stories, but again, the 15 precise impact of such improvements on practical tasks in bioinformatics has not been established. One aspect of this issue is the question of domain portability and domain adaptation for parsers and other NLP modules. Clegg and Shepherd (2007) mention that available statistical parsers appear to overfit to the newswire domain, b"
W08-0504,W06-1608,0,0.0307019,"m is supported by convincing evaluations that show that parsers trained on the WSJ Penn Treebank alone perform poorly on biomedical text in terms of accuracy of dependencies or bracketing of phrase structure, the benefits of using domainspecific data in terms of practical system performance have not been quantified. These expected benefits drive the development of domain-specific resources, such as the GENIA treebank (Tateisi et al., 2005), and parser domain adaption (Hara et al., 2007), which are of clear importance in parsing research, but of largely unconfirmed impact on practical systems. Quirk and Corston-Oliver (2006) examine a similar issue, the relationship between parser accuracy and overall system accuracy in syntaxinformed machine translation. Their research is similar to the work presented here, but they focused on the use of varying amounts of out-ofdomain training data for the parser, measuring how a translation system for technical text performed when its syntactic parser was trained with varying amounts of Wall Street Journal text. Our work, in contrast, investigates the use of domain-specific training material in parsers for biomedical text, a domain where significant amounts of effort are alloc"
W08-0504,P06-2089,1,0.841027,"protein names split, tokenized, and annotated with proteins and PPIs. 4 A data-driven dependency parser for biomedical text The parser we used as component of our PPI extraction system was a shift-reduce dependency parser that uses maximum entropy models to determine the parser’s actions. Our overall parsing approach uses a best-first probabilistic shift-reduce algorithm, working left-to right to find labeled dependencies one at a time. The algorithm is essentially a dependency version of the constituent parsing algorithm for probabilistic parsing with LR-like data-driven models described by Sagae and Lavie (2006). This dependency parser has been shown to have state-of-the-art accuracy in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre, 2007). Sagae and Tsujii (2007) present a detailed description of the parsing approach used in our work, including the parsing algorithm and the features used to classify parser actions. In summary, the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic structures, and a queue of remaining input tokens. At each step in the parsing process, the parser can apply a shift acti"
W08-0504,I05-2038,1,0.931944,"the newswire domain, because of their extensive use of the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994) during development and training. While this claim is supported by convincing evaluations that show that parsers trained on the WSJ Penn Treebank alone perform poorly on biomedical text in terms of accuracy of dependencies or bracketing of phrase structure, the benefits of using domainspecific data in terms of practical system performance have not been quantified. These expected benefits drive the development of domain-specific resources, such as the GENIA treebank (Tateisi et al., 2005), and parser domain adaption (Hara et al., 2007), which are of clear importance in parsing research, but of largely unconfirmed impact on practical systems. Quirk and Corston-Oliver (2006) examine a similar issue, the relationship between parser accuracy and overall system accuracy in syntaxinformed machine translation. Their research is similar to the work presented here, but they focused on the use of varying amounts of out-ofdomain training data for the parser, measuring how a translation system for technical text performed when its syntactic parser was trained with varying amounts of Wall"
W08-0504,D07-1096,0,\N,Missing
W08-1305,P06-4020,0,0.125901,"n tree to an HPSG (Pollard and Sag, 1994) phrase structure tree with AVM) 2. Converting analyses given in different framework-specific formats to some simpler format proposed as a framework-independent evaluation schema (e.g. converting from c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 29 Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 29–35 Manchester, August 2008 HPSG phrase structure tree with AVM to GR (Briscoe et al., 2006)) of a binary relation between a token assigned as the head of the relation and other tokens assigned as its dependents. Notice however that grammar frameworks considerably disagree in the way they assign heads and non-heads. This would raise the doubt that, no matter how much information is removed, there could still remain disagreements between grammar formalisms in what is left. The simplicity of GR, or other dependencybased metrics, may give the impression that conversion from a more complex representation into it is easier than conversion between two complex representations. In other word"
W08-1305,de-marneffe-etal-2006-generating,0,0.0635719,"Missing"
W08-1305,C96-2120,0,0.01087,"and complex sentence, is a probable reason for the annotation errors in the gold standard described in (Briscoe et al., 2006). To avoid the same problem in our creation of a gold standard, we propose to allow non-exhaustive annotation. In fact, our proposal is to limit the number of phenomena assigned to a sentence to one. This decision on which phenomenon to be assigned is made, when the test suite is constructed, for each of the sentences contained in it. Following the traditional approach, we include every sentence in the test suite, along with the core phenomenon we intend to test it on (Lehmann and Oepen, 1996). Thus, Sentence 1 would be assigned the phenomenon of unshifted ditransitive. Sentence 2 would be assigned the phenomenon of 1. assigns a monotransitive verb analysis to ‘give’ and an adjunct analysis to ‘to Mary’ in 1 2. assigns a ditransitive verb analysis to ‘give’ in 2 The list of pairs we obtain from running the recogniser on the results produced by batch parsing the test suite with the parser to be evaluated is the following: h1,hproper noun,monotransitive,preposition,adjunctii, h2, hproper noun,dative-shifted ditransitivei i 3.3 Performance Measure Calculation Comparing the two list of"
W09-3828,D07-1112,0,0.0673841,"Missing"
W09-3828,gimenez-marquez-2008-towards,0,0.0279212,"Missing"
W09-3828,W07-2202,1,0.842975,"dination” and one error of “this” with the category “Head selection for noun phrase.” When we correct an error in the interdependent error group (a), the correction leads to not only correction of the other errors in (a) but also correction of the error in (b) via correction propagation from (a) to (b). Therefore, a correction effect of an error in group (a) results in 6.0. 6 Further applications of our methods In this section, as an example of the further application of our methods, we attempt to analyze parsing behaviors in domain adaptation from the viewpoints of error cause categories. In Hara et al. (2007), we proposed a method for adapting Enju to a target domain, and then succeeded in improving the parser performance for the GENIA corpus (Kim et al., 2003), a biomedical domain. Table 6 summarizes the parsing results for three types of settings respectively: parsing PTB with Enju (“Enju for PTB”), parsing GENIA with Enju (“Enju for GENIA”), and parsing GENIA with the adapted model (“Adapted for GENIA”). We then analyzed the performance transition among these settings from the viewpoint of the cause categories given in Section 4.1 (Table 7). In order to compare the error frequencies among diffe"
W09-3828,H94-1020,0,0.175357,"ect evaluation methods and the methods of analyzing errors. Table 3: Errors classified into cause categories (c) shows what the resultant network looks like. An inter-dependent error group of 1, 2, 3 and 4 is recognized by [Step 3] and represented as a single node. Error 5 is propagated to this node in the final network. 5 1,811 (1,009) 4,709 (3,085) 1,978 501 90.69 (90.78/90.59) Table 4: Summary of inter-dependencies 㪝㫉㪼㫈㫌㪼㫅㪺㫐 Cause categories of errors Experiments We applied our methods to the analyses of actual errors produced by Enju. This version of Enju was trained on the Penn Treebank (Marcus et al., 1994) Section 2-21. 5.1 Observation of identified cause categories We first parsed sentences in PTB Section 22, and based on the observation of errors, we defined the patterns in Section 4. We then parsed sentences in Section 0. The errors in Section 0 were mapped to error cause categories by the pattern rules created for Section 22. Table 3 summarizes the distribution across the causes of errors. The left and right numbers in the table show the number of erroneous triplets classified into the categories and the frequency of the patterns matched, respectively. The table shows that, with the 14 patt"
W09-3828,D07-1013,0,0.124779,"Missing"
W09-3828,P05-1011,1,0.81661,"hallow parsing is followed by another component for semantic label assignment. In order to address these two issues, we propose two methods in this paper. One is to recognize cause categories of errors and the other is to capture inter-dependencies among errors. The former method defines various patterns of errors to identify categories of error causes. The latter method re-parses a sentence with a single target error corrected, and regards the errors which are corrected in re-parse as errors dependent on the target. Although these two methods are implemented for a specific parser using HPSG (Miyao and Tsujii, 2005; Ninomiya et al., 2006), the same ideas can be applied to any type of parsing models. In this paper, we propose two methods for analyzing errors in parsing. One is to classify errors into categories which grammar developers can easily associate with defects in grammar or a parsing model and thus its improvement. The other is to discover inter-dependencies among errors, and thus grammar developers can focus on errors which are crucial for improving the performance of a parsing model. The first method uses patterns of errors to associate them with categories of causes for those errors, such as"
W09-3828,W06-1619,1,0.846152,"Missing"
W10-1816,A00-2031,0,0.0367762,"sentences. The annotation of more sentences in the science domain is ongoing. The current annotation of the NICT Chinese Treebank is informative for some language analysis tasks, such as syntactic parsing and word segmentation. However, the deep information, which includes both the grammatical functional tags and the traces, are omitted in the annotation. Without grammatical functions, the simple bracketing structure is not informative enough to represent the semantics for Chinese. Furthermore, the traces are critical elements in detecting long-distance dependencies. Gabbard et al. (2006) and Blaheta and Charniak (2000) applied machine learning models to automatically assign the empty categories and functional tags to an English treebank. However, considering about the different domains that the Penn Chinese Treebank and the NICT Chinese Treebank belong to, the machine learning model trained on the Penn Chinese Treebank may not work successfully on the NICT Chinese Treebank. In order to guarantee the high annotation quality, in our work, we manually re-annotate both the grammatical functional tags and the traces to the NICT Chinese Treebank. With the deep re-annotation, the NICT Chinese Treebank could be use"
W10-1816,N06-1024,0,0.017187,"00 Chinese sentences. The annotation of more sentences in the science domain is ongoing. The current annotation of the NICT Chinese Treebank is informative for some language analysis tasks, such as syntactic parsing and word segmentation. However, the deep information, which includes both the grammatical functional tags and the traces, are omitted in the annotation. Without grammatical functions, the simple bracketing structure is not informative enough to represent the semantics for Chinese. Furthermore, the traces are critical elements in detecting long-distance dependencies. Gabbard et al. (2006) and Blaheta and Charniak (2000) applied machine learning models to automatically assign the empty categories and functional tags to an English treebank. However, considering about the different domains that the Penn Chinese Treebank and the NICT Chinese Treebank belong to, the machine learning model trained on the Penn Chinese Treebank may not work successfully on the NICT Chinese Treebank. In order to guarantee the high annotation quality, in our work, we manually re-annotate both the grammatical functional tags and the traces to the NICT Chinese Treebank. With the deep re-annotation, the NI"
W10-1816,hockenmaier-steedman-2002-acquiring,0,0.0151834,"k and the NICT Chinese Treebank belong to, the machine learning model trained on the Penn Chinese Treebank may not work successfully on the NICT Chinese Treebank. In order to guarantee the high annotation quality, in our work, we manually re-annotate both the grammatical functional tags and the traces to the NICT Chinese Treebank. With the deep re-annotation, the NICT Chinese Treebank could be used not only for the shallow natural language processing tasks, but also as a resource for deep applications, such as the lexicalized grammar development from treebanks (Miyao 2006; Guo 2009; Xia 1999; Hockenmaier and Steedman 2002). Considering that the translation quality of the sentences in the NICT Chinese Treebank may affect the quality of re-annotation, in the current phase, we only selected 2,363 sentences that are of good translation quality, for re-annotation. In the future, with the expansion of the NICT Chinese Treebank, we will continue this reannotation work on large-scale sentences. 2 Content of Re-annotation Because the NICT Chinese Treebank follows the annotation guideline of the Penn Chinese Treebank, our re-annotation uses similar annotation criteria in the Penn Chinese Treebank. Figure 1 exemplifies ou"
W10-1816,C02-1145,0,0.0993283,"Missing"
W11-0221,W06-1615,0,0.0295068,"difficult for the parser. If two question sentences were concatenated by conjunctions into one sentence, the parser would tend to fail to analyze the sentence construction for the latter sentence. The remaining errors in Table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data. 6 Related work Since domain adaptation has been an extensive research area in parsing research (Nivre et al., 2007), a lot of ideas have been proposed, including un/semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). Their main focus was on adapting parsing models trained with a specific genre of text (in most cases PTB-WSJ) to other genres of text, such as biomedical research papers. A major problem tackled in such a task setting is the handling of unknown words and domain-specific ways of expressions. However, as we explored, parsing NL queries involves a significantly different problem; even when all words in a sentence are known, the"
W11-0221,W05-1102,0,0.0259069,"d by conjunctions into one sentence, the parser would tend to fail to analyze the sentence construction for the latter sentence. The remaining errors in Table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data. 6 Related work Since domain adaptation has been an extensive research area in parsing research (Nivre et al., 2007), a lot of ideas have been proposed, including un/semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). Their main focus was on adapting parsing models trained with a specific genre of text (in most cases PTB-WSJ) to other genres of text, such as biomedical research papers. A major problem tackled in such a task setting is the handling of unknown words and domain-specific ways of expressions. However, as we explored, parsing NL queries involves a significantly different problem; even when all words in a sentence are known, the sentence has a very different construction from declarative sentences. A"
W11-0221,W07-2202,1,0.870942,"ower is very high. If NL queries can be automatically translated into SPARQL queries, human users can access their desired knowledge without learning the complex query language of SPARQL. This paper presents our preliminary work for NL query processing, with focus on syntactic parsing. We first build a small treebank of natural language queries, which are from Genomics track (Hersh et al., 2004; Hersh et al., 2005; Hersh et al., 2006; Hersh et al., 2007) topics (Section 2 and 3). The small treebank is then used to test the performance of a state-of-the-art parser, Enju (Ninomiya et al., 2007; Hara et al., 2007) (Section 4). The results show that a parser trained on Wall-StreetJournal (WSJ) articles and Medline abstracts will not work well on query sentences. Next, we experiment an adaptive learning technique, to seek the chance to improve the parsing performance on query sentences. Despite the small scale of the experiments, the results enlighten directions for effective 3 http://www.biomoby.org/ http://esw.w3.org/HCLSIG BioRDF Subgroup 4 http://www.w3.org/TR/rdf-sparql-query/ http://www.w3.org/RDF/ 164 Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 1"
W11-0221,P06-1063,0,0.0664676,"Missing"
W11-0221,P03-1054,0,0.00401217,"+ entity type” (45 sentences), “which + entity type” (4 sentences), or “In what + entity type” (1 sentence). In contrast, the GENIA Treebank Corpus (Tateisi et al., 2005)5 is estimated to have no imperative sentences and only seven interrogative sentences (see Section 5.2.2). Thus, the sentence constructions in GTREC04–07 are very different from those in the GENIA treebank. 3 Treebanking GTREC query sentences We built a treebank (with POS) on 196 query sentences following the guidelines of the GENIA Treebank (Tateisi and Tsujii, 2006). The queries were first parsed using the Stanford Parser (Klein and Manning, 2003), and manual correction was made 5 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/ wiki.cgi?page=GENIA+Treebank SBARQ 4 Parsing system and extraction of imperative and question sentences SQ VP WHNP[i168] NP[i169→i168] WDT NNS VBP What toxicities are NP[→i169] VBN [] associated [] PP IN NN with cytarabine Figure 2: The tree structure for an interrogative sentence by the second author. We tried to follow the guideline of the GENIA Treebank as closely as possible, but for the constructions that are rare in GENIA, we used the ATIS corpus in Penn Treebank (Bies et al., 1995), which is also a colle"
W11-0221,H94-1020,0,0.0238544,"ysis of the 2004 track and other known biologist information needs. The derived templates were used as the commands to find articles describing biological interests such as methods or roles of genes. Although the templates were in the form “Find articles describing ...”, actual obtained imperatives begin with “Describe the procedure or method for” (12 sentences), “Provide information about” (36 sentences) or “Provide information on” (12 sentences). GTREC06 consists only of wh-questions where a wh-word constitutes a noun phrase by itself (i.e. its 165 part-of-speech is the WP in Penn Treebank (Marcus et al., 1994) POS tag set) or is an adverb (WRB). In the 2006 track, the templates for the 2005 track were reformulated into the constructions of questions and were then utilized for deriving the questions. For example, the templates to find articles describing the role of a gene involved in a given disease is reformulated into the question “What is the role of gene in disease?” GTREC07 consists only of wh-questions where a wh-word serves as a pre-nominal modifier (WDT). In the 2007 track, unlike in those of last two years, questions were not categorized by the templates, but were based on biologists’ info"
W11-0221,P06-1043,0,0.026801,"tences were concatenated by conjunctions into one sentence, the parser would tend to fail to analyze the sentence construction for the latter sentence. The remaining errors in Table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data. 6 Related work Since domain adaptation has been an extensive research area in parsing research (Nivre et al., 2007), a lot of ideas have been proposed, including un/semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). Their main focus was on adapting parsing models trained with a specific genre of text (in most cases PTB-WSJ) to other genres of text, such as biomedical research papers. A major problem tackled in such a task setting is the handling of unknown words and domain-specific ways of expressions. However, as we explored, parsing NL queries involves a significantly different problem; even when all words in a sentence are known, the sentence has a very different construction fro"
W11-0221,N10-1004,0,0.02207,"sentence, the parser would tend to fail to analyze the sentence construction for the latter sentence. The remaining errors in Table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data. 6 Related work Since domain adaptation has been an extensive research area in parsing research (Nivre et al., 2007), a lot of ideas have been proposed, including un/semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). Their main focus was on adapting parsing models trained with a specific genre of text (in most cases PTB-WSJ) to other genres of text, such as biomedical research papers. A major problem tackled in such a task setting is the handling of unknown words and domain-specific ways of expressions. However, as we explored, parsing NL queries involves a significantly different problem; even when all words in a sentence are known, the sentence has a very different construction from declarative sentences. Although sentence constru"
W11-0221,W07-2208,1,0.810083,"t, yet the expressive power is very high. If NL queries can be automatically translated into SPARQL queries, human users can access their desired knowledge without learning the complex query language of SPARQL. This paper presents our preliminary work for NL query processing, with focus on syntactic parsing. We first build a small treebank of natural language queries, which are from Genomics track (Hersh et al., 2004; Hersh et al., 2005; Hersh et al., 2006; Hersh et al., 2007) topics (Section 2 and 3). The small treebank is then used to test the performance of a state-of-the-art parser, Enju (Ninomiya et al., 2007; Hara et al., 2007) (Section 4). The results show that a parser trained on Wall-StreetJournal (WSJ) articles and Medline abstracts will not work well on query sentences. Next, we experiment an adaptive learning technique, to seek the chance to improve the parsing performance on query sentences. Despite the small scale of the experiments, the results enlighten directions for effective 3 http://www.biomoby.org/ http://esw.w3.org/HCLSIG BioRDF Subgroup 4 http://www.w3.org/TR/rdf-sparql-query/ http://www.w3.org/RDF/ 164 Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, A"
W11-0221,D08-1050,0,0.0222677,"ce are known, the sentence has a very different construction from declarative sentences. Although sentence constructions have gained little attention, a notable exception is (Judge et al., 2006). They pointed out low accuracy of state-ofthe-art parsers on questions, and proposed super171 vised parser adaptation by manually creating a treebank of questions. The question sentences are annotated with phrase structure trees in the PTB scheme, although function tags and empty categories are omitted. An LFG parser trained on the treebank then achieved a significant improvement in parsing accuracy. (Rimell and Clark, 2008) also worked on question parsing. They collected question sentences from TREC 9-12, and annotated the sentences with POSs and CCG (Steedman, 2000) lexical categories. They reported a significant improvement in CCG parsing without phrase structure annotations. On the other hand, (Judge et al., 2006) also implied that just increasing the training data would not be enough. We went further from their work, built a small but complete treebank for NL queries, and explored what really occurred in HPSG parsing. 7 Conclusion In this paper, we explored the problem in parsing queries. We first attempted"
W11-0221,N03-1027,0,0.0338607,"m conjunctions seems to be difficult for the parser. If two question sentences were concatenated by conjunctions into one sentence, the parser would tend to fail to analyze the sentence construction for the latter sentence. The remaining errors in Table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data. 6 Related work Since domain adaptation has been an extensive research area in parsing research (Nivre et al., 2007), a lot of ideas have been proposed, including un/semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). Their main focus was on adapting parsing models trained with a specific genre of text (in most cases PTB-WSJ) to other genres of text, such as biomedical research papers. A major problem tackled in such a task setting is the handling of unknown words and domain-specific ways of expressions. However, as we explored, parsing NL queries involves a significantly different problem; even when all words in a se"
W11-0221,E03-1008,0,0.0380273,"er. If two question sentences were concatenated by conjunctions into one sentence, the parser would tend to fail to analyze the sentence construction for the latter sentence. The remaining errors in Table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data. 6 Related work Since domain adaptation has been an extensive research area in parsing research (Nivre et al., 2007), a lot of ideas have been proposed, including un/semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). Their main focus was on adapting parsing models trained with a specific genre of text (in most cases PTB-WSJ) to other genres of text, such as biomedical research papers. A major problem tackled in such a task setting is the handling of unknown words and domain-specific ways of expressions. However, as we explored, parsing NL queries involves a significantly different problem; even when all words in a sentence are known, the sentence has a very dif"
W11-0221,I05-2038,1,0.816043,"involved in a given disease is reformulated into the question “What is the role of gene in disease?” GTREC07 consists only of wh-questions where a wh-word serves as a pre-nominal modifier (WDT). In the 2007 track, unlike in those of last two years, questions were not categorized by the templates, but were based on biologists’ information needs where the answers were lists of named entities of a given type. The obtained questions begin with “what + entity type” (45 sentences), “which + entity type” (4 sentences), or “In what + entity type” (1 sentence). In contrast, the GENIA Treebank Corpus (Tateisi et al., 2005)5 is estimated to have no imperative sentences and only seven interrogative sentences (see Section 5.2.2). Thus, the sentence constructions in GTREC04–07 are very different from those in the GENIA treebank. 3 Treebanking GTREC query sentences We built a treebank (with POS) on 196 query sentences following the guidelines of the GENIA Treebank (Tateisi and Tsujii, 2006). The queries were first parsed using the Stanford Parser (Klein and Manning, 2003), and manual correction was made 5 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/ wiki.cgi?page=GENIA+Treebank SBARQ 4 Parsing system and extract"
W11-0221,W06-2902,0,0.0135891,"e the sentence construction for the latter sentence. The remaining errors in Table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data. 6 Related work Since domain adaptation has been an extensive research area in parsing research (Nivre et al., 2007), a lot of ideas have been proposed, including un/semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). Their main focus was on adapting parsing models trained with a specific genre of text (in most cases PTB-WSJ) to other genres of text, such as biomedical research papers. A major problem tackled in such a task setting is the handling of unknown words and domain-specific ways of expressions. However, as we explored, parsing NL queries involves a significantly different problem; even when all words in a sentence are known, the sentence has a very different construction from declarative sentences. Although sentence constructions have gained little attention, a notable except"
W11-0221,D07-1096,0,\N,Missing
W11-0328,P04-1015,0,0.0634212,"scores on test data). clearly demonstrate that the lookahead search boosts parsing accuracy. As expected, training and test speed decreases, almost by a factor of three, which is the branching factor of the dependency parser. The table also lists accuracy figures reported in the literature on shift-reduce dependency parsing. Most of the latest studies on shift-reduce dependency parsing employ dynamic programing or beam search, which implies that deterministic methods were not as competitive as those methods. It should also be noted that all of the listed studies learn structured perceptrons (Collins and Roark, 2004), while our parser learns locally optimized perceptrons. In this table, our parser without lookahead search (i.e. depth = 0) resulted in significantly lower accuracy than the previous studies. In fact, it is worse than the deterministic parser of Huang et al. (2009), which uses (almost) the same set of features. This is presumably due to the difference between locally optimized perceptrons and globally optimized structured perceptrons. However, our parser with lookahead search is significantly better than their deterministic parser, and its accuracy is close to the levels of the parsers with b"
W11-0328,W02-1001,0,0.356736,"3 in Figure 3). The only difference between our algorithm and the standard algorithm for margin perceptrons is that we use the states and their scores obtained from lookahead searches (Line 11 in Figure 3), which are backed up from the leaves of the search trees. In Appendix A, we provide a proof of the convergence of our training algorithm and show that the margin will approach at least half the true margin (assuming that the training data are linearly separable). As in many studies using perceptrons, we average the weight vector over the whole training iterations at the end of the training (Collins, 2002). 4 Experiments This section presents four sets of experimental results to show how the lookahead process improves the accuracy of history-based models in common NLP tasks. 4.1 Sequence prediction tasks First, we evaluate our framework with three sequence prediction tasks: POS tagging, chunking, and named entity recognition. We compare our method with the CRF model, which is one of the de facto standard machine learning models for such sequence prediction tasks. We trained L1-regularized first-order CRF models using the efficient stochastic gradient descent (SGD)-based training method presente"
W11-0328,N10-1115,0,0.0164406,"search. In that case, the search queue is not necessarily truncated. 244 (Tesauro, 2001; Hoki, 2006) in that the parameters are optimized based on the differences of the feature vectors realized by the correct and incorrect actions. In history-based models, the order of actions is often very important. For example, backward tagging is considerably more accurate than forward tagging in biomedical named entity recognition. Our lookahead method is orthogonal to more elaborate techniques for determining the order of actions such as easy-first tagging/parsing strategies (Tsuruoka and Tsujii, 2005; Elhadad, 2010). We expect that incorporating such elaborate techniques in our framework will lead to improved accuracy, but we leave it for future work. 6 Conclusion We have presented a simple and general framework for incorporating a lookahead process in historybased models and a perceptron-based training algorithm for the framework. We have conducted experiments using standard data sets for POS tagging, chunking, named entity recognition and dependency parsing, and obtained very promising results—the accuracy achieved by the history-based models enhanced with lookahead was as competitive as globally optim"
W11-0328,P10-1110,0,0.0167726,"subsection to assign the POS tags for the development and test data. Unlabeled attachment scores for all words excluding punctuations are reported. The development set is used for tuning the meta parameters, while the test set is used for evaluating the final accuracy. The parsing algorithm is the “arc-standard” method (Nivre, 2004), which is briefly described in Section 2. With this algorithm, state S corresponds to a parser configuration, i.e., the stack and the queue, and action a corresponds to shift, reduceL , and reduceR . In this experiment, we use the same set of feature templates as Huang and Sagae (2010). Table 4 shows training time, test time, and parsing accuracy. In this table, “No lookahead (depth = 0)” corresponds to a conventional shift-reduce parsing method without any lookahead search. The results 3 Penn2Malt is applied for this conversion, while dependency labels are removed. CRF (L1 regularization & SGD training) No lookahead (depth = 0) Lookahead (depth = 1) Lookahead (depth = 2) No lookahead (depth = 0) + tag trigram features Lookahead (depth = 1) + tag trigram features Lookahead (depth = 2) + tag trigram features Structured perceptron (Collins, 2002) Guided learning (Shen et al.,"
W11-0328,D09-1127,0,0.0172615,"Missing"
W11-0328,W04-1213,0,0.0354731,"Missing"
W11-0328,N01-1025,0,0.0492284,"l., 2007; Lavergne et al., 2010). The second set of experiments is about chunking. We used the data set for the CoNLL 2000 shared task, which contains 8,936 sentences where each token is annotated with the “IOB” tags representing text chunks. The experimental results are shown in Table 2. Again, our history-based models with lookahead were slightly more accurate than the CRF model using exactly the same set of features. The accuracy achieved by the lookahead model with a search depth of 2 was comparable to the accuracy achieved by a computationally heavy combination of max-margin classifiers (Kudo and Matsumoto, 2001). We also tested the effectiveness of additional features of tag trigrams using the development data, but there was no improvement in the accuracy. The third set of experiments is about named entity recognition. We used the data provided for the BioNLP/NLPBA 2004 shared task (Kim et al., 242 2004), which contains 18,546 sentences where each token is annotated with the “IOB” tags representing biomedical named entities. We performed the tagging in the right-to-left fashion because it is known that backward tagging is more accurate than forward tagging on this data set (Yoshida and Tsujii, 2007)."
W11-0328,P10-1052,0,0.0421723,"Missing"
W11-0328,W04-2407,0,0.00845323,"ed on partof-speech tagging, chunking, named entity recognition and dependency parsing, using standard data sets and features. Experimental results demonstrate that history-based models with lookahead are as competitive as globally optimized models including conditional random fields (CRFs) and structured perceptrons. 1 Introduction History-based models have been a popular approach in a variety of natural language processing (NLP) tasks including part-of-speech (POS) tagging, named entity recognition, and syntactic parsing (Ratnaparkhi, 1996; McCallum et al., 2000; Yamada and Matsumoto, 2003; Nivre et al., 2004). The idea is to decompose the complex structured prediction problem into a series of simple classification problems and use a machine learning-based classifier to make each decision using the information about the past decisions and partially completed structures as features. In this paper, we argue that history-based models are not something that should be left behind in research history, by demonstrating that their accuracy can be significantly improved by incorporating a lookahead mechanism into their decisionmaking process. It should be emphasized that we use the word “lookahead” differen"
W11-0328,W04-0308,0,0.0142111,"ead rules of Yamada and Matsumoto (2003).3 The data is split into training (section 02-21), development (section 22), and test (section 23) sets. The parsing accuracy was evaluated with auto-POS data, i.e., we used our lookahead POS tagger (depth = 2) presented in the previous subsection to assign the POS tags for the development and test data. Unlabeled attachment scores for all words excluding punctuations are reported. The development set is used for tuning the meta parameters, while the test set is used for evaluating the final accuracy. The parsing algorithm is the “arc-standard” method (Nivre, 2004), which is briefly described in Section 2. With this algorithm, state S corresponds to a parser configuration, i.e., the stack and the queue, and action a corresponds to shift, reduceL , and reduceR . In this experiment, we use the same set of feature templates as Huang and Sagae (2010). Table 4 shows training time, test time, and parsing accuracy. In this table, “No lookahead (depth = 0)” corresponds to a conventional shift-reduce parsing method without any lookahead search. The results 3 Penn2Malt is applied for this conversion, while dependency labels are removed. CRF (L1 regularization & S"
W11-0328,P06-1059,1,0.636231,"Missing"
W11-0328,W96-0213,0,0.382157,"and show its convergence properties. The proposed framework is evaluated on partof-speech tagging, chunking, named entity recognition and dependency parsing, using standard data sets and features. Experimental results demonstrate that history-based models with lookahead are as competitive as globally optimized models including conditional random fields (CRFs) and structured perceptrons. 1 Introduction History-based models have been a popular approach in a variety of natural language processing (NLP) tasks including part-of-speech (POS) tagging, named entity recognition, and syntactic parsing (Ratnaparkhi, 1996; McCallum et al., 2000; Yamada and Matsumoto, 2003; Nivre et al., 2004). The idea is to decompose the complex structured prediction problem into a series of simple classification problems and use a machine learning-based classifier to make each decision using the information about the past decisions and partially completed structures as features. In this paper, we argue that history-based models are not something that should be left behind in research history, by demonstrating that their accuracy can be significantly improved by incorporating a lookahead mechanism into their decisionmaking pr"
W11-0328,P07-1096,0,0.0216958,"e experimental results are shown in Table 1. Note that the models in the top four rows use exactly the same feature set. It is clearly seen that the lookahead improves tagging accuracy, and our historybased models with lookahead is as accurate as the CRF model. We also created another set of models by simply adding tag trigram features, which cannot be employed by first-order CRF models. These features have slightly improved the tagging accuracy, and the final accuracy achieved by a search depth of 3 was comparable to some of the best results achieved by pure supervised learning in this task (Shen et al., 2007; Lavergne et al., 2010). The second set of experiments is about chunking. We used the data set for the CoNLL 2000 shared task, which contains 8,936 sentences where each token is annotated with the “IOB” tags representing text chunks. The experimental results are shown in Table 2. Again, our history-based models with lookahead were slightly more accurate than the CRF model using exactly the same set of features. The accuracy achieved by the lookahead model with a search depth of 2 was comparable to the accuracy achieved by a computationally heavy combination of max-margin classifiers (Kudo and"
W11-0328,H05-1059,1,0.67598,"ch strategies such as beam search. In that case, the search queue is not necessarily truncated. 244 (Tesauro, 2001; Hoki, 2006) in that the parameters are optimized based on the differences of the feature vectors realized by the correct and incorrect actions. In history-based models, the order of actions is often very important. For example, backward tagging is considerably more accurate than forward tagging in biomedical named entity recognition. Our lookahead method is orthogonal to more elaborate techniques for determining the order of actions such as easy-first tagging/parsing strategies (Tsuruoka and Tsujii, 2005; Elhadad, 2010). We expect that incorporating such elaborate techniques in our framework will lead to improved accuracy, but we leave it for future work. 6 Conclusion We have presented a simple and general framework for incorporating a lookahead process in historybased models and a perceptron-based training algorithm for the framework. We have conducted experiments using standard data sets for POS tagging, chunking, named entity recognition and dependency parsing, and obtained very promising results—the accuracy achieved by the history-based models enhanced with lookahead was as competitive a"
W11-0328,P09-1054,1,0.841931,"xperiments This section presents four sets of experimental results to show how the lookahead process improves the accuracy of history-based models in common NLP tasks. 4.1 Sequence prediction tasks First, we evaluate our framework with three sequence prediction tasks: POS tagging, chunking, and named entity recognition. We compare our method with the CRF model, which is one of the de facto standard machine learning models for such sequence prediction tasks. We trained L1-regularized first-order CRF models using the efficient stochastic gradient descent (SGD)-based training method presented in Tsuruoka et al. (2009). Since our main interest is not in achieving the state-of-the-art results for those tasks, we did not conduct feature engineering to come up with elaborate features—we simply adopted the feature sets described in their paper (with an exception being tag trigram features tested in the POS tagging experiments). The experiments for these sequence prediction tasks were carried out using one core of a 3.33GHz Intel Xeon W5590 processor. The first set of experiments is about POS tagging. The training and test data were created from the Wall Street Journal corpus of the Penn Treebank (Marcus et al.,"
W11-0328,W03-3023,0,0.114016,"roposed framework is evaluated on partof-speech tagging, chunking, named entity recognition and dependency parsing, using standard data sets and features. Experimental results demonstrate that history-based models with lookahead are as competitive as globally optimized models including conditional random fields (CRFs) and structured perceptrons. 1 Introduction History-based models have been a popular approach in a variety of natural language processing (NLP) tasks including part-of-speech (POS) tagging, named entity recognition, and syntactic parsing (Ratnaparkhi, 1996; McCallum et al., 2000; Yamada and Matsumoto, 2003; Nivre et al., 2004). The idea is to decompose the complex structured prediction problem into a series of simple classification problems and use a machine learning-based classifier to make each decision using the information about the past decisions and partially completed structures as features. In this paper, we argue that history-based models are not something that should be left behind in research history, by demonstrating that their accuracy can be significantly improved by incorporating a lookahead mechanism into their decisionmaking process. It should be emphasized that we use the word"
W11-0328,W07-1033,0,0.0609087,"(Kudo and Matsumoto, 2001). We also tested the effectiveness of additional features of tag trigrams using the development data, but there was no improvement in the accuracy. The third set of experiments is about named entity recognition. We used the data provided for the BioNLP/NLPBA 2004 shared task (Kim et al., 242 2004), which contains 18,546 sentences where each token is annotated with the “IOB” tags representing biomedical named entities. We performed the tagging in the right-to-left fashion because it is known that backward tagging is more accurate than forward tagging on this data set (Yoshida and Tsujii, 2007). Table 3 shows the experimental results, together with some previous performance reports achieved by pure machine leaning methods (i.e. without rulebased post processing or external resources such as gazetteers). Our history-based model with no lookahead was considerably worse than the CRF model using the same set of features, but it was significantly improved by the introduction of lookahead and resulted in accuracy figures better than that of the CRF model. 4.2 Dependency parsing We also evaluate our method in dependency parsing. We follow the most standard experimental setting for English"
W11-0328,D08-1059,0,0.029985,"Missing"
W11-0328,J93-2004,0,\N,Missing
W11-0407,W97-1502,0,0.0760306,"parser that converts the human annotations automatically into a richly annotated HPSG treebank. In order to check the proposed scheme’s effectiveness, we performed automatic pseudo-annotations that emulate the system’s idealized behavior and measured the performance of the parser trained on those annotations. In addition, we implemented a prototype system and conducted manual annotation experiments on a small test set. There has been a number of research projects to efficiently develop richly annotated corpora with the help of parsers, one of which is called a discriminant-based treebanking (Carter, 1997). In discriminant-based treebanking, the annotation process consists of two steps: a parser first generates the parse trees, which are annotation candidates, and then a human annotator selects the most plausible one. One of the most important characteristics of this methodology is to use easily-understandable questions called discriminants for picking up the final annotation results. Human annotators can perform annotations simply by answering those questions without closely examining the whole tree. Although this approach has been successful in breaking down the difficult annotations into a s"
W11-0407,W07-2202,1,0.843251,"tic dependency. In both of S-full and SL-full, the improvement from the baseline is significant. Especially, SL-full for “in-chart” data has almost complete agreement with the gold-standard HPSG annotations. The detailed figures are shown in Table 4. Therefore, we can therefore conclude that high quality CFG annotations lead to high quality HPSG annotations when the are combined with a good statistical HPSG parser. 4.2 Domain Adaptation We evaluated the parser accuracy adapted with the automatically created treebank on the Brown Corpus. In this experiment, we used the adaptation algorithm by (Hara et al., 2007), with the same hyperparameters used there. Table 5 shows the result of the adapted parser. Each line of this table stands for the parser adapted with different data. “Gold” is the result adapted on the gold-standard annotations, and “Gold (only covered)” is that adapted on the gold data which is covered by the original Enju HPSG grammar that was extracted from the WSJ portion of the Penn Treebank. “SL-full” is the result adapted on our automatically created data. “Baseline” is the result by the original Enju parser, which is trained only on the WSJ-PTB and whose grammar was extracted from the"
W11-0407,P10-2013,0,0.0218851,"or an unavailable link due to the death of the source edge. tion is that the stochastic model of the HPSG parser properly resolves the remaining ambiguities in the HPSG annotation within the constraints given by a part of the CFG trees. In order to check the validity of this expectation and to measure to what extent the CFG-based annotations can achieve correct HPSG annotations, we performed a pseudo-annotation experiment. In this experiment, we used bracketed sentences in the Brown Corpus (Kuˇcera and Francis, 1967), and a court transcript portion of the Manually Annotated Sub-Corpus (MASC) (Ide et al., 2010). We automatically created HPSG annotations that mimic the annotation results by an ideal annotator in the following four steps. First, HPSG treebanks for these sentences are created by the treebank conversion program distributed with the Enju parser. This program converts a syntactic tree annotated by Penn Treebank style into an HPSG tree. Since this program cannot convert the sentences that are not covered by the basic design of the grammar, we used only those that are successfully converted by the program throughout the experiments and considered this converted treebank as the gold-standard"
W11-0407,H94-1020,0,0.066519,"ottleneck to reduce the cost of annotator training and can restrict the size of annotations. Introduction On the basis of the success of the research on the corpus-based development in NLP, the demand for a variety of corpora has increased, for use as both a training resource and an evaluation data-set. However, the development of a richly annotated corpus such as an HPSG treebank is not an easy task, since the traditional two-step annotation, in which a parser first generates the candidates and then an annotator checks each candidate, needs intensive efforts even for well-trained annotators (Marcus et al., 1994; Kurohashi and Nagao, 1998). Among many NLP problems, adapting a parser for out-domain texts, which is usually referred to as domain adaptation problem, is one of the most remarkable problems. The main cause of this problem is the lack of corpora in that domain. Because it is difficult to prepare a sufficient corpus for each domain without Interactive predictive parsing (S´anchez-S´aez et al., 2009; S´anchez-S´aez et al., 2010) is another approach of annotations, which focuses on CFG trees. In this system, an annotator revises the currently proposed CFG tree until he or she gets the correct t"
W11-0407,W07-2208,1,0.841369,"to each word, and then, the lexical signs for “Dogs” and “run” are combined by SubjectHead schema. In this way, lexical signs and phrasal signs are combined until the whole sentence becomes one sign. Compared to Context Free Grammar (CFG), since each sign of HPSG has rich information about the phrase, such as subcategorization frame or predicate-argument structure, a corpus annotated in an HPSG manner is more difficult to build than CFG corpus. In our system, we aim at building HPSG treebanks with low-cost in which even nonexperts can perform annotations. 2.2 HPSG Deep Parser The Enju parser (Ninomiya et al., 2007) is a statistical deep parser based on the HPSG formalism. It produces an analysis of a sentence that includes the 57 2 3 HEAD noun 6 7 &lt;> 5 4SUBJ COMPS &lt;> Dogs 2 3 HEAD verb 6 7 &lt; noun >5 4SUBJ COMPS &lt;> Drung ⇓ 2 3 HEAD verb 6 7 &lt;> 5 4SUBJ COMPS &lt;> 2 Subject 3 HEAD noun 6 7 &lt;> 5 1 4SUBJ COMPS &lt;> Headj 3 HEAD verb 6 7 6SUBJ &lt; 1 >7 4 5 COMPS &lt;> 2 Figure 1: Example of HPSG parsing for “Dogs run.” syntactic structure (i.e., parse tree) and the semantic structure represented as a set of predicate-argument dependencies. The grammar design is based on the standard HPSG analysis of English (Pollard a"
W11-0407,W09-3835,0,0.0520196,"Missing"
W11-0407,N10-2010,0,0.046945,"Missing"
W11-0407,C10-2166,0,0.0201854,"3, and annotated 200 sentences in total on the system. Half of the sentences were taken from the Brown corpus and the other half were taken from a court-debate section of the MASC corpus. All of the sentences were annotated twice by two annotators. Both of the annotators has background in computer science and linguistics. Table 6 shows the statistics of the annotation procedures. This table indicates that human annotators strongly prefer “S” operation to others, and that the manual annotation on the prototype system is at least comparable to the recent discriminant-based annotation system by (Zhang and Kordoni, 2010), although the comparison is not strict because of the difference of the text. Table 7 shows the automatic evaluation results. We can see that the interactive annotation gave slight improvements in all accuracy metrics. The improvements were however not as much as we desired. By classifying the remaining errors in the annotation results, we identified several classes of major errors: 1. Truly ambiguous structures, which require the context or world-knowledge to correctly resolve them. Brown (train.) MASC in 10,576 / 10,394 864 / 857 out 7,190 / 6,464 489 / 449 in+out 17,766 / 16,858 1,353 / 1,"
W11-2907,W09-1201,0,0.0278561,"Missing"
W11-2907,N04-1013,0,0.190045,"Missing"
W11-2907,W09-1206,0,0.0202207,"nglish. Instead of training a parser based on the obtained LFG resources, Guo used an external PCFG parser to create cstructure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). Besides of Guo’s work, some researchers worked on joint dependency parsing and semantic role labeling to fulfill Chinese deep parsing (Li et al., 2010; Morante et al., 2009; Gesmundo et al., 2009; Dai et al., 2009; Lluis et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into LFG resources. Moreover, Tse and Curran (2010) built a Chinese CCGbank, which was also automatically induced from the Penn Chinese Treebank. (the person who wrote the book) Figure 12: The syntac"
W11-2907,J94-4001,0,0.317999,"Missing"
W11-2907,P04-1014,0,0.027432,"Missing"
W11-2907,P03-1056,0,0.547035,"arser can be explained in the following way: Given a segmented and pos-tagged input sentence, (1) the supertagger offers all the maybeparsable supertag (i.e. lexical template) sequences with scores to the parser; (2) the feature forest model applies beam threshold on the scored supertag sequences, and then obtains a well-formed HPSG parse tree. 吃/eat 过 了 。 ((Somebody) has eaten (something).) (b) A Chinese sentence with both subject and object pro-drop Figure 1: Examples of pro-drop in Chinese The other significant linguistic property in Chinese is the frequent pro-drop phenomena. For example, Levy and Manning (2003) showed that unlike English, the subject pro-drop (the null realization of uncontrolled pronominal subjects) is widespread in Chinese; this is exemplified in Figure 1 (a). Huang (1989) further provided a detailed analysis to show that subjects as well as objects may drop from finite Chinese sentences (as shown in Figure 1 (b)). 3 3.1 Figure 2 shows a supertag sequence provided by the supertagger for a Chinese sentence, in which the supertag of the word ‘写(wrote)’ indicates a lexical template for the transitive verb with an extracted object. Figure 3 illustrates the HPSG parse tree output from"
W11-2907,W09-1202,0,0.0187412,"ese Treebank. This is the only previous work that had been conducted on Chinese deep parsing based on lexicalized grammars, although many related works had been done on English. Instead of training a parser based on the obtained LFG resources, Guo used an external PCFG parser to create cstructure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). Besides of Guo’s work, some researchers worked on joint dependency parsing and semantic role labeling to fulfill Chinese deep parsing (Li et al., 2010; Morante et al., 2009; Gesmundo et al., 2009; Dai et al., 2009; Lluis et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into LFG resources. Mo"
W11-2907,W09-1212,0,0.0170433,"is the only previous work that had been conducted on Chinese deep parsing based on lexicalized grammars, although many related works had been done on English. Instead of training a parser based on the obtained LFG resources, Guo used an external PCFG parser to create cstructure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). Besides of Guo’s work, some researchers worked on joint dependency parsing and semantic role labeling to fulfill Chinese deep parsing (Li et al., 2010; Morante et al., 2009; Gesmundo et al., 2009; Dai et al., 2009; Lluis et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into LFG resources. Moreover, Tse and Curra"
W11-2907,2007.tmi-papers.10,0,0.0165638,"hough many related works had been done on English. Instead of training a parser based on the obtained LFG resources, Guo used an external PCFG parser to create cstructure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). Besides of Guo’s work, some researchers worked on joint dependency parsing and semantic role labeling to fulfill Chinese deep parsing (Li et al., 2010; Morante et al., 2009; Gesmundo et al., 2009; Dai et al., 2009; Lluis et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into LFG resources. Moreover, Tse and Curran (2010) built a Chinese CCGbank, which was also automatically induced from the Penn Chinese Treebank. (the"
W11-2907,W06-2932,0,0.08923,"Missing"
W11-2907,W09-1213,0,0.0213047,"ng a parser based on the obtained LFG resources, Guo used an external PCFG parser to create cstructure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). Besides of Guo’s work, some researchers worked on joint dependency parsing and semantic role labeling to fulfill Chinese deep parsing (Li et al., 2010; Morante et al., 2009; Gesmundo et al., 2009; Dai et al., 2009; Lluis et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into LFG resources. Moreover, Tse and Curran (2010) built a Chinese CCGbank, which was also automatically induced from the Penn Chinese Treebank. (the person who wrote the book) Figure 12: The syntactic dependency tree correspo"
W11-2907,N04-1032,0,0.0174061,"works had been done on English. Instead of training a parser based on the obtained LFG resources, Guo used an external PCFG parser to create cstructure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). Besides of Guo’s work, some researchers worked on joint dependency parsing and semantic role labeling to fulfill Chinese deep parsing (Li et al., 2010; Morante et al., 2009; Gesmundo et al., 2009; Dai et al., 2009; Lluis et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into LFG resources. Moreover, Tse and Curran (2010) built a Chinese CCGbank, which was also automatically induced from the Penn Chinese Treebank. (the person who wrote the bo"
W11-2907,C10-1122,0,0.16879,"et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into LFG resources. Moreover, Tse and Curran (2010) built a Chinese CCGbank, which was also automatically induced from the Penn Chinese Treebank. (the person who wrote the book) Figure 12: The syntactic dependency tree corresponding to Figure 10 (the reason that someone wrote the book) Figure 13: The syntactic dependency tree corresponding to Figure 11 However, since the syntactic analysis does not consider predicate-argument dependencies, such an ambiguity in semantic parsing does not exist in syntactic parsing. For instance, for both the two semantic analyses listed in Figure 10 and Figure 11, the syntactic dependencies are similar, as shown"
W11-2907,J08-1002,1,0.878911,"sentences (as shown in Figure 1 (b)). 3 3.1 Figure 2 shows a supertag sequence provided by the supertagger for a Chinese sentence, in which the supertag of the word ‘写(wrote)’ indicates a lexical template for the transitive verb with an extracted object. Figure 3 illustrates the HPSG parse tree output from the parser with this supertag sequence. Chinese Deep Parsing based on HPSG Parsing Model In this paper, we used an HPSG parser - Enju1, which was successfully applied in English deep parsing, to obtain the deep analysis of Chinese. This HPSG parser uses the feature forest model proposed by Miyao and Tsujii (2008), which is a maximum entropy model that is defined over feature forests, as a parsing disambiguation model. The feature forest model provides a solution to the problem of probabilistic modeling of complex data structures. Moreover, in order to reduce the search space and further increase the parsing efficiency, in this parser, a supertagger (Matsuzaki et al. 2007) is applied before parsing. This supertager provides the maybe-parsable supertag (i.e. lexical template) sequences to the parser. In short, in the HPSG parser, the probability, p(t|w), of producing a parse tree t for a given sentence"
W11-2907,W09-1219,0,0.0532946,"Missing"
W11-2907,W09-1205,0,0.0193571,"uced from the Penn Chinese Treebank. This is the only previous work that had been conducted on Chinese deep parsing based on lexicalized grammars, although many related works had been done on English. Instead of training a parser based on the obtained LFG resources, Guo used an external PCFG parser to create cstructure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). Besides of Guo’s work, some researchers worked on joint dependency parsing and semantic role labeling to fulfill Chinese deep parsing (Li et al., 2010; Morante et al., 2009; Gesmundo et al., 2009; Dai et al., 2009; Lluis et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into"
W11-2907,C10-2162,1,0.789144,"esearchers worked on joint dependency parsing and semantic role labeling to fulfill Chinese deep parsing (Li et al., 2010; Morante et al., 2009; Gesmundo et al., 2009; Dai et al., 2009; Lluis et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into LFG resources. Moreover, Tse and Curran (2010) built a Chinese CCGbank, which was also automatically induced from the Penn Chinese Treebank. (the person who wrote the book) Figure 12: The syntactic dependency tree corresponding to Figure 10 (the reason that someone wrote the book) Figure 13: The syntactic dependency tree corresponding to Figure 11 However, since the syntactic analysis does not consider predicate-argument dependencies, such an am"
W11-2907,W09-1208,0,0.0161437,"ained LFG resources, Guo used an external PCFG parser to create cstructure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). Besides of Guo’s work, some researchers worked on joint dependency parsing and semantic role labeling to fulfill Chinese deep parsing (Li et al., 2010; Morante et al., 2009; Gesmundo et al., 2009; Dai et al., 2009; Lluis et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into LFG resources. Moreover, Tse and Curran (2010) built a Chinese CCGbank, which was also automatically induced from the Penn Chinese Treebank. (the person who wrote the book) Figure 12: The syntactic dependency tree corresponding to Figure 10 ("
W11-2907,J93-2004,0,\N,Missing
W11-2907,J08-2001,0,\N,Missing
W11-2907,J05-3003,0,\N,Missing
W11-2907,P10-1113,0,\N,Missing
W11-2907,D07-1096,0,\N,Missing
W12-5005,dinh-etal-2008-word,0,0.239988,"Missing"
W12-5005,W07-0723,0,0.0123915,"ic, stock, entertainment, education, and fashion are used. The sizes of the training and test data sets are summarized in Table 8. Topic Music Stock Entertainment Education Fashion Total Training (documents) 900 382 825 821 412 3340 Test (documents) 813 320 707 707 302 2849 Table 8: Data used in the text classification experiment 4.2.3 Statistical machine translation (SMT) A phrase-based SMT system for English-Vietnamese translation was implemented. In this system, we used SRILM (Stolcke, 2002) to build the language model, GIZA++ (Och and Ney, 2003) to train the word-aligned model, and Moses (Holmqvist et al., 2007) to train the phrase-based statistical translation model. Translation results are evaluated using the word-based BLEU score (Papineni et al., 2002). Both training and test data are word-segmented using the word segmentation models achieved. For the experiment, we used the VCL_EVC bilingual corpus (Dinh and Hoang, 2005), 18000 pairs of sentences for training, and 1000 pairs for testing. 4.3 Experimental results and analysis ORG BASE VAR_COMB VAR_SPLIT VAR_FREQ STRUCT_NC STRUCT_AFFIX Recall 95.89 96.00 96.05 96.53 96.20 95.08 96.03 Precision 95.44 95.60 95.69 96.27 95.85 94.79 95.59 F-score 95.6"
W12-5005,P03-1004,0,0.0338893,"E + combine all classifier nouns detected in Section 3.2 with the words they modify. • STRUCT_AFFIX : BASE + combine all suffixes detected in Section 3.2 with the words they modify. These data sets are used in our experiments as illustrated in Figure 1. The names of the data sets are also used to label our experimental configurations. 4.2 Experimental settings In this section, we briefly describe the task settings and the methods used for word segmentation (WS), text classification (TC), and English-Vietnamese statistical machine translation (SMT). 4.2.1 Word segmentation (WS) We used YamCha (Kudo and Matsumoto, 2003), a multi-purpose chunking tool, to train our word segmentation models. The core of YamCha is the Support Vector Machine (SVM) machine learning method, which has been proven to be effective for NLP tasks. For the Vietnamese word segmentation problem, each token is labeled with standard B, I, or O labels, corresponding to the beginning, inside, and outside positions, respectively. The label of each token is determined based on the lexical features of two preceding words, and the two following words of that token. Since the Vietnamese language is not inflectional, we cannot utilize inflection fe"
W12-5005,Y06-1028,0,0.389481,"Missing"
W12-5005,W09-3425,0,0.0260275,"Missing"
W12-5005,W09-3035,0,0.152062,"e development of computational processing technologies for Vietnamese, a language spoken by over 90 million people, but also for similar languages such as Thai, Laos, and so on. This study also promotes the computational linguistic studies on how to transfer methods developed for a popular language, like English, to a language that has not yet intensively studied. 2 Word segmentation in VTB Word segmentation in VTB aims at establishing a standard for word segmentation in a context of multi-level language processing. VTB specifies 12 types of units that should be identified as words (Table 1) (Nguyen et al., 2009b), which can be divided up into three groups: single, compound, and special “words.” Single words contain only one token. The terminology tokens refers to text spans that are separated from each other by blank spaces. Compound words have two or more tokens, and are divided into four types: compound words composed by semantic coordination (semantic-coordinated compound), compound words composed by semantic subordination (semantic-subordinated compound), compound words with an affix, and reduplicated words. Special “words” include idioms, locutions, proper names, date times, numbers, symbols, s"
W12-5005,J03-1002,0,0.00279743,"fixing other configurations. News articles of five topics: music, stock, entertainment, education, and fashion are used. The sizes of the training and test data sets are summarized in Table 8. Topic Music Stock Entertainment Education Fashion Total Training (documents) 900 382 825 821 412 3340 Test (documents) 813 320 707 707 302 2849 Table 8: Data used in the text classification experiment 4.2.3 Statistical machine translation (SMT) A phrase-based SMT system for English-Vietnamese translation was implemented. In this system, we used SRILM (Stolcke, 2002) to build the language model, GIZA++ (Och and Ney, 2003) to train the word-aligned model, and Moses (Holmqvist et al., 2007) to train the phrase-based statistical translation model. Translation results are evaluated using the word-based BLEU score (Papineni et al., 2002). Both training and test data are word-segmented using the word segmentation models achieved. For the experiment, we used the VCL_EVC bilingual corpus (Dinh and Hoang, 2005), 18000 pairs of sentences for training, and 1000 pairs for testing. 4.3 Experimental results and analysis ORG BASE VAR_COMB VAR_SPLIT VAR_FREQ STRUCT_NC STRUCT_AFFIX Recall 95.89 96.00 96.05 96.53 96.20 95.08 96"
W12-5005,P02-1040,0,0.0837604,"Entertainment Education Fashion Total Training (documents) 900 382 825 821 412 3340 Test (documents) 813 320 707 707 302 2849 Table 8: Data used in the text classification experiment 4.2.3 Statistical machine translation (SMT) A phrase-based SMT system for English-Vietnamese translation was implemented. In this system, we used SRILM (Stolcke, 2002) to build the language model, GIZA++ (Och and Ney, 2003) to train the word-aligned model, and Moses (Holmqvist et al., 2007) to train the phrase-based statistical translation model. Translation results are evaluated using the word-based BLEU score (Papineni et al., 2002). Both training and test data are word-segmented using the word segmentation models achieved. For the experiment, we used the VCL_EVC bilingual corpus (Dinh and Hoang, 2005), 18000 pairs of sentences for training, and 1000 pairs for testing. 4.3 Experimental results and analysis ORG BASE VAR_COMB VAR_SPLIT VAR_FREQ STRUCT_NC STRUCT_AFFIX Recall 95.89 96.00 96.05 96.53 96.20 95.08 96.03 Precision 95.44 95.60 95.69 96.27 95.85 94.79 95.59 F-score 95.66 95.80 95.87 96.40 96.02 94.93 95.81 Table 9: Evaluation results of automatic word segmentation with different WS criteria Evaluation of word segm"
W12-5005,P06-1055,0,0.128906,"Missing"
W13-2303,2010.jeptalnrecital-long.36,0,0.0212206,"ology, Ho Chi Minh City quynt@uit.edu.vn Ngan L.T. Nguyen National Institute of Informatics, Tokyo ngan@nii.ac.jp Abstract Nguyen et al. (2012) proposed methods of improving the annotations of word segmentation (WS) for VTB. They also evaluated different WS criteria in two applications, i.e., machine translation and text classification. This paper focuses on improving the quality of parts-of-speech (POS) annotations by using state-of-the-art parsers to provide feedback for this process. The difficulties with Vietnamese POS tagging have been recognized by many researchers (Nghiem et al., 2008; Le et al., 2010). There is little consensus as to the methodology for classifying words. Polysemous words, words with the same surface form but having different meanings and grammar functions, are very popular in the Vietnamese language. For example, the word “cổ” can be a noun that means neck/she, or an adjective that means ancient depending on the context. This characteristic makes it difficult to tag POSs for Vietnamese, both manually and automatically. The rest of this paper is organized as follows: a brief introduction to VTB and its annotation schemes are provided in Section 2. Then, previous work is su"
W13-2303,W12-5005,1,0.739443,"d positions of noun phrases by investigating the VTB corpus. There is currently little consensus as to the methodology for annotating Nc-nouns (Hoang, 1998; Nguyen et al., 2010b; Nguyen et al., 2010a). Table 1: VTB part-of-speech tag set characteristics. 2 Brief introduction to VTB 3 The VTB corpus contains 10.433 sentences (274.266 tokens), semi-manually annotated with three layers of WS, POS tagging, and bracketing. The first annotation is produced for each annotation layer by using automatic tools. Then, the annotators revise these data. The WS and POS annotation schemes were introduced by Nguyen et al. (2012). This section briefly introduces POS tag set and a bracketing annotation scheme. VTB specifies the 18 different POS tags summarized in Table 1 (Nguyen et al., 2010a). Each unit in this table goes with several example words. English translations of these words are included in braces. However, as we could not find any appropriate English translations for some words, these empty translations have been denoted by asterisks (*). The VTB corpus is annotated with three syntactic tag types: constituency tags, functional tags, and null-element tags. There are 18 constituency tags in VTB. The functiona"
W13-2303,W07-2416,0,0.0757466,"Missing"
W13-2303,E06-1011,0,0.046185,"Missing"
W13-2303,W06-2932,0,0.0371907,"Missing"
W13-2303,P06-1055,0,0.16912,"Missing"
W13-2303,W09-3035,0,0.191958,"ging influenced the quality of Vietnamese parsing by analyzing the parsing results. were due to four main reasons: (1) The POS of a polysemous word changes according to the function of that polysemous word in each phrase category or changes according to the meaning of surrounding words. Although polysemous words are annotated with different POS tags, they do not change their word form. (2) The way polysemous words are tagged according to their context is not completely clear in the POS tagging guidelines. (3) Annotators referred to a dictionary that had been built as part of the VLSP project (Nguyen et al., 2009) (VLSP dictionary) to annotate the VTB corpus. However, this dictionary lacked various words and did not cover all contexts for the words. For example, “hơn {more than}” in Vietnamese is an adjective when it is the head word of an adjectival phrase, but “hơn {over}” is an adverb when it is the modifier of a quantifier noun (such as “hơn 200 sinh viên {over 200 students}”). However, the VLSP dictionary only considered “hơn” to be an adjective (“tôi hơn nó hai tuổi {I am more than him two years old}”). No cases where “hơn” was an adverb were mentioned in this dictionary. (4) There are several ov"
W13-2318,E12-2021,0,0.139895,"Missing"
W13-2318,D09-1155,0,0.0572349,"tionships between a technique and its applications (Gupta and Manning, 2011). Answers to this query can be found in various forms in published papers, for example, (1) CRF-based POS tagging has achieved state-ofthe-art accuracy. (2) CRFs have been successfully applied to sequence labeling problems including POS tagging and named entity recognition. 140 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 140–148, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics al., 2012), and chemistry and computational linguistics (Teufel et al., 2009). Zoning provides a sentence-based information structure of papers to help identify the components such as the proposed method and the results obtained in the study. As such, zoning can narrow down the sections of a paper in which the answer to a query can be found. However, zoning alone cannot always capture the relation between the concepts described in the sections as it focuses on relation at a sentence level. For example, the examples (1), (2), (3) in the previous section require intra-sentence analysis to capture the relation between CRF and POS tagging. Our annotation scheme, which can"
W13-2318,brants-2000-inter,0,0.553416,"Missing"
W13-2318,varga-etal-2012-unsupervised,0,0.398072,"Missing"
W13-2318,D11-1025,0,0.0716884,"resented in this study can be viewed as a starting point for research focusing on representative schemata of human activities. 2 Related Work Traditionally, research on searching research papers has focused more on the social aspects of papers and their authors, such as citation links and co-authorship analysis implemented in the aforementioned services. Recently, research on content-based analysis of research papers has been emerging. For example, methods of document zoning have been proposed for research papers in biomedicine (Mizuta et al., 2006; Agarwal and Yu, 2009; Liakata et al., 2010; Guo et al., 2011; Varga et 141 In this paper, we propose a novel strategy for parallel preconditioning of large scale linear systems by means of a two-level approximate inverse technique with AISM method. According to the numerical results on an origin 2400 by using MPI, the proposed parallel technique of computing the approximate inverse makes the speedup of about 136.72 times with 16 processors. mation that we also want to capture, such as how the author evaluates current systems and methods or the previous efforts of others. An attempt to identify the evaluation and other meta-aspects of scientific papers"
W13-2318,I11-1001,0,0.200657,"nce proceedings such as the ACL Anthology. These services focus on simple keywordbased searches as well as extralinguistic relations among research papers, authors, and research topics. However, because contemporary research is becoming increasingly complicated and interrelated, intelligent content-based search systems are desired (Banchs, 2012). A typical query in computational linguistics could be what tasks have CRFs been used for?, which includes the elements of a typical schema for searching research papers; researchers want to find relationships between a technique and its applications (Gupta and Manning, 2011). Answers to this query can be found in various forms in published papers, for example, (1) CRF-based POS tagging has achieved state-ofthe-art accuracy. (2) CRFs have been successfully applied to sequence labeling problems including POS tagging and named entity recognition. 140 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 140–148, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics al., 2012), and chemistry and computational linguistics (Teufel et al., 2009). Zoning provides a sentence-based information structure"
W13-2318,W11-1801,0,0.0439783,"Missing"
W13-2318,liakata-etal-2010-corpora,0,0.121121,"Thus, the framework presented in this study can be viewed as a starting point for research focusing on representative schemata of human activities. 2 Related Work Traditionally, research on searching research papers has focused more on the social aspects of papers and their authors, such as citation links and co-authorship analysis implemented in the aforementioned services. Recently, research on content-based analysis of research papers has been emerging. For example, methods of document zoning have been proposed for research papers in biomedicine (Mizuta et al., 2006; Agarwal and Yu, 2009; Liakata et al., 2010; Guo et al., 2011; Varga et 141 In this paper, we propose a novel strategy for parallel preconditioning of large scale linear systems by means of a two-level approximate inverse technique with AISM method. According to the numerical results on an origin 2400 by using MPI, the proposed parallel technique of computing the approximate inverse makes the speedup of about 136.72 times with 16 processors. mation that we also want to capture, such as how the author evaluates current systems and methods or the previous efforts of others. An attempt to identify the evaluation and other meta-aspects of"
W13-2318,P11-4002,0,0.0949154,"Missing"
W13-2318,W12-3200,0,\N,Missing
W13-2806,P07-1091,0,0.0205674,"reordered, but their positions will be decided relative to their verb. In what follows, we describe in detail how to identify verbal blocks, their objects and the invariable grammatical particles that will play a role in our reordering method. As mentioned earlier, the only information that will be used to perform this task will be the POS tags of the words and their unlabeled dependency structures. Related Work Although there are many works on pre-reordering methods for other languages to English translation or inverse (Xia and McCord, 2004; Xu et al., 2009; Habash, 2007; Wang et al., 2007; Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifically, HPSG parsers (Miyao and Tsuji"
W13-2806,W08-0336,0,0.0552701,"Missing"
W13-2806,J08-1002,1,0.869913,"Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifically, HPSG parsers (Miyao and Tsujii, 2008; Yu et al., 2011) are used to extract the structure of sentences in the form of binary trees, and head branches are swapped with their dependents according to certain heuristics to resemble the word order of the target language. However, those strategies are sensitive to parsing errors, and the binary structure of their parse trees impose hard constraints in sentences with loose word order. Moreover, as Han et al. (2012) noted, reordering strategies that are derived from the HPSG theory may not perform well when the head definition is inconsistent in the language pair under study. A typical e"
W13-2806,W09-2307,0,0.0211337,"d sentences using a phrasebased SMT system. However, Chinese parsers 25 Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 25–33, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics limited the extensibility of their method. Our approach follows the idea of using dependency tree structures and POS tags, but we discard the information on dependency labels since we did not find them informative to guide our reordering strategies in our preliminary experiments, partly due to Chinese showing less dependencies and a larger label variability (Chang et al., 2009). volve either Chinese or Japanese, and explain how our method builds upon them. From a linguistic perspective, we describe in section 3 our observations of reordering issues between Chinese and Japanese and detail how our framework solves those issues. In section 4 we assess to what extent our pre-reordering method succeeds in reordering words in Chinese sentences to resemble the order of Japanese sentences, and measure its impact on translation quality. The last section is dedicated to discuss our findings and point to future directions. 2 3 Methodology In Subject-Verb-Object (SVO) languages"
W13-2806,J03-1002,0,0.00360351,"es are necessary, and linguistic knowledge on structural difference can be encoded in the form of reordering rules. We show significant improvements in translation quality of sentences from news domain, when compared to state-of-the-art reordering methods. 1 Introduction Translation between Chinese and Japanese languages gains interest as their economic and political relationship intensifies. Despite their linguistic influences, these languages have different syntactic structures and phrase-based statistical machine translation (SMT) systems do not perform well. Current word alignment models (Och and Ney, 2003) account for local differences in word order between bilingual sentences, but fail at capturing long distance word alignments. One of the main problems in the search of the best word alignment is the combinatorial explosion of word orders, but linguistically-motivated heuristics can help to guide the search. This work explores syntax-informed prereordering for Chinese; that is, we obtain syntactic structures of Chinese sentences, reorder the words to resemble the Japanese word order, and then translate the reordered sentences using a phrasebased SMT system. However, Chinese parsers 25 Proceedi"
W13-2806,P03-1021,0,0.0334338,"010b), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method. DPC and Refined-HFC pre-reordering strategies were followed in the pipeline by a standard Moses-based baseline system (Koehn et al., 2007), using a default distance reordering model and a lexicalized reordering model “msd-bidirectionalfe”. A 5-gram language model was built using SRILM (Stolcke, 2002) on the target side of the corresponding training corpus. Word alignments were extracted using MGIZA++ (Gao and Vogel, 2008) and the parameters of the log-linear combination were tuned using MERT (Och, 2003). Table 3 summarizes the results of the Baseline system (no pre-reordering nor particle word insertion), the Refined-HFC (Ref-HFC) and our DPC method, using the well-known BLEU score (Papineni et al., 2002) and a word order sensitive metric named RIBES (Isozaki et al., 2010a). 5 Conclusions In the present paper, we have analyzed the differences in word order between Chinese and Japanese sentences. We captured the regularities of ordering differences between Chinese and Japanese sentences, and proposed a framework to reorder Chinese sentences to resemble the word order of Japanese. Our framewor"
W13-2806,W08-0509,0,0.0596627,"nju to obtain HPSG parsing trees. For comparison purposes with the work in (Isozaki et al., 2010b), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method. DPC and Refined-HFC pre-reordering strategies were followed in the pipeline by a standard Moses-based baseline system (Koehn et al., 2007), using a default distance reordering model and a lexicalized reordering model “msd-bidirectionalfe”. A 5-gram language model was built using SRILM (Stolcke, 2002) on the target side of the corresponding training corpus. Word alignments were extracted using MGIZA++ (Gao and Vogel, 2008) and the parameters of the log-linear combination were tuned using MERT (Och, 2003). Table 3 summarizes the results of the Baseline system (no pre-reordering nor particle word insertion), the Refined-HFC (Ref-HFC) and our DPC method, using the well-known BLEU score (Papineni et al., 2002) and a word order sensitive metric named RIBES (Isozaki et al., 2010a). 5 Conclusions In the present paper, we have analyzed the differences in word order between Chinese and Japanese sentences. We captured the regularities of ordering differences between Chinese and Japanese sentences, and proposed a framewor"
W13-2806,P02-1040,0,0.0874926,"ses-based baseline system (Koehn et al., 2007), using a default distance reordering model and a lexicalized reordering model “msd-bidirectionalfe”. A 5-gram language model was built using SRILM (Stolcke, 2002) on the target side of the corresponding training corpus. Word alignments were extracted using MGIZA++ (Gao and Vogel, 2008) and the parameters of the log-linear combination were tuned using MERT (Och, 2003). Table 3 summarizes the results of the Baseline system (no pre-reordering nor particle word insertion), the Refined-HFC (Ref-HFC) and our DPC method, using the well-known BLEU score (Papineni et al., 2002) and a word order sensitive metric named RIBES (Isozaki et al., 2010a). 5 Conclusions In the present paper, we have analyzed the differences in word order between Chinese and Japanese sentences. We captured the regularities of ordering differences between Chinese and Japanese sentences, and proposed a framework to reorder Chinese sentences to resemble the word order of Japanese. Our framework consists in three steps. First, we identify verbal blocks, which consist of Chinese words that will move all together as a block without altering their relative inner order. Second, we identify the right-"
W13-2806,2007.mtsummit-papers.29,0,0.039146,"vicinity of the verb will also be reordered, but their positions will be decided relative to their verb. In what follows, we describe in detail how to identify verbal blocks, their objects and the invariable grammatical particles that will play a role in our reordering method. As mentioned earlier, the only information that will be used to perform this task will be the POS tags of the words and their unlabeled dependency structures. Related Work Although there are many works on pre-reordering methods for other languages to English translation or inverse (Xia and McCord, 2004; Xu et al., 2009; Habash, 2007; Wang et al., 2007; Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifi"
W13-2806,P06-1055,0,0.00949413,"and the combination of both corpora were used for training. sets of parallel sentences, namely an in-housecollected Chinese-Japanese news corpus (News), and the News corpus augmented with the CWMT (Zhao et al., 2011) corpus. We extracted disjoint development and test sets from News corpus, containing 1, 000 and 2, 000 sentences respectively. Table 2 shows the corpora statistics. We used MeCab 4 (Kudo and Matsumoto, 2000) and the Stanford Chinese segmenter 5 (Chang et al., 2008) to segment Japanese and Chinese sentences. POS tags of Chinese sentences were obtained using the Berkeley parser 6 (Petrov et al., 2006), while dependency trees were extracted using Corbit 7 (Hatori et al., 2011). Following the work in (Han et al., 2012), we re-implemented the Refined-HFC using the Chinese Enju to obtain HPSG parsing trees. For comparison purposes with the work in (Isozaki et al., 2010b), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method. DPC and Refined-HFC pre-reordering strategies were followed in the pipeline by a standard Moses-based baseline system (Koehn et al., 2007), using a default distance reordering model and a lexicalized reordering model “msd-bidirectio"
W13-2806,W12-4207,1,0.839206,"saaki}@lab.ntt.co.jp Abstract have difficulties in extracting reliable syntactic information, mainly because Chinese has a loose word order and few syntactic clues such as inflection and function words. On one hand, parsers implementing head-driven phrase structure grammars infer a detailed constituent structure, and such a rich syntactic structure can be exploited to design well informed reordering methods. However, inferring abundant syntactic information often implies introducing errors, and reordering methods that heavily rely on detailed information are sensitive to those parsing errors (Han et al., 2012). On the other hand, dependency parsers are committed to the simpler task of finding dependency relations and dependency labels, which can also be useful to guide reordering (Xu et al., 2009). However, reordering methods that rely on those dependency labels will also be prone to errors, specially in the case of Chinese since it has a richer set of dependency labels when compared to other languages. Since improving parsers for Chinese is challenging, we thus aim at reducing the influence of parsing errors in the reordering procedure. We present a hybrid approach that boosts the performance of p"
W13-2806,I11-1136,1,0.782054,"l sentences, namely an in-housecollected Chinese-Japanese news corpus (News), and the News corpus augmented with the CWMT (Zhao et al., 2011) corpus. We extracted disjoint development and test sets from News corpus, containing 1, 000 and 2, 000 sentences respectively. Table 2 shows the corpora statistics. We used MeCab 4 (Kudo and Matsumoto, 2000) and the Stanford Chinese segmenter 5 (Chang et al., 2008) to segment Japanese and Chinese sentences. POS tags of Chinese sentences were obtained using the Berkeley parser 6 (Petrov et al., 2006), while dependency trees were extracted using Corbit 7 (Hatori et al., 2011). Following the work in (Han et al., 2012), we re-implemented the Refined-HFC using the Chinese Enju to obtain HPSG parsing trees. For comparison purposes with the work in (Isozaki et al., 2010b), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method. DPC and Refined-HFC pre-reordering strategies were followed in the pipeline by a standard Moses-based baseline system (Koehn et al., 2007), using a default distance reordering model and a lexicalized reordering model “msd-bidirectionalfe”. A 5-gram language model was built using SRILM (Stolcke, 2002) on the"
W13-2806,D07-1077,0,0.0192007,"e verb will also be reordered, but their positions will be decided relative to their verb. In what follows, we describe in detail how to identify verbal blocks, their objects and the invariable grammatical particles that will play a role in our reordering method. As mentioned earlier, the only information that will be used to perform this task will be the POS tags of the words and their unlabeled dependency structures. Related Work Although there are many works on pre-reordering methods for other languages to English translation or inverse (Xia and McCord, 2004; Xu et al., 2009; Habash, 2007; Wang et al., 2007; Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifically, HPSG parsers"
W13-2806,D10-1092,1,0.903219,"Missing"
W13-2806,I11-1004,1,0.842316,"heir positions will be decided relative to their verb. In what follows, we describe in detail how to identify verbal blocks, their objects and the invariable grammatical particles that will play a role in our reordering method. As mentioned earlier, the only information that will be used to perform this task will be the POS tags of the words and their unlabeled dependency structures. Related Work Although there are many works on pre-reordering methods for other languages to English translation or inverse (Xia and McCord, 2004; Xu et al., 2009; Habash, 2007; Wang et al., 2007; Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifically, HPSG parsers (Miyao and Tsujii, 2008; Yu et al."
W13-2806,W10-1736,1,0.962597,"their unlabeled dependency structures. Related Work Although there are many works on pre-reordering methods for other languages to English translation or inverse (Xia and McCord, 2004; Xu et al., 2009; Habash, 2007; Wang et al., 2007; Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifically, HPSG parsers (Miyao and Tsujii, 2008; Yu et al., 2011) are used to extract the structure of sentences in the form of binary trees, and head branches are swapped with their dependents according to certain heuristics to resemble the word order of the target language. However, those strategies are sensitive to parsing errors, and the binary structure of their parse trees impose hard constraints in s"
W13-2806,C04-1073,0,0.237416,"grammatical particles in the original vicinity of the verb will also be reordered, but their positions will be decided relative to their verb. In what follows, we describe in detail how to identify verbal blocks, their objects and the invariable grammatical particles that will play a role in our reordering method. As mentioned earlier, the only information that will be used to perform this task will be the POS tags of the words and their unlabeled dependency structures. Related Work Although there are many works on pre-reordering methods for other languages to English translation or inverse (Xia and McCord, 2004; Xu et al., 2009; Habash, 2007; Wang et al., 2007; Li et al., 2007; Wu et al., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based"
W13-2806,P07-2045,0,0.00976568,"Chinese sentences. POS tags of Chinese sentences were obtained using the Berkeley parser 6 (Petrov et al., 2006), while dependency trees were extracted using Corbit 7 (Hatori et al., 2011). Following the work in (Han et al., 2012), we re-implemented the Refined-HFC using the Chinese Enju to obtain HPSG parsing trees. For comparison purposes with the work in (Isozaki et al., 2010b), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method. DPC and Refined-HFC pre-reordering strategies were followed in the pipeline by a standard Moses-based baseline system (Koehn et al., 2007), using a default distance reordering model and a lexicalized reordering model “msd-bidirectionalfe”. A 5-gram language model was built using SRILM (Stolcke, 2002) on the target side of the corresponding training corpus. Word alignments were extracted using MGIZA++ (Gao and Vogel, 2008) and the parameters of the log-linear combination were tuned using MERT (Och, 2003). Table 3 summarizes the results of the Baseline system (no pre-reordering nor particle word insertion), the Refined-HFC (Ref-HFC) and our DPC method, using the well-known BLEU score (Papineni et al., 2002) and a word order sensit"
W13-2806,N09-1028,0,0.465198,"ion words. On one hand, parsers implementing head-driven phrase structure grammars infer a detailed constituent structure, and such a rich syntactic structure can be exploited to design well informed reordering methods. However, inferring abundant syntactic information often implies introducing errors, and reordering methods that heavily rely on detailed information are sensitive to those parsing errors (Han et al., 2012). On the other hand, dependency parsers are committed to the simpler task of finding dependency relations and dependency labels, which can also be useful to guide reordering (Xu et al., 2009). However, reordering methods that rely on those dependency labels will also be prone to errors, specially in the case of Chinese since it has a richer set of dependency labels when compared to other languages. Since improving parsers for Chinese is challenging, we thus aim at reducing the influence of parsing errors in the reordering procedure. We present a hybrid approach that boosts the performance of phrase-based SMT systems by pre-reordering the source language using unlabeled parse trees augmented with constituent information derived from Part-of-Speech tags. Specifically, we propose a f"
W13-2806,W11-2907,1,0.651863,"l., 2011), reordering method for Chineseto-Japanese translation, which is a representative of long distance language pairs, has received little attention. The most related work to ours is in (Han et al., 2012), in which the authors introduced a refined reordering approach by importing an existing reordering method for English proposed in (Isozaki et al., 2010b). These reordering strategies are based on Head-driven phrase structure grammars (HPSG) (Pollard and Sag, 1994), in that the reordering decisions are made based on the head of phrases. Specifically, HPSG parsers (Miyao and Tsujii, 2008; Yu et al., 2011) are used to extract the structure of sentences in the form of binary trees, and head branches are swapped with their dependents according to certain heuristics to resemble the word order of the target language. However, those strategies are sensitive to parsing errors, and the binary structure of their parse trees impose hard constraints in sentences with loose word order. Moreover, as Han et al. (2012) noted, reordering strategies that are derived from the HPSG theory may not perform well when the head definition is inconsistent in the language pair under study. A typical example for the lan"
W13-2806,W00-1303,0,0.708324,"proposed DPC method obtained p-values 0.002 and 0.0, which indicates significant improvements over the phrase-based system. Table 3: Evaluation of translation quality of two test sets when CWMT, News and the combination of both corpora were used for training. sets of parallel sentences, namely an in-housecollected Chinese-Japanese news corpus (News), and the News corpus augmented with the CWMT (Zhao et al., 2011) corpus. We extracted disjoint development and test sets from News corpus, containing 1, 000 and 2, 000 sentences respectively. Table 2 shows the corpora statistics. We used MeCab 4 (Kudo and Matsumoto, 2000) and the Stanford Chinese segmenter 5 (Chang et al., 2008) to segment Japanese and Chinese sentences. POS tags of Chinese sentences were obtained using the Berkeley parser 6 (Petrov et al., 2006), while dependency trees were extracted using Corbit 7 (Hatori et al., 2011). Following the work in (Han et al., 2012), we re-implemented the Refined-HFC using the Chinese Enju to obtain HPSG parsing trees. For comparison purposes with the work in (Isozaki et al., 2010b), particle seed words were inserted at a preprocessing stage for Refined-HFC and our DPC method. DPC and Refined-HFC pre-reordering st"
W13-2806,D08-1076,0,\N,Missing
W13-4403,C02-1126,0,0.0866221,"Missing"
W13-4403,A00-2018,0,0.0149568,"Missing"
W13-4403,D09-1127,0,0.077969,"Missing"
W13-4403,J93-2004,0,0.0424905,"Missing"
W13-4403,P03-1056,0,0.085125,"Missing"
W13-4403,W12-6332,1,0.808052,"Missing"
W13-4403,W09-3825,0,0.0151052,"tion Penn Treebank (PTB) was built based on the idea of context-free PSG (Marcus et al., 1993). It is now a common practice to develop data-driven English parsers using PTB annotation and encouraging performances have been reported (Collins, 2000; Charniak, 2000). Following the success of PTB, Xue et al. 2000 built Penn Chinese Treebank (CTB). CTB is also based on context-free PSG. Since CTB provides training data for Chinese parsing, researchers attempted to train Chinese parsing with CTB (Bikel and Chiang, 2000; Chiang and Bikel, 2002; Levy and Manning, 2003; Bikel, 2004; Wang et al., 2006; Zhang and Clark, 2009; 1a. Students process data 1b. Data processing system 1c. Data was processed 2a. 学生 处理 数据 Student process data Students process data 2b. 数据 处理 系统 Data process system Data processing system 11 Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing (SIGHAN-7), pages 11–19, Nagoya, Japan, 14 October 2013. 2c. 数据 处理 了 Data process le Data was processed 4a. 鸟儿 飞 向 南方 Bird fly towards south Birds fly towards the south 4b. *鸟儿 吃 向 南方 Bird eat towards south Birds eat towards the south 4c. *鸟儿 喜欢 向 南方 Bird like towards south Birds like towards the south 5a. Agent Direction V 5b. Age"
W13-4403,W00-1201,0,\N,Missing
W13-4403,P06-1054,0,\N,Missing
W14-2414,W07-1422,0,0.0787364,"Missing"
W14-2414,P11-1060,0,0.0852828,"Missing"
W14-2414,P14-1008,1,0.780015,"logically sound transformations, but tree transformations can be seen as an approximation of logical inference. For (ii), abstract denotation is more efficient than FOL formula, because abstract denotation eliminates quantifiers and meanings of natural language texts can be represented by atomic sentences. To elaborate the above discussion and to provide more topics to the literature, in this paper we discuss the following four questions: (§2) How well can tree transformation approximate logical inference? (§3) With rigorous inference on DCS trees, where does logic contribute in the system of Tian et al. (2014)? (§4) Does logical inference have further potentials in Recognizing Textual Entailment (RTE) task? and (§5) How efficient is abstract denotation compared to FOL formula? We provide examples or experimental results to the above questions. Dependency-based Compositional Semantics (DCS) provides a precise and expressive way to model semantics of natural language queries on relational databases, by simple dependency-like trees. Recently abstract denotation is proposed to enable generic logical inference on DCS. In this paper, we discuss some other possibilities to equip DCS with logical inference"
W14-2414,N13-1090,0,\N,Missing
W14-5205,andersen-etal-2008-bnc,0,0.0302135,"1, and ... . text sentence indexmark cite note Notice that … . sentence sentence (c) New UI is shown. The UI is more useful than XYZ Cite1, and ... . text indexmark cite note Notice that … . (d) &lt;sentence&gt;&lt;text&gt;New UI&lt;/text&gt; is shown.&lt;/sentence&gt; &lt;sentence&gt;The UI is more useful than XYZ&lt;indexmark&gt; … &lt;/indexmark&gt; in &lt;cite&gt;[…]&lt;/cite&gt;&lt;note&gt;&lt;sentence&gt;Notice that … . &lt;/sentence&gt;&lt;/note&gt;, and … .&lt;/sentence&gt; Figure 2: Example of executing our strategy not explain how the given text can be used in a target annotation process such as parsing. Some projects based on the UIMA framework, such as RASP4UIMA (Andersen et al., 2008), U-compare (Kano et al., 2011), and Kachako (Kano, 2012)2 , have developed systems where the connections between various documents and various tools are already established. Users, however, can utilize only the text and tool pairs that have already been integrated into the systems. GATE (Cunningham et al., 2013) is based on a similar concept to UIMA; it supports XML documents as its input, while the framework also requires integration of tools into the systems. In our framework, although availability of XML documents is assumed, a user can apply NLP tools to the documents without modifying th"
W14-5205,J07-4004,0,0.0136817,"plemented. In Section 4, the efficiency of our framework and the adequacy of the obtained text sequences for use in NLP tools are examined using several types of documents. 2 Related Work To the best of our knowledge, no significant work on a unified methodology for data conversion between target text and the input/output formats of NLP tools has been published. Some NLP tools provide scripts for extracting valid input text for the tools from real-world documents; however, even these scripts assume specific formats for the documents. For example, deep syntactic parsers such as the C&C Parser (Clark and Curran, 2007) and Enju (Ninomiya et al., 2007) assume POS-tagged sentences as input, and therefore the distributed packages for the parsers1 contain POS-taggers together with the parsers. The POS-taggers assume plain text sentences as their input. As the work most relevant to our study, UIMA (Ferruci et al., 2006) deals with various annotations in an integrated framework. However, in this work, the authors merely proposed the framework and did 1 45 / [Enju]: http://kmcs.nii.ac.jp/enju/ [C&C Parser]: http://svn.ask.it.usyd.edu.au/trac/candc/wiki Decoration Meta-info (a) &lt;text&gt;New UI&lt;/text&gt; is shown. The UI"
W14-5205,de-marneffe-etal-2006-generating,0,0.0285424,"Missing"
W14-5205,W07-2208,1,0.753574,"iency of our framework and the adequacy of the obtained text sequences for use in NLP tools are examined using several types of documents. 2 Related Work To the best of our knowledge, no significant work on a unified methodology for data conversion between target text and the input/output formats of NLP tools has been published. Some NLP tools provide scripts for extracting valid input text for the tools from real-world documents; however, even these scripts assume specific formats for the documents. For example, deep syntactic parsers such as the C&C Parser (Clark and Curran, 2007) and Enju (Ninomiya et al., 2007) assume POS-tagged sentences as input, and therefore the distributed packages for the parsers1 contain POS-taggers together with the parsers. The POS-taggers assume plain text sentences as their input. As the work most relevant to our study, UIMA (Ferruci et al., 2006) deals with various annotations in an integrated framework. However, in this work, the authors merely proposed the framework and did 1 45 / [Enju]: http://kmcs.nii.ac.jp/enju/ [C&C Parser]: http://svn.ask.it.usyd.edu.au/trac/candc/wiki Decoration Meta-info (a) &lt;text&gt;New UI&lt;/text&gt; is shown. The UI is more useful than XYZ &lt;indexmar"
W14-7008,P14-1023,0,0.0388694,"itched. Finally, Rule 3 is applied in step 5 and affects only the Japanese text. いる and を, both labeled as function words by the parser. 2.1.2 Compositional Distributed Semantics Many machine learning algorithms require fixed length vectors as input. Of the various different ways to map words to vectors, neural network based models have proven very effective recently. Vector representations (embeddings) created by these models have been utilized as features to achieve state-of-the-art results in several different Natural Language Processing tasks (Collobert et al., 2011; Mikolov et al., 2013; Baroni et al., 2014), giving rise to many new variants. Methods have been developed that are capable of embedding words from different languages into the same vector space (Zou et al., 2013), there are models that can induce representations of whole phrases or sentences instead of only single tokens (Socher et al., 2010; Le and Mikolov, 2014; Blacoe and Lapata, 2012) and there are models implementing a mixture of these two ideas, embedding phrases from different languages into the same vector space (Hermann and Blunsom, 2014; Cho et al., 2014). Work has been published about integrating embeddings and neural netwo"
W14-7008,D12-1050,0,0.0318483,"very effective recently. Vector representations (embeddings) created by these models have been utilized as features to achieve state-of-the-art results in several different Natural Language Processing tasks (Collobert et al., 2011; Mikolov et al., 2013; Baroni et al., 2014), giving rise to many new variants. Methods have been developed that are capable of embedding words from different languages into the same vector space (Zou et al., 2013), there are models that can induce representations of whole phrases or sentences instead of only single tokens (Socher et al., 2010; Le and Mikolov, 2014; Blacoe and Lapata, 2012) and there are models implementing a mixture of these two ideas, embedding phrases from different languages into the same vector space (Hermann and Blunsom, 2014; Cho et al., 2014). Work has been published about integrating embeddings and neural network models into the statistical machine translation pipeline for various language combinations, however, we are not aware of any previous work attempting this for the Japanese/English language pair. As a first step in this direction, we integrate the model introduced by Hermann et al. (Hermann and Blunsom, 2014) into our baseline system, specifical"
W14-7008,J81-4005,0,0.775336,"Missing"
W14-7008,P14-1129,0,0.0359334,"ne translation should take word order into account and should be trained towards objectives that do not neglect syntactic features. The semantic similarity that can be captured by the model described in (Hermann and Blunsom, 2014) appears to be already sufficiently covered by the default features of our baseline system for the case of machine translation. Employing the recursive neural network language model introduced by Mikolov et al. (Mikolov et al., 2011a) has proven successful for reranking translation candidates and significantly improved the RIBES score in our experiments. Recent work (Devlin et al., 2014; Cho et al., 2014; Zhang et al., 2014; Liu et al., 2014) proves, there is a lot of potential in utilizing neural network based models in the machine translation pipeline. With the lessons learned from our work we hope for successful applications of this combination to the Japanese-English language pair in the future. ample 2 shows a very natural structure, following the word order of the preordered Japanese source almost exactly. 5 Conclusion and Future Work In this work we investigated several modifications and insertions to the pipeline of our baseline statistical machine translation system"
W14-7008,D13-1139,0,0.0174431,"word order of the Japanese input sentence and the word order of the English target sentence. Parts of a Japanese sentence can be scrambled/shuffled without changing the meaning of the sentence or making it grammatically incorrect. Therefore, one English sentence can potentially correspond to several shuffled variations of the same Japanese sentence. Yoshida et al. (2014) show that normalizing the word order of Japanese sentences can benefit readability. Taking scrambling into account not only increases readability, it also plays an important role in machine translation (Isozaki et al., 2014). Hayashi et al. (2013) were able to improve the results of statistical machine translation systems by generating English determiners in the Japanese input text and reordering its words. Kudo et al. (2014) applied preordering and generated zerosubjects to achieve state-of-the-art results on a web-text corpus. We employ the preordering rules introduced in (Hoshino et al., 2014) to reduce order ambiguity in the Japanese input text. This method has achieved state-of-the-art results on the NTCIR patent corpus. Following this method, we parse input sentences with the Japanese dependency parser KNP to obtain chunked1 depe"
W14-7008,W14-7001,0,0.0417255,"ores into the translation model of our baseline system that are computed from semantically meaningful distributed vector representations. • As postprocessing, we utilize a recurrent neural network language model to re-score the 100 best translation candidates for each output sentence of our system. Being able to handle variable length context, it complements the n-gram based language model used within the pipeline. 2 System We built our baseline system with Moses (Koehn et al., 2007) as a phrase-based machine translation system loosely following the setup described by the WAT 2014 organizers (Nakazawa et al., 2014), with some modifications. We will quickly go through every step of our training. Introduction Modern models for statistical machine translation constitute a pipeline of different components. This pipeline usually involves a preprocessing part, a language model, a translation model and a postprocessing part. While this or a similar structure is basis of most systems and generally agreed upon, a lot of research has been focusing on modifying and extending the individual components. Many parts rely on probabilities acquired through word frequencies in fixed contexts, discarding additional syntac"
W14-7008,P14-1006,0,0.273852,"ral different Natural Language Processing tasks (Collobert et al., 2011; Mikolov et al., 2013; Baroni et al., 2014), giving rise to many new variants. Methods have been developed that are capable of embedding words from different languages into the same vector space (Zou et al., 2013), there are models that can induce representations of whole phrases or sentences instead of only single tokens (Socher et al., 2010; Le and Mikolov, 2014; Blacoe and Lapata, 2012) and there are models implementing a mixture of these two ideas, embedding phrases from different languages into the same vector space (Hermann and Blunsom, 2014; Cho et al., 2014). Work has been published about integrating embeddings and neural network models into the statistical machine translation pipeline for various language combinations, however, we are not aware of any previous work attempting this for the Japanese/English language pair. As a first step in this direction, we integrate the model introduced by Hermann et al. (Hermann and Blunsom, 2014) into our baseline system, specifically, we use a slightly modified version of the “BI” model described in the paper. Figure 2: Illustration of model described in (Hermann and Blunsom, 2014), slight"
W14-7008,D10-1092,0,0.299884,"Missing"
W14-7008,C14-1112,0,0.0698528,"ill not be split when a predecessor chunk cx−1 is a noun. 2.1.1 Preordering Japanese Text We employ a preordering method for Japanese-toEnglish translation. We preorder the input text in the preprocessing stage to reduce the difference between the word order of the Japanese input sentence and the word order of the English target sentence. Parts of a Japanese sentence can be scrambled/shuffled without changing the meaning of the sentence or making it grammatically incorrect. Therefore, one English sentence can potentially correspond to several shuffled variations of the same Japanese sentence. Yoshida et al. (2014) show that normalizing the word order of Japanese sentences can benefit readability. Taking scrambling into account not only increases readability, it also plays an important role in machine translation (Isozaki et al., 2014). Hayashi et al. (2013) were able to improve the results of statistical machine translation systems by generating English determiners in the Japanese input text and reordering its words. Kudo et al. (2014) applied preordering and generated zerosubjects to achieve state-of-the-art results on a web-text corpus. We employ the preordering rules introduced in (Hoshino et al., 2"
W14-7008,W10-1736,0,0.112691,"nly marginally change the composed representation. This issue becomes particularly problematic for pairs of the form Japanese English トークン トークン トークン token token ! the token the the System averaged Kendall’s tau Baseline Baseline + Preordering 0.2990 0.3712 Table 2: Preordering Evaluation with Kendall’s tau. the neural model score can substitute the default Moses scores. 4.2 Preordering To separate the evaluation of our preordering method from the machine translation evaluation, we calculated an intrinsic quality measure specific to preordering. We apply the procedure previously introduced by (Isozaki et al., 2010b; Hoshino et al., 2014). In our baseline method we rely on MGiza++ to align Japanese and English sub-phrases. Without preordering MGiza++ will perform a lot of nonmonotonic alignments. The goal of preordering is to reduce the number of these non-monotonic alignments, in the best case leading to exclusively monotonic alignments. Utilizing Kendall’s tau we can compare the alignments resulting from input with and without preordering. The closer this coefficient is to 1.0, the more monotonic are the alignments and the higher is the intrinsic benefit of the preordering. Table 2 lists the averaged"
W14-7008,P14-1011,0,0.136009,"ditional similarity score computed by the neural model in the same way as described above for the feature function case. This does not require extending our baseline system, the system only needs to incorporate one more feature when reading the phrase table. 2.1.3 Reranking with the Recurrent Neural Network Language Model Neural Networks have been applied successfully in language modeling tasks over the past years (Mikolov et al., 2011a) and are increasingly often found in combination with or as an extension to statistical machine translation systems (Devlin et 58 al., 2014; Cho et al., 2014; Zhang et al., 2014; Liu et al., 2014). We employ the Recurrent Neural Network Language Model Toolkit (RNNLM) (Mikolov et al., 2011b) to rerank the 100 best translation candidates for each sentence and use the best candidates for our submission. 3 Data Embeddings BLEU RIBES No No No None (Baseline) Feature Function Phrase Table (full) 19.16 18.95 18.82 63.41 63.48 63.23 Yes Yes Yes Feature Function None (Baseline) Phrase Table (full) 18.92 18.55 - 61.76 61.44 - No Phrase Table (1 col) 14.53 60.59 Table 1: Scores for different ways to include the Compositional Distributed Semantics model, with and without preorde"
W14-7008,W14-3335,0,0.0136998,"difference between the word order of the Japanese input sentence and the word order of the English target sentence. Parts of a Japanese sentence can be scrambled/shuffled without changing the meaning of the sentence or making it grammatically incorrect. Therefore, one English sentence can potentially correspond to several shuffled variations of the same Japanese sentence. Yoshida et al. (2014) show that normalizing the word order of Japanese sentences can benefit readability. Taking scrambling into account not only increases readability, it also plays an important role in machine translation (Isozaki et al., 2014). Hayashi et al. (2013) were able to improve the results of statistical machine translation systems by generating English determiners in the Japanese input text and reordering its words. Kudo et al. (2014) applied preordering and generated zerosubjects to achieve state-of-the-art results on a web-text corpus. We employ the preordering rules introduced in (Hoshino et al., 2014) to reduce order ambiguity in the Japanese input text. This method has achieved state-of-the-art results on the NTCIR patent corpus. Following this method, we parse input sentences with the Japanese dependency parser KNP"
W14-7008,D13-1141,0,0.0322374,"ntics Many machine learning algorithms require fixed length vectors as input. Of the various different ways to map words to vectors, neural network based models have proven very effective recently. Vector representations (embeddings) created by these models have been utilized as features to achieve state-of-the-art results in several different Natural Language Processing tasks (Collobert et al., 2011; Mikolov et al., 2013; Baroni et al., 2014), giving rise to many new variants. Methods have been developed that are capable of embedding words from different languages into the same vector space (Zou et al., 2013), there are models that can induce representations of whole phrases or sentences instead of only single tokens (Socher et al., 2010; Le and Mikolov, 2014; Blacoe and Lapata, 2012) and there are models implementing a mixture of these two ideas, embedding phrases from different languages into the same vector space (Hermann and Blunsom, 2014; Cho et al., 2014). Work has been published about integrating embeddings and neural network models into the statistical machine translation pipeline for various language combinations, however, we are not aware of any previous work attempting this for the Japa"
W14-7008,P07-2045,0,0.00420608,"apply preordering to the input text as a way of compensating for syntactic differences between English and Japanese. • We insert scores into the translation model of our baseline system that are computed from semantically meaningful distributed vector representations. • As postprocessing, we utilize a recurrent neural network language model to re-score the 100 best translation candidates for each output sentence of our system. Being able to handle variable length context, it complements the n-gram based language model used within the pipeline. 2 System We built our baseline system with Moses (Koehn et al., 2007) as a phrase-based machine translation system loosely following the setup described by the WAT 2014 organizers (Nakazawa et al., 2014), with some modifications. We will quickly go through every step of our training. Introduction Modern models for statistical machine translation constitute a pipeline of different components. This pipeline usually involves a preprocessing part, a language model, a translation model and a postprocessing part. While this or a similar structure is basis of most systems and generally agreed upon, a lot of research has been focusing on modifying and extending the ind"
W14-7008,P14-2091,0,0.0145239,"entence or making it grammatically incorrect. Therefore, one English sentence can potentially correspond to several shuffled variations of the same Japanese sentence. Yoshida et al. (2014) show that normalizing the word order of Japanese sentences can benefit readability. Taking scrambling into account not only increases readability, it also plays an important role in machine translation (Isozaki et al., 2014). Hayashi et al. (2013) were able to improve the results of statistical machine translation systems by generating English determiners in the Japanese input text and reordering its words. Kudo et al. (2014) applied preordering and generated zerosubjects to achieve state-of-the-art results on a web-text corpus. We employ the preordering rules introduced in (Hoshino et al., 2014) to reduce order ambiguity in the Japanese input text. This method has achieved state-of-the-art results on the NTCIR patent corpus. Following this method, we parse input sentences with the Japanese dependency parser KNP to obtain chunked1 dependency and coordination labels corresponding to the dependency. After that, we Rule 1-B Even if Rule 1-A is applied to a chunk cx , the chunk will be split into three new chunks (wor"
W14-7008,P14-1140,0,0.160292,"score computed by the neural model in the same way as described above for the feature function case. This does not require extending our baseline system, the system only needs to incorporate one more feature when reading the phrase table. 2.1.3 Reranking with the Recurrent Neural Network Language Model Neural Networks have been applied successfully in language modeling tasks over the past years (Mikolov et al., 2011a) and are increasingly often found in combination with or as an extension to statistical machine translation systems (Devlin et 58 al., 2014; Cho et al., 2014; Zhang et al., 2014; Liu et al., 2014). We employ the Recurrent Neural Network Language Model Toolkit (RNNLM) (Mikolov et al., 2011b) to rerank the 100 best translation candidates for each sentence and use the best candidates for our submission. 3 Data Embeddings BLEU RIBES No No No None (Baseline) Feature Function Phrase Table (full) 19.16 18.95 18.82 63.41 63.48 63.23 Yes Yes Yes Feature Function None (Baseline) Phrase Table (full) 18.92 18.55 - 61.76 61.44 - No Phrase Table (1 col) 14.53 60.59 Table 1: Scores for different ways to include the Compositional Distributed Semantics model, with and without preordering. In the Featur"
W15-2203,W02-1210,0,0.0172364,"for the English CCGbank, and obtained wide-coverage CCG resources for Japanese. A treebank is created in this method by converting chunk-based annotation resources. The treebank is then converted into a CCGbank for Japanese, which can be used to obtain widecoverage lexicon and parsers for Japanese CCG. Other than the one mentioned above, there are several other studies on Japanese deep parsing. The theoretical work by Gunji (1987) describes Japanese phenomenon based HPSG. Komagata (1999) proposed a CCG-based theory and implementation, but the focus is not on processing real world texts. JACY (Siegel and Bender, 2002) is a type of hand-crafted Japanese grammar based on HPSG that can compute a detailed semantic representation. One of our future goals is to obtain CCG resources that allow for a more precise and detailed description by incorporating additional annotations into CCGbank. PCOMP Verb AUX 対し confront-‐CONT て CONT Figure 4: Subtrees with compound postposition “ に対して” before (left) and after manual annotation (right). S CAUSER PP Noun PostP 先生 teacher は TOPIC VP PP ARG-‐ ga Noun PostP 学生 student に DAT VP VP Verb AUX AUX た PAST-‐BASE させ 調べ inquire-‐NEG cause-‐CONT Figure 5: Causative sentence “T"
W15-2203,W13-4913,0,0.0607779,"verting the chunk-dependency, it potentially includes even more errors in the phrase structures and other types of information such as the functional tags. For instance, the chunk-dependency is often insufficient for correctly deciding on the phrase structure of coordinated NPs with modifiers. Since the dependencies do not encode the left boundary of each NP (Asahara, 2013), a manual annotation is needed for the precise structure. In fact, it essentially lacks any linguistic information which is difficult to represent in the chunk-dependency, e.g., the coordinated arguments. The NTT treebank (Tanaka and Nagata, 2013) was used to improve the Japanese CCGbank for this paper. Basically the treebank is a manually corrected version of the source treebank used to create the CCGbank, but we treat the treebank as a collection of additional annotations to the source treebank. We specifically use its cleaner phrase structures to correct the phrase structure errors and its functional tags to properly deal with the coordinations in the derivations. Moreover, we use the annotations of causer roles in causative constructions in the NTT treebank, which are not available in the original syntactic resources, to recognize"
W15-2203,boxwell-white-2008-projecting,0,0.0205865,"対し confront-‐CONT CONT The corpus-based acquisition of wide-coverage CCG resources has been very successful for English (Hockenmaier and Steedman, 2007). Their method converts the Penn Treebank (Marcus et al., 1993) into CCGbank, which is a collection of CCG derivations, and extracts a wide-coverage lexicon from the derivations. The CCGbank is also used to train a robust CCG parser (Clark and Curran, 2007). Complementary resources on the Penn Treebank are used to improved the derivations because the treebank does not contain some of the linguistic information necessary for a CCG derivation. Boxwell and White (2008) augmented the English CCGbank with the semantic roles in PropBank (Palmer et al., 2005). Honnibal et al. (2010) integrated several types of additional annotations such as PropBank and NP structure annotation (Vadas and Curran, 2007), to improve the CCGbank. Our work basically follows these methods, but we have to deal with noises in the treebank that are caused by the dependency-to-tree conversion errors. Our previous work (Uematsu et al., 2015) extended the method used for the English CCGbank, and obtained wide-coverage CCG resources for Japanese. A treebank is created in this method by conv"
W15-2203,J07-4004,0,0.0409044,"ins tions, verbs, and auxiliaries works as one postfunctional tags and predicate argument structures. 22 PP VP PP VP Noun PostP 環境 environment に DAT Verb 4 Related work PCOMP Noun AUX 環境 environment PostP に DAT て 対し confront-‐CONT CONT The corpus-based acquisition of wide-coverage CCG resources has been very successful for English (Hockenmaier and Steedman, 2007). Their method converts the Penn Treebank (Marcus et al., 1993) into CCGbank, which is a collection of CCG derivations, and extracts a wide-coverage lexicon from the derivations. The CCGbank is also used to train a robust CCG parser (Clark and Curran, 2007). Complementary resources on the Penn Treebank are used to improved the derivations because the treebank does not contain some of the linguistic information necessary for a CCG derivation. Boxwell and White (2008) augmented the English CCGbank with the semantic roles in PropBank (Palmer et al., 2005). Honnibal et al. (2010) integrated several types of additional annotations such as PropBank and NP structure annotation (Vadas and Curran, 2007), to improve the CCGbank. Our work basically follows these methods, but we have to deal with noises in the treebank that are caused by the dependency-to-t"
W15-2203,P07-1031,0,0.0317506,"ich is a collection of CCG derivations, and extracts a wide-coverage lexicon from the derivations. The CCGbank is also used to train a robust CCG parser (Clark and Curran, 2007). Complementary resources on the Penn Treebank are used to improved the derivations because the treebank does not contain some of the linguistic information necessary for a CCG derivation. Boxwell and White (2008) augmented the English CCGbank with the semantic roles in PropBank (Palmer et al., 2005). Honnibal et al. (2010) integrated several types of additional annotations such as PropBank and NP structure annotation (Vadas and Curran, 2007), to improve the CCGbank. Our work basically follows these methods, but we have to deal with noises in the treebank that are caused by the dependency-to-tree conversion errors. Our previous work (Uematsu et al., 2015) extended the method used for the English CCGbank, and obtained wide-coverage CCG resources for Japanese. A treebank is created in this method by converting chunk-based annotation resources. The treebank is then converted into a CCGbank for Japanese, which can be used to obtain widecoverage lexicon and parsers for Japanese CCG. Other than the one mentioned above, there are several"
W15-2203,J07-3004,0,0.489951,"rase structures and a lack of linguistic information, which is difficult to represent in chunk-dependency. The NTT treebank provides cleaner trees and functional and semantic information, e.g., coordinations and predicate-argument structures. The effect of the improvement process is empirically evaluated in terms of the changes in the dependency relations extracted from the resulting derivations. 1 Introduction Wide-coverage resources for lexicalized grammars have been created by converting the existing treebanks into collections of derivations for the target grammars (Miyao and Tsujii, 2008; Hockenmaier and Steedman, 2007; Hockenmaier, 2006). However, the source corpora, such as the Penn Treebank (Marcus et al., 1993), often lack the necessary linguistic information for constructing these derivations, and this can create noise in the resulting derivations. Therefore, complementary annotations to the source treebank, e.g., NP bracketing and semantic roles, have been used used to improve the derivations (Honnibal et al., 2010; Vadas and Curran, 2008). It is especially important to reduce the amount of noise in the derivations in the CCGbank for 20 Proceedings of the 14th International Conference on Parsing Techn"
W15-2203,P08-1039,0,0.0201834,"lexicalized grammars have been created by converting the existing treebanks into collections of derivations for the target grammars (Miyao and Tsujii, 2008; Hockenmaier and Steedman, 2007; Hockenmaier, 2006). However, the source corpora, such as the Penn Treebank (Marcus et al., 1993), often lack the necessary linguistic information for constructing these derivations, and this can create noise in the resulting derivations. Therefore, complementary annotations to the source treebank, e.g., NP bracketing and semantic roles, have been used used to improve the derivations (Honnibal et al., 2010; Vadas and Curran, 2008). It is especially important to reduce the amount of noise in the derivations in the CCGbank for 20 Proceedings of the 14th International Conference on Parsing Technologies, pages 20–29, c Bilbao, Spain; July 22–24, 2015. 2015 Association for Computational Linguistics X/Y : f Y : a → X : f a Y : a XY : a → X : f a X/Y : f Y/Z : g → X/Z : λx.f (gx) Y : g XY : f → X : λx.f (gx) (&gt;) (<) (&gt; B) (< B) Cat. Feature Value NP case ga o ni to nc S form stem base neg cont vo s Figure 1: Combinatory rules in Japanese CCGbank. resources are hopefully applicable to other grammar formalisms. Interpreta"
W15-2203,P06-1064,0,0.0339577,"nguistic information, which is difficult to represent in chunk-dependency. The NTT treebank provides cleaner trees and functional and semantic information, e.g., coordinations and predicate-argument structures. The effect of the improvement process is empirically evaluated in terms of the changes in the dependency relations extracted from the resulting derivations. 1 Introduction Wide-coverage resources for lexicalized grammars have been created by converting the existing treebanks into collections of derivations for the target grammars (Miyao and Tsujii, 2008; Hockenmaier and Steedman, 2007; Hockenmaier, 2006). However, the source corpora, such as the Penn Treebank (Marcus et al., 1993), often lack the necessary linguistic information for constructing these derivations, and this can create noise in the resulting derivations. Therefore, complementary annotations to the source treebank, e.g., NP bracketing and semantic roles, have been used used to improve the derivations (Honnibal et al., 2010; Vadas and Curran, 2008). It is especially important to reduce the amount of noise in the derivations in the CCGbank for 20 Proceedings of the 14th International Conference on Parsing Technologies, pages 20–29"
W15-2203,P10-1022,0,0.217801,"-coverage resources for lexicalized grammars have been created by converting the existing treebanks into collections of derivations for the target grammars (Miyao and Tsujii, 2008; Hockenmaier and Steedman, 2007; Hockenmaier, 2006). However, the source corpora, such as the Penn Treebank (Marcus et al., 1993), often lack the necessary linguistic information for constructing these derivations, and this can create noise in the resulting derivations. Therefore, complementary annotations to the source treebank, e.g., NP bracketing and semantic roles, have been used used to improve the derivations (Honnibal et al., 2010; Vadas and Curran, 2008). It is especially important to reduce the amount of noise in the derivations in the CCGbank for 20 Proceedings of the 14th International Conference on Parsing Technologies, pages 20–29, c Bilbao, Spain; July 22–24, 2015. 2015 Association for Computational Linguistics X/Y : f Y : a → X : f a Y : a XY : a → X : f a X/Y : f Y/Z : g → X/Z : λx.f (gx) Y : g XY : f → X : λx.f (gx) (&gt;) (<) (&gt; B) (< B) Cat. Feature Value NP case ga o ni to nc S form stem base neg cont vo s Figure 1: Combinatory rules in Japanese CCGbank. resources are hopefully applicable to other gramm"
W15-2203,W07-1522,0,0.0204731,"postposition ” に/DAT/ni 対し/confront-CONT/taishi て /aux-CONT/te” typically functions similarly to the postposition “に/DAT/ni”. Fig. 4 shows the subtrees before and after the update. Since the structure on the left is the same as the structure for a continuous clause, it is difficult to distinguish compound postpositions and VPs. After the manual annotation, the compound postpositions are marked with “PCOMP” tags and have a specific structure, as shown on the right in Fig. 4 The PAS annotation on the base treebank, which is obtained by converting the word-to-word annotation of the NAIST corpus (Iida et al., 2007), was also manually corrected and populated. An important addition to the PAS is the annotation of causative and beneficial constructions. The original treebank (and the NAIST corpus) also identifies causative constructions, but there are very few annotations for the causer role, which typically occurs with the case marker “が/ga” or its topicalized form. Fig. 5 shows an example of the annotation of a causative construction with a causer role. 5 Incorporating additional annotation into CCGbank We describe the two steps needed to incorporate the annotations of the NTT treebank (NTB) into the Jap"
W15-2203,J93-2004,0,0.0532345,"he NTT treebank provides cleaner trees and functional and semantic information, e.g., coordinations and predicate-argument structures. The effect of the improvement process is empirically evaluated in terms of the changes in the dependency relations extracted from the resulting derivations. 1 Introduction Wide-coverage resources for lexicalized grammars have been created by converting the existing treebanks into collections of derivations for the target grammars (Miyao and Tsujii, 2008; Hockenmaier and Steedman, 2007; Hockenmaier, 2006). However, the source corpora, such as the Penn Treebank (Marcus et al., 1993), often lack the necessary linguistic information for constructing these derivations, and this can create noise in the resulting derivations. Therefore, complementary annotations to the source treebank, e.g., NP bracketing and semantic roles, have been used used to improve the derivations (Honnibal et al., 2010; Vadas and Curran, 2008). It is especially important to reduce the amount of noise in the derivations in the CCGbank for 20 Proceedings of the 14th International Conference on Parsing Technologies, pages 20–29, c Bilbao, Spain; July 22–24, 2015. 2015 Association for Computational Lingui"
W15-2203,J08-1002,1,0.782704,"ors caused by noisier phrase structures and a lack of linguistic information, which is difficult to represent in chunk-dependency. The NTT treebank provides cleaner trees and functional and semantic information, e.g., coordinations and predicate-argument structures. The effect of the improvement process is empirically evaluated in terms of the changes in the dependency relations extracted from the resulting derivations. 1 Introduction Wide-coverage resources for lexicalized grammars have been created by converting the existing treebanks into collections of derivations for the target grammars (Miyao and Tsujii, 2008; Hockenmaier and Steedman, 2007; Hockenmaier, 2006). However, the source corpora, such as the Penn Treebank (Marcus et al., 1993), often lack the necessary linguistic information for constructing these derivations, and this can create noise in the resulting derivations. Therefore, complementary annotations to the source treebank, e.g., NP bracketing and semantic roles, have been used used to improve the derivations (Honnibal et al., 2010; Vadas and Curran, 2008). It is especially important to reduce the amount of noise in the derivations in the CCGbank for 20 Proceedings of the 14th Internati"
W15-2203,J05-1004,0,0.1107,"n very successful for English (Hockenmaier and Steedman, 2007). Their method converts the Penn Treebank (Marcus et al., 1993) into CCGbank, which is a collection of CCG derivations, and extracts a wide-coverage lexicon from the derivations. The CCGbank is also used to train a robust CCG parser (Clark and Curran, 2007). Complementary resources on the Penn Treebank are used to improved the derivations because the treebank does not contain some of the linguistic information necessary for a CCG derivation. Boxwell and White (2008) augmented the English CCGbank with the semantic roles in PropBank (Palmer et al., 2005). Honnibal et al. (2010) integrated several types of additional annotations such as PropBank and NP structure annotation (Vadas and Curran, 2007), to improve the CCGbank. Our work basically follows these methods, but we have to deal with noises in the treebank that are caused by the dependency-to-tree conversion errors. Our previous work (Uematsu et al., 2015) extended the method used for the English CCGbank, and obtained wide-coverage CCG resources for Japanese. A treebank is created in this method by converting chunk-based annotation resources. The treebank is then converted into a CCGbank f"
W16-0109,P14-1133,0,0.0290346,"et al., 2014). Every system outputs only one answer. The system’s answer is the entity with highest score (randomly pick one if there is a tie). No answer is produced if the highest score is below a certain threshold. An answer is considered correct if the entity in the system’s answer appears in the gold answer. The precision, recall and F1 score are calculated globally: # questions with correct answers # questions with answers # questions with correct answers # questions Precision = Recall = 2. Average F1 score (accuracy). This is used by semantic parsing question answering systems such as (Berant and Liang, 2014; Yih et al., 2015). For every question, the precision, recall and F1 score are computed between the systems’ answer set and the gold answer set. Then, the F1 scores are averaged across all questions in the test set. This metric is used to reward partially-complete system answers. In the following experiments, we will compare our system with Fader’s with respect to the first metric, and the rest with the second metric. 58 8.1 The Effect of Dataset size We demonstrate the effect of different dataset sizes by estimating a paraphrase PMI model from a smaller subset of our data, and then comparing"
W16-0109,D13-1160,0,0.295046,"ssful attempts to create a robust semantic parser. It worked by first syntactically parsing a question, augmenting the result with semantic information, and then transforming the result into a logical language. However, this process requires a large volume of training supervision, namely “gold standard” annotations of semantically-augmented syntactic trees paired with their logical representations. Its demonstration was limited to GeoQuery (Zelle and Mooney, 1996), which is a very restricted domain and database. A more recent approach to achieve robust, open-domain semantic parsing is that of Berant et al. (2013), where the training supervision is limited to pairs of questions and answers. In their approach, they use latent λ-DCS (Liang, 2013) logical formulas for their meaning representations, which can be converted deterministically into Sparql queries on Freebase. Yih et al. (2015) developed the best performing semantic parser on the WebQuestion set (Yih et al., 2015). Inspired by (Yao and Van Durme, 2014), instead of searching on the whole knowledge base, they defined a query graph which is more closely related to the target entity in the questions. Regarding IE QA systems, there are two represent"
W16-0109,W05-0602,0,0.12867,"Missing"
W16-0109,J13-2005,0,0.0128573,"nformation, and then transforming the result into a logical language. However, this process requires a large volume of training supervision, namely “gold standard” annotations of semantically-augmented syntactic trees paired with their logical representations. Its demonstration was limited to GeoQuery (Zelle and Mooney, 1996), which is a very restricted domain and database. A more recent approach to achieve robust, open-domain semantic parsing is that of Berant et al. (2013), where the training supervision is limited to pairs of questions and answers. In their approach, they use latent λ-DCS (Liang, 2013) logical formulas for their meaning representations, which can be converted deterministically into Sparql queries on Freebase. Yih et al. (2015) developed the best performing semantic parser on the WebQuestion set (Yih et al., 2015). Inspired by (Yao and Van Durme, 2014), instead of searching on the whole knowledge base, they defined a query graph which is more closely related to the target entity in the questions. Regarding IE QA systems, there are two representatives. Yao and Van Durme (2014) constructs a graph structure to represent each KB topic, which is searched to retrieve answers to qu"
W16-0109,Q14-1030,0,0.0302097,"Missing"
W16-0109,N13-1008,0,0.0737204,"Missing"
W16-0109,P15-2046,1,0.888459,"Missing"
W16-0109,P14-1090,0,0.041604,"Missing"
W16-0109,P15-1128,0,0.0618681,", namely “gold standard” annotations of semantically-augmented syntactic trees paired with their logical representations. Its demonstration was limited to GeoQuery (Zelle and Mooney, 1996), which is a very restricted domain and database. A more recent approach to achieve robust, open-domain semantic parsing is that of Berant et al. (2013), where the training supervision is limited to pairs of questions and answers. In their approach, they use latent λ-DCS (Liang, 2013) logical formulas for their meaning representations, which can be converted deterministically into Sparql queries on Freebase. Yih et al. (2015) developed the best performing semantic parser on the WebQuestion set (Yih et al., 2015). Inspired by (Yao and Van Durme, 2014), instead of searching on the whole knowledge base, they defined a query graph which is more closely related to the target entity in the questions. Regarding IE QA systems, there are two representatives. Yao and Van Durme (2014) constructs a graph structure to represent each KB topic, which is searched to retrieve answers to questions. Fader et al. (2014)’s system constructs the KB as a triple database, where each triple consists of two entities and one relation phrase"
W18-6009,L18-1287,1,0.271859,"fundamental issues due to hypotactic attributes in terms of syntax in coordinate structures. This paper points out the issues in the treatment of coordinate structures with evidence of linguistic plausibility and the trainability of parsers, reports on the current status of the corpora in those languages, and proposes alternative representations. Section 2 describes the linguistic features of head-final languages, and Section 3 points out the problems in the left-headed coordinate structures in head-final languages. Section 4 summarizes the current status of UD Japanese (Tanaka et al., 2016; Asahara et al., 2018) and UD Korean (Chun et al., 2018) corpora released as version 2.2. Section 5 shows the experimental results on multiple corpora in Japanese and Korean to attest the difficulty in training with left-headed coordination. Section 6 proposes a revision to the UD guidelines more suited to head-final languages. This paper discusses the representation of coordinate structures in the Universal Dependencies framework for two head-final languages, Japanese and Korean. UD applies a strict principle that makes the head of coordination the left-most conjunct. However, the guideline may produce syntactic t"
W18-6009,W17-6508,0,0.118231,"Missing"
W18-6009,W11-3801,1,0.783345,"the complexities outlined in the previous section, the UD Japanese and UD Korean teams had to work within the bounds of the principles laid out by the Universal Dependencies version 2. Therefore, in the official version 2.2 release used for the CoNLL 2018 shared task (Zeman et al., 2018), UD Japanese and UD Korean adopted two separate strategies in order to ensure compliance, 79 the verb ‘run’ with the (nsubj) label. 4.2 UD Korean Unlike the Japanese UD, the Korean UD effort has made a conscious decision to use right-headedness for conjunction following the coordination guidelines proposed by Choi and Palmer (2011). Thus, the coordinate structures in all three of the Korean UD corpora (Chun et al., 2018) were developed with the rightmost conjunct as the head of the phrase, with each conjunct pointing to its right sibling as its head. For the latest available UD_Korean-GSD, however, the dependencies were converted to leftheaded structures post-development in an effort to fully comply with the UD guidelines despite the problems left-headed structures pose for the language as described in Section 3. The other two Korean UD corpora, namely the Kaist and the Korean Penn Treebank, reflect right-headed coordin"
W18-6009,L18-1347,1,0.849927,"attributes in terms of syntax in coordinate structures. This paper points out the issues in the treatment of coordinate structures with evidence of linguistic plausibility and the trainability of parsers, reports on the current status of the corpora in those languages, and proposes alternative representations. Section 2 describes the linguistic features of head-final languages, and Section 3 points out the problems in the left-headed coordinate structures in head-final languages. Section 4 summarizes the current status of UD Japanese (Tanaka et al., 2016; Asahara et al., 2018) and UD Korean (Chun et al., 2018) corpora released as version 2.2. Section 5 shows the experimental results on multiple corpora in Japanese and Korean to attest the difficulty in training with left-headed coordination. Section 6 proposes a revision to the UD guidelines more suited to head-final languages. This paper discusses the representation of coordinate structures in the Universal Dependencies framework for two head-final languages, Japanese and Korean. UD applies a strict principle that makes the head of coordination the left-most conjunct. However, the guideline may produce syntactic trees which are difficult to accept"
W18-6009,de-marneffe-etal-2014-universal,0,0.118166,"Missing"
W18-6009,W15-2113,0,0.550814,"Missing"
W18-6009,W14-4202,1,0.883365,"Missing"
W18-6009,C18-1324,0,0.0259098,"on scheme the conjunctive particle かわいい ⽝ と 猫 が ⾛る “와” (wa) is kept suffixized in the left nominal conkawaii inu -to neko -ga hashiru junct eojeol, thus the conjunction relation cc is not ADJ NOUN CCONJ NOUN ADP VERB overtly marked. ‘cute’ ‘dog’ ‘and’ ‘cat’ -NOM ‘run’ A common problem with adjectival modification in UD shown in Figures 4 and 5 is that there Figure 4: Left-headed representation of a nominal cois no way to distinguish between modification ordination in Japanese “⽝と猫” (‘dog and cat’), in a of the full coordination vs. of the first conjunct sentence “かわいい⽝と猫が⾛る” (‘A cute dog and (Przepiórkowski and Patejuk, 2018) . For examcat run’). root ple, there is no way to specify the scope of the adjective ‘cute’: the two readings (1) only a dog is nsubj cute and (2) both animals are cute. acl conj 3.2 Verbal coordination 예쁜 개와 고양이가 달린다 yeyppun kay+wa koyangi+ka tali+nta ADJ NOUN NOUN VERB ‘cute’ ‘dog+and’ ‘cat-NOM’ ‘run’ Further critical issues are attested in the verbal coordinate structures. Figure 6 shows the left-headed verbal coordination “⾷べて⾛る” (‘eat and run’) in a noun phrase “⾷べて⾛る⼈” (‘a person who eats and runs’), where verb “⾷べ” (‘eat’) is the child of “⼈” (‘person’). Despite this dependency relatio"
W18-6009,L16-1376,0,0.0419402,"Missing"
W18-6009,L16-1680,0,0.017083,"on strictly reduces the dependency distance in head-final languages. These results support the advantages of the rightheaded strategy in Japanese coordinate structures. nsubj 오바마 대통령이 말한다 obama taythonglyeng+i malha+nta PROPN NOUN VERB ‘Obama’ ‘president-NOM’ ‘say’ (a) Korean right-headed flat structure. root nsubj flat 오바마 대통령이 말한다 obama taythonglyeng+i malha+nta PROPN NOUN VERB ‘Obama’ ‘president-NOM’ ‘say’ (b) (a) converted to left-headed structure as reflected in the UD_Korean-GSD. Figure 12: The use of flat in Korean UD v2.2. guages? To answer this question, we trained and tested UDPipe (Straka et al., 2016) on multiple versions of UD Japanese and Korean corpora. 5.1 Japanese As described in Section 4.1, the current UD Japanese-GSD corpus does not use conj tags. The corpus was converted into another version with coordinations without changing the dependency structures (right-headed coordination), that is, some of nmod and advcl labels are converted into conj label when the original manual annotation used conj regarding them as nominal or verbal coordinations. Also CCONJ tag and cc label are assigned to the coordinative case markers. The corpus was further converted into left-headed coordination,"
W18-6009,L16-1261,1,0.697171,"structure poses some fundamental issues due to hypotactic attributes in terms of syntax in coordinate structures. This paper points out the issues in the treatment of coordinate structures with evidence of linguistic plausibility and the trainability of parsers, reports on the current status of the corpora in those languages, and proposes alternative representations. Section 2 describes the linguistic features of head-final languages, and Section 3 points out the problems in the left-headed coordinate structures in head-final languages. Section 4 summarizes the current status of UD Japanese (Tanaka et al., 2016; Asahara et al., 2018) and UD Korean (Chun et al., 2018) corpora released as version 2.2. Section 5 shows the experimental results on multiple corpora in Japanese and Korean to attest the difficulty in training with left-headed coordination. Section 6 proposes a revision to the UD guidelines more suited to head-final languages. This paper discusses the representation of coordinate structures in the Universal Dependencies framework for two head-final languages, Japanese and Korean. UD applies a strict principle that makes the head of coordination the left-most conjunct. However, the guideline"
W18-6515,P09-1011,0,0.00975975,"of the previous day. Generating Market Comments We describe our model for generating comments. We extend the encoder part of the model proposed by Murakami et al. (2017), which had a limitation in generating informative market comments due to the lack of a capability to consider multiple data sources as input. We first explain the encoder used in the existing model and then show how we extend it. 3.1 LSTM Linear There has been a lot of work on generating text from numerical time series or structured data including weather data (Belz, 2008), healthcare data (Portet et al., 2009), sports data (Liang et al., 2009), and market data (Kukich, 1983). Approaches to such tasks are traditionally dependent on handcrafted rules (Goldberg et al., 1994; Dale et al., 2003) or are template-based. Neural encoder-decoders (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) have also been successfully applied to various data-to-text generation tasks. While many generate text from table data, such as reviews from product attributes (Dong et al., 2017) and biographies from the infoboxes of Wikipedia (Lebret et al., 2016), there is an attempt to generate text from numerical data (Murakami et al., 2017), i"
W18-6515,D15-1166,0,0.00938938,"ity to consider multiple data sources as input. We first explain the encoder used in the existing model and then show how we extend it. 3.1 LSTM Linear There has been a lot of work on generating text from numerical time series or structured data including weather data (Belz, 2008), healthcare data (Portet et al., 2009), sports data (Liang et al., 2009), and market data (Kukich, 1983). Approaches to such tasks are traditionally dependent on handcrafted rules (Goldberg et al., 1994; Dale et al., 2003) or are template-based. Neural encoder-decoders (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) have also been successfully applied to various data-to-text generation tasks. While many generate text from table data, such as reviews from product attributes (Dong et al., 2017) and biographies from the infoboxes of Wikipedia (Lebret et al., 2016), there is an attempt to generate text from numerical data (Murakami et al., 2017), in which market comments are generated from a time-series of stock prices. However, the model of Murakami et al. (2017), which is based on an encoderdecoder, takes only a target time series and ignores the fact that there are many mentions of external resources. 3 T"
W18-6515,E17-1059,0,0.0179969,"on generating text from numerical time series or structured data including weather data (Belz, 2008), healthcare data (Portet et al., 2009), sports data (Liang et al., 2009), and market data (Kukich, 1983). Approaches to such tasks are traditionally dependent on handcrafted rules (Goldberg et al., 1994; Dale et al., 2003) or are template-based. Neural encoder-decoders (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) have also been successfully applied to various data-to-text generation tasks. While many generate text from table data, such as reviews from product attributes (Dong et al., 2017) and biographies from the infoboxes of Wikipedia (Lebret et al., 2016), there is an attempt to generate text from numerical data (Murakami et al., 2017), in which market comments are generated from a time-series of stock prices. However, the model of Murakami et al. (2017), which is based on an encoderdecoder, takes only a target time series and ignores the fact that there are many mentions of external resources. 3 T gains &lt;price/&gt; main. We encode each of the external resources in addition to the target index, i.e., Nikkei 225, and feed them to the decoder. The experimental results show that o"
W18-6515,P83-1022,0,0.190223,"t Comments We describe our model for generating comments. We extend the encoder part of the model proposed by Murakami et al. (2017), which had a limitation in generating informative market comments due to the lack of a capability to consider multiple data sources as input. We first explain the encoder used in the existing model and then show how we extend it. 3.1 LSTM Linear There has been a lot of work on generating text from numerical time series or structured data including weather data (Belz, 2008), healthcare data (Portet et al., 2009), sports data (Liang et al., 2009), and market data (Kukich, 1983). Approaches to such tasks are traditionally dependent on handcrafted rules (Goldberg et al., 1994; Dale et al., 2003) or are template-based. Neural encoder-decoders (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) have also been successfully applied to various data-to-text generation tasks. While many generate text from table data, such as reviews from product attributes (Dong et al., 2017) and biographies from the infoboxes of Wikipedia (Lebret et al., 2016), there is an attempt to generate text from numerical data (Murakami et al., 2017), in which market comments are gene"
W18-6515,D16-1128,0,0.0147791,"luding weather data (Belz, 2008), healthcare data (Portet et al., 2009), sports data (Liang et al., 2009), and market data (Kukich, 1983). Approaches to such tasks are traditionally dependent on handcrafted rules (Goldberg et al., 1994; Dale et al., 2003) or are template-based. Neural encoder-decoders (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) have also been successfully applied to various data-to-text generation tasks. While many generate text from table data, such as reviews from product attributes (Dong et al., 2017) and biographies from the infoboxes of Wikipedia (Lebret et al., 2016), there is an attempt to generate text from numerical data (Murakami et al., 2017), in which market comments are generated from a time-series of stock prices. However, the model of Murakami et al. (2017), which is based on an encoderdecoder, takes only a target time series and ignores the fact that there are many mentions of external resources. 3 T gains &lt;price/&gt; main. We encode each of the external resources in addition to the target index, i.e., Nikkei 225, and feed them to the decoder. The experimental results show that our proposed model outperforms the existing single-source model in term"
W19-8640,W18-6554,0,0.020087,"advantages of this approach. 2 periments show the fluency and fidelity of the generated document in terms of BLEU and humanevaluation. Secondly, compared the generated documents between with human-designed labels and automatically extracted keywords, humandesigned labels are more useful as the ease of understanding. Related study 3 Generation of Market Comments Controllability of text generation has been an intensive research focus recently. Examples include suggestive content control such as tense, sentiment, gender, or automatically learned hidden states (Hu et al., 2017; Zhao et al., 2018; Juraska and Walker, 2018; Bau et al., 2019). Another series of work is focused on controlling surface textual features such as length, descriptiveness and politeness (Li et al., 2016; Sennrich et al., 2016; Kikuchi et al., 2016; Ficler and Goldberg, 2017; Shen et al., 2017; Prabhumoye et al., 2018). The target of these previous methods is on controlling generic contentindependent features of texts. That is, they aim at varying surface strings while preserving main information content. Wiseman et al. (2018) proposed a neural model that generates diverse texts by learning templates. They control diversity through templ"
W19-8640,D16-1140,1,0.842464,"abels and automatically extracted keywords, humandesigned labels are more useful as the ease of understanding. Related study 3 Generation of Market Comments Controllability of text generation has been an intensive research focus recently. Examples include suggestive content control such as tense, sentiment, gender, or automatically learned hidden states (Hu et al., 2017; Zhao et al., 2018; Juraska and Walker, 2018; Bau et al., 2019). Another series of work is focused on controlling surface textual features such as length, descriptiveness and politeness (Li et al., 2016; Sennrich et al., 2016; Kikuchi et al., 2016; Ficler and Goldberg, 2017; Shen et al., 2017; Prabhumoye et al., 2018). The target of these previous methods is on controlling generic contentindependent features of texts. That is, they aim at varying surface strings while preserving main information content. Wiseman et al. (2018) proposed a neural model that generates diverse texts by learning templates. They control diversity through templates rather than contents or the order of them. The present work is more closely related to methods for controlling topical content by using automatically extracted or human-designed keywords (Wang et al"
W19-8640,D16-1128,0,0.0418044,"Missing"
W19-8640,P16-1094,0,0.0312147,"documents between with human-designed labels and automatically extracted keywords, humandesigned labels are more useful as the ease of understanding. Related study 3 Generation of Market Comments Controllability of text generation has been an intensive research focus recently. Examples include suggestive content control such as tense, sentiment, gender, or automatically learned hidden states (Hu et al., 2017; Zhao et al., 2018; Juraska and Walker, 2018; Bau et al., 2019). Another series of work is focused on controlling surface textual features such as length, descriptiveness and politeness (Li et al., 2016; Sennrich et al., 2016; Kikuchi et al., 2016; Ficler and Goldberg, 2017; Shen et al., 2017; Prabhumoye et al., 2018). The target of these previous methods is on controlling generic contentindependent features of texts. That is, they aim at varying surface strings while preserving main information content. Wiseman et al. (2018) proposed a neural model that generates diverse texts by learning templates. They control diversity through templates rather than contents or the order of them. The present work is more closely related to methods for controlling topical content by using automatically ext"
W19-8640,P09-1011,0,0.0177701,"tion. Experiments show both models using additional information of target document achieved higher performance in terms of BLEU and human evaluation. We found that human-designed topic labels are superior to extracted keywords in terms of controllability. 1 Introduction Data-to-text is one of the challenging tasks in natural language generation, which aims to generate summaries of input data such as statistics from sports games (Robin, 1995; Barzilay and Lapata, 2005; Wiseman et al., 2017), financial data (Murakami et al., 2017; Aoki et al., 2018), and database records (Reiter and Dale, 1997; Liang et al., 2009; Mei et al., 2016; Lebret et al., 2016; Novikova et al., 2017; Liu et al., 2018; Wiseman et al., 2017). Over the past several years, end-to-end neural language generation models have successfully 323 Proceedings of The 12th International Conference on Natural Language Generation, pages 323–332, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics closed set of domain-specific labels by investigating financial news articles. In the experiments on generating daily summaries of financial markets, we will empirically show the eﬀectiveness of topic labels and potent"
W19-8640,H05-1042,0,0.0892773,"ormation: human-designed topic labels indicating the contents of a sentence and automatically extracted keywords as the referential information for generation. Experiments show both models using additional information of target document achieved higher performance in terms of BLEU and human evaluation. We found that human-designed topic labels are superior to extracted keywords in terms of controllability. 1 Introduction Data-to-text is one of the challenging tasks in natural language generation, which aims to generate summaries of input data such as statistics from sports games (Robin, 1995; Barzilay and Lapata, 2005; Wiseman et al., 2017), financial data (Murakami et al., 2017; Aoki et al., 2018), and database records (Reiter and Dale, 1997; Liang et al., 2009; Mei et al., 2016; Lebret et al., 2016; Novikova et al., 2017; Liu et al., 2018; Wiseman et al., 2017). Over the past several years, end-to-end neural language generation models have successfully 323 Proceedings of The 12th International Conference on Natural Language Generation, pages 323–332, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics closed set of domain-specific labels by investigating financial news ar"
W19-8640,N16-1000,0,0.230953,"Missing"
W19-8640,N16-1086,0,0.0182776,"ow both models using additional information of target document achieved higher performance in terms of BLEU and human evaluation. We found that human-designed topic labels are superior to extracted keywords in terms of controllability. 1 Introduction Data-to-text is one of the challenging tasks in natural language generation, which aims to generate summaries of input data such as statistics from sports games (Robin, 1995; Barzilay and Lapata, 2005; Wiseman et al., 2017), financial data (Murakami et al., 2017; Aoki et al., 2018), and database records (Reiter and Dale, 1997; Liang et al., 2009; Mei et al., 2016; Lebret et al., 2016; Novikova et al., 2017; Liu et al., 2018; Wiseman et al., 2017). Over the past several years, end-to-end neural language generation models have successfully 323 Proceedings of The 12th International Conference on Natural Language Generation, pages 323–332, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics closed set of domain-specific labels by investigating financial news articles. In the experiments on generating daily summaries of financial markets, we will empirically show the eﬀectiveness of topic labels and potential advantages/dis"
W19-8640,C16-1100,0,0.0312858,"al., 2016; Ficler and Goldberg, 2017; Shen et al., 2017; Prabhumoye et al., 2018). The target of these previous methods is on controlling generic contentindependent features of texts. That is, they aim at varying surface strings while preserving main information content. Wiseman et al. (2018) proposed a neural model that generates diverse texts by learning templates. They control diversity through templates rather than contents or the order of them. The present work is more closely related to methods for controlling topical content by using automatically extracted or human-designed keywords (Wang et al., 2016; Yao et al., 2017, 2018; Miao et al., 2018). Our method resembles the idea of using keywords to control topics of sentences and their orders, but it primarily focuses on describing given data and uses topic labels as auxiliary information. We will empirically attest added eﬀects of introducing topic labels in the data-to-document scenario. Besides, Gkatzia et al. (2017) and Portet et al. (2009) proposed non-neural language generation models for the data-to-text task with higher controllability on the output. They assumed that the important contents and their descriptions are determined primar"
W19-8640,W17-5525,0,0.0254598,"tion of target document achieved higher performance in terms of BLEU and human evaluation. We found that human-designed topic labels are superior to extracted keywords in terms of controllability. 1 Introduction Data-to-text is one of the challenging tasks in natural language generation, which aims to generate summaries of input data such as statistics from sports games (Robin, 1995; Barzilay and Lapata, 2005; Wiseman et al., 2017), financial data (Murakami et al., 2017; Aoki et al., 2018), and database records (Reiter and Dale, 1997; Liang et al., 2009; Mei et al., 2016; Lebret et al., 2016; Novikova et al., 2017; Liu et al., 2018; Wiseman et al., 2017). Over the past several years, end-to-end neural language generation models have successfully 323 Proceedings of The 12th International Conference on Natural Language Generation, pages 323–332, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics closed set of domain-specific labels by investigating financial news articles. In the experiments on generating daily summaries of financial markets, we will empirically show the eﬀectiveness of topic labels and potential advantages/disadvantages of this approach. 2 periments sho"
W19-8640,D18-1356,0,0.0237067,"such as tense, sentiment, gender, or automatically learned hidden states (Hu et al., 2017; Zhao et al., 2018; Juraska and Walker, 2018; Bau et al., 2019). Another series of work is focused on controlling surface textual features such as length, descriptiveness and politeness (Li et al., 2016; Sennrich et al., 2016; Kikuchi et al., 2016; Ficler and Goldberg, 2017; Shen et al., 2017; Prabhumoye et al., 2018). The target of these previous methods is on controlling generic contentindependent features of texts. That is, they aim at varying surface strings while preserving main information content. Wiseman et al. (2018) proposed a neural model that generates diverse texts by learning templates. They control diversity through templates rather than contents or the order of them. The present work is more closely related to methods for controlling topical content by using automatically extracted or human-designed keywords (Wang et al., 2016; Yao et al., 2017, 2018; Miao et al., 2018). Our method resembles the idea of using keywords to control topics of sentences and their orders, but it primarily focuses on describing given data and uses topic labels as auxiliary information. We will empirically attest added eﬀe"
W19-8640,P19-1193,0,0.0264327,"e University for Advanced Studies ♡ National Institute of Informatics ♠ Tokyo Institute of Technology ♯ Waseda University ♮ The University of Tokyo {g1120501, koba}@is.ocha.ac.jp miyazawa-a@nii.ac.jp {aoki, ishigaki}@lr.pi.titech.ac.jp hiroshi.noji@aist.go.jp keiichi.goshima@aoni.waseda.jp takamura@pi.titech.ac.jp yusuke@is.s.u-tokyo.ac.jp Abstract been applied to versatile data-to-text tasks, because they can generate fluent texts without task-specific knowledge and resources. However, it has also been pointed out that texts generated by neural models suﬀer from low diversity in expressions (Yang et al., 2019). Especially on the data-to-text tasks, since they are developed under the assumption that the important contents could be uniquely determined, previous methods did not focus on controlling the contents in terms of user’s interests. However, each user may expect diﬀerent contents in a summary depending on what they are interested in, and thus it is appealing to develop a method to generate various summaries which reflect user’s interests. This paper investigates a method for guiding data-to-document generation in the finance domain, by referring to a sequence of additional information for inpu"
W19-8640,P18-1080,0,0.0132777,"more useful as the ease of understanding. Related study 3 Generation of Market Comments Controllability of text generation has been an intensive research focus recently. Examples include suggestive content control such as tense, sentiment, gender, or automatically learned hidden states (Hu et al., 2017; Zhao et al., 2018; Juraska and Walker, 2018; Bau et al., 2019). Another series of work is focused on controlling surface textual features such as length, descriptiveness and politeness (Li et al., 2016; Sennrich et al., 2016; Kikuchi et al., 2016; Ficler and Goldberg, 2017; Shen et al., 2017; Prabhumoye et al., 2018). The target of these previous methods is on controlling generic contentindependent features of texts. That is, they aim at varying surface strings while preserving main information content. Wiseman et al. (2018) proposed a neural model that generates diverse texts by learning templates. They control diversity through templates rather than contents or the order of them. The present work is more closely related to methods for controlling topical content by using automatically extracted or human-designed keywords (Wang et al., 2016; Yao et al., 2017, 2018; Miao et al., 2018). Our method resemble"
W19-8640,D17-1233,0,0.0298286,"and Goldberg, 2017; Shen et al., 2017; Prabhumoye et al., 2018). The target of these previous methods is on controlling generic contentindependent features of texts. That is, they aim at varying surface strings while preserving main information content. Wiseman et al. (2018) proposed a neural model that generates diverse texts by learning templates. They control diversity through templates rather than contents or the order of them. The present work is more closely related to methods for controlling topical content by using automatically extracted or human-designed keywords (Wang et al., 2016; Yao et al., 2017, 2018; Miao et al., 2018). Our method resembles the idea of using keywords to control topics of sentences and their orders, but it primarily focuses on describing given data and uses topic labels as auxiliary information. We will empirically attest added eﬀects of introducing topic labels in the data-to-document scenario. Besides, Gkatzia et al. (2017) and Portet et al. (2009) proposed non-neural language generation models for the data-to-text task with higher controllability on the output. They assumed that the important contents and their descriptions are determined primarily by experts, an"
W19-8640,N16-1005,0,0.0299349,"n with human-designed labels and automatically extracted keywords, humandesigned labels are more useful as the ease of understanding. Related study 3 Generation of Market Comments Controllability of text generation has been an intensive research focus recently. Examples include suggestive content control such as tense, sentiment, gender, or automatically learned hidden states (Hu et al., 2017; Zhao et al., 2018; Juraska and Walker, 2018; Bau et al., 2019). Another series of work is focused on controlling surface textual features such as length, descriptiveness and politeness (Li et al., 2016; Sennrich et al., 2016; Kikuchi et al., 2016; Ficler and Goldberg, 2017; Shen et al., 2017; Prabhumoye et al., 2018). The target of these previous methods is on controlling generic contentindependent features of texts. That is, they aim at varying surface strings while preserving main information content. Wiseman et al. (2018) proposed a neural model that generates diverse texts by learning templates. They control diversity through templates rather than contents or the order of them. The present work is more closely related to methods for controlling topical content by using automatically extracted or human-designe"
W98-0127,P87-1033,0,0.0223494,"Missing"
W98-0127,P98-2132,1,0.604697,"ssigned to a word, and many constituents are produced during parsing. The experimental results show that our method leads to a significant speed-up. The results also suggest the possihility of optimizing the XTAG system by introducing packing offeature structures and packing of tree structures, although these operations are not currently so apparent. 2 The XHPSG System This section describes the current status of the XHPSG system and the efficiency problem in the system. Both of the grammar and the parser in the XHPSG system are implemented with feature structure description language, LiLFeS (Makino et al., 1998). The grammar consists of lexical entries for about 317 ,000 words, and 10 schemata, which follows schemata of the &apos;This work is partially founded by Japan Society for the Promotion of Science (JSPS-RFTF96P00502). 104 twfica5on ol Pad<ed FllSllK9 Sltvetvn&quot; CKYTable Orir;lnLl XllPSG SJ&gt;lcm SJ&gt;ltm w!lh lhe Pacldo& Moowo Figure 1: Data flow in the parsers for the X11PSG system. HPSG framework in (Pollard and Sag, 199~) with slight modifications. The parser is a simpltCKY-based parser. Currently, the parsing speed of this system is not satisfactory, and we need further impro&apos;e· ment of the parsin"
W98-0127,W98-0141,1,0.86765,"Missing"
W98-0127,C96-2160,1,0.340345,"Missing"
W98-0141,P98-2132,1,0.831682,"Missing"
W98-0141,W98-0127,1,0.868547,"Missing"
W98-0141,C96-2160,1,0.844226,"Missing"
W98-0141,C98-2128,1,\N,Missing
Y09-2048,W02-1502,0,0.136567,"; these two structures involve dislocation of phrases from their basic positions which these six schemas require. These structures are covered by our Chinese HPSG framework, and we will introduce the details in the next section. 3 The Design of Chinese HPSG Framework The formalized framework HPSG uses a small number of rule schemas and a large number of lexical entries to describe language. Our basic policy of Chinese HPSG is to exploit rule schemas defined for English with minimum changes. Although a possible solution would be to create an initial grammar with the help of the Grammar Matrix (Bender et al., 2002), we refer to the rule schemas used in an existing HPSG parser (Miyao, 2006), because we intend to apply the technology of this parser to our Chinese parser. This does not only reduce the cost of development of Chinese grammar but also confirm the assumption that, despite surface diversity, human languages share the same organization principles. For example, we do not introduce new rule schemas specific to Chinese unless they are absolutely necessary. We generalize Chinese syntactic structures into five structures based on the Chinese syntactic structure system that we proposed in the previous"
Y09-2048,P05-1022,0,0.0135203,"PSG framework. As the first step of our work, we design a Chinese HPSG framework, which can be used as the basis for a practical parser. In this paper, 1) we present a Chinese syntactic structure system and 2) we design a primary Chinese HPSG framework. Keywords: HPSG, data-driven parsing, Chinese HPSG framework, coverage, consistency. 1 Introduction Data-driven parsing has been proven to be the most effective approach to development of a practical parser. It can deliver a parser with broad-coverage and high-accuracy. Some English data-driven syntactic parsers have been developed in the past (Charniak and Johnson, 2005; McDonald and Pereira, 2006; Miyao and Tsujii, 2005). Following the success of the research on English data-driven parsing, the same methodology has been applied to Chinese parsing (Levy and Manning, 2003; Wang et al., 2005; Guo et al., 2007). The goal of our research is to develop a data-driven Chinese parser that is based on Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003). Since an English data-driven parser based on the HPSG framework has been developed by our group (Miyao and Tsujii, 2005), we follow the same methodology for developing a Chinese parser. We first convert an"
Y09-2048,P03-1056,0,0.0293544,"we design a primary Chinese HPSG framework. Keywords: HPSG, data-driven parsing, Chinese HPSG framework, coverage, consistency. 1 Introduction Data-driven parsing has been proven to be the most effective approach to development of a practical parser. It can deliver a parser with broad-coverage and high-accuracy. Some English data-driven syntactic parsers have been developed in the past (Charniak and Johnson, 2005; McDonald and Pereira, 2006; Miyao and Tsujii, 2005). Following the success of the research on English data-driven parsing, the same methodology has been applied to Chinese parsing (Levy and Manning, 2003; Wang et al., 2005; Guo et al., 2007). The goal of our research is to develop a data-driven Chinese parser that is based on Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003). Since an English data-driven parser based on the HPSG framework has been developed by our group (Miyao and Tsujii, 2005), we follow the same methodology for developing a Chinese parser. We first convert an existing Chinese treebank into an HPSG treebank, based on which we can obtain a large lexicon and a statistical model for choosing the most plausible interpretation. Since the HPSG framework for English ha"
Y09-2048,E06-1011,0,0.0317725,"step of our work, we design a Chinese HPSG framework, which can be used as the basis for a practical parser. In this paper, 1) we present a Chinese syntactic structure system and 2) we design a primary Chinese HPSG framework. Keywords: HPSG, data-driven parsing, Chinese HPSG framework, coverage, consistency. 1 Introduction Data-driven parsing has been proven to be the most effective approach to development of a practical parser. It can deliver a parser with broad-coverage and high-accuracy. Some English data-driven syntactic parsers have been developed in the past (Charniak and Johnson, 2005; McDonald and Pereira, 2006; Miyao and Tsujii, 2005). Following the success of the research on English data-driven parsing, the same methodology has been applied to Chinese parsing (Levy and Manning, 2003; Wang et al., 2005; Guo et al., 2007). The goal of our research is to develop a data-driven Chinese parser that is based on Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003). Since an English data-driven parser based on the HPSG framework has been developed by our group (Miyao and Tsujii, 2005), we follow the same methodology for developing a Chinese parser. We first convert an existing Chinese treebank in"
Y09-2048,P05-1011,1,0.743126,"a Chinese HPSG framework, which can be used as the basis for a practical parser. In this paper, 1) we present a Chinese syntactic structure system and 2) we design a primary Chinese HPSG framework. Keywords: HPSG, data-driven parsing, Chinese HPSG framework, coverage, consistency. 1 Introduction Data-driven parsing has been proven to be the most effective approach to development of a practical parser. It can deliver a parser with broad-coverage and high-accuracy. Some English data-driven syntactic parsers have been developed in the past (Charniak and Johnson, 2005; McDonald and Pereira, 2006; Miyao and Tsujii, 2005). Following the success of the research on English data-driven parsing, the same methodology has been applied to Chinese parsing (Levy and Manning, 2003; Wang et al., 2005; Guo et al., 2007). The goal of our research is to develop a data-driven Chinese parser that is based on Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003). Since an English data-driven parser based on the HPSG framework has been developed by our group (Miyao and Tsujii, 2005), we follow the same methodology for developing a Chinese parser. We first convert an existing Chinese treebank into an HPSG treebank, base"
Y09-2048,W05-1516,0,0.0188136,"nese HPSG framework. Keywords: HPSG, data-driven parsing, Chinese HPSG framework, coverage, consistency. 1 Introduction Data-driven parsing has been proven to be the most effective approach to development of a practical parser. It can deliver a parser with broad-coverage and high-accuracy. Some English data-driven syntactic parsers have been developed in the past (Charniak and Johnson, 2005; McDonald and Pereira, 2006; Miyao and Tsujii, 2005). Following the success of the research on English data-driven parsing, the same methodology has been applied to Chinese parsing (Levy and Manning, 2003; Wang et al., 2005; Guo et al., 2007). The goal of our research is to develop a data-driven Chinese parser that is based on Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003). Since an English data-driven parser based on the HPSG framework has been developed by our group (Miyao and Tsujii, 2005), we follow the same methodology for developing a Chinese parser. We first convert an existing Chinese treebank into an HPSG treebank, based on which we can obtain a large lexicon and a statistical model for choosing the most plausible interpretation. Since the HPSG framework for English has been studied comp"
Y10-1055,C04-1180,0,0.198739,"conversion program can fill this gap, and consequently we achieve a wide-coverage system for translating unrestricted natural language texts into predicate logic formulas. 481 482 Poster Papers A notable advantage of the modular architecture is that system development is comparatively easy. As we prove in the experiments, a compact grammar of SCT and a small program for parsed syntactic structure conversion can achieve high coverage on real-world texts. This attests the portability of our method to other languages, as well as to other forms of semantic representation. To our knowledge, Boxer (Bos et al., 2004; Curran et al., 2007) is the only alternative to our system that translates unrestricted texts into logical formulas (Discourse Representation Structures) with resolved intra and inter argument dependencies and anaphoric dependencies. We discuss the relationship with Boxer in Section 6 and note some added benefits of our approach. 2 Scope Control Theory Scope Control Theory or SCT (Butler, 2010) is a system of semantic evaluation that attempts to approximate the dependency restrictions of natural language by a fine-grained and restricted management of binding dependencies, following insights"
Y10-1055,P05-1022,0,0.0465809,"Missing"
Y10-1055,P04-1014,0,0.0364764,"Missing"
Y10-1055,P07-2009,0,0.143156,"can fill this gap, and consequently we achieve a wide-coverage system for translating unrestricted natural language texts into predicate logic formulas. 481 482 Poster Papers A notable advantage of the modular architecture is that system development is comparatively easy. As we prove in the experiments, a compact grammar of SCT and a small program for parsed syntactic structure conversion can achieve high coverage on real-world texts. This attests the portability of our method to other languages, as well as to other forms of semantic representation. To our knowledge, Boxer (Bos et al., 2004; Curran et al., 2007) is the only alternative to our system that translates unrestricted texts into logical formulas (Discourse Representation Structures) with resolved intra and inter argument dependencies and anaphoric dependencies. We discuss the relationship with Boxer in Section 6 and note some added benefits of our approach. 2 Scope Control Theory Scope Control Theory or SCT (Butler, 2010) is a system of semantic evaluation that attempts to approximate the dependency restrictions of natural language by a fine-grained and restricted management of binding dependencies, following insights from static reformulat"
Y10-1055,E06-1011,0,0.0490435,"Missing"
Y10-1055,J08-1002,1,0.920856,"at is extra to a conventional parsed form. Notably the co-indexing of syntactic constituents is rendered unnecessary. Moreover sentence information and discourse information are dealt with equally at the time of evaluation as providing binding information, which allows for their seamless integration. The language formalism of SCT is supplemented by a compact grammar to determine the contribution of morphosyntactic information. This offers flexibility in what is acceptable as input, so there is no need to develop a SCT-specific parser from scratch. Here we adopt an HPSG-based syntactic parser (Miyao and Tsujii, 2008), because it offers wide coverage and high accuracy, and provides detailed syntactic information that is sufficient for SCT. Although the output of the parser is not directly compatible with the input assumed by SCT, a small conversion program can fill this gap, and consequently we achieve a wide-coverage system for translating unrestricted natural language texts into predicate logic formulas. 481 482 Poster Papers A notable advantage of the modular architecture is that system development is comparatively easy. As we prove in the experiments, a compact grammar of SCT and a small program for pa"
Y10-1055,P05-1013,0,0.0922836,"Missing"
Y10-1055,J93-2004,0,\N,Missing
Y13-1026,W10-1736,1,0.929613,"dering local permutations of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing errors, but insight into their relationship has been elusive. The contribution of thi"
Y13-1026,D11-1017,0,0.019774,"pes of errors. One most relevant work to ours is observing the impact of parsing accuracy on a SMT system introduced in Quirk and Corston-Oliver (2006). They showed the general idea that syntax-based SMT models are sensitive to syntactic analysis. However, they did not further analyze concrete parsing error types that affect task accuracy. Green (2011) explored the effects of noun phrase bracketing in dependency parsing in English, and further on English to Czech machine translation. But the work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing error patterns on predicate argument relation and tested them with a Headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) parser (Miyao and Tsujii, 2008). McDonald and Nivre (2007) explored parsing errors for data-driven dependency parsing by 1 In this work, P"
Y13-1026,W08-0509,0,0.0238615,"of CTB-7 which is split according to (Wang et al., 2011). However, we found that sentences in BC and NW are mainly from spoken language, which tend to have faults like repetitions, incomplete sentences, corrections, or incorrect sentence segmentation. Therefore, we randomly selected another 2, 126 unique sentences 269 PACLIC-27 set-1 set-2 Total AL Voc. BN 100 797 897 29.8 5.5K BC 100 100 20.0 690 NM 100 578 678 33.5 5K NS 117 751 868 28.4 5.1K NW 100 100 25.9 972 Total 517 2, 126 2, 643 29.8 9.5K M-reordered Gold-DPC Auto-DPC 0.88 0.95 nese sentences. Word alignments are produced by MGIZA++ (Gao and Vogel, 2008). In both scenarios, we carry out the reordering method DPC (See Section 2.1). Auto-parse trees are generated by an unlabeled Chinese dependency parser, Corbit4 (Hatori et al., 2011). Gold trees5 are converted from CTB-7 parsed text which are created by human annotators. More specifically, we refer to auto-parse tree based reordering system as Auto-DPC and to gold-tree based reordering system as Gold-DPC. Baseline system uses unreordered Chinese sentences. Scenario 1 Preliminary observation about the effects of parsing errors on reordering performance is to compare word order similarities betw"
Y13-1026,W00-1303,0,0.0484713,"PC is limited to dependency structure and POS tags, for analysis on the causes of reordering errors, we examine parsing errors from these two linguistic categories. In this section, the value of Kendall’s tau measures the word order similarity between Gold-DPC and Auto-DPC. Figure 2: The distribution of Kendall’s tau values for 2, 236 bilingual sentences (Chinese-Japanese) in which the Chinese is from three systems of baseline, Auto-DPC, and Gold-DPC. file, ch-ja.A3.final. The comparison implies how monotonically the Chinese sentences have been reordered to align with Japanese. We use MeCab6 (Kudo and Matsumoto, 2000) to segment Japanese sentences and also filter out sentences with more than 64 tokens. There are 2, 236 valid Chinese-Japanese bilingual sentences in total. Figure 2 shows the distribution of Kendall’s tau from three systems in which the baseline is built up by using ordinary Chinese. In Figure 2, baseline system contains a large numbers of non-monotonic aligned sentences, whereas both Auto-DPC and Gold-DPC increased the amount of sentences that achieved high τ values. Reordering based on gold-tree reduced more percentage of low τ sentences than reordering based on automatically parsed trees."
Y13-1026,C10-1043,0,0.040679,"ord correspondence due to the combinatorial complexity. Considering local permutations of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing errors, but insig"
Y13-1026,D07-1013,0,0.0632143,"noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing error patterns on predicate argument relation and tested them with a Headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) parser (Miyao and Tsujii, 2008). McDonald and Nivre (2007) explored parsing errors for data-driven dependency parsing by 1 In this work, POS tag definitions follow the POS tag guidelines of the Penn Chinese Treebank v3.0. 2 According to (Han et al., 2013), a Vb includes the head of the Vb (Vb-H) and an optional component (Vb-D). 268 PACLIC-27 influence reordering. Meanwhile, the former measurement shows additionally the general figure of the upper bound of the reordering method. However, since it is not only time-consuming but also labor-intensive to set up the benchmark in scenario 1, we use the Japanese reference as the benchmark in scenario 2 and"
Y13-1026,gimenez-marquez-2008-towards,0,0.0609847,"Missing"
Y13-1026,J08-1002,1,0.759309,"he work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing error patterns on predicate argument relation and tested them with a Headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) parser (Miyao and Tsujii, 2008). McDonald and Nivre (2007) explored parsing errors for data-driven dependency parsing by 1 In this work, POS tag definitions follow the POS tag guidelines of the Penn Chinese Treebank v3.0. 2 According to (Han et al., 2013), a Vb includes the head of the Vb (Vb-H) and an optional component (Vb-D). 268 PACLIC-27 influence reordering. Meanwhile, the former measurement shows additionally the general figure of the upper bound of the reordering method. However, since it is not only time-consuming but also labor-intensive to set up the benchmark in scenario 1, we use the Japanese reference as the b"
Y13-1026,P11-3013,0,0.0255294,"ependency parsing as well as its extensibility to other language pairs. 2.2 Related Work Although there are studies on analyzing parsing errors and reordering errors, as far as we know, there is not any work on observing the relationship between these two types of errors. One most relevant work to ours is observing the impact of parsing accuracy on a SMT system introduced in Quirk and Corston-Oliver (2006). They showed the general idea that syntax-based SMT models are sensitive to syntactic analysis. However, they did not further analyze concrete parsing error types that affect task accuracy. Green (2011) explored the effects of noun phrase bracketing in dependency parsing in English, and further on English to Czech machine translation. But the work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing erro"
Y13-1026,W06-1608,0,0.178502,"of parsing errors on reordering performance. In this analysis, we borrow this state-of-the-art pre-reordering model for our experiments since it is a rule-based pre-reordering method for a distant language pair based on dependency parsing as well as its extensibility to other language pairs. 2.2 Related Work Although there are studies on analyzing parsing errors and reordering errors, as far as we know, there is not any work on observing the relationship between these two types of errors. One most relevant work to ours is observing the impact of parsing accuracy on a SMT system introduced in Quirk and Corston-Oliver (2006). They showed the general idea that syntax-based SMT models are sensitive to syntactic analysis. However, they did not further analyze concrete parsing error types that affect task accuracy. Green (2011) explored the effects of noun phrase bracketing in dependency parsing in English, and further on English to Czech machine translation. But the work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. B"
Y13-1026,W12-4207,1,0.931686,"ons of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing errors, but insight into their relationship has been elusive. The contribution of this work is two fold"
Y13-1026,I11-1035,0,0.0178476,"nce separately, we quantify the extent of parsing errors that 4 4.1 Preliminary Experiment Gold Data In order to build up gold parse tree sets for comparison, we used the annotated sentences from Chinese Penn Treebank ver. 7.0 (CTB-7) which is a well known corpus that consists of parsed text in five genres. They are Chinese newswire (NS), magazine news (NM), broadcast news (BN), broadcast conversation programs (BC), and web newsgroups, weblogs (NW). We first randomly selected 517 unique sentences (hereinafter set-1) from all five genres in development set of CTB-7 which is split according to (Wang et al., 2011). However, we found that sentences in BC and NW are mainly from spoken language, which tend to have faults like repetitions, incomplete sentences, corrections, or incorrect sentence segmentation. Therefore, we randomly selected another 2, 126 unique sentences 269 PACLIC-27 set-1 set-2 Total AL Voc. BN 100 797 897 29.8 5.5K BC 100 100 20.0 690 NM 100 578 678 33.5 5K NS 117 751 868 28.4 5.1K NW 100 100 25.9 972 Total 517 2, 126 2, 643 29.8 9.5K M-reordered Gold-DPC Auto-DPC 0.88 0.95 nese sentences. Word alignments are produced by MGIZA++ (Gao and Vogel, 2008). In both scenarios, we carry out th"
Y13-1026,W13-2806,1,0.816096,"Missing"
Y13-1026,C04-1073,0,0.102245,"plore every possible word correspondence due to the combinatorial complexity. Considering local permutations of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing er"
Y13-1026,N09-1028,0,0.0647854,"complexity. Considering local permutations of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing errors, but insight into their relationship has been elusive. T"
Y13-1026,D09-1121,1,0.844566,"sing error types that affect task accuracy. Green (2011) explored the effects of noun phrase bracketing in dependency parsing in English, and further on English to Czech machine translation. But the work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing error patterns on predicate argument relation and tested them with a Headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) parser (Miyao and Tsujii, 2008). McDonald and Nivre (2007) explored parsing errors for data-driven dependency parsing by 1 In this work, POS tag definitions follow the POS tag guidelines of the Penn Chinese Treebank v3.0. 2 According to (Han et al., 2013), a Vb includes the head of the Vb (Vb-H) and an optional component (Vb-D). 268 PACLIC-27 influence reordering. Meanwhile, the former measurement shows additionally the genera"
Y13-1026,W11-2907,1,0.848779,"ubclasses according to the methodology of the reordering method. We then plot the distribution of these parsing errors for various reordering qualities. In Section 5.2, we illustrate these parsing errors with examples. comparing a graph-based parser with a transitionbased parser, which are representing two dominant parsing models. At the same time, Dredze et al. (2007) provided a comparison analysis on differences in annotation guidelines among treebanks which were suspected to be responsible for dependency parsing errors in domain adaptation tasks. Unlike analyzing parsing errors, authors in Yu et al. (2011) focused on the difficulties in Chinese deep parsing by comparing the linguistic properties between Chinese and English. There are also works on reordering error analysis like Han et al. (2012) which examined an existing reordering method and refined it after a detailed linguistic analysis on reordering issues. Although they discovered that parsing errors affect the reordering quality, they did not observe the concrete relationship. On the other hand, Gim´enez and M`arquez (2008) proposed an automatic error analysis method of machine translation output, by compiling a set of metric variants. H"
Y13-1026,I11-1136,1,0.828769,"ncomplete sentences, corrections, or incorrect sentence segmentation. Therefore, we randomly selected another 2, 126 unique sentences 269 PACLIC-27 set-1 set-2 Total AL Voc. BN 100 797 897 29.8 5.5K BC 100 100 20.0 690 NM 100 578 678 33.5 5K NS 117 751 868 28.4 5.1K NW 100 100 25.9 972 Total 517 2, 126 2, 643 29.8 9.5K M-reordered Gold-DPC Auto-DPC 0.88 0.95 nese sentences. Word alignments are produced by MGIZA++ (Gao and Vogel, 2008). In both scenarios, we carry out the reordering method DPC (See Section 2.1). Auto-parse trees are generated by an unlabeled Chinese dependency parser, Corbit4 (Hatori et al., 2011). Gold trees5 are converted from CTB-7 parsed text which are created by human annotators. More specifically, we refer to auto-parse tree based reordering system as Auto-DPC and to gold-tree based reordering system as Gold-DPC. Baseline system uses unreordered Chinese sentences. Scenario 1 Preliminary observation about the effects of parsing errors on reordering performance is to compare word order similarities between manually reordered Chinese sentences and automatically reordered Chinese sentences from set-1. Table 3 shows the average τ value. For baseline system, the average τ value shows h"
Y13-1026,D07-1112,0,\N,Missing
Y14-1067,de-marneffe-etal-2006-generating,0,0.0612038,"Missing"
Y14-1067,Q13-1015,0,0.239652,"nvolved in this paper. Although it has been recognized that it is important to encode GQs for solving textual entailment problems, this remains a big challenge. MacCartney et al. (2006), for example, tried to capture the use of GQs in feature vectors, but the capabilities of which are greatly limited without an inference engine. Even for systems that are backed by inference engines like in this paper, the focus still needs to be put on practical NLP rather than logic, linguistics, or semantic theory, and model complexity may need to be purposely traded for computation efficiency. For example, Lewis and Steedman (2013) used firstorder logic for semantic representation, which is theoretically very expressive, but still unable to define GQs without some extensions (Barwise and Cooper, 1981) that are nontrivial especially for practical inference. Some works made the compromise similar to ours: only encode the important properties of GQs rather than their perfect semantics. A notable recent work that focused on monotonicity is MacCartney and Manning (2008), in which the notion of monotonicity was generalized to support recursive determination of entailments of a compound expression from its constituents. To a l"
Y14-1067,P11-1060,0,0.23263,"Missing"
Y14-1067,J13-2005,0,0.368902,"inference framework provided decent support for both quantifiers all (universal quantifier) and no (negated existential quantifier), attention is required for an RTE system to cope with generalized quantifiers (GQ), including “at most n”, “at least n”, “most”, etc., which can affect the direction or even the existence of an entailment relation, as demonstrated in Examples 1 to 3. ⇤ This work was conducted during an internship at the National Institute of Informatics, Japan. In this paper, we explore ways of encoding GQs in a recent framework of Dependency-based Compositional Semantics (DCS) (Liang et al., 2013; Tian et al., 2014a), especially aiming to correctly handle linguistic knowledge like hyponymy when GQs are involved. We use selection operators, an extension mechanism described in Tian et al. (2014a), to implement a sub-type of GQs (Section 3.1). To deal with downward monotonicity of the predicate argument, we also propose a simple extension called “relation” to the framework (Section 3.2). This approach does not encode the exact semantics of every specific GQ, but instead captures some major properties that are both easily implementable with the current technology and useful in many cases."
Y14-1067,W07-1431,0,0.46901,"perties can be implemented as composable and reusable units9 , so that each GQ can be created by simply composing the units that corresponds to the properties it has. This makes implementing new GQs very easy. TIFMO uses the Stanford Parser10 to obtain Stanford dependencies (de Marneffe et al., 2006) and POS tags, which are used to construct DCS trees based on a set of pre-defined rules. We extend those rules in order to recognize GQs in this step, and encode them under one of four settings, namely “Baseline”, “Selection”, “Relation”, and “Selec5 We used the version converted to XML format by MacCartney and Manning (2007). 6 6 problems that do not have a defined solution are excluded. 7 FraCaS dubiously interpreted “many” as denoting “a large proportion” rather than “a large absolute number”, whereas “few” as denoting “a small absolute number” rather than “a small proportion”. We also treat “a lot of” as a synonym of “many”. 8 http://kmcs.nii.ac.jp/tifmo/ 9 We implement GQ properties as stackable traits in Scala (Odersky et al., 2011), each consists of no more than a few dozen lines of code. 10 http://nlp.stanford.edu/software/ lex-parser.shtml GQ Entailed by 8 Entails 9 many a lot of few a few most at most n"
Y14-1067,C08-1066,0,0.767291,"n practical NLP rather than logic, linguistics, or semantic theory, and model complexity may need to be purposely traded for computation efficiency. For example, Lewis and Steedman (2013) used firstorder logic for semantic representation, which is theoretically very expressive, but still unable to define GQs without some extensions (Barwise and Cooper, 1981) that are nontrivial especially for practical inference. Some works made the compromise similar to ours: only encode the important properties of GQs rather than their perfect semantics. A notable recent work that focused on monotonicity is MacCartney and Manning (2008), in which the notion of monotonicity was generalized to support recursive determination of entailments of a compound expression from its constituents. To a large extent, this approach handled the interaction between multiple GQs in a single sentence. However, inference was based on a chain of shallow syntactic edit operations linking premise to hypothesis, which not only failed to include various inference patterns, but also was unreliable when there are multiple sentences in the premise, or when the premise is relatively long. The DCS inference framework, on the other hand, gracefully handle"
Y14-1067,N06-1006,0,0.0635846,"Missing"
Y14-1067,P14-1008,1,0.630722,"Missing"
Y14-1067,W14-2414,1,0.879303,"Missing"
Y15-1058,D12-1050,0,0.0153921,"fficials told key senators a week ago to expect a terrorist attack in Saudi Arabia, Sen. Pat Roberts (R-Kan.) said yesterday. Intelligence officials in Washington warned lawmakers a week ago to expect a terrorist attack in Saudi Arabia, it was reported today. Figure 1: Example paraphrase pair taken from MSRP corpus. 2 2.1 Related work Paraphrase detection The baseline for paraphrase detection is based on vector-based similarity. Each source message and target message is represented as a vector using the frequencies of its words (such as term frequency (Mihalcea et al., 2006) and cooccurrence (Blacoe and Lapata, 2012)). The similarity of the two vectors is quantified using various measures (e.g., cosine (Mihalcea et al., 2006), addition and point-wise multiplication (Blacoe and Lapata, 2012)). The problem with vector-based methods is to focus on the frequency of separate words or phrases. However, plagiarists can paraphrase by replacing words with similar words that have a very different frequency. Moreover, they can delete and/or insert minor words that do not change the meaning of the original sentences. Such manipulations change the quality of the representation vector, which reduces paraphrase detectio"
Y15-1058,P08-1007,0,0.134276,"sive autoencoders (Socher et al., 2011), heuristic similarity (Qiu et al., 2006), and probabilistic inference (Das and Smith, 2009). However, these algorithms are affected by manipulation (deleting, inserting, reordering, etc.) of the words in the sentences. Such manipulations can significantly change the structures of the parsing trees. 504 29th Pacific Asia Conference on Language, Information and Computation pages 504 - 512 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Hoang-Quoc Nguyen-Son, Yusuke Miyao and Isao Echizen PACLIC 29 Other researchers (Mihalcea et al., 2006; Chan and Ng, 2008) have used matching algorithms to determine the similarity of two sentences. Mihalcea et al. (2006), for example, proposed a method for finding the best matching of a word in a sentence with the nearest word in the other sentence. However, word meaning is often contextual (e.g., ‘make sure of,’ ‘take care of’). Machine translation (MT) metrics, which are generally used to evaluate the quality of translated text, can also be used to judge two texts in the same language. Due to the similarity of machine translation and paraphrase detection, many MT metrics have been applied to paraphrase detecti"
Y15-1058,P09-1053,0,0.0192898,"ists attempt to thwart this comparison by modifying the copied sentence by inserting or removing a few minor words, replacing words with similar words that have different usage frequencies, etc. Such modification reduces the effectiveness of vector-based similarity analysis. Introduction Paraphrase detection is used to determine whether two texts (phrases, sentences, paragraphs, documents, etc.) of arbitrary lengths have the same Other researchers have analyzed the difference in meaning between two sentences on the basis of their syntactic parsing trees (Socher et al., 2011; Qiu et al., 2006; Das and Smith, 2009). The structure of the trees is a major factor used various sophisticated algorithms such as recursive autoencoders (Socher et al., 2011), heuristic similarity (Qiu et al., 2006), and probabilistic inference (Das and Smith, 2009). However, these algorithms are affected by manipulation (deleting, inserting, reordering, etc.) of the words in the sentences. Such manipulations can significantly change the structures of the parsing trees. 504 29th Pacific Asia Conference on Language, Information and Computation pages 504 - 512 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Hoang-Q"
Y15-1058,N10-1031,0,0.10444,"2) and the NIST metric (Doddington, 2002) (an extension of the BLEU metric). Both also quantify similarity on the basis of matching words in the original text segment with words in the translated segment. Whereas the BLEU metric simply calculates the number of matching words, the NIST metric takes into account the importance of matching with different levels. The main drawback of these word matching metrics is that a word in a segment can match more than one word in the other segment. Two MT metrics based on non-duplicate matching have been devised to overcome this problem. The METEOR metric (Denkowski and Lavie, 2010) uses explicit ordering to identify matching tuples with minimized cross edges. However, it simply performs word-by-word matching. The maximum similarity (MAXSIM) metric (Chan and Ng, 2008) finds the maximum matching of unigram, bigram, and trigram words by using the Kuhn-Munkres algorithm. However, the maximum length of the phrase is a trigram. Moreover, the similarities of the phrases (unigram, bigram, and trigram) are disjointly combined. To overcome these drawbacks with the standard MT metrics, we have developed a heuristic method for finding the maximum of matching tuples up to the length"
Y15-1058,C04-1051,0,0.0877637,"e-art paraphrase detection approach (Madnani et al., 2012). However, the objectives of machine translation and paraphrase detection differ: machine translation tries to effectively translate text from one language to another while paraphrase detection tries to identify paraphrased text. This difference affects the application of MT metrics to paraphrase detection. A paraphrase is a restatement of the meaning of a text using other words. It is a specific type of plagiarisms. We identify common practices plagiarizers who try to paraphrase a text. The Microsoft Research Paraphrase (MSRP) corpus (Dolan et al., 2004) is commonly used to identify the common practices. An example paraphrase pair extracted from this corpus is shown in Figure 1. Plagiarists frequently cut and paste several phrases of different lengths. This can result in a sentence pair containing identical phrases. The two sentences in Figure 1 have two identical phrases: “Intelligence officials” and “a week ago to expect a terrorist attack in Saudi Arabia.” Plagiarists also add and delete minor words to improve the naturalness of the text. In the example pair, the preposition “in” (in bold) in the second sentence is considered a minor word."
Y15-1058,I05-5003,0,0.0376945,"e used matching algorithms to determine the similarity of two sentences. Mihalcea et al. (2006), for example, proposed a method for finding the best matching of a word in a sentence with the nearest word in the other sentence. However, word meaning is often contextual (e.g., ‘make sure of,’ ‘take care of’). Machine translation (MT) metrics, which are generally used to evaluate the quality of translated text, can also be used to judge two texts in the same language. Due to the similarity of machine translation and paraphrase detection, many MT metrics have been applied to paraphrase detection (Finch et al., 2005; Madnani et al., 2012). For example, eight standard MT metrics have been combined to create a state-of-the-art paraphrase detection approach (Madnani et al., 2012). However, the objectives of machine translation and paraphrase detection differ: machine translation tries to effectively translate text from one language to another while paraphrase detection tries to identify paraphrased text. This difference affects the application of MT metrics to paraphrase detection. A paraphrase is a restatement of the meaning of a text using other words. It is a specific type of plagiarisms. We identify com"
Y15-1058,N12-1019,0,0.172049,"rithms to determine the similarity of two sentences. Mihalcea et al. (2006), for example, proposed a method for finding the best matching of a word in a sentence with the nearest word in the other sentence. However, word meaning is often contextual (e.g., ‘make sure of,’ ‘take care of’). Machine translation (MT) metrics, which are generally used to evaluate the quality of translated text, can also be used to judge two texts in the same language. Due to the similarity of machine translation and paraphrase detection, many MT metrics have been applied to paraphrase detection (Finch et al., 2005; Madnani et al., 2012). For example, eight standard MT metrics have been combined to create a state-of-the-art paraphrase detection approach (Madnani et al., 2012). However, the objectives of machine translation and paraphrase detection differ: machine translation tries to effectively translate text from one language to another while paraphrase detection tries to identify paraphrased text. This difference affects the application of MT metrics to paraphrase detection. A paraphrase is a restatement of the meaning of a text using other words. It is a specific type of plagiarisms. We identify common practices plagiariz"
Y15-1058,P14-5010,0,0.00346909,"Lem:the study be be publish today in the journal science Lem:they find be publish today in s2: Their findings were published today in science . Science . Figure 3: Matching identical phrases with their maximum lengths (Step 1). sentences, which is an actual paraphrase pair from the MSRP corpus. s1 : “The study is being published today in the journal Science” s2 : “Their findings were published today in Science.” 3.1 Match identical phrases (Step 1) The individual words in the two input sentences are normalized using lemmas. The Natural Language Processing (NLP) library of Stanford University (Manning et al., 2014) is used to identify the lemmas. The lemmas for the two example sentences are shown in Figure 3. The heuristic algorithm we developed for matching the lemmas in the two sentences repeatedly finds a new matching pair in each round. In each round, a new pair with the maximum phrase length is established. The pseudo code of the algorithm is illustrated in Algorithm 1. The stop condition is when there is no new matching pair. For example, two identical lemma of phrases, “be publish today in” and “science,” are matched (as shown as Figure 3). In algorithm 1, the function getLemmas(s) extracts the l"
Y15-1058,P02-1040,0,0.0999028,"istance represents for probability that one segment is a paraphrase of the other. The SEPIA MT metric (Habash and Elkholy, 2008) is based on the dependence tree and is used to calculate the similarity of two text segments. It extends the tree to obtain the surface span, which is used as the main component of the similarity score. After the components of the tree are matched, a brevity penalty factor is suggested for deciding the difference in tree lengths for the two text segments. Two other MT metrics commonly used in machine translation are the bilingual evaluation understudy (BLEU) metric (Papineni et al., 2002) and the NIST metric (Doddington, 2002) (an extension of the BLEU metric). Both also quantify similarity on the basis of matching words in the original text segment with words in the translated segment. Whereas the BLEU metric simply calculates the number of matching words, the NIST metric takes into account the importance of matching with different levels. The main drawback of these word matching metrics is that a word in a segment can match more than one word in the other segment. Two MT metrics based on non-duplicate matching have been devised to overcome this problem. The METEOR metric (De"
Y15-1058,W06-1603,0,0.0345598,"imilarity. Plagiarists attempt to thwart this comparison by modifying the copied sentence by inserting or removing a few minor words, replacing words with similar words that have different usage frequencies, etc. Such modification reduces the effectiveness of vector-based similarity analysis. Introduction Paraphrase detection is used to determine whether two texts (phrases, sentences, paragraphs, documents, etc.) of arbitrary lengths have the same Other researchers have analyzed the difference in meaning between two sentences on the basis of their syntactic parsing trees (Socher et al., 2011; Qiu et al., 2006; Das and Smith, 2009). The structure of the trees is a major factor used various sophisticated algorithms such as recursive autoencoders (Socher et al., 2011), heuristic similarity (Qiu et al., 2006), and probabilistic inference (Das and Smith, 2009). However, these algorithms are affected by manipulation (deleting, inserting, reordering, etc.) of the words in the sentences. Such manipulations can significantly change the structures of the parsing trees. 504 29th Pacific Asia Conference on Language, Information and Computation pages 504 - 512 Shanghai, China, October 30 - November 1, 2015 Cop"
Y15-1058,2006.amta-papers.25,0,0.0478425,"n the integration of eight metrics (TER, TERp, BADGER, SEPIA, BLEU, NIST, METEOR, and MAXSIM). However, the main purpose of these metrics is for translating, and their integration is unsuitable for detecting paraphrases. To overcome these weaknesses, we developed a similarity metric and combined it with eight standard metrics, as described below. 2.2 Standard MT metrics Two basic MT metrics for measuring the similarity of two text segments are based on finding the minimum number of operators needed to change one segment so that it matches the other one. The translation edit rate (TER) metric (Snover et al., 2006) supports standard operators, including shift, substitution, deletion, and insertion. The TER-Plus (TERp) metric (Snover et al., 2009) supports even more operators, including stemming and synonymizing. The BADGER MT metric (Parker, 2008) uses compression and information theory. It is used to calculate the compression distance of two text segments by using Burrows-Wheeler transformation. 1 Fig 3: matching identical phrases (updated) PACLIC 29 This distance represents for probability that one segment is a paraphrase of the other. The SEPIA MT metric (Habash and Elkholy, 2008) is based on the dep"
