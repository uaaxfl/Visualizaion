2020.lrec-1.189,P15-2127,0,0.0230599,"effect on how emotions are conveyed. In text-based communication, for example, there are various mediums through which people might express their emotions, such as social media, personal diaries, news articles, blogs. Basic emotions are usually classified into classes such as anger, disgust, fear, joy, sadness, surprise using established emotion annotation schemes (Ekman, 1992; Shaver et al., 1987; Oatley and Johnson-Laird, 1987). While most research on emotion analysis focuses on emotion classification, including predicting the emotions of the writer as well as those of the reader of a text (Chang et al., 2015), a few studies so far have focused on identifying what might have triggered that emotion (Sailunaz et al., 2018). In particular, the task has been framed as Emotion Cause Extraction (Lee et al., 2010a) from news and microblogs, where the cause of an emotion is usually a single clause (Chen et al., 2010) connected by a discourse relation to another clause that explicitly expresses a given emotion (Cheng et al., 2017), as in this example from Gui et al. (2016a): “< cause > Talking about his honours, < /cause > Mr. Zhu is so < emotion > proud < /emotion >.” However, if we move to other text genr"
2020.lrec-1.189,C10-1021,0,0.155049,"dness, surprise using established emotion annotation schemes (Ekman, 1992; Shaver et al., 1987; Oatley and Johnson-Laird, 1987). While most research on emotion analysis focuses on emotion classification, including predicting the emotions of the writer as well as those of the reader of a text (Chang et al., 2015), a few studies so far have focused on identifying what might have triggered that emotion (Sailunaz et al., 2018). In particular, the task has been framed as Emotion Cause Extraction (Lee et al., 2010a) from news and microblogs, where the cause of an emotion is usually a single clause (Chen et al., 2010) connected by a discourse relation to another clause that explicitly expresses a given emotion (Cheng et al., 2017), as in this example from Gui et al. (2016a): “< cause > Talking about his honours, < /cause > Mr. Zhu is so < emotion > proud < /emotion >.” However, if we move to other text genres beyond news and microblogs, it might be more complicated to identify the causes of given emotions from the text. In this work, we focus on spoken personal narratives. A Personal Narrative (PN) can be defined as a recollection of an event or connected sequence of events the narrator has been part of, a"
2020.lrec-1.189,esuli-sebastiani-2006-sentiwordnet,0,0.158497,"Missing"
2020.lrec-1.189,N19-1052,0,0.0263128,"part of a PN annotated with emotion carriers. Inspired by such evidence, in this work, we investigate the possibility of annotating emotion carriers in spoken PNs. This type of annotation could then be used to experiment with training automatic Emotion Carriers Extraction systems. Such models, together with emotion prediction models, could provide a deeper understanding of the emotional state of the narrator. The task of emotion carriers extraction can be classified as a task of Automatic Narrative Understanding (ANU), which encompasses tasks that extract various information from narratives (Fu et al., 2019). Emotion carriers extraction, for example, could be a useful task for conversational mental healthcare applications. Applications aimed at the mental well-being of the users, often col1517 German (original) English (translated) Okay. Ähm also eine Situation, in der ich mich kompetent gefühlt hab, war, als ich meinen ähm Praktikumsplatz OK. Um, so a situation in which I felt competent, was when I got my um internship position and er in bekommen hab und ähm ich in dem Praktikum dann auch ähm Sachen selbstständig machen durfte und auch die Rückmeldung bekommen hab von den Personen dort, dass das"
2020.lrec-1.189,D16-1170,0,0.0415947,"Missing"
2020.lrec-1.189,D17-1167,0,0.041701,"Missing"
2020.lrec-1.189,W10-2910,0,0.0102152,"2014). The Equation 1 defines the positive agreement in terms of true positives (TP), false positives (FP) and false negatives (FN). We can see that the knowledge of true negatives (TN) is not required for the calculation of the positive agreement. Also, notice that the equation is similar to the widely used F1 -measure (Hripcsak and Rothschild, 2005). In our experiments, we calculate the positive agreement for each pair of annotators. Ppos = 2 × TP 2 × TP + FP + FN which is important to be considered. Thus, we report results on exact matches as well as partial matches, following the work by (Johansson and Moschitti, 2010). For the partial match, they calculate “soft” F1 -measure by calculating the coverage of the hypothesis spans. The coverage of a span(s) with respect to another span (s0 ) is calculated as defined in Equation 2, with the help of the number of tokens common in the two spans. The operator|. |counts the number of tokens. c(s, s0 ) = (2) Next, a span set coverage C is defined for a set of spans S with respect to another set of spans S 0 using the Equation 3. X X C(S, S 0 ) = c(si , s0j ) (3) si ∈S sj ∈S 0 In order to calculate the soft F1 -measure, first soft precision and soft recall are calcula"
2020.lrec-1.189,W10-0206,0,0.0731237,"Missing"
2020.lrec-1.189,lee-etal-2010-emotion,0,0.130454,"ticles, blogs. Basic emotions are usually classified into classes such as anger, disgust, fear, joy, sadness, surprise using established emotion annotation schemes (Ekman, 1992; Shaver et al., 1987; Oatley and Johnson-Laird, 1987). While most research on emotion analysis focuses on emotion classification, including predicting the emotions of the writer as well as those of the reader of a text (Chang et al., 2015), a few studies so far have focused on identifying what might have triggered that emotion (Sailunaz et al., 2018). In particular, the task has been framed as Emotion Cause Extraction (Lee et al., 2010a) from news and microblogs, where the cause of an emotion is usually a single clause (Chen et al., 2010) connected by a discourse relation to another clause that explicitly expresses a given emotion (Cheng et al., 2017), as in this example from Gui et al. (2016a): “< cause > Talking about his honours, < /cause > Mr. Zhu is so < emotion > proud < /emotion >.” However, if we move to other text genres beyond news and microblogs, it might be more complicated to identify the causes of given emotions from the text. In this work, we focus on spoken personal narratives. A Personal Narrative (PN) can"
2020.lrec-1.189,D18-1506,0,0.0641221,"Missing"
2020.lrec-1.189,I13-1121,0,0.024399,"news reports, frames from FrameNet (Tokuhisa et al., 2008; Ghazi et al., 2015; Lee et al., 2010a; Gui et al., 2016a) and informal texts such as microblogs (Gui et al., 2014; Gao et al., 2015b; Cheng et al., 2017). Some of these are annotated manually, while some are created automatically. While Lee et al. (2010a) defined the task of ECE as the extraction of word-level emotion causes, Chen et al. (2010) suggested that a clause would be a more appropriate choice of unit for the extraction of an emotion cause. There have been works trying to solve the problem using different methods: Rule-based (Neviarouskaya and Aono, 2013; Li and Xu, 2014; Gao et al., 2015b; Gao et al., 2015a; Yada et al., 2017); Machine Learning based (Ghazi et al., 2015; Song and Meng, 2015; Gui et al., 2016a; Gui et al., 2016b; Xu et al., 2017) and Deep Learning based (Gui et al., 2017; 1518 Li et al., 2018; Yu et al., 2019; Xu et al., 2019). While most of the previous works are focused on either the news domain or microblogs, our work focuses on personal narratives. Personal narratives are more complex than the other domains as they are typically longer and contain multiple sub-events. Moreover, each sub-event has attributes (such as chara"
2020.lrec-1.189,C08-1111,0,0.060571,"our task has been conducted on the task of Emotion Cause Extraction (ECE). The task of ECE focuses on finding the cause of emotion from the given text. According to Talmy (2000), the cause of an emotion should be an event itself. The cause-event refers to the immediate cause of the emotion, which can be the actual trigger event or the perception of the trigger event (Lee et al., 2010a). The cause events are further categorized into two types: verbal events and nominal events (Lee et al., 2010b). There are a few emotion cause corpora on formal texts such as news reports, frames from FrameNet (Tokuhisa et al., 2008; Ghazi et al., 2015; Lee et al., 2010a; Gui et al., 2016a) and informal texts such as microblogs (Gui et al., 2014; Gao et al., 2015b; Cheng et al., 2017). Some of these are annotated manually, while some are created automatically. While Lee et al. (2010a) defined the task of ECE as the extraction of word-level emotion causes, Chen et al. (2010) suggested that a clause would be a more appropriate choice of unit for the extraction of an emotion cause. There have been works trying to solve the problem using different methods: Rule-based (Neviarouskaya and Aono, 2013; Li and Xu, 2014; Gao et al."
2020.sigdial-1.21,J08-1001,0,0.212235,"and how this concept can be formalized and evaluated. Our approach could be applied to both task-oriented and non-task-oriented dialogue. Coherence in language, i.e., the property which determines that a given text is a logical and consistent whole rather than a random collection of sentences, is a complex multifaced concept which has Approaches to coherence based on entities have been studied extensively by the Natural Language Processing literature (Joshi and Kuhn, 1979; Grosz et al., 1995), especially in text (e.g., news, summaries). Coherence evaluation tasks proposed by this literature (Barzilay and Lapata, 2008) have the advantage of using weakly supervised training methodologies, but mainly considering documents as-a-whole, rather than evaluating coherence at the utterance level. The dialogue literature (Sacks and Jefferson, 1995; Schegloff, 1968), on the other hand, has focused mainly on coherence in connection to DAs, a generalized version of intents in dialogue (e.g., yes-no-question, acknowledgement). Recent work (Cervone et al., 2018), in particular, showed the importance of both DAs and entities information for coherence modeling in dialogue. However, even in this case dialogue coherence was r"
2020.sigdial-1.21,J95-2003,0,0.671246,"defined a priori. In this work, we address the problem of dialogue evaluation from the perspective of dialogue coherence and how this concept can be formalized and evaluated. Our approach could be applied to both task-oriented and non-task-oriented dialogue. Coherence in language, i.e., the property which determines that a given text is a logical and consistent whole rather than a random collection of sentences, is a complex multifaced concept which has Approaches to coherence based on entities have been studied extensively by the Natural Language Processing literature (Joshi and Kuhn, 1979; Grosz et al., 1995), especially in text (e.g., news, summaries). Coherence evaluation tasks proposed by this literature (Barzilay and Lapata, 2008) have the advantage of using weakly supervised training methodologies, but mainly considering documents as-a-whole, rather than evaluating coherence at the utterance level. The dialogue literature (Sacks and Jefferson, 1995; Schegloff, 1968), on the other hand, has focused mainly on coherence in connection to DAs, a generalized version of intents in dialogue (e.g., yes-no-question, acknowledgement). Recent work (Cervone et al., 2018), in particular, showed the importa"
2020.sigdial-1.21,P18-1052,0,0.0778519,"e coherent than documents with only one sentence randomly misplaced. These tasks 1 The Switchboard Coherence corpus is available for download at: https://github.com/alecervi/ switchboard-coherence-corpus 2 The code for the models presented in this work can be found at: https://github.com/alecervi/ turn-coherence-rating are still considered standard to this day and found wide applications, especially for text (Farag et al., 2018; Clark et al., 2018). Recent models proposed for these tasks are based on Convolutional Neural Networks (Nguyen and Joty, 2017), also applied to thread reconstruction (Joty et al., 2018), while the current State-of-the-art is based on a combination of bidirectional Long Short-Term Memory encoders and convolution-pooling layers (Moon et al., 2019). These tasks, however, consider documents as-a-whole and rely mainly on entities information. Coherence evaluation in dialogue Models for dialogue coherence evaluation have mainly been explored using supervised approaches, i.e., training on corpora with human annotations for coherence, mostly at the turn level (Higashinaka et al., 2014; Gandhe and Traum, 2016; Venkatesh et al., 2017; Lowe et al., 2016; Yi et al., 2019). Different app"
2020.sigdial-1.21,D19-1205,0,0.0198717,"; Joty et al., 2018; Mesgar et al., 2019; Zhou et al., 2019). In particular, Cervone et al. (2018) found that discrimination might be over-simplistic for dialogue coherence evaluation when considering Dialogue Act (DA) information. In this work, we propose a novel framework to model entities and DAs information for turn coherence prediction using a weakly supervised training methodology. Furthermore, our focus is on predicting coherence of single turns rather than entire dialogues. Response Selection As a task, response selection has become a standard (Lowe et al., 2017; Yoshino et al., 2019; Kumar et al., 2019) for training both task-oriented and non-task-oriented retrieval-based dialogue models. The task proved to be useful for evaluating models in task-oriented (Ubuntu), social media threads (Twitter Corpus), and movie dialogues (SubTle Corpus) (Lowe et al., 2016). Recently the task has also been proposed for pretraining models for task-oriented dialogue (Henderson et al., 2019) and for Dialogue Act tagging (Mehri et al., 2019). In this work, we investigate response selection as a task for training coherence rating models for spoken dialogue. Additionally, while response selection models are usual"
2020.sigdial-1.21,J06-4002,0,0.015906,"each response makes sense as the next natural turn in the dialogue. All workers (37) who annotated the dataset were first evaluated on a common subset of 5 dialogues where they had an average Weighted Kappa agreement with quadratic weights with two gold (internal) annotators of κ = 0.659 (min: 0.425, max: 0.809, STD: 0.101) and among each other an average leave-one-out correlation of ρ = 0.78 (i.e. correlating the scores of each worker with mean scores of all other workers who annotated the same data), following the approach used in other coherence rating datasets (Barzilay and Lapata, 2008; Lapata, 2006). 3 Scores for each candidate turn were then averaged across all annotators. Original turns were regarded on average as more coherent (µ = 2.6, SD= 0.5) than adversarial turns, while turns generated with IS were considered more coherent (µ = 1.8, SD= 0.7) than the ones generated via ES (µ = 1.4, SD= 0.6). 165 5 Data analysis In this section, we analyse the Switchboard Coherence (SWBD-Coh) dataset in regards to the dis3 More details about our data collection procedure are available in Appendix A. tribution of Dialogue Acts (DAs) and entities. In particular, we are interested in analysing which"
2020.sigdial-1.21,N16-1014,0,0.0412031,"stical analysis of the corpus indicates how turn coherence perception is affected by patterns of distribution of entities previously introduced and the Dialogue Acts used. Second, we experiment with different architectures to model entities, Dialogue Acts and their combination and evaluate their performance in predicting human coherence ratings on SWBD-Coh. We find that models combining both DA and entity information yield the best performances both for response selection and turn coherence rating. 1 While much recent work has focused on coherence for response generation (Serban et al., 2016; Li et al., 2016; Yi et al., 2019), we argue that there is still much to be understood regarding the mechanisms and substructures that affect human perception of dialogue coherence. In our approach, in particular, we are interested in studying the patterns of distribution of entities and Dialogue Acts (DAs), in regards to dialogue coherence. Introduction Dialogue evaluation is an unsolved challenge in current human-machine interaction research. This is particularly true for open-domain conversation, where compared to task-oriented dialogue (i.e., restaurant reservations), we do not have a finite set of entiti"
2020.sigdial-1.21,W16-3634,0,0.0273353,"plied to thread reconstruction (Joty et al., 2018), while the current State-of-the-art is based on a combination of bidirectional Long Short-Term Memory encoders and convolution-pooling layers (Moon et al., 2019). These tasks, however, consider documents as-a-whole and rely mainly on entities information. Coherence evaluation in dialogue Models for dialogue coherence evaluation have mainly been explored using supervised approaches, i.e., training on corpora with human annotations for coherence, mostly at the turn level (Higashinaka et al., 2014; Gandhe and Traum, 2016; Venkatesh et al., 2017; Lowe et al., 2016; Yi et al., 2019). Different approaches tried to apply the standard coherence tasks to conversational domains such as dialogue and threads, but mainly considering the evaluation of dialogues as-a-whole (Purandare and Litman, 2008; Elsner and Charniak, 2011; Cervone et al., 2018; Vakulenko et al., 2018; Joty et al., 2018; Mesgar et al., 2019; Zhou et al., 2019). In particular, Cervone et al. (2018) found that discrimination might be over-simplistic for dialogue coherence evaluation when considering Dialogue Act (DA) information. In this work, we propose a novel framework to model entities and"
2020.sigdial-1.21,P19-1373,0,0.0205737,"ing coherence of single turns rather than entire dialogues. Response Selection As a task, response selection has become a standard (Lowe et al., 2017; Yoshino et al., 2019; Kumar et al., 2019) for training both task-oriented and non-task-oriented retrieval-based dialogue models. The task proved to be useful for evaluating models in task-oriented (Ubuntu), social media threads (Twitter Corpus), and movie dialogues (SubTle Corpus) (Lowe et al., 2016). Recently the task has also been proposed for pretraining models for task-oriented dialogue (Henderson et al., 2019) and for Dialogue Act tagging (Mehri et al., 2019). In this work, we investigate response selection as a task for training coherence rating models for spoken dialogue. Additionally, while response selection models are usually based on the entire text as input (Lowe et al., 2017), we rely solely on entities and DAs information, in or163 der to investigate their effect on turn coherence perception. 3 No. source dialogues No. insertion points No. pos/neg pairs Methodology In this work, we are interested in the relation between Dialogue Acts (DAs) and entities and how they can be modelled to train automatic predictors of next turn coherence in no"
2020.sigdial-1.21,D19-1231,0,0.0138645,"om/alecervi/ switchboard-coherence-corpus 2 The code for the models presented in this work can be found at: https://github.com/alecervi/ turn-coherence-rating are still considered standard to this day and found wide applications, especially for text (Farag et al., 2018; Clark et al., 2018). Recent models proposed for these tasks are based on Convolutional Neural Networks (Nguyen and Joty, 2017), also applied to thread reconstruction (Joty et al., 2018), while the current State-of-the-art is based on a combination of bidirectional Long Short-Term Memory encoders and convolution-pooling layers (Moon et al., 2019). These tasks, however, consider documents as-a-whole and rely mainly on entities information. Coherence evaluation in dialogue Models for dialogue coherence evaluation have mainly been explored using supervised approaches, i.e., training on corpora with human annotations for coherence, mostly at the turn level (Higashinaka et al., 2014; Gandhe and Traum, 2016; Venkatesh et al., 2017; Lowe et al., 2016; Yi et al., 2019). Different approaches tried to apply the standard coherence tasks to conversational domains such as dialogue and threads, but mainly considering the evaluation of dialogues as-"
2020.sigdial-1.21,P17-1121,0,0.258006,"ermuted, and insertion, i.e., ranking original documents as more coherent than documents with only one sentence randomly misplaced. These tasks 1 The Switchboard Coherence corpus is available for download at: https://github.com/alecervi/ switchboard-coherence-corpus 2 The code for the models presented in this work can be found at: https://github.com/alecervi/ turn-coherence-rating are still considered standard to this day and found wide applications, especially for text (Farag et al., 2018; Clark et al., 2018). Recent models proposed for these tasks are based on Convolutional Neural Networks (Nguyen and Joty, 2017), also applied to thread reconstruction (Joty et al., 2018), while the current State-of-the-art is based on a combination of bidirectional Long Short-Term Memory encoders and convolution-pooling layers (Moon et al., 2019). These tasks, however, consider documents as-a-whole and rely mainly on entities information. Coherence evaluation in dialogue Models for dialogue coherence evaluation have mainly been explored using supervised approaches, i.e., training on corpora with human annotations for coherence, mostly at the turn level (Higashinaka et al., 2014; Gandhe and Traum, 2016; Venkatesh et al"
2020.sigdial-1.21,J00-3003,0,0.748712,"Missing"
2020.sigdial-1.21,P18-1103,0,0.0198983,"For evaluating response selection, we use pairwise Accuracy, the metric used in standard coherence tasks, which evaluates the ability of the model to rank original turns higher than each adversarial one. However, this metric is not indicative of the global ranking of all candidate turns for a given context. For this reason, we add two ranking metrics to evaluate our models: Mean Reciprocal Rank (MRR), which evaluates the average of reciprocal ranks of all candidate turns for a context, and Recall at One (R@1) and Two (R@2), also used in previous work on response selection (Lowe et al., 2017; Zhou et al., 2018) to assess the ability of the model to rank original turns respectively within the first or second rank among all candidates. Compared to response selection, where we have a binary choice between coherent and negative turns, in turn coherence rating, we have a set of candidate turns each associated to a coherence score. In this case, we use Accuracy, MRR, R@1 and Normalized Discounted Cumulative Gain (nDCG) to evaluate our models. Accuracy was computed only for cases in which the rating of the turn was not identical across two candidate turns. MRR and R@1 were computed dynamically, that is con"
2021.nlpmc-1.1,D18-1547,0,0.0247466,"conversations about mental state issues are very complex because they usually encompass personal feelings, user-specific situations, different spaces of entities, and emotions. In this domain, the state-of-the-art data-driven frameworks are not applicable and domain knowledge is very scarce. The two main approaches to collect dialogue data for the purpose of developing data-driven dialogue agents are either acquiring user interaction data via user simulators and hand-designed policies (Li et al., 2016), or to collect large sets of human-human conversations in different user-agnostic settings (Budzianowski et al., 2018; Gopalakrishnan et al., 2019; Zhang et al., 2018). These approaches have been used for goal-oriented agents (e.g. reservations of restaurants) or open-domain agents answering questions about a finite set of topics (e.g. news, music, weather, games etc.). However, neither of the above approaches can address the need for personal conversations which include user-specific recollections of events, objects, entities and their relations. Last but not least, state-of-the-art conversational agents cannot carry out engaging and appropriate singleuser multi-session conversations. However, personal conv"
2021.nlpmc-1.1,W15-4640,0,0.0700151,"Missing"
2021.nlpmc-1.1,W16-0305,0,0.0281617,"associated to an emotion and in the next step is paired with another worker to have a conversation about the mentioned situation. While useful for chitchat and open-domain conversations, unfortunately these resources are not a good fit to address the needs of the mental health support domain. Mental health support dialogue corpora The research in this domain is very recent and resources are scarce. “Counseling and Psychotherapy Transcripts” published by Alexander Street Press2 is a dataset of 4000 therapy session transcriptions on various topics, used as a resource for therapistsin-training. Pérez-Rosas et al. (2016) collected a dataset of 277 Motivational Interviewing (MI) session videos and obtained the transcriptions for each session either directly from the data source, or by recruiting AMT workers. Guntakandla and Nielsen (2018) conducted a data collection process of therapeutic dialogues in Wizard of Oz manner where the therapists were impersonating a Personal Healthcare Agent. The authors recorded 324 sessions of therapeutic dialogues which were then manually transcribed. Furthermore, in the physical health coaching domain, Gupta et al. (2020) collected a dataset of conversations where the expert i"
2021.nlpmc-1.1,P19-1534,0,0.0231351,"e corpora Previously published research have addressed the problem of collecting dialogue data starting from world knowledge facts or predefined persona descriptions. In this regard, Zhang et al. (2018) collected a dataset of conversations conditioned on synthetic persona descriptions for each side of the dialogue using Amazon Mechanical Turk (AMT) workers. Gopalakrishnan et al. (2019) collected a dataset of dialogues grounded in world knowledge by pairing AMT workers to have a conversation based on selected reading sets from Wikipedia and The Washington Post over various topics. Furthermore, Rashkin et al. (2019) have crowdsourced a dataset • We present a methodology for data collection and elicitation of follow-up dialogues in the mental health domain. • We present an algorithm for automatically generating conversation stimuli for follow-up dialogues in the mental health domain from a 1 This data collection has been approved by the Ethical Committee of the University of Trento. 2 of conversations with implied user feelings in the context, using AMT workers where a worker writes a personal situation associated to an emotion and in the next step is paired with another worker to have a conversation abou"
2021.nlpmc-1.1,L18-1631,0,0.015131,"s are not a good fit to address the needs of the mental health support domain. Mental health support dialogue corpora The research in this domain is very recent and resources are scarce. “Counseling and Psychotherapy Transcripts” published by Alexander Street Press2 is a dataset of 4000 therapy session transcriptions on various topics, used as a resource for therapistsin-training. Pérez-Rosas et al. (2016) collected a dataset of 277 Motivational Interviewing (MI) session videos and obtained the transcriptions for each session either directly from the data source, or by recruiting AMT workers. Guntakandla and Nielsen (2018) conducted a data collection process of therapeutic dialogues in Wizard of Oz manner where the therapists were impersonating a Personal Healthcare Agent. The authors recorded 324 sessions of therapeutic dialogues which were then manually transcribed. Furthermore, in the physical health coaching domain, Gupta et al. (2020) collected a dataset of conversations where the expert impersonates a PHA that engages the users into a healthier life style. For this purpose, a certified health coach interacted with 28 patients using a messaging application. 3 Figure 2: The user interface of the mobile appl"
2021.nlpmc-1.1,2020.sigdial-1.30,0,0.0358325,"Missing"
2021.nlpmc-1.1,P18-1205,0,0.128449,"because they usually encompass personal feelings, user-specific situations, different spaces of entities, and emotions. In this domain, the state-of-the-art data-driven frameworks are not applicable and domain knowledge is very scarce. The two main approaches to collect dialogue data for the purpose of developing data-driven dialogue agents are either acquiring user interaction data via user simulators and hand-designed policies (Li et al., 2016), or to collect large sets of human-human conversations in different user-agnostic settings (Budzianowski et al., 2018; Gopalakrishnan et al., 2019; Zhang et al., 2018). These approaches have been used for goal-oriented agents (e.g. reservations of restaurants) or open-domain agents answering questions about a finite set of topics (e.g. news, music, weather, games etc.). However, neither of the above approaches can address the need for personal conversations which include user-specific recollections of events, objects, entities and their relations. Last but not least, state-of-the-art conversational agents cannot carry out engaging and appropriate singleuser multi-session conversations. However, personal conversations’ requirements include the ability of car"
C02-1134,J93-2003,0,\N,Missing
C02-1134,P99-1068,0,\N,Missing
C02-1134,P98-1006,1,\N,Missing
C02-1134,C98-1006,1,\N,Missing
C02-1134,P98-2186,0,\N,Missing
C02-1134,C98-2181,0,\N,Missing
C10-2104,W05-0601,1,0.893251,"Missing"
C10-2104,magnini-etal-2006-cab,0,0.0269896,"ning models like previous work on NER, the composite kernel not only captures most of the flat features but also efficiently exploits structured features. More interestingly, this kernel yields significant improvement when applied to two corpora of two different languages. The evaluation in the Italian corpus shows that our method outperforms the best reported methods whereas on the English data it reaches the state-of-the-art. 2 2.1 Italian corpus and the well-known CoNLL 2003 English shared task corpus. The EVALITA 2009 Italian dataset is based on I-CAB, the Italian Content Annotation Bank (Magnini et al., 2006), annotated with four entity types: Person (PER), Organization (ORG), Geo-Political Entity (GPE) and Location (LOC). The training data, taken from the local newspaper “L’Adige”, consists of 525 news stories which belong to five categories: News Stories, Cultural News, Economic News, Sports News and Local News. Test data, on the other hand, consist of completely new data, taken from the same newspaper and consists of 180 news stories. The CoNLL 2003 English dataset is created within the shared task of CoNLL-2003 (Sang and Meulder, 2003). It is a collection of news wire articles from the Reuters"
C10-2104,P04-1043,1,0.951356,"Missing"
C10-2104,H05-1091,0,0.410254,"Missing"
C10-2104,W02-2004,0,0.0903869,"Missing"
C10-2104,C02-1025,0,0.0358435,"Collins, 2002b; Collins, 2002a) but the employed algorithm just exploited arbitrary information regarding word types and linguistic patterns. In contrast, we define and study diverse features by also considering n-grams patterns preceding, and following the target entity. Complementary context In supervised learning, NER systems often suffer from low recall, which is caused by lack of both resource and context. For example, a word like “Arkansas” may not appear in the training set and in the test set, there may not be enough context to infer its NE tag. In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help. To overcome this deficiency, we employed the following unsupervised procedure: first, the baseline NER is applied to the target un-annotated corpus. Second, we associate each word of the corpus with the most frequent NE category assigned in the previous step. Finally, the above tags are used as features during the training of the improved NER and also for building the feature representation for a new classification instance. This way, for any unknown word w of the test set, we can rely on the most probable NE category as feature. The adva"
C10-2104,D09-1143,1,0.887061,"Missing"
C10-2104,W03-0423,0,0.0190266,"yed algorithm just exploited arbitrary information regarding word types and linguistic patterns. In contrast, we define and study diverse features by also considering n-grams patterns preceding, and following the target entity. Complementary context In supervised learning, NER systems often suffer from low recall, which is caused by lack of both resource and context. For example, a word like “Arkansas” may not appear in the training set and in the test set, there may not be enough context to infer its NE tag. In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help. To overcome this deficiency, we employed the following unsupervised procedure: first, the baseline NER is applied to the target un-annotated corpus. Second, we associate each word of the corpus with the most frequent NE category assigned in the previous step. Finally, the above tags are used as features during the training of the improved NER and also for building the feature representation for a new classification instance. This way, for any unknown word w of the test set, we can rely on the most probable NE category as feature. The advantage is that we derived it by using the aver"
C10-2104,M98-1028,0,0.0172663,"e Processing (NLP) community. Basically, this method first employs a probabilistic model to generate a list of top-n candidates and then reranks this n-best list with additional features. One appeal of this approach is its flexibility of incorporating arbitrary features into a model. These features help in discriminating good from bad hypotheses and consequently their automatic learning. Various algorithms have been applied for reranking in NLP applications (Huang, 2008; Named-entities (NEs) are essential for defining the semantics of a document. NEs are objects that can be referred by names (Chinchor and Robinson, 1998), such as people, organizations, and locations. The research on NER has been promoted by the Message Understanding Conferences (MUCs, 1987-1998), the shared task of the Conference on Natural Language Learning (CoNLL, 2002-2003), and the Automatic Content Extraction program (ACE, 2002-2005). In the literature, there exist various learning approaches to extract named-entities from text. A NER sys901 Coling 2010: Poster Volume, pages 901–909, Beijing, August 2010 tem often builds some generative/discriminative model, then, either uses only one classifier (Carreras et al., 2002) or combines many c"
C10-2104,P02-1034,0,0.312481,"erence on Natural Language Learning (CoNLL, 2002-2003), and the Automatic Content Extraction program (ACE, 2002-2005). In the literature, there exist various learning approaches to extract named-entities from text. A NER sys901 Coling 2010: Poster Volume, pages 901–909, Beijing, August 2010 tem often builds some generative/discriminative model, then, either uses only one classifier (Carreras et al., 2002) or combines many classifiers using some heuristics (Florian et al., 2003). To the best of our knowledge, reranking has not been applied to NER except for the reranking algorithms defined in (Collins, 2002b; Collins, 2002a), which only targeted the entity detection (and not entity classification) task. Besides, since kernel methods offer a natural way to exploit linguistic properties, applying kernels for NE reranking is worthwhile. In this paper, we describe how kernel methods can be applied for reranking, i.e. detection and classification of named-entities, in standard corpora for Italian and English. The key aspect of our reranking approach is how structured and flat features can be employed in discriminating candidate tagged sequences. For this purpose, we apply tree kernels to a tree struc"
C10-2104,D09-1112,1,0.82235,"Missing"
C10-2104,W03-0425,0,0.0442864,"nd locations. The research on NER has been promoted by the Message Understanding Conferences (MUCs, 1987-1998), the shared task of the Conference on Natural Language Learning (CoNLL, 2002-2003), and the Automatic Content Extraction program (ACE, 2002-2005). In the literature, there exist various learning approaches to extract named-entities from text. A NER sys901 Coling 2010: Poster Volume, pages 901–909, Beijing, August 2010 tem often builds some generative/discriminative model, then, either uses only one classifier (Carreras et al., 2002) or combines many classifiers using some heuristics (Florian et al., 2003). To the best of our knowledge, reranking has not been applied to NER except for the reranking algorithms defined in (Collins, 2002b; Collins, 2002a), which only targeted the entity detection (and not entity classification) task. Besides, since kernel methods offer a natural way to exploit linguistic properties, applying kernels for NE reranking is worthwhile. In this paper, we describe how kernel methods can be applied for reranking, i.e. detection and classification of named-entities, in standard corpora for Italian and English. The key aspect of our reranking approach is how structured and"
C10-2104,W03-0419,0,0.0260008,"Missing"
C10-2104,N04-1023,0,0.249552,"Missing"
C10-2104,P03-1002,0,0.111166,"Missing"
C10-2104,P08-1067,0,0.0308094,"wo input strings. Introduction Reranking is a promising computational framework, which has drawn special attention in the Natural Language Processing (NLP) community. Basically, this method first employs a probabilistic model to generate a list of top-n candidates and then reranks this n-best list with additional features. One appeal of this approach is its flexibility of incorporating arbitrary features into a model. These features help in discriminating good from bad hypotheses and consequently their automatic learning. Various algorithms have been applied for reranking in NLP applications (Huang, 2008; Named-entities (NEs) are essential for defining the semantics of a document. NEs are objects that can be referred by names (Chinchor and Robinson, 1998), such as people, organizations, and locations. The research on NER has been promoted by the Message Understanding Conferences (MUCs, 1987-1998), the shared task of the Conference on Natural Language Learning (CoNLL, 2002-2003), and the Automatic Content Extraction program (ACE, 2002-2005). In the literature, there exist various learning approaches to extract named-entities from text. A NER sys901 Coling 2010: Poster Volume, pages 901–909, Be"
C10-2104,W09-1119,0,\N,Missing
C10-2104,P02-1062,0,\N,Missing
C18-1300,L16-1020,1,0.842965,"c; since many of these schemes lack coverage of some crucial aspects of dialogic interaction. Furthermore, given that all these datasets utilize different schemes, these resources are hardly compatible. The ISO 24617-2 (Bunt et al., 2010; Bunt et al., 2012), the international ISO standard for DA annotation, represents the first attempt to create a truly domain and task independent scheme. Given its holistic approach compared to previous schemes, ISO 24617-2 can be used as a lingua franca for cross-corpora DA mapping, as confirmed by successful attempts to remap single corpora to the standard (Chowdhury et al., 2016; Fang et al., 2012). However, there is no reference training set for the standard, since the only public resource currently available with ISO 24617-2 annotation (DialogBank, (Bunt et al., 2016)) is too small to be used to train classifiers. Therefore, most DA tagging research still focuses on in-domain studies on large datasets with incompatible DA annotations (Stolcke et al., 2000; Ji et al., 2016). Moreover, most publicly available corpora are imbalanced with respect to the distribution of various DA dimensions such as Information Transfer (e.g. “What’s your favourite book?”) or Action Dis"
C18-1300,W14-4337,0,0.0838486,"Missing"
C18-1300,N16-1037,0,0.0313848,"approach compared to previous schemes, ISO 24617-2 can be used as a lingua franca for cross-corpora DA mapping, as confirmed by successful attempts to remap single corpora to the standard (Chowdhury et al., 2016; Fang et al., 2012). However, there is no reference training set for the standard, since the only public resource currently available with ISO 24617-2 annotation (DialogBank, (Bunt et al., 2016)) is too small to be used to train classifiers. Therefore, most DA tagging research still focuses on in-domain studies on large datasets with incompatible DA annotations (Stolcke et al., 2000; Ji et al., 2016). Moreover, most publicly available corpora are imbalanced with respect to the distribution of various DA dimensions such as Information Transfer (e.g. “What’s your favourite book?”) or Action Discussion (e.g. “Tell me the news.”), which are required for successful open-domain conversational systems. In this work, we show how to reuse and combine publicly available annotated resources to create a large training corpus for domain-independent DA tagging experiments. We map different corpora using an ISO standard compliant DA taxonomy, following the previous research on the topic (Fang et al., 20"
C18-1300,petukhova-etal-2014-dbox,0,0.180241,"reover, most publicly available corpora are imbalanced with respect to the distribution of various DA dimensions such as Information Transfer (e.g. “What’s your favourite book?”) or Action Discussion (e.g. “Tell me the news.”), which are required for successful open-domain conversational systems. In this work, we show how to reuse and combine publicly available annotated resources to create a large training corpus for domain-independent DA tagging experiments. We map different corpora using an ISO standard compliant DA taxonomy, following the previous research on the topic (Fang et al., 2012; Petukhova et al., 2014b) and we share this resource with the research community.1 In order to investigate the soundness of our approach compared to in-domain models we further experiment with domain-independent DA tagging. As previously done in the literature we cast the Dialogue Act tagging task as a supervised multi-class classification problem using Support Vector Machines. The correctness of the approach is first tested on the de facto DA tagging standard – the Switchboard (SWDA) corpus (Godfrey et al., 1992), using the reference training and test sets and achieving performance comparable to the state-of-the-ar"
C18-1300,petukhova-etal-2014-interoperability,0,0.089502,"reover, most publicly available corpora are imbalanced with respect to the distribution of various DA dimensions such as Information Transfer (e.g. “What’s your favourite book?”) or Action Discussion (e.g. “Tell me the news.”), which are required for successful open-domain conversational systems. In this work, we show how to reuse and combine publicly available annotated resources to create a large training corpus for domain-independent DA tagging experiments. We map different corpora using an ISO standard compliant DA taxonomy, following the previous research on the topic (Fang et al., 2012; Petukhova et al., 2014b) and we share this resource with the research community.1 In order to investigate the soundness of our approach compared to in-domain models we further experiment with domain-independent DA tagging. As previously done in the literature we cast the Dialogue Act tagging task as a supervised multi-class classification problem using Support Vector Machines. The correctness of the approach is first tested on the de facto DA tagging standard – the Switchboard (SWDA) corpus (Godfrey et al., 1992), using the reference training and test sets and achieving performance comparable to the state-of-the-ar"
C18-1300,H90-1020,0,0.129799,"Missing"
C18-1300,W04-2319,0,0.209948,"Missing"
C18-1300,J00-3003,0,0.749171,"Missing"
C18-1300,T75-2026,0,0.553122,"Missing"
D09-1112,W05-0601,1,0.501549,"Missing"
D09-1112,P02-1034,0,0.833297,"U hypotheses, which are re-ranked by SVMs. To effectively design our re-ranker, we use all pos1076 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1076–1085, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP sible word/concept subsequences with gaps of the spoken sentence as features (i.e. all possible ngrams). Gaps allow for encoding long distance dependencies between words in relatively small sequences. Since the space of such features is huge, we adopted kernel methods, i.e. sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Collins and Duffy, 2002; Moschitti, 2006a) to implicitly encode them along with other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the french MEDIA corpus (BonneauMaynard et al., 2005) and a corpus made available by the European project LUNA1 (Dinarelli et al., 2009b). In particular, the new contents with respect to our previous work (Dinarelli et al., 2009a) are: • We designed a new sequential structure (SK2) and two new hierarchical tree structures (MULTILEVEL and FEATURES) for re-ranking models (see Section 4.2). The la"
D09-1112,W06-2909,1,0.902359,"ine translation. Notice that our goal is different from the one tackled in such paper and, in general, it is more difficult: we try to learn which is the best annotation of a given input sentence, while in (Shen et al., 2004), they learn to distinguish between ”good” and ”bad” translations of a sentence. Even if our goal is more difficult, our approach is very effective, as shown in (Dinarelli et al., 2009a). It is more appropriate since in parse re-ranking there is only one best hypothesis, while in machine translation a sentence can have more than one correct translations. Additionally, in (Moschitti et al., 2006; Moschitti et al., 2008) a tree kernel was applied to semantic trees similar to the one introduced in the next section to re-rank Semantic Role Labeling annotations. 4.4 Re-ranking models using trees Since the aim of concept annotation re-ranking is to exploit innovative and effective source of information, we can use, in addition to sequence kernels, the power of tree kernels to generate correlation between concepts and word structures. Figures 2(a), 2(b) and 3 describe the structural association between the concept and the word 1080 (a) FLAT Tree (b) MULTILEVEL Tree Figure 2: Examples of st"
D09-1112,P04-1054,0,0.0372046,"The remainder of the paper is organized as follows: Section 2 introduces kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1 Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we u"
D09-1112,P07-1098,1,0.842965,"or structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1 Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as symbols in a style similar to (Cancedda et al., 2003; Moschi"
D09-1112,E09-1024,1,0.941097,"possible ngrams). Gaps allow for encoding long distance dependencies between words in relatively small sequences. Since the space of such features is huge, we adopted kernel methods, i.e. sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Collins and Duffy, 2002; Moschitti, 2006a) to implicitly encode them along with other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the french MEDIA corpus (BonneauMaynard et al., 2005) and a corpus made available by the European project LUNA1 (Dinarelli et al., 2009b). In particular, the new contents with respect to our previous work (Dinarelli et al., 2009a) are: • We designed a new sequential structure (SK2) and two new hierarchical tree structures (MULTILEVEL and FEATURES) for re-ranking models (see Section 4.2). The latter combined with two different tree kernels originate four new different models. • We experimented with automatic speech transcriptions thus assessing the robustness to noise of our models. • We compare our models against Conditional Random Field (CRF) approaches described in (Hahn et al., 2008), which are the current state-of-the-art"
D09-1112,W09-0505,1,0.909585,"possible ngrams). Gaps allow for encoding long distance dependencies between words in relatively small sequences. Since the space of such features is huge, we adopted kernel methods, i.e. sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Collins and Duffy, 2002; Moschitti, 2006a) to implicitly encode them along with other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the french MEDIA corpus (BonneauMaynard et al., 2005) and a corpus made available by the European project LUNA1 (Dinarelli et al., 2009b). In particular, the new contents with respect to our previous work (Dinarelli et al., 2009a) are: • We designed a new sequential structure (SK2) and two new hierarchical tree structures (MULTILEVEL and FEATURES) for re-ranking models (see Section 4.2). The latter combined with two different tree kernels originate four new different models. • We experimented with automatic speech transcriptions thus assessing the robustness to noise of our models. • We compare our models against Conditional Random Field (CRF) approaches described in (Hahn et al., 2008), which are the current state-of-the-art"
D09-1112,J08-2003,1,0.944424,"he generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1 Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as symbols in a style similar to (Cancedda et al., 2003; Moschitti, 2008). For example, given the senten"
D09-1112,E06-1015,1,0.941864,"e-ranked by SVMs. To effectively design our re-ranker, we use all pos1076 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1076–1085, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP sible word/concept subsequences with gaps of the spoken sentence as features (i.e. all possible ngrams). Gaps allow for encoding long distance dependencies between words in relatively small sequences. Since the space of such features is huge, we adopted kernel methods, i.e. sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Collins and Duffy, 2002; Moschitti, 2006a) to implicitly encode them along with other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the french MEDIA corpus (BonneauMaynard et al., 2005) and a corpus made available by the European project LUNA1 (Dinarelli et al., 2009b). In particular, the new contents with respect to our previous work (Dinarelli et al., 2009a) are: • We designed a new sequential structure (SK2) and two new hierarchical tree structures (MULTILEVEL and FEATURES) for re-ranking models (see Section 4.2). The latter combined wit"
D09-1112,hahn-etal-2008-comparison,0,0.0496457,"able by the European project LUNA1 (Dinarelli et al., 2009b). In particular, the new contents with respect to our previous work (Dinarelli et al., 2009a) are: • We designed a new sequential structure (SK2) and two new hierarchical tree structures (MULTILEVEL and FEATURES) for re-ranking models (see Section 4.2). The latter combined with two different tree kernels originate four new different models. • We experimented with automatic speech transcriptions thus assessing the robustness to noise of our models. • We compare our models against Conditional Random Field (CRF) approaches described in (Hahn et al., 2008), which are the current state-of-the-art in SLU. Learning curves clearly show that our models improve CRF, especially when small data sets are used. The remainder of the paper is organized as follows: Section 2 introduces kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer fea"
D09-1112,N01-1025,0,0.0418612,"Section 4. The initial annotation to be re-ranked is the list of the ten best hypotheses output by an FST model. We point out that, on the large Media dataset the processing time is considerably high2 so we could not run all the models. We trained all the SCLMs used in our experiments with the SRILM toolkit (Stolcke, 2002) and we used an interpolated model for probability estimation with the Kneser-Ney discount (Chen and Goodman, 1998). We then converted the model in an FST again with SRILM toolkit. The model used to obtain the SVM baseline for concept classification was trained using YamCHA (Kudo and Matsumoto, 2001). As re-ranking models based on structure kernels and SVMs, we used the SVM-Light-TK toolkit (available at disi.unitn.it/moschitti). For λ (see Section 3), costfactor and trade-off parameters, we used, 0.4, 1 and 1, respectively (i.e. the default parameters). The number m of hypotheses was always set to 10. The CRF model we compare with was trained with the CRF++ tool, available at http://crfpp.sourceforge.net/. The model is equivalent to the one described in (Hahn et al., 2008). As features, we used word and morpho-syntactic categories in a window of [-2, +2] with respect to the current token"
D09-1112,W03-1012,0,0.189001,"nce kernel is used to evaluate the number of common ngrams between si and sj . Since the string kernel skips some elements of the target sequences, the counted n-grams include: concept sequences, word sequences and any subsequence of words and concepts at any distance in the sentence. Such counts are used in our re-ranking function 1 2 as follows: let ek be the pair sk , sk we evaluate the kernel: KR (e1 , e2 ) = SK(s11 , s12 ) + SK(s21 , s22 ) (2) − SK(s11 , s22 ) − SK(s21 , s12 ) This schema, consisting in summing four different kernels, has been already applied in (Collins and Duffy, 2002; Shen et al., 2003) for syntactic parsing re-ranking, where the basic kernel was a tree kernel instead of SK. It was also used also in (Shen et al., 2004) to re-rank different candidates of the same hypothesis for machine translation. Notice that our goal is different from the one tackled in such paper and, in general, it is more difficult: we try to learn which is the best annotation of a given input sentence, while in (Shen et al., 2004), they learn to distinguish between ”good” and ”bad” translations of a sentence. Even if our goal is more difficult, our approach is very effective, as shown in (Dinarelli et a"
D09-1112,P03-1004,0,0.0318646,"curves clearly show that our models improve CRF, especially when small data sets are used. The remainder of the paper is organized as follows: Section 2 introduces kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1 Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm d"
D09-1112,N04-1023,0,0.0904065,"t sequences, the counted n-grams include: concept sequences, word sequences and any subsequence of words and concepts at any distance in the sentence. Such counts are used in our re-ranking function 1 2 as follows: let ek be the pair sk , sk we evaluate the kernel: KR (e1 , e2 ) = SK(s11 , s12 ) + SK(s21 , s22 ) (2) − SK(s11 , s22 ) − SK(s21 , s12 ) This schema, consisting in summing four different kernels, has been already applied in (Collins and Duffy, 2002; Shen et al., 2003) for syntactic parsing re-ranking, where the basic kernel was a tree kernel instead of SK. It was also used also in (Shen et al., 2004) to re-rank different candidates of the same hypothesis for machine translation. Notice that our goal is different from the one tackled in such paper and, in general, it is more difficult: we try to learn which is the best annotation of a given input sentence, while in (Shen et al., 2004), they learn to distinguish between ”good” and ”bad” translations of a sentence. Even if our goal is more difficult, our approach is very effective, as shown in (Dinarelli et al., 2009a). It is more appropriate since in parse re-ranking there is only one best hypothesis, while in machine translation a sentence"
D09-1112,P05-1024,0,0.107111,"Section 2 introduces kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1 Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as symbols in a style"
D09-1112,W04-3222,0,0.0642139,"s organized as follows: Section 2 introduces kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1 Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as"
D09-1112,W04-2403,1,0.91097,"Missing"
D09-1112,P08-2029,1,0.82344,"ucing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1 Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as symbols in a style similar to (Cancedda et al., 2003; Moschitti, 2008). For example, given the sentence: How may I help you ? sample s"
D09-1143,W05-0601,1,0.778199,"nt tree or of the path linking two entities of the dependency tree. For the design of automatic relation classifiers, we have investigated the impact of dependency structures to the RE task. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provid"
D09-1143,H05-1091,0,0.860771,"ng relevant semantic relations between pairs of entities in texts. Figure 1 shows part of a document from ACE 2004 corpus, a collection of news articles. In the text, the relation between president and NBC’s entertainment division describes the relationship between the first entity (person) and the second (organization) where the person holds a managerial position. Several approaches have been proposed for automatically learning semantic relations from texts. Among others, there has been increased interest in the application of kernel methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2005; Wang, 2008). Their main property is the ability of exploiting a huge amount of This work has been partially funded by the LiveMemories project (http://www.livememories.org/) and Expert System (http://www.expertsystem.net/) research grant. features without an explicit feature representation. This can be done by computing a kernel function between a pair of linguistic objects, where such function is a kind of similarity measure satisfying certain properties. An example is the sequence kernel (Lodhi et al., 2002), where the objects are strings of"
D09-1143,P04-1053,0,0.0120628,"vious work on RE is described in Section 2. Section 3 introduces support vector machines and kernel methods whereas our specific kernels for RE are described is Section 4. The experiments and conclusions are presented in sections 5 and 6, respectively. 2 Related Work To identify semantic relations using machine learning, three learning settings have mainly been applied, namely supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method (Hasegawa et al., 2004). In a supervised learning setting, representative related work can be classified into generative models (Miller et al., 2000), feature-based (Roth and tau Yih, 2002; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005) or kernel-based methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008; Zhang et al., 2006). The learning model employed in (Miller et al., 2000) used statistical parsing techniques to learn syntactic parse trees. It demonstrated that a lexicalized, probabilistic context-free parser with head rules can be use"
D09-1143,P03-1054,0,0.00259989,"ata portion includes 348 documents and 4400 relation instances. It defines seven entity types and seven relation types. Every relation is assigned one of the seven types: Physical, Person/Social, Employment/Membership/Subsidiary, Agent-Artifact, PER/ORG Affiliation, GPE Affiliation, and Discourse. For sake of space, we do not explain these relationships here, nevertheless, they are explicitly described in the ACE document guidelines. There are 4400 positive and 38,696 negative examples when generating pairs of entity mentions as potential relations. Documents are parsed using Stanford Parser (Klein and Manning, 2003) to produce parse trees. Potential relations are generated by iterating all pairs of entity mentions in the same sentence. Entity information, namely entity type, is integrated into parse trees. To train and test our binary relation classifier, we used SVMs. Here, relation detection is formulated as a multiclass classification problem. The one vs. rest strategy is employed by selecting the instance with largest margin as the final answer. For experimentation, we use 5-fold cross-validation with the Tree Kernel Tools (Moschitti, 2004) (available at http://disi.unitn.it/˜moschitt/Tree-Kernel.htm"
D09-1143,J93-2004,0,0.0366907,"Missing"
D09-1143,A00-2030,0,0.0308941,"performance than previous sequence and dependency models for RE. 1 The function defined on (Culotta and Sorensen, 2004), although on dependency trees, is not a convolution tree kernel. A review of previous work on RE is described in Section 2. Section 3 introduces support vector machines and kernel methods whereas our specific kernels for RE are described is Section 4. The experiments and conclusions are presented in sections 5 and 6, respectively. 2 Related Work To identify semantic relations using machine learning, three learning settings have mainly been applied, namely supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method (Hasegawa et al., 2004). In a supervised learning setting, representative related work can be classified into generative models (Miller et al., 2000), feature-based (Roth and tau Yih, 2002; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005) or kernel-based methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008; Zhang et al., 2006). The learning m"
D09-1143,W04-2403,1,0.85905,"task. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provide a better generalization (with a risk of underfitting). iii) Design a new kernel which can integrate the advantages of the constituent and dependency tree. The new tree kernel should inherit"
D09-1143,P07-1098,1,0.48532,"ernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provide a better generalization (with a risk of underfitting). iii) Design a new kernel which can integrate the advantages of the constituent and dependency tree. The new tree kernel should inherit the benefits of the thr"
D09-1143,J08-2003,1,0.717886,"ic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provide a better generalization (with a risk of underfitting). iii) Design a new kernel which can integrate the advantages of the constituent and dependency tree. The new tree kernel should inherit the benefits of the three available tree kernels: ST, SST or PT."
D09-1143,P04-1043,1,0.911584,"alculation since the size of a complete parse tree may be very large (up to 300 nodes in the Penn Treebank (Marcus et al., 1993)); second, there is ambiguity on the target pairs of NEs, i.e. different NEs associated with different relations are described by the same parse tree. Therefore, it is necessary to identify the portion of the parse tree that best represent the useful syntactic information. Let e1 and e2 be two entity mentions in the same sentence such that they are in a relationship R. For the constituent parse tree, we used the pathenclosed tree (PET), which was firstly proposed in (Moschitti, 2004) for Semantic Role Labeling and then adapted by (Zhang et al., 2005) for relation extraction. It is the smallest common subtree including the two entities of a relation. The dashed frame in Figure 2.a surrounds PET associated with the two mentions, officials and Washington. Moreover, to improve the representation, two extra nodes T1-PER, denoting the type PERSON, and T2-LOC, denoting the type LOCATION, are added to the parse tree, above the two target NEs, respectively. In this example, the above PET is designed to capture the relation Located-in between the entities ”officials” and ”Washingto"
D09-1143,P04-1054,0,0.963726,"in ACE as the task of finding relevant semantic relations between pairs of entities in texts. Figure 1 shows part of a document from ACE 2004 corpus, a collection of news articles. In the text, the relation between president and NBC’s entertainment division describes the relationship between the first entity (person) and the second (organization) where the person holds a managerial position. Several approaches have been proposed for automatically learning semantic relations from texts. Among others, there has been increased interest in the application of kernel methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2005; Wang, 2008). Their main property is the ability of exploiting a huge amount of This work has been partially funded by the LiveMemories project (http://www.livememories.org/) and Expert System (http://www.expertsystem.net/) research grant. features without an explicit feature representation. This can be done by computing a kernel function between a pair of linguistic objects, where such function is a kind of similarity measure satisfying certain properties. An example is the sequence kernel (Lodhi et al., 2002), where t"
D09-1143,P06-1117,1,0.379513,"Missing"
D09-1143,C02-1151,0,0.098702,"Missing"
D09-1143,I08-2119,0,0.419818,"1 shows part of a document from ACE 2004 corpus, a collection of news articles. In the text, the relation between president and NBC’s entertainment division describes the relationship between the first entity (person) and the second (organization) where the person holds a managerial position. Several approaches have been proposed for automatically learning semantic relations from texts. Among others, there has been increased interest in the application of kernel methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2005; Wang, 2008). Their main property is the ability of exploiting a huge amount of This work has been partially funded by the LiveMemories project (http://www.livememories.org/) and Expert System (http://www.expertsystem.net/) research grant. features without an explicit feature representation. This can be done by computing a kernel function between a pair of linguistic objects, where such function is a kind of similarity measure satisfying certain properties. An example is the sequence kernel (Lodhi et al., 2002), where the objects are strings of characters and the kernel function computes the number of com"
D09-1143,W02-1010,0,0.685626,"action (RE) is defined in ACE as the task of finding relevant semantic relations between pairs of entities in texts. Figure 1 shows part of a document from ACE 2004 corpus, a collection of news articles. In the text, the relation between president and NBC’s entertainment division describes the relationship between the first entity (person) and the second (organization) where the person holds a managerial position. Several approaches have been proposed for automatically learning semantic relations from texts. Among others, there has been increased interest in the application of kernel methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2005; Wang, 2008). Their main property is the ability of exploiting a huge amount of This work has been partially funded by the LiveMemories project (http://www.livememories.org/) and Expert System (http://www.expertsystem.net/) research grant. features without an explicit feature representation. This can be done by computing a kernel function between a pair of linguistic objects, where such function is a kind of similarity measure satisfying certain properties. An example is the sequence kernel ("
D09-1143,I05-1034,0,0.599525,"es in texts. Figure 1 shows part of a document from ACE 2004 corpus, a collection of news articles. In the text, the relation between president and NBC’s entertainment division describes the relationship between the first entity (person) and the second (organization) where the person holds a managerial position. Several approaches have been proposed for automatically learning semantic relations from texts. Among others, there has been increased interest in the application of kernel methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2005; Wang, 2008). Their main property is the ability of exploiting a huge amount of This work has been partially funded by the LiveMemories project (http://www.livememories.org/) and Expert System (http://www.expertsystem.net/) research grant. features without an explicit feature representation. This can be done by computing a kernel function between a pair of linguistic objects, where such function is a kind of similarity measure satisfying certain properties. An example is the sequence kernel (Lodhi et al., 2002), where the objects are strings of characters and the kernel function computes the"
D09-1143,P06-1104,0,0.669457,"es over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Zhang et al., 2005) or subsequence kernels over dependency graphs (Bunescu and Mooney, 2005a; Wang, 2008). More specifically, (Bunescu and Mooney, 2005a; Culotta and Sorensen, 2004) use kernels over dependency trees, which showed much lower accuracy than feature-based methods (Zhao and Grishman, 2005). One problem of the dependency kernels above is that they do not exploit the overall structural aspects of dependency trees. A more effective solution is the application of convolution kernels to constituent parse trees (Zhang et al., 2006) but this is not satisfactory from a general per1378 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1378–1387, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP spective since dependency structures offer some unique advantages, which should be exploited by an appropriate kernel. Therefore, studying convolution tree kernels for dependency trees is worthwhile also considering that, to the best of our knowledge, these models have not been previously used for relation extraction1 task. Additionally, sequence kernels should be included in such global st"
D09-1143,P05-1052,0,0.738863,"ubtree shared by two input trees. An example is that of syntactic (or subset) tree kernel (SST) (Collins and Duffy, 2001), where trees encode grammatical derivations. Previous work on the use of kernels for RE has exploited some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Zhang et al., 2005) or subsequence kernels over dependency graphs (Bunescu and Mooney, 2005a; Wang, 2008). More specifically, (Bunescu and Mooney, 2005a; Culotta and Sorensen, 2004) use kernels over dependency trees, which showed much lower accuracy than feature-based methods (Zhao and Grishman, 2005). One problem of the dependency kernels above is that they do not exploit the overall structural aspects of dependency trees. A more effective solution is the application of convolution kernels to constituent parse trees (Zhang et al., 2006) but this is not satisfactory from a general per1378 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1378–1387, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP spective since dependency structures offer some unique advantages, which should be exploited by an appropriate kernel. Therefore, studying convolution t"
D09-1143,P05-1053,0,0.286051,"on (Culotta and Sorensen, 2004), although on dependency trees, is not a convolution tree kernel. A review of previous work on RE is described in Section 2. Section 3 introduces support vector machines and kernel methods whereas our specific kernels for RE are described is Section 4. The experiments and conclusions are presented in sections 5 and 6, respectively. 2 Related Work To identify semantic relations using machine learning, three learning settings have mainly been applied, namely supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method (Hasegawa et al., 2004). In a supervised learning setting, representative related work can be classified into generative models (Miller et al., 2000), feature-based (Roth and tau Yih, 2002; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005) or kernel-based methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008; Zhang et al., 2006). The learning model employed in (Miller et al., 2000) used statistical parsing techniques to learn syn"
D09-1143,D07-1076,0,0.704331,"ptured by the shortest path between them in the dependency graph. Although approaches in RE have been dominated by kernel-based methods, until now, most of research in this line has used the kernel as some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008). These are not convolution kernels and produce a much lower number of substructures than the PT kernel. A recent approach successfully employs a convolution tree kernel (of type SST) over constituent syntactic parse tree (Zhang et al., 2006; Zhou et al., 2007), but it does not capture grammatical relations in dependency structure. We believe that an efficient and appropriate kernel can be used to solve the RE problem, exploiting the advantages of dependency structures, convolution tree kernels and sequence kernels. 3 Support Vector Machines and Kernel Methods In this section we give a brief introduction to support vector machines, kernel methods, diverse tree and sequence kernel spaces, which can be applied to the RE task. 3.1 Support Vector Machines (SVMs) Support Vector Machines refer to a supervised machine learning technique based on the latest"
D11-1066,W05-0601,1,0.863421,"Missing"
D11-1066,H05-1091,0,0.0862146,"Missing"
D11-1066,P07-1073,0,0.0212609,"Missing"
D11-1066,A00-2018,0,0.0135374,"[when][hit][by][electrons][,][a][phosphor][gives] Tools: for SVM learning, we used the SVMLight[off][electromagnetic][energy][in][this][form] TK software4 , which includes structural kernels in PS: [wrb][vbn][in][nns][,][dt][nn][vbz][rp][jj][nn][in] SVMLight (Joachims, 1999)5 . For generating con[dt][nn] 3 Additionally, we use constituency trees (CTs), see 2 From here the name syntactic tree kernels 716 Past Jeopardy! games can be downloaded from http://www.j-archive.com. 4 Available at http://dit.unitn.it/∼moschitt 5 http://svmlight.joachims.org stituency trees, we used the Charniak parser (Charniak, 2000). We also used the syntactic–semantic parser by Johansson and Nugues (2008) to generate dependency trees (Mel’ˇcuk, 1988) and predicate argument trees according to the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Baseline Model: the first model that we used as a baseline is a rule-based classifier (RBC). The RBC leverages a set of rules that matches against lexical and syntactic information in the clue to make a binary decision on whether or not the clue is considered definitional. The rule set was manually developed by a human expert, and consists of rules that"
D11-1066,P06-2010,0,0.0474861,"Missing"
D11-1066,P02-1034,0,0.0610751,"ing three different kernels: • Sequence Kernels (SK); we implemented the 1 discontinuous string kernels described in (ShaweTaylor and Cristianini, 2004). This allows for representing a string of symbols in terms of its possible substrings with gaps, i.e. an arbitrary number of symbols can be skipped during the generation of a substring. The symbols we used in the sequential descriptions of questions are words and part-of-speech tags (in two separate sequences). Consequently, all possible multiwords with gaps are features of the implicitly generated vector space. • Syntactic Tree Kernel (STK) (Collins and Duffy, 2002) applied to constituency parse trees. This generates all possible tree fragments as features with the conditions that sibling nodes from the original trees cannot be separated. In other words, substructures are composed by atomic building blocks corresponding to nodes along with all their direct children. These, in case of a syntactic parse tree, are complete production rules of the associated parser grammar2 . • Partial Tree Kernel (PTK) (Moschitti, 2006) applied to both constituency and dependency parse trees. This generates all possible tree fragments, as above, but sibling nodes can be sep"
D11-1066,P04-1054,0,0.155675,"Missing"
D11-1066,W04-3233,0,0.0718242,"Missing"
D11-1066,P03-1003,0,0.031502,"reviously mentioned Jeopardy! questions are stated as affirmative sentences, which are different from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki 720 et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008). Our work and methods are similar to (Zhang and Lee, 2003; Moschitti et al., 2007), which achieved the state-of-the-art in QC by applying SVMs along with STK-CT. The results were derived by exper"
D11-1066,P05-1050,0,0.0605612,"Missing"
D11-1066,W08-2123,0,0.0740335,"Missing"
D11-1066,W07-1206,0,0.023133,"n annuity is an investment or retirement fund that pays out this often (answer: yearly) Even though the clue is nearly identical to (3), the clue does not provide a definition for the answer yearly, although at first glance we may have been misled. The source of complexity is given by the fact that Jeopardy! clues are not phrased in interrogative form as questions typically are. This complicates the design of definition classifiers since we cannot directly use either typical structural patterns that characterize definition/description questions, or previous approaches, e.g. (Ahn et al., 2004; Kaisser and Webber, 2007; Blunsom et al., 2006). Given the complexity and the novelty of the task, we found it useful to exploit the kernel methods technology. This has shown state-of-the-art performance in Question Classification (QC), e.g. (Zhang and Lee, 2003; Suzuki et al., 2003; Moschitti et al., 2007) and it is very well suited for engineering feature representations for novel tasks. In this paper, we apply SVMs and kernel methods to syntactic/semantic structures for modeling accurate classification of Jeopardy! definition questions. For this purpose, we use several levels of linguistic information: word and PO"
D11-1066,H05-1018,0,0.0419426,"Missing"
D11-1066,P03-1004,0,0.093103,"Missing"
D11-1066,P05-1024,0,0.0648996,"Missing"
D11-1066,C02-1150,0,0.510942,"Missing"
D11-1066,W04-2705,0,0.247345,"e 2 and dependency structures converted into the dependency trees (DTs), e.g. shown in Figure 3. Note that, the POS-tags are central nodes, the grammatical relation label is added as a father node and all the relations with the other nodes are described by means of the connecting edges. Words are considered additional children of the POS-tag nodes (in this case the connecting edge just serves to add a lexical feature to the target POS-tag node). Finally, we also use predicate argument structures generated by verbal and nominal relations according to PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). Given the target sentence, the set of its predicates are extracted and converted into a forest, then a fake root node, PAS, is used to connect these trees. For example, Figure 4 illustrates a Predicate Argument Structures Set (PASS) encoding two relations, give and hit, as well as the nominalization energy along with all their arguments. 4 Experiments on Definition Question Classification In these experiments, we study the role of kernel technology for the design of accurate classification of definition questions. We build several classifiers based on SVMs and kernel methods. Each classifier"
D11-1066,W05-0630,1,0.864591,"Missing"
D11-1066,P07-1098,1,0.630672,"the fact that Jeopardy! clues are not phrased in interrogative form as questions typically are. This complicates the design of definition classifiers since we cannot directly use either typical structural patterns that characterize definition/description questions, or previous approaches, e.g. (Ahn et al., 2004; Kaisser and Webber, 2007; Blunsom et al., 2006). Given the complexity and the novelty of the task, we found it useful to exploit the kernel methods technology. This has shown state-of-the-art performance in Question Classification (QC), e.g. (Zhang and Lee, 2003; Suzuki et al., 2003; Moschitti et al., 2007) and it is very well suited for engineering feature representations for novel tasks. In this paper, we apply SVMs and kernel methods to syntactic/semantic structures for modeling accurate classification of Jeopardy! definition questions. For this purpose, we use several levels of linguistic information: word and POS tag sequences, dependency, constituency and predicate argument structures and we combined them using state-ofthe-art structural kernels, e.g. (Collins and Duffy, 713 2002; Shawe-Taylor and Cristianini, 2004; Moschitti, 2006). The extensive empirical analysis of several advanced mod"
D11-1066,J08-2003,1,0.882524,"Missing"
D11-1066,D09-1143,1,0.876937,"Missing"
D11-1066,J05-1004,0,0.262489,"finition Jeopardy! question: Figure 2 and dependency structures converted into the dependency trees (DTs), e.g. shown in Figure 3. Note that, the POS-tags are central nodes, the grammatical relation label is added as a father node and all the relations with the other nodes are described by means of the connecting edges. Words are considered additional children of the POS-tag nodes (in this case the connecting edge just serves to add a lexical feature to the target POS-tag node). Finally, we also use predicate argument structures generated by verbal and nominal relations according to PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). Given the target sentence, the set of its predicates are extracted and converted into a forest, then a fake root node, PAS, is used to connect these trees. For example, Figure 4 illustrates a Predicate Argument Structures Set (PASS) encoding two relations, give and hit, as well as the nominalization energy along with all their arguments. 4 Experiments on Definition Question Classification In these experiments, we study the role of kernel technology for the design of accurate classification of definition questions. We build several classifiers based on SVMs a"
D11-1066,P05-1027,0,0.0409807,"Missing"
D11-1066,D07-1002,0,0.0310238,"rk Our paper studies the use of advanced representation for QC in the Jeopardy! domain. As previously mentioned Jeopardy! questions are stated as affirmative sentences, which are different from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki 720 et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008). Our work and methods are similar to (Zhang and Lee, 2003; Moschitti et al., 2007), which achieved"
D11-1066,W03-1012,0,0.08161,"Missing"
D11-1066,C04-1189,0,0.0266264,"the other hand, the StatDef system outperformed the two other systems, and its accuracy improvement upon the RuleDef system is statistically significant at p&lt;0.05. 6 Related Work Our paper studies the use of advanced representation for QC in the Jeopardy! domain. As previously mentioned Jeopardy! questions are stated as affirmative sentences, which are different from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki 720 et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lap"
D11-1066,P08-1082,0,0.0261139,"Jeopardy! domain. As previously mentioned Jeopardy! questions are stated as affirmative sentences, which are different from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki 720 et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008). Our work and methods are similar to (Zhang and Lee, 2003; Moschitti et al., 2007), which achieved the state-of-the-art in QC by applying SVMs along with STK-CT. The re"
D11-1066,C02-1119,0,0.0667303,"Missing"
D11-1066,W03-1208,0,0.12756,"omplexity is given by the fact that Jeopardy! clues are not phrased in interrogative form as questions typically are. This complicates the design of definition classifiers since we cannot directly use either typical structural patterns that characterize definition/description questions, or previous approaches, e.g. (Ahn et al., 2004; Kaisser and Webber, 2007; Blunsom et al., 2006). Given the complexity and the novelty of the task, we found it useful to exploit the kernel methods technology. This has shown state-of-the-art performance in Question Classification (QC), e.g. (Zhang and Lee, 2003; Suzuki et al., 2003; Moschitti et al., 2007) and it is very well suited for engineering feature representations for novel tasks. In this paper, we apply SVMs and kernel methods to syntactic/semantic structures for modeling accurate classification of Jeopardy! definition questions. For this purpose, we use several levels of linguistic information: word and POS tag sequences, dependency, constituency and predicate argument structures and we combined them using state-ofthe-art structural kernels, e.g. (Collins and Duffy, 713 2002; Shawe-Taylor and Cristianini, 2004; Moschitti, 2006). The extensive empirical analysi"
D11-1066,W06-2902,0,0.0654666,"Missing"
D11-1066,W04-3222,0,0.0727528,"Missing"
D11-1066,C08-1121,1,0.905995,"Missing"
D11-1066,P06-1006,0,0.0626542,"Missing"
D11-1066,W02-1010,0,0.0866442,"Missing"
D11-1066,I05-1034,0,0.0321659,"Missing"
D11-1066,N06-1037,0,0.0640236,"Missing"
D11-1066,P06-1136,0,\N,Missing
D11-1066,J80-1003,0,\N,Missing
E09-1024,P06-1051,1,0.85368,"Missing"
E09-1024,P02-1034,0,0.421577,"substructures called partial trees fragments (PTFs). These can be generated by the application of partial production rules of the grammar, consequently [VP [V]] and [VP [NP]] are valid PTFs. Figure 1(b) shows that the number of PTFs derived from the same tree as before is still higher (i.e. 30 PTs). 3.4 nc(n1 ) ∆(n1 , n2 ) = Y (σ + ∆(cjn1 , cjn2 )) (1) j=1 where σ ∈ {0, 1}, nc(n1 ) is the number of children of n1 and cjn is the j-th child of the node n. Note that, since the productions are the same, nc(n1 ) = nc(n2 ). ∆(n1 , n2 ) evaluates the number of STFs common to n1 and n2 as proved in (Collins and Duffy, 2002). Moreover, a decay factor λ can be added by modifying steps (2) and (3) as follows2 : 2. ∆(n1 , n2 ) = λ, Qnc(n ) 3. ∆(n1 , n2 ) = λ j=1 1 (σ + ∆(cjn1 , cjn2 )). The computational complexity of Eq. 1 is O(|NT1 |× |NT2 |) but as shown in (Moschitti, 2006), the average running time tends to be linear, i.e. O(|NT1 |+ |NT2 |), for natural language syntactic trees. Counting Shared SubTrees The main idea of tree kernels is to compute the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space. To evaluate the above kernels between two T1 an"
E09-1024,N01-1025,0,0.123821,"xperiments (CER) using FST and SVMs with the Sytntactic Tree Kernel (STK) on two different corpora: LUNA WOZ + HH, and MEDIA. Split Training based on SVMs and STK3 , on the largest datasets, i.e. WOZ merged with HH dialogs and Media. We trained all the SCLMs used in our experiments with the SRILM toolkit (Stolcke, 2002) and we used an interpolated model for probability estimation with the Kneser-Ney discount (Chen and Goodman, 1998). We then converted the model in an FST as described in Section 2.1. The model used to obtain the SVM baseline for concept classification was trained using YamCHA (Kudo and Matsumoto, 2001). For the reranking models based on structure kernels, SVMs or perceptron, we used the SVM-Light-TK toolkit (available at dit.unitn.it/moschitti). For λ (see Section 3.2), cost-factor and trade-off parameters, we used, 0.4, 1 and 1, respectively. 4.3 STK 18.5 18.5 18.5 Monolithic Training SVM PCT PTK SK STK PTK 19.3 19.1 24.2 28.3 19.3 19.0 29.4 23.7 19.3 19.1 31.5 30.0 WOZ RR-A RR-B RR-C STK 20.0 19.0 19.0 SVM PTK 18.0 19.0 18.4 SK 16.1 19.0 16.6 STK 28.4 26.3 27.1 PCT PTK 29.8 30.0 26.2 SK 27.8 25.6 30.3 Table 5: Results of experiments, in terms of Concept Error Rate (CER), on the LUNA WOZ c"
E09-1024,W04-2403,1,0.906112,"d concepts. In the last decade two major approaches have been proposed to find this correlation: (i) generative models, whose parameters refer to the joint Proceedings of the 12th Conference of the European Chapter of the ACL, pages 202–210, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 202 ing of long distance dependencies between words in relatively small n-grams. Given the huge size of this feature space, we adopted kernel methods and in particular sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Raymond and Riccardi, 2007; Moschitti and Bejan, 2004; Moschitti, 2006) to implicitly encode n-grams and other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the well-known MEDIA corpus (Bonneau-Maynard et al., 2005) and a new corpus acquired in the European project LUNA1 (Raymond et al., 2007). The results show a great improvement with respect to both the FST-based model and the SVM model alone, which are the current state-of-the-art for concept classification on such corpora. The rest of the paper is organized as follows: Sections 2 and 3 show the gene"
E09-1024,W06-2909,1,0.887769,"kernel: eral than Eq. 1. Indeed, if we only consider the contribution of the longest child sequence from KR (e1 , e2 ) = SK(s11 , s12 ) + SK(s21 , s22 ) (3) node pairs that have the same children, we imple− SK(s11 , s22 ) − SK(s21 , s12 ) ment the STK kernel. This schema, consisting in summing four differ3.7 Re-ranking models using sequences ent kernels, has been already applied in (Collins and Duffy, 2002) for syntactic parsing re-ranking, The FST generates the m most likely concept anwhere the basic kernel was a tree kernel instead of notations. These are used to build annotation SK and in (Moschitti et al., 2006), where, to repairs, si , sj , which are positive instances if si j rank Semantic Role Labeling annotations, a tree has a lower concept annotation error than s , with kernel was used on a semantic tree similar to the respect to the manual annotation in the corpus. i one introduced in the next section. Thus, a trained binary classifier can decide if s j is more accurate than s . Each candidate anno3.8 Re-ranking models using trees tation si is described by a word sequence where Since the aim in concept annotation re-ranking is each word is followed by its concept annotation. to exploit innovati"
E09-1024,J08-2003,1,0.896247,"Missing"
ghosh-etal-2012-improving,J93-2004,0,\N,Missing
ghosh-etal-2012-improving,W10-2910,1,\N,Missing
ghosh-etal-2012-improving,W05-1506,0,\N,Missing
ghosh-etal-2012-improving,J08-2005,0,\N,Missing
ghosh-etal-2012-improving,prasad-etal-2008-penn,0,\N,Missing
ghosh-etal-2012-improving,P09-2004,0,\N,Missing
ghosh-etal-2012-improving,I08-7009,0,\N,Missing
ghosh-etal-2012-improving,I11-1120,1,\N,Missing
ghosh-etal-2012-improving,I08-7010,0,\N,Missing
ghosh-etal-2012-improving,D07-1010,0,\N,Missing
I11-1120,W05-0305,0,0.333358,"results. However, their task was limited to retrieving the argument heads. In contrast, we integrate discourse segmentation in the parsing pipeline because we believe that spans are necessary when using the discourse arguments as input to applications such as opinion mining, where attributions need to be explicitly marked. Besides, no gold data are available for head-based discourse parsing evaluation and they have to be automatically derived from parse trees with a further processing step. With our approach, instead, we can directly use PDTB argument spans both for training and for testing. Dinesh et al. (2005) extracted complete arguments with boundaries, but only for a restricted class of connectives. The recent work by Prasad et al. (2010) is also limited, since their system only extracts the sentences containing the arguments. In our work, we assume that explicit discourse connectives are given beforehand, either taken directly from a gold standard or automatically identified. The second task based on PDTB was tackled among others by Pitler et al. (2008) and Pitler and Nenkova (2009). In addition to the work on finding explicit connectives and their arguments, there has been recent work on class"
I11-1120,W10-2910,1,0.667362,"out explicit relations and Arg1 extension. combinations are also represented. We used this tool because the output of CRF++ is compatible to CoNLL 2000 chunking shared task, and we view our task as a discourse chunking task. On the other hand, linear-chain CRFs for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Also Sha and Pereira (2003) claim that, as a single model, CRFs outperform other models for shallow parsing. 6.1 Evaluation methodology We present our results using precision, recall and F1 measures. Following Johansson and Moschitti (2010), we use three scoring schemes: exact, intersection (or partial), and overlap scoring. In the exact scoring scheme, a span extracted by the system is counted as correct if its extent exactly coincides with one in the gold standard. However, we also use the two other scoring schemes since exact scoring may be uninformative in some situations where it is enough to have a rough approximation of the argument spans. In the overlap scheme, an expression is counted as correctly detected if it overlaps with a gold standard argument, i.e. if their intersection is nonempty. The intersection scheme assig"
I11-1120,D09-1036,0,0.133389,"a restricted class of connectives. The recent work by Prasad et al. (2010) is also limited, since their system only extracts the sentences containing the arguments. In our work, we assume that explicit discourse connectives are given beforehand, either taken directly from a gold standard or automatically identified. The second task based on PDTB was tackled among others by Pitler et al. (2008) and Pitler and Nenkova (2009). In addition to the work on finding explicit connectives and their arguments, there has been recent work on classification of implicit discourse relations, see for instance Lin et al. (2009). In a similar classification experiment, Pitler et al. (2009) investigated features ranging from low-level word pairs to high-level linguistic cues, and demonstrated that it is useful to model the sequence of discourse relations using a sequence labeler. Although they both outperformed their respective baselines, this task is very difficult and performances are still very low. 3 The Penn Discourse Treebank (PDTB) The Penn Discourse Treebank (Prasad et al., 2008) is a resource including one million words from the Wall Street Journal (Marcus et al., 1993), annotated with discourse relations. Ba"
I11-1120,J93-2004,0,0.045018,"icit discourse relations, see for instance Lin et al. (2009). In a similar classification experiment, Pitler et al. (2009) investigated features ranging from low-level word pairs to high-level linguistic cues, and demonstrated that it is useful to model the sequence of discourse relations using a sequence labeler. Although they both outperformed their respective baselines, this task is very difficult and performances are still very low. 3 The Penn Discourse Treebank (PDTB) The Penn Discourse Treebank (Prasad et al., 2008) is a resource including one million words from the Wall Street Journal (Marcus et al., 1993), annotated with discourse relations. Based on the observation that “no discourse connective has yet been identified in any language that has other than two arguments” (Webber et al. (2010), p. 15), connectives in the PTDB are treated as discourse predicates taking two text spans as arguments, i.e. parts of the text that describe events, propositions, facts, situations. Such two arguments in the PDTB are just called Arg1 and Arg2 and are chosen according to syntactic criteria: Arg2 is the argument syntactically bound to the connective, while Arg1 is the other one. This means that the numbering"
I11-1120,prasad-etal-2008-penn,0,0.873646,"extraction of discourse arguments for given ex1071 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1071–1079, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP plicit discourse connectives – has been attempted a number of times. Soon after the initial release of the PDTB, it was realized that sentence-internal arguments may be located and classified using techniques similar to semantic role detection and classification methods. Wellner and Pustejovsky (2007) were the first to carry out such an experiment on the PDTB, and Elwell and Baldridge (2008) later improved over their results. However, their task was limited to retrieving the argument heads. In contrast, we integrate discourse segmentation in the parsing pipeline because we believe that spans are necessary when using the discourse arguments as input to applications such as opinion mining, where attributions need to be explicitly marked. Besides, no gold data are available for head-based discourse parsing evaluation and they have to be automatically derived from parse trees with a further processing step. With our approach, instead, we can directly use PDTB argument spans both for"
I11-1120,prasad-etal-2010-exploiting,0,0.494984,"empts have already been made in the direction of automatic classification of connectives, while token-level argument segmentation has not been explored. Therefore in this paper we will focus on the segmentation and labeling of discourse arguments (Arg1 and Arg2) with full spans, as defined in the annotation protocol of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). We present a methodology that, given explicit discourse connectives, automatically extracts discourse arguments by identifying Arg1 and Arg2 including the corresponding text spans. We call this approach shallow following Prasad et al. (2010) as opposed to tree-like representations of discourse, as in Rhetorical Structure Theory (Mann and Thompson, 1988). Indeed, we provide a flat chunk classification of discourse relations, building a non-hierarchical representation of the relations in a text. The discourse parser is designed as a cascade of argument-specific CRFs trained on different sets of lexical, syntactic and semantic features. The evaluation is made in terms of exact and partial match of arguments. The partial match condition may be useful in the case of noisy input or for applications that do not require exact alignment."
I11-1120,N03-1028,0,0.452917,"Missing"
I11-1120,D09-1018,0,0.0106177,"e show that the best combination of features includes syntactic and semantic features. The comparative error analysis investigates the performance variability over connective types and argument positions. 1 Introduction Automatic discourse processing is considered one of the most challenging NLP tasks due to its dependency on lexical and syntactic features and on the inter-sentential relations. While automatic discourse processing of structured documents or free text is still in its infancy, a number of applications of this technology in practical NLP systems have been proposed. For instance, Somasundaran et al. (2009) describe the use of discourse structure for opinion analysis. Other applications include conversational analysis and dialog systems (Tonelli et al., 2010). In this work we divide the whole task of discourse parsing into two sub-tasks: connective classification and argument segmentation and classification. Several successful attempts have already been made in the direction of automatic classification of connectives, while token-level argument segmentation has not been explored. Therefore in this paper we will focus on the segmentation and labeling of discourse arguments (Arg1 and Arg2) with fu"
I11-1120,tonelli-etal-2010-annotation,1,0.786179,"ver connective types and argument positions. 1 Introduction Automatic discourse processing is considered one of the most challenging NLP tasks due to its dependency on lexical and syntactic features and on the inter-sentential relations. While automatic discourse processing of structured documents or free text is still in its infancy, a number of applications of this technology in practical NLP systems have been proposed. For instance, Somasundaran et al. (2009) describe the use of discourse structure for opinion analysis. Other applications include conversational analysis and dialog systems (Tonelli et al., 2010). In this work we divide the whole task of discourse parsing into two sub-tasks: connective classification and argument segmentation and classification. Several successful attempts have already been made in the direction of automatic classification of connectives, while token-level argument segmentation has not been explored. Therefore in this paper we will focus on the segmentation and labeling of discourse arguments (Arg1 and Arg2) with full spans, as defined in the annotation protocol of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). We present a methodology that, given explicit"
I11-1120,D07-1010,0,0.697978,". Finally, we draw some conclusions in Section 7. 2 Related Work The task that we address in this paper – automatic extraction of discourse arguments for given ex1071 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1071–1079, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP plicit discourse connectives – has been attempted a number of times. Soon after the initial release of the PDTB, it was realized that sentence-internal arguments may be located and classified using techniques similar to semantic role detection and classification methods. Wellner and Pustejovsky (2007) were the first to carry out such an experiment on the PDTB, and Elwell and Baldridge (2008) later improved over their results. However, their task was limited to retrieving the argument heads. In contrast, we integrate discourse segmentation in the parsing pipeline because we believe that spans are necessary when using the discourse arguments as input to applications such as opinion mining, where attributions need to be explicitly marked. Besides, no gold data are available for head-based discourse parsing evaluation and they have to be automatically derived from parse trees with a further pr"
I11-1120,W03-3023,0,0.0709142,"ation. It includes for example the -ing and -ed suffixes in verb endings as well as the -s to form the plural of nouns. In our 2 We extracted this feature using the Chunklink.pl script made available by Sabine Buchholz at http://ilk.uvt. nl/team/sabine/chunklink/README.html 1074 example sentence, this feature would be for example s for “traders” and “heads”, etc. As for features (F7) and (F8), they rely on information about the main verb of the current sentence. More specifically, feature (F7) is the main verb token (i.e. shook in our example), extracted following the head-finding strategy by Yamada and Matsumoto (2003), while feature (F8) is a boolean feature that indicates for each token if it is the main verb in the sentence or not.3 The previous sentence feature “Prev” (F9) is a connective-surface feature and is used to capture if the following sentence begins with a connective. Our intuition is that it may be relevant to detect Arg1 boundaries in inter-sentential relations. The feature value for each candidate token of a sentence corresponds to the connective token that appears at the beginning of the following sentence, if any. Otherwise, it is equal to 0. We also add gold-standard Arg2 labels (F10) as"
I11-1120,P09-2004,0,0.636093,"rocessing step. With our approach, instead, we can directly use PDTB argument spans both for training and for testing. Dinesh et al. (2005) extracted complete arguments with boundaries, but only for a restricted class of connectives. The recent work by Prasad et al. (2010) is also limited, since their system only extracts the sentences containing the arguments. In our work, we assume that explicit discourse connectives are given beforehand, either taken directly from a gold standard or automatically identified. The second task based on PDTB was tackled among others by Pitler et al. (2008) and Pitler and Nenkova (2009). In addition to the work on finding explicit connectives and their arguments, there has been recent work on classification of implicit discourse relations, see for instance Lin et al. (2009). In a similar classification experiment, Pitler et al. (2009) investigated features ranging from low-level word pairs to high-level linguistic cues, and demonstrated that it is useful to model the sequence of discourse relations using a sequence labeler. Although they both outperformed their respective baselines, this task is very difficult and performances are still very low. 3 The Penn Discourse Treeban"
I11-1120,C08-2022,0,0.0290911,"se trees with a further processing step. With our approach, instead, we can directly use PDTB argument spans both for training and for testing. Dinesh et al. (2005) extracted complete arguments with boundaries, but only for a restricted class of connectives. The recent work by Prasad et al. (2010) is also limited, since their system only extracts the sentences containing the arguments. In our work, we assume that explicit discourse connectives are given beforehand, either taken directly from a gold standard or automatically identified. The second task based on PDTB was tackled among others by Pitler et al. (2008) and Pitler and Nenkova (2009). In addition to the work on finding explicit connectives and their arguments, there has been recent work on classification of implicit discourse relations, see for instance Lin et al. (2009). In a similar classification experiment, Pitler et al. (2009) investigated features ranging from low-level word pairs to high-level linguistic cues, and demonstrated that it is useful to model the sequence of discourse relations using a sequence labeler. Although they both outperformed their respective baselines, this task is very difficult and performances are still very low"
I11-1120,P09-1077,0,0.0761964,"d et al. (2010) is also limited, since their system only extracts the sentences containing the arguments. In our work, we assume that explicit discourse connectives are given beforehand, either taken directly from a gold standard or automatically identified. The second task based on PDTB was tackled among others by Pitler et al. (2008) and Pitler and Nenkova (2009). In addition to the work on finding explicit connectives and their arguments, there has been recent work on classification of implicit discourse relations, see for instance Lin et al. (2009). In a similar classification experiment, Pitler et al. (2009) investigated features ranging from low-level word pairs to high-level linguistic cues, and demonstrated that it is useful to model the sequence of discourse relations using a sequence labeler. Although they both outperformed their respective baselines, this task is very difficult and performances are still very low. 3 The Penn Discourse Treebank (PDTB) The Penn Discourse Treebank (Prasad et al., 2008) is a resource including one million words from the Wall Street Journal (Marcus et al., 1993), annotated with discourse relations. Based on the observation that “no discourse connective has yet b"
K15-2003,W13-5704,1,0.947467,", or addressed particular discourse parsing subtasks (Pitler and Nenkova, 2009). PDTB adopts non-hierarchical binary view on discourse relations: a discourse connective and its two arguments – Argument 1 and Argument 2, which is syntactically attached to the connective. And, a relation is assigned particular sense from the sense hierarchy. It was identified that parsing Explicit discourse relations, that are signaled by a presence of a discourse connective (a closed 2 System Architecture The discourse parser submitted for the CoNLL 2015 Shared Task is the extension of the parser described in (Stepanov and Riccardi, 2013; Stepanov and Riccardi, 2014). The overall architecture of the parser is depicted in Figure 1. The approach structures discourse parsing into a pipeline of several subtasks, mimicking the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) annotation procedure as in (Lin et al., 2014). The first step is Discourse Connective Detection (DCD) that identifies explicit discourse connectives and their spans. Then Connective Sense Classification (CSC) is used to classify these con25 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 25–31, c Beij"
K15-2003,W14-1105,1,0.907743,"Missing"
K15-2003,P10-1040,0,0.306914,"ir Generation (NPG) step a list of adjacent sentence pairs is generated omitting the inter-sentential explicit relations identified in the APC step. In the Non-Explicit Relation Detection (NRD) step the candidate pairs are classified as holding a relation or not. The pairs identified as 3 Features Besides tokens, the PDTB corpus distributed to the participants contains Part-of-Speech tags, constituency and dependency parses. These resources are used to extract and generate both token-level and argument/relation-level features. Additionally, for argument/relation-level features Brown Clusters (Turian et al., 2010) are used. 3.1 Token-level Features Discourse Connective Detection and Argument Span Extraction tasks of discourse parsing are cast as token-level sequence labeling with CRFs. The list of features used for the models is given in Table 1. Besides tokens and POS-tags, the rest of the features is described below. Chunk-tag is the syntactic chunk prefixed with the information whether a token is at the beginning (B-), inside (I-) or outside (O) of the constituent (i.e. IOB format) (e.g. ‘B-NP’ indicates that a token is at the beginning of Noun Phrase 26 Feature Token POS-tag Chunk-tag IOB-chain Dep"
K15-2003,I11-1120,1,0.949191,"elations’ being intra- or intersentential. Non-explicit relation detection and sense assignment tasks are cast as classification. In the end-to-end closedtrack evaluation, the parser ranked second with a global F-measure of 0.2184 1 Introduction Discourse parsing is a challenging Natural Language Processing (NLP) task that has utility for many other NLP tasks such as summarization, opinion mining, etc. (Webber et al., 2011). With the release of Penn Discourse Treebank (PDTB) (Prasad et al., 2008), the researchers have developed discourse parsers for all (e.g. (Lin et al., 2014) or some (e.g. (Ghosh et al., 2011)) discourse relation types in the PDTB definition, or addressed particular discourse parsing subtasks (Pitler and Nenkova, 2009). PDTB adopts non-hierarchical binary view on discourse relations: a discourse connective and its two arguments – Argument 1 and Argument 2, which is syntactically attached to the connective. And, a relation is assigned particular sense from the sense hierarchy. It was identified that parsing Explicit discourse relations, that are signaled by a presence of a discourse connective (a closed 2 System Architecture The discourse parser submitted for the CoNLL 2015 Shared T"
K15-2003,P09-2004,0,0.192774,"cation. In the end-to-end closedtrack evaluation, the parser ranked second with a global F-measure of 0.2184 1 Introduction Discourse parsing is a challenging Natural Language Processing (NLP) task that has utility for many other NLP tasks such as summarization, opinion mining, etc. (Webber et al., 2011). With the release of Penn Discourse Treebank (PDTB) (Prasad et al., 2008), the researchers have developed discourse parsers for all (e.g. (Lin et al., 2014) or some (e.g. (Ghosh et al., 2011)) discourse relation types in the PDTB definition, or addressed particular discourse parsing subtasks (Pitler and Nenkova, 2009). PDTB adopts non-hierarchical binary view on discourse relations: a discourse connective and its two arguments – Argument 1 and Argument 2, which is syntactically attached to the connective. And, a relation is assigned particular sense from the sense hierarchy. It was identified that parsing Explicit discourse relations, that are signaled by a presence of a discourse connective (a closed 2 System Architecture The discourse parser submitted for the CoNLL 2015 Shared Task is the extension of the parser described in (Stepanov and Riccardi, 2013; Stepanov and Riccardi, 2014). The overall architec"
K15-2003,P09-1077,0,0.0603534,"nd Xue, 2014) the authors map the tokens to Brown Clusters (Turian et al., 2010) and improve the classification into top-level senses. Inspired by the previous research, we have experimented with the following features that are extracted from both arguments: 1. Bag-of-Words; 2. Bag-of-Words prefixed with the argument ID (Arg1 or Arg2); 3. Cartesian product of all the tokens from both arguments; 4. Set of unique pairs from Cartesian product of Brown Clusters of all the tokens from both arguments (inspired by (Rutherford and Xue, 2014)); 5. First, last, and first 3 words of each argument (from (Pitler et al., 2009; Rutherford and Xue, 2014)); 6. Predicate, subject (both passive and active), direct and indirect objects, extracted from dependency parses (8 features); 27 propagation, going to the second level of the hierarchy drops the performance slightly below the flat classification. None of the other features listed in Table 1 has a positive effect on classification. Adding argument spans lowered the performance as well. 7. Ternary features for pairs from 6 to indicate matches (1, 0) or NULL, if one of the arguments misses the feature (extension of ‘similar subjects or main predicates’ feature of (Rut"
K15-2003,prasad-etal-2008-penn,0,0.840195,"asks for explicit relations are cast as token-level sequence labeling. The argument span decisions are conditioned on relations’ being intra- or intersentential. Non-explicit relation detection and sense assignment tasks are cast as classification. In the end-to-end closedtrack evaluation, the parser ranked second with a global F-measure of 0.2184 1 Introduction Discourse parsing is a challenging Natural Language Processing (NLP) task that has utility for many other NLP tasks such as summarization, opinion mining, etc. (Webber et al., 2011). With the release of Penn Discourse Treebank (PDTB) (Prasad et al., 2008), the researchers have developed discourse parsers for all (e.g. (Lin et al., 2014) or some (e.g. (Ghosh et al., 2011)) discourse relation types in the PDTB definition, or addressed particular discourse parsing subtasks (Pitler and Nenkova, 2009). PDTB adopts non-hierarchical binary view on discourse relations: a discourse connective and its two arguments – Argument 1 and Argument 2, which is syntactically attached to the connective. And, a relation is assigned particular sense from the sense hierarchy. It was identified that parsing Explicit discourse relations, that are signaled by a presenc"
K15-2003,E14-1068,0,0.111768,"arse) as a string and binary feature; 3.2 Argument & Relation-level Features In this section we describe features used for detecting non-explicit discourse relations and their sense classification. Since in these tasks the unit of classification is a relation rather than token, these features are extracted per argument of a relation and a relation as a whole. Previous work on the topic makes use of wide range of features ranging from first and last tokens of arguments to a Cartesian product of all tokens in both arguments, which leads to a very sparse feature set. To reduce the sparseness in (Rutherford and Xue, 2014) the authors map the tokens to Brown Clusters (Turian et al., 2010) and improve the classification into top-level senses. Inspired by the previous research, we have experimented with the following features that are extracted from both arguments: 1. Bag-of-Words; 2. Bag-of-Words prefixed with the argument ID (Arg1 or Arg2); 3. Cartesian product of all the tokens from both arguments; 4. Set of unique pairs from Cartesian product of Brown Clusters of all the tokens from both arguments (inspired by (Rutherford and Xue, 2014)); 5. First, last, and first 3 words of each argument (from (Pitler et al."
K15-2003,K15-2001,0,\N,Missing
K16-2005,D14-1008,0,0.0223012,"onal neural networks to model implicit relations within the PDTB-framework. Along these lines and inspired by the work in Weiss (2015), we also see great potential in the use of neural network-based techniques to SDP. Similarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to"
K16-2005,P13-2013,0,0.0137898,"milarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves, and also to compare the efficiency of their individual components with other competing methods, as i.) those systems rely on different theories of discourse, e.g., P"
K16-2005,D14-1220,0,0.0130708,"for sense classification. Distributed representations for both arguments were obtained by vector concatenation of embeddings. An earlier attempt in a similar direction of representation learning (Bengio et al., 2013) has been made by Ji and Eisenstein (2014). The authors demonstrated successfully how to discriminatively learn a latent, low-dimensional feature representation for RST-style discourse parsing, which has the benefit of capturing the underlying meaning of elementary discourse units without suffering from data sparsity of the originally high dimensional input data. Closely related, Li et al. (2014) introduced a recursive neural network for discourse parsing which jointly models distributed representations for sentences based on words and syntactic information. The approach is motivated by Socher et al. (2013) and models the discourse unit’s root embedding to represent the whole discourse unit which is being obtained from its parts by an iterative process. Their system is made up of a binary structure classifier and a multi-class relation classifier and achieves similar performance compared to Ji and Eisenstein (2014). Very recently, Liu et al. (2016) and Zhang et al. (2015) have success"
K16-2005,W15-4612,0,0.0118349,"ng has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves, and also to compare the efficiency of their individual components with other competing methods, as i.) those systems rely on different theories of discourse, e.g., PDTB or RST; and ii) different (sub)modules involve custom settings, feature- and tool-specific parameters, (esp. for the most challenging task of implicit sense labeling). Furthermore, iii) most previous works are not directly comparable in terms of overall accuracies as their underl"
K16-2005,D09-1036,0,0.248149,"Missing"
K16-2005,K15-2006,1,0.739845,"e implicit relations by our neural net-based architecture described in Section 3 given only the tokens and their dependencies in both argument spans. Finally, we merge all combined explicit and re-classified implicit relations into the final set for evaluation. 4.2 3. We heuristically post-process the CRF-labeled argument tokens in order to assign connectors to same-sentence or separate-sentence Arg1 and Arg2 spans. 4. The so-obtained explicit argument pairs are sense labeled by a (linear-kernel) SVM classifier12 with the connector word as the only feature, following the minimalist setting in Chiarcos and Schenk (2015). 5. As implicit relations we consider all intersentential relations which are not already part of an explicit relation. Same-sentence relations are ignored altogether. 4.4 For the provided argument pairs, we label explicit relations (i.e. those containing a non-empty connector) by the SVM classifier which has been trained using only a single feature – the connector token. For all other relations, we again employ our neural network-based strategy described in Section 3. The overall architecture is exactly the same as for the English subtask; only the (hyper)parameters have been updated in acco"
K16-2005,P09-1075,0,0.021725,"ized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves, and also to compare the efficiency of their individual components with other competing methods, as i.) those systems rely on different theories of discourse, e.g., PDTB or RST; and ii) different (sub)modules involve custom settings, feature- and tool-specific parameters, (esp. for the most challenging task of implicit sense labeling). Furthermore, iii) most previous works are not directly comparable in terms of overall accuracies as their underlying evaluation data suffers f"
K16-2005,P12-1007,0,0.190241,"ment identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves, and also to compare the efficiency of their individual components with other competing methods, as i.) those systems rely on different theories of discourse, e.g., PDTB or RST; and ii) different (sub)modules involve custom settings, feature- and tool-specific parameters, (esp. for the most challenging task of implicit sense labeling). Furthermore, iii) most previous works are not directly comparable in terms of overall accuracies as their underlying evaluation data suffers from inconsistent label"
K16-2005,P02-1047,0,0.134441,"work in Weiss (2015), we also see great potential in the use of neural network-based techniques to SDP. Similarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves, and also to compare the efficiency of their individual comp"
K16-2005,W12-1622,1,0.854026,"ly applied convolutional neural networks to model implicit relations within the PDTB-framework. Along these lines and inspired by the work in Weiss (2015), we also see great potential in the use of neural network-based techniques to SDP. Similarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enor"
K16-2005,I11-1170,0,0.323049,"selves, and also to compare the efficiency of their individual components with other competing methods, as i.) those systems rely on different theories of discourse, e.g., PDTB or RST; and ii) different (sub)modules involve custom settings, feature- and tool-specific parameters, (esp. for the most challenging task of implicit sense labeling). Furthermore, iii) most previous works are not directly comparable in terms of overall accuracies as their underlying evaluation data suffers from inconsistent label sizes among studies (e.g., full sense inventory vs. simplified 1- or 2-level classes, cf. Huang and Chen (2011)). 42 approach substitutes the traditional sparse and hand-crafted features from the literature to account for a minimalist, but at the same time, general (latent) representation of the discourse units. In the next sections, we elaborate on our novel neural network-based approach for implicit sense labeling and how it is fit into the overall system architecture of the parser. 3 proving their predictive power in the sense classification task. Specifically, the pre-trained vectors of size 300 were updated by the skip-gram method (Mikolov et al., 2013)5 in multiple passes over the Newswire texts"
K16-2005,P08-1028,0,0.0608724,"over the Newswire texts with decreasing learning rate. This procedure is supposed to improve the quality of the embeddings and also their coverage. Our new word vector model provides general vector representations for each token in the two argument spans6 , which forms the basis for producing compositional vectors to represent the two spans. Compositional vectors that introduce a fixed-length representation of a variable-length span of tokens are practical features for feedforward neural networks. Thus, we may combine the token vectors of each span by simply averaging vectors, or – following Mitchell and Lapata (2008) – by calculating an aggregated argument vector v~0 : A Neural Sense Labeler for Implicit and Entity Relations We construct a neural network-based module for the classification of senses for both implicit and entity (EntRel) relations.3 As a very general and highly data-driven approach to modeling discourse relations, our classifier incorporates only word embeddings and basic syntactic dependency information. Also, in order to keep the setup easily adaptable to new data and other languages, we avoid the use of very specific and costly hand-crafted features (such as sentiment polarities, word-p"
K16-2005,P14-1002,0,0.015102,"ost prior works on implicit SDP, but rather relies extensively on features learned from data. 2 2.1 Deep Learning Approaches to SDP In last year’s shared task, first implementations on deep learning have seen a surge of interest: Wang et al. (2015) and Okita et al. (2015) proposed a recurrent neural network for argument identification and a paragraph vector model for sense classification. Distributed representations for both arguments were obtained by vector concatenation of embeddings. An earlier attempt in a similar direction of representation learning (Bengio et al., 2013) has been made by Ji and Eisenstein (2014). The authors demonstrated successfully how to discriminatively learn a latent, low-dimensional feature representation for RST-style discourse parsing, which has the benefit of capturing the underlying meaning of elementary discourse units without suffering from data sparsity of the originally high dimensional input data. Closely related, Li et al. (2014) introduced a recursive neural network for discourse parsing which jointly models distributed representations for sentences based on words and syntactic information. The approach is motivated by Socher et al. (2013) and models the discourse un"
K16-2005,K15-2011,0,0.0167264,"hitecture is modular, highly generic and mostly language-independent, by leveraging the full power of pre-trained word embeddings for the SDP sense classification task. Our parser performs well on both English and Chinese data and is highly competitive with the state-of-the-art, though does not require manual feature engineering as employed in most prior works on implicit SDP, but rather relies extensively on features learned from data. 2 2.1 Deep Learning Approaches to SDP In last year’s shared task, first implementations on deep learning have seen a surge of interest: Wang et al. (2015) and Okita et al. (2015) proposed a recurrent neural network for argument identification and a paragraph vector model for sense classification. Distributed representations for both arguments were obtained by vector concatenation of embeddings. An earlier attempt in a similar direction of representation learning (Bengio et al., 2013) has been made by Ji and Eisenstein (2014). The authors demonstrated successfully how to discriminatively learn a latent, low-dimensional feature representation for RST-style discourse parsing, which has the benefit of capturing the underlying meaning of elementary discourse units without"
K16-2005,W12-1614,0,0.127357,"d techniques to SDP. Similarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves, and also to compare the efficiency of their individual components with other competing methods, as i.) those systems rely on different theor"
K16-2005,K15-2002,0,0.115549,"is completed by alternative lexicalization (AltLex, discourse marker rephrased), entity relation (EntRel, i.e., anaphoric coherence), resp. the absence of any relation (NoRel). 41 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 41–49, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Fortunately, with the first edition of the shared task on SDP, Xue et al. (2015) had established a unified framework and had made an independent evaluation possible. The best performing participating systems – most notably those by Wang and Lan (2015) and Stepanov et al. (2015) – have reimplemented the well-established techniques, for example the one by Lin et al. (2014). between any given argument pair in the PDTB. Our Contribution: We participate in the CoNLL 2016 Shared Task on SDP (Xue et al., 2016; Potthast et al., 2014) and propose a novel, neural network-based approach for implicit sense labeling. Its system architecture is modular, highly generic and mostly language-independent, by leveraging the full power of pre-trained word embeddings for the SDP sense classification task. Our parser performs well on both English and Chinese dat"
K16-2005,P09-2004,0,0.0374985,"in the PDTB-framework. Along these lines and inspired by the work in Weiss (2015), we also see great potential in the use of neural network-based techniques to SDP. Similarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves,"
K16-2005,K15-2014,0,0.0209491,"abeling. Its system architecture is modular, highly generic and mostly language-independent, by leveraging the full power of pre-trained word embeddings for the SDP sense classification task. Our parser performs well on both English and Chinese data and is highly competitive with the state-of-the-art, though does not require manual feature engineering as employed in most prior works on implicit SDP, but rather relies extensively on features learned from data. 2 2.1 Deep Learning Approaches to SDP In last year’s shared task, first implementations on deep learning have seen a surge of interest: Wang et al. (2015) and Okita et al. (2015) proposed a recurrent neural network for argument identification and a paragraph vector model for sense classification. Distributed representations for both arguments were obtained by vector concatenation of embeddings. An earlier attempt in a similar direction of representation learning (Bengio et al., 2013) has been made by Ji and Eisenstein (2014). The authors demonstrated successfully how to discriminatively learn a latent, low-dimensional feature representation for RST-style discourse parsing, which has the benefit of capturing the underlying meaning of elementary"
K16-2005,P09-1077,0,0.159442,"also see great potential in the use of neural network-based techniques to SDP. Similarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves, and also to compare the efficiency of their individual components with other com"
K16-2005,P15-3003,0,0.0115197,"ntactic information. The approach is motivated by Socher et al. (2013) and models the discourse unit’s root embedding to represent the whole discourse unit which is being obtained from its parts by an iterative process. Their system is made up of a binary structure classifier and a multi-class relation classifier and achieves similar performance compared to Ji and Eisenstein (2014). Very recently, Liu et al. (2016) and Zhang et al. (2015) have successfully applied convolutional neural networks to model implicit relations within the PDTB-framework. Along these lines and inspired by the work in Weiss (2015), we also see great potential in the use of neural network-based techniques to SDP. Similarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2"
K16-2005,prasad-etal-2008-penn,0,0.162386,"Missing"
K16-2005,E14-1068,0,0.0618245,"ins a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves, and also to compare the efficiency of their individual components with other competing methods, as i.) those systems rely on different theories of discourse, e.g., PDTB or RST; and ii) differe"
K16-2005,D15-1266,0,0.279615,"losely related, Li et al. (2014) introduced a recursive neural network for discourse parsing which jointly models distributed representations for sentences based on words and syntactic information. The approach is motivated by Socher et al. (2013) and models the discourse unit’s root embedding to represent the whole discourse unit which is being obtained from its parts by an iterative process. Their system is made up of a binary structure classifier and a multi-class relation classifier and achieves similar performance compared to Ji and Eisenstein (2014). Very recently, Liu et al. (2016) and Zhang et al. (2015) have successfully applied convolutional neural networks to model implicit relations within the PDTB-framework. Along these lines and inspired by the work in Weiss (2015), we also see great potential in the use of neural network-based techniques to SDP. Similarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument"
K16-2005,D13-1170,0,0.0030223,"2013) has been made by Ji and Eisenstein (2014). The authors demonstrated successfully how to discriminatively learn a latent, low-dimensional feature representation for RST-style discourse parsing, which has the benefit of capturing the underlying meaning of elementary discourse units without suffering from data sparsity of the originally high dimensional input data. Closely related, Li et al. (2014) introduced a recursive neural network for discourse parsing which jointly models distributed representations for sentences based on words and syntactic information. The approach is motivated by Socher et al. (2013) and models the discourse unit’s root embedding to represent the whole discourse unit which is being obtained from its parts by an iterative process. Their system is made up of a binary structure classifier and a multi-class relation classifier and achieves similar performance compared to Ji and Eisenstein (2014). Very recently, Liu et al. (2016) and Zhang et al. (2015) have successfully applied convolutional neural networks to model implicit relations within the PDTB-framework. Along these lines and inspired by the work in Weiss (2015), we also see great potential in the use of neural network"
K16-2005,C10-2172,0,0.0209829,"neural network-based techniques to SDP. Similarly, our approach trains a modular component for shallow discourse parsing which incorporates distributed word representations for argument spans by abstraction from surface-level (token) information. Crucially, our Related Work Most of the literature on automated discourse parsing has focused on specialized subtasks such as: 1. Argument identification (Ghosh et al., 2012; Kong et al., 2014) 2. Explicit sense classification (Pitler and Nenkova, 2009) 3. Implicit sense classification (Marcu and Echihabi, 2002; Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) A minimal requirement for any full-fledged endto-end discourse parser is to integrate at least these three processes into a sequential pipeline. However, until recently, only a handful of such parsers have existed (Lin et al., 2014; Biran and McKeown, 2015; duVerle and Prendinger, 2009; Feng and Hirst, 2012). It has been enormously difficult to evaluate the performance of these systems among themselves, and also to compare the efficiency of their individual components with other competing methods, as i.) those systems"
K16-2005,K15-2001,0,\N,Missing
K16-2005,P12-1008,0,\N,Missing
K16-2005,K15-2003,1,\N,Missing
K16-2012,P10-1040,0,0.328163,"r-sentential (PS) relations, on the other hand, a sentence containing the connective is selected as Argument 2, and the sentence immediately preceding it as a candidate for Argument 1. 3 Features The PDTB corpus distributed to the shared task participants contains raw text and syntactic constituency and dependency parses. Besides the token and part-of-speech tags, these resources are used to extract and generate both token-level and argument/relation-level features. Additionally, for argument/relation-level features for Non-Explicit Relation Sense Classification we make use of Brown Clusters (Turian et al., 2010), MPQA subjectivity lexicon (Wilson et al., 2005) and VerbNet (Kipper et al., 2008). The feature sets for each task are selected using greedy hill climbing approach, also considering the amount of contribution of each individual feature. 3.1 Token-level Features All the discourse parsing sub-tasks (both classification and sequence labeling) except Non-Explicit Relation Sense Classification make use of tokenlevel features. However, the feature sets for each task are different. Table 1 gives an overview of feature sets per task. Besides tokens and POS86 Raw Text Non-Explicit Relation Sense Class"
K16-2012,K15-2002,0,0.0431641,"of the majority senses – EntRel and Expansion.Conjunction. The flat classification mode is considered as it yields higher performance for these senses (e.g. for EntRel the classification into 4 top-level senses + EntRel yields F1 of ≈ 0.30, while flat classification into 14 full senses + EntRel F1 of 0.44). 4.2.2 Official Evaluation Results 6 Comparison to CoNLL 2015 Systems The current shared task is the second edition of the CoNLL Shared Task on Shallow Discourse Parsing. Thus, it makes sense to compare the performances of the submission to the systems of the first edition (i.e. the winner (Wang and Lan, 2015) and (Stepanov et al., 2015), which is taken as the baseline). Since the submitted system is an extension of (Stepanov et al., 2015), the main focus of the comparison is on the changes and their effects on the performance. We first compare the system performance to the last year’s systems on the end-to-end parsing score on the blind test set (see Table 3). The current submission outperforms the baseline (Stepanov et al., 2015) as well as the best system (ECNU) (Wang and Lan, 2015). The recall of the 2015 winner is slightly higher (0.2407 vs. 0.2432 for ECNU); however, the difference is well co"
K16-2012,H05-1044,0,0.265637,"sentence containing the connective is selected as Argument 2, and the sentence immediately preceding it as a candidate for Argument 1. 3 Features The PDTB corpus distributed to the shared task participants contains raw text and syntactic constituency and dependency parses. Besides the token and part-of-speech tags, these resources are used to extract and generate both token-level and argument/relation-level features. Additionally, for argument/relation-level features for Non-Explicit Relation Sense Classification we make use of Brown Clusters (Turian et al., 2010), MPQA subjectivity lexicon (Wilson et al., 2005) and VerbNet (Kipper et al., 2008). The feature sets for each task are selected using greedy hill climbing approach, also considering the amount of contribution of each individual feature. 3.1 Token-level Features All the discourse parsing sub-tasks (both classification and sequence labeling) except Non-Explicit Relation Sense Classification make use of tokenlevel features. However, the feature sets for each task are different. Table 1 gives an overview of feature sets per task. Besides tokens and POS86 Raw Text Non-Explicit Relation Sense Classification Non-Explicit Argument Pair Generation P"
K16-2012,prasad-etal-2008-penn,0,0.361432,"low Discourse Parsing with the main focus of the parser being on argument spans and the reduction of global error through model selection. In the end-to-end closed-track evaluation the parser achieves F-measure of 0.2510 outperforming the best system of the previous year. 1 Introduction Discourse parsing is a Natural Language Processing (NLP) task with the potential utility for many other Natural Language Processing tasks (Webber et al., 2011). However, as was illustrated by the CoNLL 2015 Shared Task on Shallow Discourse Parsing (Xue et al., 2015), the task of Penn Discourse Treebank (PDTB) (Prasad et al., 2008) style discourse parsing is very challenging as the best system achieved the end-to-end parsing performance of F1 = 0.24. The main reason for the low performance is the composite nature of the task and the error propagation through the long pipeline. In PDTB discourse relations are binary: a discourse connective and its two arguments. The arguments are defined syntactically such that Argument 2 is syntactically attached to the connective, and Argument 1 is the other argument. A discourse 85 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 85–91, c"
K16-2012,W13-5704,1,0.775755,"Missing"
K16-2012,W14-1105,1,0.852017,"individual discourse parsing subtasks are described in Section 4. Section 5 describes the official CoNLL 2016 Shared Task evaluation results, and in Section 6 we compare the system to the best systems of the preceding shared task on discourse parsing (Xue et al., 2015). Section 7 provides concluding remarks. 2 System Architecture The discourse parser submitted for the CoNLL 2016 Shared Task is the modified version of the parser developed by (Stepanov et al., 2015) for the shared task of 2015. The system is an extension of the explicit relation parser described in (Stepanov and Riccardi, 2013; Stepanov and Riccardi, 2014). The overall architecture of the parser is depicted in Figure 1. The approach implements discourse parsing as a pipeline of several tasks such that connective and argument span decisions are cast as sequence labeling and sense decisions as classification. The discourse parsing pipelines starts with the identification of discourse connectives and their spans (Discourse Connective Detection (DCD)), and is followed by Connective Sense Classification (CSC) and Argument Position Classification (APC) steps. While CSC assigns sense to explicit discourse relations, APC classifies them as intraand int"
K16-2012,K15-2001,0,\N,Missing
K16-2012,K15-2003,1,\N,Missing
L16-1020,bunt-etal-2010-towards,0,0.0349316,"TRAINS (Traum, 1996), DIT++ (Bunt, 2005) to task-specific needs; thus, creating incompatible annotations. The supervised and unsupervised annotation and classification of DAs (e.g. (Joty et al., 2011)) and cross-domain and cross-media classification (e.g. forums, email, and spoken conversations (Joty et al., 2011; Tavafi et al., 2013)) have single important drawback: since the sets of considered DAs are not consistent, introduced cross-corpora mappings are at best generalizations or subsets. Recently accepted international ISO standard for DA annotation – Dialogue Act Markup Language (DiAML) (Bunt et al., 2010; Bunt et al., 2012) – could serve as a lingua franca for cross-corpora DA mapping. However, such mappings might require significant amount of manual re-annotation effort. The utility of abandoning the legacy annotations and manually or semi-automatically re-mapping them to the ISO standard could be tested under two conditions: (1) if the new annotation is equal or superior in supervised DA classification and (2) if it is indeed domain independent and allows both cross-domain application and pooling data from different domains (i.e., data aggregation). Thus, in this paper we presents experimen"
L16-1020,bunt-etal-2012-iso,0,0.655166,"), DIT++ (Bunt, 2005) to task-specific needs; thus, creating incompatible annotations. The supervised and unsupervised annotation and classification of DAs (e.g. (Joty et al., 2011)) and cross-domain and cross-media classification (e.g. forums, email, and spoken conversations (Joty et al., 2011; Tavafi et al., 2013)) have single important drawback: since the sets of considered DAs are not consistent, introduced cross-corpora mappings are at best generalizations or subsets. Recently accepted international ISO standard for DA annotation – Dialogue Act Markup Language (DiAML) (Bunt et al., 2010; Bunt et al., 2012) – could serve as a lingua franca for cross-corpora DA mapping. However, such mappings might require significant amount of manual re-annotation effort. The utility of abandoning the legacy annotations and manually or semi-automatically re-mapping them to the ISO standard could be tested under two conditions: (1) if the new annotation is equal or superior in supervised DA classification and (2) if it is indeed domain independent and allows both cross-domain application and pooling data from different domains (i.e., data aggregation). Thus, in this paper we presents experiments on the semi-autom"
L16-1020,W13-0507,0,0.012538,"akers speaking), an utterance can span several turns. Thus, the dialogue act annotation was preceded by additional utterance segmentation. Full description of the DiAML annotation scheme (Bunt et al., 2012) is out of the scope of this paper. Rather we focus on the DA tag set and dimensions. The DiAML annotation scheme consists of 56 DA tags (communicative functions), organized into 9 dimensions: 26 general (applicable to any dimension) and 30 dimension specific (see Table 1, ISO column). The issues of converting DAMSL-based corpus to the ISO standard were addressed by (Fang et al., 2012) and (Bunt et al., 2013). Following the re-annotation methodology outlined in (Fang et al., 2012) we mapped LUNA DAs to DiAML. LUNA contains only 15 tags compared to DiAML’s 56, and most of the relations in the mapping are one-tomany. Even though, some of these relations can be disambiguated with respect to context (Petukhova et al., 2014) (e.g. if the DA in the previous turn is Info-Request and the current DA is Yes-Answer, there is a high chance that the former maps to Propositional Question and the latter to Confirm), since both relations are one-to-many, such mapping is error prone. Thus, automatic mapping is man"
L16-1020,W09-0505,1,0.915528,"pora DA mapping. However, such mappings might require significant amount of manual re-annotation effort. The utility of abandoning the legacy annotations and manually or semi-automatically re-mapping them to the ISO standard could be tested under two conditions: (1) if the new annotation is equal or superior in supervised DA classification and (2) if it is indeed domain independent and allows both cross-domain application and pooling data from different domains (i.e., data aggregation). Thus, in this paper we presents experiments on the semi-automatic re-annotation of the Italian LUNA Corpus (Dinarelli et al., 2009) with DiAML and evaluation of the annotations in ABBR G SOM AutoFb AlloFb TimeM TurnM Disc OSM PSM ISO 26 10 2 3 2 6 2 2 3 56 LUNA 8 4 3 – – – – 15 Table 1: Mapping LUNA dialogue acts to DiAML ISO Standard 9 dialogue act dimensions and communicative functions with counts per dimension. cross-domain and data aggregation settings. In the rest of the paper we describe the LUNA to ISO DA Mapping (Section 2.) and the annotation procedure (Section 3.). In Section 4. we report on the supervised DA classification experiments comparing the legacy and ISO annotation schemes; and the cross-domain perform"
L16-1020,petukhova-etal-2014-interoperability,0,0.143336,"scheme consists of 56 DA tags (communicative functions), organized into 9 dimensions: 26 general (applicable to any dimension) and 30 dimension specific (see Table 1, ISO column). The issues of converting DAMSL-based corpus to the ISO standard were addressed by (Fang et al., 2012) and (Bunt et al., 2013). Following the re-annotation methodology outlined in (Fang et al., 2012) we mapped LUNA DAs to DiAML. LUNA contains only 15 tags compared to DiAML’s 56, and most of the relations in the mapping are one-tomany. Even though, some of these relations can be disambiguated with respect to context (Petukhova et al., 2014) (e.g. if the DA in the previous turn is Info-Request and the current DA is Yes-Answer, there is a high chance that the former maps to Propositional Question and the latter to Confirm), since both relations are one-to-many, such mapping is error prone. Thus, automatic mapping is manually examined. Due to data distribution and for the consistency with the legacy annotation, we did not annotate all the dimensions: Discourse Structuring, Speech and Turn Management dimensions were mapped to Other. For cross-domain experiments, on the other hand, a set of 10 call center dialogues was sampled from l"
L16-1020,W13-4017,0,0.0140996,"tten), dialogue systems, etc.; and DAs have been extensively studied in both theoretical and computational linguistics. In the absence of a single commonly accepted standard, spoken corpora often adapt existing domain independent annotation schemes like DAMSL (Core and Allen, 1997), TRAINS (Traum, 1996), DIT++ (Bunt, 2005) to task-specific needs; thus, creating incompatible annotations. The supervised and unsupervised annotation and classification of DAs (e.g. (Joty et al., 2011)) and cross-domain and cross-media classification (e.g. forums, email, and spoken conversations (Joty et al., 2011; Tavafi et al., 2013)) have single important drawback: since the sets of considered DAs are not consistent, introduced cross-corpora mappings are at best generalizations or subsets. Recently accepted international ISO standard for DA annotation – Dialogue Act Markup Language (DiAML) (Bunt et al., 2010; Bunt et al., 2012) – could serve as a lingua franca for cross-corpora DA mapping. However, such mappings might require significant amount of manual re-annotation effort. The utility of abandoning the legacy annotations and manually or semi-automatically re-mapping them to the ISO standard could be tested under two c"
L16-1451,P12-1042,0,0.0200887,"place online every day in social forums and news blogs (Ruiz et al., 2011), and participants express agreement and disagreement with respect to each others’ positions and statements. From a communication analysis perspective, conversation in social media are asynchronous and participants can reply to any other, using text messages or precoded actions (e.g. like buttons). Previous work on Agreement/Disagreement Relations (henceforth ADRs) in asynchronous online debates, focused either on messages and overall positions of participants (Murakami and Raymond, 2010) (Somasundaran and Wiebe, 2009) (Abu-Jbara et al., 2012), or on the detection of ADRs in pairs of candidate sentences or parts of messages (Andreas et al., 2012). Motivated by the interest in the analysis of argumentation structures in asynchronous conversations, we produced a corpus annotated at message and sentence level in Italian. To do so, we developed a specific annotation tool. Both the corpus and the tool are available for research purposes1 under a LGPL license. To the best of our knowledge, this is the first resource of ADRs in Italian. We believe that the two levels of annotations maybe useful for argumentation analysis (Schneider et al."
L16-1451,W13-1614,0,0.0231808,"s in asynchronous conversations, we produced a corpus annotated at message and sentence level in Italian. To do so, we developed a specific annotation tool. Both the corpus and the tool are available for research purposes1 under a LGPL license. To the best of our knowledge, this is the first resource of ADRs in Italian. We believe that the two levels of annotations maybe useful for argumentation analysis (Schneider et al., 2013) as well as summarization (Di Fabbrizio et al., 2014), irony/sarcasm detection (Reyes et al., 2013) and other kind of parasemantic analyses in the social media domain (Basile and Nissim, 2013), (Celli and Polonio, 2013). 2. Figure 1: Example of agreement and disagreement relations. Definitions of ADRs ADRs in conversations can be defined in general terms as shared public commitments, that ground the speech acts performed by the bloggers within the conversations (Lascarides and Asher, 2008). Figure 1 reports an exmple of messages in agreement and disagreement to a news article. From an operational point of view, previous works in asynchronous conversations defined ADRs in different 1 ways. Bender considered ADRs as relationships among bloggers to a multiparty conversation, expressed"
L16-1451,W14-4408,0,0.051537,"Missing"
L16-1451,W08-0104,0,0.034516,"ce of ADRs in Italian. We believe that the two levels of annotations maybe useful for argumentation analysis (Schneider et al., 2013) as well as summarization (Di Fabbrizio et al., 2014), irony/sarcasm detection (Reyes et al., 2013) and other kind of parasemantic analyses in the social media domain (Basile and Nissim, 2013), (Celli and Polonio, 2013). 2. Figure 1: Example of agreement and disagreement relations. Definitions of ADRs ADRs in conversations can be defined in general terms as shared public commitments, that ground the speech acts performed by the bloggers within the conversations (Lascarides and Asher, 2008). Figure 1 reports an exmple of messages in agreement and disagreement to a news article. From an operational point of view, previous works in asynchronous conversations defined ADRs in different 1 ways. Bender considered ADRs as relationships among bloggers to a multiparty conversation, expressed at message level, with a post or turn text unit (Bender et al., 2011); Walker defined ADRs as Quote-Response message pairs and triplets (chains of three messages such that the third one is a response to the second one which is itself a response to the first one). These pairs and triplets are linked b"
L16-1451,W13-4006,0,0.0156642,"ional point of view, previous works in asynchronous conversations defined ADRs in different 1 ways. Bender considered ADRs as relationships among bloggers to a multiparty conversation, expressed at message level, with a post or turn text unit (Bender et al., 2011); Walker defined ADRs as Quote-Response message pairs and triplets (chains of three messages such that the third one is a response to the second one which is itself a response to the first one). These pairs and triplets are linked by the structure of the thread, where each message is a reply to its parent and is about the same topic (Misra and Walker, 2013) (Walker et al., 2012) (Morgan et al., 2013). Andreas defined ADRs between pairs of sentences, belonging to messages in a parent/child relation. In their definition, ADRs have a type (“agree”, “disagree” or “none”) and a mode (“direct“ or “indirect”, “response” or “paraphrase”). Wang targeted ADRs between text segments corresponding to one or several sentences (Wang and Cardie, 2014). Celli (Celli et al., 2014) defined the ADRs as a function that maps pairs of bloggers and messages to polarity values between 1 (“agree”) and -1 (“disagree”). The language http://sisl.disi.unitn.it/ resources can"
L16-1451,C10-2100,0,0.0168441,"ion makers. A large amount of multiparty conversations take place online every day in social forums and news blogs (Ruiz et al., 2011), and participants express agreement and disagreement with respect to each others’ positions and statements. From a communication analysis perspective, conversation in social media are asynchronous and participants can reply to any other, using text messages or precoded actions (e.g. like buttons). Previous work on Agreement/Disagreement Relations (henceforth ADRs) in asynchronous online debates, focused either on messages and overall positions of participants (Murakami and Raymond, 2010) (Somasundaran and Wiebe, 2009) (Abu-Jbara et al., 2012), or on the detection of ADRs in pairs of candidate sentences or parts of messages (Andreas et al., 2012). Motivated by the interest in the analysis of argumentation structures in asynchronous conversations, we produced a corpus annotated at message and sentence level in Italian. To do so, we developed a specific annotation tool. Both the corpus and the tool are available for research purposes1 under a LGPL license. To the best of our knowledge, this is the first resource of ADRs in Italian. We believe that the two levels of annotations m"
L16-1451,P09-1026,0,0.0324641,"multiparty conversations take place online every day in social forums and news blogs (Ruiz et al., 2011), and participants express agreement and disagreement with respect to each others’ positions and statements. From a communication analysis perspective, conversation in social media are asynchronous and participants can reply to any other, using text messages or precoded actions (e.g. like buttons). Previous work on Agreement/Disagreement Relations (henceforth ADRs) in asynchronous online debates, focused either on messages and overall positions of participants (Murakami and Raymond, 2010) (Somasundaran and Wiebe, 2009) (Abu-Jbara et al., 2012), or on the detection of ADRs in pairs of candidate sentences or parts of messages (Andreas et al., 2012). Motivated by the interest in the analysis of argumentation structures in asynchronous conversations, we produced a corpus annotated at message and sentence level in Italian. To do so, we developed a specific annotation tool. Both the corpus and the tool are available for research purposes1 under a LGPL license. To the best of our knowledge, this is the first resource of ADRs in Italian. We believe that the two levels of annotations maybe useful for argumentation a"
L16-1451,W11-0707,0,0.121455,"Missing"
L16-1451,2005.mtsummit-papers.11,0,0.0192573,"ne towards the parent message), disagreement (negative tone towards parent message), neutral (no opinion or tone expressed towards the parent message), none (if the relation between messages is unclear, i.e. contains only links, or mixed, i.e. contains both agreement and disagreement). ADR Annotation at the sentence level. The annotation space of ADRs at the sentence level is much larger than at the message space. In order to reduce this space, we put some constraints. We have automatically extracted the sentences from messages with a sentence splitter designed to work for multiple languages (Koehn 2005). Then we extracted candidate ADR sentence pairs that would share a common topic the ADR relation was grounded on. We extracted the topics of articles and conversations with Hierarchical LDA (Teh et al., 2006) (McCallum 2002) and used topic matching to automatically select candidate sentence pairs for the manual annotation, keeping only the root topic of the automatically generated tree. Then we further filtered automatically paired sentences with the following constraints: 1) Sentences must be contained into parent-child reply messages; 2) Sentences must share at least one topic; 3) Sentences"
L16-1451,walker-etal-2012-corpus,0,0.0203377,"ious works in asynchronous conversations defined ADRs in different 1 ways. Bender considered ADRs as relationships among bloggers to a multiparty conversation, expressed at message level, with a post or turn text unit (Bender et al., 2011); Walker defined ADRs as Quote-Response message pairs and triplets (chains of three messages such that the third one is a response to the second one which is itself a response to the first one). These pairs and triplets are linked by the structure of the thread, where each message is a reply to its parent and is about the same topic (Misra and Walker, 2013) (Walker et al., 2012) (Morgan et al., 2013). Andreas defined ADRs between pairs of sentences, belonging to messages in a parent/child relation. In their definition, ADRs have a type (“agree”, “disagree” or “none”) and a mode (“direct“ or “indirect”, “response” or “paraphrase”). Wang targeted ADRs between text segments corresponding to one or several sentences (Wang and Cardie, 2014). Celli (Celli et al., 2014) defined the ADRs as a function that maps pairs of bloggers and messages to polarity values between 1 (“agree”) and -1 (“disagree”). The language http://sisl.disi.unitn.it/ resources can requested at 3. Annot"
L16-1451,andreas-etal-2012-annotating,0,\N,Missing
L16-1701,bechet-etal-2012-decoda,1,0.828923,"Missing"
L16-1701,W09-0505,1,0.873958,"Missing"
N01-1018,J94-3001,0,\N,Missing
N01-1018,J99-2004,1,\N,Missing
N01-1018,woszczcyna-etal-1998-modular,0,\N,Missing
N01-1018,J01-1001,0,\N,Missing
N01-1018,W00-0508,1,\N,Missing
N01-1018,knight-al-onaizan-1998-translation,0,\N,Missing
N01-1018,P91-1032,0,\N,Missing
N01-1018,J97-3002,0,\N,Missing
N01-1018,J00-1003,0,\N,Missing
N01-1018,P98-1006,1,\N,Missing
N01-1018,C98-1006,1,\N,Missing
N09-2022,P98-1013,0,0.116367,"severely hurt performance. Also, a small set of FrameNet-like manual annotations is enough for realizing accurate Semantic Role Labeling on the target domains of typical Dialog Systems. 1 2 Introduction Commercial services based on spoken dialog systems have consistently increased both in number and in application scenarios (Gorin et al., 1997). Despite its success, current Spoken Language Understanding (SLU) technology is mainly based on simple conceptual annotation, where just very simple semantic composition is attempted. In contrast, the availability of richer semantic models as FrameNet (Baker et al., 1998) is very appealing for the design of better dialog managers. The first step to enable the exploitation of frame semantics is to show that accurate automatic semantic labelers can be designed for processing conversational speech. In this paper, we face the problem of performing shallow semantic analysis of speech transcrip85 FrameNet-based Semantic Role Labeling Semantic frames represent prototypical events or situations which individually define their own set of actors, or frame participants. For example, the C OMMERCE S CENARIO frame includes participants as S ELLER, B UYER, G OODS, and M ONE"
N09-2022,W05-0620,0,0.0993304,"Missing"
N09-2022,P02-1034,0,0.0342661,"manual engineering of effective features is a complex and time consuming process. For this reason, our SVM-based SRL approach exploits the combination of two different models. We first used Polynomial Kernels over handcrafted, linguistically-motivated, “standard” SRL features (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Xue and Palmer, 2004). Nonetheless, since we aim at modeling an SRL system for a new language (Italian) and a new domain (dialog transcriptions), the above features may result ineffective. Thus, to achieve independence on the application domain, we exploited Tree Kernels (Collins and Duffy, 2002) over automatic structural features proposed in (Moschitti et al., 2005; Moschitti et al., 2008). These are complementary to standard features and are obtained by applying Tree Kernels (Collins and Duffy, 2002; Moschitti et al., 2008) to basic tree structures expressing the syntactic relation between arguments and predicates. 3.1.1 3 Experiments Our purpose is to show that an accurate automatic FrameNet parser can be designed with reasonable effort for Italian conversational speech. For this purpose, we designed and evaluated both a semantic parser for the English FrameNet (Section 3.1) and on"
N09-2022,W05-0407,1,0.881548,"process. For this reason, our SVM-based SRL approach exploits the combination of two different models. We first used Polynomial Kernels over handcrafted, linguistically-motivated, “standard” SRL features (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Xue and Palmer, 2004). Nonetheless, since we aim at modeling an SRL system for a new language (Italian) and a new domain (dialog transcriptions), the above features may result ineffective. Thus, to achieve independence on the application domain, we exploited Tree Kernels (Collins and Duffy, 2002) over automatic structural features proposed in (Moschitti et al., 2005; Moschitti et al., 2008). These are complementary to standard features and are obtained by applying Tree Kernels (Collins and Duffy, 2002; Moschitti et al., 2008) to basic tree structures expressing the syntactic relation between arguments and predicates. 3.1.1 3 Experiments Our purpose is to show that an accurate automatic FrameNet parser can be designed with reasonable effort for Italian conversational speech. For this purpose, we designed and evaluated both a semantic parser for the English FrameNet (Section 3.1) and one for a corpus of Italian spoken dialogs (Section 3.2). The accuracy of"
N09-2022,J08-2003,1,0.866587,"Proceedings of NAACL HLT 2009: Short Papers, pages 85–88, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics arguments) are detected; and (iv) Role Classification (RC) (or argument classification), which assigns semantic labels to the frame elements detected in the previous step, e.g. G OODS. Therefore, we implement the full task of FrameNet-based parsing by a combination of multiple specialized SRL-like labelers, one for each frame (Coppola et al., 2008). For the design of each single labeler, we use the state-ofthe-art strategy developed in (Pradhan et al., 2005; Moschitti et al., 2008). according to the syntactic categories of the possible target predicates, namely nouns, verbs, adjectives, adverbs and prepositions; (b) we trained 782 one-versus-all multi-role classifiers RC, one for each available frame and predicate syntactic category, for a total of 5,345 binary classifiers; and (c) we applied the above models for recognizing predicate arguments and their associated semantic labels in sentences, where the frame label and the target predicate were considered as given. 2.1 Standard versus Structural Features In machine learning tasks, the manual engineering of effective fe"
N09-2022,W04-3212,0,0.0225285,"iers; and (c) we applied the above models for recognizing predicate arguments and their associated semantic labels in sentences, where the frame label and the target predicate were considered as given. 2.1 Standard versus Structural Features In machine learning tasks, the manual engineering of effective features is a complex and time consuming process. For this reason, our SVM-based SRL approach exploits the combination of two different models. We first used Polynomial Kernels over handcrafted, linguistically-motivated, “standard” SRL features (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Xue and Palmer, 2004). Nonetheless, since we aim at modeling an SRL system for a new language (Italian) and a new domain (dialog transcriptions), the above features may result ineffective. Thus, to achieve independence on the application domain, we exploited Tree Kernels (Collins and Duffy, 2002) over automatic structural features proposed in (Moschitti et al., 2005; Moschitti et al., 2008). These are complementary to standard features and are obtained by applying Tree Kernels (Collins and Duffy, 2002; Moschitti et al., 2008) to basic tree structures expressing the syntactic relation between arguments and predicat"
N09-2022,C98-1013,0,\N,Missing
N09-2022,J02-3001,0,\N,Missing
P09-4011,P03-1031,0,0.0199561,"lied to the task of dialogue management (DM) (Levin et al., 2000; Williams and Young, 2006). A major motivation is to improve robustness in the face of uncertainty, for example due to speech recognition errors. A further motivation is to improve adaptivity w.r.t. different user behaviour and application/recognition environments. The Reinforcement Learning framework is attractive because it offers a statistical model representing the dynamics of the interaction between system and user. This is in contrast to the supervised learning approach of learning system behaviour based on a fixed corpus (Higashinaka et al., 2003). To explore the range of dialogue management strategies, a simulation environment is required that includes a simulated user (Schatzmann et al., 2006) if one wants to avoid the prohibitive cost of using human subjects. The exploration/exploitation trade-off in reinforcement learning The RL-DM maintains a policy, an internal data structure that keeps track of the values (accumulated rewards) of past state-action pairs. The goal of the learner is to optimize the long-term reward by maximizing the ‘Q-Value’ Qπ (st , a) of a policy π for taking action a at time t. The expected cumulative value V"
P09-4011,W08-0109,1,0.834263,"st of SLU hypotheses. A list of possible user goals is stored in a database table (section 3) using a frame/slot representation. For each simulated dialogue, one or more user goals are randomly selected. The User Simulator’s task is to mimic a user wanting to perform such task(s). At each turn, the US mines the where au,t+1 is the true user action. 2 Rule-based Dialogue Management A rule-based dialogue manager was developed as a meaningful comparison to the trained DM, to obtain training data from human-system interaction for the user simulator, and to understand the properties of the domain (Varges et al., 2008). Rulebased dialog management works in two stages: retrieving and preprocessing facts (tuples) taken from a dialogue state database (section 3), and inferencing over those facts to generate a system response. We distinguish between the ‘context model’ of the first phase – essentially allowing 42 more recent values for a concept to override less recent ones – and the ‘dialog move engine’ (DME) of the second phase. In the second stage, acceptor rules match SLU results to dialogue context, for example perceived user concepts to open questions. This may result in the decision to verify the applica"
raymond-etal-2008-active,W99-0606,0,\N,Missing
raymond-etal-2008-active,poesio-artstein-2008-anaphoric,0,\N,Missing
raymond-etal-2008-active,W07-1524,1,\N,Missing
raymond-etal-2008-active,C02-1101,0,\N,Missing
raymond-etal-2008-active,P98-1013,0,\N,Missing
raymond-etal-2008-active,C98-1013,0,\N,Missing
stepanov-etal-2014-development,lefevre-etal-2012-leveraging,0,\N,Missing
stepanov-etal-2014-development,W09-0505,1,\N,Missing
stepanov-etal-2014-development,P98-1013,0,\N,Missing
stepanov-etal-2014-development,C98-1013,0,\N,Missing
tonelli-etal-2010-annotation,pareti-prodanof-2010-annotating,0,\N,Missing
tonelli-etal-2010-annotation,J93-2004,0,\N,Missing
tonelli-etal-2010-annotation,mladova-etal-2008-sentence,0,\N,Missing
tonelli-etal-2010-annotation,W05-0312,0,\N,Missing
tonelli-etal-2010-annotation,W09-3029,1,\N,Missing
tonelli-etal-2010-annotation,W09-0505,1,\N,Missing
tonelli-etal-2010-annotation,prasad-etal-2008-penn,1,\N,Missing
tonelli-etal-2010-annotation,I08-7009,0,\N,Missing
W00-0508,P98-1006,1,0.859074,"the chosen target language lexical items are reordered to produce a meaningful target language string. In our approach, we will represent these two phases using stochastic finitestate models which can be composed together to result in a single stochastic finite-state model for SMT. Thus our method can be viewed as a direct translation approach of transducing strings of the source language to strings of the target language. There are other approaches to statistical machine translation where translation is achieved through transduction of source language structure to target language structure (Alshawi et al., 1998b; Wu, 1997). There are also large international multi-site projects such as VERBMOBIL (Verbmobil, 2000) and CSTAR (Woszczyna et al., 1998; Lavie et al., 1999) that are involved in speech-to-speech translation in limited domains. The systems developed in these projects employ various techniques ranging from example-based to interlingua-based translation methods for translation between English, French, German, Italian, Japanese, and Korean. Abstract Stochastic finite-state models are efficiently learnable from data, effective for decoding and are associated with a calculus for composing models"
W00-0508,J94-3001,0,0.0286732,"c finite-state machine translation that is trained automatically from pairs of source and target utterances. We use this method to develop models for English-Japanese and Japanese-English translation. We have embedded the Japanese-English translation system in a call routing task of unconstrained speech utterances. We evaluate the efficacy of the translation system :in the context of this application. 1 Introduction Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997; Riccardi et al., 1996), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Srinivas, 1997) and parsing (Roche, 1.999). Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing. I In this paper, we develop stochastic finite-state models (SFSM) for statistical machine translation (SMT) and explore the performance limits of such models in the context of translati"
W00-0508,knight-al-onaizan-1998-translation,0,0.229362,"Missing"
W00-0508,W98-1122,1,0.824994,"Missing"
W00-0508,woszczcyna-etal-1998-modular,0,0.111101,"Missing"
W00-0508,J97-3002,0,0.0566754,"uage lexical items are reordered to produce a meaningful target language string. In our approach, we will represent these two phases using stochastic finitestate models which can be composed together to result in a single stochastic finite-state model for SMT. Thus our method can be viewed as a direct translation approach of transducing strings of the source language to strings of the target language. There are other approaches to statistical machine translation where translation is achieved through transduction of source language structure to target language structure (Alshawi et al., 1998b; Wu, 1997). There are also large international multi-site projects such as VERBMOBIL (Verbmobil, 2000) and CSTAR (Woszczyna et al., 1998; Lavie et al., 1999) that are involved in speech-to-speech translation in limited domains. The systems developed in these projects employ various techniques ranging from example-based to interlingua-based translation methods for translation between English, French, German, Italian, Japanese, and Korean. Abstract Stochastic finite-state models are efficiently learnable from data, effective for decoding and are associated with a calculus for composing models which allows"
W00-0508,J99-2004,1,\N,Missing
W00-0508,J93-2003,0,\N,Missing
W00-0508,J01-1001,0,\N,Missing
W00-0508,P02-1040,0,\N,Missing
W00-0508,N01-1018,1,\N,Missing
W00-0508,P91-1032,0,\N,Missing
W00-0508,J00-1003,0,\N,Missing
W00-0508,C98-1006,1,\N,Missing
W04-3218,P02-1049,0,0.0217751,"ing methods. Preliminary results are given for cluster interpretation and dynamic model adaptation using the clusters obtained. 1 Introduction The deployment of large scale automatic spoken dialog systems, like How May I Help You?SM (HMIHY) (Gorin et al., 1997), makes available large corpora of real human-machine dialog interactions. Traditionally, this data is used for supervised system evaluation. For instance, in (Kamm et al., 1999) they propose a static analysis aimed at measuring the performance of a dialog system, especially in an attempt to automatically estimate user satisfaction. In (Hastie et al., 2002), a dynamic stratDilek Hakkani-Tur AT&T Labs Florham Park, NJ, USA dtur@ research.att.com egy in the error handling process is proposed. In all these studies, supervised learning techniques are used in order to classify dialogs to predict user satisfaction or dialog failures. A novel approach to the exploitation of dialog corpora is for speech recognition and language understanding modeling. In fact, such corpora allow for a multidimensional analysis of speech and language models of dialog systems. Our work differs from previous studies in the algorithmic approach and learning scenario. First"
W07-1524,N06-2015,0,0.0181765,"tion at one level may make other levels of annotation unusable as well, and that it is not possible for two annotators to work on different types of annotation for the same file at the same time. Most current annotation efforts, therefore, tend to adopt the ’multilevel’ approach pioneered during the development of the MAPTASK corpus and then developed as part of work on the EU-funded MATE project (McKelvie et al., 2001), in which each aspect of interpretation is annotated in a separate level, independently maintained. This approach is being followed, for instance, in the O NTO N OTES project (Hovy et al., 2006) and the SAMMIE project (Kruijff-Korbayova et al., 2006a). For the annotation of the LUNA corpus, we decided to follow the multilevel approach as well. That allows us to achieve more granularity in the annotation of each of the levels and to investigate more easily dependencies between features that belong to different levels. Furthermore, we can use different specialized off-the-shelf annotation tools, splitting up the annotation task and thus facilitating consistent annotation. 2.3 Annotation levels The LUNA corpus will contain different types of information. The first levels are necessary t"
W07-1524,W03-0804,0,0.179794,"cular levels: examples include tools for segmentation and transcription of the speech signal like PRAAT (Boersma and Weenink, 2005) and T RANSCRIBER (Barras et al., 1998), the SALSA tools for FrameNetstyle annotation (Burchardt et al., 2006), and MMAX (M¨uller and Strube, 2003) for coreference annotation. Even in these cases, however, it may still be useful, or even necessary, to be able to visualize more than one level at once, or to ‘knit’ together2 multiple levels to create a file that can be used to train a model for a particular type of annotation. The Linguistic Annotation Framework by (Ide et al., 2003) was proposed as a unifying markup format to be used to synchronize heterogeneous markup formats for such purposes. In this paper, we discuss how the PAULA representation format, a standoff format inspired by the Linguistic Annotation Framework, is being used to synchronize multiple levels of annotation in the LUNA corpus, a corpus of spoken dialogues in multiple languages and multiple domains that is being created to support the development of robust spoken language understanding models for multilingual dialogue services. The corpus is richly annotated with linguistic information that is cons"
W07-1524,W06-2711,0,0.0335333,"Missing"
W07-1524,brugman-russel-2004-annotating,0,0.0337614,"nd employs a rich meta specification, which determines—based upon the individual corpus characteristics— the concrete linearization of the respective XML representation. Furthermore, it is accompanied by a JAVA API and a query tool, forming a valuable toolkit for corpus engineers who can adapt available resources to their specific needs. The ELAN format is used by a family of tools developed primarily for language documentation, of which the most advanced one is ELAN, a robust, ready-to-use tool for multi-level annotation of video. Its underlying data model is the Abstract Corpus Model (ACM) (Brugman and Russel, 2004). PAULA aims at an application scenario different from both of these formats. First, it builds upon the usage of specialized off-the-shelf annotation tools for the variety of annotation tasks. Both the NITE XML and ELAN approaches require additional effort and skills from the user, to add the required functionality, which PAULA aims to avoid. Second, PAULA takes care of merging the annotations from different sources, which is not in focus of ELAN or NITE. We presented the LUNA dialogue corpus and its representation format, the standoff exchange format PAULA . In contrast to other formats, PAUL"
W07-1524,burchardt-etal-2006-salto,0,0.0222042,"Infso, Unit E1 and in the Collaborative Research Center 632 “Information Structure”, funded by the German Science Foundation, http://www.sfb632.uni-potsdam.de. tate as well as maintain all annotation levels (cf. the SAMMIE annotation effort (Kruijff-Korbayov´a et al., 2006b)). However, it is often the case that specialized tools are developed to facilitate the annotation of particular levels: examples include tools for segmentation and transcription of the speech signal like PRAAT (Boersma and Weenink, 2005) and T RANSCRIBER (Barras et al., 1998), the SALSA tools for FrameNetstyle annotation (Burchardt et al., 2006), and MMAX (M¨uller and Strube, 2003) for coreference annotation. Even in these cases, however, it may still be useful, or even necessary, to be able to visualize more than one level at once, or to ‘knit’ together2 multiple levels to create a file that can be used to train a model for a particular type of annotation. The Linguistic Annotation Framework by (Ide et al., 2003) was proposed as a unifying markup format to be used to synchronize heterogeneous markup formats for such purposes. In this paper, we discuss how the PAULA representation format, a standoff format inspired by the Linguistic"
W07-1524,N03-4009,0,0.0254061,"rmation; specialized annotation tools will be used for the annotation at each of these levels. In order to synchronize these multiple layers of annotation, the PAULA standoff exchange format will be used. In this paper, we present the corpus and its PAULA -based architecture.1 1 Introduction XML standoff markup (Thompson and McKelvie, 1997; Dybkjær et al., 1998) is emerging as the cleanest way to organize multi-level annotations of corpora. In many of the current annotation efforts based on standoff a single multi-purpose tool such as the NITE XML Toolkit (Carletta et al., 2003) or WordFreak (Morton and LaCivita, 2003) is used to anno1 The members of the LUNA project consortium are: Piedmont Consortium for Information Systems (IT), University of Trento (IT), Loquendo SpA (IT), RWTH-Aachen (DE), University of Avignon (FR), France Telecom R&D Division S.A. (FR), Polish-Japanese Institute of Information Technology (PL) and the Institute for Computer Science of the Polish Academy of Sciences (PL), http://www.ist-luna.eu. This research was performed in the LUNA project funded by the EC, DG Infso, Unit E1 and in the Collaborative Research Center 632 “Information Structure”, funded by the German Science Foundation"
W07-1524,W03-2117,0,0.0404835,"Missing"
W07-1524,J93-2004,0,\N,Missing
W07-1524,W00-1003,0,\N,Missing
W07-1524,P98-1013,0,\N,Missing
W07-1524,C98-1013,0,\N,Missing
W09-0505,P98-1013,0,0.607831,"ion levels of words, turns1 , attribute-value pairs, dialog acts, predicate argument structures. The annotation at word level is made with part-of-speech and morphosyntactic information following the recommendations of EAGLES corpora annotation (Leech and Wilson, 2006). The attribute-value annotation uses a predefined domain ontology to specify concepts and their relations. Dialog acts are used to annotate intention in an utterance and can be useful to find relations between different utterances as the next section will show. For predicate structure annotation, we followed the FrameNet model (Baker et al., 1998) (see Section 2.2). 2.1 2. Conventional/Discourse management acts, which maintain dialog cohesion and delimit specific phases, such as opening, continuation, closing, and apologizing; 3. Feedback/Grounding acts,used to elicit and provide feedback in order to establish or restore a common ground in the conversation. Our taxonomy, following the same three-fold partition, is summarized in Table 1. Table 1: Dialog act taxonomy Core dialog acts Speaker wants information from addressee Action-request Speaker wants addressee to perform an action Yes-answer Affirmative answer No-answer Negative answer"
W09-0505,W04-3218,1,0.880684,"lli, Alessandro Moschitti, Giuseppe Riccardi∗ University of Trento 38050 Povo - Trento, Italy {dinarelli,silviaq,moschitti,riccardi}@disi.unitn.it, satonelli@fbk.eu Abstract tities) within one or more frames (frame-slot semantics) that is defined by the application. While this model is simple and clearly insufficient to cope with interpretation and reasoning, it has supported the first generation of spoken dialog systems. Such dialog systems are thus limited by the ability to parse semantic features such as predicates and to perform logical computation in the context of a specific dialog act (Bechet et al., 2004). This limitation is reflected in the type of human-machine interactions which are mostly directed at querying the user for specific slots (e.g. “What is the departure city?”) or implementing simple dialog acts (e.g. confirmation). We believe that an important step in overcoming such limitation relies on the study of models of human-human dialogs at different levels of representation: lexical, syntactic, semantic and discourse. In this paper, we present our results in addressing the above issues in the context of the LUNA research project for next-generation spoken dialog interfaces (De Mori e"
W09-0505,burchardt-etal-2006-salto,0,0.0299062,"Missing"
W09-0505,W03-2117,0,0.0692443,"Missing"
W09-0505,W08-0109,1,\N,Missing
W09-0505,C98-1013,0,\N,Missing
W09-3924,W08-0109,1,0.882824,"Missing"
W09-3924,P03-1031,0,\N,Missing
W10-4337,W08-0109,1,0.898232,"Missing"
W10-4338,P09-2005,0,0.0272917,"y the DM, this model returns a clarifying answer. Any offer from the DM to continue the conversation will be either readily met with a new task request or denied at a fixed probability: Au (as ) = {(ˆau , 1)}. In the Bigram model, first defined in (Eckert et al., 1997), a transition matrix records the frequencies of transition from DM actions to user actions, including hang up and no input/no match. Given a DM action as , the model responds with a list of M user actions and their probabilities estimated according to action distribution in the real data: 3 Modelling Cooperativeness As in e.g. (Jung et al., 2009), we define cooperativeness at the turn level (coopt ) as a function of the number of dialog acts in the DM action as sharing concepts with the dialog acts in the user action au ; at the dialog level, coop is the average of turn-level cooperativeness. We discretize coop into a binary variable reflecting high vs low cooperativeness based on whether or not the dialog cooperativeness exceeds the median value of coop found in a reference corpus; in our ADASearch dataset, the median value found for coop is 0.28; hence, we annotate dialogs as cooperative if they exceed such a threshold, and as uncoo"
W10-4338,2005.sigdial-1.6,0,0.0418532,"Missing"
W10-4338,W09-3924,1,0.909426,"stems. We work with a modular architecture for data-driven simulation where the “intentional” component of user simulation includes a User Model representing userspecific features. We train a dialog simulator that combines traits of human behavior such as cooperativeness and context with domain-related aspects via the Expectation-Maximization algorithm. We show that cooperativeness provides a finer representation of the dialog context which directly affects task completion rate. 1 2 Simulator Architecture Data-driven simulation takes place within the rulebased version of the ADASearch system (Varges et al., 2009), which uses a taxonomy of 16 dialog acts and a dozen concepts to deal with three tasks related to tourism in Trentino (Italy): Lodging Enquiry, Lodging Reservation and Event Enquiry. Simulation in our framework occurs at the intention level, where the simulator and the Dialog Manager (DM) exchange actions, i.e. ordered sequences of dialog acts and a number of concept-value pairs. In other words, we represent the DM action as as = {da0 , .., dan }, (s is for “System”) where daj is short for a dialog act defined over zero or more concept-value pairs, daj (c0 (v0 ), .., cm (vm )). In response to"
W12-1622,N04-1015,0,0.060516,"Missing"
W12-1622,P05-1018,0,0.0673449,"Missing"
W12-1622,P05-1022,0,0.143449,"Missing"
W12-1622,P02-1034,0,0.101171,"Missing"
W12-1622,W04-3233,0,0.0661238,"Missing"
W12-1622,I11-1120,1,0.87176,"Missing"
W12-1622,ghosh-etal-2012-improving,1,0.891784,"Missing"
W12-1622,J95-2003,0,0.724653,"Missing"
W12-1622,W05-1506,0,0.0972718,"Missing"
W12-1622,W10-2910,1,0.892872,"Missing"
W12-1622,P03-1069,0,0.0879288,"Missing"
W12-1622,J93-2004,0,0.0427885,"Missing"
W12-1622,P09-2004,0,0.294877,"Missing"
W12-1622,prasad-etal-2008-penn,0,0.249655,"Missing"
W12-1622,W03-0402,0,0.0606469,"Missing"
W12-1622,N03-1030,0,0.313686,"Missing"
W12-1622,J08-2002,0,0.0684847,"Missing"
W12-1622,W03-3023,0,0.175855,"Missing"
W12-1622,J05-1003,0,\N,Missing
W12-1622,P06-2103,0,\N,Missing
W12-1622,P02-1062,0,\N,Missing
W12-1801,W10-4408,1,0.783837,"r Computational Linguistics formation is commonly captured in grammars, that are either hand-crafted or created by means of machine learning techniques. In order to be able to generate high-quality grammars with as little manual effort as possible, we aim at (semi) automating the knowledge-based generation of lexica and grammars. To achieve this, it is crucial to leverage Web resources for enriching ontologies with lexical and linguistic information, i.e. information about how ontological concepts are lexicalized in different languages, capturing in particular lexical and syntactic variation (Unger et al., 2010). This knowledge-centered grammar generation process may be merged with methods for automatically inferring structure from lightly annotated corpus, including data harvested from the Web, in a bottomup fashion (Tur and De Mori, 2011). For a dialog system to be able to exploit ontologies, lexica and grammars, these three resources need to be tightly aligned, i.e. they need to share domain-relevant vocabulary. For this alignment, we propose to build on Semantic Web standards, mainly in order to support the incorporation of already existing data, to share resources for SDS engineering, and facili"
W13-5704,W05-0305,0,0.0860478,"Missing"
W13-5704,I11-1120,1,0.419534,"inter-sentential relations separately, reduces the task complexity and significantly outperforms the single model approach. 1 Introduction Discourse analysis is one of the most challenging tasks in Natural Language Processing, that has applications in many language technology areas such as opinion mining, summarization, information extraction, etc. (see (Webber et al., 2011) and (Taboada and Mann, 2006) for detailed review). With the availability of annotated corpora, such as Penn Discourse Treebank (PDTB) (Prasad et al., 2008), statistical discourse parsers were developed (Lin et al., 2012; Ghosh et al., 2011; Xu et al., 2012). PDTB adopts non-hierarchical binary view on discourse relations: Argument 1 (Arg1) and Argument 2 (Arg2), which is syntactically attached to a discourse connective. Thus, PDTB-based discourse parsing can be roughly partitioned into discourse relation detection, argument position classification, argument span extraction, and relation sense classification. For discourse relations signaled by a connective (explicit relations), discourse relation detection is cast as classification of connectives as discourse and non-discourse. Argument position classification involves detectio"
W13-5704,ghosh-etal-2012-improving,1,0.867077,"Missing"
W13-5704,W12-1622,1,0.861562,"Missing"
W13-5704,prasad-etal-2008-penn,0,0.790426,"e problem is cast as token-level sequence labeling. We show that processing intra- and inter-sentential relations separately, reduces the task complexity and significantly outperforms the single model approach. 1 Introduction Discourse analysis is one of the most challenging tasks in Natural Language Processing, that has applications in many language technology areas such as opinion mining, summarization, information extraction, etc. (see (Webber et al., 2011) and (Taboada and Mann, 2006) for detailed review). With the availability of annotated corpora, such as Penn Discourse Treebank (PDTB) (Prasad et al., 2008), statistical discourse parsers were developed (Lin et al., 2012; Ghosh et al., 2011; Xu et al., 2012). PDTB adopts non-hierarchical binary view on discourse relations: Argument 1 (Arg1) and Argument 2 (Arg2), which is syntactically attached to a discourse connective. Thus, PDTB-based discourse parsing can be roughly partitioned into discourse relation detection, argument position classification, argument span extraction, and relation sense classification. For discourse relations signaled by a connective (explicit relations), discourse relation detection is cast as classification of connective"
W13-5704,prasad-etal-2010-exploiting,0,0.0375634,"Missing"
W13-5704,D07-1010,0,0.098254,"Missing"
W13-5704,C12-2130,0,0.607878,"lations separately, reduces the task complexity and significantly outperforms the single model approach. 1 Introduction Discourse analysis is one of the most challenging tasks in Natural Language Processing, that has applications in many language technology areas such as opinion mining, summarization, information extraction, etc. (see (Webber et al., 2011) and (Taboada and Mann, 2006) for detailed review). With the availability of annotated corpora, such as Penn Discourse Treebank (PDTB) (Prasad et al., 2008), statistical discourse parsers were developed (Lin et al., 2012; Ghosh et al., 2011; Xu et al., 2012). PDTB adopts non-hierarchical binary view on discourse relations: Argument 1 (Arg1) and Argument 2 (Arg2), which is syntactically attached to a discourse connective. Thus, PDTB-based discourse parsing can be roughly partitioned into discourse relation detection, argument position classification, argument span extraction, and relation sense classification. For discourse relations signaled by a connective (explicit relations), discourse relation detection is cast as classification of connectives as discourse and non-discourse. Argument position classification involves detection of the location"
W13-5704,W03-3023,0,0.104972,"rst word of the verb phrase (B-VP) of the main clause (I-S). PDTB Level 1 Connective sense (CONN) is the most general sense of a connective in PDTB sense hierarchy: one of Comparison, Contingency, Expansion, or Temporal. For instance, a discourse connective when might have the CONN feature ‘Temporal’ or ‘Contingency’ depending on the discourse relation it appears in, or ‘NULL’ in case of non-discourse usage. The value of the feature is ‘NULL’ for all tokens except the discourse connective. Boolean Main Verb (BMV) is a feature that indicates whether a token is a main verb of a sentence or not (Yamada and Matsumoto, 2003). For instance in the sentence Prices collapsed when the news flashed, the main verb is collapsed; thus, its BMV feature is ‘1’, whereas for the rest of tokens it is ‘0’. Previous Sentence Feature (PREV) signals if a sentence immediately precedes the sentence starting with a connective, and its value is the first token of the connective (Ghosh et al., 2011). For instance, if some sentence A is followed by a sentence B starting with discourse connective On the other hand, all the tokens of the sentence A have the PREV feature value ‘On’. The feature is similar to a heuristic to select the sente"
W13-5704,P09-2004,0,0.0555148,"depicts the architecture of the discourse parser processing intra- and inter-sentential relations separately. It is a combination of argument position classification with specific CRF models for each of the arguments of SS and PS cases, i.e. there are 4 CRF models – SS Arg1 and Arg2, and PS Arg1 and Arg2 (following sentence case (FS) is ignored). SS models are applied in a cascade and, similar to the 40 SS Discourse Connective Detection Argument Position Classif. PS SS Arg2 Extraction SS Arg1 Extraction PS Arg2 Extraction PS Arg1 Candidate Heuristic with high accuracy using addDiscourse tool (Pitler and Nenkova, 2009). In the separate models discourse parser, the steps of the process to extract argument spans given a discourse connective are as follows: PS Arg1 Extraction 1. Classify connective as SS or PS; 2. If classified as SS: Figure 2: Separate models discourse parsing architecture. CRF argument span extraction models are in bold. baseline single model parser, Arg2 label is a feature for Arg1 span extraction. These SS models are trained using exactly the same features, with the exception of PREV feature: since we consider only the sentence containing the connective, it naturally falls out. For the PS"
W14-1105,prasad-etal-2008-penn,0,0.619356,"relation detection is cast as classification of connectives as discourse and non-discourse. Argument position classification, on the other hand, involves detection of the location of Arg1 with reIntroduction Discourse analysis is one of the most challenging tasks in Natural Language Processing that has applications in many language technology areas such as opinion mining, summarization, information extraction, etc. (see (Webber et al., 2011) and (Taboada and Mann, 2006) for detailed review). The release of the large discourse relation annotated corpora, such as Penn Discourse Treebank (PDTB) (Prasad et al., 2008), marked the development of statistical discourse parsers (Lin et al., 2012; Ghosh et al., 2011; Xu et al., 2012; Stepanov and Riccardi, 2013). Recently, PDTB-style discourse annotation was applied to biomedical domain and Biomedical Discourse Relation Bank (BioDRB) (Prasad et al., 2011) was released. This milestone marks the beginning of the research on cross-domain evaluation and domain adaptation of PDTB-style discourse parsers. 30 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 30–37, c Gothenburg, Sweden, April 26-30"
W14-1105,W13-5704,1,0.677828,"o detect whether a relation is inter- or intra- sentential. Argument span extraction is the extraction (labeling) of text segments that belong to each of the arguments. Finally, relation sense classification is the annotation of relations with the senses from the sense hierarchy (PDTB or BioDRB). To the best of our knowledge, the only subtasks that were addressed cross-domain are the detection of explicit discourse connectives (Ramesh and Yu, 2010; Ramesh et al., 2012; Faiz and Mercer, 2013) and relation sense classification (Prasad et al., 2011). While the discourse parser of Faiz and Mercer (2013)1 provides models for both domains and does identification of argument head words in the style of Wellner and Pustejovsky (2007); there is no decision made on arguments spans. Moreover, there is no cross-domain evaluation available for each of the models. In this paper we address the task of cross-domain argument span extraction of explicit discourse relations. Additionally, we provide evaluation for cross-domain argument position classification as far as the data allows, since BioDRB lacks manual sentence segmentation. The paper is structured as follows. In Section 2 we present the comparativ"
W14-1105,D07-1010,0,0.114903,"labeling) of text segments that belong to each of the arguments. Finally, relation sense classification is the annotation of relations with the senses from the sense hierarchy (PDTB or BioDRB). To the best of our knowledge, the only subtasks that were addressed cross-domain are the detection of explicit discourse connectives (Ramesh and Yu, 2010; Ramesh et al., 2012; Faiz and Mercer, 2013) and relation sense classification (Prasad et al., 2011). While the discourse parser of Faiz and Mercer (2013)1 provides models for both domains and does identification of argument head words in the style of Wellner and Pustejovsky (2007); there is no decision made on arguments spans. Moreover, there is no cross-domain evaluation available for each of the models. In this paper we address the task of cross-domain argument span extraction of explicit discourse relations. Additionally, we provide evaluation for cross-domain argument position classification as far as the data allows, since BioDRB lacks manual sentence segmentation. The paper is structured as follows. In Section 2 we present the comparative analysis of PDTB and BioDRB corpora and the relevant works on crossdomain discourse parsing. In Section 3 we describe the PDTB"
W14-1105,I11-1120,1,0.918591,"ent position classification, on the other hand, involves detection of the location of Arg1 with reIntroduction Discourse analysis is one of the most challenging tasks in Natural Language Processing that has applications in many language technology areas such as opinion mining, summarization, information extraction, etc. (see (Webber et al., 2011) and (Taboada and Mann, 2006) for detailed review). The release of the large discourse relation annotated corpora, such as Penn Discourse Treebank (PDTB) (Prasad et al., 2008), marked the development of statistical discourse parsers (Lin et al., 2012; Ghosh et al., 2011; Xu et al., 2012; Stepanov and Riccardi, 2013). Recently, PDTB-style discourse annotation was applied to biomedical domain and Biomedical Discourse Relation Bank (BioDRB) (Prasad et al., 2011) was released. This milestone marks the beginning of the research on cross-domain evaluation and domain adaptation of PDTB-style discourse parsers. 30 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 30–37, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics SS Discourse Connective Detection Argument"
W14-1105,C12-2130,0,0.036532,"Missing"
W14-1105,W03-3023,0,0.0733657,"he token, prefixed with the information whether a token is at the beginning (B-) or inside (I-) the constituent. The chunklink tool (Buchholz, 2000) is used to extract this feature from syntactic trees. Arg1 Y Y Y Y Y Y Y Y Y Y Y The experimental settings for PDTB are the following: Sections 02-22 are used for training and Sections 23-24 for testing. For BioDRB, on the other hand, 12 fold cross-validation is used (2 documents in each fold, since in BioDRB there are 24 documents). • Boolean Main Verb (BMV) is a boolean feature that indicates whether a token is a main verb of a sentence or not (Yamada and Matsumoto, 2003). 4.1 Evaluation Methodology The performance of Argument Span Extraction is evaluated in terms of precision (p), recall (r), and F-measure (F1 ) using the equations 1 – 3. An argument span is considered to be correct, if it exactly matches the reference string. Following (Ghosh et al., 2011) and (Lin et al., 2012), argument initial and final punctuation marks are removed . • Arg2 Label (ARG2) is an output of Arg2 span extraction model, that is used as a feature for Arg1 span extraction. Arg2 span is easier to identify (Ghosh et al., 2011; Stepanov and Riccardi, 2013) since it is syntactically"
W15-4633,W12-1642,0,0.0704904,"Missing"
W15-4633,W09-0505,1,0.806537,"sk, manual transcripts were provided to the participants. While the original language of the conversations is French, the SENSEI project provided man• A man is calling cause he got a fine. He is waiting for a new card so he used his wife’s card. He must now write a letter asking for clemency. • A user wants to go to the Ambroise Par´e clinic but the employee misunderstands and gives her the wrong itinerary. Luckily the employee realises her mistake and gives the passenger the right information in the end. • School bag lost on line 4, not found. Luna corpus The Italian human-human Luna corpus (Dinarelli et al., 2009) consists of 572 dialogs (≈ 26.5K turns & 30 hours of speech) in the hardware/software help desk domain, where a 233 4 client and an agent are engaged in a problem solving task over the phone. The dialogs are organised in transcriptions and annotations created within the FP6 LUNA project. For the CCCS shared task, manual transcriptions were used. Metric Evaluation is performed with the ROUGE-2 metric (Lin, 2004). ROUGE-2 is the recall in term of word bigrams between a set of reference synopses and a system submission. The ROUGE 1.5.5 toolkit was adapted to deal with a conversation-dependent le"
W15-4633,W04-1013,0,0.0698795,"Missing"
W15-4633,W13-2117,0,0.0992767,"Missing"
W15-4633,stepanov-etal-2014-development,1,0.892092,"Missing"
W15-4633,bechet-etal-2012-decoda,1,\N,Missing
W16-4312,P04-1035,0,0.0223008,"Missing"
W16-4312,W14-2617,0,0.120914,"mmercial polling has taken another serious blow. By contrast, predictions using Natural Language Processing (NLP) and Computational Linguistics (CL) techniques, such as opinion mining, proved to be much more reliable. In what follows we will refer to opinion mining as the automatic task of assigning a polarity to a topic in context (Wiebe et al., 2005); to polarity classification and sentiment analysis as the tasks for the extraction of emotive polarity or scores from text; to agreement/disagreement classification as the task of recognizing the opinion of a message towards others in a thread (Wang and Cardie, 2014) or pairs of replying posts (Celli et al., 2016); and to stance classification as the task of recognizing the overall opinion of an author from text. In this paper we discuss the methods used by traditional pollsters and compare them to the predictions based on different opinion mining techniques, in particular polarity classification and agreement/disagreement classification. We describe a system that predicted the outcome of the This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 110 Proceedings of"
W16-4316,pak-paroubek-2010-twitter,0,0.75369,"ch as linguistic style, interaction, sentiment, mood and other social signals. Finding the collective information of such signals requires automatic processing, which will be useful for various professionals, specifically psychologists and social and behavioral scientists. Among other affective dimensions, mood and sentiment are particularly important for the analysis the consumer behavior towards brands and products (Pang and Lee, 2008; Stieglitz and Dang-Xuan, 2013). In the past few decades, the affective dimension of text has been mainly analyzed in terms of positive and negative polarity (Pak and Paroubek, 2010a; Kouloumpis et al., 2011; Cambria et al., 2016a), although more detailed dimensions are proven to be very useful. In particular, moods such as tension, depression, anger, vigor, fatigue, and confusion in tweets have been found to be good predictors of This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. Licence details: 143 Proceedings of the Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 143–152, Osaka, Japan, December 12 2016. stock market exchanges (Boll"
W16-4316,D09-1020,0,0.0622056,"Missing"
W16-4316,C10-2028,0,0.0992905,"Missing"
W16-4316,C16-1251,0,0.0666277,"od and other social signals. Finding the collective information of such signals requires automatic processing, which will be useful for various professionals, specifically psychologists and social and behavioral scientists. Among other affective dimensions, mood and sentiment are particularly important for the analysis the consumer behavior towards brands and products (Pang and Lee, 2008; Stieglitz and Dang-Xuan, 2013). In the past few decades, the affective dimension of text has been mainly analyzed in terms of positive and negative polarity (Pak and Paroubek, 2010a; Kouloumpis et al., 2011; Cambria et al., 2016a), although more detailed dimensions are proven to be very useful. In particular, moods such as tension, depression, anger, vigor, fatigue, and confusion in tweets have been found to be good predictors of This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. Licence details: 143 Proceedings of the Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 143–152, Osaka, Japan, December 12 2016. stock market exchanges (Bollen et al., 2011). It has also been demonstrated"
W16-4316,pianta-etal-2008-textpro,0,0.0181164,"high dimensionality, it is the simplest and is known to work well for most text-based classification tasks. As bag-of-ngrams representation yields a large dictionary which increases computational cost, we have selected 5K most frequent ngrams. Bag-of-character-ngram Similar to the bag-of-word-ngrams, we also extracted and evaluated bag-ofcharacter-ngrams, with 6 &gt;= n &gt;= 2 and tf-idf transformation. The motivation for experimenting with this feature set is its success in sentiment classification task (Abbasi et al., 2008). Part-of-Speech features (POS): To extract POS features we used TextPro (Pianta et al., 2008) and designed the feature vector using bag-of-ngram representation, with 3 &gt;= n &gt;= 1 and tf-idf transformation. Stylometric Features The use of stylometric features has its root in the domain of authorship identification (Yule, 1939; Abbasi and Chen, 2008; Bergsma et al., 2012; Cristani et al., 2012). Its use has also been reported for text categorization and discourse classification problems (Koppel et al., 2002; Celli et al., ). In authorship identification task, stylometric features are defined ias different groups such as lexical, syntactic, structural, content specific, idiosyncratic and"
W16-4316,W14-1304,0,0.0326778,"Missing"
W16-4316,P10-1141,0,0.0865535,"Missing"
W16-4316,N12-1033,0,0.019713,"the bag-of-word-ngrams, we also extracted and evaluated bag-ofcharacter-ngrams, with 6 &gt;= n &gt;= 2 and tf-idf transformation. The motivation for experimenting with this feature set is its success in sentiment classification task (Abbasi et al., 2008). Part-of-Speech features (POS): To extract POS features we used TextPro (Pianta et al., 2008) and designed the feature vector using bag-of-ngram representation, with 3 &gt;= n &gt;= 1 and tf-idf transformation. Stylometric Features The use of stylometric features has its root in the domain of authorship identification (Yule, 1939; Abbasi and Chen, 2008; Bergsma et al., 2012; Cristani et al., 2012). Its use has also been reported for text categorization and discourse classification problems (Koppel et al., 2002; Celli et al., ). In authorship identification task, stylometric features are defined ias different groups such as lexical, syntactic, structural, content specific, idiosyncratic and complexity-based (Koppel et al., 2002; Abbasi and Chen, 2008; Cristani et al., 2012). In this work, we use the term stylometric to refer to the complexity-based3 features reported in (Tanaka-Ishii and Aihara, 2015; Tweedie and Baayen, 1998). The used stylometric feature groups"
W17-4506,W10-4211,1,0.801993,"conversation summarization is an important task, since speech is the primary medium of human-human communication. Vast amounts of spoken conversation data are produced daily in call-centers. Due to this overwhelming number of conversations, call-centers can only evaluate a small percentage of the incoming calls (Stepanov et al., 2015). Automatic methods of conversation summarization have a potential to increase the capacity of the call-centers to analyze and assess their work. Earlier works on conversation summarization have mainly focused on extractive techniques. However, as pointed out in (Murray et al., 2010) and (Oya et al., 2014), abstractive summaries are preferred to extractive ones by human judges. The possible reason for this is that extractive techniques are not well suited for the conversation summarization, since there are style differ43 Proceedings of the Workshop on New Frontiers in Summarization, pages 43–47 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics Community Creation Conversation Transcript Summary Generation Conversation Transcript Slot Labeling Community Creation (Linking) Topic Segmentation Clustering Communities Speaker & Phrase Extra"
W17-4506,W14-4407,1,0.900901,"Orkan Bayer2 , Giuseppe Carenini3 , Giuseppe Riccardi2 1 SAIL, University of Southern California, Los Angeles, CA, USA 2 Signals and Interactive Systems Lab, DISI, University of Trento, Trento, Italy 3 Department of Computer Science, University of British Columbia, Vancouver, Canada singlak@usc.edu,carenini@cs.ubc.ca {evgeny.stepanov,aliorkan.bayer,giuseppe.riccardi}@unitn.it Abstract ences between spoken conversations and humanauthored summaries. Abstractive conversation summarization systems, on the other hand, are mainly based on the extraction of lexical information (Mehdad et al., 2013; Oya et al., 2014). The authors cluster conversation sentences/utterances into communities to identify most relevant ones and aggregate them using word-graph models. The graph paths are ranked to yield abstract sentences – a template. And these templates are selected for population with entities extracted from a conversation. Thus the abstractive summarization systems are limited to these templates generated by supervised data sources. The template selection strategy in these systems leverages on the manual links between summary and conversation sentences. Unfortunately, such manual links are rarely available."
W17-4506,de-marneffe-etal-2006-generating,0,0.0693538,"Missing"
W17-4506,pianta-etal-2008-textpro,0,0.0231992,"tion follows the approach of (Oya et al., 2014) and, starting from human-authored summaries, produces abstract templates applying slot labeling, summary clustering and template fusion steps. The information required for the template generation are part-of-speech (POS) tags, noun and verb phrase chunks, and root verbs from dependency parsing. For English, we use Illinois Chunker (Punyakanok and Roth, 2001) to identify noun phrases and extract part-of-speech tags; and the the tool of (De Marneffe et al., 2006) for generating dependency parses. For Italian, on the other hand, we use TextPro 2.0 (Pianta et al., 2008) to perform all the Natural Language Processing tasks. In the slot labeling step, noun phrases from human-authored summaries are replaced by WordNet (Fellbaum, 1998) SynSet IDs of the head nouns (right most for English). For a word, SynSet ID of the most frequent sense is selected with respect to the POS-tag. To get hypernyms for Italian we use MultiWordNet (Pianta et al., 2002). The clustering of the abstract templates generated in the previous step is performed using the WordNet hierarchy of the root verb of a sentence. The number of sentences selected for a community is set to 4, since it i"
W17-4506,W09-0505,1,0.761821,"(Mehdad et al., 2013); and our run of the system of (Oya et al., 2014). With manual communities we have Data Sets The two corpora used for the evaluation of the heuristics are AMI and LUNA. The AMI meeting corpus (Carletta et al., 2006) is a collection of 139 meeting records where groups of people are engaged in a ‘roleplay’ as a team and each speaker assumes a certain role in a team (e.g. project manager (PM)). Following (Oya et al., 2014), we removed 20 dialogs used by the authors for development, and use the remaining dialogs for the threefold cross-validation. The LUNA Human-Human corpus (Dinarelli et al., 2009) consists of 572 call-center dialogs where a client and an agent are engaged in a problem solving task over the phone. The 200 Italian LUNA dialogs have been annotated with summaries by 5 native speakers (5 summaries per dialog). For the Call Centre Conversation Summarization (CCCS) shared task (Favre et al., 2015) 45 Model Mehdad et al. (2013) Oya et al. (2014) (15 seg.) Manual Communities (H2) Top 4 turns: token (H3) Top 4 turns: SynSetID (H4) Top 4 turns: Av. WE ROUGE-2 0.040 0.068 0.072 0.076 0.077 0.079 For the 100 English LUNA dialogs, we observe the same pattern as for Italian dialogs a"
W17-4506,W15-4633,1,0.876042,"Missing"
W17-4506,P03-1071,0,0.0257184,"talian corpus (Koehn, 2005)2 . We empirically choose 300, 5, and 5 for the embedding size, window length, and word count threshold, respectively. 1 2 44 https://github.com/mmihaltz/word2vec-GoogleNews-vectors http://www.statmt.org/europarl/ 2.3 a set of 100 dialogs was manually translated to English. The conversations are equally split into training and testing sets as 100/100 for Italian, and 50/50 for English. Summary Generation The first step in summary generation is the segmentation of conversations into topics using a lexical cohesion-based domain-independent discourse segmenter – LCSeg (Galley et al., 2003). The purpose of this step is to cover all the conversation topics. Next, all possible slot ‘fillers’ are extracted from the topic segments and are ranked with respect to their frequency in the conversation. An abstract template for a segment is selected with respect to the average cosine similarity of the segment and the community linked to that template. The selected template slots are filled with the ‘fillers’ extracted earlier. 2.4 3.2 ROUGE-2 metric (Lin, 2004) is used for the evaluation. The metric considers bigram-level precision, recall and F-measure between a set of reference and hypo"
W17-4506,W04-3250,0,0.0700753,"Missing"
W17-4506,2005.mtsummit-papers.11,0,0.0207608,"the previous step is performed using the WordNet hierarchy of the root verb of a sentence. The number of sentences selected for a community is set to 4, since it is the average size of the manual community in the AMI corpus. We use word2vec tool (Mikolov et al., 2013) for learning distributed word embeddings. For English, we obtained pre-trained word embeddings trained on a part of Google News data set (about 3 billion words)1 . The model contains 300-dimensional vectors for 3 million words and phrases. For Italian, we use the word2vec to train word embeddings on the Europarl Italian corpus (Koehn, 2005)2 . We empirically choose 300, 5, and 5 for the embedding size, window length, and word count threshold, respectively. 1 2 44 https://github.com/mmihaltz/word2vec-GoogleNews-vectors http://www.statmt.org/europarl/ 2.3 a set of 100 dialogs was manually translated to English. The conversations are equally split into training and testing sets as 100/100 for Italian, and 50/50 for English. Summary Generation The first step in summary generation is the segmentation of conversations into topics using a lexical cohesion-based domain-independent discourse segmenter – LCSeg (Galley et al., 2003). The p"
W17-4506,W04-1013,0,0.032962,"Missing"
W17-4506,W13-2117,1,0.887244,"ny A. Stepanov2 , Ali Orkan Bayer2 , Giuseppe Carenini3 , Giuseppe Riccardi2 1 SAIL, University of Southern California, Los Angeles, CA, USA 2 Signals and Interactive Systems Lab, DISI, University of Trento, Trento, Italy 3 Department of Computer Science, University of British Columbia, Vancouver, Canada singlak@usc.edu,carenini@cs.ubc.ca {evgeny.stepanov,aliorkan.bayer,giuseppe.riccardi}@unitn.it Abstract ences between spoken conversations and humanauthored summaries. Abstractive conversation summarization systems, on the other hand, are mainly based on the extraction of lexical information (Mehdad et al., 2013; Oya et al., 2014). The authors cluster conversation sentences/utterances into communities to identify most relevant ones and aggregate them using word-graph models. The graph paths are ranked to yield abstract sentences – a template. And these templates are selected for population with entities extracted from a conversation. Thus the abstractive summarization systems are limited to these templates generated by supervised data sources. The template selection strategy in these systems leverages on the manual links between summary and conversation sentences. Unfortunately, such manual links are"
W17-4601,L16-1020,1,0.888381,"ional Discourse Corpus. The data is a subset of a large Italian call-center corpus where call center agents are engaged in conversations with real customers. The customers are calling to solve some specific problem or seek information. The inbound Italian phone conversations are recorded on two separate audio channels with a quality of 16 bits, 8kHz sample rate. The collected conversations (≈ 10K) have an average duration of 396.6±197.9 seconds. To analyze the role of silence in information flow of the conversation, we have selected 10 conversations that contain manual dialog act annotations (Chowdhury et al., 2016b) following dialog Act Markup Language (DiAML) (Bunt et al., 2010, 2012) annotation scheme. The details of the dimensions and the communicative functions considered for the annotation are given in Table 1. The dimensions such as: Discourse Structuring, Speech and Turn Management dimensions are mapped to the tag Other, as they are very infrequent. Silence Filtering Module SilInst = {s1, s2, ..., sh} Feature Design and Extraction Unsupervised Silence Clustering Clusters C = {c1, c2, ..., cl} Human Intervention Grouping Silence Clusters Groups G = {g1, g2, ..., gn} Figure 1: Framework for catego"
W17-4601,bunt-etal-2012-iso,0,\N,Missing
W19-3211,J10-3004,0,0.0149088,"een applied to the analysis of spoken (Stolcke et al., 2000; Cervone et al., 2018) as well as on-line written synchronous con2. Altruism: others offer support, reassurance, suggestions and insight. 3. Instillation of Hope: inspiration provided to participants by their peers. The selected therapeutic factors are analysed in terms of illocutionary force1 and attitude2 . Due to the multi-party and asynchronous nature of on-line social media conversations, prior to the analysis, we extract conversation threads among users – an essential prerequisite for any kind of higher-level dialogue analysis (Elsner and Charniak, 2010). Afterwards, the illocutionary force is identified using Dialogue Act tagging, whereas the attitude by using Sentiment Analysis. The quantitative analysis is then performed on these processed conversations. Ideally, the analysis would require experts to annotate each post and comment on the presence of therapeutic factors. However, due to time and cost demands of this task, it is feasible to analyse only a small fraction of the available data. Compared to previous studies (e.g. (Mayfield et al., 2012)) that analysed few tens of conversations and several thousand lines of chat; using the propo"
W19-3211,J00-3003,0,0.076617,"Missing"
W98-1122,J92-4003,0,0.0306469,"Missing"
W98-1122,P93-1024,0,0.171256,"t a t e s of America, etc..). A traditional word n-gram based language model can benefit greatly by using variable length units to capture long spanning dependencies, for any given order n of the model. Furthermore, language modeling based on longer length units is applicable to languages which do not have a predefined notion of a word. However, the problem of data sparseness is more acute in phrase-based language models than in word-based language models. Clustering words into classes has been used to overcome data sparseness in word-based language models (et.al., 1992; Kneser and Ney, 1993; Pereira et al., 1993; McCandless and Glass, 1993; Bellegarda et al., 1996; Saul and Pereira, 1997). Although the automatically acquired phrases can be later clustered into classes to overcome data sparseness, we present a novel approach 188 of combining the construction of classes during the acquisition of phrases. This integration of phrase acquisition and class construction results in the acquisition of phrase-grammar fragments. In (Gorin, 1996; Arai et al., 1997), grammar fragment acquisition is performed through Kullback-Liebler divergence techniques with application to topic classification from text. Althoug"
W98-1122,W97-0309,0,0.107987,"l can benefit greatly by using variable length units to capture long spanning dependencies, for any given order n of the model. Furthermore, language modeling based on longer length units is applicable to languages which do not have a predefined notion of a word. However, the problem of data sparseness is more acute in phrase-based language models than in word-based language models. Clustering words into classes has been used to overcome data sparseness in word-based language models (et.al., 1992; Kneser and Ney, 1993; Pereira et al., 1993; McCandless and Glass, 1993; Bellegarda et al., 1996; Saul and Pereira, 1997). Although the automatically acquired phrases can be later clustered into classes to overcome data sparseness, we present a novel approach 188 of combining the construction of classes during the acquisition of phrases. This integration of phrase acquisition and class construction results in the acquisition of phrase-grammar fragments. In (Gorin, 1996; Arai et al., 1997), grammar fragment acquisition is performed through Kullback-Liebler divergence techniques with application to topic classification from text. Although phrase-grammar fragments reduce the problem of data sparseness, they can res"
