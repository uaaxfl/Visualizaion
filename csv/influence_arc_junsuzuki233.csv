2005.iwslt-1.15,J04-4002,0,0.0250193,") roughly means the word stem (inflectional endings). For example, “Would it be possible to ship it to Japan” becomes “Woul+ it be poss+ to ship it to Japa+” by prefix-4, and “+ould it be +ible to ship it to +apan” by suffix-4, where “+” at the end or beginning of a word denotes deletion. Prefix-4 and suffix-4 are likely to contribute to word alignment and language modeling, respectively. 3.2. Phrase-based Features Our system adopts a phrase-based translation model represented by phrase-based features, which are based on phrase translation pairs extracted by the method proposed by Och and Ney [4]. First, many-to-many word alignment is set by using both one-to-many and many-to-one word alignments generated by GIZA++ toolkit. In the experiment, we used prefix-4 for word-to-word alignment. Using prefix-4 produced better translations than the original form in preliminary experiments. Next, phrase pairs consistent with word alignment are extracted. The words in a legal phrase pair are only aligned to each other and not to words outside. Hereafter, we use count(˜ e) and count(f˜, e˜) to denote the number of extracted phrase e˜ and extracted phrase pair ( f˜, e˜), respectively. We used the f"
2005.iwslt-1.15,N03-1017,0,0.0336733,"xtracted source/target phrases # of source/target phrases appearing in the corpus • Phrase pair extraction probability, i.e., # of sentences phrase pairs extracted # of sentences phrase pairs appearing in the corpus • Adjusted Dice coefficient, which is an extension of the measure proposed in [5], i.e., Dice(f˜, e˜)log(count(f˜, e˜) + 1) 3.3. Word-level Features We used the following word-level features, where w(f |e) =  count(f, e) ,  f  count(f , e) I is the number of words in the translation and J is the number of words in the input sentence. • Lexical weight pw (f˜|˜ e) and pw (˜ e|f˜) [6], where pw (f˜|˜ e) = maxa ·  J  1 |{i|(i, j) ∈ a)}| j=1 ∀(i,j)∈a w(fj |ei ) (I˜ + 1)J˜ j w(f˜j |˜ ei ) i • Viterbi IBM Model 1 score p M1 (f˜|˜ e) and pM1 (˜ e|f˜), where pM1 (f˜|˜ e) = J˜  1 (I˜ + 1)J˜ max w(f˜j |˜ ei ) j i • Noisy OR gate pN OR (f˜|˜ e) and pN OR (˜ e|f˜) [7], where e) = pN OR (f˜|˜   (1 − (1 − w(f˜j |˜ ei ))) j i e, f˜) where • Deletion penalty p del (˜ pdel (˜ e, f˜) =  ˜ del(˜ eI1 , f˜j ) j 2 • Phrase extraction probability of source/target, i.e., J˜  I˜  1 ˜ del(˜ eI1 , f˜j ) =   1  0 i does not exist s.t. w(˜ ei |f˜j ) &gt; threshold otherwise. 3.4. Lexical"
2005.iwslt-1.15,N04-1033,0,0.0174663,"posed in [5], i.e., Dice(f˜, e˜)log(count(f˜, e˜) + 1) 3.3. Word-level Features We used the following word-level features, where w(f |e) =  count(f, e) ,  f  count(f , e) I is the number of words in the translation and J is the number of words in the input sentence. • Lexical weight pw (f˜|˜ e) and pw (˜ e|f˜) [6], where pw (f˜|˜ e) = maxa ·  J  1 |{i|(i, j) ∈ a)}| j=1 ∀(i,j)∈a w(fj |ei ) (I˜ + 1)J˜ j w(f˜j |˜ ei ) i • Viterbi IBM Model 1 score p M1 (f˜|˜ e) and pM1 (˜ e|f˜), where pM1 (f˜|˜ e) = J˜  1 (I˜ + 1)J˜ max w(f˜j |˜ ei ) j i • Noisy OR gate pN OR (f˜|˜ e) and pN OR (˜ e|f˜) [7], where e) = pN OR (f˜|˜   (1 − (1 − w(f˜j |˜ ei ))) j i e, f˜) where • Deletion penalty p del (˜ pdel (˜ e, f˜) =  ˜ del(˜ eI1 , f˜j ) j 2 • Phrase extraction probability of source/target, i.e., J˜  I˜  1 ˜ del(˜ eI1 , f˜j ) =   1  0 i does not exist s.t. w(˜ ei |f˜j ) &gt; threshold otherwise. 3.4. Lexical Reordering Features We used the following features to control the reordering of phrases: • Distortion model d(a i − bi−1 ) = exp−|ai −bi−1 −1 |, where ai denotes the starting position of the foreign phrase translated into the i-th English phrase, and b i−1 denotes the end position of"
2005.iwslt-1.15,N04-1021,0,0.050617,"Missing"
2005.iwslt-1.15,W02-1021,0,0.0178786,"on of the foreign phrase translated into the i-th English phrase, and b i−1 denotes the end position of the foreign phrase translated into the (i − 1)-th English phrase [6]. • Right monotone model P R (˜ e, f˜) (and left monotone ˜ model PL (˜ e, f )) inspired by Och’s scheme [8], where PR (f˜, e˜) = countR , count(f˜, e˜) and countR denotes the number of right connected monotone phrases. 3.5. Other Features The following additional features are used. • number of words that constitute a translation • number of phrases that constitute a translation 4. Decoder The decoder is based on word graph [9] and uses a multi-pass strategy to generate n-best translations. It generates hypothesized translations in a left-to-right order by combining phrase translations for a source sentence. The first pass of our decoding algorithm generates a word graph, a compact representation of hypothesized translations, using a breadth-first beam search, as in [10][11][12][13]. Then, n-best translations are extracted from the generated word graph using A ∗ search. The search space for a beam search is constrained by restricting the reordering of source phrases. We have window size constraints that restrict the"
2005.iwslt-1.15,P97-1047,0,0.0296466,"umber of right connected monotone phrases. 3.5. Other Features The following additional features are used. • number of words that constitute a translation • number of phrases that constitute a translation 4. Decoder The decoder is based on word graph [9] and uses a multi-pass strategy to generate n-best translations. It generates hypothesized translations in a left-to-right order by combining phrase translations for a source sentence. The first pass of our decoding algorithm generates a word graph, a compact representation of hypothesized translations, using a breadth-first beam search, as in [10][11][12][13]. Then, n-best translations are extracted from the generated word graph using A ∗ search. The search space for a beam search is constrained by restricting the reordering of source phrases. We have window size constraints that restrict the number of words skipped before selecting a segment of the source sequence [6][12]. An ITG-constraint [14] is also implemented that prohibits the extension of a hypothesis that violates ITG constraints, which will be useful for language pairs with drastic reordering, such as Japanese-to-English and Korean-to-English translations. During the beam se"
2005.iwslt-1.15,J03-1005,0,0.0276819,"r of right connected monotone phrases. 3.5. Other Features The following additional features are used. • number of words that constitute a translation • number of phrases that constitute a translation 4. Decoder The decoder is based on word graph [9] and uses a multi-pass strategy to generate n-best translations. It generates hypothesized translations in a left-to-right order by combining phrase translations for a source sentence. The first pass of our decoding algorithm generates a word graph, a compact representation of hypothesized translations, using a breadth-first beam search, as in [10][11][12][13]. Then, n-best translations are extracted from the generated word graph using A ∗ search. The search space for a beam search is constrained by restricting the reordering of source phrases. We have window size constraints that restrict the number of words skipped before selecting a segment of the source sequence [6][12]. An ITG-constraint [14] is also implemented that prohibits the extension of a hypothesis that violates ITG constraints, which will be useful for language pairs with drastic reordering, such as Japanese-to-English and Korean-to-English translations. During the beam search"
2005.iwslt-1.15,2003.mtsummit-papers.53,0,0.0219439,"right connected monotone phrases. 3.5. Other Features The following additional features are used. • number of words that constitute a translation • number of phrases that constitute a translation 4. Decoder The decoder is based on word graph [9] and uses a multi-pass strategy to generate n-best translations. It generates hypothesized translations in a left-to-right order by combining phrase translations for a source sentence. The first pass of our decoding algorithm generates a word graph, a compact representation of hypothesized translations, using a breadth-first beam search, as in [10][11][12][13]. Then, n-best translations are extracted from the generated word graph using A ∗ search. The search space for a beam search is constrained by restricting the reordering of source phrases. We have window size constraints that restrict the number of words skipped before selecting a segment of the source sequence [6][12]. An ITG-constraint [14] is also implemented that prohibits the extension of a hypothesis that violates ITG constraints, which will be useful for language pairs with drastic reordering, such as Japanese-to-English and Korean-to-English translations. During the beam search sta"
2005.iwslt-1.15,C04-1006,0,0.0208502,"s in a left-to-right order by combining phrase translations for a source sentence. The first pass of our decoding algorithm generates a word graph, a compact representation of hypothesized translations, using a breadth-first beam search, as in [10][11][12][13]. Then, n-best translations are extracted from the generated word graph using A ∗ search. The search space for a beam search is constrained by restricting the reordering of source phrases. We have window size constraints that restrict the number of words skipped before selecting a segment of the source sequence [6][12]. An ITG-constraint [14] is also implemented that prohibits the extension of a hypothesis that violates ITG constraints, which will be useful for language pairs with drastic reordering, such as Japanese-to-English and Korean-to-English translations. During the beam search stage, three kinds of pruning are performed to further reduce the search space [11]. First, observation pruning limits the number of phrase translation candidates to a maximum of N candidates. Second, threshold pruning is performed by computing the most likely partial hypothesis and by discarding hypotheses whose probability is lower than the maximu"
2005.iwslt-1.15,W05-0834,0,0.0109953,"e fly and then integrated with the preceding score for beam pruning. We estimated future cost as described in [13]. Exact costs for the phrase-based features and word level features can be calculated for each extracted phrase pair. For the language model features, their costs were approximated by using only output words contained by each phrase pair. The upper bound of lexical reordering feature costs can be computed beforehand by considering the possible permutations of phrase pairs for a given input. After generating a word graph, it is then pruned using the posterior probabilities of edges [15] to further reduce the number of duplicate translations for A ∗ search. An edge is pruned if its posterior score is lower than the highest posterior score in the graph by a certain amount. 5. Experiments To validate the use of the reportedly effective features, we conducted translation experiments using all features introduced in Section 3. Also, we conducted comparable experiments in both supplied and unrestricted data tracks to study the effectiveness of additional language resources. English data sets IWSLT (supplied) ATR WEB Gigaword Corpus size (words) 190,177 1,100,194 8,482,782 1,799,53"
2005.iwslt-1.15,2004.iwslt-evaluation.13,0,0.0125922,".5 28.6 Chinese IWSLT LDC 56.6 462 56.1 449 50.7 432 Table 4: Input language perplexity of trigram trained by supplied corpora and IWSLT datasets are similar, WEB is closer to IWSLT than Gigaword, and that LDC is very different from IWSLT. Since the collection is enormous in Gigaword, the vocabulary set is first limited to that observed in the English part of supplied corpus and the ATR database. Then for decoding, an actual n-gram language model is estimated on the fly by constraining the vocabulary set to that observed in a given test set. 5.3. Other Setups Following one of the best systems [17] in IWSLT 2004, feature function scaling factors λ j are trained using NIST scores [18] in a loss function of minimum error rate training, and development set 1 (CSTAR) was used for it. For Japanese and Korean, ITG constraints of lexical reordering were applied, and for Arabic and Chinese, simple window size constraints up to 7 were used. 5.4. Results Table 5 summarizes the overall results of the supplied/unrestricted data tracks. The scores of the table are obtained by the comparable conditions for each language pair while some are not the same as those released by the organizer. “m unrestric"
2005.iwslt-1.15,W03-1506,0,\N,Missing
2005.iwslt-1.15,P02-1040,0,\N,Missing
2005.iwslt-1.15,P02-1038,0,\N,Missing
2005.iwslt-1.15,J03-1002,0,\N,Missing
2005.iwslt-1.15,2006.iwslt-evaluation.14,1,\N,Missing
2005.iwslt-1.15,P03-1021,0,\N,Missing
2006.iwslt-evaluation.14,J03-1002,0,0.00515357,"on-terminals are always instantiated with phrase translation pairs. Thus, we will be able to reduce the number of rules induced from a bilingual corpus, which, in turn, help reducing the decoding complexity. Note that we do not imply arbitrary synchronous-CFGs are transformed into the target normalized form. The form simply restricts the grammar extracted from a bilingual corpus explained in Section 2.4. 2.4. Training The phrase extraction algorithm is based on those presented by [3]. First, many-to-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ [6], in both directions and by combining the results based on a heuristic [7]. Second, phrase translation pairs are extracted from the word aligned corpus [3]. This method ) from a senexhaustively extracts phrase pairs (fjj+m , ei+n i J I tence pair (f1 , e1 ) that do not violate the word alignment constraints a. In the hierarchical phrase-based model, production rules are accumulated by computing “holes” for extracted contiguous phrases [4]: 2.6. Feature Functions Feature functions evaluated during the decoding procedure is summarized as count-based models, lexicon-based models, language model,"
2006.iwslt-evaluation.14,J04-4002,0,0.069757,"we will be able to reduce the number of rules induced from a bilingual corpus, which, in turn, help reducing the decoding complexity. Note that we do not imply arbitrary synchronous-CFGs are transformed into the target normalized form. The form simply restricts the grammar extracted from a bilingual corpus explained in Section 2.4. 2.4. Training The phrase extraction algorithm is based on those presented by [3]. First, many-to-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ [6], in both directions and by combining the results based on a heuristic [7]. Second, phrase translation pairs are extracted from the word aligned corpus [3]. This method ) from a senexhaustively extracts phrase pairs (fjj+m , ei+n i J I tence pair (f1 , e1 ) that do not violate the word alignment constraints a. In the hierarchical phrase-based model, production rules are accumulated by computing “holes” for extracted contiguous phrases [4]: 2.6. Feature Functions Feature functions evaluated during the decoding procedure is summarized as count-based models, lexicon-based models, language model, reordering models and length-based models. 1. A phrase pair (f¯, e¯) const"
2006.iwslt-evaluation.14,W05-1507,0,0.0304273,"boxed indices indicate non-terminal alignment. One of the major differences to the algorithm presented in [4] is the restriction of the target normalized form in the last step. 2.5. Decoding by Top-down Parsing Decoding is performed by parsing on the source-side and by combining the projected target-side. A conventional method of parsing is a CKY-based method in which ordering is governed by the span-size of the source words [4]. One of the problem is the high computational complexity when integrated with ngram language model of the target-side especially when the ngram’s order is quite high [8]. The complexity lies on the possible “holes” in the target-side. One of the solution is to perform a binarization so that the target-side will not contain holes [9]. We applied an Earley-style top-down parsing approach described in [5] that is similar to [10]. The basic idea is to perform a top-down parsing in order so that the projected target-side is generated in a left-to-right manner. The search is guided with a push-down automaton which keeps track of the span-size of uncovered source word positions. Combined with the rest-cost estimation aggregated in a bottom-up way, our decoder effici"
2006.iwslt-evaluation.14,N06-1033,0,0.0183208,"e last step. 2.5. Decoding by Top-down Parsing Decoding is performed by parsing on the source-side and by combining the projected target-side. A conventional method of parsing is a CKY-based method in which ordering is governed by the span-size of the source words [4]. One of the problem is the high computational complexity when integrated with ngram language model of the target-side especially when the ngram’s order is quite high [8]. The complexity lies on the possible “holes” in the target-side. One of the solution is to perform a binarization so that the target-side will not contain holes [9]. We applied an Earley-style top-down parsing approach described in [5] that is similar to [10]. The basic idea is to perform a top-down parsing in order so that the projected target-side is generated in a left-to-right manner. The search is guided with a push-down automaton which keeps track of the span-size of uncovered source word positions. Combined with the rest-cost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most-likely translation. Our decoding algorithm can be regarded as an instance of Earley algorithm, but the predicted rule’s “dot” is moved sy"
2006.iwslt-evaluation.14,P02-1038,0,0.663191,"onal phrase-based model. In addition, our reranking algorithm further boosted the performance. = argmax P r(eI1 |f1J ) (1) eI1 = argmax P eI1 e′ I1 ′ P M  I J λ h (e , f ) m m 1 1 m=1 (2) P M J ′I′ exp m=1 λm hm (e 1 , f1 ) exp In this framework, the posterior probability P r(eI1 |f1J ) is directly maximized using a log-linear combination of feature functions hm (eI1 , f1J ), such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors λm are optimized based on a maximum likelihood approach [1] or on a direct error minimization approach [2]. This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. 1. Introduction This paper describes the NTT statistical machine translation system which is experimented in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2006. 2.2. Hierarchical Phrase-based Approach In the phrase-based translation approach [3], the input foreign sentence is segmented into phrases, f¯1K , mapped into corresponding English-side e¯K 1 , then, reordered to f"
2006.iwslt-evaluation.14,P03-1021,0,0.0700907,"king algorithm further boosted the performance. = argmax P r(eI1 |f1J ) (1) eI1 = argmax P eI1 e′ I1 ′ P M  I J λ h (e , f ) m m 1 1 m=1 (2) P M J ′I′ exp m=1 λm hm (e 1 , f1 ) exp In this framework, the posterior probability P r(eI1 |f1J ) is directly maximized using a log-linear combination of feature functions hm (eI1 , f1J ), such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors λm are optimized based on a maximum likelihood approach [1] or on a direct error minimization approach [2]. This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. 1. Introduction This paper describes the NTT statistical machine translation system which is experimented in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2006. 2.2. Hierarchical Phrase-based Approach In the phrase-based translation approach [3], the input foreign sentence is segmented into phrases, f¯1K , mapped into corresponding English-side e¯K 1 , then, reordered to form the output English sentence. The approach i"
2006.iwslt-evaluation.14,N03-1017,0,0.214202,"s dropped since it depends only on f1J . Feature function scaling factors λm are optimized based on a maximum likelihood approach [1] or on a direct error minimization approach [2]. This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. 1. Introduction This paper describes the NTT statistical machine translation system which is experimented in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2006. 2.2. Hierarchical Phrase-based Approach In the phrase-based translation approach [3], the input foreign sentence is segmented into phrases, f¯1K , mapped into corresponding English-side e¯K 1 , then, reordered to form the output English sentence. The approach is able to capture phrase-wise local-reordering, or possibly neighboring phrase reordering, but does not account for long-distance reordering of phrases. In the hierarchical phrase-based translation approach [4], translation is constituted by hierarchically combining phrases with the help of non-terminals embedded in phrases themselves. Each non-terminal represented in each phrase can capture reordering of phrases. Based"
2006.iwslt-evaluation.14,P05-1033,0,0.752177,"system which is experimented in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2006. 2.2. Hierarchical Phrase-based Approach In the phrase-based translation approach [3], the input foreign sentence is segmented into phrases, f¯1K , mapped into corresponding English-side e¯K 1 , then, reordered to form the output English sentence. The approach is able to capture phrase-wise local-reordering, or possibly neighboring phrase reordering, but does not account for long-distance reordering of phrases. In the hierarchical phrase-based translation approach [4], translation is constituted by hierarchically combining phrases with the help of non-terminals embedded in phrases themselves. Each non-terminal represented in each phrase can capture reordering of phrases. Based on the hierarchical phrase-based modeling, we adopted the left-to-right target generation method described in [5]. The method is able to generate translations efficiently, first, by simplifying the grammar so that the target-side takes a phrase-prefixed form, namely target normalized form. Our simplified grammar drastically reduces the number of rules extracted from a bilingual corpu"
2006.iwslt-evaluation.14,P06-1098,1,0.921865,"hen, reordered to form the output English sentence. The approach is able to capture phrase-wise local-reordering, or possibly neighboring phrase reordering, but does not account for long-distance reordering of phrases. In the hierarchical phrase-based translation approach [4], translation is constituted by hierarchically combining phrases with the help of non-terminals embedded in phrases themselves. Each non-terminal represented in each phrase can capture reordering of phrases. Based on the hierarchical phrase-based modeling, we adopted the left-to-right target generation method described in [5]. The method is able to generate translations efficiently, first, by simplifying the grammar so that the target-side takes a phrase-prefixed form, namely target normalized form. Our simplified grammar drastically reduces the number of rules extracted from a bilingual corpus empirically presented in [5]. Second, translation is generated in a left-to-right manner, similar to a phrase-based approach, using an Earley-style top-down parsing on the source-side. Coupled with the tarOur system consists of two parts. A hierarchical phrasebased translation system that generates a large n-best list. The"
2006.iwslt-evaluation.14,W06-3119,0,0.0424367,"side and by combining the projected target-side. A conventional method of parsing is a CKY-based method in which ordering is governed by the span-size of the source words [4]. One of the problem is the high computational complexity when integrated with ngram language model of the target-side especially when the ngram’s order is quite high [8]. The complexity lies on the possible “holes” in the target-side. One of the solution is to perform a binarization so that the target-side will not contain holes [9]. We applied an Earley-style top-down parsing approach described in [5] that is similar to [10]. The basic idea is to perform a top-down parsing in order so that the projected target-side is generated in a left-to-right manner. The search is guided with a push-down automaton which keeps track of the span-size of uncovered source word positions. Combined with the rest-cost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most-likely translation. Our decoding algorithm can be regarded as an instance of Earley algorithm, but the predicted rule’s “dot” is moved synchronized with the left-to-right ordering of the projected target-side, not the left-to-right"
2006.iwslt-evaluation.14,P96-1041,0,0.0444429,"e hierarchical phrase-based model, production rules are accumulated by computing “holes” for extracted contiguous phrases [4]: 2.6. Feature Functions Feature functions evaluated during the decoding procedure is summarized as count-based models, lexicon-based models, language model, reordering models and length-based models. 1. A phrase pair (f¯, e¯) constitutes a rule: X → f¯, e¯ 96 2.6.3. Language Model 2.6.1. Count-based Models hφ (f1J |eI1 , D) and hφ (eI1 |f1J , D) estisentences f1J and eI1 over a derivaWe used mixed-cased 5-gram language model estimated with modified Kneser-Ney smoothing [11]: Y pn (ei |ei−4 ei−3 ei−2 ei−1 ) (11) hlm (eI1 ) = log Main feature functions mate the likelihood of two tion tree D. We assume that the production rules in D are independent of each other: Y φ(γ|α) (4) hφ (f1J |eI1 , D) = log i 2.6.4. Reordering Models hγ,αi∈D In order to limit the reorderings, two feature functions are employed: X height(Di ) (12) hheight (eI1 , f1J , D) = φ(γ|α) is estimated through the relative frequency on a given bilingual corpus. count(γ, α) φ(γ|α) = P γ count(γ, α) (5) Di ∈back(D) hwidth (eI1 , f1J , D) = where count(·) represents the cooccurrence frequency of rules γ"
2006.iwslt-evaluation.14,P02-1034,0,0.0145487,"titute a translation: j=1 max t(fj |ei ) &lt; τdel 0≤i≤I  (9) The deletion model simply counts the number of words whose lexicon model probability is lower than a threshold τdel . Likewise, an insertion model is integrated that penalizes the inserted English words that do not account for any foreign words in an input: hins (eI1 , f1J ) = I  X i=1 max t(ei |fj ) &lt; τins 0≤j≤J  (14) hr (D) = rule(D) (15) This section explains our discriminative reranking method which further improves the quality of baseline MT system. Our reranking method basically follows the parse reranking method explained in [12]. We first generate a n-best list of candidate outputs (translations) from a baseline MT system, the hierarchical phrase-based translation described in Section 2. Then, a reranking model is trained by a ranking voted perceptron on a development set. Finally, in the process of decoding, we re-rank the n-best list of test data fed from the baseline MT using the trained reranking model. We adopted the above method, [12], with a BLEU-score-based weight update scheme. The reranking setting of MT is an ordinal regression procedure in each output pairs, similar to the parse reranking task, which can"
2006.iwslt-evaluation.14,takezawa-etal-2002-toward,0,0.0312538,"running GIZA++ in both directions, and by refining word alignment with a heuristic. Third, from three distinctly preprocessed corpora, rules are extracted using the algorithm presented in 2.4. In this step, preprocessed corpora are recovered into their original form. When recovered, punctuation marks on the source-side were removed together with corresponding word alignments. The idea is to induce better word alignments by considering non-punctuation corpus, together with punctuation preserved corpus. 4. Tasks The experiments were carried out on the Basic Travel Expression Corpus (BTEC) task [13]. BTEC is a multilingual corpus in traveling domain which was collected from phrase books for tourists. In the IWSLT 2006 open data track, the subsets of BTEC consists of training set and three development sets (Dev1 through Dev3) indicated in Table 1. Another development set (Dev4) and the final test set were provided in this track1 . The translation pairs set up for the task are: Arabic-to-English, Italian-to-English, Japanese-toEnglish and Chinese-to-English. The task description for the IWSLT 2006 evaluation campaign can be summarized as follows: 1. Spoken language, instead of written text"
2006.iwslt-evaluation.14,W06-3115,1,0.730654,"ey algorithm, but the predicted rule’s “dot” is moved synchronized with the left-to-right ordering of the projected target-side, not the left-to-right ordering on the source-side. The use of target normalized form further simplify the decodig procedure. Since the rule form does not allow any holes for the target-side, the integration with ngram language model is straightforward: the prefixed phrases are simply concatenated and intersected. Our decoder is based on an in-house developed phrasebased decoder which uses a bit vector to represent uncovered foreign word positions for each hypothesis [14]. We basically replaced the bit vector structure to the stack structure: Almost no modification was required for the word graph structure and the beam search strategy implemented for a phrase-based modeling, since the target-side’s prefixed phrases are simply concatenated. The use of a stack structure directly models a synchronous-CFG formalism realized as a push-down automation, while the bit vector implementation is conceptualized as a finite state transducer. (3) where X is a non-terminal, γ is a source-side string of arbitrary terminals and/or non-terminals. ¯bβ is a corresponding target-s"
2006.iwslt-evaluation.14,2005.iwslt-1.8,0,0.0199399,"erings. 5.2. Results on Hierarchical Phrase-based Translation We first compared our baseline hierarchical phrase-based translation against an in-house developed phrase-based translation that performed quite well for the shared task of “Workshop on Statistical Machine Translation” [14]. Table 3 shows the number of phrases and rules extracted from each task. The grammar size for our hierarchical phrase-based system is almost twice as large as the size of the phrase table for our phrase-based system. The phrase-based system employs a lexicalized reordering model to capture phrase-wise reordering [15]. For the hierarchical phrase-based system, spansize for each non-terminal was constrained to 7 for all tasks. Window-size constraints were set to 7 in the phrase-based system. As indicated in Table 4, our hierarchical phrasebased system outperforms the phrase-based system in all tasks. mPER 56.65 54.15 48.13 41.57 55.12 52.17 59.72 57.71 53.70 5.3. Results on Reranking Table 5 shows the reranking results for IWSLT2006. The rows of ‘1-best’ in Table 5 show the performance of our baseline MT system, hierarchical phrase-based system (contrast1 system). Then, the rows of ‘SC’ display the performa"
2007.iwslt-1.16,N07-1008,0,0.0150061,"parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both systems are tuned using approximated BLEU as an objective function that scales the sentence-wise BLEU to a document-wise BLEU. Domain mismatch is handled by a simple task adaptation scheme by selecting training data that resembles a test set [7]. In order to handle the ASR’s error prone input, we decoded all the nbest translations and let the reranker choose the right translation by treating the individually translated list as a single k-best list combined with the ASR’s n-best list’s confidence measures. This paper is organized as follows: The overview of our decoder is presented in Section 2. We will describe the feature functions experimented in [3] together with additional features. The reranking system is described in Section 3. The reranker is biased to use a slightly different feature set to avoid over training. Both systems s"
2007.iwslt-1.16,P03-1021,0,0.0149653,"e feature functions experimented in [3] together with additional features. The reranking system is described in Section 3. The reranker is biased to use a slightly different feature set to avoid over training. Both systems share the same online training algorithm, but differ in that the decoder’s parameters are updated based on the dynamically generated candidate list, whereby the reranking training is based on a fixed translation candidate list. Section 4 presents the results for the evaluation campaign of IWSLT 2007. 2. Machine Translation System We use a linear feature combination approach [8] in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: eˆ = argmax w⊤ · h(f, e) (1) e where h(f, e) is a large-dimension feature vector. w is a weight vector that scales the contribution from each feature. Each feature can take any real value, such as the log of the ngram language model to represent fluency, or a lexicon model to capture the word or phrase-wise correspondence. Under this maximization scenario, our system composed of two steps: The first step is a decoder that can efficiently generate k-best list of ca"
2007.iwslt-1.16,P05-1033,0,0.107755,"by seeking a maximum solution: eˆ = argmax w⊤ · h(f, e) (1) e where h(f, e) is a large-dimension feature vector. w is a weight vector that scales the contribution from each feature. Each feature can take any real value, such as the log of the ngram language model to represent fluency, or a lexicon model to capture the word or phrase-wise correspondence. Under this maximization scenario, our system composed of two steps: The first step is a decoder that can efficiently generate k-best list of candidate translations in a left-to-right manner [1] based on the hierarchical phrase-based translation[9]. The second step rerank the k-best list using a reranking voted perceptron[2]. 2.1. Hierarchical Phrase-based Translation We use the hierarchical phrase-based translation approach, in which non-terminals are embedded in each phrase [9]. A translation is generated by hierarchically combining phrases using the non-terminals. Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrase-based approach [10]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic str"
2007.iwslt-1.16,N03-1017,0,0.00424325,"te k-best list of candidate translations in a left-to-right manner [1] based on the hierarchical phrase-based translation[9]. The second step rerank the k-best list using a reranking voted perceptron[2]. 2.1. Hierarchical Phrase-based Translation We use the hierarchical phrase-based translation approach, in which non-terminals are embedded in each phrase [9]. A translation is generated by hierarchically combining phrases using the non-terminals. Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrase-based approach [10]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic structure. Based on hierarchical phrase-based modeling, we adopted the left-to-right target generation method [1] which performed better than a phrase-based system in the last year’s evaluation[2]. This method is able to generate translations efficiently, first, by simplifying the grammar so that the target side takes a phrase-prefixed form, namely a target normalized form: X → γ, ¯bβ, ∼ (2) where X is a non-terminal, γ is a source side string of arbitrary terminals and/or non-t"
2007.iwslt-1.16,P98-2230,0,0.0172839,"s efficiently, first, by simplifying the grammar so that the target side takes a phrase-prefixed form, namely a target normalized form: X → γ, ¯bβ, ∼ (2) where X is a non-terminal, γ is a source side string of arbitrary terminals and/or non-terminals. ¯bβ is a corresponding target side where ¯b is a string of terminals, or a phrase, and β is a (possibly empty) string of non-terminals. ∼ defines one-to-one mapping between non-terminals in γ and β. Second, a translation is generated in a left-to-right manner, similar to phrase-based decoding using Earley-style topdown parsing on the source side [11, 1, 12]. The basic idea is to perform top-down parsing so that the projected target side is generated in a left-to-right manner. The search is guided with a push-down automaton, which keeps track of the span of uncovered source word positions. Combined with the restcost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most likely translation. The use of a target normalized form further simplifies the decoding procedure, at the expense for expressiveness. Since the rule form does not allow any holes in the target side, the integration with an n-gram language model is"
2007.iwslt-1.16,W06-3119,0,0.0336527,"s efficiently, first, by simplifying the grammar so that the target side takes a phrase-prefixed form, namely a target normalized form: X → γ, ¯bβ, ∼ (2) where X is a non-terminal, γ is a source side string of arbitrary terminals and/or non-terminals. ¯bβ is a corresponding target side where ¯b is a string of terminals, or a phrase, and β is a (possibly empty) string of non-terminals. ∼ defines one-to-one mapping between non-terminals in γ and β. Second, a translation is generated in a left-to-right manner, similar to phrase-based decoding using Earley-style topdown parsing on the source side [11, 1, 12]. The basic idea is to perform top-down parsing so that the projected target side is generated in a left-to-right manner. The search is guided with a push-down automaton, which keeps track of the span of uncovered source word positions. Combined with the restcost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most likely translation. The use of a target normalized form further simplifies the decoding procedure, at the expense for expressiveness. Since the rule form does not allow any holes in the target side, the integration with an n-gram language model is"
2007.iwslt-1.16,2004.iwslt-evaluation.13,0,0.0146149,"nd intersected with an n-gram. 2.2. Features 2.2.1. Baseline Features The hierarchical phrase-based translation system employs standard real valued value features: • n-gram language model to capture the fluency of the target side. • Hierarchical phrase translation probabilities in both directions, h(γ|¯bβ) and h(¯bβ|γ), estimated by relative counts, count(γ, ¯bβ) [9]. • Word-based lexically weighted models of hlex (γ|¯bβ) and hlex (¯bβ|γ) using lexical translation models[9]. • Word-based insertion/deletion penalties that penalize through the low probabilities of the lexical translation models [13]. • Word/hierarchical-phrase length penalties. • Backtrack-based penalties inspired by the distortion penalties in phrase-based modeling [1]. 2.2.2. Sparse Features In addition to the baseline features, a large number of binary features are integrated in our MT system [3]. The features are designed with decoding efficiency in mind and are based on the word alignment structure preserved in hierarchical phrase translation pairs [14]. When hierarchical phrases are extracted, the word alignment is preserved. If multiple ei−1 ei f j−1 ei+1 fj ei+2 ei+3 f j+1 f j+2 ei+4 f j+3 Figure 1: An example of"
2007.iwslt-1.16,W06-3108,0,0.0191772,"(¯bβ|γ) using lexical translation models[9]. • Word-based insertion/deletion penalties that penalize through the low probabilities of the lexical translation models [13]. • Word/hierarchical-phrase length penalties. • Backtrack-based penalties inspired by the distortion penalties in phrase-based modeling [1]. 2.2.2. Sparse Features In addition to the baseline features, a large number of binary features are integrated in our MT system [3]. The features are designed with decoding efficiency in mind and are based on the word alignment structure preserved in hierarchical phrase translation pairs [14]. When hierarchical phrases are extracted, the word alignment is preserved. If multiple ei−1 ei f j−1 ei+1 fj ei+2 ei+3 f j+1 f j+2 ei+4 f j+3 Figure 1: An example of sparse features for a phrase translation. X1 X2 f j−1 fj f j+1 f j+3 X3 f j+2 Figure 2: Example hierarchical features. word alignments are observed with the same source and target sides, only the most frequently observed word alignment is kept to reduce the grammar size. Using the word alignment structure inside hierarchical phrases, we employs following feature set. • Word pair features directly capture the source/target word co"
2007.iwslt-1.16,P06-1098,1,0.894326,"parse binary features — of the order of millions — are integrated during the search. This paper gives the details of the two steps and shows the results for the Evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2007. 1. Introduction This paper presents NTT Statistical Machine Translation System evaluated in the evaluation campaign of International Workshop on Spoken Language Translation (IWSLT) 2007. Our system is composed of two steps: First, k-best translation candidates are generated using an efficient decoder for hierarchical phrase-based translation [1]. Next, the large kbest translation is reordered using a reranking voted perceptron [2]. Both systems employ a large number of sparse features — of the order of millions — to achieve a state of the art performance [3]. The large number of parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both sys"
2007.iwslt-1.16,P04-1007,0,0.0263178,"(ei−1 , fj−1 ), (ei , fj+1 )). • Insertion/deletion features are integrated in which no word alignment is associated in the target/source side. Inserted words are associated with all the words in the source sentence, such as (ei+1 , f1 ), ..., (ei+1 , fJ ) for the non-aligned word ei+1 with the source sentence f1J in Figure 1. In the same way, we use hierarchical phrase-wise deletion features by associating each inserted source word in a phrase to all the target words in the same phrase. • Target bigram features are also included to directly capture the fluency as in the n-gram language model [15], such as (ei−1 , ei ), (ei , ei+1 ), (ei+1 , ei+2 )... in Figure 1. • Hierarchical features capture dependencies the source words in a parent phrase to the source words in child phrases, such as (fj−1 , fj ), (fj−1 , fj+1 ), (fj+3 , fj ), (fj+3 , fj+1 ), (fj , fj+2 ) and (fj+1 , fj+2 ) as indicated by the arrows in Figure 2. The hierarchical features are extracted only for those source words that are aligned with the target side to limit the feature size. Algorithm 1 Online Training Algorithm for decoder T 1: 2: 3: 4: 5: 6: 7: 8: 9: Training data: T = {(f t , et )}t=1 m-best oracles: O = {}Tt"
2007.iwslt-1.16,E99-1010,0,0.0399514,"ed only for those source words that are aligned with the target side to limit the feature size. Algorithm 1 Online Training Algorithm for decoder T 1: 2: 3: 4: 5: 6: 7: 8: 9: Training data: T = {(f t , et )}t=1 m-best oracles: O = {}Tt=1 i=0 for n = 1, ..., N do for t = 1, ..., T do C t ← bestk (f t ; wi ) Ot ← oraclem (Ot ∪ C t ; et ) wi+1 = update wi using C t w.r.t. Ot i=i+1 end for end for PN T i w return i=1 NT In order to achieve the generalization capability, we introduce normalized tokens for each surface form [3]. • Word class/part-of-speech/named entity. Words are clustered by mkcls [16]. The part-of-speech (POS) and named entity (NE) tags are also integrated to capture linguistic characteristics when taggers are available. A unique word class is assigned to each surface form. However, multiple POS/NE are potentially assigned to each surface word. In our approach, we do not disambiguate labels, but simply collect a surface word to multiple tags dictionary. Those tags are integrated by first running a tagger on the training data. Then, a surface form to POS/NE dictionary is generated by collecting all possible tags for each word. • Synsets from WordNet. In order to represent s"
2007.iwslt-1.16,D07-1080,1,0.562399,"ken Language Translation (IWSLT) 2007. 1. Introduction This paper presents NTT Statistical Machine Translation System evaluated in the evaluation campaign of International Workshop on Spoken Language Translation (IWSLT) 2007. Our system is composed of two steps: First, k-best translation candidates are generated using an efficient decoder for hierarchical phrase-based translation [1]. Next, the large kbest translation is reordered using a reranking voted perceptron [2]. Both systems employ a large number of sparse features — of the order of millions — to achieve a state of the art performance [3]. The large number of parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both systems are tuned using approximated BLEU as an objective function that scales the sentence-wise BLEU to a document-wise BLEU. Domain mismatch is handled by a simple task adaptation scheme by selecting training data that"
2007.iwslt-1.16,P06-1091,0,0.0676827,"nese and Japanese. We consider all possible combination of those token types. For example, an English/Arabic word pair feature (violate, tnthk) is normalized and expanded to (viol+, tnthk), (viol+, tnth+), (violate, tnth+), etc. using the 4-letter prefix token type. As discussed above, the POS/NE/synsets labels are assigned by a one-to-many dictionary. Then, each surface form is expanded to all possible labels, then, all possible features are extracted. 2.3. Training Algorithm 1 is our generic online training algorithm. The algorithm is slightly different from other online training algorithms [17, 18] in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a metric, e.g. BLEU [19]. In line 3, a k-best list is generated by bestk (·) using the current weight vector wi for the training instance of (f t , et ). Each training instance has multiple (or, possibly one) reference translations et for the source sentence f t . Using the k-best list, m-best oracle translations Ot are updated by oraclem (·) for every iteration (line 4). Usually, a decoder cannot generate translations that exactly match the reference translations due to its"
2007.iwslt-1.16,P05-1012,0,0.28923,"WSLT) 2007. Our system is composed of two steps: First, k-best translation candidates are generated using an efficient decoder for hierarchical phrase-based translation [1]. Next, the large kbest translation is reordered using a reranking voted perceptron [2]. Both systems employ a large number of sparse features — of the order of millions — to achieve a state of the art performance [3]. The large number of parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both systems are tuned using approximated BLEU as an objective function that scales the sentence-wise BLEU to a document-wise BLEU. Domain mismatch is handled by a simple task adaptation scheme by selecting training data that resembles a test set [7]. In order to handle the ASR’s error prone input, we decoded all the nbest translations and let the reranker choose the right translation by treating the individually translated list as a s"
2007.iwslt-1.16,P06-2098,0,0.0623452,"sed of two steps: First, k-best translation candidates are generated using an efficient decoder for hierarchical phrase-based translation [1]. Next, the large kbest translation is reordered using a reranking voted perceptron [2]. Both systems employ a large number of sparse features — of the order of millions — to achieve a state of the art performance [3]. The large number of parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both systems are tuned using approximated BLEU as an objective function that scales the sentence-wise BLEU to a document-wise BLEU. Domain mismatch is handled by a simple task adaptation scheme by selecting training data that resembles a test set [7]. In order to handle the ASR’s error prone input, we decoded all the nbest translations and let the reranker choose the right translation by treating the individually translated list as a single k-best list combined with"
2007.iwslt-1.16,P06-1096,0,0.119827,"nese and Japanese. We consider all possible combination of those token types. For example, an English/Arabic word pair feature (violate, tnthk) is normalized and expanded to (viol+, tnthk), (viol+, tnth+), (violate, tnth+), etc. using the 4-letter prefix token type. As discussed above, the POS/NE/synsets labels are assigned by a one-to-many dictionary. Then, each surface form is expanded to all possible labels, then, all possible features are extracted. 2.3. Training Algorithm 1 is our generic online training algorithm. The algorithm is slightly different from other online training algorithms [17, 18] in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a metric, e.g. BLEU [19]. In line 3, a k-best list is generated by bestk (·) using the current weight vector wi for the training instance of (f t , et ). Each training instance has multiple (or, possibly one) reference translations et for the source sentence f t . Using the k-best list, m-best oracle translations Ot are updated by oraclem (·) for every iteration (line 4). Usually, a decoder cannot generate translations that exactly match the reference translations due to its"
2007.iwslt-1.16,P02-1040,0,0.07649,"rmalized and expanded to (viol+, tnthk), (viol+, tnth+), (violate, tnth+), etc. using the 4-letter prefix token type. As discussed above, the POS/NE/synsets labels are assigned by a one-to-many dictionary. Then, each surface form is expanded to all possible labels, then, all possible features are extracted. 2.3. Training Algorithm 1 is our generic online training algorithm. The algorithm is slightly different from other online training algorithms [17, 18] in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a metric, e.g. BLEU [19]. In line 3, a k-best list is generated by bestk (·) using the current weight vector wi for the training instance of (f t , et ). Each training instance has multiple (or, possibly one) reference translations et for the source sentence f t . Using the k-best list, m-best oracle translations Ot are updated by oraclem (·) for every iteration (line 4). Usually, a decoder cannot generate translations that exactly match the reference translations due to its beam search pruning and OOV. Thus, we cannot always assign scores to each reference translation. Therefore, possible oracle translations are mai"
2007.iwslt-1.16,W04-3201,0,0.0272233,"Liang et at. [18] presented a similar updating strategy in which parameters were updated toward an oracle translation found in C t , but ignored potentially better translations discovered in the past iterations. A new wi+1 is computed using the k-best list C t with respect to the oracle translations Ot (line 5). After N iterations, the algorithm returns an averaged weight vector to avoid overfitting (line 9). When updating parameters in line 5, we use the Margin Infused Relaxed Algorithm (MIRA) [4] which is an online version of the large-margin training algorithm for structured classification [20] that has been successfully used for dependency parsing [5] and joint-labeling/chunking [6]. Line 5 of the weight vector update procedure in Algorithm 1 is reAlgorithm 2 Online Training Algorithm for Reranker placed by the solution of: X 1 ˆ i+1 = argmin ||wi+1 − wi ||2 + C ξ(ˆ e, e′ ) w wi+1 2 ′ eˆ,e subject to si+1 (f t , eˆ) − si+1 (f t , e′ ) + ξ(ˆ e, e′ ) ≥ L(ˆ e , e′ ; et ) ′ ξ(ˆ e, e ) ≥ 0 ∀ˆ e ∈ Ot , ∀e′ ∈ C t (3)  ⊤ where si (f t , e) = wi · h(f t , e). ξ(·) is a non-negative slack variable and C ≥ 0 is a constant to control the influence to the objective function. A larger C implies"
2007.iwslt-1.16,P02-1034,0,0.0176638,"ain the best oracle translations O1T = eˆ1 , ..., eˆT that is treated as a “bed” document. The approximated BLEU for a hypothesized translation e′ for the training instance (f t , et ) is computed over the bed O1T except for eˆt , which is replaced by e′ : BLEU({ˆ e1 , ..., eˆt−1 , e′ , eˆt+1 , ..., eˆT }; E) The loss computed by the approximated BLEU measures the document-wise loss of substituting the correct translation eˆt Our reranking system is basically identical to the system presented in the last year’s IWSLT 2006 evaluation [2] that is based on the parse reranking method explained in [21]. We first generate n-best lists of candidate translations from the decoder, then train reranking model using the development set with additional features by ranking voted perceptron. Finally, during the testing, we rerank the k-best list of test data from the decoder by the parameters for the reranking. A separately trained reranking model is used for the ASR’s nbest list. The reranker selects the best translation out of the merged k · n-best list generated by translating all the sentences in the n-best list. 3.1. Features The reranking system employs a slightly different feature set from the"
2007.iwslt-1.16,takezawa-etal-2002-toward,0,0.068604,"n line 8 is based on an perceptron algorithm with the update amount scaled by the loss function L(·).  wn = wn + L(Rj , Ri ; et ) · h(f t , Rj ) − h(f t , Ri ) (5) As our loss function, we employed the difference of the approximated BLEU in Section 2.4, but used a set of 1-best translations from the decoder as our bed document, instead of oracle translations. The idea is to directly measure the gain or loss by selecting the translation different from the original 1-best translation of the decoder. 4. Evaluation 4.1. Data The major training data comes from IWSLT supplied data, a subset of BTEC[22]. We also used common bilingual data either in the public domain or from the LDC as indicated in Table 1. Additional data for Arabic/English and Chinese/English comes from a set of LDC bilingual news data, lexicon and the named entity list. For Italian/English, a portion of EuroParl [23] was extracted. Additional data for Japanese/English come from the news data and misJapanese-to-English 1,055,144 10,811,003 8,646,894 384,236 254,442 NiCT, others Table 2: The source language perplexity for the “clean” development and test set. dev set test set Arabic-to-English 561.96 214.99 Italian-to-Englis"
2007.iwslt-1.16,2005.mtsummit-papers.11,0,0.00566131,"ions from the decoder as our bed document, instead of oracle translations. The idea is to directly measure the gain or loss by selecting the translation different from the original 1-best translation of the decoder. 4. Evaluation 4.1. Data The major training data comes from IWSLT supplied data, a subset of BTEC[22]. We also used common bilingual data either in the public domain or from the LDC as indicated in Table 1. Additional data for Arabic/English and Chinese/English comes from a set of LDC bilingual news data, lexicon and the named entity list. For Italian/English, a portion of EuroParl [23] was extracted. Additional data for Japanese/English come from the news data and misJapanese-to-English 1,055,144 10,811,003 8,646,894 384,236 254,442 NiCT, others Table 2: The source language perplexity for the “clean” development and test set. dev set test set Arabic-to-English 561.96 214.99 Italian-to-English 277.24 271.39 51.29 13.45 Japanese-to-English Chinese-to-English 188.49 73.18 cellaneous text data supplied by NiCT [24], together with textbook-like data, a lexicon and a named entity list in the public domain 1 . The corpus statistics is presented in Table 1. Since there exists large"
2007.iwslt-1.16,P03-1010,0,0.0124565,"ata for Arabic/English and Chinese/English comes from a set of LDC bilingual news data, lexicon and the named entity list. For Italian/English, a portion of EuroParl [23] was extracted. Additional data for Japanese/English come from the news data and misJapanese-to-English 1,055,144 10,811,003 8,646,894 384,236 254,442 NiCT, others Table 2: The source language perplexity for the “clean” development and test set. dev set test set Arabic-to-English 561.96 214.99 Italian-to-English 277.24 271.39 51.29 13.45 Japanese-to-English Chinese-to-English 188.49 73.18 cellaneous text data supplied by NiCT [24], together with textbook-like data, a lexicon and a named entity list in the public domain 1 . The corpus statistics is presented in Table 1. Since there exists larger mismatch with the IWSLT condition, we extracted texts that do not contain any digits by discarding sentences that match the regular expression, “[09]”. We used a development set of 4, 5 and 5b for estimating parameters both of the decoder and the reranker, since those data include ASR’s outputs. Tokenization/tagging are performed by the following tools: English data is POS tagged by a MaxEnt-based tool [25] for use in the decode"
2007.iwslt-1.16,H05-1059,0,0.0194319,"data supplied by NiCT [24], together with textbook-like data, a lexicon and a named entity list in the public domain 1 . The corpus statistics is presented in Table 1. Since there exists larger mismatch with the IWSLT condition, we extracted texts that do not contain any digits by discarding sentences that match the regular expression, “[09]”. We used a development set of 4, 5 and 5b for estimating parameters both of the decoder and the reranker, since those data include ASR’s outputs. Tokenization/tagging are performed by the following tools: English data is POS tagged by a MaxEnt-based tool [25] for use in the decoder, and by a rule-based Brill’s POS tagger for reranking. Arabic data is tokenized by simply isolating Arabic scripts. Italian data is POS tagged by treetagger [26]. Japanese/Chinese texts are POS tagged/NE chunked [27]. After tokenization, we removed all the punctuation marks in the source side of bilingual data and lowercased the texts. The English side of the bilingual data is case/punctuation preserved. 4.2. Task Adaptation As discussed in Section 4.1, we extracted bilingual data from various sources, ranging from in-domain travel related data to out-of-domain news, mi"
2007.iwslt-1.16,W03-1506,0,0.274806,"d texts that do not contain any digits by discarding sentences that match the regular expression, “[09]”. We used a development set of 4, 5 and 5b for estimating parameters both of the decoder and the reranker, since those data include ASR’s outputs. Tokenization/tagging are performed by the following tools: English data is POS tagged by a MaxEnt-based tool [25] for use in the decoder, and by a rule-based Brill’s POS tagger for reranking. Arabic data is tokenized by simply isolating Arabic scripts. Italian data is POS tagged by treetagger [26]. Japanese/Chinese texts are POS tagged/NE chunked [27]. After tokenization, we removed all the punctuation marks in the source side of bilingual data and lowercased the texts. The English side of the bilingual data is case/punctuation preserved. 4.2. Task Adaptation As discussed in Section 4.1, we extracted bilingual data from various sources, ranging from in-domain travel related data to out-of-domain news, miscellaneous texts and lexicons. Their characteristics are very different from the style in the IWSLT development and test conditions. Table 2 shows the development/test set perplexity of the source side language computed by the trigram of t"
2007.iwslt-1.16,N06-1014,0,0.0286367,"Missing"
2007.iwslt-1.16,2006.iwslt-evaluation.14,1,\N,Missing
2007.iwslt-1.16,C98-2225,0,\N,Missing
2008.iwslt-evaluation.13,D07-1080,1,0.906376,"Missing"
2008.iwslt-evaluation.13,2006.iwslt-evaluation.14,1,0.898254,"ted phrases, SCFG production rules are accumulated by ﬁnding “holes” in extracted contiguous phrases: • For a phrase pair (f¯, e¯), a rule X → hf¯, e¯i is extracted. • For a rule X → hγ, αi and a phrase pair (f¯, e¯) s.t. γ = γ1 f¯γ2 and α = α1 e¯α2 , a rule X → hγ1 X k γ2 , α1 X k α2 i is extracted. where boxed indices non-terminals. k indicate one-to-one mapping between 2.3. Decoder features Features used in our machine translation component are realvalued scores derived from the translation and language models. These features are those used as baseline features in our IWSLT 2006 evaluation [3]: • Hierarchical phrase translation probabilities We employs Pegasos1 , a fast optimization algorithm for linear-kernel SVMs. It uses only k samples to calculate subgradient for optimization, so learning time of Pegasos does not depend on the training data size [5]. 3.2. Approximated BLEU We use approximated BLEU [3] to choose the best translation candidates, for optimizing the reranker in terms of BLEU [11]. The approximated BLEU is independently calculated on each translation candidate for each sentence in reranker training data during pre-processing, although original BLEU required document"
2008.iwslt-evaluation.13,2007.iwslt-1.16,1,0.843688,"entences O1T = {e11 , ..., eT1 }, the approximated BLEU on i-best translation candidate for t-th input sentence eti is calculated by substituting et1 with eti , i.e. the BLEU on the sentence set t t+1 T {e11 , ..., et−1 1 , ei , e1 , ..., e1 }. 3.3. Reranker features We use a large number of sparse binary features for reranking, as well as a real-valued feature (decoder score). • Lexical translation probabilities in phrase pairs 3.3.1. Word alignment features • Word-based insertion/deletion penalties We use source-target word pairs extracted by separately running IBM Model 1 in both direction [4]. In addition to source-target word unigram pairs, we used pairs of targetside word bigram and their corresponding source-side words in terms of the word alignment. We also include POS-based features, target-side word surfaces are replaced with their POS tags in the word alignment features above. Target-side (English) POS tags are automatically annotated by Brill Tagger. • Word 5-gram language model scores • Reordering penalties • Length penalties on both words and hierarchical phrases 3. Reranking Component Our reranking component is based on Ranking SVMs [1]. Each decoder k-best translation"
2008.iwslt-evaluation.13,P03-1021,0,0.0231724,"2 - work in the experiments, because they failed to capture useful context information in the current condition. We discuss these features using distinctive examples between reranker selections and decoder 1-bests. This paper is organized as follows: Section 2 brieﬂy describes our SMT decoder. Section 3 describes our reranking component and sparse features for reranking. Section 4 presents the results for the evaluation campaign of IWSLT 2008, followed by discussion in Section 5. 2. Machine Translation Component 2.1. Statistical Machine Translation We use a linear feature combination approach [6] in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: eˆ = argmax w&gt; · h(f, e) (1) e where h(f, e) is a feature vector. w is a weight vector that scales the contribution from each feature. Feature weights (i.e. elements of w) are optimized based on minimum error rate training [6]. 2.2. Hierarchical Phrase-based Approach Our SMT component employs the hierarchical phrase-based approach [7], in which the translation model is based on a stochastic synchronous context-free grammar (SCFG). A translation is generated by hie"
2008.iwslt-evaluation.13,J07-2003,0,0.0606697,"on in Section 5. 2. Machine Translation Component 2.1. Statistical Machine Translation We use a linear feature combination approach [6] in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: eˆ = argmax w&gt; · h(f, e) (1) e where h(f, e) is a feature vector. w is a weight vector that scales the contribution from each feature. Feature weights (i.e. elements of w) are optimized based on minimum error rate training [6]. 2.2. Hierarchical Phrase-based Approach Our SMT component employs the hierarchical phrase-based approach [7], in which the translation model is based on a stochastic synchronous context-free grammar (SCFG). A translation is generated by hierarchically combining phrases using non-terminals. Each production rule of SCFG takes the following form. X → hγ, α, ∼i (2) In the notation above, X is a non-terminal symbol, γ is a source-side string of terminal and non-terminal symbols, and α is a target-side one. γ and α share the same number of non-terminals whose one-to-one mapping is deﬁned by ∼. Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by"
2008.iwslt-evaluation.13,N03-1017,0,0.0076853,"ased on a stochastic synchronous context-free grammar (SCFG). A translation is generated by hierarchically combining phrases using non-terminals. Each production rule of SCFG takes the following form. X → hγ, α, ∼i (2) In the notation above, X is a non-terminal symbol, γ is a source-side string of terminal and non-terminal symbols, and α is a target-side one. γ and α share the same number of non-terminals whose one-to-one mapping is deﬁned by ∼. Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrase-based approach [8]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic structure. Our decoder and rule extraction procedure is based on Hiero [7]. The decoder is an in-house developed CKY-based Proceedings of IWSLT 2008, Hawaii - U.S.A. one. Rules in forms of (2) are extracted using phrase pairs obtained by the phrase extraction algorithm [8]. The phrase extraction uses many-to-many word alignment, derived from heuristics on one-to-many word alignment in both directions [9, 10]. Using the extracted phrases, SCFG production rules are accumulated by"
2008.iwslt-evaluation.13,J03-1002,0,0.00281422,"hrases that is not directly modeled by a conventional phrase-based approach [8]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic structure. Our decoder and rule extraction procedure is based on Hiero [7]. The decoder is an in-house developed CKY-based Proceedings of IWSLT 2008, Hawaii - U.S.A. one. Rules in forms of (2) are extracted using phrase pairs obtained by the phrase extraction algorithm [8]. The phrase extraction uses many-to-many word alignment, derived from heuristics on one-to-many word alignment in both directions [9, 10]. Using the extracted phrases, SCFG production rules are accumulated by ﬁnding “holes” in extracted contiguous phrases: • For a phrase pair (f¯, e¯), a rule X → hf¯, e¯i is extracted. • For a rule X → hγ, αi and a phrase pair (f¯, e¯) s.t. γ = γ1 f¯γ2 and α = α1 e¯α2 , a rule X → hγ1 X k γ2 , α1 X k α2 i is extracted. where boxed indices non-terminals. k indicate one-to-one mapping between 2.3. Decoder features Features used in our machine translation component are realvalued scores derived from the translation and language models. These features are those used as baseline features in our IWSL"
2008.iwslt-evaluation.13,J04-4002,0,0.0283074,"hrases that is not directly modeled by a conventional phrase-based approach [8]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic structure. Our decoder and rule extraction procedure is based on Hiero [7]. The decoder is an in-house developed CKY-based Proceedings of IWSLT 2008, Hawaii - U.S.A. one. Rules in forms of (2) are extracted using phrase pairs obtained by the phrase extraction algorithm [8]. The phrase extraction uses many-to-many word alignment, derived from heuristics on one-to-many word alignment in both directions [9, 10]. Using the extracted phrases, SCFG production rules are accumulated by ﬁnding “holes” in extracted contiguous phrases: • For a phrase pair (f¯, e¯), a rule X → hf¯, e¯i is extracted. • For a rule X → hγ, αi and a phrase pair (f¯, e¯) s.t. γ = γ1 f¯γ2 and α = α1 e¯α2 , a rule X → hγ1 X k γ2 , α1 X k α2 i is extracted. where boxed indices non-terminals. k indicate one-to-one mapping between 2.3. Decoder features Features used in our machine translation component are realvalued scores derived from the translation and language models. These features are those used as baseline features in our IWSL"
2008.iwslt-evaluation.13,P02-1040,0,0.0764379,"in our machine translation component are realvalued scores derived from the translation and language models. These features are those used as baseline features in our IWSLT 2006 evaluation [3]: • Hierarchical phrase translation probabilities We employs Pegasos1 , a fast optimization algorithm for linear-kernel SVMs. It uses only k samples to calculate subgradient for optimization, so learning time of Pegasos does not depend on the training data size [5]. 3.2. Approximated BLEU We use approximated BLEU [3] to choose the best translation candidates, for optimizing the reranker in terms of BLEU [11]. The approximated BLEU is independently calculated on each translation candidate for each sentence in reranker training data during pre-processing, although original BLEU required document-wise calculation and is not suitable for sentence-level reranking. Given 1-best translation outputs for T input sentences O1T = {e11 , ..., eT1 }, the approximated BLEU on i-best translation candidate for t-th input sentence eti is calculated by substituting et1 with eti , i.e. the BLEU on the sentence set t t+1 T {e11 , ..., et−1 1 , ei , e1 , ..., e1 }. 3.3. Reranker features We use a large number of spar"
2008.iwslt-evaluation.13,W03-1506,0,0.0710488,"Missing"
2008.iwslt-evaluation.13,C08-1137,0,\N,Missing
2008.iwslt-evaluation.13,2007.iwslt-1.8,0,\N,Missing
2008.iwslt-evaluation.13,P07-1002,0,\N,Missing
2008.iwslt-evaluation.13,W06-3110,0,\N,Missing
2008.iwslt-evaluation.13,P06-1066,0,\N,Missing
2008.iwslt-evaluation.13,P07-2045,0,\N,Missing
2008.iwslt-evaluation.13,N04-1022,0,\N,Missing
2008.iwslt-evaluation.13,2005.iwslt-1.16,0,\N,Missing
2008.iwslt-evaluation.13,W06-3119,0,\N,Missing
2008.iwslt-evaluation.13,I08-1066,0,\N,Missing
2008.iwslt-evaluation.13,2006.iwslt-evaluation.22,0,\N,Missing
2020.acl-main.271,N18-1202,0,0.0243092,"r method, S INGLE E NS, outperformed S INGLE and 1/K E NS on all datasets. Most notably, S INGLE E NS surpassed N ORMAL E NS on IMDB and Rotten with 1/9 fewer parameters. 2005), and RCV1 (Yiming et al., 2004) datasets for text classification, and the CoNLL-2003 (Sang and Meulder, 2003) and CoNLL-2000 datasets (Sang and Sabine, 2000) for sequence labeling. We used the Transformer model (Vaswani et al., 2017) as the base model for all experiments, and its token vector representations were then empowered by pretrained vectors of GloVe, (Jeffrey et al., 2014), BERT (Devlin et al., 2018), or ELMo (Matthew et al., 2018). The models are referred to as T FM :G LOV E, T FM :BERT, and T FM :ELM O, respectively.1 For T FM :BERT, we incorporated the feature (or hidden) vectors of the final layer in the BERT model as the embedding vectors while adopting drop-net technique (Zhu et al., 2020). All the models have dropout layers to assess the complementarity of our method and dropout. We compared our method (S INGLE E NS) to a single model (S INGLE), a normal ensemble (N ORMAL E NS), and a normal ensemble in which each component has approximately 1/K parameters2 (1/K E NS).3 Although other ensemble-like methods discus"
2020.acl-main.271,Q17-1024,0,0.0303151,"the shortcomings of traditional ensemble techniques. For training Snapshot Ensembles, (Huang et al., 2017) used a single model to construct multiple models by converging into multiple local minima along the optimization path. For inference distillation, (Hinton et al., 2015) transferred the knowledge of the ensemble model into a single model. These methods use multiple models either during training or inference, which partially solves the negative effects of the traditional ensemble. The incorporation of pseudo-tags is a standard technique widely used in the NLP community, (Rico et al., 2016; Melvin et al., 2017). However, to the best of our knowledge, our approach is the first attempt to incorporate pseudo-tags as an identification marker of virtual models within a single model. The most similar approach to ours is dropout (Srivastava et al., 2014), which stochastically omits each hidden unit during each mini-batch, and in which all units are utilized for inference. Huang et al. (2017) interpreted this technique as implicitly using an exponential number of virtual models within the same network. As opposed to dropout, our method explicitly utilizes virtual models with a shared parameter, which is as"
2020.acl-main.271,N16-1005,0,0.0316138,"hods have overcome the shortcomings of traditional ensemble techniques. For training Snapshot Ensembles, (Huang et al., 2017) used a single model to construct multiple models by converging into multiple local minima along the optimization path. For inference distillation, (Hinton et al., 2015) transferred the knowledge of the ensemble model into a single model. These methods use multiple models either during training or inference, which partially solves the negative effects of the traditional ensemble. The incorporation of pseudo-tags is a standard technique widely used in the NLP community, (Rico et al., 2016; Melvin et al., 2017). However, to the best of our knowledge, our approach is the first attempt to incorporate pseudo-tags as an identification marker of virtual models within a single model. The most similar approach to ours is dropout (Srivastava et al., 2014), which stochastically omits each hidden unit during each mini-batch, and in which all units are utilized for inference. Huang et al. (2017) interpreted this technique as implicitly using an exponential number of virtual models within the same network. As opposed to dropout, our method explicitly utilizes virtual models with a shared p"
2020.acl-main.271,W03-0419,0,0.132184,"150 M 95.67 (−0.75) 2000 ELM O S INGLE E NS 100 M 96.56 (+0.14) N ORMAL E NS 900 M 96.67 (+0.25) Table 2: Test F1 score and parameter size for sequence labeling tasks. Similarly to N ORMAL E NS, S IN GLE E NS improved the score even at high performance levels. Table 1: Test accuracy and parameter size for text classification tasks. Our method, S INGLE E NS, outperformed S INGLE and 1/K E NS on all datasets. Most notably, S INGLE E NS surpassed N ORMAL E NS on IMDB and Rotten with 1/9 fewer parameters. 2005), and RCV1 (Yiming et al., 2004) datasets for text classification, and the CoNLL-2003 (Sang and Meulder, 2003) and CoNLL-2000 datasets (Sang and Sabine, 2000) for sequence labeling. We used the Transformer model (Vaswani et al., 2017) as the base model for all experiments, and its token vector representations were then empowered by pretrained vectors of GloVe, (Jeffrey et al., 2014), BERT (Devlin et al., 2018), or ELMo (Matthew et al., 2018). The models are referred to as T FM :G LOV E, T FM :BERT, and T FM :ELM O, respectively.1 For T FM :BERT, we incorporated the feature (or hidden) vectors of the final layer in the BERT model as the embedding vectors while adopting drop-net technique (Zhu et al., 2"
2020.acl-main.271,W00-0726,0,0.170064,"M 96.56 (+0.14) N ORMAL E NS 900 M 96.67 (+0.25) Table 2: Test F1 score and parameter size for sequence labeling tasks. Similarly to N ORMAL E NS, S IN GLE E NS improved the score even at high performance levels. Table 1: Test accuracy and parameter size for text classification tasks. Our method, S INGLE E NS, outperformed S INGLE and 1/K E NS on all datasets. Most notably, S INGLE E NS surpassed N ORMAL E NS on IMDB and Rotten with 1/9 fewer parameters. 2005), and RCV1 (Yiming et al., 2004) datasets for text classification, and the CoNLL-2003 (Sang and Meulder, 2003) and CoNLL-2000 datasets (Sang and Sabine, 2000) for sequence labeling. We used the Transformer model (Vaswani et al., 2017) as the base model for all experiments, and its token vector representations were then empowered by pretrained vectors of GloVe, (Jeffrey et al., 2014), BERT (Devlin et al., 2018), or ELMo (Matthew et al., 2018). The models are referred to as T FM :G LOV E, T FM :BERT, and T FM :ELM O, respectively.1 For T FM :BERT, we incorporated the feature (or hidden) vectors of the final layer in the BERT model as the embedding vectors while adopting drop-net technique (Zhu et al., 2020). All the models have dropout layers to asse"
2020.acl-main.271,D14-1162,0,0.0826061,"ccuracy and parameter size for text classification tasks. Our method, S INGLE E NS, outperformed S INGLE and 1/K E NS on all datasets. Most notably, S INGLE E NS surpassed N ORMAL E NS on IMDB and Rotten with 1/9 fewer parameters. 2005), and RCV1 (Yiming et al., 2004) datasets for text classification, and the CoNLL-2003 (Sang and Meulder, 2003) and CoNLL-2000 datasets (Sang and Sabine, 2000) for sequence labeling. We used the Transformer model (Vaswani et al., 2017) as the base model for all experiments, and its token vector representations were then empowered by pretrained vectors of GloVe, (Jeffrey et al., 2014), BERT (Devlin et al., 2018), or ELMo (Matthew et al., 2018). The models are referred to as T FM :G LOV E, T FM :BERT, and T FM :ELM O, respectively.1 For T FM :BERT, we incorporated the feature (or hidden) vectors of the final layer in the BERT model as the embedding vectors while adopting drop-net technique (Zhu et al., 2020). All the models have dropout layers to assess the complementarity of our method and dropout. We compared our method (S INGLE E NS) to a single model (S INGLE), a normal ensemble (N ORMAL E NS), and a normal ensemble in which each component has approximately 1/K paramete"
2020.acl-main.271,P11-1015,0,\N,Missing
2020.acl-main.271,P05-1015,0,\N,Missing
2020.acl-main.271,W18-6401,0,\N,Missing
2020.acl-main.271,N19-1423,0,\N,Missing
2020.acl-main.271,W19-5301,0,\N,Missing
2020.acl-main.391,D19-1435,0,0.297242,"Missing"
2020.acl-main.391,W19-4410,0,0.0426625,"orpora, however, it may not be adapted to either the GEC task or the task-specific distribution of inputs. The reason is that in the GEC model, unlike the data used for training BERT, the input can be an erroneous sentence. To fill the gap between corpora used to train GEC and BERT, we additionally train BERT on GEC corpora (BERT-fuse mask) or finetune BERT as a GED model (BERT-fuse GED) and use it for BERT-fuse. GED is a sequence labeling task that detects grammatically incorrect words in input sentences (Rei and Yannakoudakis, 2016; Kaneko et al., 2017). Since BERT is also effective in GED (Bell et al., 2019; Kaneko and Komachi, 2019), it is considered to be suitable for fine-tuning to take into account grammatical errors. Transformer (big) 30 4096 Adam (β1 = 0.9, β2 = 0.98,  = 1 × 10−8 ) 3 × 10−5 1 × 10−6 label smoothed cross-entropy (ls = 0.1) (Szegedy et al., 2016) 0.3 0.1 5 Models Hyperparameter values for the GEC model is listed in Table 1. For the BERT initialized GEC model, we provided experiments based on the open-source code2 . For the BERT-fuse GEC model, we use the code provided by Zhu et al. (2020)3 . While the training the GEC model, the model was evaluated on the development set a"
2020.acl-main.391,P17-1074,0,0.0862394,"p1 https://www.cl.cam.ac.uk/research/nl/ bea2019st/ BERT-Base (cased) 3 32 128 Adam (β1 = 0.9, β2 = 0.999,  = 1 × 10−8 ) 4e − 5 0.1 Table 1: Hyperparameters values of GEC model and Fine-tuned BERT. we use W&I-train, NUCLE, and FCE-train as training, and W&I-dev was used as development data. 4.2 Evaluating GEC Performance In GEC, it is important to evaluate the model with multiple datasets (Mita et al., 2019). Therefore, we used GEC evaluation data such as W&I-test, CoNLL-2014 (Ng et al., 2014), FCE-test and JFLEG (Napoles et al., 2017). We used ERRANT evaluation metrics (Felice et al., 2016; Bryant et al., 2017) for W&I-test, M2 score (Dahlmeier and Ng, 2012) for CoNLL-2014 and FCE-test sets, and GLEU (Napoles et al., 2015) for JFLEG. All our results (except ensemble) are the average of four distinct trials using four different random seeds. 4.3 We use the BEA-2019 (Bryant et al., 2019) official shared task data as training and development sets. Specifically, to train a GEC model, we use W&I-train (Granger, 1998; Yannakoudakis et al., 2018), NUCLE (Dahlmeier et al., 2013), FCE-train (Yannakoudakis et al., 2011) and Lang-8 (Mizumoto et al., 2011) datasets. We use W&I-dev as a development set. Note tha"
2020.acl-main.391,P19-1042,0,0.214777,"d from the GEC training data. Our experiments show that using the output of the fine-tuned BERT model as additional features in the GEC model (method (c)) is the most effective way of using BERT in most of the GEC corpora that we used in the experiments. We also show that the performance of GEC improves further by combining the BERT-fuse mask and BERTfuse GED methods. The best-performing model achieves state-of-the-art results on the BEA-2019 and CoNLL-2014 benchmarks. 2 Related Work Studies have reported that a MLM can improve the performance of GEC when it is employed either as a re-ranker (Chollampatt et al., 2019; Kaneko et al., 2019) or as a filtering tool (Asano et al., 2019; Kiyono et al., 2019). EncDec-based GEC models combined with MLMs can also be used in combination with these pipeline methods. Asano et al. (2019) proposed sequence labeling models based on correction methods. Our method can utilize the existing EncDec GEC knowledge, but these methods cannot be utilized due to the different architecture of the model. Besides, to the best of our knowledge, no research has yet been conducted that incorporates information of MLMs for effectively training the EncDec GEC model. MLMs are generally use"
2020.acl-main.391,N12-1067,0,0.545172,"019st/ BERT-Base (cased) 3 32 128 Adam (β1 = 0.9, β2 = 0.999,  = 1 × 10−8 ) 4e − 5 0.1 Table 1: Hyperparameters values of GEC model and Fine-tuned BERT. we use W&I-train, NUCLE, and FCE-train as training, and W&I-dev was used as development data. 4.2 Evaluating GEC Performance In GEC, it is important to evaluate the model with multiple datasets (Mita et al., 2019). Therefore, we used GEC evaluation data such as W&I-test, CoNLL-2014 (Ng et al., 2014), FCE-test and JFLEG (Napoles et al., 2017). We used ERRANT evaluation metrics (Felice et al., 2016; Bryant et al., 2017) for W&I-test, M2 score (Dahlmeier and Ng, 2012) for CoNLL-2014 and FCE-test sets, and GLEU (Napoles et al., 2015) for JFLEG. All our results (except ensemble) are the average of four distinct trials using four different random seeds. 4.3 We use the BEA-2019 (Bryant et al., 2019) official shared task data as training and development sets. Specifically, to train a GEC model, we use W&I-train (Granger, 1998; Yannakoudakis et al., 2018), NUCLE (Dahlmeier et al., 2013), FCE-train (Yannakoudakis et al., 2011) and Lang-8 (Mizumoto et al., 2011) datasets. We use W&I-dev as a development set. Note that we excluded sentence pairs that were not corre"
2020.acl-main.391,W13-1703,0,0.279925,"est, CoNLL-2014 (Ng et al., 2014), FCE-test and JFLEG (Napoles et al., 2017). We used ERRANT evaluation metrics (Felice et al., 2016; Bryant et al., 2017) for W&I-test, M2 score (Dahlmeier and Ng, 2012) for CoNLL-2014 and FCE-test sets, and GLEU (Napoles et al., 2015) for JFLEG. All our results (except ensemble) are the average of four distinct trials using four different random seeds. 4.3 We use the BEA-2019 (Bryant et al., 2019) official shared task data as training and development sets. Specifically, to train a GEC model, we use W&I-train (Granger, 1998; Yannakoudakis et al., 2018), NUCLE (Dahlmeier et al., 2013), FCE-train (Yannakoudakis et al., 2011) and Lang-8 (Mizumoto et al., 2011) datasets. We use W&I-dev as a development set. Note that we excluded sentence pairs that were not corrected from the training data. To train BERT for BERT-fuse mask and GED, 1 Model Architecture Number of epochs Batch size Max sentence length Optimizer BERT-fuse Mask and GED Experimental Setup 4.1 GED model Learning rate Dropout The advantage of the BERT-fuse is that it can preserve pre-trained information from raw corpora, however, it may not be adapted to either the GEC task or the task-specific distribution of input"
2020.acl-main.391,N19-1423,0,0.230041,"hiro/bert-gec. 1 Introduction Grammatical Error Correction (GEC) is a sequenceto-sequence task where a model corrects an ungrammatical sentence to a grammatical sentence. Numerous studies on GEC have successfully used encoder-decoder (EncDec) based models, and in fact, most current state-of-the-art neural GEC models employ this architecture (Zhao et al., 2019; Grundkiewicz et al., 2019; Kiyono et al., 2019). In light of this trend, one natural, intriguing question is whether neural EndDec GEC models can benefit from the recent advances of masked language models (MLMs) since MLMs such as BERT (Devlin et al., 2019) have been shown to yield substantial improvements in a variety of NLP Kentaro Inui3,2 tasks (Qiu et al., 2020). BERT, for example, builds on the Transformer architecture (Vaswani et al., 2017) and is trained on large raw corpora to learn general representations of linguistic components (e.g., words and sentences) in context, which have been shown useful for various tasks. In recent years, MLMs have been used not only for classification and sequence labeling tasks but also for language generation, where combining MLMs with EncDec models of a downstream task makes a noticeable improvement (Lamp"
2020.acl-main.391,C16-1079,0,0.36437,"elopment Sets workshop1 https://www.cl.cam.ac.uk/research/nl/ bea2019st/ BERT-Base (cased) 3 32 128 Adam (β1 = 0.9, β2 = 0.999,  = 1 × 10−8 ) 4e − 5 0.1 Table 1: Hyperparameters values of GEC model and Fine-tuned BERT. we use W&I-train, NUCLE, and FCE-train as training, and W&I-dev was used as development data. 4.2 Evaluating GEC Performance In GEC, it is important to evaluate the model with multiple datasets (Mita et al., 2019). Therefore, we used GEC evaluation data such as W&I-test, CoNLL-2014 (Ng et al., 2014), FCE-test and JFLEG (Napoles et al., 2017). We used ERRANT evaluation metrics (Felice et al., 2016; Bryant et al., 2017) for W&I-test, M2 score (Dahlmeier and Ng, 2012) for CoNLL-2014 and FCE-test sets, and GLEU (Napoles et al., 2015) for JFLEG. All our results (except ensemble) are the average of four distinct trials using four different random seeds. 4.3 We use the BEA-2019 (Bryant et al., 2019) official shared task data as training and development sets. Specifically, to train a GEC model, we use W&I-train (Granger, 1998; Yannakoudakis et al., 2018), NUCLE (Dahlmeier et al., 2013), FCE-train (Yannakoudakis et al., 2011) and Lang-8 (Mizumoto et al., 2011) datasets. We use W&I-dev as a dev"
2020.acl-main.391,W19-4427,0,0.49639,"nal features in the GEC model, maximizes the benefit of the MLM. The best-performing model achieves state-ofthe-art performances on the BEA-2019 and CoNLL-2014 benchmarks. Our code is publicly available at: https://github.com/ kanekomasahiro/bert-gec. 1 Introduction Grammatical Error Correction (GEC) is a sequenceto-sequence task where a model corrects an ungrammatical sentence to a grammatical sentence. Numerous studies on GEC have successfully used encoder-decoder (EncDec) based models, and in fact, most current state-of-the-art neural GEC models employ this architecture (Zhao et al., 2019; Grundkiewicz et al., 2019; Kiyono et al., 2019). In light of this trend, one natural, intriguing question is whether neural EndDec GEC models can benefit from the recent advances of masked language models (MLMs) since MLMs such as BERT (Devlin et al., 2019) have been shown to yield substantial improvements in a variety of NLP Kentaro Inui3,2 tasks (Qiu et al., 2020). BERT, for example, builds on the Transformer architecture (Vaswani et al., 2017) and is trained on large raw corpora to learn general representations of linguistic components (e.g., words and sentences) in context, which have been shown useful for various"
2020.acl-main.391,I17-1005,1,0.888118,"is that it can preserve pre-trained information from raw corpora, however, it may not be adapted to either the GEC task or the task-specific distribution of inputs. The reason is that in the GEC model, unlike the data used for training BERT, the input can be an erroneous sentence. To fill the gap between corpora used to train GEC and BERT, we additionally train BERT on GEC corpora (BERT-fuse mask) or finetune BERT as a GED model (BERT-fuse GED) and use it for BERT-fuse. GED is a sequence labeling task that detects grammatically incorrect words in input sentences (Rei and Yannakoudakis, 2016; Kaneko et al., 2017). Since BERT is also effective in GED (Bell et al., 2019; Kaneko and Komachi, 2019), it is considered to be suitable for fine-tuning to take into account grammatical errors. Transformer (big) 30 4096 Adam (β1 = 0.9, β2 = 0.98,  = 1 × 10−8 ) 3 × 10−5 1 × 10−6 label smoothed cross-entropy (ls = 0.1) (Szegedy et al., 2016) 0.3 0.1 5 Models Hyperparameter values for the GEC model is listed in Table 1. For the BERT initialized GEC model, we provided experiments based on the open-source code2 . For the BERT-fuse GEC model, we use the code provided by Zhu et al. (2020)3 . While the training the GEC"
2020.acl-main.391,D19-1119,1,0.899179,"el, maximizes the benefit of the MLM. The best-performing model achieves state-ofthe-art performances on the BEA-2019 and CoNLL-2014 benchmarks. Our code is publicly available at: https://github.com/ kanekomasahiro/bert-gec. 1 Introduction Grammatical Error Correction (GEC) is a sequenceto-sequence task where a model corrects an ungrammatical sentence to a grammatical sentence. Numerous studies on GEC have successfully used encoder-decoder (EncDec) based models, and in fact, most current state-of-the-art neural GEC models employ this architecture (Zhao et al., 2019; Grundkiewicz et al., 2019; Kiyono et al., 2019). In light of this trend, one natural, intriguing question is whether neural EndDec GEC models can benefit from the recent advances of masked language models (MLMs) since MLMs such as BERT (Devlin et al., 2019) have been shown to yield substantial improvements in a variety of NLP Kentaro Inui3,2 tasks (Qiu et al., 2020). BERT, for example, builds on the Transformer architecture (Vaswani et al., 2017) and is trained on large raw corpora to learn general representations of linguistic components (e.g., words and sentences) in context, which have been shown useful for various tasks. In recent year"
2020.acl-main.391,N19-1333,0,0.192243,"tting. Hyperparameter values for the GED model is listed in Table 1. We used the BERT-Base cased model, for consistency across experiments5 . The model was evaluated on the development set. 4.4 Pseudo-data We also performed experiments utilizing BERTfuse, BERT-fuse mask, and BERT-fuse GED outputs as additional features to the pre-trained on the pseudo-data GEC model. The pre-trained model using pseudo-data was initialized with the P RETL ARGE +SSE model used in the Kiyono et al. (2019)6 experiments. This pseudo-data is generated by probabilistically injecting character errors into the output (Lichtarge et al., 2019) of a back4 https://github.com/huggingface/ transformers 5 https://github.com/google-research/ bert 6 https://github.com/butsugiri/ gec-pseudodata translation (Xie et al., 2018) model that generates grammatically incorrect sentences from grammatically correct sentences (Kiyono et al., 2019). 4.5 Right-to-left (R2L) Re-ranking for Ensemble We describe the R2L re-ranking technique incorporated in our experiments proposed by Sennrich et al. (2016), which proved to be efficient for the GEC task (Grundkiewicz et al., 2019; Kiyono et al., 2019). Standard left-to-right (L2R) models generate the n-bes"
2020.acl-main.391,N19-1132,1,0.882324,"Missing"
2020.acl-main.391,I11-1017,0,0.532637,". We used ERRANT evaluation metrics (Felice et al., 2016; Bryant et al., 2017) for W&I-test, M2 score (Dahlmeier and Ng, 2012) for CoNLL-2014 and FCE-test sets, and GLEU (Napoles et al., 2015) for JFLEG. All our results (except ensemble) are the average of four distinct trials using four different random seeds. 4.3 We use the BEA-2019 (Bryant et al., 2019) official shared task data as training and development sets. Specifically, to train a GEC model, we use W&I-train (Granger, 1998; Yannakoudakis et al., 2018), NUCLE (Dahlmeier et al., 2013), FCE-train (Yannakoudakis et al., 2011) and Lang-8 (Mizumoto et al., 2011) datasets. We use W&I-dev as a development set. Note that we excluded sentence pairs that were not corrected from the training data. To train BERT for BERT-fuse mask and GED, 1 Model Architecture Number of epochs Batch size Max sentence length Optimizer BERT-fuse Mask and GED Experimental Setup 4.1 GED model Learning rate Dropout The advantage of the BERT-fuse is that it can preserve pre-trained information from raw corpora, however, it may not be adapted to either the GEC task or the task-specific distribution of inputs. The reason is that in the GEC model, unlike the data used for training B"
2020.acl-main.391,P15-2097,0,0.235789,"× 10−8 ) 4e − 5 0.1 Table 1: Hyperparameters values of GEC model and Fine-tuned BERT. we use W&I-train, NUCLE, and FCE-train as training, and W&I-dev was used as development data. 4.2 Evaluating GEC Performance In GEC, it is important to evaluate the model with multiple datasets (Mita et al., 2019). Therefore, we used GEC evaluation data such as W&I-test, CoNLL-2014 (Ng et al., 2014), FCE-test and JFLEG (Napoles et al., 2017). We used ERRANT evaluation metrics (Felice et al., 2016; Bryant et al., 2017) for W&I-test, M2 score (Dahlmeier and Ng, 2012) for CoNLL-2014 and FCE-test sets, and GLEU (Napoles et al., 2015) for JFLEG. All our results (except ensemble) are the average of four distinct trials using four different random seeds. 4.3 We use the BEA-2019 (Bryant et al., 2019) official shared task data as training and development sets. Specifically, to train a GEC model, we use W&I-train (Granger, 1998; Yannakoudakis et al., 2018), NUCLE (Dahlmeier et al., 2013), FCE-train (Yannakoudakis et al., 2011) and Lang-8 (Mizumoto et al., 2011) datasets. We use W&I-dev as a development set. Note that we excluded sentence pairs that were not corrected from the training data. To train BERT for BERT-fuse mask and"
2020.acl-main.391,E17-2037,0,0.211708,"BERT and the encoder of the GEC model. 3.3 4 Train and Development Sets workshop1 https://www.cl.cam.ac.uk/research/nl/ bea2019st/ BERT-Base (cased) 3 32 128 Adam (β1 = 0.9, β2 = 0.999,  = 1 × 10−8 ) 4e − 5 0.1 Table 1: Hyperparameters values of GEC model and Fine-tuned BERT. we use W&I-train, NUCLE, and FCE-train as training, and W&I-dev was used as development data. 4.2 Evaluating GEC Performance In GEC, it is important to evaluate the model with multiple datasets (Mita et al., 2019). Therefore, we used GEC evaluation data such as W&I-test, CoNLL-2014 (Ng et al., 2014), FCE-test and JFLEG (Napoles et al., 2017). We used ERRANT evaluation metrics (Felice et al., 2016; Bryant et al., 2017) for W&I-test, M2 score (Dahlmeier and Ng, 2012) for CoNLL-2014 and FCE-test sets, and GLEU (Napoles et al., 2015) for JFLEG. All our results (except ensemble) are the average of four distinct trials using four different random seeds. 4.3 We use the BEA-2019 (Bryant et al., 2019) official shared task data as training and development sets. Specifically, to train a GEC model, we use W&I-train (Granger, 1998; Yannakoudakis et al., 2018), NUCLE (Dahlmeier et al., 2013), FCE-train (Yannakoudakis et al., 2011) and Lang-8 ("
2020.acl-main.391,N18-1057,0,0.15554,"Missing"
2020.acl-main.391,P11-1019,0,0.514397,"CE-test and JFLEG (Napoles et al., 2017). We used ERRANT evaluation metrics (Felice et al., 2016; Bryant et al., 2017) for W&I-test, M2 score (Dahlmeier and Ng, 2012) for CoNLL-2014 and FCE-test sets, and GLEU (Napoles et al., 2015) for JFLEG. All our results (except ensemble) are the average of four distinct trials using four different random seeds. 4.3 We use the BEA-2019 (Bryant et al., 2019) official shared task data as training and development sets. Specifically, to train a GEC model, we use W&I-train (Granger, 1998; Yannakoudakis et al., 2018), NUCLE (Dahlmeier et al., 2013), FCE-train (Yannakoudakis et al., 2011) and Lang-8 (Mizumoto et al., 2011) datasets. We use W&I-dev as a development set. Note that we excluded sentence pairs that were not corrected from the training data. To train BERT for BERT-fuse mask and GED, 1 Model Architecture Number of epochs Batch size Max sentence length Optimizer BERT-fuse Mask and GED Experimental Setup 4.1 GED model Learning rate Dropout The advantage of the BERT-fuse is that it can preserve pre-trained information from raw corpora, however, it may not be adapted to either the GEC task or the task-specific distribution of inputs. The reason is that in the GEC model,"
2020.acl-main.391,P16-1112,0,0.0371852,"he advantage of the BERT-fuse is that it can preserve pre-trained information from raw corpora, however, it may not be adapted to either the GEC task or the task-specific distribution of inputs. The reason is that in the GEC model, unlike the data used for training BERT, the input can be an erroneous sentence. To fill the gap between corpora used to train GEC and BERT, we additionally train BERT on GEC corpora (BERT-fuse mask) or finetune BERT as a GED model (BERT-fuse GED) and use it for BERT-fuse. GED is a sequence labeling task that detects grammatically incorrect words in input sentences (Rei and Yannakoudakis, 2016; Kaneko et al., 2017). Since BERT is also effective in GED (Bell et al., 2019; Kaneko and Komachi, 2019), it is considered to be suitable for fine-tuning to take into account grammatical errors. Transformer (big) 30 4096 Adam (β1 = 0.9, β2 = 0.98,  = 1 × 10−8 ) 3 × 10−5 1 × 10−6 label smoothed cross-entropy (ls = 0.1) (Szegedy et al., 2016) 0.3 0.1 5 Models Hyperparameter values for the GEC model is listed in Table 1. For the BERT initialized GEC model, we provided experiments based on the open-source code2 . For the BERT-fuse GEC model, we use the code provided by Zhu et al. (2020)3 . Whil"
2020.acl-main.391,N19-1014,0,0.217076,"uned MLM as additional features in the GEC model, maximizes the benefit of the MLM. The best-performing model achieves state-ofthe-art performances on the BEA-2019 and CoNLL-2014 benchmarks. Our code is publicly available at: https://github.com/ kanekomasahiro/bert-gec. 1 Introduction Grammatical Error Correction (GEC) is a sequenceto-sequence task where a model corrects an ungrammatical sentence to a grammatical sentence. Numerous studies on GEC have successfully used encoder-decoder (EncDec) based models, and in fact, most current state-of-the-art neural GEC models employ this architecture (Zhao et al., 2019; Grundkiewicz et al., 2019; Kiyono et al., 2019). In light of this trend, one natural, intriguing question is whether neural EndDec GEC models can benefit from the recent advances of masked language models (MLMs) since MLMs such as BERT (Devlin et al., 2019) have been shown to yield substantial improvements in a variety of NLP Kentaro Inui3,2 tasks (Qiu et al., 2020). BERT, for example, builds on the Transformer architecture (Vaswani et al., 2017) and is trained on large raw corpora to learn general representations of linguistic components (e.g., words and sentences) in context, which have be"
2020.acl-main.391,W14-1701,0,\N,Missing
2020.acl-main.391,K19-1074,0,\N,Missing
2020.acl-main.391,W19-4406,0,\N,Missing
2020.acl-main.391,W19-4418,1,\N,Missing
2020.acl-main.391,W19-4422,1,\N,Missing
2020.acl-main.47,P16-1162,0,0.0235598,"rst study where evidence is collected on the validity of using LMs for word order analysis and encourages further research on collecting such evidence and examining under what conditions this validity is guaranteed. 3.5 LMs settings We used auto-regressive, unidirectional LMs with Transformer (Vaswani et al., 2017). We used two variants of LMs, a character-based LM (CLM) and a subword-based LM (SLM). In training SLM, the input sentences are once divided into morphemes by MeCab (Kudo, 2006) with a UniDic dictionary,4 and then these morphemes are split into subword units by byte-pair-encoding. (Sennrich et al., 2016)5 . 160M sentences6 randomly selected from 3B web pages were used to train the LMs. Hyperparameters are shown in Appendix A. Given a sentence s, we calculate its generation − − − probability p(s) = → p (s) · ← p (s), where → p (·) and ← − p (·) are generation probabilities calculated by a left-to-right LM and a right-to-left LM, respectively. Depending on the hypothesis, we compare the generation probabilities of various variants of s with different word orders. We assume that the word order with the highest generation probability follows their canonical word order. 4 Experiment1: comparing hu"
2020.acl-main.47,D11-1045,0,0.0393984,"nical word order Every language is assumed to have a canonical word order, even those with flexible word order (Comrie, 1989). There has been a significant linguistic effort to reveal the factors determining the canonical word order (Bresnan et al., 2007; Hoji, 1985). The motivations for revealing the canonical word order range from linguistic interests to those involved in various other fields—it relates to language acquisition and production in psycholinguistics (Slobin and Bever, 1982; Akhtar, 1999), second language education (Alonso Belmonte et al., 2000), and natural language generation (Visweswariah et al., 2011) or error correction (Cheng et al., 2014) in NLP. In Japanese, there are also many studies on its canonical word order (Hoji, 1985; Saeki, 1998; Koizumi and Tamaoka, 2004; Sasano and Okumura, 2016). Japanese canonical word order The word order of Japanese is basically subject-object-verb (SOV) order, but there is no strict rule except placing the verb at the end of the sentence (Tsujimura, 2013). For example, the following three sentences have the same denotational meaning (“A teacher gave a student a book.”): あげた. (2) a. 先生が 生徒に 本を ::::: .............. teacher-NOM student-DAT book-ACC gave. b"
2020.acl-main.47,W06-3122,0,\N,Missing
2020.acl-main.47,C14-1028,0,\N,Missing
2020.acl-main.47,P16-1211,0,\N,Missing
2020.acl-main.47,W16-4120,0,\N,Missing
2020.acl-main.47,W18-2805,0,\N,Missing
2020.acl-main.47,K18-1031,0,\N,Missing
2020.acl-main.47,W17-0706,0,\N,Missing
2020.acl-main.55,P17-1046,0,0.0634245,"on arises from a nature of dialogue, that is, there are many acceptable responses to an input context, known as the one-to-many problem (Zhao et al., 2017). To tackle this problematic issue, we focus on evaluating response generation systems via response selection. In this task, systems select an appropriate response for a given context from a set of response candidates. Each candidate has the label that indicates whether the candidate is appropriate response for the given context. Traditionally, response selection has been used to evaluate retrieval-based dialogue systems (Lowe et al., 2015; Wu et al., 2017). We consider applying this task to driving the research for dialogue generation systems. Specifically, we consider using response selection to pick out promising systems that should be evaluated more precisely by humans among a lot of candidate systems. We assume that response selection is a valid option for such a preliminary evaluation on the basis of the following assumption: systems that can generate appropriate responses can also select appropriate responses. One advantage of evaluating generation systems via response selection is that it can remedy the one-to-many problem, because we do"
2020.acl-main.55,P02-1040,0,\N,Missing
2020.acl-main.55,W05-0909,0,\N,Missing
2020.acl-main.55,W04-1013,0,\N,Missing
2020.acl-main.55,D16-1230,0,\N,Missing
2020.acl-main.55,W15-4640,0,\N,Missing
2020.acl-main.55,L18-1275,0,\N,Missing
2020.acl-main.55,W19-4107,0,\N,Missing
2020.acl-main.575,D18-1019,0,0.0147201,"One drawback of this approach is the difficulty dealing with nested entities.3 By contrast, the span classification approach, adopted in this study, can straightforwardly solve nested NER (Finkel and Manning, 2009; Sohrab and Miwa, 2018; Xia et al., 2019).4 2 Very recently, a hybrid model of these two approaches has been proposed by Liu et al. (2019). 3 Some studies have sophisticated sequence labeling models for nested NER (Ju et al., 2018; Zheng et al., 2019). 4 There is an approach specialized for nested NER using hypergraphs (Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018). 3.1 Instance-Based Span Classification NER as span classification NER can be solved as multi-class classification, where each of possible spans in a sentence is assigned a class label. As we mentioned in Section 2, this approach can naturally avoid inconsistent label prediction and straightforwardly deal with nested entities. Because of these advantages over tokenwise classification, span classification has been gaining a considerable attention (Sohrab and Miwa, 2018; Xia et al., 2019). Formally, given an input sentence of T words X = (w1 , w2 , . . . , wT ), we first enumerate possible span"
2020.acl-main.575,W04-2407,0,0.0902054,"rty. The rationales underlying the model predictions are opaque for humans to understand. Many recent studies have tried to look into classifier-based neural models (Ribeiro et al., 2016; Lundberg and Lee, 2017; Koh and Liang, 2017). In this paper, instead of looking into the black-box, we build interpretable models based on instance-based learning. Before the current neural era, instance-based learning, sometimes called memory-based learning (Daelemans and Van den Bosch, 2005), was widely used for various NLP tasks, such as part-of-speech tagging (Daelemans et al., 1996), dependency parsing (Nivre et al., 2004) and machine translation (Nagao, 1984). For NER, some instance-based models have been proposed (Tjong Kim Sang, 2002; De Meulder and Daelemans, 2003; Hendrickx and van den Bosch, 2003). Recently, despite its high interpretability, this direction has not been explored. One exception is Wiseman and Stratos (2019), which used instance-based learning of token representations. Due to BIO tagging, it faces one technical challenge: inconsistent label prediction. For example, an entity candidate “World Health Organization” can be assigned inconsistent labels such as “B-LOC I-ORG I-ORG,” whereas the gr"
2020.acl-main.575,P16-1218,0,0.0699399,"Missing"
2020.acl-main.575,D18-1191,1,0.672389,"and been underexplored. This study presents and investigates an instancebased learning method for span representations. A span is a unit that consists of one or more linguistically linked words. Why do we focus on spans instead of tokens? One reason is relevant to performance. Recent neural networks can induce good span feature representations and achieve high performance in structured prediction tasks, such as named entity recognition (NER) (Sohrab and Miwa, 2018; Xia et al., 2019), constituency parsing (Stern et al., 2017; Kitaev et al., 2019), semantic role labeling (SRL) (He et al., 2018; Ouchi et al., 2018) and coreference resolution (Lee et al., 2017). Another reason is relevant to interpretability. The tasks above require recognition of linguistic structure that consists of spans. Thus, directly classifying each span based on its representation is more interpretable than token-wise classification such as BIO tagging, which reconstructs each span label from the predicted token-wise BIO tags. Our method builds a feature space where spans with the same class label are close to each other. At inference time, each span is assigned a class label based on its neighbor spans in the feature space. We c"
2020.acl-main.575,D14-1162,0,0.0920737,"en multiply hlstm with a weight matrix W and obtain the span s representation: hs = W hlstm . For the scoring s function in Equation 1 in the instance-based span model, we use the inner product between a pair of span representations: score(si , sj ) = hsi · hsj . Model configuration We train instance-based models by using K = 50 training sentences randomly retrieved for each mini-batch. At test time, we use K = 50 nearest training sentences for each sentence based on the cosine similarities between their sentence vectors8 . For the word embeddings, we use the GloVe 100-dimensional embeddings (Pennington et al., 2014) and the BERT embeddings (Devlin et al., 2019).9 6 We use the same one pre-processed by Zheng et al. (2019) at https://github.com/thecharm/ boundary-aware-nested-ner 7 We use the different span representation from the one used for flat NER because concatenating the addition features, − → − → ← − ← − h a + h b and h a + h b , to the subtraction features improves performance in our preliminary experiments. 8 For each sentence X = (w1 , w2 , . . . , wT ), its sentence vector is defined as the vector averagedP over the word embeddings (GloVe) within the sentence: T1 t wemb . t 9 Details on the exp"
2020.acl-main.575,P19-1533,0,0.0192399,"table models based on instance-based learning. Before the current neural era, instance-based learning, sometimes called memory-based learning (Daelemans and Van den Bosch, 2005), was widely used for various NLP tasks, such as part-of-speech tagging (Daelemans et al., 1996), dependency parsing (Nivre et al., 2004) and machine translation (Nagao, 1984). For NER, some instance-based models have been proposed (Tjong Kim Sang, 2002; De Meulder and Daelemans, 2003; Hendrickx and van den Bosch, 2003). Recently, despite its high interpretability, this direction has not been explored. One exception is Wiseman and Stratos (2019), which used instance-based learning of token representations. Due to BIO tagging, it faces one technical challenge: inconsistent label prediction. For example, an entity candidate “World Health Organization” can be assigned inconsistent labels such as “B-LOC I-ORG I-ORG,” whereas the groundtruth labels are “B-ORG I-ORG I-ORG.” To remedy this issue, they presented a heuristic technique for encouraging contiguous token alignment. In contrast to such token-wise prediction, we adopt span-wise prediction, which can naturally avoid this issue because each span is assigned one label. NER is generall"
2020.acl-main.575,D19-1034,0,0.0704427,"d by using neural networks and fed into a classifier, such as conditional random fields (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). One drawback of this approach is the difficulty dealing with nested entities.3 By contrast, the span classification approach, adopted in this study, can straightforwardly solve nested NER (Finkel and Manning, 2009; Sohrab and Miwa, 2018; Xia et al., 2019).4 2 Very recently, a hybrid model of these two approaches has been proposed by Liu et al. (2019). 3 Some studies have sophisticated sequence labeling models for nested NER (Ju et al., 2018; Zheng et al., 2019). 4 There is an approach specialized for nested NER using hypergraphs (Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018). 3.1 Instance-Based Span Classification NER as span classification NER can be solved as multi-class classification, where each of possible spans in a sentence is assigned a class label. As we mentioned in Section 2, this approach can naturally avoid inconsistent label prediction and straightforwardly deal with nested entities. Because of these advantages over tokenwise classification, span classification has been gaining a considerable attent"
2020.acl-main.575,N16-3020,0,0.226569,"w much each training instance contributes to the predictions. Through empirical analysis on named entity recognition, we demonstrate that our method enables to build models that have high interpretability without sacrificing performance. 1 Introduction Neural networks have contributed to performance improvements in structured prediction. Instead, the rationales underlying the model predictions are difficult for humans to understand (Lei et al., 2016). In practical applications, interpretable rationales play a critical role for driving human’s decisions and promoting human-machine cooperation (Ribeiro et al., 2016). With this motivation, we aim to build models that have high interpretability without sacrificing performance. As an approach to this challenge, we focus on instance-based learning. Instance-based learning (Aha et al., 1991) is a machine learning method that learns similarities between instances. At inference time, the class labels of the most similar training instances are assigned to the new instances. This transparent inference process provides an answer to the following question: Which points in the training set most closely resemble a test point or influenced the prediction? This is cate"
2020.acl-main.575,D18-1309,0,0.0276638,"Missing"
2020.acl-main.575,P17-1076,0,0.027973,"al., 2010). Recently, despite its preferable property, it has received little attention and been underexplored. This study presents and investigates an instancebased learning method for span representations. A span is a unit that consists of one or more linguistically linked words. Why do we focus on spans instead of tokens? One reason is relevant to performance. Recent neural networks can induce good span feature representations and achieve high performance in structured prediction tasks, such as named entity recognition (NER) (Sohrab and Miwa, 2018; Xia et al., 2019), constituency parsing (Stern et al., 2017; Kitaev et al., 2019), semantic role labeling (SRL) (He et al., 2018; Ouchi et al., 2018) and coreference resolution (Lee et al., 2017). Another reason is relevant to interpretability. The tasks above require recognition of linguistic structure that consists of spans. Thus, directly classifying each span based on its representation is more interpretable than token-wise classification such as BIO tagging, which reconstructs each span label from the predicted token-wise BIO tags. Our method builds a feature space where spans with the same class label are close to each other. At inference time,"
2020.acl-main.575,W02-2025,0,0.0739357,"ook into classifier-based neural models (Ribeiro et al., 2016; Lundberg and Lee, 2017; Koh and Liang, 2017). In this paper, instead of looking into the black-box, we build interpretable models based on instance-based learning. Before the current neural era, instance-based learning, sometimes called memory-based learning (Daelemans and Van den Bosch, 2005), was widely used for various NLP tasks, such as part-of-speech tagging (Daelemans et al., 1996), dependency parsing (Nivre et al., 2004) and machine translation (Nagao, 1984). For NER, some instance-based models have been proposed (Tjong Kim Sang, 2002; De Meulder and Daelemans, 2003; Hendrickx and van den Bosch, 2003). Recently, despite its high interpretability, this direction has not been explored. One exception is Wiseman and Stratos (2019), which used instance-based learning of token representations. Due to BIO tagging, it faces one technical challenge: inconsistent label prediction. For example, an entity candidate “World Health Organization” can be assigned inconsistent labels such as “B-LOC I-ORG I-ORG,” whereas the groundtruth labels are “B-ORG I-ORG I-ORG.” To remedy this issue, they presented a heuristic technique for encouraging"
2020.acl-main.575,W00-0726,0,0.146652,"Missing"
2020.acl-main.575,W96-0102,0,\N,Missing
2020.acl-main.575,D09-1015,0,\N,Missing
2020.acl-main.575,W03-0435,0,\N,Missing
2020.acl-main.575,W03-0427,0,\N,Missing
2020.acl-main.575,W03-0419,0,\N,Missing
2020.acl-main.575,D15-1102,0,\N,Missing
2020.acl-main.575,N16-1030,0,\N,Missing
2020.acl-main.575,D17-1276,0,\N,Missing
2020.acl-main.575,N18-1079,0,\N,Missing
2020.acl-main.575,N18-1131,0,\N,Missing
2020.acl-main.575,P19-1138,0,\N,Missing
2020.acl-main.575,P19-1524,0,\N,Missing
2020.acl-main.575,N19-1423,0,\N,Missing
2020.acl-main.575,Q16-1026,0,\N,Missing
2020.acl-main.575,P19-1340,0,\N,Missing
2020.acl-srw.30,Q16-1026,0,0.252432,". Introduction Sequence labeling is a problem in which a label is assigned to each word in an input sentence. In many label sets, each label consists of different types of elements. For example, IOB-format entity labels (Ramshaw and Marcus, 1995), such as B-Person and I-Location, can be decomposed into span (e.g., B, I and O) and type information (e.g., Person and Location). Also, morphological feature tags (More et al., 2018), such as Gender=Masc|Number=Sing, can be decomposed into gender, number and other information. General sequence labeling models (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016), however, do not consider such components. Specifically, the probability that each word is assigned a label is computed on the basis of the inner product between word representation and label embedding (see Equation 2 in Section 2.1). Here, the label embedding is associated with each label and independently trained without considering its components. This means that labels are treated as mutually exclusive. In fact, labels often share some To investigate the effectiveness of our method, we take the task of fine-grained Named Entity Recognition (NER) as a case study. Typically, in this task, a"
2020.acl-srw.30,N19-1423,0,0.0150743,"a gap between the frequencies, i.e., how many times each label appears in the training set. We categorize each label into three classes on the basis of its frequency, shown in Table 2. For example, if a label appears 0–100 times in the training set, it is categorized into the “Low” class. Moreover, we denote how many times entities with the labels belonging to each frequency class appear in the development or test set. To better understand the model behavior, we investigate the performance of each frequency class. Model setup As the encoder f (x, X) in Equation 2 in Section 2.1, we use BERT5 (Devlin et al., 2019), which is a state-of-the-art language model.6 As the baseline model, we use the general label embedding matrix without considering label components, i.e., each label embedding W[y] in Equation 2 is randomly initialized and independently learned. In contrast, our proposed model calculates the label embedding matrix from label components (Equations 3 and 4). The only difference between these models is the label embedding matrix, so if a performance gap between them is observed, it stems from this point. Experiments Settings 224 Hyperparameters The overall settings of hyperparameters are the sam"
2020.acl-srw.30,W95-0107,0,0.538029,"s of label components (see details in Section 2.2). Specifically, we first decompose each label into its components. We then assign an embedding to each component and summarize the embeddings of all the components into one as a label embedding used in a model. This component-level operation enables the model to share information on the common components across label embeddings. Introduction Sequence labeling is a problem in which a label is assigned to each word in an input sentence. In many label sets, each label consists of different types of elements. For example, IOB-format entity labels (Ramshaw and Marcus, 1995), such as B-Person and I-Location, can be decomposed into span (e.g., B, I and O) and type information (e.g., Person and Location). Also, morphological feature tags (More et al., 2018), such as Gender=Masc|Number=Sing, can be decomposed into gender, number and other information. General sequence labeling models (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016), however, do not consider such components. Specifically, the probability that each word is assigned a label is computed on the basis of the inner product between word representation and label embedding (see Equation 2 in S"
2020.acl-srw.30,D17-1206,0,0.0224735,"them in detail, we obtained two findings. First, in the embedding space learned by the proposed model, we found that two distinct clusters were formed corresponding to the two span labels (i.e. B and I). Second, the labels that have the same top layer label (represented in the same color) also formed some smaller clusters within the B and I-label clusters. For example, Figure 3c shows the Product cluster whose members are the labels sharing the top layer label Product. Related work Sequence labeling has been widely studied and applied to many tasks, such as Chunking (Ramshaw and Marcus, 1995; Hashimoto et al., 2017), NER (Ma and Hovy, 2016; Chiu and Nichols, 2016) and Semantic Role Labeling (SRL) (Zhou and Xu, 2015; He et al., 2017). In English fine-grained entity recognition, Ling and Weld (2012) created a standard fine-grained entity typing dataset with multi-class, multi-label annotations. Ringland et al. (2019) developed a dataset for nested NER dataset. These datasets independently handle each label without considering label components. In Japanese NER, Misawa et al. (2017) combined word and character information to improve performance. Mai et al. (2018) reported that dictionary information improves"
2020.acl-srw.30,P19-1510,0,0.0170803,"smaller clusters within the B and I-label clusters. For example, Figure 3c shows the Product cluster whose members are the labels sharing the top layer label Product. Related work Sequence labeling has been widely studied and applied to many tasks, such as Chunking (Ramshaw and Marcus, 1995; Hashimoto et al., 2017), NER (Ma and Hovy, 2016; Chiu and Nichols, 2016) and Semantic Role Labeling (SRL) (Zhou and Xu, 2015; He et al., 2017). In English fine-grained entity recognition, Ling and Weld (2012) created a standard fine-grained entity typing dataset with multi-class, multi-label annotations. Ringland et al. (2019) developed a dataset for nested NER dataset. These datasets independently handle each label without considering label components. In Japanese NER, Misawa et al. (2017) combined word and character information to improve performance. Mai et al. (2018) reported that dictionary information improves the performance of finegrained NER. Their methods do not consider label components and are orthogonal to our method. Some existing studies take shared components (or information) across labels into account. In Entity Typing, Ma et al. (2016) and Shimaoka et al. (2017) proposed to calculate entity label"
2020.acl-srw.30,P17-1044,0,0.0162298,"nct clusters were formed corresponding to the two span labels (i.e. B and I). Second, the labels that have the same top layer label (represented in the same color) also formed some smaller clusters within the B and I-label clusters. For example, Figure 3c shows the Product cluster whose members are the labels sharing the top layer label Product. Related work Sequence labeling has been widely studied and applied to many tasks, such as Chunking (Ramshaw and Marcus, 1995; Hashimoto et al., 2017), NER (Ma and Hovy, 2016; Chiu and Nichols, 2016) and Semantic Role Labeling (SRL) (Zhou and Xu, 2015; He et al., 2017). In English fine-grained entity recognition, Ling and Weld (2012) created a standard fine-grained entity typing dataset with multi-class, multi-label annotations. Ringland et al. (2019) developed a dataset for nested NER dataset. These datasets independently handle each label without considering label components. In Japanese NER, Misawa et al. (2017) combined word and character information to improve performance. Mai et al. (2018) reported that dictionary information improves the performance of finegrained NER. Their methods do not consider label components and are orthogonal to our method. S"
2020.acl-srw.30,sekine-etal-2002-extended,0,0.721438,"y-type labels are predefined in a hierarchical structure, and intermediate type labels can be used as label components, as well as leaf type labels and B/I-labels. In this sense, the fine-grained NER can be seen as a good example of the potential applications of the proposed method. Furthermore, some entity labels occur more frequently than others. An interesting question is whether our method of label component sharing exhibits an improvement in recognizing entities of infrequent labels. In our experiments, we use the English and Japanese NER corpora with the Extended Named Entity Hierarchy (Sekine et al., 2002) including 200 entity tags. To sum up, our main contributions are as follows: (i) we propose a method that shares and learns label component embeddings, and (ii) through experiments on English and Japanese fine-grained NER, we demonstrate that the proposed method achieves better performance than a standard sequence labeling model, especially for instances with low-frequency labels. 222 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 222–229 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics label em"
2020.acl-srw.30,N16-1030,0,0.0112779,"ross label embeddings. Introduction Sequence labeling is a problem in which a label is assigned to each word in an input sentence. In many label sets, each label consists of different types of elements. For example, IOB-format entity labels (Ramshaw and Marcus, 1995), such as B-Person and I-Location, can be decomposed into span (e.g., B, I and O) and type information (e.g., Person and Location). Also, morphological feature tags (More et al., 2018), such as Gender=Masc|Number=Sing, can be decomposed into gender, number and other information. General sequence labeling models (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016), however, do not consider such components. Specifically, the probability that each word is assigned a label is computed on the basis of the inner product between word representation and label embedding (see Equation 2 in Section 2.1). Here, the label embedding is associated with each label and independently trained without considering its components. This means that labels are treated as mutually exclusive. In fact, labels often share some To investigate the effectiveness of our method, we take the task of fine-grained Named Entity Recognition (NER) as a case study. T"
2020.acl-srw.30,E17-1119,1,0.927102,"mbedding. Compared with the summation, one disadvantage of the concatenation is memory efficiency: the number of dimensions of the label embeddings increases according to the number of label components K. Our label embedding calculation enables models to share the embeddings of label components commonly shared across labels. For example, the embeddings of both B-Facility/GOE/Park and B-Facility/GOE/School are calculated by adding the embeddings of the shared components (i.e., B, Facility and GOE). Equations 3 and 4 can be regarded as a general form of the hierarchical label matrix proposed by Shimaoka et al. (2017) because our method can treat not only hierarchical structures but also any type of type–value set, such as morphological feature labels (e.g. Gender=Masc|Number=Sing). 3 3.1 Dataset We use the Extended Named Entity Corpus for English2 and Japanese.3 fine-grained NER (Mai et al., 2018) In this dataset, each NE is assigned one of 200 entity labels defined in the Extended Named Entity Hierarchy (Sekine et al., 2002). For the English dataset, we follow the training/development/test split defined by Mai et al. (2018). For the Japanese dataset, we follow the training/development/test split of Unive"
2020.acl-srw.30,P16-1101,0,0.241916,"ommon components across label embeddings. Introduction Sequence labeling is a problem in which a label is assigned to each word in an input sentence. In many label sets, each label consists of different types of elements. For example, IOB-format entity labels (Ramshaw and Marcus, 1995), such as B-Person and I-Location, can be decomposed into span (e.g., B, I and O) and type information (e.g., Person and Location). Also, morphological feature tags (More et al., 2018), such as Gender=Masc|Number=Sing, can be decomposed into gender, number and other information. General sequence labeling models (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016), however, do not consider such components. Specifically, the probability that each word is assigned a label is computed on the basis of the inner product between word representation and label embedding (see Equation 2 in Section 2.1). Here, the label embedding is associated with each label and independently trained without considering its components. This means that labels are treated as mutually exclusive. In fact, labels often share some To investigate the effectiveness of our method, we take the task of fine-grained Named Entity Recognition (NE"
2020.acl-srw.30,C16-1017,0,0.0197358,"yping dataset with multi-class, multi-label annotations. Ringland et al. (2019) developed a dataset for nested NER dataset. These datasets independently handle each label without considering label components. In Japanese NER, Misawa et al. (2017) combined word and character information to improve performance. Mai et al. (2018) reported that dictionary information improves the performance of finegrained NER. Their methods do not consider label components and are orthogonal to our method. Some existing studies take shared components (or information) across labels into account. In Entity Typing, Ma et al. (2016) and Shimaoka et al. (2017) proposed to calculate entity label embeddings by considering a label hierarchical structure. While their method is limited to only a hierarchical structure, our method can be applied to any set of components and can be regarded as a general form of their method. In multi-label classification, Zhong et al. (2018) assumed that the labels cooccurring in many instances are correlated with each other and share some common features, and proposed a method that learns a feature (label em226 I-labels B-labels (a) BASELINE (b) P ROPOSED :S UM (c) Enlarged view of a cluster in"
2020.acl-srw.30,P09-1003,0,0.0452038,"ication, Zhong et al. (2018) assumed that the labels cooccurring in many instances are correlated with each other and share some common features, and proposed a method that learns a feature (label em226 I-labels B-labels (a) BASELINE (b) P ROPOSED :S UM (c) Enlarged view of a cluster in (b). The embeddings of the labels sharing the top layer label Product form this cluster. Figure 3: Visualization of the label embedding space. The same color represents the labels that have the same hierarchical top layer label. bedding) space where such co-occurring labels are close to each other. The work of Matsubayashi et al. (2009) is the closest to ours in terms of decomposing the features of labels. They regard an original label comprising a mixture of components as a set of multiple labels and made models that are able to exploit the multiple components to effectively learn in the SRL task. 5 Conclusion We proposed a method that shares and learns the embeddings of label components. Through experiments on English and Japanese fine-grained NER, we demonstrated that our proposed method improves the performance, especially for instances with low-frequency labels. For future work, we envision to apply our method to other"
2020.acl-srw.30,W17-4114,0,0.0139155,"elated work Sequence labeling has been widely studied and applied to many tasks, such as Chunking (Ramshaw and Marcus, 1995; Hashimoto et al., 2017), NER (Ma and Hovy, 2016; Chiu and Nichols, 2016) and Semantic Role Labeling (SRL) (Zhou and Xu, 2015; He et al., 2017). In English fine-grained entity recognition, Ling and Weld (2012) created a standard fine-grained entity typing dataset with multi-class, multi-label annotations. Ringland et al. (2019) developed a dataset for nested NER dataset. These datasets independently handle each label without considering label components. In Japanese NER, Misawa et al. (2017) combined word and character information to improve performance. Mai et al. (2018) reported that dictionary information improves the performance of finegrained NER. Their methods do not consider label components and are orthogonal to our method. Some existing studies take shared components (or information) across labels into account. In Entity Typing, Ma et al. (2016) and Shimaoka et al. (2017) proposed to calculate entity label embeddings by considering a label hierarchical structure. While their method is limited to only a hierarchical structure, our method can be applied to any set of compo"
2020.acl-srw.32,Q13-1032,0,0.0344833,"of the corresponding rubric. In the experiments, we use the sum of these analytic scores as a ground truth score of each response.2 Table 1 shows the statistics of the dataset. In the dataset, the randomly sampled 100 responses per prompt are annotated by two human raters. Therefore, we can calculate QWKs and their Kappa values (Cohen, 1960) between the two human raters to confirm the degree of human agreement. The Kappa values on this dataset are comparable to or higher than those on other datasets for the SAS task (Leacock and Chodorow, 2003; Mohler and Mihalcea, 2009; Mohler et al., 2011; Basu et al., 2013). As additional statistics, we calculated the number of CSEs and CSRate in various settings of λ in Settings We split the dataset into training data (1, 600), validation data (200), and test data (200). We used pretrained BERT (Devlin et al., 2019) as the embedding layer of the model.3 We adopted the same optimization algorithm, learning rate, batch size, and output dimension of the recurrent layer as in Taghipour and Ng (2016). We trained the SAS models for 50 epochs and selected the parameters in the epoch in which the best QWK was achieved for the development set. We trained five models wit"
2020.acl-srw.32,N19-1423,0,0.0186021,"Missing"
2020.acl-srw.32,W19-4433,1,0.929237,"given prompt on the basis of whether the answer satisfies the rubrics prepared by a human in advance. SAS systems have mainly been developed to markedly reduce the scoring cost of human raters. Moreover, the SAS systems play a central role in providing stable and sustainable scoring in a repeated and large-scale examination and (online) self-study learning support system (Attali and Burstein, 2006; Shermis et al., 2010; Leacock and Chodorow, 2003; Burrows et al., 2015). The development of the SAS systems has a long history (Page, 1994; Foltz et al., 1999). Many recent previous studies, e.g., (Mizumoto et al., 2019; Taghipour and Ng, 2016; Riordan et al., 2017; Wang et al., 2019), utilize Quadratic Weighted Kappa (QWK) (Cohen, 1968) as a measure for the achievement and for the comparison of the performances of the SAS systems. QWK is indeed useful for measuring and comparing the overall performance of each system and the daily developments of their scoring models. In our experiments, however, we reveal that the SAS systems with high QWK potentially incur serious scoring errors (see experiments in Section 5.3). Such serious scoring errors are rarely incurred by trained human raters, therefore, we need to"
2020.acl-srw.32,P11-1076,0,0.0265483,"vidually on the basis of the corresponding rubric. In the experiments, we use the sum of these analytic scores as a ground truth score of each response.2 Table 1 shows the statistics of the dataset. In the dataset, the randomly sampled 100 responses per prompt are annotated by two human raters. Therefore, we can calculate QWKs and their Kappa values (Cohen, 1960) between the two human raters to confirm the degree of human agreement. The Kappa values on this dataset are comparable to or higher than those on other datasets for the SAS task (Leacock and Chodorow, 2003; Mohler and Mihalcea, 2009; Mohler et al., 2011; Basu et al., 2013). As additional statistics, we calculated the number of CSEs and CSRate in various settings of λ in Settings We split the dataset into training data (1, 600), validation data (200), and test data (200). We used pretrained BERT (Devlin et al., 2019) as the embedding layer of the model.3 We adopted the same optimization algorithm, learning rate, batch size, and output dimension of the recurrent layer as in Taghipour and Ng (2016). We trained the SAS models for 50 epochs and selected the parameters in the epoch in which the best QWK was achieved for the development set. We tra"
2020.acl-srw.32,E09-1065,0,0.0201369,"ch criterion was rated individually on the basis of the corresponding rubric. In the experiments, we use the sum of these analytic scores as a ground truth score of each response.2 Table 1 shows the statistics of the dataset. In the dataset, the randomly sampled 100 responses per prompt are annotated by two human raters. Therefore, we can calculate QWKs and their Kappa values (Cohen, 1960) between the two human raters to confirm the degree of human agreement. The Kappa values on this dataset are comparable to or higher than those on other datasets for the SAS task (Leacock and Chodorow, 2003; Mohler and Mihalcea, 2009; Mohler et al., 2011; Basu et al., 2013). As additional statistics, we calculated the number of CSEs and CSRate in various settings of λ in Settings We split the dataset into training data (1, 600), validation data (200), and test data (200). We used pretrained BERT (Devlin et al., 2019) as the embedding layer of the model.3 We adopted the same optimization algorithm, learning rate, batch size, and output dimension of the recurrent layer as in Taghipour and Ng (2016). We trained the SAS models for 50 epochs and selected the parameters in the epoch in which the best QWK was achieved for the de"
2020.acl-srw.32,D15-1182,0,0.0207878,"Missing"
2020.acl-srw.32,W17-5017,0,0.0153614,"satisfies the rubrics prepared by a human in advance. SAS systems have mainly been developed to markedly reduce the scoring cost of human raters. Moreover, the SAS systems play a central role in providing stable and sustainable scoring in a repeated and large-scale examination and (online) self-study learning support system (Attali and Burstein, 2006; Shermis et al., 2010; Leacock and Chodorow, 2003; Burrows et al., 2015). The development of the SAS systems has a long history (Page, 1994; Foltz et al., 1999). Many recent previous studies, e.g., (Mizumoto et al., 2019; Taghipour and Ng, 2016; Riordan et al., 2017; Wang et al., 2019), utilize Quadratic Weighted Kappa (QWK) (Cohen, 1968) as a measure for the achievement and for the comparison of the performances of the SAS systems. QWK is indeed useful for measuring and comparing the overall performance of each system and the daily developments of their scoring models. In our experiments, however, we reveal that the SAS systems with high QWK potentially incur serious scoring errors (see experiments in Section 5.3). Such serious scoring errors are rarely incurred by trained human raters, therefore, we need to avoid containing this type of errors to ensur"
2020.acl-srw.32,D16-1193,0,0.0184529,"is of whether the answer satisfies the rubrics prepared by a human in advance. SAS systems have mainly been developed to markedly reduce the scoring cost of human raters. Moreover, the SAS systems play a central role in providing stable and sustainable scoring in a repeated and large-scale examination and (online) self-study learning support system (Attali and Burstein, 2006; Shermis et al., 2010; Leacock and Chodorow, 2003; Burrows et al., 2015). The development of the SAS systems has a long history (Page, 1994; Foltz et al., 1999). Many recent previous studies, e.g., (Mizumoto et al., 2019; Taghipour and Ng, 2016; Riordan et al., 2017; Wang et al., 2019), utilize Quadratic Weighted Kappa (QWK) (Cohen, 1968) as a measure for the achievement and for the comparison of the performances of the SAS systems. QWK is indeed useful for measuring and comparing the overall performance of each system and the daily developments of their scoring models. In our experiments, however, we reveal that the SAS systems with high QWK potentially incur serious scoring errors (see experiments in Section 5.3). Such serious scoring errors are rarely incurred by trained human raters, therefore, we need to avoid containing this t"
2020.acl-srw.32,D19-6119,1,0.835486,"prepared by a human in advance. SAS systems have mainly been developed to markedly reduce the scoring cost of human raters. Moreover, the SAS systems play a central role in providing stable and sustainable scoring in a repeated and large-scale examination and (online) self-study learning support system (Attali and Burstein, 2006; Shermis et al., 2010; Leacock and Chodorow, 2003; Burrows et al., 2015). The development of the SAS systems has a long history (Page, 1994; Foltz et al., 1999). Many recent previous studies, e.g., (Mizumoto et al., 2019; Taghipour and Ng, 2016; Riordan et al., 2017; Wang et al., 2019), utilize Quadratic Weighted Kappa (QWK) (Cohen, 1968) as a measure for the achievement and for the comparison of the performances of the SAS systems. QWK is indeed useful for measuring and comparing the overall performance of each system and the daily developments of their scoring models. In our experiments, however, we reveal that the SAS systems with high QWK potentially incur serious scoring errors (see experiments in Section 5.3). Such serious scoring errors are rarely incurred by trained human raters, therefore, we need to avoid containing this type of errors to ensure the sufficient sco"
2020.coling-main.521,N19-1311,0,0.0177356,"en if the model ranked two references correctly. Similar but different way of contrastive evaluation is performed on a clean input and its noisy counterpart. Heigold et al. (2018) introduced rule-based character replacement noise to imitate misspellings found in a variety of real-world applications. Following work by Karpukhin et al. (2019) and Belinkov and Bisk (2018) extended its scope to natural noise by using edit histories from online websites. However, instead of giving translations to raw noisy sentences, they relied on a noisy version of input artificially created from the clean text. Anastasopoulos et al. (2019) is similar to our work in that they explored the effect of errors naturally created by humans. They focused on the effect of grammatical errors against NMT by adding translations to the JFLEG corpus (Napoles et al., 2017), one of the common benchmarks for grammatical error correction. Their results demonstrated that even a very small perturbation could significantly drop the performance of MT systems while exposure to similar noise during training time alleviates the problem. However, these aspects are only a small subset of possible reasons to explain why current models are still not good at"
2020.coling-main.521,N18-1118,0,0.0712839,"airs generated during the process of creating corpora, not at handling noisy input. The way of current evaluation definitely prevents us from developing truly robust systems, and motivated us to create a new dataset for focused evaluation. A range of studies have aimed to elucidate the cause of mistranslations from the viewpoint of linguistic phenomena, such as typographical errors (Heigold et al., 2018; Belinkov and Bisk, 2018; Karpukhin et al., 2019; Niu et al., 2020), grammaticality (Sennrich, 2017), presence of named entities (Ugawa et al., 2018), and identification of anaphoric pronouns (Bawden et al., 2018; M¨uller et al., 2018). One of the pioneering works to analyze the behavior of NMT is the challenge set approach proposed by Isabelle et al. (2017). They defined various subcategories of structural differences between English and French to evaluate how well models can handle them in detail. Though the approach has the potential of 1 2 https://www.reddit.com http://www.statmt.org/wmt19/robustness.html 5930 Step1: Annotating phenomena labels Ja En ��� Ja Phe.1 En Phe.2 Step2: Extracting targeted expressions MTNT corpus Ja: ������ (apude, update) �� En: That’s a plain update though Orig. Norm. E"
2020.coling-main.521,D19-5617,0,0.0200286,"re greatly disturbed by the presence of certain phenomena. 1 Introduction The advancement of Neural Machine Translation (NMT) has brought great improvement in translation quality when translating clean input, such as text from the news domain (Luong et al., 2015; Vaswani et al., 2017), and it was recently claimed that NMT has even achieved human parity in certain language pairs (Hassan et al., 2018; Barrault et al., 2019). Despite its remarkable advancements, the applicability of NMT over User-Generated Contents (UGC), such as social media text, still remains limited (Michel and Neubig, 2018; Berard et al., 2019a). Since UGC are prevailing in our real-life communication, it is undoubtedly one of the challenges we need to overcome to make MT systems invaluable for promoting cross-cultural communication. Recently, with the increasing interest in handling UGC, a shared task was organized to measure how well MT systems adapt to those texts (Li et al., 2019). However, the way in which they evaluate systems is just giving an overall score to a dataset, which is the same as traditional MT evaluation (Figure 1a). The overall score does not provide precise information for understanding what leads to the huge"
2020.coling-main.521,W19-5361,0,0.0167496,"re greatly disturbed by the presence of certain phenomena. 1 Introduction The advancement of Neural Machine Translation (NMT) has brought great improvement in translation quality when translating clean input, such as text from the news domain (Luong et al., 2015; Vaswani et al., 2017), and it was recently claimed that NMT has even achieved human parity in certain language pairs (Hassan et al., 2018; Barrault et al., 2019). Despite its remarkable advancements, the applicability of NMT over User-Generated Contents (UGC), such as social media text, still remains limited (Michel and Neubig, 2018; Berard et al., 2019a). Since UGC are prevailing in our real-life communication, it is undoubtedly one of the challenges we need to overcome to make MT systems invaluable for promoting cross-cultural communication. Recently, with the increasing interest in handling UGC, a shared task was organized to measure how well MT systems adapt to those texts (Li et al., 2019). However, the way in which they evaluate systems is just giving an overall score to a dataset, which is the same as traditional MT evaluation (Figure 1a). The overall score does not provide precise information for understanding what leads to the huge"
2020.coling-main.521,N19-1154,0,0.0145464,"ita et al., 2020), one of the largest parallel corpora available in Japanese-English. The larger model (L ARGE), is only different in the size of training data from the S MALL. We applied Byte-Pair-Encoding (BPE) models (Sennrich et al., 2016) with a joint vocabulary of 32,000 for these models using the sentencepiece toolkit (Kudo and Richardson, 2018). The character-based model (C HAR) is different from the two models in the way of segmentation. The model translates a sequence of characters in the source language into another sequence of characters in the target language (Wang et al., 2015). Durrani et al. (2019) pointed out that character-based models are more robust to noisy text than BPE-based models. We revisit the issue of segmentation to see if the model is also good at handling UGC. We shared the vocabulary between two languages in this setting as well to expect the model to capture copying behavior. For the pronunciation-based model (P RON), we applied a unique preprocessing method to the source (Japanese) sentence. More specifically, we first applied the MeCab toolkit (Kudo et al., 2004), a Japanese morphological analyzer, with naist-jdic for the dictionary to obtain the pronunciation of each"
2020.coling-main.521,W18-1807,0,0.0972352,"Missing"
2020.coling-main.521,W16-3918,0,0.0390284,"Missing"
2020.coling-main.521,D17-1263,0,0.0143517,"eloping truly robust systems, and motivated us to create a new dataset for focused evaluation. A range of studies have aimed to elucidate the cause of mistranslations from the viewpoint of linguistic phenomena, such as typographical errors (Heigold et al., 2018; Belinkov and Bisk, 2018; Karpukhin et al., 2019; Niu et al., 2020), grammaticality (Sennrich, 2017), presence of named entities (Ugawa et al., 2018), and identification of anaphoric pronouns (Bawden et al., 2018; M¨uller et al., 2018). One of the pioneering works to analyze the behavior of NMT is the challenge set approach proposed by Isabelle et al. (2017). They defined various subcategories of structural differences between English and French to evaluate how well models can handle them in detail. Though the approach has the potential of 1 2 https://www.reddit.com http://www.statmt.org/wmt19/robustness.html 5930 Step1: Annotating phenomena labels Ja En ��� Ja Phe.1 En Phe.2 Step2: Extracting targeted expressions MTNT corpus Ja: ������ (apude, update) �� En: That’s a plain update though Orig. Norm. En Phenomenon2 Orig. Step3: Normalizing the expressions Orig.: ������ (apude, update) �� Norm.: ��������� (update) �� Norm. En ��� Prefiltering with"
2020.coling-main.521,W18-6478,0,0.0185446,"we actually need to develop any UGC-specific techniques or not, we do not even know with such a many-sided dataset that how much the improvement in some metrics, such as BLEU score (Papineni et al., 2002), actually contributes to improve robustness on various noise. In fact, Berald et al. (2019b), the winning team in the shared task, reported that none of the techniques specifically designed for UGC was more effective in improving BLEU score than corpus filtering. Though there is no doubt that corpus filtering is one of the essential techniques for data-driven MT systems (Koehn et al., 2018; Junczys-Dowmunt, 2018), this is rather aimed at removing inappropriate sentence pairs generated during the process of creating corpora, not at handling noisy input. The way of current evaluation definitely prevents us from developing truly robust systems, and motivated us to create a new dataset for focused evaluation. A range of studies have aimed to elucidate the cause of mistranslations from the viewpoint of linguistic phenomena, such as typographical errors (Heigold et al., 2018; Belinkov and Bisk, 2018; Karpukhin et al., 2019; Niu et al., 2020), grammaticality (Sennrich, 2017), presence of named entities (Ugaw"
2020.coling-main.521,D19-5506,0,0.124651,"ng is one of the essential techniques for data-driven MT systems (Koehn et al., 2018; Junczys-Dowmunt, 2018), this is rather aimed at removing inappropriate sentence pairs generated during the process of creating corpora, not at handling noisy input. The way of current evaluation definitely prevents us from developing truly robust systems, and motivated us to create a new dataset for focused evaluation. A range of studies have aimed to elucidate the cause of mistranslations from the viewpoint of linguistic phenomena, such as typographical errors (Heigold et al., 2018; Belinkov and Bisk, 2018; Karpukhin et al., 2019; Niu et al., 2020), grammaticality (Sennrich, 2017), presence of named entities (Ugawa et al., 2018), and identification of anaphoric pronouns (Bawden et al., 2018; M¨uller et al., 2018). One of the pioneering works to analyze the behavior of NMT is the challenge set approach proposed by Isabelle et al. (2017). They defined various subcategories of structural differences between English and French to evaluate how well models can handle them in detail. Though the approach has the potential of 1 2 https://www.reddit.com http://www.statmt.org/wmt19/robustness.html 5930 Step1: Annotating phenomen"
2020.coling-main.521,D18-2012,0,0.0146356,". In addition, we replaced possible usernames with regular expressions. We offered this model to see whether or not the phenomena would become less problematic with increasing amount of training data. For the other four models, we additionally used JParacrawl v2.0 (Morishita et al., 2020), one of the largest parallel corpora available in Japanese-English. The larger model (L ARGE), is only different in the size of training data from the S MALL. We applied Byte-Pair-Encoding (BPE) models (Sennrich et al., 2016) with a joint vocabulary of 32,000 for these models using the sentencepiece toolkit (Kudo and Richardson, 2018). The character-based model (C HAR) is different from the two models in the way of segmentation. The model translates a sequence of characters in the source language into another sequence of characters in the target language (Wang et al., 2015). Durrani et al. (2019) pointed out that character-based models are more robust to noisy text than BPE-based models. We revisit the issue of segmentation to see if the model is also good at handling UGC. We shared the vocabulary between two languages in this setting as well to expect the model to capture copying behavior. For the pronunciation-based mode"
2020.coling-main.521,W04-3230,0,0.0299887,"in the source language into another sequence of characters in the target language (Wang et al., 2015). Durrani et al. (2019) pointed out that character-based models are more robust to noisy text than BPE-based models. We revisit the issue of segmentation to see if the model is also good at handling UGC. We shared the vocabulary between two languages in this setting as well to expect the model to capture copying behavior. For the pronunciation-based model (P RON), we applied a unique preprocessing method to the source (Japanese) sentence. More specifically, we first applied the MeCab toolkit (Kudo et al., 2004), a Japanese morphological analyzer, with naist-jdic for the dictionary to obtain the pronunciation of each morpheme in the sentences. We can transliterate any words in Japanese by using phonetic symbols such as hiragana and katakana characters. Since the MeCab toolkit outputs the pronunciation in katakana characters by default, we simply concatenated them to create a fully pronunciation-based corpus. We prepared this model with the expectation to improve the robustness against Variant expressions. More specifically, we aimed at absorbing the orthographic variations caused by hiragana-katakana"
2020.coling-main.521,W19-5303,0,0.060677,"ity in certain language pairs (Hassan et al., 2018; Barrault et al., 2019). Despite its remarkable advancements, the applicability of NMT over User-Generated Contents (UGC), such as social media text, still remains limited (Michel and Neubig, 2018; Berard et al., 2019a). Since UGC are prevailing in our real-life communication, it is undoubtedly one of the challenges we need to overcome to make MT systems invaluable for promoting cross-cultural communication. Recently, with the increasing interest in handling UGC, a shared task was organized to measure how well MT systems adapt to those texts (Li et al., 2019). However, the way in which they evaluate systems is just giving an overall score to a dataset, which is the same as traditional MT evaluation (Figure 1a). The overall score does not provide precise information for understanding what leads to the huge performance gap between the translation of clean input and that of UGC. To find a clue for improving the performance of MT systems on UGC, we need a solid basis for more detailed error analysis. As a first step towards a more refined evaluation of MT systems on UGC, we present a new dataset, PheMT: Phenomenon-wise Dataset for Machine Translation"
2020.coling-main.521,P19-1291,0,0.0185382,"transliterate any words in Japanese by using phonetic symbols such as hiragana and katakana characters. Since the MeCab toolkit outputs the pronunciation in katakana characters by default, we simply concatenated them to create a fully pronunciation-based corpus. We prepared this model with the expectation to improve the robustness against Variant expressions. More specifically, we aimed at absorbing the orthographic variations caused by hiragana-katakana confusion, which is a part of Variant. Also, previous study suggests that phonetic information is highly useful to resolve homophone noise (Liu et al., 2019). Finally, we prepared the concatenated model (C AT), trained on a joined corpus for the L ARGE and the P RON.5 In this setting, we converted the transliterated part into hiragana characters and applied the same BPE model as used in the L ARGE to the whole corpus. We expect the model to learn robust representations by forcing it to produce the same translation from the original source sentence and its transliterated counterpart. We used transformer-base architecture (Vaswani et al., 2017) implemented in the fairseq toolkit (Ott et al., 2019) and hyperparameters proposed by Murakami et al. (201"
2020.coling-main.521,D15-1166,0,0.0433624,"between the translation of clean input and that of UGC. To answer the question, we present a new dataset, PheMT, for evaluating the robustness of MT systems against specific linguistic phenomena in Japanese-English translation. Our experiments with the created dataset revealed that not only our in-house models but even widely used off-the-shelf systems are greatly disturbed by the presence of certain phenomena. 1 Introduction The advancement of Neural Machine Translation (NMT) has brought great improvement in translation quality when translating clean input, such as text from the news domain (Luong et al., 2015; Vaswani et al., 2017), and it was recently claimed that NMT has even achieved human parity in certain language pairs (Hassan et al., 2018; Barrault et al., 2019). Despite its remarkable advancements, the applicability of NMT over User-Generated Contents (UGC), such as social media text, still remains limited (Michel and Neubig, 2018; Berard et al., 2019a). Since UGC are prevailing in our real-life communication, it is undoubtedly one of the challenges we need to overcome to make MT systems invaluable for promoting cross-cultural communication. Recently, with the increasing interest in handli"
2020.coling-main.521,D18-1050,0,0.402372,"d off-the-shelf systems are greatly disturbed by the presence of certain phenomena. 1 Introduction The advancement of Neural Machine Translation (NMT) has brought great improvement in translation quality when translating clean input, such as text from the news domain (Luong et al., 2015; Vaswani et al., 2017), and it was recently claimed that NMT has even achieved human parity in certain language pairs (Hassan et al., 2018; Barrault et al., 2019). Despite its remarkable advancements, the applicability of NMT over User-Generated Contents (UGC), such as social media text, still remains limited (Michel and Neubig, 2018; Berard et al., 2019a). Since UGC are prevailing in our real-life communication, it is undoubtedly one of the challenges we need to overcome to make MT systems invaluable for promoting cross-cultural communication. Recently, with the increasing interest in handling UGC, a shared task was organized to measure how well MT systems adapt to those texts (Li et al., 2019). However, the way in which they evaluate systems is just giving an overall score to a dataset, which is the same as traditional MT evaluation (Figure 1a). The overall score does not provide precise information for understanding wh"
2020.coling-main.521,2020.lrec-1.443,1,0.710945,"n robustness, namely TED talks, KFTT (Kyoto Free Translation Task), and JESC (Japanese-English Subtitle Corpus). The MTNT dataset was also available in the task, but we didn’t include any of the sentence pairs to train our models. We replaced emojis and emoticons with placeholders following a previous study by Murakami et al. (2019). In addition, we replaced possible usernames with regular expressions. We offered this model to see whether or not the phenomena would become less problematic with increasing amount of training data. For the other four models, we additionally used JParacrawl v2.0 (Morishita et al., 2020), one of the largest parallel corpora available in Japanese-English. The larger model (L ARGE), is only different in the size of training data from the S MALL. We applied Byte-Pair-Encoding (BPE) models (Sennrich et al., 2016) with a joint vocabulary of 32,000 for these models using the sentencepiece toolkit (Kudo and Richardson, 2018). The character-based model (C HAR) is different from the two models in the way of segmentation. The model translates a sequence of characters in the source language into another sequence of characters in the target language (Wang et al., 2015). Durrani et al. (2"
2020.coling-main.521,W18-6307,0,0.0489643,"Missing"
2020.coling-main.521,W19-5365,1,0.910515,"ing methods for our experiments. The smaller model (S MALL) was trained on the data offered in the WMT 2019 shared 4 Note that a sentence could be given more than one label. These sentences are treated differently according to the label which we focus on. 5933 task for machine translation robustness, namely TED talks, KFTT (Kyoto Free Translation Task), and JESC (Japanese-English Subtitle Corpus). The MTNT dataset was also available in the task, but we didn’t include any of the sentence pairs to train our models. We replaced emojis and emoticons with placeholders following a previous study by Murakami et al. (2019). In addition, we replaced possible usernames with regular expressions. We offered this model to see whether or not the phenomena would become less problematic with increasing amount of training data. For the other four models, we additionally used JParacrawl v2.0 (Morishita et al., 2020), one of the largest parallel corpora available in Japanese-English. The larger model (L ARGE), is only different in the size of training data from the S MALL. We applied Byte-Pair-Encoding (BPE) models (Sennrich et al., 2016) with a joint vocabulary of 32,000 for these models using the sentencepiece toolkit ("
2020.coling-main.521,E17-2037,0,0.0264309,"to imitate misspellings found in a variety of real-world applications. Following work by Karpukhin et al. (2019) and Belinkov and Bisk (2018) extended its scope to natural noise by using edit histories from online websites. However, instead of giving translations to raw noisy sentences, they relied on a noisy version of input artificially created from the clean text. Anastasopoulos et al. (2019) is similar to our work in that they explored the effect of errors naturally created by humans. They focused on the effect of grammatical errors against NMT by adding translations to the JFLEG corpus (Napoles et al., 2017), one of the common benchmarks for grammatical error correction. Their results demonstrated that even a very small perturbation could significantly drop the performance of MT systems while exposure to similar noise during training time alleviates the problem. However, these aspects are only a small subset of possible reasons to explain why current models are still not good at handling UGC. To the best of our knowledge, there is no previous work aimed at investigating the effect of UGC-specific challenges in a fine-grained manner. Also, behavioral analysis of NMT in dissimilar language pairs su"
2020.coling-main.521,2020.acl-main.755,0,0.0115546,"al techniques for data-driven MT systems (Koehn et al., 2018; Junczys-Dowmunt, 2018), this is rather aimed at removing inappropriate sentence pairs generated during the process of creating corpora, not at handling noisy input. The way of current evaluation definitely prevents us from developing truly robust systems, and motivated us to create a new dataset for focused evaluation. A range of studies have aimed to elucidate the cause of mistranslations from the viewpoint of linguistic phenomena, such as typographical errors (Heigold et al., 2018; Belinkov and Bisk, 2018; Karpukhin et al., 2019; Niu et al., 2020), grammaticality (Sennrich, 2017), presence of named entities (Ugawa et al., 2018), and identification of anaphoric pronouns (Bawden et al., 2018; M¨uller et al., 2018). One of the pioneering works to analyze the behavior of NMT is the challenge set approach proposed by Isabelle et al. (2017). They defined various subcategories of structural differences between English and French to evaluate how well models can handle them in detail. Though the approach has the potential of 1 2 https://www.reddit.com http://www.statmt.org/wmt19/robustness.html 5930 Step1: Annotating phenomena labels Ja En ���"
2020.coling-main.521,N19-4009,0,0.0139419,"rmation is highly useful to resolve homophone noise (Liu et al., 2019). Finally, we prepared the concatenated model (C AT), trained on a joined corpus for the L ARGE and the P RON.5 In this setting, we converted the transliterated part into hiragana characters and applied the same BPE model as used in the L ARGE to the whole corpus. We expect the model to learn robust representations by forcing it to produce the same translation from the original source sentence and its transliterated counterpart. We used transformer-base architecture (Vaswani et al., 2017) implemented in the fairseq toolkit (Ott et al., 2019) and hyperparameters proposed by Murakami et al. (2019) for all models. The size of the training data was 3.9 M for the S MALL, 14.0 M for the L ARGE, C HAR and P RON, and 28.0 M for the C AT. In addition to the in-house models, we also investigated the impact of the phenomena on two widely used MT systems, namely, Google Translate6 and DeepL Translator. 7,8 These systems are expected to be more robust against UGC because they are by nature exposed to user input. By conducting experiments on such systems, we reveal the presence of phenomena with impending needs for improvement, and also confir"
2020.coling-main.521,P02-1040,0,0.107587,"for MT systems. Their results with the baseline systems demonstrated the difficulty of properly translating UGC. The dataset was also used as in-domain data for the first shared task on machine translation robustness held at WMT 2019.2 However, the dataset is still miscellaneous in the degree of politeness, domain of the conversations, and even in the quality of translations. Though it is still a question whether we actually need to develop any UGC-specific techniques or not, we do not even know with such a many-sided dataset that how much the improvement in some metrics, such as BLEU score (Papineni et al., 2002), actually contributes to improve robustness on various noise. In fact, Berald et al. (2019b), the winning team in the shared task, reported that none of the techniques specifically designed for UGC was more effective in improving BLEU score than corpus filtering. Though there is no doubt that corpus filtering is one of the essential techniques for data-driven MT systems (Koehn et al., 2018; Junczys-Dowmunt, 2018), this is rather aimed at removing inappropriate sentence pairs generated during the process of creating corpora, not at handling noisy input. The way of current evaluation definitely"
2020.coling-main.521,C14-1167,0,0.0262309,"Missing"
2020.coling-main.521,I13-1019,0,0.0293741,"Missing"
2020.coling-main.521,P16-1162,0,0.0320939,"ls. We replaced emojis and emoticons with placeholders following a previous study by Murakami et al. (2019). In addition, we replaced possible usernames with regular expressions. We offered this model to see whether or not the phenomena would become less problematic with increasing amount of training data. For the other four models, we additionally used JParacrawl v2.0 (Morishita et al., 2020), one of the largest parallel corpora available in Japanese-English. The larger model (L ARGE), is only different in the size of training data from the S MALL. We applied Byte-Pair-Encoding (BPE) models (Sennrich et al., 2016) with a joint vocabulary of 32,000 for these models using the sentencepiece toolkit (Kudo and Richardson, 2018). The character-based model (C HAR) is different from the two models in the way of segmentation. The model translates a sequence of characters in the source language into another sequence of characters in the target language (Wang et al., 2015). Durrani et al. (2019) pointed out that character-based models are more robust to noisy text than BPE-based models. We revisit the issue of segmentation to see if the model is also good at handling UGC. We shared the vocabulary between two lang"
2020.coling-main.521,E17-2060,0,0.0220698,"stems (Koehn et al., 2018; Junczys-Dowmunt, 2018), this is rather aimed at removing inappropriate sentence pairs generated during the process of creating corpora, not at handling noisy input. The way of current evaluation definitely prevents us from developing truly robust systems, and motivated us to create a new dataset for focused evaluation. A range of studies have aimed to elucidate the cause of mistranslations from the viewpoint of linguistic phenomena, such as typographical errors (Heigold et al., 2018; Belinkov and Bisk, 2018; Karpukhin et al., 2019; Niu et al., 2020), grammaticality (Sennrich, 2017), presence of named entities (Ugawa et al., 2018), and identification of anaphoric pronouns (Bawden et al., 2018; M¨uller et al., 2018). One of the pioneering works to analyze the behavior of NMT is the challenge set approach proposed by Isabelle et al. (2017). They defined various subcategories of structural differences between English and French to evaluate how well models can handle them in detail. Though the approach has the potential of 1 2 https://www.reddit.com http://www.statmt.org/wmt19/robustness.html 5930 Step1: Annotating phenomena labels Ja En ��� Ja Phe.1 En Phe.2 Step2: Extracti"
2020.coling-main.521,W16-2710,0,0.0247029,"Missing"
2020.coling-main.521,C18-1274,0,0.0158821,"018), this is rather aimed at removing inappropriate sentence pairs generated during the process of creating corpora, not at handling noisy input. The way of current evaluation definitely prevents us from developing truly robust systems, and motivated us to create a new dataset for focused evaluation. A range of studies have aimed to elucidate the cause of mistranslations from the viewpoint of linguistic phenomena, such as typographical errors (Heigold et al., 2018; Belinkov and Bisk, 2018; Karpukhin et al., 2019; Niu et al., 2020), grammaticality (Sennrich, 2017), presence of named entities (Ugawa et al., 2018), and identification of anaphoric pronouns (Bawden et al., 2018; M¨uller et al., 2018). One of the pioneering works to analyze the behavior of NMT is the challenge set approach proposed by Isabelle et al. (2017). They defined various subcategories of structural differences between English and French to evaluate how well models can handle them in detail. Though the approach has the potential of 1 2 https://www.reddit.com http://www.statmt.org/wmt19/robustness.html 5930 Step1: Annotating phenomena labels Ja En ��� Ja Phe.1 En Phe.2 Step2: Extracting targeted expressions MTNT corpus Ja: ������ (a"
2020.coling-main.521,1994.amta-1.25,0,0.460306,"Missing"
2020.emnlp-demos.28,N19-4009,0,0.0146983,"e, in contrast to correcting surface-level errors (Napoles et al., 2017). The diversity of the output revisions is encouraged using diverse beam search (Vijayakumar et al., 2018). In addition, these revisions are ordered by a language model that is fine-tuned for NLP papers. That is, revisions with lower perplexity are listed in the upper part of the suggestion box. Furthermore, the revisions are highlighted in colors, which makes it easier to distinguish the characteristics of each revision. Implementation. We trained a revision model using LightConv (Wu et al., 2019) implemented in Fairseq (Ott et al., 2019). The revision model generates a sentence based on a given input sentence. The model was trained on a slightly modified version of the synthetic training data used in Ito et al. (2019). As an example of these modifications, synthetic edit marks were added for a subset of the training data. These marks were attached to a part 10 The system performs sentence-level revisions. Hence the users are instructed to select the non-sentence-crossing area. 11 We allow the system to correct the parts outside the selected span because sometimes the revision for a specific part requires another adjustment fo"
2020.emnlp-demos.28,2020.acl-main.704,0,0.0263968,"pants who chose the option. Half of the participants first revised a draft with the H UMAN - ONLY setting, and then revised another draft with the H UMAN &M ACHINE setting; the other half performed the same task in the opposite order. Ultimately, we collected two H UMAN &M ACHINE revisions and two H UMAN O NLY revisions for each first draft. Comparison and results. We compared the quality of the three versions of the revised drafts: M ACHINE -O NLY revision, H UMAN -O NLY revision, and H UMAN &M ACHINE revision. We compared the revised drafts with their corresponding final draft using BLEURT (Sellam et al., 2020), the state-of-the-art automatic evaluation metric for natural language generation tasks. Details of the evaluation procedure is shown in Appendix D. Note that the score is not in the range [0, 1], and a higher score means that the revision is closer to the final draft. Table 1 shows that H UMAN &M ACHINE revisions were significantly better20 than M ACHINE ONLY and H UMAN - ONLY revisions. The results suggest the effectiveness of human–machine interaction achieved in Langsmith. Since this experiment was relatively small in scale and only used an automatic evaluation metric, we will conduct a l"
2020.emnlp-demos.28,2020.acl-demos.17,0,0.0430432,"to convey their ideas accurately. Ito et al. (2019) introduced a dataset and models for revising early-stage drafts, and the 1-to-N nature of the revisions was pointed out. We tackled this difficulty by designing an overall demonstration system, including a user interface. 2.2 Writing assistance tools Error checkers. Grammar/spelling checkers are typical writing assistance tools. Some highlight errors (e.g., Write&Improve4 ), while others suggest 3 4 This paper was also written using Langsmith. writeandimprove.com corrections (e.g., Grammarly5 , LanguageTool6 , Ginger7 , and LinggleWrite; see Tsai et al. (2020)) for writers. Langsmith has a revision feature (Ito et al., 2019), as well as a grammar/spelling checker. The revision feature suggests better versions of poor written phrases or sentences in terms of fluency and style, whereas error checkers are typically designed to correct apparent errors only. In addition, Langsmith is specialized for the NLP domain and enables domain-specific revisions, such as correcting technical terms. Text completion. Completing a text is another typical feature in writing assistance applications (WriteAhead8 , Write With Transformer9 , and Smart Compose; see Chen et"
2020.emnlp-demos.28,P10-2021,0,0.0411196,"in green are added to the original sentence, and the red points indicate tracked deletions. model. Furthermore, the communication between the server and the web frontend is achieved via a protocol specialized in writing software called the Text Editing Assistance Smartness Protocol for Natural Language (TEASPN) (Hagiwara et al., 2019). We hope that our system will help the NLP community and researchers, especially those lacking a native command of English.3 2 Related work 2.1 Natural language processing for academic writing Academic writing assistance has gained considerable attention in NLP (Wu et al., 2010; Yimam et al., 2020; Lee and Webster, 2012), and several shared tasks have been organized (Dale and Kilgarriff, 2011; Daudaraviˇcius, 2015). These tasks focus on polishing texts in already published articles or documents near completion. In contrast, this study focuses on revising texts in the earlier stages of writing (e.g., first drafts), where inexperienced, non-native authors might even struggle to convey their ideas accurately. Ito et al. (2019) introduced a dataset and models for revising early-stage drafts, and the 1-to-N nature of the revisions was pointed out. We tackled this difficu"
2020.emnlp-demos.28,2020.lrec-1.722,0,0.0243564,"d to the original sentence, and the red points indicate tracked deletions. model. Furthermore, the communication between the server and the web frontend is achieved via a protocol specialized in writing software called the Text Editing Assistance Smartness Protocol for Natural Language (TEASPN) (Hagiwara et al., 2019). We hope that our system will help the NLP community and researchers, especially those lacking a native command of English.3 2 Related work 2.1 Natural language processing for academic writing Academic writing assistance has gained considerable attention in NLP (Wu et al., 2010; Yimam et al., 2020; Lee and Webster, 2012), and several shared tasks have been organized (Dale and Kilgarriff, 2011; Daudaraviˇcius, 2015). These tasks focus on polishing texts in already published articles or documents near completion. In contrast, this study focuses on revising texts in the earlier stages of writing (e.g., first drafts), where inexperienced, non-native authors might even struggle to convey their ideas accurately. Ito et al. (2019) introduced a dataset and models for revising early-stage drafts, and the 1-to-N nature of the revisions was pointed out. We tackled this difficulty by designing an"
2020.emnlp-main.236,S14-2039,0,0.076462,"Missing"
2020.emnlp-main.236,S15-2027,0,0.0580653,"Missing"
2020.emnlp-main.236,N18-1049,0,0.457536,"antages, EMD-based methods have underperformed sentence-vector-based methods on STS tasks. The goal of this study is to identify and resolve the obstacles faced by EMD-based methods (Section 5). Sentence-vector Approach. Another popular approach is to employ general-purpose sentence vectors of given texts and to compute the cosine similarity between such vectors. A variety of methods to compute sentence vectors have been proposed, ranging from utilizing deep sentence encoders (Kiros et al., 2015; Conneau et al., 2017; Cer et al., 2018), learning and using word vectors optimized for summation (Pagliardini et al., 2018; Wieting and Gimpel, 2018), and estimating latent sentence vectors from pre-trained word vectors (Arora et al., 2017; Ethayarajh, 2018; Liu et al., 2019b). This paper demonstrates that some recently proposed sentence vectors can be reformulated as a sum of the converted word vectors. By utilizing the converted word vectors, our method can achieve similar or better performance compared to sentence-vector approaches (Section 6). 4 4.1 Word Mover’s Distance and its Issues Earth Mover’s Distance Intuitively, earth mover’s distance (EMD)2 (Villani, 2009; Santambrogio, 2015; Peyr´e and Cuturi, 2019"
2020.emnlp-main.68,D18-1431,0,0.129604,"e phrases from the table. As a result, the phrase table included 68,891 phrase pairs, which were used as the key phrase set P as described in Section 5.1. To compute the content relatedness SR , we created a sentence vector from pre-trained fastText word embeddings (Bojanowski et al., 2017; Mikolov et al., 2018) following Arora et al. (2017)’s method, i.e., using SIF weighting and common component removal. Their method is reported to be useful for computing the relatedness of two given sentences and used in many studies (Marelli et al., 2014b,a; Conneau et al., 2017; Subramanian et al., 2018; Baheti et al., 2018). We learned common components using 30K sentences randomly selected from the training costs appropriately. We then removed the first common component for all sentence vectors. Baselines. For comparison, we prepared the following: • Cs´aky et al. (2019): Entropy-based filtering to remove generic utterances from the training data for promoting less-boring response generation. SRC/TRG indicates that using the entropy of source/target utterances. • Junczys-Dowmunt (2018): Filtering for NMT based on the dual conditional cross-entropy computed by a neural encoder-decoder model. It achieved the best"
2020.emnlp-main.68,2020.acl-main.9,0,0.0901188,"Missing"
2020.emnlp-main.68,Q17-1010,0,0.0121349,"rrence frequency (here, less than 200 times) or composed 5 See Appendix B for details on our data such as the preparation procedure and statistics. (a) Cs´aky et al. (2019) TRG (b) Junczys-Dowmunt (2018) (c) Ours SC+R Figure 2: Distributions between human scores and automatically computed scores by each method (English). of the same phrases from the table. As a result, the phrase table included 68,891 phrase pairs, which were used as the key phrase set P as described in Section 5.1. To compute the content relatedness SR , we created a sentence vector from pre-trained fastText word embeddings (Bojanowski et al., 2017; Mikolov et al., 2018) following Arora et al. (2017)’s method, i.e., using SIF weighting and common component removal. Their method is reported to be useful for computing the relatedness of two given sentences and used in many studies (Marelli et al., 2014b,a; Conneau et al., 2017; Subramanian et al., 2018; Baheti et al., 2018). We learned common components using 30K sentences randomly selected from the training costs appropriately. We then removed the first common component for all sentence vectors. Baselines. For comparison, we prepared the following: • Cs´aky et al. (2019): Entropy-based f"
2020.emnlp-main.68,2020.acl-main.218,0,0.0281163,"et utterances. • Junczys-Dowmunt (2018): Filtering for NMT based on the dual conditional cross-entropy computed by a neural encoder-decoder model. It achieved the best performance on the Parallel Corpus Filtering Task at WMT 2018.6 Human evaluation. To validate the ability of the proposed method to estimate the quality of utterance pairs, we measured the correlation between its scores and those assigned by humans through crowdsourcing. We used Amazon Mechanical Turk.7 We randomly extracted 2008 scored 6 http://www.statmt.org/wmt18/ https://www.mturk.com/ 8 Same size as Sedoc et al. (2019) and Cho and May (2020). 7 Scoring method Spearman’s ρ Cs´aky et al. (2019) SRC Cs´aky et al. (2019) TRG Junczys-Dowmunt (2018) Ours SC+R Ours SC (ablation study) Ours SR (ablation study) p-value −0.1173 0.0462 0.2973 0.3751 9.8 × 10−2 5.2 × 10−1 1.9 × 10−5 4.4 × 10−8 0.2044 0.3007 3.7 × 10−3 1.5 × 10−5 Table 2: Correlation coefficient between human scores and automatically computed scores (English). utterance pairs and asked native English-speaking crowdworkers to answer the following question for each pair: Is the sequence of the two utterances acceptable as a dialogue? Workers were instructed to provide an answer"
2020.emnlp-main.68,D17-1070,0,0.0999009,", that is, content relatedness as follows:  SR (x, y) := max cos(v(x), v(y)), 0 . (3) where α, β ∈ R≥0 are hyperparameters that weigh the two viewpoints. For our experiments, we fix α and β as follows: • If a phrase pair (f, e) has a high co-occurrence, the association strength of (x, y) including (f, e) might also be high. • If a phrase f or e occupies almost the entire sentence x or y, (f, e) is a strong indicator of the association of (x, y). 5.2 Summary (2) Cosine similarity between certain kinds of sentence vectors is known to be a good proxy of the topical relatedness of two sentences (Conneau et al., 2017; Subramanian et al., 2018; Xu et al., 2018a). For the same reasons as Equation 1, we ignore the negative cos scores by the max(·, 0) operation. 944 α= 6 1 1 X , β= SC (x, y) 1 |D| (x,y)∈D X 1 SR (x, y) |D| (x,y)∈D . (4) Experiments: Data Scoring In this section, we describe our experiments that validate the effectiveness of the proposed scoring method. 6.1 Experimental Setup Dataset. We conducted our experiments on a noisy English dialogue corpus from OpenSubtitles (Lison et al., 2018) containing roughly 441M lines. As explained in Section 1, this corpus includes many unacceptable utterance p"
2020.emnlp-main.68,N13-1073,0,0.0120095,"isy English dialogue corpus from OpenSubtitles (Lison et al., 2018) containing roughly 441M lines. As explained in Section 1, this corpus includes many unacceptable utterance pairs (Section 1). We first applied several rule-based filtering as rudimentary preprocesses, which are typically used in the related literature. Then, we obtained 79,445,453 utterance pairs as our training data, which excludes our test and validation data.5 Proposed method: detailed setup. To compute the connectivity SC , we obtained a phrase table on our training data by using Moses (Koehn et al., 2007) with fastAlign (Dyer et al., 2013). We then removed phrase pairs with a low co-occurrence frequency (here, less than 200 times) or composed 5 See Appendix B for details on our data such as the preparation procedure and statistics. (a) Cs´aky et al. (2019) TRG (b) Junczys-Dowmunt (2018) (c) Ours SC+R Figure 2: Distributions between human scores and automatically computed scores by each method (English). of the same phrases from the table. As a result, the phrase table included 68,891 phrase pairs, which were used as the key phrase set P as described in Section 5.1. To compute the content relatedness SR , we created a sentence v"
2020.emnlp-main.68,N19-4011,0,0.0123439,"e entropy of source/target utterances. • Junczys-Dowmunt (2018): Filtering for NMT based on the dual conditional cross-entropy computed by a neural encoder-decoder model. It achieved the best performance on the Parallel Corpus Filtering Task at WMT 2018.6 Human evaluation. To validate the ability of the proposed method to estimate the quality of utterance pairs, we measured the correlation between its scores and those assigned by humans through crowdsourcing. We used Amazon Mechanical Turk.7 We randomly extracted 2008 scored 6 http://www.statmt.org/wmt18/ https://www.mturk.com/ 8 Same size as Sedoc et al. (2019) and Cho and May (2020). 7 Scoring method Spearman’s ρ Cs´aky et al. (2019) SRC Cs´aky et al. (2019) TRG Junczys-Dowmunt (2018) Ours SC+R Ours SC (ablation study) Ours SR (ablation study) p-value −0.1173 0.0462 0.2973 0.3751 9.8 × 10−2 5.2 × 10−1 1.9 × 10−5 4.4 × 10−8 0.2044 0.3007 3.7 × 10−3 1.5 × 10−5 Table 2: Correlation coefficient between human scores and automatically computed scores (English). utterance pairs and asked native English-speaking crowdworkers to answer the following question for each pair: Is the sequence of the two utterances acceptable as a dialogue? Workers were instruct"
2020.emnlp-main.68,P16-1162,0,0.0349849,"Missing"
2020.emnlp-main.68,P19-1021,0,0.0183406,"ility scores are given by humans on the English OpenSubtitles corpus. Introduction Some million-scale datasets such as movie scripts and social media posts have become available in recent years for building neural dialogue agents (Lison and Tiedemann, 2016; Henderson et al., 2019). Such large-scale datasets can be expected to improve the performance of dialogue response generation models based on deep neural networks (DNNs) since the combination of DNNs and large-scale training datasets has led to considerable performance improvement in many sentence generation tasks (Koehn and Knowles, 2017; Sennrich and Zhang, 2019; Adiwardana et al., 2020). In contrast to the quantity of the data, the quality of the data has often been problematic. For example, OpenSubtitles (Lison and Tiedemann, 2016; Lison et al., 2018), the most widely used largescale English dialogue corpus, was constructed by 1 The code is publicly available at https://github. com/jqk09a/CoRe-dialogue-filtering. collecting two consecutive lines of movie subtitles under the simplified assumption that one line of a movie subtitle is one utterance and the next line is the next utterance follow it. Inevitably, this corpus includes unacceptable utteran"
2020.emnlp-main.68,P15-1152,0,0.0851264,"Missing"
2020.emnlp-main.68,2020.acl-main.220,0,0.0214665,"s, these methods cannot straightforwardly be considered as alternatives to the proposed method, which aims at filtering. 10 Relationship with Evaluation Metric The proposed method SC+R maps an utterance pair to a score (scalar value) in terms of the quality of dialogue. That is, formally, our method is similar to the reference-free automatic evaluation metrics for dialogue agents; both of them evaluate the response given an input utterance and also map into a score. Recently, the novel reference-free metrics for evaluating generated responses such as USR (Mehri and Eskenazi, 2020) or M AU DE (Sinha et al., 2020) ware developed. While it is possible to use them as a scoring method for filtering noisy data, in theory, there are some concerns with applying them in practice. One is the difference of the data of interest; since evaluation metrics are intended for responses generated as dialogue, i.e., somewhat valid dialogue data, it is unclear whether they also work for apparently noisy data. Another one is the difference of desired Conclusion In light of the success of noisy corpus filtering in neural machine translation, we attempted to filter out unacceptable utterance pairs from large dialogue corpor"
2020.findings-emnlp.26,W19-4418,1,0.829117,"SR +PRET+R2L+SED 55.8 60.4 64.2 65.0 63.1 68.8 69.8 72.9 73.9 59.9 63.3 61.2 61.4 63.7 69.5 70.2 67.8 Table 7: Comparison with existing top models: a bold value denotes the best result within the column. Both SR and BEA indicate SR BEA+EF+L8 and BEA-test, respectively. ensemble of four left-to-right (L2R) models and then re-scored the hypotheses using these models. We then re-ranked the n-best hypotheses based on the sum of the both two scores. Sentence-level error detection (SED) SED is used to identify whether a given sentence contains any grammatical errors. Following the work presented by Asano et al. (2019), we employed a strategy based on reducing the number of false positives by only considering sentences that contained grammatical errors in the GEC model, using an SED model. We implemented the same model employed for SED filtering. We evaluated the performance of the proposed best denoised model incorporated with the taskspecific techniques on the three existing benchmarks: CoNLL-2014, JFLEG, and BEA-test, and then compared the scores with existing bestperforming models. Table 7 shows the results for both the single and the ensemble models after applying PRET, SED13 , and R2L to SR14 . Since"
2020.findings-emnlp.26,D19-1435,0,0.0267376,"Missing"
2020.findings-emnlp.26,W18-6404,0,0.120723,"illed annotators are corrected by an existing GEC model. This approach relies on the consistence of the GEC model’s predictions (Section 4). We evaluated the effectiveness of our method over several GEC datasets, and found that it considerably outperformed baseline methods, includ1 https://corpus.mml.cam.ac.uk/ efcamdat2/public_html/ 267 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 267–280 c November 16 - 20, 2020. 2020 Association for Computational Linguistics ing three strong denoising baselines based on a filtering approach, which is a common approach in MT (Bei et al., 2018; Junczys-Dowmunt, 2018; Rossenbach et al., 2018). We further improved the performance by applying task-specific techniques and achieved state-of-the-art performance on the CoNLL-2014, JFLEG, and BEA-2019 benchmarks. Finally, through our analysis, we found unexpected benefits to our approach: (i) the approach benefits from the advantage of self-training in neural sequence generation due to its structural similarity (Section 6.3), (ii) resulted in significant increase in recall while maintaining equal precision, indicating improved coverage of correction (Section 6.4), and (iii) there seems to"
2020.findings-emnlp.26,P06-1032,0,0.0692196,"oposed denoising method, and found that our approach leads to improved coverage of corrections and facilitated fluency edits which are reflected in higher recall and overall performance. 1 2 : Errors are left uncorrected Source: We discuss about our sales target. Target: We discuss about our sales target. Table 1: Example of an inappropriately corrected error and an unchanged error in EFCamDat. We consider these types of errors to be dataset noise that might hinder GEC model performance. Introduction Grammatical error correction (GEC) is often considered a variant of machine translation (MT) (Brockett et al., 2006; Junczys-Dowmunt et al., 2018) due to their structural similarity–“translating” from source ungrammatical text to target grammatical text. At present, several neural encoderdecoder (EncDec) approaches have been introduced for this task and have achieved remarkable results (Chollampatt and Ng, 2018; Zhao et al., 2019; Kiyono et al., 2019). EncDec models tend to further improve in performance with increasing data size (Koehn and Knowles, 2017; Sennrich and Zhang, 2019), however, this is not necessarily true in GEC. For example, Lo et al. (2018) reported that an EncDec-based GEC model trained on"
2020.findings-emnlp.26,W19-4406,0,0.0268239,"Missing"
2020.findings-emnlp.26,N12-1067,0,0.0314065,"the effectiveness of the proposed method, we followed the work by Mita et al. (2019) and evaluated the performance of the GEC models across various GEC datasets in terms of the same evaluation metrics. We report 5 https://spacy.io/ https://github.com/rsennrich/ subword-nmt 6 Dataset #sent (pairs) Split BEA-train EFCamDAT Lang-8 BEA-valid CoNLL-2014 JFLEG BEA-test 561,100 train 2,269,595 train 5,689,213 train 2,377 valid Scorer - 1,312 test M2 scorer & GLEU 747 test M2 scorer & GLEU 4,477 test ERRANT Table 4: Summary of datasets used in our experiments. the results measured by both M2 scorer (Dahlmeier and Ng, 2012)7 and GLEU metric (Napoles et al., 2015, 2016)8 on both the CoNLL-2014 test set and the JFLEG test set (Napoles et al., 2017). All reported results (except those corresponding to the ensemble models) are the average of three distinct trials using three different random seeds. Let us emphasize that our focus is on denoising the training data, and denoising the test data is out of the scope of this study. The commonly used test data, such as CoNLL-2014 and JFLEG, have multiple references which can lower the noise factor. In addition to having multiple references, both JFLEG and CoNLL-2014 have b"
2020.findings-emnlp.26,W13-1703,0,0.0762998,"Missing"
2020.findings-emnlp.26,N19-1423,0,0.04159,"Missing"
2020.findings-emnlp.26,W19-4427,0,0.0294103,"Missing"
2020.findings-emnlp.26,P15-2097,0,0.0237037,"we followed the work by Mita et al. (2019) and evaluated the performance of the GEC models across various GEC datasets in terms of the same evaluation metrics. We report 5 https://spacy.io/ https://github.com/rsennrich/ subword-nmt 6 Dataset #sent (pairs) Split BEA-train EFCamDAT Lang-8 BEA-valid CoNLL-2014 JFLEG BEA-test 561,100 train 2,269,595 train 5,689,213 train 2,377 valid Scorer - 1,312 test M2 scorer & GLEU 747 test M2 scorer & GLEU 4,477 test ERRANT Table 4: Summary of datasets used in our experiments. the results measured by both M2 scorer (Dahlmeier and Ng, 2012)7 and GLEU metric (Napoles et al., 2015, 2016)8 on both the CoNLL-2014 test set and the JFLEG test set (Napoles et al., 2017). All reported results (except those corresponding to the ensemble models) are the average of three distinct trials using three different random seeds. Let us emphasize that our focus is on denoising the training data, and denoising the test data is out of the scope of this study. The commonly used test data, such as CoNLL-2014 and JFLEG, have multiple references which can lower the noise factor. In addition to having multiple references, both JFLEG and CoNLL-2014 have been specifically constructed for GEC ev"
2020.findings-emnlp.26,E17-2037,0,0.0757371,"models across various GEC datasets in terms of the same evaluation metrics. We report 5 https://spacy.io/ https://github.com/rsennrich/ subword-nmt 6 Dataset #sent (pairs) Split BEA-train EFCamDAT Lang-8 BEA-valid CoNLL-2014 JFLEG BEA-test 561,100 train 2,269,595 train 5,689,213 train 2,377 valid Scorer - 1,312 test M2 scorer & GLEU 747 test M2 scorer & GLEU 4,477 test ERRANT Table 4: Summary of datasets used in our experiments. the results measured by both M2 scorer (Dahlmeier and Ng, 2012)7 and GLEU metric (Napoles et al., 2015, 2016)8 on both the CoNLL-2014 test set and the JFLEG test set (Napoles et al., 2017). All reported results (except those corresponding to the ensemble models) are the average of three distinct trials using three different random seeds. Let us emphasize that our focus is on denoising the training data, and denoising the test data is out of the scope of this study. The commonly used test data, such as CoNLL-2014 and JFLEG, have multiple references which can lower the noise factor. In addition to having multiple references, both JFLEG and CoNLL-2014 have been specifically constructed for GEC evaluation, while the training data (Lang-8 and EFCamDat) are more of an organic collect"
2020.findings-emnlp.26,P19-1256,0,0.0278192,"generate new target sentences Yˆi from the original target sentences Yi and pair them with their original source sentences Xi (line 4 in Algorithm 1). The consistency of the base model predictions ensures that the ˆ = {(Xi , Yˆi )}n contain resulting parallel data D i=1 noise at a less extent. It is worth noting that SR can be regarded as a variant of self-training due to its structural similarity, except that it takes the target sentences rather than the source sentences as input to the model. The algorithm itself is the key difference from existing methods based on selftraining (Wang, 2019; Nie et al., 2019; Xie et al., 2020). One challenge of this approach is that the base model may consistently make inaccurate corrections. We thus incorporate a fail-safe mechanism Experiments We evaluate the proposed method in two ways. First, we exclusively focus on investigating the effectiveness of the proposed denoising method (Section 5.3). Then, we compare our strongest model trained with denoised data (henceforth, denoised model), with current best-performing ones to investigate whether the proposed method has a complementary effect on existing task-specific techniques (Section 5.4). 5.1 Configurations"
2020.findings-emnlp.26,2020.bea-1.16,0,0.0622744,"Missing"
2020.findings-emnlp.26,N19-4009,0,0.0124714,"the scope of this study. The commonly used test data, such as CoNLL-2014 and JFLEG, have multiple references which can lower the noise factor. In addition to having multiple references, both JFLEG and CoNLL-2014 have been specifically constructed for GEC evaluation, while the training data (Lang-8 and EFCamDat) are more of an organic collection of learner and editor interactions. Naturally, we believe it is reasonable to assume that the test data are considerably cleaner. Model We employed the “Transformer (big)” settings Vaswani et al. (2017) using the implementation in the fairseq toolkit (Ott et al., 2019). Details on the hyper-parameters are listed in Appendix B. As a language model for the fail-safe mechanism, we used the PyTorch implementation of GPT-2 (Radford et al., 2019)9 . Note that to avoid a preference for shorter phrases, we normalized the perplexity by sentence length. 5.2 Baselines As argued in Section 4, we hypothesized that the filtering-based denoising approaches are not wellsuited for GEC. To verify this hypothesis, we employed the following three filtering-based denoising baseline methods in addition to a base model trained in noisy parallel data D (henceforth, no denoising)."
2020.findings-emnlp.26,N19-1132,1,0.898095,"datasets are summarized in Table 4. For preprocessing, we tokenized the training data using the spaCy tokenizer5 . Then, we removed sentence pairs where both sentences where identical or both longer than 80 tokens. Finally, we acquired subwords from the target sentence via the byte-pair-encoding (BPE) (Sennrich et al., 2016b) algorithm. We used the subword-nmt implementation6 and then applied BPE to splitting both source and target texts. The number of merge operations was set to 8,000. 270 Evaluation To investigate the effectiveness of the proposed method, we followed the work by Mita et al. (2019) and evaluated the performance of the GEC models across various GEC datasets in terms of the same evaluation metrics. We report 5 https://spacy.io/ https://github.com/rsennrich/ subword-nmt 6 Dataset #sent (pairs) Split BEA-train EFCamDAT Lang-8 BEA-valid CoNLL-2014 JFLEG BEA-test 561,100 train 2,269,595 train 5,689,213 train 2,377 valid Scorer - 1,312 test M2 scorer & GLEU 747 test M2 scorer & GLEU 4,477 test ERRANT Table 4: Summary of datasets used in our experiments. the results measured by both M2 scorer (Dahlmeier and Ng, 2012)7 and GLEU metric (Napoles et al., 2015, 2016)8 on both the Co"
2020.findings-emnlp.26,W18-6487,0,0.0574237,"Missing"
2020.findings-emnlp.26,I11-1017,0,0.0370849,"Missing"
2020.findings-emnlp.26,W17-4739,0,0.0455444,"Missing"
2020.findings-emnlp.26,W16-2323,0,0.0200525,"5.1 Configurations Dataset For the training dataset, we used the same datasets as mentioned in Section 3: BEAtrain, EFCamDat, and Lang-8. In addition, we used the BEA official validation set (henceforth, BEA-valid) provided in the BEA-2019 workshop as validation data. The characteristics of the datasets are summarized in Table 4. For preprocessing, we tokenized the training data using the spaCy tokenizer5 . Then, we removed sentence pairs where both sentences where identical or both longer than 80 tokens. Finally, we acquired subwords from the target sentence via the byte-pair-encoding (BPE) (Sennrich et al., 2016b) algorithm. We used the subword-nmt implementation6 and then applied BPE to splitting both source and target texts. The number of merge operations was set to 8,000. 270 Evaluation To investigate the effectiveness of the proposed method, we followed the work by Mita et al. (2019) and evaluated the performance of the GEC models across various GEC datasets in terms of the same evaluation metrics. We report 5 https://spacy.io/ https://github.com/rsennrich/ subword-nmt 6 Dataset #sent (pairs) Split BEA-train EFCamDAT Lang-8 BEA-valid CoNLL-2014 JFLEG BEA-test 561,100 train 2,269,595 train 5,689,2"
2020.findings-emnlp.26,P11-1019,0,0.072917,"Missing"
2020.findings-emnlp.26,P16-1162,0,0.0110038,"5.1 Configurations Dataset For the training dataset, we used the same datasets as mentioned in Section 3: BEAtrain, EFCamDat, and Lang-8. In addition, we used the BEA official validation set (henceforth, BEA-valid) provided in the BEA-2019 workshop as validation data. The characteristics of the datasets are summarized in Table 4. For preprocessing, we tokenized the training data using the spaCy tokenizer5 . Then, we removed sentence pairs where both sentences where identical or both longer than 80 tokens. Finally, we acquired subwords from the target sentence via the byte-pair-encoding (BPE) (Sennrich et al., 2016b) algorithm. We used the subword-nmt implementation6 and then applied BPE to splitting both source and target texts. The number of merge operations was set to 8,000. 270 Evaluation To investigate the effectiveness of the proposed method, we followed the work by Mita et al. (2019) and evaluated the performance of the GEC models across various GEC datasets in terms of the same evaluation metrics. We report 5 https://spacy.io/ https://github.com/rsennrich/ subword-nmt 6 Dataset #sent (pairs) Split BEA-train EFCamDAT Lang-8 BEA-valid CoNLL-2014 JFLEG BEA-test 561,100 train 2,269,595 train 5,689,2"
2020.findings-emnlp.26,N19-1014,0,0.0156961,"e of an inappropriately corrected error and an unchanged error in EFCamDat. We consider these types of errors to be dataset noise that might hinder GEC model performance. Introduction Grammatical error correction (GEC) is often considered a variant of machine translation (MT) (Brockett et al., 2006; Junczys-Dowmunt et al., 2018) due to their structural similarity–“translating” from source ungrammatical text to target grammatical text. At present, several neural encoderdecoder (EncDec) approaches have been introduced for this task and have achieved remarkable results (Chollampatt and Ng, 2018; Zhao et al., 2019; Kiyono et al., 2019). EncDec models tend to further improve in performance with increasing data size (Koehn and Knowles, 2017; Sennrich and Zhang, 2019), however, this is not necessarily true in GEC. For example, Lo et al. (2018) reported that an EncDec-based GEC model trained on EFCamDat (Geertzen et al., 2013)1 , the largest publicly available learner corpus as of today (two million sentence pairs), was outperformed by a model trained on a smaller dataset (e.g., 720K pairs). They hypothesized that this may be due to the noisiness of EFCamDat, i.e., the presence of sentence pairs whose corr"
2020.findings-emnlp.26,P19-1021,0,0.0228999,"r GEC model performance. Introduction Grammatical error correction (GEC) is often considered a variant of machine translation (MT) (Brockett et al., 2006; Junczys-Dowmunt et al., 2018) due to their structural similarity–“translating” from source ungrammatical text to target grammatical text. At present, several neural encoderdecoder (EncDec) approaches have been introduced for this task and have achieved remarkable results (Chollampatt and Ng, 2018; Zhao et al., 2019; Kiyono et al., 2019). EncDec models tend to further improve in performance with increasing data size (Koehn and Knowles, 2017; Sennrich and Zhang, 2019), however, this is not necessarily true in GEC. For example, Lo et al. (2018) reported that an EncDec-based GEC model trained on EFCamDat (Geertzen et al., 2013)1 , the largest publicly available learner corpus as of today (two million sentence pairs), was outperformed by a model trained on a smaller dataset (e.g., 720K pairs). They hypothesized that this may be due to the noisiness of EFCamDat, i.e., the presence of sentence pairs whose correction still contained grammatical errors due to inappropriate edits or being left uncorrected. For example, in Table 1, “discuss about” should most likel"
2020.findings-emnlp.26,P12-2039,0,0.0281947,"Missing"
2020.findings-emnlp.26,W19-8639,0,0.0280445,")}ni=1 , we generate new target sentences Yˆi from the original target sentences Yi and pair them with their original source sentences Xi (line 4 in Algorithm 1). The consistency of the base model predictions ensures that the ˆ = {(Xi , Yˆi )}n contain resulting parallel data D i=1 noise at a less extent. It is worth noting that SR can be regarded as a variant of self-training due to its structural similarity, except that it takes the target sentences rather than the source sentences as input to the model. The algorithm itself is the key difference from existing methods based on selftraining (Wang, 2019; Nie et al., 2019; Xie et al., 2020). One challenge of this approach is that the base model may consistently make inaccurate corrections. We thus incorporate a fail-safe mechanism Experiments We evaluate the proposed method in two ways. First, we exclusively focus on investigating the effectiveness of the proposed denoising method (Section 5.3). Then, we compare our strongest model trained with denoised data (henceforth, denoised model), with current best-performing ones to investigate whether the proposed method has a complementary effect on existing task-specific techniques (Section 5.4). 5"
2020.lantern-1.3,D19-1243,0,0.0468409,"Missing"
2020.lantern-1.3,Q18-1023,0,0.0531252,"Missing"
2020.lantern-1.3,N16-1023,0,0.02877,"tennis ball Figure 1: Example of three selected and one removed commonsense questions from two MCScript2.0 instances. questions about common properties and locations of objects that it previously answered incorrectly. The ultimate goal of our work is to discover an alternative to the expensive (in terms of time) and limited (in terms of coverage) crowdsourced-commonsense acquisition approach. 2 Related work Knowledge extraction. Previous works have already recognized the rich content of computer vision datasets and investigated its benefits for commonsense knowledge extraction. For instance, Yatskar et al. (2016) and Mukuze et al. (2018) derived 16K commonsense relations and 2,000 verb/location pairs (e.g., holds(dining-table, cutlery), eat/restaurant) from the annotations included in the Microsoft Common Objects in Context dataset (Lin et al., 2014) (MS-COCO). However, they only focused on physical commonsense. A more recent trend is to query LMs for commonsense facts. While a robust LM like BERT has shown a strong performance retrieving commonsense knowledge at a similar level to factual knowledge (Petroni et al., 2019), this seems to happen only when that knowledge is explicitly written down (Forbe"
2020.lrec-1.443,W18-6401,0,0.0525748,"Missing"
2020.lrec-1.443,2012.eamt-1.60,0,0.0283866,"llion parallel sentences. 4.1. Training NMT with JParaCrawl In this section, we trained NMT models with JParaCrawl and tested the models on several test sets to see how our corpus covers a broader range of domains. 4.1.1. Experimental Settings Data To see how our corpus covers a broader range of domains, we used four test sets: scientific paper excerpts (ASPEC, Nakazawa et al. (2016)), movie subtitles (JESC, Pryzant et al. (2017)), texts on Wikipedia articles related to Kyoto (KFTT, Neubig (2011)), and TED talks (tst2015, provided for the IWSLT 2017 evaluation campaign, Cettolo et al. (2017), Cettolo et al. (2012)). Table 2 shows the details of the test sets. During training, we used the ASPEC dev set as a validation set. We preprocessed the data with sentencepiece (Kudo and Richardson, 2018) to split the sentences into subwords. We set the vocabulary size to 32,000 and removed sentences whose length exceeded 250 subwords from the training data. Since JParaCrawl was NFKC-normalized, we also normalized the test sets. For comparison, we trained NMT models with domainspecific bitexts. Table 3 shows the number of sentences and words in the domain-specific training sets. Since the sentences in ASPEC are ord"
2020.lrec-1.443,W18-6415,0,0.0184478,"org/ 3603 ParaCrawl5 project, which is building parallel corpora by crawling the web. Their objective is to build parallel corpora to/from English for the 24 official languages of the European Union. They released an earlier version of the corpora, and this version is already quite large6 . This early release has already been used for previous WMT shared translation tasks for some language pairs (Bojar et al., 2018; Barrault et al., 2019), and task participants reported that ParaCrawl significantly improved the translation performance when it was used with a careful corpus cleaning technique (Junczys-Dowmunt, 2018). We extend this work to mine English-Japanese parallel sentences. 3. JParaCrawl To collect English-Japanese parallel sentences from the web, we took a similar approach to that used in the ParaCrawl project. Figure 1 shows how we mined parallel corpora from the web. First, we selected candidate domains to crawl that might contain English-Japanese parallel sentences from Common Crawl data (Section 3.1). Then, we crawled the candidate domains (Section 3.2). Finally, we aligned the parallel sentences from the crawled data and filtered out noisy sentence pairs (Section 3.3). 3.1. Crawling Domain S"
2020.lrec-1.443,P07-2045,0,0.00878145,"side in JParaCrawl corpus We crawled a large number of websites, but some were too small to mine for parallel sentences. Therefore, we filtered out those domains whose compressed archive size was less than 1 MB, and only 39,936 domains remained. To align parallel sentences, we used the Bitextor toolkit10 provided by the ParaCrawl project. Our experiment was based on version 7.0, and we fixed several components for Japanese sentences. To extract text from HTML files, we used extractontent11 , which was developed for Japanese text. We used split-sentences.perl12 contained in the Moses toolkit (Koehn et al., 2007) to split a text into sentences. We fixed the script to deal with Japanese end-of-sentence tokens. There are two primary approaches to align parallel text. One algorithm uses a bilingual lexicon to generate crude translations along with such other features as sentence length to find the best sentence pair (Varga et al., 2005). The other algorithm uses an external MT system to translate one language into the other and find a sentence pair that maximizes BLEU scores (Sennrich and Volk, 2011; Papineni et al., 2002). Since the latter approach needs more computational resources for external MT, we"
2020.lrec-1.443,2005.mtsummit-papers.11,0,0.248187,"onducted experiments to show 1 Currently the largest French-English parallel corpus is ParaCrawl v5, which contains 51.3 million training data. how effectively our corpus and pre-trained models worked with typical NMT training settings. These experimental settings and results are shown in Section 4. Finally, we conclude with a brief discussion of future work in Section 5. JParaCrawl and the NMT models pre-trained with it are freely available online2 for research purposes3 . 2. Related Work One typical kind of source for parallel texts is the documents of international organizations. Europarl (Koehn, 2005) is an example of the early success of creating a large parallel corpus by automatically aligning parallel texts from the proceedings of the European Parliament. The United Nations Parallel Corpus (Ziemski et al., 2016) is another similar example that was created from UN documents. These texts were translated by professionals, and aligning the documents is easy because they often have such metainformation as the identities of speakers, although their domains and language pairs are limited. Another important source of parallel texts is the web. Uszkoreit et al. (2010) proposed a large-scale dis"
2020.lrec-1.443,D18-2012,0,0.0538961,"us covers a broader range of domains. 4.1.1. Experimental Settings Data To see how our corpus covers a broader range of domains, we used four test sets: scientific paper excerpts (ASPEC, Nakazawa et al. (2016)), movie subtitles (JESC, Pryzant et al. (2017)), texts on Wikipedia articles related to Kyoto (KFTT, Neubig (2011)), and TED talks (tst2015, provided for the IWSLT 2017 evaluation campaign, Cettolo et al. (2017), Cettolo et al. (2012)). Table 2 shows the details of the test sets. During training, we used the ASPEC dev set as a validation set. We preprocessed the data with sentencepiece (Kudo and Richardson, 2018) to split the sentences into subwords. We set the vocabulary size to 32,000 and removed sentences whose length exceeded 250 subwords from the training data. Since JParaCrawl was NFKC-normalized, we also normalized the test sets. For comparison, we trained NMT models with domainspecific bitexts. Table 3 shows the number of sentences and words in the domain-specific training sets. Since the sentences in ASPEC are ordered by their alignment confidence scores, the former sentences tended to be clean and the latter might contain noisy sentence pairs. Based on previous work (Neubig, 2014), we used o"
2020.lrec-1.443,C96-2195,0,0.091788,"o primary approaches to align parallel text. One algorithm uses a bilingual lexicon to generate crude translations along with such other features as sentence length to find the best sentence pair (Varga et al., 2005). The other algorithm uses an external MT system to translate one language into the other and find a sentence pair that maximizes BLEU scores (Sennrich and Volk, 2011; Papineni et al., 2002). Since the latter approach needs more computational resources for external MT, we used the bilingual lexicon-based algorithm. We used the EDR EnglishJapanese dictionary as a bilingual lexicon (Miyoshi et al., 1996). Table 1 shows the number of collected parallel sentences and words. After the bitext alignment process, we mined over 27 million parallel sentences. However, these collected sentences contained many noisy pairs. Therefore, we filtered the corpus with Bicleaner13 (SánchezCartagena et al., 2018). The filtering model was trained with our in-house English-Japanese parallel corpora. After removing sentence pairs whose scores were lower than 0.5, we retained around 8.7 million sentences. As our initial corpus release, we open-filtered 8.7 million parallel sentences to the public. However, we still"
2020.lrec-1.443,L16-1350,0,0.159048,"lel corpus, machine translation, English, Japanese 1. Introduction Since current machine translation (MT) approaches are mainly data-driven, one key bottleneck has been the lack of parallel corpora. This problem continues with the recent neural machine translation (NMT) architecture. As the amount of training data increases, NMT performance improves (Sennrich and Zhang, 2019). Our goal is to create large parallel corpora to/from Japanese. In our first attempt, we focused on the English-Japanese language pair. Currently, ASPEC is the largest publicly available English-Japanese parallel corpus (Nakazawa et al., 2016), which contains 3.0 million sentences for training. Unfortunately, this is relatively small compared to such resource-rich language pairs as FrenchEnglish1 . Also, available domains remain limited. We address this problem, which hinders the progress of EnglishJapanese translation research, by crawling the web to mine for English-Japanese parallel sentences. Current NMT training requires a great deal of computational time, which complicates running experiments with few computational resources. We alleviate this problem by providing NMT models trained with our corpus. Since our web-based parall"
2020.lrec-1.443,W14-7002,0,0.0529333,"4.2. We also trained a model for a specific domain without fine-tuning in Section 4.3. Data ASPEC JESC KFTT IWSLT # sentences # words 1,812 2,000 1,160 1,194 39,573 13,617 22,063 20,367 Table 2: Number of sentences and words on English side in test sets Data ASPEC JESC KFTT IWSLT # sentences # words 3,008,500 2,797,388 440,288 223,108 68,929,413 19,339,040 9,737,715 3,877,868 Table 3: Number of sentences and words on English side in training sets. The original version of ASPEC contains 3.0 million sentences, but we used only the first 2.0 million sentences for training based on previous work (Neubig, 2014). with an existing corpus in Section 4.2. Finally, we trained a model for a specific domain with JParaCrawl and other corpora without fine-tuning in Section 4.3. In the following experiments, we used a filtered JParaCrawl corpus that contained 8.7 million parallel sentences. 4.1. Training NMT with JParaCrawl In this section, we trained NMT models with JParaCrawl and tested the models on several test sets to see how our corpus covers a broader range of domains. 4.1.1. Experimental Settings Data To see how our corpus covers a broader range of domains, we used four test sets: scientific paper exc"
2020.lrec-1.443,W18-6301,0,0.0530583,"ed dropout with a probability of 0.3 (Srivastava et al., 2014). We used big settings for ASPEC and JESC, base settings for KFTT, and small settings for IWSLT. As an optimizer, we used Adam with α = 0.001, β1 = 0.9, and β2 = 0.98. We used a root-square decay learning rate schedule with a linear warmup of 4000 steps (Vaswani et al., 2017). We clipped gradients to avoid exceeding their norm 1.0 to stabilize the training (Pascanu et al., 2013). For the base and small settings, each mini-batch contained about 5,000 tokens (subwords), and we accumulated the gradients of 64 mini-batches for updates (Ott et al., 2018). For the big settings, we set the mini-batch size to 2,000 tokens and accumulated 160 mini-batches for updates. We trained the model with 24,000 iterations, saved the model parameters every 200 iterations, and averaged the last eight models. To achieve maximum performance with the latest GPUs, we used mixed-precision training (Micikevicius et al., 2018). When decoding, we used a beam search with a size of six and length normalization by dividing the scores by their lengths. We slightly changed the settings for the in-domain baseline training because some of the above settings are inappropriat"
2020.lrec-1.443,N19-4009,0,0.032921,"aCrawl was NFKC-normalized, we also normalized the test sets. For comparison, we trained NMT models with domainspecific bitexts. Table 3 shows the number of sentences and words in the domain-specific training sets. Since the sentences in ASPEC are ordered by their alignment confidence scores, the former sentences tended to be clean and the latter might contain noisy sentence pairs. Based on previous work (Neubig, 2014), we used only the first 2.0 million sentences for training, although the original ASPEC corpus contained 3.0 million sentences. NMT Models We trained an NMT model with fairseq (Ott et al., 2019). Our model was based on Transformer (Vaswani et al., 2017). We trained three models for each direction by varying the hyper-parameters: small, base, and big settings. The base and big settings are based on Vaswani et al. (2017). For the base settings, we used an encoder/decoder with six layers. We set their embedding size to 512 and their feed-forward embedding size to 2048. We used eight attention heads for both the encoder and the decoder. For the big settings, we changed their embedding size to 1024 and their feed-forward embedding size to 4096. We also used 16 attention heads for both the"
2020.lrec-1.443,P02-1040,0,0.107238,"ped for Japanese text. We used split-sentences.perl12 contained in the Moses toolkit (Koehn et al., 2007) to split a text into sentences. We fixed the script to deal with Japanese end-of-sentence tokens. There are two primary approaches to align parallel text. One algorithm uses a bilingual lexicon to generate crude translations along with such other features as sentence length to find the best sentence pair (Varga et al., 2005). The other algorithm uses an external MT system to translate one language into the other and find a sentence pair that maximizes BLEU scores (Sennrich and Volk, 2011; Papineni et al., 2002). Since the latter approach needs more computational resources for external MT, we used the bilingual lexicon-based algorithm. We used the EDR EnglishJapanese dictionary as a bilingual lexicon (Miyoshi et al., 1996). Table 1 shows the number of collected parallel sentences and words. After the bitext alignment process, we mined over 27 million parallel sentences. However, these collected sentences contained many noisy pairs. Therefore, we filtered the corpus with Bicleaner13 (SánchezCartagena et al., 2018). The filtering model was trained with our in-house English-Japanese parallel corpora. Af"
2020.lrec-1.443,W18-6319,0,0.0240423,"decoding, we used a beam search with a size of six and length normalization by dividing the scores by their lengths. We slightly changed the settings for the in-domain baseline training because some of the above settings are inappropriate for smaller datasets. For IWSLT, we accumulated 16 mini-batches per update instead of 64. Since we confirmed that the model had already converged based on the validation loss, we stopped training at 20,000 iterations for all the in-domain baselines. Evaluation To evaluate the performance, we calculated the BLEU scores (Papineni et al., 2002) with sacreBLEU (Post, 2018). Since sacreBLEU does not internally tokenize Japanese text, we tokenized both the hypothesis and reference texts using MeCab14 with an IPA dictionary when evaluating the English-Japanese translations. 4.1.2. Experimental Results and Analysis Table 4 shows the BLEU scores of the in-domain and JParaCrawl NMT models (see in-domain and JParaCrawl columns). Since JParaCrawl is not supposed to contain a specific domain, its BLEU scores were lower than those of the model trained with the in-domain corpus. However, we expect that these in-domain models only focus on a specific domain and do not work"
2020.lrec-1.443,W18-6488,0,0.05153,"Missing"
2020.lrec-1.443,W11-4624,0,0.0822405,"tent11 , which was developed for Japanese text. We used split-sentences.perl12 contained in the Moses toolkit (Koehn et al., 2007) to split a text into sentences. We fixed the script to deal with Japanese end-of-sentence tokens. There are two primary approaches to align parallel text. One algorithm uses a bilingual lexicon to generate crude translations along with such other features as sentence length to find the best sentence pair (Varga et al., 2005). The other algorithm uses an external MT system to translate one language into the other and find a sentence pair that maximizes BLEU scores (Sennrich and Volk, 2011; Papineni et al., 2002). Since the latter approach needs more computational resources for external MT, we used the bilingual lexicon-based algorithm. We used the EDR EnglishJapanese dictionary as a bilingual lexicon (Miyoshi et al., 1996). Table 1 shows the number of collected parallel sentences and words. After the bitext alignment process, we mined over 27 million parallel sentences. However, these collected sentences contained many noisy pairs. Therefore, we filtered the corpus with Bicleaner13 (SánchezCartagena et al., 2018). The filtering model was trained with our in-house English-Japan"
2020.lrec-1.443,P19-1021,0,0.0195903,"ning time. Additionally, we trained the model with an in-domain dataset and JParaCrawl to show how we achieved the best performance with them. JParaCrawl and the pre-trained models are freely available online for research purposes. Keywords: parallel corpus, machine translation, English, Japanese 1. Introduction Since current machine translation (MT) approaches are mainly data-driven, one key bottleneck has been the lack of parallel corpora. This problem continues with the recent neural machine translation (NMT) architecture. As the amount of training data increases, NMT performance improves (Sennrich and Zhang, 2019). Our goal is to create large parallel corpora to/from Japanese. In our first attempt, we focused on the English-Japanese language pair. Currently, ASPEC is the largest publicly available English-Japanese parallel corpus (Nakazawa et al., 2016), which contains 3.0 million sentences for training. Unfortunately, this is relatively small compared to such resource-rich language pairs as FrenchEnglish1 . Also, available domains remain limited. We address this problem, which hinders the progress of EnglishJapanese translation research, by crawling the web to mine for English-Japanese parallel senten"
2020.lrec-1.443,P13-1135,0,0.0240656,"el corpus by automatically aligning parallel texts from the proceedings of the European Parliament. The United Nations Parallel Corpus (Ziemski et al., 2016) is another similar example that was created from UN documents. These texts were translated by professionals, and aligning the documents is easy because they often have such metainformation as the identities of speakers, although their domains and language pairs are limited. Another important source of parallel texts is the web. Uszkoreit et al. (2010) proposed a large-scale distributed system to mine parallel text from the web and books. Smith et al. (2013) proposed an algorithm that creates a parallel corpus by mining Common Crawl4 , which is a free web crawl archive. Schwenk et al. (2019) mined Wikipedia and created a parallel corpus of 1,620 language pairs. The web, which includes a broad range of domains and many language pairs, is rapidly and continually growing. Thus, it has huge potential as a source of parallel corpora, although identifying correct parallel sentences is difficult. Our work was inspired by the recent success of the 2 http://www.kecl.ntt.co.jp/icl/lirg/ jparacrawl/ 3 This research is based on JParaCrawl v1.0. During the re"
2020.lrec-1.443,C10-1124,0,0.0337521,"nternational organizations. Europarl (Koehn, 2005) is an example of the early success of creating a large parallel corpus by automatically aligning parallel texts from the proceedings of the European Parliament. The United Nations Parallel Corpus (Ziemski et al., 2016) is another similar example that was created from UN documents. These texts were translated by professionals, and aligning the documents is easy because they often have such metainformation as the identities of speakers, although their domains and language pairs are limited. Another important source of parallel texts is the web. Uszkoreit et al. (2010) proposed a large-scale distributed system to mine parallel text from the web and books. Smith et al. (2013) proposed an algorithm that creates a parallel corpus by mining Common Crawl4 , which is a free web crawl archive. Schwenk et al. (2019) mined Wikipedia and created a parallel corpus of 1,620 language pairs. The web, which includes a broad range of domains and many language pairs, is rapidly and continually growing. Thus, it has huge potential as a source of parallel corpora, although identifying correct parallel sentences is difficult. Our work was inspired by the recent success of the"
2020.lrec-1.443,L16-1561,0,0.0369002,"cal NMT training settings. These experimental settings and results are shown in Section 4. Finally, we conclude with a brief discussion of future work in Section 5. JParaCrawl and the NMT models pre-trained with it are freely available online2 for research purposes3 . 2. Related Work One typical kind of source for parallel texts is the documents of international organizations. Europarl (Koehn, 2005) is an example of the early success of creating a large parallel corpus by automatically aligning parallel texts from the proceedings of the European Parliament. The United Nations Parallel Corpus (Ziemski et al., 2016) is another similar example that was created from UN documents. These texts were translated by professionals, and aligning the documents is easy because they often have such metainformation as the identities of speakers, although their domains and language pairs are limited. Another important source of parallel texts is the web. Uszkoreit et al. (2010) proposed a large-scale distributed system to mine parallel text from the web and books. Smith et al. (2013) proposed an algorithm that creates a parallel corpus by mining Common Crawl4 , which is a free web crawl archive. Schwenk et al. (2019) m"
2020.sustainlp-1.6,P07-1056,0,0.335774,"Missing"
2020.sustainlp-1.6,N19-1423,0,0.0110327,"Networks, Inc.2 RIKEN3 sosk@preferred.jp {yokoi,jun.suzuki,inui}@ecei.tohoku.ac.jp Abstract method, which (i) is computationally more efficient while (ii) useful for applications (iii) without significant sacrifice of model performance. We propose a trick for enabling a neural network without restrictions to estimate the influence, which we refer to as turn-over dropout. This method is computationally efficient as it requires only running two forward computations after training a single model on the entire training dataset. In addition to the efficiency, we demonstrated that it enabled BERT (Devlin et al., 2019) and VGGNet (Simonyan and Zisserman, 2015) to analyze the influences of training through various experiments, including example-based interpretation of error predictions and data cleansing to improve the accuracy on a test set with a distributional shift. Understanding the influence of a training instance on a neural network model leads to improving interpretability. However, it is difficult and inefficient to evaluate the influence, which shows how a model’s prediction would be changed if a training instance were not used. In this paper, we propose an efficient method for estimating the influ"
2020.sustainlp-1.6,P16-1162,0,0.0176054,"etwork models are notorious for their black-box prediction, which harms the trust and usability (Ribeiro et al., 2016). The influence estimation can mitigate this problem by suggesting possible reasons for a wrong model prediction by identifying influential training instances. To verify this benefit, we collected the misclassified instances of the validation or test set and searched for the training instances that most influenced the wrong predictions. Figure 3 indicates a text example from the results. Rare words of named entities were divided into many subwords (Schuster and Nakajima, 2012; Sennrich et al., 2016; Wu et al., 2016) and requiring more complex processing. A guess is that BERT might fail to understand the input due to the cluttered subwords, and predict a wrong label, which depended on a training instance similarly with many subwords. Additionally, we conducted the same experiment on Yahoo Answers 10-label question classification dataset (Zhang et al., 2015)8 , which is more complex than sentiment analysis. Figure 4 shows the results on Yahoo Answers. The misclassified text shares the phrase “ch ##rist” with the two influenSanity Check: Learning Curves We first observed an interesting pro"
2020.sustainlp-1.6,2020.acl-main.492,0,0.0206173,"t influenced by z. Our estimation uses the difference between the two sub-networks. prediction on another instance ztarget . The instance of interest ztarget is typically an instance in a test or validation dataset. 2.2 3 Related Methods 3.1 Computing the influence in Equation (1) by retraining two models for each instance is computationally expensive, and several estimation methods are proposed. Koh and Liang (2017) proposed an estimation method that assumed a strongly convex loss function and a global optimal solution1 . While the method is used even with neural models (Koh and Liang, 2017; Han et al., 2020), which do not satisfy the assumption, it still requires high computational cost. Hara et al. (2019) proposed a method without these restrictions; however, it consumes large disk storage and computation time that depend on the number of optimization steps. Our proposed method is much more efficient, as shown in Table 1. For example, in a case where Koh and Liang (2017)’s method took 10 minutes to estimate the influences of 10,000 training instances on another instance with BERT (Han et al., 2020), our method only required 35 seconds2 . This efficiency will expand the scope of applications of c"
2020.sustainlp-1.6,D13-1170,0,0.00361024,"D{zi } 4 . From this analogy, the influence of a training instance can be evaluated by considering these two sub-networks. The influence I(ztarget , zi ; D) = L(fD{zi } , ztarget ) L(fD , ztarget ) is estimated as ˆ target , zi ; D) I(z f m(zi ) := L(fD , ztarget ) m(zi ) L(fD The computational efficiency of our method is discussed in Section 2. Moreover, we answer a question: even if it is efficient, does it work well on applications? To demonstrate the applicability, we conducted experiments using different models and datasets. Setup First, we used the Stanford Sentiment TreeBank (SST-2) (Socher et al., 2013) binary sentiment classification task. Five thousand instances were sampled from the training set, and 872 instances in the development set were used. We trained BERT-base classifiers (Wolf et al., 2019) with the adapter modules (Houlsby et al., 2019), which froze the pre-trained BERT parameters but newly trained branch networks in addition to the output layers. We applied the turn-over dropout on the adapter modules and output layers. In addition, we used the CIFAR-10 (Krizhevsky, 2009) 10-class image classification task, with the 50,000 training instances and 10,000 validation instances. We"
2020.sustainlp-1.6,N18-1101,0,0.0942795,"Missing"
2020.sustainlp-1.6,2020.acl-demos.14,0,0.0208975,"Missing"
2020.sustainlp-1.6,N16-3020,0,0.0880143,"bout the decrease of model performances when using turn-over dropout. While we experimented with the successful architectures only, exploring the side effect in various architectures and its remedy is important future work. 4.2 Figure 4: A misclassified text in the test set and the texts with the highest influence with the error label in the training set for BERT on Yahoo Answers. ing the flipped mask does not learn each training instance ztrain . 4.3 Interpretation of Error of Predictions Neural network models are notorious for their black-box prediction, which harms the trust and usability (Ribeiro et al., 2016). The influence estimation can mitigate this problem by suggesting possible reasons for a wrong model prediction by identifying influential training instances. To verify this benefit, we collected the misclassified instances of the validation or test set and searched for the training instances that most influenced the wrong predictions. Figure 3 indicates a text example from the results. Rare words of named entities were divided into many subwords (Schuster and Nakajima, 2012; Sennrich et al., 2016; Wu et al., 2016) and requiring more complex processing. A guess is that BERT might fail to unde"
2020.wmt-1.12,W19-5304,0,0.257626,"h as back-translation and fine-tuning. The overview of our system is shown in Figure 1. We achieved the first place in De→En on automatic evaluation and obtained strong results in other language directions. 145 Proceedings of the 5th Conference on Machine Translation (WMT), pages 145–155 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics 2 Dataset and Preprocessing 2.1 pora. These corpora are used for large-scale backtranslation (Section 3.3). Bitext For both En↔De and En↔Ja, we used all bitexts that are available for a constrained system. En↔De Following Ng et al. (2019), we applied language identification filtering (langid)1 to the bitext. In this filtering, sentence pairs were removed if a supposedly English/German sentence is identified as a non-English/German sentence. Then, we applied the clean-corpus-n script available in the Moses toolkit (Koehn et al., 2007) and removed sentence pairs that are either too long and/or their length ratio is too large2 . These two filtering processes provided us with approximately 44M sentence pairs. Then, we trained and applied the Moses truecaser independently for each language. We also trained byte-pair encoding (BPE)"
2020.wmt-1.12,D18-1332,0,0.028291,"odel and Hyperparameter The well-known Transformer model (Vaswani et al., 2017) is our base Encoder Decoder model. Specifically, we started with the “Transformer (big)” setting described by Vaswani et al. (2017) and increased the feed-forward network (FFN) size from 4,096 to 8,192. Ng et al. (2019) reported that this larger FFN setting slightly improves the performance; we also confirmed it in our preliminary experiment. Table 1 shows a list of hyperparameters for model optimization. We employed an extremely large mini-batch size of 512,000 tokens using the delaying gradient update technique (Bogoychev et al., 2018; Ott et al., 2018). This is because previous studies showed that a large mini-batch size leads to a faster convergence (Ott et al., 2018) and a better generalization (Popel and Bojar, 2018; Bawden et al., 2019; Morishita et al., 2019). We also used a large learning rate of 0.001 to further accelerate the convergence (Goyal et al., 2017; Ott et al., 2018; Liu et al., 2019). We use the fairseq toolkit (Ott et al., 2019) for the entire set of experiments. Every reported BLEU score is measured using SacreBLEU (Post, 2018). 3.2 Subword Size For En↔De, we used the subword size of 32,000, which is c"
2020.wmt-1.12,N19-1423,0,0.0146663,"uting the T2S score. Uni-directional Language Model We used the uni-directional language model (UniLM) to compute the likelihood of the decoded target sequence. To do this, we trained the Transformer-based language model (Baevski and Auli, 2019) for all languages on monolingual data. We obtained two distinct scores from two normalization methods: (1) simply dividing by the target sequence length (Yee et al., 2019) and (2) SLOR (Pauls and Klein, 2012; Lau et al., 2020). A list of hyperparameters is shown in Table 1. Masked Language Model We also used the pretrained masked language model (MLM) (Devlin et al., 2019) for computing the score. Specifically, we trained the RoBERTa-base (Liu et al., 2019) setting available in fairseq on monolingual data. First, we computed the unnormalized log-probabilities by the method described by Wang and Cho (2019). Then, we normalized the probability by (1) dividing by the sequence length and (2) PenLP (Vaswani et al., 2017; Lau et al., 2020). A list of hyperparameters is shown in Table 1. Because the uni-directional language model and MLM both have two distinct variations, we used a total of six models, namely, J = 6. 3.8 Post-processing We converted the decoded target"
2020.wmt-1.12,W19-5321,0,0.0128485,"ize to take advantage of a massive amount of training data. Specifically, we increased the number of layers l from 6 to 9 and 12 (Wang et al., 2019). Table 4 shows that the performances of BASE (l = 9)+TAGGED -BT and BASE (l = 12)+TAGGED -BT are almost comparable. We determined that BASE (l = 9)+TAGGED -BT is the best option by considering the model performance and training efficiency regarding the GPU memory constraints. 3.4 Fine-tuning Fine-tuning the model with an in-domain news corpus is acknowledged as an extremely important technique for boosting the performance (Sennrich et al., 2016b; Junczys-Dowmunt, 2019; Ng et al., 2019; Bawden et al., 2019). We fine-tuned our models as follows: En↔De For En↔De, we fine-tuned the model with a collection of newstest2008-2018 and evaluated its performance on newstest2019. For En→De, we only used sentence pairs whose source sentence is originally written in English, i.e., we never used texts with translationese on the source side for finetuning. Similarly, for De→En, we used sentence pairs whose source sentence is originally written in German. This way, we ensured that our model does not overfit to the translationese texts; since newstest2019 does not contain t"
2020.wmt-1.12,D18-1449,0,0.0247216,"ers in the source sequence are sometimes translated into hUNKi. We handled hUNKi in the decoded sequence by copying the corresponding token from the source sequence. We determined the corresponding token by finding the token that does not exist in one of the source-side or targetside vocabularies. En→Ja We did not take any special measures for hUNKi5 . We replaced the English style comma “，” and period “．” with the Japanese style “、” and “。” respectively. Ja→En We observed that the model translates the Japanese vertical bar “｜” to hUNKi. Thus, we replaced all hUNKi with “|”. 3.9 Post-ensemble Kobayashi (2018) proposed the method of taking the ensemble of multiple models after decoding the sequence, namely, post-ensemble (P OST E NSEMBLE). The underlying idea of P OST E NSEMBLE is to choose “majority-like” candidates by comparing the similarities among candidates. He applied P OST E NSEMBLE to the abstractive summarization task and reported that the performance is superior to that of the conventional ensemble. We used P OST E NSEMBLE in En→Ja6 . Specifically, we adopted the PostCosE variant in which the cosine similarity is used as a similarity metric. We created 300 dim fasttext word vectors (Boja"
2020.wmt-1.12,Q17-1010,0,0.110035,"018) proposed the method of taking the ensemble of multiple models after decoding the sequence, namely, post-ensemble (P OST E NSEMBLE). The underlying idea of P OST E NSEMBLE is to choose “majority-like” candidates by comparing the similarities among candidates. He applied P OST E NSEMBLE to the abstractive summarization task and reported that the performance is superior to that of the conventional ensemble. We used P OST E NSEMBLE in En→Ja6 . Specifically, we adopted the PostCosE variant in which the cosine similarity is used as a similarity metric. We created 300 dim fasttext word vectors (Bojanowski et al., 2017) on the Japanese monolingual corpus. 149 5 In fact, we never observed hUNKi in the decoded test set. Kobayashi (2018) introduced P OST E NSEMBLE as the method that replaces the conventional ensemble. Instead, we used two ensemble methods simultaneously. 6 4 Results Performance on the Validation Set We show the validation performance of our system in Table 5. We used newstest2019 and the official validation set for En↔De and En↔Ja, respectively, for the validation data. The table shows the effectiveness of incorporating each technique described in Section 3. Each technique consistently improves"
2020.wmt-1.12,P07-2045,0,0.00918631,"c Online, November 19–20, 2020. 2020 Association for Computational Linguistics 2 Dataset and Preprocessing 2.1 pora. These corpora are used for large-scale backtranslation (Section 3.3). Bitext For both En↔De and En↔Ja, we used all bitexts that are available for a constrained system. En↔De Following Ng et al. (2019), we applied language identification filtering (langid)1 to the bitext. In this filtering, sentence pairs were removed if a supposedly English/German sentence is identified as a non-English/German sentence. Then, we applied the clean-corpus-n script available in the Moses toolkit (Koehn et al., 2007) and removed sentence pairs that are either too long and/or their length ratio is too large2 . These two filtering processes provided us with approximately 44M sentence pairs. Then, we trained and applied the Moses truecaser independently for each language. We also trained byte-pair encoding (BPE) (Sennrich et al., 2016c) models using the sentencepiece (Kudo and Richardson, 2018) implementation. For BPE training, we used only a subset of the parallel corpus (Europarl, NewsCommentary, and RAPID) to prevent extremely rare characters from contaminating the vocabulary and the subword segmentation."
2020.wmt-1.12,W18-6315,0,0.0196798,"to improve the performance over the baseline (r = 100). One of the possible reasons for this ineffectiveness is the quality of the sentence embeddings used for PHSIC. That is, the use of fasttext and pretrained MLM might be inappropriate. Utilizing more powerful sentence encoders such as SentenceBERT (Reimers and Gurevych, 2019) and Universal Sentence Encoder (Cer et al., 2018) is an interesting option to explore in the future; however, the methods of acquiring such resources in the constrained setting is not trivial. 151 Effectiveness of Incorporating Forward-Translation Forward-translation (Burlot and Yvon, 2018) is a technique similar to back-translation; the difference is that while back-translation uses the targetside monolingual data, forward-translation uses the source-side monolingual data to generate synthetic data. Bogoychev and Sennrich (2019) reported that forward-translation is effective for improving the translation of texts that are originally written in the source language (i.e., non-translationese texts). To determine if we can take the best of the two techniques, namely, forward-translation and backtranslation, we combined the synthetic data and trained the model. As described in Secti"
2020.wmt-1.12,W19-5206,0,0.0227968,"searched for an effective setting for incorporating the synthetic data. As the most straightforward starting point, we simply combined bitext and synthetic data and trained the model. Here, we upsampled the bitext so that the model sees the bitext and synthetic data at a 1:1 ratio (Ng et al., 2019). Table 4 shows the result. Here, naively using the synthetic data (BASE+BT) decreased the performance of the model trained with the bitext only (BASE). Given this result, we considered the following two enhancements: Tagged Back-translation We used the tagged back-translation technique (Tagged-BT) (Caswell et al., 2019), which prepends a special tag token (e.g., hBTi) to the source sentence of synthetic data. This simple technique can inform the model about the origin of the given training data, i.e., whether the sentence pair is back-translated. Marie et al. (2020) empirically demonstrated that the model trained with such tagged data can avoid overfitting to the synthetic data. In Table 4, the Tagged-BT (BASE+TAGGED -BT) successfully improves the performance from BASE except for the newstest2019. We suspect that the performance does not improve on newstest2019 because it does not contain the “translationese"
2020.wmt-1.12,D18-2012,0,0.0160596,"to the bitext. In this filtering, sentence pairs were removed if a supposedly English/German sentence is identified as a non-English/German sentence. Then, we applied the clean-corpus-n script available in the Moses toolkit (Koehn et al., 2007) and removed sentence pairs that are either too long and/or their length ratio is too large2 . These two filtering processes provided us with approximately 44M sentence pairs. Then, we trained and applied the Moses truecaser independently for each language. We also trained byte-pair encoding (BPE) (Sennrich et al., 2016c) models using the sentencepiece (Kudo and Richardson, 2018) implementation. For BPE training, we used only a subset of the parallel corpus (Europarl, NewsCommentary, and RAPID) to prevent extremely rare characters from contaminating the vocabulary and the subword segmentation. En↔Ja Similar to En↔De, we applied langid to clean bitext, but we did not use clean-corpus-n since the Japanese text is not segmented. Instead, we simply removed sentence pairs in which the English sentence is longer than 500 tokens. Eventually, we obtained about 17M sentence pairs. We used truecaser for the English side only, because case information does not exist in the Japan"
2020.wmt-1.12,D18-2029,0,0.0248336,"Missing"
2020.wmt-1.12,N16-1046,0,0.018914,"erformance. First, we trained four models with different random seeds. These models were then simultaneously used for computing the score of each candidate during the beam search decoding. 3.6 Right-to-Left Models We used Right-to-Left (R2L) models for reranking the n-best candidates from Left-to-Right (L2R) models. R2L models generate sentences in reverse order. Suppose that conventional L2R models generate sentences from the beginning-of-the-sentence (BOS) to the end-of-the-sentence (EOS); R2L models generate from EOS to BOS. This reranking technique was independently proposed by Liu et al. (2016) and Sennrich et al. (2016a) to mitigate the search error of L2R models, which may occur around EOS. We trained four R2L models and used their scores for reranking the n-best candidates generated by L2R models (Section 3.5). Specifically, we computed the score of each candidate with both L2R models and R2L models. Then, we took the sum of the two scores and obtained the final score. We sorted this final score and then selected the candidate with the highest score. 3.7 Reranking We also applied a reranking method based on the scores of several translation (or generative) models, which is closel"
2020.wmt-1.12,2021.ccl-1.108,0,0.0475572,"Missing"
2020.wmt-1.12,2020.acl-main.532,0,0.14531,"ynthetic data at a 1:1 ratio (Ng et al., 2019). Table 4 shows the result. Here, naively using the synthetic data (BASE+BT) decreased the performance of the model trained with the bitext only (BASE). Given this result, we considered the following two enhancements: Tagged Back-translation We used the tagged back-translation technique (Tagged-BT) (Caswell et al., 2019), which prepends a special tag token (e.g., hBTi) to the source sentence of synthetic data. This simple technique can inform the model about the origin of the given training data, i.e., whether the sentence pair is back-translated. Marie et al. (2020) empirically demonstrated that the model trained with such tagged data can avoid overfitting to the synthetic data. In Table 4, the Tagged-BT (BASE+TAGGED -BT) successfully improves the performance from BASE except for the newstest2019. We suspect that the performance does not improve on newstest2019 because it does not contain the “translationese” text, i.e., human-generated translations, which are reported to be the main source of improvement of backtokens, or the Japanese sentence is longer than 500 characters. 147 Translation Task (KFTT) corpus and NewsCommentary as the clean bitext and Ne"
2020.wmt-1.12,W19-5333,0,0.243356,"task, such as back-translation and fine-tuning. The overview of our system is shown in Figure 1. We achieved the first place in De→En on automatic evaluation and obtained strong results in other language directions. 145 Proceedings of the 5th Conference on Machine Translation (WMT), pages 145–155 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics 2 Dataset and Preprocessing 2.1 pora. These corpora are used for large-scale backtranslation (Section 3.3). Bitext For both En↔De and En↔Ja, we used all bitexts that are available for a constrained system. En↔De Following Ng et al. (2019), we applied language identification filtering (langid)1 to the bitext. In this filtering, sentence pairs were removed if a supposedly English/German sentence is identified as a non-English/German sentence. Then, we applied the clean-corpus-n script available in the Moses toolkit (Koehn et al., 2007) and removed sentence pairs that are either too long and/or their length ratio is too large2 . These two filtering processes provided us with approximately 44M sentence pairs. Then, we trained and applied the Moses truecaser independently for each language. We also trained byte-pair encoding (BPE)"
2020.wmt-1.12,P03-1021,0,0.248609,"odels, which may occur around EOS. We trained four R2L models and used their scores for reranking the n-best candidates generated by L2R models (Section 3.5). Specifically, we computed the score of each candidate with both L2R models and R2L models. Then, we took the sum of the two scores and obtained the final score. We sorted this final score and then selected the candidate with the highest score. 3.7 Reranking We also applied a reranking method based on the scores of several translation (or generative) models, which is closely related to one iteration of Minimum Error Rate Training (MERT) (Och, 2003) often used in Statistical Machine Translation (SMT). The underlying idea is to find the balance of likelihood independently computed from the models. Suppose we have a set of candidate output sentences for each input in either the validation (training phase) or the test (evaluation phase) sets. In our case, we independently generated n-best candidates using the L2R and R2L models, and obtained 2n candidates in total for each. Here, let Ci represent the set of the obtained 2n candidates of the 148 i-th input. Next, Pj (e) ∈ [0, 1] denotes the score of the candidate e ∈ Ci obtained from the j-t"
2020.wmt-1.12,N19-4009,0,0.0167779,"ble 1 shows a list of hyperparameters for model optimization. We employed an extremely large mini-batch size of 512,000 tokens using the delaying gradient update technique (Bogoychev et al., 2018; Ott et al., 2018). This is because previous studies showed that a large mini-batch size leads to a faster convergence (Ott et al., 2018) and a better generalization (Popel and Bojar, 2018; Bawden et al., 2019; Morishita et al., 2019). We also used a large learning rate of 0.001 to further accelerate the convergence (Goyal et al., 2017; Ott et al., 2018; Liu et al., 2019). We use the fairseq toolkit (Ott et al., 2019) for the entire set of experiments. Every reported BLEU score is measured using SacreBLEU (Post, 2018). 3.2 Subword Size For En↔De, we used the subword size of 32,000, which is commonly used in previous studies (Vaswani et al., 2017; Ng et al., 2019). For En↔Ja, we conducted a hyperparameter search for a suitable subword size; Morishita et al. (2019) empirically showed that a small subword size (e.g., 4,000) is superior to those commonly adopted in the literature (e.g., 16,000 and 32,000). Given their findings, we searched for the subword size in the following range: {4000, 8000, 16000, 32000}"
2020.wmt-1.12,D19-1410,0,0.0198575,"Missing"
2020.wmt-1.12,P16-1009,0,0.434128,"we applied language identification filtering (langid)1 to the bitext. In this filtering, sentence pairs were removed if a supposedly English/German sentence is identified as a non-English/German sentence. Then, we applied the clean-corpus-n script available in the Moses toolkit (Koehn et al., 2007) and removed sentence pairs that are either too long and/or their length ratio is too large2 . These two filtering processes provided us with approximately 44M sentence pairs. Then, we trained and applied the Moses truecaser independently for each language. We also trained byte-pair encoding (BPE) (Sennrich et al., 2016c) models using the sentencepiece (Kudo and Richardson, 2018) implementation. For BPE training, we used only a subset of the parallel corpus (Europarl, NewsCommentary, and RAPID) to prevent extremely rare characters from contaminating the vocabulary and the subword segmentation. En↔Ja Similar to En↔De, we applied langid to clean bitext, but we did not use clean-corpus-n since the Japanese text is not segmented. Instead, we simply removed sentence pairs in which the English sentence is longer than 500 tokens. Eventually, we obtained about 17M sentence pairs. We used truecaser for the English si"
2020.wmt-1.12,P16-1162,0,0.527926,"we applied language identification filtering (langid)1 to the bitext. In this filtering, sentence pairs were removed if a supposedly English/German sentence is identified as a non-English/German sentence. Then, we applied the clean-corpus-n script available in the Moses toolkit (Koehn et al., 2007) and removed sentence pairs that are either too long and/or their length ratio is too large2 . These two filtering processes provided us with approximately 44M sentence pairs. Then, we trained and applied the Moses truecaser independently for each language. We also trained byte-pair encoding (BPE) (Sennrich et al., 2016c) models using the sentencepiece (Kudo and Richardson, 2018) implementation. For BPE training, we used only a subset of the parallel corpus (Europarl, NewsCommentary, and RAPID) to prevent extremely rare characters from contaminating the vocabulary and the subword segmentation. En↔Ja Similar to En↔De, we applied langid to clean bitext, but we did not use clean-corpus-n since the Japanese text is not segmented. Instead, we simply removed sentence pairs in which the English sentence is longer than 500 tokens. Eventually, we obtained about 17M sentence pairs. We used truecaser for the English si"
2020.wmt-1.12,W18-6301,0,0.029031,"The well-known Transformer model (Vaswani et al., 2017) is our base Encoder Decoder model. Specifically, we started with the “Transformer (big)” setting described by Vaswani et al. (2017) and increased the feed-forward network (FFN) size from 4,096 to 8,192. Ng et al. (2019) reported that this larger FFN setting slightly improves the performance; we also confirmed it in our preliminary experiment. Table 1 shows a list of hyperparameters for model optimization. We employed an extremely large mini-batch size of 512,000 tokens using the delaying gradient update technique (Bogoychev et al., 2018; Ott et al., 2018). This is because previous studies showed that a large mini-batch size leads to a faster convergence (Ott et al., 2018) and a better generalization (Popel and Bojar, 2018; Bawden et al., 2019; Morishita et al., 2019). We also used a large learning rate of 0.001 to further accelerate the convergence (Goyal et al., 2017; Ott et al., 2018; Liu et al., 2019). We use the fairseq toolkit (Ott et al., 2019) for the entire set of experiments. Every reported BLEU score is measured using SacreBLEU (Post, 2018). 3.2 Subword Size For En↔De, we used the subword size of 32,000, which is commonly used in pre"
2020.wmt-1.12,W19-2304,0,0.015061,", 2019) for all languages on monolingual data. We obtained two distinct scores from two normalization methods: (1) simply dividing by the target sequence length (Yee et al., 2019) and (2) SLOR (Pauls and Klein, 2012; Lau et al., 2020). A list of hyperparameters is shown in Table 1. Masked Language Model We also used the pretrained masked language model (MLM) (Devlin et al., 2019) for computing the score. Specifically, we trained the RoBERTa-base (Liu et al., 2019) setting available in fairseq on monolingual data. First, we computed the unnormalized log-probabilities by the method described by Wang and Cho (2019). Then, we normalized the probability by (1) dividing by the sequence length and (2) PenLP (Vaswani et al., 2017; Lau et al., 2020). A list of hyperparameters is shown in Table 1. Because the uni-directional language model and MLM both have two distinct variations, we used a total of six models, namely, J = 6. 3.8 Post-processing We converted the decoded target sequence from a sequence of subwords to tokens. Then we applied the Moses detruecaser to English and German sequences. We also applied language-specific postprocessing as follows: En↔De We observed that the rare tokens such as Greek let"
2020.wmt-1.12,P12-1101,0,0.0317617,"at is, it translates a given target sequence to a source sequence. For example, if a candidate sentence is generated by the En→De model, we use the De→En model for computing the T2S score. Uni-directional Language Model We used the uni-directional language model (UniLM) to compute the likelihood of the decoded target sequence. To do this, we trained the Transformer-based language model (Baevski and Auli, 2019) for all languages on monolingual data. We obtained two distinct scores from two normalization methods: (1) simply dividing by the target sequence length (Yee et al., 2019) and (2) SLOR (Pauls and Klein, 2012; Lau et al., 2020). A list of hyperparameters is shown in Table 1. Masked Language Model We also used the pretrained masked language model (MLM) (Devlin et al., 2019) for computing the score. Specifically, we trained the RoBERTa-base (Liu et al., 2019) setting available in fairseq on monolingual data. First, we computed the unnormalized log-probabilities by the method described by Wang and Cho (2019). Then, we normalized the probability by (1) dividing by the sequence length and (2) PenLP (Vaswani et al., 2017; Lau et al., 2020). A list of hyperparameters is shown in Table 1. Because the uni-"
2020.wmt-1.12,P19-1176,0,0.0158905,"of this two-step procedure is slightly better than that of the fine-tuning with the news bitext only. newstest Setting 2014 2018 2019 BASE BASE+BT BASE+TAGGED -BT BASE (l = 9)+TAGGED -BT BASE (l = 12)+TAGGED -BT 32.2 32.1 33.0 33.1 33.4 47.3 45.9 48.0 49.6 49.4 42.2 38.8 42.0 42.7 42.3 Table 4: Effectiveness of using the synthetic data on En→De 3.5 translation (Bogoychev and Sennrich, 2019; Marie et al., 2020). Deeper Model We also considered increasing the model size to take advantage of a massive amount of training data. Specifically, we increased the number of layers l from 6 to 9 and 12 (Wang et al., 2019). Table 4 shows that the performances of BASE (l = 9)+TAGGED -BT and BASE (l = 12)+TAGGED -BT are almost comparable. We determined that BASE (l = 9)+TAGGED -BT is the best option by considering the model performance and training efficiency regarding the GPU memory constraints. 3.4 Fine-tuning Fine-tuning the model with an in-domain news corpus is acknowledged as an extremely important technique for boosting the performance (Sennrich et al., 2016b; Junczys-Dowmunt, 2019; Ng et al., 2019; Bawden et al., 2019). We fine-tuned our models as follows: En↔De For En↔De, we fine-tuned the model with a c"
2020.wmt-1.12,W15-3049,0,0.0556296,"Missing"
2020.wmt-1.12,W18-6319,0,0.0132032,"of 512,000 tokens using the delaying gradient update technique (Bogoychev et al., 2018; Ott et al., 2018). This is because previous studies showed that a large mini-batch size leads to a faster convergence (Ott et al., 2018) and a better generalization (Popel and Bojar, 2018; Bawden et al., 2019; Morishita et al., 2019). We also used a large learning rate of 0.001 to further accelerate the convergence (Goyal et al., 2017; Ott et al., 2018; Liu et al., 2019). We use the fairseq toolkit (Ott et al., 2019) for the entire set of experiments. Every reported BLEU score is measured using SacreBLEU (Post, 2018). 3.2 Subword Size For En↔De, we used the subword size of 32,000, which is commonly used in previous studies (Vaswani et al., 2017; Ng et al., 2019). For En↔Ja, we conducted a hyperparameter search for a suitable subword size; Morishita et al. (2019) empirically showed that a small subword size (e.g., 4,000) is superior to those commonly adopted in the literature (e.g., 16,000 and 32,000). Given their findings, we searched for the subword size in the following range: {4000, 8000, 16000, 32000}. Table 2 shows that the largest subword size achieves the best performance, which is inconsistent wit"
2020.wmt-1.12,W18-6489,0,0.0210067,"reated in Section 3.3. The goal of this filtering is to extract and utilize the “clean” subset of synthetic data that may contribute to the model performance. 7 https://ocelot.mteval.org/ The use of the MeCab-based segmentation is recommended by SacreBLEU. 8 Training a Classifier We trained a linear support vector machine model that classifies clean and noisy sentence pairs. To train the classifier, we used newstest2009-2019 and the official validation set as clean sentence pairs for En↔De and En↔Ja, respectively. We generated the noisy sentence pairs by randomly adding the noise presented by Wang et al. (2018) to the clean sentence pairs. 150 ID Setting (a) (b) (c) (d) (e) (f) - En→De De→En En→Ja Ja→En BASE (Section 3.1) BASE (l = 9)+TAGGED -BT (Section 3.3) (b) + fine-tuning (Section 3.4) (c) × 4 (Section 3.5) (d) + 4 × (c)-R2L (Section 3.6) (e) + reranking (Section 3.7) 42.4 42.7 44.9 45.5 45.4 45.7 42.0 42.5 42.3 42.8 43.6 43.8 19.7 22.0 23.1 23.9 24.2 24.9 21.6 23.9 24.4 25.4 25.9 26.2 The best system in WMT’19 44.9 42.8 - - Table 5: Effectiveness of each technique: we use newstest2019 and official validation set for En↔De and En↔Ja respectively. The best result from WMT’19 is unavailable for E"
2020.wmt-1.12,D19-1571,0,0.0266441,"Missing"
2020.wmt-1.12,D18-1203,1,0.81503,"Finally, on the basis of the confidence scores of the classifier, we extracted the presumably clean subset of the synthetic data. 5.1.1 Features In this section, we introduce several negative results from our preliminary experiments. Our attempts include the following: (1) filtering synthetic data, (2) incorporating forward-translation, and (3) developing a more sophisticated reranking method. We also analyze the issue regarding the use of brackets in the En→Ja task. Pointwise HSIC We computed the score for each sentence pair using the pointwise HilbertSchmidt independence criterion (PHSIC) (Yokoi et al., 2018), which is a kernel-based co-occurrence measure. Given a set of sentence pairs, PHSIC can assign a high score to a sentence pair that is consistent with the rest of the sentence pairs. To do this, PHSIC utilizes kernel functions and calculates the sentence similarity. Yokoi et al. (2018) applied PHSIC to machine translation corpus filtering and reported promising results. Thus, we also employed PHSIC for synthetic data filtering. First, we learned the parameters of the PHSIC matrix with a cosine kernel by using all sentence pairs in the bitext, which are represented as sentence embeddings. The"
2021.eacl-main.214,W19-6721,0,0.0143792,"✓, ) + LMB ( ), To use inter-sentence information, we modify the NMT model by adding MBE to the input: y = f (x, z; ✓). B2D0 (x,y)2B (2) where e is a hidden dimension of the NMT model. We use mean pooling2 to make both sentence embeddings vi and MBE z. By adopting this procedure, we expect MBE z to have inter-sentence context features, which is desirable for a contextaware NMT. Note that we ignore the order of sentences in a document. This is a beneficial trait because this method is also applicable to corpora with document boundaries but without in-document sentence order, such as ParaCrawl (Esplà et al., 2019). 3.2 We concatenated MBE to the input word embeddings, and the model uses MBE as the first input token (Fig. 1). Now the encoder/decoder takes s+1 and t + 1 embeddings. The Transformer encoder layer for MBE was jointly trained with the NMT model by modifying the loss function in Eq. (1): X X LNMT (✓, ) = log P (y|x, B; ✓, ), (7) where is a hyperparameter used to control the weight of the classifier loss. We use the value predicted by the classifier as a gate. Our new weighted MBE is z˜ = ↵z, (8) where ↵ = P (d = 1|z), and we change z in Eqs. (3) to z˜. 2514 <pad> Transformer encoder <pad> Tra"
2021.eacl-main.214,D18-1325,0,0.076055,"l., 2015; Vaswani et al., 2017) translate sentences in a sentence-by-sentence manner. However, some have argued that it is critical to consider the inter-sentence context in handling discourse phenomena (Hardmeier, 2012), which include coherence, cohesion, coreference (Bawden et al., 2018; Nagata and Morishita, 2020), and writing style (Yamagishi et al., 2016). To correctly translate these linguistic features, some works provide additional context information to an NMT model by concatenating the previous sentence (Tiedemann and Scherrer, 2017), applying a context encoder (Bawden et al., 2018; Miculicich et al., 2018; Voita et al., 2018), or using a cachebased network (Tu et al., 2018; Kuang et al., 2018). Most of the previous studies have considered only a few previous context sentences. Several methods, such as the cache-based network, consider long-range context but heavily modify the standard NMT models and require additional training/decoding steps. Our goal is to make a simple but effective context-aware NMT model, which does not require heavy modification to standard NMT models and can handle a wider inter-sentence context. To this end, we propose a method to create an embedding that represents the"
2021.eacl-main.214,2020.lrec-1.443,1,0.759612,"6 Enc-Layers but uses MBE in the encoder. 3 Since our training data have document boundaries but the in-document sentence orders were shuffled, we randomly selected one in-document sentence and used it as previous context. For dev/test sets, we used the original sentence order. MBE Enc w/o Gate resembles MBE Enc, but it does not use the MBE gate described in Section 3.3. MBE Dec uses MBE in the decoder. MBE Enc/Dec uses MBE in both the encoder and the decoder. 4.2 Experimental Settings Datasets/Evaluation We trained JapaneseEnglish NMT models. As training data, we used the JParaCrawl corpus (Morishita et al., 2020). JParaCrawl was created by crawling the web and aligning parallel sentences, and each sentence-pair has a URL from which the sentences were taken. In this experiment, we regarded the sentences from the same URL as a document. We used several test sets with document boundaries: (i) scientific paper excerpts (ASPEC (Nakazawa et al., 2016)), (ii) news (newsdev2020 from WMT20 news translation shared task4 ), and (iii) TED talks (tst2012 from IWSLT translation shared task (Cettolo et al., 2012)). As a dev set to tune the NMT model, we used the ASPEC dev split. See Section A.1 in the Appendix for c"
2021.eacl-main.214,W18-6301,0,0.0495336,"Missing"
2021.eacl-main.214,P02-1040,0,0.109482,"his experiment, we regarded the sentences from the same URL as a document. We used several test sets with document boundaries: (i) scientific paper excerpts (ASPEC (Nakazawa et al., 2016)), (ii) news (newsdev2020 from WMT20 news translation shared task4 ), and (iii) TED talks (tst2012 from IWSLT translation shared task (Cettolo et al., 2012)). As a dev set to tune the NMT model, we used the ASPEC dev split. See Section A.1 in the Appendix for corpus statistics and detailed preprocessing steps. To evaluate the translation performance, we used sacreBLEU5 (Post, 2018) and report the BLEU scores (Papineni et al., 2002). Model Configurations We used the Transformer model as an NMT model (Vaswani et al., 2017). Our hyperparameters were based on the “big” settings defined by Vaswani et al. (2017). For the MBE experiments, we set in Eq. (7) to 1.0. We set the mini-batch size to 3,000 tokens. If the tokens in a document were larger than this size, we 4 We used newsdev2020 as a test set because no official test set for English-Japanese was available at the time of writing. Since we did not use newsdev2020 for tuning the model, there is no problem with using it as a test set. 5 The signature is BLEU+case.mixed+lan"
2021.eacl-main.214,W18-6319,0,0.0117096,"rom which the sentences were taken. In this experiment, we regarded the sentences from the same URL as a document. We used several test sets with document boundaries: (i) scientific paper excerpts (ASPEC (Nakazawa et al., 2016)), (ii) news (newsdev2020 from WMT20 news translation shared task4 ), and (iii) TED talks (tst2012 from IWSLT translation shared task (Cettolo et al., 2012)). As a dev set to tune the NMT model, we used the ASPEC dev split. See Section A.1 in the Appendix for corpus statistics and detailed preprocessing steps. To evaluate the translation performance, we used sacreBLEU5 (Post, 2018) and report the BLEU scores (Papineni et al., 2002). Model Configurations We used the Transformer model as an NMT model (Vaswani et al., 2017). Our hyperparameters were based on the “big” settings defined by Vaswani et al. (2017). For the MBE experiments, we set in Eq. (7) to 1.0. We set the mini-batch size to 3,000 tokens. If the tokens in a document were larger than this size, we 4 We used newsdev2020 as a test set because no official test set for English-Japanese was available at the time of writing. Since we did not use newsdev2020 for tuning the model, there is no problem with using it as"
2021.eacl-main.214,D19-1081,0,0.136274,"yer, this model has a comparable number of parameters as the following MBE models. 2-to-1 is the context-aware translation model proposed by Tiedemann and Scherrer (2017) that translates a pair of previous and current source sentences into a target sentence. Two source sentences are concatenated with a special sentence boundary token. This method is known as a strong baseline for context-aware NMT (Bawden et al., 2018; Voita et al., 2018). Other settings are identical to those of Baseline 6 EncLayers3 . DocRepair is another recent context-aware translation model, which uses two-step decoding (Voita et al., 2019). The first step generates 1-best translation with a sentence-level NMT model given a single sentence. The second step generates document-level translation given 1best translations of four consecutive sentences concatenated with a special token. We compared our proposed methods with the following settings: MBE Enc resembles Baseline 6 Enc-Layers but uses MBE in the encoder. 3 Since our training data have document boundaries but the in-document sentence orders were shuffled, we randomly selected one in-document sentence and used it as previous context. For dev/test sets, we used the original se"
2021.eacl-main.214,P18-1117,0,0.0742115,"2017) translate sentences in a sentence-by-sentence manner. However, some have argued that it is critical to consider the inter-sentence context in handling discourse phenomena (Hardmeier, 2012), which include coherence, cohesion, coreference (Bawden et al., 2018; Nagata and Morishita, 2020), and writing style (Yamagishi et al., 2016). To correctly translate these linguistic features, some works provide additional context information to an NMT model by concatenating the previous sentence (Tiedemann and Scherrer, 2017), applying a context encoder (Bawden et al., 2018; Miculicich et al., 2018; Voita et al., 2018), or using a cachebased network (Tu et al., 2018; Kuang et al., 2018). Most of the previous studies have considered only a few previous context sentences. Several methods, such as the cache-based network, consider long-range context but heavily modify the standard NMT models and require additional training/decoding steps. Our goal is to make a simple but effective context-aware NMT model, which does not require heavy modification to standard NMT models and can handle a wider inter-sentence context. To this end, we propose a method to create an embedding that represents the contextual informati"
2021.eacl-main.214,W16-4620,0,0.0227936,"erforms the translation capabilities of strong baselines and improves writing style or terminology to fit the document’s context.1 1 Introduction Current standard neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) translate sentences in a sentence-by-sentence manner. However, some have argued that it is critical to consider the inter-sentence context in handling discourse phenomena (Hardmeier, 2012), which include coherence, cohesion, coreference (Bawden et al., 2018; Nagata and Morishita, 2020), and writing style (Yamagishi et al., 2016). To correctly translate these linguistic features, some works provide additional context information to an NMT model by concatenating the previous sentence (Tiedemann and Scherrer, 2017), applying a context encoder (Bawden et al., 2018; Miculicich et al., 2018; Voita et al., 2018), or using a cachebased network (Tu et al., 2018; Kuang et al., 2018). Most of the previous studies have considered only a few previous context sentences. Several methods, such as the cache-based network, consider long-range context but heavily modify the standard NMT models and require additional training/decoding s"
2021.eacl-main.214,D19-1410,0,0.0129684,"ranslate the mini-batch or the MBE fails to contain important information for translation. To deal with such cases, we aim to make the model estimate how important MBE is for each mini-batch. Thus we added a mini-batch embedding gate to determine MBE’s importance. In this setting, we prepared two types of minibatches for training: (i) sentences from the same document and (ii) sentences from different documents. Then we trained a binary classifier that predicts whether the sentences in the mini-batch are selected from the same document: P (d|z) = softmax(W z), (3) This approach was inspired by Reimers and Gurevych (2019), who successfully created sentence embeddings from BERT embeddings (Devlin et al., 2019) by mean pooling. (5) where W 2 R2⇥e is a parameter matrix and d is a binary value that takes 1 if the sentences in the mini-batch are selected from the same document. To train the classifier, we minimize the loss function: X LMB ( ) = log P (d|B; ), (6) (d,B)2D0 where is a set of parameters for the classifier. For training, we mix the two types of mini-batches at the same ratio. Concretely, we jointly minimize the NMT and the classifier loss functions: L(✓, , ) = LNMT (✓, ) + LMB ( ), To use inter-sentenc"
2021.eacl-main.214,P16-1009,0,0.0297422,"requires only a small computational cost. NMT with Tags We used an MBE as the first input of the encoder/decoder. Our approach is similar to the work that uses special tags to control or provide additional information to NMT (Johnson et al., 2016; Takeno et al., 2017; Caswell et al., 2019). Johnson et al. (2016) added tags to a source sentence for indicating the target language in multilingual NMT models. Takeno et al. (2017) proposed a method that controls the target length or the domain by adding a tag to the decoder inputs. Caswell et al. (2019) used a tag to indicate the synthetic corpus (Sennrich et al., 2016). Our work, which automatically generates a tag (MBE) with the sentence in a mini-batch and uses a gate to control the importance of MBE, is different from the previous studies. 6 Conclusion We proposed mini-batch embedding (MBE), which is a simple but effective method to represent contextual information across documents. We incorporated MBE in the NMT model, which enabled it to outperform competitive baselines. We found that our NMT model could choose the appropriate word and writing style to match the document context. An analysis showed that our model’s performance improves with a large con"
2021.eacl-main.214,W17-5702,1,0.830148,"ment-level model that outputs document-level translation. Although this is a promising method, it requires training of three sequence-to-sequence models to translate a single direction and needs two decoding steps, which slows down the translation. Our method has an advantage in that it only trains a single model and uses single-step decoding, which requires only a small computational cost. NMT with Tags We used an MBE as the first input of the encoder/decoder. Our approach is similar to the work that uses special tags to control or provide additional information to NMT (Johnson et al., 2016; Takeno et al., 2017; Caswell et al., 2019). Johnson et al. (2016) added tags to a source sentence for indicating the target language in multilingual NMT models. Takeno et al. (2017) proposed a method that controls the target length or the domain by adding a tag to the decoder inputs. Caswell et al. (2019) used a tag to indicate the synthetic corpus (Sennrich et al., 2016). Our work, which automatically generates a tag (MBE) with the sentence in a mini-batch and uses a gate to control the importance of MBE, is different from the previous studies. 6 Conclusion We proposed mini-batch embedding (MBE), which is a sim"
2021.eacl-main.214,W17-4811,0,0.401327,"nslation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) translate sentences in a sentence-by-sentence manner. However, some have argued that it is critical to consider the inter-sentence context in handling discourse phenomena (Hardmeier, 2012), which include coherence, cohesion, coreference (Bawden et al., 2018; Nagata and Morishita, 2020), and writing style (Yamagishi et al., 2016). To correctly translate these linguistic features, some works provide additional context information to an NMT model by concatenating the previous sentence (Tiedemann and Scherrer, 2017), applying a context encoder (Bawden et al., 2018; Miculicich et al., 2018; Voita et al., 2018), or using a cachebased network (Tu et al., 2018; Kuang et al., 2018). Most of the previous studies have considered only a few previous context sentences. Several methods, such as the cache-based network, consider long-range context but heavily modify the standard NMT models and require additional training/decoding steps. Our goal is to make a simple but effective context-aware NMT model, which does not require heavy modification to standard NMT models and can handle a wider inter-sentence context. T"
2021.eacl-main.214,Q18-1029,0,0.0867773,"manner. However, some have argued that it is critical to consider the inter-sentence context in handling discourse phenomena (Hardmeier, 2012), which include coherence, cohesion, coreference (Bawden et al., 2018; Nagata and Morishita, 2020), and writing style (Yamagishi et al., 2016). To correctly translate these linguistic features, some works provide additional context information to an NMT model by concatenating the previous sentence (Tiedemann and Scherrer, 2017), applying a context encoder (Bawden et al., 2018; Miculicich et al., 2018; Voita et al., 2018), or using a cachebased network (Tu et al., 2018; Kuang et al., 2018). Most of the previous studies have considered only a few previous context sentences. Several methods, such as the cache-based network, consider long-range context but heavily modify the standard NMT models and require additional training/decoding steps. Our goal is to make a simple but effective context-aware NMT model, which does not require heavy modification to standard NMT models and can handle a wider inter-sentence context. To this end, we propose a method to create an embedding that represents the contextual information of a document. To create this embedding, we f"
2021.emnlp-main.266,P19-1285,0,0.1743,"Evaluation We evaluate the performance with sacreBLEU (Post, 2018). Throughout the experiment, we apply the moses detokenizer to the system output and then compute the detokenized BLEU5 . Models We adopt transformer-base (Vaswani et al., 2017) with APE, SHAPE, or RPE, respectively. Our implementations are based on OpenNMT-py (Klein et al., 2017). Unless otherwise stated, we use a fixed value (K = 500) for the maximum shift of SHAPE to demonstrate that SHAPE is robust against the choice of K. We set the relative distance limit in RPE to 16 following Shaw et al. (2018) and Neishi and Yoshinaga (2019)6 . Dataset We used the WMT 2016 EnglishGerman dataset for training and followed Ott et al. (2018) for tokenization and subword segmentation (Sennrich et al., 2016). We used newstest20102013 and newstest2014-2016 as the validation and test sets, respectively. Our experiments consist of the following three distinct dataset settings: (i) VANILLA: Identical to previous studies (Vaswani et al., 2017; Ott et al., 2018). (ii) E XTRAPOLATE: Shift-invariant models are typically evaluated in terms of extrapolation abil- 3.2 Experiment 1: Shift Invariance ity (Wang et al., 2021; Newman et al., 2020). We"
2021.emnlp-main.266,W17-3203,0,0.067057,"Missing"
2021.emnlp-main.266,K19-1031,0,0.387142,"coder models (Transformers) (Vaswani et al., 2017), enabling the self-attention to recognize the order of input sequences. Position representations have two categories (Dufter et al., 2021): absolute position embedding (APE) (Gehring et al., 2017; Vaswani et al., 2017) and relative position embedding (RPE) (Shaw et al., 2018). With APE, each position is represented by a unique embedding, which is added to inputs. RPE represents the position based on the relative distance between two tokens in the self-attention mechanism. RPE outperforms APE on sequence-to-sequence tasks (Narang et al., 2021; Neishi and Yoshinaga, 2019) due to extrapolation, i.e., the ability to generalize to sequences that are longer than those observed during training (Newman et al., 2020). Wang et al. (2021) reported that one of the key properties contributing to RPE’s superior performance is shift invariance2 , the property of a function to not change its output even if its input is shifted. However, unlike APE, RPE’s formulation Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3309–3321 c November 7–11, 2021. 2021 Association for Computational Linguistics 2 Position Representations Figure 1 g"
2021.emnlp-main.266,2020.blackboxnlp-1.26,0,0.22932,"have two categories (Dufter et al., 2021): absolute position embedding (APE) (Gehring et al., 2017; Vaswani et al., 2017) and relative position embedding (RPE) (Shaw et al., 2018). With APE, each position is represented by a unique embedding, which is added to inputs. RPE represents the position based on the relative distance between two tokens in the self-attention mechanism. RPE outperforms APE on sequence-to-sequence tasks (Narang et al., 2021; Neishi and Yoshinaga, 2019) due to extrapolation, i.e., the ability to generalize to sequences that are longer than those observed during training (Newman et al., 2020). Wang et al. (2021) reported that one of the key properties contributing to RPE’s superior performance is shift invariance2 , the property of a function to not change its output even if its input is shifted. However, unlike APE, RPE’s formulation Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3309–3321 c November 7–11, 2021. 2021 Association for Computational Linguistics 2 Position Representations Figure 1 gives an overview of the position representations compared in this paper. We denote a source sequence X as a sequence of I tokens, namely, X ="
2021.emnlp-main.266,W19-5333,0,0.0621577,"Missing"
2021.emnlp-main.266,2020.coling-main.319,0,0.0903647,"Missing"
2021.emnlp-main.266,2020.acl-main.89,0,0.0281093,"Missing"
2021.emnlp-main.266,2020.findings-emnlp.298,0,0.0314762,"2021). However, the self-attention mechanism of RPE involves more computation than that of APE4 . In addition, more importantly, RPE requires the modification of the architecture, while APE does not. Specifically, RPE strongly depends on the self-attention mechanism; thus, it is not necessarily compatible with studies that attempt to replace the self-attention with a more lightweight alternative (Kitaev et al., 2020; Choromanski et al., 2021; Tay et al., 2020). RPE, which was originally proposed by Shaw et al. (2018), has many variants in the literature (Dai et al., 2019; Raffel et al., 2020; Huang et al., 2020; Wang et al., 2021; Wu et al., 2021). They aim to improve the empirical performance or the computational speed compared with the original RPE. However, the original RPE is still a strong method in terms of the performance. Narang et al. (2021) conducted a thorough comparison on multiple sequence-to-sequence tasks and reported that the performance of the original RPE is comparable to or sometimes better than its variants. Thus, we exclusively use the original RPE in our experiments. 2.3 Shifted Absolute Position Embedding (SHAPE) Given the drawbacks of RPE, we investigate SHAPE (Figure 1c) as"
C02-1119,W01-0509,0,\N,Missing
C02-1119,C02-1054,0,\N,Missing
C02-1119,C02-1053,1,\N,Missing
C02-1119,J96-1002,0,\N,Missing
C02-1119,N01-1025,0,\N,Missing
C04-1064,W99-0625,0,\N,Missing
C04-1064,J97-1003,0,\N,Missing
C04-1064,W02-2016,0,\N,Missing
C04-1064,W03-1004,0,\N,Missing
C04-1064,P02-1040,0,\N,Missing
C04-1064,W03-0507,0,\N,Missing
C04-1064,P03-1005,1,\N,Missing
C18-1052,2012.eamt-1.60,0,0.0152173,"r, increasing the memory requirement and runtime is limited, which allows us to run almost the same speed and memory requirement as the baseline system3 . We can simultaneously use several subword features. In Fig. 1, we use both BPE (m =1k) and BPE (m =300) for the encoder side. By adding several subword features, the model can use more information with which we expect to improve the task performance. 4 Experiments 4.1 Setup In this paper, we focused on from/to English (EN) to/from French (FR), German (DE) translations. We carried out our experiments on the IWSLT evaluation campaign dataset (Cettolo et al., 2012), which is based on a TED talk that has been translated into several languages. We used the IWSLT 2016 training set for the training models, tst2012 as the development set, and tst2013 and tst2014 as the test sets. For preprocessing, we used the Moses tokenizer4 and the truecaser5 . For the training set, we removed sentences over 50 words to clean the corpus. Table 1 shows the cleaned corpora statistics on the IWSLT datasets. To split words into subwords, we used the scripts6 provided by Sennrich et al. (2016). As an NMT framework, we used almost the same structure as Luong et al. (2015), exce"
C18-1052,W16-4616,0,0.0249753,"ture (g) + Encoder side BPE 300 feature (g) + Encoder side BPE 1k, 300 features (a) + Decoder side BPE 1k feature (a) + Decoder side BPE 300 feature (a) + Decoder side BPE 1k, 300 features (f) + (n) (k) + (n) Table 3: Compared experimental settings learning rate to 1.0, but after 30 epochs we multiplied it by 0.8 for every epoch and continued training until 40 epochs. For decoding, we performed a beam search with a beam size of 20. To prevent the model from outputting short sentences, we applied the length normalization technique by dividing the negative log-likelihood by the sentence length (Cromieres et al., 2016; Morishita et al., 2017). As evaluation metrics, we used case-sensitive7 BLEU scores (Papineni et al., 2002) using multi-bleu.perl8 . To fix the experimental settings, we carried out a preliminary analysis to find the relation between the sentence length and the vocabulary size (≃ the number of BPE merge operations). Fig. 2 shows the results on the English sentences of the IWSLT 2016 German-English training set. When we reduce the vocabulary size, the average sentence length rapidly increases. Unfortunately, longer sentences require more computational cost and are time-consuming. Thus in our"
C18-1052,W17-3204,0,0.0348867,"tors, which are shown respectively in Eqs. 2 and 3. For the encoder embedding vectors, we introduce the following operator: $ ei = Eq φq (xi ), (5) q where φq (xi ) is a mapping function that returns a binary vector that corresponds to xi . For the decoder embedding vectors, we obtain fj by the following equation: fj = $ Fr ψr (yj−1 ), (6) r where ψr (yj−1 ) is a mapping function that returns a binary vector that corresponds to previously estimated result yj−1 . For example, if we get record as an estimation result of BPE(m =16k), mapping 1 NMT lacks the ability to translate longer sentences (Koehn and Knowles, 2017). 621 train tst2012 (development) tst2013 (test) tst2014 (test) DE-EN Tokens Sentences 3,496,036 189,318 30,900 1,700 21,037 993 24,950 1,305 FR-EN Tokens Sentences 3,800,613 208,323 21,653 1,124 21,894 1,024 24,950 1,305 Table 1: Cleaned corpora statistics on IWSLT datasets. Number of tokens is English side. Configurations Embedding dimension D Hidden dimension H Attention dimension Encoder layer Decoder layer Values 512 512 512 2 2 Configurations Optimizer Initial learning rate Gradient clipping Dropout rate Mini-batch size Values SGD 1.0 5.0 0.3 128 sent Table 2: Model and optimization conf"
C18-1052,P18-1007,0,0.192089,"618 Proceedings of the 27th International Conference on Computational Linguistics, pages 618–629 Santa Fe, New Mexico, USA, August 20-26, 2018. A subword is a fraction of a word, determined by Byte Pair Encoding (BPE) operations. By the BPE operation, a word that appears frequently can be one unit, and rare or uncommon words can be expressed by the combination of several subword units. Thus we can express any words by the combination of small subword vocabularies to alleviate the out-of-vocabulary problem. Several similar works exist to make subword units (e.g., (Schuster and Nakajima, 2012; Kudo, 2018)), but in the following, we denote the units segmented by BPE as subword units unless otherwise noted. The primary reason why we use subword units is to generate rare or unknown words on the decoder side. In other words, our purpose is to change the vocabulary at the decoder output layer into subwords. Once we decide the vocabulary in the decoder output layer, it is natural to use the same vocabulary in the decoder embedding layer. We also use subword units in the encoder side to maintain consistency with the decoder. As described, NMT has three layers that are related to subwords: the encoder"
C18-1052,Q17-1026,0,0.0318171,"larization method is significantly effective when testing accuracy with a different dataset than the training set. This means that their method is effective with open-domain settings. Our method might have a similar tendency, but we will investigate in future work. It might be interesting to verify the effect of combining our method and the subword regularization method. Several studies incorporated the Recurrent Neural Network (RNN) or the Convolutional Neural Network (CNN) into the embedding layer for encoding character-, subword-, or morpheme-level informa627 tion (Luong and Manning, 2016; Lee et al., 2017). Comparing with these approaches, our work has a significant advantage in terms of the fast computation since our method increases negligible computational cost as clarified in Section 4.2.1. 6 Conclusion In this paper, we focused on neural machine translation with subword units and experimented with hierarchical subword features. Our experiments confirmed that adding hierarchical subword features to the encoder side consistently improved the BLEU scores. Our method, which is quite simple and easy to adapt to any models that use subwords as a unit (such as text summarization and language mode"
C18-1052,P16-1100,0,0.0319877,"wed that the subword regularization method is significantly effective when testing accuracy with a different dataset than the training set. This means that their method is effective with open-domain settings. Our method might have a similar tendency, but we will investigate in future work. It might be interesting to verify the effect of combining our method and the subword regularization method. Several studies incorporated the Recurrent Neural Network (RNN) or the Convolutional Neural Network (CNN) into the embedding layer for encoding character-, subword-, or morpheme-level informa627 tion (Luong and Manning, 2016; Lee et al., 2017). Comparing with these approaches, our work has a significant advantage in terms of the fast computation since our method increases negligible computational cost as clarified in Section 4.2.1. 6 Conclusion In this paper, we focused on neural machine translation with subword units and experimented with hierarchical subword features. Our experiments confirmed that adding hierarchical subword features to the encoder side consistently improved the BLEU scores. Our method, which is quite simple and easy to adapt to any models that use subwords as a unit (such as text summarizatio"
C18-1052,D15-1166,0,0.80111,"で部分単語が用いられるが、それぞれの層で適切な部分単 語単位は異なるという仮説を立てた。我々は、Sennrich et al. (2016) に基づく部分単語は、 大きな語彙集合が小さい語彙集合を必ず包含するという特徴を利用して、複数の異なる部 分単語列を同時に一つの埋め込み層として扱えるよう NMT モデルを改良する。以降、こ の小さな語彙集合特徴を階層的部分単語特徴と呼ぶ。本仮説を検証するために、様々な部 分単語単位や階層的部分単語特徴をエンコーダ・デコーダの埋め込み層に適用して、その 精度の変化を確認する。IWSLT 評価セットを用いた実験により、エンコーダ側で階層的な 部分単語を用いたモデルは BLEU スコアが一貫して向上することが確認できた。 1 Introduction The approach of end-to-end Neural Machine Translation (NMT) continues to make rapid progress. A simple encoder-decoder model was proposed by Sutskever et al. (2014), and an attentional mechanism was added to better exploit the encoder-side information (Luong et al., 2015; Bahdanau et al., 2015). Compared to traditional Statistical Machine Translation (SMT), NMT has relatively simple architecture, which only uses a large single neural network, but its accuracy surpasses SMT (Junczys-Dowmunt et al., 2016). A conventional NMT uses a limited vocabulary and a decoder generates a “word” in the vocabulary at each time step, but a problem occurs when it encounters an out-of-vocabulary word. Since NMT cannot correctly encode and generate such out-of-vocabulary words, the task performance is degraded. To solve this problem, Sennrich et al. (2016) proposed a method that"
C18-1052,P02-1040,0,0.102586,"ture (a) + Decoder side BPE 300 feature (a) + Decoder side BPE 1k, 300 features (f) + (n) (k) + (n) Table 3: Compared experimental settings learning rate to 1.0, but after 30 epochs we multiplied it by 0.8 for every epoch and continued training until 40 epochs. For decoding, we performed a beam search with a beam size of 20. To prevent the model from outputting short sentences, we applied the length normalization technique by dividing the negative log-likelihood by the sentence length (Cromieres et al., 2016; Morishita et al., 2017). As evaluation metrics, we used case-sensitive7 BLEU scores (Papineni et al., 2002) using multi-bleu.perl8 . To fix the experimental settings, we carried out a preliminary analysis to find the relation between the sentence length and the vocabulary size (≃ the number of BPE merge operations). Fig. 2 shows the results on the English sentences of the IWSLT 2016 German-English training set. When we reduce the vocabulary size, the average sentence length rapidly increases. Unfortunately, longer sentences require more computational cost and are time-consuming. Thus in our experiments, we set the baseline system’s vocabulary size to 16,000, which balances the sentence length witho"
C18-1052,W16-2209,0,0.0235885,"ux peut-ˆetre me l’apprendre a` moi. ✿✿✿✿✿✿✿✿✿✿✿✿ I was like, “Well I’m not ✿✿✿✿✿✿✿ Britney✿✿✿✿✿✿ Spears, but maybe you could teach me. I said, “I’m not ✿✿✿✿✿✿ British✿✿✿✿✿✿✿ Speney✿✿✿✿✿✿ Spears, but maybe you can teach me. I said, “I’m not ✿✿✿✿✿✿✿ Britney ✿✿✿✿✿✿ Spears, but maybe you can teach me. Table 9: Example translation from French to English. Proposed method correctly translated rare words: “Britney Spears.” Merge operations 16k 1k 300 Subword segmentation Bri t ney S pe ars B ri t n e y S pe ars B ri t n e y S p e ar s Table 10: Example segmentation of “Britney Spears” 5 Related Work Sennrich and Haddow (2016) added linguistic features to NMT embedding layer and achieved significant improvement. They modified the embedding layer to exploit several features, such as part-of-speech tags and syntactic dependency labels. This method resembles our work in terms of providing more information to the embedding layer. However, to use these linguistic features, since we need to prepare a morphological analyzer and/or a dependency parser, applicable languages are limited. In our method, BPE features are language independent and applicable to all languages. Our method can also be interpreted as a regularizer t"
C18-1052,P16-1162,0,0.80135,"roving Neural Machine Translation by Incorporating Hierarchical Subword Features Makoto Morishita, Jun Suzuki∗ and Masaaki Nagata NTT Communication Science Laboratories, NTT Corporation, Japan {morishita.makoto, nagata.masaaki}@lab.ntt.co.jp jun.suzuki@ecei.tohoku.ac.jp Abstract This paper focuses on subword-based Neural Machine Translation (NMT). We hypothesize that in the NMT model, the appropriate subword units for the following three modules (layers) can differ: (1) the encoder embedding layer, (2) the decoder embedding layer, and (3) the decoder output layer. We find the subword based on Sennrich et al. (2016) has a feature that a large vocabulary is a superset of a small vocabulary and modify the NMT model enables the incorporation of several different subword units in a single embedding layer. We refer these small subword features as hierarchical subword features. To empirically investigate our assumption, we compare the performance of several different subword units and hierarchical subword features for both the encoder and decoder embedding layers. We confirmed that incorporating hierarchical subword features in the encoder consistently improves BLEU scores on the IWSLT evaluation datasets. Tit"
D07-1080,P05-1012,0,0.310578,"ptimized toward a set of good translations found in the k-best list across iterations. The objective function is an approximated BLEU (Watanabe et al., 2006a) that scales the loss of a sentence BLEU to a documentwise loss. The parameters are trained using the 764 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 764–773, Prague, June 2007. 2007 Association for Computational Linguistics Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006). MIRA is successfully employed in dependency parsing (McDonald et al., 2005) or the joint-labeling/chunking task (Shimizu and Haas, 2006). Experiments were carried out on an Arabicto-English translation task, and we achieved significant improvements over conventional minimum error training with a small number of features. This paper is organized as follows: First, Section 2 introduces the framework of statistical machine translation. As a baseline SMT system, we use the hierarchical phrase-based translation with an efficient left-to-right generation (Watanabe et al., 2006b) originally proposed by Chiang (2005). In Section 3, a set of binary sparse features are defined"
D07-1080,J04-4002,0,0.0244586,"achine translation by using a large number of features with an online large-margin training algorithm. The millions of parameters were tuned only on a small development set consisting of less than 1K sentences. Experiments on Arabic-toEnglish translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features. 1 Introduction The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on (hierarchical) phrase-based translation (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) or syntax-based translation (Galley et al., 2006). However, it does not scale well with a large number of features of the order of millions. Tillmann and Zhang (2006), Liang et al. (2006) and Bangalore et al. (2006) introduced sparse binary features for statistical machine translation trained on a large training corpus. In this framework, the problem of translation is regarded as a sequential labeling problem, in the same way as part-of-speech tagging, chunking or shallow parsing. However, the use of a large number of features did not provide any significant"
D07-1080,2004.iwslt-evaluation.13,0,0.0101531,"concatenated and intersected with n-gram. 3 Features 3.1 Baseline Features The hierarchical phrase-based translation system employs standard numeric value features: • n-gram language model to capture the fluency of the target side. • Hierarchical phrase translation probabilities in ¯ and h(bβ|γ), ¯ both directions, h(γ|bβ) estimated ¯ by relative counts, count(γ, bβ). • Word-based lexically weighted models of ¯ and hlex (bβ|γ) ¯ hlex (γ|bβ) using lexical translation models. • Word-based insertion/deletion penalties that penalize through the low probabilities of the lexical translation models (Bender et al., 2004). • Word/hierarchical-phrase length penalties. • Backtrack-based penalties inspired by the distortion penalties in phrase-based modeling (Watanabe et al., 2006b). 3.2 Sparse Features In addition to the baseline features, a large number of binary features are integrated in our MT system. We may use any binary features, such as   English word “violate” and Arabic    1 word “tnthk” appeared in e and f . h( f, e) =     0 otherwise. The features are designed by considering the decoding efficiency and are based on the word alignment structure preserved in hierarchical phrase translation pai"
D07-1080,P05-1033,0,0.854373,"ber of features with an online large-margin training algorithm. The millions of parameters were tuned only on a small development set consisting of less than 1K sentences. Experiments on Arabic-toEnglish translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features. 1 Introduction The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on (hierarchical) phrase-based translation (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) or syntax-based translation (Galley et al., 2006). However, it does not scale well with a large number of features of the order of millions. Tillmann and Zhang (2006), Liang et al. (2006) and Bangalore et al. (2006) introduced sparse binary features for statistical machine translation trained on a large training corpus. In this framework, the problem of translation is regarded as a sequential labeling problem, in the same way as part-of-speech tagging, chunking or shallow parsing. However, the use of a large number of features did not provide any significant improvements over a conventional s"
D07-1080,P06-1121,0,0.0405344,"training algorithm. The millions of parameters were tuned only on a small development set consisting of less than 1K sentences. Experiments on Arabic-toEnglish translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features. 1 Introduction The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on (hierarchical) phrase-based translation (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) or syntax-based translation (Galley et al., 2006). However, it does not scale well with a large number of features of the order of millions. Tillmann and Zhang (2006), Liang et al. (2006) and Bangalore et al. (2006) introduced sparse binary features for statistical machine translation trained on a large training corpus. In this framework, the problem of translation is regarded as a sequential labeling problem, in the same way as part-of-speech tagging, chunking or shallow parsing. However, the use of a large number of features did not provide any significant improvements over a conventional small feature set. Bangalore et al. (2006) trained"
D07-1080,N03-1017,0,0.244563,"by using a large number of features with an online large-margin training algorithm. The millions of parameters were tuned only on a small development set consisting of less than 1K sentences. Experiments on Arabic-toEnglish translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features. 1 Introduction The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on (hierarchical) phrase-based translation (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) or syntax-based translation (Galley et al., 2006). However, it does not scale well with a large number of features of the order of millions. Tillmann and Zhang (2006), Liang et al. (2006) and Bangalore et al. (2006) introduced sparse binary features for statistical machine translation trained on a large training corpus. In this framework, the problem of translation is regarded as a sequential labeling problem, in the same way as part-of-speech tagging, chunking or shallow parsing. However, the use of a large number of features did not provide any significant improvements over a"
D07-1080,P03-1021,0,0.59948,"on 2 introduces the framework of statistical machine translation. As a baseline SMT system, we use the hierarchical phrase-based translation with an efficient left-to-right generation (Watanabe et al., 2006b) originally proposed by Chiang (2005). In Section 3, a set of binary sparse features are defined including numeric features for our baseline system. Section 4 introduces an online large-margin training algorithm using MIRA with our key components. The experiments are presented in Section 5 followed by discussion in Section 6. 2 Statistical Machine Translation We use a log-linear approach (Och, 2003) in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: eˆ = argmax wT · h( f, e) (1) e where h( f, e) is a large-dimension feature vector. w is a weight vector that scales the contribution from each feature. Each feature can take any real value, such as the log of the n-gram language model to represent fluency, or a lexicon model to capture the word or phrase-wise correspondence. 2.1 Hierarchical Phrase-based SMT Chiang (2005) introduced the hierarchical phrasebased translation approach, in which non-terminals are emb"
D07-1080,P02-1040,0,0.107918,"ed as “@@@@/@/@@”. We consider all possible combination of those token types. For example, the word pair feature (violate, tnthk) is normalized and expanded to (viol+, tnthk), (viol+, tnth+), (violate, tnth+), etc. using the 4-letter prefix token type. 4 Online Large-Margin Training Algorithm 1 is our generic online training algorithm. The algorithm is slightly different from other online training algorithms (Tillmann and Zhang, 2006; Liang et al., 2006) in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a metric, i.e. BLEU (Papineni et al., 2002). In line 3, a k-best list is generated by bestk (·) using the current weight vector wi for the training instance of ( f t , et ). Each training instance has multiple (or, possibly one) reference translations et for the source sentence f t . Using the k-best list, m-best oracle translations Ot is updated by oraclem (·) for every iteration (line 4). Usually, a decoder cannot generate translations that exactly match the reference translations due to its beam search pruning and OOV. Thus, we cannot always assign scores for each reference translation. Therefore, possible oracle translations are ma"
D07-1080,P04-1007,0,0.0559747,"of phrase translation pairs, but it is trivial to define the features over hierarchical phrases. X1 X2 f j−1 fj f j+3 X3 f j+1 f j+2 Figure 2: Example hierarchical features. same way, we will be able to include deletion features where a non-aligned source word is associated with the target sentence. However, this would lead to complex decoding in which all the translated words are memorized for each hypothesis, and thus not integrated in our feature set. 3.2.3 Target Bigram Features Target side bigram features are also included to directly capture the fluency as in the n-gram language model (Roark et al., 2004). For instance, bigram features of (ei−1 , ei ), (ei , ei+1 ), (ei+1 , ei+2 )... are observed in Figure 1. 3.2.4 Hierarchical Features In addition to the phrase motivated features, we included features inspired by the hierarchical structure. Figure 2 shows an example of hierarchical phrases in the source side, consisting of X 1 → E E D D D E f j−1 X 2 f j+3 , X 2 → f j f j+1 X 3 and X 3 → f j+2 . Hierarchical features capture the dependency of the source words in a parent phrase to the source words in child phrases, such as ( f j−1 , f j ), ( f j−1 , f j+1 ), ( f j+3 , f j ), ( f j+3 , f j+1 )"
D07-1080,P06-2098,0,0.147558,"st list across iterations. The objective function is an approximated BLEU (Watanabe et al., 2006a) that scales the loss of a sentence BLEU to a documentwise loss. The parameters are trained using the 764 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 764–773, Prague, June 2007. 2007 Association for Computational Linguistics Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006). MIRA is successfully employed in dependency parsing (McDonald et al., 2005) or the joint-labeling/chunking task (Shimizu and Haas, 2006). Experiments were carried out on an Arabicto-English translation task, and we achieved significant improvements over conventional minimum error training with a small number of features. This paper is organized as follows: First, Section 2 introduces the framework of statistical machine translation. As a baseline SMT system, we use the hierarchical phrase-based translation with an efficient left-to-right generation (Watanabe et al., 2006b) originally proposed by Chiang (2005). In Section 3, a set of binary sparse features are defined including numeric features for our baseline system. Section"
D07-1080,W04-3201,0,0.02528,"nslation are created, which amount to m × k large-margin constraints. In this online training, only active features constrained by Eq. 3 are kept and updated, unlike offline training in which all possible features have to be extracted and selected in advance. The Lagrange dual form of Eq. 3 is:   1 X maxα(·)≥0 − || α(ˆe, e′ ) h( f t , eˆ ) − h( f t , e′ ) ||2 2 eˆ ,e′ X + α(ˆe, e′ )L(ˆe, e′ ; et ) eˆ,e′ Margin Infused Relaxed Algorithm The Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006) is an online version of the large-margin training algorithm for structured classification (Taskar et al., 2004) that has been successfully used for dependency parsing (McDonald et al., 2005) and joint-labeling/chunking (Shimizu and Haas, 2006). The basic idea is to keep the norm of the updates to the weight vector as small as possible, considering a margin at least as large as the loss of the incorrect classification. Line 5 of the weight vector update procedure in Algorithm 1 is replaced by the solution of: X ˆ i+1 = argmin ||wi+1 − wi ||+ C ξ(ˆe, e′ ) w wi+1 eˆ ,e′ subject to si+1 ( f t , eˆ ) − si+1 ( f t , e′ ) + ξ(ˆe, e′ ) ≥ L(ˆe, e′ ; et ) ξ(ˆe, e′ ) ≥ 0 ∀ˆe ∈ Ot , ∀e′ ∈ Ct (3) n oT where si ( f"
D07-1080,P06-1091,0,0.438964,"an 1K sentences. Experiments on Arabic-toEnglish translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features. 1 Introduction The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on (hierarchical) phrase-based translation (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) or syntax-based translation (Galley et al., 2006). However, it does not scale well with a large number of features of the order of millions. Tillmann and Zhang (2006), Liang et al. (2006) and Bangalore et al. (2006) introduced sparse binary features for statistical machine translation trained on a large training corpus. In this framework, the problem of translation is regarded as a sequential labeling problem, in the same way as part-of-speech tagging, chunking or shallow parsing. However, the use of a large number of features did not provide any significant improvements over a conventional small feature set. Bangalore et al. (2006) trained the lexical choice model by using Conditional Random Fields (CRF) realized on a WFST. Their modeling was reduced to M"
D07-1080,P06-1098,1,0.329931,"oice model by using Conditional Random Fields (CRF) realized on a WFST. Their modeling was reduced to Maximum Entropy Markov Model (MEMM) to handle a large number of features which, in turn, faced the labeling bias problem (Lafferty et al., 2001). Tillmann and Zhang (2006) trained their feature set using an online discriminative algorithm. Since the decoding is still expensive, their online training approach is approximated by enlarging a merged kbest list one-by-one with a 1-best output. Liang et al. (2006) introduced an averaged perceptron algorithm, but employed only 1-best translation. In Watanabe et al. (2006a), binary features were trained only on a small development set using a variant of voted perceptron for reranking k-best translations. Thus, the improvement is merely relative to the baseline translation system, namely whether or not there is a good translation in their k-best. We present a method to estimate a large number of parameters — of the order of millions — using an online training algorithm. Although it was intuitively considered to be prone to overfitting, training on a small development set — less than 1K sentences — was sufficient to achieve improved performance. In this method,"
D07-1080,P98-2230,0,0.0117577,"string of non-terminals. ∼ defines one-to-one mapping between non-terminals in γ and β. The use of phrase b¯ as a prefix maintains the strength of the phrasebase framework. A contiguous English side with a (possibly) discontiguous foreign language side preserves phrase-bounded local word reordering. At the same time, the target normalized framework still combines phrases hierarchically in a restricted manner. 2.3 Left-to-Right Target Generation Decoding is performed by parsing on the source side and by combining the projected target side. We applied an Earley-style top-down parsing approach (Wu and Wong, 1998; Watanabe et al., 2006b; Zollmann and Venugopal, 2006). The basic idea is to perform top-down parsing so that the projected target side is generated in a left-to-right manner. The search is guided with a push-down automaton, which keeps track of the span of uncovered source word positions. Combined with the rest-cost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most likely translation. The use of a target normalized form further simplifies the decoding procedure. Since the rule form does not allow any holes for the target side, the integration with an n-g"
D07-1080,W06-3108,0,0.164848,"Word/hierarchical-phrase length penalties. • Backtrack-based penalties inspired by the distortion penalties in phrase-based modeling (Watanabe et al., 2006b). 3.2 Sparse Features In addition to the baseline features, a large number of binary features are integrated in our MT system. We may use any binary features, such as   English word “violate” and Arabic    1 word “tnthk” appeared in e and f . h( f, e) =     0 otherwise. The features are designed by considering the decoding efficiency and are based on the word alignment structure preserved in hierarchical phrase translation pairs (Zens and Ney, 2006). When hierarchical phrases are extracted, the word alignment is preserved. If multiple word alignments are observed 766 ei−1 f j−1 ei ei+1 fj ei+2 ei+3 f j+1 f j+2 ei+4 f j+3 Figure 1: An example of sparse features for a phrase translation. with the same source and target sides, only the frequently observed word alignment is kept to reduce the grammar size. 3.2.1 Word Pair Features Word pair features reflect the word correspondence in a hierarchical phrase. Figure 1 illustrates an example of sparse features for a phrase translation pair f j , ..., f j+2 and ei , ..., ei+3 1 . From the word al"
D07-1080,W06-3119,0,0.0385186,"one mapping between non-terminals in γ and β. The use of phrase b¯ as a prefix maintains the strength of the phrasebase framework. A contiguous English side with a (possibly) discontiguous foreign language side preserves phrase-bounded local word reordering. At the same time, the target normalized framework still combines phrases hierarchically in a restricted manner. 2.3 Left-to-Right Target Generation Decoding is performed by parsing on the source side and by combining the projected target side. We applied an Earley-style top-down parsing approach (Wu and Wong, 1998; Watanabe et al., 2006b; Zollmann and Venugopal, 2006). The basic idea is to perform top-down parsing so that the projected target side is generated in a left-to-right manner. The search is guided with a push-down automaton, which keeps track of the span of uncovered source word positions. Combined with the rest-cost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most likely translation. The use of a target normalized form further simplifies the decoding procedure. Since the rule form does not allow any holes for the target side, the integration with an n-gram language model is straightforward: the prefixed phr"
D07-1080,P06-1096,0,\N,Missing
D07-1080,J03-1002,0,\N,Missing
D07-1080,2006.iwslt-evaluation.14,1,\N,Missing
D07-1080,C98-2225,0,\N,Missing
D07-1083,W03-0423,0,0.0156617,"Missing"
D07-1083,W03-0425,0,0.0243821,"Missing"
D07-1083,P06-1027,0,0.1027,"ediction, which is not practical as regards the large data sets used for standard sequence labeling tasks in NLP. Another discriminative approach to semi-supervised SOL involves the incorporation of an entropy regularizer (Grand791 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 791–800, Prague, June 2007. 2007 Association for Computational Linguistics valet and Bengio, 2004). Semi-supervised conditional random fields (CRFs) based on a minimum entropy regularizer (SS-CRF-MER) have been proposed in (Jiao et al., 2006). With this approach, the parameter is estimated to maximize the likelihood of labeled data and the negative conditional entropy of unlabeled data. Therefore, the structured predictor is trained to separate unlabeled data well under the entropy criterion by parameter estimation. In contrast to these previous studies, this paper proposes a semi-supervised SOL framework based on a hybrid generative and discriminative approach. A hybrid approach was first proposed in a supervised learning setting (Raina et al., 2003) for text classification. (Fujino et al., 2005) have developed a semi-supervised"
D07-1083,N01-1025,0,0.229147,"Missing"
D07-1083,N03-1028,0,0.0610035,"bialgorithm for evaluating unseen samples, as well as that of standard CRFs, without any additional cost. 4 Experiments We examined our hybrid model (HySOL) by applying it to two sequence labeling tasks, named entity recognition (NER) and syntactic chunking (Chunking). We used the same Chunking and ‘English’ NER data as those used for the shared tasks of CoNLL-2000 (Tjong Kim Sang and Buchholz, 2000) and CoNLL-2003 (Tjong Kim Sang and Meulder, 2003), respectively. For the baseline method, we performed a conditional random field (CRF), which is exactly the same training procedure described in (Sha and Pereira, 2003) with L-BFGS. Moreover, LOP-CRF (Smith et al., 2005) is also compared with our hybrid model, since the formalism of our hybrid model can be seen as an extension of LOP-CRFs as described in Section 3. For CRF, we used the Gaussian prior as the second term on the RHS in Equation (1), where δ 2 represents the hyper-parameter in the Gaussian prior. In contrast, for LOP-CRF and HySOL, we used the Dirichlet priors as the second term on the λ1 f(words ), f(lwords ), f(poss ), f(wtypes ), f(poss−1 , poss ), f(wtypes−1 , wtypes ), f(poss , poss+1 ), f(wtypes , wtypes+1 ), f(pref1s ), f(pref2s ), f(pref"
D07-1083,P05-1003,0,0.0828816,"which is equivalent to γj = 0 for all j, we obtain essentially the same objective function as that of the LOP-CRFs. Thus, our framework can also be seen as an extension of LOP-CRFs that enables us to incorporate unlabeled data. θys−1 ,ys θys ,xs , s=1 where θys−1 ,ys and θys ,xs represent the transition probability between states ys−1 and ys and the symbol emission probability of the s-th position of the corresponding input sequence, respectively, where θyS+1 ,xS+1 = 1. It can be seen that the formalization in the loglinear combination of our hybrid model is very similar to that of LOP-CRFs (Smith et al., 2005). In fact, if we only use a combination of discriminative 793 X n log R(y n |xn ; Λ, Θ, Γ)+log p(Γ). (3) where p(Γ) is a prior probability distribution of Γ. The value of Γ providing a global maximum of LHySOL (Γ|Θ) is guaranteed under an arbitrary fixed value in the Θ domain, since LHySOL (Γ|Θ) is a concave function of Γ. Thus, we can easily maximize Equation (3) by using a gradient-based optimization algorithm such as (bound constrained) L-BFGS (Liu and Nocedal, 1989). 3.2 Incorporating Unlabeled Data We cannot directly incorporate unlabeled data for discriminative training such as Equation"
D07-1083,N06-1012,0,0.081288,"ntences in training, development and test data, respectively, with four named entity tags, PERSON, LOCATION, ORGANIZATION and MISC, plus the ‘O’ tag. The unlabeled data consists of 17,003,926 words from 1,029,122 sentences. These data sets are exactly the same as those provided for the shared task of CoNLL-2003. We slightly extended the feature set of the supplied data by adding feature types such as ‘word type’, and word prefix and suffix. Examples of ‘word type’ include whether the word is capitalized, contains digit or contains punctuation, which basically follows the baseline features of (Sutton et al., 2006) without regular expressions. Note that, unlike several previous studies, we did not employ additional information from external resources such as gazetteers. All our features can be automatically extracted from the supplied data. For LOP-CRF and HySOL, we used four base discriminative models trained by CRFs with different feature sets. Table 1 shows the feature sets we used for training these models. The design of these feature sets was derived from a suggestion in (Smith et al., 2005), which exhibited the best performance in the several feature division. Note that the CRF for the comparison"
D07-1083,P06-1028,1,0.829929,"nsed to use the TREC corpus including WSJ unlabeled data that they used for their Chunking experiments (training and test data for Chunking is derived from WSJ). Therefore, we simply used the supplied unlabeled data of the CoNLL-2003 shared task for both NER and Chunking. If we consider the advantage of our approach, our hybrid model incorporating generative models seems rather intuitive, since it is sometimes difficult to find out a design of effective auxiliary problems for the target problem. Interestingly, the additional information obtained HySOL (ξ 0 =0.1,η 0 =0.0001) + w/ F-score opt. (Suzuki et al., 2006) + unlabeled data (17M → 27M words) + supplied gazetters + add dev. set for estimating Γ Fβ=1 87.20 88.02 88.41 88.90 89.27 (gain) (+0.82) (+0.39) (+0.49) (+0.37) Table 7: The HySOL performance with the Fscore optimization technique and some additional resources in NER (CoNLL-2003) experiments Fβ=1 (gain) HySOL (ξ 0 =0.1,η 0 =0.0001) 94.30 + w/ F-score opt. (Suzuki et al., 2006) 94.36 (+0.06) Table 8: The HySOL performance with the F-score optimization technique on Chunking (CoNLL-2000) experiments from unlabeled data appear different from each other. ASO-semi uses unlabeled data for construct"
D07-1083,W00-0726,0,0.0784923,"ation is independent of the number of models used in the hybrid model. In addition, after training, we can easily merge all the parameter values in a single parameter vector. This means that we can simply employ the Viterbialgorithm for evaluating unseen samples, as well as that of standard CRFs, without any additional cost. 4 Experiments We examined our hybrid model (HySOL) by applying it to two sequence labeling tasks, named entity recognition (NER) and syntactic chunking (Chunking). We used the same Chunking and ‘English’ NER data as those used for the shared tasks of CoNLL-2000 (Tjong Kim Sang and Buchholz, 2000) and CoNLL-2003 (Tjong Kim Sang and Meulder, 2003), respectively. For the baseline method, we performed a conditional random field (CRF), which is exactly the same training procedure described in (Sha and Pereira, 2003) with L-BFGS. Moreover, LOP-CRF (Smith et al., 2005) is also compared with our hybrid model, since the formalism of our hybrid model can be seen as an extension of LOP-CRFs as described in Section 3. For CRF, we used the Gaussian prior as the second term on the RHS in Equation (1), where δ 2 represents the hyper-parameter in the Gaussian prior. In contrast, for LOP-CRF and HySOL"
D07-1083,W03-0419,0,0.0373196,"Missing"
D07-1083,P05-1001,0,\N,Missing
D09-1058,D07-1101,1,0.74923,"dency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, more specifically the model of (Carreras, 2007), using a two-stage semi-supervised learning approach. We conduct experiments on dependency parsing of English (on Penn Treebank data) and Czech (on the Prague Dependency Treebank). Our experiments investigate the effectiveness of: 1) the basic SS-SCM for dependency parsing; 2) a combination of the SS-SCM with Koo et al. (2008)’s semisupervised approach (even in the case we used the same unlabeled data for both methods); 3) the twostage semi-supervised learning approach that inThis paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning app"
D09-1058,P99-1065,1,0.699382,"at the input sentences include both words and part-of-speech (POS) tags. Our baseline features (“baseline”) are very similar to those described in (McDonald et al., 2005a; Koo et al., 2008): these features track word and POS bigrams, contextual features surrounding dependencies, distance features, and so on. English POS tags were assigned by MXPOST (Ratnaparkhi, 1996), which was trained on the training data described in Section 4.1. Czech POS tags were obtained by the following two steps: First, we used ‘feature-based tagger’ included with the PDT3 , and then, we used the method described in (Collins et al., 1999) to convert the assigned rich POS tags into simplified POS tags. 4.2.2 Cluster-based Features In a second set of experiments, we make use of the feature set used in the semi-supervised approach of (Koo et al., 2008). We will refer to this as the “cluster-based feature set” (CL). The BLLIP (43M tokens) and PDT (39M tokens) unlabeled data sets shown in Table 1 were used to construct the hierarchical clusterings used within the approach. Note that when this feature set is used within the SSSCM approach, the same set of unlabeled data is used to both induce the clusters, and to estimate the genera"
D09-1058,C96-1058,0,0.336642,"describes how the parameters θj,a are trained on unlabeled data. Given parameters θj,a , we can simply define the functions q1 . . . qk to be log probabilities under the generative model: ˆrj = = i=1 y p(y|x0i ; w, v, q) rj,a (x, h, m, l) log θj,a . a=1 dj X rj,a (x, h, m, l) log a=1 (h,m,l)∈y rj (x0i , h, m, l). rˆj,a θj,a = Pdj . ˆj,a a=1 r We modify this definition slightly, be introducing scaling factors cj,a > 0, and defining qj (x, h, m, l) = X Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then qj (x, h, m, l) = log qj0 (x, h, m, l) dj X M X X In a slight modification, we employ the following estimates in our model, where η > 1 is a parameter of the model: θj,a (4) cj,a θj,a = In our experiments, cj,a is simply a count of the number of times the feature indexed by (j, a) appears in unlabeled data. Thus more frequent feat"
D09-1058,D07-1015,1,0.813809,". qk to be log probabilities under the generative model: ˆrj = = i=1 y p(y|x0i ; w, v, q) rj,a (x, h, m, l) log θj,a . a=1 dj X rj,a (x, h, m, l) log a=1 (h,m,l)∈y rj (x0i , h, m, l). rˆj,a θj,a = Pdj . ˆj,a a=1 r We modify this definition slightly, be introducing scaling factors cj,a > 0, and defining qj (x, h, m, l) = X Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then qj (x, h, m, l) = log qj0 (x, h, m, l) dj X M X X In a slight modification, we employ the following estimates in our model, where η > 1 is a parameter of the model: θj,a (4) cj,a θj,a = In our experiments, cj,a is simply a count of the number of times the feature indexed by (j, a) appears in unlabeled data. Thus more frequent features have their contribution down-weighted in the model. We have found this modification to be beneficial. (η − 1) + rˆj,a dj × (η −"
D09-1058,P08-1068,1,0.098982,"for dependency parsing. Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008). We extend it for dependency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, more specifically the model of (Carreras, 2007), using a two-stage semi-supervised learning approach. We conduct experiments on dependency parsing of English (on Penn Treebank data) and Czech (on the Prague Dependency Treebank). Our experiments investigate the effectiveness of: 1) the basic SS-SCM for dependency parsing; 2) a combination of the SS-SCM with Koo et al. (2008)’s semisupervised approach (even in the case we used the same unlabeled data for both methods); 3) the twostage semi-supervised learning approac"
D09-1058,E06-1011,0,0.589255,".mit.edu Abstract supervised methods for dependency parsing includes (Smith and Eisner, 2007; Koo et al., 2008; Wang et al., 2008). In particular, Koo et al. (2008) describe a semi-supervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech. This is a very simple approach, but provided significant performance improvements comparing with the stateof-the-art supervised dependency parsers such as (McDonald and Pereira, 2006). This paper introduces an alternative method for semi-supervised learning for dependency parsing. Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008). We extend it for dependency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension"
D09-1058,W07-2216,0,0.0658637,"enerative model: ˆrj = = i=1 y p(y|x0i ; w, v, q) rj,a (x, h, m, l) log θj,a . a=1 dj X rj,a (x, h, m, l) log a=1 (h,m,l)∈y rj (x0i , h, m, l). rˆj,a θj,a = Pdj . ˆj,a a=1 r We modify this definition slightly, be introducing scaling factors cj,a > 0, and defining qj (x, h, m, l) = X Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then qj (x, h, m, l) = log qj0 (x, h, m, l) dj X M X X In a slight modification, we employ the following estimates in our model, where η > 1 is a parameter of the model: θj,a (4) cj,a θj,a = In our experiments, cj,a is simply a count of the number of times the feature indexed by (j, a) appears in unlabeled data. Thus more frequent features have their contribution down-weighted in the model. We have found this modification to be beneficial. (η − 1) + rˆj,a dj × (η − 1) + Pdj ˆj,a a=1 r . (6) This corresponds to a MA"
D09-1058,P05-1012,0,0.451267,"d l is the label of the dependency. We use h = 0 for the root of the sentence. We assume access to a set of labeled training examples, {xi , yi }N i=1 , and in addition a set of unlabeled examples, {x0i }M i=1 . In conditional log-linear models for dependency parsing (which are closely related to conditional random fields (Lafferty et al., 2001)), a distribution over dependency structures for a sentence x is defined as follows: 2.2 X (1) w · f (x, h, m, l) (h,m,l)∈y Here f (x, h, m, l) is a feature vector representing the dependency (h, m, l) in the context of the sentence x (see for example (McDonald et al., 2005a)). In this paper we extend the definition of g(x, y) to include features that are induced from unlabeled data. Specifically, we define g(x, y) = X The Generative Models We now describe how the generative models q1 . . . qk are defined, and how they are induced from unlabeled data. These models make direct use of the feature-vector definition f (x, y) used in the original, fully supervised, dependency parser. The first step is to partition the d features in f (x, y) into k separate feature vectors, r1 (x, y) . . . rk (x, y) (with the result that f is the concatenation of the k feature vectors"
D09-1058,H05-1066,0,0.116818,"Missing"
D09-1058,W06-1615,0,0.174516,"Missing"
D09-1058,J92-4003,0,0.385752,"parameters (w1 , v1 , q1 ). Note that it is possible to iterate the method—steps 2 and 3 can be repeated multiple times (Suzuki and Isozaki, 2008)—but in our experiments we only performed these steps once. 3 Second-order Parsing Models Extensions 3.1 Incorporating Cluster-Based Features Koo et al. (2008) describe a semi-supervised approach that incorporates cluster-based features, and that gives competitive results on dependency parsing benchmarks. The method is a two-stage approach. First, hierarchical word clusters are derived from unlabeled data using the Brown et al. clustering algorithm (Brown et al., 1992). Second, a new feature set is constructed by representing words by bit-strings of various lengths, corresponding to clusters at different levels of the hierarchy. These features are combined with conventional features based on words and part-of-speech 1 We used a slightly modified version of 1-best MIRA, whose difference can be found in the third line in Eq. 7, namely, including L(yi , y). 554 (a) English dependency parsing Data set (WSJ Sec. IDs) # of sentences # of tokens Training (02–21) 39,832 950,028 Development (22) 1,700 40,117 Test (23) 2,012 47,377 Unlabeled 1,796,379 43,380,315 Corp"
D09-1058,W06-2920,0,0.027869,"Missing"
D09-1058,W96-0213,0,0.382393,"ther created through random sampling or by using a predefined subset of document IDs from the labeled training data. mately 4,000 times larger than the size of labeled training data. 4.2 Features 4.2.1 Baseline Features In general we will assume that the input sentences include both words and part-of-speech (POS) tags. Our baseline features (“baseline”) are very similar to those described in (McDonald et al., 2005a; Koo et al., 2008): these features track word and POS bigrams, contextual features surrounding dependencies, distance features, and so on. English POS tags were assigned by MXPOST (Ratnaparkhi, 1996), which was trained on the training data described in Section 4.1. Czech POS tags were obtained by the following two steps: First, we used ‘feature-based tagger’ included with the PDT3 , and then, we used the method described in (Collins et al., 1999) to convert the assigned rich POS tags into simplified POS tags. 4.2.2 Cluster-based Features In a second set of experiments, we make use of the feature set used in the semi-supervised approach of (Koo et al., 2008). We will refer to this as the “cluster-based feature set” (CL). The BLLIP (43M tokens) and PDT (39M tokens) unlabeled data sets shown"
D09-1058,D07-1070,0,0.0431948,"Missing"
D09-1058,D07-1014,0,0.0889514,"obabilities under the generative model: ˆrj = = i=1 y p(y|x0i ; w, v, q) rj,a (x, h, m, l) log θj,a . a=1 dj X rj,a (x, h, m, l) log a=1 (h,m,l)∈y rj (x0i , h, m, l). rˆj,a θj,a = Pdj . ˆj,a a=1 r We modify this definition slightly, be introducing scaling factors cj,a > 0, and defining qj (x, h, m, l) = X Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. The estimates that maximize Eq. 5 are then qj (x, h, m, l) = log qj0 (x, h, m, l) dj X M X X In a slight modification, we employ the following estimates in our model, where η > 1 is a parameter of the model: θj,a (4) cj,a θj,a = In our experiments, cj,a is simply a count of the number of times the feature indexed by (j, a) appears in unlabeled data. Thus more frequent features have their contribution down-weighted in the model. We have found this modification to be beneficial. (η − 1) + rˆj,a dj × (η − 1) + Pdj ˆj,a a=1 r . ("
D09-1058,P08-1076,1,0.72351,"upervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech. This is a very simple approach, but provided significant performance improvements comparing with the stateof-the-art supervised dependency parsers such as (McDonald and Pereira, 2006). This paper introduces an alternative method for semi-supervised learning for dependency parsing. Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008). We extend it for dependency parsing, which we will refer to as a Semi-supervised Structured Conditional Model (SS-SCM). In this framework, a structured conditional model is constructed by incorporating a series of generative models, whose parameters are estimated from unlabeled data. This paper describes a basic method for learning within this approach, and in addition describes two extensions. The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, more spe"
D09-1058,P08-1061,0,0.0411284,"Missing"
D09-1058,W03-3023,0,0.20564,"ent, test data (labeled data sets) and unlabeled data used in our experiments parameter-estimation method for the second-order parsing model. In particular, we perform the following optimizations on each update t = 1, ..., T for re-estimating w and v: min ||w(t+1) − w(t) ||+ ||v(t+1) − v(t) || ˆ ) ≥ L(yi , y ˆ) s.t. S(xi , yi ) − S(xi , y ˆ = arg maxy S(xi , y) + L(yi , y), y as those described in (McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006; Koo et al., 2008). The English dependencyparsing data sets were constructed using a standard set of head-selection rules (Yamada and Matsumoto, 2003) to convert the phrase structure syntax of the Treebank to dependency tree representations. We split the data into three parts: sections 02-21 for training, section 22 for development and section 23 for test. The Czech data sets were obtained from the predefined training/development/test partition in the PDT. The unlabeled data for English was derived from the Brown Laboratory for Linguistic Information Processing (BLLIP) Corpus (LDC2000T43)2 , giving a total of 1,796,379 sentences and 43,380,315 tokens. The raw text section of the PDT was used for Czech, giving 2,349,224 sentences and 39,336,"
D09-1058,J93-2004,0,\N,Missing
D09-1058,D07-1096,0,\N,Missing
D13-1139,P05-1033,0,0.11802,"uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Sourceordered. Target Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for in"
D13-1139,P04-1015,0,0.056235,"wlef t1 of Y, and the rightmost word of phrase X is set to rightmost word wright0 of Z. Variable a is set to a1 of Y. The difference between reduce-MR-X and reduce-SR-X actions is new stack element s0 . The reduce-SR-X action generates s0 by combining s′0 and s′1 with binary rule X# →Y Z: s0 = {X# , h0 , wlef t0 , wright1 , a0 }. This action expands Y and Z in a reverse order, and the leftmost word of X# is set to wlef t0 of Z, and the rightmost word of X# is set to wright1 of Y. Variable a is set to a0 of Z. We use a linear model that is discriminatively trained with the averaged perceptron (Collins and Roark, 2004). Table 1 shows the feature templates used in our experiments and we call the features in the bottom two rows “non-local” features. # of sent. ave. leng. (J) ave. leng. (E) train dev test9 test10 3,191,228 36.4 33.3 2,000 36.6 33.3 2,000 37.0 33.7 2,300 43.1 39.6 Table 2: NTCIR-9 and 10 data statistics. 4 Experiments 4.1 Experimental Setups We conducted experiments for NTCIR-9 and 10 patent data using a Japanese-English language pair. Mecab2 was used for the Japanese morphological analysis. The data are summarized in Table 2. We used Enju (Miyao and Tsujii, 2008) for parsing the English traini"
D13-1139,P05-1066,0,0.27903,"Missing"
D13-1139,D11-1018,0,0.218258,"30.15 BLEU scores of the baseline PBMT system. 1 Sourceordered. Target Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction grammars (ITG) (Wu, 1997). To the best of our knowledge, this is the first study on a shift-reduce parser for word reordering. The parser-based reordering approach uses rich syntax parsing features for reordering decisions. Our propoesd method can also easily define such non-local features a"
D13-1139,P06-1121,0,0.202823,"tax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Sourceordered. Target Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction g"
D13-1139,P12-2061,0,0.351117,"arget Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction grammars (ITG) (Wu, 1997). To the best of our knowledge, this is the first study on a shift-reduce parser for word reordering. The parser-based reordering approach uses rich syntax parsing features for reordering decisions. Our propoesd method can also easily define such non-local features as the N -gram words of reordered strings. Even when usin"
D13-1139,N04-1014,0,0.0387663,"ction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Sourceordered. Target Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce"
D13-1139,P10-1110,0,0.0256258,"j, S|s0 ⟩ : π where s′0 is {X, h, wlef t , wright , a} and s0 is {X, h, wlef t , wright , x} (i ≤ h, lef t, right < j). The side condition prevents the parser from inserting articles into phrase X more than twice. During parsing, articles are not explicitly inserted into the input string: they are inserted into it when backtracking to generate a reordered string after parsing. The reduce-MR-X action has a deduction rule: X→YZ∈P ∧q ∈π z q }| { : ⟨k, i, S|s′2 |s′1 ⟩ : π ′ ℓ : ⟨i, j, S|s′1 |s′0 ⟩ : π ℓ + 1 : ⟨k, j, S|s′2 |s0 ⟩ : π ′ 1 Since our notion of predictor states is identical to that in (Huang and Sagae, 2010), we omit the details here. 1384 s0 = {X, h0 , wlef t1 , wright0 , a1 }. New nonterminal X is lexicalized with head word wh0 of right nonterminal Z. This action expands Y and Z in a straight order. The leftmost word of phrase X is set to leftmost word wlef t1 of Y, and the rightmost word of phrase X is set to rightmost word wright0 of Z. Variable a is set to a1 of Y. The difference between reduce-MR-X and reduce-SR-X actions is new stack element s0 . The reduce-SR-X action generates s0 by combining s′0 and s′1 with binary rule X# →Y Z: s0 = {X# , h0 , wlef t0 , wright1 , a0 }. This action expa"
D13-1139,P07-2045,0,0.0104477,"ift-reduce parser for inversion transduction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Sourceordered. Target Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word r"
D13-1139,J08-1002,0,0.0206303,"with the averaged perceptron (Collins and Roark, 2004). Table 1 shows the feature templates used in our experiments and we call the features in the bottom two rows “non-local” features. # of sent. ave. leng. (J) ave. leng. (E) train dev test9 test10 3,191,228 36.4 33.3 2,000 36.6 33.3 2,000 37.0 33.7 2,300 43.1 39.6 Table 2: NTCIR-9 and 10 data statistics. 4 Experiments 4.1 Experimental Setups We conducted experiments for NTCIR-9 and 10 patent data using a Japanese-English language pair. Mecab2 was used for the Japanese morphological analysis. The data are summarized in Table 2. We used Enju (Miyao and Tsujii, 2008) for parsing the English training data and converted parse trees into HFE trees by a head-finalization scheme. We extracted grammar rules from all the HFE trees and randomly selected 500,000 HFE trees to train the shift-reduce parser. We used Moses (Koehn et al., 2007) with lexicalized reordering and a 6-gram language model (LM) trained using SRILM (Stolcke et al., 2011) to translate the Japanese sentences into HFE sentences. To recover the English sentences, our shift-reduce parser reordered only the 1-best HFE sentence. Our strategy is much simpler than Goto et al. (2012)’s because they used"
D13-1139,N07-1051,0,0.0337815,"articles “ga” “wo” “wa” into English sentences. We privilege the nonterminals of a phrase modified by a deleted article to determine which “the” “a/an” or “no articles” should be inserted at the front of the phrase. Note that an original English sentence can be recovered from its HFE tree by using # symbols and annotated articles and deleting Japanese particles. As well as Goto et al. (2012), we solve postordering by a parser whose model is trained with a set of HFE trees. The main difference between Goto et al. (2012)’s model and ours is that while the former simply used the Berkeley parser (Petrov and Klein, 2007), our shift-reduce parsing model can use such non-local task specific features as the N -gram words of reordered strings without sacrificing efficiency. Our method integrates postediting (Knight and Chander, 1994) with reordering and inserts articles into English translations by learning an additional “insert” action of the parser. Goto et al. (2012) solved the article generation problem by using an 1383 N -gram language model, but this somewhat complicates their approach. Compared with other parsers, one advantage of the shift-reduce parser is to easily define such additional operations as “i"
D13-1139,2011.mtsummit-papers.36,1,0.941844,". 1 Sourceordered. Target Sentence (HFE) Target Sentence (E) reordering Figure 1: A description of the postordering MT system. Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction grammars (ITG) (Wu, 1997). To the best of our knowledge, this is the first study on a shift-reduce parser for word reordering. The parser-based reordering approach uses rich syntax parsing features for reordering decisions. Our propoesd method can also easily define such non-local features as the N -gram words of reordered str"
D13-1139,J97-3002,0,0.358339,"chieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction grammars (ITG) (Wu, 1997). To the best of our knowledge, this is the first study on a shift-reduce parser for word reordering. The parser-based reordering approach uses rich syntax parsing features for reordering decisions. Our propoesd method can also easily define such non-local features as the N -gram words of reordered strings. Even when using these non-local features, the complexity of the shift-reduce parser does not increase at all due to give up achieving an optimal solution. Therefore, it works much more efficient. In our experiments, we apply our proposed method to postordering for J-to-E patent tasks becaus"
D14-1196,P02-1057,0,0.242742,"Missing"
D14-1196,D13-1158,1,0.413854,"Missing"
D14-1196,P14-2052,1,0.463564,"an automatically parsed RST-DT. To improve the ROUGE score, we propose a novel discourse parser that directly generates the DEP-DT. The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser, and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT. 1 Introduction Discourse structures of documents are believed to be highly beneficial for generating informative and coherent summaries. Several discoursebased summarization methods have been developed, such as (Marcu, 1998; Daum´e III and Marcu, 2002; Hirao et al., 2013; Kikuchi et al., 2014). Moreover, the current best ROUGE score for the summarization benchmark data of the RSTdiscourse Treebank (Carlson et al., 2002) has been provided by (Hirao et al., 2013), whose method also utilizes discourse trees. Thus, the discoursebased summarization approach is one promising way to obtain high-quality summaries. One possible weakness of discourse-based summarization techniques is that they rely greatly on the accuracy of the discourse parser they use. For example, the above discourse-based summarization methods utilize discourse trees based on the Rhetorical Structure Theory (RST) (Mann"
D14-1196,P14-1003,0,0.218582,"Nucleus, we obtain an incorrect DEP-DT in Figure 1(b) because the transformation algorithm uses the Nucleus-Satellite relationships in the RST-DT. The dependency relationships in Figure 1(b) are quite different from that of the correct DEP-DT in Figure 1(c). In this example, the parser failed to determine the most salient EDU e2 , that is the root EDU of the gold DEP-DT. Thus, the summary extracted from this DEP-DT will have a low ROUGE score. The results motivated us to design a new discourse parser fully trained on the DEP-DTs and 1 Li et al. also defined a similar transformation algorithm (Li et al., 2014). In this paper, we follow the transformation algorithm defined in (Hirao et al., 2013). 1835 Parser)Training)Phase Transforma;on)Algorithm Elabora7on$ N$ Root$ Elabora7on$ Root$ Example$ Elabora7on$ S$ N$ Elabora7on$ S$ S$ N$ Example$ Elabora7on$ S$ N$ Background$ Concession$ S$ S$ Example$An7thesis$ N$ N$ Elabora7on$ Elabora7on$ Background$ N$ N$ S$ N$ Elabora7on$ S$ S$ e1$ e2$ e1$ e1$ S$ e3$ e4$ e2$ e3$ e2$ e5$ N$ e4$ e5$ e3$ e4$ N$ N$ S$ Concession$ S$ Contrast$ Background$ N$ N$ S$ Elabora7on$ N$ Contrast$ N$ N$ N$ S$ Evidence$ N$ Contrast$N$ N$ S$ Evidence$ N$ RST=DTs Root$ Elabora7on$ R"
D14-1196,W98-1124,0,0.568676,"with a gold DEP-DT and a system with a DEP-DT obtained from an automatically parsed RST-DT. To improve the ROUGE score, we propose a novel discourse parser that directly generates the DEP-DT. The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser, and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT. 1 Introduction Discourse structures of documents are believed to be highly beneficial for generating informative and coherent summaries. Several discoursebased summarization methods have been developed, such as (Marcu, 1998; Daum´e III and Marcu, 2002; Hirao et al., 2013; Kikuchi et al., 2014). Moreover, the current best ROUGE score for the summarization benchmark data of the RSTdiscourse Treebank (Carlson et al., 2002) has been provided by (Hirao et al., 2013), whose method also utilizes discourse trees. Thus, the discoursebased summarization approach is one promising way to obtain high-quality summaries. One possible weakness of discourse-based summarization techniques is that they rely greatly on the accuracy of the discourse parser they use. For example, the above discourse-based summarization methods utiliz"
D14-1196,P05-1012,0,0.102477,"-DT. (b) Overview of (Hirao et al., 2013). In the parser training phase, the parser is trained on RST-DTs, and in the summarization phase, the document is parsed into the RST-DT, and then transformed into the DEP-DT. that could directly generate the DEP-DT. Figure 2(a) shows an overview of the TKP combined with our DEP-DT parser. In the parser training phase, we transform RST-DTs into DEP-DTs, and directly train our parser with the DEP-DTs. In the summarization phase, our method parses a raw document directly into a DEP-DT, and generates a summary with the TKP. fused Relaxed Algorithm (MIRA) (McDonald et al., 2005a; Crammer et al., 2006). We denote s(w, y) = wT fy as a score function given a weight vector w and a DEP-DT y. L(y, y? ) is a loss function, and we define it as the number of EDUs that have an incorrect parent EDU in a predicted DEP-DT y? = arg max s(w, y). Then, we y solve the following optimization problem: 3.2 Description of Discourse Dependency Parser min ||w − w(t) || w Our parser is based on the first-order Maximum Spanning Tree (MST) algorithm (McDonald et al., 2005b). Our parser extracts the features from the EDU ei and the EDU ej . We use almost the features as those shown in (Hernau"
D14-1196,H05-1066,0,0.071798,"-DT. (b) Overview of (Hirao et al., 2013). In the parser training phase, the parser is trained on RST-DTs, and in the summarization phase, the document is parsed into the RST-DT, and then transformed into the DEP-DT. that could directly generate the DEP-DT. Figure 2(a) shows an overview of the TKP combined with our DEP-DT parser. In the parser training phase, we transform RST-DTs into DEP-DTs, and directly train our parser with the DEP-DTs. In the summarization phase, our method parses a raw document directly into a DEP-DT, and generates a summary with the TKP. fused Relaxed Algorithm (MIRA) (McDonald et al., 2005a; Crammer et al., 2006). We denote s(w, y) = wT fy as a score function given a weight vector w and a DEP-DT y. L(y, y? ) is a loss function, and we define it as the number of EDUs that have an incorrect parent EDU in a predicted DEP-DT y? = arg max s(w, y). Then, we y solve the following optimization problem: 3.2 Description of Discourse Dependency Parser min ||w − w(t) || w Our parser is based on the first-order Maximum Spanning Tree (MST) algorithm (McDonald et al., 2005b). Our parser extracts the features from the EDU ei and the EDU ej . We use almost the features as those shown in (Hernau"
D14-1196,N03-1030,0,0.477924,"Missing"
D16-1112,W13-2322,0,0.0542076,"ted edges represent a relationship between nodes. Concepts &lt;s&gt; canada od m a1 a3 country na m e a2 nato prime op 1 name … summary AMR of the input sentence “canadian” yi+1 encAMR aj … announce … … a1 &lt;s&gt; E 0 yi C+1 canada … nato headline prime op 1 name a3 country od m na m e a2 E 0 yi ABS “canadian” AMR of the input sentence Figure 2: Model structure of our proposed attentionbased AMR encoder; it outputs a headline using ABS and encoded AMR with attention. consist of English words, PropBank event predicates, and special labels such as “person”. For edges, AMR has approximately 100 relations (Banarescu et al., 2013) including semantic roles based on the PropBank annotations in OntoNotes (Hovy et al., 2006). To acquire AMRs for input sentences, we use the state-of-the-art transition-based AMR parser (Wang et al., 2015). 3.2 Attention-Based AMR Encoder Figure 2 shows a brief sketch of the model structure of our attention-based AMR encoder model. We utilize a variant of child-sum Tree-LSTM originally proposed in (Tai et al., 2015) to encode syntactic and semantic information obtained from output of the AMR parser into certain fixed-length embedding vectors. To simplify the computation, we transform a DAG st"
D16-1112,N16-1012,0,0.30206,"e 3 supports this observation. For example, ABS+AMR successfully added the correct modifier ‘saudi’ to “crown prince” in the first example. Moreover, ABS+AMR generated a consistent subject in the third example. The comparison between ABS+AMR(w/o attn) and ABS+AMR (with attention) suggests that the attention mechanism is necessary for AMR encoding. In other words, the encoder without the attention mechanism tends to be overfitting. 1058 5 Related Work Recently, the Recurrent Neural Network (RNN) and its variant have been applied successfully to various NLP tasks. For headline generation tasks, Chopra et al. (2016) exploited the RNN decoder (and its variant) with the attention mechanism instead of the method of Rush et al. (2015): the combination of the feed-forward neural network language model and attention-based sentence encoder. Nallapati et al. (2016) also adapted the RNN encoder-decoder with attention for headline generation tasks. Moreover, they made some efforts such as hierarchical attention to improve the performance. In addition to using a variant of RNN, Gulcehre et al. (2016) proposed a method to handle infrequent words in natural language generation. Note that these recent developments do"
D16-1112,P16-1014,0,0.0597202,"ural Network (RNN) and its variant have been applied successfully to various NLP tasks. For headline generation tasks, Chopra et al. (2016) exploited the RNN decoder (and its variant) with the attention mechanism instead of the method of Rush et al. (2015): the combination of the feed-forward neural network language model and attention-based sentence encoder. Nallapati et al. (2016) also adapted the RNN encoder-decoder with attention for headline generation tasks. Moreover, they made some efforts such as hierarchical attention to improve the performance. In addition to using a variant of RNN, Gulcehre et al. (2016) proposed a method to handle infrequent words in natural language generation. Note that these recent developments do not conflict with our method using the AMR encoder. This is because the AMR encoder can be straightforwardly incorporated into their methods as we have done in this paper, incorporating the AMR encoder into the baseline. We believe that our AMR encoder can possibly further improve the performance of their methods. We will test that hypothesis in future study. 6 Conclusion This paper mainly discussed the usefulness of incorporating structural syntactic and semantic information in"
D16-1112,N06-2015,0,0.0110087,"2 nato prime op 1 name … summary AMR of the input sentence “canadian” yi+1 encAMR aj … announce … … a1 &lt;s&gt; E 0 yi C+1 canada … nato headline prime op 1 name a3 country od m na m e a2 E 0 yi ABS “canadian” AMR of the input sentence Figure 2: Model structure of our proposed attentionbased AMR encoder; it outputs a headline using ABS and encoded AMR with attention. consist of English words, PropBank event predicates, and special labels such as “person”. For edges, AMR has approximately 100 relations (Banarescu et al., 2013) including semantic roles based on the PropBank annotations in OntoNotes (Hovy et al., 2006). To acquire AMRs for input sentences, we use the state-of-the-art transition-based AMR parser (Wang et al., 2015). 3.2 Attention-Based AMR Encoder Figure 2 shows a brief sketch of the model structure of our attention-based AMR encoder model. We utilize a variant of child-sum Tree-LSTM originally proposed in (Tai et al., 2015) to encode syntactic and semantic information obtained from output of the AMR parser into certain fixed-length embedding vectors. To simplify the computation, we transform a DAG structure of AMR parser output to a tree structure, which we refer to as “tree-converted AMR s"
D16-1112,W04-1013,0,0.222018,"esults if we observed statistical difference (p &lt; 0.05) between ABS (re-run) and ABS+AMR on the t-test. (R-1: ROUGE-1, R-2: ROUGE-2, R-L: ROUGE-L) For a fair comparison, we followed their evaluation setting. The training data was obtained from the first sentence and the headline of a document in the annotated Gigaword corpus (Napoles et al., 2012)4 . The development data is DUC-2003 data, and test data are both DUC-2004 (Over et al., 2007) and sentence-headline pairs obtained from the annotated Gigaword corpus as well as training data5 . All of the generated headlines were evaluated by ROUGE (Lin, 2004)6 . For evaluation on DUC2004, we removed strings after 75-characters for each generated headline as described in the DUC2004 evaluation. For evaluation on Gigaword, we forced the system outputs to be at most 8 words as in Rush et al. (2015) since the average length of headline in Gigaword is 8.3 words. For the preprocessing for all data, all letters were converted to lower case, all digits were replaced with ‘#’, and words appearing less than five times with ‘UNK’. Note that, for further evaluation, we prepared 2,000 sentence-headline pairs randomly sampled from the test data section of the G"
D16-1112,K16-1028,0,0.0442533,"/o attn) and ABS+AMR (with attention) suggests that the attention mechanism is necessary for AMR encoding. In other words, the encoder without the attention mechanism tends to be overfitting. 1058 5 Related Work Recently, the Recurrent Neural Network (RNN) and its variant have been applied successfully to various NLP tasks. For headline generation tasks, Chopra et al. (2016) exploited the RNN decoder (and its variant) with the attention mechanism instead of the method of Rush et al. (2015): the combination of the feed-forward neural network language model and attention-based sentence encoder. Nallapati et al. (2016) also adapted the RNN encoder-decoder with attention for headline generation tasks. Moreover, they made some efforts such as hierarchical attention to improve the performance. In addition to using a variant of RNN, Gulcehre et al. (2016) proposed a method to handle infrequent words in natural language generation. Note that these recent developments do not conflict with our method using the AMR encoder. This is because the AMR encoder can be straightforwardly incorporated into their methods as we have done in this paper, incorporating the AMR encoder into the baseline. We believe that our AMR e"
D16-1112,W12-3018,0,0.0606696,"Missing"
D16-1112,D15-1044,0,0.311302,"ral attention-based model. We encode results obtained from an abstract meaning representation (AMR) parser using a modified version of Tree-LSTM. Our proposed attention-based AMR encoder-decoder model improves headline generation benchmarks compared with the baseline neural attention-based model. 1 Introduction Neural network-based encoder-decoder models are cutting-edge methodologies for tackling natural language generation (NLG) tasks, i.e., machine translation (Cho et al., 2014), image captioning (Vinyals et al., 2015), video description (Venugopalan et al., 2015), and headline generation (Rush et al., 2015). This paper also shares a similar goal and motivation to previous work: improving the encoderdecoder models for natural language generation. There are several directions for enhancement. This paper respects the fact that NLP researchers have expended an enormous amount of effort to develop fundamental NLP techniques such as POS tagging, dependency parsing, named entity recognition, and semantic role labeling. Intuitively, this structural, syntactic, and semantic information underlying input text has the potential for improving the quality of NLG tasks. However, to the best of our knowledge, t"
D16-1112,P15-1150,0,0.0400517,"ing the quality of NLG tasks. However, to the best of our knowledge, there is no clear evidence that syntactic and semantic information can enhance the recently developed encoder-decoder models in NLG tasks. To answer this research question, this paper proposes and evaluates a headline generation method based on an encoder-decoder architecture on Abstract Meaning Representation (AMR). The method is essentially an extension of attention-based summarization (ABS) (Rush et al., 2015). Our proposed method encodes results obtained from an AMR parser by using a modified version of TreeLSTM encoder (Tai et al., 2015) as additional information of the baseline ABS model. Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences. We expect that the quality of headlines will improve with this reasonable combination (ABS and AMR). 2 Attention-based summarization (ABS) ABS proposed in Rush et al. (2015) has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dat"
D16-1112,N15-1173,0,0.0191451,"ormation additionally incorporated in a baseline neural attention-based model. We encode results obtained from an abstract meaning representation (AMR) parser using a modified version of Tree-LSTM. Our proposed attention-based AMR encoder-decoder model improves headline generation benchmarks compared with the baseline neural attention-based model. 1 Introduction Neural network-based encoder-decoder models are cutting-edge methodologies for tackling natural language generation (NLG) tasks, i.e., machine translation (Cho et al., 2014), image captioning (Vinyals et al., 2015), video description (Venugopalan et al., 2015), and headline generation (Rush et al., 2015). This paper also shares a similar goal and motivation to previous work: improving the encoderdecoder models for natural language generation. There are several directions for enhancement. This paper respects the fact that NLP researchers have expended an enormous amount of effort to develop fundamental NLP techniques such as POS tagging, dependency parsing, named entity recognition, and semantic role labeling. Intuitively, this structural, syntactic, and semantic information underlying input text has the potential for improving the quality of NLG ta"
D16-1112,N15-1040,0,0.0546549,"ed method encodes results obtained from an AMR parser by using a modified version of TreeLSTM encoder (Tai et al., 2015) as additional information of the baseline ABS model. Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences. We expect that the quality of headlines will improve with this reasonable combination (ABS and AMR). 2 Attention-based summarization (ABS) ABS proposed in Rush et al. (2015) has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007). Figure 1 illustrates the model structure of ABS. The model predicts a word sequence (summary) based on the combination of the neural network language model and an input sentence encoder. Let V be a vocabulary. xi is the i-th indicator vector corresponding to the i-th word in the input sentence. Suppose we have M words of an input sentence. X represents an input sentence, which 1054 Proceedings of the 2016 Conference on Empirical Methods in Natural Language"
D18-1203,Q17-1010,0,0.0317567,"on in the context of machine translation (Section 6.3). 6.1 PHSIC Settings To take advantage of recent developments in representation learning, we used several pre-trained models for encoding sentences into vectors and several kernels between these vectors for PHSIC. Encoders As sentence encorders, we used two pre-trained models without fine-tuning. First, the sum of the word vectors effectively represents a sentence (Mikolov et al., 2013a): P P x = w∈x vec(w), y = w∈y vec(w). (22) For vec(·), we used the pre-trained fastText model7 , which is a high-accuracy and popular word embedding model (Bojanowski et al., 2017); models in 157 languages are publicly distributed (Grave et al., 2018). Second, we also used a DNN-based sentence encoder, called the universal sentence encoder (Cer et al., 2018), which utilizes the deep averaging network (DAN) (Iyyer et al., 2015). The pre-trained model for English sentences we used is publicly available8 . Kernels As kernels between these vectors, we used cosine similarity (cos) k(x, x0 ) = cos(x, x0 ) (23) and the Gaussian kernel (also known as the radial basis function kernel; RBF kernel)   kx − x0 k22 0 k(x, x ) = exp − , (24) 2σ 2 and similarly for `(y, y 0 ). The ex"
D18-1203,D18-2029,0,0.0224654,"ding sentences into vectors and several kernels between these vectors for PHSIC. Encoders As sentence encorders, we used two pre-trained models without fine-tuning. First, the sum of the word vectors effectively represents a sentence (Mikolov et al., 2013a): P P x = w∈x vec(w), y = w∈y vec(w). (22) For vec(·), we used the pre-trained fastText model7 , which is a high-accuracy and popular word embedding model (Bojanowski et al., 2017); models in 157 languages are publicly distributed (Grave et al., 2018). Second, we also used a DNN-based sentence encoder, called the universal sentence encoder (Cer et al., 2018), which utilizes the deep averaging network (DAN) (Iyyer et al., 2015). The pre-trained model for English sentences we used is publicly available8 . Kernels As kernels between these vectors, we used cosine similarity (cos) k(x, x0 ) = cos(x, x0 ) (23) and the Gaussian kernel (also known as the radial basis function kernel; RBF kernel)   kx − x0 k22 0 k(x, x ) = exp − , (24) 2σ 2 and similarly for `(y, y 0 ). The experiments are ran with hyperparameter σ = 1.0 for the RBF kernel, and d = 100 for incomplete Cholesky decomposition (for more detail, see Section B). 6.2 Ranking: Dialogue Response"
D18-1203,P08-1090,0,0.0893409,"Missing"
D18-1203,P89-1010,0,0.602018,"we also demonstrate that PHSIC is beneficial as a criterion of a data selection task for machine translation owing to its ability to give high (low) scores to a consistent (inconsistent) pair with other pairs. 1 Kentaro Inui 1,2 PMI log P log n · c(x, y) P c(x, y 0 ) x0 c(x0, y) y0 X Eq. 1 b RNN (y|x) P b RNN (y) P Eq. 2 X PHSIC bXY (ψ(y)−ψ(y)) Sec. 5.1 (φ(x)−φ(x))> C X X bICD (b−b) (a−a)> C X X Sec. 5.2 Table 1: The proposed co-occurrence norm, PHSIC, eliminates the trade-off between robustness to data sparsity and learning time, which PMI has (Section 1). Pointwise mutual information (PMI) (Church and Hanks, 1989) is frequently used to model the co-occurrence strength of linguistic expression pairs. There are two typical types of PMI estimation (computation) method. One is a countingbased estimator using maximum likelihood estimation, sometimes with smoothing techniques, for example, Introduction Computing the co-occurrence strength between two linguistic expressions is a fundamental task in natural language processing (NLP). For example, in collocation extraction (Manning and Sch¨utze, 1999), word bigrams are collected from corpora and then strongly co-occurring bigrams (e.g., “New York”) are found. I"
D18-1203,N13-1073,0,0.0352961,"R; Recall@1,2. The best result in each column is in bold. The other notation is the same as in Table 4. Selection Criteria (all the training set) Random fast align PHSIC # of Selected Data K 0.5M 1M 3M - - 41.02 34.26 39.82 - 38.63 40.56 - Encoder Kernel fastText RBF 38.95 40.95 - Table 6: BLEU scores with the Transformer for each data selection criterion and each size of selected data K for the parallel corpus filtering task.“Random” represents the baseline method of selecting sentences at random. Baseline Measure As a baseline measure, we utilize a publicly available script17 of fast align (Dyer et al., 2013), which is one of the state-of-theart word aligner. We firstly used the fast align for the training set D = {(xi , yi )}i to obtain the word alignment between each sentence pair (xi , yi ), i.e., a set of aligned word pairs with its probabilities. We then computed the co-occurrence score of (xi , yi ) with sentence-length normalization, i.e., the average log probability of aligned word pairs. Experimental Results Table 6 shows the results of our data selection evaluation. It is common knowledge in NMT that more data gives better performance in general. However, we observed that PHSIC successfu"
D18-1203,L18-1550,0,0.013819,"o take advantage of recent developments in representation learning, we used several pre-trained models for encoding sentences into vectors and several kernels between these vectors for PHSIC. Encoders As sentence encorders, we used two pre-trained models without fine-tuning. First, the sum of the word vectors effectively represents a sentence (Mikolov et al., 2013a): P P x = w∈x vec(w), y = w∈y vec(w). (22) For vec(·), we used the pre-trained fastText model7 , which is a high-accuracy and popular word embedding model (Bojanowski et al., 2017); models in 157 languages are publicly distributed (Grave et al., 2018). Second, we also used a DNN-based sentence encoder, called the universal sentence encoder (Cer et al., 2018), which utilizes the deep averaging network (DAN) (Iyyer et al., 2015). The pre-trained model for English sentences we used is publicly available8 . Kernels As kernels between these vectors, we used cosine similarity (cos) k(x, x0 ) = cos(x, x0 ) (23) and the Gaussian kernel (also known as the radial basis function kernel; RBF kernel)   kx − x0 k22 0 k(x, x ) = exp − , (24) 2σ 2 and similarly for `(y, y 0 ). The experiments are ran with hyperparameter σ = 1.0 for the RBF kernel, and d"
D18-1203,N16-1162,0,0.0410635,"Missing"
D18-1203,P15-1162,0,0.0572835,"Missing"
D18-1203,W17-5715,0,0.0223865,"Missing"
D18-1203,N16-1014,0,0.395639,". In either case, a set of linguistic expression pairs D = {(xi , yi )}ni=1 is first collected and then the co-occurrence strength of a (new) pair (x, y) is computed. n · c(x, y) P , 0 0 y 0 c(x, y ) x0 c(x , y) d MLE (x, y; D) = log P PMI (1) where c(x, y) denotes the frequency of the pair (x, y) in given data D. This is easy to compute and is commonly used to measure co-occurrence between words, such as in collocation extraction1 ; however, when data D is sparse, i.e., when x or y is a phrase or sentence, this approach is unrealistic. The second method uses recurrent neural networks (RNNs). Li et al. (2016) proposed to em1 In collocation extraction, simple counting c(x, y) ∝ b P(x, y), rather than PMI, ranks undesirable function-word pairs (e.g., “of the”) higher (Manning and Sch¨utze, 1999). 1763 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1763–1775 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ploy PMI to suppress dull responses for utterance generation in dialogue systems2 . They estimated P(y) and P(y|x) using RNN language models and estimated PMI as follows: b d RNN (x, y; D) = log PRNN (y"
D18-1203,W15-4640,0,0.0969236,"e co-occurrence strength of linguistic expression pairs. There are two typical types of PMI estimation (computation) method. One is a countingbased estimator using maximum likelihood estimation, sometimes with smoothing techniques, for example, Introduction Computing the co-occurrence strength between two linguistic expressions is a fundamental task in natural language processing (NLP). For example, in collocation extraction (Manning and Sch¨utze, 1999), word bigrams are collected from corpora and then strongly co-occurring bigrams (e.g., “New York”) are found. In dialogue response selection (Lowe et al., 2015), pairs comprising a context and its response sentence are collected from dialogue corpora and the goal is to rank the candidate responses for each given context sentence. In either case, a set of linguistic expression pairs D = {(xi , yi )}ni=1 is first collected and then the co-occurrence strength of a (new) pair (x, y) is computed. n · c(x, y) P , 0 0 y 0 c(x, y ) x0 c(x , y) d MLE (x, y; D) = log P PMI (1) where c(x, y) denotes the frequency of the pair (x, y) in given data D. This is easy to compute and is commonly used to measure co-occurrence between words, such as in collocation extrac"
D18-1203,E06-1015,0,0.148985,"Missing"
D18-1203,Q16-1019,0,0.0607695,"Missing"
D18-1203,P02-1040,0,0.100679,"sed in the NMT community and known as one of the current state-of-the-art architectures. We utilized fairseq15 , a publicly available tool for neural sequence-to-sequence models, for building our models. Experimental Procedure We used the following procedure for this evaluation: (1) rank all parallel sentences in a given parallel corpus according to each criterion, (2) extract the top K ranked parallel sentences, (3) train the NMT model using the extracted parallel sentences, and (4) evaluate the translation quality of the test data using a typical MT automatic evaluation measure, i.e., BLEU (Papineni et al., 2002)16 . In our experiments we evaluated PHSIC with K = 0.5M and 1M. 13 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2017/ 15 https://github.com/pytorch/fairseq 16 We used multi-bleu.perl in the Moses tool (https:// github.com/moses-smt/mosesdecoder). 14 1769 Models Config Size of Training Set n 104 105 103 Chance Level 5 × 105 .50; .29; .10, .20 .50; .29; .10, .20 .50; .29; .10, .20 .50; .29; .10, .20 Dim. Init. b RNN (y) P 1200 fastText .50; .29; .10, .21 .50; .30; .11, .21 .50; .30; .10, .21 .50; .30; .13, .25 b RNN (y|x) P 1200 fastText .50; .29; .10, .21 .50;"
D18-1203,D14-1162,0,0.0846055,"Missing"
D18-1203,N15-1020,0,0.0131251,"experiments are ran with hyperparameter σ = 1.0 for the RBF kernel, and d = 100 for incomplete Cholesky decomposition (for more detail, see Section B). 6.2 Ranking: Dialogue Response Selection In the first experiment, we applied PHSIC as a ranking criterion of the task of dialogue response 7 https://fasttext.cc/docs/en/english-vectors. html, https://fasttext.cc/docs/en/crawl-vectors. html 8 https://www.tensorflow.org/hub/modules/google/ universal-sentence-encoder/1 1767 Experimental Settings Dataset For the training data, we gathered approximately 5 × 105 reply chains from Twitter, following Sordoni et al. (2015)9 . In addition, we randomly selected {103 , 104 , 105 } reply chains from that dataset. Using these small subsets, we confirmed the effect of the difference in the size of the training set (data sparseness) on the learning time and predictive performance. For validation and test data, we used a small (approximately 2000 pairs each) but highly reliable dataset created by Sordoni et al. (2015)10 , which consists only of conversations given high scores by human annotators. Therefore, this set was not expected to include dull responses. For each dataset, we converted each contextmessage-response"
D18-1489,A00-2018,0,0.377682,") and reranking them based on the perplexity obtained by a neural language model. To investigate the effectiveness of DOC, we evaluate our language models following their configurations. 7.1 Dataset We used the Wall Street Journal of the Penn Treebank dataset. We used the section 2-21 for training, 22 for validation, and 23 for testing. We applied the preprocessing codes of Choe and Charniak (2016)12 to the dataset and converted a token that appears fewer than ten times in the training dataset into a special token unk. For reranking, we prepared 500 candidates obtained by the Charniak parser (Charniak, 2000). 7.2 Models We compare AWD-LSTM-DOC with AWDLSTM (Merity et al., 2018) and AWD-LSTMMoS (Yang et al., 2018). We trained each model with the same hyperparameters from our language modeling experiments (Section 5). We selected the model that achieved the best perplexity on the validation set during the training. 7.3 Results Table 11 shows the bracketing F1 scores on the PTB test set. This table is divided into three parts by horizontal lines; the upper part describes the scores by single language modeling based rerankers, the middle part shows the results by ensembling five rerankers, and the lo"
D18-1489,P96-1041,0,0.345939,"ev and Klein, 2018). 8 Related Work Bengio et al. (2003) are pioneers of neural language models. To address the curse of dimensionality in language modeling, they proposed a method using word embeddings and a feed-forward neural network (FFNN). They demonstrated that their approach outperformed n-gram language models, but FFNN can only handle fixed-length contexts. Instead of FFNN, Mikolov et al. (2010) applied RNN (Elman, 1990) to language modeling to address the entire given sequence as a context. Their method outperformed the Kneser-Ney smoothed 5-gram language model (Kneser and Ney, 1995; Chen and Goodman, 1996). Researchers continue to try to improve the performance of RNN language models. Zaremba et al. (2014) used LSTM (Hochreiter and Schmidhuber, 1997) instead of a simple RNN for language modeling and significantly improved an RNN language model by applying dropout (Srivastava et al., 2014) to all the connections except for the recurrent connections. To regularize the recurrent connections, Gal and Ghahramani (2016) proposed variational inference-based dropout. Their method uses the same dropout mask at each timestep. Zolna et al. (2018) proposed fraternal dropout, which minimizes the differences"
D18-1489,D16-1257,0,0.358111,"ps://github.com/nttcslabnlp/doc lm. 1 p(w1:T ) = p(w1 ) TY −1 p(wt+1 |w1:t ). (1) t=1 Introduction Neural network language models have played a central role in recent natural language processing (NLP) advances. For example, neural encoderdecoder models, which were successfully applied to various natural language generation tasks including machine translation (Sutskever et al., 2014), summarization (Rush et al., 2015), and dialogue (Wen et al., 2015), can be interpreted as conditional neural language models. Neural language models also positively influence syntactic parsing (Dyer et al., 2016; Choe and Charniak, 2016). Moreover, such word embedding methods as Skipgram (Mikolov et al., 2013) and vLBL (Mnih and Kavukcuoglu, 2013) originated from neural language models designed to handle much larger vocabulary and data sizes. Neural language models can also be used as contextualized word representations (Peters et al., 2018). Thus, language modeling is a good benchmark task for investigating the general frameworks of neural methods in NLP field. p(w1 ) is generally assumed to be 1 in this literature, that is, p(w1 ) = 1, and thus we can ignore its calculation. See the implementation of Zaremba et al. (2014)1"
D18-1489,N16-1024,0,0.259497,"y available at: https://github.com/nttcslabnlp/doc lm. 1 p(w1:T ) = p(w1 ) TY −1 p(wt+1 |w1:t ). (1) t=1 Introduction Neural network language models have played a central role in recent natural language processing (NLP) advances. For example, neural encoderdecoder models, which were successfully applied to various natural language generation tasks including machine translation (Sutskever et al., 2014), summarization (Rush et al., 2015), and dialogue (Wen et al., 2015), can be interpreted as conditional neural language models. Neural language models also positively influence syntactic parsing (Dyer et al., 2016; Choe and Charniak, 2016). Moreover, such word embedding methods as Skipgram (Mikolov et al., 2013) and vLBL (Mnih and Kavukcuoglu, 2013) originated from neural language models designed to handle much larger vocabulary and data sizes. Neural language models can also be used as contextualized word representations (Peters et al., 2018). Thus, language modeling is a good benchmark task for investigating the general frameworks of neural methods in NLP field. p(w1 ) is generally assumed to be 1 in this literature, that is, p(w1 ) = 1, and thus we can ignore its calculation. See the implementation"
D18-1489,P17-2025,0,0.0638382,"Missing"
D18-1489,D15-1166,0,0.0301067,"ted sentenceheadline pairs into three parts: training, validation, and test sets. The training set contains about 3.8M sentence-headline pairs. For our evaluation, we used the test set constructed by Zhou et al. (2017) because the one constructed by Rush et al. (2015) contains some invalid instances, as reported in Zhou et al. (2017). 6.2 Encoder-Decoder Model For the base model, we adopted an encoder-decoder with an attention mechanism described in Kiyono et al. (2017). The encoder consists of a 2-layer bidirectional LSTM, and the decoder consists of a 2-layer LSTM with attention proposed by Luong et al. (2015). We interpreted the layer after computing the attention as the 3rd layer of the decoder. We refer to this encoder-decoder as EncDec. For the hyperparameters, we followed the setting of Kiyono et al. (2017) except for the sizes of hidden 9 De-En 28.18 29.12 29.33 En-Fr 34.37 36.09 36.11 Fr-En 34.07 34.41 34.72 Table 9: BLEU scores on test sets in the IWSLT 2016 dataset. We report averages of three runs. Experiments on Application Tasks 6.1 En-De 23.05 23.62 23.97 RG-1 46.77 46.91 46.99 37.41 46.86 46.34 RG-2 24.87 24.91 25.29 15.87 24.58 24.85 RG-L 43.58 43.73 43.83 34.70 43.53 43.49 Table 10:"
D18-1489,J93-2004,0,0.0612474,"for a mini-batch consisting of wb , wb+1 , ..., w˜b : B= ˜b X πct Vocab Train #Token Valid Test β= std(B) avg(B) Hyperparameter Learning rate Batch size Non-monotone interval De Dh1 Dh2 Dh3 Dropout rate for xt Dropout rate for h0t Dropout rate for h1t , h2t Dropout rate for h3t Dropout rate for kj,ct Recurrent weight dropout (12) 2 , (13) where functions std(·) and avg(·) are functions that respectively return an input’s standard deviation and its average. In the training step, we add λβ multiplied by weight coefficient β to the loss function. 5 5.1 Datasets We used the Penn Treebank (PTB) (Marcus et al., 1993) and WikiText-2 (Merity et al., 2017) datasets, which are the standard benchmark datasets for the word-level language modeling task. Mikolov et al. (2010) and Merity et al. (2017) respectively published preprocessed PTB3 and WikiText-24 datasets. Table 1 describes their statistics. We used these preprocessed datasets for fair comparisons with previous studies. 5.2 Hyperparameters Our implementation is based on the averaged stochastic gradient descent Weight-Dropped LSTM (AWD-LSTM)5 proposed by Merity et al. (2018). AWD-LSTM consists of three LSTMs with various regularizations. For the hyperpar"
D18-1489,P18-1249,0,0.0249447,"ing order because it samples candidates based on their scores. Thus, we prepared more candidates (i.e., 700) to be able to obtain correct instances as candidates. 14 We used the deep bidirectional encoder described at http://opennmt.net/OpenNMT/training/models/ instead of a basic bidirectional encoder. 4606 the probability distributions in the decoder. The retrained parser achieved 93.12 F1 score. Finally, we achieved 94.47 F1 score by reranking its candidates with AWD-LSTM-DOC. We expect that we can achieve even better score by replacing the base parser with the current state-of-the-art one (Kitaev and Klein, 2018). 8 Related Work Bengio et al. (2003) are pioneers of neural language models. To address the curse of dimensionality in language modeling, they proposed a method using word embeddings and a feed-forward neural network (FFNN). They demonstrated that their approach outperformed n-gram language models, but FFNN can only handle fixed-length contexts. Instead of FFNN, Mikolov et al. (2010) applied RNN (Elman, 1990) to language modeling to address the entire given sequence as a context. Their method outperformed the Kneser-Ney smoothed 5-gram language model (Kneser and Ney, 1995; Chen and Goodman, 1"
D18-1489,W12-3018,0,0.0661917,"Missing"
D18-1489,N18-1202,0,0.0245701,"generation tasks including machine translation (Sutskever et al., 2014), summarization (Rush et al., 2015), and dialogue (Wen et al., 2015), can be interpreted as conditional neural language models. Neural language models also positively influence syntactic parsing (Dyer et al., 2016; Choe and Charniak, 2016). Moreover, such word embedding methods as Skipgram (Mikolov et al., 2013) and vLBL (Mnih and Kavukcuoglu, 2013) originated from neural language models designed to handle much larger vocabulary and data sizes. Neural language models can also be used as contextualized word representations (Peters et al., 2018). Thus, language modeling is a good benchmark task for investigating the general frameworks of neural methods in NLP field. p(w1 ) is generally assumed to be 1 in this literature, that is, p(w1 ) = 1, and thus we can ignore its calculation. See the implementation of Zaremba et al. (2014)1 , for an example. RNN language models obtain conditional probability p(wt+1 |w1:t ) from the probability distribution of each word. To compute the probability distribution, RNN language models encode sequence w1:t into a fixed-length vector and apply a transformation matrix and the softmax function. Previous"
D18-1489,E17-2025,0,0.0351521,"To regularize the recurrent connections, Gal and Ghahramani (2016) proposed variational inference-based dropout. Their method uses the same dropout mask at each timestep. Zolna et al. (2018) proposed fraternal dropout, which minimizes the differences between outputs from different dropout masks to be invariant to the dropout mask. Melis et al. (2018) used black-box optimization to find appropriate hyperparameters for RNN language models and demonstrated that the standard LSTM with proper regularizations can outperform other architectures. Apart from dropout techniques, Inan et al. (2017) and Press and Wolf (2017) proposed the word tying method (WT), which unifies word embeddings (E in Equation 4) with the weight matrix to compute probability distributions (W in Equation 2). In addition to quantitative evaluation, Inan et al. (2017) provided a theoretical justification for WT and proposed the augmented loss technique (AL), which computes an objective probability based on word embeddings. In addition to these regularization techniques, Merity et al. (2018) used DropConnect (Wan et al., 2013) and averaged SGD (Polyak and Juditsky, 1992) for an LSTM language model. Their AWD-LSTM achieved lower perplexity"
D18-1489,D15-1044,0,0.43792,"e the standard benchmark datasets. Moreover, we indicate our proposed method contributes to two application tasks: machine translation and headline generation. Our code is publicly available at: https://github.com/nttcslabnlp/doc lm. 1 p(w1:T ) = p(w1 ) TY −1 p(wt+1 |w1:t ). (1) t=1 Introduction Neural network language models have played a central role in recent natural language processing (NLP) advances. For example, neural encoderdecoder models, which were successfully applied to various natural language generation tasks including machine translation (Sutskever et al., 2014), summarization (Rush et al., 2015), and dialogue (Wen et al., 2015), can be interpreted as conditional neural language models. Neural language models also positively influence syntactic parsing (Dyer et al., 2016; Choe and Charniak, 2016). Moreover, such word embedding methods as Skipgram (Mikolov et al., 2013) and vLBL (Mnih and Kavukcuoglu, 2013) originated from neural language models designed to handle much larger vocabulary and data sizes. Neural language models can also be used as contextualized word representations (Peters et al., 2018). Thus, language modeling is a good benchmark task for investigating the general frame"
D18-1489,P16-1009,0,0.0259373,"in the IWSLT 2016 dataset. We report averages of three runs. Experiments on Application Tasks 6.1 En-De 23.05 23.62 23.97 RG-1 46.77 46.91 46.99 37.41 46.86 46.34 RG-2 24.87 24.91 25.29 15.87 24.58 24.85 RG-L 43.58 43.73 43.83 34.70 43.53 43.49 Table 10: ROUGE F1 scores in headline generation test data provided by Zhou et al. (2017). RG in table denotes ROUGE. For our implementations (the upper part), we report averages of three runs. states and embeddings. We used 500 for machine translation and 400 for headline generation. We constructed a vocabulary set by using Byte-PairEncoding10 (BPE) (Sennrich et al., 2016). We set the number of BPE merge operations at 16K for the machine translation and 5K for the headline generation. In this experiment, we compare DOC to the base EncDec. We prepared two DOC settings: using only the final layer, that is, a setting that is identical to MoS, and using both the final and middle layers. We used the 2nd and 3rd layers in the latter setting because this case achieved the best performance on the language modeling task in Section 5.3. We set i3 = 2 and i2 = 2, i3 = 2. For this experiment, we modified a publicly available encode-decoder implementation11 . 6.3 Results Ta"
D18-1489,P18-2097,1,0.851186,"esults indicate that DOC positively influences a neural encoder-decoder model. Using the middle layer also yields further improvement because EncDec+DOC (i3 = i2 = 2) outperformed EncDec+DOC (i3 = 2). 7 Model Base Reranking with single model Choe and Charniak (2016) 89.7 AWD-LSTM 89.7 AWD-LSTM-MoS 89.7 AWD-LSTM-DOC 89.7 Reranking with model ensemble AWD-LSTM × 5 (ensemble) 89.7 AWD-LSTM-MoS × 5 (ensemble) 89.7 AWD-LSTM-DOC × 5 (ensemble) 89.7 AWD-LSTM-DOC × 5 (ensemble) 91.2 AWD-LSTM-DOC × 5 (ensemble) 93.12 State-of-the-art results Dyer et al. (2016) 91.7 Fried et al. (2017) (ensemble) 92.72 Suzuki et al. (2018) (ensemble) 92.74 Kitaev and Klein (2018) 95.13 Experiments on Constituency Parsing Choe and Charniak (2016) achieved high F1 scores on the Penn Treebank constituency parsing task by transforming candidate trees into a symbol sequence (S-expression) and reranking them based on the perplexity obtained by a neural language model. To investigate the effectiveness of DOC, we evaluate our language models following their configurations. 7.1 Dataset We used the Wall Street Journal of the Penn Treebank dataset. We used the section 2-21 for training, 22 for validation, and 23 for testing. We applied th"
D18-1489,I17-2008,1,0.256626,"the number of kj,ct from hnt . Then we PNdefine the sum of in for all n as J; that is, n=0 in = J. In short, DOC computes J probability distributions from all the layers, including the input embedding (h0 ). For iN = J, DOC becomes identical to MoS. In addition to increasing the rank, we expect that DOC weakens the vanishing gradient problem during backpropagation because a middle layer is directly connected to the output, such as with the auxiliary classifiers described in Szegedy et al. (2015). For a network that computes the weights for several vectors, such as Equation 10, Shazeer et al. (2017) indicated that it often converges to a state where it always produces large weights for few vectors. In fact, we observed that DOC tends to assign large weights to shallow layers. To prevent this phenomenon, we compute the coefficient of 4601 variation of Equation 10 in each mini-batch as a regularization term following Shazeer et al. (2017). In other words, we try to adjust the sum of the weights for each probability distribution with identical values in each mini-batch. Formally, we compute the following equation for a mini-batch consisting of wb , wb+1 , ..., w˜b : B= ˜b X πct Vocab Train"
D18-1489,D15-1199,0,0.0428095,"Missing"
D18-1489,P17-1101,0,0.0291304,"En), from English to French (En-Fr), and its reverse (Fr-En). Headline generation is a task that creates a short summarization of an input sentence(Rush et al., 2015). Rush et al. (2015) constructed a headline generation dataset by extracting pairs of first sentences of news articles and their headlines from the annotated English Gigaword corpus (Napoles et al., 2012). They also divided the extracted sentenceheadline pairs into three parts: training, validation, and test sets. The training set contains about 3.8M sentence-headline pairs. For our evaluation, we used the test set constructed by Zhou et al. (2017) because the one constructed by Rush et al. (2015) contains some invalid instances, as reported in Zhou et al. (2017). 6.2 Encoder-Decoder Model For the base model, we adopted an encoder-decoder with an attention mechanism described in Kiyono et al. (2017). The encoder consists of a 2-layer bidirectional LSTM, and the decoder consists of a 2-layer LSTM with attention proposed by Luong et al. (2015). We interpreted the layer after computing the attention as the 3rd layer of the decoder. We refer to this encoder-decoder as EncDec. For the hyperparameters, we followed the setting of Kiyono et al."
D19-1054,P18-1063,0,0.241541,"The encoder first encodes each xi into a vector hi . At each time step, the decoder pays attentions to some source embeddings and outputs the probability of the next token by p(yt |y1:t−1 , Ct ). Ct is a weighted average of source embeddings: X Ct = αt,i hi discrepancy when integrating them together. 2. Soft-select: Learn a soft mask to filter useless information (Mei et al., 2016; Zhou et al., 2017). However, the mask is deterministic without any probabilistic variations, making it hard to model the content-level diversity. 3. Reinforce-select: Train the selector with reinforcement learning (Chen and Bansal, 2018), which has high training variance and low diversity on content selection. i ef (hi ,dt ) αt,i = P f (h ,d ) j t je In this paper, we treat the content selection as latent variables and train with amortized variational inference (Kingma and Welling, 2014; Mnih and Gregor, 2014). This provides a lower training variance than Reinforce-select. The selector and generator are co-trained within the same objective, the generations are thus more faithful to the selected contents than Bottom-up methods. Our model is task-agnostic, end-to-end trainable and can be seamlessly inserted into any encoder-dec"
D19-1054,D18-1443,0,0.369633,"e supposed to have two levels of diversity: (1) content-level diversity reflecting multiple possibilities of content selection (what to say) and (2) surface-level diversity reflecting the linguistic variations of verbalizing the selected contents (how to say) (Reiter and Dale, 2000; Nema et al., 2017). Recent neural network models handle these tasks with the encoder-decoder (Enc-Dec) framework (Sutskever et al., 2014; Bahdanau et al., 2015), which simultaneously performs selecting and verbalizing in a 1. Bottom-up: Train a separate content selector to constrain the attention to source tokens (Gehrmann et al., 2018), but the separate training of selector/generator might lead to ∗ Work mostly done while at RIKEN AIP. Correspondence to xshen@mpi-inf.mpg.de 1 The source code is available on https://github. com/chin-gyou/controllable-selection 579 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 579–590, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2015; Xu et al., 2015). The encoder first encodes each xi into a vector hi . At each time step, the dec"
D19-1054,D16-1011,0,0.0681628,"Missing"
D19-1054,D18-1205,0,0.079091,"), then decode with pθ (Y |X, β). Source-text pairs are available for training, but the ground-truth content selection for each pair is unknown. 3.1 3.2 Soft-select falls back on a deterministic network to output the likelihood function’s first-order Taylor series approximation expanded at Eβ∼B(γ) β: Bottom-up log Eβ∼B(γ) pθ (Y |X, β) The most intuitive way is training the content selector to target some heuristically extracted contents. For example, we can train the selector to select overlapped words between the source and target (Gehrmann et al., 2018), sentences with higher tf-idf scores (Li et al., 2018) or identified image objects that appear in the caption (Wang et al., 2017). A standard encoder-decoder model is independently trained. In the testing stage, the prediction of the content selector is used to hard-mask the attention vector to guide the text generation in a bottom-up way. Though easy to train, Bottomup generation has the following two problems: (1) The heuristically extracted contents might be coarse and cannot reflect the variety of human languages and (2) The selector and decoder are independently trained towards different objectives thus might not adapt to each other well. β"
D19-1054,P18-2027,0,0.0154788,"and target. In Gigaword, as the headline is more abstractive, we select the closest source word for each target word in the embedding space. Stop words and punctuations are prohibited from being selected. Choice of α/: As seen in Sec 3.5, we need to set the hyperparameter α for RS/SS and  for Most content selection models train the selector with heuristic rules (Hsu et al., 2018; Li et al., 2018; Yu et al., 2018; Gehrmann et al., 2018; Yao et al., 2019; Moryossef et al., 2019), which fail to fully capture the relation between selection and generation. Mei et al. (2016); Zhou et al. (2017); Lin et al. (2018); Li et al. (2018) “soft-select” word or sentence embeddings based on a gating function. The output score from the gate is a deterministic vector without any probabilistic variations, so controlling the selection to generate diverse text is impossible. Very few works explicitly define a bernoulli distribution for the selector, then train with the REINFORCE algorithm (Ling and Rush, 2017; Chen and Bansal, 2018), but the selection targets at a high recall regardless of the low precision, so the controllability over generated text is weak. Fan et al. (2018) control the generation by manually conc"
D19-1054,P18-1013,0,0.0209586,"a bi-LSTM encoder for better performance. Heuristically extracted content: This is used to train the selector for bottom up models and pretrain the RS and VRS model. For wikibio, we simply extract overlapped words between the source and target. In Gigaword, as the headline is more abstractive, we select the closest source word for each target word in the embedding space. Stop words and punctuations are prohibited from being selected. Choice of α/: As seen in Sec 3.5, we need to set the hyperparameter α for RS/SS and  for Most content selection models train the selector with heuristic rules (Hsu et al., 2018; Li et al., 2018; Yu et al., 2018; Gehrmann et al., 2018; Yao et al., 2019; Moryossef et al., 2019), which fail to fully capture the relation between selection and generation. Mei et al. (2016); Zhou et al. (2017); Lin et al. (2018); Li et al. (2018) “soft-select” word or sentence embeddings based on a gating function. The output score from the gate is a deterministic vector without any probabilistic variations, so controlling the selection to generate diverse text is impossible. Very few works explicitly define a bernoulli distribution for the selector, then train with the REINFORCE algorith"
D19-1054,W17-4505,0,0.299395,"on weight will first be “soft-masked” by γ before being passed to the decoder. soft-select is fully differentiable and can be easily trained by gradient descent. However, this soft-approximation is normally inaccurate, especially when B(γ) has a high entropy, which is common in one-to-many text generation tasks. The gap between log Eβ∼B(γ) pθ (Y |X, β) and log pθ (Y |X, Eβ∼B(γ) ) will be large (Ma et al., 2017; Deng et al., 2018). In practice, this would lead to unrealistic generations when sampling β from the deterministically trained distribution. 3.3 Reinforce-Select Reinforce-select (RS) (Ling and Rush, 2017; Chen and Bansal, 2018) utilizes reinforcement learning to approximate the marginal likelihood. Specifically, it is trained to maximize a lower bound of the likelihood by applying the Jensen inequalily: log Eβ∼B(γ) pθ (Y |X, β) ≥ Eβ∼B(γ) log pθ (Y |X, β) The gradient to γ is approximated with MonteCarlo sampling by applying the REINFORCE algorithm (Williams, 1992; Glynn, 1990). To speed up convergence, we pre-train the selector by some distant supervision, which is a common practice in reinforcement learning. REINFORCE is unbiased but has a high variance. Many research have proposed sophistic"
D19-1054,D15-1166,0,0.0535494,"elector and generated text (Alemi et al., 2018; Zhao et al., 2018). A higher CMI leads to stronger controllability with a bit more risk of text disfluency. In summary, our contributions are (1) systematically studying the problem of controllable content selection for Enc-Dec text generation, (2) proposing a task-agnostic training framework achieving promising results and (3) introducing an effective way to achieve the trade-off between performance and controllability. 2 (1) dt is the hidden state of the decoder at time step t. f is a score function to compute the similarity between hi and dt (Luong et al., 2015). 3 Content Selection Our goal is to decouple the content selection from the decoder by introducing an extra content selector. We hope the content-level diversity can be fully captured by the content selector for a more interpretable and controllable generation process. Following Gehrmann et al. (2018); Yu et al. (2018), we define content selection as a sequence labeling task. Let β1 , β2 , . . . , βn denote a sequence of binary selection masks. βi = 1 if hi is selected and 0 otherwise. βi is assumed to be independent from each other and is sampled from a bernoulli distribution B(γi )2 . γi is"
D19-1054,N16-1086,0,0.493898,"in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 579–590, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2015; Xu et al., 2015). The encoder first encodes each xi into a vector hi . At each time step, the decoder pays attentions to some source embeddings and outputs the probability of the next token by p(yt |y1:t−1 , Ct ). Ct is a weighted average of source embeddings: X Ct = αt,i hi discrepancy when integrating them together. 2. Soft-select: Learn a soft mask to filter useless information (Mei et al., 2016; Zhou et al., 2017). However, the mask is deterministic without any probabilistic variations, making it hard to model the content-level diversity. 3. Reinforce-select: Train the selector with reinforcement learning (Chen and Bansal, 2018), which has high training variance and low diversity on content selection. i ef (hi ,dt ) αt,i = P f (h ,d ) j t je In this paper, we treat the content selection as latent variables and train with amortized variational inference (Kingma and Welling, 2014; Mnih and Gregor, 2014). This provides a lower training variance than Reinforce-select. The selector and g"
D19-1054,N19-1236,0,0.0404738,"ain the selector for bottom up models and pretrain the RS and VRS model. For wikibio, we simply extract overlapped words between the source and target. In Gigaword, as the headline is more abstractive, we select the closest source word for each target word in the embedding space. Stop words and punctuations are prohibited from being selected. Choice of α/: As seen in Sec 3.5, we need to set the hyperparameter α for RS/SS and  for Most content selection models train the selector with heuristic rules (Hsu et al., 2018; Li et al., 2018; Yu et al., 2018; Gehrmann et al., 2018; Yao et al., 2019; Moryossef et al., 2019), which fail to fully capture the relation between selection and generation. Mei et al. (2016); Zhou et al. (2017); Lin et al. (2018); Li et al. (2018) “soft-select” word or sentence embeddings based on a gating function. The output score from the gate is a deterministic vector without any probabilistic variations, so controlling the selection to generate diverse text is impossible. Very few works explicitly define a bernoulli distribution for the selector, then train with the REINFORCE algorithm (Ling and Rush, 2017; Chen and Bansal, 2018), but the selection targets at a high recall regardles"
D19-1054,P17-1098,0,0.0234923,"ve concerns properly. Current methods can be categorized into the following three classes and have different limits: Introduction Many text generation tasks, e.g., data-to-text, summarization and image captioning, can be naturally divided into two steps: content selection and surface realization. The generations are supposed to have two levels of diversity: (1) content-level diversity reflecting multiple possibilities of content selection (what to say) and (2) surface-level diversity reflecting the linguistic variations of verbalizing the selected contents (how to say) (Reiter and Dale, 2000; Nema et al., 2017). Recent neural network models handle these tasks with the encoder-decoder (Enc-Dec) framework (Sutskever et al., 2014; Bahdanau et al., 2015), which simultaneously performs selecting and verbalizing in a 1. Bottom-up: Train a separate content selector to constrain the attention to source tokens (Gehrmann et al., 2018), but the separate training of selector/generator might lead to ∗ Work mostly done while at RIKEN AIP. Correspondence to xshen@mpi-inf.mpg.de 1 The source code is available on https://github. com/chin-gyou/controllable-selection 579 Proceedings of the 2019 Conference on Empirical"
D19-1054,P04-1011,0,0.221726,"Missing"
D19-1054,D14-1162,0,0.0812737,"Missing"
D19-1054,N19-1269,0,0.0384767,"Missing"
D19-1054,D18-1411,0,0.0685899,"elected : sri lankan, announced, closure, schools Text: sri lanka declares closure of schools. Table 1: Headline generation examples from our model. We can generate text describing various contents by sampling different content selections. The selected source word and its corresponding realizations in the text are highlighted with the same color. black-box way. Therefore, both levels of diversity are entangled within the generation. This entanglement, however, sacrifices the controllability and interpretability, making it diffifcult to specify the content to be conveyed in the generated text (Qin et al., 2018; Wiseman et al., 2018). With this in mind, this paper proposes decoupling content selection from the Enc-Dec framework to allow finer-grained control over the generation. Table 1 shows an example. We can easily modify the content selection to generate text with various focuses, or sample multiple paraphrases by fixing the content selection. Though there has been much work dealing with content selection for the Enc-Dec, none of them is able to address the above concerns properly. Current methods can be categorized into the following three classes and have different limits: Introduction Many te"
D19-1054,D15-1044,0,0.0525735,"etup, then present the evaluation results. In practice, we can set  to adjust the degree of controllability we want. Later we will show it leads to a trade-off with performance. The final algorithm is detailed in Algorithm 1. To keep fairness, we trian RS and VRS with the same control variate and pre-training strategy.4 4 Experiments 5.1 Related Work Tasks and Setup We test content-selection models on the headline and data-to-text generation task. Both tasks share the same framework with the only difference of source-side encoders. Headline Generation: We use English Gigaword preprocessed by Rush et al. (2015), which pairs first sentences of news articles with their headlines. We keep most settings same as in Zhou et al. (2017), but use a vocabulary built by bytepair-encoding (Sennrich et al., 2016). We find it speeds up training with superior performance. Data-to-Text Generation: We use the Wikibio dataset (Lebret et al., 2016). The source is a Wikipedia infobox and the target is a one-sentence biography description. Most settings are the same as in Liu et al. (2018), but we use a bi-LSTM encoder for better performance. Heuristically extracted content: This is used to train the selector for bottom"
D19-1054,P16-1162,0,0.0204755,"algorithm is detailed in Algorithm 1. To keep fairness, we trian RS and VRS with the same control variate and pre-training strategy.4 4 Experiments 5.1 Related Work Tasks and Setup We test content-selection models on the headline and data-to-text generation task. Both tasks share the same framework with the only difference of source-side encoders. Headline Generation: We use English Gigaword preprocessed by Rush et al. (2015), which pairs first sentences of news articles with their headlines. We keep most settings same as in Zhou et al. (2017), but use a vocabulary built by bytepair-encoding (Sennrich et al., 2016). We find it speeds up training with superior performance. Data-to-Text Generation: We use the Wikibio dataset (Lebret et al., 2016). The source is a Wikipedia infobox and the target is a one-sentence biography description. Most settings are the same as in Liu et al. (2018), but we use a bi-LSTM encoder for better performance. Heuristically extracted content: This is used to train the selector for bottom up models and pretrain the RS and VRS model. For wikibio, we simply extract overlapped words between the source and target. In Gigaword, as the headline is more abstractive, we select the clos"
D19-1054,D15-1199,0,0.0719588,"Missing"
D19-1054,D18-1356,0,0.0459964,"an, announced, closure, schools Text: sri lanka declares closure of schools. Table 1: Headline generation examples from our model. We can generate text describing various contents by sampling different content selections. The selected source word and its corresponding realizations in the text are highlighted with the same color. black-box way. Therefore, both levels of diversity are entangled within the generation. This entanglement, however, sacrifices the controllability and interpretability, making it diffifcult to specify the content to be conveyed in the generated text (Qin et al., 2018; Wiseman et al., 2018). With this in mind, this paper proposes decoupling content selection from the Enc-Dec framework to allow finer-grained control over the generation. Table 1 shows an example. We can easily modify the content selection to generate text with various focuses, or sample multiple paraphrases by fixing the content selection. Though there has been much work dealing with content selection for the Enc-Dec, none of them is able to address the above concerns properly. Current methods can be categorized into the following three classes and have different limits: Introduction Many text generation tasks, e."
D19-1054,C18-1091,0,0.133731,"raining framework achieving promising results and (3) introducing an effective way to achieve the trade-off between performance and controllability. 2 (1) dt is the hidden state of the decoder at time step t. f is a score function to compute the similarity between hi and dt (Luong et al., 2015). 3 Content Selection Our goal is to decouple the content selection from the decoder by introducing an extra content selector. We hope the content-level diversity can be fully captured by the content selector for a more interpretable and controllable generation process. Following Gehrmann et al. (2018); Yu et al. (2018), we define content selection as a sequence labeling task. Let β1 , β2 , . . . , βn denote a sequence of binary selection masks. βi = 1 if hi is selected and 0 otherwise. βi is assumed to be independent from each other and is sampled from a bernoulli distribution B(γi )2 . γi is the bernoulli parameter, which we estimate using a two-layer feedforward network on top of the source encoder. Text are generated by first sampling β from B(γ) to decide which content to cover, then decode with the conditional distribution pθ (Y |X, β). The text is expected to faithfully convey all selected contents an"
D19-1054,P17-1101,0,0.347238,"ge Processing and the 9th International Joint Conference on Natural Language Processing, pages 579–590, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2015; Xu et al., 2015). The encoder first encodes each xi into a vector hi . At each time step, the decoder pays attentions to some source embeddings and outputs the probability of the next token by p(yt |y1:t−1 , Ct ). Ct is a weighted average of source embeddings: X Ct = αt,i hi discrepancy when integrating them together. 2. Soft-select: Learn a soft mask to filter useless information (Mei et al., 2016; Zhou et al., 2017). However, the mask is deterministic without any probabilistic variations, making it hard to model the content-level diversity. 3. Reinforce-select: Train the selector with reinforcement learning (Chen and Bansal, 2018), which has high training variance and low diversity on content selection. i ef (hi ,dt ) αt,i = P f (h ,d ) j t je In this paper, we treat the content selection as latent variables and train with amortized variational inference (Kingma and Welling, 2014; Mnih and Gregor, 2014). This provides a lower training variance than Reinforce-select. The selector and generator are co-trai"
D19-1119,W19-4418,1,0.806202,"t, or transposition of adjacent characters. Right-to-left Re-ranking (R2L) Following Sennrich et al. (2016a, 2017); Grundkiewicz et al. (2019), we train four right-to-left models. The ensemble of four left-to-right models generate n-best candidates and their corresponding scores (i.e., conditional probabilities). We then pass each candidate to the ensemble of the four right-to-left models and compute the score. Finally, we re-rank the n-best candidates based on the sum of the two scores. Sentence-level Error Detection (SED) SED classifies whether a given sentence contains a grammatical error. Asano et al. (2019) proposed incorporating SED into the evaluation pipeline and reported improved precision. Here, the GEC model is applied only if SED detects a grammatical error in the given source sentence. The motivation is that SED could potentially reduce the number of false-positive errors of the GEC model. We use the re-implementation of the BERT-based SED model (Asano et al., 2019). Table 5 presents the results of applying SSE, 13 R2L, and SED. It is noteworthy that P RETL ARGE+SSE+R2L achieves state-of-the-art performance on both CoNLL-2014 (F0.5 = 65.0) and BEA-test (F0.5 = 69.8), which are better tha"
D19-1119,N18-2046,0,0.137737,"Missing"
D19-1119,W19-4427,0,0.584056,"s the D IRECT N OISE method. In this study, we investigate these decisions regarding pseudo data, our goal being to provide the research community with an improved understanding of the incorporation of pseudo data. Through extensive experiments, we determine suitable settings for GEC. We justify the reliability of the proposed settings by demonstrating their strong performance on benchmark datasets. Specifically, without any task-specific techniques or architecture, our model outperforms not only all previous single-model results but also all ensemble results except for the ensemble result by Grundkiewicz et al. (2019)1 . By applying task-specific techniques, we further improve the performance and achieve state-of-the-art performance on the CoNLL-2014 test set and the official test set of the BEA-2019 shared task. 2 Problem Formulation and Notation In this section, we formally define the GEC task discussed in this paper. Let D be the GEC training data that comprise pairs of an ungrammatical source sentence X and grammatical target sentence 1 The paper (Grundkiewicz et al. 2019) has not been published yet at the time of submission. 1236 Proceedings of the 2019 Conference on Empirical Methods in Natural Langu"
D19-1119,P17-1074,0,0.175646,". As a seed corpus T , we use SimpleWiki6 , Wikipedia7 or Gigaword8 . We apply the noizing methods described in Section 3 to each corpus and generate pseudo data Dp . The characteristics of each dataset are summarized in Table 1. Evaluation We report results on BEA-valid, the official test set of the BEA-2019 shared task (BEA-test), the CoNLL-2014 test set (CoNLL2014) (Ng et al., 2014), and the JFLEG test set (JFLEG) (Napoles et al., 2017). All reported results (except ensemble) are the average of five distinct trials using five different random seeds. We report the scores measured by ERRANT (Bryant et al., 2017; Felice et al., 2016) for BEA-valid, BEA-test, and CoNLL-2014. As the reference sentences of BEAtest are publicly unavailable, we evaluate the model outputs on CodaLab9 for BEA-test. We also report results measured by the M 2 scorer (Dahlmeier and Ng, 2012) on CoNLL-2014 to compare them with those of previous studies. We use the GLEU metric (Napoles et al., 2015, 2016) for JFLEG. Model We adopt the Transformer EncDec model (Vaswani et al., 2017) using the fairseq toolkit (Ott et al., 2019) and use the “Transformer (big)” settings of Vaswani et al. (2017). Optimization For the J OINT setting,"
D19-1119,P17-1070,0,0.0792226,"making any modifications to the model architecture. 1 Introduction To date, many studies have tackled grammatical error correction (GEC) as a machine translation (MT) task, in which ungrammatical sentences are regarded as the source language and grammatical sentences are regarded as the target language. This approach allows cutting-edge neural MT models to be adopted. For example, the encoder-decoder (EncDec) model (Sutskever et al., 2014; Bahdanau et al., 2015), which was originally proposed for MT, has been applied widely to GEC and has achieved remarkable results in the GEC research field (Ji et al., 2017; Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018). However, a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data (Koehn and Knowles, 2017), but the largest set of publicly available parallel data (Lang-8) in GEC has only two million sentence pairs (Mizumoto et al., 2011). Consequently, the method of augmenting the data by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several de"
D19-1119,N18-1055,0,0.640986,"Missing"
D19-1119,N12-1067,0,0.230458,"ults on BEA-valid, the official test set of the BEA-2019 shared task (BEA-test), the CoNLL-2014 test set (CoNLL2014) (Ng et al., 2014), and the JFLEG test set (JFLEG) (Napoles et al., 2017). All reported results (except ensemble) are the average of five distinct trials using five different random seeds. We report the scores measured by ERRANT (Bryant et al., 2017; Felice et al., 2016) for BEA-valid, BEA-test, and CoNLL-2014. As the reference sentences of BEAtest are publicly unavailable, we evaluate the model outputs on CodaLab9 for BEA-test. We also report results measured by the M 2 scorer (Dahlmeier and Ng, 2012) on CoNLL-2014 to compare them with those of previous studies. We use the GLEU metric (Napoles et al., 2015, 2016) for JFLEG. Model We adopt the Transformer EncDec model (Vaswani et al., 2017) using the fairseq toolkit (Ott et al., 2019) and use the “Transformer (big)” settings of Vaswani et al. (2017). Optimization For the J OINT setting, we opti4 Details of the dataset is in Appendix B. The detailed data preparation process is in Appendix C. 6 https://simple.wikipedia.org 7 We used 2019-02-25 dump file at https://dumps. wikimedia.org/other/cirrussearch/. 8 We used the English Gigaword Fifth"
D19-1119,W13-1703,0,0.626857,"Missing"
D19-1119,D18-1045,0,0.0186027,"l sentence, is trained. The output of the reverse model is paired with the input and then used as pseudo data. BACKTRANS ( NOISY ) is a variant of backtranslation that was proposed by Xie et al. (2018)2 . This method adds rβrandom to the score of each hypothesis in the beam for every time step. Here, noise r is sampled uniformly from the interval [0, 1], and βrandom ∈ R≥0 is a hyper-parameter that controls the noise scale. If we set βrandom = 0, then BACK TRANS ( NOISY ) is identical to standard backtranslation. BACKTRANS ( SAMPLE ) is another variant of backtranslation, which was proposed by Edunov et al. (2018) for MT. In BACKTRANS ( SAMPLE ), sentences are decoded by sampling from the distribution of the reverse model. D IRECT N OISE Whereas BACKTRANS ( NOISY ) and BACKTRANS ( SAMPLE ) generate ungrammatical sentences with a reverse model, D IRECTN OISE injects noise “directly” into grammatical sentences (Edunov et al., 2018; Zhao et al., 2019). Specifically, for each token in the given sentence, this method probabilistically chooses one of the following operations: (i) masking with a placeholder token hmaski, (ii) deletion, (iii) insertion of a random token, and (iv) keeping the original3 . For ea"
D19-1119,C16-1079,0,0.350702,", we use SimpleWiki6 , Wikipedia7 or Gigaword8 . We apply the noizing methods described in Section 3 to each corpus and generate pseudo data Dp . The characteristics of each dataset are summarized in Table 1. Evaluation We report results on BEA-valid, the official test set of the BEA-2019 shared task (BEA-test), the CoNLL-2014 test set (CoNLL2014) (Ng et al., 2014), and the JFLEG test set (JFLEG) (Napoles et al., 2017). All reported results (except ensemble) are the average of five distinct trials using five different random seeds. We report the scores measured by ERRANT (Bryant et al., 2017; Felice et al., 2016) for BEA-valid, BEA-test, and CoNLL-2014. As the reference sentences of BEAtest are publicly unavailable, we evaluate the model outputs on CodaLab9 for BEA-test. We also report results measured by the M 2 scorer (Dahlmeier and Ng, 2012) on CoNLL-2014 to compare them with those of previous studies. We use the GLEU metric (Napoles et al., 2015, 2016) for JFLEG. Model We adopt the Transformer EncDec model (Vaswani et al., 2017) using the fairseq toolkit (Ott et al., 2019) and use the “Transformer (big)” settings of Vaswani et al. (2017). Optimization For the J OINT setting, we opti4 Details of th"
D19-1119,P18-1097,0,0.398051,"ich was originally proposed for MT, has been applied widely to GEC and has achieved remarkable results in the GEC research field (Ji et al., 2017; Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018). However, a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data (Koehn and Knowles, 2017), but the largest set of publicly available parallel data (Lang-8) in GEC has only two million sentence pairs (Mizumoto et al., 2011). Consequently, the method of augmenting the data by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization setting (Section 2). However, consensus on these decisions in the GEC research field is yet to be formulated. For example, Xie et al. (2018) found that a variant of the backtranslation (Sennrich et al., 2016b) method (BACKTRANS ( NOISY )) outperforms the generation of pseudo data from r"
D19-1119,W17-3204,0,0.0149658,"hich ungrammatical sentences are regarded as the source language and grammatical sentences are regarded as the target language. This approach allows cutting-edge neural MT models to be adopted. For example, the encoder-decoder (EncDec) model (Sutskever et al., 2014; Bahdanau et al., 2015), which was originally proposed for MT, has been applied widely to GEC and has achieved remarkable results in the GEC research field (Ji et al., 2017; Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018). However, a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data (Koehn and Knowles, 2017), but the largest set of publicly available parallel data (Lang-8) in GEC has only two million sentence pairs (Mizumoto et al., 2011). Consequently, the method of augmenting the data by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization set"
D19-1119,N19-1333,0,0.693335,"y proposed for MT, has been applied widely to GEC and has achieved remarkable results in the GEC research field (Ji et al., 2017; Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018). However, a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data (Koehn and Knowles, 2017), but the largest set of publicly available parallel data (Lang-8) in GEC has only two million sentence pairs (Mizumoto et al., 2011). Consequently, the method of augmenting the data by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization setting (Section 2). However, consensus on these decisions in the GEC research field is yet to be formulated. For example, Xie et al. (2018) found that a variant of the backtranslation (Sennrich et al., 2016b) method (BACKTRANS ( NOISY )) outperforms the generation of pseudo data from raw grammatical sentences"
D19-1119,I11-1017,1,0.592899,"proach allows cutting-edge neural MT models to be adopted. For example, the encoder-decoder (EncDec) model (Sutskever et al., 2014; Bahdanau et al., 2015), which was originally proposed for MT, has been applied widely to GEC and has achieved remarkable results in the GEC research field (Ji et al., 2017; Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018). However, a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data (Koehn and Knowles, 2017), but the largest set of publicly available parallel data (Lang-8) in GEC has only two million sentence pairs (Mizumoto et al., 2011). Consequently, the method of augmenting the data by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization setting (Section 2). However, consensus on these decisions in the GEC research field is yet to be formulated. For example, Xie et al. (2"
D19-1119,P15-2097,0,0.292767,"L2014) (Ng et al., 2014), and the JFLEG test set (JFLEG) (Napoles et al., 2017). All reported results (except ensemble) are the average of five distinct trials using five different random seeds. We report the scores measured by ERRANT (Bryant et al., 2017; Felice et al., 2016) for BEA-valid, BEA-test, and CoNLL-2014. As the reference sentences of BEAtest are publicly unavailable, we evaluate the model outputs on CodaLab9 for BEA-test. We also report results measured by the M 2 scorer (Dahlmeier and Ng, 2012) on CoNLL-2014 to compare them with those of previous studies. We use the GLEU metric (Napoles et al., 2015, 2016) for JFLEG. Model We adopt the Transformer EncDec model (Vaswani et al., 2017) using the fairseq toolkit (Ott et al., 2019) and use the “Transformer (big)” settings of Vaswani et al. (2017). Optimization For the J OINT setting, we opti4 Details of the dataset is in Appendix B. The detailed data preparation process is in Appendix C. 6 https://simple.wikipedia.org 7 We used 2019-02-25 dump file at https://dumps. wikimedia.org/other/cirrussearch/. 8 We used the English Gigaword Fifth Edition (LDC Catalog No.: LDC2011T07). 9 https://competitions.codalab.org/ competitions/20228 5 Table 2: Pe"
D19-1119,E17-2037,0,0.346319,"periments. Hereinafter, we refer to the training data as BEA-train. We create validation data (BEA-valid) by randomly sampling sentence pairs from the official validation split5 . As a seed corpus T , we use SimpleWiki6 , Wikipedia7 or Gigaword8 . We apply the noizing methods described in Section 3 to each corpus and generate pseudo data Dp . The characteristics of each dataset are summarized in Table 1. Evaluation We report results on BEA-valid, the official test set of the BEA-2019 shared task (BEA-test), the CoNLL-2014 test set (CoNLL2014) (Ng et al., 2014), and the JFLEG test set (JFLEG) (Napoles et al., 2017). All reported results (except ensemble) are the average of five distinct trials using five different random seeds. We report the scores measured by ERRANT (Bryant et al., 2017; Felice et al., 2016) for BEA-valid, BEA-test, and CoNLL-2014. As the reference sentences of BEAtest are publicly unavailable, we evaluate the model outputs on CodaLab9 for BEA-test. We also report results measured by the M 2 scorer (Dahlmeier and Ng, 2012) on CoNLL-2014 to compare them with those of previous studies. We use the GLEU metric (Napoles et al., 2015, 2016) for JFLEG. Model We adopt the Transformer EncDec mo"
D19-1119,P12-2039,0,0.528357,"Missing"
D19-1119,N19-4009,0,0.0231069,"ge of five distinct trials using five different random seeds. We report the scores measured by ERRANT (Bryant et al., 2017; Felice et al., 2016) for BEA-valid, BEA-test, and CoNLL-2014. As the reference sentences of BEAtest are publicly unavailable, we evaluate the model outputs on CodaLab9 for BEA-test. We also report results measured by the M 2 scorer (Dahlmeier and Ng, 2012) on CoNLL-2014 to compare them with those of previous studies. We use the GLEU metric (Napoles et al., 2015, 2016) for JFLEG. Model We adopt the Transformer EncDec model (Vaswani et al., 2017) using the fairseq toolkit (Ott et al., 2019) and use the “Transformer (big)” settings of Vaswani et al. (2017). Optimization For the J OINT setting, we opti4 Details of the dataset is in Appendix B. The detailed data preparation process is in Appendix C. 6 https://simple.wikipedia.org 7 We used 2019-02-25 dump file at https://dumps. wikimedia.org/other/cirrussearch/. 8 We used the English Gigaword Fifth Edition (LDC Catalog No.: LDC2011T07). 9 https://competitions.codalab.org/ competitions/20228 5 Table 2: Performance of models on BEA-valid: a value in bold indicates the best result within the column. The seed corpus T is SimpleWiki. mi"
D19-1119,W17-4739,0,0.044832,"Missing"
D19-1119,W16-2323,0,0.359924,"by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization setting (Section 2). However, consensus on these decisions in the GEC research field is yet to be formulated. For example, Xie et al. (2018) found that a variant of the backtranslation (Sennrich et al., 2016b) method (BACKTRANS ( NOISY )) outperforms the generation of pseudo data from raw grammatical sentences (D IRECT N OISE). By contrast, the current state of the art model (Zhao et al., 2019) uses the D IRECT N OISE method. In this study, we investigate these decisions regarding pseudo data, our goal being to provide the research community with an improved understanding of the incorporation of pseudo data. Through extensive experiments, we determine suitable settings for GEC. We justify the reliability of the proposed settings by demonstrating their strong performance on benchmark datasets. Spe"
D19-1119,P16-1009,0,0.596655,"by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization setting (Section 2). However, consensus on these decisions in the GEC research field is yet to be formulated. For example, Xie et al. (2018) found that a variant of the backtranslation (Sennrich et al., 2016b) method (BACKTRANS ( NOISY )) outperforms the generation of pseudo data from raw grammatical sentences (D IRECT N OISE). By contrast, the current state of the art model (Zhao et al., 2019) uses the D IRECT N OISE method. In this study, we investigate these decisions regarding pseudo data, our goal being to provide the research community with an improved understanding of the incorporation of pseudo data. Through extensive experiments, we determine suitable settings for GEC. We justify the reliability of the proposed settings by demonstrating their strong performance on benchmark datasets. Spe"
D19-1119,N18-1057,0,0.263454,"et al., 2015), which was originally proposed for MT, has been applied widely to GEC and has achieved remarkable results in the GEC research field (Ji et al., 2017; Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018). However, a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data (Koehn and Knowles, 2017), but the largest set of publicly available parallel data (Lang-8) in GEC has only two million sentence pairs (Mizumoto et al., 2011). Consequently, the method of augmenting the data by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization setting (Section 2). However, consensus on these decisions in the GEC research field is yet to be formulated. For example, Xie et al. (2018) found that a variant of the backtranslation (Sennrich et al., 2016b) method (BACKTRANS ( NOISY )) outperforms the generation of p"
D19-1119,P11-1019,0,0.524872,"Missing"
D19-1119,N19-1014,0,0.585128,"een applied widely to GEC and has achieved remarkable results in the GEC research field (Ji et al., 2017; Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018). However, a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data (Koehn and Knowles, 2017), but the largest set of publicly available parallel data (Lang-8) in GEC has only two million sentence pairs (Mizumoto et al., 2011). Consequently, the method of augmenting the data by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization setting (Section 2). However, consensus on these decisions in the GEC research field is yet to be formulated. For example, Xie et al. (2018) found that a variant of the backtranslation (Sennrich et al., 2016b) method (BACKTRANS ( NOISY )) outperforms the generation of pseudo data from raw grammatical sentences (D IRECT N OISE). B"
D19-1119,P16-1162,0,0.635333,"by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization setting (Section 2). However, consensus on these decisions in the GEC research field is yet to be formulated. For example, Xie et al. (2018) found that a variant of the backtranslation (Sennrich et al., 2016b) method (BACKTRANS ( NOISY )) outperforms the generation of pseudo data from raw grammatical sentences (D IRECT N OISE). By contrast, the current state of the art model (Zhao et al., 2019) uses the D IRECT N OISE method. In this study, we investigate these decisions regarding pseudo data, our goal being to provide the research community with an improved understanding of the incorporation of pseudo data. Through extensive experiments, we determine suitable settings for GEC. We justify the reliability of the proposed settings by demonstrating their strong performance on benchmark datasets. Spe"
D19-1379,N09-1014,0,0.0297852,"ce on Natural Language Processing, pages 3665–3671, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work Transductive learning. Vapnik advocated and formalized transductive learning (Vapnik, 1998; Gammerman et al., 1998), which has been applied to text classification (Joachims, 1999; Ifrim and Weikum, 2006) and image processing (Bruzzone et al., 2006; Sener et al., 2016; Liu et al., 2019). Although some studies have presented transductive methods for linear models in other tasks (Duh and Kirchhoff, 2006; Ueffing et al., 2007; Chen et al., 2008; Alexandrescu and Kirchhoff, 2009), transductive methods for neural models are underexplored in NLP. Unsupervised domain adaptation. Transductive learning is related to unsupervised domain adaptation, in which models are adapted to a target domain by using unlabeled target domain texts (Ben-David et al., 2010; Shi and Sha, 2012). This setting does not allow models to access the test set, which is the main difference between unsupervised domain adaptation and transductive learning. Various unsupervised adaptation methods have been proposed for linear models (Blitzer et al., 2006; Jiang and Zhai, 2007; Tsuboi et al., 2009; Søgaa"
D19-1379,P98-1013,0,0.412296,"2005)4 . The goal of syntactic chunking is to divide a sentence into non-overlapping phrases that consist of syntactically related words. The goal of SRL is to identify semantic arguments for each predicate. For example, consider the following sentence: S YN C HUNK S EM ROLE 3 The man kept a cat [ NP ] [ NP ] [ A0 ] [ A1 ] In our experiments (Section 4), both losses were given by the negative log-likelihood (Appendix A). 4 This paper addresses span-based, PropBank-style SRL. Detailed descriptions on other lines of SRL research (e.g. dependency-based SRL and FrameNet-based SRL) can be found in Baker et al. (1998); Das et al. (2014); Surdeanu et al. (2008); Hajiˇc et al. (2009). In syntactic chunking, given the input sentence, systems have to recognize “The man” and “a cat” as noun phrases (NP). In SRL, given the input sentence and the target predicate “kept”, systems have to recognize “The man” as the A0 argument and “a cat” as the A1 argument. For syntactic chunking, we adopted the experimental protocol by Ponvert et al. (2011) and for SRL, we followed Ouchi et al. (2018) (details in Appendix A). Datasets. We perform experiments using the CoNLL-2012 dataset5 . To investigate the performances under in"
D19-1379,W06-1615,0,0.0896134,"ing et al., 2007; Chen et al., 2008; Alexandrescu and Kirchhoff, 2009), transductive methods for neural models are underexplored in NLP. Unsupervised domain adaptation. Transductive learning is related to unsupervised domain adaptation, in which models are adapted to a target domain by using unlabeled target domain texts (Ben-David et al., 2010; Shi and Sha, 2012). This setting does not allow models to access the test set, which is the main difference between unsupervised domain adaptation and transductive learning. Various unsupervised adaptation methods have been proposed for linear models (Blitzer et al., 2006; Jiang and Zhai, 2007; Tsuboi et al., 2009; Søgaard, 2013). In the context of neural models, adversarial domain adaptation (Ganin and Lempitsky, 2015; Ganin et al., 2016; Guo et al., 2018), importance weighting (Wang et al., 2017), structural correspondence learning (Ziser and Reichart, 2017), self/tri/co-training (Saito et al., 2017; Ruder and Plank, 2018), and other techniques orthogonal to transductive LM fine-tuning have been applied successfully in unsupervised domain adaptation2 . Integrating these methods with transductive LM fine-tuning is an interesting direction for future research."
D19-1379,W05-0620,0,0.288203,"Missing"
D19-1379,I08-2132,0,0.0285215,"onal Joint Conference on Natural Language Processing, pages 3665–3671, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work Transductive learning. Vapnik advocated and formalized transductive learning (Vapnik, 1998; Gammerman et al., 1998), which has been applied to text classification (Joachims, 1999; Ifrim and Weikum, 2006) and image processing (Bruzzone et al., 2006; Sener et al., 2016; Liu et al., 2019). Although some studies have presented transductive methods for linear models in other tasks (Duh and Kirchhoff, 2006; Ueffing et al., 2007; Chen et al., 2008; Alexandrescu and Kirchhoff, 2009), transductive methods for neural models are underexplored in NLP. Unsupervised domain adaptation. Transductive learning is related to unsupervised domain adaptation, in which models are adapted to a target domain by using unlabeled target domain texts (Ben-David et al., 2010; Shi and Sha, 2012). This setting does not allow models to access the test set, which is the main difference between unsupervised domain adaptation and transductive learning. Various unsupervised adaptation methods have been proposed for linear models (Blitzer et al., 2006; Jiang and Zha"
D19-1379,D18-1217,0,0.0173867,"et domain unlabeled data. 8 As target domain unlabeled data, we use the CoNLL2012 training set of each domain. 3668 BC BN MZ NW PT TC WB Syntactic chunking U U+T 90.5 91.0 91.3 91.6 90.2 90.6 92.1 92.5 87.3 87.7 87.2 87.6 91.8 92.2 Semantic role labeling U U+T 79.0 79.4 80.1 80.6 78.3 78.7 81.5 81.9 73.6 74.3 71.4 72.0 76.8 77.2 Table 4: Performance comparison between LM finetuning on target domain unlabeled data (U) and on the combination of the unlabeled data and test sets (U + T). Cells show the F1 scores averaged across the target domains. CoNLL 2000 BASE T RANS 96.6 96.7 97.0 96.4 95.8 - Clark et al. (2018) Peters et al. (2017) Hashimoto et al. (2017) Wang et al. (2019) Li et al. (2019) Ouchi et al. (2018) He et al. (2018) 2005 WSJ Brown 87.7 78.3 87.9* 79.5* 88.2 79.3 87.7 80.5 87.6 78.7 87.4 80.4 2012 86.2 86.6* 86.4 86.0 86.2 85.5 use the CoNLL-2000 dataset (Sang and Buchholz, 2000) and follow the standard experimental protocol (Hashimoto et al., 2017). For SRL, we use the CoNLL-2005 (Carreras and M`arquez, 2005) and CoNLL-2012 datasets (Pradhan et al., 2012) and follow the standard experimental protocol (Ouchi et al., 2018). Table 5 shows the F1 scores of our models and those of existing mod"
D19-1379,P07-1033,0,0.0514405,"Missing"
D19-1379,W17-3203,0,0.0311095,"of using both datasets. Table 4 shows the F1 scores averaged across all the target domains. Fine-tuning the LMs on the target domain unlabeled data as well as each test set (U + T) showed better performance than fine-tuning them only on the target domain unlabeled data (U). This combination of tranduction with unsupervised domain adaptation further improves performance. Effects in standard benchmarks. Some studies indicated that when promising new techniques are only evaluated on very basic models, determining how much (if any) improvement will carry over to stronger models can be difficult (Denkowski and Neubig, 2017; Suzuki et al., 2018). Motivated by such studies, we provide the results in standard benchmark settings. For syntactic chunking, we Conclusion In this study, we investigated the impact of transductive learning on state-of-the-art neural models in syntactic and semantic tasks. Specifically, we fine-tuned an LM on an unlabeled test set. Through extensive experiments, we demonstrated that, despite its simplicity, transductive LM finetuning contributes to consistent performance improvement of state-of-the-art syntactic and semantic models in cross-domain settings. One interesting line of future w"
D19-1379,N19-1423,0,0.161704,"or text classification, transductive methods for linear models have been investigated in only a few tasks, such as lexical acquisition (Duh and Kirchhoff, 2006) and machine translation (Ueffing et al., 2007). In particular, transductive learning with neural networks is underexplored. Here, we investigate the impact of transductive learning on state-of-the-art neural models in syntactic and semantic tasks, namely syntactic chunking and semantic role labeling (SRL). Specifically, inspired by recent findings that language model (LM)-based word representations yield large performance improvement (Devlin et al., 2019), we fine-tune Embeddings from Language Models (ELMo) (Peters et al., 2018) on an unlabeled test set and use them in each task-specific model. Typically, LMs are trained on a large-scale corpus whose word distributions are different from the test set. By contrast, transductive learning allows us to fit LMs directly to the distributions of the test set. Our experiments show the effectiveness of transductive LM fine-tuning. In summary, our main contributions are: • This work is the first to introduce an LM finetuning method to transductive learning1 . • Through extensive experiments in both indo"
D19-1379,W06-1647,0,0.190913,"unlabeled test set is given in the training phase. That is, the inputs of the test set, i.e., the raw texts, can be used during training, but the labels are never used. In the test phase, the trained model is evaluated on the same test set. Despite its practical advantages, transductive learning has received little attention in natural language processing (NLP). After the pioneering work of Joachims (1999), who proposed a transductive support vector machine for text classification, transductive methods for linear models have been investigated in only a few tasks, such as lexical acquisition (Duh and Kirchhoff, 2006) and machine translation (Ueffing et al., 2007). In particular, transductive learning with neural networks is underexplored. Here, we investigate the impact of transductive learning on state-of-the-art neural models in syntactic and semantic tasks, namely syntactic chunking and semantic role labeling (SRL). Specifically, inspired by recent findings that language model (LM)-based word representations yield large performance improvement (Devlin et al., 2019), we fine-tune Embeddings from Language Models (ELMo) (Peters et al., 2018) on an unlabeled test set and use them in each task-specific mode"
D19-1379,J02-3001,0,0.473299,"In the LM pre-training and fine-tuning phases (Eqs. 1 and 2), we first train the initial LM parameters Θ and then fine-tune the pre-trained parameters Θ0 . In the task-specific training phase (Eq. 3), we fix the fine-tuned LM parameters Θ00 used for the embedding layer of a task-specific model, and train only the task-specific model parameters Φ. 4 Experiments Tasks. To investigate the effectiveness of transductive LM fine-tuning for syntactic and semantic analysis, we conduct experiments in syntactic chunking (Ramshaw and Marcus, 1999; Sang and Buchholz, 2000; Ponvert et al., 2011) and SRL (Gildea and Jurafsky, 2002; Palmer et al., 2005; Carreras and M`arquez, 2005)4 . The goal of syntactic chunking is to divide a sentence into non-overlapping phrases that consist of syntactically related words. The goal of SRL is to identify semantic arguments for each predicate. For example, consider the following sentence: S YN C HUNK S EM ROLE 3 The man kept a cat [ NP ] [ NP ] [ A0 ] [ A1 ] In our experiments (Section 4), both losses were given by the negative log-likelihood (Appendix A). 4 This paper addresses span-based, PropBank-style SRL. Detailed descriptions on other lines of SRL research (e.g. dependency-base"
D19-1379,D18-1498,0,0.0229667,"s related to unsupervised domain adaptation, in which models are adapted to a target domain by using unlabeled target domain texts (Ben-David et al., 2010; Shi and Sha, 2012). This setting does not allow models to access the test set, which is the main difference between unsupervised domain adaptation and transductive learning. Various unsupervised adaptation methods have been proposed for linear models (Blitzer et al., 2006; Jiang and Zhai, 2007; Tsuboi et al., 2009; Søgaard, 2013). In the context of neural models, adversarial domain adaptation (Ganin and Lempitsky, 2015; Ganin et al., 2016; Guo et al., 2018), importance weighting (Wang et al., 2017), structural correspondence learning (Ziser and Reichart, 2017), self/tri/co-training (Saito et al., 2017; Ruder and Plank, 2018), and other techniques orthogonal to transductive LM fine-tuning have been applied successfully in unsupervised domain adaptation2 . Integrating these methods with transductive LM fine-tuning is an interesting direction for future research. LM-based word representations. Recently, LM-based word representations pre-trained on unlabeled data have gained considerable attention (Peters et al., 2018; Radford et al., 2018; Devlin e"
D19-1379,D17-1206,0,0.0802734,"Missing"
D19-1379,P18-2058,0,0.0469233,"Missing"
D19-1379,P18-1031,0,0.0238789,"ues orthogonal to transductive LM fine-tuning have been applied successfully in unsupervised domain adaptation2 . Integrating these methods with transductive LM fine-tuning is an interesting direction for future research. LM-based word representations. Recently, LM-based word representations pre-trained on unlabeled data have gained considerable attention (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). The most related method to ours is Universal Language Model Fine-tuning (ULMFiT), which pre-trains an LM on a large general-domain corpus and fine-tunes it on the target task (Howard and Ruder, 2018). Inspired by these studies, we introduce LM-based word representation in transductive learning. 2 Feature augmentation is considered a supervised domain adaptation method (Daume III, 2007; Kim et al., 2016). (3) (1) Transductive (2) LM Fine-tuning Figure 1: Training procedure. (1) LM pre-training: the LM is firstly pre-trained on the large-scale unlalarge beled corpus Dlarge = {Xilarge }N i=1 . (2) Transductive LM fine-tuning: the LM is then fine-tuned on the test unlabeled test set Dtest = {Xitest }N i=1 . Note that the test set used for training is the identical one used in evaluation. (3)"
D19-1379,P07-1034,0,0.110447,"et al., 2008; Alexandrescu and Kirchhoff, 2009), transductive methods for neural models are underexplored in NLP. Unsupervised domain adaptation. Transductive learning is related to unsupervised domain adaptation, in which models are adapted to a target domain by using unlabeled target domain texts (Ben-David et al., 2010; Shi and Sha, 2012). This setting does not allow models to access the test set, which is the main difference between unsupervised domain adaptation and transductive learning. Various unsupervised adaptation methods have been proposed for linear models (Blitzer et al., 2006; Jiang and Zhai, 2007; Tsuboi et al., 2009; Søgaard, 2013). In the context of neural models, adversarial domain adaptation (Ganin and Lempitsky, 2015; Ganin et al., 2016; Guo et al., 2018), importance weighting (Wang et al., 2017), structural correspondence learning (Ziser and Reichart, 2017), self/tri/co-training (Saito et al., 2017; Ruder and Plank, 2018), and other techniques orthogonal to transductive LM fine-tuning have been applied successfully in unsupervised domain adaptation2 . Integrating these methods with transductive LM fine-tuning is an interesting direction for future research. LM-based word represe"
D19-1379,P18-1110,0,0.045924,"Missing"
D19-1379,C16-1038,0,0.0178784,"esearch. LM-based word representations. Recently, LM-based word representations pre-trained on unlabeled data have gained considerable attention (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). The most related method to ours is Universal Language Model Fine-tuning (ULMFiT), which pre-trains an LM on a large general-domain corpus and fine-tunes it on the target task (Howard and Ruder, 2018). Inspired by these studies, we introduce LM-based word representation in transductive learning. 2 Feature augmentation is considered a supervised domain adaptation method (Daume III, 2007; Kim et al., 2016). (3) (1) Transductive (2) LM Fine-tuning Figure 1: Training procedure. (1) LM pre-training: the LM is firstly pre-trained on the large-scale unlalarge beled corpus Dlarge = {Xilarge }N i=1 . (2) Transductive LM fine-tuning: the LM is then fine-tuned on the test unlabeled test set Dtest = {Xitest }N i=1 . Note that the test set used for training is the identical one used in evaluation. (3) Task-specific model training: the taskspecific model is trained on the training set Dtrain = train {(Xitrain , Yitrain )}N i=1 . L denotes the loss function. 3 Neural Transductive Learning Motivation. Suppos"
D19-1379,W00-0726,0,0.635178,"ctions for an LM and task-specific model, respectively.3 In the LM pre-training and fine-tuning phases (Eqs. 1 and 2), we first train the initial LM parameters Θ and then fine-tune the pre-trained parameters Θ0 . In the task-specific training phase (Eq. 3), we fix the fine-tuned LM parameters Θ00 used for the embedding layer of a task-specific model, and train only the task-specific model parameters Φ. 4 Experiments Tasks. To investigate the effectiveness of transductive LM fine-tuning for syntactic and semantic analysis, we conduct experiments in syntactic chunking (Ramshaw and Marcus, 1999; Sang and Buchholz, 2000; Ponvert et al., 2011) and SRL (Gildea and Jurafsky, 2002; Palmer et al., 2005; Carreras and M`arquez, 2005)4 . The goal of syntactic chunking is to divide a sentence into non-overlapping phrases that consist of syntactically related words. The goal of SRL is to identify semantic arguments for each predicate. For example, consider the following sentence: S YN C HUNK S EM ROLE 3 The man kept a cat [ NP ] [ NP ] [ A0 ] [ A1 ] In our experiments (Section 4), both losses were given by the negative log-likelihood (Appendix A). 4 This paper addresses span-based, PropBank-style SRL. Detailed descrip"
D19-1379,D18-1191,1,0.909061,"style SRL. Detailed descriptions on other lines of SRL research (e.g. dependency-based SRL and FrameNet-based SRL) can be found in Baker et al. (1998); Das et al. (2014); Surdeanu et al. (2008); Hajiˇc et al. (2009). In syntactic chunking, given the input sentence, systems have to recognize “The man” and “a cat” as noun phrases (NP). In SRL, given the input sentence and the target predicate “kept”, systems have to recognize “The man” as the A0 argument and “a cat” as the A1 argument. For syntactic chunking, we adopted the experimental protocol by Ponvert et al. (2011) and for SRL, we followed Ouchi et al. (2018) (details in Appendix A). Datasets. We perform experiments using the CoNLL-2012 dataset5 . To investigate the performances under in-domain and out-of-domain settings, we use each of the seven domains in the CoNLL-2012 dataset. Table 1 shows the data statistics. Each test set contains at most 2,000 sentences. Compared with previous studies, such as Xiao and Guo (2013) that used 570,000 sentences as unlabeled data for unsupervised domain adaptation of syntactic chunking, our transductive experiments can be regarded as a low-resource adaptation setting. As a large-scale unlabeled raw corpus for L"
D19-1379,J05-1004,0,0.0940305,"fine-tuning phases (Eqs. 1 and 2), we first train the initial LM parameters Θ and then fine-tune the pre-trained parameters Θ0 . In the task-specific training phase (Eq. 3), we fix the fine-tuned LM parameters Θ00 used for the embedding layer of a task-specific model, and train only the task-specific model parameters Φ. 4 Experiments Tasks. To investigate the effectiveness of transductive LM fine-tuning for syntactic and semantic analysis, we conduct experiments in syntactic chunking (Ramshaw and Marcus, 1999; Sang and Buchholz, 2000; Ponvert et al., 2011) and SRL (Gildea and Jurafsky, 2002; Palmer et al., 2005; Carreras and M`arquez, 2005)4 . The goal of syntactic chunking is to divide a sentence into non-overlapping phrases that consist of syntactically related words. The goal of SRL is to identify semantic arguments for each predicate. For example, consider the following sentence: S YN C HUNK S EM ROLE 3 The man kept a cat [ NP ] [ NP ] [ A0 ] [ A1 ] In our experiments (Section 4), both losses were given by the negative log-likelihood (Appendix A). 4 This paper addresses span-based, PropBank-style SRL. Detailed descriptions on other lines of SRL research (e.g. dependency-based SRL and FrameNet-ba"
D19-1379,P17-1161,0,0.0680021,"Missing"
D19-1379,N18-1202,0,0.518315,"vestigated in only a few tasks, such as lexical acquisition (Duh and Kirchhoff, 2006) and machine translation (Ueffing et al., 2007). In particular, transductive learning with neural networks is underexplored. Here, we investigate the impact of transductive learning on state-of-the-art neural models in syntactic and semantic tasks, namely syntactic chunking and semantic role labeling (SRL). Specifically, inspired by recent findings that language model (LM)-based word representations yield large performance improvement (Devlin et al., 2019), we fine-tune Embeddings from Language Models (ELMo) (Peters et al., 2018) on an unlabeled test set and use them in each task-specific model. Typically, LMs are trained on a large-scale corpus whose word distributions are different from the test set. By contrast, transductive learning allows us to fit LMs directly to the distributions of the test set. Our experiments show the effectiveness of transductive LM fine-tuning. In summary, our main contributions are: • This work is the first to introduce an LM finetuning method to transductive learning1 . • Through extensive experiments in both indomain and out-of-domain settings, we demonstrate that transductive LM fine-t"
D19-1379,P11-1108,0,0.0567729,"Missing"
D19-1379,W12-4501,0,0.0181753,"ata and test sets (U + T). Cells show the F1 scores averaged across the target domains. CoNLL 2000 BASE T RANS 96.6 96.7 97.0 96.4 95.8 - Clark et al. (2018) Peters et al. (2017) Hashimoto et al. (2017) Wang et al. (2019) Li et al. (2019) Ouchi et al. (2018) He et al. (2018) 2005 WSJ Brown 87.7 78.3 87.9* 79.5* 88.2 79.3 87.7 80.5 87.6 78.7 87.4 80.4 2012 86.2 86.6* 86.4 86.0 86.2 85.5 use the CoNLL-2000 dataset (Sang and Buchholz, 2000) and follow the standard experimental protocol (Hashimoto et al., 2017). For SRL, we use the CoNLL-2005 (Carreras and M`arquez, 2005) and CoNLL-2012 datasets (Pradhan et al., 2012) and follow the standard experimental protocol (Ouchi et al., 2018). Table 5 shows the F1 scores of our models and those of existing models. The results of the baseline model were comparable with those of the state-of-the-art models, and the transductive model consistently outperformed the baseline model9 . Note that we cannot fairly compare the transductive and existing models due to the difference in settings. These results, however, demonstrate that transductive LM fine-tuning improves state-of-the-art chunking and SRL models. 6 Table 5: Standard benchmark results. Cells show the F1 scores"
D19-1379,P18-1096,0,0.0204231,"a, 2012). This setting does not allow models to access the test set, which is the main difference between unsupervised domain adaptation and transductive learning. Various unsupervised adaptation methods have been proposed for linear models (Blitzer et al., 2006; Jiang and Zhai, 2007; Tsuboi et al., 2009; Søgaard, 2013). In the context of neural models, adversarial domain adaptation (Ganin and Lempitsky, 2015; Ganin et al., 2016; Guo et al., 2018), importance weighting (Wang et al., 2017), structural correspondence learning (Ziser and Reichart, 2017), self/tri/co-training (Saito et al., 2017; Ruder and Plank, 2018), and other techniques orthogonal to transductive LM fine-tuning have been applied successfully in unsupervised domain adaptation2 . Integrating these methods with transductive LM fine-tuning is an interesting direction for future research. LM-based word representations. Recently, LM-based word representations pre-trained on unlabeled data have gained considerable attention (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). The most related method to ours is Universal Language Model Fine-tuning (ULMFiT), which pre-trains an LM on a large general-domain corpus and fine-tunes it o"
D19-1379,W08-2121,0,0.0654377,"Missing"
D19-1379,P18-2097,1,0.775367,"le 4 shows the F1 scores averaged across all the target domains. Fine-tuning the LMs on the target domain unlabeled data as well as each test set (U + T) showed better performance than fine-tuning them only on the target domain unlabeled data (U). This combination of tranduction with unsupervised domain adaptation further improves performance. Effects in standard benchmarks. Some studies indicated that when promising new techniques are only evaluated on very basic models, determining how much (if any) improvement will carry over to stronger models can be difficult (Denkowski and Neubig, 2017; Suzuki et al., 2018). Motivated by such studies, we provide the results in standard benchmark settings. For syntactic chunking, we Conclusion In this study, we investigated the impact of transductive learning on state-of-the-art neural models in syntactic and semantic tasks. Specifically, we fine-tuned an LM on an unlabeled test set. Through extensive experiments, we demonstrated that, despite its simplicity, transductive LM finetuning contributes to consistent performance improvement of state-of-the-art syntactic and semantic models in cross-domain settings. One interesting line of future work is to explore effe"
D19-1379,P07-1004,0,0.296097,". That is, the inputs of the test set, i.e., the raw texts, can be used during training, but the labels are never used. In the test phase, the trained model is evaluated on the same test set. Despite its practical advantages, transductive learning has received little attention in natural language processing (NLP). After the pioneering work of Joachims (1999), who proposed a transductive support vector machine for text classification, transductive methods for linear models have been investigated in only a few tasks, such as lexical acquisition (Duh and Kirchhoff, 2006) and machine translation (Ueffing et al., 2007). In particular, transductive learning with neural networks is underexplored. Here, we investigate the impact of transductive learning on state-of-the-art neural models in syntactic and semantic tasks, namely syntactic chunking and semantic role labeling (SRL). Specifically, inspired by recent findings that language model (LM)-based word representations yield large performance improvement (Devlin et al., 2019), we fine-tune Embeddings from Language Models (ELMo) (Peters et al., 2018) on an unlabeled test set and use them in each task-specific model. Typically, LMs are trained on a large-scale"
D19-1379,D17-1155,0,0.0196039,"on, in which models are adapted to a target domain by using unlabeled target domain texts (Ben-David et al., 2010; Shi and Sha, 2012). This setting does not allow models to access the test set, which is the main difference between unsupervised domain adaptation and transductive learning. Various unsupervised adaptation methods have been proposed for linear models (Blitzer et al., 2006; Jiang and Zhai, 2007; Tsuboi et al., 2009; Søgaard, 2013). In the context of neural models, adversarial domain adaptation (Ganin and Lempitsky, 2015; Ganin et al., 2016; Guo et al., 2018), importance weighting (Wang et al., 2017), structural correspondence learning (Ziser and Reichart, 2017), self/tri/co-training (Saito et al., 2017; Ruder and Plank, 2018), and other techniques orthogonal to transductive LM fine-tuning have been applied successfully in unsupervised domain adaptation2 . Integrating these methods with transductive LM fine-tuning is an interesting direction for future research. LM-based word representations. Recently, LM-based word representations pre-trained on unlabeled data have gained considerable attention (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). The most related method to o"
D19-1379,P19-1529,0,0.0343404,"Missing"
D19-1379,K17-1040,0,0.0184403,"ing unlabeled target domain texts (Ben-David et al., 2010; Shi and Sha, 2012). This setting does not allow models to access the test set, which is the main difference between unsupervised domain adaptation and transductive learning. Various unsupervised adaptation methods have been proposed for linear models (Blitzer et al., 2006; Jiang and Zhai, 2007; Tsuboi et al., 2009; Søgaard, 2013). In the context of neural models, adversarial domain adaptation (Ganin and Lempitsky, 2015; Ganin et al., 2016; Guo et al., 2018), importance weighting (Wang et al., 2017), structural correspondence learning (Ziser and Reichart, 2017), self/tri/co-training (Saito et al., 2017; Ruder and Plank, 2018), and other techniques orthogonal to transductive LM fine-tuning have been applied successfully in unsupervised domain adaptation2 . Integrating these methods with transductive LM fine-tuning is an interesting direction for future research. LM-based word representations. Recently, LM-based word representations pre-trained on unlabeled data have gained considerable attention (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). The most related method to ours is Universal Language Model Fine-tuning (ULMFiT), which pre"
D19-3039,W00-0507,0,0.193737,"Missing"
D19-3039,N15-3019,0,0.0338054,"Missing"
D19-3039,N15-3022,0,0.0645812,"frontend AFTER Editor Word Processor Editor Editor TEASPN Research Models GEC Model Suggestion Model Search OSS Spell Checker Commercial API Figure 1: Writing software before and after TEASPN. pletion of a prompt with long, realistic looking yet coherent passages (Radford et al., 2019). However, real-world users such as writers who can and should benefit the most from WATs are yet to reap the fruits from these research efforts. Aside from a small number of commercial products, notably Grammarly2 and Smart Compose (Chen et al., 2019), and research systems such as WriteAhead (Yen et al., 2015; Chang and Chang, 2015) and CroVeWA (Soyer et al., 2015), we see few examples of user-facing applications and experiments that make use of recent development in WATs. Many models are confined in research implementations that are not easily accessible to end users and the larger society. WATs, however, are not truly useful until they are integrated into userfacing writing applications such as editors and word processors (collectively called writing software in this paper) and interact with end users in a dynamic and intuitive manner. This “great divide” (see Figure 1 BEFORE) between applicaIntroduction Language techn"
D19-3039,P12-3027,0,0.0195084,"for assisting writing in a second language (L2) has been extensively explored, especially for non-native English speakers. One of the most active research areas is GEC (Leacock et al., 2010), where several new models are published every year and commercial systems such as Grammarly are actively developed. Other research-based systems include WriteAhead (Yen et al., 2015; Chang and Chang, 2015), an interactive writing environment that provides users with grammatical patterns mined from large corpora, and CroVeWA (Soyer et al., 2015), a crosslingual sentence search system for L2 writers. FLOW (Chen et al., 2012) is another writing assistance system that allows users to type in their first languages (L1) and suggests words and phrases in L2. Running syntactic analysis and visualizing sentence structures have also been explored for L2 reading assistance (Faltin, 2003; Srdanovi´c, 2011). In addition to L2 learners, the use of technologies for assisting human translators has also been a focus of research. TransType (Langlais et al., 2000) is a translation assistance system that suggests completions for the text to the human translator in an interactive manner. In SemEval 2014, van Gompel et al. (2014) pr"
D19-3039,P18-1042,0,0.0141069,"wn in Table 1. See Figure 4 for the screenshot. For syntax highlighting, we used the dependency parser SpaCy9 . Head tokens with specific dependency relation10 were highlighted in different colors. As for the GEC and GED features, we used the open-source GEC tool LanguageTool 3.211 . We implemented two types of completion features: one which suggests the likely next phrases given the context using a neural language model (Radford et al., 2019) and the other one which suggests a set of words consistent with the characters being typed. We built a seq2seq paraphrase model trained on PARANMT-50M (Wieting and Gimpel, 2018) for the text rewriting feature, which allows the writer to select a part of the text and chooses among paraphrases. As for the jump feature, we used a coreference resolution model12 to jump from a selected expression to its antecedent. The hover feature shows the definition of a hovered word using WordNet13 . Finally, we implemented a full-text search feature using the open multilingual sentence dataset Tatoeba14 and used Elasticsearch 7.1.115 for indexing and search. Condition Perplexity # Chars. (mean ± std) BASELINE 37.8 379 ± 116 INTEGRATED 26.4 335 ± 91 Table 2: Statistics of the written"
D19-3039,P15-4024,0,0.0930178,"iting Software Web frontend AFTER Editor Word Processor Editor Editor TEASPN Research Models GEC Model Suggestion Model Search OSS Spell Checker Commercial API Figure 1: Writing software before and after TEASPN. pletion of a prompt with long, realistic looking yet coherent passages (Radford et al., 2019). However, real-world users such as writers who can and should benefit the most from WATs are yet to reap the fruits from these research efforts. Aside from a small number of commercial products, notably Grammarly2 and Smart Compose (Chen et al., 2019), and research systems such as WriteAhead (Yen et al., 2015; Chang and Chang, 2015) and CroVeWA (Soyer et al., 2015), we see few examples of user-facing applications and experiments that make use of recent development in WATs. Many models are confined in research implementations that are not easily accessible to end users and the larger society. WATs, however, are not truly useful until they are integrated into userfacing writing applications such as editors and word processors (collectively called writing software in this paper) and interact with end users in a dynamic and intuitive manner. This “great divide” (see Figure 1 BEFORE) between applicaInt"
D19-5211,D18-2012,0,0.0992129,"We crawled the listed candidate domains and aligned parallel sentences using bitextor4 . Then we filtered out noisy sentences with bicleaner5 (S´anchez-Cartagena et al., 2018). After corpus cleaning, we retained 7.5M sentences. We named this corpus “JParaCrawl” and we plan to release it publicly with a detailed corpus description paper. further improve the performance: (1) model ensembling and (2) Right-to-Left (R2L) re-ranking. 2.2.3 Data Preprocessing This year, we decided not to employ any external morphological analyzer like KyTea (Neubig et al., 2011). Instead we utilized sentencepiece6 (Kudo and Richardson, 2018), which tokenizes a sentence into a sequence of subwords without requiring any other tokenizers. Note here that we did not apply any filtering method, such as sentence length filtering. The JParaCrawl domain basically differs from the scientific paper task. To effectively incorporate with the JParaCrawl, we first pre-trained the model with the mixed data of ASPEC and JParaCrawl.7 Then we fine-tuned the pre-trained model using only ASPEC. 2.3.1 Ensembling We independently trained four models with different random seeds and simultaneously utilized them for model ensembling to boost the translati"
D19-5211,N19-4009,0,0.0874362,"n exploit both the advantages of the L2R and R2L models and improve their performance. 2.3.3 Model incorporation with JParaCrawl 2.4 Hyper-parameter As a base NMT model, we selected the Transformer model with the “big” hyper-parameter setting. During training, we used mixed-precision training (Micikevicius et al., 2018) that can boost the training speed and reduce the memory consumption. We saved the model each epoch and used the average of the last ten models for decoding. We set the beam size to six and normalized the scores by their length. All implementations are based on fairseq toolkit (Ott et al., 2019). Table 2 shows the selected set of the 2.3 System Details We selected the Transformer model (Vaswani et al., 2017) as our base NMT model. We also incorporated two techniques to 1 http://paracrawl.eu/ https://commoncrawl.org/ 3 https://github.com/paracrawl/ extractor 4 https://github.com/bitextor/bitextor 5 https://github.com/bitextor/bicleaner 6 https://github.com/google/ sentencepiece 2 7 We mixed ASPEC and JParaCrawl by upsampling ASPEC twice. 100 Hyper-parameter Subword (vocabulary) size Gradient clipping Dropout rate Mini-batch size Update frequency Beam search (n-best) Selected Value src"
D19-5211,N16-1046,0,0.030925,"PEC. 2.3.1 Ensembling We independently trained four models with different random seeds and simultaneously utilized them for model ensembling to boost the translation performance. 2.3.2 Right-to-left (R2L) re-ranking The NMT model has auto-regressive architecture in its decoder that uses previously generated tokens for predicting the next token. In other words, we normally decode a sentence from the beginning-of-the-sentence (BOS), which is its left side, to the end-of-the-sentence (EOS), which is on the right. Here we call this normal decoding process as Left-to-Right (L2R) decoding. However, Liu et al. (2016) pointed out that L2R decoding lacks reliability near the EOS tokens because if the previous tokens contain errors, the next prediction might have error as well. To alleviate this problem, Liu et al. (2016) proposed a method that generates the n-best hypotheses with the L2R model and re-ranks them with the R2L model, which decodes the sentences from the EOS tokens to the BOS tokens. By R2L re-ranking, we can exploit both the advantages of the L2R and R2L models and improve their performance. 2.3.3 Model incorporation with JParaCrawl 2.4 Hyper-parameter As a base NMT model, we selected the Tran"
D19-5211,W18-6301,0,0.093188,"ality of the English sentences in the latter half of the provided data looks somewhat awful (not very well). Therefore, we then tried to make synthetic data by using forward-translation instead of the standard back-translation. This means that we used the synthetic data for the En-Ja translation setting as the synthetic data of the Ja-En translation setting. This slightly improved the performance of the Ja-En translation setting. 2.4.3 Mini-batch size/Update frequency According to a previously introduced finding, Transformer models tend to provide better results with a larger mini-batch size (Ott et al., 2018). Based on this observation, we explored the effectiveness of the mini-batch size in our setting. Table 5 shows the results. We found that an overly large mini-batch, i.e., 512, degraded the performance. In our experiments, an update frequency of 128, which means 128 × 4, 000 = 512, 000 tokens per mini-batch, was an appropriate value. 2.4.4 Ensemble and R2L re-ranking Ensembling and re-ranking are currently the standard techniques for further improving the translation quality in the NMT models. Following this public knowledge, we also applied standard ensembling and right-to-left (R2L) re-rank"
D19-5211,P02-1040,0,0.10354,"9 the model for 800 updates. During training, we used mixed-precision training (Micikevicius et al., 2018), like the scientific paper subtasks. We saved the model every 100 updates and used the average of the last eight models for decoding. We set the beam size to six and normalized the scores by length. All implementations are based on fairseq toolkit (Ott et al., 2019). Table 10: Number of sentences in timely disclosure document corpus: We split training set into two categories. See Section 3.2 for details. 3.4 Experimental Results and Analysis Table 11 shows the case-sensitive BLEU scores (Papineni et al., 2002) of the provided blind test sets. All the reported BLEU scores were calculated on the organizers’ submission website. 3.3 Experimental Settings For preprocessing, we only relied on sentencepiece (Kudo and Richardson, 2018), which tokenizes a sentence into subwords without requiring any other tokenizers. We set the vocabulary size to 32k9 . The provided training data were split by their released years, but we concatenated them without distinguishing them. As an NMT model, we used the Transformer (Vaswani et al., 2017) with big hyperparameter settings and dropout (Srivastava et al., 2014) with a"
D19-5211,W17-5706,1,0.859249,"e scientific paper subtask in ∗ # Sentences 3,008,500 (1,500,000) (1,508,500) 1,790 1,784 1,812 2.2 Data and Data Preparation 2.2.1 Provided data: constrained setting As training/dev/test data, the task organizer provided the Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016) whose statistics are shown in Table 1. ASPEC was created by automatically aligning parallel documents and sentences, and the training sentences are ordered by sentence alignment scores. Thus, the previous participants generally removed the latter sentences (Neubig, 2014) or used them as synthetic data (Morishita et al., 2017). This year, we used the former 1.5M training sentences as bitext data and the latter 1.5M as monolingual data and created synthetic data (Sennrich et al., 2016). Equal contribution. 99 Proceedings of the 6th Workshop on Asian Translation, pages 99–105 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics 2.2.2 JParaCrawl: unconstrained setting The ParaCrawl1 project is building parallel corpora by largely crawling the web. Their objective is to build parallel corpora for the 24 official languages of the European Union. They already released earlier versions of t"
D19-5211,W18-6319,0,0.0133432,"n changing mini-batch size (update frequency) for each update in NMT training. Scores here were calculated by sacreBLEU. Table 3: Comparison of translation performance on changing subword size. Scores here were calculated by sacreBLEU. hyper-parameters we used for the final submission. In our preliminary experiments, we evaluated extensive combinations of hyper-parameters and we found that this setting was optimal in our hyper-parameter search. Hereafter, the reported performance in the rest of this paper was obtained using this setting unless otherwise specified. performance using sacreBLEU (Post, 2018) for all the results shown in this section. We clearly observe a tendency that the fewer subwords got better performance. This observation is actually a bit surprising since many recent previous studies in the NMT community often employed a larger amount of subwords like 16,000 or 32,000. 2.4.1 Back- and forward-translation for building synthetic data We first investigated the effectiveness of incorporating synthetic data generated by the backtranslation technique. Table 3 shows the results. We significantly improved the performance of the En-Ja translation setting by adding the synthetic data"
D19-5211,W18-6488,0,0.05847,"Missing"
D19-5211,W18-6421,1,0.837884,"05 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics 2.2.2 JParaCrawl: unconstrained setting The ParaCrawl1 project is building parallel corpora by largely crawling the web. Their objective is to build parallel corpora for the 24 official languages of the European Union. They already released earlier versions of the corpora and they were used on the WMT 2018 news shared translation tasks (Bojar et al., 2018). The WMT shared task participants reported that this corpora boosted translation accuracy when used with careful corpus cleaning (Junczys-Dowmunt, 2018; Morishita et al., 2018). Inspired by these previous works, we constructed a web-based Japanese-English parallel corpus. We followed almost the same procedure as ParaCrawl to make this corpus. First, we listed 100,000 candidate domains that might contain parallel Japanese and English sentences by analyzing the whole Common Crawl text data2 on how each domain contains Japanese or English data with extractor3 . We crawled the listed candidate domains and aligned parallel sentences using bitextor4 . Then we filtered out noisy sentences with bicleaner5 (S´anchez-Cartagena et al., 2018). After corpus cleaning, we retained"
D19-5211,P16-1009,0,0.0329958,"ting As training/dev/test data, the task organizer provided the Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016) whose statistics are shown in Table 1. ASPEC was created by automatically aligning parallel documents and sentences, and the training sentences are ordered by sentence alignment scores. Thus, the previous participants generally removed the latter sentences (Neubig, 2014) or used them as synthetic data (Morishita et al., 2017). This year, we used the former 1.5M training sentences as bitext data and the latter 1.5M as monolingual data and created synthetic data (Sennrich et al., 2016). Equal contribution. 99 Proceedings of the 6th Workshop on Asian Translation, pages 99–105 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics 2.2.2 JParaCrawl: unconstrained setting The ParaCrawl1 project is building parallel corpora by largely crawling the web. Their objective is to build parallel corpora for the 24 official languages of the European Union. They already released earlier versions of the corpora and they were used on the WMT 2018 news shared translation tasks (Bojar et al., 2018). The WMT shared task participants reported that this corpora boo"
D19-5211,L16-1350,0,0.13062,"Missing"
D19-5211,W14-7002,0,0.0181288,"we first explain the systems developed for the scientific paper subtask in ∗ # Sentences 3,008,500 (1,500,000) (1,508,500) 1,790 1,784 1,812 2.2 Data and Data Preparation 2.2.1 Provided data: constrained setting As training/dev/test data, the task organizer provided the Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016) whose statistics are shown in Table 1. ASPEC was created by automatically aligning parallel documents and sentences, and the training sentences are ordered by sentence alignment scores. Thus, the previous participants generally removed the latter sentences (Neubig, 2014) or used them as synthetic data (Morishita et al., 2017). This year, we used the former 1.5M training sentences as bitext data and the latter 1.5M as monolingual data and created synthetic data (Sennrich et al., 2016). Equal contribution. 99 Proceedings of the 6th Workshop on Asian Translation, pages 99–105 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics 2.2.2 JParaCrawl: unconstrained setting The ParaCrawl1 project is building parallel corpora by largely crawling the web. Their objective is to build parallel corpora for the 24 official languages of the Eur"
D19-5211,N19-4007,0,0.017437,"ategories. This means that the model already outputs quite similar hypotheses as references. 3.4.2 Fine-tuning with a specific category We found that fine-tuning with specific category data significantly increased the BLEU scores: +3.65 points for texts and +1.56 points for the items categories. Table 13 shows the example translations of the baseline and fine-tuned systems10 . The fine-tuned system perfectly gener9 10 In contrast to the scientific paper subtasks, we did not see improvement with a smaller vocabulary in the preliminary experiments. For finding good examples, we used compare-mt (Neubig et al., 2019), which is a toolkit that compares two MT outputs. 103 Task Texts Items Auto Eval BLEU (Rank) 61.19 (1) 57.34 (1) Pairwise 55.50 34.00 Human Eval (Rank) Adequacy (1) 4.46 (2) 4.47 (Rank) (1) (1) Table 12: Official results of our submitted systems for timely disclosure subtask: Shown rank is only ordered among constrained submissions. Input 実績値、類似建物の修繕費水準、エンジニアリング・レポートの修繕更新費等を考慮し査定 Reference Based on historical data, comparable assets and estimates in the engineering report Baseline Assessed by taking into account the actual results, the level of repair expenses of similar buildings, the level"
E17-1037,C02-1053,1,0.520056,"mploys another summarization approach, the extractive summarization paradigm is worthwhile to leverage research resources. As another benefit, identifying an oracle summary for a set of reference summaries allows us to utilize yet another evaluation measure. Since both oracle and extractive summaries are sets of sentences, it is easy to check whether a system summary contains sentences in the oracle summary. As a result, F-measures, which are available to evaluate a system summary, are useful for evaluating classification-based extractive summarization (Mani and Bloedorn, 1998; Osborne, 2002; Hirao et al., 2002). Since ROUGEn evaluation does not identify which sentence is important, an Fmeasure conveys useful information in terms of “important sentence extraction.” Thus, combining ROUGEn and an F-measure allows us to scrutinize the failure analysis of systems. Note that more than one oracle summary might exist for a set of reference summaries because ROUGEn scores are based on the unweighted counting of n-grams. As a result, an F-measure might not be identical among multiple oracle summaries. Thus, we need to enumerate the oracle summaries for a set of reference summaries and compute the F-measures b"
E17-1037,P13-1020,0,0.0615489,"maries to exploit F-measures that evaluate which system summaries contain how many sentences that are extracted as an oracle summary. Our experimental results obtained from Document Understanding Conference (DUC) corpora demonstrated the following: (1) room still exists to improve the performance of extractive summarization; (2) the F-measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries. 1 Introduction Recently, compressive and abstractive summarization are attracting attention (e.g., Almeida and Martins (2013), Qian and Liu (2013), Yao et al. (2015), Banerjee et al. (2015), Bing et al. (2015)). However, extractive summarization remains a primary research topic because the linguistic quality of the resultant summaries is guaranteed, at least at the sentence level, which is a key requirement for practical use (e.g., Hong and Nenkova (2014), Hong et al. (2015), Yogatama et al. (2015), Parveen et al. (2015)). The summarization research community is experiencing a paradigm shift from extractive to compressive or abstractive summarization. Currently our question is: “Is extractive summariza386 Proceeding"
E17-1037,E14-1075,0,0.0408382,"measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries. 1 Introduction Recently, compressive and abstractive summarization are attracting attention (e.g., Almeida and Martins (2013), Qian and Liu (2013), Yao et al. (2015), Banerjee et al. (2015), Bing et al. (2015)). However, extractive summarization remains a primary research topic because the linguistic quality of the resultant summaries is guaranteed, at least at the sentence level, which is a key requirement for practical use (e.g., Hong and Nenkova (2014), Hong et al. (2015), Yogatama et al. (2015), Parveen et al. (2015)). The summarization research community is experiencing a paradigm shift from extractive to compressive or abstractive summarization. Currently our question is: “Is extractive summariza386 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 386–396, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics summary from a set of reference summaries and a source document(s). To the best of our knowledge, this is the firs"
E17-1037,hong-etal-2014-repository,0,0.410501,"Missing"
E17-1037,P15-1153,0,0.0190065,"that are extracted as an oracle summary. Our experimental results obtained from Document Understanding Conference (DUC) corpora demonstrated the following: (1) room still exists to improve the performance of extractive summarization; (2) the F-measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries. 1 Introduction Recently, compressive and abstractive summarization are attracting attention (e.g., Almeida and Martins (2013), Qian and Liu (2013), Yao et al. (2015), Banerjee et al. (2015), Bing et al. (2015)). However, extractive summarization remains a primary research topic because the linguistic quality of the resultant summaries is guaranteed, at least at the sentence level, which is a key requirement for practical use (e.g., Hong and Nenkova (2014), Hong et al. (2015), Yogatama et al. (2015), Parveen et al. (2015)). The summarization research community is experiencing a paradigm shift from extractive to compressive or abstractive summarization. Currently our question is: “Is extractive summariza386 Proceedings of the 15th Conference of the European Chapter of the Association for Computationa"
E17-1037,D15-1011,0,0.027651,"Missing"
E17-1037,N10-1133,0,0.0286592,"here the ROUGEn scores of the oracle summaries are significantly higher than those of the state-ofthe-art summarization systems. 3 Definition of Extractive Oracle Summaries We first briefly describe ROUGEn . Given set of reference summaries R and system summary S, ROUGEn is defined as follows: ROUGEn (R, S) = |R ||U (Rk )| X X k=1 min{N (gjn , Rk ), N (gjn , S)} j=1 |R ||U (Rk )| X X k=1 . Related Work Lin and Hovy (2003) utilized a naive exhaustive search method to obtain oracle summaries in terms of ROUGEn and exploited them to understand the limitations of extractive summarization systems. Ceylan et al. (2010) proposed another naive exhaustive search method to derive a probability density function from the ROUGEn scores of oracle summaries for the domains to which source documents belong. The computational complexity of naive exhaustive methods is exponential to the size of the sentence set. Thus, it may be possible to apply them to single document summarization tasks involving a dozen sentences, but it is infeasible to apply them to multiple document summarization tasks that involve several hundred sentences. To describe the difference between the ROUGEn scores of oracle and system summaries in mu"
E17-1037,P11-1052,0,0.204221,"nt Lmax (line 11). When we do not have room to add w, we update U by adding the score obtained by multiplying the density of w by the remaining length, Lmax (line 13), and exit the while loop. 5.3 Initial Score for Search 1. ROUGEn (R, V ) ≥ τ ; 2. ROUGEn (R, V ) < τ , R OUGE n (R, V ) < τ ; 3. ROUGEn (R, V ) < τ , R OUGE n (R, V ) ≥ τ . Since the branch and bound technique prunes the search by comparing the best solution found so far with the upper bounds, obtaining a good solution in the early stage is critical for raising search efficiency. Since ROUGEn is a monotone submodular function (Lin and Bilmes, 2011), we can obtain a good approximate solution by a greedy algorithm (Khuller et al., 1999). It is guaranteed that the score of the obtained approximate solution is larger than 12 (1 − 1e )OPT, where OPT is the score of the optimal solution. We employ the solution as the initial ROUGEn score of the candidate oracle summary. Algorithm 2 shows the greedy algorithm. In it, S denotes a summary and D denotes a set of sentences. The algorithm iteratively adds sentence s∗ that yields the largest gain in the ROUGEn score to current summary S, provided the length of the summary does not violate length con"
E17-1037,W13-3108,0,0.06166,"Missing"
E17-1037,W03-0510,0,0.049582,"inatorial optimization problem, and no polynomial time algorithms exist that can attain an optimal solution. 1. Room still exists for the further improvement of extractive summarization, i.e., where the ROUGEn scores of the oracle summaries are significantly higher than those of the state-ofthe-art summarization systems. 3 Definition of Extractive Oracle Summaries We first briefly describe ROUGEn . Given set of reference summaries R and system summary S, ROUGEn is defined as follows: ROUGEn (R, S) = |R ||U (Rk )| X X k=1 min{N (gjn , Rk ), N (gjn , S)} j=1 |R ||U (Rk )| X X k=1 . Related Work Lin and Hovy (2003) utilized a naive exhaustive search method to obtain oracle summaries in terms of ROUGEn and exploited them to understand the limitations of extractive summarization systems. Ceylan et al. (2010) proposed another naive exhaustive search method to derive a probability density function from the ROUGEn scores of oracle summaries for the domains to which source documents belong. The computational complexity of naive exhaustive methods is exponential to the size of the sentence set. Thus, it may be possible to apply them to single document summarization tasks involving a dozen sentences, but it is"
E17-1037,W09-1802,0,0.38159,"Missing"
E17-1037,W02-0401,0,0.114143,"a system that employs another summarization approach, the extractive summarization paradigm is worthwhile to leverage research resources. As another benefit, identifying an oracle summary for a set of reference summaries allows us to utilize yet another evaluation measure. Since both oracle and extractive summaries are sets of sentences, it is easy to check whether a system summary contains sentences in the oracle summary. As a result, F-measures, which are available to evaluate a system summary, are useful for evaluating classification-based extractive summarization (Mani and Bloedorn, 1998; Osborne, 2002; Hirao et al., 2002). Since ROUGEn evaluation does not identify which sentence is important, an Fmeasure conveys useful information in terms of “important sentence extraction.” Thus, combining ROUGEn and an F-measure allows us to scrutinize the failure analysis of systems. Note that more than one oracle summary might exist for a set of reference summaries because ROUGEn scores are based on the unweighted counting of n-grams. As a result, an F-measure might not be identical among multiple oracle summaries. Thus, we need to enumerate the oracle summaries for a set of reference summaries and com"
E17-1037,W12-2601,0,0.0227224,"sentence from V and return to the top of the recurrence. 6 6.1 Experiments Experimental Setting We conducted experiments on the corpora developed for a multiple document summarization task in DUC 2001 to 2007. Table 1 show the statistics of the data. In particular, the DUC-2005 to -2007 data sets not only have very large numbers of sentences and words but also a long target length (the reference summary length) of 250 words. All the words in the documents were stemmed by Porter’s stemmer (Porter, 1980). We computed ROUGE1 scores, excluding stopwords, and computed ROUGE2 scores, keeping them. Owczarzak et al. (2012) suggested using ROUGE1 and keeping stopwords. However, as Takamura et al. argued (Takamura and Okumura, 2009), the summaries optimized with non-content words failed to consider the actual quality. Thus, we excluded stopwords for computing the ROUGE1 scores. We enumerated the following two types of oracle summaries: those for a set of references for a given topic and those for each reference in the set of references. 6.2 6.2.1 Results and Discussion Impact of Oracle ROUGEn scores Table 2 shows the average ROUGE1,2 scores of the oracle summaries obtained from both a set of references and each r"
E17-1037,D15-1226,0,0.0258891,"tly stronger correlations with human judgment than those derived from single oracle summaries. 1 Introduction Recently, compressive and abstractive summarization are attracting attention (e.g., Almeida and Martins (2013), Qian and Liu (2013), Yao et al. (2015), Banerjee et al. (2015), Bing et al. (2015)). However, extractive summarization remains a primary research topic because the linguistic quality of the resultant summaries is guaranteed, at least at the sentence level, which is a key requirement for practical use (e.g., Hong and Nenkova (2014), Hong et al. (2015), Yogatama et al. (2015), Parveen et al. (2015)). The summarization research community is experiencing a paradigm shift from extractive to compressive or abstractive summarization. Currently our question is: “Is extractive summariza386 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 386–396, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics summary from a set of reference summaries and a source document(s). To the best of our knowledge, this is the first ILP formulation that extracts oracle summaries. Second, since it"
E17-1037,P16-1172,0,0.0151764,"denotes the multiple set of n-grams that appear in system-generated summary S (a set of sentences). N (gjn , Rk ) and N (gjn , S) return the number of occurrences of n-gram gjn in the k-th reference and system summaries, respectively. Function U (·) transforms a multiple set into a normal set. ROUGEn takes values in the range of [0, 1], and when the n-gram occurrences of the system summary agree with those of the reference summary, the value is 1. In this paper, we focus on extractive summarization, employ ROUGEn as an evaluation measure, 387 with oracle summaries found by a greedy algorithm. Peyrard and Eckle-Kohler (2016) proposed a method to find a summary that approximates a ROUGE score based on the ROUGE scores of individual sentences and exploited the framework to train their summarizer. As mentioned above, such summaries do not always agree with the oracle summaries defined in our paper. Thus, the quality of the training data is suspect. Moreover, since these studies fail to consider that a set of reference summaries has multiple oracle summaries, the score of the loss function defined between their oracle and system summaries is not appropriate in most cases. As mentioned above, no known efficient algori"
E17-1037,D13-1156,0,0.0742105,"that evaluate which system summaries contain how many sentences that are extracted as an oracle summary. Our experimental results obtained from Document Understanding Conference (DUC) corpora demonstrated the following: (1) room still exists to improve the performance of extractive summarization; (2) the F-measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries. 1 Introduction Recently, compressive and abstractive summarization are attracting attention (e.g., Almeida and Martins (2013), Qian and Liu (2013), Yao et al. (2015), Banerjee et al. (2015), Bing et al. (2015)). However, extractive summarization remains a primary research topic because the linguistic quality of the resultant summaries is guaranteed, at least at the sentence level, which is a key requirement for practical use (e.g., Hong and Nenkova (2014), Hong et al. (2015), Yogatama et al. (2015), Parveen et al. (2015)). The summarization research community is experiencing a paradigm shift from extractive to compressive or abstractive summarization. Currently our question is: “Is extractive summariza386 Proceedings of the 15th Confere"
E17-1037,E12-1023,0,0.0998676,"widely used to solve NP-hard combinatorial optimization problems, the solutions are not always optimal. Thus, the summary does not always have a maximum ROUGEn score for the set of reference summaries. Both works called the summary found by their methods the oracle, but it differs from the definition in our paper. Since summarization systems cannot reproduce human-made reference summaries in most cases, oracle summaries, which can be reproduced by summarization systems, have been used as training data to tune the parameters of summarization systems. For example, Kulesza and Tasker (2011) and Sipos et al. (2012) trained their summarizers 2. The F-measures derived from multiple oracle summaries obtain significantly stronger correlations with human judgment than those derived from single oracle summaries. 2 (2) (1) N (gjn , Rk ) j=1 Rk denotes the multiple set of n-grams that occur in k-th reference summary Rk , and S denotes the multiple set of n-grams that appear in system-generated summary S (a set of sentences). N (gjn , Rk ) and N (gjn , S) return the number of occurrences of n-gram gjn in the k-th reference and system summaries, respectively. Function U (·) transforms a multiple set into a normal"
E17-1037,E09-1089,0,0.140371,"ted experiments on the corpora developed for a multiple document summarization task in DUC 2001 to 2007. Table 1 show the statistics of the data. In particular, the DUC-2005 to -2007 data sets not only have very large numbers of sentences and words but also a long target length (the reference summary length) of 250 words. All the words in the documents were stemmed by Porter’s stemmer (Porter, 1980). We computed ROUGE1 scores, excluding stopwords, and computed ROUGE2 scores, keeping them. Owczarzak et al. (2012) suggested using ROUGE1 and keeping stopwords. However, as Takamura et al. argued (Takamura and Okumura, 2009), the summaries optimized with non-content words failed to consider the actual quality. Thus, we excluded stopwords for computing the ROUGE1 scores. We enumerated the following two types of oracle summaries: those for a set of references for a given topic and those for each reference in the set of references. 6.2 6.2.1 Results and Discussion Impact of Oracle ROUGEn scores Table 2 shows the average ROUGE1,2 scores of the oracle summaries obtained from both a set of references and each reference in the set (“multi” and “single”), those of the best conventional system (Peer), and those obtained f"
E17-1037,D15-1228,0,0.0364183,"Missing"
E17-2047,D15-1166,0,0.0367292,"ting generation behavior can become more severe in some NLG tasks than in MT. The very short ABS task in DUC-2003 and 2004 (Over et al., 2007) is a typical example because it requires the generation of a summary in a pre-defined limited output space, such as ten words or 75 bytes. Thus, the repeated output consumes precious limited output space. Unfortunately, the coverage approach cannot be directly applied to ABS tasks since they require us to optimally find salient ideas 2 Baseline RNN-based EncDec Model The baseline of our proposal is an RNN-based EncDec model with an attention mechanism (Luong et al., 2015). In fact, this model has already been used as a strong baseline for ABS tasks (Chopra et al., 2016; Kikuchi et al., 2016) as well as in the NMT literature. More specifically, as a case study we employ a 2-layer bidirectional LSTM encoder and a 2-layer LSTM decoder with a global attention (Bahdanau et al., 2014). We omit a detailed review of the descriptions due to space limitations. The following are the necessary parts for explaining our proposed method. Let X = (xi )Ii=1 and Y = (yj )Jj=1 be input and output sequences, respectively, where xi and 291 Proceedings of the 15th Conference of the"
E17-2047,D14-1179,0,0.0183778,"Missing"
E17-2047,D16-1096,0,0.0741514,"ased encoder-decoder (EncDec) approach has recently been providing significant progress in various natural language generation (NLG) tasks, i.e., machine translation (MT) (Sutskever et al., 2014; Cho et al., 2014) and abstractive summarization (ABS) (Rush et al., 2015). Since a scheme in this approach can be interpreted as a conditional language model, it is suitable for NLG tasks. However, one potential weakness is that it sometimes repeatedly generates the same phrase (or word). This issue has been discussed in the neural MT (NMT) literature as a part of a coverage problem (Tu et al., 2016; Mi et al., 2016). Such repeating generation behavior can become more severe in some NLG tasks than in MT. The very short ABS task in DUC-2003 and 2004 (Over et al., 2007) is a typical example because it requires the generation of a summary in a pre-defined limited output space, such as ten words or 75 bytes. Thus, the repeated output consumes precious limited output space. Unfortunately, the coverage approach cannot be directly applied to ABS tasks since they require us to optimally find salient ideas 2 Baseline RNN-based EncDec Model The baseline of our proposal is an RNN-based EncDec model with an attention"
E17-2047,N16-1012,0,0.176215,"cy of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step. We refer to our additional component as a wordfrequency estimation (WFE) sub-model. The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process. Thus, we expect to decisively prohibit excessive generation. Finally, we evaluate the effectiveness of our method on well-studied ABS benchmark data provided by Rush et al. (2015), and evaluated in (Chopra et al., 2016; Nallapati et al., 2016b; Kikuchi et al., 2016; Takase et al., 2016; Ayana et al., 2016; Gulcehre et al., 2016). This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark. 1 Introduction The RNN"
E17-2047,K16-1028,0,0.0150191,"abulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step. We refer to our additional component as a wordfrequency estimation (WFE) sub-model. The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process. Thus, we expect to decisively prohibit excessive generation. Finally, we evaluate the effectiveness of our method on well-studied ABS benchmark data provided by Rush et al. (2015), and evaluated in (Chopra et al., 2016; Nallapati et al., 2016b; Kikuchi et al., 2016; Takase et al., 2016; Ayana et al., 2016; Gulcehre et al., 2016). This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark. 1 Introduction The RNN-based encoder-decoder ("
E17-2047,D15-1044,0,0.611359,"intly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step. We refer to our additional component as a wordfrequency estimation (WFE) sub-model. The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process. Thus, we expect to decisively prohibit excessive generation. Finally, we evaluate the effectiveness of our method on well-studied ABS benchmark data provided by Rush et al. (2015), and evaluated in (Chopra et al., 2016; Nallapati et al., 2016b; Kikuchi et al., 2016; Takase et al., 2016; Ayana et al., 2016; Gulcehre et al., 2016). This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summariz"
E17-2047,P16-1159,0,0.0255766,"le 2 and Figure 3. We can expect to further gain the overall performance by improving the performance of the WFE sub-model. Table 4: Confusion matrix of WFE on Gigaword data: only evaluated true frequency ≥ 1. imum risk estimation, while we trained all the models in our experiments with standard (pointwise) log-likelihood maximization. MRT essentially complements our method. We expect to further improve its performance by applying MRT for its training since recent progress of NMT has suggested leveraging a sequence-wise optimization technique for improving performance (Wiseman and Rush, 2016; Shen et al., 2016). We leave this as our future work. 4.3 5 This paper discussed the behavior of redundant repeating generation often observed in neural EncDec approaches. We proposed a method for reducing such redundancy by incorporating a submodel that directly estimates and manages the frequency of each target vocabulary in the output. Experiments on ABS benchmark data showed the effectiveness of our method, EncDec+WFE, for both improving automatic evaluation performance and reducing the actual redundancy. Our method is suitable for lossy compression tasks such as image caption generation tasks. Generation e"
E17-2047,P16-1014,0,0.121269,"on to control the output words in each decoding step. We refer to our additional component as a wordfrequency estimation (WFE) sub-model. The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process. Thus, we expect to decisively prohibit excessive generation. Finally, we evaluate the effectiveness of our method on well-studied ABS benchmark data provided by Rush et al. (2015), and evaluated in (Chopra et al., 2016; Nallapati et al., 2016b; Kikuchi et al., 2016; Takase et al., 2016; Ayana et al., 2016; Gulcehre et al., 2016). This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark. 1 Introduction The RNN-based encoder-decoder (EncDec) approach has recently been providing significant progress in various natural lan"
E17-2047,D16-1140,0,0.224773,"a summary during the encoding process and exploit the estimation to control the output words in each decoding step. We refer to our additional component as a wordfrequency estimation (WFE) sub-model. The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process. Thus, we expect to decisively prohibit excessive generation. Finally, we evaluate the effectiveness of our method on well-studied ABS benchmark data provided by Rush et al. (2015), and evaluated in (Chopra et al., 2016; Nallapati et al., 2016b; Kikuchi et al., 2016; Takase et al., 2016; Ayana et al., 2016; Gulcehre et al., 2016). This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark. 1 Introduction The RNN-based encoder-decoder (EncDec) approach has re"
E17-2047,D16-1112,1,0.505499,"encoding process and exploit the estimation to control the output words in each decoding step. We refer to our additional component as a wordfrequency estimation (WFE) sub-model. The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process. Thus, we expect to decisively prohibit excessive generation. Finally, we evaluate the effectiveness of our method on well-studied ABS benchmark data provided by Rush et al. (2015), and evaluated in (Chopra et al., 2016; Nallapati et al., 2016b; Kikuchi et al., 2016; Takase et al., 2016; Ayana et al., 2016; Gulcehre et al., 2016). This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark. 1 Introduction The RNN-based encoder-decoder (EncDec) approach has recently been providing"
E17-2047,P16-1008,0,0.0948626,"duction The RNN-based encoder-decoder (EncDec) approach has recently been providing significant progress in various natural language generation (NLG) tasks, i.e., machine translation (MT) (Sutskever et al., 2014; Cho et al., 2014) and abstractive summarization (ABS) (Rush et al., 2015). Since a scheme in this approach can be interpreted as a conditional language model, it is suitable for NLG tasks. However, one potential weakness is that it sometimes repeatedly generates the same phrase (or word). This issue has been discussed in the neural MT (NMT) literature as a part of a coverage problem (Tu et al., 2016; Mi et al., 2016). Such repeating generation behavior can become more severe in some NLG tasks than in MT. The very short ABS task in DUC-2003 and 2004 (Over et al., 2007) is a typical example because it requires the generation of a summary in a pre-defined limited output space, such as ten words or 75 bytes. Thus, the repeated output consumes precious limited output space. Unfortunately, the coverage approach cannot be directly applied to ABS tasks since they require us to optimally find salient ideas 2 Baseline RNN-based EncDec Model The baseline of our proposal is an RNN-based EncDec model"
E17-2047,D16-1137,0,0.0591229,"Missing"
I08-2116,H05-1087,0,0.392191,"on binary classification. With this approach, we assume the independence of categories and design a binary classifier for each category that determines whether or not to assign a category label to data samples. Statistical classifiers such as the logistic regression model (LRM), the support vector machine (SVM), and naive Bayes are employed as binary classifiers (Joachims, 1998). In text categorization, the F1 -score is often used to evaluate classifier performance. Recently, methods for training binary classifiers to maximize the F1 -score have been proposed for SVM (Joachims, 2005) and LRM (Jansche, 2005). It was confirmed experimentally that these training methods were more effective for obtaining binary classifiers with better F1 -score performance than the minimum error rate and maximum likelihood used for training conventional classifiers, especially when there was a large imbalance between positive and negative samples. In multi-label categorization, macroand micro-averaged F1 -scores are often used to evaluate classification performance. Therefore, we can expect to improve multi-label classification performance by using binary classifiers trained to maximize the F1 -score. On the other h"
I11-1073,W08-0304,0,0.0816198,"faster parameter tuning algorithm would have a positive impact on research on all the components of the SMT system. Imagine a researcher designing a new pruning algorithm for decoding, a new word alignment model, or a new domain adaptation method. Any of these methods need to be evaluated in the context of a full SMT system, which requires parameter tuning. If we can reduce the parameter tuning time from 10 hours to 1 hour, this can greatly increase the pace of innovation. Thus our motivation is orthogonal to recent research on improving MERT, such as efforts to escape local maxima problems (Cer et al., 2008; 649 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 649–657, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP proposed by (Och, 2003). There are several variations for updating weights during the iterative tuning process in MERT. The most commonly used algorithm for MERT is usually called Koehncoordinate descent (KCD), which is used in the MERT utility packaged in the popular Moses statistical machine translation system (Koehn et al., 2007). Another choice is Powell’s method that was advocated when MERT was first introduced for SMT (Och, 2"
I11-1073,W09-0439,0,0.549104,"sentences in the tuning dataset with the current weights to generate N -best lists of translations. Then, the method employs the inner loop procedure to optimize the weights based on those N -best lists instead of decoding the tuning dataset. After obtaining the optimal weights from the inner loop, we repeat the outer loop to generate a new N -best list. Figure 1 shows the system outline, which is described in detail elsewhere (Bertoldi et al., 2009). We note here that the method proposed in this paper essentially involves the replacement of the inner loop algorithm of the conventional MERT. Foster and Kuhn, 2009; Moore and Quirk, 2008), or incorporate lattices (Macherey et al., 2008). 2 Parameter Tuning for SMT Systems Most recently developed SMT systems consist of several model components, such as translation models, language models, and reordering models. To combine evidence obtained from these different components, we often define a discriminative (log-)linear model. Suppose the SMT system has D components. The log probabilities of the components are usually treated as features in the discriminative model. We denote the d-th feature, or log probability of the d-th component given a source sentence"
I11-1073,P07-2045,0,0.0612959,"on is orthogonal to recent research on improving MERT, such as efforts to escape local maxima problems (Cer et al., 2008; 649 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 649–657, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP proposed by (Och, 2003). There are several variations for updating weights during the iterative tuning process in MERT. The most commonly used algorithm for MERT is usually called Koehncoordinate descent (KCD), which is used in the MERT utility packaged in the popular Moses statistical machine translation system (Koehn et al., 2007). Another choice is Powell’s method that was advocated when MERT was first introduced for SMT (Och, 2003). Since KCD tends to be marginally more effective at optimizing the MERT objective, and is much simpler to implement than Powell’s method, this paper focuses only on KCD. KCD is a variant of a coordinate ascent (or descent) algorithm. At each iteration, it moves along the coordinate, which allows for the greatest progress of the maximization. The routine performs a trial line maximization along each coordinate to determine which one should be selected. It then updates the weight vector with"
I11-1073,D08-1076,0,0.0441913,"-best lists of translations. Then, the method employs the inner loop procedure to optimize the weights based on those N -best lists instead of decoding the tuning dataset. After obtaining the optimal weights from the inner loop, we repeat the outer loop to generate a new N -best list. Figure 1 shows the system outline, which is described in detail elsewhere (Bertoldi et al., 2009). We note here that the method proposed in this paper essentially involves the replacement of the inner loop algorithm of the conventional MERT. Foster and Kuhn, 2009; Moore and Quirk, 2008), or incorporate lattices (Macherey et al., 2008). 2 Parameter Tuning for SMT Systems Most recently developed SMT systems consist of several model components, such as translation models, language models, and reordering models. To combine evidence obtained from these different components, we often define a discriminative (log-)linear model. Suppose the SMT system has D components. The log probabilities of the components are usually treated as features in the discriminative model. We denote the d-th feature, or log probability of the d-th component given a source sentence f and its translation e, as φd (e, f ). We also denote the d-th weight a"
I11-1073,C08-1074,0,0.0654341,"g dataset with the current weights to generate N -best lists of translations. Then, the method employs the inner loop procedure to optimize the weights based on those N -best lists instead of decoding the tuning dataset. After obtaining the optimal weights from the inner loop, we repeat the outer loop to generate a new N -best list. Figure 1 shows the system outline, which is described in detail elsewhere (Bertoldi et al., 2009). We note here that the method proposed in this paper essentially involves the replacement of the inner loop algorithm of the conventional MERT. Foster and Kuhn, 2009; Moore and Quirk, 2008), or incorporate lattices (Macherey et al., 2008). 2 Parameter Tuning for SMT Systems Most recently developed SMT systems consist of several model components, such as translation models, language models, and reordering models. To combine evidence obtained from these different components, we often define a discriminative (log-)linear model. Suppose the SMT system has D components. The log probabilities of the components are usually treated as features in the discriminative model. We denote the d-th feature, or log probability of the d-th component given a source sentence f and its translation e"
I11-1073,P03-1021,0,0.718943,"rd alignment model, or a new domain adaptation method. Any of these methods need to be evaluated in the context of a full SMT system, which requires parameter tuning. If we can reduce the parameter tuning time from 10 hours to 1 hour, this can greatly increase the pace of innovation. Thus our motivation is orthogonal to recent research on improving MERT, such as efforts to escape local maxima problems (Cer et al., 2008; 649 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 649–657, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP proposed by (Och, 2003). There are several variations for updating weights during the iterative tuning process in MERT. The most commonly used algorithm for MERT is usually called Koehncoordinate descent (KCD), which is used in the MERT utility packaged in the popular Moses statistical machine translation system (Koehn et al., 2007). Another choice is Powell’s method that was advocated when MERT was first introduced for SMT (Och, 2003). Since KCD tends to be marginally more effective at optimizing the MERT objective, and is much simpler to implement than Powell’s method, this paper focuses only on KCD. KCD is a vari"
I11-1073,P02-1040,0,0.0919199,"Missing"
I17-2008,D15-1199,0,0.115814,"Missing"
I17-2008,J93-2004,0,0.0604496,"Missing"
I17-2008,E17-2025,0,0.0493625,"IOG reduced the perplexity. In other words, IOG boosted the performance of the baseline models. We emphasize that IOG is not restricted to a neural architecture of a language model because it improved the RHN and LSTM performances. In addition to the comparison with the baselines, Table 2 and Table 3 contain the scores published in previous studies. Merity et al. (2017b) and Grave et al. (2017) proposed similar methods. Their methods, which are called “cache mechanism” (or ‘pointer’), keep multiple hidden states at past timesteps to select words from previous sequences. Inan et al. (2016) and Press and Wolf (2017) introduced a technique that shares word embeddings with the weight matrix of the final layer (represented as ‘WT’ in Table 2). Inan et al. (2016) also proposed using word embeddings to augment loss function (represented as ‘AL’ in Table 2). Zoph and Le (2017) adopted RNNs and reinforcement learning to automatically construct a novel RNN architecture. We expect that IOG will improve these models since it can be combined with any RNN language models. In fact, Table 2 and Table 3 6 In contrast to other comparisons, we used the following implementation by the authors: https://github.com/salesforc"
I17-2008,D15-1044,0,0.0689356,"anguage models. Hereafter, we denote a word sequence with length T , namely, w1 , ..., wT as w1:T for short. Formally, a typical RNN language model computes the joint probability of word sequence w1:T by the product of the conditional probabilities of each timestep t: A neural language model is a central technology of recently developed neural architectures in the natural language processing (NLP) field. For example, neural encoder-decoder models, which were successfully applied to various natural language generation tasks including machine translation (Sutskever et al., 2014), summarization (Rush et al., 2015), and dialogue (Wen et al., 2015), can be interpreted as conditional neural language models. Moreover, word embedding methods, such as Skip-gram (Mikolov et al., 2013) and vLBL (Mnih and Kavukcuoglu, 2013), are also originated from neural language models that aim to handle much larger vocabulary and data sizes. Thus, language modeling is a good benchmark task for investigating the general frameworks of neural methods in the NLP field. In this paper, we address improving the performance on the language modeling task. In particular, we focus on boosting the quality of existing Recurrent Neural N"
I17-2044,D16-1161,0,0.0422605,"ta. When we sample dialect pattern ms from MatchedList, we use two types of p(ms |mt ). The first type is fixed probability. We set p(ms |mt ) = 1/len(MatchedList) for all matched patterns. The second type is generative probability, which is calculated from the training data (see the previous subsection). The comparison of these two types of probabilities is discussed in the experimental section. 4.2 Settings For the baseline model other than encoderdecoder models, we used Moses. Moses is a tool of training statistical machine translation and a strong baseline for the text-normalization task (Junczys-Dowmunt and Grundkiewicz, 2016). For such a task, we can ignore the word reordering; therefore, we set the distortion limit to 0. We used MERT on the development set for tuning. We confirmed that using both manually annotated and augmented data for building LM greatly degraded its final BLUE score in our preliminary experiments and used only manually annotated data as the training data of LM. We used beam search for the encoder-decoder model (EncDec) and set the beam size to 10. When in the n beam search step, we used length normalized score S(t, s), where 3.2 Generating Augmented Data using Character-level Conversion For o"
I17-2044,D15-1166,0,0.0797738,"ence Laboratories 3 Nara Institute of Science and Technology {saito.itsumi, suzuki.jun, nishida.kyosuke}@lab.ntt.co.jp, {masumura.ryo, kobashikawa.satoshi, tomita.junji}@lab.ntt.co.jp matsu@is.naist.jp, k.sadamitsu.ic@future.co.jp Abstract using statistical machine translation (SMT) techniques (Junczys-Dowmunt and Grundkiewicz, 2016; Yuan and Briscoe, 2016), in particular, utilizing the Moses toolkit (Koehn et al., 2007). In recent years, encoder-decoder models with an attention mechanism (Bahdanau et al., 2014) have made great progress regarding many NLP tasks, including machine translation (Luong et al., 2015; Sennrich et al., 2016), text summarization (Rush et al., 2015) and text normalization (Xie et al., 2016; Yuan and Briscoe, 2016; Ikeda et al., 2017). We can also simply apply an encoder-decoder model to text normalization tasks. However, it is well-known that encoderdecoder models often fail to perform better than conventional methods when the availability of training data is insufficient. Unfortunately, the amount of training data for text normalization tasks is generally relatively small to sufficiently train encoder-decoder models. Therefore, data utilization and augmentation are importan"
I17-2044,P02-1040,0,0.0995881,"p(t|s, θ))/|t|. We maximize S(t, s) to find normalized sentence. We set the embedding size of the character and hidden layer to 300 and 256, respectively. We used ”mrphaug (mr)” as the augmented data generated from morphological-level conversion and ”mosesaug (mo)” as augmented data generated from character-level conversion (Moses). The “mr:R” and “mr:W” represent the difference in generative probability p(ms |mt ), which is used when generating augmented data; “mr:R” indicates fixed generative probability and “mr:W” indicates weighted generative probability. For the evaluation, we used BLEU (Papineni et al., 2002), which is widely used for machine translation. 4.3 Results Table 3 lists the normalization results. Notransformation indicates the result of evaluating input sentences without transformation. Moses achieved a reasonable BLEU score with a small amount of human-annotated data. However, the improvement of adding augmented data was limited. On the other hand, the encoder-decoder model showed a very low BLEU score with a small amount of human-annotated data. With this amount of data, the encoder-decoder model gen5 Discussion Oracle Analysis To investigate the further improvement on normalization a"
I17-2044,D15-1044,0,0.0499875,"ito.itsumi, suzuki.jun, nishida.kyosuke}@lab.ntt.co.jp, {masumura.ryo, kobashikawa.satoshi, tomita.junji}@lab.ntt.co.jp matsu@is.naist.jp, k.sadamitsu.ic@future.co.jp Abstract using statistical machine translation (SMT) techniques (Junczys-Dowmunt and Grundkiewicz, 2016; Yuan and Briscoe, 2016), in particular, utilizing the Moses toolkit (Koehn et al., 2007). In recent years, encoder-decoder models with an attention mechanism (Bahdanau et al., 2014) have made great progress regarding many NLP tasks, including machine translation (Luong et al., 2015; Sennrich et al., 2016), text summarization (Rush et al., 2015) and text normalization (Xie et al., 2016; Yuan and Briscoe, 2016; Ikeda et al., 2017). We can also simply apply an encoder-decoder model to text normalization tasks. However, it is well-known that encoderdecoder models often fail to perform better than conventional methods when the availability of training data is insufficient. Unfortunately, the amount of training data for text normalization tasks is generally relatively small to sufficiently train encoder-decoder models. Therefore, data utilization and augmentation are important to take full advantage of encoder-decoder models. Xie et al. ("
I17-2044,P13-2001,0,0.0292238,"n encoder-decoder model and achieve higher BLEU score than that of baselines. We also investigated the oracle performance and revealed that there is sufficient room for improving an encoder-decoder model. 1 Introduction Text normalization is an important fundamental technology in actual natural language processing (NLP) systems to appropriately handle texts such as those for social media. This is because social media texts contain non-standard texts, such as typos, dialects, chat abbreviations1 , and emoticons; thus, current NLP systems often fail to correctly analyze such texts (Huang, 2015; Sajjad et al., 2013; Han et al., 2013). Normalization can help correctly analyze and understand these texts. One of the most promising conventional approaches for tackling text normalizing tasks is ∗ Present affiliation: Future Architect,Inc. short forms of words or phrases such as “4u” to represent “for you” 1 257 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 257–262, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP region. We propose two-level data-augmentation methods that do not use prior knowledge. The contributions of this study are summarized as"
I17-2044,P16-1009,0,0.0383275,"Nara Institute of Science and Technology {saito.itsumi, suzuki.jun, nishida.kyosuke}@lab.ntt.co.jp, {masumura.ryo, kobashikawa.satoshi, tomita.junji}@lab.ntt.co.jp matsu@is.naist.jp, k.sadamitsu.ic@future.co.jp Abstract using statistical machine translation (SMT) techniques (Junczys-Dowmunt and Grundkiewicz, 2016; Yuan and Briscoe, 2016), in particular, utilizing the Moses toolkit (Koehn et al., 2007). In recent years, encoder-decoder models with an attention mechanism (Bahdanau et al., 2014) have made great progress regarding many NLP tasks, including machine translation (Luong et al., 2015; Sennrich et al., 2016), text summarization (Rush et al., 2015) and text normalization (Xie et al., 2016; Yuan and Briscoe, 2016; Ikeda et al., 2017). We can also simply apply an encoder-decoder model to text normalization tasks. However, it is well-known that encoderdecoder models often fail to perform better than conventional methods when the availability of training data is insufficient. Unfortunately, the amount of training data for text normalization tasks is generally relatively small to sufficiently train encoder-decoder models. Therefore, data utilization and augmentation are important to take full advantage"
I17-2044,N16-1042,0,0.025244,"rmalization with Data Augmentation at Character- and Morphological Levels Itsumi Saito1 Jun Suzuki2 Kyosuke Nishida1 Kugatsu Sadamitsu1∗ Satoshi Kobashikawa1 Ryo Masumura1 Yuji Matsumoto3 Junji Tomita1 1 NTT Media Intelligence Laboratories, 2 NTT Communication Science Laboratories 3 Nara Institute of Science and Technology {saito.itsumi, suzuki.jun, nishida.kyosuke}@lab.ntt.co.jp, {masumura.ryo, kobashikawa.satoshi, tomita.junji}@lab.ntt.co.jp matsu@is.naist.jp, k.sadamitsu.ic@future.co.jp Abstract using statistical machine translation (SMT) techniques (Junczys-Dowmunt and Grundkiewicz, 2016; Yuan and Briscoe, 2016), in particular, utilizing the Moses toolkit (Koehn et al., 2007). In recent years, encoder-decoder models with an attention mechanism (Bahdanau et al., 2014) have made great progress regarding many NLP tasks, including machine translation (Luong et al., 2015; Sennrich et al., 2016), text summarization (Rush et al., 2015) and text normalization (Xie et al., 2016; Yuan and Briscoe, 2016; Ikeda et al., 2017). We can also simply apply an encoder-decoder model to text normalization tasks. However, it is well-known that encoderdecoder models often fail to perform better than conventional methods wh"
N16-1135,N09-1003,0,0.11674,"Missing"
N16-1135,D15-1159,0,0.0232145,"requirements imposed by many different uses and applications. 1 Introduction Word embedding vectors obtained from ‘neural word embedding methods’, such as SkipGram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tagging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), machine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answering (Wang and Nyberg, 2015). The main purpose of this paper is to further enhance the ‘usability’ of obtained embedding vectors in actual use. To briefly explain our motivation, we first introduce the following concept: This paper focuses on the fact that the appropriate dimension of embedding vectors strongly depends on applications and uses, and is basically determined based on the performance and memory space (or cal"
N16-1135,D14-1082,0,0.0225864,"tor can manage a wide range of dimensional requirements imposed by many different uses and applications. 1 Introduction Word embedding vectors obtained from ‘neural word embedding methods’, such as SkipGram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tagging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), machine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answering (Wang and Nyberg, 2015). The main purpose of this paper is to further enhance the ‘usability’ of obtained embedding vectors in actual use. To briefly explain our motivation, we first introduce the following concept: This paper focuses on the fact that the appropriate dimension of embedding vectors strongly depends on applications and uses, and is basically determined based o"
N16-1135,P15-1033,0,0.0173416,"nge of dimensional requirements imposed by many different uses and applications. 1 Introduction Word embedding vectors obtained from ‘neural word embedding methods’, such as SkipGram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tagging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), machine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answering (Wang and Nyberg, 2015). The main purpose of this paper is to further enhance the ‘usability’ of obtained embedding vectors in actual use. To briefly explain our motivation, we first introduce the following concept: This paper focuses on the fact that the appropriate dimension of embedding vectors strongly depends on applications and uses, and is basically determined based on the performance a"
N16-1135,P12-1092,0,0.0423807,"32.1 ∗ 34.3 ∗ 35.4 SGNS (trunc) 24.2 26.2 29.9 32.1 32.3 GloVe (trunc) 22.8 24.3 26.7 26.7 28.2 SGNS (retrain) 24.8 29.7 ∗ 33.0 32.7 33.9 GloVe (retrain) 26.3 27.3 27.1 28.1 28.1 1000 ∗ 65.6 63.2 ∗ 65.6 – – 1000 62.9 ∗ 64.5 59.9 – – 1000 36.6 36.1 27.7 – – ∗ Table 1: Results of right-truncated embedding vectors (trunc), and standard embedding vectors (retrain). ‘∗ ’ represents the best results in the corresponding column. nine datasets for Similarity (Rubenstein and Goodenough, 1965; Miller and Charles, 1991; Agirre et al., 2009; Agirre et al., 2009; Bruni et al., 2014; Radinsky et al., 2011; Huang et al., 2012; Luong et al., 2013; Hill et al., 2014), three for Analogy (Mikolov et al., 2013a; Mikolov et al., 2013c) , and one for SentComp (Mikolov et al., 2013a). Table 1 shows all the results of our experiments8 . The rows labeled ‘(trunc)’ show the performance of D-right-truncated embedding vectors, whose original vector of dimension is D = 1000. Thus, they were obtained from a single set of embedding vectors with D = 1000 for each corresponding method. Next, the rows labeled ‘(retrain)’ show the performance provided by SGNS or GloVe that were independently constructed with using a standard setting"
N16-1135,D15-1089,0,0.0249387,"ram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tagging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), machine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answering (Wang and Nyberg, 2015). The main purpose of this paper is to further enhance the ‘usability’ of obtained embedding vectors in actual use. To briefly explain our motivation, we first introduce the following concept: This paper focuses on the fact that the appropriate dimension of embedding vectors strongly depends on applications and uses, and is basically determined based on the performance and memory space (or calculation speed) trade-off. Indeed, the actual dimensions of the previous studies listed above are diverse; often around 50, and at most 1000. It is worth no"
N16-1135,Q15-1016,0,0.0385286,"rocedure of updateParams1D in Fig. 1 using the ACO-based algorithm. this norm constraint: 1 |U |ei  |V| 2 ∀i ||¯ q||p ||¯r||p |V ¯ q||p |U| 1 oj  |V| 2 o˜j = ||¯ q||p ||¯r||p ∀j, ||¯r||p |U| e˜i = (12) which also maintain e˜i o˜j = ei oj , and the objective value. Thus, we can safely apply them at any time during the optimization. Finally, Fig. 4 shows the optimization procedure when using the ACO framework. 4 Experiments As in previously reported neural word embedding papers, our training data was taken from a Wikipedia dump (Aug. 2014). We used hyperwords tool5 for our data preparation (Levy et al., 2015). We compared our method, ITACO, with the widely used conventional methods, SGNS and GloVe. We used the word2vec implementation6 to obtain word embeddings of SGNS, and glove implementation7 for GloVe. Many tunable hyper-parameters were selected based on the recommended default values of each implementation, or suggestion explained in (Levy et al., 2015). For ITACO, we selected the Glove objective to solve Eqs. 6 and 7 since it requires a lower calculation cost than the SGNS objective. We prepared three types of linguistic benchmark tasks, namely word similarity estimation (Similarity), word an"
N16-1135,D15-1176,0,0.0200786,"dding. Therefore, a single embedding vector can manage a wide range of dimensional requirements imposed by many different uses and applications. 1 Introduction Word embedding vectors obtained from ‘neural word embedding methods’, such as SkipGram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tagging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), machine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answering (Wang and Nyberg, 2015). The main purpose of this paper is to further enhance the ‘usability’ of obtained embedding vectors in actual use. To briefly explain our motivation, we first introduce the following concept: This paper focuses on the fact that the appropriate dimension of embedding vectors strongly depends on applications a"
N16-1135,W13-3512,0,0.0331257,"GNS (trunc) 24.2 26.2 29.9 32.1 32.3 GloVe (trunc) 22.8 24.3 26.7 26.7 28.2 SGNS (retrain) 24.8 29.7 ∗ 33.0 32.7 33.9 GloVe (retrain) 26.3 27.3 27.1 28.1 28.1 1000 ∗ 65.6 63.2 ∗ 65.6 – – 1000 62.9 ∗ 64.5 59.9 – – 1000 36.6 36.1 27.7 – – ∗ Table 1: Results of right-truncated embedding vectors (trunc), and standard embedding vectors (retrain). ‘∗ ’ represents the best results in the corresponding column. nine datasets for Similarity (Rubenstein and Goodenough, 1965; Miller and Charles, 1991; Agirre et al., 2009; Agirre et al., 2009; Bruni et al., 2014; Radinsky et al., 2011; Huang et al., 2012; Luong et al., 2013; Hill et al., 2014), three for Analogy (Mikolov et al., 2013a; Mikolov et al., 2013c) , and one for SentComp (Mikolov et al., 2013a). Table 1 shows all the results of our experiments8 . The rows labeled ‘(trunc)’ show the performance of D-right-truncated embedding vectors, whose original vector of dimension is D = 1000. Thus, they were obtained from a single set of embedding vectors with D = 1000 for each corresponding method. Next, the rows labeled ‘(retrain)’ show the performance provided by SGNS or GloVe that were independently constructed with using a standard setting and corresponding D."
N16-1135,N13-1090,0,0.43353,"Global Vectors. Since our method iteratively generates embedding vectors one dimension at a time, obtained vectors equip a unique property. Namely, any right-truncated vector matches the solution of the corresponding lower-dimensional embedding. Therefore, a single embedding vector can manage a wide range of dimensional requirements imposed by many different uses and applications. 1 Introduction Word embedding vectors obtained from ‘neural word embedding methods’, such as SkipGram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tagging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), machine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answering (Wang and Nyberg, 2015). The main purpose of this paper is to further enhance the ‘usability’ o"
N16-1135,D14-1162,0,0.0854519,"in this form by a simple reformulation from the original objective of SGNS (Mikolov et al., 2013b). 1146 Input: X : training data, D: maximum number of dimensions (iterations) 1: E(0) ← ∅, O(0) ← ∅, and B(0) ← 0, d ← 0 2: repeat 3: d ← d + 1 4: (¯ qd , ¯rd ) ← updateParams1D(X , B(d−1) ) // Eq. 5 ¯d) 5: E(d) ← appendVec(E(d−1) , q 6: O(d) ← appendVec(O(d−1) , ¯rd ) ¯ d , ¯rd ) // Eq. 4 7: B(d) ← updateBias(B(d−1) , q 8: until d = D Output: (E(D) , O(D) ) Figure 1: An algorithm for solving an iterative additional coordinate optimization formulation for obtaining embedding vectors. lowing form (Pennington et al., 2014): Ψ= 1X βi,j (xi,j − mi,j )2 , 2 (3) (i,j) where mi,j and βi,j represent certain co-occurrence and weighting factors of the i-th input and the jth output words, respectively. For example, βi,j = min(1, (ci,j /xmax )γ ), and mi,j = log(ci,j ) are used in (Pennington et al., 2014), where xmax and γ are tunable hyper-parameters. 3 Incremental Construction of Embedding This section explains our proposed method. The basic idea is very simple and clear: we convert the minimization problem shown in Eq. 1 to a series of minimization problems, each of whose individual problem determines one additional"
N16-1135,P15-2031,1,0.753243,"D = 1000. Thus, they were obtained from a single set of embedding vectors with D = 1000 for each corresponding method. Next, the rows labeled ‘(retrain)’ show the performance provided by SGNS or GloVe that were independently constructed with using a standard setting and corresponding D. Note that the results of ‘ITACO (retrain)’ are identical to those of ‘ITACO (trunc)’. Moreover, ‘GloVe (trunc)’ and ‘GloVe (retrain)’ in D = 1000 are equivalent, as are ‘SGNS (trunc)’ and ‘SGNS (retrain)’. Thus, these results 8 Results for SGNS and GloVe are the average performance of ten runs as suggested in (Suzuki and Nagata, 2015) 1149 were omitted from the table. First, comparing ‘(retrain)’ and ‘(trunc)’ in SGNS and GloVe, our experimental results first explicitly revealed that SGNS and GloVe with the simple truncation approach ‘(trunc)’ cannot provide effective lower-dimensional embedding vectors. This observation strongly supports the significance of existence of our proposed method, ITACO. Second, in most cases ITACO successfully provided almost the same performance level as the best SGNS and GloVe (retrain) results. We emphasize that ITACO constructed embedding vectors ‘just once’, while SGNS and GloVe required u"
N16-1135,D14-1101,0,0.0250268,"mensional embedding. Therefore, a single embedding vector can manage a wide range of dimensional requirements imposed by many different uses and applications. 1 Introduction Word embedding vectors obtained from ‘neural word embedding methods’, such as SkipGram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tagging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), machine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answering (Wang and Nyberg, 2015). The main purpose of this paper is to further enhance the ‘usability’ of obtained embedding vectors in actual use. To briefly explain our motivation, we first introduce the following concept: This paper focuses on the fact that the appropriate dimension of embedding vectors strongly depen"
N16-1135,P15-2116,0,0.0396451,"Missing"
N16-1135,D15-1295,0,0.0263285,"troduction Word embedding vectors obtained from ‘neural word embedding methods’, such as SkipGram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tagging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), machine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answering (Wang and Nyberg, 2015). The main purpose of this paper is to further enhance the ‘usability’ of obtained embedding vectors in actual use. To briefly explain our motivation, we first introduce the following concept: This paper focuses on the fact that the appropriate dimension of embedding vectors strongly depends on applications and uses, and is basically determined based on the performance and memory space (or calculation speed) trade-off. Indeed, the actual dimensions of the previou"
N16-1135,P15-1109,0,0.0154058,"applications. 1 Introduction Word embedding vectors obtained from ‘neural word embedding methods’, such as SkipGram, continuous bag-of-words (CBoW) and the family of vector log-bilinear (vLBL) models (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013b) have now become an important fundamental resource for tackling many natural language processing (NLP) tasks. These NLP tasks include part-of-speech tagging (Tsuboi, 2014; Ling et al., 2015), dependency parsing (Chen and Manning, 2014; Dyer et al., 2015; Alberti et al., 2015), semantic role labeling (Zhou and Xu, 2015; Woodsend and Lapata, 2015), machine translation (Sutskever et al., 2014), sentiment analysis (Kim et al., 2015), and question answering (Wang and Nyberg, 2015). The main purpose of this paper is to further enhance the ‘usability’ of obtained embedding vectors in actual use. To briefly explain our motivation, we first introduce the following concept: This paper focuses on the fact that the appropriate dimension of embedding vectors strongly depends on applications and uses, and is basically determined based on the performance and memory space (or calculation speed) trade-off. Indeed, the actu"
N19-1353,N09-1003,0,0.186925,"Missing"
N19-1353,Q17-1010,0,0.755952,"ty degradation across several linguistic benchmark datasets, and can simultaneously predict effective embeddings of OOV words. We also demonstrate the effectiveness of our reconstruction method when we apply them to downstream tasks1 . 1 Introduction Pre-trained word embeddings (or embedding vectors), especially those trained on a vast amount of text data, such as the Common Crawl (CC) corpus2 , are now considered as highly beneficial, fundamental language resources. Typical examples of large, well-trained word embeddings are those trained on the CC corpus with 600 billion tokens by fastText (Bojanowski et al., 2017) and with 840 billion tokens by GloVe (Pennington et al., 2014), which we refer to as fastText.600B3 and GloVe.840B4 , respectively. In fact, we often 1 Our code and reconstructed subword-based word embeddings trained from GloVe.840B and fastText.600B are available: https://github.com/losyer/ compact_reconstruction 2 http://commoncrawl.org 3 https://fasttext.cc/docs/en/ english-vectors.html 4 https://nlp.stanford.edu/projects/ glove/ leverage such word embeddings to further improve the task performance of many natural language processing (NLP) tasks, such as constituency parsing (Suzuki et al."
N19-1353,D15-1075,0,0.0128679,"r reconstruction methods outperformed BoS, which was the previous stateof-the-art method, with substantial improvements by 2-6 points. Moreover, KVQ-FH achieved the best performance in this comparison. 6.3 Evaluation on downstream tasks To investigate the effectiveness of our reconstucted embeddings in downstream tasks, we evaluated them in the named entity recognition (NER) and the textual entailment (TE) tasks. 6.3.1 Settings Evaluation data: We used the CoNLL 2003 dataset (Tjong Kim Sang and De Meulder, 2003) for an NER experiment and the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) for a TE experiment. Other settings: We used fastText.600B and GloVe.840B as the target word embeddings for the reconstruction. For our reconstruction embeddings, we calculated the embeddings of all the words in the datasets, thus there exist no OOV words when using our methods. We used AllenNLP10 to train base NER and TE models. We basically used the provided hyperparameter values in their repository for both training and testing. Additionally, we added one hyperparameter K to re-scale embeddings (i.e., multiply all the elements in the embeddings by K) 10 https://allennlp.org/ 7 Conclusion W"
N19-1353,P18-1068,0,0.0145534,"ively. In fact, we often 1 Our code and reconstructed subword-based word embeddings trained from GloVe.840B and fastText.600B are available: https://github.com/losyer/ compact_reconstruction 2 http://commoncrawl.org 3 https://fasttext.cc/docs/en/ english-vectors.html 4 https://nlp.stanford.edu/projects/ glove/ leverage such word embeddings to further improve the task performance of many natural language processing (NLP) tasks, such as constituency parsing (Suzuki et al., 2018; G´omez-Rodr´ıguez and Vilares, 2018), discourse parsing (Yu et al., 2018), semantic parsing (Groschwitz et al., 2018; Dong and Lapata, 2018), and semantic role labeling (Strubell et al., 2018). Despite their significant impact on the NLP community, well-trained word embeddings still have several disadvantages. In this paper, we focus on two issues surrounding well-trained word embeddings: i) the massive memory requirement and ii) the inapplicability of out-of-vocabulary (OOV) words. It is crucial to address such issues, especially when applying them to real-world open systems. The total number of embeddings (i.e., the total memory requirement of such word embeddings) often becomes unacceptably large, especially in limited-memory e"
N19-1353,D18-1162,0,0.0246395,"Missing"
N19-1353,P18-1170,0,0.049615,"Missing"
N19-1353,D17-1030,0,0.0172065,"ord embedding research, which several researches have recently attempted to solve. For example, methods that leverage subword information, such as character N -grams (including character unigrams) (Bojanowski et al., 2017; Pinter et al., 2017; Zhao et al., 2018) and morphological features (Luong et al., 2013), have recently been discussed as means of constructing word embeddings that consider the applicability of OOV words. Moreover, Pilehvar and Collier (2017) have proposed a method called SemLand, which induces OOV word embeddings by leveraging external resources. Bahdanau et al. (2017) and Herbelot and Baroni (2017) have also proposed methods that estimate OOV word embeddings using an additional LSTM and leveraging a small additional dataset, respectively. Among them, the study most closely related to ours is that of Zhao et al. (2018). Their basic idea is to reconstruct each pre-trained word embedding using a bag-of-character N -grams. We refer to their method as ‘BoS’. The motivation for reconstructing pre-trained word embeddings and utilizing character N -grams in our approach is substantially the same, however, an essential difference from BoS is that we additionally consider jointly reducing the tot"
N19-1353,J15-4004,0,0.0664027,"Missing"
N19-1353,P12-1092,0,0.0657688,"o a subword index. Then, we introduce a key-valuequery (KVQ) self-attention operation inspired by 3502 data number of OOV data abbre. size fastText.600B GloVe.840B Word similarity estimation (WordSim) MEN 3,000 0 0 M&C 30 0 0 287 0 0 MTurk RW 2,034 37 36 65 0 0 R&G SCWS 2,003 2 2 SLex 998 0 0 WSR 252 0 0 WSS 203 0 0 Word analogy estimation (Analogy) GL 19,544 0 0 MSYN 8,000 1000 1000 Table 2: Evaluation datasets used in our experiments. MEM (Bruni et al., 2014), M&C (Miller and Charles, 1991), MTurk (Radinsky et al., 2011), RW (Luong et al., 2013), R&G (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012), SLex (Hill et al., 2014), WSR and WSS (Agirre et al., 2009), GL (Mikolov et al., 2013a), and MSYN (Mikolov et al., 2013b). method hyper-parameters fastText.600B SUM-F F = 0.5M SUM-H H = 0.5M KVQ-H H = 0.5M SUM-FH F = 1.0M H = 0.5M KVQ-FH F = 1.0M H = 0.5M SUM-F F = 0.2M SUM-H H = 0.2M KVQ-H H = 0.2M SUM-FH F = 1.0M H = 0.2M KVQ-FH F = 1.0M H = 0.2M |W| 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M |S |size (GB) – 2.23GB 0.5M 0.59GB 21.8M 0.59GB 21.8M 0.59GB 1.0M 0.59GB 1.0M 0.59GB 0.2M 0.23GB 21.8M 0.23GB 21.8M 0.23GB 1.0M 0.23GB 1.0M 0.23GB Table 3: Statistics for our methods. benchmark datasets, i.e.,"
N19-1353,W13-3512,0,0.550308,"structed embeddings for representing the embeddings of OOV words. Lastly, we confirm the performance of our reconstructed embeddings on several downstream tasks from the named entity recognition task and the textual entailment task. 2 Related Work The OOV word issue is one of the widely discussed topics in word embedding research, which several researches have recently attempted to solve. For example, methods that leverage subword information, such as character N -grams (including character unigrams) (Bojanowski et al., 2017; Pinter et al., 2017; Zhao et al., 2018) and morphological features (Luong et al., 2013), have recently been discussed as means of constructing word embeddings that consider the applicability of OOV words. Moreover, Pilehvar and Collier (2017) have proposed a method called SemLand, which induces OOV word embeddings by leveraging external resources. Bahdanau et al. (2017) and Herbelot and Baroni (2017) have also proposed methods that estimate OOV word embeddings using an additional LSTM and leveraging a small additional dataset, respectively. Among them, the study most closely related to ours is that of Zhao et al. (2018). Their basic idea is to reconstruct each pre-trained word e"
N19-1353,N13-1090,0,0.132949,"Missing"
N19-1353,D14-1162,0,0.10704,"d can simultaneously predict effective embeddings of OOV words. We also demonstrate the effectiveness of our reconstruction method when we apply them to downstream tasks1 . 1 Introduction Pre-trained word embeddings (or embedding vectors), especially those trained on a vast amount of text data, such as the Common Crawl (CC) corpus2 , are now considered as highly beneficial, fundamental language resources. Typical examples of large, well-trained word embeddings are those trained on the CC corpus with 600 billion tokens by fastText (Bojanowski et al., 2017) and with 840 billion tokens by GloVe (Pennington et al., 2014), which we refer to as fastText.600B3 and GloVe.840B4 , respectively. In fact, we often 1 Our code and reconstructed subword-based word embeddings trained from GloVe.840B and fastText.600B are available: https://github.com/losyer/ compact_reconstruction 2 http://commoncrawl.org 3 https://fasttext.cc/docs/en/ english-vectors.html 4 https://nlp.stanford.edu/projects/ glove/ leverage such word embeddings to further improve the task performance of many natural language processing (NLP) tasks, such as constituency parsing (Suzuki et al., 2018; G´omez-Rodr´ıguez and Vilares, 2018), discourse parsing"
N19-1353,N18-1202,0,0.034374,"ing techniques. Our method can also be interpreted as a kind of parameter reduction method based on the subword features. However, their method only considers the model shrinkage, and does not utilize any subword information nor consider the OOV issue. To summarize, none of the previous studies have attempted to simultaneously achieve a smaller number of embedding vectors and higher applicability of OOV words. Thus, in this paper, we report the first attempt to investigate how we 3499 can simultaneously achieve them. Additionally, deep contextualized pre-trained language models, such as ELMo (Peters et al., 2018), have recently been proposed as alternatives to the pre-trained word embeddings to further improve task performances. However, ELMo still takes advantage of Glove.840B to achieve its state-of-the-art performance. This fact implies that we can still combine the pre-trained word embeddings with strong pre-trained language models; thus, the importance of word embeddings in the literature remains unchanged even though stronger pre-trained models have been established. RD×|Iw |. Then, we assume that the following relation always holds between ew and E: ew = E[ze ] where ze = ζ(w). (2) Therefore, t"
N19-1353,E17-2062,0,0.0205981,"nstream tasks from the named entity recognition task and the textual entailment task. 2 Related Work The OOV word issue is one of the widely discussed topics in word embedding research, which several researches have recently attempted to solve. For example, methods that leverage subword information, such as character N -grams (including character unigrams) (Bojanowski et al., 2017; Pinter et al., 2017; Zhao et al., 2018) and morphological features (Luong et al., 2013), have recently been discussed as means of constructing word embeddings that consider the applicability of OOV words. Moreover, Pilehvar and Collier (2017) have proposed a method called SemLand, which induces OOV word embeddings by leveraging external resources. Bahdanau et al. (2017) and Herbelot and Baroni (2017) have also proposed methods that estimate OOV word embeddings using an additional LSTM and leveraging a small additional dataset, respectively. Among them, the study most closely related to ours is that of Zhao et al. (2018). Their basic idea is to reconstruct each pre-trained word embedding using a bag-of-character N -grams. We refer to their method as ‘BoS’. The motivation for reconstructing pre-trained word embeddings and utilizing"
N19-1353,D17-1010,0,0.158576,"nd analogy tasks. We also demonstrate the effectiveness of our reconstructed embeddings for representing the embeddings of OOV words. Lastly, we confirm the performance of our reconstructed embeddings on several downstream tasks from the named entity recognition task and the textual entailment task. 2 Related Work The OOV word issue is one of the widely discussed topics in word embedding research, which several researches have recently attempted to solve. For example, methods that leverage subword information, such as character N -grams (including character unigrams) (Bojanowski et al., 2017; Pinter et al., 2017; Zhao et al., 2018) and morphological features (Luong et al., 2013), have recently been discussed as means of constructing word embeddings that consider the applicability of OOV words. Moreover, Pilehvar and Collier (2017) have proposed a method called SemLand, which induces OOV word embeddings by leveraging external resources. Bahdanau et al. (2017) and Herbelot and Baroni (2017) have also proposed methods that estimate OOV word embeddings using an additional LSTM and leveraging a small additional dataset, respectively. Among them, the study most closely related to ours is that of Zhao et al"
N19-1353,D18-1548,0,0.0217056,"Missing"
N19-1353,P18-2097,1,0.826503,"et al., 2017) and with 840 billion tokens by GloVe (Pennington et al., 2014), which we refer to as fastText.600B3 and GloVe.840B4 , respectively. In fact, we often 1 Our code and reconstructed subword-based word embeddings trained from GloVe.840B and fastText.600B are available: https://github.com/losyer/ compact_reconstruction 2 http://commoncrawl.org 3 https://fasttext.cc/docs/en/ english-vectors.html 4 https://nlp.stanford.edu/projects/ glove/ leverage such word embeddings to further improve the task performance of many natural language processing (NLP) tasks, such as constituency parsing (Suzuki et al., 2018; G´omez-Rodr´ıguez and Vilares, 2018), discourse parsing (Yu et al., 2018), semantic parsing (Groschwitz et al., 2018; Dong and Lapata, 2018), and semantic role labeling (Strubell et al., 2018). Despite their significant impact on the NLP community, well-trained word embeddings still have several disadvantages. In this paper, we focus on two issues surrounding well-trained word embeddings: i) the massive memory requirement and ii) the inapplicability of out-of-vocabulary (OOV) words. It is crucial to address such issues, especially when applying them to real-world open systems. The total numb"
N19-1353,C18-1047,0,0.0177485,"which we refer to as fastText.600B3 and GloVe.840B4 , respectively. In fact, we often 1 Our code and reconstructed subword-based word embeddings trained from GloVe.840B and fastText.600B are available: https://github.com/losyer/ compact_reconstruction 2 http://commoncrawl.org 3 https://fasttext.cc/docs/en/ english-vectors.html 4 https://nlp.stanford.edu/projects/ glove/ leverage such word embeddings to further improve the task performance of many natural language processing (NLP) tasks, such as constituency parsing (Suzuki et al., 2018; G´omez-Rodr´ıguez and Vilares, 2018), discourse parsing (Yu et al., 2018), semantic parsing (Groschwitz et al., 2018; Dong and Lapata, 2018), and semantic role labeling (Strubell et al., 2018). Despite their significant impact on the NLP community, well-trained word embeddings still have several disadvantages. In this paper, we focus on two issues surrounding well-trained word embeddings: i) the massive memory requirement and ii) the inapplicability of out-of-vocabulary (OOV) words. It is crucial to address such issues, especially when applying them to real-world open systems. The total number of embeddings (i.e., the total memory requirement of such word embedding"
N19-1353,D18-1059,0,0.361126,"also demonstrate the effectiveness of our reconstructed embeddings for representing the embeddings of OOV words. Lastly, we confirm the performance of our reconstructed embeddings on several downstream tasks from the named entity recognition task and the textual entailment task. 2 Related Work The OOV word issue is one of the widely discussed topics in word embedding research, which several researches have recently attempted to solve. For example, methods that leverage subword information, such as character N -grams (including character unigrams) (Bojanowski et al., 2017; Pinter et al., 2017; Zhao et al., 2018) and morphological features (Luong et al., 2013), have recently been discussed as means of constructing word embeddings that consider the applicability of OOV words. Moreover, Pilehvar and Collier (2017) have proposed a method called SemLand, which induces OOV word embeddings by leveraging external resources. Bahdanau et al. (2017) and Herbelot and Baroni (2017) have also proposed methods that estimate OOV word embeddings using an additional LSTM and leveraging a small additional dataset, respectively. Among them, the study most closely related to ours is that of Zhao et al. (2018). Their basi"
P03-1005,C02-1054,0,\N,Missing
P03-1005,W02-2016,0,\N,Missing
P04-1016,W02-2016,0,0.0299937,"dded into an original kernel calculation process by using sub-structure mining algorithms. Experiments are undertaken on real NLP tasks to confirm the problem with a conventional method and to compare its performance with that of the proposed method. 1 Introduction Over the past few years, many machine learning methods have been successfully applied to tasks in natural language processing (NLP). Especially, state-of-the-art performance can be achieved with kernel methods, such as Support Vector Machine (Cortes and Vapnik, 1995). Examples include text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2002) and parsing (Collins and Duffy, 2001). Another feature of this kernel methodology is that it not only provides high accuracy but also allows us to design a kernel function suited to modeling the task at hand. Since natural language data take the form of sequences of words, and are generally analyzed using discrete structures, such as trees (parsed trees) and graphs (relational graphs), discrete kernels, such as sequence kernels (Lodhi et al., 2002), tree kernels (Collins and Duffy, 2001), and graph kernels (Suzuki et al., 2003a), have been shown to offer excellent results. These discrete kern"
P04-1016,P03-1004,0,0.0485189,"can be embedded in an original kernel calculation process, which allows us to use the same calculation procedure as the conventional methods. The only difference between the original sequence kernels and the proposed method is that the latter calculates a statistical metric χ2 (u) by using a sub-structure mining algorithm in the kernel calculation. Third, although the kernel calculation, which unifies our proposed method, requires a longer training time because of the feature selection, the selected sub-sequences have a TRIE data structure. This means a fast calculation technique proposed in (Kudo and Matsumoto, 2003) can be simply applied to our method, which yields classification very quickly. In the classification part, the features (subsequences) selected in the learning part must be known. Therefore, we store the TRIE of selected sub-sequences and use them during classification. 5 Proposed Method Applied to Other Convolution Kernels We have insufficient space to discuss this subject in detail in relation to other convolution kernels. However, our proposals can be easily applied to tree kernels (Collins and Duffy, 2001) by using string encoding for trees. We enumerate nodes (labels) of tree in postorde"
P04-1016,C02-1150,0,0.0611917,"Missing"
P04-1016,P03-1005,1,0.935231,"ples include text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2002) and parsing (Collins and Duffy, 2001). Another feature of this kernel methodology is that it not only provides high accuracy but also allows us to design a kernel function suited to modeling the task at hand. Since natural language data take the form of sequences of words, and are generally analyzed using discrete structures, such as trees (parsed trees) and graphs (relational graphs), discrete kernels, such as sequence kernels (Lodhi et al., 2002), tree kernels (Collins and Duffy, 2001), and graph kernels (Suzuki et al., 2003a), have been shown to offer excellent results. These discrete kernels are related to convolution kernels (Haussler, 1999), which provides the concept of kernels over discrete structures. Convolution kernels allow us to treat structural features without explicitly representing the feature vectors from the input object. That is, convolution kernels are well suited to NLP tasks in terms of both accuracy and concept. Unfortunately, experiments have shown that in some cases there is a critical issue with convolution kernels, especially in NLP tasks (Collins and Duffy, 2001; Cancedda et al., 2003;"
P05-1024,A00-2018,0,0.0491847,"was used as test data. As a baseline model, we used a shallow parser based on Conditional Random Fields (CRFs), very similar to that described in (Sha and Pereira, 2003). CRFs have shown remarkable results in a number of tagging and chunking tasks in NLP. n-best outputs were obtained by a combination of forward 194 Table 1: Results for section 23 of the WSJ Treebank LR/LP = labeled recall/precision. CBs is the average number of cross brackets per sentence. 0 CBs, and 2CBs are the percentage of sentences with 0 or ≤ 2 crossing brackets, respectively. COL99 = Model 2 of (Collins, 1999). CH00 = (Charniak, 2000), CO00=(Collins, 2000). CO02=(Collins and Duffy, 2002). Viterbi search and backward A* search. Note that this search algorithm yields optimal n-best results in terms of the CRFs score. Each sentence has at most 20 distinct parses. The log probability from the CRFs shallow parser was incorporated into the reranking. Following (Collins, 2000), the training set was split into 5 portions, and the CRFs shallow parser was trained on 4/5 of the data, then used to decode the remaining 1/5. The outputs of the base parser, which consist of base phrases, were converted into right-branching trees by assum"
P05-1024,P02-1034,0,0.816258,"and how it selects a small and relevant feature set efficiently. Two experiments on parse reranking show that our method achieves comparable or even better performance than kernel methods and also improves the testing efficiency. 1 Introduction formance. However, they are highly task dependent and require careful design to create the optimal feature set for each task. Kernel methods offer an elegant solution to these problems. They can work on a potentially huge or even infinite number of features without a loss of generalization. The best known kernel for modeling a tree is the tree kernel (Collins and Duffy, 2002), which argues that a feature vector is implicitly composed of the counts of subtrees. Although kernel methods are general and can cover almost all useful features, the set of subtrees that is used is extremely redundant. The main question addressed in this paper concerns whether it is possible to achieve a comparable or even better accuracy using just a small and non-redundant set of subtrees. In this paper, we present a new application of boosting for parse reranking. While tree kernel implicitly uses the all-subtrees representation, our boosting algorithm uses it explicitly. Although this s"
P05-1024,N01-1025,1,0.654592,"f base phrases, were converted into right-branching trees by assuming that two adjacent base phrases are in a parent-child relationship. Figure 4 shows an example of the tree for shallow parsing task. We also put two virtual nodes, left/right boundaries, to capture local transitions. The size parameter s and frequency parameter f were experimentally set at 6 and 5, respectively. Table 2 lists results on test data for the baseline CRFs parser, for several previous studies, and for our best model. Our model achieves a 94.12 Fmeasure, and outperforms the baseline CRFs parser and the SVMs parser (Kudo and Matsumoto, 2001). (Zhang et al., 2002) reported a higher F-measure with a generalized winnow using additional linguistic features. The accuracy of our model is very similar to that of (Zhang et al., 2002) without using such additional features. Table 3 shows the results for our best model per chunk type. TOP ADJP ADVP CONJP INTJ LST NP PP PRT SBAR VP Overall NP PRP (L) I VP (R) VBD (L) saw (R) NP DT (L) NN EOS a girl (R) Figure 4: Tree representation for shallow parsing Represented in a right-branching tree with two virtual nodes MODEL CRFs (baseline) 8 SVMs-voting (Kudo and Matsumoto, 2001) RW + linguistic f"
P05-1024,W04-3239,1,0.841064,"n each boosting iteration, we have to solve the following optimization problem: kˆ = argmax gain(tk ), k=1,...,m q ¯q ¯ ¯ ¯ where gain(tk ) = ¯ Wk+ − Wk− ¯. 192 It is non-trivial to find the optimal tree tkˆ that maximizes gain(tk ), since the number of subtrees is exponential to its size. In fact, the problem is known to be NP-hard (Yang, 2004). However, in real applications, the problem is manageable, since the maximum number of subtrees is usually bounded by a constant. To solve the problem efficiently, we now adopt a variant of the branch-and-bound algorithm, similar to that described in (Kudo and Matsumoto, 2004) 4.1 Efficient Enumeration of Trees Abe and Zaki independently proposed an efficient method, rightmost-extension, for enumerating all subtrees from a given tree (Abe et al., 2002; Zaki, 2002). First, the algorithm starts with a set of trees consisting of single nodes, and then expands a given tree of size (n−1) by attaching a new node to it to obtain trees of size n. However, it would be inefficient to expand nodes at arbitrary positions of the tree, as duplicated enumeration is inevitable. The algorithm, rightmost extension, avoids such duplicated enumerations by restricting the position of a"
P05-1024,N03-1028,0,0.046633,"Missing"
P05-1024,W00-0726,0,0.0242374,"Missing"
P05-1024,J03-4003,0,\N,Missing
P05-1024,P01-1010,0,\N,Missing
P05-1024,P02-1062,0,\N,Missing
P06-1028,N01-1025,0,0.386988,"γ2 ZD 1 ZD + , (γ 2 +1)·ZN 2 ZD , if δ(s∗j ) = 1 otherwise where ZN and ZD represent the numerator and denominator of Eq. 16, respectively. In the optimization process of the segmentation F-score objective function, we can efficiently calculate Eq. 15 by using the forward and backward Viterbi algorithm, which is almost the same as calculating Eq. 3 with a variant of the forwardbackward algorithm (Sha and Pereira, 2003). The same numerical optimization methods described in Sec. 3.3 can be employed for this optimization. 5 5.2 Features As regards the basic feature set for Chunking, we followed (Kudo and Matsumoto, 2001), which is the same feature set that provided the best result in CoNLL-2000. We expanded the basic features by using bigram combinations of the same types of features, such as words and part-of-speech tags, within window size 5. In contrast to the above, we used the original feature set for NER. We used features derived only from the data provided by CoNLL-2003 with the addition of character-level regular expressions of uppercases [A-Z], lowercases [a-z], digits [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] or others, and prefixes and suffixes of one to four letters. We also expanded the above basic features"
P06-1028,W03-0430,0,0.0241101,"et al., 2001) for representing a conditional model p(y|x), where both a set of inputs, x, and a set of outputs, y, display non-trivial interdependency. CRFs are basically defined as a discriminative model of Markov random fields conditioned on inputs (observations) x. Unlike generative models, CRFs model only the output y’s distribution over x. This allows CRFs to use flexible features such as complicated functions of multiple observations. The modeling power of CRFs has been of great benefit in several applications, such as shallow parsing (Sha and Pereira, 2003) and information extraction (McCallum and Li, 2003). Since the introduction of CRFs, intensive research has been undertaken to boost their effectiveness. The first approach to estimating CRF parameters is the maximum likelihood (ML) criterion over conditional probability p(y|x) itself (Lafferty et al., 2001). The ML criterion, however, 217 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 217–224, c Sydney, July 2006. 2006 Association for Computational Linguistics − log p(y ∗k |xk ; λ). We minimize the following loss function for the ML criterion training of CRFs: P known in"
P06-1028,W00-0726,0,0.214004,"know, the maximum output can be efficiently calculated with the Viterbi algorithm, which is the same as calculating Eq. 1. Therefore, we can find the maximum incorrect output by using the A* algorithm (Hart et al., 1968), if the maximum output is the correct output, and by using the Viterbi algorithm otherwise. It may be feared that since the objective function is not differentiable everywhere for ψ = ∞, problems for optimization would occur. However, it has been shown (Le Roux and McDer4.2 Segmentation F-score Loss for SSTs The standard evaluation measure of SSTs is the segmentation F-score (Sang and Buchholz, 2000): Fγ = 220 γ2 (γ 2 + 1) · T P · F N + F P + (γ 2 + 1) · T P (13) Named Entity Recognition Text Chunking Seg.: NP VP NP VP PP Seg.: NP x: He reckons the current account deficit will narrow to only # 1.8 billion . y: B-NP B-VP B-NP I-NP I-NP I-NP B-VP I-VP B-PP B-NP I-NP I-NP I-NP O Dep.: y1 y2 y3 y4 y5 y6 y7 y8 y9 y10 y11 y12 y13 y14 ORG PER LOC x: United Nation official Ekeus Smith heads for Baghdad . y: B-ORG I-ORG O B-PER I-PER O O B-LOC O Dep.: y1 y2 y3 y4 y5 y6 y7 y8 y9 Figure 1: Examples of sequential segmentation tasks (SSTs): text chunking (Chunking) and named entity recognition (NER)."
P06-1028,W03-0419,0,0.0343031,"Missing"
P06-1028,W03-1019,0,0.151784,"ORG I-ORG O B-PER I-PER O O B-LOC O Dep.: y1 y2 y3 y4 y5 y6 y7 y8 y9 Figure 1: Examples of sequential segmentation tasks (SSTs): text chunking (Chunking) and named entity recognition (NER). where T P , F P and F N represent true positive, false positive and false negative counts, respectively. The individual evaluation units used to calculate T P , F N and P N , are not individual outputs yi or output sequences y, but rather segments. We need to define a segment-wise loss, in contrast to the standard CRF loss, which is sometimes referred to as an (entire) sequential loss (Kakade et al., 2002; Altun et al., 2003). First, we consider the point-wise decision w.r.t. Eq. 1, that is, yˆi = arg maxyi ∈Y1 g(y, x, i, λ). The point-wise discriminant function can be written as follows: g(y, x, i, λ) = max y 0 ∈Y|y |[yi ] 0 λ · F (y , x) viously reduces to Eq. 14 if the length of all segments is 1. Then, the segment-wise misclassification measure d(y ∗ , x, sj , λ) can be obtained simply by replacing the discriminant function of the entire sequence g(y, x, λ) with that of segmentwise g(y, x, sj , λ) in Eq. 7. Let s∗k be a segment sequence corresponding to the correct output y ∗k for a given xk , and S(xk ) be al"
P06-1028,N03-1028,0,0.775517,"CRFs) are a recently introduced formalism (Lafferty et al., 2001) for representing a conditional model p(y|x), where both a set of inputs, x, and a set of outputs, y, display non-trivial interdependency. CRFs are basically defined as a discriminative model of Markov random fields conditioned on inputs (observations) x. Unlike generative models, CRFs model only the output y’s distribution over x. This allows CRFs to use flexible features such as complicated functions of multiple observations. The modeling power of CRFs has been of great benefit in several applications, such as shallow parsing (Sha and Pereira, 2003) and information extraction (McCallum and Li, 2003). Since the introduction of CRFs, intensive research has been undertaken to boost their effectiveness. The first approach to estimating CRF parameters is the maximum likelihood (ML) criterion over conditional probability p(y|x) itself (Lafferty et al., 2001). The ML criterion, however, 217 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 217–224, c Sydney, July 2006. 2006 Association for Computational Linguistics − log p(y ∗k |xk ; λ). We minimize the following loss functio"
P06-1028,H05-1087,0,0.0219381,"erage accuracies, or error rates, rather than a given task-specific evaluation measure. For example, sequential segmentation tasks (SSTs), such as text chunking and named entity recognition, are generally evaluated with the segmentation F-score. This inconsistency between the objective function during training and the task evaluation measure might produce a suboptimal result. In fact, to overcome this inconsistency, an SVM-based multivariate optimization method has recently been proposed (Joachims, 2005). Moreover, an F-score optimization method for logistic regression has also been proposed (Jansche, 2005). In the same spirit as the above studies, we first propose a generalization framework for CRF training that allows us to optimize directly not only the error rate, but also any evaluation measure. In other words, our framework can incorporate any evaluation measure of interest into the loss function and then optimize this loss function as the training objective function. Our proposed framework is fundamentally derived from an approach to (smoothed) error rate minimization well This paper proposes a framework for training Conditional Random Fields (CRFs) to optimize multivariate evaluation mea"
P06-1028,P03-1021,0,\N,Missing
P08-1076,P05-1001,0,0.835839,"n traditional and important NLP tasks, namely part-of-speech (POS) tagging, syntactic chunking, and named entity recognition (NER). These are also typical supervised learning applications in NLP, and are referred to as sequential labeling and segmentation problems. In some cases, these tasks have relatively large amounts of labeled training data. In this situation, supervised learning can provide competitive results, and it is difficult to improve them any further by using SSL. In fact, few papers have succeeded in showing significantly better results than state-of-theart supervised learning. Ando and Zhang (2005) reported a substantial performance improvement compared with state-of-the-art supervised learning results for syntactic chunking with the CoNLL’00 shared task data (Tjong Kim Sang and Buchholz, 2000) and NER with the CoNLL’03 shared task data (Tjong Kim Sang and Meulder, 2003). One remaining question is the behavior of SSL when using as much labeled and unlabeled data as possible. This paper investigates this question, namely, the use of a large amount of unlabeled data in the presence of (fixed) large labeled data. To achieve this, it is paramount to make the SSL method scalable with regard"
P08-1076,W03-0425,0,0.114297,"Missing"
P08-1076,P05-1046,0,0.0212165,"s that are expected to capture a good feature representation of the target problem. As regards syntactic chunking, JESS-CM significantly outperformed ASO-semi for the same 15M-word unlabeled data size obtained from the Wall Street Journal in 1991 as described in (Ando and Zhang, 2005). Unfortunately with NER, JESS-CM is slightly inferior to ASO-semi for the same 27M-word unlabeled data size extracted from the Reuters corpus. In fact, JESS-CM using 37M-words of unlabeled data provided a comparable result. We observed that ASOsemi prefers ‘nugget extraction’ tasks to ’field segmentation’ tasks (Grenager et al., 2005). We cannot provide details here owing to the space limitation. Intuitively, their word prediction auxiliary problems can capture only a limited number of characteristic behaviors because the auxiliary problems are constructed by a limited number of ‘binary’ classifiers. Moreover, we should remember that ASOsemi used the human knowledge that ‘named entities mostly consist of nouns or adjectives’ during the auxiliary problem construction in their NER experiments. In contrast, our results require no such additional knowledge or limitation. In addition, the design and training of auxiliary proble"
P08-1076,N01-1025,0,0.361443,"Missing"
P08-1076,N03-1028,0,0.433554,"e written as: L1 (λ0 |Θ) = 0 X n log P (y n |xn ; λ0 , Θ) + log p(λ0 ), where p(λ ) is a prior probability distribution of λ0 . Clearly, JESS-CM shown in Equation 2 has exactly the same form as Equation 1. With a fixed Θ, the log-likelihood, log pj , can be seen simply as the feature functions of JESS-CM as with fi . Therefore, embedded joint PMs do not violate the global convergence conditions. As a result, as with supervised CRFs, it is guaranteed that λ0 has a value that achieves the global maximum of L1 (λ0 |Θ). Moreover, we can obtain the same form of gradient as that of supervised CRFs (Sha and Pereira, 2003), that is, £ ¤ ∇L1 (λ0 |Θ) = EP˜ (Y,X ;λ0 ,Θ) h(Y, X ) X £ ¤ − EP (Y|xn ;λ0 ,Θ) h(Y, xn ) +∇ log p(λ0 ). n Thus, we can easily optimize L1 by using the forward-backward algorithm since this paper solely focuses on a sequence model and a gradient-based optimization algorithm in the same manner as those used in supervised CRF parameter estimation. We cannot naturally incorporate unlabeled data into standard discriminative learning methods since the correct outputs y for unlabeled data are unknown. On the other hand with a generative approach, a well-known way to achieve this incorporation is to"
P08-1076,P07-1096,0,0.101655,"621 Development 30–31/08/96 3,466 51,362 Test 06–07/12/96 3,684 46,435 Table 1: Details of training, development, and test data (labeled data set) used in our experiments data Tipster Reuters Corpus English Gigaword total In our experiments, we report POS tagging, syntactic chunking and NER performance incorporating up to 1G-words of unlabeled data. 3.1 Data Set To compare the performance with that of previous studies, we selected widely used test collections. For our POS tagging experiments, we used the Wall Street Journal in PTB III (Marcus et al., 1994) with the same data split as used in (Shen et al., 2007). For our syntactic chunking and NER experiments, we used exactly the same training, development and test data as those provided for the shared tasks of CoNLL’00 (Tjong Kim Sang and Buchholz, 2000) and CoNLL’03 (Tjong Kim Sang and Meulder, 2003), respectively. The training, development and test data are detailed in Table 11 . The unlabeled data for our experiments was taken from the Reuters corpus, TIPSTER corpus (LDC93T3C) and the English Gigaword corpus, third edition (LDC2007T07). As regards the TIP1 The second-order encoding used in our NER experiments is the same as that described in (Sha"
P08-1076,N06-1012,0,0.0174311,"of JESS-CM We used the same graph structure as the linear chain CRF for JESS-CM. As regards the design of the feature functions fi , Table 3 shows the feature templates used in our experiments. In the table, s indicates a focused token position. Xs−1:s represents the bi-gram of feature X obtained from s − 1 and s positions. {Xu }B u=A indicates that u ranges from A to B. For example, {Xu }s+2 u=s−2 is equal to five feature templates, {Xs−2 , Xs−1 , Xs , Xs+1 , Xs+2 }. ‘word type’ or wtp represents features of a word such as capitalization, the existence of digits, and punctuation as shown in (Sutton et al., 2006) without regular expressions. Although it is common to use external (a) POS tagging:(total 47 templates) [ys ], [ys−1:s ], {[ys , pf-Ns ], [ys , sf-Ns ]}9N =1 , {[ys , wdu ], [ys , wtpu ], [ys−1:s , wtpu ]}s+2 u=s−2 , {[ys , wdu−1:u ], [ys , wtpu−1:u ], [ys−1:s , wtpu−1:u ]}s+2 u=s−1 (b) Syntactic chunking: (total 39 templates) [ys ], [ys−1:s ], {[ys , wdu ], [ys , posu ], [ys , wdu , posu ], [ys−1:s , wdu ], [ys−1:s , posu ]}s+2 u=s−2 , {[ys , wdu−1:u ], [ys , posu−1:u ], {[ys−1:s , posu−1:u ]}s+2 u=s−1 , (c) NER: (total 79 templates) [ys ], [ys−1:s ], {[ys , wdu ], [ys , lwdu ], [ys , posu ]"
P08-1076,D07-1083,1,0.417996,". Second, we report the best current results for the widely used test 665 Proceedings of ACL-08: HLT, pages 665–673, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics collections described above. Third, we confirm that the use of more unlabeled data in SSL can really lead to further improvements. 2 Conditional Model for SSL We design our model for SSL as a natural semisupervised extension of conventional supervised conditional random fields (CRFs) (Lafferty et al., 2001). As our approach for incorporating unlabeled data, we basically follow the idea proposed in (Suzuki et al., 2007). 2.1 Conventional Supervised CRFs Let x ∈ X and y ∈ Y be an input and output, where X and Y represent the set of possible inputs and outputs, respectively. C stands for the set of cliques in an undirected graphical model G(x, y), which indicates the interdependency of a given x and y. y c denotes the output from the corresponding clique c. Each clique c ∈ C has a potential function Ψc . Then, the CRFs define the conditional probability p(y|x) as a product of Ψc s. In addition, let f = (f1, . . ., fI ) be a feature vector, and λ = (λ1, . . ., λI ) be a parameter vector, whose lengths are I. p("
P08-1076,W00-0726,0,0.6312,"nd are referred to as sequential labeling and segmentation problems. In some cases, these tasks have relatively large amounts of labeled training data. In this situation, supervised learning can provide competitive results, and it is difficult to improve them any further by using SSL. In fact, few papers have succeeded in showing significantly better results than state-of-theart supervised learning. Ando and Zhang (2005) reported a substantial performance improvement compared with state-of-the-art supervised learning results for syntactic chunking with the CoNLL’00 shared task data (Tjong Kim Sang and Buchholz, 2000) and NER with the CoNLL’03 shared task data (Tjong Kim Sang and Meulder, 2003). One remaining question is the behavior of SSL when using as much labeled and unlabeled data as possible. This paper investigates this question, namely, the use of a large amount of unlabeled data in the presence of (fixed) large labeled data. To achieve this, it is paramount to make the SSL method scalable with regard to the size of unlabeled data. We first propose a scalable model for SSL. Then, we apply our model to widely used test collections, namely Penn Treebank (PTB) III data (Marcus et al., 1994) for POS ta"
P08-1076,W03-0419,0,0.0369952,"Missing"
P08-1076,N03-1033,0,0.0458738,"Missing"
P08-1076,J93-2004,0,\N,Missing
P09-1093,P06-1048,0,0.241824,"del (Takeuchi and Matsumoto, 2001; Nomoto, 2008) through the use of simple linear combined features (Oguro et al., 2002). The tree trimming approach guarantees that the compressed sentence is grammatical if the source sentence does not trigger parsing error. However, as we mentioned in Section 2, the tree trimming approach is not suitable for Japanese sentence compression because in many cases it cannot reproduce human-produced compressions. As an alternative to these tree trimming approaches, sequence-oriented approaches have been proposed (McDonald, 2006; Nomoto, 2007; Hori and Furui, 2003; Clarke and Lapata, 2006). Nomoto (2007) and McDonald (2006) employed the random field based approach. Hori et al. (2003) and Clarke et al. (2006) employed the linear model with simple combined features. They simply regard a sentence as a word sequence and structural information, such as full parse tree or dependency trees, are encoded in the sequence as features. The advantage of these methods over the tree trimming approach is that they have the potential to drop arbitrary words from the original sentence without the need to consider the boundaries determined by the tree structures. This approach is more suitable fo"
P09-1093,W04-3239,0,0.0304391,"s output by a state-of-the-art Japanese dependency parser contain at least one error (Kudo and Matsumoto, 2005). Even more, it is well known that if we parse a sentence whose source is different from the training data of the parser, the performance could be much worse. This critically degrades the overall performance of sentence compression. Moreover, summarization systems often have to process megabytes of documents. Parsers are still slow and users of on2 Generally, a dependency relation is defined between bunsetsu. Therefore, in order to identify word dependencies, we followed Kudo’s rule (Kudo and Matsumoto, 2004) 827 3.2.1 Intra-sentence Positional Term Weighting (IPTW) IDF is a global term weighting scheme in that it measures the significance score of a word in a text corpus, which could be extremely large. By contrast, this paper proposes another type of term weighting; it measures the positional significance score of a word within its sentence. Here, we assume the following hypothesis: demand summarization systems are not prepared to wait for parsing to finish. 3 A Syntax Free Sequence-oriented Sentence Compression Method As an alternative to syntactic parsing, we propose two novel features, intra-"
P09-1093,E06-1038,0,0.127006,"thods trim dependency trees by the discriminative model (Takeuchi and Matsumoto, 2001; Nomoto, 2008) through the use of simple linear combined features (Oguro et al., 2002). The tree trimming approach guarantees that the compressed sentence is grammatical if the source sentence does not trigger parsing error. However, as we mentioned in Section 2, the tree trimming approach is not suitable for Japanese sentence compression because in many cases it cannot reproduce human-produced compressions. As an alternative to these tree trimming approaches, sequence-oriented approaches have been proposed (McDonald, 2006; Nomoto, 2007; Hori and Furui, 2003; Clarke and Lapata, 2006). Nomoto (2007) and McDonald (2006) employed the random field based approach. Hori et al. (2003) and Clarke et al. (2006) employed the linear model with simple combined features. They simply regard a sentence as a word sequence and structural information, such as full parse tree or dependency trees, are encoded in the sequence as features. The advantage of these methods over the tree trimming approach is that they have the potential to drop arbitrary words from the original sentence without the need to consider the boundaries determ"
P09-1093,P08-1035,0,0.413878,"Hori’s method. 1 Introduction In order to compress a sentence while retaining its original meaning, the subject-predicate relationship of the original sentence should be preserved after compression. In accordance with this idea, conventional sentence compression methods employ syntactic parsers. English sentences are usually analyzed by a full parser to make parse trees, and the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1 Hereafter, we refer these compression processes as “tree trimming.” 826 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 826–833, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Chunk 1 推定 し suitei shi Chunk 2 推定 し た suitei shi ta Chunk 3 配点 について haiten Chunk 1 nitsuite 福武 Chunk 3 配点 について haiten ga Chunk 4 ta Chunk 2 が fukutake た nitsuite 福武 fukutake が ga Chunk 7 枝問 部分 の Compression Chunk 5 公表 し て い kouhyou shi te i センタ試験枝問 center shiken edamon edamon bub"
P09-1093,P02-1040,0,0.0783887,"Corpus and Evaluation Measures We randomly selected 1,000 lead sentences (a lead sentence is the first sentence of an article excluding the headline.) whose length (number of words) was greater than 30 words from the Mainichi Newspaper from 1994 to 2002. There were five different ideal compressions (reference compressions produced by human) for each sentence; all had a 0.6 compression rate. The average length of the input sentences was about 42 words and that of the reference compressions was about 24 words. For MCE learning, we selected the reference compression that maximize the BLEU score (Papineni et al., 2002) (= argmaxr∈R BLEU(r, R)) from the set of reference compressions and used it as correct data for training. Note that r is a reference compression and R is the set of reference compressions. We employed both automatic evaluation and human subjective evaluation. For automatic evaluation, we employed BLEU (Papineni et al., 2002) by following (Unno et al., 2006). We utilized 5fold cross validation, i.e., we broke the whole data set into five blocks and used four of them for training and the remainder for testing and repeated the evaluation on the test data five times changing the test block each"
P09-1093,P05-1036,0,0.566966,"show that our method outperforms Hori’s method, a state-of-theart conventional technique. Because our method does not use a syntactic parser, it is 4.3 times faster than Hori’s method. 1 Introduction In order to compress a sentence while retaining its original meaning, the subject-predicate relationship of the original sentence should be preserved after compression. In accordance with this idea, conventional sentence compression methods employ syntactic parsers. English sentences are usually analyzed by a full parser to make parse trees, and the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1 Hereafter, we refer these compression processes as “tree trimming.” 826 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 826–833, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Chunk 1 推定 し suitei shi Chunk 2 推定 し た suitei shi ta Chunk 3 配点 について haiten Chunk 1 nitsuite 福武 Chunk 3 配点 について haiten ga Ch"
P09-1093,P06-2109,0,0.213731,"forms Hori’s method, a state-of-theart conventional technique. Because our method does not use a syntactic parser, it is 4.3 times faster than Hori’s method. 1 Introduction In order to compress a sentence while retaining its original meaning, the subject-predicate relationship of the original sentence should be preserved after compression. In accordance with this idea, conventional sentence compression methods employ syntactic parsers. English sentences are usually analyzed by a full parser to make parse trees, and the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1 Hereafter, we refer these compression processes as “tree trimming.” 826 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 826–833, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Chunk 1 推定 し suitei shi Chunk 2 推定 し た suitei shi ta Chunk 3 配点 について haiten Chunk 1 nitsuite 福武 Chunk 3 配点 について haiten ga Chunk 4 ta Chunk 2 が f"
P09-1093,P05-1012,0,\N,Missing
P11-2112,P05-1001,0,0.758669,"orrelation with the best ˆ given by the base supervised model. This output y implies that fn is an informative and potent feature in the model. Then, the distribution of fn has very ˆ if |VD (fn )| small (or no) correlation to determine y is zero or near zero. In this case, fn can be evaluated as an uninformative feature in the model. From this perspective, we treat VD (fn ) as a measure of feature potency in terms of the base supervised model. The essence of this idea, evaluating features against each other on a certain model, is widely used in the context of semi-supervised learning, i.e., (Ando and Zhang, 2005; Suzuki and Isozaki, We define VD∗ (fn ) as VD∗ (fn ) = dδVD0 (fn )e if VD0 (fn ) > 0 and VD∗ (fn ) = bδVD0 (fn )c otherwise, where δ is a positive user-specified constant. Note that VD∗ (fn ) always becomes an integer, that is, VD∗ (fn ) ∈ N where N = {. . . , −2, −1, 0, 1, 2, . . .}. This calculation can be seen as mapping each feature into a discrete (integer) space with respect to VD0 (fn ). δ controls the range of VD0 (fn ) mapping into the same integer. with the same (similar) feature potency are given the same weight by supervised learning since they have ˆ. δ the same potency with reg"
P11-2112,D09-1060,0,0.211446,"e summation of a data-wise calculation (map phase), and VD∗ (fn ) can be calculated independently by each feature (reduce phase). We emphasize that our feature potency estimation can be performed in a ‘single’ map-reduce process. 4 Experiments We conducted experiments on two different NLP tasks, namely NER and dependency parsing. To facilitate comparisons with the performance of previous methods, we adopted the experimental settings used to examine high-performance semi-supervised NLP systems; i.e., NER (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) and dependency parsing (Koo et al., 2008; Chen et al., 2009; Suzuki et al., 2009). For the supervised datasets, we used CoNLL’03 (Tjong Kim Sang and De Meulder, 2003) shared task data for NER, and the Penn Treebank III 639 (PTB) corpus (Marcus et al., 1994) for dependency parsing. We prepared a total of 3.72 billion token text data as unsupervised data following the instructions given in (Suzuki et al., 2009). 4.1 Comparative Methods We mainly compare the effectiveness of COFER with that of CWR derived by the Brown algorithm. The iCWR approach yields the state-of-the-art results with both dependency parsing data derived from PTB-III (Koo et al., 2008)"
P11-2112,P10-1001,0,0.0505475,"Missing"
P11-2112,P08-1068,0,0.618373,"dels of many natural language processing (NLP) systems. One simple but powerful approach for further enhancing the performance is to utilize a large amount of unsupervised data to supplement supervised data. Specifically, an approach that involves incorporating ‘clusteringbased word representations (CWR)’ induced from unsupervised data as additional features of supervised learning has demonstrated substantial performance gains over state-of-the-art supervised learning systems in typical NLP tasks, such as named entity recognition (Lin and Wu, 2009; Turian et al., 2010) and dependency parsing (Koo et al., 2008). We refer to this approach as the iCWR approach, The iCWR approach has become popular for enhancement because of its simplicity and generality. The goal of this paper is to provide yet another simple and general framework, like the iCWR approach, to enhance existing state-of-the-art supervised NLP systems. The differences between the iCWR approach and our method are as follows; suppose F is the original feature set used in supervised learning, C is the CWR feature set, and H is the new feature set generated by our method. Then, with the iCWR approach, C is induced independently from F, and us"
P11-2112,P09-1116,0,0.546608,", supervised learning has become a standard way to train the models of many natural language processing (NLP) systems. One simple but powerful approach for further enhancing the performance is to utilize a large amount of unsupervised data to supplement supervised data. Specifically, an approach that involves incorporating ‘clusteringbased word representations (CWR)’ induced from unsupervised data as additional features of supervised learning has demonstrated substantial performance gains over state-of-the-art supervised learning systems in typical NLP tasks, such as named entity recognition (Lin and Wu, 2009; Turian et al., 2010) and dependency parsing (Koo et al., 2008). We refer to this approach as the iCWR approach, The iCWR approach has become popular for enhancement because of its simplicity and generality. The goal of this paper is to provide yet another simple and general framework, like the iCWR approach, to enhance existing state-of-the-art supervised NLP systems. The differences between the iCWR approach and our method are as follows; suppose F is the original feature set used in supervised learning, C is the CWR feature set, and H is the new feature set generated by our method. Then, w"
P11-2112,D10-1004,0,0.034847,"Missing"
P11-2112,W09-1119,0,0.107647,"Missing"
P11-2112,P08-1076,1,0.954742,"mputing framework. This is because Rn and An can be calculated by the summation of a data-wise calculation (map phase), and VD∗ (fn ) can be calculated independently by each feature (reduce phase). We emphasize that our feature potency estimation can be performed in a ‘single’ map-reduce process. 4 Experiments We conducted experiments on two different NLP tasks, namely NER and dependency parsing. To facilitate comparisons with the performance of previous methods, we adopted the experimental settings used to examine high-performance semi-supervised NLP systems; i.e., NER (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) and dependency parsing (Koo et al., 2008; Chen et al., 2009; Suzuki et al., 2009). For the supervised datasets, we used CoNLL’03 (Tjong Kim Sang and De Meulder, 2003) shared task data for NER, and the Penn Treebank III 639 (PTB) corpus (Marcus et al., 1994) for dependency parsing. We prepared a total of 3.72 billion token text data as unsupervised data following the instructions given in (Suzuki et al., 2009). 4.1 Comparative Methods We mainly compare the effectiveness of COFER with that of CWR derived by the Brown algorithm. The iCWR approach yields the state-of-the-art results with both dep"
P11-2112,D09-1058,1,0.917404,"Missing"
P11-2112,W03-0419,0,0.288854,"Missing"
P11-2112,P09-1054,0,0.1595,"arning. 3.2 Feature potency discounting To discount low potency values, we redefine feature potency as VD0 (fn ) instead of VD (fn ) as follows:   log [Rn +C]−log[An ] if Rn −An &lt; −C if − C ≤ Rn −An ≤ C VD0 (fn ) = 0  log [Rn −C]−log[An ] if C &lt; Rn −An where Rn and An are defined in Figure 2. Note that VD (fn ) = VD+ (fn ) − VD− (fn ) = Rn − An . The difference from VD (fn ) is that we cast it in the log-domain and introduce a non-negative constant C. The introduction of C is inspired by the L1 regularization technique used in supervised learning algorithms such as (Duchi and Singer, 2009; Tsuruoka et al., 2009). C controls how much we discount VD (fn ) toward zero, and is given by the user. 3.3 Feature potency quantization 638 and given input x. We write z ∈ y when the local sub-structure z is a part of output y, assuming that output y is constructed by a set of local substructures. Then formally, the ∑ n-th feature is written as fn (x, z), and fn (x, y) = z∈y fn (x, z) holds. Similarly, we introduce r(x, z), where r(x, z) = 1 ˆ , and r(x, z) = 0 otherwise, namely z ∈ ˆ. if z ∈ y /y We define Z(x) as the set of all local substructures possibly generated for all y in Y(x). Z(x) can be enumerated easi"
P11-2112,P10-1040,0,0.749404,"ing has become a standard way to train the models of many natural language processing (NLP) systems. One simple but powerful approach for further enhancing the performance is to utilize a large amount of unsupervised data to supplement supervised data. Specifically, an approach that involves incorporating ‘clusteringbased word representations (CWR)’ induced from unsupervised data as additional features of supervised learning has demonstrated substantial performance gains over state-of-the-art supervised learning systems in typical NLP tasks, such as named entity recognition (Lin and Wu, 2009; Turian et al., 2010) and dependency parsing (Koo et al., 2008). We refer to this approach as the iCWR approach, The iCWR approach has become popular for enhancement because of its simplicity and generality. The goal of this paper is to provide yet another simple and general framework, like the iCWR approach, to enhance existing state-of-the-art supervised NLP systems. The differences between the iCWR approach and our method are as follows; suppose F is the original feature set used in supervised learning, C is the CWR feature set, and H is the new feature set generated by our method. Then, with the iCWR approach,"
P11-2112,J93-2004,0,\N,Missing
P13-2004,P08-1068,0,0.0276306,"of most previous studies. We used CoNLL’03 data (Tjong Kim Sang and De Meulder, 2003) for NER, and the Penn Treebank (PTB) III corpus (Marcus et al., 1994) converted to dependency trees for DEPAR (McDonald et al., 2005). 20 4.1 Our decoding models are the Viterbi algorithm on CRF (Lafferty et al., 2001), and the secondorder parsing model proposed by (Carreras, 2007) for NER and DEPAR, respectively. Features are automatically generated according to the predefined feature templates widely-used in the previous studies. We also integrated the cluster features obtained by the method explained in (Koo et al., 2008) as additional features for evaluating our method in the range of the current best systems. Configurations of Our Method Base learning algorithm: The settings of our method in our experiments imitate L1 -regularized learning algorithm since the purpose of our experiments is to investigate the effectiveness against standard L1 -regularized learning algorithms. Then, we have the following two possible settings; DC-ADMM: we leveraged the baseline L1 -regularized learning algorithm to solve Step1, and set λ1 = 0 and λ2 = 0 for Step2. DCwL1ADMM: we leveraged the baseline L2 -regularized learning al"
P13-2004,D07-1101,0,0.0175757,"s 2 through 4 is relatively much smaller than that of Step1. 4 Experiments We conducted experiments on two well-studied NLP tasks, namely named entity recognition (NER) and dependency parsing (DEPAR). Basic settings: We simply reused the settings of most previous studies. We used CoNLL’03 data (Tjong Kim Sang and De Meulder, 2003) for NER, and the Penn Treebank (PTB) III corpus (Marcus et al., 1994) converted to dependency trees for DEPAR (McDonald et al., 2005). 20 4.1 Our decoding models are the Viterbi algorithm on CRF (Lafferty et al., 2001), and the secondorder parsing model proposed by (Carreras, 2007) for NER and DEPAR, respectively. Features are automatically generated according to the predefined feature templates widely-used in the previous studies. We also integrated the cluster features obtained by the method explained in (Koo et al., 2008) as additional features for evaluating our method in the range of the current best systems. Configurations of Our Method Base learning algorithm: The settings of our method in our experiments imitate L1 -regularized learning algorithm since the purpose of our experiments is to investigate the effectiveness against standard L1 -regularized learning al"
P13-2004,P05-1012,0,0.446259,"2011). Note that the total calculation cost of our method does not increase much from original online learning algorithm since the calculation cost of Steps 2 through 4 is relatively much smaller than that of Step1. 4 Experiments We conducted experiments on two well-studied NLP tasks, namely named entity recognition (NER) and dependency parsing (DEPAR). Basic settings: We simply reused the settings of most previous studies. We used CoNLL’03 data (Tjong Kim Sang and De Meulder, 2003) for NER, and the Penn Treebank (PTB) III corpus (Marcus et al., 1994) converted to dependency trees for DEPAR (McDonald et al., 2005). 20 4.1 Our decoding models are the Viterbi algorithm on CRF (Lafferty et al., 2001), and the secondorder parsing model proposed by (Carreras, 2007) for NER and DEPAR, respectively. Features are automatically generated according to the predefined feature templates widely-used in the previous studies. We also integrated the cluster features obtained by the method explained in (Koo et al., 2008) as additional features for evaluating our method in the range of the current best systems. Configurations of Our Method Base learning algorithm: The settings of our method in our experiments imitate L1"
P13-2004,W03-0419,0,0.0592831,"Missing"
P13-2004,P09-1054,0,0.0224915,"gative integers from zero to ζ − 1, that is, Sζ = {m}ζ−1 m=0 . For example, if we set η = 0.1, δ = 0.4, κ = 4, and ζ = 3, then Sη,δ,κ,ζ = {−2.0, −0.8, −0.5, 0, 0.5, 0.8, 2.0}. The intuition of this template is that the distribution of the feature weights in trained model often takes a form a similar to that of the ‘power law’ in the case of the large feature sets. Therefore, using an exponential function with a scale and bias seems to be appropriate for fitting them. 4 RDA provided better results at least in our experiments than L1 -regularized FOBOS (Duchi and Singer, 2009), and its variant (Tsuruoka et al., 2009), which are more familiar to the NLP community. 5 L2PA is also known as a loss augmented variant of onebest MIRA, well-known in DEPAR (McDonald et al., 2005). 21 quantized 89.0 87.0 83.0 DC-ADMM L1CRF (w/ QT) L1CRF L2CRF 81.0 1.0E+00 1.0E+03 1.0E+06 # of degrees of freedom (#DoF) [log-scale] Complete Sentence Accuracy Complete Sentence Accuracy quantized 85.0 Test NER COMP F-sc L2CRF 84.88 89.97 L1CRF 84.85 89.99 (w/ QT ζ = 4) 78.39 85.33 73.40 81.45 (w/ QT ζ = 2) (w/ QT ζ = 1) 65.53 75.87 DC-ADMM (ζ = 4) 84.96 89.92 (ζ = 2) 84.04 89.35 (ζ = 1) 83.06 88.62 Test DEPER COMP UAS L2PA 49.67 93.51"
P13-2004,P07-1104,0,0.199476,"ion variables, which are also interpreted as feature weights. L(w; D) and Ω(w) represent a loss function and a regularization term, respectively. Nowadays, we, in most cases, utilize a supervised learning method expressed as the above optimization problem to estimate the feature weights of many natural language processing (NLP) tasks, such as text classification, POS-tagging, named entity recognition, dependency parsing, and semantic role labeling. In the last decade, the L1 -regularization technique, which incorporates L1 -norm into Ω(w), has become popular and widely-used in many NLP tasks (Gao et al., 2007; Tsuruoka et al., Feature Grouping Concept Going beyond L1 -regularized sparse modeling, the idea of ‘automatic feature grouping’ has recently been developed. Examples are fused lasso (Tibshirani et al., 2005), grouping pursuit (Shen and Huang, 2010), and OSCAR (Bondell and Reich, 2008). The concept of automatic feature grouping is to find accurate models that have fewer degrees of freedom. This is equivalent to enforce every optimization variables to be equal as much as possible. A simple example is ˆ 1 = (0.1, 0.5, 0.1, 0.5, 0.1) is preferred over that w ˆ 2 = (0.1, 0.3, 0.2, 0.5, 0.3) sinc"
P13-2004,J93-2004,0,\N,Missing
P15-2031,N09-1003,0,0.258112,"Missing"
P15-2031,P12-1092,0,0.164755,"Missing"
P15-2031,Q15-1016,0,0.646872,"ity and analogy tasks. Introduction Neural-network-inspired word embedding methods such as Skip-Gram (SkipGram) have been proven to capture high quality syntactic and semantic relationships between words in a vector space (Mikolov et al., 2013a). A similar embedding method, called ‘Global Vector (GloVe)’, was recently proposed. It has demonstrated significant improvements over SkipGram on the widely used ‘Word Analogy’ and ‘Word Similarity’ benchmark datasets (Pennington et al., 2014). Unfortunately, a later deep re-evaluation has revealed that GloVe does not consistently outperform SkipGram (Levy et al., 2015); both methods provided basically the same level of performance, and SkipGram even seems ‘more robust (not yielding very poor results)’ than GloVe. Moreover, some other papers, i.e., (Shi and Liu, 2014), and some researchers in the community have discussed a relationship, and/or which is superior, SkipGram or GloVe. From this background, we revisit the relationship between SkipGram and GloVe from a machine learning viewpoint. We show that it is nat2 SkipGram and GloVe Table 1 shows the notations used in this paper. 2.1 Matrix factorization view of SkipGram SkipGram can be categorized as one of"
P15-2031,W13-3512,0,0.161407,"Missing"
P15-2031,N13-1090,0,0.406305,"learning configurations. The final goal of this paper is to provide a unified learning framework that encompasses the configurations used in SkipGram and GloVe to gain a deeper understanding of the behavior of these embedding methods. We also empirically investigate which learning configuration most clearly elucidates the performance difference often observed in word similarity and analogy tasks. Introduction Neural-network-inspired word embedding methods such as Skip-Gram (SkipGram) have been proven to capture high quality syntactic and semantic relationships between words in a vector space (Mikolov et al., 2013a). A similar embedding method, called ‘Global Vector (GloVe)’, was recently proposed. It has demonstrated significant improvements over SkipGram on the widely used ‘Word Analogy’ and ‘Word Similarity’ benchmark datasets (Pennington et al., 2014). Unfortunately, a later deep re-evaluation has revealed that GloVe does not consistently outperform SkipGram (Levy et al., 2015); both methods provided basically the same level of performance, and SkipGram even seems ‘more robust (not yielding very poor results)’ than GloVe. Moreover, some other papers, i.e., (Shi and Liu, 2014), and some researchers"
P15-2031,D14-1162,0,0.121142,"empirically investigate which learning configuration most clearly elucidates the performance difference often observed in word similarity and analogy tasks. Introduction Neural-network-inspired word embedding methods such as Skip-Gram (SkipGram) have been proven to capture high quality syntactic and semantic relationships between words in a vector space (Mikolov et al., 2013a). A similar embedding method, called ‘Global Vector (GloVe)’, was recently proposed. It has demonstrated significant improvements over SkipGram on the widely used ‘Word Analogy’ and ‘Word Similarity’ benchmark datasets (Pennington et al., 2014). Unfortunately, a later deep re-evaluation has revealed that GloVe does not consistently outperform SkipGram (Levy et al., 2015); both methods provided basically the same level of performance, and SkipGram even seems ‘more robust (not yielding very poor results)’ than GloVe. Moreover, some other papers, i.e., (Shi and Liu, 2014), and some researchers in the community have discussed a relationship, and/or which is superior, SkipGram or GloVe. From this background, we revisit the relationship between SkipGram and GloVe from a machine learning viewpoint. We show that it is nat2 SkipGram and GloV"
P16-2066,P07-2045,0,0.0148058,"ed in the training corpus1 . Thus our algorithm does not cause exponential explosion of the computation time with longer phrases. log [c(X, fik ) + 1] i=1 k=1 + |ei | N X X 4 log [c(X, eik ) + 1] , i=1 k=1 Evaluation 4.1 where c(X, fik ) is the number of phrase pairs contained in X that cover fik , the k-th word of the ith source sentence fi . Similarly, c(X, eik ) is the number of phrase pairs that cover eik . Settings We conducted experiments on the ChineseEnglish and Arabic-English datasets used in NIST OpenMT 2012. In each experiment, English was set as the target language. We used Moses (Koehn et al., 2007) as the phrase-based machine translation system. We used the 5-gram Kneser-Ney language model trained separately using the English GigaWord V5 corpus (LDC2011T07), a monolingual corpus distributed at WMT 2012, and Google Web 1T 5-gram data (LDC2006T13). Word alignments are obtained by running giza++ (Och and Ney, 2003) included in the Moses system. As the test data, we used 1378 segments for the Arabic-English dataset and 2190 segments for the Chinese-English dataset, where all test segments have 4 references (LDC2013T07, LDC2013T03). The tuning set consists of about 5000 segments gathered fro"
P16-2066,N10-1134,0,0.211612,"en it satisfies the condition g(X ∪ {x}) − g(X) ≥ g(Y ∪ {x}) − g(Y ) , x∈ΩX X ← X ∪ {x∗ } 5: output X 4: 2Ω , where X, Y ∈ X ⊆ Y , and x ∈ Ω  Y . This condition represents the diminishing return property of a submodular function, i.e., the increase in the value of the function due to the addition of item x to Y is always smaller than that obtained by adding x to any subset X ⊆ Y . We say a submodular function is monotone if g(Y ) ≥ g(X) for any X, Y ∈ 2Ω satisfying X ⊆ Y . Since a submodular function has many useful properties, it appears in a wide range of applications (Kempe et al., 2003; Lin and Bilmes, 2010; Kirchhoff and Bilmes, 2014). The maximization problem of a monotone submodular function under cardinality constraints is formulated as could solve the problems in 24 hours. Moreover, further enhancement can be achieved by applying distributed algorithms (Mirzasoleiman et al., 2013) and stochastic greedy algorithms (Mirzasoleiman et al., 2015). 3 Phrase Table Pruning We first define some notations. Let Ω = {x1 , . . . , xM } be a phrase table that has M phrase pairs. Each phrase pair, xi , consists of a source language phrase, pi , and a target language phrase, qi , and is written as xi = hpi"
P16-2066,N09-1015,0,0.0479695,"Missing"
P16-2066,P11-1052,0,0.0349508,"on of the number of phrase pairs (Chinese-English). methods (Ling et al., 2012; Zens et al., 2012), a significance-based method (Johnson et al., 2007), and our method are self-contained methods. Non self-contained methods exploit usage statistics for phrase pairs (Eck et al., 2007) and additional bilingual corpora (Chen et al., 2009). Since self contained methods require additional resources, it is easy to apply to existing MT systems. Effectiveness of the submodular functions maximization formulation is confirmed in various NLP applications including text summarization (Lin and Bilmes, 2010; Lin and Bilmes, 2011) and training data selection for machine translation (Kirchhoff and Bilmes, 2014). These methods are used for selecting a subset that contains important items but not redundant items. This paper can be seen as applying the subset selection formulation to the phrase table pruning problem. Related Work 6 Previous phrase table pruning methods fall into two groups. Self-contained methods only use resources already used in the MT system, e.g., training corpus and phrase tables. Entropy-based Conclusion We have introduced a method that solves the phrase table pruning problem as a submodular function"
P16-2066,P08-2007,0,0.0293061,"aki,suzuki.jun,nagata.masaaki}@lab.ntt.co.jp Abstract ing method (Ling et al., 2012; Zens et al., 2012) offers the best performance. The entropy-based pruning method uses entropy to measure the redundancy of a phrase pair, where we say a phrase pair is redundant if it can be replaced by other phrase pairs. The entropy-based pruning method runs in time linear to the number of phrase-pairs. Unfortunately, its running time is also exponential to the length of phrases contained in the phrase pairs, since it contains the problem of finding an optimal phrase alignment, which is known to be NP-hard (DeNero and Klein, 2008). Therefore, the method can be impractical if the phrase pairs consist of longer phrases. Phrase table pruning is the act of removing phrase pairs from a phrase table to make it smaller, ideally removing the least useful phrases first. We propose a phrase table pruning method that formulates the task as a submodular function maximization problem, and solves it by using a greedy heuristic algorithm. The proposed method can scale with input size and long phrases, and experiments show that it achieves higher BLEU scores than state-of-the-art pruning methods. 1 Introduction In this paper, we intro"
P16-2066,D12-1088,0,0.0392014,"Missing"
P16-2066,N07-2006,0,0.071812,"Missing"
P16-2066,J03-1002,0,0.0102312,"ith source sentence fi . Similarly, c(X, eik ) is the number of phrase pairs that cover eik . Settings We conducted experiments on the ChineseEnglish and Arabic-English datasets used in NIST OpenMT 2012. In each experiment, English was set as the target language. We used Moses (Koehn et al., 2007) as the phrase-based machine translation system. We used the 5-gram Kneser-Ney language model trained separately using the English GigaWord V5 corpus (LDC2011T07), a monolingual corpus distributed at WMT 2012, and Google Web 1T 5-gram data (LDC2006T13). Word alignments are obtained by running giza++ (Och and Ney, 2003) included in the Moses system. As the test data, we used 1378 segments for the Arabic-English dataset and 2190 segments for the Chinese-English dataset, where all test segments have 4 references (LDC2013T07, LDC2013T03). The tuning set consists of about 5000 segments gathered from MT02 to MT06 evaluation sets (LDC2010T10, LDC2010T11, LDC2010T12, LDC2010T14, LDC2010T17). We set the maximum length of extracted phrases to 7. Table 1 shows the sizes of phrase tables. Following the settings used in (Zens et al., 2012), we reduce the effects of other components by using the same feature weights obta"
P16-2066,P03-1021,0,0.0406272,"ta, we used 1378 segments for the Arabic-English dataset and 2190 segments for the Chinese-English dataset, where all test segments have 4 references (LDC2013T07, LDC2013T03). The tuning set consists of about 5000 segments gathered from MT02 to MT06 evaluation sets (LDC2010T10, LDC2010T11, LDC2010T12, LDC2010T14, LDC2010T17). We set the maximum length of extracted phrases to 7. Table 1 shows the sizes of phrase tables. Following the settings used in (Zens et al., 2012), we reduce the effects of other components by using the same feature weights obtained by running the MERT training algorithm (Och, 2003) on full size phrase tables and tuning data to all pruned tables. We run MERT for 10 times to obtain 10 different feature weights. The BLEU scores reported in the following experiments are the averages of the results obtained by using these different feature weights. We adopt the entropy-based pruning method used in (Ling et al., 2012; Zens et al., 2012) as the baseline method, since it shows best BLEU Example 1. Consider phrase table X holding phrase pairs x1 = h(das Haus), (the house)i, x2 = h(Haus), (house)i, and x3 = h(das Haus), (the building)i. If a corpus consists of a pair of sentences"
P16-2066,D12-1089,0,0.0901783,"nvironments such as mobile phones. Even if a computer has enough resources, the large phrase tables increase turnaround time and prevent the rapid development of MT systems. Phrase table pruning is the technique of removing ineffective phrase pairs from a phrase table to make it smaller while minimizing the performance degradation. Existing phrase table pruning methods use different metrics to rank the phrase pairs contained in the table, and then remove lowranked pairs. Metrics used in previous work are frequency, conditional probability, and Fisher’s exact test score (Johnson et al., 2007). Zens et al. (2012) evaluated many phrase table pruning methods, and concluded that entropy-based prunOne key factor of the proposed method is its carefully designed objective function that evaluates the quality of a given phrase table. In this paper, we use a simple monotone submodular function that evaluates the quality of a given phrase table by its coverage of a training corpus. Our method is simple, parameter free, and does not cause exponential explosion of the computation time with longer phrases. We conduct experiments with two different language pairs, and show that the proposed method shows higher BLEU"
P16-2066,D07-1103,0,\N,Missing
P16-2066,D14-1014,0,\N,Missing
P18-2097,D16-1140,0,0.0340332,"Vinyals et al. (2015); Durrett and Klein (2015). To maintain as much reproducibility of our experiments as possible, we simply applied publicly available pre-trained word embeddings, i.e., glove.840B.300d7 , as initial values of the encoder embedding layer. (3) Output length controlling As described in Vinyals et al. (2015), not all the outputs (predicted linearized parse trees) obtained from the Seq2seq parser are valid (well-formed) as a parse tree. Toward guaranteeing that every output is a valid tree, we introduce a simple extension of the method for controlling the Seq2seq output length (Kikuchi et al., 2016). First, we introduce an additional bias term b in the decoder output layer to prevent the selection of certain output words: pj = softmax(oj + b). 200 2 0.3 1.0 1.0 3.6 Model ensemble Ensembling several independently trained models together significantly improves many NLP tasks. In the ensembling process, we predict the output tokens using the arithmetic mean of predicted probabilities computed by each model: (4) pj = If we set a large negative value at the m-th element in b, namely bm ≈ −∞, then the m-th element in pj becomes approximately 0, namely pj,m ≈ 0, regardless of the value of the k"
P18-2097,Q17-1010,0,0.0362725,"ize the unknown embedding as a bias term b of linear layer (W x + b) when obtaining every encoder embeddings for overcoming infrequent word problem. Then, we modify Eq. 2 as follows: X ei = (Exk + u) + (F sk0 + u). (2) Applying subword decomposition has recently become a leading technique in NMT literature (Sennrich et al., 2016; Wu et al., 2016). Its primary advantage is a significant reduction of the serious out-of-vocabulary (OOV) problem. We incorporated subword information as an additional feature of the original input words. A similar usage of subword features was previously proposed in Bojanowski et al. (2017). Formally, the encoder embedding vector at encoder position i, namely, ei , is calculated as follows: X ei = Exk + F sk0 , (1) k0 ∈ψ(wi ) 3 We did not substitute POS-tags for punctuation symbols such as “.”, and “,”. 4 Several recently developed neural-based constituency parsers ignore POS tags since they are not evaluated in the standard evaluation metric of constituency parsing (Bracketing F-measure). 5 Figure in the supplementary material shows the brief sketch of the method explained in the following section. k0 ∈ψ(wi ) Note that if wi is unknown token, then Eq. 2 beP comes ei = 2u + k0 ∈"
P18-2097,D14-1179,0,0.019755,"Missing"
P18-2097,E17-1117,0,0.0940347,"Missing"
P18-2097,Q17-1004,0,0.197093,"Missing"
P18-2097,D16-1257,0,0.248642,"closed brackets. (2) if the number of predicted XX-tags (or POS-tags) is equivalent to that of the words in a given input sentence, then we mask the XX-tags (or all the POS-tags) and all the open brackets. If both conditions (1) and (2) are satisfied, then the decoding process is finished. The additional cost for controlling the mask is to count the number of XX-tags and the open and closed brackets so far generated in the decoding process. 1 XA (a) p , a=1 j A (5) (a) where pj represents the probability distribution at position j predicted by the a-th model. 3.7 Language model (LM) reranking Choe and Charniak (2016) demonstrated that reranking the predicted parser output candidates with an RNN language model (LM) significantly improves performance. We refer to this reranking process as LM-rerank. Following their success, we also trained RNN-LMs on the PTB dataset with their published preprocessing code8 to reproduce the experiments in Choe and Charniak (2016) for our LM-rerank. We selected the current stateof-the-art LM (Yang et al., 2018)9 as our LMreranker, which is a much stronger LM than was used in Choe and Charniak (2016). 7 https://nlp.stanford.edu/projects/glove/ https://github.com/cdg720/emnlp20"
P18-2097,P16-2006,0,0.0315668,"Missing"
P18-2097,D15-1166,0,0.0781473,"sions This section describes several generic techniques that improve Seq2seq performance5 . Table 2 lists the notations used in this paper for a convenient reference. 3.1 : encoder embedding matrix for V (e) , where E ∈ RD×|V E trees (Vinyals et al., 2015). Roughly speaking, a linearized parse tree consists of open, close bracketing and POS-tags that correspond to a given input raw sentence. Since a one-to-one mapping exists between a parse tree and its linearized form (if the linearized form is a valid tree), we can recover parse trees from the predicted linearized parse tree. Vinyals et al. (2015) also introduced the part-of-speech (POS) tag normalization technique. They substituted each POS tag in a linearized parse tree to a single XX-tag3 , which allows Seq2seq models to achieve a more competitive performance range than the current state-ofthe-art parses4 . Table 1 shows an example of a parse tree to which linearization and POS-tag normalization was applied. : dimension of the embeddings : dimension of the hidden states : index of the (token) position in input sentence : index of the (token) position in output linearized format of parse tree : vocabulary of word for input (encoder)"
P18-2097,D16-1001,0,0.0734685,"Missing"
P18-2097,P15-1002,0,0.03537,"Missing"
P18-2097,W17-3203,0,0.0729695,"Missing"
P18-2097,P15-1030,0,0.070847,"e POS-tag normalization are independently and simultaneously estimated as oj and qj , respectively, in the decoder output layer by following equation: oj = W (o) zj , 3.4 and qj = W (q) zj . Table 3: List of model and optimization configurations (hyper-parameters) in our experiments 3.5 Pre-trained word embeddings The pre-trained word embeddings obtained from a large external corpora often boost the final task performance even if they only initialize the input embedding layer. In constituency parsing, several systems also incorporate pre-trained word embeddings, such as Vinyals et al. (2015); Durrett and Klein (2015). To maintain as much reproducibility of our experiments as possible, we simply applied publicly available pre-trained word embeddings, i.e., glove.840B.300d7 , as initial values of the encoder embedding layer. (3) Output length controlling As described in Vinyals et al. (2015), not all the outputs (predicted linearized parse trees) obtained from the Seq2seq parser are valid (well-formed) as a parse tree. Toward guaranteeing that every output is a valid tree, we introduce a simple extension of the method for controlling the Seq2seq output length (Kikuchi et al., 2016). First, we introduce an a"
P18-2097,N16-1024,0,0.163382,"s, especially for natural language generation (NLG) tasks, such as machine translation (MT) (Sutskever et al., 2014; Cho et al., 2014) and abstractive summarization (Rush et al., 2015). Seq2seq models have also been applied to constituency parsing (Vinyals et al., 2015) and provided a fairly good result. However one obvious, intuitive drawback of Seq2seq models when they are applied to constituency parsing is that they have no explicit architecture to model latent nested relationships among the words and phrases in constituency parse trees, Thus, models that directly model them, such as RNNG (Dyer et al., 2016), are an intuitively more promising approach. In fact, RNNG and its extensions (Kuncoro et al., 2017; Fried et al., 2017) provide the current stateof-the-art performance. Sec2seq models are currently considered a simple baseline of neuralbased constituency parsing. After the first proposal of an Seq2seq constituency parser, many task-independent techniques have been developed, mainly in the NLG 2 Constituency Parsing by Seq2seq Our starting point is an RNN-based Seq2seq model with an attention mechanism that was applied to constituency parsing (Vinyals et al., 2015). We omit detailed descripti"
P18-2097,D15-1044,0,0.181955,"rate several techniques that were mainly developed in natural language generation tasks, e.g., machine translation and summarization, and demonstrate that the sequenceto-sequence model achieves the current top-notch parsers’ performance without requiring explicit task-specific knowledge or architecture of constituent parsing. 1 Introduction Sequence-to-sequence (Seq2seq) models have successfully improved many well-studied NLP tasks, especially for natural language generation (NLG) tasks, such as machine translation (MT) (Sutskever et al., 2014; Cho et al., 2014) and abstractive summarization (Rush et al., 2015). Seq2seq models have also been applied to constituency parsing (Vinyals et al., 2015) and provided a fairly good result. However one obvious, intuitive drawback of Seq2seq models when they are applied to constituency parsing is that they have no explicit architecture to model latent nested relationships among the words and phrases in constituency parse trees, Thus, models that directly model them, such as RNNG (Dyer et al., 2016), are an intuitively more promising approach. In fact, RNNG and its extensions (Kuncoro et al., 2017; Fried et al., 2017) provide the current stateof-the-art performa"
P18-2097,P17-2025,0,0.137599,"et al., 2014) and abstractive summarization (Rush et al., 2015). Seq2seq models have also been applied to constituency parsing (Vinyals et al., 2015) and provided a fairly good result. However one obvious, intuitive drawback of Seq2seq models when they are applied to constituency parsing is that they have no explicit architecture to model latent nested relationships among the words and phrases in constituency parse trees, Thus, models that directly model them, such as RNNG (Dyer et al., 2016), are an intuitively more promising approach. In fact, RNNG and its extensions (Kuncoro et al., 2017; Fried et al., 2017) provide the current stateof-the-art performance. Sec2seq models are currently considered a simple baseline of neuralbased constituency parsing. After the first proposal of an Seq2seq constituency parser, many task-independent techniques have been developed, mainly in the NLG 2 Constituency Parsing by Seq2seq Our starting point is an RNN-based Seq2seq model with an attention mechanism that was applied to constituency parsing (Vinyals et al., 2015). We omit detailed descriptions due to space limitations, but note that our model architecture is identical to the one introduced in Luong et al. (20"
P18-2097,P16-1162,0,0.0377396,"coder output scores at decoder position j : output scores of auxiliary task at decoder position j : additional bias term in the decoder output layer for mask : vector format of output probability at decoder position j : number of models for ensembling : number of candidates generating for LM-reranking Table 2: List of notations used in this paper. where k = φ(wi ). Note that the second term of RHS indicates our additional subword features, and the first represents the standard word embedding extraction procedure. Among several choices, we used the byte-pair encoding (BPE) approach proposed in Sennrich et al. (2016) applying 1,000 merge operations6 . Task-independent Extensions This section describes several generic techniques that improve Seq2seq performance5 . Table 2 lists the notations used in this paper for a convenient reference. 3.1 : encoder embedding matrix for V (e) , where E ∈ RD×|V E trees (Vinyals et al., 2015). Roughly speaking, a linearized parse tree consists of open, close bracketing and POS-tags that correspond to a given input raw sentence. Since a one-to-one mapping exists between a parse tree and its linearized form (if the linearized form is a valid tree), we can recover parse trees"
P18-2097,I17-2002,1,0.908979,"Missing"
P18-2097,P12-1046,1,0.886858,"Missing"
P18-2097,P17-1076,0,0.124719,"Missing"
P18-2097,D17-1178,0,0.0767172,"Missing"
P18-2097,P15-1113,0,0.0496068,"Missing"
P19-1020,2012.eamt-1.60,0,0.010685,"in the results reported by Miyato et al. (2016). As discussed in Kurakin et al. (2017), AdvT generates the adversarial examples from correct examples, and thus, the models trained by AdvT tend to overfit to training data rather than those trained by VAT. They referred to this phenomenon of AdvT as label leaking. Table 2: BLEU scores averaged over five models in various configurations of perturbation positions (enc-emb, dec-emb, or enc-dec-emb) and adversarial regularization techniques (AdvT or VAT). 6 Experiments 6.1 Datasets We conducted experiments on the IWSLT evaluation campaign dataset (Cettolo et al., 2012). We used the IWSLT 2016 training set for training models, 2012 test set (test2012) as the development set, and 2013 and 2014 test sets (test2013 and test2014) as our test sets. Table 1 shows the statistics of datasets used in our experiments. For preprocessing of our experimental datasets, we used the Moses tokenizer2 and the truecaser3 . We removed sentences over 50 words from the training set. We also applied the byte-pair encoding (BPE) based subword splitting script4 with 16,000 merge operations (Sennrich et al., 2016b). 6.2 Results Results on four language pairs Table 3 shows the BLEU sc"
P19-1020,P02-1040,0,0.10407,"e) 27.73 23.98 enc-emb 28.73 24.90 dec-emb 27.44 23.71 enc-dec-emb 28.47 24.78 +VAT enc-emb 29.03 24.75 dec-emb 27.49 23.20 enc-dec-emb 29.47 24.92 Transformer (None) 29.15 25.19 +AdvT enc-emb 29.04 25.16 dec-emb 28.95 25.75 enc-dec-emb 29.61 25.78 +VAT enc-emb 29.95 26.00 dec-emb 29.62 25.88 enc-dec-emb 30.13 26.06 Model LSTM +AdvT 6.3 Investigation of effective configuration Table 2 shows the experimental results with configurations of perturbation positions (enc-emb, decemb, or enc-dec-emb) and adversarial regularization techniques (AdvT or VAT). As evaluation metrics, we used BLEU scores (Papineni et al., 2002)6 . Note that all reported BLEU scores are averaged over five models. Firstly, in terms of the effective perturbation position, enc-dec-emb configurations, which add perturbations to both encoder and decoder embeddings, consistently outperformed other configurations, which used either encoder or decoder only. Moreover, we achieved better performance when we added perturbation to the encoder-side (encemb) rather than the decoder-side (dec-emb). Furthermore, the results of VAT was consistently better than those of AdvT. This tendency was also observed in the results reported by Miyato et al. (20"
P19-1020,N16-1012,0,0.0254915,"ication of their study is that their method can be interpreted as a regularization method, and thus, they do not focus on generating adversarial examples. We refer to this regularization technique as adversarial regularization. We aim to further leverage this promising methodology into more sophisticated and critical neural models, i.e., neural machine translation (NMT) models, since NMT models recently play one of the central roles in the NLP research community; NMT models have been widely utilized for not only NMT but also many other NLP tasks, such as text summarization (Rush et al., 2015; Chopra et al., 2016), grammatical error correction (Ji et al., 2017), dialog generation (Shang et al., 2015), and parsing (Vinyals et al., 2015; Suzuki et al., 2018). Unfortunately, this application is not fully trivial since we potentially have several configurations for applying adversarial perturbations into NMT models (see details in Section 5). Figure 1 illustrates the model architecture of NMT models with adversarial perturbation. Therefore, the goal of this paper is to reIntroduction The existence of (small) perturbations that induce a critical prediction error in machine learning models was first discover"
P19-1020,D15-1044,0,0.0640142,". An important implication of their study is that their method can be interpreted as a regularization method, and thus, they do not focus on generating adversarial examples. We refer to this regularization technique as adversarial regularization. We aim to further leverage this promising methodology into more sophisticated and critical neural models, i.e., neural machine translation (NMT) models, since NMT models recently play one of the central roles in the NLP research community; NMT models have been widely utilized for not only NMT but also many other NLP tasks, such as text summarization (Rush et al., 2015; Chopra et al., 2016), grammatical error correction (Ji et al., 2017), dialog generation (Shang et al., 2015), and parsing (Vinyals et al., 2015; Suzuki et al., 2018). Unfortunately, this application is not fully trivial since we potentially have several configurations for applying adversarial perturbations into NMT models (see details in Section 5). Figure 1 illustrates the model architecture of NMT models with adversarial perturbation. Therefore, the goal of this paper is to reIntroduction The existence of (small) perturbations that induce a critical prediction error in machine learning mod"
P19-1020,D18-1217,0,0.0171461,"applied adversarial training to NLP tasks, e.g., (Jia and Liang, 2017; Belinkov and Bisk, 2018; Hosseini et al., 2017; Samanta and Mehta, 2017; Miyato et al., 2017; Sato et al., 2018). For example, Belinkov and Bisk (2018); Hosseini et al. (2017) proposed methods that generate input sentences with random character swaps. They utilized the generated (input) sentences as additional training data. However, the main focus of these methods is the incorporation of adversarial examples in the training phase, which is orthogonal to our attention, adversarial regularization, as described in Section 1. Clark et al. (2018) used virtual adversarial training (VAT), which is a semi-supervised extension of the adversarial regularization technique originally proposed in Miyato et al. (2016), in their experiments to compare the results with those of their proposed method. Therefore, the focus of the neural models differs from this paper. Namely, they focused on sequential labeling, whereas we discuss NMT models. In parallel to our work, Wang et al. (2019) also investigated the effectiveness of the adversarial regularization technique in neural language modeling and NMT. They also demonstrated the impacts of the adver"
P19-1020,P16-1009,0,0.0356095,"sets We conducted experiments on the IWSLT evaluation campaign dataset (Cettolo et al., 2012). We used the IWSLT 2016 training set for training models, 2012 test set (test2012) as the development set, and 2013 and 2014 test sets (test2013 and test2014) as our test sets. Table 1 shows the statistics of datasets used in our experiments. For preprocessing of our experimental datasets, we used the Moses tokenizer2 and the truecaser3 . We removed sentences over 50 words from the training set. We also applied the byte-pair encoding (BPE) based subword splitting script4 with 16,000 merge operations (Sennrich et al., 2016b). 6.2 Results Results on four language pairs Table 3 shows the BLEU scores of averaged over five models on four different language pairs (directions), namely German!English, French!English, English!German, and English!French. Furthermore, the row (b) shows the results obtained when we incorporated pseudo-parallel corpora generated using the back-translation method (Sennrich et al., 2016a) as additional training data. For Model Configurations We selected two widely used model architectures, namely, LSTM-based encoder-decoder 2 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ to"
P19-1020,P16-1162,0,0.091072,"sets We conducted experiments on the IWSLT evaluation campaign dataset (Cettolo et al., 2012). We used the IWSLT 2016 training set for training models, 2012 test set (test2012) as the development set, and 2013 and 2014 test sets (test2013 and test2014) as our test sets. Table 1 shows the statistics of datasets used in our experiments. For preprocessing of our experimental datasets, we used the Moses tokenizer2 and the truecaser3 . We removed sentences over 50 words from the training set. We also applied the byte-pair encoding (BPE) based subword splitting script4 with 16,000 merge operations (Sennrich et al., 2016b). 6.2 Results Results on four language pairs Table 3 shows the BLEU scores of averaged over five models on four different language pairs (directions), namely German!English, French!English, English!German, and English!French. Furthermore, the row (b) shows the results obtained when we incorporated pseudo-parallel corpora generated using the back-translation method (Sennrich et al., 2016a) as additional training data. For Model Configurations We selected two widely used model architectures, namely, LSTM-based encoder-decoder 2 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ to"
P19-1020,P17-1070,0,0.0304056,"nterpreted as a regularization method, and thus, they do not focus on generating adversarial examples. We refer to this regularization technique as adversarial regularization. We aim to further leverage this promising methodology into more sophisticated and critical neural models, i.e., neural machine translation (NMT) models, since NMT models recently play one of the central roles in the NLP research community; NMT models have been widely utilized for not only NMT but also many other NLP tasks, such as text summarization (Rush et al., 2015; Chopra et al., 2016), grammatical error correction (Ji et al., 2017), dialog generation (Shang et al., 2015), and parsing (Vinyals et al., 2015; Suzuki et al., 2018). Unfortunately, this application is not fully trivial since we potentially have several configurations for applying adversarial perturbations into NMT models (see details in Section 5). Figure 1 illustrates the model architecture of NMT models with adversarial perturbation. Therefore, the goal of this paper is to reIntroduction The existence of (small) perturbations that induce a critical prediction error in machine learning models was first discovered and discussed in the field of image processin"
P19-1020,P15-1152,0,0.0796438,"Missing"
P19-1020,D17-1215,0,0.0558469,"ut and output sentences, respectively, i.e. xi 2 {0, 1}|Vs |and yj 2 {0, 1}|Vt |. Here, we introduce a short notation xi:j for representing a sequence of vectors (xi , . . . , xj ). To explain the NMT model concisely, we assume that its input and output are both sequences of one-hot vectors x1:I and y1:J that correspond to input and output sentences whose lengths are I and J, respectively. Thus, the NMT model approximates the following conditional probability: YJ+1 p(Y |X) = p(yj |y0:j 1 , X), (1) Related Work j=1 Several studies have recently applied adversarial training to NLP tasks, e.g., (Jia and Liang, 2017; Belinkov and Bisk, 2018; Hosseini et al., 2017; Samanta and Mehta, 2017; Miyato et al., 2017; Sato et al., 2018). For example, Belinkov and Bisk (2018); Hosseini et al. (2017) proposed methods that generate input sentences with random character swaps. They utilized the generated (input) sentences as additional training data. However, the main focus of these methods is the incorporation of adversarial examples in the training phase, which is orthogonal to our attention, adversarial regularization, as described in Section 1. Clark et al. (2018) used virtual adversarial training (VAT), which is"
P19-1020,P18-2097,1,0.822001,"examples. We refer to this regularization technique as adversarial regularization. We aim to further leverage this promising methodology into more sophisticated and critical neural models, i.e., neural machine translation (NMT) models, since NMT models recently play one of the central roles in the NLP research community; NMT models have been widely utilized for not only NMT but also many other NLP tasks, such as text summarization (Rush et al., 2015; Chopra et al., 2016), grammatical error correction (Ji et al., 2017), dialog generation (Shang et al., 2015), and parsing (Vinyals et al., 2015; Suzuki et al., 2018). Unfortunately, this application is not fully trivial since we potentially have several configurations for applying adversarial perturbations into NMT models (see details in Section 5). Figure 1 illustrates the model architecture of NMT models with adversarial perturbation. Therefore, the goal of this paper is to reIntroduction The existence of (small) perturbations that induce a critical prediction error in machine learning models was first discovered and discussed in the field of image processing (Szegedy et al., 2014). Such perturbed inputs are often referred to as adversarial examples in"
P19-1020,D15-1166,0,0.13731,"Missing"
P19-1464,P17-1091,0,0.146498,"span models and the current state-of-the-art model to predict, focusing on the most challenging subtask, i.e., LI. Stab and Gurevych (2017) reported that their model tends to output shallow trees even if the corresponding gold trees are deeper. We therefore investigated the performance according to different depths of ADU trees. Figure 3 indi4694 Overall LI LTC ATC Avg. Macro Link No-Link Macro Support Attack Macro MC Claim Premise LSTM+dist (ELMo) 81.8 80.7 67.8 93.7 79.0 96.8 61.1 85.7 91.6 73.3 92.1 PEC Joint PointerNet (Potash et al., 2017) 76.7 60.8 92.5 84.9 89.4 73.2 92.1 St. SVM-full (Niculae et al., 2017) - 60.1 77.6 78.2 64.5 90.2 ILP Joint (Stab and Gurevych 2017) 75.2 75.1 58.5 91.8 68.0 94.7 41.3 82.6 89.1 68.2 90.3 LSTM+dist (ELMo) 78.2 73.9 57.5 90.3 77.2 84.2 70.3 83.5 - 72.9 94.0 MTC Joint PointerNet (Potash et al., 2017) 74.0 57.7 90.3 81.3 - 69.2 93.4 New Best EG (Afantenos et al., 2018) 78.5 72.2 75.7 87.6 ILP Joint (Stab and Gurevych 2017) 76.2 68.3 48.6 88.1 74.5 85.5 62.8 85.7 - 77.0 94.3 Data. Model Table 3: Comparison with existing models on the PEC and the MTC. MC denotes M AJOR C LAIM. Gold structure support ADU1: In addition, I believe that city provides more work opportunit"
P19-1464,D18-1191,1,0.851766,"parsing (Mann and Thompson, 1987; Prasad et al., 2008), which is closely related to ASP, discourse connectives are strong clues for identifying discourse relations (Marcu, 2000; Braud and Denis, 2016). Exploring effective ways to use the discourse connective information has received wide attention in various NLP fields (Sileo et al., 2019; Pan et al., 2018). Span representation Span representation design is gaining considerable attention for several NLP tasks, such as syntactic parsing (Wang and Chang, 2016; Stern et al., 2017; Kitaev and Klein, 2018), semantic role labeling (He et al., 2018; Ouchi et al., 2018), and coreference resolution (Lee et al., 2017, 2018). One common practice for effective design is to use bidirectional long shortterm memory networks (BiLSTMs). Wang and Chang (2016) proposed span representation called “LSTM-minus,” which represents each span (i, j) as the difference between the LSTM’s hidden states over time steps, i.e. hj − hi . The work most similar to ours is Li et al. (2016), where LSTM-minus is used for discourse structure prediction. This study extends it by integrating discourse properties into span representation. 3 Model This section describes (i) an LSTM-minus-base"
P19-1464,N18-1202,0,0.0740259,"Missing"
P19-1464,D17-1143,0,0.127346,"ive text. Figure 1 shows an example of an argumentative text and its structure. The structure forms a tree, the nodes of which are referred to as argumentative discourse units (ADUs) and the edges represent argumentative relations between the ADUs. ASP systems must identify such edges, edge labels (e.g., S UPPORT and ATTACK), and node labels (e.g., P REMISE, C LAIM, and M AJOR C LAIM). A key to achieving high performance is feature representation design for segmental discourse units (spans), such as ADUs. The aim of this study is to update the foundation of span representation design for ASP. Potash et al. (2017) introduced a model exploiting neural network-based span representation for ASP and reported state-of-the-art performance. Similarly, for other natural language processing (NLP) tasks, such as syntactic and semantic parsing, neural network-based span representation design is attracting considerable attention as a promising new technique; some effective designs have been reported (Wang and Chang, 2016; Stern et al., 2017; He et al., 2018). Starting from such basis, task-dependent extensions for ASP are an interesting direction to explore. Although the neural network-based approach (Potash et al"
P19-1464,prasad-etal-2008-penn,0,0.0527405,"for argument structures, has been published (Stab and Gurevych, 2017); moreover, a variety of models have been proposed for ASP. There are two main approaches for ASP: (i) a discrete feature-based approach (Stab and Gurevych, 2017; Nguyen and Litman, 2016; Peldszus and Stede, 2015) and (ii) a neural network-based approach (Potash et al., 2017; Eger et al., 2017). Because neural network-based models have achieved high performance for this task (Potash et al., 2017), this study also explores the neural network-based approach. Discourse connectives In discourse parsing (Mann and Thompson, 1987; Prasad et al., 2008), which is closely related to ASP, discourse connectives are strong clues for identifying discourse relations (Marcu, 2000; Braud and Denis, 2016). Exploring effective ways to use the discourse connective information has received wide attention in various NLP fields (Sileo et al., 2019; Pan et al., 2018). Span representation Span representation design is gaining considerable attention for several NLP tasks, such as syntactic parsing (Wang and Chang, 2016; Stern et al., 2017; Kitaev and Klein, 2018), semantic role labeling (He et al., 2018; Ouchi et al., 2018), and coreference resolution (Lee e"
P19-1464,N19-1351,0,0.0235569,"(ii) a neural network-based approach (Potash et al., 2017; Eger et al., 2017). Because neural network-based models have achieved high performance for this task (Potash et al., 2017), this study also explores the neural network-based approach. Discourse connectives In discourse parsing (Mann and Thompson, 1987; Prasad et al., 2008), which is closely related to ASP, discourse connectives are strong clues for identifying discourse relations (Marcu, 2000; Braud and Denis, 2016). Exploring effective ways to use the discourse connective information has received wide attention in various NLP fields (Sileo et al., 2019; Pan et al., 2018). Span representation Span representation design is gaining considerable attention for several NLP tasks, such as syntactic parsing (Wang and Chang, 2016; Stern et al., 2017; Kitaev and Klein, 2018), semantic role labeling (He et al., 2018; Ouchi et al., 2018), and coreference resolution (Lee et al., 2017, 2018). One common practice for effective design is to use bidirectional long shortterm memory networks (BiLSTMs). Wang and Chang (2016) proposed span representation called “LSTM-minus,” which represents each span (i, j) as the difference between the LSTM’s hidden states ov"
P19-1464,J17-3005,0,0.451822,"ttention as a promising new technique; some effective designs have been reported (Wang and Chang, 2016; Stern et al., 2017; He et al., 2018). Starting from such basis, task-dependent extensions for ASP are an interesting direction to explore. Although the neural network-based approach (Potash et al., 2017; Eger et al., 2017) achieves high performances for ASP, it does not explicitly take into account useful linguistic clues. However, prior works demonstrate that linguistic features, particularly discourse connectives, are strong clues to predict the structure for ASP (Lawrence and Reed, 2015; Stab and Gurevych, 2017). We integrate such linguistic properties into span representation design as task-dependent extensions for ASP. In summary, our contributions are as follows. • We investigate (i) span representation originally developed for other NLP tasks and (ii) a simple task-dependent extension for ASP. • Empirical results show that such representations improve the performance of ASP and yield state-of-the-art scores. • Extensive analysis reveals that such representations especially improve the performance when parsing argumentative texts with a complex structure (deeper ADU trees). To facilitate ASP resea"
P19-1464,P17-1076,0,0.184751,"is feature representation design for segmental discourse units (spans), such as ADUs. The aim of this study is to update the foundation of span representation design for ASP. Potash et al. (2017) introduced a model exploiting neural network-based span representation for ASP and reported state-of-the-art performance. Similarly, for other natural language processing (NLP) tasks, such as syntactic and semantic parsing, neural network-based span representation design is attracting considerable attention as a promising new technique; some effective designs have been reported (Wang and Chang, 2016; Stern et al., 2017; He et al., 2018). Starting from such basis, task-dependent extensions for ASP are an interesting direction to explore. Although the neural network-based approach (Potash et al., 2017; Eger et al., 2017) achieves high performances for ASP, it does not explicitly take into account useful linguistic clues. However, prior works demonstrate that linguistic features, particularly discourse connectives, are strong clues to predict the structure for ASP (Lawrence and Reed, 2015; Stab and Gurevych, 2017). We integrate such linguistic properties into span representation design as task-dependent extens"
P19-1464,P16-1218,0,0.338201,"ving high performance is feature representation design for segmental discourse units (spans), such as ADUs. The aim of this study is to update the foundation of span representation design for ASP. Potash et al. (2017) introduced a model exploiting neural network-based span representation for ASP and reported state-of-the-art performance. Similarly, for other natural language processing (NLP) tasks, such as syntactic and semantic parsing, neural network-based span representation design is attracting considerable attention as a promising new technique; some effective designs have been reported (Wang and Chang, 2016; Stern et al., 2017; He et al., 2018). Starting from such basis, task-dependent extensions for ASP are an interesting direction to explore. Although the neural network-based approach (Potash et al., 2017; Eger et al., 2017) achieves high performances for ASP, it does not explicitly take into account useful linguistic clues. However, prior works demonstrate that linguistic features, particularly discourse connectives, are strong clues to predict the structure for ASP (Lawrence and Reed, 2015; Stab and Gurevych, 2017). We integrate such linguistic properties into span representation design as t"
S19-2185,P15-1162,0,0.0525847,"Missing"
S19-2185,S19-2145,0,0.132591,"Missing"
S19-2185,D14-1181,0,0.0107513,"f1 ∈ RD1 , f2 ∈ RD2 and f3 ∈ RD3 and D = D1 + D2 + D3 . As f1 , f2 and f3 , we design the following features. • f1 : BERT feature (Section 2.2) 1057 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1057–1061 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics • f2 : Article length feature (Section 2.3) 2. BiLSTM: Using the representations as input to BiLSTM. This is the same method as the best performing one reported by Devlin et al. (2018). 3. CNN: Using the representations as input to CNN in the same way as Kim (2014). • f3 : Informative phrase feature (Section 2.4) For training our classifiers, we used only the by-article dataset but not the by-publisher dataset. This is because the labels of the by-publisher dataset turned out rather noisy. In our preliminary experiments, we found that the performance drops when training the classifiers on the by-publisher dataset. Furthermore, we apply the following three techniques. 1. Word dropout: We adopted word dropout (Iyyer et al., 2015) for regularization. The dropout rate was set to 0.3. 2. Over sampling: As mentioned above, the gold label distribution of the t"
S19-2185,D14-1162,0,0.100011,"Spl is defined as follows: Spl = {xj |ftrue (xj ) > Tf , j ≤ Tm }. (8) 4. ftrue -ffalse ratio-based: The fourth setting is to select N -grams based on ratios between ftrue and ffalse . Concretely, Spl is defined as follows:   ftrue (xj ) Spl = xj > Tr , j ≤ Tm . (9) ffalse (xj ) To , Tc , Tf , Tr and Tm are hyper-parameters2 . Next, we obtain Sp defined as follows: [ Sp = Spl . (10) l At last, we obtain an filtered N -gram set S defined as follows: S = Sh  Sp . (11) 2.4.2 Phrase Embedding We map each of the obtained N -gram phrase set S to a feature vector f3 . We exploited GloVe vectors (Pennington et al., 2014) instead of one-hot vectors in order to facilitate generalization. First, we enumerate N -grams included in an article and compute each N -gram vector. Each vector is the average of GloVe vectors of included words. For example, the vector for the phrase “news article” is computed as follows: GloVe(news) + GloVe(article) . 2 Here, GloVe(w) denotes the GloVe (glove.840B3 ) vector of the word w. Then, we compute f3 as the average of all N -gram vectors included in the article. 2 In our experiments, we fix Tm to 200,000. https://nlp.stanford.edu/projects/ glove/ 1059 3 Method Average BiLSTM CNN Ac"
S19-2185,P18-1022,0,0.0392065,"Missing"
W03-1208,C02-1054,0,\N,Missing
W03-1208,W02-2016,0,\N,Missing
W03-1208,W01-1203,0,\N,Missing
W03-1208,C02-1150,0,\N,Missing
W03-1208,C02-1119,1,\N,Missing
W03-1208,H01-1069,0,\N,Missing
W03-1208,P03-1005,1,\N,Missing
W17-5706,W16-4616,0,0.0209001,"rpus. mance. In an ensembling process, several models are run at each time step and an arithmetic mean of predicted probability is obtained, which is used to determine the next word. In our settings, we trained eight models independently and used them for the ensemble. 2.4 Testing 2.4.1 Length Normalized Re-ranking Naive beam searches with a large beam size may tend to output shorter sentences, leading to a drop in performance (Tu et al., 2017). To reduce this negative effect, we re-ranked the candidate output sentences t by using the following score function once we finished the beam search (Cromieres et al., 2016): } { p(t) ˆ t = arg max , (1) |t| t∈t 3 Task-Specific Settings 3.1 ASPEC 3.1.1 Synthetic Corpus As we mentioned in section 2.2, ASPEC contains some unreliable sentence pairs. For SMT, we can use these sentences as monolingual data to train a language model. However in the current NMT model architecture, the model cannot be trained with monolingual data, so the previous participants with NMT models simply ignored these parts of the data (Neubig, 2016; Eriguchi et al., 2016). In a way similar to that reported by Sennrich et al. Sennrich et al. (2016b), we tried to use the unreliable part of the"
W17-5706,W16-4617,0,0.0185076,"t, we re-ranked the candidate output sentences t by using the following score function once we finished the beam search (Cromieres et al., 2016): } { p(t) ˆ t = arg max , (1) |t| t∈t 3 Task-Specific Settings 3.1 ASPEC 3.1.1 Synthetic Corpus As we mentioned in section 2.2, ASPEC contains some unreliable sentence pairs. For SMT, we can use these sentences as monolingual data to train a language model. However in the current NMT model architecture, the model cannot be trained with monolingual data, so the previous participants with NMT models simply ignored these parts of the data (Neubig, 2016; Eriguchi et al., 2016). In a way similar to that reported by Sennrich et al. Sennrich et al. (2016b), we tried to use the unreliable part of the corpus by making a synthetic corpus. Figure 1 illustrates the overview of how we made the synthetic corpus. First, we made an NMT model with the reliable part of the provided data (in our case, the first 2.0M sentences), then translated the unreliable part of the corpus by using it to make a synthetic corpus. Finally, we made a corpus of 3.0M sentences by concatenating this where p(t) is the predicted log-probability of a candidate output sentence t and |t |is the length o"
W17-5706,2015.iwslt-evaluation.11,0,0.0335328,"I 5.2 Model Fine-tuning 3.2.1 Model Fine-tuning We thought that training with a larger amount of data would enable the model to use more sentences and that this would be beneficial for further training. However, as is clear from Table 3, we couldn’t find any improvements over fine-tuning. We suspect that the parallel corpus used to initialize the model is quite out-of-domain, so the model couldn’t get any benefits from it. We thought the JIJI corpus was too small to train an NMT model, so we tried to train the model with other large parallel corpora and then fine-tune it with the JIJI corpus (Luong and Manning, 2015). In our settings, we first trained the model with ASPEC (2.0M) and Japan Patent Office Patent Corpus (JPC) (1.0M). We learned BPE codes with the JIJI corpus and applied them to ASPEC and JPC. We trained the model with ASPEC and JPC for 20 epochs, then continued training with the JIJI corpus for a further 20 epochs. 4 5.3 JIJI Corpus Quality In the JIJI corpus subtasks, we were only able to see a small correlation between BLEU scores and human evaluation. To find out the reason for this, we manually looked into the JIJI corpus. In doing so, we found that it was too noisy for efficient learning"
W17-5706,D15-1166,0,0.0626273,"l Fine-tuning 3.2.1 Model Fine-tuning We thought that training with a larger amount of data would enable the model to use more sentences and that this would be beneficial for further training. However, as is clear from Table 3, we couldn’t find any improvements over fine-tuning. We suspect that the parallel corpus used to initialize the model is quite out-of-domain, so the model couldn’t get any benefits from it. We thought the JIJI corpus was too small to train an NMT model, so we tried to train the model with other large parallel corpora and then fine-tune it with the JIJI corpus (Luong and Manning, 2015). In our settings, we first trained the model with ASPEC (2.0M) and Japan Patent Office Patent Corpus (JPC) (1.0M). We learned BPE codes with the JIJI corpus and applied them to ASPEC and JPC. We trained the model with ASPEC and JPC for 20 epochs, then continued training with the JIJI corpus for a further 20 epochs. 4 5.3 JIJI Corpus Quality In the JIJI corpus subtasks, we were only able to see a small correlation between BLEU scores and human evaluation. To find out the reason for this, we manually looked into the JIJI corpus. In doing so, we found that it was too noisy for efficient learning"
W17-5706,P16-1009,0,0.49681,"the previous state-of-the-art system. We also tried to make use of the unreliable part of the provided parallel corpus by backtranslating and making a synthetic corpus. Our submitted system achieved the new state-of-the-art performance in terms of the BLEU score, as well as human evaluation. 1 2.2 Data Preprocessing First, we tokenize the provided corpus using KyTea (Neubig et al., 2011) for the Japanese side, and Moses tokenizer2 for the English side. We remove the sentences over 60 words to clean the corpus. Then we further split it into sub-words using joint byte pair encoding (joint-BPE) (Sennrich et al., 2016c) with applying 16,000 merge operations. For ASPEC subtasks, though the provided training data contained over 3.0M sentences, we only used the first 2.0M sentences, in the same way as the previous participants (Neubig, 2014). ASPEC was collected by aligning parallel sentences automatically and sorting them on the basis of the alignment confidence score (Nakazawa et al., 2016). This means that the latter side of the corpus may contain noisy parallel sentences, which would have a negative impact on training. We used the latter 1.0M sentences as a monolingual corpus and made a synthetic corpus ("
W17-5706,P16-1162,0,0.84021,"the previous state-of-the-art system. We also tried to make use of the unreliable part of the provided parallel corpus by backtranslating and making a synthetic corpus. Our submitted system achieved the new state-of-the-art performance in terms of the BLEU score, as well as human evaluation. 1 2.2 Data Preprocessing First, we tokenize the provided corpus using KyTea (Neubig et al., 2011) for the Japanese side, and Moses tokenizer2 for the English side. We remove the sentences over 60 words to clean the corpus. Then we further split it into sub-words using joint byte pair encoding (joint-BPE) (Sennrich et al., 2016c) with applying 16,000 merge operations. For ASPEC subtasks, though the provided training data contained over 3.0M sentences, we only used the first 2.0M sentences, in the same way as the previous participants (Neubig, 2014). ASPEC was collected by aligning parallel sentences automatically and sorting them on the basis of the alignment confidence score (Nakazawa et al., 2016). This means that the latter side of the corpus may contain noisy parallel sentences, which would have a negative impact on training. We used the latter 1.0M sentences as a monolingual corpus and made a synthetic corpus ("
W17-5706,W14-7002,0,0.0491336,"ce in terms of the BLEU score, as well as human evaluation. 1 2.2 Data Preprocessing First, we tokenize the provided corpus using KyTea (Neubig et al., 2011) for the Japanese side, and Moses tokenizer2 for the English side. We remove the sentences over 60 words to clean the corpus. Then we further split it into sub-words using joint byte pair encoding (joint-BPE) (Sennrich et al., 2016c) with applying 16,000 merge operations. For ASPEC subtasks, though the provided training data contained over 3.0M sentences, we only used the first 2.0M sentences, in the same way as the previous participants (Neubig, 2014). ASPEC was collected by aligning parallel sentences automatically and sorting them on the basis of the alignment confidence score (Nakazawa et al., 2016). This means that the latter side of the corpus may contain noisy parallel sentences, which would have a negative impact on training. We used the latter 1.0M sentences as a monolingual corpus and made a synthetic corpus (see section 3.1.1 for details). Introduction In this paper, we describe our systems submitted to this year’s translation shared tasks at WAT 2017 (Nakazawa et al., 2017). For this year, we focused on scientific paper (ASPEC J"
W17-5706,W16-4610,0,0.0139501,"negative effect, we re-ranked the candidate output sentences t by using the following score function once we finished the beam search (Cromieres et al., 2016): } { p(t) ˆ t = arg max , (1) |t| t∈t 3 Task-Specific Settings 3.1 ASPEC 3.1.1 Synthetic Corpus As we mentioned in section 2.2, ASPEC contains some unreliable sentence pairs. For SMT, we can use these sentences as monolingual data to train a language model. However in the current NMT model architecture, the model cannot be trained with monolingual data, so the previous participants with NMT models simply ignored these parts of the data (Neubig, 2016; Eriguchi et al., 2016). In a way similar to that reported by Sennrich et al. Sennrich et al. (2016b), we tried to use the unreliable part of the corpus by making a synthetic corpus. Figure 1 illustrates the overview of how we made the synthetic corpus. First, we made an NMT model with the reliable part of the provided data (in our case, the first 2.0M sentences), then translated the unreliable part of the corpus by using it to make a synthetic corpus. Finally, we made a corpus of 3.0M sentences by concatenating this where p(t) is the predicted log-probability of a candidate output sentence t"
W17-5706,P11-2093,0,0.0500419,"further experiments1 . In this year, we participated in four translation subtasks at WAT 2017. Our model structure is quite simple but we used it with well-tuned hyper-parameters, leading to a significant improvement compared to the previous state-of-the-art system. We also tried to make use of the unreliable part of the provided parallel corpus by backtranslating and making a synthetic corpus. Our submitted system achieved the new state-of-the-art performance in terms of the BLEU score, as well as human evaluation. 1 2.2 Data Preprocessing First, we tokenize the provided corpus using KyTea (Neubig et al., 2011) for the Japanese side, and Moses tokenizer2 for the English side. We remove the sentences over 60 words to clean the corpus. Then we further split it into sub-words using joint byte pair encoding (joint-BPE) (Sennrich et al., 2016c) with applying 16,000 merge operations. For ASPEC subtasks, though the provided training data contained over 3.0M sentences, we only used the first 2.0M sentences, in the same way as the previous participants (Neubig, 2014). ASPEC was collected by aligning parallel sentences automatically and sorting them on the basis of the alignment confidence score (Nakazawa et"
W17-5706,W16-2323,0,0.194381,"the previous state-of-the-art system. We also tried to make use of the unreliable part of the provided parallel corpus by backtranslating and making a synthetic corpus. Our submitted system achieved the new state-of-the-art performance in terms of the BLEU score, as well as human evaluation. 1 2.2 Data Preprocessing First, we tokenize the provided corpus using KyTea (Neubig et al., 2011) for the Japanese side, and Moses tokenizer2 for the English side. We remove the sentences over 60 words to clean the corpus. Then we further split it into sub-words using joint byte pair encoding (joint-BPE) (Sennrich et al., 2016c) with applying 16,000 merge operations. For ASPEC subtasks, though the provided training data contained over 3.0M sentences, we only used the first 2.0M sentences, in the same way as the previous participants (Neubig, 2014). ASPEC was collected by aligning parallel sentences automatically and sorting them on the basis of the alignment confidence score (Nakazawa et al., 2016). This means that the latter side of the corpus may contain noisy parallel sentences, which would have a negative impact on training. We used the latter 1.0M sentences as a monolingual corpus and made a synthetic corpus ("
W18-5410,D14-1179,0,0.0293295,"Missing"
W18-5410,P17-1106,0,0.0248262,"et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has been an epoch-making development that has led to great progress in many natural language generation tasks, such as machine translation (Bahdanau et al., 2015), dialog generation (Shang et al., 2015), and headline generation (Rush et al., 2015). An enormous number of studies have attempted to enhance the ability of EncDec. Furthermore, several intensive studies have also attempted to analyze and interpret the inside of black-box EncDec models, especially how they translate a given source sentence to the corresponding target sentence (Ding et al., 2017). One typical approach to this is to visualize an attention matrix, which is a collection of attention vectors (Bahdanau et al., 2015; Luong et al., 2015; Tu et al., 2016). ∗ This work is a product of collaborative research program of Tohoku University and NTT Communication Science Laboratories. 1 Our code for reproducing the experiments is available at https://github.com/butsugiri/UAM 74 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 74–81 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics 3.1 sp"
W18-5410,I17-1004,0,0.0451013,"EN Center for Advanced Intelligence Project {kiyono,jun.suzuki,inui}@ecei.tohoku.ac.jp, {takase.sho, nagata.masaaki}@lab.ntt.co.jp, okazaki@c.titech.ac.jp Abstract The assumption behind this interpretation is that the attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this pa"
W18-5410,P15-1152,0,0.0223308,"se a method that explicitly models the token-wise alignment between the source and target sequences to provide a better analysis. Experiments show that our method can acquire token-wise alignments that are superior to those of an attention mechanism1 . 1 Introduction The Encoder-Decoder model with an attention mechanism (EncDec) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has been an epoch-making development that has led to great progress in many natural language generation tasks, such as machine translation (Bahdanau et al., 2015), dialog generation (Shang et al., 2015), and headline generation (Rush et al., 2015). An enormous number of studies have attempted to enhance the ability of EncDec. Furthermore, several intensive studies have also attempted to analyze and interpret the inside of black-box EncDec models, especially how they translate a given source sentence to the corresponding target sentence (Ding et al., 2017). One typical approach to this is to visualize an attention matrix, which is a collection of attention vectors (Bahdanau et al., 2015; Luong et al., 2015; Tu et al., 2016). ∗ This work is a product of collaborative research program of Tohoku"
W18-5410,D17-1227,0,0.0166013,"ns q˜ with the sum of the source-side ˜ as an objective function `src . tokens x Since oj for each j is a vector representation of the ˆ 0:j−1 , X, θ) over the target probabilities of p(y|y vocabularies yˆ ∈ Vt , we can calculate `trg as (8) `trg (Y , X, θ) = − J+1 X  yj> · log oj . (12) j=1 3.3 Inference of EncDec In the inference step, we use the trained parameters to search for the best target sequence. We use beam search to find the target sequence that maximizes the product of the conditional probabilities as described in Equation 1. From among several stopping criteria for beam search (Huang et al., 2017), we adopt the widely used “shrinking beam” implemented in RNNsearch (Bahdanau et al., 2015)3 . (9) where Ws ∈ RH×2H is a parameter matrix. Finally, zj is fed into the softmax layer. The model generates a target-side token based on the probability distribution oj ∈ RVt as (10) where Wo ∈ RVt ×H is a parameter matrix and bo ∈ RVt is a bias term. 3.2 & ? + (?"":$ ) Next, the source-side information is mixed with the decoder hidden state to derive final hidden state zj . Concretely, the context vector cj is concate~j to form vector uj ∈ R2H . uj is then nated with z fed into a single fully-connect"
W18-5410,E17-2047,1,0.939025,"ecent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation task, which is categorized as a lossycompression generation (lossy-gen) task (Nallapati et al., 2016). Compared with a machine translation task, which is categorized as a loss-less generation (lossless-gen) task, the headline generation task additionally requires EncDec models to appropriately select salient ideas in given source sentences (Suzuki and Nagata, 2017). Therefore, the lossy-gen task seems to make modeling by EncDec much harder. In fact, our preliminary experiments revealed that the attention mechanism in EncDec models largely fails to capture token-wise alignments, e.g., less than 10 percent accuracy, even if we use one of the current state-of-the-art EncDec models (Table 3). To obtain a better analysis of how EncDec models translate a given source sentence to the correDeveloping a method for understanding the inner workings of black-box neural methods is an important research endeavor. Conventionally, many studies have used an attention ma"
W18-5410,W17-3204,0,0.0232542,"attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation task, which is categorized as a lossycompression generation (lossy-gen) task (Nallapati et al., 2016). Compared with a machine translation task, which is categorized a"
W18-5410,N06-1014,0,0.0899348,"pretation is that the attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation task, which is categorized as a lossycompression generation (lossy-gen) task (Nallapati et al., 2016). Compared with a machine translation ta"
W18-5410,D16-1033,0,0.017834,"hat used in Rush et al. (2015). The dataset consists of pairs of the first sentence of each article and its headline from the annotated English Gigaword corpus (Napoles et al., 2012). Rush et al. (2015) defined the training, validation and test splits, which contain approximately 3.8M, 200K and 400K source-headline pairs, respectively We used the entire training split for training as in the previous studies. We randomly sampled 8K instances as validation data and 10K instances as test data from the validation split. Moreover, we experimented on the test data provided by Zhou et al. (2017) and Toutanova et al. (2016) for comparison with the reported state-of-the-art performance (Zhou et al., 2017). We refer to those test data sets as Test (Ours), Test (Zhou), and MSRATC respectively. Among these test sets, MSRATC is the only dataset created by a human worker. 5.2 5131 5131 200 400 Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) 2-layer bidirectional-LSTM 2-layer LSTM with attention (Luong et al., 2015) Adam (Kingma and Ba, 2015) 0.001 0.5 for each epoch (after epoch 9) 10 256 (shuffled at each epoch) 5 Max 15 epochs with early stopping Dropout (rate 0.3) Beam size 20 with the length norma"
W18-5410,C16-1291,0,0.0193027,"Intelligence Project {kiyono,jun.suzuki,inui}@ecei.tohoku.ac.jp, {takase.sho, nagata.masaaki}@lab.ntt.co.jp, okazaki@c.titech.ac.jp Abstract The assumption behind this interpretation is that the attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclu"
W18-5410,P16-1008,0,0.10152,"as machine translation (Bahdanau et al., 2015), dialog generation (Shang et al., 2015), and headline generation (Rush et al., 2015). An enormous number of studies have attempted to enhance the ability of EncDec. Furthermore, several intensive studies have also attempted to analyze and interpret the inside of black-box EncDec models, especially how they translate a given source sentence to the corresponding target sentence (Ding et al., 2017). One typical approach to this is to visualize an attention matrix, which is a collection of attention vectors (Bahdanau et al., 2015; Luong et al., 2015; Tu et al., 2016). ∗ This work is a product of collaborative research program of Tohoku University and NTT Communication Science Laboratories. 1 Our code for reproducing the experiments is available at https://github.com/butsugiri/UAM 74 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 74–81 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics 3.1 sponding target sentence in the headline generation task, this paper introduces the Unsupervised tokenwise Alignment Module (UAM), a novel component that can be plugged into"
W18-5410,D15-1166,0,0.627426,".suzuki,inui}@ecei.tohoku.ac.jp, {takase.sho, nagata.masaaki}@lab.ntt.co.jp, okazaki@c.titech.ac.jp Abstract The assumption behind this interpretation is that the attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation"
W18-5410,P17-1101,0,0.406026,"ments is identical to that used in Rush et al. (2015). The dataset consists of pairs of the first sentence of each article and its headline from the annotated English Gigaword corpus (Napoles et al., 2012). Rush et al. (2015) defined the training, validation and test splits, which contain approximately 3.8M, 200K and 400K source-headline pairs, respectively We used the entire training split for training as in the previous studies. We randomly sampled 8K instances as validation data and 10K instances as test data from the validation split. Moreover, we experimented on the test data provided by Zhou et al. (2017) and Toutanova et al. (2016) for comparison with the reported state-of-the-art performance (Zhou et al., 2017). We refer to those test data sets as Test (Ours), Test (Zhou), and MSRATC respectively. Among these test sets, MSRATC is the only dataset created by a human worker. 5.2 5131 5131 200 400 Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) 2-layer bidirectional-LSTM 2-layer LSTM with attention (Luong et al., 2015) Adam (Kingma and Ba, 2015) 0.001 0.5 for each epoch (after epoch 9) 10 256 (shuffled at each epoch) 5 Max 15 epochs with early stopping Dropout (rate 0.3) Beam s"
W18-5410,K16-1028,0,0.201448,"y inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation task, which is categorized as a lossycompression generation (lossy-gen) task (Nallapati et al., 2016). Compared with a machine translation task, which is categorized as a loss-less generation (lossless-gen) task, the headline generation task additionally requires EncDec models to appropriately select salient ideas in given source sentences (Suzuki and Nagata, 2017). Therefore, the lossy-gen task seems to make modeling by EncDec much harder. In fact, our preliminary experiments revealed that the attention mechanism in EncDec models largely fails to capture token-wise alignments, e.g., less than 10 percent accuracy, even if we use one of the current state-of-the-art EncDec models (Table 3). To"
W18-5410,W12-3018,0,0.197412,"Missing"
W18-5410,D15-1044,0,0.46313,"ise alignment between the source and target sequences to provide a better analysis. Experiments show that our method can acquire token-wise alignments that are superior to those of an attention mechanism1 . 1 Introduction The Encoder-Decoder model with an attention mechanism (EncDec) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has been an epoch-making development that has led to great progress in many natural language generation tasks, such as machine translation (Bahdanau et al., 2015), dialog generation (Shang et al., 2015), and headline generation (Rush et al., 2015). An enormous number of studies have attempted to enhance the ability of EncDec. Furthermore, several intensive studies have also attempted to analyze and interpret the inside of black-box EncDec models, especially how they translate a given source sentence to the corresponding target sentence (Ding et al., 2017). One typical approach to this is to visualize an attention matrix, which is a collection of attention vectors (Bahdanau et al., 2015; Luong et al., 2015; Tu et al., 2016). ∗ This work is a product of collaborative research program of Tohoku University and NTT Communication Science Lab"
W18-5410,P16-1162,0,0.0510147,"ly for MSR-ATC and a performance comparable to EncDec+sGate in Test (Ours) and Test (Zhou). Considering that the MSR-ATC dataset was created by a human worker, we believe that the improvement in MSR-ATC is the most remarkable result among the three test sets, since it indicates that our model most closely fits the human-generated summary. Implementation Details In the experiment, we selected the hyper-parameter settings commonly used in previous studies e.g., (Rush et al., 2015; Nallapati et al., 2016; Suzuki and Nagata, 2017) We constructed the vocabulary set using byte pair encoding4 (BPE) (Sennrich et al., 2016) to handle low-frequency words, since this is now a common practice in the field of neural machine translation. The BPE merge operations are jointly learned from the source and the target. We set the number of BPE merge operations at 5, 000. 6 Discussion We investigated whether the UAM improves tokenwise alignment between the source and target se5 We restored sub-words to the standard token split for the evaluation. 6 ROUGE script option is: “-n2 -m -w 1.2” 4 https://github.com/rsennrich/ subword-nmt 78 Test (Ours) Test (Zhou) MSR-ATC Model RG-1 RG-2 RG-L RG-1 RG-2 RG-L RG-1 RG-2 RG-L EncDec+s"
W18-6421,W12-3131,0,0.112139,"ora3 and translated News Crawl 2017 to make a synthetic corpus. From here, we discuss these features and experimentally verify each one. 3.1 Synthetic Corpus Noisy Data Filtering This year, ParaCrawl and Common Crawl corpora, which were created by crawling parallel websites, were provided for training. Since these web-based corpora are large but noisy, it seems essential to filter out noisy sentence pairs. Since the ParaCrawl corpus has already been cleaned by Zipporah (Xu and Koehn, 2017), we chose another method for further cleaning1 . To clean the corpus, we selected the qe-clean2 toolkit (Denkowski et al., 2012), which uses a language model to evaluate a sentences naturalness and a word alignment model to check whether the sentence pair has the same meaning. Both models are trained with clean data for scoring possibly noisy parallel sentence pairs and removes sentences with scores below a threshold. For more details, see Denkowski et al. (2012). We used Europarl, News Commentary, and Rapid corpora as clean parallel data for training the word alignment model. We also used News Crawl 2017 as an additional monolingual corpus for language modeling. Since our target is news translation, using a news-relat"
W18-6421,N13-1073,0,0.0592128,"the sentence pair has the same meaning. Both models are trained with clean data for scoring possibly noisy parallel sentence pairs and removes sentences with scores below a threshold. For more details, see Denkowski et al. (2012). We used Europarl, News Commentary, and Rapid corpora as clean parallel data for training the word alignment model. We also used News Crawl 2017 as an additional monolingual corpus for language modeling. Since our target is news translation, using a news-related monolingual corpus is beneficial to train language models. We used KenLM (Heafield, 2011) and fast align (Dyer et al., 2013, 2010) for language modeling and word alignment. To find the appropriate 3.3 Back-translation BLEU-based Filtering for Synthetic Corpus A synthetic corpus might contain noise due to translation errors. Since these noisy sentences might deleteriously affect the training, we filtered them out. In this work, we did back-translation BLEUbased synthetic corpus filtering (Imankulova et al., 2017). We hypothesize that synthetic sentence pairs can be correctly back-translated to the target language unless they contains translation errors. Based on this hypothesis, we found better synthetic sentence p"
W18-6421,C04-1072,0,0.0299765,"monolingual sentences in the target language to the source language by a target-to1 Although the provided ParaCrawl corpus was already filtered by Zipporah (Xu and Koehn, 2017), a cursory glance suggested that it still contains many noisy sentence pairs. 2 https://github.com/cmu-mtlab/qe-clean 3 Europarl + News Commentary + Rapid + a filtered version of Common Crawl and ParaCrawl corpora 462 source translation model. After getting the translation, we back-translated it with the source-totarget model. Then we evaluated how well it restored the original sentences by sentence-level BLEU scores (Lin and Och, 2004), selected the high-scoring sentence pairs, and created a synthetic corpus whose size equals the naturally occurring parallel corpus. all the datasets used in our experiments. Then we split the words into subwords by joint BytePair-Encoding (BPE) (Sennrich et al., 2016b) with 32,000 merge operations. Finally, we discarded from the training data the sentence pairs that exceed 80 subwords either in the source or target sentences. As a development set, we used newstest 2017 (3004 sentences). 3.4 4.2 Right-to-Left Re-ranking Liu et al. (2016) pointed out that RNN-based sequence generation models l"
W18-6421,D15-1166,0,0.160226,"Missing"
W18-6421,W17-5706,1,0.911703,"Missing"
W18-6421,P02-1040,0,0.100892,"training the model. Experimental Results and Discussions Table 1 shows the provided and filtered corpus sizes for training. The Original Common Crawl and ParaCrawl corpora contain around 35.56M sentences. However, since most of the sentence pairs are noisy, we only retained the cleanest 4.01M sentences that were selected by the qe-clean toolkit. For the synthetic corpus, we chose the same size as the filtered parallel corpus based on the back-translation BLEU+1 scores. Table 2 shows the evaluation results of our submission and baseline systems. Here, we report the case-sensitive BLEU scores (Papineni et al., 2002) evaluated by the provided automatic evaluation system8 . In the following, unless specified, we mainly discuss the Transformer model results. 4.3.1 4.3.2 Effect of Corpus Filtering By re-ranking the n-best hypothesis by the R2L model, we saw a gain of 1.5 points for En-De and 0.5 points for De-En (Setting (6)). We submitted these results as our primary submission. R2L n-best re-ranking works well with the RNN-based model, but we confirmed that it also works well with the Transformer model. We suppose both the Transformer and the RNN models lack the ability to decode the end of the sentence, b"
W18-6421,D17-1319,0,0.120434,"ion model and used it as additional parallel data. In our case, we trained a baseline NMT model with a provided parallel corpora3 and translated News Crawl 2017 to make a synthetic corpus. From here, we discuss these features and experimentally verify each one. 3.1 Synthetic Corpus Noisy Data Filtering This year, ParaCrawl and Common Crawl corpora, which were created by crawling parallel websites, were provided for training. Since these web-based corpora are large but noisy, it seems essential to filter out noisy sentence pairs. Since the ParaCrawl corpus has already been cleaned by Zipporah (Xu and Koehn, 2017), we chose another method for further cleaning1 . To clean the corpus, we selected the qe-clean2 toolkit (Denkowski et al., 2012), which uses a language model to evaluate a sentences naturalness and a word alignment model to check whether the sentence pair has the same meaning. Both models are trained with clean data for scoring possibly noisy parallel sentence pairs and removes sentences with scores below a threshold. For more details, see Denkowski et al. (2012). We used Europarl, News Commentary, and Rapid corpora as clean parallel data for training the word alignment model. We also used Ne"
W18-6421,E17-2025,0,0.0283163,"Transformer model. Our hyper-parameters are based on the previously introduced Transformer big setting (Vaswani et al., 2017), and we also referred Popel and Bojar (2018) for tuning hyper-parameters. We used six layers for both the encoder and the decoder. All the sub-layers and the embeddings layers output 1024 dimension vectors, and the inner-layer of the position-wise feed-forward layers has 4096 dimensions. For multi-head attention, we used 16 parallel attention layers. We use the same weights for the encoder/decoder embedding layers and the decoder output layer by three-way-weight-tying (Press and Wolf, 2017). As an optimizer, we used Adam (Kingma and Ba, 2015) with 1 = 0.9 and 2 = 0.997 and set dropout (Srivastava et al., 2014) with a probability of 0.1. We used a learning rate decaying method proposed by (Vaswani et al., 2017) with 16,000 warm-up steps and trained the model for 300,000 steps. Each mini-batch contained roughly 20,000 tokens. We saved a model every hour and averaged the last 16 model parameters for decoding. The training took about three days for both En-De and De-En with eight GTX 1080Ti GPUs. During decoding, we used a beam search with a size of ten and a length normalization te"
W18-6421,P16-1009,0,0.181746,"C Model TRG monolingual Figure 1: Overview of back-translation BLEU-based synthetic corpus filtering back-translation BLEU-based filtering (Section 3.2). weights for each feature, we used newstest 2017 as a development set and fixed the threshold as one standard deviation. • n-best re-ranking by a right-to-left translation model (Section 3.4). 3.2 One drawback of NMT is that it can only be trained with parallel data. Using synthetic corpora, which are pseudo-parallel corpora created by translating monolingual data with an existing NMT model, is one of the ways to make use of monolingual data (Sennrich et al., 2016a). We created a synthetic corpus by translating monolingual sentences with a target-to-source translation model and used it as additional parallel data. In our case, we trained a baseline NMT model with a provided parallel corpora3 and translated News Crawl 2017 to make a synthetic corpus. From here, we discuss these features and experimentally verify each one. 3.1 Synthetic Corpus Noisy Data Filtering This year, ParaCrawl and Common Crawl corpora, which were created by crawling parallel websites, were provided for training. Since these web-based corpora are large but noisy, it seems essentia"
W18-6421,P16-1162,0,0.364195,"C Model TRG monolingual Figure 1: Overview of back-translation BLEU-based synthetic corpus filtering back-translation BLEU-based filtering (Section 3.2). weights for each feature, we used newstest 2017 as a development set and fixed the threshold as one standard deviation. • n-best re-ranking by a right-to-left translation model (Section 3.4). 3.2 One drawback of NMT is that it can only be trained with parallel data. Using synthetic corpora, which are pseudo-parallel corpora created by translating monolingual data with an existing NMT model, is one of the ways to make use of monolingual data (Sennrich et al., 2016a). We created a synthetic corpus by translating monolingual sentences with a target-to-source translation model and used it as additional parallel data. In our case, we trained a baseline NMT model with a provided parallel corpora3 and translated News Crawl 2017 to make a synthetic corpus. From here, we discuss these features and experimentally verify each one. 3.1 Synthetic Corpus Noisy Data Filtering This year, ParaCrawl and Common Crawl corpora, which were created by crawling parallel websites, were provided for training. Since these web-based corpora are large but noisy, it seems essentia"
W19-2605,S17-2091,0,0.064957,"Missing"
W19-2605,N18-1202,0,0.0114985,"del We formulate the automatic extraction task as a BIO sequence tagging task. Specifically, given a sentence, the model tags each word as one of {O, B-POS, I-POS, B-NEG, I-NEG, B-NEU, I-NEU}, where a combination of BI tags represents a Positive (POS), Negative (NEG), and Neutral (NEU) technical term span. We use the BiLSTM-CRF model proposed by Lample et al. (2016) which was originally designed for the task of named entity recognition.5 Regarding word embedding, we use word2vec (Mikolov et al., 2013) embeddings trained on ACL Anthology Corpus (Aizawa et al., 2018) (henceforth, CL), and ELMo (Peters et al., 2018) embeddings trained on 1 Billion Word Benchmark (henceforth, EL). (4) In essence, ranking models directly capture ::::::::::::: during training the competition among potential antecedent candidates, instead of considering them independently. (D08-1069) We found a large number of cases where sentences took the form of concession. In Example (5), one annotator labeled the pairwise approach as Negative and the other Neutral. We speculate that annotators were confused because the pairwise approach is evaluated positively by the phrase high precision in the subordinate clause, but negatively by the"
W19-2605,W16-3002,0,0.0661037,"Missing"
W19-2605,S15-2082,0,0.0611279,"Missing"
W19-2605,S18-1111,0,0.0446898,"Missing"
W19-2605,E12-2021,0,0.129566,"Missing"
W19-2605,P11-1016,0,0.414103,"pproaches such as BioNLP (Del´eger et al., 2016). These technologies are the foundation of scientific search engines or knowledge discovery tools, such as Semantic Scholar2 and Dr. Inventor (Ronzano and Saggion, 2015). Nevertheless, less attention has been paid to the mining of the pros and cons of technologies. This study performs a preliminary investigation on automatically identifying technologies and their pros/cons from computer science papers (henceforth referred to as pros/cons identification). We frame pros/cons identification as the well-known NLP task of targeted sentiment analysis (Jiang et al., 2011) and conduct an annotation study. Futhermore, we build a neural baseline model to identify the challenges of pros/cons identification task. The annotation study indicates that the pros/cons identification task can be reasonably framed as the task of targeted sentiment analysis. The experimental results of automatic extraction show that pros/cons identification is difficult mainly owing to the requirement of domainspecific knowledge. The annotated dataset is made Introduction The number of scientific publications has been rapidly increasing. Johnson et al. (2018) showed that over 3 million rese"
W19-2605,N16-1030,0,0.0352673,"ositive and the other labeled them as Neutral. To judge the sentiment attributes correctly, one required the domain knowledge of coreference resolution that directly capturing the competition among potential antecedent candidates is appropriate. 4.2 Model We formulate the automatic extraction task as a BIO sequence tagging task. Specifically, given a sentence, the model tags each word as one of {O, B-POS, I-POS, B-NEG, I-NEG, B-NEU, I-NEU}, where a combination of BI tags represents a Positive (POS), Negative (NEG), and Neutral (NEU) technical term span. We use the BiLSTM-CRF model proposed by Lample et al. (2016) which was originally designed for the task of named entity recognition.5 Regarding word embedding, we use word2vec (Mikolov et al., 2013) embeddings trained on ACL Anthology Corpus (Aizawa et al., 2018) (henceforth, CL), and ELMo (Peters et al., 2018) embeddings trained on 1 Billion Word Benchmark (henceforth, EL). (4) In essence, ranking models directly capture ::::::::::::: during training the competition among potential antecedent candidates, instead of considering them independently. (D08-1069) We found a large number of cases where sentences took the form of concession. In Example (5), o"
W19-4418,W19-4406,0,0.0536973,"veloped by Edunov et al. (2018). A target-tosource model is trained, and back-translation is applied to monolingual data to generate pseudoparallel data via sampling from the distribution of the target-to-source model. 4 4.1.2 For training our transformer-based GEC system, we used the BEA-2019 workshop official data: the First Certificate in English corpus (FCE) (Yannakoudakis et al., 2011), the Lang8 Corpus of Learner English (Lang-8) (Mizumoto et al., 2011; Tajiri et al., 2012), the National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013), and W&I+LOCNESS (Bryant et al., 2019; Granger, 1998). Our pre-processing for training data is the same as that reported previously (Chollampatt and Ng, 2018). As the result, we obtained 564,565 sentence pairs. In generating erroneous sentences, we used Simple Wikipedia and essay scoring data sets (i.e., International Corpus of Learner English (Granger et al., 2009), and International Corpus Network of Asian Learners of English (Ishikawa, 2013), the Automated Student Assessment Prize dataset3 , ETS Corpus of Non-Native English (TOEFL Experiment 4.1 Experimental Setting We will now describe the training data and tools used to trai"
W19-4418,P17-1074,0,0.0885058,"llampatt and Ng (2018), both source and target embeddings are of 500 dimensions. Each of the source and target vocabularies comprises the 30K most frequent BPE tokens. The hidden size of encoders and decoders is 1,024 with a convolution window width of 3. The output of each encoder and decoder layer is 1,024 dimensions. We set the dropout rate to 0.3. The parameters are optimized using the Nesterov Accelerated Gradient (Sutskever et al., 2013) optimizer with a momentum value of 0.99. We set the initial learning rate to 0.25, using early stopping. For evaluating the system outputs, the ERRANT (Bryant et al., 2017) is used as a scorer. In this study, all the results shown are “span-based correction F0.5”. 2. Fine-tuning SED model: After dividing the given text by proficiency based on the label estimated by the PPM, the SED model is fine-tuned for each level of proficiency. Then, the SED module performs sentence-level binary classification of sentences that need editing. Table 1 shows the performance of SED on our dev set. Here, we split the official development set into test/dev set for our experiments. Our proposed SED model achieved a significant improvement both in precision and recall, by considerin"
W19-4418,W18-2707,0,0.020728,"simple problem setting because there is no need to identify the location of errors. CoLA (Warstadt et al., 2018) and SST-2 (Socher et al., 2013). For setting up a training set for the base SED model, we preprocessed it to obtain binary labeled data (e.g., 0 for correct and 1 for incorrect, respectively). 2.2 3.2.3 Proposed Model Figure 1 shows the architecture of our proposed SED model. The key ideas of our proposed model are as follows: Error Generation In the field of machine translation, backtranslation is an effective method for neural machine translation systems (Sennrich et al., 2016b; Imamura et al., 2018). Edunov et al. (2018) reported that back-translation obtained via sampling or noised beam outputs is effective for neural machine translation systems. Recently, back-translation has been applied to grammatical error detection and correction. Rei et al. (2017) proposed artificial error generation with statistical machine translation and syntactic patterns for error detection. Kasewa et al. (2018) constructed synthetic samples using a seq2seq neural model with greedy search and temperature sampling for error detection. Xie et al. (2018) proposed certain noising methods for error generation, and"
W19-4418,C12-1038,0,0.0262817,"rected the input sentences by detecting errors using a sentence-level error detection model (which we denote as SED). We applied the bidirectional encoder representations from transformers (BERT) model (Devlin et al., 2018) for sentence-level error detection. In order to improve the performance of SED, we propose an SED model taking the learner’s proficiency into 2 2.1 Related Work Error Detection The field of grammatical error detection (GED) has a long history. Many previous studies have treated GED as a token-level binary classification task (Tetreault and Chodorow, 2008; Han et al., 2006; Chodorow et al., 2012; Rei and Yannakoudakis, 2016; Rei et al., 2016; Rei, 2017). Kaneko et al. (2017) improved grammatical error detection by learning word embeddings that consider grammaticality and error patterns. Yannakoudakis et al. (2017) propose an approach to N-best list re-ranking using neural sequencelabelling models. While many studies in GED focus on tokenlevel error detection, there are studies that perform sentence-level binary classification of sentences that need some editing (Han et al., 2006; Tetreault and Chodorow, 2008; Chodorow et al., 2012; ∗ Current affiliation: Yahoo Japan Corporation, hiro"
W19-4418,D16-1161,0,0.0748766,"Missing"
W19-4418,W13-1703,0,0.119123,"or generation system follows the system developed by Edunov et al. (2018). A target-tosource model is trained, and back-translation is applied to monolingual data to generate pseudoparallel data via sampling from the distribution of the target-to-source model. 4 4.1.2 For training our transformer-based GEC system, we used the BEA-2019 workshop official data: the First Certificate in English corpus (FCE) (Yannakoudakis et al., 2011), the Lang8 Corpus of Learner English (Lang-8) (Mizumoto et al., 2011; Tajiri et al., 2012), the National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013), and W&I+LOCNESS (Bryant et al., 2019; Granger, 1998). Our pre-processing for training data is the same as that reported previously (Chollampatt and Ng, 2018). As the result, we obtained 564,565 sentence pairs. In generating erroneous sentences, we used Simple Wikipedia and essay scoring data sets (i.e., International Corpus of Learner English (Granger et al., 2009), and International Corpus Network of Asian Learners of English (Ishikawa, 2013), the Automated Student Assessment Prize dataset3 , ETS Corpus of Non-Native English (TOEFL Experiment 4.1 Experimental Setting We will now describe th"
W19-4418,N18-1055,0,0.134824,"Missing"
W19-4418,D18-1045,0,0.151438,"hoku.ac.jp, masato.mita@riken.jp tomoya.mizumoto@riken.jp, jun.suzuki@ecei.tohoku.ac.jp Abstract account. To the best of our knowledge, this is the first study that has combined GEC with SED. Because grammatical correctness is required for output sentences in GEC, the target side of parallel training corpora should not contain noisy sentences. Our correction model is trained to correct sentence pairs, which were identified by our sentence-level grammatical error detection model. We call this data cleaning process BERT-Cleaning. For Track 1, similar to back-translation (Sennrich et al., 2016b; Edunov et al., 2018), we augmented the parallel training corpus with errors generated from monolingual data. After addition of the generated data and SED process, the F0.5 score on the base model improved. For Track 2, we used the EF-Cambridge Open Language Database (EFCAMDAT) (Geertzen et al., 2013) and non-public Lang-8 as the external language learner corpus. We introduce the AIP-Tohoku grammatical error correction (GEC) system for the BEA2019 shared task in Track 1 (Restricted Track) and Track 2 (Unrestricted Track) using the same system architecture. Our system comprises two key components: error generation"
W19-4418,I17-1005,0,0.0861052,"ion model (which we denote as SED). We applied the bidirectional encoder representations from transformers (BERT) model (Devlin et al., 2018) for sentence-level error detection. In order to improve the performance of SED, we propose an SED model taking the learner’s proficiency into 2 2.1 Related Work Error Detection The field of grammatical error detection (GED) has a long history. Many previous studies have treated GED as a token-level binary classification task (Tetreault and Chodorow, 2008; Han et al., 2006; Chodorow et al., 2012; Rei and Yannakoudakis, 2016; Rei et al., 2016; Rei, 2017). Kaneko et al. (2017) improved grammatical error detection by learning word embeddings that consider grammaticality and error patterns. Yannakoudakis et al. (2017) propose an approach to N-best list re-ranking using neural sequencelabelling models. While many studies in GED focus on tokenlevel error detection, there are studies that perform sentence-level binary classification of sentences that need some editing (Han et al., 2006; Tetreault and Chodorow, 2008; Chodorow et al., 2012; ∗ Current affiliation: Yahoo Japan Corporation, hiroasan@yahoo-corp.jp † Current affiliation: Future Corporation, mizumoto.tomoya.mh7"
W19-4418,P18-1097,0,0.197375,"Missing"
W19-4418,D18-1541,0,0.0118745,"deas of our proposed model are as follows: Error Generation In the field of machine translation, backtranslation is an effective method for neural machine translation systems (Sennrich et al., 2016b; Imamura et al., 2018). Edunov et al. (2018) reported that back-translation obtained via sampling or noised beam outputs is effective for neural machine translation systems. Recently, back-translation has been applied to grammatical error detection and correction. Rei et al. (2017) proposed artificial error generation with statistical machine translation and syntactic patterns for error detection. Kasewa et al. (2018) constructed synthetic samples using a seq2seq neural model with greedy search and temperature sampling for error detection. Xie et al. (2018) proposed certain noising methods for error generation, and Ge et al. (2018) proposed back-boost learning using fluency scores. 3 • There is a correlation between the error rate and the learner’s level of proficiency. • The performance of SED can be improved by fine-tuning the model according to the learners proficiency. The first idea is based on the following observation on the W&I+LOCNESS development set: Looking at the word error rate (WER) across th"
W19-4418,P16-1009,0,0.577205,"ce Project asano@ecei.tohoku.ac.jp, masato.mita@riken.jp tomoya.mizumoto@riken.jp, jun.suzuki@ecei.tohoku.ac.jp Abstract account. To the best of our knowledge, this is the first study that has combined GEC with SED. Because grammatical correctness is required for output sentences in GEC, the target side of parallel training corpora should not contain noisy sentences. Our correction model is trained to correct sentence pairs, which were identified by our sentence-level grammatical error detection model. We call this data cleaning process BERT-Cleaning. For Track 1, similar to back-translation (Sennrich et al., 2016b; Edunov et al., 2018), we augmented the parallel training corpus with errors generated from monolingual data. After addition of the generated data and SED process, the F0.5 score on the base model improved. For Track 2, we used the EF-Cambridge Open Language Database (EFCAMDAT) (Geertzen et al., 2013) and non-public Lang-8 as the external language learner corpus. We introduce the AIP-Tohoku grammatical error correction (GEC) system for the BEA2019 shared task in Track 1 (Restricted Track) and Track 2 (Unrestricted Track) using the same system architecture. Our system comprises two key compon"
W19-4418,C18-2018,0,0.0198804,"(Blanchard et al., 2013). With respect to Simple Wikipedia, we ignored sentences that were longer than 60 tokens. To remove erroneous sentences, we applied BERT-Cleaning to the essay scoring data sets. After BERT-Cleaning and preprocessing (Chollampatt and Ng, 2018), we obtained 1,426,354 sentence pairs by error generation. 4.1.3 Prec. 68.62 70.60 External Dataset for Track-2 We used EFCAMDAT (Geertzen et al., 2013) and non-public Lang-8 as the external language learner corpus. The EFCAMDAT is constructed by the Department of Theoretical and Applied Linguistics at the University of Cambridge. Lo et al. (2018) were the first the researchers to use the EFCAMDAT for the GEC task. However, the system trained with the EFCAMDAT gave lower performance than the system trained with the Lang-8 Corpus. One of the causes of the lower performance is that many errors are found in the EFCAMDAT corrected sentences. Thus, we applied BERT-Cleaning to the EFCAMDAT to remove the erroneous sentences. Consequently, the number of sentence pairs of EFCAMDAT was reduced from 1,157,339 to 760,393. Finally, we used 7,739,577 sentence pairs (non-public Lang-8 + Cleand EFCAMDAT) by using pre-processing (Chollampatt and Ng, 20"
W19-4418,D13-1170,0,0.0043786,"Current affiliation: Yahoo Japan Corporation, hiroasan@yahoo-corp.jp † Current affiliation: Future Corporation, mizumoto.tomoya.mh7@is.naist.jp 176 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 176–182 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics Schmaltz et al., 2016). Compared with tokenlevel grammatical error correction, sentence-level grammatical error correction is a simple problem setting because there is no need to identify the location of errors. CoLA (Warstadt et al., 2018) and SST-2 (Socher et al., 2013). For setting up a training set for the base SED model, we preprocessed it to obtain binary labeled data (e.g., 0 for correct and 1 for incorrect, respectively). 2.2 3.2.3 Proposed Model Figure 1 shows the architecture of our proposed SED model. The key ideas of our proposed model are as follows: Error Generation In the field of machine translation, backtranslation is an effective method for neural machine translation systems (Sennrich et al., 2016b; Imamura et al., 2018). Edunov et al. (2018) reported that back-translation obtained via sampling or noised beam outputs is effective for neural m"
W19-4418,I11-1017,1,0.881058,"1.3 Rec. 79.8 95.6 F 83.9 93.4 Table 1: Performance of sent-level error detection (SED). 3.3 Error Generation Our error generation system follows the system developed by Edunov et al. (2018). A target-tosource model is trained, and back-translation is applied to monolingual data to generate pseudoparallel data via sampling from the distribution of the target-to-source model. 4 4.1.2 For training our transformer-based GEC system, we used the BEA-2019 workshop official data: the First Certificate in English corpus (FCE) (Yannakoudakis et al., 2011), the Lang8 Corpus of Learner English (Lang-8) (Mizumoto et al., 2011; Tajiri et al., 2012), the National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013), and W&I+LOCNESS (Bryant et al., 2019; Granger, 1998). Our pre-processing for training data is the same as that reported previously (Chollampatt and Ng, 2018). As the result, we obtained 564,565 sentence pairs. In generating erroneous sentences, we used Simple Wikipedia and essay scoring data sets (i.e., International Corpus of Learner English (Granger et al., 2009), and International Corpus Network of Asian Learners of English (Ishikawa, 2013), the Automated Student Assessme"
W19-4418,N19-4009,0,0.0302698,"reported previously (Chollampatt and Ng, 2018). As the result, we obtained 564,565 sentence pairs. In generating erroneous sentences, we used Simple Wikipedia and essay scoring data sets (i.e., International Corpus of Learner English (Granger et al., 2009), and International Corpus Network of Asian Learners of English (Ishikawa, 2013), the Automated Student Assessment Prize dataset3 , ETS Corpus of Non-Native English (TOEFL Experiment 4.1 Experimental Setting We will now describe the training data and tools used to train our model. 4.1.1 Tools We used the Transformer implemented in Fairseq1 (Ott et al., 2019) as our GEC model. For the Transformer, we used a token embedding size of dimension 512. The hidden size is set to 512, and the filter size is set to 2048. The multi-head attention has eight individual attention heads, whereas the encoder and decoder have six layers. We use Adam optimizer with β1 = 0.9, β2 = 0.98, and  = 10−9 . We use inverse squared root decay. We set the dropout to 0.3. Rather than using words directly, we used byte pair encoding (BPE) (Sennrich et al., 2016a), and each 1 Dataset for Track-1 2 https://github.com/huggingface/ pytorch-pretrained-BERT 3 https://www.kaggle.com/"
W19-4418,P12-2039,0,0.374648,".9 93.4 Table 1: Performance of sent-level error detection (SED). 3.3 Error Generation Our error generation system follows the system developed by Edunov et al. (2018). A target-tosource model is trained, and back-translation is applied to monolingual data to generate pseudoparallel data via sampling from the distribution of the target-to-source model. 4 4.1.2 For training our transformer-based GEC system, we used the BEA-2019 workshop official data: the First Certificate in English corpus (FCE) (Yannakoudakis et al., 2011), the Lang8 Corpus of Learner English (Lang-8) (Mizumoto et al., 2011; Tajiri et al., 2012), the National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013), and W&I+LOCNESS (Bryant et al., 2019; Granger, 1998). Our pre-processing for training data is the same as that reported previously (Chollampatt and Ng, 2018). As the result, we obtained 564,565 sentence pairs. In generating erroneous sentences, we used Simple Wikipedia and essay scoring data sets (i.e., International Corpus of Learner English (Granger et al., 2009), and International Corpus Network of Asian Learners of English (Ishikawa, 2013), the Automated Student Assessment Prize dataset3 , ET"
W19-4418,C08-1109,0,0.0550841,"precision score on the test set, our system corrected the input sentences by detecting errors using a sentence-level error detection model (which we denote as SED). We applied the bidirectional encoder representations from transformers (BERT) model (Devlin et al., 2018) for sentence-level error detection. In order to improve the performance of SED, we propose an SED model taking the learner’s proficiency into 2 2.1 Related Work Error Detection The field of grammatical error detection (GED) has a long history. Many previous studies have treated GED as a token-level binary classification task (Tetreault and Chodorow, 2008; Han et al., 2006; Chodorow et al., 2012; Rei and Yannakoudakis, 2016; Rei et al., 2016; Rei, 2017). Kaneko et al. (2017) improved grammatical error detection by learning word embeddings that consider grammaticality and error patterns. Yannakoudakis et al. (2017) propose an approach to N-best list re-ranking using neural sequencelabelling models. While many studies in GED focus on tokenlevel error detection, there are studies that perform sentence-level binary classification of sentences that need some editing (Han et al., 2006; Tetreault and Chodorow, 2008; Chodorow et al., 2012; ∗ Current a"
W19-4418,P17-1194,0,0.0204367,"error detection model (which we denote as SED). We applied the bidirectional encoder representations from transformers (BERT) model (Devlin et al., 2018) for sentence-level error detection. In order to improve the performance of SED, we propose an SED model taking the learner’s proficiency into 2 2.1 Related Work Error Detection The field of grammatical error detection (GED) has a long history. Many previous studies have treated GED as a token-level binary classification task (Tetreault and Chodorow, 2008; Han et al., 2006; Chodorow et al., 2012; Rei and Yannakoudakis, 2016; Rei et al., 2016; Rei, 2017). Kaneko et al. (2017) improved grammatical error detection by learning word embeddings that consider grammaticality and error patterns. Yannakoudakis et al. (2017) propose an approach to N-best list re-ranking using neural sequencelabelling models. While many studies in GED focus on tokenlevel error detection, there are studies that perform sentence-level binary classification of sentences that need some editing (Han et al., 2006; Tetreault and Chodorow, 2008; Chodorow et al., 2012; ∗ Current affiliation: Yahoo Japan Corporation, hiroasan@yahoo-corp.jp † Current affiliation: Future Corporatio"
W19-4418,C16-1030,0,0.0237581,"a sentence-level error detection model (which we denote as SED). We applied the bidirectional encoder representations from transformers (BERT) model (Devlin et al., 2018) for sentence-level error detection. In order to improve the performance of SED, we propose an SED model taking the learner’s proficiency into 2 2.1 Related Work Error Detection The field of grammatical error detection (GED) has a long history. Many previous studies have treated GED as a token-level binary classification task (Tetreault and Chodorow, 2008; Han et al., 2006; Chodorow et al., 2012; Rei and Yannakoudakis, 2016; Rei et al., 2016; Rei, 2017). Kaneko et al. (2017) improved grammatical error detection by learning word embeddings that consider grammaticality and error patterns. Yannakoudakis et al. (2017) propose an approach to N-best list re-ranking using neural sequencelabelling models. While many studies in GED focus on tokenlevel error detection, there are studies that perform sentence-level binary classification of sentences that need some editing (Han et al., 2006; Tetreault and Chodorow, 2008; Chodorow et al., 2012; ∗ Current affiliation: Yahoo Japan Corporation, hiroasan@yahoo-corp.jp † Current affiliation: Futur"
W19-4418,W17-5032,0,0.0189476,"orrect and 1 for incorrect, respectively). 2.2 3.2.3 Proposed Model Figure 1 shows the architecture of our proposed SED model. The key ideas of our proposed model are as follows: Error Generation In the field of machine translation, backtranslation is an effective method for neural machine translation systems (Sennrich et al., 2016b; Imamura et al., 2018). Edunov et al. (2018) reported that back-translation obtained via sampling or noised beam outputs is effective for neural machine translation systems. Recently, back-translation has been applied to grammatical error detection and correction. Rei et al. (2017) proposed artificial error generation with statistical machine translation and syntactic patterns for error detection. Kasewa et al. (2018) constructed synthetic samples using a seq2seq neural model with greedy search and temperature sampling for error detection. Xie et al. (2018) proposed certain noising methods for error generation, and Ge et al. (2018) proposed back-boost learning using fluency scores. 3 • There is a correlation between the error rate and the learner’s level of proficiency. • The performance of SED can be improved by fine-tuning the model according to the learners proficien"
W19-4418,N18-1057,0,0.103303,"Missing"
W19-4418,P16-1112,0,0.0413515,"ces by detecting errors using a sentence-level error detection model (which we denote as SED). We applied the bidirectional encoder representations from transformers (BERT) model (Devlin et al., 2018) for sentence-level error detection. In order to improve the performance of SED, we propose an SED model taking the learner’s proficiency into 2 2.1 Related Work Error Detection The field of grammatical error detection (GED) has a long history. Many previous studies have treated GED as a token-level binary classification task (Tetreault and Chodorow, 2008; Han et al., 2006; Chodorow et al., 2012; Rei and Yannakoudakis, 2016; Rei et al., 2016; Rei, 2017). Kaneko et al. (2017) improved grammatical error detection by learning word embeddings that consider grammaticality and error patterns. Yannakoudakis et al. (2017) propose an approach to N-best list re-ranking using neural sequencelabelling models. While many studies in GED focus on tokenlevel error detection, there are studies that perform sentence-level binary classification of sentences that need some editing (Han et al., 2006; Tetreault and Chodorow, 2008; Chodorow et al., 2012; ∗ Current affiliation: Yahoo Japan Corporation, hiroasan@yahoo-corp.jp † Current"
W19-4418,P11-1019,0,0.217563,"by considering learner proficiency. Base Model Proposed Model Prec. 88.5 91.3 Rec. 79.8 95.6 F 83.9 93.4 Table 1: Performance of sent-level error detection (SED). 3.3 Error Generation Our error generation system follows the system developed by Edunov et al. (2018). A target-tosource model is trained, and back-translation is applied to monolingual data to generate pseudoparallel data via sampling from the distribution of the target-to-source model. 4 4.1.2 For training our transformer-based GEC system, we used the BEA-2019 workshop official data: the First Certificate in English corpus (FCE) (Yannakoudakis et al., 2011), the Lang8 Corpus of Learner English (Lang-8) (Mizumoto et al., 2011; Tajiri et al., 2012), the National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013), and W&I+LOCNESS (Bryant et al., 2019; Granger, 1998). Our pre-processing for training data is the same as that reported previously (Chollampatt and Ng, 2018). As the result, we obtained 564,565 sentence pairs. In generating erroneous sentences, we used Simple Wikipedia and essay scoring data sets (i.e., International Corpus of Learner English (Granger et al., 2009), and International Corpus Network of Asian"
W19-4418,W16-0528,0,0.0148253,"ing models. While many studies in GED focus on tokenlevel error detection, there are studies that perform sentence-level binary classification of sentences that need some editing (Han et al., 2006; Tetreault and Chodorow, 2008; Chodorow et al., 2012; ∗ Current affiliation: Yahoo Japan Corporation, hiroasan@yahoo-corp.jp † Current affiliation: Future Corporation, mizumoto.tomoya.mh7@is.naist.jp 176 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 176–182 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics Schmaltz et al., 2016). Compared with tokenlevel grammatical error correction, sentence-level grammatical error correction is a simple problem setting because there is no need to identify the location of errors. CoLA (Warstadt et al., 2018) and SST-2 (Socher et al., 2013). For setting up a training set for the base SED model, we preprocessed it to obtain binary labeled data (e.g., 0 for correct and 1 for incorrect, respectively). 2.2 3.2.3 Proposed Model Figure 1 shows the architecture of our proposed SED model. The key ideas of our proposed model are as follows: Error Generation In the field of machine translation"
W19-4418,D17-1297,0,0.0187723,"2018) for sentence-level error detection. In order to improve the performance of SED, we propose an SED model taking the learner’s proficiency into 2 2.1 Related Work Error Detection The field of grammatical error detection (GED) has a long history. Many previous studies have treated GED as a token-level binary classification task (Tetreault and Chodorow, 2008; Han et al., 2006; Chodorow et al., 2012; Rei and Yannakoudakis, 2016; Rei et al., 2016; Rei, 2017). Kaneko et al. (2017) improved grammatical error detection by learning word embeddings that consider grammaticality and error patterns. Yannakoudakis et al. (2017) propose an approach to N-best list re-ranking using neural sequencelabelling models. While many studies in GED focus on tokenlevel error detection, there are studies that perform sentence-level binary classification of sentences that need some editing (Han et al., 2006; Tetreault and Chodorow, 2008; Chodorow et al., 2012; ∗ Current affiliation: Yahoo Japan Corporation, hiroasan@yahoo-corp.jp † Current affiliation: Future Corporation, mizumoto.tomoya.mh7@is.naist.jp 176 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 176–182 c Floren"
W19-4418,P16-1162,0,0.499417,"ce Project asano@ecei.tohoku.ac.jp, masato.mita@riken.jp tomoya.mizumoto@riken.jp, jun.suzuki@ecei.tohoku.ac.jp Abstract account. To the best of our knowledge, this is the first study that has combined GEC with SED. Because grammatical correctness is required for output sentences in GEC, the target side of parallel training corpora should not contain noisy sentences. Our correction model is trained to correct sentence pairs, which were identified by our sentence-level grammatical error detection model. We call this data cleaning process BERT-Cleaning. For Track 1, similar to back-translation (Sennrich et al., 2016b; Edunov et al., 2018), we augmented the parallel training corpus with errors generated from monolingual data. After addition of the generated data and SED process, the F0.5 score on the base model improved. For Track 2, we used the EF-Cambridge Open Language Database (EFCAMDAT) (Geertzen et al., 2013) and non-public Lang-8 as the external language learner corpus. We introduce the AIP-Tohoku grammatical error correction (GEC) system for the BEA2019 shared task in Track 1 (Restricted Track) and Track 2 (Unrestricted Track) using the same system architecture. Our system comprises two key compon"
W19-4418,N19-1014,0,0.196493,"Missing"
W19-8606,D15-1075,0,0.0754827,"Missing"
W19-8606,P12-2049,0,0.0255439,"ompletion The drafts in the revising stage may contain gaps denoted with &lt;*>. This setting is similar to text infilling (Zhu et al., 2019), masking-based language modeling (Fedus et al., 2018; Devlin et al., 2019), or the sentence completion task (Zweig et al., 2012), where the models are required to replace mask tokens with plausible words. Notably, SentRev differs from such tasks because systems for these tasks are expected to keep all the original tokens unchanged and only fill the &lt;*> token, with one or more other tokens. 8 Other corpora for revisions are available in the academic domain (Lee and Webster, 2012; Tan and Lee, 2014; Zhang et al., 2017). Thus, we provide a notable contribution by exploring the methods to create a dataset of revisions with a scalable crowdsourcing approach. By contrast, Zhang et al. (2017) recruited 60 students over 2 weeks and Lee and Webster (2012) collected data from a language learning project where over 300 tutors reviewed academic essays written by 4500 students. 7.2 Style transfer Conclusion and future work We proposed the SentRev task, where an incomplete, rough draft sentence is transformed into a more fluent, complete sentence in the academic writing domain. W"
W19-8606,P00-1037,0,0.405963,"ent perfomance good results in this task. in this task. proofreading final version Our model shows excellent performance in this task. Our model shows excellent performance in this task. Figure 1: Overview of the estimated process of writing a sentence Our model shows excellent performance in this task.. Writing activity consists of four stages: (i) drafting, (ii) revising, (iii) editing, and (iv) proofreading. thus a concern for both individual researchers and the academic community. Writing assistance technologies have been extensively studied in the natural language processing (NLP) field (Brill and Moore, 2000; Ng et al., 2014; Grangier and Auli, 2018). We focus on helping inexperienced authors in writing fluent grammatical sentences. Models developed for academic writing assistance using existing datasets can serve as a support system during the final stages by editing a nearly finished version of the draft. For example, Daudaravicius (2015) collects scientific papers before and after professional editing from publishing companies, and Dale and Kilgarriff (2011) extract already published papers that still contain errors and correct the errors to obtain target fragments of text. Introduction Academ"
W19-8606,N19-1333,0,0.0124588,"of posts . Style removal This experiment suggested that there were ambiguities in these pointing gestures and led to a redesign of the system . This experiment indicated the ambiguity found in the pointing gestures and caused a renewal of the system . Entailed sentence generation Figure 2 illustrates the effectiveness of different features class. There is different feature in figure 2 . Table 5: Examples of generated training dataset. baseline model for the SentRev task (henceforth, H-ND). have gained attention in the GEC and machine translation fields (Edunov et al., 2018; Xie et al., 2018; Lichtarge et al., 2019). We combined these two factors to train baseline models on noised final version sentences. 5.1.2 Enc-Dec noising and denoising model As an extension of the heuristic noising and denoising model, we changed the noising methods to better simulate the characteristics of X in S MITH than the heuristic rules in Section 5.1.1. As described in Section 4, the drafts tended to (i) contain grammatical errors, (ii) use stylistically improper wording, and (iii) lack certain words. We used the following three neural Encoder-Decoder (Enc-Dec) models to generate the synthetic draft sentences. First, we coll"
W19-8606,P17-1074,0,0.0218339,"from the set of accepted papers (reference sentences Y ). We randomly selected a set of 500 pairs from S MITH as the development set for analysis. Statistics 4.1 Table 3 shows the statistics of our S MITH dataset and a comparison with major datasets for building a writing assistance system (Napoles et al., 2017; Mizumoto et al., 2011; Daudaravicius, 2015). The size of our dataset (10k sentence pairs) is six times greater than that of JFLEG, which contains both Error type comparison To obtain the approximate distributions of error types between the source and target sentences, we used ERRANT (Bryant et al., 2017; Felice et al., 2016). Next, we compared them with three datasets: S MITH, AESW (the same domain as S MITH), and JFLEG (has a relatively close Levenshtein distance to S MITH). To calculate the er6 We corrected spelling errors using https: //github.com/barrust/pyspellchecker 43 Draft: the best models are very effective on the that they are far greater than human. condition OTHER Reference: The best models are very effective in the local context condition where they significantly outperform humans. Draft: Results show MARM tend to generate &lt;*> and very short responces. OTHER Reference: The resu"
W19-8606,I11-1017,0,0.024963,"compared with major GEC corpora. This finding implies that our dataset emulates more drastic rephrasing. 4 Analysis of the S MITH dataset In this section, we run extensive analyses on the sentences written by non-native workers (draft sentences X), and the original sentences extracted from the set of accepted papers (reference sentences Y ). We randomly selected a set of 500 pairs from S MITH as the development set for analysis. Statistics 4.1 Table 3 shows the statistics of our S MITH dataset and a comparison with major datasets for building a writing assistance system (Napoles et al., 2017; Mizumoto et al., 2011; Daudaravicius, 2015). The size of our dataset (10k sentence pairs) is six times greater than that of JFLEG, which contains both Error type comparison To obtain the approximate distributions of error types between the source and target sentences, we used ERRANT (Bryant et al., 2017; Felice et al., 2016). Next, we compared them with three datasets: S MITH, AESW (the same domain as S MITH), and JFLEG (has a relatively close Levenshtein distance to S MITH). To calculate the er6 We corrected spelling errors using https: //github.com/barrust/pyspellchecker 43 Draft: the best models are very effect"
W19-8606,D16-1228,0,0.0496988,"Missing"
W19-8606,W15-0607,0,0.315697,"stages: (i) drafting, (ii) revising, (iii) editing, and (iv) proofreading. thus a concern for both individual researchers and the academic community. Writing assistance technologies have been extensively studied in the natural language processing (NLP) field (Brill and Moore, 2000; Ng et al., 2014; Grangier and Auli, 2018). We focus on helping inexperienced authors in writing fluent grammatical sentences. Models developed for academic writing assistance using existing datasets can serve as a support system during the final stages by editing a nearly finished version of the draft. For example, Daudaravicius (2015) collects scientific papers before and after professional editing from publishing companies, and Dale and Kilgarriff (2011) extract already published papers that still contain errors and correct the errors to obtain target fragments of text. Introduction Academic writing can be a daunting task, even for experienced writers with a native or near-native command of English. Inexperienced, non-native speakers find themselves in an even more difficult situation—in addition to grammatical or spelling errors, their sentences may lack fluidity, have an awkward style, contain collocation errors, or hav"
W19-8606,E17-2037,0,0.0204427,"ferences between them compared with major GEC corpora. This finding implies that our dataset emulates more drastic rephrasing. 4 Analysis of the S MITH dataset In this section, we run extensive analyses on the sentences written by non-native workers (draft sentences X), and the original sentences extracted from the set of accepted papers (reference sentences Y ). We randomly selected a set of 500 pairs from S MITH as the development set for analysis. Statistics 4.1 Table 3 shows the statistics of our S MITH dataset and a comparison with major datasets for building a writing assistance system (Napoles et al., 2017; Mizumoto et al., 2011; Daudaravicius, 2015). The size of our dataset (10k sentence pairs) is six times greater than that of JFLEG, which contains both Error type comparison To obtain the approximate distributions of error types between the source and target sentences, we used ERRANT (Bryant et al., 2017; Felice et al., 2016). Next, we compared them with three datasets: S MITH, AESW (the same domain as S MITH), and JFLEG (has a relatively close Levenshtein distance to S MITH). To calculate the er6 We corrected spelling errors using https: //github.com/barrust/pyspellchecker 43 Draft: the best"
W19-8606,N19-1423,0,0.00930276,"ors, and the AESW dataset contains a collection of text extracts from published journal articles before and after proofreading. Rather than adding finishing touches to almost completed sentences, our task is to convert unfinished, rough drafts into complete sentences. In addition, these studies tackled the task of the identification of errors while SentRev goes further by rewriting the drafts. 7.4 Text completion The drafts in the revising stage may contain gaps denoted with &lt;*>. This setting is similar to text infilling (Zhu et al., 2019), masking-based language modeling (Fedus et al., 2018; Devlin et al., 2019), or the sentence completion task (Zweig et al., 2012), where the models are required to replace mask tokens with plausible words. Notably, SentRev differs from such tasks because systems for these tasks are expected to keep all the original tokens unchanged and only fill the &lt;*> token, with one or more other tokens. 8 Other corpora for revisions are available in the academic domain (Lee and Webster, 2012; Tan and Lee, 2014; Zhang et al., 2017). Thus, we provide a notable contribution by exploring the methods to create a dataset of revisions with a scalable crowdsourcing approach. By contrast,"
W19-8606,D18-1045,0,0.0205145,"of sentiments express in these sequence of posts . Style removal This experiment suggested that there were ambiguities in these pointing gestures and led to a redesign of the system . This experiment indicated the ambiguity found in the pointing gestures and caused a renewal of the system . Entailed sentence generation Figure 2 illustrates the effectiveness of different features class. There is different feature in figure 2 . Table 5: Examples of generated training dataset. baseline model for the SentRev task (henceforth, H-ND). have gained attention in the GEC and machine translation fields (Edunov et al., 2018; Xie et al., 2018; Lichtarge et al., 2019). We combined these two factors to train baseline models on noised final version sentences. 5.1.2 Enc-Dec noising and denoising model As an extension of the heuristic noising and denoising model, we changed the noising methods to better simulate the characteristics of X in S MITH than the heuristic rules in Section 5.1.1. As described in Section 4, the drafts tended to (i) contain grammatical errors, (ii) use stylistically improper wording, and (iii) lack certain words. We used the following three neural Encoder-Decoder (Enc-Dec) models to generate th"
W19-8606,N19-4009,0,0.0335665,"Missing"
W19-8606,P18-1080,0,0.030559,"th ERRANT. In addition, to handle the lexical and compositional diversity of valid revisions, we used BERT-score (Zhang et al., 2019), a contextualized embedding-based evaluation metric. Furthermore, we used two reference-less evaluation metrics: grammaticality score (Napoles et al., 2016) and PPL. Grammaticality was scored as 10 https://github.com/languagetool-org/ languagetool/releases/tag/v3.2 47 7 7.1 Related work 7.3 Style transfer is the task of rephrasing the text to conform to specific stylistic properties while preserving the text’s original semantic content (Logeswaran et al., 2018; Prabhumoye et al., 2018). From the perspective of automatic academic writing assistance, the assistance systems are required to convert nonacademic-style drafts into academic-style drafts. This type of transfer is regarded as a subproblem in the revising stage of the writing process. Writing assistance in the academic domain Several shared tasks for assisting academic writing have been organized. The Helping Our Own (HOO) 2011 Pilot Shared Task (Dale and Kilgarriff, 2011) aimed to promote the development of tools and techniques to assist authors in writing, with a specific focus on writing within the NLP community. T"
W19-8606,C16-1079,0,0.0192342,"pted papers (reference sentences Y ). We randomly selected a set of 500 pairs from S MITH as the development set for analysis. Statistics 4.1 Table 3 shows the statistics of our S MITH dataset and a comparison with major datasets for building a writing assistance system (Napoles et al., 2017; Mizumoto et al., 2011; Daudaravicius, 2015). The size of our dataset (10k sentence pairs) is six times greater than that of JFLEG, which contains both Error type comparison To obtain the approximate distributions of error types between the source and target sentences, we used ERRANT (Bryant et al., 2017; Felice et al., 2016). Next, we compared them with three datasets: S MITH, AESW (the same domain as S MITH), and JFLEG (has a relatively close Levenshtein distance to S MITH). To calculate the er6 We corrected spelling errors using https: //github.com/barrust/pyspellchecker 43 Draft: the best models are very effective on the that they are far greater than human. condition OTHER Reference: The best models are very effective in the local context condition where they significantly outperform humans. Draft: Results show MARM tend to generate &lt;*> and very short responces. OTHER Reference: The results indicate that MARM"
W19-8606,Q16-1013,0,0.0598999,"Missing"
W19-8606,N18-1025,0,0.0234959,". in this task. proofreading final version Our model shows excellent performance in this task. Our model shows excellent performance in this task. Figure 1: Overview of the estimated process of writing a sentence Our model shows excellent performance in this task.. Writing activity consists of four stages: (i) drafting, (ii) revising, (iii) editing, and (iv) proofreading. thus a concern for both individual researchers and the academic community. Writing assistance technologies have been extensively studied in the natural language processing (NLP) field (Brill and Moore, 2000; Ng et al., 2014; Grangier and Auli, 2018). We focus on helping inexperienced authors in writing fluent grammatical sentences. Models developed for academic writing assistance using existing datasets can serve as a support system during the final stages by editing a nearly finished version of the draft. For example, Daudaravicius (2015) collects scientific papers before and after professional editing from publishing companies, and Dale and Kilgarriff (2011) extract already published papers that still contain errors and correct the errors to obtain target fragments of text. Introduction Academic writing can be a daunting task, even for"
W19-8606,P17-4015,0,0.0217931,"Missing"
W19-8606,P14-2066,0,0.0171537,"the revising stage may contain gaps denoted with &lt;*>. This setting is similar to text infilling (Zhu et al., 2019), masking-based language modeling (Fedus et al., 2018; Devlin et al., 2019), or the sentence completion task (Zweig et al., 2012), where the models are required to replace mask tokens with plausible words. Notably, SentRev differs from such tasks because systems for these tasks are expected to keep all the original tokens unchanged and only fill the &lt;*> token, with one or more other tokens. 8 Other corpora for revisions are available in the academic domain (Lee and Webster, 2012; Tan and Lee, 2014; Zhang et al., 2017). Thus, we provide a notable contribution by exploring the methods to create a dataset of revisions with a scalable crowdsourcing approach. By contrast, Zhang et al. (2017) recruited 60 students over 2 weeks and Lee and Webster (2012) collected data from a language learning project where over 300 tutors reviewed academic essays written by 4500 students. 7.2 Style transfer Conclusion and future work We proposed the SentRev task, where an incomplete, rough draft sentence is transformed into a more fluent, complete sentence in the academic writing domain. We created the S MIT"
W19-8606,P18-1042,0,0.0595374,"Missing"
W19-8606,N18-1101,0,0.0606623,"Missing"
W19-8606,N18-1057,0,0.0648811,"in these sequence of posts . Style removal This experiment suggested that there were ambiguities in these pointing gestures and led to a redesign of the system . This experiment indicated the ambiguity found in the pointing gestures and caused a renewal of the system . Entailed sentence generation Figure 2 illustrates the effectiveness of different features class. There is different feature in figure 2 . Table 5: Examples of generated training dataset. baseline model for the SentRev task (henceforth, H-ND). have gained attention in the GEC and machine translation fields (Edunov et al., 2018; Xie et al., 2018; Lichtarge et al., 2019). We combined these two factors to train baseline models on noised final version sentences. 5.1.2 Enc-Dec noising and denoising model As an extension of the heuristic noising and denoising model, we changed the noising methods to better simulate the characteristics of X in S MITH than the heuristic rules in Section 5.1.1. As described in Section 4, the drafts tended to (i) contain grammatical errors, (ii) use stylistically improper wording, and (iii) lack certain words. We used the following three neural Encoder-Decoder (Enc-Dec) models to generate the synthetic draft"
W19-8606,N16-1042,0,0.0139549,"t. We believe that this task can increase the effectiveness of the process of academic writing. In future work, we plan to improve the information gap-filling aspect of revision by considering the surrounding context of target sentences. In addition, to develop a more holistic writing assistance tool, we plan to extend our system to be able to suggest diverse correction candidates, provide interactive assistance, and integrate translation systems. Grammatical error correction GEC is the task of correcting errors in text such as spelling, punctuation, grammar, and word choice (Ng et al., 2014; Yuan and Briscoe, 2016). GEC falls within the editing and proofreading phases of the writing process, while SentRev subsumes GEC and a broader range of text generation (e.g., increasing the fluency of the sentence and complementing missing information). Napoles et al. (2017) and Sakaguchi et al. (2016) explored fluency edits to correct grammatical errors and to make a text more “native sounding.” Although this direction is similar to SentRev, our task used sentences that required many more corrections. 9 Acknowledgements We thank the Tohoku NLP laboratory members who provided us with their valuable advice. We are gr"
W19-8606,P17-1144,0,0.0253616,"may contain gaps denoted with &lt;*>. This setting is similar to text infilling (Zhu et al., 2019), masking-based language modeling (Fedus et al., 2018; Devlin et al., 2019), or the sentence completion task (Zweig et al., 2012), where the models are required to replace mask tokens with plausible words. Notably, SentRev differs from such tasks because systems for these tasks are expected to keep all the original tokens unchanged and only fill the &lt;*> token, with one or more other tokens. 8 Other corpora for revisions are available in the academic domain (Lee and Webster, 2012; Tan and Lee, 2014; Zhang et al., 2017). Thus, we provide a notable contribution by exploring the methods to create a dataset of revisions with a scalable crowdsourcing approach. By contrast, Zhang et al. (2017) recruited 60 students over 2 weeks and Lee and Webster (2012) collected data from a language learning project where over 300 tutors reviewed academic essays written by 4500 students. 7.2 Style transfer Conclusion and future work We proposed the SentRev task, where an incomplete, rough draft sentence is transformed into a more fluent, complete sentence in the academic writing domain. We created the S MITH dataset with crowds"
W19-8606,N19-1014,0,0.0516118,"eline model by error types. The results are shown in Figure 7. Overall, typical grammatical errors such as noun number errors or orthographic errors are well corrected, but the model struggles with drastic revisions (“OTHER” type errors). 0.3 0.2 0.1 AD J H OR T SP EL L B P CT PU N VE R M PR E :N U NO UN DE T NO UN OT HE R 0 Figure 7: Performance of the ED-ND baseline model on top 10 most error types in SMITH. sentence generation model caused a lack of information. 5.1.3 GEC model The GEC task is closely related to SentRev. We examined the performance of the current state-ofthe-art GEC model (Zhao et al., 2019) in our task. We applied spelling correction before evaluation following Zhao et al. (2019). 5.2 Evaluation metrics The SentRev task has a very diverse space of valid revisions to a given context, which is challenging to evaluate. As one solution, we evaluated the performance from multiple aspects by using various reference and reference-less evaluation metrics. We used BLEU, ROUGE-L, and F0.5 score, which are widely used metrics in related tasks (machine translation, style-transfer, GEC). We used nlgeval (Sharma et al., 2017) to compute the BLEU and ROUGE-L scores and calculated F0.5 scores w"
W19-8606,P12-1063,0,0.0329555,"extracts from published journal articles before and after proofreading. Rather than adding finishing touches to almost completed sentences, our task is to convert unfinished, rough drafts into complete sentences. In addition, these studies tackled the task of the identification of errors while SentRev goes further by rewriting the drafts. 7.4 Text completion The drafts in the revising stage may contain gaps denoted with &lt;*>. This setting is similar to text infilling (Zhu et al., 2019), masking-based language modeling (Fedus et al., 2018; Devlin et al., 2019), or the sentence completion task (Zweig et al., 2012), where the models are required to replace mask tokens with plausible words. Notably, SentRev differs from such tasks because systems for these tasks are expected to keep all the original tokens unchanged and only fill the &lt;*> token, with one or more other tokens. 8 Other corpora for revisions are available in the academic domain (Lee and Webster, 2012; Tan and Lee, 2014; Zhang et al., 2017). Thus, we provide a notable contribution by exploring the methods to create a dataset of revisions with a scalable crowdsourcing approach. By contrast, Zhang et al. (2017) recruited 60 students over 2 week"
W19-8606,W14-1701,0,\N,Missing
W19-8606,W11-2838,0,\N,Missing
