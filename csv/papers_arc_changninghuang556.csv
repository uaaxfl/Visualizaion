Y15-1001,Two-level Word Class Categorization Model in Analytic Languages and Its Implications for {POS} Tagging in {M}odern {C}hinese Corpora,2015,15,0,2,0,36305,renqiang wang,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"The study of word classes has a history of over 4000 years, and the word class problem in over 1000 analytic languages like Modern Chinese can be seen as the Goldbach Conjecture in linguistics. This paper first outlines the existing problems in the POS tagging of Modern Chinese corpora with a case study of xe8x87xaaxe4xbfxa1. Then it introduces the Two-level Word Class Categorization Model in analytic languages, which is based on the perspectives of language as a complex adaptive system and the nature of major parts of speech as propositional speech act functions. Finally, the implications of Two-level Word Class Categorization Model for POS tagging in Modern Chinese corpora are explored."
C10-1079,Semantic Role Labeling for News Tweets,2010,25,13,7,0,32491,xiaohua liu,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"News tweets that report what is happening have become an important real-time information source. We raise the problem of Semantic Role Labeling (SRL) for news tweets, which is meaningful for fine grained information extraction and retrieval. We present a self-supervised learning approach to train a domain specific SRL system to resolve the problem. A large volume of training data is automatically labeled, by leveraging the existing SRL system on news domain and content similarity between news and news tweets. On a human annotated test set, our system achieves state-of-the-art performance, outperforming the SRL system trained on news."
Y09-2035,Extracting Keyphrases from {C}hinese News Articles Using {T}ext{R}ank and Query Log Knowledge,2009,7,8,2,0,46714,weiming liang,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"Keyphrases extracted from articles are beneficial in helping people boost brows- ing speed, but unfortunately keyphrases are rarely available for news articles due to the high expense of labor and time for manual annotation. This paper proposes a practical approach to extracting keyphrases for Chinese news articles using the TextRank and query log knowl- edge. Previous work is word based, while our approach uses phrase as its basic element. We generate phrases by employing several statistical criteria with the huge amount of queries as a training corpus. We use TextRank, a graph-based learning algorithm, for extracting keyphrases from Chinese news articles. In addition, two instructive features, lengths and positions of phrases, are incorporated into the TextRank model. Experimental results demon- strate that our methods improve the performance significantly."
I08-4009,Which Performs Better on In-Vocabulary Word Segmentation: Based on Word or Character?,2008,11,1,2,0,48568,zhenxing wang,Proceedings of the Sixth {SIGHAN} Workshop on {C}hinese Language Processing,0,"Since the first Chinese Word Segmentation (CWS) Bakeoff on 2003, CWS has experienced a prominent flourish because Bakeoff provides a platform for the participants, which helps them recognize the merits and drawbacks of their segmenters. However, the evaluation metric of bakeoff is not sufficient enough to measure the performance thoroughly, sometimes even misleading. One typical example caused by this insufficiency is that there is a popular belief existing in the research field that segmentation based on word can yield a better result than character-based tagging (CT) on in-vocabulary (IV) word segmentation even within closed tests of Bakeoff. Many efforts were paid to balance the performance on IV and out-ofvocabulary (OOV) words by combining these two methods according to this belief. In this paper, we provide a more detailed evaluation metric of IV and OOV words than Bakeoff to analyze CT method and combination method, which is a typical way to seek such balance. Our evaluation metric shows that CT outperforms dictionary-based (or so called word-based in general) segmentation on both IV and OOV words within Bakeoff * The work is done when the first author is working in MSRA as an intern. closed tests. Furthermore, our analysis shows that using confidence measure to combine the two segmentation results should be under certain limitation."
I08-4015,The Character-based {CRF} Segmenter of {MSRA}{\\&}{NEU} for the 4th Bakeoff,2008,10,6,2,0,48568,zhenxing wang,Proceedings of the Sixth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper describes the Chinese Word Segmenter for the fourth International Chinese Language Processing Bakeoff. Base on Conditional Random Field (CRF) model, a basic segmenter is designed as a problem of character-based tagging. To further improve the performance of our segmenter, we employ a word-based approach to increase the in-vocabulary (IV) word recall and a post-processing to increase the out-of-vocabulary (OOV) word recall. We participate in the word segmentation closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora."
Y06-1001,Which Is Essential for {C}hinese Word Segmentation: Character versus Word,2006,22,18,1,1,36306,changning huang,"Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation",0,"This paper proposes an empirical comparison between word-based method and character-based method for Chinese word segmentation. In three Chinese word segmentation Bakeoffs, character-based method quickly rose as a mainstream technique in this field. We disclose the linguistic background and statistical feature behind this observation. Also, an empirical study between wordbased method and character-based method are performed. Our results show that character-based method alone can work well for Chinese word segmentation without additional explicit word information from training corpus."
Y06-1012,Effective Tag Set Selection in {C}hinese Word Segmentation via Conditional Random Field Modeling,2006,10,91,2,0,305,hai zhao,"Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation",0,"This paper is concerned with Chinese word segmentation, which is regarded as a character based tagging problem under conditional random field framework. It is different in our method that we consider both feature template selection and tag set selection, instead of feature template focused only method in existing work. Thus, there comes an empirical comparison study of performance among different tag sets in this paper. We show that there is a significant performance difference as different tag sets are selected. Based on the proposed method, our system gives the state-of-the-art performance."
W06-0127,An Improved {C}hinese Word Segmentation System with Conditional Random Field,2006,10,142,2,0,305,hai zhao,Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing,0,"In this paper, we describe a Chinese word segmentation system that we developed for the Third SIGHAN Chinese Language Processing Bakeoff (Bakeoff2006). We took part in six tracks, namely the closed and open track on three corpora, Academia Sinica (CKIP), City University of Hong Kong (CityU), and University of Pennsylvania/University of Colorado (UPUC). Based on a conditional random field based approach, our word segmenter achieved the highest F measures in four tracks, and the third highest in the other two tracks. We found that the use of a 6-tag set, tone feature of Chinese character and assistant segmenters trained on other corpora further improve Chinese word segmentation performance."
O06-3002,{C}hinese Chunking Based on Maximum Entropy {M}arkov Models,2006,30,13,2,0,45149,guanglu sun,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 11, Number 2, June 2006",0,"This paper presents a new Chinese chunking method based on maximum entropy Markov models. We firstly present two types of Chinese chunking specifications and data sets, based on which the chunking models are applied. Then we describe the hidden Markov chunking model and maximum entropy chunking model. Based on our analysis of the two models, we propose a maximum entropy Markov chunking model that combines the transition probabilities and conditional probabilities of states. Experimental results for two types of data sets show that this approach achieves impressive accuracy in terms of the F-score: 91.02% and 92.68%, respectively. Compared with the hidden Markov chunking model and maximum entropy chunking model, based on the same data set, the new chunking model achieves better performance."
O05-2004,{C}hinese Main Verb Identification: From Specification to Realization,2005,15,6,2,0,50934,binggong ding,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 1, March 2005",0,"Main verb identification is the task of automatically identifying the predicate-verb in a sentence. It is useful for many applications in Chinese Natural Language Processing. Although most studies have focused on the model used to identify the main verb, the definition of the main verb should not be overlooked. In our specification design, we have found many complicated issues that still need to be resolved since they haven't been well discussed in previous works. Thus, the first novel aspect of our work is that we carefully design a specification for annotating the main verb and investigate various complicated cases. We hope this discussion will help to uncover the difficulties involved in this problem. Secondly, we present an approach to realizing main verb identification based on the use of chunk information, which leads to better results than the approach based on part-of-speech. Finally, based on careful observation of the studied corpus, we propose new local and contextual features for main verb identification. According to our specification, we annotate a corpus and then use a Support Vector Machine (SVM) to integrate all the features we propose. Our model, which was trained on our annotated corpus, achieved a promising F score of 92.8%. Furthermore, we show that main verb identification can improve the performance of the Chinese Sentence Breaker, one of the applications of main verb identification, by 2.4%."
J05-4005,{C}hinese Word Segmentation and Named Entity Recognition: A Pragmatic Approach,2005,63,169,4,0.672728,3502,jianfeng gao,Computational Linguistics,0,"This article presents a pragmatic approach to Chinese word segmentation. It differs from most previous approaches mainly in three respects. First, while theoretical linguists have defined Chinese words using various linguistic criteria, Chinese words in this study are defined pragmatically as segmentation units whose definition depends on how they are used and processed in realistic computer applications. Second, we propose a pragmatic mathematical framework in which segmenting known words and detecting unknown words of different types (i.e., morphologically derived words, factoids, named entities, and other unlisted words) can be performed simultaneously in a unified way. These tasks are usually conducted separately in other systems. Finally, we do not assume the existence of a universal word segmentation standard that is application-independent. Instead, we argue for the necessity of multiple segmentation standards due to the pragmatic fact that different natural language processing applications might require different granularities of Chinese words.These pragmatic approaches have been implemented in an adaptive Chinese word segmenter, called MSRSeg, which will be described in detail. It consists of two components: (1) a generic segmenter that is based on the framework of linear mixture models and provides a unified approach to the five fundamental features of word-level Chinese language processing: lexicon word processing, morphological analysis, factoid detection, named entity recognition, and new word identification; and (2) a set of output adaptors for adapting the output of (1) to different application-specific standards. Evaluation on five test sets with different standards shows that the adaptive system achieves state-of-the-art performance on all the test sets."
I05-3001,Detecting Segmentation Errors in {C}hinese Annotated Corpus,2005,6,3,2,0,1800,chengjie sun,Proceedings of the Fourth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper proposes a semi-automatic method to detect segmentation errors in a manually annotated Chinese corpus in order to improve its quality further. A particular Chinese character string occurring more than once in a corpus may be assigned different segmentations during a segmentation process. Based on these differences our approach outputs the segmentation error candidates found in a segmented corpus and then on which the segmentation errors are identified manually. Segmentation error rate of a gold standard corpus can be given using our method. In Peking University (PK) and Academic Sinica (AS) test corpora of Special Interest Group for Chinese Language Processing (SIGHAN) Bakeoff1, 1.29% and 2.26% segmentation error rates are detected by our method. These errors decrease the F-measure of SIGHAN Bakeoff1 baseline test by 1.36% in PK test data and 1.93% in AS test data respectively. This work was done while Chengjie Sun was visiting Microsoft Research Asia."
I05-2040,Transformation Based {C}hinese Entity Detection and Tracking,2005,14,7,2,0,9056,yaqian zhou,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,"This paper proposes a unified Transformation Based Learning (TBL, Brill, 1995) framework for Chinese Entity Detection and Tracking (EDT). It consists of two sub models: a mention detection model and an entity tracking/coreference model. The first sub-model is used to adapt existing Chinese word segmentation and Named Entity (NE) recognition results to a specific EDT standard to find all the mentions. The second sub-model is used to find the coreference relation between the mentions. In addition, a feedback technique is proposed to further improve the performance of the system. We evaluated our methods on the Automatic Content Extraction (ACE, NIST, 2003) Chinese EDT corpus. Results show that it outperforms the baseline, and achieves comparable performance with the stateof-the-art methods."
W04-1107,{C}hinese Chunking with Another Type of Spec,2004,15,22,2,0,51600,hongqiao li,Proceedings of the Third {SIGHAN} Workshop on {C}hinese Language Processing,0,"Spec is a critical issue for automatic chunking. This paper proposes a solution of Chinese chunking with another type of spec, which is not derived from a complete syntactic tree but only based on the un-bracketed, POS tagged corpus. With this spec, a chunked data is built and HMM is used to build the chunker. TBLbased error correction is used to further improve chunking performance. The average chunk length is about 1.38 tokens, F measure of chunking achieves 91.13%, labeling accuracy alone achieves 99.80% and the ratio of crossing brackets is 2.87%. We also find that the hardest point of Chinese chunking is to identify the chunking boundary inside noun-noun sequences1."
P04-1059,Adaptive {C}hinese Word Segmentation,2004,19,45,4,0.901295,3502,jianfeng gao,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,This paper presents a Chinese word segmentation system which can adapt to different domains and standards. We first present a statistical framework where domain-specific words are identified in a unified approach to word segmentation based on linear models. We explore several features and describe how to create training data by sampling. We then describe a transformation-based learning method used to adapt our system to different word segmentation standards. Evaluation of the proposed system on five test sets with different standards shows that the system achieves state- of-the-art performance on all of them.
W03-1701,Unsupervised Training for Overlapping Ambiguity Resolution in {C}hinese Word Segmentation,2003,16,38,3,1,908,mu li,Proceedings of the Second {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper proposes an unsupervised training approach to resolving overlapping ambiguities in Chinese word segmentation. We present an ensemble of adapted Naive Bayesian classifiers that can be trained using an unlabelled Chinese text corpus. These classifiers differ in that they use context words within windows of different sizes as features. The performance of our approach is evaluated on a manually annotated test set. Experimental results show that the proposed approach achieves an accuracy of 94.3%, rivaling the rule-based and supervised training methods."
W03-1718,Single Character {C}hinese Named Entity Recognition,2003,9,10,4,0,1624,xiaodan zhu,Proceedings of the Second {SIGHAN} Workshop on {C}hinese Language Processing,0,"Single character named entity (SCNE) is a name entity (NE) composed of one Chinese character, such as [Abstract contained text which could not be captured.] (zhong1, China) and [Abstract contained text which could not be captured.] (e2, Russia). SCNE is very common in written Chinese text. However, due to the lack of in-depth research, SCNE is a major source of errors in named entity recognition (NER). This paper formulates the SCNE recognition within the source-channel model framework. Our experiments show very encouraging results: an F-score of 81.01% for single character location name recognition, and an F-score of 68.02% for single character person name recognition. An alternative view of the SCNE recognition problem is to formulate it as a classification task. We construct two classifiers based on maximum entropy model (ME) and vector space model (VSM), respectively. We compare all proposed approaches, showing that the source-channel model performs the best in most cases."
P03-1035,Improved Source-Channel Models for {C}hinese Word Segmentation,2003,12,56,3,0.901295,3502,jianfeng gao,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"This paper presents a Chinese word segmentation system that uses improved source-channel models of Chinese sentence generation. Chinese words are defined as one of the following four types: lexicon words, morphologically derived words, factoids, and named entities. Our system provides a unified approach to the four fundamental features of word-level Chinese language processing: (1) word segmentation, (2) morphological analysis, (3) factoid detection, and (4) named entity recognition. The performance of the system is evaluated on a manually annotated test set, and is also compared with several state-of-the-art systems, taking into account the fact that the definition of Chinese words often varies from system to system."
W02-2224,"On the Affinity of {TAG} with Projective, Bilexical Dependency Grammar",2002,0,0,2,1,50464,tom lai,Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+6),0,None
W02-1804,Finding the Better Indexing units for {C}hinese Information Retrieval,2002,9,3,4,0,53147,hongzhao he,{COLING}-02: The First {SIGHAN} Workshop on {C}hinese Language Processing,0,"In the processing of Chinese documents and queries in information retrieval (IR), one has to identify the units that are used as indexes. Words and n-grams had been used as indexes in several previous studies, which showed that both kinds of indexes lead to comparable IR performances. In this study, we carried out more experiments to find the better way to index Chinese texts. First, we investigated the inpacts on IR performance of the accuracy of word segmentation. Second, fifteen different groups of indexing units, which were the possible combination of words and character n-grams, were discussed detailedly. Experiments showed that better segmentation results in better IR performances, and a combination of words with uni-grams is the better choice to index Chinese texts for IR."
C02-1010,Structure Alignment Using Bilingual Chunking,2002,12,18,4,0,4596,wei wang,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"A new statistical method called bilingual chunking for structure alignment is proposed. Different with the existing approaches which align hierarchical structures like sub-trees, our method conducts alignment on chunks. The alignment is finished through a simultaneous bilingual chunking algorithm. Using the constrains of chunk correspondence between source language (SL) and target language (TL), our algorithm can dramatically reduce search space, support time synchronous DP algorithm, and lead to highly consistent chunking. Furthermore, by unifying the POS tagging and chunking in the search process, our algorithm alleviates effectively the influence of POS tagging deficiency to the chunking result.The experimental results with English-Chinese structure alignment show that our model can produce 90% in precision for chunking, and 87% in precision for chunk alignment."
C02-1012,{C}hinese Named Entity Identification Using Class-based Language Model,2002,9,109,5,0,9014,jian sun,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"We consider here the problem of Chinese named entity (NE) identification using statistical language model(LM). In this research, word segmentation and NE identification have been integrated into a unified framework that consists of several class-based language models. We also adopt a hierarchical structure for one of the LMs so that the nested entities in organization names can be identified. The evaluation on a large test set shows consistent improvements. Our experiments further demonstrate the improvement after seamlessly integrating with linguistic heuristic information, cache-based model and NE abbreviation identification."
O01-2001,Improving Translation Selection with a New Translation Model Trained by Independent Monolingual Corpora,2001,96,21,3,1,4082,ming zhou,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 6, Number 1, {F}ebruary 2001: Special Issue on Natural Language Processing Researches in {MSRA}",0,"We propose a novel statistical translation model to improve translation selection of collocation. In the statistical approach that has been popularly applied for translation selection, bilingual corpora are used to train the translation model. However, there exists a formidable bottleneck in acquiring large-scale bilingual corpora, in particular for language pairs involving Chinese. In this paper, we propose a new approach to training the translation model by using unrelated monolingual corpora. First, a Chinese corpus and an English corpus are parsed with dependency parsers, respectively, and two dependency triple databases are generated. Then, the similarity between a Chinese word and an English word can be estimated using the two monolingual dependency triple databases with the help of a simple Chinese-English dictionary. This cross-language word similarity is used to simulate the word translation probability. Finally, the generated translation model is used together with the language model trained with the English dependency database to realize translation of Chinese collocations into English. To demonstrate the effectiveness of this method, we performed various experiments with verb-object collocation translation. The experiments produced very promising results."
O01-2004,Automatic Translation Template Acquisition Based on Bilingual Structure Alignment,2001,16,11,4,0,37737,yajuan lu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 6, Number 1, {F}ebruary 2001: Special Issue on Natural Language Processing Researches in {MSRA}",0,Knowledge acquisition is a bottleneck in machine translation and many NLP tasks. A method for automatically acquiring translation templates from bilingual corpora is proposed in this paper. Bilingual sentence pairs are first aligned in syntactic structure by combining a language parsing with a statistical bilingual language model. The alignment results are used to extract translation templates which turn out to be very useful in real machine translation.
W00-1902,Dependency-based Syntactic Annotation of a Chiense Corpus,2000,0,0,2,1,50464,tom lai,Proceedings of the {COLING}-2000 Workshop on Linguistically Interpreted Corpora,0,None
W00-1218,A Clustering Algorithm for {C}hinese Adjectives and Nouns,2000,0,0,3,0,27550,yang wen,Second {C}hinese Language Processing Workshop,0,This paper proposes a bidirctional hierarchical clustering algorithm for simultaneously clustering words of different parts of speech based on collocations. The algorithm is composed of cycles of two kinds of alternate clustering processes. We construct an objective function based on Minimum Description Length. To partly solve the problem caused by sparse data two concepts of collocational degree and revisional distance are presented.
P00-1015,A Unified Statistical Model for the Identification of {E}nglish {B}ase{NP},2000,8,51,2,0,11753,endong xun,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"This paper presents a novel statistical model for automatic identification of English baseNP. It uses two steps: the N-best Part-Of-Speech (POS) tagging and baseNP identification given the N-best POS-sequences. Unlike the other approaches where the two steps are separated, we integrate them into a unified statistical framework. Our model also integrates lexical information. Finally, Viterbi algorithm is applied to make global search in the entire sentence, allowing us to obtain linear complexity for the entire process. Compared with other methods using the same testing set, our approach achieves 92.3% in precision and 93.2% in recall. The result is comparable with or better than the previously reported results."
P00-1032,Automatic Detecting/Correcting Errors in {C}hinese Text by an Approximate Word-Matching Algorithm,2000,2,35,3,0.57971,2095,lei zhang,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"An approximate word-matching algorithm for Chinese is presented. Based on this algorithm, an effective approach to Chinese spelling error detection and correction is implemented. With a word tri-gram language model, the optimal string is searched from all possible derivation of the input sentence using operations of character substitution, insertion, and deletion. Comparing the original sentence with the optimal string, spelling error detection and correction is realized simultaneously."
P00-1033,Dependency-based Syntactic Analysis of {C}hinese and Annotation of Parsed Corpus,2000,13,9,2,1,50464,tom lai,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"Following a dependency-based framework that admits no intermediate phrasal nodes and allows no crossing of syntactic dependency links, we discuss how Chinese sentences are analysed and annotated using an SGML-based scheme. Issues related to tolerance of errors at various levels of analysis and compatibility with other syntactic frameworks are addressed."
P00-1067,{PENS}: A Machine-aided {E}nglish Writing System for {C}hinese Users,2000,10,29,5,0,1018,ting liu,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"Writing English is a big barrier for most Chinese users. To build a computer-aided system that helps Chinese users not only on spelling checking and grammar checking but also on writing in the way of native-English is a challenging task. Although machine translation is widely used for this purpose, how to find an efficient way in which human collaborates with computers remains an open issue. In this paper, based on the comprehensive study of Chinese users requirements, we propose an approach to machine aided English writing system, which consists of two components: 1) a statistical approach to word spelling help, and 2) an information retrieval based approach to intelligent recommendation by providing suggestive example sentences. Both components work together in a unified way, and highly improve the productivity of English writing. We also developed a pilot system, namely PENS (Perfect ENglish System). Preliminary experiments show very promising results."
sun-etal-2000-hua,Hua Yu: A Word-segmented and Part-Of-Speech Tagged {C}hinese Corpus,2000,3,2,3,0.740741,4517,maosong sun,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"As the outcome of a 3-year joint effort of Department of Computer Science, Tsinghua University and Language Information Processing Institute, Beijing Language and Culture University, Beijing, China, a word-segmented and part-of-speech tagged Chinese corpus with size of 2 million Chinese characters, named HuaYu, has been established. This paper firstly introduces some basics about HuaYu in brief, as its genre distribution, fundamental considerations in designing it, word segmentation and part-of-speech tagging standards. Then the complete list of tag set used in HuaYu is given, along with typical examples for each tag accordingly. Several pieces of annotated texts in each genre are also included at last for readerxe2x80x99s reference."
Y99-1017,Free Word Order in a Constraint-based Implementation of Dependency Grammar,1999,15,0,2,1,50464,tom lai,"Proceedings of the 13th Pacific Asia Conference on Language, Information and Computation",0,"Parsing of sentences based on Dependency Grammar is emulated with a constraintand unification-based mechanism that preserves single-headedness and projectivity in syntactic dependency. Working on Chinese, and referring to English at times, the authors have treated subcategorization properties of verbs on the assumptions that relative positions of obligatory complements of a verb are fixed, and that optional verbal adjuncts, while relatively free in their choice of positions in the sentence, are constrained to be placed farther away from the governing verb than any complements. A pair of subcategorization lists residing in the governing verb have been used to capture these properties. However when one tries to extend the model to languages with relatively free word order like Japanese, for which the assumptions mentioned above are not valid, straight-forward list-manipulations on subcategorization lists will not be adequate. This paper discusses how additional subcategorization handling mechanisms can be introduced to deal with verbal complements scrambling and intervening adjuncts, while retaining the mechanisms to capture default word order."
O99-4001,A Model for Word Sense Disambiguation,1999,0,3,2,0,7875,juanzi li,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 4, Number 2, August 1999",0,"Word sense disambiguation is one of the most difficult problems in natural language processing. This paper puts forward a model for mapping a structural semantic space from a thesaurus into a multi-dimensional, real-valued vector space and gives a word sense disambiguation method based on this mapping. The model, which uses an unsupervised learning method to acquire the disambiguation knowledge, not only saves extensive manual work, but also realizes the sense tagging of a large number of content words. Firstly, a Chinese thesaurus Cilin and a very large-scale corpus are used to construct the structure of the semantic space. Then, a dynamic disambiguation model is developed to disambiguate an ambiguous word according to the vectors of monosemous words in each of its possible categories. In order to resolve the problem of data sparseness, a method is proposed to make the model more robust. Testing results show that the model has relatively good performance and can also be used for other languages."
W98-0512,Complements and Adjuncts in Dependency Grammar Parsing Emulated by a Constrained Context-Free Grammar,1998,-1,-1,2,1,50464,tom lai,Processing of Dependency-Based Grammars,0,None
P98-1001,A Quasi-Dependency Model for the Structural Analysis of {C}hinese {B}ase{NP}s,1998,4,6,2,0,1194,jun zhao,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,The paper puts forward a quasi-dependency model for structural analysis of Chinese baseNPs and a MDL-based algorithm for quasi-dependency-strength acquisition. The experiments show that the proposed model is more suitable for Chinese baseNP analysis and the proposed MDL-based algorithm is superior to the traditional ML-based algorithm. The paper also discusses the problem of incorporating the linguistic knowledge into the above statistical model.
P98-1098,Combining a {C}hinese Thesaurus with a {C}hinese Dictionary,1998,7,1,3,1,7622,donghong ji,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"In this paper, we study the problem of combining a Chinese thesaurus with a Chinese dictionary by linking the word entries in the thesaurus with the word senses in the dictionary, and propose a similar word strategy to solve the problem. The method is based on the definitions given in the dictionary, but without any syntactic parsing or sense disambiguation on them at all. As a result, their combination makes the thesaurus specify the similarity between senses which accounts for the similarity between words, produces a kind of semantic classification of the senses defined in the dictionary, and provides reliable information about the lexical items on which the resources don't conform with each other."
C98-1001,A Quasi-Dependency Model for Structural Analysis it of {C}hinese {B}ase{NP}s,1998,4,6,2,0,1194,jun zhao,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,The paper puts forward a quasi-dependency model for structural analysis of Chinese baseNPs and a MDL-based algorithm for quasi-dependency-strength acquisition. The experiments show that the proposed model is more suitable for Chinese baseNP analysis and the proposed MDL-based algorithm is superior to the traditional ML-based algorithm. The paper also discusses the problem of incorporating the linguistic knowledge into the above statistical model.
C98-1095,Combining a {C}hinese Thesaurus with a {C}hinese Dictionary,1998,7,1,3,1,7622,donghong ji,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"In this paper, we study the problem of combining a Chinese thesaurus with a Chinese dictionary by linking the word entries in the thesaurus with the word senses in the dictionary, and propose a similar word strategy to solve the problem. The method is based on the definitions given in the dictionary, but without any syntactic parsing or sense disambiguation on them at all. As a result, their combination makes the thesaurus specify the similarity between senses which accounts for the similarity between words, produces a kind of semantic classification of the senses defined in the dictionary, and provides reliable information about the lexical items on which the resources don't conform with each other."
W97-1004,Learning New Compositions from Given Ones,1997,16,1,3,1,7622,donghong ji,{C}o{NLL}97: Computational Natural Language Learning,0,"In this paper, we study the problem of learning new compositions of words from given ones with a specific syntactic structure, e.g., A-N or V-N structures. We first cluster words according to the given compositions, then construct a cluster-based compositional frame for each word cluster, which contains both new and given compositions relevant with the words in the cluster. In contrast to other methods, we don't pre-define the number of clusters, and formalize the problem of clustering words as a non-linear optimization one, in which we specify the environments of words based on word clusters to be determined, rather than their neighboring words. To solve the problem, we make use of a kind of cooperative evolution strategy to design an evolutionary algorithm."
W97-0321,Word Sense Disambiguation Based on Structured Semantic Space,1997,9,0,2,1,7622,donghong ji,Second Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we propose a framework, structured semantic space, as a foundation for word sense disarnbiguation tasks, and present a strategy to identify the correct sense of a word in some context based on the space. The semantic space is a set of multidimensional real-valued vectors, which formally describe the contexts of words. Instead of locating all word senses in the space, we only make use of mono-sense words to outline it. We design a merging procedure to establish the dendrogram structure of the space and give an heuristic algorithm to find the nodes (sense clusters) corresponding with sets of similar senses in the dendrogram. Given a word in a particular context' the context would activate some clusters in the dendrogram, based on its similarity with the contexts of the words in the clusters, then the correct sense of the word could be determined by comparing its definitions with those of the words in the clusters."
A97-1018,{CS}eg{\\&}Tagl.0: A Practical Word Segmenter and {POS} Tagger for {C}hinese Texts,1997,5,21,3,0.740741,4517,maosong sun,Fifth Conference on Applied Natural Language Processing,0,"Chinese word segmentation and POS tagging are two key techniques in many applications in Chinese information processing. Great efforts have been paid to the research in the last decade, but unfortunately, no practical system with high performance for unrestricted texts is available up to date. CSeg&Tag1.0, a Chinese word segmenter and POS tagger which unifies these two procedures into one model, is introduced in this paper. The preliminary open tests show that the segmentation precision of CSeg&Tag1.0 is about 98.0% - 99.3%, POS tagging precision about 91.0% - 97.1%, and the recall and precision for unknown words are ranging from 95.0% to 99.0% and from 87.6% to 95.3% respectively. The processing speed is about 100 characters per second on Pentium 133 PC. The work of improving the performance of the system is still ongoing."
C94-2153,An Efficient Syntactic Tagging Tool for Corpora,1994,6,6,2,1,4082,ming zhou,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The tree bank is an important resoures for MT and linguistics researches, but it requires that large number of sentences be annotated with syntactic information. It is time consuming and troublesome, and difficult to keep consistency, if annotation is done manually. In this paper, we presented a new technique for the semi-automatic tagging of Chinese text. The system takes as input Chinese text, and outputs the syntactically tagged sentence(dependency tree). We use dependency grammar and employ a stack based shift/ reduce context-dependent parser as the tagging mechanism. The system works in human-machine cooperative way, in which the machine can acquire tagging rules from human intervention. The automation level can be improved step by step by accumulating rules during annotation. In addition, good consistency of tagging is guaranteed."
C94-2200,The Evolution of Machine-Tractable Dictionaries,1994,7,0,2,0,56481,chengming guo,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,None
W93-0312,Example-Based Sense Tagging of Running {C}hinese Text,1993,-1,-1,2,0,55898,xiang tong,{V}ery {L}arge {C}orpora: Academic and Industrial Perspectives,0,None
C92-4211,Knowledge Acquisition and {C}hinese Parsing Based on Corpus,1992,0,5,2,0,49975,chunfa yuan,{COLING} 1992 Volume 4: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In Natural Language Processing (NLP), one key problem is how to design a robust and effective parsing system. In this paper, we will introduce a corpus- based Chinese parsing system. Our efforts are concetrated on: (1) knowledge acquisition and representation; and (2) the parsing scheme. The knowledge of this system is principally extracted from analyzed corpus, others are a few grammatical principles, i.e. the four axioms of the Dependency Grammar (DG). In addition, we also propose the fifth axiom of DG to support the parsing of Chinese sentences."
