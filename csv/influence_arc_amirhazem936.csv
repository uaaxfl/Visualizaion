2011.jeptalnrecital-long.13,C02-1166,1,0.855233,"Missing"
2011.jeptalnrecital-long.13,W97-0119,0,0.0741503,"Missing"
2011.jeptalnrecital-long.13,P98-1069,0,0.200369,"Missing"
2011.jeptalnrecital-long.13,W09-1117,0,0.0346531,"Missing"
2011.jeptalnrecital-long.13,P04-1067,1,0.88383,"Missing"
2011.jeptalnrecital-long.13,C10-1070,0,0.0270019,"Missing"
2011.jeptalnrecital-long.13,C10-1073,1,0.752074,"Missing"
2011.jeptalnrecital-long.13,P07-1084,1,0.870134,"Missing"
2011.jeptalnrecital-long.13,J03-1002,0,0.0041682,"Missing"
2011.jeptalnrecital-long.13,P99-1067,0,0.145921,"Missing"
2011.jeptalnrecital-long.13,E06-1029,0,0.0447777,"Missing"
2011.jeptalnrecital-long.13,P10-1011,0,0.0250748,"Missing"
2011.jeptalnrecital-long.13,N09-2031,0,0.0263672,"Missing"
2011.jeptalnrecital-long.19,I05-1062,1,0.831123,"Missing"
2011.jeptalnrecital-long.19,C02-1166,0,0.0740113,"Missing"
2011.jeptalnrecital-long.19,J93-1003,0,0.102225,"Missing"
2011.jeptalnrecital-long.19,P98-1069,0,0.230763,"Missing"
2011.jeptalnrecital-long.19,W97-0119,0,0.188053,"Missing"
2011.jeptalnrecital-long.19,C10-1070,0,0.0285164,"Missing"
2011.jeptalnrecital-long.19,2009.jeptalnrecital-long.6,1,0.798033,"Missing"
2011.jeptalnrecital-long.19,2007.mtsummit-papers.26,0,0.100096,"Missing"
2011.jeptalnrecital-long.19,P95-1050,0,0.21892,"Missing"
2011.jeptalnrecital-long.19,2009.mtsummit-posters.26,0,0.0719661,"Missing"
2019.jeptalnrecital-court.28,P18-2026,0,0.031103,"Missing"
2019.jeptalnrecital-court.28,J93-1003,0,0.0902183,"Missing"
2019.jeptalnrecital-court.28,D15-1181,0,0.0548646,"Missing"
2020.bucc-1.9,D19-1134,0,0.0115186,"y of the seed lexicons, and finally, Section 6 concludes our work. Cross-lingual word embeddings learning has triggered great attention in the recent years and several bilingual supervised (Mikolov et al., 2013; Xing et al., 2015; Artetxe et al., 2018a) and unsupervised (Artetxe et al., 2018b; Conneau et al., 2017) alignment methods have been proposed so far. Also, multilingual alignment approaches which consists in mapping several languages in one common space via a pivot language (Smith et al., 2017) or by training all language pairs simultaneously (Chen and Cardie, 2018; Wada et al., 2019; Taitelbaum et al., 2019b; Taitelbaum et al., 2019a; Alaux et al., 2018) are attracting a great attention. Among possible downstream applications of cross-lingual embedding models: Bilingual Lexicon Induction (BLI) which consists in the identification of translation pairs based on a comparable corpus. The BUCC shared task offers the first evaluation framework on BLI from comparable corpora. It covers six languages (English, French, German, Russian, Spanish and Chinese) and two corpora (Wikipedia and WaCKy). We describe in this paper our participation at the BLI shared task. We start by evaluating the cross-lingual wo"
2020.bucc-1.9,P19-1300,0,0.0111168,"iscusses the quality of the seed lexicons, and finally, Section 6 concludes our work. Cross-lingual word embeddings learning has triggered great attention in the recent years and several bilingual supervised (Mikolov et al., 2013; Xing et al., 2015; Artetxe et al., 2018a) and unsupervised (Artetxe et al., 2018b; Conneau et al., 2017) alignment methods have been proposed so far. Also, multilingual alignment approaches which consists in mapping several languages in one common space via a pivot language (Smith et al., 2017) or by training all language pairs simultaneously (Chen and Cardie, 2018; Wada et al., 2019; Taitelbaum et al., 2019b; Taitelbaum et al., 2019a; Alaux et al., 2018) are attracting a great attention. Among possible downstream applications of cross-lingual embedding models: Bilingual Lexicon Induction (BLI) which consists in the identification of translation pairs based on a comparable corpus. The BUCC shared task offers the first evaluation framework on BLI from comparable corpora. It covers six languages (English, French, German, Russian, Spanish and Chinese) and two corpora (Wikipedia and WaCKy). We describe in this paper our participation at the BLI shared task. We start by evalua"
2020.bucc-1.9,N15-1104,0,0.217484,"compared to individual strategies, except for English-Russian and Russian-English language pairs for which the concatenation approach was preferred. Keywords: Bilingual lexicon induction, Comparable corpora, Cognates, Word embeddings 1. Introduction sets, Section 3 presents the tested approaches and the chosen strategy. The results are given in Section 4, Section 5 discusses the quality of the seed lexicons, and finally, Section 6 concludes our work. Cross-lingual word embeddings learning has triggered great attention in the recent years and several bilingual supervised (Mikolov et al., 2013; Xing et al., 2015; Artetxe et al., 2018a) and unsupervised (Artetxe et al., 2018b; Conneau et al., 2017) alignment methods have been proposed so far. Also, multilingual alignment approaches which consists in mapping several languages in one common space via a pivot language (Smith et al., 2017) or by training all language pairs simultaneously (Chen and Cardie, 2018; Wada et al., 2019; Taitelbaum et al., 2019b; Taitelbaum et al., 2019a; Alaux et al., 2018) are attracting a great attention. Among possible downstream applications of cross-lingual embedding models: Bilingual Lexicon Induction (BLI) which consists"
2020.bucc-1.9,D18-1024,0,0.0172524,"Missing"
2020.bucc-1.9,P18-1073,0,0.115454,"ual strategies, except for English-Russian and Russian-English language pairs for which the concatenation approach was preferred. Keywords: Bilingual lexicon induction, Comparable corpora, Cognates, Word embeddings 1. Introduction sets, Section 3 presents the tested approaches and the chosen strategy. The results are given in Section 4, Section 5 discusses the quality of the seed lexicons, and finally, Section 6 concludes our work. Cross-lingual word embeddings learning has triggered great attention in the recent years and several bilingual supervised (Mikolov et al., 2013; Xing et al., 2015; Artetxe et al., 2018a) and unsupervised (Artetxe et al., 2018b; Conneau et al., 2017) alignment methods have been proposed so far. Also, multilingual alignment approaches which consists in mapping several languages in one common space via a pivot language (Smith et al., 2017) or by training all language pairs simultaneously (Chen and Cardie, 2018; Wada et al., 2019; Taitelbaum et al., 2019b; Taitelbaum et al., 2019a; Alaux et al., 2018) are attracting a great attention. Among possible downstream applications of cross-lingual embedding models: Bilingual Lexicon Induction (BLI) which consists in the identification"
2020.bucc-1.9,L18-1550,0,0.0175764,"portant questions about the evaluation process and suggest a careful handcrafted validation which will undoubtedly strengthen the BLI shared task. Table 4: Ratio of target words per source words for the validation lists for some language pair on different lists results than other language pairs (10 to almost 30 points). Unlike other pairs trained on WaCKy, this pair is the only one trained on Wikipedia, contradicting the idea that ”the WaCKy corpora seem somewhat better suited for the dictionary induction task than Wikipedia”. To verify this statement, we used pre-trained word embeddings from Grave et al. (2018) to check if the corpus was really the main problem. And actually, using the pre-trained embeddings on Wikipedia or Common Crawl led to much better results than the results obtained using the WaCKy corpora, reaching about the same F1-score as the English-Spanish language pair. Our final results for the shared task were reported from the mixed approach for all language pairs but the two with Russian, for which we only took the results from the concatenation approach. 6. 5. Seed Lexicon Analysis Conclusion We presented in this paper the participation of the TALN/LS2N team at the BUCC shared task"
2020.bucc-1.9,C18-1080,1,0.932267,"nsists in the identification of translation pairs based on a comparable corpus. The BUCC shared task offers the first evaluation framework on BLI from comparable corpora. It covers six languages (English, French, German, Russian, Spanish and Chinese) and two corpora (Wikipedia and WaCKy). We describe in this paper our participation at the BLI shared task. We start by evaluating the cross-lingual word embedding mapping approach (VecMap) (Artetxe et al., 2018a) using fastText embeddings. Then, we present an extension of VecMap approach that uses the concatenation of two mapped embedding models (Hazem and Morin, 2018). Finally, we present a cognates matching approach, merely an exact match string similarity. Based on the obtained results of the studied approaches, we derive our proposed system –Mix (Conc + Dist)– which combines the outputs of the embeddings concatenation and the cognates matching approaches. Overall, the obtained results on the validation data sets are in favor of our system for all language pairs except for English-Russian and Russian-English pairs, where the cognates matching approach obviously showed very weak results and for which the concatenation approach was preferred. In the follow"
2020.bucc-1.9,D14-1162,0,0.092491,"Missing"
2020.bucc-1.9,D19-1363,0,0.0115401,"y of the seed lexicons, and finally, Section 6 concludes our work. Cross-lingual word embeddings learning has triggered great attention in the recent years and several bilingual supervised (Mikolov et al., 2013; Xing et al., 2015; Artetxe et al., 2018a) and unsupervised (Artetxe et al., 2018b; Conneau et al., 2017) alignment methods have been proposed so far. Also, multilingual alignment approaches which consists in mapping several languages in one common space via a pivot language (Smith et al., 2017) or by training all language pairs simultaneously (Chen and Cardie, 2018; Wada et al., 2019; Taitelbaum et al., 2019b; Taitelbaum et al., 2019a; Alaux et al., 2018) are attracting a great attention. Among possible downstream applications of cross-lingual embedding models: Bilingual Lexicon Induction (BLI) which consists in the identification of translation pairs based on a comparable corpus. The BUCC shared task offers the first evaluation framework on BLI from comparable corpora. It covers six languages (English, French, German, Russian, Spanish and Chinese) and two corpora (Wikipedia and WaCKy). We describe in this paper our participation at the BLI shared task. We start by evaluating the cross-lingual wo"
2020.coling-main.527,D16-1250,0,0.0197432,". Using these approaches, words are embedded into a low-dimensional vector space. For instance, Mikolov et al. (2013) proposed an approach to learn a linear transformation from the source language into the target language. Faruqui and Dyer (2014) used the Canonical Correlation Analysis to project the embeddings in both languages into a shared vector space. Xing et al. (2015) proposed an orthogonal transformation based on normalized word vectors to improve the inconsistency among the objective function used to learn the word vectors and to learn the linear transformation matrix. More recently, Artetxe et al. (2016; 2018a) proposed and then improved an approach that generalizes previous works in order to preserve monolingual invariance using several meaningful and intuitive constraints (i.e. orthogonality, vector length normalization, mean centering, whitening, etc.). Finally, Conneau et al. (2017) and Artetxe et al. (2018b) proposed unsupervised mapping methods getting results close to supervised methods. A thorough comparison of cross-lingual word embeddings has been proposed by Ruder (2017). A comparison of the approaches of Mikolov et al. (2013) and Faruqui and Dyer (2014) with the standard bag of w"
2020.coling-main.527,P18-1073,0,0.0116683,"n both languages into a shared vector space. Xing et al. (2015) proposed an orthogonal transformation based on normalized word vectors to improve the inconsistency among the objective function used to learn the word vectors and to learn the linear transformation matrix. More recently, Artetxe et al. (2016; 2018a) proposed and then improved an approach that generalizes previous works in order to preserve monolingual invariance using several meaningful and intuitive constraints (i.e. orthogonality, vector length normalization, mean centering, whitening, etc.). Finally, Conneau et al. (2017) and Artetxe et al. (2018b) proposed unsupervised mapping methods getting results close to supervised methods. A thorough comparison of cross-lingual word embeddings has been proposed by Ruder (2017). A comparison of the approaches of Mikolov et al. (2013) and Faruqui and Dyer (2014) with the standard bag of words approach was carried out by Jakubina and Langlais (2017). The authors showed that embedding approaches perform well when the terms to be translated occur very frequently while the standard approach is slightly better when the terms are less frequent. On specialized domains, Hazem and Morin (2018) compared di"
2020.coling-main.527,D11-1033,0,0.03061,"arginal phenomenon in technical and scientific domains. In this sense, associating out-of-domain data with a specialized comparable corpus can be seen as a “heresy” or, as in the previous examples, an unfortunate way to introduce polysemy. In the context of Statistical Machine Translation (SMT), Wang et al. (2014) demonstrated that adding out-of-domain data to the training material of a system was detrimental when translating scientific and technical domains. Instead of using a data augmentation strategy, data selection is proposed to improve the quality of SMT systems (Moore and Lewis, 2010; Axelrod et al., 2011; Wang et al., 2014). The basic idea is that in-domain training data can be enriched with suitable sub-parts of out-of-domain data. Within this context, we apply two well-established data selection techniques often used in machine translation that is: Tf-Idf and cross entropy. We also propose for the first time to exploit BERT for data selection. We show that a subtle selection of the contexts of out-of-domain data allows to improve the representation of specialized domains, while preventing the introduction of polysemy induced by data augmentation. Overall, all the proposed techniques improve"
2020.coling-main.527,P13-2133,0,0.0168824,"l., 2020). In technical and scientific domains, comparable corpora suffer from their modest and limited size compared to general language corpora. This phenomenon is amplified by the difficulty to obtain many specialized documents in a language other than English (Morin and Hazem, 2014). This aspect, which is gradually changing with the deployment of open access data, is nevertheless a major difficulty. One way to overcome the limited size of specialized (in-domain) comparable corpora for bilingual lexicon induction (BLI) is to associate them with out-of-domain resources as lexical databases (Bouamor et al., 2013) or large general domain corpora (Hazem and Morin, 2018). For instance, by combining a specialized comparable corpus with a general domain corpus, methods based on distributional analysis are boosted (Hazem and Morin, 2018). General domain corpora greatly enhance the representation of specialized vocabulary by adding new contexts. The main drawbacks of this data augmentation approach is the introduction of polysemous information as well as the tremendous increase of computation time. Consider for instance, a French/English comparable corpus in the medical domain and the French terms os (bone)"
2020.coling-main.527,C02-2020,0,0.19169,"1998; Rapp, 1999), known as the standard bag of words approach, builds a context vector for each word of the source and the target languages, translates the source context vectors into the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. While this approach gives interesting results for large general comparable corpora (Gaussier et al., 2004; Laroche and Langlais, 2010; Vuli´c et al., 2011), it is rather unsuitable for small specialized comparable corpora due to the context vectors sparsity. (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson et al., 2009). More recent distributed approaches, based on deep neural network models (Mikolov et al., 2013), have come to renew traditional ones. Using these approaches, words are embedded into a low-dimensional vector space. For instance, Mikolov et al. (2013) proposed an approach to learn a linear transformation from the source language into the target language. Faruqui and Dyer (2014) used the Canonical Correlation Analysis to project the embeddings in both languages into a shared vector space. Xing et al. (2015) proposed an orthogonal transformation based"
2020.coling-main.527,N19-1423,0,0.101375,"1 represents the history of the word wi . Formally, HLMI,s (W ) represents the cross entropy of the sentence W given the language model LMI,s . The cross entropy is computed for every sentence of the out-of-domain corpus given the out-of-domain and in-domain language models. The source and target sentences are then evaluated by HLMI,b (WO ) − HLMO,b (WO ) (where b ∈ {s, t} refers to the side of the corpus) and ranked accordingly. The cross entropy is computed using the xenC tool (Rousseau, 2013). BERT is a supervised learning model that has proven to be efficient in many downstream NLP tasks (Devlin et al., 2019). It has been trained on the Masked Language Model (MLM) and Next Sentence Prediction (NSP) objectives (Devlin et al., 2019). Hence, it can be used whether for word representation or for sentence classification. In order to perform data selection, we use BERT as a binary classifier to predict if a given input sentence is in-domain or out-of-domain. The intuition behind this strategy is that BERT can learn shared features between in-domain sentences. For each positive (in-domain) training sentence, we randomly select a negative sentence from a general domain data set to keep a balanced training"
2020.coling-main.527,P19-2041,0,0.0527158,"Missing"
2020.coling-main.527,E14-1049,0,0.0298046,"ier et al., 2004; Laroche and Langlais, 2010; Vuli´c et al., 2011), it is rather unsuitable for small specialized comparable corpora due to the context vectors sparsity. (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson et al., 2009). More recent distributed approaches, based on deep neural network models (Mikolov et al., 2013), have come to renew traditional ones. Using these approaches, words are embedded into a low-dimensional vector space. For instance, Mikolov et al. (2013) proposed an approach to learn a linear transformation from the source language into the target language. Faruqui and Dyer (2014) used the Canonical Correlation Analysis to project the embeddings in both languages into a shared vector space. Xing et al. (2015) proposed an orthogonal transformation based on normalized word vectors to improve the inconsistency among the objective function used to learn the word vectors and to learn the linear transformation matrix. More recently, Artetxe et al. (2016; 2018a) proposed and then improved an approach that generalizes previous works in order to preserve monolingual invariance using several meaningful and intuitive constraints (i.e. orthogonality, vector length normalization, m"
2020.coling-main.527,P04-1067,0,0.106425,"Missing"
2020.coling-main.527,C16-1321,1,0.818387,"a scientific portal vs crawled from the web). On the other hand, for the general corpora, W IKI is way bigger and diverse in terms of topics than JRC. Table 1 shows the size of each side of the corpora and their vocabulary. Corpus BC WE JRC W IKI English # tokens # types 525,934 14,800 311,898 15,344 64.2M 229,836 300M 3M French # tokens # types 521,262 11,746 656,178 15,799 70.3M 231,126 300M 3.1M Table 1: Content words’ size corpora. The bilingual terminology reference lists required to evaluate the quality of bilingual terminology induction from comparable corpora are the same as used in (Hazem and Morin, 2016) and was derived from the UMLS meta-thesaurus for the breast cancer domain and are provided with the corpora for wind energy domain (see footnote 2). Each word in the reference list appears at least 5 times in the specialized corpus. The reference list for the breast cancer is composed of a set of 248 French/English single word pairs and of 145 single word pairs for the wind energy domain. We are aware that the reference lists are small, but considering the fact that we are in a specialized domain, they remain representative of its vocabulary, and getting larger lists is difficult since the sp"
2020.coling-main.527,C18-1080,1,0.856764,"ble corpora suffer from their modest and limited size compared to general language corpora. This phenomenon is amplified by the difficulty to obtain many specialized documents in a language other than English (Morin and Hazem, 2014). This aspect, which is gradually changing with the deployment of open access data, is nevertheless a major difficulty. One way to overcome the limited size of specialized (in-domain) comparable corpora for bilingual lexicon induction (BLI) is to associate them with out-of-domain resources as lexical databases (Bouamor et al., 2013) or large general domain corpora (Hazem and Morin, 2018). For instance, by combining a specialized comparable corpus with a general domain corpus, methods based on distributional analysis are boosted (Hazem and Morin, 2018). General domain corpora greatly enhance the representation of specialized vocabulary by adding new contexts. The main drawbacks of this data augmentation approach is the introduction of polysemous information as well as the tremendous increase of computation time. Consider for instance, a French/English comparable corpus in the medical domain and the French terms os (bone) and sein (breast). When looking for additional contexts"
2020.coling-main.527,E17-2096,1,0.84798,"ed an approach that generalizes previous works in order to preserve monolingual invariance using several meaningful and intuitive constraints (i.e. orthogonality, vector length normalization, mean centering, whitening, etc.). Finally, Conneau et al. (2017) and Artetxe et al. (2018b) proposed unsupervised mapping methods getting results close to supervised methods. A thorough comparison of cross-lingual word embeddings has been proposed by Ruder (2017). A comparison of the approaches of Mikolov et al. (2013) and Faruqui and Dyer (2014) with the standard bag of words approach was carried out by Jakubina and Langlais (2017). The authors showed that embedding approaches perform well when the terms to be translated occur very frequently while the standard approach is slightly better when the terms are less frequent. On specialized domains, Hazem and Morin (2018) compared different approaches and observed that the one described by Bojanowski et al. (2016) (an enhanced variant of the skip-gram and C-BOW models) outperformed the others. Recently, Peters et al. (2018) proposed a deep contextualized word representation that improves the state-of-the-art across different challenging NLP tasks. This representation is use"
2020.coling-main.527,C10-1070,0,0.0199229,"taining a gain of about 4 points in MAP while decreasing computation time by a factor of 10. 2 Related Work The historical distributional approach (Fung, 1998; Rapp, 1999), known as the standard bag of words approach, builds a context vector for each word of the source and the target languages, translates the source context vectors into the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. While this approach gives interesting results for large general comparable corpora (Gaussier et al., 2004; Laroche and Langlais, 2010; Vuli´c et al., 2011), it is rather unsuitable for small specialized comparable corpora due to the context vectors sparsity. (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson et al., 2009). More recent distributed approaches, based on deep neural network models (Mikolov et al., 2013), have come to renew traditional ones. Using these approaches, words are embedded into a low-dimensional vector space. For instance, Mikolov et al. (2013) proposed an approach to learn a linear transformation from the source language into the target language. Faruqui and Dyer (2014) used the Canonical C"
2020.coling-main.527,C10-1073,0,0.0329296,"to automate the process of generating and extending dictionaries from bilingual corpora is a well known Natural Language Processing (NLP) application. Initially conducted on parallel data, this modus operandi has rapidly moved to using comparable corpora, mainly due to their availability and the fact that they are easier to acquire (Sharoff et al., 2013). Comparable corpora gather texts of the same domain, often over the same period without being in a translation relation. Journalistic or encyclopedic websites as well as web crawl data are usually popular for this purpose (Rapp, 1999; Li and Gaussier, 2010; Rapp et al., 2020). In technical and scientific domains, comparable corpora suffer from their modest and limited size compared to general language corpora. This phenomenon is amplified by the difficulty to obtain many specialized documents in a language other than English (Morin and Hazem, 2014). This aspect, which is gradually changing with the deployment of open access data, is nevertheless a major difficulty. One way to overcome the limited size of specialized (in-domain) comparable corpora for bilingual lexicon induction (BLI) is to associate them with out-of-domain resources as lexical"
2020.coling-main.527,P10-2041,0,0.336699,"often considered as a marginal phenomenon in technical and scientific domains. In this sense, associating out-of-domain data with a specialized comparable corpus can be seen as a “heresy” or, as in the previous examples, an unfortunate way to introduce polysemy. In the context of Statistical Machine Translation (SMT), Wang et al. (2014) demonstrated that adding out-of-domain data to the training material of a system was detrimental when translating scientific and technical domains. Instead of using a data augmentation strategy, data selection is proposed to improve the quality of SMT systems (Moore and Lewis, 2010; Axelrod et al., 2011; Wang et al., 2014). The basic idea is that in-domain training data can be enriched with suitable sub-parts of out-of-domain data. Within this context, we apply two well-established data selection techniques often used in machine translation that is: Tf-Idf and cross entropy. We also propose for the first time to exploit BERT for data selection. We show that a subtle selection of the contexts of out-of-domain data allows to improve the representation of specialized domains, while preventing the introduction of polysemy induced by data augmentation. Overall, all the propo"
2020.coling-main.527,P14-1121,1,0.796818,"and the fact that they are easier to acquire (Sharoff et al., 2013). Comparable corpora gather texts of the same domain, often over the same period without being in a translation relation. Journalistic or encyclopedic websites as well as web crawl data are usually popular for this purpose (Rapp, 1999; Li and Gaussier, 2010; Rapp et al., 2020). In technical and scientific domains, comparable corpora suffer from their modest and limited size compared to general language corpora. This phenomenon is amplified by the difficulty to obtain many specialized documents in a language other than English (Morin and Hazem, 2014). This aspect, which is gradually changing with the deployment of open access data, is nevertheless a major difficulty. One way to overcome the limited size of specialized (in-domain) comparable corpora for bilingual lexicon induction (BLI) is to associate them with out-of-domain resources as lexical databases (Bouamor et al., 2013) or large general domain corpora (Hazem and Morin, 2018). For instance, by combining a specialized comparable corpus with a general domain corpus, methods based on distributional analysis are boosted (Hazem and Morin, 2018). General domain corpora greatly enhance th"
2020.coling-main.527,P07-1084,1,0.566345,"the standard bag of words approach, builds a context vector for each word of the source and the target languages, translates the source context vectors into the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. While this approach gives interesting results for large general comparable corpora (Gaussier et al., 2004; Laroche and Langlais, 2010; Vuli´c et al., 2011), it is rather unsuitable for small specialized comparable corpora due to the context vectors sparsity. (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson et al., 2009). More recent distributed approaches, based on deep neural network models (Mikolov et al., 2013), have come to renew traditional ones. Using these approaches, words are embedded into a low-dimensional vector space. For instance, Mikolov et al. (2013) proposed an approach to learn a linear transformation from the source language into the target language. Faruqui and Dyer (2014) used the Canonical Correlation Analysis to project the embeddings in both languages into a shared vector space. Xing et al. (2015) proposed an orthogonal transformation based on normalized word"
2020.coling-main.527,N18-1202,0,0.00783104,". A comparison of the approaches of Mikolov et al. (2013) and Faruqui and Dyer (2014) with the standard bag of words approach was carried out by Jakubina and Langlais (2017). The authors showed that embedding approaches perform well when the terms to be translated occur very frequently while the standard approach is slightly better when the terms are less frequent. On specialized domains, Hazem and Morin (2018) compared different approaches and observed that the one described by Bojanowski et al. (2016) (an enhanced variant of the skip-gram and C-BOW models) outperformed the others. Recently, Peters et al. (2018) proposed a deep contextualized word representation that improves the state-of-the-art across different challenging NLP tasks. This representation is useful to model different types of syntactic and semantic information about words-in-context. It is thus possible from a general 6003 language corpus to have different embeddings for a given word, with each of these embeddings reflecting one of the word’s meanings. In specialized domains, it is widely accepted that words can have only one meaning even if their contexts may convey different meanings in a general language corpus. El Boukkouri et al"
2020.coling-main.527,2009.mtsummit-posters.14,1,0.689516,"words approach, builds a context vector for each word of the source and the target languages, translates the source context vectors into the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. While this approach gives interesting results for large general comparable corpora (Gaussier et al., 2004; Laroche and Langlais, 2010; Vuli´c et al., 2011), it is rather unsuitable for small specialized comparable corpora due to the context vectors sparsity. (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson et al., 2009). More recent distributed approaches, based on deep neural network models (Mikolov et al., 2013), have come to renew traditional ones. Using these approaches, words are embedded into a low-dimensional vector space. For instance, Mikolov et al. (2013) proposed an approach to learn a linear transformation from the source language into the target language. Faruqui and Dyer (2014) used the Canonical Correlation Analysis to project the embeddings in both languages into a shared vector space. Xing et al. (2015) proposed an orthogonal transformation based on normalized word vectors to improve the inc"
2020.coling-main.527,2020.bucc-1.2,0,0.0199533,"process of generating and extending dictionaries from bilingual corpora is a well known Natural Language Processing (NLP) application. Initially conducted on parallel data, this modus operandi has rapidly moved to using comparable corpora, mainly due to their availability and the fact that they are easier to acquire (Sharoff et al., 2013). Comparable corpora gather texts of the same domain, often over the same period without being in a translation relation. Journalistic or encyclopedic websites as well as web crawl data are usually popular for this purpose (Rapp, 1999; Li and Gaussier, 2010; Rapp et al., 2020). In technical and scientific domains, comparable corpora suffer from their modest and limited size compared to general language corpora. This phenomenon is amplified by the difficulty to obtain many specialized documents in a language other than English (Morin and Hazem, 2014). This aspect, which is gradually changing with the deployment of open access data, is nevertheless a major difficulty. One way to overcome the limited size of specialized (in-domain) comparable corpora for bilingual lexicon induction (BLI) is to associate them with out-of-domain resources as lexical databases (Bouamor e"
2020.coling-main.527,P99-1067,0,0.318478,"slation equivalents to automate the process of generating and extending dictionaries from bilingual corpora is a well known Natural Language Processing (NLP) application. Initially conducted on parallel data, this modus operandi has rapidly moved to using comparable corpora, mainly due to their availability and the fact that they are easier to acquire (Sharoff et al., 2013). Comparable corpora gather texts of the same domain, often over the same period without being in a translation relation. Journalistic or encyclopedic websites as well as web crawl data are usually popular for this purpose (Rapp, 1999; Li and Gaussier, 2010; Rapp et al., 2020). In technical and scientific domains, comparable corpora suffer from their modest and limited size compared to general language corpora. This phenomenon is amplified by the difficulty to obtain many specialized documents in a language other than English (Morin and Hazem, 2014). This aspect, which is gradually changing with the deployment of open access data, is nevertheless a major difficulty. One way to overcome the limited size of specialized (in-domain) comparable corpora for bilingual lexicon induction (BLI) is to associate them with out-of-domai"
2020.coling-main.527,tiedemann-2012-parallel,0,0.0292801,"itle or the keywords contain the term breast cancer in English and its translation in French. The Wind Energy corpus (WE) has been released within the TTC project2 and is composed of documents harvested from the Web using a focused crawler based on several keywords such as wind, energy, rotor in English and their translation in French. We considered two separate out-of-domain data sets in English and French: i) JRC acquis corpus (JRC) composed of legislative texts of the European Union3 (we used the French/English version at OPUS which is based on the paragraph-aligned corpus provided by JRC (Tiedemann, 2012)) and ii) a fraction of Wikipedia corpus (W IKI)4 . The used corpora differ in size and quality. On the one hand, for the specialized corpora, BC is of better quality than WE, due to their construction methods (crawled from a scientific portal vs crawled from the web). On the other hand, for the general corpora, W IKI is way bigger and diverse in terms of topics than JRC. Table 1 shows the size of each side of the corpora and their vocabulary. Corpus BC WE JRC W IKI English # tokens # types 525,934 14,800 311,898 15,344 64.2M 229,836 300M 3M French # tokens # types 521,262 11,746 656,178 15,79"
2020.coling-main.527,P11-2084,0,0.0836974,"Missing"
2020.coling-main.527,N15-1104,0,0.019821,"to the context vectors sparsity. (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson et al., 2009). More recent distributed approaches, based on deep neural network models (Mikolov et al., 2013), have come to renew traditional ones. Using these approaches, words are embedded into a low-dimensional vector space. For instance, Mikolov et al. (2013) proposed an approach to learn a linear transformation from the source language into the target language. Faruqui and Dyer (2014) used the Canonical Correlation Analysis to project the embeddings in both languages into a shared vector space. Xing et al. (2015) proposed an orthogonal transformation based on normalized word vectors to improve the inconsistency among the objective function used to learn the word vectors and to learn the linear transformation matrix. More recently, Artetxe et al. (2016; 2018a) proposed and then improved an approach that generalizes previous works in order to preserve monolingual invariance using several meaningful and intuitive constraints (i.e. orthogonality, vector length normalization, mean centering, whitening, etc.). Finally, Conneau et al. (2017) and Artetxe et al. (2018b) proposed unsupervised mapping methods ge"
2020.coling-main.549,Q19-1011,0,0.0255003,"Missing"
2020.coling-main.549,A00-2004,0,0.500584,"segments are merged if they are highly correlated. On the contrary, if their similarity is below a certain threshold, a shift is determined (Hearst, 1997; Riedl and Biemann, 2012). When sufficient topically annotated training data are available, deep neural approaches based on CNN (Wang et al., 2017) or LSTM (Koshorek et al., 2018) can be efficiently applied (Li et al., 2018; Arnold et al., 2019). Until now, text segmentation methods have exclusively addressed data sets lying within the scope of narrative and expository texts or user dialogues texts and sometimes artificially generated data (Choi, 2000; Jeong and Titov, 2010; Glavaˇs et al., 2016; Koshorek et al., 2018). In this paper, we address transcriptions of ancient devotional manuscripts (thereafter also “MS”), from the Middle Ages, known as “books of hours”. Books of hours were used by lay people as a guidance in their daily prayers. They represent an important source of information on the late Middle Ages’ religious and social practices, and provide opportunities for historical analysis in order to better understand the cultures and faiths of the European society. More than 10,000 manuscripts of ca. 300 pages in average are preserv"
2020.coling-main.549,N19-1423,0,0.00606591,"e MS Arsenal 637. The first column refers to transcriptions, the second column to the gold reference of level 1 (Gospel Lections), and the third to the predicted labels of the SVM classifier. We consider the task of segmentation as a classification problem at the line break level. Each line is represented by its corresponding section’s label. We assume that if enough lines of a given section are correctly classified, segmentation can be efficiently performed thanks to a greedy merging approach. To perform line classification, we chose to experiment with Support Vector machines (SVM) and BERT (Devlin et al., 2019)6 . For SVM, Tf-Idf features are calculated over unigrams and bigrams at the line level. We also used BERT for multi-class classification (the multi-class resides in all the section labels as depicted in Table 1). Each line is associated with its corresponding class (of level 1, level 2 or level 3). Then, BERT is trained to predict the section label of each line. We also, used BERT for sentence pair classification assuming that more information can be captured if we take advantage of the next line of books of hours to predict the current line label. We refer to this approach as BERT* by contra"
2020.coling-main.549,N09-1040,0,0.260281,"This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons. org/licenses/by/4.0/. 6240 Proceedings of the 28th International Conference on Computational Linguistics, pages 6240–6251 Barcelona, Spain (Online), December 8-13, 2020 of books of hours segmentation as a classification problem and propose a greedy two-step bottom-up approach that achieves significant results on books of hours. 2 Related Work Text segmentation is the task of splitting documents into topically coherent fragments for a better text readability and analysis (Hearst, 1994; Eisenstein, 2009; Glavaˇs et al., 2016). It is also useful in other NLP and IR (Moens and Busser, 2001) applications such as: summarization, document navigation and indexing, passage retrieval, etc. Segmentation can be content-based where each topic is characterised by a specific vocabulary and each vocabulary change implies a topic change (Hearst, 1994). It can also use topic markers (Fauconnier et al., 2014) whether (i) oral: such as prosody, silence; (ii) written: using connectors, introductory expressions or (iii) visual: using line breaks, bullets, numbering, bold, etc. A broad range of unsupervised appr"
2020.coling-main.549,C98-1062,0,0.449671,"rised by a specific vocabulary and each vocabulary change implies a topic change (Hearst, 1994). It can also use topic markers (Fauconnier et al., 2014) whether (i) oral: such as prosody, silence; (ii) written: using connectors, introductory expressions or (iii) visual: using line breaks, bullets, numbering, bold, etc. A broad range of unsupervised approaches exploit lexical cohesion to detect coherent segments (Hearst, 1997; Choi, 2000) thanks to term repetitions (Hearst, 1994), semantic relations using lexical chains (Morris and Hirst, 1991), dictionary (Kozima, 1993), collocation networks (Ferret et al., 1998), or patterns of lexical co-occurrences (Hearst, 1997) such as discourse structures (Nomoto and Nitta, 1994). Early unsupervised methods include: TextTiling, a TF Cosine based approach (Hearst, 1994), LCSeg, based on lexical chains (Galley et al., 2003), U00, a probabilistic dynamic programming approach (Utiyama and Isahara, 2001), TopicTiling, a topic modeling approach based on Latent Dirichlet Analysis (LDA) (Riedl and Biemann, 2012). Glavaˇs et al. (2016) proposed a semantic relatedness graph approach that exploits word embeddings. Alemi and Ginsparg (2015) and Naili et al. (2017) studied t"
2020.coling-main.549,P03-1071,0,0.351987,"or (iii) visual: using line breaks, bullets, numbering, bold, etc. A broad range of unsupervised approaches exploit lexical cohesion to detect coherent segments (Hearst, 1997; Choi, 2000) thanks to term repetitions (Hearst, 1994), semantic relations using lexical chains (Morris and Hirst, 1991), dictionary (Kozima, 1993), collocation networks (Ferret et al., 1998), or patterns of lexical co-occurrences (Hearst, 1997) such as discourse structures (Nomoto and Nitta, 1994). Early unsupervised methods include: TextTiling, a TF Cosine based approach (Hearst, 1994), LCSeg, based on lexical chains (Galley et al., 2003), U00, a probabilistic dynamic programming approach (Utiyama and Isahara, 2001), TopicTiling, a topic modeling approach based on Latent Dirichlet Analysis (LDA) (Riedl and Biemann, 2012). Glavaˇs et al. (2016) proposed a semantic relatedness graph approach that exploits word embeddings. Alemi and Ginsparg (2015) and Naili et al. (2017) studied the contribution of word embeddings on classical segmentation approaches. Text segmentation has also been addressed as a multi-document segmentation problem. Sun et al. (2007) for instance, proposed a method for shared topic detection and topic segmentat"
2020.coling-main.549,S16-2016,0,0.0512384,"Missing"
2020.coling-main.549,2020.lrec-1.97,1,0.318447,"prayers are used in several hours of the day and several times within one copy, thus generating section ambiguities. For historians, automating the generation of table of contents is key to understand this complex historical source. While the building block of the mainstream text segmentation methods is closely related to topical shifts, the liturgical aspect of books of hours exhibits shallow topical relations and a strong correlation between their sections and subsections. As consequence, the topical shift hypothesis becomes inconsistent for this type of data, as has been recently shown in (Hazem et al., 2020). We address the task This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons. org/licenses/by/4.0/. 6240 Proceedings of the 28th International Conference on Computational Linguistics, pages 6240–6251 Barcelona, Spain (Online), December 8-13, 2020 of books of hours segmentation as a classification problem and propose a greedy two-step bottom-up approach that achieves significant results on books of hours. 2 Related Work Text segmentation is the task of splitting documents into topically coherent fragments for a better text readability and an"
2020.coling-main.549,P94-1002,0,0.805753,"ress the task This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons. org/licenses/by/4.0/. 6240 Proceedings of the 28th International Conference on Computational Linguistics, pages 6240–6251 Barcelona, Spain (Online), December 8-13, 2020 of books of hours segmentation as a classification problem and propose a greedy two-step bottom-up approach that achieves significant results on books of hours. 2 Related Work Text segmentation is the task of splitting documents into topically coherent fragments for a better text readability and analysis (Hearst, 1994; Eisenstein, 2009; Glavaˇs et al., 2016). It is also useful in other NLP and IR (Moens and Busser, 2001) applications such as: summarization, document navigation and indexing, passage retrieval, etc. Segmentation can be content-based where each topic is characterised by a specific vocabulary and each vocabulary change implies a topic change (Hearst, 1994). It can also use topic markers (Fauconnier et al., 2014) whether (i) oral: such as prosody, silence; (ii) written: using connectors, introductory expressions or (iii) visual: using line breaks, bullets, numbering, bold, etc. A broad range of"
2020.coling-main.549,J97-1003,0,0.913737,"overarching differences underlying conception about Church. 1 Introduction Text segmentation is essential in many downstream applications including document understanding and navigation, summarization, information retrieval and discourse parsing (Purver, 2011; Riedl and Biemann, 2012; Li et al., 2018). Traditional unsupervised approaches assume a high correlation between segments and subtopics. Therefore, based on a prior text decomposition, two adjacent segments are merged if they are highly correlated. On the contrary, if their similarity is below a certain threshold, a shift is determined (Hearst, 1997; Riedl and Biemann, 2012). When sufficient topically annotated training data are available, deep neural approaches based on CNN (Wang et al., 2017) or LSTM (Koshorek et al., 2018) can be efficiently applied (Li et al., 2018; Arnold et al., 2019). Until now, text segmentation methods have exclusively addressed data sets lying within the scope of narrative and expository texts or user dialogues texts and sometimes artificially generated data (Choi, 2000; Jeong and Titov, 2010; Glavaˇs et al., 2016; Koshorek et al., 2018). In this paper, we address transcriptions of ancient devotional manuscript"
2020.coling-main.549,J15-3002,0,0.0256454,"classical segmentation approaches. Text segmentation has also been addressed as a multi-document segmentation problem. Sun et al. (2007) for instance, proposed a method for shared topic detection and topic segmentation of multiple similar documents based on weighted mutual information, while Jeong and Titov (2010) proposed an unsupervised bayesian approach that models both shared and document-specific topics. Supervised approaches have also modeled semantic cohesion. Some methods performed segmentation at the sentence level to discover Elementary Discourse Units (EDU) (Hernault et al., 2010; Joty et al., 2015) while others focused on dialogue. Neural network approaches have also been applied such as: TextTiling-like embedding approach for query-reply dialogue segmentation (Song et al., 2016), multi-party dialogue for EDU using sequential model (Shi and Huang, 2019) and reinforcement learning (Takanobu et al., 2018). Recently, Li et al. (2018) proposed SegBot, a bidirectional RNN coupled with a pointer network that addresses both topic segmentation and EDU. Also, LSTM or CNN based approaches have been proposed, for instance through bidirectional layers (Sheikh et al., 2017), sentence embedding-based"
2020.coling-main.549,C14-1005,0,0.0173235,"(Morris, 1988) as well as synthetic data sets (Choi, 2000; Galley et al., 2003) were often used. Later on, data sets with hierarchical structure were addressed, which required a more fine-grained subtopic structure analysis (Yaari, 1997; Eisenstein, 2009). Yaari (1997) proposed one of the first approaches for hierarchical text segmentation: a supervised agglomerative bottom-up clustering method exploiting paragraph hierarchy. A pioneer unsupervised approach for hierarchical text segmentation was introduced by Eisenstein (2009) using a bayesian generative model with dynamic programming. Also, Kazantseva and Szpakowicz (2014) proposed a clustering algorithm based on topical trees to perform hierarchical segmentation. Recently, several data sets have been published showing various types of structures: artificial added to automatic speech recognition transcripts of news videos (Sheikh et al., 2017), encyclopedic reflecting Wikipedia article structure (Koshorek et al., 2018; Arnold et al., 2019) or topical in goal-oriented dialogues (Takanobu et al., 2018). Our data set encompasses books of hours, each of them with a complex original structure described in the following section. 6241 3 Books of Hours Books of hours c"
2020.coling-main.549,N18-2075,0,0.14391,"and navigation, summarization, information retrieval and discourse parsing (Purver, 2011; Riedl and Biemann, 2012; Li et al., 2018). Traditional unsupervised approaches assume a high correlation between segments and subtopics. Therefore, based on a prior text decomposition, two adjacent segments are merged if they are highly correlated. On the contrary, if their similarity is below a certain threshold, a shift is determined (Hearst, 1997; Riedl and Biemann, 2012). When sufficient topically annotated training data are available, deep neural approaches based on CNN (Wang et al., 2017) or LSTM (Koshorek et al., 2018) can be efficiently applied (Li et al., 2018; Arnold et al., 2019). Until now, text segmentation methods have exclusively addressed data sets lying within the scope of narrative and expository texts or user dialogues texts and sometimes artificially generated data (Choi, 2000; Jeong and Titov, 2010; Glavaˇs et al., 2016; Koshorek et al., 2018). In this paper, we address transcriptions of ancient devotional manuscripts (thereafter also “MS”), from the Middle Ages, known as “books of hours”. Books of hours were used by lay people as a guidance in their daily prayers. They represent an important"
2020.coling-main.549,P93-1041,0,0.750297,"nt-based where each topic is characterised by a specific vocabulary and each vocabulary change implies a topic change (Hearst, 1994). It can also use topic markers (Fauconnier et al., 2014) whether (i) oral: such as prosody, silence; (ii) written: using connectors, introductory expressions or (iii) visual: using line breaks, bullets, numbering, bold, etc. A broad range of unsupervised approaches exploit lexical cohesion to detect coherent segments (Hearst, 1997; Choi, 2000) thanks to term repetitions (Hearst, 1994), semantic relations using lexical chains (Morris and Hirst, 1991), dictionary (Kozima, 1993), collocation networks (Ferret et al., 1998), or patterns of lexical co-occurrences (Hearst, 1997) such as discourse structures (Nomoto and Nitta, 1994). Early unsupervised methods include: TextTiling, a TF Cosine based approach (Hearst, 1994), LCSeg, based on lexical chains (Galley et al., 2003), U00, a probabilistic dynamic programming approach (Utiyama and Isahara, 2001), TopicTiling, a topic modeling approach based on Latent Dirichlet Analysis (LDA) (Riedl and Biemann, 2012). Glavaˇs et al. (2016) proposed a semantic relatedness graph approach that exploits word embeddings. Alemi and Ginsp"
2020.coling-main.549,P06-1004,0,0.0820343,"6.1 Experimental Setup Data To test SVM and BERT classifiers as well as books of hours segmentation, we used four books of hours (Arsenal 637, Beaune 55, Caen FMM.273 and Zurich Rh.169). The 6 remaining books were used for training. As books of hours are mostly written in Latin, we used the bert-base-multilingualcased model. For the fine-tuning phase of BERT, we used the simpletransformers7 library and its default parameters setting with 50 epochs8 . Baselines We evaluated: (i) five unsupervised approaches: TextTiling (Hearst, 1994), C99 (Choi, 2000), U00 (Utiyama and Isahara, 2001), MinCut (Malioutov and Barzilay, 2006), HierBays (Eisenstein, 2009). Due to the lack of large annotated training data, we did not evaluate other classifiers-based and deep learning-based approaches on the book of hours corpus. Evaluation Metrics The approaches are evaluated in terms of Pk (Beeferman et al., 1999) and Windowdiff (W D) (Pevzner and Hearst, 2002) metrics. Pk is an error metric which combines precision and recall to estimate the relative contributions of the different feature types. Nonetheless, it exhibits several drawbacks. Pk is affected by segment size variation. It also penalizes more heavily false negatives than"
2020.coling-main.549,J91-1002,0,0.482448,"ieval, etc. Segmentation can be content-based where each topic is characterised by a specific vocabulary and each vocabulary change implies a topic change (Hearst, 1994). It can also use topic markers (Fauconnier et al., 2014) whether (i) oral: such as prosody, silence; (ii) written: using connectors, introductory expressions or (iii) visual: using line breaks, bullets, numbering, bold, etc. A broad range of unsupervised approaches exploit lexical cohesion to detect coherent segments (Hearst, 1997; Choi, 2000) thanks to term repetitions (Hearst, 1994), semantic relations using lexical chains (Morris and Hirst, 1991), dictionary (Kozima, 1993), collocation networks (Ferret et al., 1998), or patterns of lexical co-occurrences (Hearst, 1997) such as discourse structures (Nomoto and Nitta, 1994). Early unsupervised methods include: TextTiling, a TF Cosine based approach (Hearst, 1994), LCSeg, based on lexical chains (Galley et al., 2003), U00, a probabilistic dynamic programming approach (Utiyama and Isahara, 2001), TopicTiling, a topic modeling approach based on Latent Dirichlet Analysis (LDA) (Riedl and Biemann, 2012). Glavaˇs et al. (2016) proposed a semantic relatedness graph approach that exploits word"
2020.coling-main.549,C94-2187,0,0.650551,"lso use topic markers (Fauconnier et al., 2014) whether (i) oral: such as prosody, silence; (ii) written: using connectors, introductory expressions or (iii) visual: using line breaks, bullets, numbering, bold, etc. A broad range of unsupervised approaches exploit lexical cohesion to detect coherent segments (Hearst, 1997; Choi, 2000) thanks to term repetitions (Hearst, 1994), semantic relations using lexical chains (Morris and Hirst, 1991), dictionary (Kozima, 1993), collocation networks (Ferret et al., 1998), or patterns of lexical co-occurrences (Hearst, 1997) such as discourse structures (Nomoto and Nitta, 1994). Early unsupervised methods include: TextTiling, a TF Cosine based approach (Hearst, 1994), LCSeg, based on lexical chains (Galley et al., 2003), U00, a probabilistic dynamic programming approach (Utiyama and Isahara, 2001), TopicTiling, a topic modeling approach based on Latent Dirichlet Analysis (LDA) (Riedl and Biemann, 2012). Glavaˇs et al. (2016) proposed a semantic relatedness graph approach that exploits word embeddings. Alemi and Ginsparg (2015) and Naili et al. (2017) studied the contribution of word embeddings on classical segmentation approaches. Text segmentation has also been add"
2020.coling-main.549,J02-1002,0,0.175458,"For the fine-tuning phase of BERT, we used the simpletransformers7 library and its default parameters setting with 50 epochs8 . Baselines We evaluated: (i) five unsupervised approaches: TextTiling (Hearst, 1994), C99 (Choi, 2000), U00 (Utiyama and Isahara, 2001), MinCut (Malioutov and Barzilay, 2006), HierBays (Eisenstein, 2009). Due to the lack of large annotated training data, we did not evaluate other classifiers-based and deep learning-based approaches on the book of hours corpus. Evaluation Metrics The approaches are evaluated in terms of Pk (Beeferman et al., 1999) and Windowdiff (W D) (Pevzner and Hearst, 2002) metrics. Pk is an error metric which combines precision and recall to estimate the relative contributions of the different feature types. Nonetheless, it exhibits several drawbacks. Pk is affected by segment size variation. It also penalizes more heavily false negatives than false positives and overpenalizes near misses. Hence, a second measure, W D, a variant of Pk , is also used as it equally penalizes false positives and near misses. 6.2 Results Arsenal637 Beaune55 Caen FMM.273 Zurich Rh.169 Level 1 73.89 68.39 67.78 66.63 SVM Level 2 60.02 54.74 57.30 46.42 Level 3 48.79 47.85 53.14 43.63"
2020.coling-main.549,W12-3307,0,0.113902,"tation boundaries. We show that the main state-of-the-art segmentation methods are either inefficient or inapplicable for books of hours and propose a bottom-up greedy approach that considerably enhances the segmentation results. We stress the importance of such hierarchical segmentation of books of hours for historians to explore their overarching differences underlying conception about Church. 1 Introduction Text segmentation is essential in many downstream applications including document understanding and navigation, summarization, information retrieval and discourse parsing (Purver, 2011; Riedl and Biemann, 2012; Li et al., 2018). Traditional unsupervised approaches assume a high correlation between segments and subtopics. Therefore, based on a prior text decomposition, two adjacent segments are merged if they are highly correlated. On the contrary, if their similarity is below a certain threshold, a shift is determined (Hearst, 1997; Riedl and Biemann, 2012). When sufficient topically annotated training data are available, deep neural approaches based on CNN (Wang et al., 2017) or LSTM (Koshorek et al., 2018) can be efficiently applied (Li et al., 2018; Arnold et al., 2019). Until now, text segmenta"
2020.coling-main.549,P01-1064,0,0.615875,"ad range of unsupervised approaches exploit lexical cohesion to detect coherent segments (Hearst, 1997; Choi, 2000) thanks to term repetitions (Hearst, 1994), semantic relations using lexical chains (Morris and Hirst, 1991), dictionary (Kozima, 1993), collocation networks (Ferret et al., 1998), or patterns of lexical co-occurrences (Hearst, 1997) such as discourse structures (Nomoto and Nitta, 1994). Early unsupervised methods include: TextTiling, a TF Cosine based approach (Hearst, 1994), LCSeg, based on lexical chains (Galley et al., 2003), U00, a probabilistic dynamic programming approach (Utiyama and Isahara, 2001), TopicTiling, a topic modeling approach based on Latent Dirichlet Analysis (LDA) (Riedl and Biemann, 2012). Glavaˇs et al. (2016) proposed a semantic relatedness graph approach that exploits word embeddings. Alemi and Ginsparg (2015) and Naili et al. (2017) studied the contribution of word embeddings on classical segmentation approaches. Text segmentation has also been addressed as a multi-document segmentation problem. Sun et al. (2007) for instance, proposed a method for shared topic detection and topic segmentation of multiple similar documents based on weighted mutual information, while J"
2020.coling-main.549,D17-1139,0,0.0718028,"uding document understanding and navigation, summarization, information retrieval and discourse parsing (Purver, 2011; Riedl and Biemann, 2012; Li et al., 2018). Traditional unsupervised approaches assume a high correlation between segments and subtopics. Therefore, based on a prior text decomposition, two adjacent segments are merged if they are highly correlated. On the contrary, if their similarity is below a certain threshold, a shift is determined (Hearst, 1997; Riedl and Biemann, 2012). When sufficient topically annotated training data are available, deep neural approaches based on CNN (Wang et al., 2017) or LSTM (Koshorek et al., 2018) can be efficiently applied (Li et al., 2018; Arnold et al., 2019). Until now, text segmentation methods have exclusively addressed data sets lying within the scope of narrative and expository texts or user dialogues texts and sometimes artificially generated data (Choi, 2000; Jeong and Titov, 2010; Glavaˇs et al., 2016; Koshorek et al., 2018). In this paper, we address transcriptions of ancient devotional manuscripts (thereafter also “MS”), from the Middle Ages, known as “books of hours”. Books of hours were used by lay people as a guidance in their daily praye"
2020.computerm-1.13,P08-3001,0,0.0241656,"Five teams have participated in the TermEval shared task. All teams submitted results for English, three submitted for French and two for Dutch. We submitted results for the French and English data sets. The Precision, recall, and F1-score were calculated twice: once including and once excluding Named Entities. Automatic terminology extraction (ATE) is a very challenging task beneficial to a broad range of natural language processing applications, including machine translation, bilingual lexicon induction, thesauri construction (Lin, 1998; Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Hagiwara, 2008; Andrade et al., 2013; Rigouts Terryn et al., 2019), to cite a few. Traditionally, this task is conducted by a terminologist, but hand-operated exploration, indexation, and maintenance of domain-specific corpora and terminologies is a costly enterprise. The automatization aims to ease effort demanded to manually identify terms in domain-specific corpora by automatically providing a ranked list of candidate terms. Despite being a well-established research domain for decades, NLP methods still fail to meet human standards, and ATE is still considered an unsolved problem with considerable room f"
2020.computerm-1.13,P14-1119,0,0.0952674,"Missing"
2020.computerm-1.13,P98-2127,0,0.195329,"a sets are described in detail in (Rigouts Terryn et al., 2019). Five teams have participated in the TermEval shared task. All teams submitted results for English, three submitted for French and two for Dutch. We submitted results for the French and English data sets. The Precision, recall, and F1-score were calculated twice: once including and once excluding Named Entities. Automatic terminology extraction (ATE) is a very challenging task beneficial to a broad range of natural language processing applications, including machine translation, bilingual lexicon induction, thesauri construction (Lin, 1998; Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Hagiwara, 2008; Andrade et al., 2013; Rigouts Terryn et al., 2019), to cite a few. Traditionally, this task is conducted by a terminologist, but hand-operated exploration, indexation, and maintenance of domain-specific corpora and terminologies is a costly enterprise. The automatization aims to ease effort demanded to manually identify terms in domain-specific corpora by automatically providing a ranked list of candidate terms. Despite being a well-established research domain for decades, NLP methods still fail to meet human standards, and"
2020.computerm-1.13,L18-1284,0,0.024631,"nually identify terms in domain-specific corpora by automatically providing a ranked list of candidate terms. Despite being a well-established research domain for decades, NLP methods still fail to meet human standards, and ATE is still considered an unsolved problem with considerable room for improvement. If it is generally admitted that terms are single words or multiword expressions representing domain-specific concepts and that terminologies are the body of terms used with a particular domain, the lack of annotated data and agreement between researchers make ATE evaluation very difficult (Terryn et al., 2018). In order to gather researchers around a common evaluation scheme, TermEval shared task (Rigouts Terryn et al., 2019) offers a unified framework aiming a better ATE’s comprehension and analysis 1 . The shared task provides four data sets: Corruption, dressage, wind energy and heart failure; in three languages: English, French and Dutch. With the advance of neural network language models and following the current trend and excellent results obtained by transformer architecture on other NLP tasks, we have decided to experiment and compare two classification methods, one feature-based and the BE"
2020.computerm-1.13,W16-4702,0,0.287767,"we have empirically determined that only the elements that correlate at more than a certain threshold (mean correlation) with our target class are retained for classification (bolded in 1). 3.2. BERT BERT has proven to be efficient in many downstream NLP tasks (Devlin et al., 2018) including next sentence prediction, question answering and named entity recognition (NER). It can also be used for feature extraction or classification. Prior to the emergence of transformer-based architectures like BERT, several deep learning architectures for terminology extraction have been proposed. Wang et al. (2016) introduce a weakly-supervised classification-based approach. Amjadian et al. (2016) leverage local and global embeddings to encapsulate the meaning and behavior of the term for the classification step, although they only work with unigram terms. We must note that exploring these architectures is not the focus of this work; we mainly want to observe how BERTbased models can be used for ATE and how they perform in comparison to more traditional feature-based methods. In order to do that, we use different versions of BERT as a binary classifier for term prediction. For English, we use RoBERTa (L"
2020.computerm-1.13,I13-1150,0,0.0188269,"participated in the TermEval shared task. All teams submitted results for English, three submitted for French and two for Dutch. We submitted results for the French and English data sets. The Precision, recall, and F1-score were calculated twice: once including and once excluding Named Entities. Automatic terminology extraction (ATE) is a very challenging task beneficial to a broad range of natural language processing applications, including machine translation, bilingual lexicon induction, thesauri construction (Lin, 1998; Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Hagiwara, 2008; Andrade et al., 2013; Rigouts Terryn et al., 2019), to cite a few. Traditionally, this task is conducted by a terminologist, but hand-operated exploration, indexation, and maintenance of domain-specific corpora and terminologies is a costly enterprise. The automatization aims to ease effort demanded to manually identify terms in domain-specific corpora by automatically providing a ranked list of candidate terms. Despite being a well-established research domain for decades, NLP methods still fail to meet human standards, and ATE is still considered an unsolved problem with considerable room for improvement. If it"
2020.computerm-1.13,P06-2111,0,0.102401,"Missing"
2020.computerm-1.13,I08-2084,0,0.0686425,"Missing"
2020.computerm-1.13,U16-1011,0,0.248174,"eral tests, we have empirically determined that only the elements that correlate at more than a certain threshold (mean correlation) with our target class are retained for classification (bolded in 1). 3.2. BERT BERT has proven to be efficient in many downstream NLP tasks (Devlin et al., 2018) including next sentence prediction, question answering and named entity recognition (NER). It can also be used for feature extraction or classification. Prior to the emergence of transformer-based architectures like BERT, several deep learning architectures for terminology extraction have been proposed. Wang et al. (2016) introduce a weakly-supervised classification-based approach. Amjadian et al. (2016) leverage local and global embeddings to encapsulate the meaning and behavior of the term for the classification step, although they only work with unigram terms. We must note that exploring these architectures is not the focus of this work; we mainly want to observe how BERTbased models can be used for ATE and how they perform in comparison to more traditional feature-based methods. In order to do that, we use different versions of BERT as a binary classifier for term prediction. For English, we use RoBERTa (L"
2020.computerm-1.13,W03-1610,0,0.176588,"described in detail in (Rigouts Terryn et al., 2019). Five teams have participated in the TermEval shared task. All teams submitted results for English, three submitted for French and two for Dutch. We submitted results for the French and English data sets. The Precision, recall, and F1-score were calculated twice: once including and once excluding Named Entities. Automatic terminology extraction (ATE) is a very challenging task beneficial to a broad range of natural language processing applications, including machine translation, bilingual lexicon induction, thesauri construction (Lin, 1998; Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Hagiwara, 2008; Andrade et al., 2013; Rigouts Terryn et al., 2019), to cite a few. Traditionally, this task is conducted by a terminologist, but hand-operated exploration, indexation, and maintenance of domain-specific corpora and terminologies is a costly enterprise. The automatization aims to ease effort demanded to manually identify terms in domain-specific corpora by automatically providing a ranked list of candidate terms. Despite being a well-established research domain for decades, NLP methods still fail to meet human standards, and ATE is still consi"
2020.computerm-1.9,P16-4003,1,0.830232,"Nist, the terms alphabetically ordered with their definitions; 6.1. Term Extraction Approaches Term Extraction Tools In this section we provide a description of the chosen tools to execute the terminology extraction. Cyber The Cybersecurity term list contains candidate terms taken from the post-processed texts connected together through the main semantic relationships proper to thesauri (Broughton, 2008), i.e., hierarchical, synonymy, association. These relations are respectively formalized by standard tags (ISO/TC 46/SC 9 2011 and 2013): 6.1.1. TermSuite - Variants Detection Tool TermSuite (Cram and Daille, 2016) is a toolkit for terminology extraction and multilingual term alignment. Its performance is quite immediate when it runs over big data sets. The term extraction provided by TermSuite is a list of representative terms that are presented together with different properties, e.g., their frequency, accuracy, specificity. Terms are therefore ordered according to their unithood and application to the domain. One of the main feature that broader term broader term (BT) that stands for hyperonyms; 65 shapes the quality of this software is its syntactic and morphological variants detection among terms,"
2020.computerm-1.9,daille-hazem-2014-semi,1,0.903205,"that can provide semantic knowledge density and granularity about the lexicon that is meant to be represented (Barri`ere, 2006). These structures are in literature known as TKBs (Terminological Knowledge Bases) (Condamines, 2018), and, indeed, they support the modalities of systematizing the specialized knowledge by merging the skills proper to linguistics and knowledge engineering. The ways in which the candidate terms are extracted from a specific domainoriented corpus (Loginova Clouet et al., 2012) usually follow text pre-processing procedures and extraction of single and multi-word units (Daille and Hazem, 2014) from texts filtered out by frequency measures, then they can undergo a phase of variation recognition (Weller et al., 2011) and other statistical calculations to determine the specificity, accuracy, similarity in the texts from which they come from (Cabr´e et al., 2001). The reason why the domain-oriented terms are called ‘candidates’ (Condamines, 2018) is linked to the fact that in the terminologists’ activity the need of experts’ validation is frequently required, this because just the subjective selection by terminologists might not be exhaustive and fully consistent with the domain expert"
2020.computerm-1.9,dellorletta-etal-2014-t2k,0,0.0273813,"Missing"
2020.computerm-1.9,N19-1423,0,0.0126985,"Missing"
2020.computerm-1.9,J06-1005,0,0.0593317,"nguistic structures that are very frequent within a corpus of documents (Lefeuvre, 2017), patterns allow to discover among terms which are the conceptual relations (Bernier-Colborne and Barri`ere, 2018). The study of patterns dates way back, at the end of 90’ the works of Hearst (1992) were, for instance, firstly focused on the configuration of Noun Phrases followed by other morpho-syntactic structures to be found 63 in texts. Many authors in the literature studied the ways nominal and verbal phrases allow to identify semantic relations between terms through syntagmatic or phrasal structures (Girju et al., 2006). The typologies of lexico-syntactic markers help in retrieving the desired semantic information about the terminology proper to a specialized domain (Nguyen et al., 2017), that’s the case of the casual relationships between terms. This particular kind of connection is notably described in the works of Barri`ere (2002) in which the author gives a wide-ranging perspective for investigating the causal relationships in informative texts. As the author underlines, it is not an easy task to group the causative verbs that should isolate the representative terms of a domain to be linked through a cau"
2020.computerm-1.9,P08-3001,0,0.0494634,"is a context-dependent procedure: in considering the source area of study and having some technical knowledge about it, terminologists can much easily analyse in an autonomous and accurate way a combination of semantic relationships (Condamines, 2008). For what concerns semantic similarity methods in the literature, they have firstly been applied to single word terms (SWTs) using a variety of approaches such as: lexiconbased approaches (Blondel and Senellart, 2002), multilingual approaches (Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Andrade et al., 2013), distributional approaches (Hagiwara, 2008; Hazem and Daille, 2014) and distributed approaches such in (Mikolov et al., 2013; Bojanowski et al., 2016). This procedure helps in configuring the associations between terms with respect to synonyms connections retrieved from corpora. On this point, it is important to highlight the relevance of extracting reliable lists of candidate terms that could represent the starting point from which to set up a conceptual modeling of a thesaurus as well as a basis to analyse and define the internal domainspecific synonyms and hyperonyms (Meyer and Mackintosh, 1996). 4. The structure phase of the thesa"
2020.computerm-1.9,L18-1045,1,0.827773,"xtracted terms. This work represents the first attempt to use BERT model for terminology extraction. Overall, BERT obtained the best results with minimum Word Embedding-based Word embedding models have been showing to be very effective in word representation. They have been applied in several NLP tasks including word disambiguation, semantic similarity, bilingual lexicon induction (Mikolov et al., 2013; Arora et al., 2017; Bojanowski et al., 2016), etc. For semantic similarity, and more precisely synonym extraction of multi-word terms, two compositionality-based techniques have been proposed (Hazem and Daille, 2018). The first technique called Semi-compositional word embeddings is based on distributional analysis (Hazem and Daille, 2014) and assumes that the head or a tail is shared by two semantically related terms. The second technique called Full-compositional word embeddings is inspired by the idea that phrases can be represented by an element-wise sum of the word embeddings of semantically related words of its parts (Arora et al., 2017). In our experiments we follow the principle of the second technique and apply it to the automatic extraction of hyperonyms, synonyms, related and causative terms. Th"
2020.computerm-1.9,C92-2082,0,0.578753,"rsued, starting from lexico-syntactic patterns conformation (Condamines, 2007), and experimenting other solutions such as the ones proposed by (Grefenstette, 1994) with “Sextant”, or (Kageura et al., 2000) with their methodology in considering the common entries in two different thesauri and constructing pairs of codes. As linguistic structures that are very frequent within a corpus of documents (Lefeuvre, 2017), patterns allow to discover among terms which are the conceptual relations (Bernier-Colborne and Barri`ere, 2018). The study of patterns dates way back, at the end of 90’ the works of Hearst (1992) were, for instance, firstly focused on the configuration of Noun Phrases followed by other morpho-syntactic structures to be found 63 in texts. Many authors in the literature studied the ways nominal and verbal phrases allow to identify semantic relations between terms through syntagmatic or phrasal structures (Girju et al., 2006). The typologies of lexico-syntactic markers help in retrieving the desired semantic information about the terminology proper to a specialized domain (Nguyen et al., 2017), that’s the case of the casual relationships between terms. This particular kind of connection"
2020.computerm-1.9,I13-1150,0,0.0586893,"Missing"
2020.computerm-1.9,S18-1116,0,0.0525625,"Missing"
2020.computerm-1.9,C16-2015,0,0.02004,"through the denominative, conceptual and linguistic variants included in the terminological output it is possible to detect in which ways terms are expanded by other semantic elements, reduced, related to an opposite one, or appearing in several linguistic conformations, e.g., cyber security or cybersecurity. Below a list of few examples to show the variations given by the outputs in TermSuite terminological extraction for Cybersecurity domain in Italian language that can help in detecting semantic associations to be included in the thesaurus: 6.1.3. Pke - Keyphrases Identification Tool PKE (Boudin, 2016) is an open-source python keyphrase extraction toolkit that implements several keyphrase extraction approaches. From a linguistic point of view, PKE resulted to be very efficient in terms of providing a semiautomatic structuring of information since many candidate terms, which have been selected as being part of the Cybersecurity thesaurus, are grouped alongside with other ones that, in turn, could represent their associative semantic chains. For this section we provide as well related examples for the terms outputs precision: • denominative variants: NPN: hacker (21 matches) del telefono (mob"
2020.computerm-1.9,C00-1058,0,0.223007,"mantic similarity procedures and patterns configuration related to the causative connections. The automatized methodologies used for the configuration of thesauri’s structure (Yang and Powers, 2008b; Morin and Jacquemin, 1999), can quicken the process related to the arrangement of textual relations network to shape the informative tissue of a domain. To achieve this framework system different approaches can be pursued, starting from lexico-syntactic patterns conformation (Condamines, 2007), and experimenting other solutions such as the ones proposed by (Grefenstette, 1994) with “Sextant”, or (Kageura et al., 2000) with their methodology in considering the common entries in two different thesauri and constructing pairs of codes. As linguistic structures that are very frequent within a corpus of documents (Lefeuvre, 2017), patterns allow to discover among terms which are the conceptual relations (Bernier-Colborne and Barri`ere, 2018). The study of patterns dates way back, at the end of 90’ the works of Hearst (1992) were, for instance, firstly focused on the configuration of Noun Phrases followed by other morpho-syntactic structures to be found 63 in texts. Many authors in the literature studied the ways"
2020.computerm-1.9,2019.jeptalnrecital-tia.1,1,0.792748,"alignment. Its performance is quite immediate when it runs over big data sets. The term extraction provided by TermSuite is a list of representative terms that are presented together with different properties, e.g., their frequency, accuracy, specificity. Terms are therefore ordered according to their unithood and application to the domain. One of the main feature that broader term broader term (BT) that stands for hyperonyms; 65 shapes the quality of this software is its syntactic and morphological variants detection among terms, e.g, lexical reduction, composition, coordination, derivation (Lanza and Daille, 2019). Variants identification given by the output list in TermSuite represents one of the methods selected to retrieve hyperonyms as well as synonyms in the source corpus. In fact, through the denominative, conceptual and linguistic variants included in the terminological output it is possible to detect in which ways terms are expanded by other semantic elements, reduced, related to an opposite one, or appearing in several linguistic conformations, e.g., cyber security or cybersecurity. Below a list of few examples to show the variations given by the outputs in TermSuite terminological extraction"
2020.computerm-1.9,N13-1090,0,0.625383,"nd having some technical knowledge about it, terminologists can much easily analyse in an autonomous and accurate way a combination of semantic relationships (Condamines, 2008). For what concerns semantic similarity methods in the literature, they have firstly been applied to single word terms (SWTs) using a variety of approaches such as: lexiconbased approaches (Blondel and Senellart, 2002), multilingual approaches (Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Andrade et al., 2013), distributional approaches (Hagiwara, 2008; Hazem and Daille, 2014) and distributed approaches such in (Mikolov et al., 2013; Bojanowski et al., 2016). This procedure helps in configuring the associations between terms with respect to synonyms connections retrieved from corpora. On this point, it is important to highlight the relevance of extracting reliable lists of candidate terms that could represent the starting point from which to set up a conceptual modeling of a thesaurus as well as a basis to analyse and define the internal domainspecific synonyms and hyperonyms (Meyer and Mackintosh, 1996). 4. The structure phase of the thesaurus for Cybersecurity has started by evaluating the list of terms extracted by us"
2020.computerm-1.9,P99-1050,0,0.553306,"ge of a specific domain of study as a controlled vocabulary. This paper aims at presenting an analysis of the best performing NLP approaches, i.e., patterns configuration, semantic similarity, morphosyntactic variation given by term extractors, in enhancing a semantic structure of an existing Italian thesaurus about the technical domain of Cybersecurity. Constructing thesauri by carrying out minimum handcrafted activities is currently highly demanded (Azevedo et al., 2015). Hence, several methods to automatically build and maintain a thesaurus have been proposed so far (G¨untzer et al., 1989; Morin and Jacquemin, 1999; Yang and Powers, 2008a; Schandl and Blumauer, 2010). However, the quality of automatically generated thesauri tends to be rather weaker in their content and structure with respect to the conventional handcrafted ones (Ryan, 2014). To guarantee the currency of a thesaurus (Batini et al., 2009) it is crucial to whether improve existing methods or to develop new efficient techniques for discovering terms and their relations. On the perspective of using existing NLP tools for constructing a thesaurus, choosing the most appropriate ones is not an easy task since the performance varies depending o"
2020.computerm-1.9,D17-1022,0,0.0385766,"Missing"
2020.computerm-1.9,W16-4706,0,0.0899767,"Missing"
2020.computerm-1.9,L18-1284,0,0.368874,"demonstrate that the TKBs comply with the specialized corpus knowledge flow. Hence, together with certain groups of experts’ supervision, other tools support the accuracy validation, i.e., the gold standards (Barri`ere, 2006). This task is meant to give results on the way terms that have been selected to be part of a semantic resource – designed to represent a specialized language – can be aligned with others included in reference texts. These target texts can be in the same language as the one of the source corpus, and could present less difficulties in the matching system, or multilingual (Terryn et al., 2018), in these cases using translations from existing semantic resources could represent a solution. In this paper, the gold standards taken into account are in Italian language or have been translated in Italian – Nist and Iso – this reflects the native purpose of the project that was intended to provide a guidance for the understanding of the Cybersecurity domain in Italian language. 1. Pattern based system: the causative patterns aim at enhancing the associative relationship proper to thesauri configuration; 2. Variants recognition: semantic variation is useful to detect hierarchical and associ"
2020.computerm-1.9,P06-2111,0,0.163905,"Missing"
2020.computerm-1.9,W03-1610,0,0.163541,"ry given the domain-oriented nature of the casual connections. Indeed, retrieving this type of patterns is a context-dependent procedure: in considering the source area of study and having some technical knowledge about it, terminologists can much easily analyse in an autonomous and accurate way a combination of semantic relationships (Condamines, 2008). For what concerns semantic similarity methods in the literature, they have firstly been applied to single word terms (SWTs) using a variety of approaches such as: lexiconbased approaches (Blondel and Senellart, 2002), multilingual approaches (Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Andrade et al., 2013), distributional approaches (Hagiwara, 2008; Hazem and Daille, 2014) and distributed approaches such in (Mikolov et al., 2013; Bojanowski et al., 2016). This procedure helps in configuring the associations between terms with respect to synonyms connections retrieved from corpora. On this point, it is important to highlight the relevance of extracting reliable lists of candidate terms that could represent the starting point from which to set up a conceptual modeling of a thesaurus as well as a basis to analyse and define the internal doma"
2020.lrec-1.97,A00-2004,0,0.735819,"Level1 Virgin Virgin Virgin Virgin Virgin Virgin Level2 Level3 Matins Nocturn Matins Nocturn Matins Nocturn Matins Nocturn Matins Nocturn Matins Nocturn Figure 5: CSV representation of annotations of a book of hours (Extracts from the Arsenal 1194). Figure 5 illustrates an example of the provided annotations extracted from Arsenal 1194. The ”Virgin” label refers to the Hours of the Virgin (level 1), the ”Matins” label refers to the Matins of level 2 and ”Nocturn” label, refers to Nocturn of level 3. For text segmentation, we provide the gold standard files with section delimitation following Choi (2000) format. Hence, each of the annotated books has one Choi format per level. This allows to evaluate the segmentation of each level separately or all together. 5. Text Segmentation Text segmentation is closely related to topic analysis and can be addressed following three axis: (i) syntagmatic axis where a text is delimited into homogeneous topics (ex: audio transcriptions); (ii) paradigmatic axis in which topics are identified and (iii) functional axis in which segment topics relations are linked for text structuring (ex: summarization) (Ferret et al., 1998). Segmentation can be content-based w"
2020.lrec-1.97,N09-1040,0,0.929946,"is characterized by a specific vocabulary and each vocabulary change implies a topic change (Hearst, 1994). It can also use topic markers whether (i) oral: such as prosody, silence; (ii) written: using connectors, introductory expressions or (iii) visual: using line breaks, bullets, numbering, bold, etc. The addressed texts were mainly linear, in which case a sequential analysis of topical changes was applied (Hearst, 1994; Choi, 2000) (expository texts were usually used). Later on, hierarchical texts were addressed which required a more fine-grained subtopic structure analysis (Yaari, 1997; Eisenstein, 2009). Most of the approaches dedicated to text segmentation perform a lexical level analysis to detect segments coherence, and use (i) patterns of lexical co-occurrence (Hearst, 1997) such as discourse structure (Nomoto and Nitta, 1994) or (ii) lexical cohesion (Morris and Hirst, 1991) based on term repetition (Hearst, 1994) and semantic relations extracted via a thesaurus (Morris and Hirst, 1991), a dictionary (Kozima, 1993) or automatically using a collocation network (Ferret et al., 1998). Lexical cohesion has inspired a broad range of unsupervised approaches including TextTiling (Hearst, 1994)"
2020.lrec-1.97,C98-1062,0,0.817574,"ard files with section delimitation following Choi (2000) format. Hence, each of the annotated books has one Choi format per level. This allows to evaluate the segmentation of each level separately or all together. 5. Text Segmentation Text segmentation is closely related to topic analysis and can be addressed following three axis: (i) syntagmatic axis where a text is delimited into homogeneous topics (ex: audio transcriptions); (ii) paradigmatic axis in which topics are identified and (iii) functional axis in which segment topics relations are linked for text structuring (ex: summarization) (Ferret et al., 1998). Segmentation can be content-based where each topic is characterized by a specific vocabulary and each vocabulary change implies a topic change (Hearst, 1994). It can also use topic markers whether (i) oral: such as prosody, silence; (ii) written: using connectors, introductory expressions or (iii) visual: using line breaks, bullets, numbering, bold, etc. The addressed texts were mainly linear, in which case a sequential analysis of topical changes was applied (Hearst, 1994; Choi, 2000) (expository texts were usually used). Later on, hierarchical texts were addressed which required a more fin"
2020.lrec-1.97,P03-1071,0,0.534219,"lexical level analysis to detect segments coherence, and use (i) patterns of lexical co-occurrence (Hearst, 1997) such as discourse structure (Nomoto and Nitta, 1994) or (ii) lexical cohesion (Morris and Hirst, 1991) based on term repetition (Hearst, 1994) and semantic relations extracted via a thesaurus (Morris and Hirst, 1991), a dictionary (Kozima, 1993) or automatically using a collocation network (Ferret et al., 1998). Lexical cohesion has inspired a broad range of unsupervised approaches including TextTiling (Hearst, 1994) (a tfxIdf Cosine-based approach), LSeg based on lexical chains (Galley et al., 2003), U00 (Utiyama and Isahara, 2001) a probabilistic dynamic programming approach, TopicTiling (Riedl and Biemann, 2012) a topic modeling approach based on Latent Dirichlet Analysis (LDA), etc. Two main types of texts were addressed by lexical cohesion based approaches, that is: technical and scientific documents (Hearst, 1997) in which term repetition is a strong indicator when a specific vocabulary is used; and narrative texts (Morris and Hirst, 1991; Kozima, 1993) where term repetition is not sufficient as concepts may be expressed in different ways and so, thesaurus and dictionaries may be re"
2020.lrec-1.97,P94-1002,0,0.926069,"content of books of hours. This is a great challenge in digital humanities that brings together researchers from the fields of document recognition, NLP and history. The study of the content of books of hours aims at providing opportunities for historical analysis to better understand the cultures and faiths from the 13th c. to the 16th c. In addition, its complex logical entangled structure offers a new type of resource for text segmentation. Since traditional segmentation data sets lie within the scope of expository texts, narrative or issued from spoken an written dialogues (Kozima, 1993; Hearst, 1994; Nomoto and Nitta, 1994; Utiyama and Isahara, 2001) or more recently from Wikipedia (Arnold et 1 https://library.harvard.edu/collections/ picturingprayer/exhibition.html 776 Figure 1: Examples of text recognition results (hyp) and their manual annotations (ref) on three books of hours manuscripts: Harvard Typ 32, Metz 1581 and Harvard Rich 9. The writings are of type ”Textualis” on the left and center sides, and of type ”Cursiva” on the right side. al., 2019), books of hours arguably constitute a new genre of segmentation texts that exhibits difficulties with regard to length, structure and m"
2020.lrec-1.97,J97-1003,0,0.924055,"(ii) written: using connectors, introductory expressions or (iii) visual: using line breaks, bullets, numbering, bold, etc. The addressed texts were mainly linear, in which case a sequential analysis of topical changes was applied (Hearst, 1994; Choi, 2000) (expository texts were usually used). Later on, hierarchical texts were addressed which required a more fine-grained subtopic structure analysis (Yaari, 1997; Eisenstein, 2009). Most of the approaches dedicated to text segmentation perform a lexical level analysis to detect segments coherence, and use (i) patterns of lexical co-occurrence (Hearst, 1997) such as discourse structure (Nomoto and Nitta, 1994) or (ii) lexical cohesion (Morris and Hirst, 1991) based on term repetition (Hearst, 1994) and semantic relations extracted via a thesaurus (Morris and Hirst, 1991), a dictionary (Kozima, 1993) or automatically using a collocation network (Ferret et al., 1998). Lexical cohesion has inspired a broad range of unsupervised approaches including TextTiling (Hearst, 1994) (a tfxIdf Cosine-based approach), LSeg based on lexical chains (Galley et al., 2003), U00 (Utiyama and Isahara, 2001) a probabilistic dynamic programming approach, TopicTiling (R"
2020.lrec-1.97,E06-1035,0,0.0881898,"scientific documents (Hearst, 1997) in which term repetition is a strong indicator when a specific vocabulary is used; and narrative texts (Morris and Hirst, 1991; Kozima, 1993) where term repetition is not sufficient as concepts may be expressed in different ways and so, thesaurus and dictionaries may be required to extract semantic relations between terms. Ferret et al. (1998), introduced a mixed approach to deal with both types of texts. Also, a bunch of supervised approaches was introduced mainly to deal with discourse (Joty et al., 2015), multiparty dialogue and chat forums segmentation (Hsueh et al., 2006; Hernault et al., 2010) and to perform segmentation at the sentence level to discover Elementary Discourse Units 781 (EDU) (Hernault et al., 2010; Joty et al., 2015). These approaches often combine lexical coherence information with dialogue features using for instance a decision tree classifier (Hsueh et al., 2006), Conditional Random Fields (CRF) (Hernault et al., 2010; Joty et al., 2015) or Neural network approaches such as TextTiling-like embedding approach for query-reply dialog segmentation (Song et al., 2016), multi-party dialog for EDU using sequential model (Shi and Huang, 2019), rei"
2020.lrec-1.97,J15-3002,0,0.355241,"ddressed by lexical cohesion based approaches, that is: technical and scientific documents (Hearst, 1997) in which term repetition is a strong indicator when a specific vocabulary is used; and narrative texts (Morris and Hirst, 1991; Kozima, 1993) where term repetition is not sufficient as concepts may be expressed in different ways and so, thesaurus and dictionaries may be required to extract semantic relations between terms. Ferret et al. (1998), introduced a mixed approach to deal with both types of texts. Also, a bunch of supervised approaches was introduced mainly to deal with discourse (Joty et al., 2015), multiparty dialogue and chat forums segmentation (Hsueh et al., 2006; Hernault et al., 2010) and to perform segmentation at the sentence level to discover Elementary Discourse Units 781 (EDU) (Hernault et al., 2010; Joty et al., 2015). These approaches often combine lexical coherence information with dialogue features using for instance a decision tree classifier (Hsueh et al., 2006), Conditional Random Fields (CRF) (Hernault et al., 2010; Joty et al., 2015) or Neural network approaches such as TextTiling-like embedding approach for query-reply dialog segmentation (Song et al., 2016), multi-"
2020.lrec-1.97,N18-2075,0,0.216686,"veral state of the art approaches: (i) five unsupervised approaches: TextTiling (Hearst, 1994), clustering model (C99) (Choi, 2000), probabilistic dynamic programming model (U00) (Utiyama and Isahara, 2001), minimum cut model (MinCut) (Malioutov and Barzilay, 2006), the hierarchical bayesian model (HierBays) (Eisenstein, 2009); and (ii) one supervised approach: TopicTiling (Riedl and Biemann, 2012), a topic modeling-based approach. Due to the lack of large annotated training data, we do not evaluate other supervised (Hsueh et al., 2006; Joty et al., 2015) and deep learning (Song et al., 2016; Koshorek et al., 2018; Shi and Huang, 2019) approaches. The experiments were conducted on the manual transcribed book Medievalist and on the automatically transcribed book Harvard 253. The Arsenal 1194 book of hours was used as training data for the TopicTiling approach. 6.1. If a change in shifts corresponds to a topical change in traditional segmentation data sets, this is not the case for books of hours where some segments can be highly correlated. A shift corresponds to a frontier between two segments or categories Medievalist Level1 Level2 Harvard 253 Level1 Level2 Pk WD Pk WD Pk WD Pk WD TextTiling 64.4 94.2"
2020.lrec-1.97,P93-1041,0,0.876088,"cess the whole content of books of hours. This is a great challenge in digital humanities that brings together researchers from the fields of document recognition, NLP and history. The study of the content of books of hours aims at providing opportunities for historical analysis to better understand the cultures and faiths from the 13th c. to the 16th c. In addition, its complex logical entangled structure offers a new type of resource for text segmentation. Since traditional segmentation data sets lie within the scope of expository texts, narrative or issued from spoken an written dialogues (Kozima, 1993; Hearst, 1994; Nomoto and Nitta, 1994; Utiyama and Isahara, 2001) or more recently from Wikipedia (Arnold et 1 https://library.harvard.edu/collections/ picturingprayer/exhibition.html 776 Figure 1: Examples of text recognition results (hyp) and their manual annotations (ref) on three books of hours manuscripts: Harvard Typ 32, Metz 1581 and Harvard Rich 9. The writings are of type ”Textualis” on the left and center sides, and of type ”Cursiva” on the right side. al., 2019), books of hours arguably constitute a new genre of segmentation texts that exhibits difficulties with regard to length, s"
2020.lrec-1.97,P06-1004,0,0.541533,"than false positives and overpenalizes near misses. Hence, a second measure, WindowDiff, a variant of Pk , has been also used as it equally penalizes false positives and near misses. 6.3. Results Table 4 reports the segmentation results of the Medievalist and the Harvard 253 books of hours on the first and second levels24 . Experiments and Results We evaluate several state of the art approaches: (i) five unsupervised approaches: TextTiling (Hearst, 1994), clustering model (C99) (Choi, 2000), probabilistic dynamic programming model (U00) (Utiyama and Isahara, 2001), minimum cut model (MinCut) (Malioutov and Barzilay, 2006), the hierarchical bayesian model (HierBays) (Eisenstein, 2009); and (ii) one supervised approach: TopicTiling (Riedl and Biemann, 2012), a topic modeling-based approach. Due to the lack of large annotated training data, we do not evaluate other supervised (Hsueh et al., 2006; Joty et al., 2015) and deep learning (Song et al., 2016; Koshorek et al., 2018; Shi and Huang, 2019) approaches. The experiments were conducted on the manual transcribed book Medievalist and on the automatically transcribed book Harvard 253. The Arsenal 1194 book of hours was used as training data for the TopicTiling app"
2020.lrec-1.97,J91-1002,0,0.81773,"bullets, numbering, bold, etc. The addressed texts were mainly linear, in which case a sequential analysis of topical changes was applied (Hearst, 1994; Choi, 2000) (expository texts were usually used). Later on, hierarchical texts were addressed which required a more fine-grained subtopic structure analysis (Yaari, 1997; Eisenstein, 2009). Most of the approaches dedicated to text segmentation perform a lexical level analysis to detect segments coherence, and use (i) patterns of lexical co-occurrence (Hearst, 1997) such as discourse structure (Nomoto and Nitta, 1994) or (ii) lexical cohesion (Morris and Hirst, 1991) based on term repetition (Hearst, 1994) and semantic relations extracted via a thesaurus (Morris and Hirst, 1991), a dictionary (Kozima, 1993) or automatically using a collocation network (Ferret et al., 1998). Lexical cohesion has inspired a broad range of unsupervised approaches including TextTiling (Hearst, 1994) (a tfxIdf Cosine-based approach), LSeg based on lexical chains (Galley et al., 2003), U00 (Utiyama and Isahara, 2001) a probabilistic dynamic programming approach, TopicTiling (Riedl and Biemann, 2012) a topic modeling approach based on Latent Dirichlet Analysis (LDA), etc. Two ma"
2020.lrec-1.97,C94-2187,0,0.516968,"oks of hours. This is a great challenge in digital humanities that brings together researchers from the fields of document recognition, NLP and history. The study of the content of books of hours aims at providing opportunities for historical analysis to better understand the cultures and faiths from the 13th c. to the 16th c. In addition, its complex logical entangled structure offers a new type of resource for text segmentation. Since traditional segmentation data sets lie within the scope of expository texts, narrative or issued from spoken an written dialogues (Kozima, 1993; Hearst, 1994; Nomoto and Nitta, 1994; Utiyama and Isahara, 2001) or more recently from Wikipedia (Arnold et 1 https://library.harvard.edu/collections/ picturingprayer/exhibition.html 776 Figure 1: Examples of text recognition results (hyp) and their manual annotations (ref) on three books of hours manuscripts: Harvard Typ 32, Metz 1581 and Harvard Rich 9. The writings are of type ”Textualis” on the left and center sides, and of type ”Cursiva” on the right side. al., 2019), books of hours arguably constitute a new genre of segmentation texts that exhibits difficulties with regard to length, structure and many ambiguities. Also, t"
2020.lrec-1.97,J02-1002,0,0.499244,"Table 3 resumes the number of segments according to the first and second levels of segmentation. At the first level, we note that the number of shifts23 is equal except (Hardvard 253 that does not contain the Suffrages section). However, for the second level, we see different number of shifts. 23 Number of categories BoH #Level1 #Level2 Arsenal 1194 8 34 Medievalist 8 55 Harvard 253 7 38 Table 3: Illustration of the number of boundaries or shifts per level for each book of hours. 6.2. Evaluation Metrics The approaches are evaluated in terms of Pk (Beeferman et al., 1999) and Windowdiff (W D) (Pevzner and Hearst, 2002) metrics. Pk is an error metric which combines precision and recall to estimates the relative contributions of the different feature types. Nonetheless, it exhibits several drawbacks as mentioned in (Pevzner and Hearst, 2002). Pk is affected by segment size variation. It also penalizes more heavily false negatives than false positives and overpenalizes near misses. Hence, a second measure, WindowDiff, a variant of Pk , has been also used as it equally penalizes false positives and near misses. 6.3. Results Table 4 reports the segmentation results of the Medievalist and the Harvard 253 books of"
2020.lrec-1.97,W12-3307,0,0.902157,") such as discourse structure (Nomoto and Nitta, 1994) or (ii) lexical cohesion (Morris and Hirst, 1991) based on term repetition (Hearst, 1994) and semantic relations extracted via a thesaurus (Morris and Hirst, 1991), a dictionary (Kozima, 1993) or automatically using a collocation network (Ferret et al., 1998). Lexical cohesion has inspired a broad range of unsupervised approaches including TextTiling (Hearst, 1994) (a tfxIdf Cosine-based approach), LSeg based on lexical chains (Galley et al., 2003), U00 (Utiyama and Isahara, 2001) a probabilistic dynamic programming approach, TopicTiling (Riedl and Biemann, 2012) a topic modeling approach based on Latent Dirichlet Analysis (LDA), etc. Two main types of texts were addressed by lexical cohesion based approaches, that is: technical and scientific documents (Hearst, 1997) in which term repetition is a strong indicator when a specific vocabulary is used; and narrative texts (Morris and Hirst, 1991; Kozima, 1993) where term repetition is not sufficient as concepts may be expressed in different ways and so, thesaurus and dictionaries may be required to extract semantic relations between terms. Ferret et al. (1998), introduced a mixed approach to deal with bo"
2020.lrec-1.97,P01-1064,0,0.901129,"great challenge in digital humanities that brings together researchers from the fields of document recognition, NLP and history. The study of the content of books of hours aims at providing opportunities for historical analysis to better understand the cultures and faiths from the 13th c. to the 16th c. In addition, its complex logical entangled structure offers a new type of resource for text segmentation. Since traditional segmentation data sets lie within the scope of expository texts, narrative or issued from spoken an written dialogues (Kozima, 1993; Hearst, 1994; Nomoto and Nitta, 1994; Utiyama and Isahara, 2001) or more recently from Wikipedia (Arnold et 1 https://library.harvard.edu/collections/ picturingprayer/exhibition.html 776 Figure 1: Examples of text recognition results (hyp) and their manual annotations (ref) on three books of hours manuscripts: Harvard Typ 32, Metz 1581 and Harvard Rich 9. The writings are of type ”Textualis” on the left and center sides, and of type ”Cursiva” on the right side. al., 2019), books of hours arguably constitute a new genre of segmentation texts that exhibits difficulties with regard to length, structure and many ambiguities. Also, the vast majority of state of"
C16-1321,D11-1033,0,0.0245098,"11-17 2016. document aligned data at the topic level, we only deal with the standard approach and show at least that our approach improve drastically bilingual terminology extraction while adding well selected external data. The small size of specialized comparable corpora renders unreliable word co-occurrences which are the basis of the standard approach. In this paper, we propose to improve the reliability of word cooccurrences in specialized comparable corpora by adding general-domain data. This idea has already been successfully employed in machine translation task (Moore and Lewis, 2010; Axelrod et al., 2011; Wang et al., 2014, among others). The approach of using adapted external data, also known as data selection is often applied in Statistical Machine Translation (SMT) to improve the quality of the language and translation models, and hence, to increase the performance of SMT systems. If data selection has become a mainstream in SMT, it is still not the case in the task of bilingual lexicon extraction from specialized comparable corpora. The majority of the studies in this area support the principle that the quality of the comparable corpus is more important than its size and consequently, inc"
C16-1321,D13-1046,0,0.0678367,"968) and weighted Jaccard (Grefenstette, 1994) 3 Related Work In the past few years, several contributions have been proposed to improve each step of the standard approach. Prochasson et al. (2009) enhance the representativeness of the context vectors by strengthening the context words that happen to be transliterated and scientific compound words in the target 3402 language. Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (in-domain terms), and thus propose a method for filtering the noise of the context vectors. Bouamor et al. (2013) propose an adaption of the standard approach that exploits Wikipedia to improve the context vector representation. From the context vector of a word to be translated, they build a vector of Wikipedia concepts using the ESA inverted index (Explicit Semantic Analysis). This vector of concepts is then translated into the target language. The candidate translations are found by projecting the translated vector of concepts using the ESA direct index onto the context vector of the target language. Prochasson and Fung (2011) propose to use a machine learning approach based both on the context-vector"
C16-1321,P13-2133,0,0.456183,"968) and weighted Jaccard (Grefenstette, 1994) 3 Related Work In the past few years, several contributions have been proposed to improve each step of the standard approach. Prochasson et al. (2009) enhance the representativeness of the context vectors by strengthening the context words that happen to be transliterated and scientific compound words in the target 3402 language. Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (in-domain terms), and thus propose a method for filtering the noise of the context vectors. Bouamor et al. (2013) propose an adaption of the standard approach that exploits Wikipedia to improve the context vector representation. From the context vector of a word to be translated, they build a vector of Wikipedia concepts using the ESA inverted index (Explicit Semantic Analysis). This vector of concepts is then translated into the target language. The candidate translations are found by projecting the translated vector of concepts using the ESA direct index onto the context vector of the target language. Prochasson and Fung (2011) propose to use a machine learning approach based both on the context-vector"
C16-1321,C02-2020,0,0.774202,"parability. 5.2 Bilingual Dictionary The bilingual dictionary used in our experiments is the French/English dictionary ELRA-M00337 . This resource is a general language dictionary which contains around 244,000 entries. 5.3 Gold Standard To evaluate the quality of bilingual terminology extraction from comparable corpora, a bilingual terminology reference list that reflects the technical vocabulary of the comparable corpus is required. The 4 opus.lingfil.uu.se commoncrawl.org 6 www.ldc.upenn.edu 7 www.elra.info 5 3405 list is usually composed of more or less 100 single words: 95 single words in Chiao and Zweigenbaum (2002), 100 in Morin et al. (2010), 125 and 79 in Bouamor et al. (2013a). We build a reference list for each of the three comparable corpora using specialized glossaries available on the Web. For instance, the list is derived from the UMLS8 for the breast cancer corpus. Concerning wind energy, the list is provided with the corpus1 . In order to focus only on the vocabulary characteristic of the specialized corpus we remove technical terms that have a common meaning in the general domain such as analysis, factor, method, result, study, etc. Without this precaution, these terms would be mechanically b"
C16-1321,C12-1046,1,0.816226,"nslation (SMT) to improve the quality of the language and translation models, and hence, to increase the performance of SMT systems. If data selection has become a mainstream in SMT, it is still not the case in the task of bilingual lexicon extraction from specialized comparable corpora. The majority of the studies in this area support the principle that the quality of the comparable corpus is more important than its size and consequently, increasing the size of specialized comparable corpora by adding out-of-domain documents decreases the quality of bilingual lexicons (Li and Gaussier, 2010; Delpech et al., 2012). This statement remains true as long as the used data is not adapted to the domain. We propose two data selection techniques based on the combination of a specialized comparable corpus with external resources. Our hypothesis is that word co-occurrences learned from a general-domain corpus for general words (as opposed to the terms of the domain) improve the characterization of the specific vocabulary of the corpus (the terms of the domain). By enriching the general words representation in specialized comparable corpora, we improve their characterization and therefore improve the characterizat"
C16-1321,J93-1003,0,0.593415,"Missing"
C16-1321,W95-0114,0,0.831326,"the difficulty to obtain many specialized documents in a language other than English. For example, a single query on the Elsevier portal1 of documents containing in their title the term “breast cancer” returns 40,000 documents in English, where the same query returns 1,500 documents in French, 693 in Spanish and only 7 in German. The historical context-based approach dedicated to the task of bilingual lexicon extraction from comparable corpora, and also known as the standard approach, relies on the simple observation that a word and its translation tend to appear in the same lexical contexts (Fung, 1995; Rapp, 1999). In this approach, each word is described by its lexical contexts in both source and target languages, and words in translation relationship should have similar lexical contexts in both languages. To enhance bilingual lexicon induction, recent approaches use more sophisticated techniques such as topic models based on bilingual latent dirichlet allocation (BiLDA) (Vulic and Moens, 2013b; Vulic and Moens, 2013a) or bilingual word embeddings based on neural networks (Gouws et al., 2014; Chandar et al., 2014; Vulic and Moens, 2015; Vulic and Moens, 2016) (approaches respectively note"
C16-1321,P04-1067,0,0.690562,"Missing"
C16-1321,fiser-etal-2012-addressing,0,0.0208808,"co-occurrence counts in specialized comparable corpus more reliable by reestimating their probabilities. Morin and Hazem (2016) show the unfounded assumption of the balance in terms of quantity of data of the specialized comparable corpora and that the use of unbalanced corpora significantly improves the results of the standard approach. Other improvements to the standard approach have been proposed by introducing other paradigms. For instance, Gaussier et al. (2004) propose to apply Canonical Correlation Analysis (CCA) which is a bilingual extension of Latent Semantic Analysis (LSA) whereas Hazem and Morin (2012) propose to use Independent Component Analysis (ICA) which is basically an extension of the Principal Component Analysis (PCA). Vuli´c et al. (2011) also propose an extension of the Latent Dirichlet Allocation (LDA) taking into account bilinguality and called bilingual LDA (BiLDA), improvements of this latter can be found in (Vulic and Moens, 2013b; Vulic and Moens, 2013a). Gouws et al. (2014) and Chandar et al. (2014) use multilingual word embeddings based on sentence-aligned parallel data and/or translation dictionaries whereas Vuli´c and Moens. (2015; 2016) learn bilingual word embeddings f"
C16-1321,I13-1196,1,0.917301,"ector of Wikipedia concepts using the ESA inverted index (Explicit Semantic Analysis). This vector of concepts is then translated into the target language. The candidate translations are found by projecting the translated vector of concepts using the ESA direct index onto the context vector of the target language. Prochasson and Fung (2011) propose to use a machine learning approach based both on the context-vector similarity and the co-occurrence features to learn a model for rare words from one pair of languages and this model can be used to find translations from another pair of languages. Hazem and Morin (2013) study different word co-occurrence prediction models in order to make the observed co-occurrence counts in specialized comparable corpus more reliable by reestimating their probabilities. Morin and Hazem (2016) show the unfounded assumption of the balance in terms of quantity of data of the specialized comparable corpora and that the use of unbalanced corpora significantly improves the results of the standard approach. Other improvements to the standard approach have been proposed by introducing other paradigms. For instance, Gaussier et al. (2004) propose to apply Canonical Correlation Analy"
C16-1321,C10-2055,0,0.0205901,"context vector i to each context vector of the target language t through a similarity measure and rank the candidate translations according to this measure. The similarity measures employed are Cosine (Salton and Lesk, 1968) and weighted Jaccard (Grefenstette, 1994) 3 Related Work In the past few years, several contributions have been proposed to improve each step of the standard approach. Prochasson et al. (2009) enhance the representativeness of the context vectors by strengthening the context words that happen to be transliterated and scientific compound words in the target 3402 language. Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (in-domain terms), and thus propose a method for filtering the noise of the context vectors. Bouamor et al. (2013) propose an adaption of the standard approach that exploits Wikipedia to improve the context vector representation. From the context vector of a word to be translated, they build a vector of Wikipedia concepts using the ESA inverted index (Explicit Semantic Analysis). This vector of concepts is then translated into the target language. The candidate translations are found by project"
C16-1321,C10-1073,0,0.378307,"ical Machine Translation (SMT) to improve the quality of the language and translation models, and hence, to increase the performance of SMT systems. If data selection has become a mainstream in SMT, it is still not the case in the task of bilingual lexicon extraction from specialized comparable corpora. The majority of the studies in this area support the principle that the quality of the comparable corpus is more important than its size and consequently, increasing the size of specialized comparable corpora by adding out-of-domain documents decreases the quality of bilingual lexicons (Li and Gaussier, 2010; Delpech et al., 2012). This statement remains true as long as the used data is not adapted to the domain. We propose two data selection techniques based on the combination of a specialized comparable corpus with external resources. Our hypothesis is that word co-occurrences learned from a general-domain corpus for general words (as opposed to the terms of the domain) improve the characterization of the specific vocabulary of the corpus (the terms of the domain). By enriching the general words representation in specialized comparable corpora, we improve their characterization and therefore im"
C16-1321,P10-2041,0,0.0673422,"Osaka, Japan, December 11-17 2016. document aligned data at the topic level, we only deal with the standard approach and show at least that our approach improve drastically bilingual terminology extraction while adding well selected external data. The small size of specialized comparable corpora renders unreliable word co-occurrences which are the basis of the standard approach. In this paper, we propose to improve the reliability of word cooccurrences in specialized comparable corpora by adding general-domain data. This idea has already been successfully employed in machine translation task (Moore and Lewis, 2010; Axelrod et al., 2011; Wang et al., 2014, among others). The approach of using adapted external data, also known as data selection is often applied in Statistical Machine Translation (SMT) to improve the quality of the language and translation models, and hence, to increase the performance of SMT systems. If data selection has become a mainstream in SMT, it is still not the case in the task of bilingual lexicon extraction from specialized comparable corpora. The majority of the studies in this area support the principle that the quality of the comparable corpus is more important than its size"
C16-1321,P11-1133,0,0.0935735,"erms), and thus propose a method for filtering the noise of the context vectors. Bouamor et al. (2013) propose an adaption of the standard approach that exploits Wikipedia to improve the context vector representation. From the context vector of a word to be translated, they build a vector of Wikipedia concepts using the ESA inverted index (Explicit Semantic Analysis). This vector of concepts is then translated into the target language. The candidate translations are found by projecting the translated vector of concepts using the ESA direct index onto the context vector of the target language. Prochasson and Fung (2011) propose to use a machine learning approach based both on the context-vector similarity and the co-occurrence features to learn a model for rare words from one pair of languages and this model can be used to find translations from another pair of languages. Hazem and Morin (2013) study different word co-occurrence prediction models in order to make the observed co-occurrence counts in specialized comparable corpus more reliable by reestimating their probabilities. Morin and Hazem (2016) show the unfounded assumption of the balance in terms of quantity of data of the specialized comparable corp"
C16-1321,2009.mtsummit-posters.14,1,0.909256,"atio (Evert, 2005). 2. Translate with a bilingual dictionary the context vector of a word to be translated from the source to the target language (i the translated context vector). 3. Compare the translated context vector i to each context vector of the target language t through a similarity measure and rank the candidate translations according to this measure. The similarity measures employed are Cosine (Salton and Lesk, 1968) and weighted Jaccard (Grefenstette, 1994) 3 Related Work In the past few years, several contributions have been proposed to improve each step of the standard approach. Prochasson et al. (2009) enhance the representativeness of the context vectors by strengthening the context words that happen to be transliterated and scientific compound words in the target 3402 language. Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (in-domain terms), and thus propose a method for filtering the noise of the context vectors. Bouamor et al. (2013) propose an adaption of the standard approach that exploits Wikipedia to improve the context vector representation. From the context vector of a word to be translated, they bui"
C16-1321,2009.mtsummit-posters.15,0,0.039283,"tains about 21 languages. We used the French-English version 7 used for the WMT translation task3 JRC acquis corpus (JRC) is a collection of legislative European union texts4 . We used the FrenchEnglish aligned version at OPUS provided by JRC (Tiedemann, 2012). Common crawl corpus (CC) is a petabytes of data collected over 7 years of web crawling set of raw web page data and text extracts5 . Gigaword corpus (GW) is a set of monolingual newswire corpora provided by LDC6 . United nations corpus (UN) is a six language parallel text of the United Nations originally provided as translation memory (Rafalovitch and Dale, 2009). The French/English corpora were then normalized through the following linguistic pre-processing steps: tokenization, part-of-speech tagging, and lemmatization. Finally, the function words were removed and the words occurring less than twice in the French and in the English parts were discarded. Table 1 shows the size of the comparable corpora and also indicates the comparability degree in percentages (Comp.) between the French and the English parts of each comparable corpus. The comparability measure (Li and Gaussier, 2010) is based on the expectation of finding the translation for each word"
C16-1321,P99-1067,0,0.854117,"ty to obtain many specialized documents in a language other than English. For example, a single query on the Elsevier portal1 of documents containing in their title the term “breast cancer” returns 40,000 documents in English, where the same query returns 1,500 documents in French, 693 in Spanish and only 7 in German. The historical context-based approach dedicated to the task of bilingual lexicon extraction from comparable corpora, and also known as the standard approach, relies on the simple observation that a word and its translation tend to appear in the same lexical contexts (Fung, 1995; Rapp, 1999). In this approach, each word is described by its lexical contexts in both source and target languages, and words in translation relationship should have similar lexical contexts in both languages. To enhance bilingual lexicon induction, recent approaches use more sophisticated techniques such as topic models based on bilingual latent dirichlet allocation (BiLDA) (Vulic and Moens, 2013b; Vulic and Moens, 2013a) or bilingual word embeddings based on neural networks (Gouws et al., 2014; Chandar et al., 2014; Vulic and Moens, 2015; Vulic and Moens, 2016) (approaches respectively noted: Gouws, Cha"
C16-1321,tiedemann-2012-parallel,0,0.0532329,"11 88.52 87.90 85.30 86.13 85.56 84.73 Table 1: Characteristics of the specialized corpora and the external data. News commentary corpus (NC) is a twelve language parallel corpus of news commentaries provided by the WMT workshop for SMT4 . Europarl corpus (EP7) is a parallel corpus for SMT extracted from the proceedings of the European Parliament. It contains about 21 languages. We used the French-English version 7 used for the WMT translation task3 JRC acquis corpus (JRC) is a collection of legislative European union texts4 . We used the FrenchEnglish aligned version at OPUS provided by JRC (Tiedemann, 2012). Common crawl corpus (CC) is a petabytes of data collected over 7 years of web crawling set of raw web page data and text extracts5 . Gigaword corpus (GW) is a set of monolingual newswire corpora provided by LDC6 . United nations corpus (UN) is a six language parallel text of the United Nations originally provided as translation memory (Rafalovitch and Dale, 2009). The French/English corpora were then normalized through the following linguistic pre-processing steps: tokenization, part-of-speech tagging, and lemmatization. Finally, the function words were removed and the words occurring less t"
C16-1321,N13-1011,0,0.0572174,"k of bilingual lexicon extraction from comparable corpora, and also known as the standard approach, relies on the simple observation that a word and its translation tend to appear in the same lexical contexts (Fung, 1995; Rapp, 1999). In this approach, each word is described by its lexical contexts in both source and target languages, and words in translation relationship should have similar lexical contexts in both languages. To enhance bilingual lexicon induction, recent approaches use more sophisticated techniques such as topic models based on bilingual latent dirichlet allocation (BiLDA) (Vulic and Moens, 2013b; Vulic and Moens, 2013a) or bilingual word embeddings based on neural networks (Gouws et al., 2014; Chandar et al., 2014; Vulic and Moens, 2015; Vulic and Moens, 2016) (approaches respectively noted: Gouws, Chandar and BWESG+cos). All these approaches require at least sentence-aligned/document aligned parallel data (BiLDA, Gouws, Chandar) or non-parallel document-aligned data at the topic level (BWESG+cos). Since specialized comparable corpora are of small size, sentence-aligned (document aligned) parallel data are unavailable and nonparallel document-aligned data at the topic level can’t be"
C16-1321,D13-1168,0,0.25195,"k of bilingual lexicon extraction from comparable corpora, and also known as the standard approach, relies on the simple observation that a word and its translation tend to appear in the same lexical contexts (Fung, 1995; Rapp, 1999). In this approach, each word is described by its lexical contexts in both source and target languages, and words in translation relationship should have similar lexical contexts in both languages. To enhance bilingual lexicon induction, recent approaches use more sophisticated techniques such as topic models based on bilingual latent dirichlet allocation (BiLDA) (Vulic and Moens, 2013b; Vulic and Moens, 2013a) or bilingual word embeddings based on neural networks (Gouws et al., 2014; Chandar et al., 2014; Vulic and Moens, 2015; Vulic and Moens, 2016) (approaches respectively noted: Gouws, Chandar and BWESG+cos). All these approaches require at least sentence-aligned/document aligned parallel data (BiLDA, Gouws, Chandar) or non-parallel document-aligned data at the topic level (BWESG+cos). Since specialized comparable corpora are of small size, sentence-aligned (document aligned) parallel data are unavailable and nonparallel document-aligned data at the topic level can’t be"
C16-1321,P15-2118,0,0.0312426,"and its translation tend to appear in the same lexical contexts (Fung, 1995; Rapp, 1999). In this approach, each word is described by its lexical contexts in both source and target languages, and words in translation relationship should have similar lexical contexts in both languages. To enhance bilingual lexicon induction, recent approaches use more sophisticated techniques such as topic models based on bilingual latent dirichlet allocation (BiLDA) (Vulic and Moens, 2013b; Vulic and Moens, 2013a) or bilingual word embeddings based on neural networks (Gouws et al., 2014; Chandar et al., 2014; Vulic and Moens, 2015; Vulic and Moens, 2016) (approaches respectively noted: Gouws, Chandar and BWESG+cos). All these approaches require at least sentence-aligned/document aligned parallel data (BiLDA, Gouws, Chandar) or non-parallel document-aligned data at the topic level (BWESG+cos). Since specialized comparable corpora are of small size, sentence-aligned (document aligned) parallel data are unavailable and nonparallel document-aligned data at the topic level can’t be provided since specialized comparable corpora usually deal with one single topic. Based on the recent comparison in (Vulic and Moens, 2015; Vuli"
C16-1321,P11-2084,0,0.448124,"Missing"
C18-1080,D16-1250,0,0.190285,"each step of the standard approach (Gaussier et al., 2004; Gamallo, 2007; Ismail and Manandhar, 2010; Prochasson and Fung, 2011; Hazem and Morin, 2012; Bouamor et al., 2013, among others). With the advent of neural network techniques, Mikolov et al. (2013a) were the first to propose a method to learn a linear transformation from the source language into the target language to improve the task of lexicon extraction from bilingual corpora. Faruqui and Dyer (2014) introduced canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. More recently, Artetxe et al. (2016) presented an approach for learning bilingual mappings of word embeddings that preserves monolingual invariance using several meaningful and intuitive constraints related to other proposed methods (Faruqui and Dyer, 2014; Xing et al., 2015). Jakubina and Langlais (2017) made a careful comparison of the approaches of Mikolov et al. (2013a) and Faruqui and Dyer (2014) with the standard approach. They have clearly shown that the two previous approaches outperform the standard approach for very frequent terms to be translated (the number of occurrences is at least 250 from an English-Spanish compa"
C18-1080,P13-2133,0,0.603035,"ural network models (Bengio et al., 2003; Mikolov et al., 2013b). The historical standard approach builds a context vector for each word of the source and the target languages, translates the source context vectors to the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. Different contributions have been proposed in the past few years to improve each step of the standard approach (Gaussier et al., 2004; Gamallo, 2007; Ismail and Manandhar, 2010; Prochasson and Fung, 2011; Hazem and Morin, 2012; Bouamor et al., 2013, among others). With the advent of neural network techniques, Mikolov et al. (2013a) were the first to propose a method to learn a linear transformation from the source language into the target language to improve the task of lexicon extraction from bilingual corpora. Faruqui and Dyer (2014) introduced canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. More recently, Artetxe et al. (2016) presented an approach for learning bilingual mappings of word embeddings that preserves monolingual invariance using several meaningful and intuitive c"
C18-1080,C02-2020,0,0.493512,"resource is a general language dictionary which contains only a few terms related to the medical and wind energy domains. 5.3 Gold Standard To evaluate the quality of bilingual terminology extraction from specialized comparable corpora, a bilingual terminology reference list is required. In the general domain, the reference list is randomly composed of a sub-part of the bilingual dictionary (Gaussier et al., 2004; Jakubina and Langlais, 2017). In the specialized domain, this list is usually composed of few words that reflect the terminology of the specialized comparable corpus. For instance, Chiao and Zweigenbaum (2002) used a list composed of 95 single words, Morin et al. (2007) used 100 single words and Bouamor et al. (2013) used 125 and 79 single words. For breast cancer, the lists are derived from the UMLS12 meta-thesaurus. Concerning wind energy, the lists are provided with the corpora (see footnote 5). Each word composing a pair of terms of a reference list appears at least 5 times in the comparable corpus. The bilingual terminology reference list is composed of 248 French/English single words for the Breast cancer corpus and 150 French/English single words for the Wind energy corpus. 6 Experiments and"
C18-1080,E14-1049,0,0.0759754,"nslated context vectors to each target context vector using a similarity measure. Different contributions have been proposed in the past few years to improve each step of the standard approach (Gaussier et al., 2004; Gamallo, 2007; Ismail and Manandhar, 2010; Prochasson and Fung, 2011; Hazem and Morin, 2012; Bouamor et al., 2013, among others). With the advent of neural network techniques, Mikolov et al. (2013a) were the first to propose a method to learn a linear transformation from the source language into the target language to improve the task of lexicon extraction from bilingual corpora. Faruqui and Dyer (2014) introduced canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. More recently, Artetxe et al. (2016) presented an approach for learning bilingual mappings of word embeddings that preserves monolingual invariance using several meaningful and intuitive constraints related to other proposed methods (Faruqui and Dyer, 2014; Xing et al., 2015). Jakubina and Langlais (2017) made a careful comparison of the approaches of Mikolov et al. (2013a) and Faruqui and Dyer (2014) with the standard approach. They have clearly shown that the two previous ap"
C18-1080,W97-0119,0,0.220071,"alized comparable corpora may be beneficial to bilingual terminology extraction task. The combination of external resources such as a general-domain comparable corpus with a specialized comparable corpus can be performed using word embedding models. We recently conducted a first attempt in Hazem and Morin (2017) and have shown under which conditions external resources introduced in the form of Skip-gram and CBOW models can be jointly used to improve the performance of bilingual terms extraction. However, our approach was not able to compete with the historical count-based projection approach (Fung and McKeown, 1997; Rapp, 1999). Our current work pursues this direction by contrasting different neural embedding models and by showing how to take advantage of their combination. More specifically, we show that the character-based Skip-gram and CBOW models (Bojanowski et al., 2016) drastically outperform other models including Skip-gram and CBOW. We also propose a new approach based on Ensemble models which combines specialized and general domain embeddings to obtain a unified Meta-Embedding model. Our approach shows significant improvements and obtains the best results on two specialized English/French compa"
C18-1080,2007.mtsummit-papers.26,0,0.0641027,", 1999). While in the latter, words are embedded into a low-dimensional vector space using neural network models (Bengio et al., 2003; Mikolov et al., 2013b). The historical standard approach builds a context vector for each word of the source and the target languages, translates the source context vectors to the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. Different contributions have been proposed in the past few years to improve each step of the standard approach (Gaussier et al., 2004; Gamallo, 2007; Ismail and Manandhar, 2010; Prochasson and Fung, 2011; Hazem and Morin, 2012; Bouamor et al., 2013, among others). With the advent of neural network techniques, Mikolov et al. (2013a) were the first to propose a method to learn a linear transformation from the source language into the target language to improve the task of lexicon extraction from bilingual corpora. Faruqui and Dyer (2014) introduced canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. More recently, Artetxe et al. (2016) presented an approach for learning bilingual mappin"
C18-1080,W15-1513,0,0.0248905,"2013a) while acting at the word embedding level representation. Our idea is to enrich the word embedding representation of the source and target languages in order to improve the mapping matrix and so, bilingual terminology extraction from specialized comparable corpora. To do so, we present several ways to take advantage of word embedding models and ensemble approaches. The principle of ensemble approaches is to combine different models in order to capture the strengths of each individual model. The main combination techniques that have shown their effectiveness so far are vectors addition (Garten et al., 2015) and vectors concatenation (Garten et al., 2015; Yin and Sch¨utze, 2016). For vectors addition, given two embedding models, the procedure consists in applying a simple dimension-wise vectors addition2 . For vectors concatenation, given two embedding models of dimensions dim1 and dim2, the resulting concatenated embedding vector will be of size dim1+dim2. The vectors have to be normalized before concatenation. Usually L2 norm is applied3 . Yin and Sch¨utze (2016) performed a weighted concatenation of five embedding models. They also experienced the SVD (Singular Value Decomposition) on top of w"
C18-1080,P04-1067,0,0.543962,"Missing"
C18-1080,fiser-etal-2012-addressing,0,0.0243461,"l vector space using neural network models (Bengio et al., 2003; Mikolov et al., 2013b). The historical standard approach builds a context vector for each word of the source and the target languages, translates the source context vectors to the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. Different contributions have been proposed in the past few years to improve each step of the standard approach (Gaussier et al., 2004; Gamallo, 2007; Ismail and Manandhar, 2010; Prochasson and Fung, 2011; Hazem and Morin, 2012; Bouamor et al., 2013, among others). With the advent of neural network techniques, Mikolov et al. (2013a) were the first to propose a method to learn a linear transformation from the source language into the target language to improve the task of lexicon extraction from bilingual corpora. Faruqui and Dyer (2014) introduced canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. More recently, Artetxe et al. (2016) presented an approach for learning bilingual mappings of word embeddings that preserves monolingual invariance using several mean"
C18-1080,I17-1069,1,0.82322,"nal Conference on Computational Linguistics, pages 937–949 Santa Fe, New Mexico, USA, August 20-26, 2018. According to Jakubina and Langlais (2017), word embeddings are more effective on large comparable corpora than on small comparable corpora. This statement lend support the idea that enriching small specialized comparable corpora may be beneficial to bilingual terminology extraction task. The combination of external resources such as a general-domain comparable corpus with a specialized comparable corpus can be performed using word embedding models. We recently conducted a first attempt in Hazem and Morin (2017) and have shown under which conditions external resources introduced in the form of Skip-gram and CBOW models can be jointly used to improve the performance of bilingual terms extraction. However, our approach was not able to compete with the historical count-based projection approach (Fung and McKeown, 1997; Rapp, 1999). Our current work pursues this direction by contrasting different neural embedding models and by showing how to take advantage of their combination. More specifically, we show that the character-based Skip-gram and CBOW models (Bojanowski et al., 2016) drastically outperform o"
C18-1080,P14-1006,0,0.0343235,"s direction by contrasting different neural embedding models and by showing how to take advantage of their combination. More specifically, we show that the character-based Skip-gram and CBOW models (Bojanowski et al., 2016) drastically outperform other models including Skip-gram and CBOW. We also propose a new approach based on Ensemble models which combines specialized and general domain embeddings to obtain a unified Meta-Embedding model. Our approach shows significant improvements and obtains the best results on two specialized English/French comparable corpora. 2 Related Work According to Hermann and Blunsom (2014), methods dealing with bilingual lexicon extraction from comparable corpora can be classified as distributional-based or distributed-based approaches. In the former, words are represented by their context vectors using a distributional count-based approach also known as the standard approach (Fung, 1998; Rapp, 1999). While in the latter, words are embedded into a low-dimensional vector space using neural network models (Bengio et al., 2003; Mikolov et al., 2013b). The historical standard approach builds a context vector for each word of the source and the target languages, translates the sourc"
C18-1080,C10-2055,0,0.0224508,"in the latter, words are embedded into a low-dimensional vector space using neural network models (Bengio et al., 2003; Mikolov et al., 2013b). The historical standard approach builds a context vector for each word of the source and the target languages, translates the source context vectors to the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. Different contributions have been proposed in the past few years to improve each step of the standard approach (Gaussier et al., 2004; Gamallo, 2007; Ismail and Manandhar, 2010; Prochasson and Fung, 2011; Hazem and Morin, 2012; Bouamor et al., 2013, among others). With the advent of neural network techniques, Mikolov et al. (2013a) were the first to propose a method to learn a linear transformation from the source language into the target language to improve the task of lexicon extraction from bilingual corpora. Faruqui and Dyer (2014) introduced canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. More recently, Artetxe et al. (2016) presented an approach for learning bilingual mappings of word embeddings that p"
C18-1080,E17-2096,0,0.378099,"corpora of different types of discourse and gender (e.g. a corpus of popular science discourse supplementing a corpus of scientific discourse), corpora of general language or out-of-domain data. The main challenge is to know how to associate such resources with a comparable specialized corpus. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 937 Proceedings of the 27th International Conference on Computational Linguistics, pages 937–949 Santa Fe, New Mexico, USA, August 20-26, 2018. According to Jakubina and Langlais (2017), word embeddings are more effective on large comparable corpora than on small comparable corpora. This statement lend support the idea that enriching small specialized comparable corpora may be beneficial to bilingual terminology extraction task. The combination of external resources such as a general-domain comparable corpus with a specialized comparable corpus can be performed using word embedding models. We recently conducted a first attempt in Hazem and Morin (2017) and have shown under which conditions external resources introduced in the form of Skip-gram and CBOW models can be jointly"
C18-1080,P14-2050,0,0.0401562,"h word vectors with subwords information. It takes into account the internal structure of words which can be very useful for morphologically rich languages. It also incorporates character ngram embeddings where each word is represented by a bag-of character n-gram (Bojanowski et al., 2016). More precisely, it uses character embedding and word embedding models jointly performing the vector sum of both to form the final embedding representation of words. We refer to the character Skip-gram model by CharSG and the character CBOW model by CharCBOW. Other models such as the dependency-based model (Levy and Goldberg, 2014) and generalized-based model (Li et al., 2017) were assessed but not presented in this paper for sake of clarity and because of the very low results obtained on the specialized domains when compared to the above presented models. Several pre-trained embedding models are publicly available such as CBOW and Skip-gram models (Mikolov et al., 2013b), global word representation-based models (Pennington et al., 2014), character skip-gram-based models (Bojanowski et al., 2016), etc. If it is interesting to study the impact of pretrained embeddings on bilingual terminology extraction from comparable c"
C18-1080,D17-1257,0,0.0207493,"o account the internal structure of words which can be very useful for morphologically rich languages. It also incorporates character ngram embeddings where each word is represented by a bag-of character n-gram (Bojanowski et al., 2016). More precisely, it uses character embedding and word embedding models jointly performing the vector sum of both to form the final embedding representation of words. We refer to the character Skip-gram model by CharSG and the character CBOW model by CharCBOW. Other models such as the dependency-based model (Levy and Goldberg, 2014) and generalized-based model (Li et al., 2017) were assessed but not presented in this paper for sake of clarity and because of the very low results obtained on the specialized domains when compared to the above presented models. Several pre-trained embedding models are publicly available such as CBOW and Skip-gram models (Mikolov et al., 2013b), global word representation-based models (Pennington et al., 2014), character skip-gram-based models (Bojanowski et al., 2016), etc. If it is interesting to study the impact of pretrained embeddings on bilingual terminology extraction from comparable corpora. The major part of the above cited pre-"
C18-1080,N15-1142,0,0.0248719,"hat makes use of a global factorization model and local context window methods to represent words in a global vector space model (Pennington et al., 2014). This model directly captures the global statistics from the corpus based on co-occurrence word probabilities. Its training objective is to learn word vectors such that their dot product equals the log-probability of word’s co-occurrence. Glove has shown good results in word analogy, word similarity, and named entity recognition tasks. Structured Embeddings are two adaptations of CBOW and Skip-gram models that include ordering information1 (Ling et al., 2015). While word2vec is insensitive to word order, the structured embedding model includes position information in the context representation of words. Given the embedding of the center word w, the Skip-gram model for instance uses a single output matrix to predict every contextual word. In contrast, the structured Skip-gram adapts the model to the positioning of the surrounding words. It defines an output for each relative position to the center word. The adaptation of CBOW is the continuous window model where the input is the concatenation of the embeddings of context words. While in the standar"
C18-1080,P14-1121,1,0.899972,"rse, etc. without having a parallel source text-target text relationship, allow access to the original vocabulary without falling under the influence of the human translation. Compiling a large comparable corpus is easier, especially for general language (Talvensaari et al., 2007). In contrast, specialized comparable corpora are traditionally of modest size due to the difficulty to obtain many specialized documents in a language other than English. Specialized comparable corpora have a size of around one million words whereas general-domain comparable corpora can gather several million words (Morin and Hazem, 2014). One way to overcome the small size of specialized comparable corpora is to associate external resources. These resources may be close specialized corpora (e.g. a breast cancer corpus may benefit from contexts derived from a more general oncology corpus), corpora of different types of discourse and gender (e.g. a corpus of popular science discourse supplementing a corpus of scientific discourse), corpora of general language or out-of-domain data. The main challenge is to know how to associate such resources with a comparable specialized corpus. This work is licensed under a Creative Commons A"
C18-1080,P07-1084,1,0.829391,"erms related to the medical and wind energy domains. 5.3 Gold Standard To evaluate the quality of bilingual terminology extraction from specialized comparable corpora, a bilingual terminology reference list is required. In the general domain, the reference list is randomly composed of a sub-part of the bilingual dictionary (Gaussier et al., 2004; Jakubina and Langlais, 2017). In the specialized domain, this list is usually composed of few words that reflect the terminology of the specialized comparable corpus. For instance, Chiao and Zweigenbaum (2002) used a list composed of 95 single words, Morin et al. (2007) used 100 single words and Bouamor et al. (2013) used 125 and 79 single words. For breast cancer, the lists are derived from the UMLS12 meta-thesaurus. Concerning wind energy, the lists are provided with the corpora (see footnote 5). Each word composing a pair of terms of a reference list appears at least 5 times in the comparable corpus. The bilingual terminology reference list is composed of 248 French/English single words for the Breast cancer corpus and 150 French/English single words for the Wind energy corpus. 6 Experiments and Results We conducted two sets of experiments. The first one"
C18-1080,J03-1002,0,0.0174345,"of different word embedding models for bilingual terminology extraction from specialized comparable corpora. We emphasize how the character-based embedding model outperforms other models on the quality of the extracted bilingual lexicons. Further more, we propose a new efficient way to combine different embedding models learned from specialized and general-domain data sets. Our approach leads to higher performance than the best individual embedding model. 1 Introduction Bilingual lexicons are fundamental resources in multilingual natural language processing tasks such as machine translation (Och and Ney, 2003), cross-language information retrieval (Nie, 2010) or computerassisted translation (Delpech, 2014). Because a manual compilation of bilingual lexicons requires substantial human efforts, bilingual lexicons are automatically extracted from bilingual corpora. These corpora can be parallel or comparable data sets. Despite good results obtained when compiling bilingual lexicons from parallel corpora, the latter are scarce resources, especially for specialized and technical domains and for language pairs not involving English. In this context, comparable corpora are an interesting and practical alt"
C18-1080,D14-1162,0,0.0971268,"the representations of the middle word. If these models exhibit similar architectures, CBOW is faster and more suitable for large data sets while Skip-gram gives better word representations when monolingual data is small (Mikolov et al., 2013a). Glove takes advantage of the main benefits of count data while simultaneously capturing the meaningful linear substructures prevalent in prediction-based methods such as word2vec. It is a global log-linear regression model that makes use of a global factorization model and local context window methods to represent words in a global vector space model (Pennington et al., 2014). This model directly captures the global statistics from the corpus based on co-occurrence word probabilities. Its training objective is to learn word vectors such that their dot product equals the log-probability of word’s co-occurrence. Glove has shown good results in word analogy, word similarity, and named entity recognition tasks. Structured Embeddings are two adaptations of CBOW and Skip-gram models that include ordering information1 (Ling et al., 2015). While word2vec is insensitive to word order, the structured embedding model includes position information in the context representatio"
C18-1080,P11-1133,0,0.0189904,"edded into a low-dimensional vector space using neural network models (Bengio et al., 2003; Mikolov et al., 2013b). The historical standard approach builds a context vector for each word of the source and the target languages, translates the source context vectors to the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. Different contributions have been proposed in the past few years to improve each step of the standard approach (Gaussier et al., 2004; Gamallo, 2007; Ismail and Manandhar, 2010; Prochasson and Fung, 2011; Hazem and Morin, 2012; Bouamor et al., 2013, among others). With the advent of neural network techniques, Mikolov et al. (2013a) were the first to propose a method to learn a linear transformation from the source language into the target language to improve the task of lexicon extraction from bilingual corpora. Faruqui and Dyer (2014) introduced canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. More recently, Artetxe et al. (2016) presented an approach for learning bilingual mappings of word embeddings that preserves monolingual invari"
C18-1080,P99-1067,0,0.39862,"a may be beneficial to bilingual terminology extraction task. The combination of external resources such as a general-domain comparable corpus with a specialized comparable corpus can be performed using word embedding models. We recently conducted a first attempt in Hazem and Morin (2017) and have shown under which conditions external resources introduced in the form of Skip-gram and CBOW models can be jointly used to improve the performance of bilingual terms extraction. However, our approach was not able to compete with the historical count-based projection approach (Fung and McKeown, 1997; Rapp, 1999). Our current work pursues this direction by contrasting different neural embedding models and by showing how to take advantage of their combination. More specifically, we show that the character-based Skip-gram and CBOW models (Bojanowski et al., 2016) drastically outperform other models including Skip-gram and CBOW. We also propose a new approach based on Ensemble models which combines specialized and general domain embeddings to obtain a unified Meta-Embedding model. Our approach shows significant improvements and obtains the best results on two specialized English/French comparable corpora"
C18-1080,tiedemann-2012-parallel,0,0.0157051,"itle or the keywords contain the term breast cancer in English and its translation in French. Wind energy corpus (WE) has been released in the TTC project5 . This corpus has been crawled from the Web using Babouk crawler (Groc, 2011) based on several keywords such as wind, energy, rotor in English and its translation in French. In addition, we use three corpora of general language as external resources: JRC acquis corpus (JRC) is a collection of legislative texts of the European Union6 . We used the French-English version at OPUS which is based on the paragraph-aligned corpus provided by JRC (Tiedemann, 2012). Common crawl corpus (CC) is an open repository of data collected over 7 years of web crawling sets of raw web page data and text extracts7 . 4 www.sciencedirect.com/ www.ttc-project.eu/index.php/releases-publications 6 opus.lingfil.uu.se/JRC-Acquis.php 7 commoncrawl.org 5 941 Wikipedia corpus (Wiki) The English wikipedia corpus8 is a dump which was released on 03-Feb-2018 and the French wikipedia corpus 9 was released on 02-Feb-2018. Even if JRC, CC are parallel corpora, we didn’t explicitly exploit their parallel relationship. We considered these external data sets as if they were comparabl"
C18-1080,P15-2118,0,0.0434677,"Missing"
C18-1080,N15-1104,0,0.115686,"et al. (2013a) were the first to propose a method to learn a linear transformation from the source language into the target language to improve the task of lexicon extraction from bilingual corpora. Faruqui and Dyer (2014) introduced canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. More recently, Artetxe et al. (2016) presented an approach for learning bilingual mappings of word embeddings that preserves monolingual invariance using several meaningful and intuitive constraints related to other proposed methods (Faruqui and Dyer, 2014; Xing et al., 2015). Jakubina and Langlais (2017) made a careful comparison of the approaches of Mikolov et al. (2013a) and Faruqui and Dyer (2014) with the standard approach. They have clearly shown that the two previous approaches outperform the standard approach for very frequent terms to be translated (the number of occurrences is at least 250 from an English-Spanish comparable corpus obtained from the 6th workshop on statistical machine translation gathering 2.55 giga words). On the other hand, when the terms are less frequent (the number of occurrences is less than 25 from a French/English comparable corpo"
C18-1080,P16-1128,0,0.0365979,"Missing"
daille-hazem-2014-semi,ferret-2010-testing,0,\N,Missing
daille-hazem-2014-semi,W04-2607,0,\N,Missing
daille-hazem-2014-semi,P08-3001,0,\N,Missing
daille-hazem-2014-semi,P90-1034,0,\N,Missing
daille-hazem-2014-semi,W03-1610,0,\N,Missing
daille-hazem-2014-semi,C10-1070,0,\N,Missing
daille-hazem-2014-semi,P06-2111,0,\N,Missing
daille-hazem-2014-semi,P07-1084,1,\N,Missing
daille-hazem-2014-semi,P98-2127,0,\N,Missing
daille-hazem-2014-semi,C98-2122,0,\N,Missing
daille-hazem-2014-semi,I13-1150,0,\N,Missing
daille-hazem-2014-semi,C12-1110,1,\N,Missing
F13-1018,C10-1013,0,0.0571207,"Missing"
F13-1018,C02-1166,0,0.101792,"Missing"
F13-1018,W95-0114,0,0.163721,"Missing"
F13-1018,P98-1069,0,0.229019,"Missing"
F13-1018,2007.mtsummit-papers.26,0,0.0985598,"Missing"
F13-1018,W09-1117,0,0.0459622,"Missing"
F13-1018,P04-1067,0,0.0670818,"Missing"
F13-1018,P08-1088,0,0.0472545,"Missing"
F13-1018,C10-1070,0,0.052701,"Missing"
F13-1018,C10-1073,0,0.0386019,"Missing"
F13-1018,2009.jeptalnrecital-long.6,1,0.844479,"Missing"
F13-1018,2004.jeptalnrecital-long.13,1,0.832715,"Missing"
F13-1018,2009.jeptalnrecital-long.10,1,0.85046,"Missing"
F13-1018,P95-1050,0,0.294849,"Missing"
F13-1018,P99-1067,0,0.10024,"Missing"
hazem-morin-2012-adaptive,W97-0119,0,\N,Missing
hazem-morin-2012-adaptive,I05-1062,1,\N,Missing
hazem-morin-2012-adaptive,C02-1166,0,\N,Missing
hazem-morin-2012-adaptive,C02-2020,0,\N,Missing
hazem-morin-2012-adaptive,P98-1069,0,\N,Missing
hazem-morin-2012-adaptive,C98-1066,0,\N,Missing
hazem-morin-2012-adaptive,P99-1067,0,\N,Missing
hazem-morin-2012-adaptive,P04-1067,0,\N,Missing
hazem-morin-2012-adaptive,C10-1070,0,\N,Missing
hazem-morin-2012-adaptive,P07-1084,1,\N,Missing
I13-1196,C02-1011,0,0.0385796,"Missing"
I13-1196,C02-2020,0,0.876242,"Missing"
I13-1196,I05-1062,1,0.841234,"nguage is computed using a similarity measure (cosine (Salton and Lesk, 1968), Jaccard (Grefenstette, 1994)...). Finally, the translation candidates are ranked according to their similarity score. Many variants of the Standard Approach have been proposed. They can differ in context representation (window-based, syntactic-based) (Morin et al., 2007; Gamallo, 2008), corpus characteristics (small, large, general or domain specific...)(Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007), type of words to translate (single word terms (SWTs) or multiword terms (MWTs))(Rapp, 1999; Daille and Morin, 2005), words frequency (less frequent, rare...)(Pekar et al., 2006), etc. There exist other approaches for bilingual lexicon extraction. D´ejean et al. (2002) introduce the Extended Approach to avoid the insufficient coverage of the bilingual dictionary required for the translation of source context vectors. A variation of the latter method based on centroid is proposed by Daille and Morin (2005). Haghighi et al. (2008) employ dimension reduction using canonical component analysis (CCA) and Rubino and Linares (2011) propose a multi-view approach based on linear discriminant analysis (LDA) among oth"
I13-1196,C02-1166,0,0.885949,"Missing"
I13-1196,J93-1003,0,0.644393,"ith similar meaning tend to occur in similar contexts, has been extended to the bilingual scenario (Fung, 1998; Rapp, 1999). Hence, using comparable corpora, a translation of a source word can be found by identifying a target word with the most similar context. A popular method often used as a baseline is the Standard Approach (Fung, 1998). It consists of using the bag-ofwords paradigm to represent words of source and target language by their context vector. After word contexts have been weighted using an association measure (the point-wise mutual information (Fano, 1961), the log-likelihood (Dunning, 1993), the discounted odds-ratio (Laroche and Langlais, 2010)), the similarity between a source word’s context vector and all the context vectors in the target language is computed using a similarity measure (cosine (Salton and Lesk, 1968), Jaccard (Grefenstette, 1994)...). Finally, the translation candidates are ranked according to their similarity score. Many variants of the Standard Approach have been proposed. They can differ in context representation (window-based, syntactic-based) (Morin et al., 2007; Gamallo, 2008), corpus characteristics (small, large, general or domain specific...)(Chiao a"
I13-1196,P07-2008,0,0.064878,"Missing"
I13-1196,W97-0119,0,0.627014,"tion of source context vectors. A variation of the latter method based on centroid is proposed by Daille and Morin (2005). Haghighi et al. (2008) employ dimension reduction using canonical component analysis (CCA) and Rubino and Linares (2011) propose a multi-view approach based on linear discriminant analysis (LDA) among others. 3 Standard Approach The Standard Approach is based on words cooccurrence vectors. The basic idea is to go through a corpus and to count the number of times n(c, t) each context word c occurs within a window of a certain size w around each target word t. According to (Fung and Mckeown, 1997; Fung, 1998; Rapp, 1999), the Standard Approach can be carried out as follows: For a source word to translate wis , we first build its context vector vwis . The vector vwis contains all the words that co-occur with wis within windows of n words. Let’s denote by coocc(wis , wjs ) the co-occurrence value of wis and a given word of its context wjs . The process of building context vectors is repeated for all the words of the target language. An association measure such as the point-wise mutual information (Fano, 1961), the log-likelihood (Dunning, 1993) or the discounted odds-ratio (Laroche and"
I13-1196,W95-0114,0,0.922129,"Missing"
I13-1196,P08-1088,0,0.025024,"ain specific...)(Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007), type of words to translate (single word terms (SWTs) or multiword terms (MWTs))(Rapp, 1999; Daille and Morin, 2005), words frequency (less frequent, rare...)(Pekar et al., 2006), etc. There exist other approaches for bilingual lexicon extraction. D´ejean et al. (2002) introduce the Extended Approach to avoid the insufficient coverage of the bilingual dictionary required for the translation of source context vectors. A variation of the latter method based on centroid is proposed by Daille and Morin (2005). Haghighi et al. (2008) employ dimension reduction using canonical component analysis (CCA) and Rubino and Linares (2011) propose a multi-view approach based on linear discriminant analysis (LDA) among others. 3 Standard Approach The Standard Approach is based on words cooccurrence vectors. The basic idea is to go through a corpus and to count the number of times n(c, t) each context word c occurs within a window of a certain size w around each target word t. According to (Fung and Mckeown, 1997; Fung, 1998; Rapp, 1999), the Standard Approach can be carried out as follows: For a source word to translate wis , we fir"
I13-1196,C10-1070,0,0.237652,"edicting models with the traditional Standard Approach (Fung, 1998) and show that once we have identified the best procedures, our method increases significantly the performance of extracting word translations from comparable corpora. 1 Introduction Using comparable corpora for bilingual lexicon extraction is becoming more and more a matter of interest, especially because of the easier availability of this kind of corpora comparing to parallel ones. Many researchers proposed a variety of approaches (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007; Laroche and Langlais, 2010, among others). While different improvements were achieved, the starting point remains words’co-occurrences as they represent the observable evidence that can be distilled from a corpus. Hence, frequency counts for word pairs often serve as a basis for distributional methods. The main assumption underlying bilingual lexicon extraction is: two words are more likely to be a translation of each other if they share the same lexical contexts (Fung, 1998). The most popular approach named, the Standard Approach (Fung and Mckeown, 1997; Rapp, 1999), makes use of this assumption to perform bilingual l"
I13-1196,P07-1084,1,0.807102,"compare different predicting models with the traditional Standard Approach (Fung, 1998) and show that once we have identified the best procedures, our method increases significantly the performance of extracting word translations from comparable corpora. 1 Introduction Using comparable corpora for bilingual lexicon extraction is becoming more and more a matter of interest, especially because of the easier availability of this kind of corpora comparing to parallel ones. Many researchers proposed a variety of approaches (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007; Laroche and Langlais, 2010, among others). While different improvements were achieved, the starting point remains words’co-occurrences as they represent the observable evidence that can be distilled from a corpus. Hence, frequency counts for word pairs often serve as a basis for distributional methods. The main assumption underlying bilingual lexicon extraction is: two words are more likely to be a translation of each other if they share the same lexical contexts (Fung, 1998). The most popular approach named, the Standard Approach (Fung and Mckeown, 1997; Rapp, 1999), makes use of this assum"
I13-1196,2009.mtsummit-posters.14,1,0.887361,"ect 3 http://www.nlm.nih.gov/research/umls 1396 5.4 Training Dataset Predicting models such as linear regression or the Good-Turing estimator need a large training corpus to estimate the adjusted co-occurrences. For that purpose, we chose a training corpus composed of two sets. A small set of 500,000 words and a large set of 10 million words. We selected the documents published in 1994 from newspapers Los Angeles Times/Le Monde. 6 Experiments and Results The baseline in our experiments is the Standard Approach (Fung, 1998) which is often used for comparison (Pekar et al., 2006; Gamallo, 2008; Prochasson and Morin, 2009), etc. In this section, we first give the parameters of the standard approach, than we present the results of the experiments conducted on the two corpora presented above: ’Breast cancer’ and ’Wind energy’. 6.1 Experimental Setup Using the Standard Approach, three major parameters need to be set: 1. The size of the window used to build the context vectors (Morin et al., 2007; Gamallo, 2008) 2. The association measure (the log-likelihood (Dunning, 1993), the point-wise mutual information (Fano, 1961), the discounted oddsratio (Laroche and Langlais, 2010)...) 3. The similarity measure (the weigh"
I13-1196,P99-1067,0,0.965104,"e building a predictive model of word co-occurrence counts. We compare different predicting models with the traditional Standard Approach (Fung, 1998) and show that once we have identified the best procedures, our method increases significantly the performance of extracting word translations from comparable corpora. 1 Introduction Using comparable corpora for bilingual lexicon extraction is becoming more and more a matter of interest, especially because of the easier availability of this kind of corpora comparing to parallel ones. Many researchers proposed a variety of approaches (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007; Laroche and Langlais, 2010, among others). While different improvements were achieved, the starting point remains words’co-occurrences as they represent the observable evidence that can be distilled from a corpus. Hence, frequency counts for word pairs often serve as a basis for distributional methods. The main assumption underlying bilingual lexicon extraction is: two words are more likely to be a translation of each other if they share the same lexical contexts (Fung, 1998). The most popular approach named, the Standard"
I17-1069,C02-2020,0,0.726673,"Missing"
I17-1069,E14-1049,0,0.103919,"over specialized and general domain data. min W n X kW xi − zi k2 (1) i=1 At prediction time, we can transfer the word embedding x for a word to be translated in the target language using the translation matrix such as z = W x. The candidate translations are obtained by ranking the closest target words to z according to a similarity measure such as the Cosine measure. Recently, Artetxe et al. (2016) presented an approach for learning bilingual mappings of word embeddings that preserves monolingual invariance using several meaningful and intuitive constraints related to other proposed methods (Faruqui and Dyer, 2014; Xing et al., 2015). These constraints are orthogonality, vectors length normalization for maximum cosine and mean centering for maximum covariance. Monolingual invariance tends to preserve the dot products after mapping, in order to avoid performance drop in monolingual tasks, while dimension-wise mean centering tends to insure that two randomly taken words would not be semantically related. This approach has shown meaningful improvements for both monolingual and bilingual tasks. Data Combination Using Neural Networks 3.1 Global Data Combination Using Neural Network Models This approach can"
I17-1069,W15-1513,0,0.207687,"e counts of specialized comparable corpora (Hazem and Morin, 2016). Our work is in this line and attempts to find out how a general-domain data can enrich a specialized comparable corpora to improve bilingual terminology extraction from specialized comparable corpora. Since bilingual word embeddings have recently provided efficient models for learning bilingual distributed representation of words from large general-domain data (Mikolov et al., 2013), we contrast different popular word embedding models for this task. In addition, we explore combinations of word embedding models as suggested by Garten et al. (2015) to improve distributed representations. We compare the results obtained with the state-of-the-art context-based projection approach. Our results show under which conditions the proposed model can compete with stateBilingual lexicon extraction from comparable corpora is constrained by the small amount of available data when dealing with specialized domains. This aspect penalizes the performance of distributionalbased approaches, which is closely related to the reliability of word’s cooccurrence counts extracted from comparable corpora. A solution to avoid this limitation is to associate extern"
I17-1069,P04-1067,0,0.402529,"Missing"
I17-1069,D16-1250,0,0.584267,"Projection Approach The historical context-based projection approach, known as the standard approach, has been studied by a number of researchers (Fung, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson and Fung, 2011; Bouamor et al., 2013; Morin and Hazem, 2016, among others). Its implementation can be carried out by applying the following steps: 2.2 Word Embedding Based Approach Bilingual word embeddings has become a source of great interest in recent times (Mikolov et al., 2013; Vuli´c and Moens, 2013; Zou et al., 2013; Chandar et al., 2014; Gouws et al., 2014; Artetxe et al., 2016, among others). Mikolov et al. (2013) was the first to propose a method to learn a linear transformation from the source language to the 1. For each word w of the source and the target languages, we build a context vector (resp. s and t for source and target languages) consisting in the measure of association of each word that appears in a short window of words 686 target language to improve the task of lexicon extraction from bilingual corpora. 3 During the training time of Mikolov’s method, for all {xi , zi }ni=1 bilingual word pairs of the seed lexicon, the word embedding xi ∈ Rd1 of word"
I17-1069,P13-2133,0,0.864718,"orpora are unreliable in specialized domain. This problem persists with other paradigms such as Canonical Correlation Analysis (CCA) (Gaussier et al., 2004), Independent Component Analysis (ICA) (Hazem and Morin, 2012) and Bilingual Latent Dirichlet Allocation (BiLDA) (Vuli´c et al., 2011). A solution to avoid this limitation and to increase the representativity of distributional representations is to associate external resources with the specialized comparable corpus. These resources can be lexical databases such as WordNet which allows the disambiguation of translations of polysemous words (Bouamor et al., 2013) or general-domain data to improve word cooccurrence counts of specialized comparable corpora (Hazem and Morin, 2016). Our work is in this line and attempts to find out how a general-domain data can enrich a specialized comparable corpora to improve bilingual terminology extraction from specialized comparable corpora. Since bilingual word embeddings have recently provided efficient models for learning bilingual distributed representation of words from large general-domain data (Mikolov et al., 2013), we contrast different popular word embedding models for this task. In addition, we explore com"
I17-1069,hazem-morin-2012-adaptive,1,0.819257,"manuel.morin}@univ-nantes.fr Abstract rable corpora are often of modest size (around 1 million words) due to the difficulty to obtain many specialized documents in a language other than English. Consequently, word co-occurrence counts of the historical context-based projection approach, known as the standard approach (Fung, 1995; Rapp, 1995), dedicated to bilingual lexicon extraction from comparable corpora are unreliable in specialized domain. This problem persists with other paradigms such as Canonical Correlation Analysis (CCA) (Gaussier et al., 2004), Independent Component Analysis (ICA) (Hazem and Morin, 2012) and Bilingual Latent Dirichlet Allocation (BiLDA) (Vuli´c et al., 2011). A solution to avoid this limitation and to increase the representativity of distributional representations is to associate external resources with the specialized comparable corpus. These resources can be lexical databases such as WordNet which allows the disambiguation of translations of polysemous words (Bouamor et al., 2013) or general-domain data to improve word cooccurrence counts of specialized comparable corpora (Hazem and Morin, 2016). Our work is in this line and attempts to find out how a general-domain data ca"
I17-1069,C16-1321,1,0.620478,"n Analysis (CCA) (Gaussier et al., 2004), Independent Component Analysis (ICA) (Hazem and Morin, 2012) and Bilingual Latent Dirichlet Allocation (BiLDA) (Vuli´c et al., 2011). A solution to avoid this limitation and to increase the representativity of distributional representations is to associate external resources with the specialized comparable corpus. These resources can be lexical databases such as WordNet which allows the disambiguation of translations of polysemous words (Bouamor et al., 2013) or general-domain data to improve word cooccurrence counts of specialized comparable corpora (Hazem and Morin, 2016). Our work is in this line and attempts to find out how a general-domain data can enrich a specialized comparable corpora to improve bilingual terminology extraction from specialized comparable corpora. Since bilingual word embeddings have recently provided efficient models for learning bilingual distributed representation of words from large general-domain data (Mikolov et al., 2013), we contrast different popular word embedding models for this task. In addition, we explore combinations of word embedding models as suggested by Garten et al. (2015) to improve distributed representations. We co"
I17-1069,P11-2084,0,0.0576393,"Missing"
I17-1069,P14-1006,0,0.0364936,"used to extract bilingual lexicons from comparable corpora. These approaches are both based on monolingual lexical context analysis and relies on the distributional hypothesis (Harris, 1968) which postulates that a word and its translation tend to appear in the same lexical contexts. This is the hypothesis that tends to be reduced to the famous sentence of the British linguist J. R. Firth (1957, p. 11) who said: “You shall know a word by the company it keeps.” even if the context was related to collocates. The two approaches are known as distributional and distributed semantics (according to Hermann and Blunsom (2014)). The first one is based on vector space models while the second one is based on neural language models. 2.1 Context-Based Projection Approach The historical context-based projection approach, known as the standard approach, has been studied by a number of researchers (Fung, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson and Fung, 2011; Bouamor et al., 2013; Morin and Hazem, 2016, among others). Its implementation can be carried out by applying the following steps: 2.2 Word Embedding Based Approach Bilingual word embeddings has become a source of great interest"
I17-1069,D13-1168,0,0.0434916,"Missing"
I17-1069,C10-1070,0,0.0231867,"s our conclusion. 2 2. For a word i to be translated, its context vector i is projected from the source to the target language by translating each element of its context vector thanks to a bilingual seed lexicon. 3. The translated context vector i is compared to each context vector t of the target language using a similarity measure such as Cosine or weighted Jaccard. The candidate translations are then ranked according to the scores of a given similarity measure. State-of-the-Art Approaches This approach is very sensitive to the choice of parameters. We invite readers to consult the study of Laroche and Langlais (2010) in which the influence of parameters such as the size of the context, the choice of the association and similarity measures have been examined. In order to improve the quality of bilingual terminology extraction from specialized comparable corpora, Hazem and Morin (2016) have proposed two ways to combine specialized comparable corpora with external resources. The hypothesis is that word co-occurrences learned from a large general-domain corpus for general words improve the characterisation of the specific vocabulary of the specialized corpus. The first adaptation called Global Standard Approa"
I17-1069,P15-2118,0,0.0643238,"Missing"
I17-1069,N15-1104,0,0.0388832,"eral domain data. min W n X kW xi − zi k2 (1) i=1 At prediction time, we can transfer the word embedding x for a word to be translated in the target language using the translation matrix such as z = W x. The candidate translations are obtained by ranking the closest target words to z according to a similarity measure such as the Cosine measure. Recently, Artetxe et al. (2016) presented an approach for learning bilingual mappings of word embeddings that preserves monolingual invariance using several meaningful and intuitive constraints related to other proposed methods (Faruqui and Dyer, 2014; Xing et al., 2015). These constraints are orthogonality, vectors length normalization for maximum cosine and mean centering for maximum covariance. Monolingual invariance tends to preserve the dot products after mapping, in order to avoid performance drop in monolingual tasks, while dimension-wise mean centering tends to insure that two randomly taken words would not be semantically related. This approach has shown meaningful improvements for both monolingual and bilingual tasks. Data Combination Using Neural Networks 3.1 Global Data Combination Using Neural Network Models This approach can be seen as similar t"
I17-1069,P07-1084,1,0.954959,"ntence of the British linguist J. R. Firth (1957, p. 11) who said: “You shall know a word by the company it keeps.” even if the context was related to collocates. The two approaches are known as distributional and distributed semantics (according to Hermann and Blunsom (2014)). The first one is based on vector space models while the second one is based on neural language models. 2.1 Context-Based Projection Approach The historical context-based projection approach, known as the standard approach, has been studied by a number of researchers (Fung, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson and Fung, 2011; Bouamor et al., 2013; Morin and Hazem, 2016, among others). Its implementation can be carried out by applying the following steps: 2.2 Word Embedding Based Approach Bilingual word embeddings has become a source of great interest in recent times (Mikolov et al., 2013; Vuli´c and Moens, 2013; Zou et al., 2013; Chandar et al., 2014; Gouws et al., 2014; Artetxe et al., 2016, among others). Mikolov et al. (2013) was the first to propose a method to learn a linear transformation from the source language to the 1. For each word w of the source and the target languages, we"
I17-1069,D13-1141,0,0.0202803,"nd one is based on neural language models. 2.1 Context-Based Projection Approach The historical context-based projection approach, known as the standard approach, has been studied by a number of researchers (Fung, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson and Fung, 2011; Bouamor et al., 2013; Morin and Hazem, 2016, among others). Its implementation can be carried out by applying the following steps: 2.2 Word Embedding Based Approach Bilingual word embeddings has become a source of great interest in recent times (Mikolov et al., 2013; Vuli´c and Moens, 2013; Zou et al., 2013; Chandar et al., 2014; Gouws et al., 2014; Artetxe et al., 2016, among others). Mikolov et al. (2013) was the first to propose a method to learn a linear transformation from the source language to the 1. For each word w of the source and the target languages, we build a context vector (resp. s and t for source and target languages) consisting in the measure of association of each word that appears in a short window of words 686 target language to improve the task of lexicon extraction from bilingual corpora. 3 During the training time of Mikolov’s method, for all {xi , zi }ni=1 bilingual word"
I17-1069,P11-1133,0,0.0879748,"h linguist J. R. Firth (1957, p. 11) who said: “You shall know a word by the company it keeps.” even if the context was related to collocates. The two approaches are known as distributional and distributed semantics (according to Hermann and Blunsom (2014)). The first one is based on vector space models while the second one is based on neural language models. 2.1 Context-Based Projection Approach The historical context-based projection approach, known as the standard approach, has been studied by a number of researchers (Fung, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson and Fung, 2011; Bouamor et al., 2013; Morin and Hazem, 2016, among others). Its implementation can be carried out by applying the following steps: 2.2 Word Embedding Based Approach Bilingual word embeddings has become a source of great interest in recent times (Mikolov et al., 2013; Vuli´c and Moens, 2013; Zou et al., 2013; Chandar et al., 2014; Gouws et al., 2014; Artetxe et al., 2016, among others). Mikolov et al. (2013) was the first to propose a method to learn a linear transformation from the source language to the 1. For each word w of the source and the target languages, we build a context vector (re"
I17-1069,P95-1050,0,0.339488,"reliability of word’s cooccurrence counts extracted from comparable corpora. A solution to avoid this limitation is to associate external resources with the comparable corpus. Since bilingual word embeddings have recently shown efficient models for learning bilingual distributed representation of words, we explore different word embedding models and show how a general-domain comparable corpus can enrich a specialized comparable corpus via neural networks. 1 Introduction Bilingual lexicon extraction from comparable corpora has shown substantial growth since the seminal work of Fung (1995) and Rapp (1995). Comparable corpora, which are comprised of texts sharing common features such as domain, genre, sampling period, etc. and without having a source text/target text relationship (McEnery and Xiao, 2007), are more abundant and reliable resources than parallel corpora. On the one hand, parallel corpora are difficult to obtain for language pairs not involving English. On the other hand, as parallel corpora are comprised of a pair of translated texts, the vocabulary appearing in the translated texts is highly influenced by the source texts. These problems are aggravated in specialized and technica"
I17-1069,P99-1067,0,0.532227,"that tends to be reduced to the famous sentence of the British linguist J. R. Firth (1957, p. 11) who said: “You shall know a word by the company it keeps.” even if the context was related to collocates. The two approaches are known as distributional and distributed semantics (according to Hermann and Blunsom (2014)). The first one is based on vector space models while the second one is based on neural language models. 2.1 Context-Based Projection Approach The historical context-based projection approach, known as the standard approach, has been studied by a number of researchers (Fung, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson and Fung, 2011; Bouamor et al., 2013; Morin and Hazem, 2016, among others). Its implementation can be carried out by applying the following steps: 2.2 Word Embedding Based Approach Bilingual word embeddings has become a source of great interest in recent times (Mikolov et al., 2013; Vuli´c and Moens, 2013; Zou et al., 2013; Chandar et al., 2014; Gouws et al., 2014; Artetxe et al., 2016, among others). Mikolov et al. (2013) was the first to propose a method to learn a linear transformation from the source language to the 1. For each"
I17-4034,D16-1250,0,0.149339,"1, 2017. 2017 AFNLP training set, 669 questions for the development set and 2012 for the test set. Hereafter an example extracted from the Earth Science domain: is: given a set of similar sentences, the goal is to build a more discriminant and representative sentence embedding space. We first compute word embeddings of the entire corpus, then, each sentence is represented by an element-wise addition of its word embedding vectors. Finally, a mapping matrix is built using the SVD decomposition to project sentences in a new subspace. Similar sentences are moved closer thanks to a mapping matrix (Artetxe et al., 2016) learned from a training dataset containing annotated similar sentences. Basically, a set of similar sentence pairs is used as seed information to build the mapping matrix. The optimal mapping is computed by minimizing the Euclidean distance between the seed sentence pairs. MappSent approach consists of the following steps: • Most tsunami are caused by: 1. 2. 3. 4. 3 (A) Earthquakes (B) Meteorites (C) Volcanic eruptions (D) Collisions of ships at sea System Description In order to understand the principle behind MappSent approach, it is important to introduce the task for which it has been des"
I17-4034,Q14-1017,0,0.0865026,"Missing"
I17-4034,P15-1150,0,0.0691051,"Missing"
I17-4034,hazem-etal-2017-mappsent,1,0.87435,"ws. Section 2 presents the multi-choice question answering in examination task and the provided training datasets. Section 3 describes MappSent, the textual similarity approach and its two adaptations to multi-choice question answering. Section 4 is devoted to the evaluation of the different approaches. Section 5 discusses the results and finally, Section 6 presents our conclusion. In this paper we present MappSent, a textual similarity approach that we applied to the multi-choice question answering in exams shared task. MappSent has initially been proposed for question-toquestion similarity (Hazem et al., 2017). In this work, we present the results of two adaptations of MappSent for the question answering task on the English dataset. 1 Introduction Question-Answering is certainly one of the most challenging area of research of information retrieval (IR) and natural language processing (NLP) domains. If many investigations and countless approaches have been proposed so far, the developed systems still have difficulties to deal with text understanding. Mainly because of the complexity of the language in terms of lexical, semantic and pragmatic representations. However, with the boom of neural networks"
I17-4034,P14-1062,0,0.00988522,"proposed so far, the developed systems still have difficulties to deal with text understanding. Mainly because of the complexity of the language in terms of lexical, semantic and pragmatic representations. However, with the boom of neural networks, various deep learning approaches ranging from a word level embedding representation (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) to a longer textual level embedding representation such as phrases, sentences, paragraphs or documents (Socher et al., 2011; Mikolov et al., 2013; Le and Mikolov, 2014; Kalchbrenner et al., 2014; Kiros et al., 2015; Wieting et al., 2016; Arora et al., 2017) have been proposed and have shown promising results in many applications. Not to mention other more sophisticated approaches like recurrent neural networks (RNN) (Socher et al., 2011, 2014; Kiros et al., 2015), long short-term memory (LSTM) to capture long distance dependency (Tai et al., 2015) or convolutional neural networks (CNNs) (Kalchbrenner et al., 2014) to represent sentences. 2 Task and Resource Description The task consists of a multi-choice question challenge. Given a question, four answers are provided and the purpose"
I17-4034,D14-1162,0,0.0816581,"g is certainly one of the most challenging area of research of information retrieval (IR) and natural language processing (NLP) domains. If many investigations and countless approaches have been proposed so far, the developed systems still have difficulties to deal with text understanding. Mainly because of the complexity of the language in terms of lexical, semantic and pragmatic representations. However, with the boom of neural networks, various deep learning approaches ranging from a word level embedding representation (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) to a longer textual level embedding representation such as phrases, sentences, paragraphs or documents (Socher et al., 2011; Mikolov et al., 2013; Le and Mikolov, 2014; Kalchbrenner et al., 2014; Kiros et al., 2015; Wieting et al., 2016; Arora et al., 2017) have been proposed and have shown promising results in many applications. Not to mention other more sophisticated approaches like recurrent neural networks (RNN) (Socher et al., 2011, 2014; Kiros et al., 2015), long short-term memory (LSTM) to capture long distance dependency (Tai et al., 2015) or convolutional neural networks (CNNs) (Kalc"
L16-1496,D10-1115,0,0.0411602,"e the composed vector c is a weighted sum of the two input vectors: c = αu + βv (α and β being two scalars); the full additive model (fulladd) where the two vectors u and v are first multiplied by weight matrices and then added as follows: c = Au + Bv; the dilation model where one of the input vectors (u or v) is first decomposed into a vector parallel to the other and an orthogonal vector. Before recombining, the parallel vector is dilated by a factor λ giving the following result: c = (λ − 1)hu, vi + hu, viv. In addition, Lazaridou et al. (2013) applyed the lexical function model (lexfunc) (Baroni and Zamparelli, 2010) where the distributional representation of one element in a composition is not a vector but a function. They also used the DSM at the stem level as a baseline. Our approach is inspired by the work of Lazaridou et al. (2013) and uses the additive model (wadd) to combine several distributional modellings at the morpheme level. 3. origin, such as hydro + logy = hydrology. Neoclassical elements are not considered as lexical units because they never independently occur in the texts, that is they are always seen in the combined form with other elements (e.g., biology) (Amiot and Dal, 2008; Namer, 2"
L16-1496,C02-2020,0,0.351904,"sh compounds. We show promising results using distributional analysis at the root and affix levels. We also show that the adapted approach significantly improve bilingual lexicon extraction from comparable corpora compared to the approach at the word level. Keywords: Bilingual lexicon extraction, comparable corpora, morphemes, compound term, distributional analysis 1. Introduction Nowadays comparable corpora are widely used in many applications of natural language processing, particularly in bilingual terminology extraction where parallel corpora are a scarce resource (Rapp, 1995; Fung, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). In the task of bilingual lexicon extraction from comparable corpora, the acquisition of translational pairs is mainly based on the Harris’ distributional hypothesis (Harris, 1954) which states that words with similar meaning tend to occur in similar contexts (hypothesis that has been extended to the bilingual scenario). It is well-known that the efficiency of distributional methods heavily depends on the quality and the size of comparable corpora (Morin et al., 2007). If the quality of bilingual lexicons can always be improved by using more data, this is true onl"
L16-1496,I08-1013,1,0.803067,"ays be improved by using more data, this is true only if the training data is reasonably well-matched to the desired output (Morin and Hazem, 2014). In the case of specialised comparable corpora, the amount of data is limited and often small (Rapp, 1995; Morin et al., 2007). This presents a problem for distributional methods that fail to extract infrequent single-word terms (SWTs) as well as multiword terms (MWTs). In the case of MWTs, it has become a standard practice to apply the principle of compositionality on its parts to extract their corresponding translations (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). While in the case of SWTs, distributional methods often treat them as single tokens without considering their compositional property (Rapp, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). One can note that a reasonable amount of SWTs are compound terms consisting of a combination of two (or more) lexical elements to form a unit of meaning (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). To handle derivational morphology, Guevara (2010) and Lazaridou et al. (2013) identify the stem of rare derived words and use its derivational"
L16-1496,C12-1046,1,0.906043,"more data, this is true only if the training data is reasonably well-matched to the desired output (Morin and Hazem, 2014). In the case of specialised comparable corpora, the amount of data is limited and often small (Rapp, 1995; Morin et al., 2007). This presents a problem for distributional methods that fail to extract infrequent single-word terms (SWTs) as well as multiword terms (MWTs). In the case of MWTs, it has become a standard practice to apply the principle of compositionality on its parts to extract their corresponding translations (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). While in the case of SWTs, distributional methods often treat them as single tokens without considering their compositional property (Rapp, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). One can note that a reasonable amount of SWTs are compound terms consisting of a combination of two (or more) lexical elements to form a unit of meaning (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). To handle derivational morphology, Guevara (2010) and Lazaridou et al. (2013) identify the stem of rare derived words and use its derivational vector to derive the d"
L16-1496,2012.amta-papers.5,1,0.927083,"more data, this is true only if the training data is reasonably well-matched to the desired output (Morin and Hazem, 2014). In the case of specialised comparable corpora, the amount of data is limited and often small (Rapp, 1995; Morin et al., 2007). This presents a problem for distributional methods that fail to extract infrequent single-word terms (SWTs) as well as multiword terms (MWTs). In the case of MWTs, it has become a standard practice to apply the principle of compositionality on its parts to extract their corresponding translations (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). While in the case of SWTs, distributional methods often treat them as single tokens without considering their compositional property (Rapp, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). One can note that a reasonable amount of SWTs are compound terms consisting of a combination of two (or more) lexical elements to form a unit of meaning (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). To handle derivational morphology, Guevara (2010) and Lazaridou et al. (2013) identify the stem of rare derived words and use its derivational vector to derive the d"
L16-1496,J93-1003,0,0.208853,"addition to the lexeme and to the single term, we can add the context words of the single term that have been already observed in the corpus. Hence, all the words that appear in the context of abnormal will be added to the context vector of the prefix ab-. This approach is noted V ect(lem) for distributional approach using lexemes and lemmas and the context words of the lemmas. 3. Each lemma or lexeme of the context vector is weighted according to a given association measure such as the point-wise mutual information (Fano, 1961), the discounted odds ratio (Evert, 2005) or the log-likelihood (Dunning, 1993); 4. Each source context vector is translated into the target language using a bilingual dictionary; 5. A similarity measure such as the Cosine (Salton and Lesk, 1968) or the weighted Jaccard (Grefenstette, 1994) is applied between each translated source context vector and all the target vectors; 6. The translation candidates are ranked according to their similarity scores. The correct translation of the English prefix ab- is the French prefix a-. Knowing that (automatically thanks to our approach), we can derive from the single word abnormal that its French translation is anormal. This can be"
L16-1496,P13-1055,0,0.0581787,"Missing"
L16-1496,W95-0114,0,0.0706073,"ch and English compounds. We show promising results using distributional analysis at the root and affix levels. We also show that the adapted approach significantly improve bilingual lexicon extraction from comparable corpora compared to the approach at the word level. Keywords: Bilingual lexicon extraction, comparable corpora, morphemes, compound term, distributional analysis 1. Introduction Nowadays comparable corpora are widely used in many applications of natural language processing, particularly in bilingual terminology extraction where parallel corpora are a scarce resource (Rapp, 1995; Fung, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). In the task of bilingual lexicon extraction from comparable corpora, the acquisition of translational pairs is mainly based on the Harris’ distributional hypothesis (Harris, 1954) which states that words with similar meaning tend to occur in similar contexts (hypothesis that has been extended to the bilingual scenario). It is well-known that the efficiency of distributional methods heavily depends on the quality and the size of comparable corpora (Morin et al., 2007). If the quality of bilingual lexicons can always be improved by usin"
L16-1496,2007.mtsummit-papers.26,0,0.0784106,"Missing"
L16-1496,W10-2805,0,0.10104,"ty on its parts to extract their corresponding translations (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). While in the case of SWTs, distributional methods often treat them as single tokens without considering their compositional property (Rapp, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). One can note that a reasonable amount of SWTs are compound terms consisting of a combination of two (or more) lexical elements to form a unit of meaning (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). To handle derivational morphology, Guevara (2010) and Lazaridou et al. (2013) identify the stem of rare derived words and use its derivational vector to derive the distributional meaning of morphologically complex words from their parts. Delpech et al. (2012a) extract translations of morphologically constructed terms by exploiting a manually constructed translation list of equivalence at the morpheme-level. In this paper, we apply the compositional property at the morpheme level to automatically build a bilingual list of morphemes (roots and affixes), resource that is not always available and difficult to construct manually. We evaluate our"
L16-1496,E03-1076,0,0.0852286,"pounds (including hyphen-separated). The major kinds of compounds are native and neoclassical compounds. The first kind includes only native elements, which means not borrowed from another language, suh as parrot + fish = parrotfish. The second kind, neoclassical compounding, combines some elements of Greek or Latin etymological 3111 1. Each single term of the source and the target language is split into roots or affixes and lexemes. However, many splitting tools are available, either designed for one language such as DeriF (Namer 2003) for the French language, or language independent such as Koehn and Knight (2003) algorithm or COMPOST (Loginova Clouet and Daille, 2014). The single term abnormal for instance is split into the prefix ab- and the lexeme normal; 2. Each lexeme is added to the context vector of its corresponding affix or root according to the co-occurrence of the lexeme with the affix or the root. The lexeme normal for instance will be added to the context vector of the prefix ab- with the co-occurrence value of abwith normal. This corresponds to the occurrence of abnormal in the source corpus. This approach is noted lexem for distributional approach using lexemes. At this step four variant"
L16-1496,C10-1070,0,0.274466,"ng results using distributional analysis at the root and affix levels. We also show that the adapted approach significantly improve bilingual lexicon extraction from comparable corpora compared to the approach at the word level. Keywords: Bilingual lexicon extraction, comparable corpora, morphemes, compound term, distributional analysis 1. Introduction Nowadays comparable corpora are widely used in many applications of natural language processing, particularly in bilingual terminology extraction where parallel corpora are a scarce resource (Rapp, 1995; Fung, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). In the task of bilingual lexicon extraction from comparable corpora, the acquisition of translational pairs is mainly based on the Harris’ distributional hypothesis (Harris, 1954) which states that words with similar meaning tend to occur in similar contexts (hypothesis that has been extended to the bilingual scenario). It is well-known that the efficiency of distributional methods heavily depends on the quality and the size of comparable corpora (Morin et al., 2007). If the quality of bilingual lexicons can always be improved by using more data, this is true only if the training data is rea"
L16-1496,P13-1149,0,0.337426,"extract their corresponding translations (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). While in the case of SWTs, distributional methods often treat them as single tokens without considering their compositional property (Rapp, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). One can note that a reasonable amount of SWTs are compound terms consisting of a combination of two (or more) lexical elements to form a unit of meaning (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). To handle derivational morphology, Guevara (2010) and Lazaridou et al. (2013) identify the stem of rare derived words and use its derivational vector to derive the distributional meaning of morphologically complex words from their parts. Delpech et al. (2012a) extract translations of morphologically constructed terms by exploiting a manually constructed translation list of equivalence at the morpheme-level. In this paper, we apply the compositional property at the morpheme level to automatically build a bilingual list of morphemes (roots and affixes), resource that is not always available and difficult to construct manually. We evaluate our automatic bilingual morpheme"
L16-1496,W14-5702,1,0.908419,"Missing"
L16-1496,P11-1140,0,0.0241671,"cal compounds. Thus, the morphemes under consideration for the distributional analysis are neoclassical elements and prefixes, including elements that are not purely neoclassical elements but look like them, such as the element radio in radiology. 4. Bilingual morpheme extraction Our aim is to extract for each source morpheme (root or affix) its corresponding translation in a target language. To do so, we adapted the well-known distributional method to the morpheme level as follows: Various forms of compounds Compounding has different forms. First of all, we can talk about “closed compounds” (Macherey et al., 2011) written as single words (e.g, toolbar) in contrast to “open compounds”, which are space-separated but form a unit of meaning (e.g., operating system). We only deal with closed compounds (including hyphen-separated). The major kinds of compounds are native and neoclassical compounds. The first kind includes only native elements, which means not borrowed from another language, suh as parrot + fish = parrotfish. The second kind, neoclassical compounding, combines some elements of Greek or Latin etymological 3111 1. Each single term of the source and the target language is split into roots or aff"
L16-1496,P14-1121,1,0.897411,"n from comparable corpora, the acquisition of translational pairs is mainly based on the Harris’ distributional hypothesis (Harris, 1954) which states that words with similar meaning tend to occur in similar contexts (hypothesis that has been extended to the bilingual scenario). It is well-known that the efficiency of distributional methods heavily depends on the quality and the size of comparable corpora (Morin et al., 2007). If the quality of bilingual lexicons can always be improved by using more data, this is true only if the training data is reasonably well-matched to the desired output (Morin and Hazem, 2014). In the case of specialised comparable corpora, the amount of data is limited and often small (Rapp, 1995; Morin et al., 2007). This presents a problem for distributional methods that fail to extract infrequent single-word terms (SWTs) as well as multiword terms (MWTs). In the case of MWTs, it has become a standard practice to apply the principle of compositionality on its parts to extract their corresponding translations (Robitaille et al., 2006; Daille and Morin, 2008; Delpech et al., 2012b). While in the case of SWTs, distributional methods often treat them as single tokens without conside"
L16-1496,P07-1084,1,0.784604,"minology extraction where parallel corpora are a scarce resource (Rapp, 1995; Fung, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). In the task of bilingual lexicon extraction from comparable corpora, the acquisition of translational pairs is mainly based on the Harris’ distributional hypothesis (Harris, 1954) which states that words with similar meaning tend to occur in similar contexts (hypothesis that has been extended to the bilingual scenario). It is well-known that the efficiency of distributional methods heavily depends on the quality and the size of comparable corpora (Morin et al., 2007). If the quality of bilingual lexicons can always be improved by using more data, this is true only if the training data is reasonably well-matched to the desired output (Morin and Hazem, 2014). In the case of specialised comparable corpora, the amount of data is limited and often small (Rapp, 1995; Morin et al., 2007). This presents a problem for distributional methods that fail to extract infrequent single-word terms (SWTs) as well as multiword terms (MWTs). In the case of MWTs, it has become a standard practice to apply the principle of compositionality on its parts to extract their corresp"
L16-1496,P95-1050,0,0.445959,"bset of French and English compounds. We show promising results using distributional analysis at the root and affix levels. We also show that the adapted approach significantly improve bilingual lexicon extraction from comparable corpora compared to the approach at the word level. Keywords: Bilingual lexicon extraction, comparable corpora, morphemes, compound term, distributional analysis 1. Introduction Nowadays comparable corpora are widely used in many applications of natural language processing, particularly in bilingual terminology extraction where parallel corpora are a scarce resource (Rapp, 1995; Fung, 1995; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). In the task of bilingual lexicon extraction from comparable corpora, the acquisition of translational pairs is mainly based on the Harris’ distributional hypothesis (Harris, 1954) which states that words with similar meaning tend to occur in similar contexts (hypothesis that has been extended to the bilingual scenario). It is well-known that the efficiency of distributional methods heavily depends on the quality and the size of comparable corpora (Morin et al., 2007). If the quality of bilingual lexicons can always be impr"
L16-1496,P99-1067,0,0.166048,"ix levels and hope that this work can serve as a cornerstone for future work on this task involving more languages. We also confirm that the adapted approach significantly improves bilingual terminology extraction from comparable corpora compared to the baseline system. 2. Related work Distributional semantic models (DSM) have been successfully used in many natural language processing tasks (Guevara, 2010). Bilingual terminology extraction from comparable corpora for instance, is usually based on the bilingual distributional semantic models (BDSMs) when dealing with single word terms (SWT’s) (Rapp, 1999; Gamallo, 3110 2007; Laroche and Langlais, 2010; Morin and Hazem, 2014). To extract a SWT’s translation, a similarity measure is applied between the translated context vector of the source SWT and the context vectors of all the target SWTs. The candidates are ranked according to their similarity scores. One of the main problems that encounter distributional methods such as BDSMs is data sparseness. Taking into account the derivational morphology property of a SWT should resolve the latter problem. Compositional methods which have originally been developed for phrases have been successfully ap"
L16-1496,E06-1029,0,\N,Missing
L16-1661,C02-2020,0,0.0671227,"ikelihood (Dunning, 1993), the discounted odds-ratio (Laroche and Langlais, 2010)), the similarity between a source word’s context vector and all the context vectors in the target language is computed using a similarity measure (cosine (Salton and Lesk, 1968), Jaccard (Grefenstette, 1994)...). Finally, the translation candidates are ranked according to their similarity score. Many variants of the Standard Approach have been proposed. They can differ in context representation (window-based, syntactic-based) (Gamallo, 2008), corpus characteristics (small, large, general or domain spe´ cific...)(Chiao and Zweigenbaum, 2002; D´ejean and Eric Gaussier, 2002; Morin et al., 2007), type of words to translate (single word terms (SWTs) or multi-word terms (MWTs))(Rapp, 1999; Daille and Morin, 2005), words frequency (less frequent, rare...)(Pekar et al., 2006), etc. There exist other approaches for bilingual lexicon extraction. D´ejean et al. (2002) introduce the extended approach to avoid the insufficient coverage of the bilingual dictionary required for the translation of source context vectors. A variation of the latter method based on centroid is proposed by (Daille and Morin, 2005). Haghighi et al. (2008) employ d"
L16-1661,I05-1062,1,0.819286,"get language is computed using a similarity measure (cosine (Salton and Lesk, 1968), Jaccard (Grefenstette, 1994)...). Finally, the translation candidates are ranked according to their similarity score. Many variants of the Standard Approach have been proposed. They can differ in context representation (window-based, syntactic-based) (Gamallo, 2008), corpus characteristics (small, large, general or domain spe´ cific...)(Chiao and Zweigenbaum, 2002; D´ejean and Eric Gaussier, 2002; Morin et al., 2007), type of words to translate (single word terms (SWTs) or multi-word terms (MWTs))(Rapp, 1999; Daille and Morin, 2005), words frequency (less frequent, rare...)(Pekar et al., 2006), etc. There exist other approaches for bilingual lexicon extraction. D´ejean et al. (2002) introduce the extended approach to avoid the insufficient coverage of the bilingual dictionary required for the translation of source context vectors. A variation of the latter method based on centroid is proposed by (Daille and Morin, 2005). Haghighi et al. (2008) employ dimension reduction using canonical component analysis (CCA). The majority of the proposed approaches rely on context similarity. The starting point of context characterizat"
L16-1661,J93-1003,0,0.097408,"ngs tend to occur in similar contexts (Harris, 1954), has been extended to the bilingual scenario (Fung, 1998; Rapp, 1999). Hence, using comparable corpora, a translation of a source word can be found by identifying a target word with the most similar context. A popular method often used as a baseline is the Standard Approach (Fung, 1998). It consists of using the bag-of-words paradigm to represent words of source and target language by their context vector. After word contexts have been weighted using an association measure (the point-wise mutual information (Fano, 1961), the log-likelihood (Dunning, 1993), the discounted odds-ratio (Laroche and Langlais, 2010)), the similarity between a source word’s context vector and all the context vectors in the target language is computed using a similarity measure (cosine (Salton and Lesk, 1968), Jaccard (Grefenstette, 1994)...). Finally, the translation candidates are ranked according to their similarity score. Many variants of the Standard Approach have been proposed. They can differ in context representation (window-based, syntactic-based) (Gamallo, 2008), corpus characteristics (small, large, general or domain spe´ cific...)(Chiao and Zweigenbaum, 20"
L16-1661,C02-1166,0,0.141347,"Missing"
L16-1661,P04-1067,0,0.12927,"Missing"
L16-1661,P08-1088,0,0.0437467,"..)(Chiao and Zweigenbaum, 2002; D´ejean and Eric Gaussier, 2002; Morin et al., 2007), type of words to translate (single word terms (SWTs) or multi-word terms (MWTs))(Rapp, 1999; Daille and Morin, 2005), words frequency (less frequent, rare...)(Pekar et al., 2006), etc. There exist other approaches for bilingual lexicon extraction. D´ejean et al. (2002) introduce the extended approach to avoid the insufficient coverage of the bilingual dictionary required for the translation of source context vectors. A variation of the latter method based on centroid is proposed by (Daille and Morin, 2005). Haghighi et al. (2008) employ dimension reduction using canonical component analysis (CCA). The majority of the proposed approaches rely on context similarity. The starting point of context characterization is word co-occurrence statistics. It can provide a natural basis for semantic representation. Corpus-based word space models allow to go from distributional statistics to a geometric representation that induce the semantic representation of words from their patterns of co-occurrence in text. While literature suggest numerous techniques that could be used for that purpose, it is not obvious which is the best and"
L16-1661,C10-1070,0,0.145335,"is, 1954), has been extended to the bilingual scenario (Fung, 1998; Rapp, 1999). Hence, using comparable corpora, a translation of a source word can be found by identifying a target word with the most similar context. A popular method often used as a baseline is the Standard Approach (Fung, 1998). It consists of using the bag-of-words paradigm to represent words of source and target language by their context vector. After word contexts have been weighted using an association measure (the point-wise mutual information (Fano, 1961), the log-likelihood (Dunning, 1993), the discounted odds-ratio (Laroche and Langlais, 2010)), the similarity between a source word’s context vector and all the context vectors in the target language is computed using a similarity measure (cosine (Salton and Lesk, 1968), Jaccard (Grefenstette, 1994)...). Finally, the translation candidates are ranked according to their similarity score. Many variants of the Standard Approach have been proposed. They can differ in context representation (window-based, syntactic-based) (Gamallo, 2008), corpus characteristics (small, large, general or domain spe´ cific...)(Chiao and Zweigenbaum, 2002; D´ejean and Eric Gaussier, 2002; Morin et al., 2007)"
L16-1661,P07-1084,1,0.733431,"and Langlais, 2010)), the similarity between a source word’s context vector and all the context vectors in the target language is computed using a similarity measure (cosine (Salton and Lesk, 1968), Jaccard (Grefenstette, 1994)...). Finally, the translation candidates are ranked according to their similarity score. Many variants of the Standard Approach have been proposed. They can differ in context representation (window-based, syntactic-based) (Gamallo, 2008), corpus characteristics (small, large, general or domain spe´ cific...)(Chiao and Zweigenbaum, 2002; D´ejean and Eric Gaussier, 2002; Morin et al., 2007), type of words to translate (single word terms (SWTs) or multi-word terms (MWTs))(Rapp, 1999; Daille and Morin, 2005), words frequency (less frequent, rare...)(Pekar et al., 2006), etc. There exist other approaches for bilingual lexicon extraction. D´ejean et al. (2002) introduce the extended approach to avoid the insufficient coverage of the bilingual dictionary required for the translation of source context vectors. A variation of the latter method based on centroid is proposed by (Daille and Morin, 2005). Haghighi et al. (2008) employ dimension reduction using canonical component analysis"
L16-1661,2009.mtsummit-posters.14,1,0.702726,"f filtering, 321 French/English SWTs were extracted (from the UMLS4 meta-thesaurus.) for the breast cancer corpus, 150 pairs for the wind-energy corpus and 158 for the volcano corpus. 3.3. Evaluation Measure Three major parameters need to be set, namely the association measure, the similarity measure and the size of 2 www.elsevier.com ELRA dictionary has been done by Sciper in the Technolangue/Euradic project 4 http://www.nlm.nih.gov/research/umls 3 3.4. Baseline The baseline in our experiments is the standard approach (Fung, 1998) often used for comparison (Pekar et al., 2006; Gamallo, 2008; Prochasson and Morin, 2009), etc. 4. Experiments and Results We note that ’Top k’ means that the correct translation is present in the k first candidates of the list returned by a given method. We use also the mean average precision MAP (Manning and Schuze, 2008). 4.1. Experimental Setup The experiments have been carried out on three EnglishFrench comparable corpora. A specialized corpus of 1 million words from the medical domain within the sub-domain of ’breast cancer’2 , a specialized corpus from the domain of ’wind-energy’ of 600,000 words and a specialized corpus from the domain of geology within the sub-domain of V"
L16-1661,P99-1067,0,0.642571,"ce, we present in this paper a systematic exploration of the principal corpus-based word space models for bilingual terminology extraction from comparable corpora. We find that, once we have identified the best procedures, a very simple combination approach leads to significant improvements compared to individual models. Keywords: Comparable corpora, Bilingual lexicon extraction, word-space models 1. Introduction The distributional hypothesis which states that words with similar meanings tend to occur in similar contexts (Harris, 1954), has been extended to the bilingual scenario (Fung, 1998; Rapp, 1999). Hence, using comparable corpora, a translation of a source word can be found by identifying a target word with the most similar context. A popular method often used as a baseline is the Standard Approach (Fung, 1998). It consists of using the bag-of-words paradigm to represent words of source and target language by their context vector. After word contexts have been weighted using an association measure (the point-wise mutual information (Fano, 1961), the log-likelihood (Dunning, 1993), the discounted odds-ratio (Laroche and Langlais, 2010)), the similarity between a source word’s context ve"
L18-1045,I13-1150,0,0.253919,"MWTs that manage length variability. We evaluate our approach on two specialized domain corpora, a French/English corpus of the wind energy domain and a French/English corpus of the breast cancer domain and show superior results compared to baseline approaches. Keywords: Synonym extraction, Multi-word terms, Compositionality, Word embeddings 1. Introduction Synonyms acquisition has mainly concerned single word terms (SWTs) using a variety of approaches such as: lexicon-based approaches (Blondel and Senellart, 2002), multilingual approaches (Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Andrade et al., 2013), distributional approaches (Lin, 1998; Hagiwara, 2008), etc. However, exploring multi-word terms (MWTs) and their synonyms or semantically related terms can be useful in many applications such as: word sense disambiguation, machine translation, information retrieval, text simplification, etc. MWTs are motivated combinations that clearly convey the concept they designate. The requirement of term transparency argues in favor of compositional semantics for complex terms. Compositionality means that the whole meaning can be deduced from the meaning of its components and the syntactic rule by whic"
L18-1045,J93-1003,0,0.258676,"reafter the main steps of the distributional approach: • The context vector vwis of a given source word wis is first built. The vector vwis contains all the words that co-occur with wis within a window of n words that surround wis . Let us denote by occ(wis , wjs ) the co-occurrence count of wis and a given word of its context wjs . • The process of building context vectors is repeated for all words of the specialized corpus. • Words of the context vectors are weighted using association measures such as the point-wise mutual information (noted MI) (Fano, 1961), the log-likelihood (noted LLR) (Dunning, 1993) or the discounted odds-ratio (noted LO) (Laroche and Langlais, 2010). These measures aim at strengthening the correlation between a word and all the words of its context vector. Semi-Compositional Approach Like the compositional approach, the semi-compositional variant is based on the principle of compositionality of MWTs. The main difference lies on the nature of the substituted elements of the MWT. It is no longer constrained by the sole relation of synonymy like in Hamon and Nazarenko (2001). Hazem and Daille (2014) generalized the substitution on MWT elements to semantically related terms"
L18-1045,P08-3001,0,0.433597,"on two specialized domain corpora, a French/English corpus of the wind energy domain and a French/English corpus of the breast cancer domain and show superior results compared to baseline approaches. Keywords: Synonym extraction, Multi-word terms, Compositionality, Word embeddings 1. Introduction Synonyms acquisition has mainly concerned single word terms (SWTs) using a variety of approaches such as: lexicon-based approaches (Blondel and Senellart, 2002), multilingual approaches (Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Andrade et al., 2013), distributional approaches (Lin, 1998; Hagiwara, 2008), etc. However, exploring multi-word terms (MWTs) and their synonyms or semantically related terms can be useful in many applications such as: word sense disambiguation, machine translation, information retrieval, text simplification, etc. MWTs are motivated combinations that clearly convey the concept they designate. The requirement of term transparency argues in favor of compositional semantics for complex terms. Compositionality means that the whole meaning can be deduced from the meaning of its components and the syntactic rule by which they are combined (Partee et al., 1990). Pirrelli et"
L18-1045,daille-hazem-2014-semi,1,0.832183,"h such as: wind turbine/wind machine1 ; MWT synonyms of variable length such as: wind farm/wind power plant; to non compositional MWT synonyms such as: pole tower/mast. Few works addressed the acquisition of MWT synonyms. The main approaches that have been proposed in the experimental literature deal with the acquisition of synonyms of MWTs that are compositional and often of the same length. Synonym extraction approaches implement the principle of compositionality by substituting parts of the MWT by synonyms provided by a dictionary (Hamon and Nazarenko, 2001), or by distributional analysis (Hazem and Daille, 2014). It has been recently shown that words, phrases, sentences, paragraphs and more generally, pieces of texts of any length can be efficiently represented by word embeddings using operations on vectors and matrices like addition or multipli1 In the renewable energy domain. cation (Mitchell and Lapata, 2010; Mikolov et al., 2013b; Socher et al., 2011; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014; Kiros et al., 2015; Wieting et al., 2016; Arora et al., 2017; Hazem et al., 2017). For phrase representation, Mikolov et al. (2013b) have shown for instance that the embedding v"
L18-1045,hazem-etal-2017-mappsent,1,0.924887,"nonyms provided by a dictionary (Hamon and Nazarenko, 2001), or by distributional analysis (Hazem and Daille, 2014). It has been recently shown that words, phrases, sentences, paragraphs and more generally, pieces of texts of any length can be efficiently represented by word embeddings using operations on vectors and matrices like addition or multipli1 In the renewable energy domain. cation (Mitchell and Lapata, 2010; Mikolov et al., 2013b; Socher et al., 2011; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014; Kiros et al., 2015; Wieting et al., 2016; Arora et al., 2017; Hazem et al., 2017). For phrase representation, Mikolov et al. (2013b) have shown for instance that the embedding vector of the phrase Volga river is similar to the addition of the embedding vector of Volga and the embedding vector of river. The addition property that word embbeding models exhibit offers key information for representing phrases and by extension MWTs and there synonyms or quasi-synonyms. Drawing inspiration from these findings and based on the principle of compositionality and distributed approaches, we propose several techniques based on word embedding models to deal with synonyms acquisition of"
L18-1045,P14-1062,0,0.00832632,"oaches implement the principle of compositionality by substituting parts of the MWT by synonyms provided by a dictionary (Hamon and Nazarenko, 2001), or by distributional analysis (Hazem and Daille, 2014). It has been recently shown that words, phrases, sentences, paragraphs and more generally, pieces of texts of any length can be efficiently represented by word embeddings using operations on vectors and matrices like addition or multipli1 In the renewable energy domain. cation (Mitchell and Lapata, 2010; Mikolov et al., 2013b; Socher et al., 2011; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014; Kiros et al., 2015; Wieting et al., 2016; Arora et al., 2017; Hazem et al., 2017). For phrase representation, Mikolov et al. (2013b) have shown for instance that the embedding vector of the phrase Volga river is similar to the addition of the embedding vector of Volga and the embedding vector of river. The addition property that word embbeding models exhibit offers key information for representing phrases and by extension MWTs and there synonyms or quasi-synonyms. Drawing inspiration from these findings and based on the principle of compositionality and distributed approaches, we propose sev"
L18-1045,E14-1057,0,0.0603223,"Missing"
L18-1045,C10-1070,0,0.162668,"The context vector vwis of a given source word wis is first built. The vector vwis contains all the words that co-occur with wis within a window of n words that surround wis . Let us denote by occ(wis , wjs ) the co-occurrence count of wis and a given word of its context wjs . • The process of building context vectors is repeated for all words of the specialized corpus. • Words of the context vectors are weighted using association measures such as the point-wise mutual information (noted MI) (Fano, 1961), the log-likelihood (noted LLR) (Dunning, 1993) or the discounted odds-ratio (noted LO) (Laroche and Langlais, 2010). These measures aim at strengthening the correlation between a word and all the words of its context vector. Semi-Compositional Approach Like the compositional approach, the semi-compositional variant is based on the principle of compositionality of MWTs. The main difference lies on the nature of the substituted elements of the MWT. It is no longer constrained by the sole relation of synonymy like in Hamon and Nazarenko (2001). Hazem and Daille (2014) generalized the substitution on MWT elements to semantically related terms of any type. They extended the compositional rules R1 and R2 by repl"
L18-1045,P98-2127,0,0.248239,"ur approach on two specialized domain corpora, a French/English corpus of the wind energy domain and a French/English corpus of the breast cancer domain and show superior results compared to baseline approaches. Keywords: Synonym extraction, Multi-word terms, Compositionality, Word embeddings 1. Introduction Synonyms acquisition has mainly concerned single word terms (SWTs) using a variety of approaches such as: lexicon-based approaches (Blondel and Senellart, 2002), multilingual approaches (Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Andrade et al., 2013), distributional approaches (Lin, 1998; Hagiwara, 2008), etc. However, exploring multi-word terms (MWTs) and their synonyms or semantically related terms can be useful in many applications such as: word sense disambiguation, machine translation, information retrieval, text simplification, etc. MWTs are motivated combinations that clearly convey the concept they designate. The requirement of term transparency argues in favor of compositional semantics for complex terms. Compositionality means that the whole meaning can be deduced from the meaning of its components and the syntactic rule by which they are combined (Partee et al., 19"
L18-1045,P07-1084,1,0.692211,"amon and Nazarenko (2001). To extract French synonyms of single-word terms we used the on-line dictionary DES 7 . DES contains 49,168 entries and 201,511 synonym relations. The initial database has been constructed from seven dictionaries. The extraction of English synonyms has been conducted using the lexical database WordNet 8 . WordNet contains approximately 117,000 synsets. The main relation among words in WordNet is synonymy. 4.2. Distributional Method Settings Using the distributional method, three main parameters need to be set: the size of the window used to build the context vectors (Morin et al., 2007; Gamallo, 2008), the association measure (the log-likelihood (Dunning, 1993), the point-wise mutual information (Fano, 1961), the discounted odds-ratio (Laroche and Langlais, 2010),...) and the similarity measure (the weighted Jaccard index 7 http://www.crisco.unicaen.fr/des/ synonyms 8 http://wordnetweb.princeton.edu/perl/ webwn/ 300 Method Hamon&Nazarenko Mikolov Semi-Comp (MI-COS) Semi-Comp (LO-COS) Semi-Comp (LLR-JAC) Semi-Comp (SG50) Semi-Comp (SG100) Semi-Comp (SG300) Semi-Comp (CBOW50) Semi-Comp (CBOW100) Semi-Comp (CBOW300) Full-Comp (SG100) Full-Comp (SG200) Full-Comp (SG300) Full-Co"
L18-1045,P06-2111,0,0.295935,"Missing"
L18-1045,W03-1610,0,0.48785,"pproach for the automatic acquisition of synonyms of MWTs that manage length variability. We evaluate our approach on two specialized domain corpora, a French/English corpus of the wind energy domain and a French/English corpus of the breast cancer domain and show superior results compared to baseline approaches. Keywords: Synonym extraction, Multi-word terms, Compositionality, Word embeddings 1. Introduction Synonyms acquisition has mainly concerned single word terms (SWTs) using a variety of approaches such as: lexicon-based approaches (Blondel and Senellart, 2002), multilingual approaches (Wu and Zhou, 2003; van der Plas and Tiedemann, 2006; Andrade et al., 2013), distributional approaches (Lin, 1998; Hagiwara, 2008), etc. However, exploring multi-word terms (MWTs) and their synonyms or semantically related terms can be useful in many applications such as: word sense disambiguation, machine translation, information retrieval, text simplification, etc. MWTs are motivated combinations that clearly convey the concept they designate. The requirement of term transparency argues in favor of compositional semantics for complex terms. Compositionality means that the whole meaning can be deduced from the"
L18-1330,H05-2006,0,0.137866,"Missing"
L18-1330,P06-4018,0,0.255891,"Missing"
L18-1432,D16-1250,0,0.0315166,"more generally, pieces of texts of any length. A prior condition is to have a training dataset of pairs of similar sentences. The main idea is: given a set of similar sentences, the goal is to build a more discriminant and representative sentence embedding space. Word embeddings of the entire corpus are first computed, then, each sentence is represented by an element-wise addition of its word embedding vectors. Finally, a mapping matrix is built using the Singular Values Decomposition (SVD) to project sentences in a new subspace. Similar sentences are moved closer thanks to a mapping matrix (Artetxe et al., 2016) learned from a training dataset containing pairs of similar sentences. Basically, a set of similar sentence pairs is used as seed information to build the mapping matrix. The optimal mapping is computed by minimizing the Euclidean distance between the seed sentence pairs. MappSent approach consists of the following steps: 1. We train a Skip-gram 11 model using Gensim toolkit12 on a lemmatized training dataset. 2. Each training and test sentence is pre-processed. We remove stop-words and only keep nouns, verbs and adjectives while computing sentence embedding vectors and the mapping matrix. Th"
L18-1432,S16-1138,0,0.0585033,"Missing"
L18-1432,S16-1172,0,0.0508242,"Missing"
L18-1432,S16-1126,0,0.0217945,"question. If several users contribute to a given post, it is important to automatically extract the correct answers among tens or hundreds of answers since a manual exploration becomes hard to achieve. These tasks offer a key challenge while they have to deal with textual similarity not only in terms of lexical similarity but also in terms of reformulation, paraphrasing, duplicates and near duplicates, textual entailment, semantics, etc. Over the past years, there have been several studies on community question answering (Qiu and Huang, 2015; Filice et al., 2016; Barr´on-Cede˜no et al., 2016; Franco-Salvador et al., 2016; Nakov et al., 2016; Nakov et al., 2017; Patra, 2017), etc. Most of them addressed this task through specific datasets such as the programming Q&A website Stackoverflow 1 , Quora dataset for duplicate extraction 2 , Yahoo!Answers dataset (Qiu and Huang, 2015), Qatar living corpus via SemEval shared task 3 , etc. Also, in most of the evaluations, the candidates of a given question are often limited in number (around 10 per question). For in1 https://stackoverflow.com/ https://data.quora.com/ First-Quora-Dataset-Release-Question-Pairs 3 http://alt.qcri.org/semeval2017/task3/ 2 stance, in the Qa"
L18-1432,hazem-etal-2017-mappsent,1,0.921457,"aluation workflow since the candidates are not limited in number but concerns the entire set of the forum questions. Hence, based on StackExchange datasets, we provide 19 corpora of several domains ranging from politics, economics, history, philosophy to music, sport, travel, cooking, etc. We evaluate two baselines on the question-to-question and question-answering similarity tasks. The first baseline is a sentence similarity approach based on word embeddings (SentEmb) and the second approach (M appSent) is a textual similarity approach based on a mapping matrix. Recently, M appSent approach (Hazem et al., 2017) obtained better results than the winner system of 2016 and 2017 SemEval sharedtask editions on the question-to-question similarity task. By providing a large coverage of datasets and a more realistic evaluation procedure, we hope that this work serves as a cornerstone for future evaluations on question-to-question and question-answering similarity tasks. On the short term, we also aim at enriching the framework with the entire StackExchange datasets which consists of about 180 corpora. The remainder of this paper is organized as follows. Section 2. describes the different linguistic resources"
L18-1432,N13-1090,0,0.314032,"r is a systematic evaluation of two textual similarity-based approaches (SentEmb and MappSent) on the 19 datasets for question-to-question and question-answering similarity tasks. 3. Baseline Approaches In this Section we describe the two implemented baselines that is: (i) the sentence embedding approach (SentEmb) and (ii) the mapping approach (MappSent). 3.1. SentEmb The sentence embedding approach consists of representing each question (original or related) and each answer by an embedding vector. The embedding vector is the sum of the vector embedding of each word of the question or answer (Mikolov et al., 2013b; Wieting et al., 2016; Arora et al., 2017; Hazem et al., 2017). Then, to extract similar pairs of questions or pairs of questions/answers, the cosine similarity is computed. The related questions (in the question-to-question similarity task) and the answers (in the question-answering similarity task) are ranked according to their scores regarding the original questions. It is to note that each sentence (question or answer) is preprocessed10 . We also remove stop-words and only keep nouns, verbs and adjectives. We also conducted experiments without the POS-TAG and stop-words filtering process"
L18-1432,S16-1083,0,0.0491675,"Missing"
L18-1432,S17-2003,0,0.0691816,"Missing"
N15-1103,D11-1033,0,0.0329288,"ng the translators to post-edit translations of a Legal document from English into French (about 15k words) over five days (i.e. about 3k words/day). An in-domain adapted (DA) system was used as baseline system for the first day, before project adapted (PA) systems have taken over. 4.1 Domain adapted system Before the human translator starts working, our DA system is trained using an extracted subset of bilingual training data that is mostly relevant to our specific domain. The extraction process, widely known as data selection, is applied using cross-entropy difference algorithm proposed by (Axelrod et al., 2011)2 . In order to augment the amount of training data3 (about 22M words) we also select a bilingual subset from Europarl, JRC-Acquis, news commentary, software manuals of the OPUS corpus, translation memories and the United Nations corpus. About 700M additional newspaper monolingual data selected from WMT evaluation campaign are also used for language modeling. 4.2 Project adapted system Our project-adaptation scenario, which is repeated iteratively during the lifetime of the translation 2 3 We used the XenC tool for data selection DGT+ECB corpora (see http://opus.lingfil.uu.se) project, is achi"
N15-1103,P07-2045,0,0.00512288,"of data is translated every day by each translator. The systems are then adapted, individually for each translator, using previous days of work, and used by the translators during the next day, and so on. 3 4 Evaluation Protocol We defined an adaptation protocol with the goal to assess the same task with and without adaptation procedure. Like in (Guerberof, 2009; Plitt and Masselot, 2010), three professional translators were involved in a two parts experiment: during the first part, translators receive MT suggestions from a state-of-the-art domain-adapted engine built with the Moses toolkit (Koehn et al., 2007), without being adapted with the data generated during the translation of the project.For the second part, the MT suggestions are provided by a MT system which was previously adapted to the current project using the human translations of prior working days. Since we asked the same translators to post-edit the same document twice (i.e. with and without MT adaptation), the second run was launched after a sufficient delay: the human memory impact is reduced since translators worked on other projects in between. To measure the user productivity, we considered two performance indicators: (i) the po"
N15-1103,P02-1040,0,0.0934212,"th and without MT adaptation), the second run was launched after a sufficient delay: the human memory impact is reduced since translators worked on other projects in between. To measure the user productivity, we considered two performance indicators: (i) the post-editing effort measured with TER (Snover et al., 2006) which corresponds to the number of edits made individually by each translator, (ii) the time-to-edit rate expressed in number of translated words per hour. In addition to these two key indicators, we evaluated the translation quality using an automatic measure, namely BLEU score (Papineni et al., 2002). This measure is used to make sure that no regression in the translation quality is observed after several days 1002 Experimental framework We ran contrastive experiments by asking the translators to post-edit translations of a Legal document from English into French (about 15k words) over five days (i.e. about 3k words/day). An in-domain adapted (DA) system was used as baseline system for the first day, before project adapted (PA) systems have taken over. 4.1 Domain adapted system Before the human translator starts working, our DA system is trained using an extracted subset of bilingual trai"
N15-1103,2006.amta-papers.25,0,0.060854,"generated during the translation of the project.For the second part, the MT suggestions are provided by a MT system which was previously adapted to the current project using the human translations of prior working days. Since we asked the same translators to post-edit the same document twice (i.e. with and without MT adaptation), the second run was launched after a sufficient delay: the human memory impact is reduced since translators worked on other projects in between. To measure the user productivity, we considered two performance indicators: (i) the post-editing effort measured with TER (Snover et al., 2006) which corresponds to the number of edits made individually by each translator, (ii) the time-to-edit rate expressed in number of translated words per hour. In addition to these two key indicators, we evaluated the translation quality using an automatic measure, namely BLEU score (Papineni et al., 2002). This measure is used to make sure that no regression in the translation quality is observed after several days 1002 Experimental framework We ran contrastive experiments by asking the translators to post-edit translations of a Legal document from English into French (about 15k words) over five"
N15-1103,W09-0441,0,0.0199861,"mber of edits performed by the translator in order to obtain a suitable translation. The first column indicates the day of the experiment. The second column represents three SMT systems, namely: the baseline system adapted to the domain (DA), the same system with a CSLM (DA+CSLM) and the project adapted system (all models were updated, including the CSLM) noted “PA+CSLM-adapt”. The third, fourth and fifth columns represent respectively the TER scores for the three translators. The first score is calculated using the reference produced by the translator himself. It could be considered as HTER (Snover et al., 2009). The second score (in parenthesis) is calculated using the three references produced by the translators. The third score (in brackets) is calculated according to an official “generic” reference provided by the European Commission. By these additional results, we aim to assess whether their is a tendency of the systems to adapt strongly to the particular style of one translator, or whether they still perform well with respect to independent references. On day 1, only the DA and DA+CSLM systems are presented since the project adaptation can only start after the first working day. First of all,"
N15-1103,2009.mtsummit-btm.7,0,\N,Missing
N15-1103,W15-4006,1,\N,Missing
P14-1121,P13-2133,0,0.733507,"complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010). The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; 1285 References Tanaka and Iwasaki (1996) Fung and McKeown (1997) Rapp (1999) Chiao and Zweigenbaum (2002) D´ejean et al. (2002) Morin et al. (2007) Otero (2007) Ismail and Manandhar (2010) Bouamor et al. (2013) - Domain Newspaper Newspaper Newspaper Medical Medical Medical European Parliament European Parliament Financial Medical Languages EN/JP EN/JP GE/EN FR/EN GE/EN FR/JP SP/EN EN/SP FR/EN FR/EN Source/Target Sizes 30/33 million words 49/60 million bytes of data 135/163 million words 602,484/608,320 words 100,000/100,000 words 693,666/807,287 words 14/17 million words 500,000/500,000 sentences 402,486/756,840 words 396,524/524,805 words Table 2: Characteristics of the comparable corpora used for bilingual lexicon extraction Bouamor et al., 2013, among others) with the implicit hypothesis that com"
P14-1121,C02-2020,0,0.846181,"Missing"
P14-1121,C02-1166,0,0.696471,"Missing"
P14-1121,C04-1151,0,0.0832384,"Missing"
P14-1121,W97-0119,0,0.16547,"Missing"
P14-1121,W95-0114,0,0.598568,"carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction through different experiments. Moreover, we have introduced a regression model that boosts the observations of word cooccurrences used in the context-based projection method. Our results show that the use of unbalanced specialized comparable corpora induces a significant gain in the quality of extracted lexicons. 1 The bilingual lexicon extraction task from comparable corpora inherits this filiation. For instance, the historical context-based projection method (Fung, 1995; Rapp, 1995), known as the standard approach, dedicated to this task seems implicitly to lead to work with balanced comparable corpora in the same way as for parallel corpora (i.e. each part of the corpus is composed of the same amount of data). Introduction The bilingual lexicon extraction task from bilingual corpora was initially addressed by using parallel corpora (i.e. a corpus that contains source texts and their translation). However, despite good results in the compilation of bilingual lexicons, parallel corpora are scarce resources, especially for technical domains and for language pa"
P14-1121,P04-1067,0,0.463295,"Missing"
P14-1121,I13-1196,1,0.836171,"easure (for instance, this can be done by dividing each entry of a given context vector by the sum of its association scores). 2.2 Prediction Model Since comparable corpora are usually small in specialized domains (see Table 2), the discrimina1 We only found mention of this aspect in Diab and Finch (2000, p. 1501) “In principle, we do not have to have the same size corpora in order for the approach to work”. tive power of context vectors (i.e. the observations of word co-occurrences) is reduced. One way to deal with this problem is to re-estimate co-occurrence counts by a prediction function (Hazem and Morin, 2013). This consists in assigning to each observed co-occurrence count of a small comparable corpora, a new value learned beforehand from a large training corpus. In order to make co-occurrence counts more discriminant and in the same way as Hazem and Morin (2013), one strategy consists in addressing this problem through regression: given training corpora of small and large size (abundant in the general domain), we predict word cooccurrence counts in order to make them more reliable. We then apply the resulting regression function to each word co-occurrence count as a pre-processing step of the sta"
P14-1121,C10-2055,0,0.301609,"imilarity measures. The most complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010). The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; 1285 References Tanaka and Iwasaki (1996) Fung and McKeown (1997) Rapp (1999) Chiao and Zweigenbaum (2002) D´ejean et al. (2002) Morin et al. (2007) Otero (2007) Ismail and Manandhar (2010) Bouamor et al. (2013) - Domain Newspaper Newspaper Newspaper Medical Medical Medical European Parliament European Parliament Financial Medical Languages EN/JP EN/JP GE/EN FR/EN GE/EN FR/JP SP/EN EN/SP FR/EN FR/EN Source/Target Sizes 30/33 million words 49/60 million bytes of data 135/163 million words 602,484/608,320 words 100,000/100,000 words 693,666/807,287 words 14/17 million words 500,000/500,000 sentences 402,486/756,840 words 396,524/524,805 words Table 2: Characteristics of the comparable corpora used for bilingual lexicon extraction Bouamor et al., 2013, among others) with the implic"
P14-1121,P11-1133,0,0.208858,"k t t t assoc√ ∑ 2 l k2 t assoct t assoct Cosinevvkl = √∑ (1) (2) This approach is sensitive to the choice of parameters such as the size of the context, the choice of the association and similarity measures. The most complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010). The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; 1285 References Tanaka and Iwasaki (1996) Fung and McKeown (1997) Rapp (1999) Chiao and Zweigenbaum (2002) D´ejean et al. (2002) Morin et al. (2007) Otero (2007) Ismail and Manandhar (2010) Bouamor et al. (2013) - Domain Newspaper Newspaper Newspaper Medical Medical Medical European Parliament European Parliament Financial Medical Languages EN/JP EN/JP GE/EN FR/EN GE/EN FR/JP SP/EN EN/SP FR/EN FR/EN Source/Target Sizes 30/33 million words 49/60 million bytes of data 135/163 million words 602,484/608,320 words 100,000/100,000 words 693,666/807,287 words 14/17 million words 500,000/500,000 sen"
P14-1121,W02-0902,0,0.0465966,"that a word and its candidate translations share thematic similarities. Yu and Tsujii (2009) and Otero (2007) propose, for their part, to replace the window-based method by a syntax-based method in order to improve the representation of the lexical context. To improve the transfer context vectors step, and increase the number of elements of translated context vectors, Chiao and Zweigenbaum (2003) and Morin and Prochasson (2011) combine a standard general language dictionary with a specialized dictionary, whereas D´ejean et al. (2002) use the hierarchical properties of a specialized thesaurus. Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts. As regards the problem of words ambiguities, Bouamor et al. (2013) carried out word sense disambiguation process only in the target language whereas Gaussier et al. (2004) solve the problem through the source and target languages by using approaches based on CCA (Canonical Correlation Analysis) and multilingual PLSA (Probabilistic Latent Semantic Analysis). The rank of candidate translations can be improved by integrating different heuristics. For instance, Ch"
P14-1121,2009.mtsummit-posters.14,1,0.891893,"y ∈ R using one of the regression models presented below: yˆLin = β0 + β1 x yˆLogit = 1 1 + exp(−(β0 + β1 x)) yˆP olyn = β0 + β1 x + β2 x2 + ... + βn xn (3) (4) (5) where βi are the parameters to estimate. Let us denote by f the regression function and by cooc(wi , wj ) the co-occurrence count of the words wi and wj . The resulting predicted value of cooc(wi , wj ), noted cooc(w ˆ i , wj ) is given by the following equation: cooc(w ˆ i , wj ) = f (cooc(wi , wj )) (6) 2.3 Related Work In the past few years, several contributions have been proposed to improve each step of the standard approach. Prochasson et al. (2009) enhance the representativeness of the context vector by strengthening the context words that happen to be transliterated words and scientific compound words in the target language. Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (indomain terms), and thus propose a method for filtering the noise of the context vectors. In another way, Rubino and Linar`es (2011) improve the context words based on the hypothesis that a word and its candidate translations share thematic similarities. Yu and Tsujii (2009) and Otero (2"
P14-1121,C10-1070,0,0.650654,"rd window approximates syntactic dependencies). In order to emphasize significant words in the context vector and to reduce word-frequency effects, the context vectors are normalized according to an association measure. Then, the translation is obtained by comparing the source context vector to each translation candidate vector after having translated each element of the source vector with a general dictionary. The implementation of the standard approach can be carried out by applying the following three steps (Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007; Laroche and Langlais, 2010, among others): Computing context vectors We collect all the words in the context of each word i and count their occurrence frequency in a window of n words around i. For each word i of the source and the target languages, we obtain a context vector vi which gathers the set of co-occurrence words j associated with the number of times that j and i occur together cooc(i, j). In order to identify specific words in the lexical context and to reduce wordfrequency effects, we normalize context vectors using an association score such as Mutual Information, Log-likelihood, or the discounted log-odds"
P14-1121,P95-1050,0,0.448913,"a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction through different experiments. Moreover, we have introduced a regression model that boosts the observations of word cooccurrences used in the context-based projection method. Our results show that the use of unbalanced specialized comparable corpora induces a significant gain in the quality of extracted lexicons. 1 The bilingual lexicon extraction task from comparable corpora inherits this filiation. For instance, the historical context-based projection method (Fung, 1995; Rapp, 1995), known as the standard approach, dedicated to this task seems implicitly to lead to work with balanced comparable corpora in the same way as for parallel corpora (i.e. each part of the corpus is composed of the same amount of data). Introduction The bilingual lexicon extraction task from bilingual corpora was initially addressed by using parallel corpora (i.e. a corpus that contains source texts and their translation). However, despite good results in the compilation of bilingual lexicons, parallel corpora are scarce resources, especially for technical domains and for language pairs not invol"
P14-1121,C10-1073,0,0.148459,"inguistic preprocessing steps: tokenisation, part-of-speech tagging, and lemmatisation. These steps were carried out using the TTC TermSuite4 that applies the same method to several languages including French and English. Finally, the function words were removed and the words occurring less than twice in the French part and in each English part were discarded. Table 3 shows the number of distinct words (# words) after these steps. It also indicates the comparability degree in percentage (comp.) between the French part and each English part of each comparable corpus. The comparability measure (Li and Gaussier, 2010) is based on the expectation of finding the translation for each word in the corpus and gives a good idea about how two corpora are comparable. We can notice that all the comparable corpora have a high degree of comparability with a better comparability of the breast cancer corpora as opposed to the diabetes corpora. In the remainder of this article, [breast cancer corpus i] for instance stands for the breast cancer comparable corpus composed of the unique French part and the English part i (i ∈ [1, 14]). 3.2 Bilingual Dictionary The bilingual dictionary used in our experiments is the French/E"
P14-1121,P99-1067,0,0.959406,"a word which occurs within the window of the word to be translated (e.g. a seven-word window approximates syntactic dependencies). In order to emphasize significant words in the context vector and to reduce word-frequency effects, the context vectors are normalized according to an association measure. Then, the translation is obtained by comparing the source context vector to each translation candidate vector after having translated each element of the source vector with a general dictionary. The implementation of the standard approach can be carried out by applying the following three steps (Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007; Laroche and Langlais, 2010, among others): Computing context vectors We collect all the words in the context of each word i and count their occurrence frequency in a window of n words around i. For each word i of the source and the target languages, we obtain a context vector vi which gathers the set of co-occurrence words j associated with the number of times that j and i occur together cooc(i, j). In order to identify specific words in the lexical context and to reduce wordfrequency effects, we normalize context vectors"
P14-1121,C96-2098,0,0.212954,"n the compilation of bilingual lexicons, parallel corpora are scarce resources, especially for technical domains and for language pairs not involving English. For these reasons, research in bilingual lexicon extraction has focused on another kind of bilingual corpora comprised of texts sharing common features such as domain, genre, sampling period, etc. without having a source text/target text relationship (McEnery and Xiao, 2007). These corpora, well known now as comparable corpora, have also initially been introduced as non-parallel corpora (Fung, 1995; Rapp, 1995), and non-aligned corpora (Tanaka and Iwasaki, 1996). According to Fung and CheIn this paper we want to show that the assumption that comparable corpora should be balanced for bilingual lexicon extraction task is unfounded. Moreover, this assumption is prejudicial for specialized comparable corpora, especially when involving the English language for which many documents are available due the prevailing position of this language as a standard for international scientific publications. Within this context, our main contribution consists in a re-reading of the standard approach putting emphasis on the unfounded assumption of the balance of the spe"
P14-1121,W11-1205,1,0.889297,"vant words (indomain terms), and thus propose a method for filtering the noise of the context vectors. In another way, Rubino and Linar`es (2011) improve the context words based on the hypothesis that a word and its candidate translations share thematic similarities. Yu and Tsujii (2009) and Otero (2007) propose, for their part, to replace the window-based method by a syntax-based method in order to improve the representation of the lexical context. To improve the transfer context vectors step, and increase the number of elements of translated context vectors, Chiao and Zweigenbaum (2003) and Morin and Prochasson (2011) combine a standard general language dictionary with a specialized dictionary, whereas D´ejean et al. (2002) use the hierarchical properties of a specialized thesaurus. Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts. As regards the problem of words ambiguities, Bouamor et al. (2013) carried out word sense disambiguation process only in the target language whereas Gaussier et al. (2004) solve the problem through the source and target languages by using approaches based on CCA (Canonica"
P14-1121,N09-2031,0,0.0141912,"pproach. Prochasson et al. (2009) enhance the representativeness of the context vector by strengthening the context words that happen to be transliterated words and scientific compound words in the target language. Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (indomain terms), and thus propose a method for filtering the noise of the context vectors. In another way, Rubino and Linar`es (2011) improve the context words based on the hypothesis that a word and its candidate translations share thematic similarities. Yu and Tsujii (2009) and Otero (2007) propose, for their part, to replace the window-based method by a syntax-based method in order to improve the representation of the lexical context. To improve the transfer context vectors step, and increase the number of elements of translated context vectors, Chiao and Zweigenbaum (2003) and Morin and Prochasson (2011) combine a standard general language dictionary with a specialized dictionary, whereas D´ejean et al. (2002) use the hierarchical properties of a specialized thesaurus. Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using"
P14-1121,P07-1084,1,0.964614,"ted (e.g. a seven-word window approximates syntactic dependencies). In order to emphasize significant words in the context vector and to reduce word-frequency effects, the context vectors are normalized according to an association measure. Then, the translation is obtained by comparing the source context vector to each translation candidate vector after having translated each element of the source vector with a general dictionary. The implementation of the standard approach can be carried out by applying the following three steps (Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007; Laroche and Langlais, 2010, among others): Computing context vectors We collect all the words in the context of each word i and count their occurrence frequency in a window of n words around i. For each word i of the source and the target languages, we obtain a context vector vi which gathers the set of co-occurrence words j associated with the number of times that j and i occur together cooc(i, j). In order to identify specific words in the lexical context and to reduce wordfrequency effects, we normalize context vectors using an association score such as Mutual Information, Log-likelihood,"
P14-1121,2007.mtsummit-papers.26,0,0.926132,"ciation and similarity measures. The most complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010). The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; 1285 References Tanaka and Iwasaki (1996) Fung and McKeown (1997) Rapp (1999) Chiao and Zweigenbaum (2002) D´ejean et al. (2002) Morin et al. (2007) Otero (2007) Ismail and Manandhar (2010) Bouamor et al. (2013) - Domain Newspaper Newspaper Newspaper Medical Medical Medical European Parliament European Parliament Financial Medical Languages EN/JP EN/JP GE/EN FR/EN GE/EN FR/JP SP/EN EN/SP FR/EN FR/EN Source/Target Sizes 30/33 million words 49/60 million bytes of data 135/163 million words 602,484/608,320 words 100,000/100,000 words 693,666/807,287 words 14/17 million words 500,000/500,000 sentences 402,486/756,840 words 396,524/524,805 words Table 2: Characteristics of the comparable corpora used for bilingual lexicon extraction Bouamor et al., 2013, a"
R19-1054,D14-1162,0,0.104354,". Disruptions are a set of extreme embedding values that are more likely to be noise than reliable features. We consider this phenomenon as a negative side effect closely re460 Proceedings of Recent Advances in Natural Language Processing, pages 460–464, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_054 tence embeddings using StackExchange Philosophy data set over several in-domain 1 and pretrained embedding models on question to question similarity task and show significant improvements. The used pre-trained models are skipgram (Sg) (Mikolov et al., 2013), Glove (Pennington et al., 2014), dependency relation (Deps) (Levy and Goldberg, 2014), and the character Skipgram (ChSg) and character CBOW (ChC)2 (Bojanowski et al., 2016). 2 tecture and the training procedure of word embeddings (Nematzadeh et al., 2017). CBOW model for instance, predicts the target word based on a mean average weights of its context words. While, skip-gram maximizes the average log-probability of each word’s context (Mikolov et al., 2013). Also, the process of weights computation is often based on batches which leads to a mean error minimization instead of a specific attention to each single training exam"
R19-1054,P14-2050,0,0.0386275,"at are more likely to be noise than reliable features. We consider this phenomenon as a negative side effect closely re460 Proceedings of Recent Advances in Natural Language Processing, pages 460–464, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_054 tence embeddings using StackExchange Philosophy data set over several in-domain 1 and pretrained embedding models on question to question similarity task and show significant improvements. The used pre-trained models are skipgram (Sg) (Mikolov et al., 2013), Glove (Pennington et al., 2014), dependency relation (Deps) (Levy and Goldberg, 2014), and the character Skipgram (ChSg) and character CBOW (ChC)2 (Bojanowski et al., 2016). 2 tecture and the training procedure of word embeddings (Nematzadeh et al., 2017). CBOW model for instance, predicts the target word based on a mean average weights of its context words. While, skip-gram maximizes the average log-probability of each word’s context (Mikolov et al., 2013). Also, the process of weights computation is often based on batches which leads to a mean error minimization instead of a specific attention to each single training example. This optimization process might lead to some comp"
R19-1054,N18-1049,0,0.0277087,"t than a standard normalization. We show that dealing with disruptions is of a substantial benefit to bottom-up sentence embedding representation. A bottom-up sentence representation is a weighted sum of the embedding vectors of its constituent words. This simple approach turned out to be very competitive in many NLP applications (Wieting et al., 2016; Arora et al., 2017) and outperformed several advanced RNNs and LSTM-based models of which the performance heavily depends on the quality and the large size of the training data set (Socher et al., 2011; Le and Mikolov, 2014; Kiros et al., 2015; Pagliardini et al., 2018). By contrast to sophisticated approaches, bottom-up sentence embedding models are less constrained and more easy to acquire. The core of the bottom-up model is the word embedding unit. An efficient sentence representation is then closely related to the quality of the used word embedding model. We state that the additive process of bottom-up sentence representation amplifies disruption’s negative impact and propose to manage this phenomenon by introducing two tweaking techniques. Our approaches take into account and reduce the effect of disruptions in order to improve bottom-up sentence embedd"
R19-1055,C04-1051,0,0.322448,"Missing"
R19-1055,W15-1513,0,0.0145083,"l. (2013a) and Wieting et al. (2016), their approach is based on word embedding sum, but the difference is remarkable on the weighted schema and on the use of principal component analysis (PCA) method to remove the correlation of sentence vectors dimensions. They significantly achieved better performance than the unweighted average on a variety of textual sim3.1 Ensemble Approach The principle of the ensemble approach is to combine different models in order to catch the strength of each individual model. The main combination techniques that have shown their effectiveness are: vector addition (Garten et al., 2015) and vector concatenation (Garten et al., 2015; Yin and Sch¨utze, 2016). For vector addition, given two embedding models, the procedure consists of applying a simple dimension-wise vector addition1 . For vector concatenation, given two embedding models of dimensions dim1 and dim2, the resulting concatenated embedding vector will be of size dim1 + dim2. The vectors have to be normalized before concatenation. Usually L2 norm is performed2 . Yin and Sch¨utze (2016) performed a weighted concatenation of 5 embedding models. They also experienced the SVD on top of weighted concatenation vectors of d"
R19-1055,I17-1069,1,0.836738,"dding sentence representation (MetaSentEmb). In the next sections we first recall the principle of ensemble approach from which we drawn our inspiration, then we give the details of our approach. Another type of sentence embedding representation, also called bottom-up approach, represents sentences by a weighted sum of the embedding vectors of their individual words. This naive approach turned out to be competitive and outperformed sophisticated approaches based on RNNs and LSTMs in many natural language processing applications (Mikolov et al., 2013a; Wieting et al., 2016; Arora et al., 2017; Hazem and Morin, 2017). Mikolov et al. (2013a) for instance demonstrated the effectiveness of their model on the phrase analogy task. They used the hierarchical softmax and subsampling using large amount of data. Wieting et al. (2016) have shown that a simple but supervised word averaging model of sentence embeddings leads to better performance on the paraphrase pairs data set (PPDB). However, the performance of their approach is closely related to the supervision from the date set, while without supervision, their approach did not perform well on textual similarity tasks. More recently, Arora et al. (2017) propose"
R19-1055,N16-1162,0,0.0231626,"data to map dictionary definitions of words with their pre-trained embeddings. With the encouraging results and simplicity of bottomup approaches, we focus in this paper on this type of approaches and show their potential while used jointly with meta-embeddings. ory (LSTM) and Convolutional Neural Networks (CNN). Some sentence embedding representations can be seen as a direct inspiration from word embedding models. For instance, while the skipgram model (Mikolov et al., 2013a) predicts the surrounding words given a source word, in the same way, SkipThought (Kiros et al., 2015a) and FastSent (Hill et al., 2016) models predict surrounding sentences given a source sentence. Also, the paragraph DBOW model (Le and Mikolov, 2014) learns representations for variable length pieces of texts and learns to predict the surrounding words based on contexts sampled from paragraphs. Recently, Pagliardini et al. (2018) introduced Sent2Vec, an approach based on word vectors along with n-gram embeddings simultaneously to represent sentences. 3 Sentence Meta-Embedding Representation To deal with textual similarity, we propose a new approach that we refer to as meta-embedding sentence representation (MetaSentEmb). In t"
R19-1055,P14-1062,0,0.0122372,"on at the sentence level for textual similarity. Significant improvements are observed in several tasks including question-to-question similarity, paraphrasing and next utterance ranking. 1 2 Introduction Related Work Embedding models at the word level representations have been widely explored in many applications (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2016). Naturally, they have been extended to sentence, paragraph and document level representations (Socher et al., 2011; Mikolov et al., 2013a; Le and Mikolov, 2014; Kalchbrenner et al., 2014; Kiros et al., 2015b; Wieting et al., 2016; Arora et al., 2017), thanks to the continuous advances of deep neural embedding methods such as Recurrent Neural Networks (RNN), Long Short Term MemAccording to Enkvist (1987): ”a model is a simplified representation of reality. It is simplified because it aims at reproducing a selection of relevant elements of reality rather than all of reality at once.”. If several word embedding models (Mikolov et al., 2013a; Pennington et al., 2014; Yin and Sch¨utze, 2016; Arora et al., 2017) capture a selection of relevant features, different embedding sets can"
R19-1055,N18-5000,0,0.200226,"Missing"
R19-1055,D14-1162,0,0.10274,"trasting in-domain and pre-trained embedding models, we show under which conditions they can be jointly used for bottom-up sentence embeddings. Finally, we propose the first bottom-up meta-embedding representation at the sentence level for textual similarity. Significant improvements are observed in several tasks including question-to-question similarity, paraphrasing and next utterance ranking. 1 2 Introduction Related Work Embedding models at the word level representations have been widely explored in many applications (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2016). Naturally, they have been extended to sentence, paragraph and document level representations (Socher et al., 2011; Mikolov et al., 2013a; Le and Mikolov, 2014; Kalchbrenner et al., 2014; Kiros et al., 2015b; Wieting et al., 2016; Arora et al., 2017), thanks to the continuous advances of deep neural embedding methods such as Recurrent Neural Networks (RNN), Long Short Term MemAccording to Enkvist (1987): ”a model is a simplified representation of reality. It is simplified because it aims at reproducing a selection of relevant elements of reality rather than all of re"
R19-1055,P14-2050,0,0.254099,"on and (iv) the Ubuntu Dialogue Corpus used for Next Utterance Ranking (NUR). 2013b; Wieting et al., 2016; Arora et al., 2017). This representation is illustrated in the following equation: Senti = n X (Embedding(wj )) 4.1 (1) To study the impact of external data and context representation, we chose different embedding models. In addition to the word2vec model trained on Google News (Mikolov et al., 2013a), we used the two Glove models respectively trained on Wikipedia+GigaWord (Glove6B) and on Common Crawl (Glove42B) (Pennington et al., 2014). We also used three Wikipedia pre-trained models (Levy and Goldberg, 2014), that is, two linear bag of word contexts and one dependencybased context. The bag of word models use a context size of 5 (Bow5C corresponds to CBow and Bow5W to skipgram). The dependency-based model used syntactic relations (Deps). Finally, we experienced the recent proposed character ngram model (Bojanowski et al., 2016) by using the character Skip-gram model trained on Wikipedia (ChSG). A summary of the pre-trained outof-domain embedding sets is presented in Table 1. We also trained embedding models (CBOW, Skipgram, Glove and character n-gram models) on in-domain data sets (Qatar Living an"
R19-1055,P16-1128,0,0.0330683,"Missing"
R19-1055,W15-4640,0,0.0191986,"nces are entailed or not. In any case and for the sake of comparison, we perform the same evaluation as the state of the art approaches by keeping the three classes (Neutral, Entailment or Contradiction) instead of two classes (Entailment or Not Entailment). As this work is mainly dedicated to the evaluation of sentence representations in sentence similarity, we only focus on the entailment part and don’t consider the relatedness (we only report the results of the accuracy). 4.2.3 4.2.4 Ubuntu Dialogue Corpus The Ubuntu Dialogue Corpus is a large freely available multi-turn dialogue data set (Lowe et al., 2015) constructed from the Ubuntu chat logs4 . The corpus (Human-Human chat) consists of approximately 930,000 two person dialogues, 7,100,000 utterances5 and 100,000,000 words. The task of NUR consists of retrieving the most probable utterance among a database of existing human productions given a similar context. This task offers a key challenge for sentence similarity approaches since the relations between dialogue utterances are more generic. Here also, evaluating sentence similarity based approaches on a different task, should give some insights about their behaviour and to what extent it migh"
R19-1055,N13-1090,0,0.667853,"representation. By contrasting in-domain and pre-trained embedding models, we show under which conditions they can be jointly used for bottom-up sentence embeddings. Finally, we propose the first bottom-up meta-embedding representation at the sentence level for textual similarity. Significant improvements are observed in several tasks including question-to-question similarity, paraphrasing and next utterance ranking. 1 2 Introduction Related Work Embedding models at the word level representations have been widely explored in many applications (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2016). Naturally, they have been extended to sentence, paragraph and document level representations (Socher et al., 2011; Mikolov et al., 2013a; Le and Mikolov, 2014; Kalchbrenner et al., 2014; Kiros et al., 2015b; Wieting et al., 2016; Arora et al., 2017), thanks to the continuous advances of deep neural embedding methods such as Recurrent Neural Networks (RNN), Long Short Term MemAccording to Enkvist (1987): ”a model is a simplified representation of reality. It is simplified because it aims at reproducing a selection of relevant elements of rea"
R19-1055,S17-2003,0,0.0740737,"Missing"
R19-1055,S16-1083,0,0.0702566,"Missing"
R19-1055,N18-1049,0,0.0236836,"l Neural Networks (CNN). Some sentence embedding representations can be seen as a direct inspiration from word embedding models. For instance, while the skipgram model (Mikolov et al., 2013a) predicts the surrounding words given a source word, in the same way, SkipThought (Kiros et al., 2015a) and FastSent (Hill et al., 2016) models predict surrounding sentences given a source sentence. Also, the paragraph DBOW model (Le and Mikolov, 2014) learns representations for variable length pieces of texts and learns to predict the surrounding words based on contexts sampled from paragraphs. Recently, Pagliardini et al. (2018) introduced Sent2Vec, an approach based on word vectors along with n-gram embeddings simultaneously to represent sentences. 3 Sentence Meta-Embedding Representation To deal with textual similarity, we propose a new approach that we refer to as meta-embedding sentence representation (MetaSentEmb). In the next sections we first recall the principle of ensemble approach from which we drawn our inspiration, then we give the details of our approach. Another type of sentence embedding representation, also called bottom-up approach, represents sentences by a weighted sum of the embedding vectors of t"
W11-1206,C02-1011,0,0.057467,"Missing"
W11-1206,C02-2020,0,0.272078,"Missing"
W11-1206,I05-1062,1,0.959184,"r affinities show which words share the same environments. Words sharing second-order affinities need never appear together themselves, but their environments are similar (Grefenstette, 1994a, p. 280). Generally speaking, a bilingual dictionary is a bridge between two languages established by its entries. The extended approach is based on this observation and avoids explicit translation of vectors as shown in Figure 1. The implementation of this extended approach can be carried out in four steps where the first and last steps are identical to the standard approach (D´ejean and Gaussier, 2002; Daille and Morin, 2005): Reformulation in the target language For a lexical unit i to be translated, we identify the k-nearest lexical units (k nlu), among the dictionary entries corresponding to words in the source language, according to sim(i, s). Each nlu is translated via the bilingual dictionary, and the vector in 37 the target language, s, corresponding to the translation is selected. If the bilingual dictionary provides several translations for a given unit, s is given by the union of the vectors corresponding to the translations. It is worth noting that the context vectors are not translated directly, thus r"
W11-1206,C02-1166,0,0.231507,"Missing"
W11-1206,J93-1003,0,0.467787,"l dependencies). The implementation of this approach can be carried out by applying the following four steps (Rapp, 1995; Fung and McKeown, 1997): Context characterization All the lexical units in the context of each lexical unit i are collected, and their frequency in a window of n words around i extracted. For each lexical unit i of the source and the target languages, we obtain a context vector i where each entry, ij , of the vector is given by a function of the co-occurrences of units j and i. Usually, association measures such as the mutual information (Fano, 1961) or the log-likelihood (Dunning, 1993) are used to define vector entries. Vector transfer The lexical units of the context vector i are translated using a bilingual dictionary. Whenever the bilingual dictionary provides several translations for a lexical unit, all the entries are considered but weighted according to their frequency in the target language. Lexical units with no entry in the dictionary are discarded. Target language vector matching A similarity measure, sim(i, t), is used to score each lexical unit, t, in the target language with respect to the translated context vector, i. Usual measures of vector similarity includ"
W11-1206,P98-1069,0,0.804521,"ask. We then reinterpret the extended method, and motivate a novel model to reformulate this approach inspired by the metasearch engines in information retrieval. The empirical results show that performances of our model are always better than the baseline obtained with the extended approach and also competitive with the standard approach. 1 ˜ Saldarriaga Sebastian Pena 1100 rue Notre-Dame Ouest, Montr´eal, Qu´ebec, Canada H3C 1K3 spena@synchromedia.ca Introduction Bilingual lexicon extraction from comparable corpora has received considerable attention since the 1990s (Rapp, 1995; Fung, 1998; Fung and Lo, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002a; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010, among others). This attention has been motivated by the scarcity of parallel corpora, especially for countries with only one official language and for language pairs not involving English. Furthermore, as a parallel corpus is comprised of a pair of texts (a source text and a translated text), the vocabulary appearing in the translated text is highly influenced by the source text, especially in technical domains. Consequently, comparab"
W11-1206,W97-0119,0,0.123041,"of this observation consists in the identification of first-order affinities for each source and target language: First-order affinities de36 scribe what other words are likely to be found in the immediate vicinity of a given word (Grefenstette, 1994a, p. 279). These affinities can be represented by context vectors, and each vector element represents a word which occurs within the window of the word to be translated (for instance a seven-word window approximates syntactical dependencies). The implementation of this approach can be carried out by applying the following four steps (Rapp, 1995; Fung and McKeown, 1997): Context characterization All the lexical units in the context of each lexical unit i are collected, and their frequency in a window of n words around i extracted. For each lexical unit i of the source and the target languages, we obtain a context vector i where each entry, ij , of the vector is given by a function of the co-occurrences of units j and i. Usually, association measures such as the mutual information (Fano, 1961) or the log-likelihood (Dunning, 1993) are used to define vector entries. Vector transfer The lexical units of the context vector i are translated using a bilingual dict"
W11-1206,P04-1067,0,0.216071,"Missing"
W11-1206,C10-1070,0,0.183241,"Zweigenbaum, 2002a), and 100 SWTs in (Daille and Morin, 2005)). To build our reference list, we selected 400 French/English SWTs from the UMLS2 meta-thesaurus and the Grand dictionnaire terminologique3 . We kept only the French/English pair of SWTs which occur more than five times in each part of the comparable corpus. As a result of filtering, 122 French/English SWTs were extracted. 4.2 Three major parameters need to be set to the extended approach, namely the similarity measure, the association measure defining the entry vectors and the size of the window used to build the context vectors. Laroche and Langlais (2010) carried out a complete study about the influence of these parameters on the quality of bilingual alignment. As similarity measure, we chose to use the weighted jaccard index: P min (it , jt ) sim(i, j) = P t (5) t max (it , jt ) The entries of the context vectors were determined by the log-likelihood (Dunning, 1993), and we used a seven-word window since it approximates syntactic dependencies. Other combinations of parameters were assessed but the previous parameters turned out to give the best performance. 2 3 www.elsevier.com 39 Experimental Setup http://www.nlm.nih.gov/research/umls http:/"
W11-1206,P07-1084,1,0.90184,"Missing"
W11-1206,P95-1050,0,0.766389,"ally dedicated to this task. We then reinterpret the extended method, and motivate a novel model to reformulate this approach inspired by the metasearch engines in information retrieval. The empirical results show that performances of our model are always better than the baseline obtained with the extended approach and also competitive with the standard approach. 1 ˜ Saldarriaga Sebastian Pena 1100 rue Notre-Dame Ouest, Montr´eal, Qu´ebec, Canada H3C 1K3 spena@synchromedia.ca Introduction Bilingual lexicon extraction from comparable corpora has received considerable attention since the 1990s (Rapp, 1995; Fung, 1998; Fung and Lo, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002a; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010, among others). This attention has been motivated by the scarcity of parallel corpora, especially for countries with only one official language and for language pairs not involving English. Furthermore, as a parallel corpus is comprised of a pair of texts (a source text and a translated text), the vocabulary appearing in the translated text is highly influenced by the source text, especially in technical"
W11-1206,P99-1067,0,0.634338,"Missing"
W11-1206,C98-1066,0,\N,Missing
W12-1107,P04-3031,0,0.0172939,"Missing"
W12-1107,J06-4003,0,0.0702651,"Missing"
W12-1107,N03-1033,0,0.0801798,"Missing"
W13-2504,P91-1034,0,0.5039,"ss-based models provide an alternative to the independence assumption on the cooccurrence of given words w1 and w2 : the more frequent w2 is, the higher estimate of P (w2 |w1 ) will be, regardless of w1 . Introduction Cooccurrences play an important role in many corpus based approaches in the field of naturallanguage processing (Dagan et al., 1993). They represent the observable evidence that can be distilled from a corpus and are employed for a variety of applications such as machine translation (Brown et al., 1992), information retrieval (Maarek and Smadja, 1989), word sense disambiguation (Brown et al., 1991), etc. In bilingual lexicon extraction from comparable corpora, frequency counts for word pairs often serve as a basis for distributional methods, such as the standard approach (Fung, 1998) which compares the cooccurrence profile of a given source word, a 24 Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 24–33, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics method are then described in Section 3. Section 4 describes the experimental setup and our resources. Section 5 presents the experiments and comments on several results. We"
W13-2504,J92-4003,0,0.42182,"Missing"
W13-2504,C02-2020,0,0.165679,"steps 2, 3 and 4 of the standard approach are unchanged). We chose not to study the prediction of unseen cooccurrences. The latter has been carried out successfully by (Pekar et al., 2006). We concentrate on the evaluation of smoothing techniques of known cooccurrences and their effect according to different association and similarity measures. The main idea for identifying translations of terms in comparable corpora relies on the distributional hypothesis 1 that has been extended to the bilingual scenario (Fung, 1998; Rapp, 1999). If many variants of the standard approach have been proposed (Chiao and Zweigenbaum, 2002; Herv´e D´ejean and Gaussier, 2002; Morin et al., 2007; Gamallo, 2008)[among others], they mainly differ in the way they implement each step and define its parameters. The standard approach can be carried out as follows: Step 1 For a source word to translate wis , we first build its context vector vwis . The vector vwis contains all the words that cooccur with wis within windows of n words. Lets denote by cooc(wis , wjs ) the cooccurrence value of wis and a given word of its context wjs . The process of building context vectors is repeated for all the words of the target language. Step 2 An a"
W13-2504,P93-1022,0,0.146892,"Goodman, 1999). Class-based models (Pereira et al., 1993) use classes of similar words to distinguish between unseen cooccurrences. The relationship between given words is modeled by analogy with other words that are in some sense similar to the given ones. Hence, class-based models provide an alternative to the independence assumption on the cooccurrence of given words w1 and w2 : the more frequent w2 is, the higher estimate of P (w2 |w1 ) will be, regardless of w1 . Introduction Cooccurrences play an important role in many corpus based approaches in the field of naturallanguage processing (Dagan et al., 1993). They represent the observable evidence that can be distilled from a corpus and are employed for a variety of applications such as machine translation (Brown et al., 1992), information retrieval (Maarek and Smadja, 1989), word sense disambiguation (Brown et al., 1991), etc. In bilingual lexicon extraction from comparable corpora, frequency counts for word pairs often serve as a basis for distributional methods, such as the standard approach (Fung, 1998) which compares the cooccurrence profile of a given source word, a 24 Proceedings of the 6th Workshop on Building and Using Comparable Corpora"
W13-2504,I05-1062,1,0.809718,"the ith term (always 1 in our case), and P (Rik ) is 0 if the reference translation is not found for the ith term or 1/r if it is (r is the rank of the reference translation in the translation candidates). Table 2: Dictionary coverage 4.3 Evaluation Measure Reference Lists In bilingual terminology extraction from specialized comparable corpora, the terminology reference list required to evaluate the performance of the alignment programs is often composed of 100 single-word terms (SWTs) (180 SWTs in (Herv´e D´ejean and Gaussier, 2002), 95 SWTs in (Chiao and Zweigenbaum, 2002), and 100 SWTs in (Daille and Morin, 2005)). To build our reference lists, we selected only the French/English pair of SWTs which occur more than five times in each part of the comparable corpus. As a result 4.5 Baseline The baseline in our experiments is the standard approach (Fung, 1998) without any smoothing of the data. The standard approach is often used for comparison (Pekar et al., 2006; Gamallo, 2008; Prochasson and Morin, 2009), etc. 4.6 Training Data Set Some smoothing techniques such as the GoodTuring estimators need a large training corpus to 3 ELRA dictionary has been created by Sciper in the Technolangue/Euradic project"
W13-2504,J93-1003,0,0.563583,"inly differ in the way they implement each step and define its parameters. The standard approach can be carried out as follows: Step 1 For a source word to translate wis , we first build its context vector vwis . The vector vwis contains all the words that cooccur with wis within windows of n words. Lets denote by cooc(wis , wjs ) the cooccurrence value of wis and a given word of its context wjs . The process of building context vectors is repeated for all the words of the target language. Step 2 An association measure such as the pointwise mutual information (Fano, 1961), the log-likelihood (Dunning, 1993) or the discounted odds-ratio (Laroche and Langlais, 2010) is used to score the strength of correlation between a word and all the words of its context vector. 4 Experimental Setup In order to evaluate the smoothing techniques, several resources and parameters are needed. We present hereafter the experiment data and the parameters of the standard approach. 4.1 Step 3 The context vector vwis is projected into t . Each word w s of v s the target language vw s wi j i is translated with the help of a bilingual dictionary D. If wjs is not present in D, wjs is discarded. Whenever the bilingual dicti"
W13-2504,C10-1070,0,0.064413,"ep and define its parameters. The standard approach can be carried out as follows: Step 1 For a source word to translate wis , we first build its context vector vwis . The vector vwis contains all the words that cooccur with wis within windows of n words. Lets denote by cooc(wis , wjs ) the cooccurrence value of wis and a given word of its context wjs . The process of building context vectors is repeated for all the words of the target language. Step 2 An association measure such as the pointwise mutual information (Fano, 1961), the log-likelihood (Dunning, 1993) or the discounted odds-ratio (Laroche and Langlais, 2010) is used to score the strength of correlation between a word and all the words of its context vector. 4 Experimental Setup In order to evaluate the smoothing techniques, several resources and parameters are needed. We present hereafter the experiment data and the parameters of the standard approach. 4.1 Step 3 The context vector vwis is projected into t . Each word w s of v s the target language vw s wi j i is translated with the help of a bilingual dictionary D. If wjs is not present in D, wjs is discarded. Whenever the bilingual dictionary provides several translations for a word, all the en"
W13-2504,P07-2008,0,0.179104,"resent a study of some widelyused smoothing algorithms for language n-gram modeling (Laplace, Good-Turing, Kneser-Ney...). Our main contribution is to investigate how the different smoothing techniques affect the performance of the standard approach (Fung, 1998) traditionally used for bilingual lexicon extraction. We show that using smoothing as a preprocessing step of the standard approach increases its performance significantly. 1 As has been known, words and other type-rich linguistic populations do not contain instances of all types in the population, even the largest samples (Zipf, 1949; Evert and Baroni, 2007). Therefore, the number and distribution of types in the available sample are not reliable estimators (Evert and Baroni, 2007), especially for small comparable corpora. The literature suggests two major approaches for solving the data sparseness problem: smoothing and class-based methods. Smoothing techniques (Good, 1953) are often used to better estimate probabilities when there is insufficient data to estimate probabilities accurately. They tend to make distributions more uniform, by adjusting low probabilities such as zero probabilities upward, and high probabilities downward. Generally, sm"
W13-2504,P04-1066,0,0.0457126,"Missing"
W13-2504,P07-1084,1,0.917396,"se not to study the prediction of unseen cooccurrences. The latter has been carried out successfully by (Pekar et al., 2006). We concentrate on the evaluation of smoothing techniques of known cooccurrences and their effect according to different association and similarity measures. The main idea for identifying translations of terms in comparable corpora relies on the distributional hypothesis 1 that has been extended to the bilingual scenario (Fung, 1998; Rapp, 1999). If many variants of the standard approach have been proposed (Chiao and Zweigenbaum, 2002; Herv´e D´ejean and Gaussier, 2002; Morin et al., 2007; Gamallo, 2008)[among others], they mainly differ in the way they implement each step and define its parameters. The standard approach can be carried out as follows: Step 1 For a source word to translate wis , we first build its context vector vwis . The vector vwis contains all the words that cooccur with wis within windows of n words. Lets denote by cooc(wis , wjs ) the cooccurrence value of wis and a given word of its context wjs . The process of building context vectors is repeated for all the words of the target language. Step 2 An association measure such as the pointwise mutual informa"
W13-2504,P93-1024,0,0.193297,"The literature suggests two major approaches for solving the data sparseness problem: smoothing and class-based methods. Smoothing techniques (Good, 1953) are often used to better estimate probabilities when there is insufficient data to estimate probabilities accurately. They tend to make distributions more uniform, by adjusting low probabilities such as zero probabilities upward, and high probabilities downward. Generally, smoothing methods not only prevent zero probabilities, but they also attempt to improve the accuracy of the model as a whole (Chen and Goodman, 1999). Class-based models (Pereira et al., 1993) use classes of similar words to distinguish between unseen cooccurrences. The relationship between given words is modeled by analogy with other words that are in some sense similar to the given ones. Hence, class-based models provide an alternative to the independence assumption on the cooccurrence of given words w1 and w2 : the more frequent w2 is, the higher estimate of P (w2 |w1 ) will be, regardless of w1 . Introduction Cooccurrences play an important role in many corpus based approaches in the field of naturallanguage processing (Dagan et al., 1993). They represent the observable evidenc"
W13-2504,2009.mtsummit-posters.14,1,0.839373,"Missing"
W13-2504,P99-1067,0,0.148079,"sed for calculating the association measure between wis and wjs and so on (steps 2, 3 and 4 of the standard approach are unchanged). We chose not to study the prediction of unseen cooccurrences. The latter has been carried out successfully by (Pekar et al., 2006). We concentrate on the evaluation of smoothing techniques of known cooccurrences and their effect according to different association and similarity measures. The main idea for identifying translations of terms in comparable corpora relies on the distributional hypothesis 1 that has been extended to the bilingual scenario (Fung, 1998; Rapp, 1999). If many variants of the standard approach have been proposed (Chiao and Zweigenbaum, 2002; Herv´e D´ejean and Gaussier, 2002; Morin et al., 2007; Gamallo, 2008)[among others], they mainly differ in the way they implement each step and define its parameters. The standard approach can be carried out as follows: Step 1 For a source word to translate wis , we first build its context vector vwis . The vector vwis contains all the words that cooccur with wis within windows of n words. Lets denote by cooc(wis , wjs ) the cooccurrence value of wis and a given word of its context wjs . The process of"
W15-3413,E09-1003,0,0.0617287,"arallel corpora, that is, collections of documents that are mutual translations, are used in many natural language processing applications, particularly for statistical machine translation. Building such resources is however exceedingly expensive, requiring highly skilled annotators or professional translators (Preiss, 2012). Comparable corpora, that are sets of texts in two or more languages without being translations of each other, are often considered as a solution for the lack of parallel corpora, and many techniques have been proposed to extract parallel sentences (Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010), or mine word translations (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Vuli´c and Moens, 2012). Identifying comparable resources in a large amount of multilingual data remains a very challenging task. The purpose of the Building and Using Comparable Corpora (BUCC) 2015 shared task1 is to provide the first evaluation of existing approaches for identifying comparable resources. More precisely, given a large collection of Wikipedia pages in several languages, the task is to identify the most similar pages across languages. 1 Proposed Method webs"
W15-3413,C02-2020,0,0.0184779,"ral language processing applications, particularly for statistical machine translation. Building such resources is however exceedingly expensive, requiring highly skilled annotators or professional translators (Preiss, 2012). Comparable corpora, that are sets of texts in two or more languages without being translations of each other, are often considered as a solution for the lack of parallel corpora, and many techniques have been proposed to extract parallel sentences (Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010), or mine word translations (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Vuli´c and Moens, 2012). Identifying comparable resources in a large amount of multilingual data remains a very challenging task. The purpose of the Building and Using Comparable Corpora (BUCC) 2015 shared task1 is to provide the first evaluation of existing approaches for identifying comparable resources. More precisely, given a large collection of Wikipedia pages in several languages, the task is to identify the most similar pages across languages. 1 Proposed Method website major gaston links flutist marcel debost states sources college crunelle conservatoire principal"
W15-3413,N07-2008,0,0.198412,"{elizaveta.loginova,florian.boudin,emmanuel.morin}@univ-nantes.fr 2 Abstract In this paper, we describe the system that we developed for the BUCC 2015 shared track and show that a language agnostic approach can achieve promising results. This paper describes the LINA system for the BUCC 2015 shared track. Following (Enright and Kondrak, 2007), our system identify comparable documents by collecting counts of hapax words. We extend this method by filtering out document pairs sharing target documents using pigeonhole reasoning and cross-lingual information. 1 2 The method we propose is based on (Enright and Kondrak, 2007)’s approach to parallel document identification. Documents are treated as bags of words, in which only blank separated strings that are at least four characters long and that appear only once in the document (hapax words) are indexed. Given a document in language A, the document in language B that share the largest number of these words is considered as parallel. Although very simple, this approach was shown to perform very well in detecting parallel documents in Wikipedia (Patry and Langlais, 2011). The reason for this is that most hapax words are in practice proper nouns or numerical entitie"
W15-3413,W95-0114,0,0.101621,"s, are used in many natural language processing applications, particularly for statistical machine translation. Building such resources is however exceedingly expensive, requiring highly skilled annotators or professional translators (Preiss, 2012). Comparable corpora, that are sets of texts in two or more languages without being translations of each other, are often considered as a solution for the lack of parallel corpora, and many techniques have been proposed to extract parallel sentences (Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010), or mine word translations (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Vuli´c and Moens, 2012). Identifying comparable resources in a large amount of multilingual data remains a very challenging task. The purpose of the Building and Using Comparable Corpora (BUCC) 2015 shared task1 is to provide the first evaluation of existing approaches for identifying comparable resources. More precisely, given a large collection of Wikipedia pages in several languages, the task is to identify the most similar pages across languages. 1 Proposed Method website major gaston links flutist marcel debost states sources"
W15-3413,P07-1084,1,0.797261,"cations, particularly for statistical machine translation. Building such resources is however exceedingly expensive, requiring highly skilled annotators or professional translators (Preiss, 2012). Comparable corpora, that are sets of texts in two or more languages without being translations of each other, are often considered as a solution for the lack of parallel corpora, and many techniques have been proposed to extract parallel sentences (Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010), or mine word translations (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Vuli´c and Moens, 2012). Identifying comparable resources in a large amount of multilingual data remains a very challenging task. The purpose of the Building and Using Comparable Corpora (BUCC) 2015 shared task1 is to provide the first evaluation of existing approaches for identifying comparable resources. More precisely, given a large collection of Wikipedia pages in several languages, the task is to identify the most similar pages across languages. 1 Proposed Method website major gaston links flutist marcel debost states sources college crunelle conservatoire principal rampal united curren"
W15-3413,N04-1034,0,0.187111,"a pages. Introduction Parallel corpora, that is, collections of documents that are mutual translations, are used in many natural language processing applications, particularly for statistical machine translation. Building such resources is however exceedingly expensive, requiring highly skilled annotators or professional translators (Preiss, 2012). Comparable corpora, that are sets of texts in two or more languages without being translations of each other, are often considered as a solution for the lack of parallel corpora, and many techniques have been proposed to extract parallel sentences (Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010), or mine word translations (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Vuli´c and Moens, 2012). Identifying comparable resources in a large amount of multilingual data remains a very challenging task. The purpose of the Building and Using Comparable Corpora (BUCC) 2015 shared task1 is to provide the first evaluation of existing approaches for identifying comparable resources. More precisely, given a large collection of Wikipedia pages in several languages, the task is to identify the most similar pages across lan"
W15-3413,W11-1212,0,0.109518,"ing pigeonhole reasoning and cross-lingual information. 1 2 The method we propose is based on (Enright and Kondrak, 2007)’s approach to parallel document identification. Documents are treated as bags of words, in which only blank separated strings that are at least four characters long and that appear only once in the document (hapax words) are indexed. Given a document in language A, the document in language B that share the largest number of these words is considered as parallel. Although very simple, this approach was shown to perform very well in detecting parallel documents in Wikipedia (Patry and Langlais, 2011). The reason for this is that most hapax words are in practice proper nouns or numerical entities, which are often cognates. An example of hapax words extracted from a document is given in Table 1. We purposely keep urls and special characters, as these are useful clues for identifying translated Wikipedia pages. Introduction Parallel corpora, that is, collections of documents that are mutual translations, are used in many natural language processing applications, particularly for statistical machine translation. Building such resources is however exceedingly expensive, requiring highly skille"
W15-3413,N12-1065,0,0.0243744,"ords are in practice proper nouns or numerical entities, which are often cognates. An example of hapax words extracted from a document is given in Table 1. We purposely keep urls and special characters, as these are useful clues for identifying translated Wikipedia pages. Introduction Parallel corpora, that is, collections of documents that are mutual translations, are used in many natural language processing applications, particularly for statistical machine translation. Building such resources is however exceedingly expensive, requiring highly skilled annotators or professional translators (Preiss, 2012). Comparable corpora, that are sets of texts in two or more languages without being translations of each other, are often considered as a solution for the lack of parallel corpora, and many techniques have been proposed to extract parallel sentences (Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010), or mine word translations (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Vuli´c and Moens, 2012). Identifying comparable resources in a large amount of multilingual data remains a very challenging task. The purpose of the Building and Using Compara"
W15-3413,P99-1067,0,0.179393,"in many natural language processing applications, particularly for statistical machine translation. Building such resources is however exceedingly expensive, requiring highly skilled annotators or professional translators (Preiss, 2012). Comparable corpora, that are sets of texts in two or more languages without being translations of each other, are often considered as a solution for the lack of parallel corpora, and many techniques have been proposed to extract parallel sentences (Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010), or mine word translations (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Vuli´c and Moens, 2012). Identifying comparable resources in a large amount of multilingual data remains a very challenging task. The purpose of the Building and Using Comparable Corpora (BUCC) 2015 shared task1 is to provide the first evaluation of existing approaches for identifying comparable resources. More precisely, given a large collection of Wikipedia pages in several languages, the task is to identify the most similar pages across languages. 1 Proposed Method website major gaston links flutist marcel debost states sources college crun"
W15-3413,N10-1063,0,0.0848939,"ections of documents that are mutual translations, are used in many natural language processing applications, particularly for statistical machine translation. Building such resources is however exceedingly expensive, requiring highly skilled annotators or professional translators (Preiss, 2012). Comparable corpora, that are sets of texts in two or more languages without being translations of each other, are often considered as a solution for the lack of parallel corpora, and many techniques have been proposed to extract parallel sentences (Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010), or mine word translations (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Vuli´c and Moens, 2012). Identifying comparable resources in a large amount of multilingual data remains a very challenging task. The purpose of the Building and Using Comparable Corpora (BUCC) 2015 shared task1 is to provide the first evaluation of existing approaches for identifying comparable resources. More precisely, given a large collection of Wikipedia pages in several languages, the task is to identify the most similar pages across languages. 1 Proposed Method website major gaston link"
W15-3413,E12-1046,0,0.0407071,"Missing"
W19-4730,J93-1003,0,0.400846,"t, sunt numerati, numerati cum, cum eis; 3grams: Levitae autem in, autem in tribu, non sunt numerati, sunt numerati cum, numerati cum eis; 4grams: Levitae autem in tribu, non sunt numerati cum, sunt numerati cum eis; and 5grams: non sunt numerati cum eis. Once the context vectors have been computed, an association measure is used as a way to better characterize the contextual relation between the head of the vector (familiarum suarum) and its constituents. We consider three different association measures: mutual information (Fano, 1961), discounted odds ratio (Evert, 2005) and log-likelihood (Dunning, 1993). Finally, to extract the candidates, we compute cosine similarity (Salton and Lesk, 1968) between all ngrams of the corpus. Our adaptation takes into account broken ngrams. Hence, in addition to the above cited ngrams, based on non sunt numerati cum eis, we add the following bigrams: non numerati, non cum, non eis, sunt cum, sunt eis, numerati eis. Therefore, we assume that the unigrams sunt, numerati, and cum may not appear or were omitted. (1) where D(i,j) represents the distance between two ngrams i et j and Suppcost(i), InsCost(i) represent respectively the deletion, insertion costs of i."
