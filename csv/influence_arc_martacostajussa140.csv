2005.iwslt-1.23,2002.tmi-tutorials.2,0,0.0759388,"ared under monotone conditions. We finally report comparative results in terms of translation accuracy, computation time and memory size. Results show how the ngram-based approach outperforms the phrase-based approach by achieving similar accuracy scores in less computational time and with less memory needs. 1. Introduction From the initial word-based translation models [1], research on statistical machine translation has been strongly boosted. At the end of the last decade the use of context in the translation model (phrase-based approach) lead to a clear improvement in translation quality ( [2], [3], [4]). Nowadays the introduction of some reordering abilities is of crucial importance for some language pairs and is an important focus of research in the area of SMT. In parallel to the phrase-based approach, the ngrambased approach [5] also introduces the word context in the translation model, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phras"
2005.iwslt-1.23,P01-1067,0,0.0561396,"under monotone conditions. We finally report comparative results in terms of translation accuracy, computation time and memory size. Results show how the ngram-based approach outperforms the phrase-based approach by achieving similar accuracy scores in less computational time and with less memory needs. 1. Introduction From the initial word-based translation models [1], research on statistical machine translation has been strongly boosted. At the end of the last decade the use of context in the translation model (phrase-based approach) lead to a clear improvement in translation quality ( [2], [3], [4]). Nowadays the introduction of some reordering abilities is of crucial importance for some language pairs and is an important focus of research in the area of SMT. In parallel to the phrase-based approach, the ngrambased approach [5] also introduces the word context in the translation model, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phrases, w"
2005.iwslt-1.23,W02-1018,0,0.0292349,"monotone conditions. We finally report comparative results in terms of translation accuracy, computation time and memory size. Results show how the ngram-based approach outperforms the phrase-based approach by achieving similar accuracy scores in less computational time and with less memory needs. 1. Introduction From the initial word-based translation models [1], research on statistical machine translation has been strongly boosted. At the end of the last decade the use of context in the translation model (phrase-based approach) lead to a clear improvement in translation quality ( [2], [3], [4]). Nowadays the introduction of some reordering abilities is of crucial importance for some language pairs and is an important focus of research in the area of SMT. In parallel to the phrase-based approach, the ngrambased approach [5] also introduces the word context in the translation model, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phrases, where"
2005.iwslt-1.23,N04-1033,0,0.415123,"tuples methods. As can be seen,to produce the source sentence, the extracted unfolded tuples must be reordered. It is not the case of the target sentence, as it can be produced in order using both sequence of units. Figure 1 shows the bilingual units extracted using the extract-tuples and extract-unfold-tuples methods, for a given word-to-word aligned sentence pair. 2.2. Phrase-based Translation Model The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [12]. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [13] and the modification in phrase length in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any gi"
2005.iwslt-1.23,J04-4002,0,0.0562728,"t is not the case of the target sentence, as it can be produced in order using both sequence of units. Figure 1 shows the bilingual units extracted using the extract-tuples and extract-unfold-tuples methods, for a given word-to-word aligned sentence pair. 2.2. Phrase-based Translation Model The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [12]. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [13] and the modification in phrase length in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provid"
2005.iwslt-1.23,W05-0827,1,0.833763,"t can be produced in order using both sequence of units. Figure 1 shows the bilingual units extracted using the extract-tuples and extract-unfold-tuples methods, for a given word-to-word aligned sentence pair. 2.2. Phrase-based Translation Model The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [12]. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [13] and the modification in phrase length in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provide a significant improve in quality [16] as the"
2005.iwslt-1.23,P05-1032,0,0.0181694,"he target sentence from these phrase translations [12]. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [13] and the modification in phrase length in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provide a significant improve in quality [16] as the probability of reappearence of larger phrases decreases. In our system we considered two length limits. We first extract all the phrases of length X or less (usually X equal to 3 or 4). Then, we also add phrases up to length Y (Y greater than X) if they cannot be generated by smaller phrases. Basically, we select additional phrases with source words that otherwise would be missed because"
2005.iwslt-1.23,N03-1017,0,0.0564353,"gth in [14]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is infesible to build a dictionary with all the phrases (recent papers show related work to tackle this problem, see [15]). That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provide a significant improve in quality [16] as the probability of reappearence of larger phrases decreases. In our system we considered two length limits. We first extract all the phrases of length X or less (usually X equal to 3 or 4). Then, we also add phrases up to length Y (Y greater than X) if they cannot be generated by smaller phrases. Basically, we select additional phrases with source words that otherwise would be missed because of cross or long alignments [14]. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frecuency. N (f, e) P (f |e) = N (e) (3) where N(f,e) means t"
2005.iwslt-1.23,N04-1021,0,0.198833,"s is approximated by the product of word 3-gram probabilities: p(Tk ) ≈ k Y p(wn |wn−2 , wn−1 ) (4) n=1 where Tk refers to the partial translation hypothesis and wn to the nth word in it. As default language model feature, we use a standard word-based trigram language model generated with smoothing Kneser-Ney and interpolation of higher and lower order ngrams (by using SRILM [17]). • The following two feature functions correspond to a forward and backwards lexicon models. These models provides lexicon translation probabilities for each tuple based on the word-to-word IBM model 1 probabilities [18]. These lexicon models are computed according to the following equation: p((t, s)n ) = J X I Y 1 pIBM 1 (tin |sjn ) (I + 1)J j=1 i=0 (5) where sjn and tin are the j th and ith words in the source and target sides of tuple (t, s)n , being J and I the corresponding total number words in each side of it. For computing the forward lexicon model, IBM model 1 probabilities from GIZA++ [19] source-to-target alignments are used. In the case of the backwards lexicon model, GIZA++ target-to-source alignments are used instead. • The last feature in common we consider corresponds to a word penalty model."
2005.iwslt-1.23,2005.mtsummit-papers.37,1,0.848127,"model, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phrases, where the sequential composition of the phrases source words corresponds to the source sentence reordered. This procedure poses additional difficulties when applied to the ngram-based approach, because the characteristics of the ngram-based translation model. Despite of this, recent works ( [7], [8]) have shown how applying a reordering schema in the training process the ngram-based approach can also take advantage of the distortion capabilities. In this paper we study the differences and similarities of both approaches (ngram-based and phrase-based), focusing on the translation model, where the translation context is differently taken into account. We also investigate the differences in the translation (bilingual) units (tuples and phrases) and show efficiency results in terms of computation time and memory size for both systems. We have extended the comparison in [6] to a Chinese"
2005.iwslt-1.23,W05-0831,0,0.0547079,"l, what allows to obtain comparable results under monotone conditions (as shown in [6]). The addition of reordering abilities in the phrase-based approach is achieved by enabling a certain level of reordering in the source sentence. Though, the translation process consists of a composition of phrases, where the sequential composition of the phrases source words corresponds to the source sentence reordered. This procedure poses additional difficulties when applied to the ngram-based approach, because the characteristics of the ngram-based translation model. Despite of this, recent works ( [7], [8]) have shown how applying a reordering schema in the training process the ngram-based approach can also take advantage of the distortion capabilities. In this paper we study the differences and similarities of both approaches (ngram-based and phrase-based), focusing on the translation model, where the translation context is differently taken into account. We also investigate the differences in the translation (bilingual) units (tuples and phrases) and show efficiency results in terms of computation time and memory size for both systems. We have extended the comparison in [6] to a Chinese to En"
2005.iwslt-1.23,P02-1038,0,0.234488,"els taken into account in the log-linear combination of features (see equation 1), and the bilingual units extraction methods (namely tuples and phrases). In section 3 is discussed the decoder used in both systems (MARIE) [9], giving details of pruning and reordering techniques. The comparison framework, experiments and results are shown in section 4, while conclusions are detailed in section 5. 2. Modeling Alternatively to the classical source channel approach, statistical machine translation models directly the posterior probability p(eI1 |f1J ) as a log-linear combination of feature models [10], based on the maximum entropy framework, as shown in [11]. This simplifies the introduction of several additional models explaining the translation process, as the search becomes: arg max{exp( eI1 X λi hi (e, f ))} (1) i where the feature functions hi are the system models (translation model, language model, reordering model, ...), and the λi weights are typically optimized to maximize a scoring function on a development set. The Translation Model is based on bilingual units (here called tuples and phrases). A bilingual unit consists of two monolingual fragments, where each one is supposed to"
2005.iwslt-1.23,J96-1002,0,0.0712527,"atures (see equation 1), and the bilingual units extraction methods (namely tuples and phrases). In section 3 is discussed the decoder used in both systems (MARIE) [9], giving details of pruning and reordering techniques. The comparison framework, experiments and results are shown in section 4, while conclusions are detailed in section 5. 2. Modeling Alternatively to the classical source channel approach, statistical machine translation models directly the posterior probability p(eI1 |f1J ) as a log-linear combination of feature models [10], based on the maximum entropy framework, as shown in [11]. This simplifies the introduction of several additional models explaining the translation process, as the search becomes: arg max{exp( eI1 X λi hi (e, f ))} (1) i where the feature functions hi are the system models (translation model, language model, reordering model, ...), and the λi weights are typically optimized to maximize a scoring function on a development set. The Translation Model is based on bilingual units (here called tuples and phrases). A bilingual unit consists of two monolingual fragments, where each one is supposed to be the translation of its counterpart. During training, t"
2005.iwslt-1.23,takezawa-etal-2002-toward,0,0.0468191,"sary trade-off between quality and efficiency. 4. Comparison 4.1. Evaluation Framework Figure 2: Search graph corresponding to a source sentence with four words. Details of constraints are given in following sections. The search loops expanding available hypotheses. The expansion proceeds incrementally starting in the group of lists covering 1 source word, ending with the group of lists covering J − 1 source words (J is the size in words of the source sentence). See [9] for further details. Experiments have been carried out using two databases: the EPPS database (Spanish-English) and the BTEC [20] database (Chinese-English). The BTEC is a small corpus translation task, used in the IWSLT’04 spoken language campaign1. Table 1 shows the main statistics of the used data, namely number of sentences, words, vocabulary, and mean sentence lengths for each language. The EPPS data set corresponds to the parliamentary session transcriptions of the European Parliament and is currently available at the Parliament’s website (http://www.euro parl.eu.int/). In the case of the results presented here, we have used the version of the EPPS data that was made available by RWTH Aachen University through the"
2005.iwslt-1.23,2005.iwslt-1.24,1,0.680304,"ated to the corpus size). Similar accuracy results in all tasks are reached for the baseline configurations. When upgrading the systems with additional features, slight differences appear. Although improvements added by each feature depends on the task and system, similar performances are reached in the best system’s configurations. Under reordering conditions, the ngram-based system seems to take advantage of the unfolding method applied in training, outperforming the phrase-based system. However, last results obtained for the IWSLT’05 show an opposite behaviour of both systems, see [22] and [23]. We can conclude that both approaches have a similar performance in terms of translation quality. The slight differences seen in the experiments are related to how the systems take advantage of each feature model and to the current system’s implementation. In terms of the memory size and computation time, the ngram-based system has obtained consistently better results. This indicates how even though using a smaller vocabulary of bilingual units, it has been more efficiently built and managed. The last characteristic becomes of great importance when working with large databases. 6. Acknowledgm"
2005.iwslt-1.24,J96-1002,0,0.0402617,"e translation model P (f |e) [6]. In the last few years, new systems tend to use sequences of words, commonly called phrases [7], aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework.   M  λm hm (e, f ) (3) e˜ = argmax e m=1 The features functions, h m , are the system models (translation model, language model and others) and weights, λi , are typically optimized to maximize a scoring function [12]. It is derived from the Maximum Entropy approach as shown in [1] and has the advantage that additional features functions can be easily integrated in the overall system. This paper addresses a modification of the phraseextraction algorithm in [13] and results in Chinese to English and Arabic to English tasks are reported. It also combines several alignments before extracting phrases and interesting features. It is organized as follows. Section 2 explains the SMT system: the phrase extraction, its modification and shows the different features which have been taken into account and, briefly, the decoding; section 3 presents the evaluation framework and the r"
2005.iwslt-1.24,W05-0827,1,0.783429,"-gram probabilities: 2.1.1. Word alignment Given a sentence pair, we use GIZA++ [10] to align each of them word-to-word. We can train in both translation directions and we obtain: (1) the alignment in the source to target direction (s2t); and (2) the alignment in the target to source direction. If we compose the union of both alignments (sU t), we get a higher recall and a lower precision of the combined alignment. 2.1.2. Phrase-extraction Phrases are extracted from sentence pairs and theirs corespondents word alignments following the criterion in [13] and the modification in phrase length in [4]. A phrase is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase. It is unfeasible to build a dictionary with all the phrases. That is why we limit the maximum size of any given phrase. Also, the huge increase in computational and storage cost of including longer phrases does not provide a significant improve in quality [7] as the probability of reappearance of larger phrases decreases. In our system we considered two"
2005.iwslt-1.24,2005.iwslt-1.23,1,0.726606,"ts are pruned out according to the accumulated probabilities of their hypotheses. Worst hypotheses with minor probabilities are discarded to make the search feasible. Also the decoder allows reordering. The use of the reordering strategies suppose a necessary trade-off between quality and efficiency. That is why two reordering strategies are used: • A distortion limit (m). A source word (phrase or tuple) is only allowed to be reordered if it does not exceed a distortion limit, measured in words. • A reorderings limit (j). Any translation path is only allowed to perform j reordering jumps. See [5] for further details. 3. Evaluation Framework 3.1. Corpus Statistics Experiments have been carried out in two tasks of the IWSLT’05 evaluation 1: Chinese to English (BTEC Corpus [15]) and Arabic to English. The BTEC is a small corpus translation task. Table 1 shows the main statistics of the used data, namely number of sentences, words, vocabulary, and mean sentence lengths for each language. 1 www.slt.atr.jp/IWSLT2005 BTEC Training Sentences Words Vocabulary Development Sentences Words Vocabulary Test Sentences Words Vocabulary Chinese 20 k 176.2 k 8.7 k 1006 7.3 k 1.4 k 506 3.7 k 963 English"
2005.iwslt-1.24,N03-1017,0,0.0943422,"to each, which has to be learned from a bilingual text corpus. Thus, the translation of a source sentence f can be formulated as the search of the target sentence e that maximizes the translation probability P (e|f ), e˜ = argmax P (e|f ) e (1) If we use Bayes rule to reformulate the translation probability, we obtain, e˜ = argmax P (f |e)P (e) e (2) This translation model is known as the source-channel approach [2] and it consists on a language model P (e) and a separate translation model P (f |e) [6]. In the last few years, new systems tend to use sequences of words, commonly called phrases [7], aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework.   M  λm hm (e, f ) (3) e˜ = argmax e m=1 The features functions, h m , are the system models (translation model, language model and others) and weights, λi , are typically optimized to maximize a scoring function [12]. It is derived from the Maximum Entropy approach as shown in [1] and has the advantage that additional features functions can be easily integrated in the overall system. This paper addresse"
2005.iwslt-1.24,W02-1018,0,0.0257822,"used which lead to a clear improvement in the performance of translation. Finally, the system manages to do reordering. We report results in terms of translation accuracy by using the BTEC corpus in the tasks of Chinese to English and Arabic to English, in the framework of IWSLT’05 evaluation. 1. Introduction From the initial word-based translation models [3], research on statistical machine translation has been strongly improved. At the end of the last decade the use of context in the translation model (phrase-based approach) supposed a clear improvement in translation quality ( [17], [16], [8]). Statistical Machine Translation (SMT) is based on the assumption that every sentence e in the target language is a possible translation of a given sentence f in the source language. The main difference between two possible translations of a given sentence is a probability assigned to each, which has to be learned from a bilingual text corpus. Thus, the translation of a source sentence f can be formulated as the search of the target sentence e that maximizes the translation probability P (e|f ), e˜ = argmax P (e|f ) e (1) If we use Bayes rule to reformulate the translation probability, we ob"
2005.iwslt-1.24,N04-1021,0,0.0284561,"n |...wn−3 , wn−2 , wn−1 ) (5) n=1 where Tk refers to the partial translation hypothesis and wn to the nth word in it. • As translation model we use the conditional probability. Note that no smoothing is performed, which may cause an overestimation of the probability of rare phrases. This is specially harmful given a bilingual phrase where the source part has a big frequency of appearance but the target part appears rarely. That is why we use the posterior phrase probability, we compute again the relative frequency but replacing the count of the target phrase by the count of the source phrase [11]. P (e|f ) = N  (f, e) N (f ) (6) where N’(f,e) means the number of times the phrase e is translated by f. If a phrase f has N &gt; 1 possible translations, then each one contributes as 1/N. Adding this feature function we reduce the number of cases in which the overall probability is overestimated. • The following two feature functions correspond to a forward and backward lexicon models. These models provides lexicon translation probabilities for each tuple based on the word-to-word IBM model 1 probabilities [11]. These lexicon models are computed according to the following equation: J  I  1"
2005.iwslt-1.24,P02-1038,0,0.0539036,"proach [2] and it consists on a language model P (e) and a separate translation model P (f |e) [6]. In the last few years, new systems tend to use sequences of words, commonly called phrases [7], aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework.   M  λm hm (e, f ) (3) e˜ = argmax e m=1 The features functions, h m , are the system models (translation model, language model and others) and weights, λi , are typically optimized to maximize a scoring function [12]. It is derived from the Maximum Entropy approach as shown in [1] and has the advantage that additional features functions can be easily integrated in the overall system. This paper addresses a modification of the phraseextraction algorithm in [13] and results in Chinese to English and Arabic to English tasks are reported. It also combines several alignments before extracting phrases and interesting features. It is organized as follows. Section 2 explains the SMT system: the phrase extraction, its modification and shows the different features which have been taken into account and, briefly, th"
2005.iwslt-1.24,J04-4002,0,0.320048,"n model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework.   M  λm hm (e, f ) (3) e˜ = argmax e m=1 The features functions, h m , are the system models (translation model, language model and others) and weights, λi , are typically optimized to maximize a scoring function [12]. It is derived from the Maximum Entropy approach as shown in [1] and has the advantage that additional features functions can be easily integrated in the overall system. This paper addresses a modification of the phraseextraction algorithm in [13] and results in Chinese to English and Arabic to English tasks are reported. It also combines several alignments before extracting phrases and interesting features. It is organized as follows. Section 2 explains the SMT system: the phrase extraction, its modification and shows the different features which have been taken into account and, briefly, the decoding; section 3 presents the evaluation framework and the results in Chinese to English and Arabic to English tasks are reported; and the final section shows some conclusions on the experiments and in the evaluation of IWSLT’05. 2. SMT system"
2005.iwslt-1.24,takezawa-etal-2002-toward,0,0.0301298,"er allows reordering. The use of the reordering strategies suppose a necessary trade-off between quality and efficiency. That is why two reordering strategies are used: • A distortion limit (m). A source word (phrase or tuple) is only allowed to be reordered if it does not exceed a distortion limit, measured in words. • A reorderings limit (j). Any translation path is only allowed to perform j reordering jumps. See [5] for further details. 3. Evaluation Framework 3.1. Corpus Statistics Experiments have been carried out in two tasks of the IWSLT’05 evaluation 1: Chinese to English (BTEC Corpus [15]) and Arabic to English. The BTEC is a small corpus translation task. Table 1 shows the main statistics of the used data, namely number of sentences, words, vocabulary, and mean sentence lengths for each language. 1 www.slt.atr.jp/IWSLT2005 BTEC Training Sentences Words Vocabulary Development Sentences Words Vocabulary Test Sentences Words Vocabulary Chinese 20 k 176.2 k 8.7 k 1006 7.3 k 1.4 k 506 3.7 k 963 English 20 k 182.3 k 7.3 k 1006 6k 1.3 k 506 - Table 1: Chinese to English task. BTEC Corpus: Training, Development and Test data sets. The Development data set has 16 references, (k stands"
2005.iwslt-1.24,P01-1067,0,0.0695767,"es are used which lead to a clear improvement in the performance of translation. Finally, the system manages to do reordering. We report results in terms of translation accuracy by using the BTEC corpus in the tasks of Chinese to English and Arabic to English, in the framework of IWSLT’05 evaluation. 1. Introduction From the initial word-based translation models [3], research on statistical machine translation has been strongly improved. At the end of the last decade the use of context in the translation model (phrase-based approach) supposed a clear improvement in translation quality ( [17], [16], [8]). Statistical Machine Translation (SMT) is based on the assumption that every sentence e in the target language is a possible translation of a given sentence f in the source language. The main difference between two possible translations of a given sentence is a probability assigned to each, which has to be learned from a bilingual text corpus. Thus, the translation of a source sentence f can be formulated as the search of the target sentence e that maximizes the translation probability P (e|f ), e˜ = argmax P (e|f ) e (1) If we use Bayes rule to reformulate the translation probability,"
2005.iwslt-1.24,2002.tmi-tutorials.2,0,0.0513375,"features are used which lead to a clear improvement in the performance of translation. Finally, the system manages to do reordering. We report results in terms of translation accuracy by using the BTEC corpus in the tasks of Chinese to English and Arabic to English, in the framework of IWSLT’05 evaluation. 1. Introduction From the initial word-based translation models [3], research on statistical machine translation has been strongly improved. At the end of the last decade the use of context in the translation model (phrase-based approach) supposed a clear improvement in translation quality ( [17], [16], [8]). Statistical Machine Translation (SMT) is based on the assumption that every sentence e in the target language is a possible translation of a given sentence f in the source language. The main difference between two possible translations of a given sentence is a probability assigned to each, which has to be learned from a bilingual text corpus. Thus, the translation of a source sentence f can be formulated as the search of the target sentence e that maximizes the translation probability P (e|f ), e˜ = argmax P (e|f ) e (1) If we use Bayes rule to reformulate the translation probabi"
2005.iwslt-1.24,N04-1033,0,0.557737,"ecoding that build this system. The Translation Model is based on bilingual phrase (or phrases). A bilingual unit consists of two monolingual fragments, where each one is supposed to be the translation of its counterpart. During training, the system learns a dictionary of these bilingual fragments, the actual core of the translation systems. 2.1. Phrase-based Translation Model 2.2. Additional features The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [18]. • Firstly, we consider the target language model. It actually consists of an n-gram model, in which the probability of a translation hypothesis is approximated by the product of word n-gram probabilities: 2.1.1. Word alignment Given a sentence pair, we use GIZA++ [10] to align each of them word-to-word. We can train in both translation directions and we obtain: (1) the alignment in the source to target direction (s2t); and (2) the alignment in the target to source direction. If we compose the union of both alignments (sU t), we get a higher recall and a lower precision of the combined alignm"
2005.iwslt-1.24,J90-2002,0,\N,Missing
2006.iwslt-evaluation.17,W05-0820,0,0.0116848,", namely from Arabic, Chinese, Italian and Japanese into English for the open data track, thoroughly explaining all language-related preprocessing and optimization schemes. 1. Introduction Rooted in the Finite-State Transducers approach to SMT [1, 2] and estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternative to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in [3]. This is specially true when dealing with pairs of languages with a relatively similar word order [4, 5]. Given the language pairs involved in this year’s evaluation, efforts have been focused on improving the word reordering strategies for Ngram-based SMT. Specifically, a novel reordering strategy based on extending the search graph with automatically-extracted reordering patterns is explored. Results are very promising while keeping computational expenses at a similar level of monotone search. Additionally, a novel tuple segmentation strategy based on the entropy of Part-Of-Speech distributions was used with slight improvements in model estimation. This paper is organized as follows. Section 2"
2006.iwslt-evaluation.17,W06-3114,0,0.0159274,", namely from Arabic, Chinese, Italian and Japanese into English for the open data track, thoroughly explaining all language-related preprocessing and optimization schemes. 1. Introduction Rooted in the Finite-State Transducers approach to SMT [1, 2] and estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternative to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in [3]. This is specially true when dealing with pairs of languages with a relatively similar word order [4, 5]. Given the language pairs involved in this year’s evaluation, efforts have been focused on improving the word reordering strategies for Ngram-based SMT. Specifically, a novel reordering strategy based on extending the search graph with automatically-extracted reordering patterns is explored. Results are very promising while keeping computational expenses at a similar level of monotone search. Additionally, a novel tuple segmentation strategy based on the entropy of Part-Of-Speech distributions was used with slight improvements in model estimation. This paper is organized as follows. Section 2"
2006.iwslt-evaluation.17,2005.mtsummit-papers.36,1,0.472143,"g patterns. Section 4 focuses on tuple segmentation strategies, and contrasts the criterion on IBM model 1 probabilities from 2005 with a novel criterion based on Part-Of-Speech entropy distributions. Later on, Section 5 reports on all experiments carried out from Arabic, Chinese, Italian and Japanese into English for IWSLT 2006. Finally, Section 6 sums up the main conclusions from the paper and discusses future research lines. 2. 2005 system review The TALP Ngram-based SMT system performs a log-linear combination of a translation model and additional feature functions (see further details in [6, 7]). In contrast to phrasebased models, our translation model is estimated as a standard n-gram model of a bilingual language expressed in tuples. This way it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(sJ1 , tI1 ) = K Y p((s, t)i |(s, t)i−N +1 , ..., (s, t)i−1 ) (1) i=1 where (s, t)i refers to the ith tuple of a sentence pair being segmented into K tuples. A detailed comparison between Ngram-based and phrase-based SMT can be found in [8]. 2.1. Tuple extraction Given a certain word-aligned parallel"
2006.iwslt-evaluation.17,2005.iwslt-1.23,1,0.868702,"l and additional feature functions (see further details in [6, 7]). In contrast to phrasebased models, our translation model is estimated as a standard n-gram model of a bilingual language expressed in tuples. This way it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(sJ1 , tI1 ) = K Y p((s, t)i |(s, t)i−N +1 , ..., (s, t)i−1 ) (1) i=1 where (s, t)i refers to the ith tuple of a sentence pair being segmented into K tuples. A detailed comparison between Ngram-based and phrase-based SMT can be found in [8]. 2.1. Tuple extraction Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints 116 3. Word ordering strategies 2.2. Feature functions As additional feature functions to better guide the translation process, the system incorporates a target language model, a word bonus model and two lexicon models. The target language model is estimated"
2006.iwslt-evaluation.17,N04-1033,0,0.120187,"ated as a standard n-gram model of a bilingual language expressed in tuples. This way it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(sJ1 , tI1 ) = K Y p((s, t)i |(s, t)i−N +1 , ..., (s, t)i−1 ) (1) i=1 where (s, t)i refers to the ith tuple of a sentence pair being segmented into K tuples. A detailed comparison between Ngram-based and phrase-based SMT can be found in [8]. 2.1. Tuple extraction Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints 116 3. Word ordering strategies 2.2. Feature functions As additional feature functions to better guide the translation process, the system incorporates a target language model, a word bonus model and two lexicon models. The target language model is estimated as a standard ngram over the target words, as follows: pLM (tk ) ≈ k Y p(wn |wn−N +1 , ..., wn−1 ) (2) n=1 where tk refers to the par"
2006.iwslt-evaluation.17,2005.mtsummit-papers.37,1,0.788658,"n |wn−N +1 , ..., wn−1 ) (2) n=1 where tk refers to the partial hypothesis and wn to the nth word in it. Usually, this feature is accompanied by a word bonus model based on sentence length, compensating the target language model preference for short sentences (in number of target words). This bonus depends on the number of target words in the partial hypothesis, denoted as: pW P (tk ) = exp(number of words in tk ) When dealing with pairs of languages with non-monotonic word order, a certain reordering strategy is required. Apart from that, tuples need to be extracted by an unfolding technique [11]. This means that the tuples are broken into smaller tuples, and these are sequenced in the order of the target words. In order not to lose the information on the correct order, the decoder performs then a reordered search (or a monotone search extended with reordering paths), which is guided by the n-gram model of the unfolded tuples and the additional feature models. Figure 1 shows an example of tuple unfolding compared to the monotonic extraction. The unfolding technique produces a different bilingual n-gram language model with reordered source words. (3) where tk refers to the partial hypo"
2006.iwslt-evaluation.17,A00-1031,0,0.0103336,"Language-dependent preprocessing For all language pairs, training sentences were split by using final dots on both sides of the bilingual text (when the number of dots was equal), increasing the number of sentences and reducing its length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. English Table 1: Arabic→English corpus statistics. sent. it en it it it it wrds 155k 166k 5,193 2,807 5,978 5,767 refs. 1 7 16 7 7 Table 2: Chinese→English corpus statistics. English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [15] and lemmatization using wnmorph, included in the WordNet package [16]. The English Penn Treebank Tag Set used contains 36 different tags. 5.2.2. Arabic Following a similar approach to that in [17], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [18], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronom"
2006.iwslt-evaluation.17,N06-2013,0,0.0357882,"mber of sentences and reducing its length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. English Table 1: Arabic→English corpus statistics. sent. it en it it it it wrds 155k 166k 5,193 2,807 5,978 5,767 refs. 1 7 16 7 7 Table 2: Chinese→English corpus statistics. English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [15] and lemmatization using wnmorph, included in the WordNet package [16]. The English Penn Treebank Tag Set used contains 36 different tags. 5.2.2. Arabic Following a similar approach to that in [17], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [18], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. The Arabic Treebank tag set used contains 20 different tags. Corpora statistics for all language pairs can be found in 2 Ver"
2006.iwslt-evaluation.17,P05-1071,0,0.0218458,"n it it it it wrds 155k 166k 5,193 2,807 5,978 5,767 refs. 1 7 16 7 7 Table 2: Chinese→English corpus statistics. English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [15] and lemmatization using wnmorph, included in the WordNet package [16]. The English Penn Treebank Tag Set used contains 36 different tags. 5.2.2. Arabic Following a similar approach to that in [17], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [18], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. The Arabic Treebank tag set used contains 20 different tags. Corpora statistics for all language pairs can be found in 2 Version 119 2.0. Linguistic Data Consortium Catalog: LDC2004L02. 5.2.3. Chinese official Chinese preprocessing included resegmentation and POStagging. These tasks were done by using ICTCLAS [19]. Resultan"
2006.iwslt-evaluation.17,W03-1730,0,0.0274114,"Missing"
2006.iwslt-evaluation.17,2004.iwslt-evaluation.14,1,\N,Missing
2006.iwslt-evaluation.17,E99-1010,0,\N,Missing
2006.iwslt-evaluation.17,J96-1002,0,\N,Missing
2006.iwslt-evaluation.17,W05-0823,1,\N,Missing
2006.iwslt-evaluation.17,N07-2022,1,\N,Missing
2006.iwslt-evaluation.17,2005.iwslt-1.1,0,\N,Missing
2006.iwslt-evaluation.17,J06-4004,1,\N,Missing
2006.iwslt-evaluation.17,N03-1017,0,\N,Missing
2006.iwslt-evaluation.17,J03-1002,0,\N,Missing
2006.iwslt-evaluation.17,2006.iwslt-evaluation.1,0,\N,Missing
2006.iwslt-evaluation.17,2005.iwslt-1.24,1,\N,Missing
2006.iwslt-evaluation.17,2006.iwslt-papers.2,1,\N,Missing
2006.iwslt-evaluation.17,atserias-etal-2006-freeling,0,\N,Missing
2006.iwslt-evaluation.17,2005.iwslt-1.11,0,\N,Missing
2006.iwslt-evaluation.17,P00-1056,0,\N,Missing
2006.iwslt-evaluation.17,2006.iwslt-papers.5,1,\N,Missing
2006.iwslt-evaluation.18,2005.iwslt-1.24,1,0.787344,"TALP phrase-based statistical machine translation system, enriched with the statistical machine reordering technique. We also report the combination of this system and the TALP-tuple, the n-gram-based statistical machine translation system. We report the results for all the tasks (Chinese, Arabic, Italian and Japanese to English) in the framework of the third evaluation campaign of the International Workshop on Spoken Language Translation. 1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework. e˜ = argmax e ( M X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report th"
2006.iwslt-evaluation.18,W06-1609,1,0.921033,"e combination of this system and the TALP-tuple, the n-gram-based statistical machine translation system. We report the results for all the tasks (Chinese, Arabic, Italian and Japanese to English) in the framework of the third evaluation campaign of the International Workshop on Spoken Language Translation. 1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework. e˜ = argmax e ( M X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to Eng"
2006.iwslt-evaluation.18,W06-3125,1,0.836838,"ional Workshop on Spoken Language Translation. 1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework. e˜ = argmax e ( M X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to English. 2. Description of the TALP-phrase System 2.1. Phrase-based Model The basic idea of phrase-based translation is to segment the given source sentence into units (here called phrases), then translate each phrase and finally compose the target sentence from th"
2006.iwslt-evaluation.18,P02-1038,0,0.0577834,"re consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase. 2.2. Feature functions The baseline phrase-based system implements a log-linear combination of four feature functions, which are described as follows. • The translation model is estimated with relative frequencies. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency in both directions. (1) The feature functions, hm , and weights, λi , are typically optimized to maximize the scoring function [4]. Two basic issues differentiate the n-gram-based system from the phrase-based system: the bilingual units are extracted from a monotonic segmentation of the training data; the unit probabilities are based on a standard back-off language model rather than directly on relative frequencies. In both systems, the introduction of reordering capabilities is crucial for certain language pairs. This paper is organized as follows. Section 2 describes the TALP-phrase system, with particular emphasis on a new reordering technique: the statistical machine reordering approach. In Section 3, we combine the"
2006.iwslt-evaluation.18,J04-4002,0,0.0302302,"X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to English. 2. Description of the TALP-phrase System 2.1. Phrase-based Model The basic idea of phrase-based translation is to segment the given source sentence into units (here called phrases), then translate each phrase and finally compose the target sentence from these phrase translations. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [5]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints (1) words are consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase. 2.2. Feature functions The baseline phrase-based system implements a log-linear combination of four feature functions, which are described as follows. • The translation model is estimated with relative frequencies. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequen"
2006.iwslt-evaluation.18,2005.iwslt-1.6,0,0.0322379,"using the GIZA++ tool [7]. During word alignment, we used 50 classes per language. We aligned both translation directions and combined the two alignments with the union operation. train dev4 dev123 test ASRtest • Word classes (which were used to help the aligner and to perform the SMR process) were determined using “mkcls”, a tool freely-available with GIZA++. • The language model was estimated using the SRILM toolkit [8]. • The decoder was MARIE [9]. • The optimization tool used for computing log-linear weights was based on the simplex method [6]. Following the consensus strategy proposed in [10], the objective function was set to 100 · BLEU + 4 · N IST . 489 500 500 500 voc. 9.7k 9.6k 1,096 909 1,292 1,311 slen. 6.7 7.0 11.2 6.0 11.7 11.6 refs. 1 7 16 7 7 Corpus statistics for all language pairs can be found in Tables 1, 2, 3 and 4, respectively, where number of sentences, running words, vocabulary, sentence length and human references are shown. sent. train dev4 dev123 test ASRtest Experiments were carried out for all tasks of the IWSLT06 evaluation (Zh2En, Jp2En, Ar2En and It2En) using the BTEC Corpus provided for the open data track1 . it en it it it it 24.6k 489 500 500 500 wrds"
2006.iwslt-evaluation.18,A00-1031,0,0.0229282,"while randomly selecting 500 sentences from developments 1, 2 and 3 (around 160 sentences from each) to build an internal test set (dev123). zh en zh zh zh zh 4.4. Language-dependent preprocessing For all language pairs, training sentences were split by using full stops on both sides of the bilingual text (when the number of stops was equal), increasing the number of sentences and reducing their length. Specific preprocessing for each language is detailed in the respective section below. 4.4.1. English English preprocessing includes part-of-speech tagging using the freely-available TnT tagger [11] and lemmatization using wnmorph, included in the WordNet package [12]. 126 sent. train dev4 dev123 test ASRtest jp en jp jp jp jp 45.2k 489 500 500 500 wrds 390k 325k 6,758 3,818 7,367 7,494 voc. 10.6k 9.6k 1,169 936 1,301 1,331 slen. 8.6 7.2 13.8 7.6 14.7 15.0 refs. 1 7 16 7 7 Table 4: Japanese→English corpus statistics. 4.4.2. Arabic Following a similar approach to that taken in [13], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [14], kindly p"
2006.iwslt-evaluation.18,N06-2013,0,0.0411805,"ing their length. Specific preprocessing for each language is detailed in the respective section below. 4.4.1. English English preprocessing includes part-of-speech tagging using the freely-available TnT tagger [11] and lemmatization using wnmorph, included in the WordNet package [12]. 126 sent. train dev4 dev123 test ASRtest jp en jp jp jp jp 45.2k 489 500 500 500 wrds 390k 325k 6,758 3,818 7,367 7,494 voc. 10.6k 9.6k 1,169 936 1,301 1,331 slen. 8.6 7.2 13.8 7.6 14.7 15.0 refs. 1 7 16 7 7 Table 4: Japanese→English corpus statistics. 4.4.2. Arabic Following a similar approach to that taken in [13], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [14], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. search (with m = 5 and j = 3) for all tasks and for all systems (with or without SMR technique); except for the Italian to E"
2006.iwslt-evaluation.18,P05-1071,0,0.0382995,"nT tagger [11] and lemmatization using wnmorph, included in the WordNet package [12]. 126 sent. train dev4 dev123 test ASRtest jp en jp jp jp jp 45.2k 489 500 500 500 wrds 390k 325k 6,758 3,818 7,367 7,494 voc. 10.6k 9.6k 1,169 936 1,301 1,331 slen. 8.6 7.2 13.8 7.6 14.7 15.0 refs. 1 7 16 7 7 Table 4: Japanese→English corpus statistics. 4.4.2. Arabic Following a similar approach to that taken in [13], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [14], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. search (with m = 5 and j = 3) for all tasks and for all systems (with or without SMR technique); except for the Italian to English task where a monotonic search was used. The primary system of each task is that which had the best performance in the internal test. In all tasks, the SMR improved the results in the internal te"
2006.iwslt-evaluation.18,W03-1730,0,0.0516051,"Missing"
2006.iwslt-evaluation.18,atserias-etal-2006-freeling,0,0.0138517,"for the internal test set (specially, for the Arabic and Japanese tasks). The higher the number of unknown words, the worse the SMR output and, consequently, the quality of translation. Here, a possible solution would be to predict word classes for unknown words in order to avoid their bad influence in the SMR output. 4.4.3. Chinese Set development test evaluation Chinese preprocessing included re-segmentation and POStagging. These tasks were performed using ICTCLAS [15]. 4.4.4. Italian Italian was POS-tagged and lemmatized using the freelyavailable FreeLing morpho-syntactic analysis package [16]. Additionally, Italian contracted prepositions were separated into preposition + article, for example ’alla’→’a la’, ’degli’→’di gli’ or ’dallo’→’da lo’. 4.4.5. Japanese When dealing with Japanese, one has to come up with new methods for overcoming the absence of delimiters between words. We addressed this issue by word segmentation using the freely available JUMAN tool [17] version 5.1. This tool was also used for POS-tagging of the Japanese text. 4.5. Results In Table 6 we show the results for all the TALP systems that participated in the IWSLT 2006: the TALP-phrase, the TALP-tuple and the"
2006.iwslt-evaluation.18,2004.iwslt-evaluation.8,0,\N,Missing
2006.iwslt-papers.2,2005.iwslt-1.5,0,0.0293593,"ent speeches (T C -S TAR evaluations). Therefore, new techniques must be deployed to take the best advantage of the limited resources. For instance, it was proposed to use a translation lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation systems (SMT) that participated in the 2005 I WSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram prefix and suffix LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the B TEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the I WSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator"
2006.iwslt-papers.2,2005.iwslt-1.12,0,0.0293042,"ent speeches (T C -S TAR evaluations). Therefore, new techniques must be deployed to take the best advantage of the limited resources. For instance, it was proposed to use a translation lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation systems (SMT) that participated in the 2005 I WSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram prefix and suffix LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the B TEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the I WSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator"
2006.iwslt-papers.2,2005.iwslt-1.15,0,0.0276075,"ore, new techniques must be deployed to take the best advantage of the limited resources. For instance, it was proposed to use a translation lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation systems (SMT) that participated in the 2005 I WSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram prefix and suffix LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the B TEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the I WSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator operating on this space [8]. Since the re"
2006.iwslt-papers.2,W05-0821,0,0.0268435,", it was proposed to use a translation lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation systems (SMT) that participated in the 2005 I WSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram prefix and suffix LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the B TEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the I WSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator operating on this space [8]. Since the resulting probability functions are smooth functions of the word representation, better generalization t"
2006.iwslt-papers.2,2003.mtsummit-papers.6,0,0.224734,"on lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation systems (SMT) that participated in the 2005 I WSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram prefix and suffix LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the B TEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the I WSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator operating on this space [8]. Since the resulting probability functions are smooth functions of the word representation, better generalization to unknown n-grams can be expected. A"
2006.iwslt-papers.2,P06-2093,1,0.833984,"epresentation, better generalization to unknown n-grams can be expected. A neural network can be used to simultaneously learn the projection of the words onto the continuous space and to estimate the ngram probabilities. This is still a n-gram approach, but the LM posterior probabilities are ”interpolated” for any possible context of length n-1 instead of backing-off to shorter contexts. This approach was successfully used in large vocabulary continuous speech recognition [9], and initial experiments have shown that it can be used to improve a word-based statistical machine translation system [10]. Here, the continuous space LM is applied the first time to a state-of-the-art phrase-based SMT system. Translation of four different languages is considered: Mandarin, Japanese, Arabic and Italian to English. These languages exhibit very different characteristics, e.g. with respect to word order, which may affect the role of the target LM, although a reordering model is used in the SMT systems. We also investigate the use of the continuous space LM in a SMT system based on bilingual n-grams. This paper is organized as follows. In the next section we first describe the baseline statistical ma"
2006.iwslt-papers.2,2005.iwslt-1.23,1,0.849141,"ine statistical machine translation systems. Section 3 presents the architecture and training algorithms of the continuous space LM and section 4 summarizes the experimental evaluation. The paper concludes with a discussion of future research directions. 166 2. Baseline systems During the last few years, the use of context in SMT systems has provided great improvements in translation. SMT has evolved from the original word-based approach to phrasebased translation systems. In parallel to the phrase-based approach, the use of bilingual n-grams gives comparable results, as shown by Crego et al. [11]. Two basic issues differentiate the n-gram-based system from the phrase-based: training data are monotonically segmented into bilingual units; and the model considers n-gram probabilities rather than relative frequencies. This translation approach is described in detail by Mari˜no et al. [12]. Both systems follow a maximum entropy approach, in which a log-linear combination of multiple models is implemented, as an alternative to the source-channel approach: This simplifies the introduction of several additional models explaining the translation process, as the search becomes: e∗ = arg max p(e"
2006.iwslt-papers.2,2005.mtsummit-papers.36,1,0.808081,"few years, the use of context in SMT systems has provided great improvements in translation. SMT has evolved from the original word-based approach to phrasebased translation systems. In parallel to the phrase-based approach, the use of bilingual n-grams gives comparable results, as shown by Crego et al. [11]. Two basic issues differentiate the n-gram-based system from the phrase-based: training data are monotonically segmented into bilingual units; and the model considers n-gram probabilities rather than relative frequencies. This translation approach is described in detail by Mari˜no et al. [12]. Both systems follow a maximum entropy approach, in which a log-linear combination of multiple models is implemented, as an alternative to the source-channel approach: This simplifies the introduction of several additional models explaining the translation process, as the search becomes: e∗ = arg max p(e|f ) X λi hi (e, f ))} = arg max{exp( e (1) i where f and e are sentences in the source and target language respectively. The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set. Both the n-grambased and the"
2006.iwslt-papers.2,2006.iwslt-evaluation.18,1,0.876569,"e λi weights are typically optimized to maximize a scoring function on a development set. Both the n-grambased and the phrase-based system use a language model on the target language as feature function, i.e. P (e), but they differ in the translation model. In both cases, it is based on bilingual units. A bilingual unit consists of two monolingual fragments, where each one is supposed to be the translation of its counterpart. During training, each system learns its dictionary of bilingual fragments. Both SMT approaches were evaluated in I WSLT’06 evaluation and they are described in detail in [13, 14]. Therefore, we only give a short summary in the following two sections. • no smaller tuples can be extracted without violating the previous constraints. As a consequence of these constraints, only one segmentation is possible for a given sentence pair. Two important issues regarding this translation model must be considered. First, it often occurs that a large number of single-word translation probabilities are left out of the model. This happens for all words that are always embedded in tuples containing two or more words, then no translation probability for an independent occurrence of thes"
2006.iwslt-papers.2,takezawa-etal-2002-toward,0,0.0402314,"corresponds basically to a table look-up using hashing techniques, while a forward pass through the neural network is necessary for the continuous space LM. Very efficient optimizations are possible, in particular when n-grams with the same context can be grouped together, but a reorganization of the decoder may be necessary. More details on optimizing the neural network LM can be found in [9]. 4. Experimental Evaluation In this work we report results on the Basic Traveling Expression Corpus (B TEC). This corpus consists of typical sentences from phrase books for tourists in several languages [16]. Translation to English from four languages is considered: Mandarin, Japanese, Arabic and Italian. The reference phrase- and n-gram-based SMT systems participated in the open data track of the 2006 I WSLT evaluation [13, 14], i.e. only the supplied subset of the full B TEC corpus was used to train all the statistical models. Details on the data preprocessed as in [13, 14] are summarized in Table 1. We report results on the supplied development corpus of 489 sentences (less than 6k words) using the BLEU score with seven references translations. The scoring is case insensitive and punctuations"
2006.iwslt-papers.2,2005.iwslt-1.6,0,0.0223383,"st advantage of the limited resources. For instance, it was proposed to use a translation lexicon that was extracted by applying the Competitive Linking Algorithm on the bilingual training data [1]. By that way, important improvements in the BLEU score were obtained. With respect to language modeling, most of the statistical machine translation systems (SMT) that participated in the 2005 I WSLT evaluation used 4-gram back-off LM. Some sites reported improvements using 5-gram word or class-based LMs [2, 3], or even 9-gram prefix and suffix LMs [4]. Language model adaptation was investigated in [5]. Other interesting approaches include factored [6] or syntax-based language models [7], but to the best of our knowledge, there were not yet applied to the B TEC corpus. In this paper, we investigate if the so-called continuous space language model can be used in a state-of-the-art statistical machine translation system for the I WSLT task. The basic idea of the continuous space LM, also called neural network LM, is to project the word indices onto a continuous space and to use a probability estimator operating on this space [8]. Since the resulting probability functions are smooth functions"
2006.iwslt-papers.2,2005.iwslt-1.11,0,\N,Missing
2007.iwslt-1.26,2006.iwslt-papers.2,1,0.882609,"4]. Efforts have been focused on improving translation according to human evaluation by further developing different stages of the SMT system: alignment and rescoring. As in previous years, we aligned the training corpus using Giza++ software. However, instead of keeping the default parameters, we performed a minimum translation error training procedure to adjust Giza++ smoothing parameters to the task. This procedure had been successful with an alignment system based on discriminative training [5]. For the rescoring we incorporate a neural network language model as previously experienced in [6]. The neural network language model mainly is able to produce a better generalization in the translation system. This paper is organized as follows. Section 2 briefly reviews last year’s system, including tuple definition and extraction, translation model and feature functions, decoding tool and reordering and optimization criterion. Section 3 describes the alignment translation-minimum-error training procedure. Section 4 focuses on rescoring using a neural language model (NNLM). Next, Section 5 reports on all experiments carried out from Arabic and Chinese into English for IWSLT 2007. Finally"
2007.iwslt-1.26,N04-1033,0,0.0623584,"ls, our translation model is estimated as a standard n-gram model of a bilingual language expressed in tuples. In this way, it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(S, T ) = K Y p((˜ s, t˜)k |(˜ s, t˜)k−N +1 , ..., (˜ s, t˜)k−1 ) (1) k=1 where s refers to source, t to target, and (˜ s, t˜)k to the k th tuple of a given bilingual sentence pair segmented in K tuples. 2.2. Tuple extraction Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints However, when dealing with pairs of languages with nonmonotonic word order, a certain reordering strategy is required to extract more reusable units (less sparse). Hence, we allow the source words to be reordered before extracting translation units from training sentence pairs by following the word-to-word alignments. The unfolding technique is fully described in [10]. Figure 1 shows an ex"
2007.iwslt-1.26,2005.mtsummit-papers.37,1,0.856996,"following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints However, when dealing with pairs of languages with nonmonotonic word order, a certain reordering strategy is required to extract more reusable units (less sparse). Hence, we allow the source words to be reordered before extracting translation units from training sentence pairs by following the word-to-word alignments. The unfolding technique is fully described in [10]. Figure 1 shows an example of tuple unfolding compared to the monotonic extraction. The unfolding technique produces a different bilingual n-gram language model with reordered source words. where tn refers to the nth word in the partial translation hypothesis T . Usually, this feature is accompanied by a word bonus model based on sentence length, compensating the target language model preference for short sentences (in number of target words). This bonus depends on the number of target words in the partial hypothesis, denoted as: pW P (T ) = exp(number of words in T ). The third and fourth fe"
2007.iwslt-1.26,2006.iwslt-papers.5,1,0.867108,"ChineseEnglish task, a secondary run was performed with a rescoring module, as described in Sections 4 and 5.3.2. 2.5. Feature Weights Optimization To tune the weight of each feature function in the SMT system, we used the Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm [12]. SPSA is a stochastic implementation of the conjugate gradient method which requires only two evaluations of the objective function in each iteration, regardless of the dimension of the optimization problem. It was observed to be more robust than the Downhill Simplex method when tuning SMT coefficients [13]. The SPSA procedure is in the general recursive stochastic approximation form: ˆ k+1 = λ ˆ k − ak g ˆk ) ˆk (λ λ (5) lation tuples (as no word within a tuple can be linked to a word out of it [9]). Starting from the monotonic graph, each sequence of input POS tags fulfilling a source-side rewrite rule implies the addition of a reordering arc (which encodes the reordering detailed in the target-side of the rule). Figure 2 shows how three rewrite rules applied over an input sentence extend the search graph given the reordering patterns that match the source POS tag sequence 1 . ˆ k ) is the esˆ"
2007.iwslt-1.26,N07-2022,1,0.823419,"o phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in [3, 4]. Efforts have been focused on improving translation according to human evaluation by further developing different stages of the SMT system: alignment and rescoring. As in previous years, we aligned the training corpus using Giza++ software. However, instead of keeping the default parameters, we performed a minimum translation error training procedure to adjust Giza++ smoothing parameters to the task. This procedure had been successful with an alignment system based on discriminative training [5]. For the rescoring we incorporate a neural network language model as previously experienced in [6]. The neural network language model mainly is able to produce a better generalization in the translation system. This paper is organized as follows. Section 2 briefly reviews last year’s system, including tuple definition and extraction, translation model and feature functions, decoding tool and reordering and optimization criterion. Section 3 describes the alignment translation-minimum-error training procedure. Section 4 focuses on rescoring using a neural language model (NNLM). Next, Section 5"
2007.iwslt-1.26,N06-2013,0,0.076004,"fluency and METEOR is well correlated to adequacy [4], we supposed that adding all references was beneficial to monolingual language models but not to the bilingual language model. Table 2: Chinese→English corpus statistics. 5.2. Data Preprocessing For all language pairs, training sentences were split by using final dots on both sides of the bilingual text (when the number of dots was equal), increasing the number of sentences and reducing its length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. Arabic Following a similar approach to that in [16], we used the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic uni-gram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The D3 scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available"
2007.iwslt-1.26,W03-1730,0,0.018621,"ts length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. Arabic Following a similar approach to that in [16], we used the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic uni-gram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The D3 scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal"
2007.iwslt-1.26,A00-1031,0,0.0277074,"he MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic uni-gram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The D3 scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal method and Och’s refined method [20]), as well as concatenations of various of these combinations. Using stems and classes in the alignment improved translation results i"
2007.iwslt-1.26,E99-1010,0,0.0816899,"ion produces Bies POS tags on all taggable tokens. 5.2.2. Chinese Chinese preprocessing included re-segmentation using ICTCLAS [17] and POS tagging using the freely available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal method and Och’s refined method [20]), as well as concatenations of various of these combinations. Using stems and classes in the alignment improved translation results in all cases, and the best combination for the system with pattern-based reordering was the union6 . At the end, the best alignment configuration for our baseline system was obtained with Giza++ software, running respectively 5, 5, 3 and 3 iterations of models 1, HMM, 3 and 4, using English stems and 50"
2007.iwslt-1.26,J03-1002,0,0.00791937,"y available Stanford Parser4 . 5.2.3. English English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [18]. For alignment purpose only (of the ZhEn system), the English corpus was stemmed using the Snowball stemmer 5 , based on Porter’s algorithm. 5.3. Results 5.3.1. Alignment In the ZhEn system development work, we tried to improve word alignment by stemming the English corpus and make use of classes [19]. We also performed several combinations of source-target and target-source GIZA++ alignments (union, growing forward diagonal method and Och’s refined method [20]), as well as concatenations of various of these combinations. Using stems and classes in the alignment improved translation results in all cases, and the best combination for the system with pattern-based reordering was the union6 . At the end, the best alignment configuration for our baseline system was obtained with Giza++ software, running respectively 5, 5, 3 and 3 iterations of models 1, HMM, 3 and 4, using English stems and 50 classes and taking the union of source-target and target-source alignments. Table 3 show results for the new features of this year’s system. We optimized the foll"
2008.amta-papers.6,J96-1002,0,0.0194792,"volved from the initial word-based translation models to more advanced models that take the context surrounding the words into account, i.e. the so-called phrase-based system (Koehn et al., 2003). The phrase-based model is usually the main feature in a log-linear framework, reminiscent of the maximum entropy modeling approach. One of the best known reordering approach is permitting arbitrary word-reorderings. However, the exact decoding problem was shown to be NPhard (Knight, 1999). To solve this problem, several approaches have defined different kinds of constraints as for example heuristic (Berger et al., 1996) (Crego et al., 2005) or linguistic (Wu, 1996). Other approaches try to reorder the source language in a way that better matches the target language (Popovic and Ney, 2006) (Collins et al., 2005). 82 A natural evolution of the source reordering strategies consists in using a word graph, containing the N -best reordering decisions, instead of the single-best used in the above strategies. The reordering problem is equally approached by alleviating the difficulty of needing highly accurate reordering decisions in preprocessing. The final decision is delayed, to be subsequently in the global searc"
2008.amta-papers.6,P05-1066,0,0.201459,"Missing"
2008.amta-papers.6,W06-1609,1,0.929925,"Missing"
2008.amta-papers.6,2005.iwslt-1.23,1,0.903582,"Missing"
2008.amta-papers.6,W05-0831,0,0.155057,"005). 82 A natural evolution of the source reordering strategies consists in using a word graph, containing the N -best reordering decisions, instead of the single-best used in the above strategies. The reordering problem is equally approached by alleviating the difficulty of needing highly accurate reordering decisions in preprocessing. The final decision is delayed, to be subsequently in the global search, where all the information is then available. Inspired by (Knight and Al-Onaizan, 1998), they permute the source sentence to provide a source input graph that extends the search graph. In (Kanthak et al., 2005), they train the system using a monotonized source corpora and they translate the test set allowing source reorderings which are limited by constraints such as IBM or ITG. Similarly in (Crego and Mari˜no, 2007; Zhang et al., 2007), reordering is addressed through a source input graph. In this case, the reordering hypotheses are defined from a set of linguistically motivated rules (either using Part of Speech; chunks; or parse trees). Previous work (Costa-juss`a and Fonollosa, 2006) presents the SMR approach which is based on using the powerful SMT techniques to generate a reordered source inpu"
2008.amta-papers.6,knight-al-onaizan-1998-translation,0,0.0262969,"hes try to reorder the source language in a way that better matches the target language (Popovic and Ney, 2006) (Collins et al., 2005). 82 A natural evolution of the source reordering strategies consists in using a word graph, containing the N -best reordering decisions, instead of the single-best used in the above strategies. The reordering problem is equally approached by alleviating the difficulty of needing highly accurate reordering decisions in preprocessing. The final decision is delayed, to be subsequently in the global search, where all the information is then available. Inspired by (Knight and Al-Onaizan, 1998), they permute the source sentence to provide a source input graph that extends the search graph. In (Kanthak et al., 2005), they train the system using a monotonized source corpora and they translate the test set allowing source reorderings which are limited by constraints such as IBM or ITG. Similarly in (Crego and Mari˜no, 2007; Zhang et al., 2007), reordering is addressed through a source input graph. In this case, the reordering hypotheses are defined from a set of linguistically motivated rules (either using Part of Speech; chunks; or parse trees). Previous work (Costa-juss`a and Fonollo"
2008.amta-papers.6,J99-4005,0,0.0460704,"in the TC-STAR task (Es/En) at a relatively low computational cost. 1 Introduction Statistical machine translation (SMT) has evolved from the initial word-based translation models to more advanced models that take the context surrounding the words into account, i.e. the so-called phrase-based system (Koehn et al., 2003). The phrase-based model is usually the main feature in a log-linear framework, reminiscent of the maximum entropy modeling approach. One of the best known reordering approach is permitting arbitrary word-reorderings. However, the exact decoding problem was shown to be NPhard (Knight, 1999). To solve this problem, several approaches have defined different kinds of constraints as for example heuristic (Berger et al., 1996) (Crego et al., 2005) or linguistic (Wu, 1996). Other approaches try to reorder the source language in a way that better matches the target language (Popovic and Ney, 2006) (Collins et al., 2005). 82 A natural evolution of the source reordering strategies consists in using a word graph, containing the N -best reordering decisions, instead of the single-best used in the above strategies. The reordering problem is equally approached by alleviating the difficulty o"
2008.amta-papers.6,N03-1017,0,0.00898152,"erful techniques of the SMT systems to solve reordering problems. Here, the novelties yield in: (1) using the SMR approach in a SMT phrase-based system, (2) adding a feature function in the SMR step, and (3) analyzing the reordering hypotheses at several stages. Coherent improvements are reported in the TC-STAR task (Es/En) at a relatively low computational cost. 1 Introduction Statistical machine translation (SMT) has evolved from the initial word-based translation models to more advanced models that take the context surrounding the words into account, i.e. the so-called phrase-based system (Koehn et al., 2003). The phrase-based model is usually the main feature in a log-linear framework, reminiscent of the maximum entropy modeling approach. One of the best known reordering approach is permitting arbitrary word-reorderings. However, the exact decoding problem was shown to be NPhard (Knight, 1999). To solve this problem, several approaches have defined different kinds of constraints as for example heuristic (Berger et al., 1996) (Crego et al., 2005) or linguistic (Wu, 1996). Other approaches try to reorder the source language in a way that better matches the target language (Popovic and Ney, 2006) (C"
2008.amta-papers.6,popovic-ney-2006-pos,0,0.0136571,"stem (Koehn et al., 2003). The phrase-based model is usually the main feature in a log-linear framework, reminiscent of the maximum entropy modeling approach. One of the best known reordering approach is permitting arbitrary word-reorderings. However, the exact decoding problem was shown to be NPhard (Knight, 1999). To solve this problem, several approaches have defined different kinds of constraints as for example heuristic (Berger et al., 1996) (Crego et al., 2005) or linguistic (Wu, 1996). Other approaches try to reorder the source language in a way that better matches the target language (Popovic and Ney, 2006) (Collins et al., 2005). 82 A natural evolution of the source reordering strategies consists in using a word graph, containing the N -best reordering decisions, instead of the single-best used in the above strategies. The reordering problem is equally approached by alleviating the difficulty of needing highly accurate reordering decisions in preprocessing. The final decision is delayed, to be subsequently in the global search, where all the information is then available. Inspired by (Knight and Al-Onaizan, 1998), they permute the source sentence to provide a source input graph that extends the"
2008.amta-papers.6,W07-0721,1,0.785303,"Missing"
2008.amta-papers.6,P96-1021,0,0.0368004,"more advanced models that take the context surrounding the words into account, i.e. the so-called phrase-based system (Koehn et al., 2003). The phrase-based model is usually the main feature in a log-linear framework, reminiscent of the maximum entropy modeling approach. One of the best known reordering approach is permitting arbitrary word-reorderings. However, the exact decoding problem was shown to be NPhard (Knight, 1999). To solve this problem, several approaches have defined different kinds of constraints as for example heuristic (Berger et al., 1996) (Crego et al., 2005) or linguistic (Wu, 1996). Other approaches try to reorder the source language in a way that better matches the target language (Popovic and Ney, 2006) (Collins et al., 2005). 82 A natural evolution of the source reordering strategies consists in using a word graph, containing the N -best reordering decisions, instead of the single-best used in the above strategies. The reordering problem is equally approached by alleviating the difficulty of needing highly accurate reordering decisions in preprocessing. The final decision is delayed, to be subsequently in the global search, where all the information is then available"
2008.amta-papers.6,W07-0401,0,0.0165022,"approached by alleviating the difficulty of needing highly accurate reordering decisions in preprocessing. The final decision is delayed, to be subsequently in the global search, where all the information is then available. Inspired by (Knight and Al-Onaizan, 1998), they permute the source sentence to provide a source input graph that extends the search graph. In (Kanthak et al., 2005), they train the system using a monotonized source corpora and they translate the test set allowing source reorderings which are limited by constraints such as IBM or ITG. Similarly in (Crego and Mari˜no, 2007; Zhang et al., 2007), reordering is addressed through a source input graph. In this case, the reordering hypotheses are defined from a set of linguistically motivated rules (either using Part of Speech; chunks; or parse trees). Previous work (Costa-juss`a and Fonollosa, 2006) presents the SMR approach which is based on using the powerful SMT techniques to generate a reordered source input for an Ngram-based SMT system both in training and decoding steps. One step further, (R. Costa-juss`a and R. Fonollosa, 2007) shows how the SMR system generates a weighted reordering graph, allowing the SMT decoder to make the r"
2009.eamt-1.8,carreras-etal-2004-freeling,0,0.0189299,"Missing"
2009.eamt-1.8,C00-2162,0,0.168791,"anslator with open-source tools as long as a parallel corpus is available. If the languages involved in the translation belong to the same linguistic family, the translation quality can be surprisingly nice. Furthermore, one of the most attractive reasons to build an statistical system instead of an standard rule-based system is the little human effort required. Theoretically, when using SMT, no linguistic knowledge is required. In practice, once the system is built and specially, if the translation quality is high, then the linguistic knowledge becomes necessary to make further improvements (Niessen and Ney, 2000; Popovi´c and Ney, 2004; Popovi´c et al., 2006). In fact, the main question that arose at the beginning of this work was: which are the steps to follow when the intention is to improve a high quality statistical translation? Let’s consider a high quality statistical translation defined as the system which has a BLEU 2 Ngram-based statistical translation system An Ngram-based SMT system regards translation as a stochastic process. In recent systems, such an approach is faced using a general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented"
2009.eamt-1.8,popovic-ney-2004-towards,0,0.0695144,"Missing"
2009.eamt-1.8,W06-3101,1,0.938373,"Missing"
2009.eamt-1.8,popovic-ney-2006-pos,0,\N,Missing
2009.eamt-1.8,J06-4004,1,\N,Missing
2009.eamt-1.8,P03-1021,0,\N,Missing
2009.iwslt-evaluation.3,2007.tmi-papers.28,0,0.0210389,"ith the - 24 - novel technique. Section 6 discusses the results obtained on the evaluation campaign and, finally, Section 7 presents the conclusions. 2. Related work The phrase-based translation model allows to introduce both source and target context information in comparison to the word-based translation model. However, the idea of introducing context information is simplified in the phrase-based systems given that all training sentences contribute equally to the final translation. More complex works which introduce source context information can be found in the SMT literature. For example, [10, 4] incorporate source language context using neighbouring words, part-of-speech tags and/or supertags. They use a memory-based classification approach to obtain the probability for the given additional contexts with the source phrase. Works such as [2] embed context-rich approaches from Word Sense Disambiguation methods. Other related works focus on extending the translation and target language model using neural networks [8] which aims at smoothing both the translation and target language model in order to use the n-grams more adequate in the translated sentence. 3. Phrase-based Baseline System"
2009.iwslt-evaluation.3,takezawa-etal-2002-toward,0,0.0155654,"re more, optimized with a modified mert algorithm which translates one sentence at a time. The resulting increment in translation time (i.e. the optimization time as well) is around three times with respect to the translation time of the standard Moses baseline system. The proposed methodology is graphically illustrated in Figure 1. - 25 - Figure 1: Example of source context information methodology. 5. Experiments We participated on the Arabic and Chinese to English BTEC task (correct recognition results). Experiments with the Arabic and Chinese to English MT were carried out on the BTEC data [11]. Corpus statistics are shown Tables 1, 2 and 3. Model weights were tuned with the 2006 development corpus (Dev6), containing 489 sentences and 6 reference translations. The internal test set was the 2007 development set (486 sentences and 16 reference translations), according to which we make a decision about better or worse system performance. Weights obtained in the optimization where used as well for the evaluation test. However, in the evaluation campaign, we concatenated the training, development and test sets from Table 1, 2, 3 and we used the concatenation as training data for translat"
2009.iwslt-evaluation.3,D07-1007,0,0.209434,"formation in comparison to the word-based translation model. However, the idea of introducing context information is simplified in the phrase-based systems given that all training sentences contribute equally to the final translation. More complex works which introduce source context information can be found in the SMT literature. For example, [10, 4] incorporate source language context using neighbouring words, part-of-speech tags and/or supertags. They use a memory-based classification approach to obtain the probability for the given additional contexts with the source phrase. Works such as [2] embed context-rich approaches from Word Sense Disambiguation methods. Other related works focus on extending the translation and target language model using neural networks [8] which aims at smoothing both the translation and target language model in order to use the n-grams more adequate in the translated sentence. 3. Phrase-based Baseline System The basic idea of phrase-based translation is to segment the given source sentence into units (hereinafter called phrases), then translate each phrase and finally compose the target sentence from these phrase translations. Basically, a bilingual phr"
2009.iwslt-evaluation.3,N04-4026,0,0.0277623,"stems were phrase-based systems as described in Section 3 and both were based on MOSES open source package [6]. IBM word reordering constraints [1] were applied during decoding to reduce the computational complexity. The other models and feature functions employed by MOSES decoder were: • TM(s), direct and inverse phrase/word based TM (10 words as maximum length per phrase). • Distortion model, which assigns a cost linear to the reordering distance, while the cost is based on the number of source words which are skipped when translating a new source phrase. • Lexicalized word reordering model [5, 12]. • Word and phrase penalties, which count the number of words and phrases in the target string. • Target-side LM (4-gram). Table 2: Chinese training, development, test and evaluation sets. The TM and reordering model were trained using the standard MOSES tools. Weights of feature functions were tuned by using the optimization tools from the MOSES package. The search operation was accomplished by MOSES decoder. In the primary submission, we introduced context information as explained in Section 4. Several tools provided in the Moses package were modified in order to introduce the novel techniq"
2009.iwslt-evaluation.3,N06-2013,0,0.0276593,"ions. The internal test set was the 2007 development set (486 sentences and 16 reference translations), according to which we make a decision about better or worse system performance. Weights obtained in the optimization where used as well for the evaluation test. However, in the evaluation campaign, we concatenated the training, development and test sets from Table 1, 2, 3 and we used the concatenation as training data for translating the evaluation set. 5.1. Arabic data One first run we participated was the Arabic to English BTEC translation task. We used a similar approach to that shown in [3], namely the MADA+TOKAN system for disambiguation and tokenization. For disambiguation only diacritic unigram statistics were employed. For tokenization we used the D3 scheme with -TAGBIES option. The scheme splits the following set of enclitics: w+, f+, b+, k+, l+, Al+ and pronominal enclitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. Table 1 gives details about the training, developement and test set that we used to make experiments. The first column shows the Arabic corpus statistics without processing and the second column shows the Arabic corpus statistics after"
2009.iwslt-evaluation.3,2009.eamt-1.32,0,0.0144978,"ith the - 24 - novel technique. Section 6 discusses the results obtained on the evaluation campaign and, finally, Section 7 presents the conclusions. 2. Related work The phrase-based translation model allows to introduce both source and target context information in comparison to the word-based translation model. However, the idea of introducing context information is simplified in the phrase-based systems given that all training sentences contribute equally to the final translation. More complex works which introduce source context information can be found in the SMT literature. For example, [10, 4] incorporate source language context using neighbouring words, part-of-speech tags and/or supertags. They use a memory-based classification approach to obtain the probability for the given additional contexts with the source phrase. Works such as [2] embed context-rich approaches from Word Sense Disambiguation methods. Other related works focus on extending the translation and target language model using neural networks [8] which aims at smoothing both the translation and target language model in order to use the n-grams more adequate in the translated sentence. 3. Phrase-based Baseline System"
2009.iwslt-evaluation.3,2005.iwslt-1.8,0,0.0324938,"stems were phrase-based systems as described in Section 3 and both were based on MOSES open source package [6]. IBM word reordering constraints [1] were applied during decoding to reduce the computational complexity. The other models and feature functions employed by MOSES decoder were: • TM(s), direct and inverse phrase/word based TM (10 words as maximum length per phrase). • Distortion model, which assigns a cost linear to the reordering distance, while the cost is based on the number of source words which are skipped when translating a new source phrase. • Lexicalized word reordering model [5, 12]. • Word and phrase penalties, which count the number of words and phrases in the target string. • Target-side LM (4-gram). Table 2: Chinese training, development, test and evaluation sets. The TM and reordering model were trained using the standard MOSES tools. Weights of feature functions were tuned by using the optimization tools from the MOSES package. The search operation was accomplished by MOSES decoder. In the primary submission, we introduced context information as explained in Section 4. Several tools provided in the Moses package were modified in order to introduce the novel techniq"
2009.iwslt-evaluation.3,P07-2045,0,0.0054081,"21 820 - Table 3: English training, development, test and evaluation sets before the preprocessing (English) and after (English’) Table 1: Arabic training, development, test and evaluation sets before the preprocessing (Arabic) and after (Arabic’) Training Sentences Words Vocabulary Sentences Words Vocabulary Sentences Words Vocabulary Chinese 21,484 182,2k 8,773 489 3,169 881 507 3,352 888 469 3,019 859 a contrastive system we submitted the MOSES-based system. Both Machine Translation Systems were phrase-based systems as described in Section 3 and both were based on MOSES open source package [6]. IBM word reordering constraints [1] were applied during decoding to reduce the computational complexity. The other models and feature functions employed by MOSES decoder were: • TM(s), direct and inverse phrase/word based TM (10 words as maximum length per phrase). • Distortion model, which assigns a cost linear to the reordering distance, while the cost is based on the number of source words which are skipped when translating a new source phrase. • Lexicalized word reordering model [5, 12]. • Word and phrase penalties, which count the number of words and phrases in the target string. • Targ"
2009.iwslt-evaluation.3,D07-1045,1,0.856591,"ng sentences contribute equally to the final translation. More complex works which introduce source context information can be found in the SMT literature. For example, [10, 4] incorporate source language context using neighbouring words, part-of-speech tags and/or supertags. They use a memory-based classification approach to obtain the probability for the given additional contexts with the source phrase. Works such as [2] embed context-rich approaches from Word Sense Disambiguation methods. Other related works focus on extending the translation and target language model using neural networks [8] which aims at smoothing both the translation and target language model in order to use the n-grams more adequate in the translated sentence. 3. Phrase-based Baseline System The basic idea of phrase-based translation is to segment the given source sentence into units (hereinafter called phrases), then translate each phrase and finally compose the target sentence from these phrase translations. Basically, a bilingual phrase is a pair of m source words and n target words. For extraction from a bilingual word aligned training corpus, two additional constraints are considered: 1. the words are con"
2010.eamt-1.12,popovic-ney-2006-pos,0,0.0248284,"Missing"
2010.eamt-1.12,W06-3101,1,0.93751,"Missing"
2010.eamt-1.12,2009.eamt-1.8,1,0.756731,"Missing"
2010.eamt-1.12,C00-2162,0,0.0200814,"mpute another probability to the translation units based on the probability of translating word per word of the unit. The probability estimated by lexical models tends to be in some situations less sparse than the probability given directly by the translation model. Many additional feature functions can also be introduced in the SMT framework to improve the translation, like the word or the phrase bonus. Although SMT systems provide, in general, good performance, it has been demonstrated in recent papers that the addition of linguistic information can be highly useful in this kind of systems (Niessen and Ney, 2000; Popovi´c and Ney, 2004; Popovi´c and Ney, 2006; Popovi´c et al., 2006). automatic translation meets their standards. Large amounts of bilingual texts are needed to further develop new systems. N-II3 , developed at the UPC mainly for the Spanish-Catalan pair, is an engine based on an Ngram translation model integrated in an optimized log-linear combination of additional features. Although it is mainly statistical, additional linguistic rules are included in order to solve some errors caused by the statistical translation, such as ambiguity in adjective and possessive pronouns, orthographic er"
2010.eamt-1.12,P03-1021,0,0.0146839,"the overlap in phrases. Thus, given a source string sJ1 = s1 . . . sj . . . sJ to be translated into a target string tI1 = t1 . . . ti . . . tI , the aim is to choose, among all possible target strings, the string with the highest probability: t˜I1 = argmax P (tI1 |sJ1 ) tI1 where I and J are the number of words of the target and source sentence, respectively. The first SMT systems were reformulated using Bayes’ rule. In recent systems, such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och, 2003). This approach leads to maximising a linear combination of feature functions: t˜ = argmax t nP M o m=1 λm hm (t, s) . Given a target sentence and a foreign sentence, the translation model tries to assign a probability that tI1 generates sJ1 . While these probabilities can be estimated by thinking about how each individual word is translated, modern statistical MT is based on the intuition that a better way to compute these probabilities is by considering the behavior of phrases (sequences of words). The intuition of phrase-based statistical MT is to use phrases as well as single words as the"
2010.eamt-1.12,P02-1040,0,0.0928963,"human evaluation based on the expert knowledge about the errors encountered at several linguistic levels: orthographic, morphological, lexical, semantic and syntactic. The results obtained in these experiments show that some linguistic errors could have more influence than other at the time of performing a perceptual evaluation. 1 Introduction One of the aims in the research community is to find accurate evaluation methods that allow analyzing and comparing the performance of these translation systems. The most commonly used evaluation methods are the standard automatic measures such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover and Dorr, 2006) and WER (McCowan, 2004 et al.), as well as the use of human native evaluators that analyze and compare translated sentences according to a general perception of the linguistic quality. In this paper, these evaluation methods are used to evaluate and compare two translation systems based on the statistical approaches in the Catalanto-Spanish language pair: Google Translate and Nc 2010 European Association for Machine Translation. II; this one developed at the Universitat Polit`ecnica de Catalunya (UPC). In addition, a new human evaluation m"
2010.eamt-1.12,popovic-ney-2004-towards,0,0.0153117,"Missing"
2010.eamt-1.12,2006.amta-papers.25,0,0.0482924,"he errors encountered at several linguistic levels: orthographic, morphological, lexical, semantic and syntactic. The results obtained in these experiments show that some linguistic errors could have more influence than other at the time of performing a perceptual evaluation. 1 Introduction One of the aims in the research community is to find accurate evaluation methods that allow analyzing and comparing the performance of these translation systems. The most commonly used evaluation methods are the standard automatic measures such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover and Dorr, 2006) and WER (McCowan, 2004 et al.), as well as the use of human native evaluators that analyze and compare translated sentences according to a general perception of the linguistic quality. In this paper, these evaluation methods are used to evaluate and compare two translation systems based on the statistical approaches in the Catalanto-Spanish language pair: Google Translate and Nc 2010 European Association for Machine Translation. II; this one developed at the Universitat Polit`ecnica de Catalunya (UPC). In addition, a new human evaluation method is applied, based on an expert linguistic evalua"
2010.eamt-1.12,vilar-etal-2006-error,0,0.775854,"Missing"
2010.eamt-1.12,W09-0401,0,\N,Missing
2010.eamt-1.12,J06-4004,1,\N,Missing
2010.eamt-1.17,W06-1009,0,0.0232147,"ure function which is ‘1’ in case of appearing in both segmentations or ’0’ in the opposite case. 5 Experimental framework The phrase-based system used in this paper is based on the well-known MOSES toolkit, which is nowadays considered as a state-of-theart SMT system (Koehn et al., 2007). The training and weights tuning procedures are explained in details in the above-mentioned publication, as well as, on the MOSES web page: http://www.statmt.org/moses/. 5.1 Corpus statistics Experiments were carried out on the English to Spanish Bible task, which have been proven to be a valid NLP resource (Chew et al., 2006). The main advantages of using this corpus are that it is the world’s most translated book, with translations in over 2,100 languages (often, multiple translations per language) and easy availability, often in electronic form and in the public domain; it covers a variety of literary styles including narrative, poetry, and correspondence; great care is taken over the translations; it has a standard structure which Figure 4: Example of the phrase extraction process in the CONCAT approach. New phrases added by the collocation-based system are marked with a ∗∗. allows parallel alignment on a verse"
2010.eamt-1.17,W06-2402,1,0.789473,"ociation for Machine Translation. Introducing chunking in the standard phrasebased SMT system is a relatively frequent study (Zhou et al., 2004; Wang et al., 2002; Ma et al., 2007). Chunking may be used either to improve reordering or to enhance the translation table. For example, authors in (Zhang et al., 2007) present a shallow chunking based on syntactic information and they use the chunks to reorder phrases. Other studies report the impact on the quality of word alignment and in translation after using various types of multi-word expressions which can be regarded as a type of chunks, see (Lambert and Banchs, 2006) or sub-sentential sequences (Macken et al., 2008; Groves and Way, 2005). Chunking is usually performed on a syntactic or semantic basis which forces to have a tool for parsing or similar. We propose to introduce the collocation segmentation developed by (Daudaravicius, 2009) which is language independent. This collocation segmentation was applied in keyword assigment task and a high classification improvement was achieved (Daudaravicius, 2010). We use this collocation segmentation technique to enrich the phrase translation table. The phrase translation table is composed of phrase units which"
2010.eamt-1.17,2007.tmi-papers.14,0,0.0199906,"is the phrase-based system (Koehn et al., 2003) which implements a maximum entropy approach based on a combination of feature functions. The Moses system (Koehn et al., 2007) is an implementation of this phrase-based machine translation approach. An input sentence is first split into sequences of words (so-called phrases), which are then mapped one-to-one to target phrases using a large phrase translation table. c 2010 European Association for Machine Translation. Introducing chunking in the standard phrasebased SMT system is a relatively frequent study (Zhou et al., 2004; Wang et al., 2002; Ma et al., 2007). Chunking may be used either to improve reordering or to enhance the translation table. For example, authors in (Zhang et al., 2007) present a shallow chunking based on syntactic information and they use the chunks to reorder phrases. Other studies report the impact on the quality of word alignment and in translation after using various types of multi-word expressions which can be regarded as a type of chunks, see (Lambert and Banchs, 2006) or sub-sentential sequences (Macken et al., 2008; Groves and Way, 2005). Chunking is usually performed on a syntactic or semantic basis which forces to ha"
2010.eamt-1.17,C08-1067,0,0.015889,"in the standard phrasebased SMT system is a relatively frequent study (Zhou et al., 2004; Wang et al., 2002; Ma et al., 2007). Chunking may be used either to improve reordering or to enhance the translation table. For example, authors in (Zhang et al., 2007) present a shallow chunking based on syntactic information and they use the chunks to reorder phrases. Other studies report the impact on the quality of word alignment and in translation after using various types of multi-word expressions which can be regarded as a type of chunks, see (Lambert and Banchs, 2006) or sub-sentential sequences (Macken et al., 2008; Groves and Way, 2005). Chunking is usually performed on a syntactic or semantic basis which forces to have a tool for parsing or similar. We propose to introduce the collocation segmentation developed by (Daudaravicius, 2009) which is language independent. This collocation segmentation was applied in keyword assigment task and a high classification improvement was achieved (Daudaravicius, 2010). We use this collocation segmentation technique to enrich the phrase translation table. The phrase translation table is composed of phrase units which generally are extracted from a word aligned paral"
2010.eamt-1.17,J96-1001,0,0.487108,"Missing"
2010.eamt-1.17,J93-1007,0,0.933659,"same phrase. This paper is organized as follows. First, we detail the different collocation segmentation techniques proposed. Secondly, we make a brief description of the phrase-based SMT system and how we introduce the collocation segmentation to improve the phrase-based SMT system. Then, we present experiments performed in an standard phrase-based system comparing the phrase extraction. Finally, we present the conclusions. 2 Collocation segmentation The Dice score is used to measure the association strength of two words. This score is used, for instance, in the collocation compiler XTract (Smadja, 1993) and in the lexicon extraction system Champollion (Smadja and Hatzivassiloglou, 1996). Dice is defined as follows: Dice(x; y) = 2f (x, y) f (x) + f (y) where f (x, y) is the frequency of co-occurrence of x and y, and f (x) and f (y) the frequencies of occurrence of x and y anywhere in the text. If x and y tend to occur in conjunction, their Dice score will be high. The text is seen as a changing curve of the word associativity values (see Figure 1 and Figure 2). The collocation segmentation is the process of detecting the boundaries of collocation segments within a text. A collocation segment"
2010.eamt-1.17,W00-0726,0,0.176514,"Missing"
2010.eamt-1.17,N03-1017,0,0.0169827,"ent over 0.7 BLEU absolute) are achieved in translation quality. 1 Introduction Machine Translation (MT) investigates the use of computer software to translate text or speech from one language to another. Statistical machine translation (SMT) has become one of the most popular MT approaches given the combination of several factors. Among them, it is relatively straightforward to build an SMT system given the freely available software and, additionally, the system construction does not require of any language experts. Nowadays, one of the most popular SMT approaches is the phrase-based system (Koehn et al., 2003) which implements a maximum entropy approach based on a combination of feature functions. The Moses system (Koehn et al., 2007) is an implementation of this phrase-based machine translation approach. An input sentence is first split into sequences of words (so-called phrases), which are then mapped one-to-one to target phrases using a large phrase translation table. c 2010 European Association for Machine Translation. Introducing chunking in the standard phrasebased SMT system is a relatively frequent study (Zhou et al., 2004; Wang et al., 2002; Ma et al., 2007). Chunking may be used either to"
2010.eamt-1.17,P07-2045,0,0.0104666,"f computer software to translate text or speech from one language to another. Statistical machine translation (SMT) has become one of the most popular MT approaches given the combination of several factors. Among them, it is relatively straightforward to build an SMT system given the freely available software and, additionally, the system construction does not require of any language experts. Nowadays, one of the most popular SMT approaches is the phrase-based system (Koehn et al., 2003) which implements a maximum entropy approach based on a combination of feature functions. The Moses system (Koehn et al., 2007) is an implementation of this phrase-based machine translation approach. An input sentence is first split into sequences of words (so-called phrases), which are then mapped one-to-one to target phrases using a large phrase translation table. c 2010 European Association for Machine Translation. Introducing chunking in the standard phrasebased SMT system is a relatively frequent study (Zhou et al., 2004; Wang et al., 2002; Ma et al., 2007). Chunking may be used either to improve reordering or to enhance the translation table. For example, authors in (Zhang et al., 2007) present a shallow chunkin"
2010.eamt-1.17,2002.tmi-tutorials.2,0,0.0275678,"ch forces to have a tool for parsing or similar. We propose to introduce the collocation segmentation developed by (Daudaravicius, 2009) which is language independent. This collocation segmentation was applied in keyword assigment task and a high classification improvement was achieved (Daudaravicius, 2010). We use this collocation segmentation technique to enrich the phrase translation table. The phrase translation table is composed of phrase units which generally are extracted from a word aligned parallel corpus. Given this word alignment, an extraction of contiguous phrases is carried out (Zens et al., 2002), specifically all extracted phrases fulfill the following restrictions: all source (target) words within a phrase are aligned only to target (source) words within the same phrase. This paper is organized as follows. First, we detail the different collocation segmentation techniques proposed. Secondly, we make a brief description of the phrase-based SMT system and how we introduce the collocation segmentation to improve the phrase-based SMT system. Then, we present experiments performed in an standard phrase-based system comparing the phrase extraction. Finally, we present the conclusions. 2 C"
2010.eamt-1.17,W07-0401,0,0.0282134,"ctions. The Moses system (Koehn et al., 2007) is an implementation of this phrase-based machine translation approach. An input sentence is first split into sequences of words (so-called phrases), which are then mapped one-to-one to target phrases using a large phrase translation table. c 2010 European Association for Machine Translation. Introducing chunking in the standard phrasebased SMT system is a relatively frequent study (Zhou et al., 2004; Wang et al., 2002; Ma et al., 2007). Chunking may be used either to improve reordering or to enhance the translation table. For example, authors in (Zhang et al., 2007) present a shallow chunking based on syntactic information and they use the chunks to reorder phrases. Other studies report the impact on the quality of word alignment and in translation after using various types of multi-word expressions which can be regarded as a type of chunks, see (Lambert and Banchs, 2006) or sub-sentential sequences (Macken et al., 2008; Groves and Way, 2005). Chunking is usually performed on a syntactic or semantic basis which forces to have a tool for parsing or similar. We propose to introduce the collocation segmentation developed by (Daudaravicius, 2009) which is la"
2010.iwslt-evaluation.26,2010.eamt-1.17,1,0.915535,"ase is a pair of m source words and n target words. For extraction from a bilingual word aligned training corpus, two additional constraints are considered: similar threshold definition conditions for different associativity measures. Different associativity measures have different scale of values and it is difficult to set threshold manually. Threshold level is kept as low as possible. Higher threshold value makes shorter collocation segments and vice versa. Shorter collocation segments are more confident collocations and we may expect better translation results. Nevertheless, the results of [3] show that longer collocation segments are more preferable. There are many associativity measures that could be used to calculate the associativity values between tokens (a more comprehensive list could be found in [11]). To explore different measures we included the six following metrics: 1. Mutual Information (MI): M I(wi , wi+1 ) = N ∗ f (wi , wi+1 ) f (w1 ) + f (wi+1 ) dice(wi , wi+1 ) = 2 ∗ f (wi , wi+1 ) f (wi ) + f (wi+1 ) 1. the words are consecutive, and, 2. they are consistent with the word alignment matrix. Given the collected phrase pairs, the phrase translation probability distrib"
2010.iwslt-evaluation.26,J04-4002,0,0.0195448,"segmented data into the phrase-based system. As follows, Section 6 shows the experimental details of the system and the experiments performed with the novel technique. Finally, Section 7 presents the conclusions. 2. Related work One of the main problems in the statistical machine translation approach is how to segment the bilingual corpus in order to build the most appropriate translation dictionary. Standard phrase-based SMT systems first align the parallel corpus at the word level by using IBM probabilities and then use standard constraints (see section 3) to extract final translation units [10]. Variations of this type of segmentation can be found in [8, 1, 6]. Other approaches consist in integrating the phrase segmentation and alignment, one example is in [14] where they use the point-wise mutual information between the source and target words to identify aligned phrase pairs. In [9] they use a greedy algorithm to compute recursive alignments from a bilingual parallel corpus. Here, we propose to combine the standard phrase-based segmentation [10] with a complementary bilingual segmentation which is learned from a statistical collocation segmen189 Proceedings of the 7th Internationa"
2010.iwslt-evaluation.26,P06-2084,0,0.0305339,"tivity measures. Different associativity measures have different scale of values and it is difficult to set threshold manually. Threshold level is kept as low as possible. Higher threshold value makes shorter collocation segments and vice versa. Shorter collocation segments are more confident collocations and we may expect better translation results. Nevertheless, the results of [3] show that longer collocation segments are more preferable. There are many associativity measures that could be used to calculate the associativity values between tokens (a more comprehensive list could be found in [11]). To explore different measures we included the six following metrics: 1. Mutual Information (MI): M I(wi , wi+1 ) = N ∗ f (wi , wi+1 ) f (w1 ) + f (wi+1 ) dice(wi , wi+1 ) = 2 ∗ f (wi , wi+1 ) f (wi ) + f (wi+1 ) 1. the words are consecutive, and, 2. they are consistent with the word alignment matrix. Given the collected phrase pairs, the phrase translation probability distribution is commonly estimated by relative frequency in both directions. The translation model is combined together with the following six additional feature models: the target language model, the word and the phrase bonus"
2010.iwslt-evaluation.26,J93-1007,0,0.203913,", four tokens). The boundary of a segment is set between adjacent tokens when the value of associativity between these two adjacent tokens is lower than the average of preceding and following associativity values. Some examples of segmentation of English and French sentences are presented in Table 1. The result of collocation segmentation is a segmented text, no dictionaries are produces and no evaluation of segments is made. The segmented text could be used to create a dictionary of collocations. Such dictionary accepts all collocation segments. The main difference from Choueka [2] and Smadja[12] methods is that collocation segmentation accepts all collocations and no significance tests for collocations are performed. The main advantage of this segmentation is the ability to perform collocation segmentation of both small and large corpora, and no manually segmented corpora or other databases and language processing tools are required. 5. Introducing the collocation segmentation into a phrase-based system In order to build the augmented phrase table with the technique mentioned in section 4, we segmented each language of the bilingual corpus independently and then, using the collocatio"
2010.iwslt-evaluation.26,takezawa-etal-2002-toward,0,0.0385848,"integrate the baseline segmentation with the new phrases (hereinafter, collocation segmentation new phrases) or the baseline segmentation with the existing phrases (hereinafter, collocation segmentation smooth). chi2(wi , wi+1 ) = N = f (wi ) ∗ f (wi+1 ) N ∗ f (wi , wi+1 ) − f (wi ) ∗ f (wi+1 ) ∗ N − f (wi ) + f (wi , wi+1 ) N ∗ f (wi , wi+1 ) − f (wi ) ∗ f (wi+1 ) ∗ N − f (wi+1 ) + f (wi , wi+1 ) 5. Gravity-Counts (GC)[5]: gc(wi , wi+1 ) =   f (wi ) ∗ f (wi , wi+1 ) = log n(wi )   f (wi+1 ) ∗ f (wi , wi+1 ) +log n0 (wi+1 ) 6. Experiments We participated in the French-to-English BTEC task [13] in the correct recognition results. We build our baseline system using MOSES with the standard configuration http://www.statmt.org/moses/. 6. T-score: tscore(wi , wi+1 ) = f (wi , wi+1 ) − f (wi )fN(wi+1 ) p f (wi , wi+1 ) The next step after setting the associativity threshold boundaries is to apply an average minimum law (AML) as described in [3] and [4]. The average minimum law is applied to the three adjacent associativity values (i.e., four tokens). The boundary of a segment is set between adjacent tokens when the value of associativity between these two adjacent tokens is lower than the"
2010.iwslt-evaluation.26,W10-1712,1,0.834596,"systems. 1. Introduction The Universitat Polit`ecnica de Catalunya (UPC), Barcelona Media Innovation Center (BMIC) and Vytautas Magnus University (VMU) participated together in the IWSLT 2010 evaluation campaign. This paper describes the UPC-BMICVMU system, which is basically a statistical phrase-based system enriched with collocation segmentation information. Adding a novel segmentation in an SMT system allows to enrich the translation dictionary and/or to smooth the existing translation probabilities. Basically, we extend the work presented in the WMT 2010 evaluation for Spanish-to-English [7] by experimenting with different statistical scores to segment a monolingual training corpus and by analysing if it is better to add new translation phrases and/or to smooth the existing ones. We participated in the French-to-English BTEC task. Our primary and contrastive systems were two standard phrase-based SMT systems enriched with different novel segmentations. This paper is organized as follows. Section 2 makes a brief description of some related work to the introduction of new segmentations in SMT. Section 3 describes the baseline system. Then, Section 4 reports different statistical cr"
2010.iwslt-evaluation.26,J06-4004,1,0.847561,"on 6 shows the experimental details of the system and the experiments performed with the novel technique. Finally, Section 7 presents the conclusions. 2. Related work One of the main problems in the statistical machine translation approach is how to segment the bilingual corpus in order to build the most appropriate translation dictionary. Standard phrase-based SMT systems first align the parallel corpus at the word level by using IBM probabilities and then use standard constraints (see section 3) to extract final translation units [10]. Variations of this type of segmentation can be found in [8, 1, 6]. Other approaches consist in integrating the phrase segmentation and alignment, one example is in [14] where they use the point-wise mutual information between the source and target words to identify aligned phrase pairs. In [9] they use a greedy algorithm to compute recursive alignments from a bilingual parallel corpus. Here, we propose to combine the standard phrase-based segmentation [10] with a complementary bilingual segmentation which is learned from a statistical collocation segmen189 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3"
2020.acl-srw.10,D14-1179,0,0.0492172,"Missing"
2020.acl-srw.10,P16-2058,1,0.884703,"Missing"
2020.acl-srw.10,N18-1032,0,0.0573029,"to wordlevel information in a task-agnostic way. While the approaches by Bojanowski et al. (2017), Zhao et al. (2018) and Li et al. (2018) aim at computing pre-trained word representations, other proposals integrate the computation of the word representation in the overall NMT model, either combining information from character level, like those by Luong and Manning (2016) Costa-juss`a and Fonollosa (2016), from n-gram level, like the one by Ataman and Federico (2018), or from multiple granularities like the work by Chen et al. (2018). Some other approaches like those by Wang et al. (2019) and Gu et al. (2018b) try to extend this idea to obtain multilingual conceptual representations from character-level representations. Nevertheless, in all those approaches, the decoder only has access to the aggregated wordlevel information and not to the original subwordlevel information. This, while mitigating the unknown word problem, cannot handle the scenario where copying from source to target is necessary, like with unseen proper names or with compositional structures like numbers. To the best of our knowledge, this type of neural architectures that condense subword/character-level information into word-l"
2020.acl-srw.10,P82-1020,0,0.653665,"Missing"
2020.acl-srw.10,W18-1205,0,0.0182072,"er is then used at every layer of the decoder as key and value of the multi-head attention. In these operations, the token representations in the sequences in the source batch are masked according to the original sequence lengths in tokens. Related Work The main difficulty in profiting from word-level information in subword-based NMT architectures is the word-subword token level mismatch. Several lines of research have studied how to combine subword-level representations into wordlevel information in a task-agnostic way. While the approaches by Bojanowski et al. (2017), Zhao et al. (2018) and Li et al. (2018) aim at computing pre-trained word representations, other proposals integrate the computation of the word representation in the overall NMT model, either combining information from character level, like those by Luong and Manning (2016) Costa-juss`a and Fonollosa (2016), from n-gram level, like the one by Ataman and Federico (2018), or from multiple granularities like the work by Chen et al. (2018). Some other approaches like those by Wang et al. (2019) and Gu et al. (2018b) try to extend this idea to obtain multilingual conceptual representations from character-level representations. Neverthe"
2020.acl-srw.10,P16-1100,0,0.0551171,"Missing"
2020.acl-srw.10,P18-2049,0,0.0173807,"rd-based NMT architectures is the word-subword token level mismatch. Several lines of research have studied how to combine subword-level representations into wordlevel information in a task-agnostic way. While the approaches by Bojanowski et al. (2017), Zhao et al. (2018) and Li et al. (2018) aim at computing pre-trained word representations, other proposals integrate the computation of the word representation in the overall NMT model, either combining information from character level, like those by Luong and Manning (2016) Costa-juss`a and Fonollosa (2016), from n-gram level, like the one by Ataman and Federico (2018), or from multiple granularities like the work by Chen et al. (2018). Some other approaches like those by Wang et al. (2019) and Gu et al. (2018b) try to extend this idea to obtain multilingual conceptual representations from character-level representations. Nevertheless, in all those approaches, the decoder only has access to the aggregated wordlevel information and not to the original subwordlevel information. This, while mitigating the unknown word problem, cannot handle the scenario where copying from source to target is necessary, like with unseen proper names or with compositional struct"
2020.acl-srw.10,P14-5010,0,0.00326134,"l information. Table 1: BLEU scores on IWSLT14 German-English for different subword combination strategies. Nsw 3 3 5 en-de 28.75 28.29 Base Transformer Word-level info copied to subwords Word-subword model + word-level info BLEU 33.53 34.02 34.46 en-ro 27.02 27.29 27.82 Table 4: BLEU scores measured on the WMT16 English-Romanian data, with lemmas as linguistic info. The word-level linguistic information used was only the lemma (using a vocabulary of 40k lemmas), which is the feature that should provide the largest improvement according to Sennrich and Haddow (2016). We used Stanford CoreNLP (Manning et al., 2014) to annotate the corpus with the English lemmas. The obtained results are shown in Table 4, where our proposed approach obtains the best BLEU score compared to the base Transformer model (Vaswani et al., 2017) without any wordlevel information, and to copying the word-level info to subwords (Sennrich and Haddow, 2016). Table 2: BLEU scores on the IWSLT14 German(e) (d) English test set for different values of Nsw and Nsw , using GRU as subword combination strategy. Once determined that using GRU as subword (e) (d) combination and setting Nsw = 5 and Nsw = 3 is the hyperparameter configuration t"
2020.acl-srw.10,Q17-1010,0,0.0471234,"e input token embeddings. The output of the encoder is then used at every layer of the decoder as key and value of the multi-head attention. In these operations, the token representations in the sequences in the source batch are masked according to the original sequence lengths in tokens. Related Work The main difficulty in profiting from word-level information in subword-based NMT architectures is the word-subword token level mismatch. Several lines of research have studied how to combine subword-level representations into wordlevel information in a task-agnostic way. While the approaches by Bojanowski et al. (2017), Zhao et al. (2018) and Li et al. (2018) aim at computing pre-trained word representations, other proposals integrate the computation of the word representation in the overall NMT model, either combining information from character level, like those by Luong and Manning (2016) Costa-juss`a and Fonollosa (2016), from n-gram level, like the one by Ataman and Federico (2018), or from multiple granularities like the work by Chen et al. (2018). Some other approaches like those by Wang et al. (2019) and Gu et al. (2018b) try to extend this idea to obtain multilingual conceptual representations from"
2020.acl-srw.10,N19-4009,0,0.0216232,"ality with different hyperparameter sets in order to understand their effect on the model. In order to study the effectiveness of the proposed model with other approaches to incorporate word-level information into a subword-based model, we used the WMT16 English-Romanian data with the back-translated synthetic data from (Sennrich et al., 2016a), using a shared subword vocabulary of 40k merge operations. We used the proposal by (Sennrich and Haddow, 2016) as baseline, and compared it to a vanilla Transformer baseline and to our proposed method. For all experiments, we used the fairseq library (Ott et al., 2019), either with its built-in models for the baselines or with custom model implementations for the approach by Sennrich and Haddow (2016) and for our own proposed architecture. For the IWSLT14 de-en and en-de baselines we used the Transformer architecture (Vaswani et al., 2017) with the hyperparameters proposed by the fairseq authors1 , namely 6 layers in encoder and decoder, 4 attention heads, embedding size of 512 and 1024 for the feedforward expansion size, together with dropout of 0.3 and a total batch size of 4000 tokens, using label smoothing of 0.1. For the WMT16 en-ro baseline we used th"
2020.acl-srw.10,W16-2209,0,0.162527,"ower and vocabulary size, especially statistically extracted subword vocabulary strategies like Byte Pair Encoding (BPE) (Sennrich et al., 2016b). Models with word-level token vocabularies can incorporate word-level information as extra input to the model by combining it one-to-one with the token representations. Some examples of word-level information are Part of Speech (POS) tags, syntactic dependency relationships or lemmas. In order to make use of word-level information in models with subword-level token vocabularies, a usual approach is to assign the word information to all its subwords (Sennrich and Haddow, 2016). This approach, despite improving the translation quality, introduces an information assignment mismatch. We propose to modify the Transformer architecture (Vaswani et al., 2017) to combine the learned subword representations into word representations in the encoder block. This allows to naturally incorporate any extra word-level information directly at the level of word-level representations. This work is structured as follows: the relevant related work is described in section 2; the proposed In Neural Machine Translation, using wordlevel tokens leads to degradation in translation quality. T"
2020.acl-srw.10,W16-2323,0,0.152549,"tokens, which is hardly enough to fit the number of symbols in a complete word-based vocabulary. Compositional word structures like numbers pose further problems with such a granularity level, as well as proper nouns. When word-based vocabularies are used, the vocabulary is built with the most frequent surface forms in the training data, which normally leads to degradation of translation quality. Subword-level token granularity offers a compromise between representational power and vocabulary size, especially statistically extracted subword vocabulary strategies like Byte Pair Encoding (BPE) (Sennrich et al., 2016b). Models with word-level token vocabularies can incorporate word-level information as extra input to the model by combining it one-to-one with the token representations. Some examples of word-level information are Part of Speech (POS) tags, syntactic dependency relationships or lemmas. In order to make use of word-level information in models with subword-level token vocabularies, a usual approach is to assign the word information to all its subwords (Sennrich and Haddow, 2016). This approach, despite improving the translation quality, introduces an information assignment mismatch. We propose"
2020.acl-srw.10,P16-1162,0,0.371237,"tokens, which is hardly enough to fit the number of symbols in a complete word-based vocabulary. Compositional word structures like numbers pose further problems with such a granularity level, as well as proper nouns. When word-based vocabularies are used, the vocabulary is built with the most frequent surface forms in the training data, which normally leads to degradation of translation quality. Subword-level token granularity offers a compromise between representational power and vocabulary size, especially statistically extracted subword vocabulary strategies like Byte Pair Encoding (BPE) (Sennrich et al., 2016b). Models with word-level token vocabularies can incorporate word-level information as extra input to the model by combining it one-to-one with the token representations. Some examples of word-level information are Part of Speech (POS) tags, syntactic dependency relationships or lemmas. In order to make use of word-level information in models with subword-level token vocabularies, a usual approach is to assign the word information to all its subwords (Sennrich and Haddow, 2016). This approach, despite improving the translation quality, introduces an information assignment mismatch. We propose"
2020.acl-srw.10,D18-1059,0,0.0210813,"The output of the encoder is then used at every layer of the decoder as key and value of the multi-head attention. In these operations, the token representations in the sequences in the source batch are masked according to the original sequence lengths in tokens. Related Work The main difficulty in profiting from word-level information in subword-based NMT architectures is the word-subword token level mismatch. Several lines of research have studied how to combine subword-level representations into wordlevel information in a task-agnostic way. While the approaches by Bojanowski et al. (2017), Zhao et al. (2018) and Li et al. (2018) aim at computing pre-trained word representations, other proposals integrate the computation of the word representation in the overall NMT model, either combining information from character level, like those by Luong and Manning (2016) Costa-juss`a and Fonollosa (2016), from n-gram level, like the one by Ataman and Federico (2018), or from multiple granularities like the work by Chen et al. (2018). Some other approaches like those by Wang et al. (2019) and Gu et al. (2018b) try to extend this idea to obtain multilingual conceptual representations from character-level repr"
2020.acl-srw.36,W13-3520,0,0.0640127,"Missing"
2020.acl-srw.36,P17-1042,0,0.0311541,"PI. Experiments Pre-trained Word Embeddings. In order to evaluate our proposed approach as well as to compare our results with respect to current state-ofthe-art post-specialization approaches, we use popular and readily available 300-dimensional pretrained word vectors. Word2Vec (Mikolov et al., 2013) embeddings for English were trained using skip-gram with negative sampling on the cleaned and tokenized Polyglot Wikipedia (Al-Rfou’ et al., 2013) by Levy and Goldberg (2014), while German and Italian embeddings were trained using CBOW with negative sampling on WacKy corpora (Dinu et al., 2015; Artetxe et al., 2017, 2018). Moreover, GloVe vectors for English were trained on Common Crawl (Pennington et al., 2014). Linguistic Constraints. To perform semantic specialization of word vector spaces, we exploit linguistic constraints used in previous works (Zhang et al., 2014; Ono et al., 2015; Vuli´c et al., 2018) (referred to as external) as well as introduce a new set of constraints collected by us (referred to as babelnet) for three languages: English, German and Italian. We use constraints in two different settings: disjoint and overlap. In the first setting, we remove all linguistic constraints that cont"
2020.acl-srw.36,N15-1184,0,0.170133,"Missing"
2020.acl-srw.36,D16-1235,0,0.0410693,"Missing"
2020.acl-srw.36,P18-1004,0,0.256264,"Missing"
2020.acl-srw.36,W14-4337,0,0.072561,"Missing"
2020.acl-srw.36,J15-4004,0,0.0854096,"sh were trained on Common Crawl (Pennington et al., 2014). Linguistic Constraints. To perform semantic specialization of word vector spaces, we exploit linguistic constraints used in previous works (Zhang et al., 2014; Ono et al., 2015; Vuli´c et al., 2018) (referred to as external) as well as introduce a new set of constraints collected by us (referred to as babelnet) for three languages: English, German and Italian. We use constraints in two different settings: disjoint and overlap. In the first setting, we remove all linguistic constraints that contain any of the words available in SimLex (Hill et al., 2015), SimVerb (Gerz et al., 2016) and WordSim (Leviant and Reichart, 2015) evaluation datasets. In the overlap setting, we let the SimLex, SimVerb and WordSim words remain in the constraints. To summarize, we present the number of word pairs for English, German and Italian constraints in Table 1. Let us discuss in more detail how the lists of constraints were constructed. In this work, we use two sets of linguistic constraints: external and babelnet. The first set of constraints was retrieved from WordNet (Fellbaum, 1998) and Roget’s Thesaurus (Kipfer, 2009), resulting in 1,023,082 synonymy and 38"
2020.acl-srw.36,P14-2050,0,0.0418728,"we collected each word with its related BabelNetID (a sense database identifier) to extract the list of its synonyms and antonyms using BabelNet API. Experiments Pre-trained Word Embeddings. In order to evaluate our proposed approach as well as to compare our results with respect to current state-ofthe-art post-specialization approaches, we use popular and readily available 300-dimensional pretrained word vectors. Word2Vec (Mikolov et al., 2013) embeddings for English were trained using skip-gram with negative sampling on the cleaned and tokenized Polyglot Wikipedia (Al-Rfou’ et al., 2013) by Levy and Goldberg (2014), while German and Italian embeddings were trained using CBOW with negative sampling on WacKy corpora (Dinu et al., 2015; Artetxe et al., 2017, 2018). Moreover, GloVe vectors for English were trained on Common Crawl (Pennington et al., 2014). Linguistic Constraints. To perform semantic specialization of word vector spaces, we exploit linguistic constraints used in previous works (Zhang et al., 2014; Ono et al., 2015; Vuli´c et al., 2018) (referred to as external) as well as introduce a new set of constraints collected by us (referred to as babelnet) for three languages: English, German and Ita"
2020.acl-srw.36,N16-1018,0,0.107458,"Missing"
2020.acl-srw.36,P17-1163,0,0.0321001,"Missing"
2020.acl-srw.36,Q17-1022,0,0.110934,"Missing"
2020.acl-srw.36,N15-1100,0,0.0213805,"(Mikolov et al., 2013) embeddings for English were trained using skip-gram with negative sampling on the cleaned and tokenized Polyglot Wikipedia (Al-Rfou’ et al., 2013) by Levy and Goldberg (2014), while German and Italian embeddings were trained using CBOW with negative sampling on WacKy corpora (Dinu et al., 2015; Artetxe et al., 2017, 2018). Moreover, GloVe vectors for English were trained on Common Crawl (Pennington et al., 2014). Linguistic Constraints. To perform semantic specialization of word vector spaces, we exploit linguistic constraints used in previous works (Zhang et al., 2014; Ono et al., 2015; Vuli´c et al., 2018) (referred to as external) as well as introduce a new set of constraints collected by us (referred to as babelnet) for three languages: English, German and Italian. We use constraints in two different settings: disjoint and overlap. In the first setting, we remove all linguistic constraints that contain any of the words available in SimLex (Hill et al., 2015), SimVerb (Gerz et al., 2016) and WordSim (Leviant and Reichart, 2015) evaluation datasets. In the overlap setting, we let the SimLex, SimVerb and WordSim words remain in the constraints. To summarize, we present the"
2020.acl-srw.36,D14-1162,0,0.0975402,"network with the Wasserstein distance allows to gain improvements over state-of-the-art methods on two tasks: word similarity and dialog state tracking. 1 • We show that the proposed approach achieves performance improvements on an intrinsic task (word similarity) as well as on a downstream task (dialog state tracking). 2 Introduction Vector representations of words (embeddings) have become the cornerstone of modern Natural Language Processing (NLP), as learning word vectors and utilizing them as features in downstream NLP tasks is the de facto standard. Word embeddings (Mikolov et al., 2013; Pennington et al., 2014) are typically trained in an unsupervised way on large monolingual corpora. Whilst such word representations are able to capture some syntactic as well as semantic information, their ability to map relations (e.g. synonymy, antonymy) between words is limited. To alleviate this deficiency, a set of refinement post-processing methods–called retrofitting or semantic specialization–has been introduced. In the next section, we discuss the intricacies of these methods in more detail. To summarize, our contributions in this work are as follows: • We introduce a set of new linguistic constraints (i.e."
2020.acl-srw.36,D18-1026,0,0.232961,"Missing"
2020.acl-srw.36,W14-5814,0,0.0238613,"Missing"
2020.acl-srw.36,N18-1048,0,0.212114,"Missing"
2020.acl-srw.36,E17-1042,0,0.0407218,"Missing"
2020.acl-srw.36,D14-1161,0,0.186652,"d vectors. Word2Vec (Mikolov et al., 2013) embeddings for English were trained using skip-gram with negative sampling on the cleaned and tokenized Polyglot Wikipedia (Al-Rfou’ et al., 2013) by Levy and Goldberg (2014), while German and Italian embeddings were trained using CBOW with negative sampling on WacKy corpora (Dinu et al., 2015; Artetxe et al., 2017, 2018). Moreover, GloVe vectors for English were trained on Common Crawl (Pennington et al., 2014). Linguistic Constraints. To perform semantic specialization of word vector spaces, we exploit linguistic constraints used in previous works (Zhang et al., 2014; Ono et al., 2015; Vuli´c et al., 2018) (referred to as external) as well as introduce a new set of constraints collected by us (referred to as babelnet) for three languages: English, German and Italian. We use constraints in two different settings: disjoint and overlap. In the first setting, we remove all linguistic constraints that contain any of the words available in SimLex (Hill et al., 2015), SimVerb (Gerz et al., 2016) and WordSim (Leviant and Reichart, 2015) evaluation datasets. In the overlap setting, we let the SimLex, SimVerb and WordSim words remain in the constraints. To summariz"
2020.cl-2.1,2020.cl-2.3,0,0.0613442,"Missing"
2020.cl-2.1,2020.acl-main.447,0,0.0363097,"he 2010s; red line). The fraction of papers mentioning two or more languages (yellow line) and the average per year (green line) showed increases in the 1990s and 2000s, though these appear to have slowed recently.3 The other trend is a matter of increasing supply: The diversity of computational tools now available—from conceptual definitions of language meaning to operationalizations in downloadable models—has exploded in the past decade. The term “semantic representation” was, not long ago, one that referred to a range of linguistic abstractions. 1 We explored ACL Anthology papers in S2ORC (Lo et al. 2020) with publication years 1980–2019, a total of 40,402 papers. 2 The list is Ethnologue’s list of the 20 most spoken languages in 2019, with Mandarin and Wu Chinese mapped to the string chinese. See https://www.ethnologue.com/guides/ethnologue200. Less dominant languages are, of course, also interesting, but also more sparse in the data. 3 The leveling off of these last two trends is, we speculate, due to the emergence of new representation learning methods that work best with very large data sets. We expect increasing multilinguality of the largest data sets and pretrained representations will"
2020.cl-2.1,2020.cl-2.2,0,0.426794,"cept may help to obtain more robust embeddings at sense level as shown by one of the works presented here. The contributions to this special issue are summarized in Table 1. The papers selected cover the different points we wanted to emphasize in our call. Three of the contributions refer to representations at word level and the others at sentence level, but the breadth of the field is reflected in the range of specific topics addressed. This issue presents novel work and reviews on interlingual representations (Ranta et al. 2020); semantic representations learned through translation at word (Mohiuddin and Joty 2020) and sentence level (V´azquez et al. 2020); senses, ambiguity, and polysemy (Colla, Mensa, and Radicioni 2020); and evaluation (Sahin 2020). Multilinguality is clearly the aim for all of them, with systems that cover from 4 up to 40 languages. Some systems also have the virtue to deal with text in low-resource languages such as Macedonian, Nepali, and Telugu. 4 http://universaldependencies.org. 251 Computational Linguistics Volume 46, Number 2 Table 1 Summary of contributions to the special issue. Granularity Word Paper (Mohiuddin and Joty 2020) Technique Application Languages Unsupervised Adv"
2020.cl-2.1,2020.cl-2.6,0,0.07929,"entary might also be true, and realizations in different languages of the same concept may help to obtain more robust embeddings at sense level as shown by one of the works presented here. The contributions to this special issue are summarized in Table 1. The papers selected cover the different points we wanted to emphasize in our call. Three of the contributions refer to representations at word level and the others at sentence level, but the breadth of the field is reflected in the range of specific topics addressed. This issue presents novel work and reviews on interlingual representations (Ranta et al. 2020); semantic representations learned through translation at word (Mohiuddin and Joty 2020) and sentence level (V´azquez et al. 2020); senses, ambiguity, and polysemy (Colla, Mensa, and Radicioni 2020); and evaluation (Sahin 2020). Multilinguality is clearly the aim for all of them, with systems that cover from 4 up to 40 languages. Some systems also have the virtue to deal with text in low-resource languages such as Macedonian, Nepali, and Telugu. 4 http://universaldependencies.org. 251 Computational Linguistics Volume 46, Number 2 Table 1 Summary of contributions to the special issue. Granulari"
2020.cl-2.1,J82-2005,0,0.392652,"us lines of work that illustrate a range of creative advances exploring natural language meaning, specifically with a multilingual focus. In inviting submissions, we encouraged a broad reading of the term “representations,” in granularity (words, sentences, paragraphs, etc.) and in theoretical assumptions (symbolic, neural, hybrid, etc.). We anticipated breadth as well in the set of motivating applications and evaluation methods. Our deliberate reference to interlingual—not only multilingual—representations evokes recent re-imaginings of interlingual machine translation, a classical approach (Richens 1958). We explicitly encouraged submissions that consider less-commonly studied languages and that go beyond mere projection of representations from text in one language to another. Of particular interest to our editorial team is the potential for multilingual representations (of any kind) to help overcome challenges of polysemy in individual languages. It has been shown that translations into other languages can help at distinguishing senses monolingually (Resnik and Yarowsky 1999). But the complementary might also be true, and realizations in different languages of the same concept may help to ob"
2020.cl-2.1,2020.cl-2.4,0,0.0730249,"ummarized in Table 1. The papers selected cover the different points we wanted to emphasize in our call. Three of the contributions refer to representations at word level and the others at sentence level, but the breadth of the field is reflected in the range of specific topics addressed. This issue presents novel work and reviews on interlingual representations (Ranta et al. 2020); semantic representations learned through translation at word (Mohiuddin and Joty 2020) and sentence level (V´azquez et al. 2020); senses, ambiguity, and polysemy (Colla, Mensa, and Radicioni 2020); and evaluation (Sahin 2020). Multilinguality is clearly the aim for all of them, with systems that cover from 4 up to 40 languages. Some systems also have the virtue to deal with text in low-resource languages such as Macedonian, Nepali, and Telugu. 4 http://universaldependencies.org. 251 Computational Linguistics Volume 46, Number 2 Table 1 Summary of contributions to the special issue. Granularity Word Paper (Mohiuddin and Joty 2020) Technique Application Languages Unsupervised Adversarial Translation en, es, de, it, fi, ar, ms, he Linked Data Intrinsic/extrinsic evaluation Word Similarity POS, dependencies, SRL, NER,"
2020.cl-2.1,2020.cl-2.5,0,0.0495885,"Missing"
2020.cl-2.1,D18-1268,0,0.0204012,"ion. The challenges addressed include learning unsupervised representations, introducing priors and linguistic knowledge to compute the representations, and evaluating the quality of these representations, taking into account linguistic features. Unsupervised Word Translation with Adversarial Encoder (Mohiuddin and Joty 2020). Crosslingual word embeddings are becoming crucial in multilingual natural language processing tasks and, recently, several authors claim that unsupervised methods even outperform the supervised ones (see for instance Lample et al. 2018, Artetxe, Labaka, and Agirre 2018, Xu et al. 2018), making them appealing also in the low-resource setting. This is not true in all cases, and specifically, adversarial techniques for dictionary induction show stability and convergence issues for some language pairs (Zhang et al. 2017; Lample et al. 2018). In general, unsupervised adversarial bilingual embeddings are learned in two phases: (i) induction of an initial seed dictionary using an adversarial network and (ii) refinement of the initial mapping, and therefore, dictionary, until convergence. This paper tries to address those limitations by extending adversarial autoencoders. One of th"
2020.cl-2.1,D17-1207,0,0.0236204,"tic features. Unsupervised Word Translation with Adversarial Encoder (Mohiuddin and Joty 2020). Crosslingual word embeddings are becoming crucial in multilingual natural language processing tasks and, recently, several authors claim that unsupervised methods even outperform the supervised ones (see for instance Lample et al. 2018, Artetxe, Labaka, and Agirre 2018, Xu et al. 2018), making them appealing also in the low-resource setting. This is not true in all cases, and specifically, adversarial techniques for dictionary induction show stability and convergence issues for some language pairs (Zhang et al. 2017; Lample et al. 2018). In general, unsupervised adversarial bilingual embeddings are learned in two phases: (i) induction of an initial seed dictionary using an adversarial network and (ii) refinement of the initial mapping, and therefore, dictionary, until convergence. This paper tries to address those limitations by extending adversarial autoencoders. One of the main contributions is training the adversarial mapping in a latent space, with the hope that this will minimize the effect of a lack of isomorphism between the two original embedding spaces. In addition, the authors combine several l"
2020.cl-2.1,Q17-1010,0,\N,Missing
2020.cl-2.1,Q19-1038,0,\N,Missing
2020.coling-main.574,D19-1165,0,0.103565,"k (DEN, Yoon et al., 2018), Reinforced Continual Learning (RCL, Xu and Zhu, 2018), are prominent examples. The main drawback of such strategies is the substantially growing number of parameters. Similar to PNN, BatchEnsemble (Wen et al., 2020) is also immune to CF, in addition it supports parallel order of tasks and consumes less memory than PNN thanks to training only fast weights. In a similar vein, adapter modules aim to overcome the problem of a large number of parameters. They act as additional network layers with a small number of parameters (Rebuffi et al., 2017a; Houlsby et al., 2019; Bapna and Firat, 2019; Pfeiffer et al., 2020) that reconfigure the original network on-the-fly for a target task, while keeping the parameters of the original network untouched and shared between different tasks. 3 Evaluation Even though CL is now experiencing a surge in the number of proposed new methods, there is no unified approach when it comes to their evaluation using benchmark datasets and metrics (Parisi et al., 2019). And as we will show in this section, this is especially true in the NLP domain. There is a scarcity of datasets and benchmark evaluation schemes available specifically for CL in NLP. 3.1 Pro"
2020.coling-main.574,C18-1070,0,0.0283539,"yide and Vlachos (2019) proposed to extend the work of Wang et al. (2019b) by framing the lifelong relation extraction as a meta-learning problem; however, without the costly need for learning additional parameters. 4.4 Sentiment Analysis and Text Classification Sentiment analysis (SA) is a popular choice for evaluating models on text classification. Arguably the most pressing problem of current approaches to SA is their poor performance on new domains. Therefore, various domain adaptation methods have been proposed to improve the performance of SA models in the multi-domain scenario (consult Barnes et al., 2018). This issue is of utmost importance if one thinks about CL in sentiment classification. One of the earliest approaches to CL for SA was proposed in Chen et al. (2015). According to Chen and Liu (2018), CL can enable SA models to adapt to a large number of domains, since many new domains may already be covered by other past domains. Additionally, SA systems should become more accurate not only in classification but also in the discovery of word polarities in specific domains. Research in opinion about aspects has been conducted as well. Shu et al. (2016) presented an unsupervised CL approach t"
2020.coling-main.574,P15-2123,0,0.0187169,"ostly need for learning additional parameters. 4.4 Sentiment Analysis and Text Classification Sentiment analysis (SA) is a popular choice for evaluating models on text classification. Arguably the most pressing problem of current approaches to SA is their poor performance on new domains. Therefore, various domain adaptation methods have been proposed to improve the performance of SA models in the multi-domain scenario (consult Barnes et al., 2018). This issue is of utmost importance if one thinks about CL in sentiment classification. One of the earliest approaches to CL for SA was proposed in Chen et al. (2015). According to Chen and Liu (2018), CL can enable SA models to adapt to a large number of domains, since many new domains may already be covered by other past domains. Additionally, SA systems should become more accurate not only in classification but also in the discovery of word polarities in specific domains. Research in opinion about aspects has been conducted as well. Shu et al. (2016) presented an unsupervised CL approach to classify opinion targets into entities and aspects. Furthermore, Shu et al. (2017) proposed a method based on conditional random fields to improve supervised aspect"
2020.coling-main.574,W18-2705,0,0.060366,"ranslation The approach introduced by Luong and Manning (2015) laid the groundwork for subsequent studies in adapting neural machine translation (NMT). More specifically, the authors explored the adaptation through continued training, where an NMT model trained using large corpora in one domain can later initialize a new NMT model for another domain. Their findings suggested that fine-tuning of the NMT model trained on out-of-domain data using a small in-domain parallel corpus boosts performance. Likewise, other works (e.g. Freitag and Al-Onaizan, 2016; Chu et al., 2017) supported this claim. Khayrallah et al. (2018) pointed out that, due to over-fitting, some amount of knowledge learned from the outof-domain corpus is being forgotten during fine-tuning. Hence, such domain adaptation techniques are prone to CF. NMT models experience difficulties when dealing with data from diverse domains, hence we argue this is not a sufficient solution. As dominant fine-tuning approaches require training and maintaining a separate model for each language or domain, Bapna and Firat (2019) proposed to add light-weight task-specific adapter modules to support parameter-efficient adaptation. We further argue that NMT – as o"
2020.coling-main.574,D16-1139,0,0.167984,"MER (Riemer et al., 2018), or a method originating from NLP - MBPA++ (d’Autume et al., 2019). Knowledge distillation methods bear a close resemblance to episodic memory methods, but unlike GEM they keep the predictions at past tasks invariant (Rebuffi et al., 2017b; Lopez-Paz and Ranzato, 2017). In particular, it is a class of methods alleviating CF by relying on knowledge transfer from a large network model (teacher) to a new, smaller network (student) (Hinton et al., 2015). The underlying idea is that the student model learns to generate predictions of the teacher model. As demonstrated in Kim and Rush (2016) and Wei et al. (2019), knowledge distillation approaches can prove especially suitable for neural machine translation models, which are mostly large, and hence reduction in size is beneficial. Architectural methods prevent forgetting by applying modular changes to the network’s architecture and introducing task-specific parameters. Typically, previous task parameters are kept fixed (Rusu et al., 2016; Mancini et al., 2018) or masked out (Serra et al., 2018; Mallya and Lazebnik, 2018). Moreover, new layers are often injected dynamically to augment a model with additional modules to accommodate"
2020.coling-main.574,C18-1117,0,0.028035,"suffer from inherent limitations. Typically, word embeddings are trained on large-size general corpora, as the size of in-domain corpora is in most cases not sufficient. This comes at a cost, since embeddings trained on general-purpose corpora are often not suitable for domain-specific downstream tasks, and in result, the overall performance suffers. In a CL setting, this also implies that vocabulary may change with respect to two dimensions: time and domain. There is an established consensus that the meaning of words changes over time due to complicated linguistic and social processes (e.g. Kutuzov et al., 2018; Shoemark et al., 2019). Hence, it is important to detect and accommodate shifts in meaning and data distribution, while preventing previously learned representations from CF. In general, a CL scenario for word and sentence embeddings has not received much attention so far, except for a handful of works. To tackle this problem, for example Xu et al. (2018) proposed a metalearning method, which leverages knowledge from past multi-domain corpora to generate improved new domain embeddings. Liu et al. (2019) introduced a sentence encoder updated over time using matrix conceptors to continually le"
2020.coling-main.574,K18-1033,0,0.0161381,"when dealing with data from diverse domains, hence we argue this is not a sufficient solution. As dominant fine-tuning approaches require training and maintaining a separate model for each language or domain, Bapna and Firat (2019) proposed to add light-weight task-specific adapter modules to support parameter-efficient adaptation. We further argue that NMT – as opposed to phrase-based MT – rarely incorporates translation memory, and so it is inherently harder for NMT models to adapt using active or interactive learning. However, some attempts have been made (e.g. Peris and Casacuberta, 2018; Liu et al., 2018; Kaiser et al., 2017; Tu et al., 2018). In a similar vein, there were approaches (Sokolov et al., 2017) to incorporate bandit learners, which implicitly involve domain adaptation and on-line learning, for MT systems. We share the viewpoint of Farajian et al. (2017), that NMT models ultimately should be able to adapt on-the-fly to on-line streams of diverse data (i.e. language, domain), and thus CL for NMT is essential. While domain adaptation methods are widely used in the context of adapting NMT models, there have also been other attempts. Multilingual NMT (Dong et al., 2015; Firat et al., 2"
2020.coling-main.574,N19-1331,0,0.0189272,"ng of words changes over time due to complicated linguistic and social processes (e.g. Kutuzov et al., 2018; Shoemark et al., 2019). Hence, it is important to detect and accommodate shifts in meaning and data distribution, while preventing previously learned representations from CF. In general, a CL scenario for word and sentence embeddings has not received much attention so far, except for a handful of works. To tackle this problem, for example Xu et al. (2018) proposed a metalearning method, which leverages knowledge from past multi-domain corpora to generate improved new domain embeddings. Liu et al. (2019) introduced a sentence encoder updated over time using matrix conceptors to continually learn corpus-dependent features. Importantly, Wang et al. (2019b) argued that when a NN model is trained on a new task, the embedding vector space undergoes undesired changes, and in result the embeddings are infeasible for previous tasks. To mitigate the problem of embedding space distortion, they proposed to align sentence embeddings using anchoring. Recently a research line at the intersection of word embeddings and language modeling, termed contextual embeddings, has emerged and demonstrates state-of-th"
2020.coling-main.574,2015.iwslt-evaluation.11,0,0.0268545,"ld become more accurate not only in classification but also in the discovery of word polarities in specific domains. Research in opinion about aspects has been conducted as well. Shu et al. (2016) presented an unsupervised CL approach to classify opinion targets into entities and aspects. Furthermore, Shu et al. (2017) proposed a method based on conditional random fields to improve supervised aspect extraction across time. Experiments on text classification in the CL setting were performed in d’Autume et al. (2019) and Sun et al. (2020). 6530 4.5 Machine Translation The approach introduced by Luong and Manning (2015) laid the groundwork for subsequent studies in adapting neural machine translation (NMT). More specifically, the authors explored the adaptation through continued training, where an NMT model trained using large corpora in one domain can later initialize a new NMT model for another domain. Their findings suggested that fine-tuning of the NMT model trained on out-of-domain data using a small in-domain parallel corpus boosts performance. Likewise, other works (e.g. Freitag and Al-Onaizan, 2016; Chu et al., 2017) supported this claim. Khayrallah et al. (2018) pointed out that, due to over-fitting"
2020.coling-main.574,W19-5903,0,0.053363,"Missing"
2020.coling-main.574,D17-1156,0,0.0235449,"r Wees et al. (2017) and Zhang et al. (2019) adapted a model to a domain by introducing samples which are increasingly domain-relevant or domain-distant respectively. Curriculum methods based on difficulty and competence were explored in Zhang et al. (2018) and Platanios et al. (2019). Ruiter et al. (2020) proposed a self-supervised NMT model, that uses data selection to train on increasingly complex and task-related samples in combination with a denoising curriculum. A stream of research focused on techniques more traditionally associated with CL has also been present. In the works of Miceli Barone et al. (2017); Khayrallah et al. (2018); Variˇs and Bojar (2019); Thompson et al. (2019) regularization approaches (e.g. EWC) were leveraged. Furthermore, Kim and Rush (2016) explored knowledge distillation, where the student model learns to match the teacher’s actions at the word- and sequence-level. Wei et al. (2019) proposed an on-line knowledge distillation approach, in which the best checkpoints are utilized as the teacher model. Lately, Li et al. (2020) demonstrated that label prediction continual learning leveraging compositionality brings improvements in NMT. 5 Research Gaps and Future Directions A"
2020.coling-main.574,W19-4326,0,0.117559,"line fashion. Interestingly, information and relation extraction were an early subject of research interest in CL. Information extraction is considered one of the first research areas, which embraced the goal of never-ending learning. A semi-supervised NELL (Carlson et al., 2010) and an unsupervised ALICE (Banko and Etzioni, 2007) systems, which iteratively extract information and build general domain knowledge, were at the forefront of CL in NLP. In the case of relation extraction, Wang et al. (2019b) introduced an embedding alignment method to enable CL for relation extraction models. Also, Obamuyide and Vlachos (2019) proposed to extend the work of Wang et al. (2019b) by framing the lifelong relation extraction as a meta-learning problem; however, without the costly need for learning additional parameters. 4.4 Sentiment Analysis and Text Classification Sentiment analysis (SA) is a popular choice for evaluating models on text classification. Arguably the most pressing problem of current approaches to SA is their poor performance on new domains. Therefore, various domain adaptation methods have been proposed to improve the performance of SA models in the multi-domain scenario (consult Barnes et al., 2018). T"
2020.coling-main.574,K18-1015,0,0.025088,"dels experience difficulties when dealing with data from diverse domains, hence we argue this is not a sufficient solution. As dominant fine-tuning approaches require training and maintaining a separate model for each language or domain, Bapna and Firat (2019) proposed to add light-weight task-specific adapter modules to support parameter-efficient adaptation. We further argue that NMT – as opposed to phrase-based MT – rarely incorporates translation memory, and so it is inherently harder for NMT models to adapt using active or interactive learning. However, some attempts have been made (e.g. Peris and Casacuberta, 2018; Liu et al., 2018; Kaiser et al., 2017; Tu et al., 2018). In a similar vein, there were approaches (Sokolov et al., 2017) to incorporate bandit learners, which implicitly involve domain adaptation and on-line learning, for MT systems. We share the viewpoint of Farajian et al. (2017), that NMT models ultimately should be able to adapt on-the-fly to on-line streams of diverse data (i.e. language, domain), and thus CL for NMT is essential. While domain adaptation methods are widely used in the context of adapting NMT models, there have also been other attempts. Multilingual NMT (Dong et al., 201"
2020.coling-main.574,N18-1202,0,0.275932,"d LeCun, 2004) (Cesa-Bianchi and Lugosi, 2006) (Shalev-Shwartz, 2012) (C. de Souza et al., 2015) (Hoi et al., 2018) On-the-job learning Discovering new tasks, learning and adapting on-the-fly. On-the-job learning operates in an open-world environment, and it involves interaction with humans and the environment. It belongs to the CL family of methods. + + + + + + – on-line learning forward transfer backward transfer knowledge retention no task boundaries open-world learning interactive learning (Xu et al., 2019) (Mazumder et al., 2019) (Liu, 2020) (Pan and Yang, 2010) (Howard and Ruder, 2018) (Peters et al., 2018) (Radford et al., 2019) (Devlin et al., 2019) (Houlsby et al., 2019) (Raffel et al., 2020) Table 2: Comparison of related ML paradigms. * Properties aligned (+) and unaligned (–) with CL. backward transfer deteriorates the performance on previous tasks (if high, it enables CF). Similarly, negative forward transfer impedes learning of new concepts, while positive forward transfer allows to learn a new task with just a few examples (if high, it enables zero-shot learning). 2.3 Approaches to Continual Learning The majority of existing CL approaches tend to apply a single model structure to all ta"
2020.coling-main.574,2020.emnlp-demos.7,0,0.0773135,"Missing"
2020.coling-main.574,N19-1119,0,0.0289102,"mapped into a shared space, and either encoder or decoder is frozen when training on a new language. Another related research line is curriculum learning. Most approaches concentrate on the selection of training samples according to their relevance to the translation task at hand. Different methods have been applied, for example, van der Wees et al. (2017) and Zhang et al. (2019) adapted a model to a domain by introducing samples which are increasingly domain-relevant or domain-distant respectively. Curriculum methods based on difficulty and competence were explored in Zhang et al. (2018) and Platanios et al. (2019). Ruiter et al. (2020) proposed a self-supervised NMT model, that uses data selection to train on increasingly complex and task-related samples in combination with a denoising curriculum. A stream of research focused on techniques more traditionally associated with CL has also been present. In the works of Miceli Barone et al. (2017); Khayrallah et al. (2018); Variˇs and Bojar (2019); Thompson et al. (2019) regularization approaches (e.g. EWC) were leveraged. Furthermore, Kim and Rush (2016) explored knowledge distillation, where the student model learns to match the teacher’s actions at the w"
2020.coling-main.574,2020.lrec-1.226,0,0.0269687,"a new task. Specifically, the metric called online codelength `(D) is defined as follows: `(D) = log2 |y |− N X log2 p yi |xi ; θDi−1  i=2 where |y |denotes the number of possible labels (classes) in the dataset D, and θDi stands for the model parameters trained on a particular subset of the dataset. Similar to LCA (Chaudhry et al., 2019a), online codelength is also related to an area under the learning curve. While most CL methods consider settings without human-in-the-loop, some allow a human domain expert to provide the model with empirical knowledge about the task at hand. For instance, Prokopalo et al. (2020) introduced the evaluation of human assisted learning across time by leveraging user-defined model adaptation policies for NLP and speech tasks, such as machine translation and speaker diarization. 3.3 Evaluation Datasets Most widely adopted CL benchmark datasets are image corpora such as P ERMUTED MNIST (Kirkpatrick et al., 2016), CUB-200 (Welinder et al., 2010; Wah et al., 2011), or split CIFAR-10/100 (Lopez-Paz and Ranzato, 2017). Benchmark corpora have also been proposed for objects - C ORE 50 (Lomonaco and Maltoni, 2017) and sound - AUDIO S ET (Gemmeke et al., 2017). However, none of the"
2020.coling-main.574,2020.emnlp-main.202,0,0.0302251,"Missing"
2020.coling-main.574,D19-1007,0,0.0401845,"Missing"
2020.coling-main.574,D16-1022,0,0.0276489,"n the multi-domain scenario (consult Barnes et al., 2018). This issue is of utmost importance if one thinks about CL in sentiment classification. One of the earliest approaches to CL for SA was proposed in Chen et al. (2015). According to Chen and Liu (2018), CL can enable SA models to adapt to a large number of domains, since many new domains may already be covered by other past domains. Additionally, SA systems should become more accurate not only in classification but also in the discovery of word polarities in specific domains. Research in opinion about aspects has been conducted as well. Shu et al. (2016) presented an unsupervised CL approach to classify opinion targets into entities and aspects. Furthermore, Shu et al. (2017) proposed a method based on conditional random fields to improve supervised aspect extraction across time. Experiments on text classification in the CL setting were performed in d’Autume et al. (2019) and Sun et al. (2020). 6530 4.5 Machine Translation The approach introduced by Luong and Manning (2015) laid the groundwork for subsequent studies in adapting neural machine translation (NMT). More specifically, the authors explored the adaptation through continued training,"
2020.coling-main.574,P17-2023,0,0.0250705,"iment classification. One of the earliest approaches to CL for SA was proposed in Chen et al. (2015). According to Chen and Liu (2018), CL can enable SA models to adapt to a large number of domains, since many new domains may already be covered by other past domains. Additionally, SA systems should become more accurate not only in classification but also in the discovery of word polarities in specific domains. Research in opinion about aspects has been conducted as well. Shu et al. (2016) presented an unsupervised CL approach to classify opinion targets into entities and aspects. Furthermore, Shu et al. (2017) proposed a method based on conditional random fields to improve supervised aspect extraction across time. Experiments on text classification in the CL setting were performed in d’Autume et al. (2019) and Sun et al. (2020). 6530 4.5 Machine Translation The approach introduced by Luong and Manning (2015) laid the groundwork for subsequent studies in adapting neural machine translation (NMT). More specifically, the authors explored the adaptation through continued training, where an NMT model trained using large corpora in one domain can later initialize a new NMT model for another domain. Their"
2020.coling-main.574,W17-4756,0,0.0553593,"Missing"
2020.coling-main.574,P15-1022,0,0.0423269,"Missing"
2020.coling-main.574,N19-1209,0,0.0177961,"by introducing samples which are increasingly domain-relevant or domain-distant respectively. Curriculum methods based on difficulty and competence were explored in Zhang et al. (2018) and Platanios et al. (2019). Ruiter et al. (2020) proposed a self-supervised NMT model, that uses data selection to train on increasingly complex and task-related samples in combination with a denoising curriculum. A stream of research focused on techniques more traditionally associated with CL has also been present. In the works of Miceli Barone et al. (2017); Khayrallah et al. (2018); Variˇs and Bojar (2019); Thompson et al. (2019) regularization approaches (e.g. EWC) were leveraged. Furthermore, Kim and Rush (2016) explored knowledge distillation, where the student model learns to match the teacher’s actions at the word- and sequence-level. Wei et al. (2019) proposed an on-line knowledge distillation approach, in which the best checkpoints are utilized as the teacher model. Lately, Li et al. (2020) demonstrated that label prediction continual learning leveraging compositionality brings improvements in NMT. 5 Research Gaps and Future Directions Although there is a growing number of task-specific approaches to CL in NLP,"
2020.coling-main.574,Q18-1029,0,0.029212,"ains, hence we argue this is not a sufficient solution. As dominant fine-tuning approaches require training and maintaining a separate model for each language or domain, Bapna and Firat (2019) proposed to add light-weight task-specific adapter modules to support parameter-efficient adaptation. We further argue that NMT – as opposed to phrase-based MT – rarely incorporates translation memory, and so it is inherently harder for NMT models to adapt using active or interactive learning. However, some attempts have been made (e.g. Peris and Casacuberta, 2018; Liu et al., 2018; Kaiser et al., 2017; Tu et al., 2018). In a similar vein, there were approaches (Sokolov et al., 2017) to incorporate bandit learners, which implicitly involve domain adaptation and on-line learning, for MT systems. We share the viewpoint of Farajian et al. (2017), that NMT models ultimately should be able to adapt on-the-fly to on-line streams of diverse data (i.e. language, domain), and thus CL for NMT is essential. While domain adaptation methods are widely used in the context of adapting NMT models, there have also been other attempts. Multilingual NMT (Dong et al., 2015; Firat et al., 2016; Ha et al., 2016; Johnson et al., 2"
2020.coling-main.574,P19-2017,0,0.0529602,"Missing"
2020.coling-main.574,W18-5446,0,0.0682406,"Missing"
2020.coling-main.574,N19-1086,0,0.386261,"e pass over the data, which is motivated by the need for a faster learning process. Another recent approach, proposed by d’Autume et al. (2019), relies on a sequentially presented stream of examples derived from various datasets in one pass, without revealing dataset boundary or identity to the model. 3.2 Benchmarks and Metrics For years the NLP domain has lagged behind computer vision and other ML areas (e.g. Kirkpatrick et al., 2016; Zenke et al., 2017; Lomonaco and Maltoni, 2017; Rebuffi et al., 2017b) when it comes to the availability of standard CL-related benchmarks (Greco et al., 2019; Wang et al., 2019b). However, the situation has slightly improved recently with an introduction of a handful of multi-task benchmarks. In particular, GLUE (Wang et al., 2018; Greco et al., 2019) and S UPER GLUE (Wang et al., 2019a) benchmarks track performance on eleven and ten language understanding tasks respectively, using existing NLP datasets. Along the same line, McCann et al. (2018) presented the Natural Language Decathlon (DECA NLP) benchmark for evaluating the performance of models across ten NLP tasks. The decathlon score (decaScore) is an additive combination of various metrics specific for each of"
2020.coling-main.574,D17-1147,0,0.041871,"Missing"
2020.coling-main.574,N19-1192,0,0.0752392,"8), or a method originating from NLP - MBPA++ (d’Autume et al., 2019). Knowledge distillation methods bear a close resemblance to episodic memory methods, but unlike GEM they keep the predictions at past tasks invariant (Rebuffi et al., 2017b; Lopez-Paz and Ranzato, 2017). In particular, it is a class of methods alleviating CF by relying on knowledge transfer from a large network model (teacher) to a new, smaller network (student) (Hinton et al., 2015). The underlying idea is that the student model learns to generate predictions of the teacher model. As demonstrated in Kim and Rush (2016) and Wei et al. (2019), knowledge distillation approaches can prove especially suitable for neural machine translation models, which are mostly large, and hence reduction in size is beneficial. Architectural methods prevent forgetting by applying modular changes to the network’s architecture and introducing task-specific parameters. Typically, previous task parameters are kept fixed (Rusu et al., 2016; Mancini et al., 2018) or masked out (Serra et al., 2018; Mallya and Lazebnik, 2018). Moreover, new layers are often injected dynamically to augment a model with additional modules to accommodate new tasks. Progressiv"
2020.coling-main.574,N19-1189,0,0.0345731,"Missing"
2020.ecomnlp-1.1,N18-1022,0,0.0185781,"ts to predict user’s tastes and recommend product items that exceptionally are interesting for them. In recommendation engines, the product and the user information is gathered to predict the score or choice of a user to a product. The information collected from a user is either directly or indirectly while the data for products is collected explicitly. In content-based filtering recommendations (Schafer et al., 2007), we explicitly gather information for both users and products and then compare the similarity to recommend the best choice. On the other hand, the collaborative-based algorithm (Bhagavatula et al., 2018) uses “User Behavior” for recommending items. They explore the behavior of users and products in terms of rating, selection, purchase history, and cookies (Isinkaye et al., 2015). We introduce two settings of recommendation systems for the fashion industry using both types of collaborative and content-based filtering. In the first approach, we propose a survey content-based recommender system and as a second, we introduce an enriched collaborative-based recommender using a novel weighted system. Finally, we initiate style rules that bring simplicity in selecting outfits based on available prod"
2020.ecomnlp-1.1,C10-2079,0,0.0425969,"etwork with an attention mechanism to mine visual attributes of products and then a recurrent neural network to translate the visual information into text format. As a result, the mutual information among products is discovered and the system can recommend outfits using these features. In (Akshaya et al., 2018) they used K-Nearest Neighbour algorithm to detect the nearest neighbor or a cluster based on the k value. There is also a considerable amount of work on recommending different types of categories such as dresses, backbags, heels and handbags along with rating top k products to display (Li et al., 2010). One implemented solution uses a text mining approach to improve ranked based recommender systems. Also, in (Ahuja et al., 2019) a combination of k-means Clustering (Singh et al., 2020) (Jin and Han, 2010) along with k-Nearest Neighbor is implemented on the movielens dataset (Harper and Konstan, 2015) to achieve an improving result. In their proposed technique, the recommender system predicts the user’s preference for a movie based on different parameters which also can be applied to product categories in the fashion industry. 3 Proposed Methods This section describes the two proposed methods"
2020.gebnlp-1.3,2020.winlp-1.25,1,0.75637,"Missing"
2020.gebnlp-1.3,2020.acl-main.485,0,0.0763767,"Missing"
2020.gebnlp-1.3,W19-3821,1,0.822621,"Missing"
2020.gebnlp-1.3,W19-3621,0,0.122756,"ch is a vectorization of words following the Word2Vec (Mikolov et al., 2013) technique, and we assume that the presence of bias in word embeddings is a kind of reflection of the biases in the dataset (Caliskan et al., 2017). We use 128 as the number of dimensions for these vectors, a minimum count of 5 to remove poorly represented words and a bidirectional window of 3 words, that is, given a word x[n], its ”context” is x[n − 3], . . . , x[n], . . . , x[n + 3] To perform the gender bias analysis of these words embeddings, we use the measures proposed in previous works (Bolukbasi et al., 2016b; Gonen and Goldberg, 2019a). Inspired by these previous studies, we make use of the following lists of words: • Definitional List 8 pairs (he/she; boy/girl; father/mother; male/female; his/her; himself/herself; man/woman; son/daughter) 27 • Biased List, which contains 1000 words, 500 female-biased, and 500 male-biased. (e.g., diet for female and hero for male) • Extended Biased List, extended version of Biased List (5000 words, 2500 female-biased, and 2500 male-biased) • Professional List 319 tokens (e.g., accountant, surgeon) 3.2.1 Gender Direction and Direct Bias 0.30 0.30 0.25 0.25 0.20 0.20 0.15 0.15 0.10 0.10 0.0"
2020.gebnlp-1.3,P07-2045,0,0.0171193,"ced dataset, having different proportions of the large dataset into it, and Concat, which contains the entire Large and Balanced datasets (see Figure 3). 4.2 Experimental Framework Generic Training Data To train the parent model, we used the English-Spanish EuroParl corpus (Koehn, 2005), which contains parallel data from the European Parliament’s proceedings. We extract a part of the corpus that consists of 2 million parallel sentences. We applied a preprocessing step that consisted of tokenizing, truecasing, and filtering. We performed all these steps using scripts from the well-known Moses (Koehn et al., 2007) scripts. Parameters We train the network for an undefined number of epochs until convergence with an early stopping policy. That policy consists of setting a patience, which means that if the validation loss does not improve in patience epochs, stop the training. We established that to 5 as it gives good results empirically. We used 512 embeddings dimension, 6 layers in the encoder and decoder, 8 attention heads. We used a batch size of 16, a dropout of 0.1, and a learning rate of 0.001. We optimized with Adam. Architecture We use the Transformer (Vaswani et al., 2017) as baseline NMT model a"
2020.gebnlp-1.3,2005.mtsummit-papers.11,0,0.742387,"ect and select entries in the desired languages; (2) a corpus aligner, which finds the parallel sentences within a text and provides a quality check of the parallel sentences given a few restrictions; (3) a gender classifier which includes a filtering module that classifies the gender of the entry and outputs the final parallel corpus. Hereinafter, we refer to this dataset as Balanced. We quantify the amount of gender bias in the collected dataset due to gender bias in word embeddings. This quantification of bias is also compared to the case of word embeddings computed on the EuroParl corpus (Koehn, 2005). 3.1 Balanced Dataset Generation We used the available Gebiotoolkit (Costa-juss`a et al., 2019) to extract the Balanced dataset. Gebiotoolkit is a tool for extracting multilingual parallel corpora at the sentence level, together with document and gender information from Wikipedia biographies. In this sense, the collected data set is not synthetic. We can generate this dataset from any of the languages available on Wikipedia. In our case, we have selected the English-Spanish language pair, which have considerable differences at the morphological level, and exhibit gender bias issues in NMT (Fo"
2020.gebnlp-1.3,P02-1040,0,0.106718,"Missing"
2020.gebnlp-1.3,2020.acl-main.690,0,0.0854739,"ocusing on trying to solve gender biases (Costa-juss`a, 2019). With this objective in mind, and in the specific context of NMT, we propose the use of balanced data sets to mitigate gender biases in a standard NMT system taking advantage of domain adaptation techniques. Previous research in the area of NMT has proposed to either mitigate biases using debiased word embeddings (Font and Costa-juss`a, 2019) and using contextual information (Basta et al., 2020) or evaluating and measuring the amount of bias present in the translation (Stanovsky et al., 2019). The closest work to ours is the one by Saunders and Byrne (2020) where authors generate a small genderbalanced dataset and use Elastic Weight Consolidation techniques to perform transfer learning and mitigate the consequences of training with unbalanced datasets. Differently from this one, we use a larger nonsynthetic balanced dataset to perform fine-tuning on an unbalanced-dataset and evaluate the reduction of gender bias in the final translation. 2 Bias statement As proposed in previous work (Blodgett et al., 2020), we formulate the bias statement of our work. Our work consists of studying the effects of using a gender-balanced dataset to mitigate gender"
2020.gebnlp-1.3,P19-1164,0,0.0919734,"ge processing applications, and, in particular, many works are focusing on trying to solve gender biases (Costa-juss`a, 2019). With this objective in mind, and in the specific context of NMT, we propose the use of balanced data sets to mitigate gender biases in a standard NMT system taking advantage of domain adaptation techniques. Previous research in the area of NMT has proposed to either mitigate biases using debiased word embeddings (Font and Costa-juss`a, 2019) and using contextual information (Basta et al., 2020) or evaluating and measuring the amount of bias present in the translation (Stanovsky et al., 2019). The closest work to ours is the one by Saunders and Byrne (2020) where authors generate a small genderbalanced dataset and use Elastic Weight Consolidation techniques to perform transfer learning and mitigate the consequences of training with unbalanced datasets. Differently from this one, we use a larger nonsynthetic balanced dataset to perform fine-tuning on an unbalanced-dataset and evaluate the reduction of gender bias in the final translation. 2 Bias statement As proposed in previous work (Blodgett et al., 2020), we formulate the bias statement of our work. Our work consists of studying"
2020.gebnlp-1.3,W17-1606,0,0.0266298,"in datasets is causing big disruptions in artificial intelligence applications. In this paper, we propose using an automatically extracted genderbalanced dataset parallel corpus from Wikipedia. This balanced set is used to perform fine-tuning techniques from a bigger model trained on unbalanced datasets to mitigate gender biases in neural machine translation. 1 Introduction Misrepresentation of individual communities in current datasets is causing severe disruptions in artificial intelligence applications. Examples of this are a lower performance of speech recognizers for women than for men (Tatman, 2017), a lower accuracy in face recognition for Asian faces than American or European ones (Xiong et al., 2018) and an amplification of stereotypes in Neural Machine Translation (NMT) (Font and Costa-juss`a, 2019). These challenges are at the core of natural language processing applications, and, in particular, many works are focusing on trying to solve gender biases (Costa-juss`a, 2019). With this objective in mind, and in the specific context of NMT, we propose the use of balanced data sets to mitigate gender biases in a standard NMT system taking advantage of domain adaptation techniques. Previo"
2020.lrec-1.191,S19-2138,0,0.0614279,"Missing"
2020.lrec-1.502,P19-1309,0,0.0815915,"ences based on the available metadata in Wikipedia texts. Both Yasuda and Sumita (2008) and Plamada and Volk (2012) extracted parallel sentences by translating the articles into a common language and consider those sentences with a high translation quality to be parallel. The ACCURAT project (S¸tef˘anescu et al., 2012; Skadin¸a et al., 2012) also devoted efforts in parallel sentence mining in Wikipedia. Later, Barr´on-Cede˜no et al. (2015) used the combination of cross-lingual similarity measures to extract domain specific parallel sentences. The most recent initiative is the so-called LASER (Artetxe and Schwenk, 2019b), which relies on vector representations of sentences to extract similar pairs. This toolkit has been used to extract the WikiMatrix corpus (Schwenk et al., 2019) which contains 135 million parallel sentences for 1,620 different language pairs in 85 different languages. As far as we are concerned, there is no gender-balanced dataset for machine translation, except for the artificial gold standard created for English–Spanish (Font and Costajuss`a, 2019). However, there are a notable number of works towards doing research in the area: from balancing data sets in monolingual tasks (Webster et a"
2020.lrec-1.502,Q19-1038,0,0.124746,"ences based on the available metadata in Wikipedia texts. Both Yasuda and Sumita (2008) and Plamada and Volk (2012) extracted parallel sentences by translating the articles into a common language and consider those sentences with a high translation quality to be parallel. The ACCURAT project (S¸tef˘anescu et al., 2012; Skadin¸a et al., 2012) also devoted efforts in parallel sentence mining in Wikipedia. Later, Barr´on-Cede˜no et al. (2015) used the combination of cross-lingual similarity measures to extract domain specific parallel sentences. The most recent initiative is the so-called LASER (Artetxe and Schwenk, 2019b), which relies on vector representations of sentences to extract similar pairs. This toolkit has been used to extract the WikiMatrix corpus (Schwenk et al., 2019) which contains 135 million parallel sentences for 1,620 different language pairs in 85 different languages. As far as we are concerned, there is no gender-balanced dataset for machine translation, except for the artificial gold standard created for English–Spanish (Font and Costajuss`a, 2019). However, there are a notable number of works towards doing research in the area: from balancing data sets in monolingual tasks (Webster et a"
2020.lrec-1.502,W15-3402,1,0.87962,"Missing"
2020.lrec-1.502,W19-3805,1,0.731759,"Missing"
2020.lrec-1.502,2012.eamt-1.60,0,0.0168473,"al dataset for 22 European languages plus Arabic with more than 12,000 sentences coming from JRCAcquis, that is, a collection of legislative texts of the European Union. In order to make the test set equal in all the languages, only sentences that are parallel simultaneously in the 22 languages were extracted (Koehn et al., 2009) and, therefore, the document structure of the data is lost. The Web Inventory of Transcribed and Translated Talks, WIT3(2) , includes English subtitles from TED talks and their translations currently in 109 languages. Parallel corpora are extracted for several pairs (Cettolo et al., 2012) and test sets are annually prepared for the International Workshop on Spoken Language Translation (IWSLT) evaluation campaigns. Test sets exist for all the pairs among German, English, Italian, Dutch and Romanian; and from English to Arabic, Basque, Chinese, Czech, Dutch, Farsi, French, German, Hebrew, Italian, Japanese, Korean, Polish, Portuguese-Brazil, Romanian, Russian, Slovak, Slovenian, Spanish, Thai, Turkish and Vietnamese. In this case the whole talks are aligned at sentence level, so, the document structure is kept but the set is not equivalent in all the languages. Similarly, the ne"
2020.lrec-1.502,2012.eamt-1.37,0,0.0225878,"Missing"
2020.lrec-1.502,W19-3821,1,0.718121,"Missing"
2020.lrec-1.502,E17-2038,0,0.0193044,"lel sen1 4081 http://www.statmt.org/wmt19/similar.html tence extraction from Wikipedia and a brief mention to general research lines on gender bias in NLP. Section 3. describes the general architecture and Section 4. the methodology, evaluation and characteristics of the extracted corpora. Finally, Section 5. summarizes the work and points at several future extensions of GeBioToolkit. 2. Related Work There are several multilingual parallel datasets available to evaluate MT outputs. The corpora covering more languages are JRC-Acquis (Acquis Communautaire) and the TED talks corpus. Arab-Acquis (Habash et al., 2017) is a multilingual dataset for 22 European languages plus Arabic with more than 12,000 sentences coming from JRCAcquis, that is, a collection of legislative texts of the European Union. In order to make the test set equal in all the languages, only sentences that are parallel simultaneously in the 22 languages were extracted (Koehn et al., 2009) and, therefore, the document structure of the data is lost. The Web Inventory of Transcribed and Translated Talks, WIT3(2) , includes English subtitles from TED talks and their translations currently in 109 languages. Parallel corpora are extracted for"
2020.lrec-1.502,2009.mtsummit-papers.7,0,0.0397743,"eral future extensions of GeBioToolkit. 2. Related Work There are several multilingual parallel datasets available to evaluate MT outputs. The corpora covering more languages are JRC-Acquis (Acquis Communautaire) and the TED talks corpus. Arab-Acquis (Habash et al., 2017) is a multilingual dataset for 22 European languages plus Arabic with more than 12,000 sentences coming from JRCAcquis, that is, a collection of legislative texts of the European Union. In order to make the test set equal in all the languages, only sentences that are parallel simultaneously in the 22 languages were extracted (Koehn et al., 2009) and, therefore, the document structure of the data is lost. The Web Inventory of Transcribed and Translated Talks, WIT3(2) , includes English subtitles from TED talks and their translations currently in 109 languages. Parallel corpora are extracted for several pairs (Cettolo et al., 2012) and test sets are annually prepared for the International Workshop on Spoken Language Translation (IWSLT) evaluation campaigns. Test sets exist for all the pairs among German, English, Italian, Dutch and Romanian; and from English to Arabic, Basque, Chinese, Czech, Dutch, Farsi, French, German, Hebrew, Itali"
2020.lrec-1.502,W19-5404,0,0.0178211,"able per language. Figure 2 shows the amount of these articles for the 20 languages with the largest number of biographies. The edition with the most available entries is the English one with 922,120 entries. The Spanish Wikipedia contains 148,445 entries (6th largest one) and the Catalan edition 40,983 (20th largest one). All three are highlighted in Figure 2. Even if the Catalan Wikipedia is not as big as the English and Spanish ones, there is a noticeable amount of comparable articles between Spanish and Catalan which translates into significant number of parallel sentences —Schwenk et al. (2019) extracted in WikiMatrix 3,377 k sentences for en–es, 1,580 k sentences for ca–es and 210 k sentences for en–ca from the full editions. GeBioToolkit extracts multilingually aligned parallel sentences, so it is interesting to study also the union of languages. The more languages involved, the lesser number or comparable articles one will obtain. Figure 3 shows the number of articles per sets of languages and broken-down by gender. The total number of documents decays when starting with English and Arabic (set with only 2 languages) and one incrementally adds French, German, Italian, Spanish, Po"
2020.lrec-1.502,D18-1512,0,0.0368879,"Missing"
2020.lrec-1.502,D18-1302,0,0.0436951,"Missing"
2020.lrec-1.502,N18-2002,0,0.114616,"Missing"
2020.lrec-1.502,skadina-etal-2012-collecting,0,0.0297572,"ations of PoS, lemmas, syntactic dependencies, anaphora, discourse connectives, classified named entities and temporal expressions. The authors in Bamman and Smith (2014) also extract 927,403 biographies in this case from the English Wikipedia. The set is pre-processed in order to learn event classes in biographies. Regarding the automatic extraction of parallel corpora, Wikipedia has been traditionally used as a resource. In Adafre and de Rijke (2006), the authors extract parallel sentences based on the available metadata in Wikipedia texts. Both Yasuda and Sumita (2008) and Plamada and Volk (2012) extracted parallel sentences by translating the articles into a common language and consider those sentences with a high translation quality to be parallel. The ACCURAT project (S¸tef˘anescu et al., 2012; Skadin¸a et al., 2012) also devoted efforts in parallel sentence mining in Wikipedia. Later, Barr´on-Cede˜no et al. (2015) used the combination of cross-lingual similarity measures to extract domain specific parallel sentences. The most recent initiative is the so-called LASER (Artetxe and Schwenk, 2019b), which relies on vector representations of sentences to extract similar pairs. This too"
2020.lrec-1.502,P19-1164,0,0.0646622,"Missing"
2020.lrec-1.502,Q18-1042,0,0.09189,": corpora, gender bias, Wikipedia, machine translation 1. Introduction Gender biases are present in many natural language processing applications (Costa-juss`a, 2019). This comes as an undesired characteristic of deep learning architectures where their outputs seem to reflect demographic asymmetries (Prates et al., 2019). This is of course not due to the architecture itself but to the data used to train a system. Recent research is being devoted to correct the asymmetries mainly by data augmentation techniques in fields such as coreference resolution (Rudinger et al., 2018; Zhao et al., 2018; Webster et al., 2018) or abusive language detection (Park et al., 2018). Test sets have been created in those cases, but we are not aware of any test set available for machine translation (MT). From another side, machine translation either neural, statistical or rule-based, usually operates in a sentence-bysentence basis. However, when translating consistently a document, surrounding sentences may have valuable information. The translation of pronouns, verb tenses and even content words might depend on other fragments within a document. This affects also the translation of the gender markers, specially when transl"
2020.lrec-1.502,N18-2003,0,0.122658,"Missing"
2020.lrec-1.677,P19-1620,0,0.0174709,"English, hindering advancement in Multilingual QA research. Several approaches based on cross-lingual learning and synthetic corpora generation have been proposed. Crosslingual learning refers to zero, and few-shot techniques applied to transfer the knowledge of a QA model trained on many source examples to a given target language with fewer training data. (Artetxe et al., 2019; Lee and Lee, 2019; Liu et al., 2019a) On the other hand, synthetic corpora generation methods are machine-translation (MT) based designed to automatically generate language-specific QA datasets as training resources (Alberti et al., 2019; Lee et al., 2018; T¨ure and Boschee, 2016). Additionally, a multilingual QA system based on MT at test time has also been explored (Asai et al., 2018) In this paper, we follow the synthetic corpora generation approach. In particular, we developed the Translate-Align-Retrieve (TAR) method, based on MT and unsupervised alignment algorithm to translate an English QA dataset to Spanish automatically. Indeed, we applied our method to the popular SQuAD v1.1 generating its first Spanish version. We then trained two Spanish QA systems by fine-tuning the pre-trained Multilingual-BERT model. Finally,"
2020.lrec-1.677,N19-1423,0,0.122724,"Missing"
2020.lrec-1.677,P17-4012,0,0.0106044,"entually a joint source-target BPE segmentation (Sennrich et al., 2016) with a maximum of 50k BPE symbols. Then, we filtered out sentences longer than 80 tokens and removed all source-target duplicates. The final corpora consist of almost 6.5M parallel sentences for the training set, 5k sentence for the validation and 1k for the test set. The pre-processing pipeline is performed with the scripts in the Moses repository3 and the Subword-nmt repository4 . We then trained the NMT system with the Transformer model (Vaswani et al., 2017). We used the implementation available in OpenNMT-py toolkit (Klein et al., 2017) in its default configuration for 200k steps with one GeForce GTX TITAN X device. Additionally, we shared the source and target vocabularies and consequently, we also share the corresponding source an target embeddings between the encoder and decoder. After the training, our best model is obtained by averaging across the final three consecutive checkpoints. Finally, we evaluated the NMT system with the BLEU score (Papineni et al., 2002) on our test set. The model achieved a BLEU score of 45.60 point showing that the it is good enough to be used as a pre-trained English-Spanish translator suita"
2020.lrec-1.677,Q19-1026,0,0.0459891,"Missing"
2020.lrec-1.677,L18-1437,0,0.0608273,"Missing"
2020.lrec-1.677,D19-1283,0,0.0280763,"Missing"
2020.lrec-1.677,P19-1227,0,0.0373135,"Missing"
2020.lrec-1.677,P02-1040,0,0.108564,"rd-nmt repository4 . We then trained the NMT system with the Transformer model (Vaswani et al., 2017). We used the implementation available in OpenNMT-py toolkit (Klein et al., 2017) in its default configuration for 200k steps with one GeForce GTX TITAN X device. Additionally, we shared the source and target vocabularies and consequently, we also share the corresponding source an target embeddings between the encoder and decoder. After the training, our best model is obtained by averaging across the final three consecutive checkpoints. Finally, we evaluated the NMT system with the BLEU score (Papineni et al., 2002) on our test set. The model achieved a BLEU score of 45.60 point showing that the it is good enough to be used as a pre-trained English-Spanish translator suitable for our purpose. 2.2. Source-Translation Context-Alignment The role of the alignment component is to compute the alignment between the context sentences and their trans2 https://github.com/facebookresearch/LASER https://github.com/moses-smt/mosesdecoder 4 https://github.com/rsennrich/subword-nmt lations. We relied on an efficient and accurate unsuper¨ vised word alignment called eflomal (Ostling and Tiedemann, 2016) based on a Bayes"
2020.lrec-1.677,P18-2124,0,0.0320292,"AR method to generated the SQuAD-es v1.1 dataset, the first large-scale training resources for Spanish QA. Finally, we employed the SQuADes v1.1 dataset to train QA systems that achieved state-ofthe-art perfomance on the Spanish QA task, demonstrating the efficacy of the TAR approach for synthetic corpora generation. Therefore, we make the SQuAD-es dataset freely available and encourage its usage for multilingual QA. The results achieved so far encourage us to look forward and extend our approach in future works. First of all, we will apply the TAR method to translated the SQuAD v2.0 dataset (Rajpurkar et al., 2018) and other large-scale extractive QA such as Natural Questions(Kwiatkowski et al., 2019). Moreover, we will also exploit the modularity of the TAR method to support languages other than Spanish to prove the validity of our approach for synthetic corpora generation. 6 https://rajpurkar.github.io/SQuAD-explorer/ Acknowledgements This work is supported in part by the Spanish Ministerio de Econom´ıa y Competitividad, the European Regional Development Fund and the Agencia Estatal de Investigaci´on, through the postdoctoral senior grant Ram´on y Cajal (FEDER/MINECO) amd the project PCIN-2017-079 (AE"
2020.lrec-1.677,P16-1162,0,0.00591661,"rtetxe and Schwenk, 2018a; Artetxe and Schwenk, 2018b) to extract N-way parallel corpora from Wikipedia. Then, to further increase the size of the parallel data, we gathered additional resources from the open-source OPUS corpus (Tiedemann, 2012). Eventually, we selected data from 5 different resources, such as Wikipedia, TED-2013, News-Commentary, Tatoeba and OpenSubTitles (Lison and Tiedemann, 2016; Wolk and Marasek, 2015; Tiedemann, 2012). The data pre-processing pipeline consisted of punctuation normalization, tokenisation, true-casing and eventually a joint source-target BPE segmentation (Sennrich et al., 2016) with a maximum of 50k BPE symbols. Then, we filtered out sentences longer than 80 tokens and removed all source-target duplicates. The final corpora consist of almost 6.5M parallel sentences for the training set, 5k sentence for the validation and 1k for the test set. The pre-processing pipeline is performed with the scripts in the Moses repository3 and the Subword-nmt repository4 . We then trained the NMT system with the Transformer model (Vaswani et al., 2017). We used the implementation available in OpenNMT-py toolkit (Klein et al., 2017) in its default configuration for 200k steps with on"
2020.lrec-1.677,D16-1055,0,0.0544352,"Missing"
2020.lrec-1.677,tiedemann-2012-parallel,0,0.0191968,"ted the first TAR component from scratch, by training an NMT model for English to Spanish direction. Our NMT parallel corpus is created by collecting the en-es parallel data from several resources. We first collected data from the WikiMatrix project (Schwenk et al., 2019) which uses state-of-the-art multilingual sentence embeddings techniques from the LASER toolkit2 (Artetxe and Schwenk, 2018a; Artetxe and Schwenk, 2018b) to extract N-way parallel corpora from Wikipedia. Then, to further increase the size of the parallel data, we gathered additional resources from the open-source OPUS corpus (Tiedemann, 2012). Eventually, we selected data from 5 different resources, such as Wikipedia, TED-2013, News-Commentary, Tatoeba and OpenSubTitles (Lison and Tiedemann, 2016; Wolk and Marasek, 2015; Tiedemann, 2012). The data pre-processing pipeline consisted of punctuation normalization, tokenisation, true-casing and eventually a joint source-target BPE segmentation (Sennrich et al., 2016) with a maximum of 50k BPE symbols. Then, we filtered out sentences longer than 80 tokens and removed all source-target duplicates. The final corpora consist of almost 6.5M parallel sentences for the training set, 5k senten"
2020.spnlp-1.1,D19-1633,0,0.0126353,"h a pattern describing the left and right dependencies of the token at that position in the Otok sequence. An example of dependency expansion could be [nsubj-advmod-HEAD-xcomp] for the word “likes” in the dependency parse tree from Figure 1. After each iteration, the output of the model is expanded.1 This consists of creating a new sequence Iterative Refinement Lee et al. (2018) propose a latent variable nonautoregressive machine translation model where first the target length is predicted by the model, and then, the decoder is iteratively applied to its own output to refine it. Mask-predict (Ghazvininejad et al., 2019) also predicts the target sentence length and then nonautoregressively predicts the sentence itself, iteratively refining it a fixed number of times, masking out and regenerating the tokens it is least confident 1 The expansion of the output to be fed as input in the next iteration occurs in the CPU outside of the neural model itself. 2 by combining the tokens from Itok , Otok and Oexp . This process is illustrated in Figure 2, making use of the dependency tree from Figure 1. When there is a padding token [pad] in the output (either Otok or Oexp ), this means that the output at that position i"
2020.spnlp-1.1,Q19-1042,0,0.0129358,"to train a new kind of language model where the token generation order is driven by the dependency parse tree of the sentence and where the generation process is iterative. 3 Insertion-based Generation Stern et al. (2019) propose a conditional generative model that iteratively generates tokens plus the position at which they should be inserted within the sequence. Emelianenko et al. (2019) further propose to optimize the generation order by sampling from the ordering permutations. Instead, Chan et al. (2019) optimize a lower bound of the marginalized probability over every possible ordering. Gu et al. (2019a) handle the generation order as a latent variable that is captured as the relative position through self-attention, optimizing the ELBO to train the model. Levenshtein Transformer (Gu et al., 2019b) is a non-autoregressive approach trained with reinforcement learning (RL) to generate token insertion and deletion actions. While it benefits from the same generation speed-ups over autoregressive models as our model, it has the added difficulty of learning an insertion/deletion policy using RL without any linguistically or empirically motivated priors, which can be slow or difficult to obtain co"
2020.spnlp-1.1,P19-1122,0,0.0190442,"et al., 2016) are recursive models that operate with a stack of symbols that can be populated with terminals or nonterminals, or “reduced” to generate a syntactic constituent, obtaining as a result a sentence and its associated constituency parse tree. Shen et al. (2018) use skip-connections to integrate constituent relations with RNNs, learning the underlying dependency structures by leveraging a syntactic distance together with structured 1 Proceedings of 4th Workshop on Structured Prediction for NLP, pages 1–10 c November 20, 2020. 2020 Association for Computational Linguistics attention. Akoury et al. (2019) use a simplified constituency tree as latent variables, modeling it autoregressively to later use it as input for a nonautoregressive transformer that generates the output sentence. Ordered neurons (Shen et al., 2019) are modified LSTMs where the latent sentence tree structure is used to control the dependencies between recurrent units with a special “master” input and forget gates. about. Lawrence et al. (2019) follow a similar approach and start with a sequence of placeholder tokens (all the same) of a specified length, and they iteratively replace them with normal tokens via masked LM-styl"
2020.spnlp-1.1,N18-1086,0,0.0212141,"with a similar structure to an n-gram LM, but where the context of a word is its preceding bigram plus a list of preceding words whose parent does not precede it. Shen et al. (2008) make use of the dependency tree in a probabilistic LM, computing the probability of each word conditioned on its parent and the sibling words between both. Mirowski and Vlachos (2015) propose a dependency LM based on RNNs, where the dependency tree is decomposed into a collection of unrolls, that is, paths from the root to one of the leaves, and where the probability of a word can be predicted from these unrolls. Buys and Blunsom (2018) propose a shift-reduce transition-based LSTM (Hochreiter and Schmidhuber, 1997) dependency LM that can be used for language modeling and generation by means of dynamic programming. Our experiments show that this paradigm is effective at text generation, with quality between LSTMs and Transformers, and comparable diversity, requiring less than half their decoding steps, and its generation process allows direct control over the syntactic constructions of the generated text, enabling the induction of stylistic variations. 1 Related Work Introduction The currently dominant text generation paradig"
2020.spnlp-1.1,D19-1001,0,0.016544,"stance together with structured 1 Proceedings of 4th Workshop on Structured Prediction for NLP, pages 1–10 c November 20, 2020. 2020 Association for Computational Linguistics attention. Akoury et al. (2019) use a simplified constituency tree as latent variables, modeling it autoregressively to later use it as input for a nonautoregressive transformer that generates the output sentence. Ordered neurons (Shen et al., 2019) are modified LSTMs where the latent sentence tree structure is used to control the dependencies between recurrent units with a special “master” input and forget gates. about. Lawrence et al. (2019) follow a similar approach and start with a sequence of placeholder tokens (all the same) of a specified length, and they iteratively replace them with normal tokens via masked LM-style inference. As the masking strategy for the training data, the authors propose different stochastic processes to randomly select which placeholders are to be uncovered. 2.3 Our proposal is to train a new kind of language model where the token generation order is driven by the dependency parse tree of the sentence and where the generation process is iterative. 3 Insertion-based Generation Stern et al. (2019) prop"
2020.spnlp-1.1,D18-1149,0,0.0201327,"m a vocabulary with all possible textual tokens (terminal tokens). The second output, Oexp , is a sequence of tokens called expansion placeholders, which are taken from a separate vocabulary. Each expansion placeholder is associated with a pattern describing the left and right dependencies of the token at that position in the Otok sequence. An example of dependency expansion could be [nsubj-advmod-HEAD-xcomp] for the word “likes” in the dependency parse tree from Figure 1. After each iteration, the output of the model is expanded.1 This consists of creating a new sequence Iterative Refinement Lee et al. (2018) propose a latent variable nonautoregressive machine translation model where first the target length is predicted by the model, and then, the decoder is iteratively applied to its own output to refine it. Mask-predict (Ghazvininejad et al., 2019) also predicts the target sentence length and then nonautoregressively predicts the sentence itself, iteratively refining it a fixed number of times, masking out and regenerating the tokens it is least confident 1 The expansion of the output to be fed as input in the next iteration occurs in the CPU outside of the neural model itself. 2 by combining th"
2020.spnlp-1.1,W02-0109,0,0.412669,"Missing"
2020.spnlp-1.1,N16-1024,0,0.0318915,"Zipser, 1989). Other architectures, such as Transformer (Vaswani et al., 2017), while not intrinsically sequential, have also been targeted for sequential generation. On the other hand, some recent lines of research have focused on nonsequential generation. In this work, we propose a new paradigm for text generation and language modeling called Iterative Expansion Language Model, which generates the final sequence following a token ordering defined by the sentence dependency parse by iteratively expanding each level of the tree. 2.2 Syntax-driven Generation Recurrent neural network grammars (Dyer et al., 2016) are recursive models that operate with a stack of symbols that can be populated with terminals or nonterminals, or “reduced” to generate a syntactic constituent, obtaining as a result a sentence and its associated constituency parse tree. Shen et al. (2018) use skip-connections to integrate constituent relations with RNNs, learning the underlying dependency structures by leveraging a syntactic distance together with structured 1 Proceedings of 4th Workshop on Structured Prediction for NLP, pages 1–10 c November 20, 2020. 2020 Association for Computational Linguistics attention. Akoury et al."
2020.spnlp-1.1,P15-2084,0,0.0176595,"ency treedriven LMs (§2.1), syntax-driven generation (§2.2), insertion-based approaches (§2.3) and iterative refinement approaches (§2.4). 2.1 Dependency LMs The use of dependency parse trees to drive a language model was first proposed by Chelba et al. (1997), with a similar structure to an n-gram LM, but where the context of a word is its preceding bigram plus a list of preceding words whose parent does not precede it. Shen et al. (2008) make use of the dependency tree in a probabilistic LM, computing the probability of each word conditioned on its parent and the sibling words between both. Mirowski and Vlachos (2015) propose a dependency LM based on RNNs, where the dependency tree is decomposed into a collection of unrolls, that is, paths from the root to one of the leaves, and where the probability of a word can be predicted from these unrolls. Buys and Blunsom (2018) propose a shift-reduce transition-based LSTM (Hochreiter and Schmidhuber, 1997) dependency LM that can be used for language modeling and generation by means of dynamic programming. Our experiments show that this paradigm is effective at text generation, with quality between LSTMs and Transformers, and comparable diversity, requiring less th"
2020.spnlp-1.1,P02-1040,0,0.107163,"onducted this experiment with the wordlevel models trained on EMNLP2017 News data. We compute the ratio of adjectives per sentence to verify the increased presence of adjectives, while controlling quality and diversity measures over the generated text for potential degradation. Experimental Setup Unconditional Text Generation We conducted experiments on unconditional text generation following the methodology used by Caccia et al. (2020). The goal is to assess both the quality and diversity of the text generated by the model and the baselines. For the quality evaluation, we use the BLEU score (Papineni et al., 2002) over the test set, where each generated sentence is evaluated against the whole test set as a reference. For diversity, we used the self-BLEU score (Zhu et al., 2018), computed using as references the rest of the generated sentences. For each model, the temperature of the final softmax τ is tuned to generate text in the closest quality/diversity regime to the training data. Iterative expansion LMs are compared against a standard LM baselines, namely, AWD-LSTM2 (Merity et al., 2018) and a Transformer LM (Vaswani et al., 2017), both with word (w) and BPE subword (sw) vocabularies. The models 5"
2020.spnlp-1.1,P16-1162,0,0.0177126,"ne dependency to the left and one to the right. For each word with more than one dependency on any of its sides, we rearrange the tree to force left-to-right dependencies. Although this tree binarization reduces the degree of parallelism, it reduces data sparsity and allows handling constructions with a number of dependencies may otherwise be too large for the model to properly capture, such as enumerations (e.g., “I bought a pair of shoes, an umbrella, a beautiful jacket and a bracelet”). Iterative expansion LMs can be naturally extended to subword vocabularies, like byte-pair encoding (BPE; Sennrich et al., 2016): for each word, we decompose its node in the tree into as many 3.2 Training For training iterative expansion LMs, the main input of the model is the tokens at one of the levels of the dependency parse tree (Itok ), while the output is the following level tokens (Otok ) and expansion placeholders (Oexp ). A secondary input to the model are the dependency indexes, which are used in the head position embedding. The model is trained with the categorical crossentropy for both tokens and expansion placeholders, then adding both sublosses into the final loss (with equal weights). Tokens generated in"
2020.spnlp-1.1,W19-3620,0,0.0201931,"et al., 2019b) is a non-autoregressive approach trained with reinforcement learning (RL) to generate token insertion and deletion actions. While it benefits from the same generation speed-ups over autoregressive models as our model, it has the added difficulty of learning an insertion/deletion policy using RL without any linguistically or empirically motivated priors, which can be slow or difficult to obtain convergence in practice. By comparison, our approachmakes uses a linguistically motivated prior for word insertion in a fully supervised way, avoiding the optimization difficulties of RL. Welleck et al. (2019) use cost minimization imitation learning to learn a policy to generate a binary tree that is used to drive the token generation. 2.4 Iterative Expansion LMs ROOT nsubj poss My advmod dog also xcomp likes eating dobj sausage Figure 1: Example of dependency parse tree. The input vocabulary contains terminal tokens as well as non-terminal special tokens called dependency placeholders, each of which is associated with one of the possible dependency relations to the heads. For the dependency tree in Figure 1, the dependency placeholders are [poss], [nsubj], [advmod], [xcomp], [dobj] and [ROOT]. Th"
2020.spnlp-1.1,P08-1066,0,0.0715277,"endency parse tree is used to drive the Transformer model to generate sentences iteratively. In this section, we provide an overview of works related to ours, including dependency treedriven LMs (§2.1), syntax-driven generation (§2.2), insertion-based approaches (§2.3) and iterative refinement approaches (§2.4). 2.1 Dependency LMs The use of dependency parse trees to drive a language model was first proposed by Chelba et al. (1997), with a similar structure to an n-gram LM, but where the context of a word is its preceding bigram plus a list of preceding words whose parent does not precede it. Shen et al. (2008) make use of the dependency tree in a probabilistic LM, computing the probability of each word conditioned on its parent and the sibling words between both. Mirowski and Vlachos (2015) propose a dependency LM based on RNNs, where the dependency tree is decomposed into a collection of unrolls, that is, paths from the root to one of the leaves, and where the probability of a word can be predicted from these unrolls. Buys and Blunsom (2018) propose a shift-reduce transition-based LSTM (Hochreiter and Schmidhuber, 1997) dependency LM that can be used for language modeling and generation by means o"
2020.spnlp-1.1,J93-2004,0,\N,Missing
2020.spnlp-1.1,P14-5010,0,\N,Missing
2020.spnlp-1.1,P06-4018,0,\N,Missing
2020.spnlp-1.1,N19-1423,0,\N,Missing
2020.wmt-1.1,2020.nlpcovid19-2.5,1,0.796341,"tails of the evaluation. 4.1.1 Covid Test Suite TICO-19 The TICO-19 test suite was developed to evaluate how well can MT systems handle the newlyemerged topic of COVID-19. Accurate automatic translation can play an important role in facilitating communication in order to protect at-risk populations and combat the infodemic of misinformation, as described by the World Health Organization. The test suite has no corresponding paper so its authors provided an analysis of the outcomes directly here. The submitted systems were evaluated using the test set from the recently-released TICO-19 dataset (Anastasopoulos et al., 2020). The dataset provides manually created translations of COVID19 related data. The test set consists of PubMed articles (678 sentences from 5 scientific articles), patient-medical professional conversations (104 sentences), as well as related Wikipedia articles (411 sentences), announcements (98 sentences from Wikisource), and news items (67 sentences from Wikinews), for a total of 2100 sentences. Table 15 outlines the BLEU scores by each submitted system in the English-to-X directions, also breaking down the results per domain. The analysis shows that some systems are significantly more prepar"
2020.wmt-1.1,2020.wmt-1.6,0,0.0647231,"AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua Universit"
2020.wmt-1.1,2020.wmt-1.38,0,0.0746945,"Missing"
2020.wmt-1.1,2020.wmt-1.54,1,0.802974,"Missing"
2020.wmt-1.1,W07-0718,1,0.671054,"Missing"
2020.wmt-1.1,W08-0309,1,0.762341,"Missing"
2020.wmt-1.1,W12-3102,1,0.500805,"Missing"
2020.wmt-1.1,2020.lrec-1.461,0,0.0795779,"Missing"
2020.wmt-1.1,2012.eamt-1.60,0,0.124643,"tted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS"
2020.wmt-1.1,2020.wmt-1.3,0,0.0731913,"on Machine Translation (WMT20)1 was held online with EMNLP 2020 and hosted a number of shared tasks on various aspects of machine translation. This conference built on 14 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; CallisonBurch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018; Barrault et al., 2019). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • automatic post-editing (Chatterjee et al., 2020) • biomedical translation (Bawden et al., 2020b) • chat translation (Farajian et al., 2020) • lifelong learning (Barrault et al., 2020) 1 Makoto Morishita NTT Santanu Pal WIPRO AI Abstract 1 Philipp Koehn JHU http://www.statmt.org/wmt20/ 1 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1–55 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics as “direct assessment”) that we explored in the previous years with convincing results in terms of the trade-off between annotation effort and reliable distinctions between systems. The primary objectives o"
2020.wmt-1.1,2020.wmt-1.8,0,0.0898111,"D D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Know"
2020.wmt-1.1,2009.freeopmt-1.3,0,0.088081,"ve (CONTRASTIVE) or primary (PRIMARY), and the BLEU, RIBES and TER results. The scores are sorted by BLEU. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. This year we recived major number of participants for the case of Indo-Aryan language group NUST-FJWU NUST-FJWU system is an extension of state-of-the-art Transformer model with hierarchical attention networks to incorporate contextual information. During training the model used back-translation. Prompsit This team is participating with a rulebased system based on Apertium (Forcada et al., 2009-11). Apertium is a free/open-source platform for developing rule-based machine translation systems and language technology that was first released in 2005. Apertium is hosted in Github where both language data and code are licensed under the GNU GPL. It is a research and business platform with a very active community that loves small languages. Language pairs are at a very different level of development and output quality in the platform, depending on two main variables: how much funded or in-kind effort has 32 5.4 i.e. Hindi–Marathi (in both directions). We received 22 submissions from 14 te"
2020.wmt-1.1,2020.wmt-1.80,0,0.0933589,"airs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new for this year. Furthermore, English to and from Khmer and Pashto were included, using the same test sets as in the corpus filtering task. Th"
2020.wmt-1.1,W19-5204,0,0.0543621,"Missing"
2020.wmt-1.1,2020.emnlp-main.5,0,0.0410594,"luation of out-ofEnglish translations, HITs were generated using the same method as described for the SR+DC evaluation of into-English translations in Section 3.2.1 with minor modifications. Source-based DA allows to include human references in the evaluation as another system to provide an estimate of human performance. Human references were added to the pull of system outputs prior to sampling documents for tasks generation. If multiple references are available, which is the case for English→German (3 alternative reference translations, including 1 generated using the paraphrasing method of Freitag et al. (2020)) and English→Chinese (2 translations), each reference is assessed individually. Since the annotations are made by researchers and professional translators who ensure a betTable 11: Amount of data collected in the WMT20 manual document- and segment-level evaluation campaigns for bilingual/source-based evaluation out of English and nonEnglish pairs. et al., 2020; Laubli et al., 2020). It differs from SR+DC DA introduced in WMT19 (Bojar et al., 2019), and still used in into-English human evaluation this year, where a single segment from a document is provided on a screen at a time, followed by s"
2020.wmt-1.1,W18-3931,1,0.874637,"ese improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art translation systems on trans"
2020.wmt-1.1,2020.wmt-1.18,0,0.0913945,"2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al., 2020) Baseline System from Biomedical Task (Bawden et al., 2020b) American University of Beirut (no associated paper) Zoho Corporation (no associated paper) Table 6: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion"
2020.wmt-1.1,2020.wmt-1.43,0,0.0835067,"Missing"
2020.wmt-1.1,2020.wmt-1.9,0,0.0939415,"Missing"
2020.wmt-1.1,2020.wmt-1.19,0,0.0674131,"Missing"
2020.wmt-1.1,2009.mtsummit-btm.6,0,0.103443,"Missing"
2020.wmt-1.1,W13-2305,1,0.929934,"work which can be well applied to different translation. directions. Techniques used in the submitted systems include optional multilingual pre-training (mRASP) for low resource languages, very deep Transformer or dynamic convolution models up to 50 encoder layers, iterative backtranslation, knowledge distillation, model ensemble and development set fine-tuning. The key ingredient of the process seems the strong focus on diversification of the (synthetic) training data, using multiple scalings of the Transformer model 3.1 Direct Assessment Since running a comparison of direct assessments (DA, Graham et al., 2013, 2014, 2016) and relative ranking in 2016 (Bojar et al., 2016) and verifying a high correlation of system rankings for the two methods, as well as the advantages of DA, such as quality controlled crowd-sourcing and linear growth relative to numbers of submissions, we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100"
2020.wmt-1.1,E14-1047,1,0.888167,"Missing"
2020.wmt-1.1,2020.lrec-1.312,1,0.804196,"A screenshot of OCELoT is shown in Figure 5. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, Engli"
2020.wmt-1.1,2020.emnlp-main.6,1,0.839606,"shown in Table 3, where the first and second are simple merges or splits, whereas the third is a rare case of more complex reordering. We leave a detailed analysis of the translators’ treatment of paragraph-split data for future work. development set is provided, it is a mixture of both “source-original” and “target-original” texts, in order to maximise its size, although the original language is always marked in the sgm file, except for Inuktitut↔English. The consequences of directionality in test sets has been discussed recently in the literature (Freitag et al., 2019; Laubli et al., 2020; Graham et al., 2020), and the conclusion is that it can have an effect on detrimental effect on the accuracy of system evaluation. We use “source-original” parallel sentences wherever possible, on the basis that it is the more realistic scenario for practical MT usage. Exception: the test sets for the two Inuktitut↔English translation directions contain the same data, without regard to original direction. For most news text in the test and development sets, English was the original language and Inuktitut the translation, while the parliamentary data mixes the two directions. The origins of the news test documents"
2020.wmt-1.1,2020.wmt-1.11,0,0.0940191,"N GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) S"
2020.wmt-1.1,D19-1632,1,0.881933,"ent and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS (software localization, Tatoeba, Global Voices), the Bible, and specialprepared corpora from TED Talks and the Jehova Witness web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics ab"
2020.wmt-1.1,2020.wmt-1.12,1,0.754946,"Missing"
2020.wmt-1.1,2020.wmt-1.13,0,0.0737827,"2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al"
2020.wmt-1.1,2020.wmt-1.20,0,0.057602,"Missing"
2020.wmt-1.1,2020.wmt-1.14,1,0.820019,"set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Mari"
2020.wmt-1.1,2020.wmt-1.39,1,0.812433,"kables was collected in the first phase of the annotation, which amounted to 4k assessments across the systems. The second annotation phase with 6.5k assessments compared markable translations, always checking outputs of all the 13 competing MT systems but still considering the document-level context of each of them. Among other things, the observations indicate that the better the system, the lower the variance in manual scores. Markables annotation then confirms that frequent errors like bad translation of a term need not be the most severe and conversely, 4.1.3 Gender Coreference and Bias (Kocmi et al., 2020) The test suite by Kocmi et al. (2020) focuses on the gender bias in professions (e.g. physician, teacher, secretary) for the translation from English into Czech, German, Polish and Russian. These nouns are ambiguous with respect to gender in English but exhibit gender in the examined target languages. The test suite is based on the fact that a pronoun referring to the ambiguous noun can reveal the gender of the noun in the English source sentence. Once disambiguated, the gender needs to be preserved in translation. To correctly translate the given noun, the translation system thus has to corr"
2020.wmt-1.1,2020.wmt-1.53,0,0.089538,"Missing"
2020.wmt-1.1,2020.wmt-1.78,1,0.815194,"rence on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new fo"
2020.wmt-1.1,W17-1208,0,0.0524248,"ttribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art tr"
2020.wmt-1.1,2020.wmt-1.21,0,0.0791885,"Missing"
2020.wmt-1.1,2020.wmt-1.23,0,0.0607352,"020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2"
2020.wmt-1.1,2020.wmt-1.77,1,0.84299,"slation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inu"
2020.wmt-1.1,2020.wmt-1.24,0,0.0435945,"Missing"
2020.wmt-1.1,D18-1512,0,0.05415,"Missing"
2020.wmt-1.1,2020.wmt-1.47,1,0.740067,"Missing"
2020.wmt-1.1,W18-3601,1,0.891679,", we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100 rating scale.5 No sentence or document length restriction is applied during manual evaluation. Direct Assessment is also employed for evaluation of video captioning systems at TRECvid (Graham et al., 2018; Awad et al., 2019) and multilingual surface realisation (Mille et al., 2018, 2019). 3.1.1 tion 2, most of our test sets do not include reversecreated sentence pairs, except when there were resource constraints on the creation of the test sets. 3.1.3 Prior to WMT19, the issue of including document context was raised within the community (Läubli et al., 2018; Toral et al., 2018) and at WMT19 a range of DA styles were subsequently tested that included document context. In WMT19, two options were run, firstly, an evaluation that included the document context “+DC” (with document context), and secondly, a variation that omitted document context “−DC” (without document con"
2020.wmt-1.1,2020.wmt-1.27,0,0.247738,"he original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans"
2020.wmt-1.1,D19-6301,1,0.888512,"Missing"
2020.wmt-1.1,W18-6424,0,0.0431841,"(Kocmi, 2020) combines transfer learning from a high-resource language pair Czech–English into the low-resource Inuktitut-English with an additional backtranslation step. Surprising behaviour is noticed when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. The system is the Transformer model in a constrained submission. 2.3.3 Charles University (CUNI) CUNI-D OC T RANSFORMER (Popel, 2020) is similar to the sentence-level version (CUNI-T2T2018, CUBBITT), but trained on sequences with multiple sentences of up to 3000 characters. CUNI-T2T-2018 (Popel, 2018), also called CUBBITT, is exactly the same system as in WMT2018. It is the Transformer model trained according to Popel and Bojar (2018) plus a novel concat-regime backtranslation with checkpoint averaging (Popel et al., 2020), tuned separately for CZ-domain and non CZ-domain articles, possibly handling also translation-direction (“translationese”) issues. For cs→en also a coreference preprocessing was used adding the female-gender CUNI-T RANSFORMER (Popel, 2020) is similar to the WMT2018 version of CUBBITT, but with 12 encoder layers instead of 6 and trained on CzEng 2.0 instead of CzEng 1.7."
2020.wmt-1.1,2020.wmt-1.25,0,0.094349,"Missing"
2020.wmt-1.1,2020.wmt-1.28,0,0.0792624,"cument in the test set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 20"
2020.wmt-1.1,2020.lrec-1.443,1,0.79707,"er their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained t"
2020.wmt-1.1,2020.wmt-1.48,0,0.090424,"Missing"
2020.wmt-1.1,2020.wmt-1.49,0,0.0519886,"Missing"
2020.wmt-1.1,W19-6712,0,0.0573476,"tly crawled multilingual parallel corpora from Indian government websites (Haddow and Kirefu, 2020; Siripragada et al., 2020), the Tanzil corpus (Tiedemann, 2009), the Pavlick dicParagraph-split Test Sets For the language pairs English↔Czech, English↔German and English→Chinese, we provided the translators with paragraph-split texts, instead of sentence-split texts. We did this in order to provide the translators with greater freedom and, hopefully, to improve the quality of the translation. Allowing translators to merge and split sentences removes one of the “translation shifts” identified by Popovic (2019), which can make translations create solely for MT evaluation different from translations produced for other purposes. We first show some descriptive statistics of the source texts, for Czech, English and German, in 3 Europarl Parallel Corpus Czech ↔ English German ↔ English Polish↔ English German ↔ French Sentences 645,241 1,825,745 632,435 1,801,076 Words 14,948,900 17,380,340 48,125,573 50,506,059 14,691,199 16,995,232 47,517,102 55,366,136 Distinct words 172,452 63,289 371,748 113,960 170,271 62,694 368,585 134,762 News Commentary Parallel Corpus Czech ↔ English 248,927 5,570,734 6,156,063"
2020.wmt-1.1,2020.wmt-1.26,0,0.0845151,"Missing"
2020.wmt-1.1,2020.vardial-1.10,0,0.0933804,"Missing"
2020.wmt-1.1,W18-6301,0,0.038239,"Missing"
2020.wmt-1.1,2020.wmt-1.51,0,0.0917045,"Missing"
2020.wmt-1.1,2020.wmt-1.50,1,0.78567,"Missing"
2020.wmt-1.1,2020.wmt-1.52,0,0.0485419,"Missing"
2020.wmt-1.1,P19-1164,0,0.0581517,"26 26.37 25.51 24.82 28.33 23.33 21.13 21.96 20.43 22.90 22.58 21.90 22.17 22.17 20.53 19.40 20.01 40.44 32.39 30.39 37.04 32.27 27.54 25.97 26.09 46.38 37.30 36.05 35.96 33.76 33.07 27.20 27.07 Table 15: TICO-19 test suite results on the English-to-X WMT20 translation directions. 26 4.1.5 antecedent (a less common direction of information flow), and then correctly express the noun in the target language. The success of the MT system in this test can be established automatically, whenever the gender of the target word can be automatically identified. Kocmi et al. (2020) build upon the WinoMT (Stanovsky et al., 2019) test set, which provides exactly the necessary type of sentences containing an ambiguous profession noun and a personal pronoun which unambiguously (for the human eye) refers to it based the situation described. When extending WinMT with Czech and Polish, Stanovsky et al. have to disregard some test patterns but the principle remains. The results indicate that all MT systems fail in this test, following gender bias (stereotypical patterns attributing the masculine gender to some professions and feminine gender to others) rather than the coreference link. Word Sense Disambiguation (Scherrer et"
2020.wmt-1.1,2020.wmt-1.31,0,0.0881792,"morphological segmentation of the polysynthetic Inuktitut, testing rule-based, supervised, semi-supervised as well as unsupervised word segmentation methods, (2) whether or not adding data from a related language (Greenlandic) helps, and (3) whether contextual word embeddings (XLM) improve translation. G RONINGEN - ENIU use Transformer implemented in Marian with the default setting, improving the performance also with tagged backtranslation, domain-specific data, ensembling and finetuning. 2.3.7 DONG - NMT (no associated paper) No description provided. 2.3.8 ENMT (Kim et al., 2020) Kim et al. (2020) base their approach on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. The model is then fine-tuned with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, final results are generated with an ensemble model and re-ranked with averaged models and language models. G RONINGEN - ENTAM (Dhar et al., 2020) study the effects of various techniques such as linguistically motivated segmenta"
2020.wmt-1.1,2020.wmt-1.32,0,0.0839317,"- NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ub"
2020.wmt-1.1,2020.wmt-1.33,0,0.080166,"Missing"
2020.wmt-1.1,2020.wmt-1.34,0,0.0803867,"Missing"
2020.wmt-1.1,2020.wmt-1.35,0,0.0951745,"ss web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics about the training and test materials are given in Figures 1, 2, 3 and 4. 4 8 ARIEL XV https://github.com/AppraiseDev/OCELoT English I English II English III Chinese Czech German Inuktitut Japanese Polish Russian Tamil ABC News (2), All Africa (5), Brisbane Times (1), CBS LA (1), CBS News (1), CNBC (3), CNN (2), Daily Express (1), Daily Mail (2), Fox News (1), Gateway (1), Guardian (3), Huffington Post (2), London Evening Standard (2), Metro (2), NDTV (7), RTE (7), Reuters (4), STV (2), S"
2020.wmt-1.1,2020.wmt-1.55,0,0.0877526,"Missing"
2020.wmt-1.1,P17-4012,0,0.0273892,"o SJTU-NICT using large XLM model to improve NMT but the exact relation is unclear. 2.3.14 H UAWEI TSC (Wei et al., 2020a) H UAWEI TSC use Transformer-big with a further increased model size, focussing on standard techniques of careful pre-processing and filtering, back-translation and forward translation, including self-training, i.e. translating one of the sides of the original parallel data. Ensembling of individual training runs is used in the forward as well as backward translation, and single models are created from the ensembles using knowledge distillation. The submission uses THUNMT (Zhang et al., 2017) open-source engine. 2.3.19 N IU T RANS (Zhang et al., 2020) N IU T RANS gain their performance from focussed attention to six areas: (1) careful data preprocessing and filtering, (2) iterative back-translation to generate additional training data, (3) using different model architectures, such as wider and/or deeper models, relative position representation and relative length, to enhance the diversity of translations, (4) iterative knowledge distillation by in-domain monolingual data, (5) iterative finetuning for domain adaptation using small training batches, (6) rule-based post-processing of"
2020.wmt-1.1,P98-2238,0,0.590812,"and punctuation, and we tend to attribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate th"
2020.wmt-1.1,2020.wmt-1.41,1,0.814291,"Missing"
2020.wmt-1.10,W19-5311,1,0.824015,"Missing"
2020.wmt-1.10,W18-6459,0,0.0200238,"ependently tokenized using BPE (Sennrich et al., 2016b) with 32 thousand operations. Table 1 the estatistics for each language. Tamil data has been tokenized at word-level using Indic-NLP (Kunchukuttan, 2020) and then tokenized with BPE with 16 thousand operations. sentences 1758872 1758872 1663458 1663458 1681466 1681466 1769606 1769606 1770112 1770112 words 40265543 40265543 37698204 40808518 37410662 43056346 41803882 43156309 41211543 45196313 Table 1: Corpus statistics in number of words and sentences for the language pairs of the Multilingual initial system. Related Work Previous works (Choudhary et al., 2018) have shown that Indian languages are usually a challenge for NMT systems due to their difference in terms of vocabulary and grammar compared to western languages such as English. Also, standard preprocessing methods do not always work well with them, so specific solutions are required to obtain good results. In the context of NMT, previous systems, such as MIDAS (Choudhary et al., 2018), proved that the use of subword units leads to significant improvements in translation quality when applied to Tamil by preventing Out of Vocabulary words in at generation time. 4 DE-EN lang DE EN DE ES DE FR"
2020.wmt-1.10,P19-2033,1,0.885817,"Missing"
2020.wmt-1.10,D16-1026,0,0.0366142,"Missing"
2020.wmt-1.10,W18-2703,0,0.0118072,"is usually more available, as it does not require any additional labeling. A common approach to benefit from monolingual data is back-translation (Sennrich et al., 2016a), which consists of translating a monolingual corpus to generate synthetic corpora that can be later employed to continue training. Similar techniques create a synthetic pseudo-parallel corpus through a pivot language (Casas et al., 2019) that can be then trained similarly to back-translation when data is available between the desired language pair and a pivot high resource language. More recently, iterative back-translation (Hoang et al., 2018) was proposed. This technique allows the system to generate synthetic data while updating the system, so better the new data improves as the system trains. On the other hand, several works on Multilingual NMT have shown benefits for low resource language pairs by allowing positive transfer from the high resource languages, boosting the performance of the low resource ones. Different architectures have been proposed that show this behavior, from universal models where all parameters are shared between all languages (Johnson et al., 2017), to architectures that share a common device that maps re"
2020.wmt-1.10,P07-2045,0,0.00535908,"k-translation with monolingual corpora. 3 DE-ES DE-FR EN-ES EN-FR Corpora and Data Preparation All proposed systems in this work are constrained using exclusively data provided by the task’s organization. The multilingual initial system was trained using Europarl v8, for all translation directions between English, French, Spanish, and German. For English-Tamil PMIndia, Tanzil v1, The UFAL EnTam corpus, The NLPC UOM En-Ta corpus, Wikimatrix, and Wikitiles. As monolingual Tamil data, we used News Crawl, while for English, we used News-commentary. We processed all non-Tamil data following Moses (Koehn et al., 2007) scripts provided by the organization. For each one, we applied punctuation normalization, tokenization, and true-casing. Then each language is independently tokenized using BPE (Sennrich et al., 2016b) with 32 thousand operations. Table 1 the estatistics for each language. Tamil data has been tokenized at word-level using Indic-NLP (Kunchukuttan, 2020) and then tokenized with BPE with 16 thousand operations. sentences 1758872 1758872 1663458 1663458 1681466 1681466 1769606 1769606 1770112 1770112 words 40265543 40265543 37698204 40808518 37410662 43056346 41803882 43156309 41211543 45196313 T"
2020.wmt-1.10,N19-4009,0,0.0208164,"mil translation direction is trained by freezing the English encoder and training the Tamil decoder to force the shared representation. In this case, we also notice the positive transfer compared to the baseline trained with just parallel data. See in Figure 1 the schema of the supervised pretraining that we have just described. Implementation. For this work, all encoders and decoders were implemented using the Transformer (Vaswani et al., 2017) architecture, with 6 layers, 8 heads, 512 embedding size, and 2048 feed-forward size for each of them, and everything was implemented using Fairseq’s(Ott et al., 2019) 0.6 release. The multilingual NMT model was trained in a single NVIDIA TITAN XP for 50 thousand updates using adam optimizer with 0.001 as learning, 4000 warmup updates and updating every 16 batches of 2000 tokens. Adding Tamil-English and EnglishTamil directions to the system took approximately 45 thousand updates using the same parameters and GPU configuration. 5.2 Monolingual Unsupervised Fine-tuning Methodology. The previous process has benefited from the additional corpus from the Multilingual NMT system, but as stated before, monolingual data is another common source of improvement for"
2020.wmt-1.10,W17-2619,0,0.0133349,"mance of the low resource ones. Different architectures have been proposed that show this behavior, from universal models where all parameters are shared between all languages (Johnson et al., 2017), to architectures that share a common device that maps representations into a shared represen134 Proceedings of the 5th Conference on Machine Translation (WMT), pages 134–138 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics corpus tation space (Firat et al., 2016; Zhu et al., 2020), to architectures that do not share parameters (Escolano et al., 2019; Escolano et al.; Schwenk and Douze, 2017). In the context of the WMT20 Tamil-English news shared task, as the provided parallel data is limited, we resorted to a combination of both proposed methods by incrementally train the new language pair into a Multilingual NMT system using the provided parallel data, to later fine-tune the system using iterative-back-translation with monolingual corpora. 3 DE-ES DE-FR EN-ES EN-FR Corpora and Data Preparation All proposed systems in this work are constrained using exclusively data provided by the task’s organization. The multilingual initial system was trained using Europarl v8, for all transla"
2020.wmt-1.10,P16-1009,0,0.232831,"the target language in the context of machine translation, which may affect attention and decoding in NMT systems. Low Resource NMT Modern NMT systems benefit from having hundreds of thousands or even millions of parallel sentences. When working with low resource language pairs, the two main approaches are the use of monolingual corpora and multilingual NMT. While parallel data may be difficult to obtain for low resource languages, monolingual data is usually more available, as it does not require any additional labeling. A common approach to benefit from monolingual data is back-translation (Sennrich et al., 2016a), which consists of translating a monolingual corpus to generate synthetic corpora that can be later employed to continue training. Similar techniques create a synthetic pseudo-parallel corpus through a pivot language (Casas et al., 2019) that can be then trained similarly to back-translation when data is available between the desired language pair and a pivot high resource language. More recently, iterative back-translation (Hoang et al., 2018) was proposed. This technique allows the system to generate synthetic data while updating the system, so better the new data improves as the system t"
2020.wmt-1.10,P16-1162,0,0.0630549,"the target language in the context of machine translation, which may affect attention and decoding in NMT systems. Low Resource NMT Modern NMT systems benefit from having hundreds of thousands or even millions of parallel sentences. When working with low resource language pairs, the two main approaches are the use of monolingual corpora and multilingual NMT. While parallel data may be difficult to obtain for low resource languages, monolingual data is usually more available, as it does not require any additional labeling. A common approach to benefit from monolingual data is back-translation (Sennrich et al., 2016a), which consists of translating a monolingual corpus to generate synthetic corpora that can be later employed to continue training. Similar techniques create a synthetic pseudo-parallel corpus through a pivot language (Casas et al., 2019) that can be then trained similarly to back-translation when data is available between the desired language pair and a pivot high resource language. More recently, iterative back-translation (Hoang et al., 2018) was proposed. This technique allows the system to generate synthetic data while updating the system, so better the new data improves as the system t"
2020.wmt-1.10,2020.acl-main.150,0,0.0304985,"esource language pairs by allowing positive transfer from the high resource languages, boosting the performance of the low resource ones. Different architectures have been proposed that show this behavior, from universal models where all parameters are shared between all languages (Johnson et al., 2017), to architectures that share a common device that maps representations into a shared represen134 Proceedings of the 5th Conference on Machine Translation (WMT), pages 134–138 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics corpus tation space (Firat et al., 2016; Zhu et al., 2020), to architectures that do not share parameters (Escolano et al., 2019; Escolano et al.; Schwenk and Douze, 2017). In the context of the WMT20 Tamil-English news shared task, as the provided parallel data is limited, we resorted to a combination of both proposed methods by incrementally train the new language pair into a Multilingual NMT system using the provided parallel data, to later fine-tune the system using iterative-back-translation with monolingual corpora. 3 DE-ES DE-FR EN-ES EN-FR Corpora and Data Preparation All proposed systems in this work are constrained using exclusively data pr"
2020.wmt-1.2,2020.lrec-1.226,1,0.794186,"= GRU(yt−1 , d0t−1 ) ct = Attention(A, query ← dt ) d0t Spen = Sadapt + (Simp − Scor ) (1) 0 = GRU (ct , dt ) ot = tanh(Wc ct + Wd d0t + Wy yt−1 ) lt = Wo (Wb ot + bb ) + bo with Sadapt being the score of the adapted system and Simp and Scor are the scores of this system where all sentences requested to the user simulation are considered entirely wrong and correct, respectively. Note that in the case of BLEU, the brevity penalty is not impacted by this calculation, only the correct n-gram counts will be decreased proportionally to the sentence requested for translation. For more details, see (Prokopalo et al., 2020). P (yt |X, Y<t ) = softmax(lt ) For a single training sample, we then maximise the joint likelihood of source and target sentences: L(X, Y ) = log (P (yt |X, Y<t )) (2) t=1 5 4 T X Baseline systems Adaptation techniques The first adaptation technique used is rather simple. It consists of selecting N sentences from training data that are the closest to the sentences in the document. The chosen similarity metric is the cosine between sentence embeddings obtained by a simple average of word embeddings, as described in (Arora et al., 2017). This data is then used to finetune the initial model for"
2020.wmt-1.2,E17-3017,0,0.0743157,"Missing"
2020.wmt-1.2,P16-1162,0,0.262589,"ings and GRU hidden states are set to 128 and 256, respectively. The embeddings are shared in the decoder (Press and Wolf, 2017). We use ADAM (Kingma and Ba, 2014) as the optimiser and set the learning rate and mini-batch size to 0.0004 and 64, respectively. Regularisation is done by means of a weight decay of 1e−5 and the use of dropout on the embeddings, the source context and the output (set at 0.4) (Srivastava et al., 2014). We clip the gradients if the norm of the full parameter vector exceeds 1 (Pascanu et al., 2013). The data is processed by a joint BPE model with 30k merge operations (Sennrich et al., 2016a). This leads to respectively 20.7k and 25.1k units for English and French and 17.2k and 26.5k units for English and German, respectively. We train each model for a maximum of 100 epochs and early stop the training if validation BLEU (Papineni et al., 2002) does not improve for 10 epochs (Figure 2). We also halve the learning rate if no improvement is obtained for three epochs. The number of learnable parameters is around 8.7M for En-Fr and 8.5M for En-De. Results show that a simple data selection method along with finetuning can provide a small improvement of the system’s performance for Eng"
2020.wmt-1.47,D10-1092,0,0.0372563,"410 Model Primary Contrastive1 Contrastive2 Before 24.20 23.94 24.49 After 32.56 30.43 30.2 Model Primary Contrastive1 Contrastive2 Table 2: BLEU for the models before and after finetuning for ES-PT 4 5 6 Model Primary Contrastive1 Contrastive2 BLEU 27.08 23.91 23.9 RIBES 72.98 71.55 73.73 TER 55.34 57.55 58.07 Table 4: Official results for submitted ES-PT systems In this internal evaluation of the models the primary model outperforms the baseline models by 2.7 BLEU points for ES-PT direction and 0.18 points in PT-ES. 6.1 Task results The evaluation of the task was carried using BLEU, RIBES (Isozaki et al., 2010) and TER (Snover et al., 2006) metrics the main difference between this measure and the internal one was that the internal evaluation used a tokenized lowercased version of the text and the task results used the final version. The tables 4 and 5 show the results of the submitted systems in the task evaluation for the ES-PT and PT-ES language pairs respectively. In this evaluation again the primary model outperforms the baseline models by a margin of 3 and 0.4 BLEU points for the ES-PT and PT-ES directions respectively. 7 Translations generation In order to get the translation for the evaluatio"
2020.wmt-1.47,P17-4012,0,0.0405863,"rmed by BPE sub-word units Where the primary model was the main model for the submission and the contrastive models serves as baselines. For the comparative between the three models, BLEU (Papineni et al., 2002) was computed with the Sacrebleu (Post, 2018). 2.2 Transformer model For the transformer model the configuration used consists of a model size of 6 layers, 512 feedforward size, 8 heads, trained on one GPU with a batch size of 4096 tokens using. We stored a checkpoint every 5000 steps until 200000. We used Adam optimizer with a β 2 of 0.998 The models were built using Open NMT toolkit (Klein et al., 2017). 3 Corpus description The training data was made up with the available training data for the task, that is JCR, Europarl, news commentary and wikititles corpora. The provided development set was randomly split in two disjoint sets of the same size, dev1 and dev2 sets. The data was prepossessed using the following pipeline tokenization, lowercasing and a BPE algorithm learned over the test set. 410 Model Primary Contrastive1 Contrastive2 Before 24.20 23.94 24.49 After 32.56 30.43 30.2 Model Primary Contrastive1 Contrastive2 Table 2: BLEU for the models before and after finetuning for ES-PT 4 5"
2020.wmt-1.47,P02-1040,0,0.11907,"is attempted, using the pre trained vectors in both Spanish and Portuguese languages. 2 Version 1 10 15 2 1. Primary: Model that was initialized using pre trained fast-text word embeddings, and tokens constituted by words 2. Contrastive1: Model that was initialized with random word embeddings. Used tokens formed by words 3. Contrastive2: Model that was initialized with random word embeddings. Used tokens formed by BPE sub-word units Where the primary model was the main model for the submission and the contrastive models serves as baselines. For the comparative between the three models, BLEU (Papineni et al., 2002) was computed with the Sacrebleu (Post, 2018). 2.2 Transformer model For the transformer model the configuration used consists of a model size of 6 layers, 512 feedforward size, 8 heads, trained on one GPU with a batch size of 4096 tokens using. We stored a checkpoint every 5000 steps until 200000. We used Adam optimizer with a β 2 of 0.998 The models were built using Open NMT toolkit (Klein et al., 2017). 3 Corpus description The training data was made up with the available training data for the task, that is JCR, Europarl, news commentary and wikititles corpora. The provided development set"
2020.wmt-1.47,W18-6319,0,0.0196199,"nish and Portuguese languages. 2 Version 1 10 15 2 1. Primary: Model that was initialized using pre trained fast-text word embeddings, and tokens constituted by words 2. Contrastive1: Model that was initialized with random word embeddings. Used tokens formed by words 3. Contrastive2: Model that was initialized with random word embeddings. Used tokens formed by BPE sub-word units Where the primary model was the main model for the submission and the contrastive models serves as baselines. For the comparative between the three models, BLEU (Papineni et al., 2002) was computed with the Sacrebleu (Post, 2018). 2.2 Transformer model For the transformer model the configuration used consists of a model size of 6 layers, 512 feedforward size, 8 heads, trained on one GPU with a batch size of 4096 tokens using. We stored a checkpoint every 5000 steps until 200000. We used Adam optimizer with a β 2 of 0.998 The models were built using Open NMT toolkit (Klein et al., 2017). 3 Corpus description The training data was made up with the available training data for the task, that is JCR, Europarl, news commentary and wikititles corpora. The provided development set was randomly split in two disjoint sets of th"
2020.wmt-1.47,2006.amta-papers.25,0,0.103719,"Contrastive2 Before 24.20 23.94 24.49 After 32.56 30.43 30.2 Model Primary Contrastive1 Contrastive2 Table 2: BLEU for the models before and after finetuning for ES-PT 4 5 6 Model Primary Contrastive1 Contrastive2 BLEU 27.08 23.91 23.9 RIBES 72.98 71.55 73.73 TER 55.34 57.55 58.07 Table 4: Official results for submitted ES-PT systems In this internal evaluation of the models the primary model outperforms the baseline models by 2.7 BLEU points for ES-PT direction and 0.18 points in PT-ES. 6.1 Task results The evaluation of the task was carried using BLEU, RIBES (Isozaki et al., 2010) and TER (Snover et al., 2006) metrics the main difference between this measure and the internal one was that the internal evaluation used a tokenized lowercased version of the text and the task results used the final version. The tables 4 and 5 show the results of the submitted systems in the task evaluation for the ES-PT and PT-ES language pairs respectively. In this evaluation again the primary model outperforms the baseline models by a margin of 3 and 0.4 BLEU points for the ES-PT and PT-ES directions respectively. 7 Translations generation In order to get the translation for the evaluation the translate.py script of O"
2020.wmt-1.54,N19-1388,0,0.0241365,"l., 2017). Among the alternatives, we can share encoders and decoders (Johnson et al., 2017) or have specific encoders and decoders for each language (Escolano et al., 2020). In this paper, we are using the shared approach and we are leaving as further work to compare with other ones. Shared encoder-decoder One direct approach is using a single encoder/decoder shared for all languages (Johnson et al., 2017). In this case, parameters and vocabulary are shared among all language pairs and it helps the generalization across languages improving the translation for the low resource language pairs (Aharoni et al., 2019). Additionally, the shared encoder/decoder allows using zero-shot easily, only by adding a tag in the source sentence. The source sentence has to contain the 447 Proceedings of the 5th Conference on Machine Translation (WMT), pages 447–450 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics language abbreviation of the target language. So, when translating from Catalan to Spanish, we have to include the <2es&gt; tag at the beginning of the Catalan source sentence, which means that we are translating into Spanish. <2es&gt; Bon dia -&gt; Buenos d´ ıas Therefore, it is necessary"
2020.wmt-1.54,2021.eacl-main.80,1,0.820245,"Missing"
2020.wmt-1.54,N03-1017,0,0.0434037,"e of the monolingual data. Finally, we have applied fine-tuning to improve the in-domain data. Each of these techniques brings improvements over the previous one. In the official evaluation, our system was ranked 1st in the Portuguese-to-Spanish direction, 2nd in the opposite direction, and 3rd in the Catalan-Spanish pair. 1 2 Background In this section, we show an overview of neuralbased multilingual machine translation and domain adaptation using fine-tuning. Introduction Research in the field of Machine Translation (MT) has been growing during these last years. From statistical approaches (Koehn et al., 2003) to neural ones (Bahdanau et al., 2015), the progress has been impressive. Even after having achieved exceptional results based only on attention mechanisms (Vaswani et al., 2017), there are still many challenges and improvements remaining, for instance, multilingual translation from languages other than English, which have lower resources, and domain adaptation. In order to tackle these challenges, the Similar Language Task organized in the context of the Conference on Machine Translation (WMT 2020) has provided an appropriate setting for them. Within this task, the focus is the translation b"
2020.wmt-1.54,P16-1009,0,0.0224162,"eviation of the target language. So, when translating from Catalan to Spanish, we have to include the <2es&gt; tag at the beginning of the Catalan source sentence, which means that we are translating into Spanish. <2es&gt; Bon dia -&gt; Buenos d´ ıas Therefore, it is necessary to add the tag to indicate the target language, followed by the sentence to be translated. This is necessary both in training and inference. 2.2 Monolingual corpus selection for back-translation There is a large amount of monolingual data available for this task. Monolingual data can improve the system by using back-translation (Sennrich et al., 2016). However, back-translation is a process that consumes a lot of resources, so we decided to select the monolingual data within the target domain. The selection criterion has been the TF-IDF (Term Frequency – Inverse Document Frequency), which defines the relevance of the words in a document. Using this criterion, we compared all the available monolingual data against the development set and only kept the files that had a higher score among all. 2.3 Domain adaptation One approach to improve the translation of a specific language domain is to make use of fine-tuning techniques. Fine-tuning consi"
2021.eacl-main.80,2005.mtsummit-papers.11,0,0.0194176,"Missing"
2021.eacl-main.80,P07-2045,0,0.00991062,"guage j as target, the system is trained using the language-specific encoder ei and decoder dj . Adding New Languages Since parameters are not shared between the independent encoders and decoders, the joint training enables the addition of 945 Algorithm 1 Multilingual training step languages (without being multi-parallel). For Russian-English, we used 1 million training sentences from the Yandex corpus2 . As validation and test set, we used newstest2012 and newstest2013 from WMT3 , which is multi-parallel across all the above languages. All data were preprocessed using standard Moses scripts (Koehn et al., 2007) We evaluate our approach in 3 different settings: (i) the initial training, covering all combinations of German, French, Spanish and English; (ii) adding new languages, tested with RussianEnglish in both directions; and (iii) zero-shot translation, covering all combinations between Russian and the rest of the languages. Additionally we compare two configurations which consists in using non-tied or tied embeddings. In the language-specific approach tied embeddings consists in using language-wise word embeddings: for one language, we use the same word embeddings. Whereas, in the case of non-tie"
2021.eacl-main.80,W18-6309,0,0.0605765,"e will be using when describing our approach. We denote the encoder and the decoder for the ith language in the system as ei and di , respectively. For languagespecific scenarios, both the encoder and decoder are considered independent modules that can be freely interchanged to work in all translation directions. Language-specific Encoder-Decoders which may or may not share parameters at some point. Sharing parameters. Firat et al. (2016b) proposed extending the bilingual recurrent neural machine translation architecture (Bahdanau et al., 2015) to the multilingual case (V´azquez et al., 2019; Lu et al., 2018) by designing a shared attention-based mechanism between the languagespecific encoders and decoders to create a language independent representation. As the language specific components rely on the shared modules, modifying those components to add a new language or add further data to the system would require retraining the whole system (similarly to the previous shared approach). (Lakew et al., 2018) proposes a model based on the addition of new languages to an already trained system by vocabulary adaptation and transfer learning. While limited, it requires some retraining to adapt the model t"
2021.eacl-main.80,N19-4009,0,0.0256589,"ering all combinations between Russian and the rest of the languages. Additionally we compare two configurations which consists in using non-tied or tied embeddings. In the language-specific approach tied embeddings consists in using language-wise word embeddings: for one language, we use the same word embeddings. Whereas, in the case of non-tied, the encoder and the decoder of each language have different word embeddings. Tied embeddings in the shared system means that both encoder and decoder share the same word embeddings. All experiments were done using the Transformer provided by Fairseq(Ott et al., 2019) 4 . We used 6 layers, each with 8 attention heads, an embedding size of 512 dimensions, and a vocabulary size of 32k subword tokens with Byte Pair Encoding (Sennrich et al., 2016) (in total for the shared encoders/decoders and per pair for language-specific encoder-decoders). Dropout was 0.1 for the shared approach and 0.3 for language-specific encoders/decoders. Both approaches were trained with an effective batch size of 32k tokens for approximately 200k updates, using the validation loss for early stopping. We used Adam (Kingma and Ba, 2015) as the optimizer, with learning rate of 0.001 an"
2021.eacl-main.80,P15-1166,0,0.0443479,"opportunities for improving this area have dramatically expanded. Thanks to the encoder-decoder architecture, there are viable alternatives to expensive pairwise translation based on classic paradigms1 . The main proposal in this direction is the universal encoder-decoder (joh, 2017) with massive multilingual enhancements (Arivazhagan et al., 2019). While this approach enables zero-shot translation and is beneficial for low-resource languages, it has multiple drawbacks: (i) the entire 1 2 Related Work Multilingual neural machine translation can refer to translating from one-to-many languages (Dong et al., 2015), from many-to-one (Zoph and Knight, 2016) and many-to-many (joh, 2017). Within the many-to-many paradigm, existing approaches can http://www.euromatrixplus.net 944 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 944–948 April 19 - 23, 2021. ©2021 Association for Computational Linguistics out parameter or vocabulary-sharing and on enforcing a compatible representation between the jointly trained languages. The advantage of the approach is that it does not require retraining to add new languages and increasing the number of lang"
2021.eacl-main.80,P19-2033,1,0.881824,"Missing"
2021.eacl-main.80,P16-1162,0,0.0109143,"nguage-specific approach tied embeddings consists in using language-wise word embeddings: for one language, we use the same word embeddings. Whereas, in the case of non-tied, the encoder and the decoder of each language have different word embeddings. Tied embeddings in the shared system means that both encoder and decoder share the same word embeddings. All experiments were done using the Transformer provided by Fairseq(Ott et al., 2019) 4 . We used 6 layers, each with 8 attention heads, an embedding size of 512 dimensions, and a vocabulary size of 32k subword tokens with Byte Pair Encoding (Sennrich et al., 2016) (in total for the shared encoders/decoders and per pair for language-specific encoder-decoders). Dropout was 0.1 for the shared approach and 0.3 for language-specific encoders/decoders. Both approaches were trained with an effective batch size of 32k tokens for approximately 200k updates, using the validation loss for early stopping. We used Adam (Kingma and Ba, 2015) as the optimizer, with learning rate of 0.001 and 4000 warmup steps. 1: procedure M ULTILINGUALT RAINING S TEP 2: N ← Number of languages in the system 3: S = {s0,0 , ..., sN,N } ← {(ei , dj )} 4: E = {e0 , ..., eN } ← Language-"
2021.eacl-main.80,N16-1101,0,0.152258,"hare any parameter across these modules, which allows to add new languages incrementally without retraining the entire system. 3.1 Definitions We next define the notation that we will be using when describing our approach. We denote the encoder and the decoder for the ith language in the system as ei and di , respectively. For languagespecific scenarios, both the encoder and decoder are considered independent modules that can be freely interchanged to work in all translation directions. Language-specific Encoder-Decoders which may or may not share parameters at some point. Sharing parameters. Firat et al. (2016b) proposed extending the bilingual recurrent neural machine translation architecture (Bahdanau et al., 2015) to the multilingual case (V´azquez et al., 2019; Lu et al., 2018) by designing a shared attention-based mechanism between the languagespecific encoders and decoders to create a language independent representation. As the language specific components rely on the shared modules, modifying those components to add a new language or add further data to the system would require retraining the whole system (similarly to the previous shared approach). (Lakew et al., 2018) proposes a model base"
2021.eacl-main.80,W19-4305,0,0.166274,"Missing"
2021.eacl-main.80,D16-1026,0,0.0342706,"Missing"
2021.eacl-main.80,N16-1004,0,0.0133294,"have dramatically expanded. Thanks to the encoder-decoder architecture, there are viable alternatives to expensive pairwise translation based on classic paradigms1 . The main proposal in this direction is the universal encoder-decoder (joh, 2017) with massive multilingual enhancements (Arivazhagan et al., 2019). While this approach enables zero-shot translation and is beneficial for low-resource languages, it has multiple drawbacks: (i) the entire 1 2 Related Work Multilingual neural machine translation can refer to translating from one-to-many languages (Dong et al., 2015), from many-to-one (Zoph and Knight, 2016) and many-to-many (joh, 2017). Within the many-to-many paradigm, existing approaches can http://www.euromatrixplus.net 944 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 944–948 April 19 - 23, 2021. ©2021 Association for Computational Linguistics out parameter or vocabulary-sharing and on enforcing a compatible representation between the jointly trained languages. The advantage of the approach is that it does not require retraining to add new languages and increasing the number of languages does not vary the quality of the lan"
2021.findings-emnlp.39,J93-2003,0,0.0980772,"Missing"
2021.findings-emnlp.39,2020.emnlp-main.42,0,0.0690634,"to a reference. (Zenkel et al., 2019) adds an alignment module attending encoder representations. (Garg et al., 2019) propose to supervise an attention head with GIZA++ (Brown et al., 1993) alignments. Although they improve alignment performance, these methods introduce external trainable parameters or alignments references, which makes these techniques lose interest regarding interpretability of the model. Aligments from the decoder input. A technique that solves the aforementioned issue consists of inducing alignments by comparing x with the input of the decoder yi (Kobayashi et al., 2020; Chen et al., 2020) (in force decoding setting yi = yt−1 ). So, since the ground truth target sequence is used as input in the decoder, alignments Ai,j in this setting represent the same information as gold alignments. Attention modules from the initial layers tend to extract better alignments from the input of the decoder, while alignments from the decoder output are better extracted from the final layers. Although results show that decoder input provides lower alignment error rates, it shows how similar to ej the model is able to generate representations of the decoder input, losing explanation power about the"
2021.findings-emnlp.39,W19-4828,0,0.0190856,"of information arriving from the input sequence is determined by the weighted sum of the values. If we analyze the norms of the values vectors (Figure 6) we can see that the source finalizing tokens, especially h/si, get almost zero norms. This can be interpreted as when assigning high attention weights to these tokens, the Residual + Normalization layer gets almost no information from the source. From the results obtained over 5 random seeds (Figure 7) we can state that the network picks a common token, i.e. h/si or _. and projects it to a zero vector through WhV . These results support the (Clark et al., 2019) hypothesis about the selection of a token as a ""no-op"" in the attention mechanism ([SEP] token in BERT model). Figure 5: Saliency scores ψ(y<t , yt ) for the reference model output (from top to bottom): _gentlemen, _by, _t and _for. Similarly, a non-common word such as Burtone, which gets tokenized into _bur, t and one gets source-target contributions that also match human intuition. The first token _bur is predicted by relying almost only on the source sequence. However, following tokens, although they heavily rely on the source, get information about the previous tokens. In this case, _bur"
2021.findings-emnlp.39,N19-1423,0,0.0128265,"ations (Jain and Wal- relying on word alignment task is that it ignores the lace, 2019; Serrano and Smith, 2019; Pruthi et al., words that are predicted based on the target prefix, 2020), demonstrating that attention weights distri- i.e what the model has previously translated. An butions can be modified without affecting the final extreme example of the impact of the target preprediction. However, these studies have mainly an- fix on the prediction occurs during ’hallucinations’ alyzed encoder-only or decoder-only architectures (Lee et al., 2019; Berard et al., 2019; Voita et al., like BERT (Devlin et al., 2019) or GPT-2 (Rad- 2020; Raunak et al., 2021). Although some studies ford et al., 2019), which are based on self-attention have analyzed the relative contribution of the tarmechanisms. get prefix context in a model’s prediction (Li et al., 434 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 434–443 November 7–11, 2021. ©2021 Association for Computational Linguistics 2019; Voita et al., 2020), the way NMT models decide in which proportion to use both sequences remains unexplored. In Section 3, we propose a simple method to measure the relative contribution of the sourc"
2021.findings-emnlp.39,W19-5201,0,0.018618,"e marginal effect of one sequence, we keep the other with the original embeddings. Figure 3: Source CS (yt ) and target prefix CT (yt ) contributions for reference model output. ˆ 1:N ) CS (yt ) = 4P (yt |y, x N 1 X ˆ n ) − P¯ (yt |y<t , x ˆ 1:N ))2 (P (yt |y<t , x = N ˆ a zero vector, we arrive to: Making x f (x) ≈ ∇x f (x) · x With this approximation, ∇x f (x) can be interpreted as coefficients that measure the impact of x in the output. In NLP (Li et al., 2016) propose the use of word embeddings as input features from which to calculate saliency scores. In the NMT setting, current methods (Ding et al., 2019) extract saliency scores of the input source tokens by computing the gradient with respect to source embeddings xi . Nevertheless, the Transformer model deals with two different sequences of inputs (x and y<t ), f (x) = P (yt |y<t , x). So, analyzing only the saliency of the source sequence embeddings might lead to an incomplete analysis. To have a full understanding of the influences of each input word on the model prediction we propose to extend the SmoothGrad method (Smilkov et al., 2017) to also consider the gradients w.r.t the target prefix embeddings. We compute the target prefix salienc"
2021.findings-emnlp.39,D19-1453,0,0.0618039,"ntion Weights to Induce Word Alignment h from the encoder-decoder Attention weights αt,j attention modules represent the similarity between P (yt |{y0 , · · · , yt−1 }, {x1 , · · · , x|x |}) hj and sl−1 and have been commonly presented as t a baseline to extract word alignments from words 1 Along this work we use x to represent elements xj and yt . Attention vectors αht represent a prob(scalars/words/tokens), x vectors, x sequences and X matrices. ability distributions over all source tokens x. A 435 classical approach to obtain final alignments has been to compute the average over all heads (Garg et al., 2019) in each layer and selecting the source word that yields the maximum score:  P h 1 j = arg maxj 0 H1 H h=1 αt,j 0 At,j = 0 else Erasure methods have also been applied to NMT (Li et al., 2019), which consist of techniques to measure the relevance of each input token by evaluating the changes in the output probability of the model after removing it from the input of the network (Zintgraf et al., 2017) or eliminating the connection via dropout (Srivastava et al., 2014). (Zenkel et al., 2019; Li et al., 2019; Garg et al., 2019) showed alignments induced from attention weights are noisy, although"
2021.findings-emnlp.39,D19-1088,0,0.0199057,"l prediction and extend the gradient-based analysis to understand dependency relationships between target prefix words. Decoder Encoder ladies kolleginnen und and kollegen Figure 2: Contribution from the source input sequence to the final prediction by perturbing source token embeddings. Model-agnostic methods. Several methods for inducing alignments have been proposed that work 3.1 Contributions by Input Perturbation regardless of the chosen architecture. Gradientbased methods such as gradient × input (Ding et al., We propose separately perturbing source and pre2019) or Integrated Gradients (He et al., 2019) have fix embeddings (Smilkov et al., 2017) to get the been used to obtain saliency values from the source marginal contributions of each sequence to the fiwords as a measure of source word importance. nal prediction. For each embedding we compute 436 N random samples around their neighborhood: 3.2 Saliency of Target Sequences Words Any model f (x) can be linearly approximated locally by its first-order Taylor expansion at a point ˆ x: ˆ ≈ f (x) + ∇x f (x) · (x ˆ − x) f (x) ˆ j = xj + N (0, σx2 j ) x Since input embeddings differ in their length, we adapt the noise level to each token embeddin"
2021.findings-emnlp.39,N19-1357,0,0.0392126,"Missing"
2021.findings-emnlp.39,N16-1082,0,0.0231182,"ibution CS (yt ) by measuring how large is the variation of the output probability when feeding the network with N noisy sequence samples. To get the marginal effect of one sequence, we keep the other with the original embeddings. Figure 3: Source CS (yt ) and target prefix CT (yt ) contributions for reference model output. ˆ 1:N ) CS (yt ) = 4P (yt |y, x N 1 X ˆ n ) − P¯ (yt |y<t , x ˆ 1:N ))2 (P (yt |y<t , x = N ˆ a zero vector, we arrive to: Making x f (x) ≈ ∇x f (x) · x With this approximation, ∇x f (x) can be interpreted as coefficients that measure the impact of x in the output. In NLP (Li et al., 2016) propose the use of word embeddings as input features from which to calculate saliency scores. In the NMT setting, current methods (Ding et al., 2019) extract saliency scores of the input source tokens by computing the gradient with respect to source embeddings xi . Nevertheless, the Transformer model deals with two different sequences of inputs (x and y<t ), f (x) = P (yt |y<t , x). So, analyzing only the saliency of the source sequence embeddings might lead to an incomplete analysis. To have a full understanding of the influences of each input word on the model prediction we propose to exten"
2021.findings-emnlp.39,P19-1124,0,0.0719212,"methods against human-annotated investigate the inner workings of this architecture source-target word alignments. Encoder-decoder in several tasks. One of its core components, the attention weights have been used to provide sourceattention mechanism, which provides a distribution target word alignments (Zenkel et al., 2019; Garg of scores over the input tokens, has been often pre- et al., 2019), but its low performance has made resented as showing the relative importance of the searchers sceptical about its use as an interpretable inputs. Some works have criticized the use of atten- method (Li et al., 2019). An important issue when tion weights as model explanations (Jain and Wal- relying on word alignment task is that it ignores the lace, 2019; Serrano and Smith, 2019; Pruthi et al., words that are predicted based on the target prefix, 2020), demonstrating that attention weights distri- i.e what the model has previously translated. An butions can be modified without affecting the final extreme example of the impact of the target preprediction. However, these studies have mainly an- fix on the prediction occurs during ’hallucinations’ alyzed encoder-only or decoder-only architectures (Lee et al."
2021.findings-emnlp.39,P05-1057,0,0.130196,"hat some layers seem to generate better (xj , yt ) alignments, especially the last layers of the Transformer. An issue regarding the use of this method to interpret the model predictions is that the ground truth target word yt may differ from the actual model prediction yt0 . In these cases, (xj ,yt0 ) alignments can not be compared with (xj ,yt ) gold alignments, showing limitations about its use as an interpretability method. Methods to improve alignments. Other works propose methods to improve word alignment extracted from the Transformer. (Li et al., 2019) use an explicit alignment model (Liu et al., 2005; Taskar et al., 2005) consisting of optimizing a parameter matrix to reduce the alignment distance with respect to a reference. (Zenkel et al., 2019) adds an alignment module attending encoder representations. (Garg et al., 2019) propose to supervise an attention head with GIZA++ (Brown et al., 1993) alignments. Although they improve alignment performance, these methods introduce external trainable parameters or alignments references, which makes these techniques lose interest regarding interpretability of the model. Aligments from the decoder input. A technique that solves the aforementioned"
2021.findings-emnlp.39,J03-1002,0,0.0671303,"mechanism on NMT, which we believe needs further investigation. In this work we analyze the encoder-decoder attention weights and shed light on their impact on the decoder representations and final predictions, showing how alignment errors can also give information about the model’s decision-making process. Research in NMT interpretability has mainly foRecently, Transformer-based models (Vaswani cused on understanding source words importance et al., 2017) have allowed huge improvements in when predicting a target word. The word alignperformance across multiple NLP tasks. The inclu- ment task (Och and Ney, 2003) has served to comsion of this architecture has led the field of NLP to pare explanation methods against human-annotated investigate the inner workings of this architecture source-target word alignments. Encoder-decoder in several tasks. One of its core components, the attention weights have been used to provide sourceattention mechanism, which provides a distribution target word alignments (Zenkel et al., 2019; Garg of scores over the input tokens, has been often pre- et al., 2019), but its low performance has made resented as showing the relative importance of the searchers sceptical about i"
2021.findings-emnlp.39,N19-4009,0,0.0532735,"Missing"
2021.findings-emnlp.39,2020.acl-main.432,0,0.0724246,"Missing"
2021.findings-emnlp.39,2020.findings-emnlp.49,0,0.0190327,"our analysis, we propose methods that largely reduce the word alignment error rate compared to standard induced alignments from attention weights. 1 Introduction Nonetheless, NMT models use the encoderdecoder Transformer architecture, which adds the encoder-decoder attention mechanism, in charge of distributing the information flow from the encoder representations of the source input tokens into the decoder. (Voita et al., 2019) analyze the effect of pruning different attention heads in a Transformer NMT model and conclude that the encoderdecoder attention mechanism is the most critical one. (Raganato et al., 2020) show that encoder selfattention weights can be interchanged by predefined non-learnable patterns without hindering the translation performance. These results provide evidence about the relevance of the encoder-decoder attention mechanism on NMT, which we believe needs further investigation. In this work we analyze the encoder-decoder attention weights and shed light on their impact on the decoder representations and final predictions, showing how alignment errors can also give information about the model’s decision-making process. Research in NMT interpretability has mainly foRecently, Transf"
2021.findings-emnlp.39,2021.naacl-main.92,0,0.0350102,"nment task is that it ignores the lace, 2019; Serrano and Smith, 2019; Pruthi et al., words that are predicted based on the target prefix, 2020), demonstrating that attention weights distri- i.e what the model has previously translated. An butions can be modified without affecting the final extreme example of the impact of the target preprediction. However, these studies have mainly an- fix on the prediction occurs during ’hallucinations’ alyzed encoder-only or decoder-only architectures (Lee et al., 2019; Berard et al., 2019; Voita et al., like BERT (Devlin et al., 2019) or GPT-2 (Rad- 2020; Raunak et al., 2021). Although some studies ford et al., 2019), which are based on self-attention have analyzed the relative contribution of the tarmechanisms. get prefix context in a model’s prediction (Li et al., 434 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 434–443 November 7–11, 2021. ©2021 Association for Computational Linguistics 2019; Voita et al., 2020), the way NMT models decide in which proportion to use both sequences remains unexplored. In Section 3, we propose a simple method to measure the relative contribution of the source and the target prefix by perturbing inpu"
2021.findings-emnlp.39,P16-1162,0,0.147246,"Missing"
2021.findings-emnlp.39,P19-1282,0,0.0188239,"ts core components, the attention weights have been used to provide sourceattention mechanism, which provides a distribution target word alignments (Zenkel et al., 2019; Garg of scores over the input tokens, has been often pre- et al., 2019), but its low performance has made resented as showing the relative importance of the searchers sceptical about its use as an interpretable inputs. Some works have criticized the use of atten- method (Li et al., 2019). An important issue when tion weights as model explanations (Jain and Wal- relying on word alignment task is that it ignores the lace, 2019; Serrano and Smith, 2019; Pruthi et al., words that are predicted based on the target prefix, 2020), demonstrating that attention weights distri- i.e what the model has previously translated. An butions can be modified without affecting the final extreme example of the impact of the target preprediction. However, these studies have mainly an- fix on the prediction occurs during ’hallucinations’ alyzed encoder-only or decoder-only architectures (Lee et al., 2019; Berard et al., 2019; Voita et al., like BERT (Devlin et al., 2019) or GPT-2 (Rad- 2020; Raunak et al., 2021). Although some studies ford et al., 2019), which"
2021.findings-emnlp.39,H05-1010,0,0.167002,"em to generate better (xj , yt ) alignments, especially the last layers of the Transformer. An issue regarding the use of this method to interpret the model predictions is that the ground truth target word yt may differ from the actual model prediction yt0 . In these cases, (xj ,yt0 ) alignments can not be compared with (xj ,yt ) gold alignments, showing limitations about its use as an interpretability method. Methods to improve alignments. Other works propose methods to improve word alignment extracted from the Transformer. (Li et al., 2019) use an explicit alignment model (Liu et al., 2005; Taskar et al., 2005) consisting of optimizing a parameter matrix to reduce the alignment distance with respect to a reference. (Zenkel et al., 2019) adds an alignment module attending encoder representations. (Garg et al., 2019) propose to supervise an attention head with GIZA++ (Brown et al., 1993) alignments. Although they improve alignment performance, these methods introduce external trainable parameters or alignments references, which makes these techniques lose interest regarding interpretability of the model. Aligments from the decoder input. A technique that solves the aforementioned issue consists of ind"
2021.findings-emnlp.39,2006.iwslt-papers.7,0,0.126708,"Missing"
2021.findings-emnlp.39,P19-1580,0,0.0216839,"he influence of wrong alignments on the model behavior, demonstrating that the encoder-decoder attention mechanism is well suited as an interpretability method for NMT. Finally, based on our analysis, we propose methods that largely reduce the word alignment error rate compared to standard induced alignments from attention weights. 1 Introduction Nonetheless, NMT models use the encoderdecoder Transformer architecture, which adds the encoder-decoder attention mechanism, in charge of distributing the information flow from the encoder representations of the source input tokens into the decoder. (Voita et al., 2019) analyze the effect of pruning different attention heads in a Transformer NMT model and conclude that the encoderdecoder attention mechanism is the most critical one. (Raganato et al., 2020) show that encoder selfattention weights can be interchanged by predefined non-learnable patterns without hindering the translation performance. These results provide evidence about the relevance of the encoder-decoder attention mechanism on NMT, which we believe needs further investigation. In this work we analyze the encoder-decoder attention weights and shed light on their impact on the decoder represent"
2021.iwslt-1.11,N19-1006,0,0.0611936,"Missing"
2021.iwslt-1.11,D19-1165,0,0.149095,"scription We built an end-to-end ST system, mainly composed of pre-trained modules. We couple a Wav2Vec 2.0 encoder (Baevski et al., 2020) and an mBART decoder (Liu et al., 2020a), following the strategy proposed by Li et al. (2021). When combining these two models, there is a length discrepancy between the target sentence length and the encoder output. For this reason, it is necessary to use a module to shorten the encoder output, which we refer to as the Length Adaptor. Additionally, we introduce an Adapter module to reduce the gap between the different modalities of the pre-trained models (Bapna and Firat, 2019). A method that Escolano et al. (2020) proved to be beneficial for ST models. 2.1 Pre-trained modules Our motivation is to get the most out of pretrained components, which were obtained by selfsupervision or supervised tasks. Concretely, we use a Wav2Vec 2.0 encoder and an mBART decoder, both trained initially by self-supervision and fineFigure 1: System overview. The original architecture proposed by Li et al. (2021) includes a pre-trained Wav2Vec 2.0 as the encoder, a pre-trained mBART decoder and a Length Adaptor. In this work, we add an Adapter module after the encoder. tuned for ASR and m"
2021.iwslt-1.11,2015.iwslt-evaluation.1,0,0.0772296,"Missing"
2021.iwslt-1.11,N19-1423,0,0.0322306,"Missing"
2021.iwslt-1.11,N19-1202,0,0.0518619,"Missing"
2021.iwslt-1.11,2020.iwslt-1.8,0,0.0652154,"Missing"
2021.iwslt-1.11,2020.acl-main.703,0,0.0236372,"trained with 53.2k hours of untranscribed speech from LibriVox (Kahn et al., 2020), fine-tuned on the 960h of transcribed speech from Librispeech (Panayotov et al., 2015), and on pseudo-labels (Xu et al., 2020). mBART is a sequence-to-sequence denoising autoencoder, which reconstructs the input text sen111 put sequences. It achieves an 8x down-sampling of the encoder representation by applying a stack of 3 convolutional layers with a kernel size of 3 and a stride of 2. 2.3 Figure 2: Adapter module tence given a corrupted version of it (Liu et al., 2020a). It follows the same approach as BART (Lewis et al., 2020) but, instead of using just English monolingual data, it is trained with multiple languages. This strategy does not require any parallel corpora, so it can be used as a pre-training step and then fine-tuned for MT tasks. Specifically, we use the 12-layer Transformer decoder of an mBART model, fine-tuned on multilingual MT, from English to 49 languages (Tang et al., 2020). 2.2 We follow the LayerNorm and Attention (LNA) fine-tuning strategy proposed by Li et al. (2021). The main idea is that only some of the modules of Wav2Vec 2.0 and mBART need to be fine-tuned to build a system capable of ST."
2021.iwslt-1.11,2021.acl-long.68,0,0.184511,"n those with the given segmentation, we also decided to work with a custom segmentation algorithm. We base it on the approach of Potapczyk et al. (2019), but we replace the silence detection tool with an ASR system (§3.4). Our experiments on the IWSLT 2019 test set, show that our system works better when the data are segmented with our own segmentation algorithm (§4.3). 2 System description We built an end-to-end ST system, mainly composed of pre-trained modules. We couple a Wav2Vec 2.0 encoder (Baevski et al., 2020) and an mBART decoder (Liu et al., 2020a), following the strategy proposed by Li et al. (2021). When combining these two models, there is a length discrepancy between the target sentence length and the encoder output. For this reason, it is necessary to use a module to shorten the encoder output, which we refer to as the Length Adaptor. Additionally, we introduce an Adapter module to reduce the gap between the different modalities of the pre-trained models (Bapna and Firat, 2019). A method that Escolano et al. (2020) proved to be beneficial for ST models. 2.1 Pre-trained modules Our motivation is to get the most out of pretrained components, which were obtained by selfsupervision or su"
2021.iwslt-1.11,2020.tacl-1.47,0,0.153834,"ith own segmentation algorithms are strictly better than those with the given segmentation, we also decided to work with a custom segmentation algorithm. We base it on the approach of Potapczyk et al. (2019), but we replace the silence detection tool with an ASR system (§3.4). Our experiments on the IWSLT 2019 test set, show that our system works better when the data are segmented with our own segmentation algorithm (§4.3). 2 System description We built an end-to-end ST system, mainly composed of pre-trained modules. We couple a Wav2Vec 2.0 encoder (Baevski et al., 2020) and an mBART decoder (Liu et al., 2020a), following the strategy proposed by Li et al. (2021). When combining these two models, there is a length discrepancy between the target sentence length and the encoder output. For this reason, it is necessary to use a module to shorten the encoder output, which we refer to as the Length Adaptor. Additionally, we introduce an Adapter module to reduce the gap between the different modalities of the pre-trained models (Bapna and Firat, 2019). A method that Escolano et al. (2020) proved to be beneficial for ST models. 2.1 Pre-trained modules Our motivation is to get the most out of pretrained c"
2021.iwslt-1.11,W18-6319,0,0.0135912,"eters of each effect from the ranges shown at Table 3. 3.4 5 riod, which is identified by the absence of English characters in it. The algorithm terminates when the max segment length condition is satisfied or no further splits are possible due to a minimum untranscribable period length, which we set to 0.2 seconds. We test max seg len ∈ [5, 25], and for each value we produce a segmentation, generate translations using one of our ST systems 6 , use the mwerSegmenter 7 software to align the generated translations with the reference translations, and finally obtain a BLEU score using SACREBLEU (Post, 2018). We find that the maximum BLEU score is obtained using max seg len = 22 seconds (Figure 3), which we use to segment the IWSLT 2020 and 2021 test sets for our submission. Data Segmentation Similarly to 2019 and 2020 (Niehues et al., 2019; Ansari et al., 2020), this year’s evaluation data are segmented using an automatic tool (Meignier and Merlin, 2010), which does not ensure that segments are proper sentences nor that they are aligned with the translated text. This assigns extra importance to developing methods for proper segmentation of the audio data, which was confirmed in the previous year"
2021.iwslt-1.11,2020.iwslt-1.9,0,0.158522,"the slow inference time (Weiss et al., 2017). Nevertheless, while there are plenty of data available to train ASR and MT systems, there are not as many datasets for ST, despite some recent efforts (Di Gangi et al., 2019a; Wang et al., 2020b). Moreover, this approach is inherently more difficult because the encoder has to perform both acoustic modeling and semantic encoding. For these reasons, end-to-end ST systems still struggle to achieve the performance of cascade ST models. Still, last year’s IWSLT was the first time an end-to-end system had the best performance in the evaluation campaign (Potapczyk and Przybysz, 2020; Ansari et al., 2020). Hence, given the increasing interest in end-to-end ST systems, and the potential gains from advancing research on them, we decided to focus on developing such a system for this year’s offline task. When there are not enough data for a task, a common practice is to use pre-trained components, like BERT (Devlin et al., 2019) for various NLP tasks. In the ST field, the idea of pre-training the encoder for ASR was introduced by Berard et al. (2018) and has become a standard technique for developing modern end-to-end systems (Pino et al., 2019; Di Gangi et al., 2019b). By co"
2021.iwslt-1.11,2020.aacl-demo.6,0,0.303336,"on (MT) model, which is known as cascade system. However, in recent years, end-to-end models have gained popularity within the research community. These systems are encoder-decoder architectures capable of directly translating speech without intermediate symbolic representations. This approach solves classical shortcomings of cascade ST systems, e.g. the error propagation or the slow inference time (Weiss et al., 2017). Nevertheless, while there are plenty of data available to train ASR and MT systems, there are not as many datasets for ST, despite some recent efforts (Di Gangi et al., 2019a; Wang et al., 2020b). Moreover, this approach is inherently more difficult because the encoder has to perform both acoustic modeling and semantic encoding. For these reasons, end-to-end ST systems still struggle to achieve the performance of cascade ST models. Still, last year’s IWSLT was the first time an end-to-end system had the best performance in the evaluation campaign (Potapczyk and Przybysz, 2020; Ansari et al., 2020). Hence, given the increasing interest in end-to-end ST systems, and the potential gains from advancing research on them, we decided to focus on developing such a system for this year’s off"
2021.iwslt-1.11,2020.lrec-1.517,0,0.0362141,"on (MT) model, which is known as cascade system. However, in recent years, end-to-end models have gained popularity within the research community. These systems are encoder-decoder architectures capable of directly translating speech without intermediate symbolic representations. This approach solves classical shortcomings of cascade ST systems, e.g. the error propagation or the slow inference time (Weiss et al., 2017). Nevertheless, while there are plenty of data available to train ASR and MT systems, there are not as many datasets for ST, despite some recent efforts (Di Gangi et al., 2019a; Wang et al., 2020b). Moreover, this approach is inherently more difficult because the encoder has to perform both acoustic modeling and semantic encoding. For these reasons, end-to-end ST systems still struggle to achieve the performance of cascade ST models. Still, last year’s IWSLT was the first time an end-to-end system had the best performance in the evaluation campaign (Potapczyk and Przybysz, 2020; Ansari et al., 2020). Hence, given the increasing interest in end-to-end ST systems, and the potential gains from advancing research on them, we decided to focus on developing such a system for this year’s off"
2021.ltedi-1.1,S18-2005,0,0.0160724,"ecific references in the literature (Madaan et al., 2018a). This study shows a higher disproportion in mentioning men more than women. More than the disproportion, it is the role that men are usually associated with ”highlevel” occupations compared to women who are associated with more ”care-giving” occupations. Automatic Representations. Our systems have shown that they perpetuate and amplify biases (Bolukbasi et al., 2016; Caliskan et al., 2017) as examples in word representation. We have also seen biases in applications such as machine translation (Prates et al., 2020), sentiment analysis (Kiritchenko and Mohammad, 2018) and others. 4.2 Data In this section, we describe the procedure for extracting the data that allows us to discuss the impact of COVID-19 on the scientific production of NLP papers in terms of gender, contribution and experience. This data is available to other researchers2 . 2 Inferring gender of names The study (Mohammad, 2020b) examines the citations and authors of the publications of the ACL anthology. In this dataset, called NLP-scholar, we can find the gender of the first and last authors. Thus, this was the first direction to find out the gender of the authors. To investigate the gender"
2021.ltedi-1.1,2020.acl-main.464,0,0.130234,"stems have shown that they perpetuate and amplify biases (Bolukbasi et al., 2016; Caliskan et al., 2017) as examples in word representation. We have also seen biases in applications such as machine translation (Prates et al., 2020), sentiment analysis (Kiritchenko and Mohammad, 2018) and others. 4.2 Data In this section, we describe the procedure for extracting the data that allows us to discuss the impact of COVID-19 on the scientific production of NLP papers in terms of gender, contribution and experience. This data is available to other researchers2 . 2 Inferring gender of names The study (Mohammad, 2020b) examines the citations and authors of the publications of the ACL anthology. In this dataset, called NLP-scholar, we can find the gender of the first and last authors. Thus, this was the first direction to find out the gender of the authors. To investigate the gender of authors, not available in this dataset, we used two APIs4,5 , which allow extracting the gender of the name to ensure that the gender of the name is correctly recognized. We use the former to predict the names. However, if the name is predicted with less accuracy, we use the latter for verification. It is a difficult task to"
2021.ltedi-1.1,D18-1301,0,0.0333957,"hers in the last position of authors (supervision or collaborative work). 1 3. In a collaborative work, while working remotely, the contributions of junior women seems to be disregarded and they are not included unless their contribution is predominant. This decreased of scientific production of the junior women as last authors. 2 Introduction Impact Statement This work is doing a binary gender study. In that sense, we are simplifying the real gender spectrum (D’Ignazio and Klein, 2018). Our motivation is because we want to study the effect of COVID-19 in women, in the line of previous works (Schluter, 2018). By highlighting the role of women in scientific contributions, we emphasize the importance of strengthening the educational programs, scientific grants and support that motivate woman in NLP and in STEM (science, technology, engineering and mathematics) in general. The pandemic situation is a burden for everyone. Given the uncertainty and pressure of the situation, it is expected to have a significant impact on the lives of researchers. However, it appears that women are bearing considerable burdens due to the social impact of the virus. According to the Scientist article 1 , the pandemic ex"
adell-etal-2012-buceador,J06-4004,1,\N,Missing
adell-etal-2012-buceador,garcia-mateo-etal-2004-transcrigal,0,\N,Missing
adell-etal-2012-buceador,luengo-etal-2010-modified,1,\N,Missing
avramidis-etal-2012-richly,steinberger-etal-2006-jrc,0,\N,Missing
avramidis-etal-2012-richly,vandeghinste-etal-2008-evaluation,1,\N,Missing
avramidis-etal-2012-richly,W09-0424,0,\N,Missing
avramidis-etal-2012-richly,P07-2045,0,\N,Missing
avramidis-etal-2012-richly,C04-1072,0,\N,Missing
avramidis-etal-2012-richly,W08-0309,0,\N,Missing
avramidis-etal-2012-richly,2005.mtsummit-papers.11,0,\N,Missing
avramidis-etal-2012-richly,W10-1720,1,\N,Missing
avramidis-etal-2012-richly,P03-1021,0,\N,Missing
costa-jussa-etal-2008-using,popovic-ney-2006-pos,0,\N,Missing
costa-jussa-etal-2008-using,carreras-etal-2004-freeling,0,\N,Missing
costa-jussa-etal-2008-using,2005.mtsummit-papers.36,1,\N,Missing
costa-jussa-etal-2008-using,P05-2012,0,\N,Missing
costa-jussa-etal-2008-using,W00-0508,0,\N,Missing
costa-jussa-etal-2008-using,J04-4002,0,\N,Missing
costa-jussa-etal-2008-using,knight-al-onaizan-1998-translation,0,\N,Missing
costa-jussa-etal-2008-using,J04-2004,0,\N,Missing
costa-jussa-etal-2008-using,J04-2003,0,\N,Missing
costa-jussa-etal-2008-using,A00-1031,0,\N,Missing
costa-jussa-etal-2008-using,J06-4004,1,\N,Missing
costa-jussa-etal-2008-using,W05-0831,0,\N,Missing
costa-jussa-etal-2008-using,N03-1017,0,\N,Missing
costa-jussa-etal-2008-using,J03-1002,0,\N,Missing
costa-jussa-etal-2008-using,P05-1069,0,\N,Missing
costa-jussa-etal-2008-using,N04-1033,0,\N,Missing
costa-jussa-etal-2008-using,N03-1019,0,\N,Missing
costa-jussa-etal-2008-using,P03-1019,0,\N,Missing
costa-jussa-etal-2010-automatic,E06-1032,0,\N,Missing
costa-jussa-etal-2010-automatic,P02-1040,0,\N,Missing
costa-jussa-etal-2010-automatic,J06-4004,1,\N,Missing
costa-jussa-etal-2010-automatic,P03-1021,0,\N,Missing
D07-1045,N06-2001,0,0.0137965,"or phrase-based statistical machine translation systems is needed. In particular, the problem of generalization to new translations seems to be promising to us. This could be addressed by the so-called factored phrase-based model as implemented in the Moses decoder (Koehn et al., 2007). In this approach words are decomposed into several factors. These factors are trans437 lated and a target phrase is generated. This model could be complemented by a factored continuous tuple N-gram. Factored word language models were already successfully used in speech recognition (Bilmes and Kirchhoff, 2003; Alexandrescu and Kirchhoff, 2006) and an extension to machine translation seems to be promising. The described smoothing method was explicitly developed to tackle the data sparseness problem in tasks like the B TEC corpus. It is well known from language modeling that careful smoothing is less important when large amounts of data are available. We plan to investigate whether this also holds for smoothing of the probabilities in phrase- or tuplebased statistical machine translation systems. 6 Acknowledgments This work has been partially funded by the European Union under the integrated project T C -S TAR (IST2002-FP6-506738), b"
D07-1045,N03-2002,0,0.0217703,"of probabilities in N-gram- or phrase-based statistical machine translation systems is needed. In particular, the problem of generalization to new translations seems to be promising to us. This could be addressed by the so-called factored phrase-based model as implemented in the Moses decoder (Koehn et al., 2007). In this approach words are decomposed into several factors. These factors are trans437 lated and a target phrase is generated. This model could be complemented by a factored continuous tuple N-gram. Factored word language models were already successfully used in speech recognition (Bilmes and Kirchhoff, 2003; Alexandrescu and Kirchhoff, 2006) and an extension to machine translation seems to be promising. The described smoothing method was explicitly developed to tackle the data sparseness problem in tasks like the B TEC corpus. It is well known from language modeling that careful smoothing is less important when large amounts of data are available. We plan to investigate whether this also holds for smoothing of the probabilities in phrase- or tuplebased statistical machine translation systems. 6 Acknowledgments This work has been partially funded by the European Union under the integrated project"
D07-1045,W06-1607,0,0.0566714,"or instance found in (Chen and Goodman, 1999). Language models and phrase tables have in common that the probabilities of rare events may be overestimated. However, in language modeling probability mass must be redistributed in order to account for the unseen n-grams. Generalization to unseen events is less important in phrase-based SMT systems since the system searches only for the best segmentation and the best matching phrase pair among the existing ones. We are only aware of one work that performs a systematic comparison of smoothing techniques in phrase-based machine translation systems (Foster et al., 2006). Two types of phrase-table smoothing were compared: black-box and glass-box methods. Black-methods do not look inside phrases but instead treat them as atomic objects. By these means, all the methods developed for language modeling can be used. Glass-box methods decompose P (˜ e|˜f ) ˜ into a set of lexical distributions P (e|f ). For instance, it was suggested to use IBM-1 probabilities (Och et al., 2004), or other lexical translation probabilities (Koehn et al., 2003; Zens and Ney, 2004). Some form of glass-box smoothing is now used in all state-of-the-art statistical machine translation sy"
D07-1045,2003.mtsummit-tttt.3,0,0.0660512,"ware of one work that performs a systematic comparison of smoothing techniques in phrase-based machine translation systems (Foster et al., 2006). Two types of phrase-table smoothing were compared: black-box and glass-box methods. Black-methods do not look inside phrases but instead treat them as atomic objects. By these means, all the methods developed for language modeling can be used. Glass-box methods decompose P (˜ e|˜f ) ˜ into a set of lexical distributions P (e|f ). For instance, it was suggested to use IBM-1 probabilities (Och et al., 2004), or other lexical translation probabilities (Koehn et al., 2003; Zens and Ney, 2004). Some form of glass-box smoothing is now used in all state-of-the-art statistical machine translation systems. Another approach related to phrase table smoothing is the so-called N-gram translation model (Mari˜no et al., 2006). In this model, bilingual tuples are used instead of the phrase pairs and n-gram probabilities are considered rather than relative frequencies. Therefore, smoothing is obtained using the standard techniques developed for language modeling. In addition, a context dependence of the phrases is introduced. On the other hand, some restrictions on the seg"
D07-1045,P07-2045,0,0.00408354,"ion task (over 40 BLEU percentage). Using the continuous space model for the translation and target language model, an improvement of 2.5 BLEU on the development data and 1.5 BLEU on the test data was observed. Despite these encouraging results, we believe that additional research on improved estimation of probabilities in N-gram- or phrase-based statistical machine translation systems is needed. In particular, the problem of generalization to new translations seems to be promising to us. This could be addressed by the so-called factored phrase-based model as implemented in the Moses decoder (Koehn et al., 2007). In this approach words are decomposed into several factors. These factors are trans437 lated and a target phrase is generated. This model could be complemented by a factored continuous tuple N-gram. Factored word language models were already successfully used in speech recognition (Bilmes and Kirchhoff, 2003; Alexandrescu and Kirchhoff, 2006) and an extension to machine translation seems to be promising. The described smoothing method was explicitly developed to tackle the data sparseness problem in tasks like the B TEC corpus. It is well known from language modeling that careful smoothing i"
D07-1045,P02-1038,0,0.0206542,") is to produce a target sentence e from a source sentence f . Among all possible target language sentences the one with the highest probability is chosen: e∗ = arg max Pr(e|f ) = arg max Pr(f |e) Pr(e) e X = arg max{exp( e where Pr(f |e) is the translation model and Pr(e) is the target language model. This approach is usually referred to as the noisy source-channel approach in statistical machine translation (Brown et al., 1993). λi hi (e, f ))} (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). The phrase translation probabilities P (˜ e|˜f ) and P (˜f |˜ e) are usually obtained using relative frequency estimates. Statistical learning theory, however, tells us that relative frequency estimates have several drawbacks, in particular high variance and low bias. Phrase tables may contain several millions of entries, most of which appear only once or twice, which means that we are confronted with a data sparseness problem. Surprisingly, there seems to be little work addressing the issue of smoothing of the phrase table probabilities. On the other hand, smoothing of relative frequency es"
D07-1045,W99-0604,0,0.0167882,"Missing"
D07-1045,N04-1021,0,0.0338653,"est matching phrase pair among the existing ones. We are only aware of one work that performs a systematic comparison of smoothing techniques in phrase-based machine translation systems (Foster et al., 2006). Two types of phrase-table smoothing were compared: black-box and glass-box methods. Black-methods do not look inside phrases but instead treat them as atomic objects. By these means, all the methods developed for language modeling can be used. Glass-box methods decompose P (˜ e|˜f ) ˜ into a set of lexical distributions P (e|f ). For instance, it was suggested to use IBM-1 probabilities (Och et al., 2004), or other lexical translation probabilities (Koehn et al., 2003; Zens and Ney, 2004). Some form of glass-box smoothing is now used in all state-of-the-art statistical machine translation systems. Another approach related to phrase table smoothing is the so-called N-gram translation model (Mari˜no et al., 2006). In this model, bilingual tuples are used instead of the phrase pairs and n-gram probabilities are considered rather than relative frequencies. Therefore, smoothing is obtained using the standard techniques developed for language modeling. In addition, a context dependence of the phrase"
D07-1045,2006.iwslt-papers.2,1,0.869281,"Missing"
D07-1045,takezawa-etal-2002-toward,0,0.016304,"den layer (200 to 500). Therefore, in previous applications of the continuous space n-gram model, the output was limited to the s most frequent units, s ranging between 2k and 12k (Schwenk, 2007). This is called a short-list. Train (bitexts) Dev Eval Sents 20k 489 500 Words 155.4/166.3k 5.2k 6k 4 Experimental Evaluation In this work we report results on the Basic Traveling Expression Corpus (B TEC) as used in the 2006 evaluations of the international workshop on spoken language translation (I WSLT). This corpus consists of typical sentences from phrase books for tourists in several languages (Takezawa et al., 2002). We report results on the supplied development corpus of 489 sentences and the official test set of the I WSLT’06 evaluation. The main measure is the BLEU score, using seven reference translations. The scoring is case insensitive and punctuations are ignored. Details on the available data are summarized in Table 1. We concentrated first on the translation from Italian to English. All participants in the I WSLT evaluation achieved much better performances for this language pair than for the other considered translation directions. This makes it more difficult to achieve additional improvements"
D07-1045,N04-1033,0,0.0469216,"t performs a systematic comparison of smoothing techniques in phrase-based machine translation systems (Foster et al., 2006). Two types of phrase-table smoothing were compared: black-box and glass-box methods. Black-methods do not look inside phrases but instead treat them as atomic objects. By these means, all the methods developed for language modeling can be used. Glass-box methods decompose P (˜ e|˜f ) ˜ into a set of lexical distributions P (e|f ). For instance, it was suggested to use IBM-1 probabilities (Och et al., 2004), or other lexical translation probabilities (Koehn et al., 2003; Zens and Ney, 2004). Some form of glass-box smoothing is now used in all state-of-the-art statistical machine translation systems. Another approach related to phrase table smoothing is the so-called N-gram translation model (Mari˜no et al., 2006). In this model, bilingual tuples are used instead of the phrase pairs and n-gram probabilities are considered rather than relative frequencies. Therefore, smoothing is obtained using the standard techniques developed for language modeling. In addition, a context dependence of the phrases is introduced. On the other hand, some restrictions on the segmentation of the sour"
D07-1045,2005.mtsummit-papers.36,1,\N,Missing
D19-3026,N16-1082,0,0.0279356,"gual sentence representation: distance and sentences. cross-lingual and multilingual natural language processing downstream applications in general. Linguistic insights. (Raganato and Tiedemann, 2018) show interesting findings about syntactic and semantic behavior across Transformer layers. Following this research line, our tool can further analyse how similar sentences in multiple languages evolve in their intermediate layer representations as well as monolingual sentences with same syntactic or morphological patterns. Finally, regarding related visualizations and demonstrations, authors in (Li et al., 2016) make an visual analysis of neural models specifically in natural language processing (but focusing on previous architectures to the Transformer), while (Vig, 2019) analyse the attention in the Transformer at multiple-scales and show different use cases on contextual word embeddings. Our tool further adds to these previous works by focusing on the intermediate representations. Figure 8: Required JSON structures: (Left) use cases 1 and 2 and (Right) use case 3 Multilinguality analysis. It is quite a common practice to visualize intermediate representations of sequence-to-sequence models (Johnso"
D19-3026,W18-5431,0,0.0360272,"Missing"
D19-3026,W19-3805,1,0.889464,"Missing"
D19-3026,P19-2033,1,0.860869,"Missing"
D19-3026,P19-3007,0,0.0273703,"(Raganato and Tiedemann, 2018) show interesting findings about syntactic and semantic behavior across Transformer layers. Following this research line, our tool can further analyse how similar sentences in multiple languages evolve in their intermediate layer representations as well as monolingual sentences with same syntactic or morphological patterns. Finally, regarding related visualizations and demonstrations, authors in (Li et al., 2016) make an visual analysis of neural models specifically in natural language processing (but focusing on previous architectures to the Transformer), while (Vig, 2019) analyse the attention in the Transformer at multiple-scales and show different use cases on contextual word embeddings. Our tool further adds to these previous works by focusing on the intermediate representations. Figure 8: Required JSON structures: (Left) use cases 1 and 2 and (Right) use case 3 Multilinguality analysis. It is quite a common practice to visualize intermediate representations of sequence-to-sequence models (Johnson et al., 2017; Escolano et al., 2019). Our tool is not limited to this sentence representation of the intermediate representation, but it also includes the tokenle"
D19-3026,W19-3821,1,0.887988,"Missing"
D19-3026,N19-1064,0,0.0515847,"Missing"
D19-3026,2005.mtsummit-papers.11,0,\N,Missing
D19-3026,Q17-1024,0,\N,Missing
D19-3026,L16-1561,0,\N,Missing
E14-2009,N03-1017,0,0.00763079,"lability of internet almost everywhere, have allowed for lots of traditional on-line applications and services to be deployed on these mobile platforms. In this demo paper we describe “CHISPA on the GO” a Chinese-Spanish translation service that intends to provide a portable and easy to use language assistance tool for travelers between Chinese and Spanish speaking countries. The main three characteristics of the presented demo system are as follows: 2 SMT system description The translation technology used in our system is based on the well-known phrase-based translation statistical approach (Koehn et al., 2003). This approach performs the translation splitting the source sentence in segments and assigning to each segment a bilingual phrase from a phrasetable. Bilingual phrases are translation units that contain source words and target words, and have different scores associated to them. These bilingual phrases are then selected in order to maximize a linear combination of feature functions. Such strategy is known as the log-linear model (Och and Ney, 2002). The two main feature functions are the translation model and the target language model. Additional models include lexical weights, phrase and wo"
E14-2009,P07-2045,0,0.00523924,"was synthetically produced by translating from English, (4) a large TAUS corpus (TausData, 2013) which comes from technical translation memories, and (5) an inhouse developed small corpus in the transportation and hospitality domains. In total we have 70 million words. A careful preprocessing was developed for all languages. Chinese was segmented with Stanford segmenter (Tseng et al., 2005) and Spanish was preprocessed with Freeling (Padr´o et al., 2010). When Spanish is used as a source language, it is preprocessed by lower-casing and unaccented the input. Finally, we use the MOSES decoder (Koehn et al., 2007) with standard configuration: aligngrow-final-and alignment symmetrization, 5-gram language model with interpolation and kneser-ney discount and phrase-smoothing and lexicalized reordering. We use our in-house developed corpus to optimize because our application is targeted to the travelers-in-need domain. 3 Web Translator and Mobile Application Figure 1: Block diagram of the system architecture This section describes the main system architecture and the main features of web translator and the mobile applications. 3.1 of the two PHP scripts supports Chinese to Spanish translations and the othe"
E14-2009,padro-etal-2010-freeling,0,0.0293451,"Missing"
E14-2009,2009.mtsummit-posters.15,0,0.0144715,"s. For Chinese-Spanish, we use (1) the Holy Bible corpus (Banchs and Li, 2008), (2) the • First, the system uses a direct translation between Chinese and Spanish, rather than using a pivot language as intermediate step as most of the current commercial systems do when dealing with distant languages. 33 Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 33–36, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics United Nations corpus, which was released for research purposes (Rafalovitch and Dale, 2009), (3) a small subset of the European Parliament Plenary Speeches where the Chinese part was synthetically produced by translating from English, (4) a large TAUS corpus (TausData, 2013) which comes from technical translation memories, and (5) an inhouse developed small corpus in the transportation and hospitality domains. In total we have 70 million words. A careful preprocessing was developed for all languages. Chinese was segmented with Stanford segmenter (Tseng et al., 2005) and Spanish was preprocessed with Freeling (Padr´o et al., 2010). When Spanish is used as a source language, it is pre"
E14-2009,P02-1038,0,\N,Missing
federmann-etal-2012-ml4hmt,vandeghinste-etal-2008-evaluation,1,\N,Missing
federmann-etal-2012-ml4hmt,federmann-2010-appraise,1,\N,Missing
federmann-etal-2012-ml4hmt,E06-1005,0,\N,Missing
federmann-etal-2012-ml4hmt,W09-0424,0,\N,Missing
federmann-etal-2012-ml4hmt,P02-1040,0,\N,Missing
federmann-etal-2012-ml4hmt,W02-1019,0,\N,Missing
federmann-etal-2012-ml4hmt,W05-0909,0,\N,Missing
federmann-etal-2012-ml4hmt,W08-0309,0,\N,Missing
federmann-etal-2012-ml4hmt,W10-1720,1,\N,Missing
federmann-etal-2012-ml4hmt,W11-2101,0,\N,Missing
I11-1154,P07-1092,0,0.0828654,"s on the Chinese-English corpus and EnglishSpanish corpus, and then building a pivot translation model for Chinese-Spanish translation using English as a pivot language as proposed in (Wu and Wang, 2007); the second one obtained better results and it was based on a cascade approach. The idea here is to translate from Chinese into English and then from English to Spanish, which means performing two translations. Besides the research mentioned above, which directly addressed the Chinese-Spanish language pair, we may also find in the literature another approach similar to Wu’s (2007) authored by Cohn and Lapata (2007). Basically, they also used several intermediate pivot language to create source-totarget phrases that are lately interpolate with a direct system build with a source-to-target parallel corpus. Apart from the BTEC3 corpus available through the IWSLT4 competition and Holy Bible datasets described in (Paul, 2008) and (Banchs and Li, 2008), respectively, there is a recent release of a six language parallel corpus (including both Chinese and Spanish) from United Nations (UN) for research purposes (Rafalovith and Dale, 2009). Using the recently released UN parallel corpus as a starting point, this"
I11-1154,P07-2026,0,0.0162423,"eudo-Corpus System This approach translates the pivot section of the source-pivot parallel corpus to the target language using a pivot-target system built previously. Then, a source-target SMT system is built using the source side and the translated pivot side of the source-pivot corpus. The pseudo-corpus system is tuned using a direct source-target development corpus. 2.4 Pivot combination Using the 1-best translation output from the different pivot strategies, we built an N-best list and computed the final translation using MBR. MBR has been used both during decoding (Kumar and Byrne, 2004; Ehling et al., 2007) and as a postprocess over an N-best list. The current version of the M OSES toolkit includes both MBR implementations. For the system combinations we used the second one. The MBR algorithm implemented in M OSES uses (1 − BLEU )β as the Loss Function. The value β weights the hypothesis proportionally to its translation score, but we considered all our hypothesis as equal so β was a constant and therefore could be discarded. At the end, MBR choose the hypotheses E ′ that fulfills:   X E ′ = arg min  1 − BLEU (E, Eˆ′ ) (1) Eˆ′ E6=Eˆ′ It is important to mention that all N-best list must have"
I11-1154,P05-1071,0,0.0419567,"E ′ ) is a brevity penalty if the hypothesis E ′ is shorter than the reference E. Then pn (E, E ′ ) = pn (E ′ , E) and ∀E, E ′ : length(E) &gt; length(E ′ ) : 1 − BLEU (E, E ′ ) ≥ 1 − BLEU (E ′ , E) 3 (3) Evaluation Framework This section introduces the details of the evaluation framework used. We report the UN corpus statistics, a description of how we built the systems and the evaluation details. the same training, tuning and testing sets. All corpora were tokenized, using the standard M OSES tokenizer for Spanish, English and French; ictclass (Zhang et al., 2003) for Chinese; and MADA+TOKAN (Habash and Rambow, 2005) for Arabic. The Spanish, English and French corpora were lowercased. If a sentence had more than 100 words in any language, it was deleted from all corpora. If a sentence pair had a word ratio bigger than three for any Chinese-Pivot or Pivot-Spanish parallel corpora, it was deleted from all corpora. For all languages, we identify all sentences that occur only once in the corpora. The tuning and testing sets where drawn from the available multilingual corpus by using a maximum perplexity and lowest outof-vocabulary word criterion over the English part of the dataset. In order to do this, perpl"
I11-1154,N03-1017,0,0.0114307,"nish translation which are tested in this work. Section 3 presents the evaluation framework. Then, section 4 reports the experiments (including the system combination) and the results. Finally, section 5 concludes and proposes new research directions. 2 Direct and pivot statistical machine translation approaches There are several strategies that we can follow when translating a pair of languages in Statistical Machine Translation. The next three sub-sections present the details of the ones we are using in this work. 2.1 Direct system Our direct system uses the phrase-based translation system (Koehn et al., 2003). This popular system implements a log-linear model in which a source language sentence f J = f1 , f2 , . . . , fJ is translated into another language (target) sentence eI = e1 , e2 , . . . , eI by searching for the translation hypothesis eˆI maximizing a log-linear combination of several feature models (Och, 2003). The main system models are the translation model and the language model. The first one deals with the issue of which target language phrase fj translates a source language phrase ei and the latter model estimates the probability of translation hypothesis. 1362 Apart from these two"
I11-1154,P07-2045,0,0.00650059,"target) sentence eI = e1 , e2 , . . . , eI by searching for the translation hypothesis eˆI maximizing a log-linear combination of several feature models (Och, 2003). The main system models are the translation model and the language model. The first one deals with the issue of which target language phrase fj translates a source language phrase ei and the latter model estimates the probability of translation hypothesis. 1362 Apart from these two models, there are other standard models such as the lexical models, the word bonus, and the reordering model. For decoding, we used the M OSES toolkit (Koehn et al., 2007) with the option of Minimum Bayes Risk (MBR) (Kumar and Byrne, 2004) decoding. Therefore the 1best translation obtained is not the one with highest priority but the one that is most similar to the most likely translation. The option was activated with its default parameters so it considered the top 200 distinct hypothesis to compute the 1best. 2.2 Cascade System This approach handles the source-pivot and the pivot-target system independently. They are both built and tuned to improve their local translation quality and then joined to translate from the source language to the target language in"
I11-1154,W04-3250,0,0.129168,"Missing"
I11-1154,N04-1022,0,0.224825,"International Joint Conference on Natural Language Processing, pages 1361–1365, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP problem in hand by means of pivot-language strategies through the other languages available in the UN parallel corpus, such as Arabic, English and French. More specifically, strategies such as system cascading and pseudo-corpus generation are implemented and compared against a baseline system implementing a direct translation approach. We propose a system combination different from previous ones (Wu and Wang, 2009) and based on the Minimum Bayes Risk (MBR) (Kumar and Byrne, 2004) technique using both pivot strategies which is capable to highly outperform the direct system. To the best of our knowledge, this idea was not explored before and it is a way of increasing the quality of translation between languages with scarce bilingual resources. In addition, we are performing a combination of the same system but introducing new information through the pivot language. The paper is structured as follows. Section 2 describes the main strategies for performing Chineseto-Spanish translation which are tested in this work. Section 3 presents the evaluation framework. Then, secti"
I11-1154,P03-1021,0,0.0327603,"everal strategies that we can follow when translating a pair of languages in Statistical Machine Translation. The next three sub-sections present the details of the ones we are using in this work. 2.1 Direct system Our direct system uses the phrase-based translation system (Koehn et al., 2003). This popular system implements a log-linear model in which a source language sentence f J = f1 , f2 , . . . , fJ is translated into another language (target) sentence eI = e1 , e2 , . . . , eI by searching for the translation hypothesis eˆI maximizing a log-linear combination of several feature models (Och, 2003). The main system models are the translation model and the language model. The first one deals with the issue of which target language phrase fj translates a source language phrase ei and the latter model estimates the probability of translation hypothesis. 1362 Apart from these two models, there are other standard models such as the lexical models, the word bonus, and the reordering model. For decoding, we used the M OSES toolkit (Koehn et al., 2007) with the option of Minimum Bayes Risk (MBR) (Kumar and Byrne, 2004) decoding. Therefore the 1best translation obtained is not the one with highe"
I11-1154,2009.mtsummit-posters.15,0,0.0110614,"lso find in the literature another approach similar to Wu’s (2007) authored by Cohn and Lapata (2007). Basically, they also used several intermediate pivot language to create source-totarget phrases that are lately interpolate with a direct system build with a source-to-target parallel corpus. Apart from the BTEC3 corpus available through the IWSLT4 competition and Holy Bible datasets described in (Paul, 2008) and (Banchs and Li, 2008), respectively, there is a recent release of a six language parallel corpus (including both Chinese and Spanish) from United Nations (UN) for research purposes (Rafalovith and Dale, 2009). Using the recently released UN parallel corpus as a starting point, this work focuses on the problem of developing Chinese-Spanish phrase-based SMT technologies with a limited set of bilingual resources. We explore and evaluate different alternatives for the 3 4 Basic Traveller Expressions Corpus International Workshop on Spoken Language Translation 1361 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1361–1365, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP problem in hand by means of pivot-language strategies through the other language"
I11-1154,P07-1108,0,0.345231,"-Spanish tracks. One of them was focused on direct translation and the other one on pivot translation through 1 2 www.ethnologue.org/ethno docs/distribution.asp?by=size http://mastarpj.nict.go.jp/IWSLT2008/ English. Best translation results were obtained by far in the pivot task. The best system in the pivot task (Wang et al., 2008) compared two different approaches: The first one, training two translation models on the Chinese-English corpus and EnglishSpanish corpus, and then building a pivot translation model for Chinese-Spanish translation using English as a pivot language as proposed in (Wu and Wang, 2007); the second one obtained better results and it was based on a cascade approach. The idea here is to translate from Chinese into English and then from English to Spanish, which means performing two translations. Besides the research mentioned above, which directly addressed the Chinese-Spanish language pair, we may also find in the literature another approach similar to Wu’s (2007) authored by Cohn and Lapata (2007). Basically, they also used several intermediate pivot language to create source-totarget phrases that are lately interpolate with a direct system build with a source-to-target para"
I11-1154,P09-1018,0,0.294516,"op on Spoken Language Translation 1361 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1361–1365, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP problem in hand by means of pivot-language strategies through the other languages available in the UN parallel corpus, such as Arabic, English and French. More specifically, strategies such as system cascading and pseudo-corpus generation are implemented and compared against a baseline system implementing a direct translation approach. We propose a system combination different from previous ones (Wu and Wang, 2009) and based on the Minimum Bayes Risk (MBR) (Kumar and Byrne, 2004) technique using both pivot strategies which is capable to highly outperform the direct system. To the best of our knowledge, this idea was not explored before and it is a way of increasing the quality of translation between languages with scarce bilingual resources. In addition, we are performing a combination of the same system but introducing new information through the pivot language. The paper is structured as follows. Section 2 describes the main strategies for performing Chineseto-Spanish translation which are tested in t"
I11-1154,W03-1730,0,0.0155253,"in the hypothesis E ′ with reference E; and γ(E, E ′ ) is a brevity penalty if the hypothesis E ′ is shorter than the reference E. Then pn (E, E ′ ) = pn (E ′ , E) and ∀E, E ′ : length(E) &gt; length(E ′ ) : 1 − BLEU (E, E ′ ) ≥ 1 − BLEU (E ′ , E) 3 (3) Evaluation Framework This section introduces the details of the evaluation framework used. We report the UN corpus statistics, a description of how we built the systems and the evaluation details. the same training, tuning and testing sets. All corpora were tokenized, using the standard M OSES tokenizer for Spanish, English and French; ictclass (Zhang et al., 2003) for Chinese; and MADA+TOKAN (Habash and Rambow, 2005) for Arabic. The Spanish, English and French corpora were lowercased. If a sentence had more than 100 words in any language, it was deleted from all corpora. If a sentence pair had a word ratio bigger than three for any Chinese-Pivot or Pivot-Spanish parallel corpora, it was deleted from all corpora. For all languages, we identify all sentences that occur only once in the corpora. The tuning and testing sets where drawn from the available multilingual corpus by using a maximum perplexity and lowest outof-vocabulary word criterion over the E"
I11-1154,2008.iwslt-evaluation.17,1,\N,Missing
I11-1154,2008.iwslt-evaluation.18,0,\N,Missing
I11-1154,2008.iwslt-evaluation.1,0,\N,Missing
I11-1154,2008.iwslt-evaluation.4,0,\N,Missing
J06-4004,W05-0823,1,0.838951,"Missing"
J06-4004,W00-0508,0,0.0142428,"y approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the finite-state perspective—more specifically, from"
J06-4004,J96-1002,0,0.02988,"Missing"
J06-4004,J90-2002,0,0.81424,"nslation was conceived as the problem of finding a sentence by decoding a given “encrypted” version of it (Weaver 1955). Although the idea seemed very feasible, enthusiasm faded shortly afterward because of the computational limitations of the time (Hutchins 1986). Finally, during the nineties, two factors made it possible for SMT to become an actual and practical technology: first, significant increment in both the computational power and storage capacity of computers, and second, the availability of large volumes of bilingual data. The first SMT systems were developed in the early nineties (Brown et al. 1990, 1993). These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for adequacy of translation contents, times a target language probability p(T), which accounts for fluency of target constructions. For these first SMT systems, translation-model probabilities at the sentence level were approximated from word-based translation models that were trained by using bilingual corpora (Brown et al. 1993). In the case of target"
J06-4004,J93-2003,0,0.0556776,"d in the early nineties (Brown et al. 1990, 1993). These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for adequacy of translation contents, times a target language probability p(T), which accounts for fluency of target constructions. For these first SMT systems, translation-model probabilities at the sentence level were approximated from word-based translation models that were trained by using bilingual corpora (Brown et al. 1993). In the case of target language probabilities, these were generally trained from monolingual data by using n-grams. Present SMT systems have evolved from the original ones in such a way that mainly differ from them in two respects: first, word-based translation models have been ∗ Department of Signal Theory and Communications, Campus Nord, Barcelona 08034, Spain. Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for publication: 5 July 2006 © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 4 replaced by phrase-b"
J06-4004,J04-2004,0,0.856987,"models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the finite-state perspective—more specifically, from the work of Casacuberta (2001) and Casacuberta and Vidal (2004). However, whereas in this earlier work the translation model is implemented by using a finite-state transducer, in the system presented here the translation model is implemented by using n-grams. In this way, the proposed translation system can take full advantage of the smoothing and consistency provided by standard back-off n-gram models. The translation model presented here actually constitutes a language model of a sort of “bilanguage” composed of bilin˜ 2002). An alternagual units, which will be referred to as tuples (de Gispert and Marino tive approach, which relies on bilingual-unit un"
J06-4004,N04-1033,0,0.0140655,"Missing"
J06-4004,2005.iwslt-1.23,1,0.883592,"Missing"
J06-4004,2005.mtsummit-papers.37,1,0.855856,"Missing"
J06-4004,2006.amta-papers.4,1,0.763242,"Missing"
J06-4004,P05-2012,1,0.804737,"Missing"
J06-4004,C86-1155,0,0.079146,"ament Plenary Sessions (EPPS). 1. Introduction The beginnings of statistical machine translation (SMT) can be traced back to the early fifties, closely related to the ideas from which information theory arose (Shannon and Weaver 1949) and inspired by works on cryptography (Shannon 1949, 1951) during World War II. According to this view, machine translation was conceived as the problem of finding a sentence by decoding a given “encrypted” version of it (Weaver 1955). Although the idea seemed very feasible, enthusiasm faded shortly afterward because of the computational limitations of the time (Hutchins 1986). Finally, during the nineties, two factors made it possible for SMT to become an actual and practical technology: first, significant increment in both the computational power and storage capacity of computers, and second, the availability of large volumes of bilingual data. The first SMT systems were developed in the early nineties (Brown et al. 1990, 1993). These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for"
J06-4004,knight-al-onaizan-1998-translation,0,0.230855,"more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the finite-state persp"
J06-4004,N03-1017,0,0.0258625,"Missing"
J06-4004,2005.mtsummit-papers.36,1,0.177804,"Missing"
J06-4004,P00-1056,0,0.0440511,"tence pairs are removed from the training data to allow for a better performance of the alignment tool. Sentence pairs are removed according to the following two criteria: r r Fertility filtering: removes sentence pairs with a word ratio larger than a predefined threshold value. Length filtering: removes sentence pairs with at least one sentence of more than 100 words in length. This helps to maintain bounded alignment computational times. After preprocessing, word-to-word alignments are performed in both directions, source-to-target and target-to-source. In our system implementation, GIZA++ (Och and Ney 2000) is used for computing the alignments. A total of five iterations for models IBM-1 and HMM, and three iterations for models IBM-3 and IBM-4, are performed. Then, the obtained alignment sets are used for computing the intersection and the union of alignments from which tuples and embedded-word tuples are extracted, respectively. 4.2.2 Tuple Extraction and Pruning. A tuple set for each translation direction is extracted from the union set of alignments while avoiding source-nulled tuples by using the procedure described in Section 2.2.2. Then, the resulting tuple vocabularies are pruned accordin"
J06-4004,P02-1038,0,0.884384,"034, Spain. Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for publication: 5 July 2006 © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 4 replaced by phrase-based translation models (Zens, Och, and Ney 2002; Koehn, Och, and Marcu 2003) which are directly estimated from aligned bilingual corpora by considering relative frequencies, and second, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of finitesta"
J06-4004,J03-1002,0,0.00706932,"gram model alone, in the case of Table 2, and by using the tuple n-gram model along with the additional four feature functions described in Section 3.2, in the case of Table 3. Both translation directions, Spanish to English (ES → EN) and English to Spanish (EN → ES), are considered in each table. In the case of Table 2, model size and translation accuracy are evaluated against the type of alignment set used for extracting tuples. Three different alignment sets are considered: source-to-target, the union of source-to-target and target-to-source, and the “refined” alignment method described by Och and Ney (2003). For the results presented in Table 2, a pruning parameter value of N = 20 was used for the Spanish-to-English direction, while a value of N = 30 was used for the English-to-Spanish direction. As can be clearly seen in Table 2, the union alignment set happens to be the most favorable one for extracting tuples in both translation directions since it provides a significantly better translation accuracy, in terms of BLEU score, than the other two alignment sets considered. Notice also in Table 2 that the union set is the one providing the smallest model sizes according to the number of bigrams a"
J06-4004,P02-1040,0,0.105596,"alignment sets. Notice that BLEU measurements in this table correspond to translations computed by using the tuple n-gram model alone. Direction Alignment set Tuple voc. Bigrams Trigrams BLEU ES → EN Source-to-target union refined Source-to-target union refined 1.920 2.040 2.111 1.813 2.023 2.081 6.426 6.009 6.851 6.263 6.092 6.920 2.353 1.798 2.398 2.268 1.747 2.323 0.4424 0.4745 0.4594 0.4152 0.4276 0.4193 EN → ES when tuples are extracted from different alignment sets and when different pruning parameters are used, respectively. Translation accuracy is measured in terms of the BLEU score (Papineni et al. 2002), which is computed here for translations generated by using the tuple n-gram model alone, in the case of Table 2, and by using the tuple n-gram model along with the additional four feature functions described in Section 3.2, in the case of Table 3. Both translation directions, Spanish to English (ES → EN) and English to Spanish (EN → ES), are considered in each table. In the case of Table 2, model size and translation accuracy are evaluated against the type of alignment set used for extracting tuples. Three different alignment sets are considered: source-to-target, the union of source-to-targ"
J06-4004,N03-2036,0,0.00459439,"k the translation model is implemented by using a finite-state transducer, in the system presented here the translation model is implemented by using n-grams. In this way, the proposed translation system can take full advantage of the smoothing and consistency provided by standard back-off n-gram models. The translation model presented here actually constitutes a language model of a sort of “bilanguage” composed of bilin˜ 2002). An alternagual units, which will be referred to as tuples (de Gispert and Marino tive approach, which relies on bilingual-unit unigram probabilities, was developed by Tillmann and Xia (2003); in contrast, the approach presented here considers bilingualunit n-gram probabilities. In addition to the tuple n-gram translation model, the translation system presented here implements four specific feature functions that are log-linearly combined along with the translation model for performing the decoding ˜ et al. 2005). (Marino This article is intended to provide a detailed description of the n-gram-based translation system, as well as to demonstrate the system performance in a widedomain, large-vocabulary translation task. The article is structured as follows. First, Section 2 presents"
J06-4004,2002.tmi-tutorials.2,0,0.201063,"Missing"
J06-4004,2004.iwslt-evaluation.14,1,\N,Missing
J06-4004,N04-1021,0,\N,Missing
melero-etal-2012-holaaa,N10-1060,0,\N,Missing
melero-etal-2012-holaaa,C08-1056,0,\N,Missing
melero-etal-2012-holaaa,W06-3113,0,\N,Missing
melero-etal-2012-holaaa,A88-1011,0,\N,Missing
melero-etal-2012-holaaa,quixal-etal-2008-user,1,\N,Missing
N07-2035,2006.iwslt-evaluation.18,1,0.890672,"Missing"
N07-2035,2005.iwslt-1.23,1,0.905555,"Missing"
N07-2035,J06-4004,1,0.895476,"Missing"
N07-2035,E06-1005,1,0.799617,"ingual N -gram language model. In the phrase-based model, no monotonicity restriction is imposed on the segmentation and the probabilities are normally estimated simply by relative frequencies. This paper extends the analysis of both systems performed in (Crego et al., 2005a) by additionally performing a manual error analysis of both systems, which were the ones used by UPC and RWTH in the last Tc-Star evaluation. Furthermore, we will propose a way to combine both systems in order to improve the quality of translations. Experiments combining several kinds of MT systems have been presented in (Matusov et al., 2006), based only on the single best output of each system. Recently, a more straightforward approach of both systems has been performed in (Costa-juss` a et al., 2006) which simply selects, for each sentence, one of the provided hypotheses. This paper is organized as follows. In section 2, we briefly describe the phrase and the N -gram-based baseline systems. In the next section we present the evaluation framework. In Section 4 we report a structural comparison performed for both systems and, afterwards, in Section 5, we analyze the errors of both systems. Finally, in the last two sections we resc"
N07-2035,vilar-etal-2006-error,1,0.861806,"Missing"
N07-2035,N04-1033,1,0.812475,"n N -gram-based one. The exhaustive analysis includes a comparison of the translation models in terms of efficiency (number of translation units used in the search and computational time) and an examination of the errors in each system’s output. Additionally, we combine both systems, showing accuracy improvements. 1 Introduction Statistical machine translation (SMT) has evolved from the initial word-based translation models to more advanced models that take the context surrounding the words into account. The so-called phrase-based and N -gram-based models are two examples of these approaches (Zens and Ney, 2004; Mari˜ no et al., 2006). In current state-of-the-art SMT systems, the phrase-based or the N -gram-based models are usually the main features in a log-linear framework, reminiscent of the maximum entropy modeling approach. Two basic issues differentiate the N -gram-based system from the phrase-based one: the training data is sequentially segmented into bilingual units; and the probability of these units is estimated as a bilingual N -gram language model. In the phrase-based model, no monotonicity restriction is imposed on the segmentation and the probabilities are normally estimated simply by"
N07-2035,W06-3120,1,\N,Missing
P16-2058,D15-1041,0,0.219831,"cture has changed in that we are using a convolutional neural network (CNN) and a highway network over characters before the attention-based mechanism of the encoder. This is a significant difference from previous work (Sennrich et al., 2015) which uses the neural MT architecture from (Bahdanau et al., 2015) without modification to deal with subword units (but not including unigram characters). Subword-based representations have already been explored in Natural Language Processing (NLP), e.g. for POS tagging (Santos and Zadrozny, 2014), name entity recognition (Santos and aes, 2015), parsing (Ballesteros et al., 2015), normalization (Chrupala, 2014) or learning word representations (Botha and Blunsom, 2014; Chen et al., 2015). These previous works show different advantages of using character-level information. In our case, with the new characterNeural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard look"
P16-2058,D15-1176,0,0.136834,"ch word tj is predicted based on a recurrent hidden state, the previously predicted word tj−1 , and a context vector. This context vector is obtained from the weighted sum of the annotations hk , which in turn, is computed through an alignment model αjk (a feedforward neural network). This neural MT approach has achieved competitive results against the standard phrase-based system in the WMT 2015 evaluation (Jean et al., 2015). based neural MT architecture, we take advantage of intra-word information, which is proven to be extremely useful in other NLP applications (Santos and Zadrozny, 2014; Ling et al., 2015a), especially when dealing with morphologically rich languages. When using the character-based source word embeddings in MT, there ceases to be unknown words in the source input, while the size of the target vocabulary remains unchanged. Although the target vocabulary continues with the same limitation as in the standard neural MT system, the fact that there are no unknown words in the source helps to reduce the number of unknowns in the target. Moreover, the remaining unknown target words can now be more successfully replaced with the corresponding source-aligned words. As a consequence, we"
P16-2058,W14-4012,0,0.240981,"Missing"
P16-2058,P16-2058,1,0.106153,"Missing"
P16-2058,W15-3904,0,0.0272269,"ponding word. The system architecture has changed in that we are using a convolutional neural network (CNN) and a highway network over characters before the attention-based mechanism of the encoder. This is a significant difference from previous work (Sennrich et al., 2015) which uses the neural MT architecture from (Bahdanau et al., 2015) without modification to deal with subword units (but not including unigram characters). Subword-based representations have already been explored in Natural Language Processing (NLP), e.g. for POS tagging (Santos and Zadrozny, 2014), name entity recognition (Santos and aes, 2015), parsing (Ballesteros et al., 2015), normalization (Chrupala, 2014) or learning word representations (Botha and Blunsom, 2014; Chen et al., 2015). These previous works show different advantages of using character-level information. In our case, with the new characterNeural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway"
P16-2058,P14-2111,0,0.0280027,"onvolutional neural network (CNN) and a highway network over characters before the attention-based mechanism of the encoder. This is a significant difference from previous work (Sennrich et al., 2015) which uses the neural MT architecture from (Bahdanau et al., 2015) without modification to deal with subword units (but not including unigram characters). Subword-based representations have already been explored in Natural Language Processing (NLP), e.g. for POS tagging (Santos and Zadrozny, 2014), name entity recognition (Santos and aes, 2015), parsing (Ballesteros et al., 2015), normalization (Chrupala, 2014) or learning word representations (Botha and Blunsom, 2014; Chen et al., 2015). These previous works show different advantages of using character-level information. In our case, with the new characterNeural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. T"
P16-2058,P16-1160,0,0.347901,"out of the eyes where officials lose sight of it causing the officers to lose sight of it Table 2: Translation examples. Phrase NN NN+Src CHAR CHAR+Src De-&gt;En 20.99 18.83 20.64 21.40 22.10 En-&gt;De 17.04 16.47 17.15 19.53 20.22 tation in vocabulary size. In this paper we have proposed a modification to the standard encoder/decoder neural MT architecture to use unlimited-vocabulary character-based source word embeddings. The improvement in BLEU is about 1.5 points in German-to-English and more than 3 points in English-to-German. As further work, we are currently studying different alternatives (Chung et al., 2016) to extend the character-based approach to the target side of the neural MT system. Table 3: De-En BLEU results. number of out-of-vocabulary words of the test set is shown in Table 1. The character-based embedding has an impact in learning a better translation model at various levels, which seems to include better alignment, reordering, morphological generation and disambiguation. Table 2 shows some examples of the kind of improvements that the character-based neural MT system is capable of achieving compared to baseline systems. Examples 1 and 2 show how the reduction of source unknowns impro"
P16-2058,W15-3014,0,0.0250557,"ociation for Computational Linguistics, pages 357–361, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics sentences. The decoder also becomes a GRU and each word tj is predicted based on a recurrent hidden state, the previously predicted word tj−1 , and a context vector. This context vector is obtained from the weighted sum of the annotations hk , which in turn, is computed through an alignment model αjk (a feedforward neural network). This neural MT approach has achieved competitive results against the standard phrase-based system in the WMT 2015 evaluation (Jean et al., 2015). based neural MT architecture, we take advantage of intra-word information, which is proven to be extremely useful in other NLP applications (Santos and Zadrozny, 2014; Ling et al., 2015a), especially when dealing with morphologically rich languages. When using the character-based source word embeddings in MT, there ceases to be unknown words in the source input, while the size of the target vocabulary remains unchanged. Although the target vocabulary continues with the same limitation as in the standard neural MT system, the fact that there are no unknown words in the source helps to reduce"
P16-2058,D13-1176,0,0.0623351,"e proposed MT scheme provides improved results even when the source language is not morphologically rich. Improvements up to 3 BLEU points are obtained in the German-English WMT task. 1 Introduction Machine Translation (MT) is the set of algorithms that aim at transforming a source language into a target language. For the last 20 years, one of the most popular approaches has been statistical phrase-based MT, which uses a combination of features to maximise the probability of the target sentence given the source sentence (Koehn et al., 2003). Just recently, the neural MT approach has appeared (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) and obtained state-of-the-art results. Among its different strengths neural MT does not need to pre-design feature functions beforehand; optimizes the entire system at once because 357 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 357–361, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics sentences. The decoder also becomes a GRU and each word tj is predicted based on a recurrent hidden state, the previously predicted word tj−1 , and a context v"
P16-2058,N03-1017,0,0.00982693,"MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme provides improved results even when the source language is not morphologically rich. Improvements up to 3 BLEU points are obtained in the German-English WMT task. 1 Introduction Machine Translation (MT) is the set of algorithms that aim at transforming a source language into a target language. For the last 20 years, one of the most popular approaches has been statistical phrase-based MT, which uses a combination of features to maximise the probability of the target sentence given the source sentence (Koehn et al., 2003). Just recently, the neural MT approach has appeared (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) and obtained state-of-the-art results. Among its different strengths neural MT does not need to pre-design feature functions beforehand; optimizes the entire system at once because 357 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 357–361, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics sentences. The decoder also becomes a GRU and each word tj is predicted bas"
P16-2058,P07-2045,0,0.00662165,"rd embedding In the target size we are still limited in vocabulary by the softmax layer at the output of the network and we kept the standard target word embeddings in our experiments. However, the results seem to show that the affix-aware representation of the source words has a positive influence on all the components of the network. The global optimization of the integrated model forces the translation model and the internal vector representation of the target words to follow the affix-aware codification of the source words. 4 Baseline systems The phrase-based system was built using Moses (Koehn et al., 2007), with standard parameters such as grow-final-diag for alignment, GoodTuring smoothing of the relative frequencies, 5gram language modeling using Kneser-Ney discounting, and lexicalized reordering, among others. The neural-based system was built using the software from DL4MT2 available in github. We generally used settings from previous work (Jean et al., 2015): networks have an embedding of 620 and a dimension of 1024, a batch size of 32, and no dropout. We used a vocabulary size of 90 thousand words in German-English. Also, as proposed in (Jean et al., 2015) we replaced unknown words (UNKs)"
P16-2058,P16-1100,0,\N,Missing
P19-2033,D16-1163,0,0.0564736,"Missing"
P19-2033,P19-1120,0,0.0203202,"X and Y , our objective is to train independent encoders and decoders for each language, ex , dx and ey , dy that produce compatible sentence representations h(x), h(y). For instance, given a sentence x in language X, we can obtain a representation h(x) from that the encoder ex that can be used to either generate a sentence reconstruction using decoder dx or a translation using decoder dy . With this objective in mind, we propose a training schedule that combines two tasks (auto-encoding and translation) and the two translation directions simultaneously by optimizing the following loss: 2016; Kim et al., 2019) to improve the performance of new translation directions by taking benefit of the information of a previous model. These approaches are particularly useful in low resources scenarios when a previous model trained with orders of magnitude more examples is available. This paper proposes a proof of concept of a new multilingual NMT approach. The current approach is based on joint training without parameter or vocabulary sharing by enforcing a compatible representation between the jointly trained languages and using multitask learning (Dong et al., 2015). This approach is shown to offer a scalabl"
P19-2033,2005.mtsummit-papers.11,0,0.222987,"language is required to perform the translation, once the added modules are trained the zero-shot translation is performed without generating the language used for training as the sentence representations in the shared space are compatible with all the modules in the system. A current limitation is the need to use the same vocabulary for the shared language (X) in both training steps. The use of subwords (Sennrich et al., 2015) mitigates the impact of this constraint. 5 Data and Implementation Experiments are conducted using data extracted from the UN (Ziemski et al., 2016) and EPPS datasets (Koehn, 2005) that provide 15 million parallel sentences between English and Spanish, German and French. newstest2012 and new238 System Baseline Joint Added lang ES-EN 32.60 29.70 - EN-ES 32.90 30.74 - FR-EN 31.81 30.93 DE-EN 28.96 27.63 stest2013 were used as validation and test sets, respectively. These sets provide parallel data between the four languages that allow for zero-shot evaluation. Preprocessing consisted of a pipeline of punctuation normalization, tokenization, corpus filtering of longer sentences than 80 words and true-casing. These steps were performed using the scripts available from Moses"
P19-2033,P07-2045,0,0.00799226,"hat provide 15 million parallel sentences between English and Spanish, German and French. newstest2012 and new238 System Baseline Joint Added lang ES-EN 32.60 29.70 - EN-ES 32.90 30.74 - FR-EN 31.81 30.93 DE-EN 28.96 27.63 stest2013 were used as validation and test sets, respectively. These sets provide parallel data between the four languages that allow for zero-shot evaluation. Preprocessing consisted of a pipeline of punctuation normalization, tokenization, corpus filtering of longer sentences than 80 words and true-casing. These steps were performed using the scripts available from Moses (Koehn et al., 2007). Preprocessed data is later tokenized into BPE subwords (Sennrich et al., 2015) with a vocabulary size of 32000 tokens. We ensure that the vocabularies are independent and reusable when new languages were added by creating vocabularies monolingually, i.e. without having access to other languages during the code generation. Our second experiment consists of incrementally adding different languages to the system, in this case, German and French. Note that, since we freeze the weights while adding the new language, the order in which we add new languages does not have any impact on performance."
P19-2033,P15-1166,0,\N,Missing
P19-2033,Q17-1024,0,\N,Missing
P19-2033,L16-1561,0,\N,Missing
P19-2033,W17-2619,0,\N,Missing
P19-2033,D19-3026,1,\N,Missing
P19-2033,P16-1162,0,\N,Missing
W05-0827,J90-2002,0,0.823493,"ard phrase-based translation system. We describe a modified method for the phrase extraction which deals with larger phrases while keeping a reasonable number of phrases. We also propose additional features which lead to a clear improvement in the performance of the translation. We present results with the EuroParl task in the direction Spanish to English and results from the evaluation of the shared task “Exploiting Parallel Texts for Statistical Machine Translation” (ACL Workshop on Parallel Texts 2005). e˜ = argmax P (f |e)P (e) This translation model is known as the sourcechannel approach [1] and it consists on a language model P (e) and a separate translation model P (f |e) [5]. In the last few years, new systems tend to use sequences of words, commonly called phrases [8], aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework. ( M ) X e˜ = argmax λm hm (e, f ) (3) e 1 Introduction Statistical Machine Translation (SMT) is based on the assumption that every sentence e in the target language is a possible translation of a given sentence f in the source"
W05-0827,N03-1017,0,0.157199,"se additional features which lead to a clear improvement in the performance of the translation. We present results with the EuroParl task in the direction Spanish to English and results from the evaluation of the shared task “Exploiting Parallel Texts for Statistical Machine Translation” (ACL Workshop on Parallel Texts 2005). e˜ = argmax P (f |e)P (e) This translation model is known as the sourcechannel approach [1] and it consists on a language model P (e) and a separate translation model P (f |e) [5]. In the last few years, new systems tend to use sequences of words, commonly called phrases [8], aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework. ( M ) X e˜ = argmax λm hm (e, f ) (3) e 1 Introduction Statistical Machine Translation (SMT) is based on the assumption that every sentence e in the target language is a possible translation of a given sentence f in the source language. The main difference between two possible translations of a given sentence is a probability assigned to each, which has to be learned from a bilingual text corpus. Thus, the"
W05-0827,N04-1021,0,0.0750731,"the probability was taken to be 10−40 . In addition, we have calculated the IBM−1 Model This BP is clearly overestimated due to sparse- 1. ness. On the other, note that ”la que no” cannot be considered an unusual trigram in Spanish. I X J Y 1 Hence, the language model does not penalise this P (e|f ; M 1) = p(ei |fj ) (7) target sequence either. So, the total probability (J + 1)I I=1 j=0 (P (f |e)P (e)) would be higher than desired. In order to somehow compensate these unreiliable probabilities we have studied the inclusion of 4.4 Language Model the posterior [12] and lexical probabilities [1] [10] The English language model plays an important as additional features. role in the source channel model, see equation (2), and also in its modification, see equation (3). The English language model should give an idea of the 4.2 Feature P (e|f ) sentence quality that is generated. In order to estimate the posterior phrase probabilAs default language model feature, we use a stanity, we compute again the relative frequency but re- dard word-based trigram language model generated placing the count of the target phrase by the count with smoothing Kneser-Ney and interpolation (by of the source phra"
W05-0827,J04-4002,0,0.458218,"), e˜ = argmax P (e|f ) (1) e 0 This work has been supported by the European Union under grant FP6-506738 (TC-STAR project). (2) e m=1 The features functions, hm , are the system models (translation model, language model and others) and weigths, λi , are typically optimized to maximize a scoring function. It is derived from the Maximum Entropy approach suggested by [13] [14] for a natural language understanding task. It has the advantatge that additional features functions can be easily integrated in the overall system. This paper addresses a modification of the phrase-extraction algorythm in [11]. It also combines several interesting features and it reports an important improvement from the baseline. It is organized as follows. Section 2 introduces the baseline; the following section explains the modification in the phrase extraction; section 4 shows the different features which have been taken into account; section 5 presents the evaluation framework; and 149 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 149–154, c Ann Arbor, June 2005. Association for Computational Linguistics, 2005 the final section shows some conclusions on the experiments in the pape"
W05-0827,N04-1033,0,0.504097,". Baseline The baseline is based on the source-channel approach, and it is composed of the following models which later will be combined in the decoder. The Translation Model. It is based on bilingual phrases, where a bilingual phrase (BP ) is simply two monolingual phrases (M P ) in which each one is supposed to be the translation of each other. A monolingual phrase is a sequence of words. Therefore, the basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [17]. During training, the system has to learn a dictionary of phrases. We begin by aligning the training corpus using GIZA++ [6], which is done in both translation directions. We take the union of both alignments to obtain a symmetrized word alignment matrix. This alignment matrix is the starting point for the phrase based extraction. Next, we define the criterion to extract the set of BP of the sentence pair (fjj12 ; eii21 ) and the alignment matrix A ⊆ J ∗I, which is identical to the alignment criterion described in [11]. BP (f1J , eI1 , A) = {(fjj12 , eii21 ) : 3 Phrase Extraction Motivation."
W05-0827,P02-1038,0,0.102199,") = P (you|la que no) = 0.23 case, the probability was taken to be 10−40 . In addition, we have calculated the IBM−1 Model This BP is clearly overestimated due to sparse- 1. ness. On the other, note that ”la que no” cannot be considered an unusual trigram in Spanish. I X J Y 1 Hence, the language model does not penalise this P (e|f ; M 1) = p(ei |fj ) (7) target sequence either. So, the total probability (J + 1)I I=1 j=0 (P (f |e)P (e)) would be higher than desired. In order to somehow compensate these unreiliable probabilities we have studied the inclusion of 4.4 Language Model the posterior [12] and lexical probabilities [1] [10] The English language model plays an important as additional features. role in the source channel model, see equation (2), and also in its modification, see equation (3). The English language model should give an idea of the 4.2 Feature P (e|f ) sentence quality that is generated. In order to estimate the posterior phrase probabilAs default language model feature, we use a stanity, we compute again the relative frequency but re- dard word-based trigram language model generated placing the count of the target phrase by the count with smoothing Kneser-Ney and i"
W05-0827,2001.mtsummit-papers.68,0,0.0362208,"d the development divided in two sets. This material corresponds to the transcriptions of the sessions from October the 21st to October the 28th. It has been distributed by ELDA2 . Results are reported for Spanish-to-English translations. 1 http://www.tcstar.org/ 2 http://www.elda.org/ Experiments The decoder used for the presented translation system is reported in [2]. This decoder is called MARIE and it takes into account simultaneously all the 7 features functions described above. It implements a beam-search strategy. As evaluation criteria we use: the Word Error Rate (WER), the BLEU score [15] and the NIST score [3]. As follows we report the results for several experiments that show the performance of: the baseline, adding the posterior probability, IBM Model 1 and IBM1−1 , and, finally, the modification of the phrases extraction. Optimisation. Significant improvements can be obtained by tuning the parameters of the features adequately. In the complet system we have 7 parameters to tune: the relatives frecuencies P (f |e) and P (e|f ), IBM Model 1 and its inverse, the word penalty, the phrase penalty and the weight of the language model. We applied the widely used algorithm SIMPLEX"
W05-0827,koen-2004-pharaoh,0,\N,Missing
W05-0827,P02-1040,0,\N,Missing
W06-1609,W05-0831,0,0.0718496,"Missing"
W06-1609,N03-1017,0,0.0145061,"e (S) into a reordered source language (S’), which allows for an improved translation into the target language (T). The SMT task changes from S2T to S’2T which leads to a monotonized word alignment and shorter translation units. In addition, the use of classes in SMR helps to infer new word reorderings. Experiments are reported in the EsEn WMT06 tasks and the ZhEn IWSLT05 task and show significant improvement in translation quality. 1 Introduction During the last few years, SMT systems have evolved from the original word-based approach (Brown et al., 1993) to phrase-based translation systems (Koehn et al., 2003). In parallel to the phrase-based approach, the use of bilingual n-grams gives comparable results, as shown by Crego et al. (2005a). Two basic issues differentiate the n-gram-based system from the phrasebased: training data are monotonously segmented into bilingual units; and, the model considers ngram probabilities rather than relative frequencies. This translation approach is described in detail by Mari˜no et al. (2005). The n-gram-based system follows a maximum entropy approach, in which a log-linear combination of multiple models is im70 Proceedings of the 2006 Conference on Empirical Meth"
W06-1609,2005.mtsummit-papers.36,1,0.789709,"Missing"
W06-1609,mauser-etal-2006-training,0,0.0232632,"Missing"
W06-1609,P02-1038,0,0.122392,"Missing"
W06-1609,2005.iwslt-1.23,1,0.911138,"Missing"
W06-1609,J93-2003,0,\N,Missing
W06-3120,A00-1031,0,0.0229195,"ese figures in the optimization function. 3 Shared Task Results 3.1 Data The data provided for this shared task corresponds to a subset of the official transcriptions of the European Parliament Plenary Sessions, and it is available through the shared task website at: http://www.statmt.org/wmt06/shared-task/. The development set used to tune the system consists of a subset (500 first sentences) of the official development set made available for the Shared Task. We carried out a morphological analysis of the data. The English POS-tagging has been carried out using freely available T N T tagger (Brants, 2000). In the Spanish case, we have used the F reeling (Carreras et al., 2004) analysis tool which generates the POS-tagging for each input word. 3.2 Systems configurations The baseline system is the same for all tasks and includes the following features functions: cp, pp, lm, ibm1, ibm1−1 , wb, pb. The POStag target language model has been used in those tasks for which the tagger was available. Table 1 shows the reordering configuration used for each task. The Block Reordering (application 2) has been used when the source language belongs to the Romanic family. The length of the block is limited t"
W06-3120,W05-0827,1,0.879283,"Missing"
W06-3120,2005.iwslt-1.23,1,0.907982,"Missing"
W06-3120,W06-3125,1,0.884485,"Missing"
W06-3120,P05-2012,1,0.889176,"Missing"
W06-3120,W05-0831,0,0.0297232,"Full verb forms The morphology of the verbs usually differs in each language. Therefore, it is interesting to classify the verbs in order to address the rich variety of verbal forms. Each verb is reduced into its base form and reduced POS tag as explained in (de Gispert, 2005). This transformation is only done for the alignment, and its goal is to simplify the work of the word alignment improving its quality. Block reordering (br) The difference in word order between two languages is one of the most significant sources of error in SMT. Related works either deal with reordering in general as (Kanthak et al., 2005) or deal with local reordering as (Tillmann and Ney, 2003). We report a local reordering technique, which is implemented as a preprocessing stage, with two applications: (1) to improve only alignment quality, and (2) to improve alignment quality and to infer reordering in translation. Here, we present a short explanation of the algorithm, for further details see Costa-juss`a and Fonollosa (2006). 142 Proceedings of the Workshop on Statistical Machine Translation, pages 142–145, c New York City, June 2006. 2006 Association for Computational Linguistics of the bilingual phrase, and no word on ei"
W06-3120,W06-3114,0,0.0221223,"ts to observe its efficiency in all the pairs used in this evaluation. The rgraph has been applied in those cases where: we do not use br2 (there is no sense in applying them simultaneously); and we have the tagger for the source language model available. In the case of the pair GeEn, we have not experimented any reordering, we left the application of both reordering approaches as future work. 3.3 Discussion Table 2 presents the BLEU scores evaluated on the test set (using TRUECASE) for each configuration. The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006). For both, Es2En and Fr2En tasks, br helps slightly. The improvement of the approach depends on the quality of the alignment. The better alignments allow to extract higher quality Alignment Blocks (Costa-juss`a and Fonollosa, 2006). The En2Es task is improved when adding both br1 and rgraph. Similarly, the En2Fr task seems to perform fairly well when using the rgraph. In this case, the improvement of the approach depends on the quality of the alignment patterns (Crego et al., 2006). However, it has the advantage of delaying the final decision of reordering to the overall search, where all mod"
W06-3120,N03-1017,0,0.00728769,"o infer reordering in translation. Here, we present a short explanation of the algorithm, for further details see Costa-juss`a and Fonollosa (2006). 142 Proceedings of the Workshop on Statistical Machine Translation, pages 142–145, c New York City, June 2006. 2006 Association for Computational Linguistics of the bilingual phrase, and no word on either side of the phrase is aligned to a word out of the phrase. We limit the maximum size of any given phrase to 7. The huge increase in computational and storage cost of including longer phrases does not provide a significant improvement in quality (Koehn et al., 2003) as the probability of reappearance of larger phrases decreases. 2.3 Figure 1: Example of an Alignment Block, i.e. a pair of consecutive blocks whose target translation is swapped This reordering strategy is intended to infer the most probable reordering for sequences of words, which are referred to as blocks, in order to monotonize current data alignments and generalize reordering for unseen pairs of blocks. Given a word alignment, we identify those pairs of consecutive source blocks whose translation is swapped, i.e. those blocks which, if swapped, generate a correct monotone translation. Fi"
W06-3120,J04-4002,0,0.0268059,"created). Based on this information, the source side of the bilingual corpora are reordered. In case of applying the reordering technique for purpose (1), we modify only the source training corpora to realign and then we recover the original order of the training corpora. In case of using Block Reordering for purpose (2), we modify all the source corpora (both training and test), and we use the new training corpora to realign and build the final translation system. 2.2 Phrase Extraction Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in Och and Ney (2004). A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: words are consecutive along both sides 143 Feature functions Conditional and posterior probability (cp, pp) Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency in both directions. The target language model (lm) consists of an n-gram model, in which the probability of a translation hypothesis is approximated by the product of word n-gram probabilities. As default language model feature, we use a standard word-base"
W06-3120,carreras-etal-2004-freeling,0,\N,Missing
W06-3120,J03-1005,0,\N,Missing
W06-3120,N04-1033,0,\N,Missing
W06-3125,W05-0823,1,0.872672,"Missing"
W06-3125,A00-1031,0,0.117706,"d over the development set for each of the six translation directions considered. 163 This baseline system is actually very similar to the system used for last year’s shared task “Exploiting Parallel Texts for Statistical Machine Translation” of ACL’05 Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond (Banchs et al., 2005), whose results are available at: http://www.statmt.org/wpt05/ mt-shared-task/. A more detailed description of the system can be found in (2005). The tools used for POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English. All language models were estimated using the SRI language modeling toolkit. Word-to-word alignments were extracted with GIZA++. Improvements in word-toword alignments were achieved through verb group classification as described in (de Gispert, 2005). 3 Reordering Framework In this section we outline the reordering framework used for the experiments (Crego and Mari˜no, 2006). A highly constrained reordered search is performed by means of a set of reordering patterns (linguistically motivated rewrite patterns) which are used to extend the monotone search graph with additional arcs."
W06-3125,carreras-etal-2004-freeling,0,0.0548444,"Missing"
W06-3125,W06-3120,1,0.883993,"Missing"
W06-3125,N04-1033,0,0.0842803,"Missing"
W06-3125,P05-2012,1,0.901179,"Missing"
W06-3125,N03-1017,0,0.00542142,"luation with a tagged target language model (using Part-Of-Speech tags). For both Spanish-English translation directions and the English-to-French translation task, the baseline system allows for linguistically motivated sourceside reorderings. 2 Baseline N-gram-based SMT System 1 Introduction The statistical machine translation approach used in this work implements a log-linear combination of feature functions along with a translation model which is based on bilingual n-grams (de Gispert and Mari˜no, 2002). This translation model differs from the well known phrase-based translation approach (Koehn et al., 2003) in two basic issues: first, training data is monotonously segmented into bilingual units; and second, the model considers n-gram probabilities instead of relative frequencies. This translation approach is described in detail in (Mari˜no et al., 2005). For those translation tasks with Spanish or English as target language, an additional tagged (usAs already mentioned, the translation model used here is based on bilingual n-grams. It actually constitutes a language model of bilingual units, referred to as tuples, which approximates the joint probability between source and target languages by us"
W06-3125,2005.mtsummit-papers.36,1,0.909254,"Missing"
W06-3125,J93-2003,0,\N,Missing
W07-0720,W06-3125,1,0.849606,"Missing"
W07-0720,W06-1609,1,0.795127,"Missing"
W07-0720,W06-3114,0,0.0213063,"this system participation in the ACL 2007 SECOND WORK SHOP ON STATISTICAL MACHINE TRANSLA TION . Results on three pairs of languages are reported, namely from Spanish, French and German into English (and the other way round) for both the in-domain and out-of-domain tasks. 2 Baseline N-gram-based SMT System 1 Introduction Based on estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternatively to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in (Koehn and Monz, 2005; Koehn and Monz, 2006). Given the challenge of domain adaptation, efforts have been focused on improving strategies for Ngram-based SMT which could generalize better. Specifically, a novel reordering strategy is explored. It is based on extending the search by using precomputed statistical information. Results are promising while keeping computational expenses at a similar level as monotonic search. Additionally, a bonus for tuples from the out-of-domain corpus is The translation model is based on bilingual n-grams. It actually constitutes a language model of bilingual units, referred to as tuples, which approximat"
W07-0720,J06-4004,1,0.847357,"Missing"
W07-0720,E99-1010,0,0.731366,"smaller tuples which reduces the translation vocabulary sparseness. These new tuples are used to build the SMT system. 3 Baseline System Enhanced with a Weighted Reordering Input Graph This section briefly describes the statistical machine reordering (SMR) technique. Further details on the architecture of SMR system can be found on (Costa-juss`a and Fonollosa, 2006). 3.1 Concept The SMR system can be seen as a SMT system which translates from an original source language (S) to a reordered source language (S’), given a target language (T). The SMR technique works with statistical word classes (Och, 1999) instead of words themselves (particularly, we have used 200 classes in all experiments). Figure 1: SMR approach in the (A) training step (B) in the test step (the weight of each arch is in brackets). 3.2 Using SMR technique to improve SMT training The original source corpus S is translated into the reordered source corpus S’ with the SMR system. Figure 1 (A) shows the corresponding block diagram. The reordered training source corpus and the original training target corpus are used to build the SMT system. The main difference here is that the training is computed with the S’2T task instead of"
W07-0720,W05-0820,0,\N,Missing
W07-0721,atserias-etal-2006-freeling,0,0.0228604,"Missing"
W07-0721,P05-1066,0,0.1211,"Missing"
W07-0721,W06-1609,1,0.902305,"Missing"
W07-0721,W07-0720,1,0.861306,"Missing"
W07-0721,2005.iwslt-1.23,1,0.910133,"Missing"
W07-0721,W05-0831,0,0.0437225,"rom the phrase-based: training data is monotonically segmented into bilingual units; and, the model considers ngram probabilities rather than relative frequencies. The n-gram-based system follows a maximum entropy approach, in which a log-linear combination of multiple models is implemented (Mari˜no et al., 2006), as an alternative to the source-channel approach. Introducing reordering capabilities is important in both systems. Recently, new reordering strategies have been proposed such as the reordering of each source sentence to match the word order in the corresponding target sentence, see Kanthak et al. (2005) and Mari˜no et al. (2006). These approaches are applied in the training set and they lack of reordering generalization. Applied both in the training and decoding step, Collins et al. (2005) describe a method for introducing syntactic information for reordering in SMT. This approach is applied as a pre-processing step. Differently, Crego et al. (2006) presents a reordering approach based on reordering patterns which is coupled with decoding. The reordering patterns are learned directly from word alignment and all reorderings have the same probability. In our previous work (Costa-juss`a and Fon"
W07-0721,N03-1017,0,0.0112124,"ees the translation quality improvement due to reordering at a very low increase of computational cost. The SMR approach is capable of generalizing reorderings, which have been learned during training, by using word classes instead of words themselves. We experiment with statistical and morphological classes in order to choose those which capture the most probable reorderings. Satisfactory results are reported in the WMT07 Es/En task. Our system outperforms in terms of BLEU the WMT07 Official baseline system. 1 Introduction Nowadays, statistical machine translation is mainly based on phrases (Koehn et al., 2003). In parallel to this phrasebased approach, the use of bilingual n-grams gives comparable results, as shown by Crego et al. (2005). Two basic issues differentiate the n-gram-based system from the phrase-based: training data is monotonically segmented into bilingual units; and, the model considers ngram probabilities rather than relative frequencies. The n-gram-based system follows a maximum entropy approach, in which a log-linear combination of multiple models is implemented (Mari˜no et al., 2006), as an alternative to the source-channel approach. Introducing reordering capabilities is importa"
W07-0721,J06-4004,1,0.820724,"Missing"
W07-0721,E99-1010,0,0.217093,"nstraints. As a result of these constraints, only one segmentation is possible for a given sentence pair. In addition to the bilingual n-gram translation model, the baseline system implements a log-linear combination of feature functions, which are described as follows: • A target language model. This feature consists of a 4-gram model of words, which is trained from the target side of the bilingual corpus. • A class target language model. This feature consists of a 5-gram model of words classes, which is trained from the target side of the bilingual corpus using the statistical classes from (Och, 1999). • A word bonus function. This feature introduces a bonus based on the number of target words contained in the partial-translation hypothesis. It is used to compensate for the system’s preference for short output sentences. • A source-to-target lexicon model. This feature, which is based on the lexical parameters of the IBM Model 1 (Brown et al., 1993), provides a complementary probability for each tuple in the translation table. These lexicon parameters are obtained from the source-to-target alignments. • A target-to-source lexicon model. Similarly to the previous feature, this feature is ba"
W07-0721,popovic-ney-2006-pos,0,0.0343665,"mation about order, might be solved by training classes in the reordered training source corpus. In other words, we monotonized the training corpus with the alignment information (i.e. reorder the source corpus in the way that matches the target corpus under the alignment links criterion). After that, we train the statistical classes, hereinafter, called statistical reordered classes. In some pair of languages, as for example English/Spanish, the reordering that may be performed is related to word’s morphology (i.e. TAGS). Some TAGS rules (with some lexical exceptions) can be extracted as in (Popovic and Ney, 2006) where they were applied with reordering purposes as a preprocessing step. Another approach that has related TAGS and reordering was presented in (Crego and Mari˜no, 2006) where instead of rules, they learned reordering patterns based on TAGS as named in this paper’s introduction. Hence, the SMR techTrain Dev Test Sentences Words Vocabulary Sentences Words Vocabulary Sentences Words Vocabulary Spanish English 1,3M 37,9M 35,5M 138,9k 133k 2 000 2 000 60.5k 58.7k 8.1k 6.5k 2 000 2 000 60,2k 58k 8,2k 6,5k Table 1: Corpus Statistics. nique may take advantage of the morphological information. Notic"
W07-0721,J93-2003,0,\N,Missing
W08-0315,W08-0315,1,0.0512755,"Missing"
W08-0315,J90-2002,0,0.809551,"Missing"
W08-0315,W07-0718,0,0.152941,"Missing"
W08-0315,carreras-etal-2004-freeling,0,0.138533,"Missing"
W08-0315,W06-3114,0,0.151633,"Missing"
W08-0315,P00-1056,0,0.073606,"Missing"
W08-0315,J04-4002,0,0.0735646,"Missing"
W08-0315,W05-0820,0,\N,Missing
W08-0315,A00-1031,0,\N,Missing
W08-0315,J06-4004,1,\N,Missing
W09-0414,J04-4002,0,0.0266217,"is translated into target language using translation table, (3) the target phrases are reordered to be inherent in the target language. A bilingual phrase (which in the context of SMT do not necessarily coincide with their linguistic analogies) is any pair of m source words and n target words that satisfies two basic constraints: (1) words are consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase. Given a sentence pair and a corresponding wordto-word alignment, phrases are extracted following the criterion in (Och and Ney, 2004). The probability of the phrases is estimated by relative frequencies of their appearance in the training corpus. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 85–89, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 85 Classically, a phrase-based translation system implements a log-linear model in which a foreign language sentence f1J = f1 , f2 , ..., fJ is translated into another language eI1 = e1 , e2 , ..., eI by searching for the translation hypothesis eˆI1 maximizing a log-linear combination of several feature model"
W09-0414,J90-2002,0,0.572532,"he probability of the phrases is estimated by relative frequencies of their appearance in the training corpus. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 85–89, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 85 Classically, a phrase-based translation system implements a log-linear model in which a foreign language sentence f1J = f1 , f2 , ..., fJ is translated into another language eI1 = e1 , e2 , ..., eI by searching for the translation hypothesis eˆI1 maximizing a log-linear combination of several feature models (Brown et al., 1990): eˆI1 = arg max eI1 ( M X w P (w) = λEuroparl · PEuroparl + λN C · PNwC (1) w and PNwC are probabilities aswhere PEuroparl signed to the word sequence w by the LM estimated on Europarl and NC data, respectively. The scale factor values are automatically optimized to obtain the lowest perplexity ppl(w) produced by the interpolated LM P (w). We used the standard script compute − best − mix from the SRI LM package (Stolcke, 2002) for optimization. On the next step, the optimized coefficients λEuroparl and λN C are generalized on the interpolated translation and reordering models. In other words,"
W09-0414,W07-0717,0,0.0343889,"he corresponding improvement in BLEU score is presented in Section 3.3 and summary of the obtained results (Table 4). ) λm hm (eI1 , f1J ) m=1 where the feature functions hm refer to the system models and the set of λm refers to the weights corresponding to these models. 2.1 Translation models interpolation We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estève, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation. In the framework of the evaluation campaign, there were two Spanish-to-English parallel training corpora available: Europarl v.4 corpus (about 50M tokens) and News Commentary (NC) corpus (about 2M tokens). The test dataset provided by the organizers this year was from the news domain, so we considered the Europarl training corpus as &quot;out-of-domain&quot; data and the News Commentary as &quot;in-domain&quot; training material. Unfortunately, the in-domain corpus is much smaller in size, however the Europar"
W09-0414,W07-0733,0,0.0262365,"l LMs and the 2009 development set (English and Spanish references) can be found in Table 1, while the corresponding improvement in BLEU score is presented in Section 3.3 and summary of the obtained results (Table 4). ) λm hm (eI1 , f1J ) m=1 where the feature functions hm refer to the system models and the set of λm refers to the weights corresponding to these models. 2.1 Translation models interpolation We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estève, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation. In the framework of the evaluation campaign, there were two Spanish-to-English parallel training corpora available: Europarl v.4 corpus (about 50M tokens) and News Commentary (NC) corpus (about 2M tokens). The test dataset provided by the organizers this year was from the news domain, so we considered the Europarl training corpus as &quot;out-of-domain&quot; data and the News Commentary as &quot;in-domain&quot; tra"
W09-0414,P07-2045,0,0.00511337,"investigate the translation models (TMs) interpolation for a state-of-the-art phrase-based translation system. Inspired by the work presented in (Schwenk and Estève, 2008), we attack this challenge using the coefficients obtained for the corresponding monolingual language models (LMs) for TMs interpolation. On the second step, we have performed additional word reordering experiments, comparing the results obtained with a statistiTALP-UPC phrase-based SMT The system developed for this year’s shared task is based on a state-of-the-art SMT system implemented within the open-source MOSES toolkit (Koehn et al., 2007). A phrase-based translation is considered as a three step algorithm: (1) the source sequence of words is segmented in phrases, (2) each phrase is translated into target language using translation table, (3) the target phrases are reordered to be inherent in the target language. A bilingual phrase (which in the context of SMT do not necessarily coincide with their linguistic analogies) is any pair of m source words and n target words that satisfies two basic constraints: (1) words are consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned t"
W09-0414,N04-1022,0,\N,Missing
W09-0414,2009.eamt-1.27,1,\N,Missing
W10-0718,esuli-sebastiani-2006-sentiwordnet,0,0.0127386,"gure 1: An example sentence (a) and the three HIT designs used in the experiments: (b) HIT1: a simple categorization scheme, (c) HIT2: a graded categorization scheme, and (d) HIT3: a continuous triangular scoring scheme containing both a horizontal positive-negative axis and a vertical subjective-objective axis. workers to use both a horizontal positive-negative axis and a vertical subjective-objective axis by placing the example sentence anywhere inside the triangle. The subjective-objective axis expresses the degree to which the sentence contains opinionated content and was earlier used by (Esuli and Sebastiani, 2006). For example, the sentence ‘I think this is a wonderful car’ clearly marks an opinion and should be positioned towards the subjective end, while the sentence ‘The car has six cilinders’ should be located towards the objective end. Figure 1d contains an example of HIT3. In order not to burden the workers with overly complex instructions, we did not mention this subjective-objective axis but asked them instead to place ambiguous sentences towards the center of the horizontal positive-negative axis and more objective, non-opinionated sentences towards the lower neutral tip of the triangle. For e"
W10-0718,D08-1027,0,0.37677,"Missing"
W10-1712,J93-1007,0,0.542371,"Missing"
W10-1712,2010.eamt-1.17,1,0.900144,"els among others. Introduction 1 in coop2 3 eration with BMIC and VMU participated in the The TALP Research Center of the UPC 3 Collocation segmentation Collocation segmentation is the process of deSpanish-to-English WMT task. Our primary subtecting boundaries between collocation segments mission was a phrase-based SMT system enhanced within a text (Daudaravicius and Marcinkeviciene, with POS tags and our contrastive submission was 2004). A collocation segment is a piece of text bean augmented phrase-based system using collocatween boundaries. The boundaries are established tion segmentation (Costa-jussà et al., 2010), which in two steps using two dierent measures: the Dice mainly is a way of introducing new phrases in the score and a Average Minimum Law (AML). translation table. This paper presents the descripThe Dice score is used to measure the association of both systems together with the results that tion strength between two words. It has been used we obtained in the evaluation task and is organized before in the collocation compiler XTract (Smadja, as follows: rst, Section 2 and 3 present a brief de1993) and in the lexicon extraction system Chamscription of a phrase-based SMT, followed by a genpol"
W10-1712,N04-4026,0,0.143207,"Missing"
W10-1712,P07-2045,0,0.00766655,"Missing"
W10-1712,2005.mtsummit-papers.11,0,0.00867525,"lues. 4 Ocial test sent Internal test Ocial test 137 369 408 213 119 1246 72 188 2, 106 128 168 2662 The language models were built using SRILM (Stolcke, Table 2: 2002). ocial test sets 4.1 Corpus Unknown words found in internal and It is important to notice that neither the United This year, the translation task provided four difNations nor the Gigaword corpus were used for ferent sources to collect corpora for the Spanishbilingual training. Nevertheless, the English part English pair. Bilingual corpora included version 5 from the United Nations and the monolingual of the Europarl Corpus (Koehn, 2005), the News News corpus were used to build the language model Commentary corpus and the United Nations corof our systems. pus. Additional English corpora was available from the News corpus. The organizers also allowed the 4.1.1 Unknown words use of the English Gigaword Third and Fourth EdiWe analyzed the content from the internal and oftion, released by the LDC. As for development cial test and realized that they both contained and internal test, the test sets from 2008 and 2009 many words that were not seen in the training data. translation tasks were available. Table 2 shows the number of un"
W10-1712,J03-1002,0,0.0100814,"peration with BMIC2 and Bilingual phrases are translation VMU . In phrase-based SMT, the phrase units that contain source words and target words, table is the main tool in translation. It is e.g. created extracting phrases from an aligned and have dierent scores associated to them. These parallel corpus and then computing transbilingual phrases are then sorted in order to maxlation model scores with them. Performing imize a linear combination of feature functions. a collocation segmentation over the source Such strategy is known as the log-linear model and target corpus before the alignment (Och and Ney, 2003) and it is formally dened as: < unidad de traducci´ on |translation unit >, causes that dierent and larger phrases "" are extracted from the same original doceˆ = arg max uments. We performed this segmentation e and used the union of this phrase set with 1 M X λm hm (e, f ) (1) m=1 the phrase set extracted from the nonwhere segmented corpus to compute the phrase weights table. We present the congurations conare the translation model (TM) and the target sidered and also report results obtained language model (LM). Additional models include with internal and ocial test sets. POS target langua"
W10-1712,J96-1001,0,\N,Missing
W10-1712,A00-1031,0,\N,Missing
W10-1712,W09-0414,1,\N,Missing
W10-1712,W07-0702,0,\N,Missing
W11-1014,2010.amta-papers.23,0,\N,Missing
W11-1014,J93-2003,0,\N,Missing
W11-1014,W06-1009,0,\N,Missing
W11-1014,P07-2045,0,\N,Missing
W11-1014,N03-1017,0,\N,Missing
W11-1014,P02-1038,0,\N,Missing
W11-1014,2007.tmi-papers.6,0,\N,Missing
W11-1014,carpuat-wu-2008-evaluation,0,\N,Missing
W11-1014,2009.eamt-1.32,0,\N,Missing
W11-2156,W11-1014,1,0.889484,"Missing"
W11-2156,P07-2045,0,0.0103481,"Missing"
W11-2156,P03-1021,0,0.00665573,"July 30–31, 2011. 2011 Association for Computational Linguistics 2 Phrase-based SMT baseline system The phrase-based approach to SMT performs the translation splitting the source sentence in segments and assigning to each segment a bilingual phrase from a phrase-table. Bilingual phrases are translation units that contain source words and target words, e.g. unit´e de traduction — translation unit, and have different scores associated to them. These bilingual phrases are then selected in order to maximize a linear combination of feature functions. Such strategy is known as the log-linear model (Och, 2003) and it is formally defined as: "" eˆ = arg max e M X # λm hm (e, f ) (1) m=1 where hm are different feature functions with weights λm . The two main feature functions are the translation model (TM) and the target language model (LM). Additional models include lexical weights, phrase and word penalty and reordering. 3 Semantic feature function Source context information is generally disregarded in phrase-based systems given that all training sentences contribute equally to the final translation. The main objective in this section is to motivate the use of a semantic feature function we have rec"
W12-5709,2003.mtsummit-systems.1,0,0.0818841,"Missing"
W12-5709,W11-2107,0,0.0383819,"Missing"
W12-5709,W12-3138,1,0.877314,"Missing"
W12-5709,P07-2045,0,0.00673657,"Missing"
W12-5709,W12-5706,1,0.882913,"Missing"
W12-5709,W12-5705,1,0.848415,"Missing"
W12-5709,P02-1040,0,0.0881819,"Missing"
W12-5709,2006.tc-1.12,0,0.132657,"Missing"
W12-5709,W12-5704,1,0.829445,"Missing"
W12-5709,2009.iwslt-evaluation.8,0,\N,Missing
W13-2215,W11-2107,0,0.0192041,"E models we used the data from the WMT13 shared task on quality estimation (System Selection Quality Estimation at Sentence Level task5 ), which contains the test sets from other WMT campaigns with human assessments. We used five groups of features, namely: i) QuestQE: 17 QE features provided by the Quest toolkit6 ; ii) AsiyaQE: 26 QE features provided by the Asiya toolkit for MT evaluation (Gim´enez and M`arquez, 2010a); iii) LM (and LM-PoS) perplexities trained with monolingual data; iv) PR: Classical lexical-based measures -BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Denkowski and Lavie, 2011)- computed with a pseudo-reference approach, that is, using the other system candidates as references (Soricut and Echihabi, 2010); and v) PROTHER: Reference based metrics provided by Asiya, including GTM, ROUGE, PER, TER (Snover et al., 2008), and syntax-based evaluation measures also with a pseudo-reference approach. We trained a Support Vector Machine ranker by means of pairwise comparison using the SVMlight toolkit (Joachims, 1999), but with the “-z p” parameter, which can provide system rankings for all the members of different groups. The learner algorithm was run according to the follow"
W13-2215,W12-3133,1,0.860966,"Missing"
W13-2215,2012.amta-monomt.1,1,0.796291,"Missing"
W13-2215,2013.mtsummit-papers.9,1,0.774503,"Missing"
W13-2215,padro-etal-2010-freeling,0,0.0202206,"Missing"
W13-2215,W06-1607,0,0.0198766,"s with additional information, such as POS tags or lemmas. In that case, factors other than surface (e.g. POS) are usually less sparse, allowing the construction of factor-specific language models with higher-order n-grams. Such language models can help to obtain syntactically more correct outputs. We used the standard models available in Moses as feature functions: relative frequencies, lexical weights, word and phrase penalties, wbe-msdbidirectional-fe reordering models, and two language models (one for surface and one for POS tags). Phrase scoring was computed using GoodTuring discounting (Foster et al., 2006). As aforementioned, we developed five factored Moses-based independent systems with different 134 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 134–140, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics approaches. We explain them in Section 3. As a final decision, we applied a system selection scheme (Formiga et al., 2013; Specia et al., 2010) to consider the best candidate for each sentence, according to human trained quality estimation (QE) models. We set monotone reordering of the punctuation signs for the decoding using the"
W13-2215,P02-1040,0,0.0862502,"on Statistical Machine Translation, pages 134–140, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics approaches. We explain them in Section 3. As a final decision, we applied a system selection scheme (Formiga et al., 2013; Specia et al., 2010) to consider the best candidate for each sentence, according to human trained quality estimation (QE) models. We set monotone reordering of the punctuation signs for the decoding using the Moses wall feature. We tuned the systems using the Moses MERT (Och, 2003) implementation. Our focus was on minimizing the BLEU score (Papineni et al., 2002) of the development set. Still, for exploratory purposes, we tuned configuration (c) using PRO (Hopkins and May, 2011) to set the initial weights at every iteration of the MERT algorithm. However, it showed no significant differences compared to the original MERT implementation. We trained the baseline system using all the available parallel corpora, except for common-crawl. That is, European Parliament (EPPS) (Koehn, 2005), News Commentary, and United Nations. Regarding the monolingual data, there were more News corpora organized by years for Spanish. The data is available at the Translation"
W13-2215,D08-1090,0,0.0227644,"namely: i) QuestQE: 17 QE features provided by the Quest toolkit6 ; ii) AsiyaQE: 26 QE features provided by the Asiya toolkit for MT evaluation (Gim´enez and M`arquez, 2010a); iii) LM (and LM-PoS) perplexities trained with monolingual data; iv) PR: Classical lexical-based measures -BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Denkowski and Lavie, 2011)- computed with a pseudo-reference approach, that is, using the other system candidates as references (Soricut and Echihabi, 2010); and v) PROTHER: Reference based metrics provided by Asiya, including GTM, ROUGE, PER, TER (Snover et al., 2008), and syntax-based evaluation measures also with a pseudo-reference approach. We trained a Support Vector Machine ranker by means of pairwise comparison using the SVMlight toolkit (Joachims, 1999), but with the “-z p” parameter, which can provide system rankings for all the members of different groups. The learner algorithm was run according to the following parameters: linear kernel, expanding the working set by 9 variables at each iteration, for a maximum of 50,000 iterations and with a cache size of 100 for kernel evaluations. The trade-off parameter was empirically set to 0.001. Table 2 sh"
W13-2215,2011.eamt-1.18,1,0.897081,"Missing"
W13-2215,P10-1063,0,0.0150842,"task5 ), which contains the test sets from other WMT campaigns with human assessments. We used five groups of features, namely: i) QuestQE: 17 QE features provided by the Quest toolkit6 ; ii) AsiyaQE: 26 QE features provided by the Asiya toolkit for MT evaluation (Gim´enez and M`arquez, 2010a); iii) LM (and LM-PoS) perplexities trained with monolingual data; iv) PR: Classical lexical-based measures -BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Denkowski and Lavie, 2011)- computed with a pseudo-reference approach, that is, using the other system candidates as references (Soricut and Echihabi, 2010); and v) PROTHER: Reference based metrics provided by Asiya, including GTM, ROUGE, PER, TER (Snover et al., 2008), and syntax-based evaluation measures also with a pseudo-reference approach. We trained a Support Vector Machine ranker by means of pairwise comparison using the SVMlight toolkit (Joachims, 1999), but with the “-z p” parameter, which can provide system rankings for all the members of different groups. The learner algorithm was run according to the following parameters: linear kernel, expanding the working set by 9 variables at each iteration, for a maximum of 50,000 iterations and"
W13-2215,D11-1125,0,0.0166218,"nal Linguistics approaches. We explain them in Section 3. As a final decision, we applied a system selection scheme (Formiga et al., 2013; Specia et al., 2010) to consider the best candidate for each sentence, according to human trained quality estimation (QE) models. We set monotone reordering of the punctuation signs for the decoding using the Moses wall feature. We tuned the systems using the Moses MERT (Och, 2003) implementation. Our focus was on minimizing the BLEU score (Papineni et al., 2002) of the development set. Still, for exploratory purposes, we tuned configuration (c) using PRO (Hopkins and May, 2011) to set the initial weights at every iteration of the MERT algorithm. However, it showed no significant differences compared to the original MERT implementation. We trained the baseline system using all the available parallel corpora, except for common-crawl. That is, European Parliament (EPPS) (Koehn, 2005), News Commentary, and United Nations. Regarding the monolingual data, there were more News corpora organized by years for Spanish. The data is available at the Translation Task’s website1 . We used all the News corpora to busld the language model (LM). Firstly, a LM was built for every cor"
W13-2215,P07-2045,0,0.10399,"such as morphology generation, training sentence filtering, and domain adaptation through unit derivation. The results show a coherent improvement on TER, METEOR, NIST, and BLEU scores when compared to our baseline system. 1 Introduction 2 The TALP-UPC center (Center for Language and Speech Technologies and Applications at Universitat Polit`ecnica de Catalunya) focused on the English to Spanish translation of the WMT13 shared task. Our primary (contrastive) run is an internal system selection comprised of different training approaches (without CommonCrawl, unless stated): (a) Moses Baseline (Koehn et al., 2007b), (b) Moses Baseline + Morphology Generation (Formiga et al., 2012b), (c) Moses Baseline + News Adaptation (Henr´ıquez Q. et al., 2011), (d) Moses Baseline + News Adaptation + Morphology Generation , and (e) Moses Baseline + News Adaptation + Filtered CommonCrawl Adaptation (Barr´on-Cede˜no et al., 2013). Our secondary run includes is the full training strategy marked as (e) in the previous description. The main differences with respect to our last year’s participation (Formiga et al., 2012a) are: i) the inclusion of the CommonCrawl corpus, using Baseline system: Phrase-Based SMT Our contrib"
W13-2215,2005.mtsummit-papers.11,0,0.0437825,"s for the decoding using the Moses wall feature. We tuned the systems using the Moses MERT (Och, 2003) implementation. Our focus was on minimizing the BLEU score (Papineni et al., 2002) of the development set. Still, for exploratory purposes, we tuned configuration (c) using PRO (Hopkins and May, 2011) to set the initial weights at every iteration of the MERT algorithm. However, it showed no significant differences compared to the original MERT implementation. We trained the baseline system using all the available parallel corpora, except for common-crawl. That is, European Parliament (EPPS) (Koehn, 2005), News Commentary, and United Nations. Regarding the monolingual data, there were more News corpora organized by years for Spanish. The data is available at the Translation Task’s website1 . We used all the News corpora to busld the language model (LM). Firstly, a LM was built for every corpus independently. Afterwards, they were combined to produce de final LM. For internal testing we used the News 2011 and News 2012 data and concatenated the remaining three years of News data as a single parallel corpus for development. We processed the corpora as in our participation to WMT12 (Formiga et al"
W13-2215,P03-1021,0,\N,Missing
W13-2215,2011.eamt-1.20,1,\N,Missing
W13-2801,W13-2816,0,0.0501134,"Missing"
W13-2801,W13-2813,0,0.0217301,"Missing"
W13-2801,2008.eamt-1.6,0,0.0725213,"Missing"
W13-2801,W13-2804,0,0.0365849,"Missing"
W13-2801,W13-2805,0,0.0466293,"Missing"
W13-2801,W13-2814,0,0.0251731,"d in corresponding representations (a RBMT example is LFG (Lexical Functional Grammars) analysis and the corresponding XLE translation architecture). In HyTra 2013 there are three approaches dealing with multilevel information: • Turki Khemakhem et al. (2013) present work about an English-Arabic SMT system that uses morphological decomposition and morpho-syntactic annotation of the target language and incorporates the corresponding information in a statistical feature model. Essentially, the statistical feature language model replaces words by feature arrays. 3.4 Other multilevel approaches • Pal et al. (2013) propose a combination of aligners: GIZA++, Berkeley and rule-based for English-Bengali. • Hsieh et al. (2013) use comparable corpora extracted from Wikipedia to extract parallel fragments for the purpose of extending an English-Bengali training corpus. Semantic approaches The introduction of semantics in statistical MT has been approached to solve word sense disambiguation challenges covering the area of lexical semantics and, more recently, there have been different techniques using semantic roles covering shallow semantics, as well as the use of distributional semantics for improving transl"
W13-2801,W13-2806,0,0.0246185,"Missing"
W13-2801,W13-2807,0,0.0435616,"Missing"
W13-2801,W13-2817,0,0.0294611,"responding XLE translation architecture). In HyTra 2013 there are three approaches dealing with multilevel information: • Turki Khemakhem et al. (2013) present work about an English-Arabic SMT system that uses morphological decomposition and morpho-syntactic annotation of the target language and incorporates the corresponding information in a statistical feature model. Essentially, the statistical feature language model replaces words by feature arrays. 3.4 Other multilevel approaches • Pal et al. (2013) propose a combination of aligners: GIZA++, Berkeley and rule-based for English-Bengali. • Hsieh et al. (2013) use comparable corpora extracted from Wikipedia to extract parallel fragments for the purpose of extending an English-Bengali training corpus. Semantic approaches The introduction of semantics in statistical MT has been approached to solve word sense disambiguation challenges covering the area of lexical semantics and, more recently, there have been different techniques using semantic roles covering shallow semantics, as well as the use of distributional semantics for improving translation unit selection. Approaches treating the incorporation of semantics into MT in HyTra 2013 include the fol"
W13-2801,W13-2815,0,0.0580296,"Missing"
W13-2801,W13-2811,0,0.014586,"nd corresponding POS-based restructuring of the input. Basically, they focus on taking advantage of the fact that Korean has compound words, which - for the purpose of alignment - are split and reordered similarly to Chinese. 3.5 In a number of linguistic theories information from the morphological, syntactic and semantic level is considered conjointly and merged in corresponding representations (a RBMT example is LFG (Lexical Functional Grammars) analysis and the corresponding XLE translation architecture). In HyTra 2013 there are three approaches dealing with multilevel information: • Turki Khemakhem et al. (2013) present work about an English-Arabic SMT system that uses morphological decomposition and morpho-syntactic annotation of the target language and incorporates the corresponding information in a statistical feature model. Essentially, the statistical feature language model replaces words by feature arrays. 3.4 Other multilevel approaches • Pal et al. (2013) propose a combination of aligners: GIZA++, Berkeley and rule-based for English-Bengali. • Hsieh et al. (2013) use comparable corpora extracted from Wikipedia to extract parallel fragments for the purpose of extending an English-Bengali train"
W13-2801,W13-2810,0,0.0239078,"Missing"
W13-2801,W13-2818,0,0.0263833,"extracted from Wikipedia to extract parallel fragments for the purpose of extending an English-Bengali training corpus. Semantic approaches The introduction of semantics in statistical MT has been approached to solve word sense disambiguation challenges covering the area of lexical semantics and, more recently, there have been different techniques using semantic roles covering shallow semantics, as well as the use of distributional semantics for improving translation unit selection. Approaches treating the incorporation of semantics into MT in HyTra 2013 include the following research work: • Tambouratzis et al. (2013) describe a hybrid MT architecture that uses very few bilingual corpus and a large monolingual one. The linguistic information is extracted using pattern recognition techniques. Table 1 summarizes the papers that have been presented in the Second HyTra Workshop. The papers are arranged into the table according to the linguistic level they address. • Rudnick et al. (2013) present a combination of Maximum Entropy Markov Models and HMM to perform lexical selection in the sense of cross-lingual word sense disambiguation (i.e. by choice from the set of translation alternatives). The system is meant"
W13-2801,W13-2808,0,0.058822,"Missing"
W13-2801,W10-1737,0,0.0651783,"Missing"
W13-2801,W13-2803,0,0.0495517,"Missing"
W13-2801,W13-2809,0,0.0597288,"Missing"
W13-2801,W13-2812,0,0.0196328,"often considered and represented simultaneously (not only in unification-based approaches) and the same is true for MT systems. Syntax had been addressed originally in SMT in the form of so called phrase-based SMT without any reference to linguistic structures; during 3 • Bouillon et al. (2013) presents two methodologies to correct homophone confusions. The first one is based on hand-coded rules and the second one is based on weighted graphs derived from a pronunciation resource. • Laki et al. (2013) combine pre-reordering rules with morphological and factored models for English-to-Turkish. • Li et al. (2013) propose pre-reordering rules to be used for alignment-based reordering, and corresponding POS-based restructuring of the input. Basically, they focus on taking advantage of the fact that Korean has compound words, which - for the purpose of alignment - are split and reordered similarly to Chinese. 3.5 In a number of linguistic theories information from the morphological, syntactic and semantic level is considered conjointly and merged in corresponding representations (a RBMT example is LFG (Lexical Functional Grammars) analysis and the corresponding XLE translation architecture). In HyTra 201"
W14-1015,2011.mtsummit-papers.63,0,0.0610081,"Missing"
W14-1015,2005.eamt-1.12,0,0.0604487,"Missing"
W14-1015,J03-1002,0,0.00545815,"ectives (see the list of available languages in wiki.apertium.org. In practice, we use the architecture of the system, but, differently, we are using statistical techniques to complete our system. Figure 1 shows the representative block diagram modules of the RBMT system. In this first description of the system, the only step that is not addressed is the lexical transfer. Development to date has consisted of: feeding 1 http://sourceforge.net/projects/ http://www.yellowbridge.com/chinese/chinese-parts-ofspeech.php 3 http://www.chinese-tools.com/ 2 83 level of word by using the standard GIZA++ (Och and Ney, 2003) software. Alignment was performed from source to target and target to source. Symmetrization was done using intersection because it provides the most reliable links. Then, we extracted phrases of length one, which means that we extracted translations from word-to-word. This dictionary was manually filtered to eliminate incorrect entries. This procedure allowed to add around 3,500 words in the dictionaries. Our dictionary has around 9,000 words. 2.3 ally a test of vocabulary. At the most basic level, it just expands the monolingual dictionary, and runs each possibly analyzed lexical form throu"
W14-1015,cortes-etal-2012-free,0,0.0643324,"Missing"
W14-1015,2009.mtsummit-posters.15,0,0.167225,"prefixes, prepositions, pronouns, question words, suffixes, time words and different types of verbs. For each category, each word has its corresponding translation into English. Then, this dictionary was used to feed the dictionary. But to double-check the translations provided, each word was translated using another on-line dictionary3 and Google Translate. This procedure allowed to add several hundreds of numerals, conjunctions, adverbs, pronouns, determinants, adjectives, 3,000 nouns and 2,000 verbs. The second procedure is statistical-based. The parallel corpus of the United Nations (UN) (Rafalovitch and Dale, 2009) was aligned at the System architecture The system is based on the Apertium platform (Forcada et al., 2011) which is a free/open-source toolbox for shallow transfer machine translation. As well as the platform, the linguistic data for the MT systems are also available under the terms of the GNU GPL. The platform was originally designed for the Romance languages of Spain, but it is moving away from those initial objectives (see the list of available languages in wiki.apertium.org. In practice, we use the architecture of the system, but, differently, we are using statistical techniques to comple"
W14-3306,W11-2156,1,0.605388,"Missing"
W14-3306,P07-2045,0,0.00412561,"educed dimension vector-space model, which is constructed either by means of standard latent semantic analysis or using deep representation as decribed in section 3. Introduction This paper describes the joint participation of the Instituto Polit´ecnico Nacional (IPN) and the Universitat Polit`ecnica de Valencia (UPV) in cooperation with Institute for Infocomm Research (I2R) on the 9th Workshop on Statistical Machine Translation (WMT 2014). In particular, our participation was in the English-to-Hindi translation task. Our baseline system is an standard phrasebased SMT system built with Moses (Koehn et al., 2007). Starting from this system we propose to introduce a source-context feature function inspired by previous works (R. Costa-juss`a and Banchs, 2011; Banchs and Costa-juss`a, 2011). The main novelty of this work is that the source-context feature is computed in a new deep representation. The rest of the paper is organized as follows. Section 2 presents the motivation of this semantic feature and the description of how the source context feature function is added to Moses. Section 3 explains how both the latent semantic indexing and deep representation of sentences are used to better compute simi"
W14-3306,W11-1014,1,\N,Missing
W15-4109,P08-1087,0,0.0151672,"ween Chinese and Spanish is bigger in the direction from Chinese to Spanish, given that the same Chinese word can generate multiple Spanish words. For example, the Chinese word f`an (in transcribed Pinyin) can be translated by comer, como, com´ı, comer´e1 which correspond to several tense flexions of the same verb and also by comes, comiste, comer´as2 , 1 2 2 Related Work There are numerous studies which deal with morphology in the field of SMT. Without aiming at completeness, we cite works that: • Preprocess the data to make the structure of both languages more similar by means of enriching (Avramidis and Koehn, 2008; Ueffing and Ney, 2003) or segmentation techniques in agglutinative (S.Virpioja et al., 2007) or fusional languages (Costa-juss`a, 2015a) • Modify models (Koehn and Hoang, 2007) • Post-process the data (Toutanova et al., 2008; Bojar and Tamchyna, 2011; Formiga et al., 2013). to eat, I eat, I ate, I will eat you eat, you ate, you will eat 56 Proceedings of the ACL 2015 Fourth Workshop on Hybrid Approaches to Translation (HyTra), pages 56–60, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics The research work in this area is being very active, e.g. PhD proposals us"
W15-4109,W13-2215,1,0.89823,"Missing"
W15-4109,N15-2021,0,0.0242384,"n techniques in agglutinative (S.Virpioja et al., 2007) or fusional languages (Costa-juss`a, 2015a) • Modify models (Koehn and Hoang, 2007) • Post-process the data (Toutanova et al., 2008; Bojar and Tamchyna, 2011; Formiga et al., 2013). to eat, I eat, I ate, I will eat you eat, you ate, you will eat 56 Proceedings of the ACL 2015 Fourth Workshop on Hybrid Approaches to Translation (HyTra), pages 56–60, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics The research work in this area is being very active, e.g. PhD proposals using strategies based on deep learning (Gutierrez-Vasques, 2015). Previous works on the Chinese-Spanish language pair focus on compiling corpus and using pivot stategies (Costa-juss`a et al., 2012) and on building a Rule-Based Machine Translation (RBMT) system (Costa-juss`a and Centelles, In Press 2015). A high-level description of the stateof-the-art of the translation on this language pair is detailed in (Costa-juss`a, 2015b). Our work mixes several strategies but basically it goes in the direction of (Formiga et al., 2013) that focuses on solving the challenge of morphology as a post-processing classification problem. The idea is to translate from Chine"
W15-4109,2009.mtsummit-posters.15,0,0.00981573,"f lemmas and tags. When generalizing number, note that instead of using the information of singular (S) or plural (P) in the POS tag with the respective S or P, we use the generic N. Therefore, we generalize the information of number. Similarly when generalizing gender or both (numgen). Ongoing Experiments In this section we show experiments and results with the four strategies proposed in the previous section. As discussed in the literature, there are not many parallel corpora available for ChineseSpanish (Costa-juss`a et al., 2012). In this work, we use the data set from the United Nations (Rafalovitch and Dale, 2009). The training corpus contains about 60,000 sentences (and around 2 million words) and the development and test corpus contain 1,000 sentences each one. The base58 (ERDF/FEDER) and the Seventh Framework Program of the European Commission through the International Outgoing Fellowship Marie Curie Action (IMTraP-2011-29951). Oracles get closer to the lemmas simplification when only simplifying both number and gender in Spanish. This finding is relevant in the sense that it simplifies the classification task in the further work that we are considering. System Baseline Zh2Eslemmas Zh2EsN lemmas Zh2"
W15-4109,2007.mtsummit-papers.65,0,0.0248646,"Chinese word can generate multiple Spanish words. For example, the Chinese word f`an (in transcribed Pinyin) can be translated by comer, como, com´ı, comer´e1 which correspond to several tense flexions of the same verb and also by comes, comiste, comer´as2 , 1 2 2 Related Work There are numerous studies which deal with morphology in the field of SMT. Without aiming at completeness, we cite works that: • Preprocess the data to make the structure of both languages more similar by means of enriching (Avramidis and Koehn, 2008; Ueffing and Ney, 2003) or segmentation techniques in agglutinative (S.Virpioja et al., 2007) or fusional languages (Costa-juss`a, 2015a) • Modify models (Koehn and Hoang, 2007) • Post-process the data (Toutanova et al., 2008; Bojar and Tamchyna, 2011; Formiga et al., 2013). to eat, I eat, I ate, I will eat you eat, you ate, you will eat 56 Proceedings of the ACL 2015 Fourth Workshop on Hybrid Approaches to Translation (HyTra), pages 56–60, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics The research work in this area is being very active, e.g. PhD proposals using strategies based on deep learning (Gutierrez-Vasques, 2015). Previous works on the Chinese"
W15-4109,N03-1017,0,0.0213616,"Translation with Morphology Strategies Marta R. Costa-juss`a1 Centro de Investigaci´on en Computaci´on, Instituto Polit´ecnico Nacional, Mexico 1 marta@nlp.cic.ipn.mx Abstract all of which also correspond to several person flexions of the same verb. This poses a challenge in Statistical Machine Translation (SMT) because translations are learnt by co-ocurrence of words in both languages. When a word has multiple translations, it generates sparsity in the translation model. In this study, we experiment with different strategies to add morphology knowledge in a standard phrase-based SMT system (Koehn et al., 2003) for the Chinese-to-Spanish translation direction. However, the presented techniques could be used for other pairs involving isolating and fusional languages. The rest of the paper is organized as follows. Section 2 reports a brief overview of the related work both in using morphology knowledge in SMT and in translating from Chinese-to-Spanish. Section 3 explains the theoretical framework of phrase-based SMT at a high level and the details of each strategy to introduce morphology in the mentioned system. Section 4 describes the experiments and first results obtained for each theoretical strate"
W15-4109,P07-2045,0,0.0168488,"] el[DA0GS0] derecho[NCGS000] humano[AQ0GS0] “[Fp] .[Fp] decidir[VMIP3N0] examinar[VMN0000] el[DA0GN0] cuesti´on[NCGN000] en[SPS00] el[DA0GN0] per´ıodo[NCGN000] de[SPS00] sesi´on[NCGN000] el[DA0GN0] tema[NCGN000] titular[AQ0GN0] “[Fp] cuesti´on[NCGN000] relativo[AQ0GN00] a[SPS00] el[DA0GN0] derecho[NCGN000] humano[AQ0GN0] “[Fp] .[Fp] Decide examinar la cuesti´on en el per´ıodo de sesiones el tema titulado “ Cuestiones relativas a los derechos humanos ” . Table 1: Example of Spanish simplification into lemmas and different variations line system is standard phrase-based SMT trained with Moses (Koehn et al., 2007), with the default parameters. Table 2 shows results for the strategies 1, 2 and 3 in terms of BLEU (Papineni et al., 2002). From the BLEU scores, we see that strategy 1 gives slight improvements, but strategies 2 and 3 do not. Strategy 1 2 Figure 3: Illustration of the classification-based strategy. 3 BLEU 32.29 32.54 31.80 36.40 71.79 32.11 Table 2: BLEU scores for Zh2Es translation task and different morphology strategies. In this paper, we study the first challenge of exploring different simplifications. However, we do not face the classification challenge, which is left to further work. I"
W15-4109,P08-1059,0,0.0299218,"mer, como, com´ı, comer´e1 which correspond to several tense flexions of the same verb and also by comes, comiste, comer´as2 , 1 2 2 Related Work There are numerous studies which deal with morphology in the field of SMT. Without aiming at completeness, we cite works that: • Preprocess the data to make the structure of both languages more similar by means of enriching (Avramidis and Koehn, 2008; Ueffing and Ney, 2003) or segmentation techniques in agglutinative (S.Virpioja et al., 2007) or fusional languages (Costa-juss`a, 2015a) • Modify models (Koehn and Hoang, 2007) • Post-process the data (Toutanova et al., 2008; Bojar and Tamchyna, 2011; Formiga et al., 2013). to eat, I eat, I ate, I will eat you eat, you ate, you will eat 56 Proceedings of the ACL 2015 Fourth Workshop on Hybrid Approaches to Translation (HyTra), pages 56–60, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics The research work in this area is being very active, e.g. PhD proposals using strategies based on deep learning (Gutierrez-Vasques, 2015). Previous works on the Chinese-Spanish language pair focus on compiling corpus and using pivot stategies (Costa-juss`a et al., 2012) and on building a Rule-Based"
W15-4109,P02-1040,0,0.0915745,"] en[SPS00] el[DA0GN0] per´ıodo[NCGN000] de[SPS00] sesi´on[NCGN000] el[DA0GN0] tema[NCGN000] titular[AQ0GN0] “[Fp] cuesti´on[NCGN000] relativo[AQ0GN00] a[SPS00] el[DA0GN0] derecho[NCGN000] humano[AQ0GN0] “[Fp] .[Fp] Decide examinar la cuesti´on en el per´ıodo de sesiones el tema titulado “ Cuestiones relativas a los derechos humanos ” . Table 1: Example of Spanish simplification into lemmas and different variations line system is standard phrase-based SMT trained with Moses (Koehn et al., 2007), with the default parameters. Table 2 shows results for the strategies 1, 2 and 3 in terms of BLEU (Papineni et al., 2002). From the BLEU scores, we see that strategy 1 gives slight improvements, but strategies 2 and 3 do not. Strategy 1 2 Figure 3: Illustration of the classification-based strategy. 3 BLEU 32.29 32.54 31.80 36.40 71.79 32.11 Table 2: BLEU scores for Zh2Es translation task and different morphology strategies. In this paper, we study the first challenge of exploring different simplifications. However, we do not face the classification challenge, which is left to further work. It would be interesting to use deep learning knowledge which is leading to large improvements in natural language processing"
W15-4109,D07-1091,0,\N,Missing
W16-2336,D15-1041,0,0.0220783,"ings We look at this task as a bilinear prediction task as proposed by (Madhyastha et al., 2014). The proposed model makes use of word embeddings of both languages with no additional features. The basic function is formulated —the probability of a target word given a source word— as log-linear model and takes the following form: exp{φs˜(s)&gt; W φt˜(t)} Pr(t|s; W ) = P &gt; 0 t0 exp{φs˜(s) W φt˜(t )} Segments (1) As a solution to those drawbacks, new alternative character-based word embeddings have been recently proposed for tasks as language modeling (Kim et al., 2016; Ling et al., 2015), parsing (Ballesteros et al., 2015) or part-of-speech tagging (Ling et al., 2015; Santos and Zadrozny, 2014). For our system we selected the best characterbased embedding architecture proposed by Kim et al. (Kim et al., 2016). The computation of the representation of each word starts with a characterbased embedding layer that associates each word (sequence of characters) with a sequence of vectors. This sequence of vectors is then processed with a set of 1D convolution filters of different lengths (from 1 to 7 characters) followed with a max pooling layer and two additional highway layers. The output of the second highway layer"
W16-2336,P03-1021,0,0.0143318,"these systems BTT (big translation table). For the in-domain system, a 5-gram language model is estimated on the target side of the corpus using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002) (SLM, small language model). For the extended systems, we use all the monolingual corpora available and the target side of the large parallel corpus (BLM, big language model). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007). The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric on devBio. Data Our main corpus is the compilation of the corpora assigned for the shared task, which was built using scientific publications gathered from the Scielo database. We focus on the Spanish–English language pair, for which the size of the corpora is summarised in Table 1. We further increase the vocabulary of the system by using standard parallel corpora for the Spanish–English language pair (i.e., UN corpora, Europarl corpora, News corpus, etc.2 ). This corpus appears as Quest in Table 1. For the monolingual corpus we use"
W16-2336,N03-1017,0,0.0306671,"system. http://www.statmt.org/wmt16 463 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 463–468, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 3 The Translation System Table 1: Size of the parallel (top) and monolingual (bottom) corpora used to train the translation systems The TALP-UPC translation system is built on three different components. We describe their theoretical basis in the following subsections. 3.1 Corpus Biomedical Quest Phrase-based SMT The standard phrase-based machine translation system (Koehn et al., 2003) focuses on finding the most probable target sentence given the source sentence. The phrase-based system has evolved from the noisy-channel to the log-linear model which combines a set of feature functions in the decoder, including the translation and language model, the reordering model and the lexical models. Although the phrase-based system is a commoditized technology used at the academic and commercial level, there are still many challenges to solve, such as OOVs. 3.2 Bio-mono/en Bio-mono/es Wikipedia/en Wikipedia/es 3.3 Words Vocab 6 6 0.3 · 106 0.5 · 106 1 · 10 13 · 106 0.1 · 106 0.01 ·"
W16-2336,padro-stanilovsky-2012-freeling,0,0.0258286,"Missing"
W16-2336,P02-1040,0,0.0955039,"ation table). For the in-domain system, a 5-gram language model is estimated on the target side of the corpus using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002) (SLM, small language model). For the extended systems, we use all the monolingual corpora available and the target side of the large parallel corpus (BLM, big language model). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007). The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric on devBio. Data Our main corpus is the compilation of the corpora assigned for the shared task, which was built using scientific publications gathered from the Scielo database. We focus on the Spanish–English language pair, for which the size of the corpora is summarised in Table 1. We further increase the vocabulary of the system by using standard parallel corpora for the Spanish–English language pair (i.e., UN corpora, Europarl corpora, News corpus, etc.2 ). This corpus appears as Quest in Table 1. For the monolingual corpus we use an English and Spanish Wikipedia dump3 ."
W16-2336,P07-2045,0,0.0331048,"small translation table). For more general systems, we also use the Quest data; we name these systems BTT (big translation table). For the in-domain system, a 5-gram language model is estimated on the target side of the corpus using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002) (SLM, small language model). For the extended systems, we use all the monolingual corpora available and the target side of the large parallel corpus (BLM, big language model). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007). The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric on devBio. Data Our main corpus is the compilation of the corpora assigned for the shared task, which was built using scientific publications gathered from the Scielo database. We focus on the Spanish–English language pair, for which the size of the corpora is summarised in Table 1. We further increase the vocabulary of the system by using standard parallel corpora for the Spanish–English language pair (i.e., UN corpora, Europarl corpora, News corpus, etc.2"
W16-2336,D15-1176,0,0.0263254,"n using Bilingual Word-Embeddings We look at this task as a bilinear prediction task as proposed by (Madhyastha et al., 2014). The proposed model makes use of word embeddings of both languages with no additional features. The basic function is formulated —the probability of a target word given a source word— as log-linear model and takes the following form: exp{φs˜(s)&gt; W φt˜(t)} Pr(t|s; W ) = P &gt; 0 t0 exp{φs˜(s) W φt˜(t )} Segments (1) As a solution to those drawbacks, new alternative character-based word embeddings have been recently proposed for tasks as language modeling (Kim et al., 2016; Ling et al., 2015), parsing (Ballesteros et al., 2015) or part-of-speech tagging (Ling et al., 2015; Santos and Zadrozny, 2014). For our system we selected the best characterbased embedding architecture proposed by Kim et al. (Kim et al., 2016). The computation of the representation of each word starts with a characterbased embedding layer that associates each word (sequence of characters) with a sequence of vectors. This sequence of vectors is then processed with a set of 1D convolution filters of different lengths (from 1 to 7 characters) followed with a max pooling layer and two additional highway layers. Th"
W16-2336,2006.iwslt-papers.2,1,0.820112,"Missing"
W16-2336,K15-1031,0,0.0532749,"Missing"
W16-2336,C14-1017,1,0.845249,"rms of perplexity (Mikolov et al., 2010). They are also a good re-ranking option in tasks such as speech recognition and machine translation. However, the standard lookup-based word embeddings are limited to a finite-size vocabulary for both computational and sparsity reasons. Moreover, the orthographic representation of the words is completely ignored. The standard learning process is blind to the presence of stems, prefixes, suffixes and any other kind of affixes in words. Vocabulary Expansion using Bilingual Word-Embeddings We look at this task as a bilinear prediction task as proposed by (Madhyastha et al., 2014). The proposed model makes use of word embeddings of both languages with no additional features. The basic function is formulated —the probability of a target word given a source word— as log-linear model and takes the following form: exp{φs˜(s)&gt; W φt˜(t)} Pr(t|s; W ) = P &gt; 0 t0 exp{φs˜(s) W φt˜(t )} Segments (1) As a solution to those drawbacks, new alternative character-based word embeddings have been recently proposed for tasks as language modeling (Kim et al., 2016; Ling et al., 2015), parsing (Ballesteros et al., 2015) or part-of-speech tagging (Ling et al., 2015; Santos and Zadrozny, 201"
W16-2336,D13-1140,0,0.0225269,"er-based neural language model. Section 2 presents some related work to our approach. Next, Section 3 introduces the theoretical aspects of the system components and Section 4 the experiments. Finally, we justify our choice for the final submission and draw the conclusions in Section 5. 1 Related Work On the other hand, there have been several language models used for rescoring in SMT. For example, neural feed-forward language models (Schwenk et al., 2006) have been used to rescore both n-gram-based and phrase-based systems. Mikolov (2012) re-ranks n-best lists with recurrent neural networks. Vaswani et al. (2013) combine feed-forward language models, with rectified linear units and noise-contrastive estimation. Luong et al. (2015) propose to use deeper neural models which improve re-ranking. In this paper, we are using Kim et al. (2016) a characterbased language model to re-rank the output of the phrase-based system. http://www.statmt.org/wmt16 463 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 463–468, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 3 The Translation System Table 1: Size of the parallel (top) and m"
W16-2336,P15-2118,0,0.065368,"Missing"
W16-2336,N15-1176,0,0.031756,"Missing"
W16-2336,J03-1002,0,0.00913989,"in-domain system, we use only the biomedical data made available for the task (STT systems, small translation table). For more general systems, we also use the Quest data; we name these systems BTT (big translation table). For the in-domain system, a 5-gram language model is estimated on the target side of the corpus using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002) (SLM, small language model). For the extended systems, we use all the monolingual corpora available and the target side of the large parallel corpus (BLM, big language model). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007). The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric on devBio. Data Our main corpus is the compilation of the corpora assigned for the shared task, which was built using scientific publications gathered from the Scielo database. We focus on the Spanish–English language pair, for which the size of the corpora is summarised in Table 1. We further increase the vocabulary of the system by using standard parallel corpora"
W16-2362,N03-1017,0,0.0595168,"t of the WMT 2016 Multimodal Translation Task. The obtained results show that the double-embedding approach performs significantly better than the traditional single-embedding one. 1 2 Related work Image captioning has gained interest in the community and deep learning has been applied in this area. The two most common caption-related problems are caption generation (Vinyals et al., 2014) and caption translation (Elliott et al., 2015). Similarly, machine translation approaches based on neural networks (Sutskever et al., 2014; Cho et al., 2014) are competing with standard phrase-based systems (Koehn et al., 2003). Neural machine translation uses an encoder-decoder structure (Cho et al., 2014). The implementation of an attention-based mechanism (Bahdanau et al., 2015) has allowed to achieve state-of-the-art results. The community is actively investigating in this approach and there have been enhancements related to addressing unknown words (Luong et al., 2015), integrating language modeling (G¨ulc¸ehre et al., 2015), using character information in addition to words (Costa-juss`a and Fonollosa, 2016) or even combining different languages (Firat et al., 2016), among others. Introduction Sequence-to-seque"
W16-2362,P07-2045,0,0.0130251,"evaluation official results as 1 GroundedTranslation C. We see that using BiRNNs improve vs RNNs, and double-embeddings improves over singleembeddings. Finally, adding the image information does not improve results. Therefore, the best architecture (C) is the one that participated in WMT 2016 Multimodal Translation Task. Official results ranked our system in the 14th position out of 16. We priorised participating with a pure multimodal extensible architecture. However, we know it would have improved our ranking just performing a simple technique as rescoring our system with a standard Moses (Koehn et al., 2007). System Baseline Architecture (A) Architecture (B) Architecture (C) Architecture (D) BLEU 9.41 19.16 20.89 22.74 17.74 a man sleeping in a green room on a couch ein mann schlaft in einem gr¨unen gr¨unen auf einem sofa ein mann schlaft in einem gr¨unen raum auf einem sofa 5 Conclusions Our system is not competitive compared to standard phrase-based system (Koehn et al., 2003) or the auto-encoder neural machine translation system (Bahdanau et al., 2015) as shown by our ranking in the official evaluation (14 position out of 16). However, the architecture of our system makes it feasible to introd"
W16-2362,P15-1002,0,0.0143523,"tion generation (Vinyals et al., 2014) and caption translation (Elliott et al., 2015). Similarly, machine translation approaches based on neural networks (Sutskever et al., 2014; Cho et al., 2014) are competing with standard phrase-based systems (Koehn et al., 2003). Neural machine translation uses an encoder-decoder structure (Cho et al., 2014). The implementation of an attention-based mechanism (Bahdanau et al., 2015) has allowed to achieve state-of-the-art results. The community is actively investigating in this approach and there have been enhancements related to addressing unknown words (Luong et al., 2015), integrating language modeling (G¨ulc¸ehre et al., 2015), using character information in addition to words (Costa-juss`a and Fonollosa, 2016) or even combining different languages (Firat et al., 2016), among others. Introduction Sequence-to-sequence learning is a new common approach to translation problems (Sutskever et al., 2014). The basic idea consists in mapping the input sentence into a vector of fixed dimensionality with a Recurrent Neural Network (RNN) and, then, do the reverse step to map the vector to the target sequence. From this new perspective, multimodal translation (Elliott et"
W16-2362,P02-1040,0,0.0954358,"T H DROP L2 Description Maximum sequence length Source vocabulary words Target vocabulary words Embedding size Dropout rate L2 regularizer Value 45 10364 8012 512 0.25 10−8 Table 1: Model parameters value Dropout rate of 0.25 is applied to all nonrecurrent units and a L2 regularization is applied to all weights and units. Training is performed on batches of size 10000 and on mini-batches of size 128. The target metric is the categorical cross entropy and the used optimiser is Adam (Kingma and Ba, 2014). Results are validated at each epoch on the dataset validation split using the BLEU metric (Papineni et al., 2002), along with model perplexity. BLEU scores during validation are also used as an early stop criteria in case the maximum score so-far is not surpassed on the following 10 epochs. In order to evaluate our system performance obtained results are compared against a single-embedding system trained under the same conditions and parameters. Their BLEU score monitorization can be observed in Figure 3 and the chosen parameter set is summarised in Table 1. Figure 2: Diagram of NMT architecture (D) using image and text. 4 4.1 Experiments and results Data The system is developed, trained and tested with"
W16-2362,P16-2058,1,0.872857,"Missing"
W16-2362,W16-3210,0,0.065827,"Missing"
W16-2362,N16-1101,0,0.0310392,"competing with standard phrase-based systems (Koehn et al., 2003). Neural machine translation uses an encoder-decoder structure (Cho et al., 2014). The implementation of an attention-based mechanism (Bahdanau et al., 2015) has allowed to achieve state-of-the-art results. The community is actively investigating in this approach and there have been enhancements related to addressing unknown words (Luong et al., 2015), integrating language modeling (G¨ulc¸ehre et al., 2015), using character information in addition to words (Costa-juss`a and Fonollosa, 2016) or even combining different languages (Firat et al., 2016), among others. Introduction Sequence-to-sequence learning is a new common approach to translation problems (Sutskever et al., 2014). The basic idea consists in mapping the input sentence into a vector of fixed dimensionality with a Recurrent Neural Network (RNN) and, then, do the reverse step to map the vector to the target sequence. From this new perspective, multimodal translation (Elliott et al., 2015) has become a feasible task. In particular, we are referring to the WMT 2016 multimodal task that consists in translating English sentences into German, given the English sentence itself and"
W16-2713,P16-2058,1,0.840578,"Missing"
W16-2713,davis-2012-tajik,0,0.0135499,"ask for many natural language processing applications such as cross-language information retrieval, information extraction or even machine translation. NEWS workshop has provided for various editions the opportunity to share strategies of transliteration and compare results among different sites. NEWS workshop this year offers training, development and test corpus for 14 language pairs. The final goal of this paper is to offer a baseline system for the NEWS 2016 workshop. Since a general strategy for transliteration has been to use techniques of machine translation, e.g. (Rama and Gali, 2009; David, 2012), we have chosen to use the phrase-based system (Koehn et al., 2003). The phrase-based machine translation system tries to find the most probable target sentence given the source sentence. The theory behind phrase-based system has evolved from the noisy channel to the log-linear model, which is the one used nowadays. This model combines several feature functions including the translation and language model, the reordering model and the lexical models. Next experimental section describes the preprocessing of the data and the final corpus statistics for the 14 tasks in the evaluation. We report"
W16-2713,N03-1017,0,0.0077293,"cross-language information retrieval, information extraction or even machine translation. NEWS workshop has provided for various editions the opportunity to share strategies of transliteration and compare results among different sites. NEWS workshop this year offers training, development and test corpus for 14 language pairs. The final goal of this paper is to offer a baseline system for the NEWS 2016 workshop. Since a general strategy for transliteration has been to use techniques of machine translation, e.g. (Rama and Gali, 2009; David, 2012), we have chosen to use the phrase-based system (Koehn et al., 2003). The phrase-based machine translation system tries to find the most probable target sentence given the source sentence. The theory behind phrase-based system has evolved from the noisy channel to the log-linear model, which is the one used nowadays. This model combines several feature functions including the translation and language model, the reordering model and the lexical models. Next experimental section describes the preprocessing of the data and the final corpus statistics for the 14 tasks in the evaluation. We report the parameters used to train the phrase-based system. And finally, w"
W16-2713,P07-2045,0,0.00752745,"3. In most tasks, results were in the middle of the ranking. Best ranking results were obtained in Englishto-Japanese (Kanji) and Arabic-to-English (no merit this one, because the baseline was the only participant). Worst ranking results were for English–Thai, English-to-Tamil, English-toHebrew, English-to-Korean, English-to-Japanese (Katakana). Table 2 details the corpus statistics for all 14 tasks including training, development and test sets. Preprocessing has been limited to separate characters by a blank space. 2.2 Results System Description The phrase-based system was built using Moses (Koehn et al., 2007), version 15th April 2016 from github, with standard parameters, including: grow-final-diag for alignment; Good-Turing smoothing of the relative frequencies; 3-gram language modeling using Kneser-Ney discounting and training with SRILM (Stolcke, 2002); and lexicalized reordering, which includes 6 feature functions. Optimization was done using the MERT algorithm and MBR option for decoding. It is important to note that the same system was used for the 14 tasks without any change or modification. 3 Conclusions This phrase-based system based on standard Moses has been offered to the NEWS organize"
W16-2713,W09-3528,0,0.0169245,"ntities is a useful task for many natural language processing applications such as cross-language information retrieval, information extraction or even machine translation. NEWS workshop has provided for various editions the opportunity to share strategies of transliteration and compare results among different sites. NEWS workshop this year offers training, development and test corpus for 14 language pairs. The final goal of this paper is to offer a baseline system for the NEWS 2016 workshop. Since a general strategy for transliteration has been to use techniques of machine translation, e.g. (Rama and Gali, 2009; David, 2012), we have chosen to use the phrase-based system (Koehn et al., 2003). The phrase-based machine translation system tries to find the most probable target sentence given the source sentence. The theory behind phrase-based system has evolved from the noisy channel to the log-linear model, which is the one used nowadays. This model combines several feature functions including the translation and language model, the reordering model and the lexical models. Next experimental section describes the preprocessing of the data and the final corpus statistics for the 14 tasks in the evaluati"
W17-1207,2001.mtsummit-papers.14,0,0.0278214,"Missing"
W17-1207,J07-2003,0,0.0268782,"r Universitat Polit`ecnica de Catalunya, 08034 Barcelona marta.ruiz@upc.edu Abstract Balkan and R. Lee Humphreys and Siety Meijer and Louisa Sadler, 1994). Years later, corpusbased approaches have reached both the interest in the scientific and industrial community (Hutchins, 1986). Recently, neural MT approach has been proposed. This corpus-based approach uses deep learning techniques (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) and it may be taking over previous popular corpusbased approaches such as statistical phrase or hierarchical-based (Koehn et al., 2003; Chiang, 2007). As a result, large companies, such as Google, have been using rule-based MT, then statistical MT and just very recently, they are replacing some of their statistical MT engines by neural MT engines (Wu et al., 2016). This paper analizes how standard neural MT techniques, which are briefly described in section 4.3, perform on the Catalan-Spanish task compared to popular rule-based and phrase-based MT. Additionally, we perform a naive system combination using the standard Minimum Bayes Risk (MBR) technique (Ehling et al., 2007) which reports slight improvements, in terms of standard automatic"
W17-1207,D14-1179,0,0.0371957,"Missing"
W17-1207,P16-2058,1,0.899906,"Missing"
W17-1207,N13-1073,0,0.0123771,"ram language model with Kneser-Ney smoothing, word bonus and MERT (Minimum Error Rate Training) optimisation. Neural-based The neural MT system was built using the open-source software available in github3 . This code implements the auto-encoder with attention that we presented in section 4.3. We use the parameters defined in Table 2. Regarding vocabulary limitation, we use a vocabulary size of 90,000 both in Spanish and in Catalan. We replace out-of-vocabulary words (UNKs) using the standard methodology (Jean et al., 2015): we use the word-to-word translation model learned with ’fast-align’ (Dyer et al., 2013) or, if not available, the aligned source word is used. We use an embedding of 512 and a dimension of 1024, a batch size of 32, and no dropout, learning-rate of 0.001 and adadelta optimization. Experimental Framework This section reports details on the data used for training, optimizing and testing as well as a description of the parameters for each system in the comparison. 5.1 Data We use a large corpus extracted from ten years of the paper edition of a bilingual Catalan newspaper, El Peri´odico (Costa-juss`a et al., 2014). The Spanish-Catalan corpus is partially available via ELDA (Evaluati"
W17-1207,P07-2026,0,0.0338597,"hes such as statistical phrase or hierarchical-based (Koehn et al., 2003; Chiang, 2007). As a result, large companies, such as Google, have been using rule-based MT, then statistical MT and just very recently, they are replacing some of their statistical MT engines by neural MT engines (Wu et al., 2016). This paper analizes how standard neural MT techniques, which are briefly described in section 4.3, perform on the Catalan-Spanish task compared to popular rule-based and phrase-based MT. Additionally, we perform a naive system combination using the standard Minimum Bayes Risk (MBR) technique (Ehling et al., 2007) which reports slight improvements, in terms of standard automatic measures, in in-domain test set but large improvements in out-of-domain test set. Catalan and Spanish are closely-related languages, which make them particularly interesting for MT and translation performance is quite high for rule-based and statistical-based systems. Given these similarities, we want to test how neural MT behaves on such related language pairs. This leads us to the main question that this paper tries to solve: Is neural MT competitive with current wellperforming rule-based and phrase-based MT systems? The answ"
W17-1207,2005.eamt-1.4,0,0.0442865,"mbia and Spain. See Figure 1. Catalan-Spanish bilingualism only occurs in the regions of Spain and in Andorra. The tendency is that all Catalan native speakers, in practice, also speak Spanish. However, it is not the same for Spanish native speakers. This leads us to a first example of use case for an MT system for this language pair: Spanish (native) speakers that do not understand Catalan. Other use cases include professional translations or web page translations. Related work Previous related publications on the CatalanSpanish language pair are in rule-based MT (Canals-Marote et al., 2001; Alonso, 2005) and statitical MT (Poch et al., 2009; Costa-juss`a et al., 2012). It is worth noting that given the similarity among Catalan and Spanish, Vilar et al (2007) proposed to build a statistical MT system that translated letters, whose underlying idea is similar to recent approaches in neural MT that are characterbased (Costa-juss`a and Fonollosa, 2016). As far as we are concerned, there are no previous works in neural MT covering Catalan-Spanish language pair. 3 3.2 Catalan and Spanish belong to the romance languages which are the modern languages that evolved from Latin. Since both languages are"
W17-1207,P02-1040,0,0.0979673,"Missing"
W17-1207,C86-1155,0,0.622994,"Missing"
W17-1207,P16-1162,0,0.060288,"nci´o ayer el inicio de un ciclo de conferencias que analizar´an la obra de Elliot y cuyas conclusiones se recoger´an en un libro . el cas dels profesionals e´ s diferente . el caso de los profesionales es diferente . en el caso de los profesionales es diferente caso de los profesionales es diferente el caso de los profesionales es diferente . Table 4: Translation examples. coverage neural MT (Tu et al., 2016); wrong translations may be reduced using a language model (Gulcehre et al., 2017); and out-of-vocabulary words may be reduced using existing approaches such as Byte Pair Encoding (BPE) (Sennrich et al., 2016) or character-based (Costa-juss`a and Fonollosa, 2016). The integration of these new advances for Catalan-Spanish language pair is left for future work. 7 Spanish language pair. Performance is better in the case of the neural MT system when using the in-domain test set, but best performance in the out-of-domain test set is better for the rule-based system (Spanish-to-Catalan, in BLEU) and for the phrase-based sytem (Catalan-to-Spanish). Regarding our research question: Is neural MT competitive with current well-performing rulebased and phrase-based MT systems? Based on the automatic and manual"
W17-1207,P15-1001,0,0.0273244,"phrase discounting, lexical weights, phrase bonus, accepting phrases up to length 10, 5-gram language model with Kneser-Ney smoothing, word bonus and MERT (Minimum Error Rate Training) optimisation. Neural-based The neural MT system was built using the open-source software available in github3 . This code implements the auto-encoder with attention that we presented in section 4.3. We use the parameters defined in Table 2. Regarding vocabulary limitation, we use a vocabulary size of 90,000 both in Spanish and in Catalan. We replace out-of-vocabulary words (UNKs) using the standard methodology (Jean et al., 2015): we use the word-to-word translation model learned with ’fast-align’ (Dyer et al., 2013) or, if not available, the aligned source word is used. We use an embedding of 512 and a dimension of 1024, a batch size of 32, and no dropout, learning-rate of 0.001 and adadelta optimization. Experimental Framework This section reports details on the data used for training, optimizing and testing as well as a description of the parameters for each system in the comparison. 5.1 Data We use a large corpus extracted from ten years of the paper edition of a bilingual Catalan newspaper, El Peri´odico (Costa-j"
W17-1207,D13-1176,0,0.0362218,"ture allows for an end-to-end optimization. 4.1 Rule-based MT Rule-based MT combines dictionaries and handmade rules to generate the target output given the source input. Generally, a morphological and syntactic analysis of the source input is needed before doing the transfer into a simplified target. The final target is generated adding the appropriate morphology and/or syntax. See Figure 2 for an schematic representation of this approach. 4.2 4.3 Neural MT Neural MT computes the conditional probability of the target sentence given the source sentence by means of an autoencoder architecture (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014). First, the encoder reads the source sentence (s1 , s2 ..., sN ) of N words, the encoder does a word embedding (e1 , e2 , ...en ) and encodes it into an intermediate representation (also refered to as context vector) by means of a recurrent neural network, which uses the gated recurrent unit (GRU) as activation function. The GRU function allows for a better performance with long sentences. Then, the decoder, which is also a recurrent neural network, generates a corPhrase-based Statistical MT Standard phrase-based statitical MT (Koehn et al., 2003) fo"
W17-1207,P16-5005,0,0.0191285,"de Elliot . las conclusiones se recoger´an en un libro . (...) anunci´o ayer el comienzo de un ciclo de conferencias que analizar´an la obra de Elliot , cuyas conclusiones se recoger´an en un libro . anunci´o ayer el inicio de un ciclo de conferencias que analizar´an la obra de Elliot y cuyas conclusiones se recoger´an en un libro . el cas dels profesionals e´ s diferente . el caso de los profesionales es diferente . en el caso de los profesionales es diferente caso de los profesionales es diferente el caso de los profesionales es diferente . Table 4: Translation examples. coverage neural MT (Tu et al., 2016); wrong translations may be reduced using a language model (Gulcehre et al., 2017); and out-of-vocabulary words may be reduced using existing approaches such as Byte Pair Encoding (BPE) (Sennrich et al., 2016) or character-based (Costa-juss`a and Fonollosa, 2016). The integration of these new advances for Catalan-Spanish language pair is left for future work. 7 Spanish language pair. Performance is better in the case of the neural MT system when using the in-domain test set, but best performance in the out-of-domain test set is better for the rule-based system (Spanish-to-Catalan, in BLEU) and"
W17-1207,N03-1017,0,0.156481,"TALP Research Center Universitat Polit`ecnica de Catalunya, 08034 Barcelona marta.ruiz@upc.edu Abstract Balkan and R. Lee Humphreys and Siety Meijer and Louisa Sadler, 1994). Years later, corpusbased approaches have reached both the interest in the scientific and industrial community (Hutchins, 1986). Recently, neural MT approach has been proposed. This corpus-based approach uses deep learning techniques (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) and it may be taking over previous popular corpusbased approaches such as statistical phrase or hierarchical-based (Koehn et al., 2003; Chiang, 2007). As a result, large companies, such as Google, have been using rule-based MT, then statistical MT and just very recently, they are replacing some of their statistical MT engines by neural MT engines (Wu et al., 2016). This paper analizes how standard neural MT techniques, which are briefly described in section 4.3, perform on the Catalan-Spanish task compared to popular rule-based and phrase-based MT. Additionally, we perform a naive system combination using the standard Minimum Bayes Risk (MBR) technique (Ehling et al., 2007) which reports slight improvements, in terms of stan"
W17-1207,W07-0705,0,0.0144675,"peakers, in practice, also speak Spanish. However, it is not the same for Spanish native speakers. This leads us to a first example of use case for an MT system for this language pair: Spanish (native) speakers that do not understand Catalan. Other use cases include professional translations or web page translations. Related work Previous related publications on the CatalanSpanish language pair are in rule-based MT (Canals-Marote et al., 2001; Alonso, 2005) and statitical MT (Poch et al., 2009; Costa-juss`a et al., 2012). It is worth noting that given the similarity among Catalan and Spanish, Vilar et al (2007) proposed to build a statistical MT system that translated letters, whose underlying idea is similar to recent approaches in neural MT that are characterbased (Costa-juss`a and Fonollosa, 2016). As far as we are concerned, there are no previous works in neural MT covering Catalan-Spanish language pair. 3 3.2 Catalan and Spanish belong to the romance languages which are the modern languages that evolved from Latin. Since both languages are from the same linguistic family, both share similar linguistic features such as morphological inflections or word reordering. Translation between both langua"
W17-1207,W04-3250,0,0.423867,"Missing"
W17-1207,P07-2045,0,\N,Missing
W17-1207,W07-0734,0,\N,Missing
W17-4123,P16-2058,1,0.871981,"Missing"
W17-4123,N16-1155,0,0.0299876,"n behaves. In this paper, we propose to use the fullycharacter neural machine translation architecture (Lee et al., 2016) but using bytes instead of characters. We compare the performance of character against byte-based neural machine translation among similar languages (Catalan/Spanish and Portuguese/Brazilian) and relatively far languages (in terms of alphabet) (German/Finnish/TurkishEnglish). As far as we are concerned, we are not aware of any research work in neural machine translation that has experimented with bytes. Related work can be found int the area of natural language processing. Gillick et al. (2016) propose an neural network that reads text as bytes and use this model in tasks of Part-of-Speech and Named Entity Recognition. The recent investigation of Irie et al (2017) describes the use of a byte-level convolutional layer (instead of character-level) in the neural language model (Irie et al., 2017), which is applied to low resource speech recognition. This paper presents experiments comparing character-based and byte-based neural machine translation systems. The main motivation of the byte-based neural machine translation system is to build multilingual neural machine translation systems"
W17-4123,Q17-1024,0,0.0719885,"Missing"
W17-4123,P07-2045,0,0.00993537,"pairs, we used all data parallel data provided in the evaluation. For German-English, we used: europarl v.7, news commentary v.12, common crawl and rapid corpus of EU press releases. For Finnish-English, we used europarl v.8, wiki headlines and rapid corpus of EU press releases. For Turkish-English, we used setimes2. The German and Finish test set is the news 2015 evaluation set, for Turkish the test set is the news 2016 evaluation set. Preprocessing consisted in cleaning empty sentences, limiting sentences up to 50 words, tokenization and truecasing for each language using tools from Moses (Koehn et al., 2007). Table 1 shows details about the corpus statistics after preprocessing. Byte-based Neural Machine Translation The byte-based Neural Machine Translation changes the character representation of words to the byte representation. Each sentence is represented as the concatenation of bytes that form its characters in utf-8 encoding. No explicit vocabulay is used but we can consider the byte representation as a vocabulary of 256 positions in which every possible byte can be represented. This modifications provides the following improvements over the previously seen architecture. • Both languages sha"
W17-4123,W17-2619,0,0.0250891,"tage of the current setting is that interlingua is not manually designed but it seems that it can be automatically extracted (Johnson et al., 2016). In addition, this multilingual environment seems to allow to build translation systems among language pairs that do not have parallel corpus available (Johnson et al., 2016), what is called “zeroshot translation”. These two motivations (interlingua and zeroshot translation) are strong enough to motivate the entire commmunity to experiment towards multilingual architectures. Recently, there have appeared works in multilingual word representations (Schwenk et al., 2017; Espa˜na-Bonet et al., 2017) Most multilingual works are at the level of words. As multilingual character research we can find (Lee et al., 2016) which goes from manyto-one languages in translation and achieves improvements for several language pairs. Previous work on character-based neural machine transla2 Character-based Neural Machine Translation Our system uses the architecture from (Lee et al., 2016) where a character-level neural MT model that maps the source character sequence to the target character sequence. The main difference in the encoder architecture of the standard neural 154 P"
W17-4123,tiedemann-2012-parallel,0,0.029509,"redicts each target character. 3 4 Experimental Framework In this section we detail experimental corpora, architecture and parameters that we used. 4.1 Data and Preprocessing For Catalan-Spanish, We use a large corpus extracted from ten years of the paper edition of a bilingual Catalan newspaper, El Peri´odico (Costajuss`a et al., 2014). The Spanish-Catalan corpus is partially available via ELDA (Evaluations and Language Resources Dis-tribution Agency) in catalog number ELRA-W0053. Development and test sets are extracted from the same corpus. For Portuguese-Brazilian, we used the OPUS corpus (Tiedemann, 2012) which is a growing collection of translated texts from the web. In particular, for Portuguese-Brazilian the source corpus are from Ubuntu and GNOME. We extracted the parallel text from translation memories (TMX format) and from the complete text, we extracted a collection of development and test set. Finally, we used WMT 2017 1 corpus data for German, Finish and Turkish to English. For the three language pairs, we used all data parallel data provided in the evaluation. For German-English, we used: europarl v.7, news commentary v.12, common crawl and rapid corpus of EU press releases. For Finn"
W17-4123,Q17-1026,0,\N,Missing
W17-4725,P11-2031,0,0.103711,"Missing"
W17-4725,P16-2058,1,0.895913,"Missing"
W17-4725,W16-2323,0,0.144924,"n the decoder architecture is that the single-layer feedforward network computes the attention score of next target character (instead of word) to be generated with every source segment representation. And afterwards, a two-layer character-level decoder takes the source context vector from the attention mechanism and predicts each target character. Introduction Neural Machine Translation (MT) has been proven to reach state-of-the-art results in the last couple of years. The baseline encoder-decoder architecture has been improved by an attentionbased mechanism citebahdanau:2015, subword units (Sennrich et al., 2016b), character-based encoders (Costa-juss`a and Fonollosa, 2016) or even with generative adversarial nets (Yang et al., 2017), among many others. Despite its successful beginnings, the neural MT approach still has many challenges to solve and improvements to incorporate into the system. However, since the system is computationally expensive and training models may last for several weeks, it is not feasible to conduct multiple experiments for a mid-sized laboratory. For the same 283 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 283–287 c Copenhag"
W17-4725,P07-2045,0,0.0387424,"ative results on NMT. In this system description, we describe our participation on German-English and Finnish-English for the News Task. Our system is a fully characterto-character neural MT (Lee et al., 2016) system with additional rescoring from the inverse direction model. In parallel to our final system, we also experimented with multilingual character-tocharacter system using German, Finnish and Turkish on the source side and English on the target side. Unfortunately, these last experiments did not work. All our systems are contrasted with a standard phrase-based system built with Moses (Koehn et al., 2007). In this paper, we describe the TALPUPC participation in the News Task for German-English and Finish-English. Our primary submission implements a fully character to character neural machine translation architecture with an additional rescoring of a n-best list of hypothesis using a forced back-translation to the source sentence. This model gives consistent improvements on different pairs of languages for the language direction with the lowest performance while keeping the quality in the direction with the highest performance. 2 Additional experiments are reported for multilingual character to"
W17-4725,N18-1122,0,0.0692399,"Missing"
W17-4725,Q17-1026,0,\N,Missing
W17-4725,P16-1162,0,\N,Missing
W17-5309,W17-5301,0,0.0143604,"ctors to conduct the prediction. (6) 3 Experiments It first feed all hidden states ht through a nonlinearity to get ut as the hidden representation of ht . Then it uses a sof tmax function to catch the normalized importance weight matrix αt . After that, the sentence representation vector h is computed by a weighted sum of all hidden states ht with the weight matrix αt . The context vector uω can be seen as a high-level representation of the importance of informative words. 2.4 3.1 Data We evaluated our approach on the Multi-Genre NLI (MNLI) corpus, as a shared task for RepEval 2017 workshop (Nangia et al., 2017). We train our CIAN model on a mixture of MNLI and SNLI corpus, by using a full MNLI training set and a randomly selected 20 percent of the SNLI training set at each epoch. Character-level Intra Attention Network 3.2 The overall architecture of the Character-level Intra Attention Network (CIAN) is shown in Figure 2. The CIAN model is consisted with 7 layers, of which the first and the last layers are the same with our baseline model. The 4 layers in middle are our augmented layers that has been introduced in this section. Hyper Parameters The BiLSTM encoder layer use 300D hidden states, thus 6"
W17-5309,D15-1075,0,0.0591592,"R. Costa-juss`a and Jos´e A. R. Fonollosa TALP Research Center Universitat Polit`ecnica de Catalunya han.yang@est.fib.upc.edu {marta.ruiz,jose.fonollosa}@upc.edu Abstract traditional baselines in many NLP tasks (Dai and Le, 2015). There are also convolutional neural network (CNN; LeCun et al., 1989) based encoders, which concatenate the sentence information by applying multiple convolving filters over the sentence. CNNs have achieved state-of-the-art results on various NLP tasks (Collobert et al., 2011). To evaluate the quality of the NLI model, the Stanford Natural Language Inference (SNLI; Bowman et al., 2015) corpus of 570K sentence pairs was introduced. It serves as a standard benchmark for NLI task. However, most of the sentences in SNLI corpus are short and simple, which limit the room for fine-grained comparisons between models. Currently, a more comprehensive Multi-Genre NLI corpus (MNLI; Williams et al., 2017) of 433K sentence pairs was released, aiming at evaluating large-scale NLI models. Authors gave out some baseline results accompanied by the publish of MNLI corpus, the BiLSTM model achieves an accuracy of 67.5, and the Enhanced Sequential Inference Model (Chen et al., 2016) achieves an"
W17-5309,D14-1162,0,0.0846724,"fine-grained comparisons between models. Currently, a more comprehensive Multi-Genre NLI corpus (MNLI; Williams et al., 2017) of 433K sentence pairs was released, aiming at evaluating large-scale NLI models. Authors gave out some baseline results accompanied by the publish of MNLI corpus, the BiLSTM model achieves an accuracy of 67.5, and the Enhanced Sequential Inference Model (Chen et al., 2016) achieves an accuracy of 72.4. Among those encoders for NLI task, most of them use word-level embedding, and initialize the weight of the embedding layer with pre-trained word vectors such as GloVe (Pennington et al., 2014). The pre-trained word vectors helps the encoders to catch richer semantic information. However, it also has its downside. As the growth of vocabulary size in the modern corpus, there will be more and more out-of-vocabulary (OOV) words that are not presented in the pre-trained word embedding vector. As the word-level embedding is blind to subword information (e.g. morphemes), it leads to high perplexities for those OOV words. In this paper, we use the BiLSTM model from (Williams et al., 2017) as the baseline model for the evaluation of the MNLI corpus. To augment the baseline model, firstly, a"
W17-5309,D16-1053,0,0.038415,"Missing"
W17-5309,D15-1044,0,0.0277734,"ingle representation vector of each sentence. However, this has its bottleneck as we intuitively know that not all words (hidden states) contribute equally to the sentence representation. To augment the performance of RNN based encoder, the concept of attention mechanism was introduced by (Bahdanau et al., 2014) for machine translation. Attention mechanism is a hidden layer which computes a categorical distribution to make a soft-selection over source elements (Kim et al., 2017). It has recently demonstrated success on tasks such as parsing text (Vinyals et al., 2015), sentence summarization (Rush et al., 2015) and Character-level Convolutional Neural Network In the baseline model, the input xt to the BiLSTM encoder layer at time t is sequence of pre-trained word embeddings. Those pre-trained word embeddings can boost the performance of the model. However, it is limited to the finite-size of vocabulary. Here we replace the word embedding layer with a character-level convolutional neural network (CharCNN; Kim et al., 2016) for language 47 also on a wide range of NLP tasks (Cheng et al., 2016). Here we implemented the Intra Attention mechanism introduced by (Yang et al., 2016) for document classificat"
W17-5309,P16-2058,1,0.881226,"Missing"
W17-5309,N16-1174,0,0.254284,"ory Networks (LSTM; Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (Chung et al., 2014). RNNs have surpassed the performance of 46 Proceedings of the 2nd Workshop on Evaluating Vector-Space Representations for NLP, pages 46–50, c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics convolutional neural network (CharCNN; Kim et al., 2016) is applied. We use the CharCNN to replace the word embedding layer in the baseline model, which will be computed from the characters of corresponding word. Secondly, the intra attention mechanism introduced by (Yang et al., 2016) will be applied, to enhance the model with a richer information of substructures of a sentence. 2 2.1 modeling, which also achieved success in machine translation (Costa-Juss`a and Fonollosa, 2016). We define the text sentence input as vector C k ∈ d×l R , where k ∈ K is the k-th word in a sentence, d is the dimensionality of character embeddings, l is the length of characters in k-th word. Then a set of narrow convolutions between C k and filter H is applied, followed with a max-over-time (max pooling) as shown in Equation 1-2. Model Development f k [i] = tanh(hC k [∗, i : i + ω − 1], Hi + b"
W17-5309,P82-1020,0,0.845688,"Missing"
W18-3931,D18-1549,0,0.0246348,"et al., 2016)). Far from being settled, the architecture of NMT systems is constantly evolving. Given the youth of the paradigm and while the main structure of encoder-decoder is still maintained, the implementation of such is done either using recurrent neural networks (RNN) with attention mechanisms (Bahdanau et al., 2015), to convolutional neural networks (CNN) (Gehring et al., 2017) and to only attention mechanisms (Vaswani et al., 2017). For the same reason, research in NMT goes in many directions, including minimal units (Sennrich et al., 2016), unsupervised training and low resources (Artetxe et al., 2018) or transfer learning (Zoph et al., 2016), to name and cite just a few. In this paper we tackle an under-explored problem and apply NMT techniques to translate between language varieties. In previous work (Costa-juss`a, 2017), NMT has been used to translate between Spanish and Catalan, two closely-related Romance languages from the Iberian peninsula, outperforming phrase-based SMT approaches. In this paper we test whether this is also true for national varieties of the same language taking Brazilian and European Portuguese as a case study. To the best of our knowledge the use of NMT to transla"
W18-3931,W14-4012,0,0.102819,"Missing"
W18-3931,P16-2058,1,0.900353,"Missing"
W18-3931,W17-4123,1,0.872714,"Missing"
W18-3931,W17-1207,1,0.793656,"Missing"
W18-3931,2014.eamt-1.34,0,0.696539,"Missing"
W18-3931,L16-1284,1,0.907441,"Missing"
W18-3931,W17-1208,0,0.191108,"ttish Gaelic, (Goyal and Lehal, 2010) on Hindi and Punjabi, a few studies 1 In this paper, when talking about language varieties, we use the verbs adapt, edit, and translate interchangeably. In previous work Marujo et al. (2011) used the word adaptation whereas Fancellu et al. (2014) used the word conversion. We consider it, however, as a full-fledged translation task and approach the task as such. 2 The 1990 orthographic agreement has been recently introduced in both countries diminishing these differences. 276 on Afrikaans and Dutch (Van Huyssteen and Pilon, 2009; Otte and Tyers, 2011), and Hassani (2017) on Kurdish dialects. To the best of our knowledge, however, the use of NMT is under-explored in these tasks and no language variety translation system has been developed using NMT. The most similar study is the one by (Costa-juss`a et al., 2017) who developed a neural-based MT system to translate between Catalan and Spanish. The use of NMT to translate between language varieties is the main contribution of our work. 3 Methods To be able to compare MT approaches, we trained SMT and NMT systems using the same dataset described in Section 3.1. The two systems are described in detail in Section 3"
W18-3931,W17-3204,0,0.0248067,"ystem in comparison to the statistical system. 1 Introduction In the last five years Neural Machine Translation (NMT) has evolved from a new and promising paradigm in Machine Translation (MT) to an established state-of-the-art technology. A few studies pose that performance difference between Statistical Machine Translation (SMT) and NMT is not as a great as one could imagine (Castilho et al., 2017) while others show interesting challenges for NMT (compared to SMT) such as learning with limited amount of data, out-of-domain, long sentences, low frequency words or lack of word alignment model (Koehn and Knowles, 2017). Even so, NMT systems have constantly ranked in the top positions in the competitions held in MT conferences and workshops such as WMT (Bojar et al., 2016) and WAT (Nakazawa et al., 2016). They have also been achieving commercial success (e.g. Google’s GNMT (Wu et al., 2016)). Far from being settled, the architecture of NMT systems is constantly evolving. Given the youth of the paradigm and while the main structure of encoder-decoder is still maintained, the implementation of such is done either using recurrent neural networks (RNN) with attention mechanisms (Bahdanau et al., 2015), to convol"
W18-3931,N03-1017,0,0.0589814,"The use of NMT to translate between language varieties is the main contribution of our work. 3 Methods To be able to compare MT approaches, we trained SMT and NMT systems using the same dataset described in Section 3.1. The two systems are described in detail in Section 3.2. Systems within the SMT category use statistical techniques to compose the final translation. There are a variety of alternatives that are state-of-the art, including: n-gram (Mari˜no et al., 2006), syntax (Yamada and Knight, 2001) or hierarchical to name a few. In this paper, we are using the popular phrase-based system (Koehn et al., 2003). Systems within the NMT category use a machine learning architecture based on neural networks to compose the final translation. As mentioned, there are several architectures which have been proven state-of-the-art, all of them based on an encoder-decoder schema but using either recurrent neural networks (Cho et al., 2014), convolutional neural networks (Gehring et al., 2017) or the transformer architecture based only on attention-based mechanisms (Vaswani et al., 2017). These architectures can be adapted to deal with different input representations either words, subwords (Sennrich et al., 201"
W18-3931,P07-2045,0,0.0103602,"each language for training, with over 33 million tokens for BP and over 34 million tokens for EP. Finally, 2,000 parallel sentences were kept for development and another 2,000 sentences for testing. 3.2 Systems Statistical-based. In a phrase-based system, the main model, which is the translation model, is extracted by statistical co-occurrences from a parallel corpus at the level of sentences. This translation model is combined in the decoder with other models to compose the most probable translation given a source input. We built a standard phrase-based system with Moses open source toolkit (Koehn et al., 2007). The main parameters of our implementation include: grow-diagonal-final-and word alignment symmetrization, lexicalized reordering, relative frequencies (conditional and posterior probabilities) with phrase discounting, lexical weights, phrase bonus, accepting phrases up to length 10, 5-gram language model with Kneser-Ney smoothing, word bonus and MERT (Minimum Error Rate Training) optimisation. These parameters are taken from previous work (Costa-juss`a et al., 2017). 3 4 http://opus.lingfil.uu.se/ The clean version of the corpus is available upon request. 277 Neural-based. Specifically, neur"
W18-3931,Q17-1026,0,0.0341541,"arning architecture based on neural networks to compose the final translation. As mentioned, there are several architectures which have been proven state-of-the-art, all of them based on an encoder-decoder schema but using either recurrent neural networks (Cho et al., 2014), convolutional neural networks (Gehring et al., 2017) or the transformer architecture based only on attention-based mechanisms (Vaswani et al., 2017). These architectures can be adapted to deal with different input representations either words, subwords (Sennrich et al., 2016), characters (Costa-juss`a and Fonollosa, 2016; Lee et al., 2017) or bytes (Costa-juss`a et al., 2017). In this paper, we are using the first option of recurrent neural networks with an added attention-based mechanism (Bahdanau et al., 2015) and bytes as input representations (Costa-juss`a et al., 2017). 3.1 Data Compiling suitable parallel language variety corpora for NLP tasks is not trivial. Popular and freely available data sources (e.g. Wikipedia) used in NLP do not account for regional variation. One possible data source that includes national varieties of the same language are technical user manuals which are often localized between countries. Howeve"
W18-3931,2011.eamt-1.22,0,0.0155857,"ell (2006) on Irish and Scottish Gaelic, (Goyal and Lehal, 2010) on Hindi and Punjabi, a few studies 1 In this paper, when talking about language varieties, we use the verbs adapt, edit, and translate interchangeably. In previous work Marujo et al. (2011) used the word adaptation whereas Fancellu et al. (2014) used the word conversion. We consider it, however, as a full-fledged translation task and approach the task as such. 2 The 1990 orthographic agreement has been recently introduced in both countries diminishing these differences. 276 on Afrikaans and Dutch (Van Huyssteen and Pilon, 2009; Otte and Tyers, 2011), and Hassani (2017) on Kurdish dialects. To the best of our knowledge, however, the use of NMT is under-explored in these tasks and no language variety translation system has been developed using NMT. The most similar study is the one by (Costa-juss`a et al., 2017) who developed a neural-based MT system to translate between Catalan and Spanish. The use of NMT to translate between language varieties is the main contribution of our work. 3 Methods To be able to compare MT approaches, we trained SMT and NMT systems using the same dataset described in Section 3.1. The two systems are described in"
W18-3931,L16-1095,1,0.85961,"Missing"
W18-3931,P02-1040,0,0.100921,"CAT tool developed for translation process research. We ask native speakers of Brazilian Portuguese first to compare segments translated by NMT and SMT, choosing the best output, and subsequently to rate translations taking both fluency and adequacy into account using a 1 to 7 Likert scale. More information and the results of these experiments are presented in Section 4.2. 4 Results 4.1 Automatic Metrics In this section we present the results obtained by the statistical-based system based of phrases and the neural-based system based on seq2seq with attention and bytes in terms of BLEU score (Papineni et al., 2002). Table 1 presents the results obtained by the three systems when translating from EP to BP and Table 2 presents results obtained from BP to EP. The best results for each setting are presented in bold. System BLEU Score Phrase-based SMT Neural MT 47.68 48.58 Table 1: European to Brazilian Portuguese translation results in terms of BLEU score. System BLEU Score Phrase-based SMT Neural MT 47.34 47.54 Table 2: Brazilian to European Portuguese translation results in terms of BLEU score. We observed that in both directions the NMT system outperformed the SMT approach. The neural system obtained the"
W18-3931,P16-1162,0,0.471326,"ve also been achieving commercial success (e.g. Google’s GNMT (Wu et al., 2016)). Far from being settled, the architecture of NMT systems is constantly evolving. Given the youth of the paradigm and while the main structure of encoder-decoder is still maintained, the implementation of such is done either using recurrent neural networks (RNN) with attention mechanisms (Bahdanau et al., 2015), to convolutional neural networks (CNN) (Gehring et al., 2017) and to only attention mechanisms (Vaswani et al., 2017). For the same reason, research in NMT goes in many directions, including minimal units (Sennrich et al., 2016), unsupervised training and low resources (Artetxe et al., 2018) or transfer learning (Zoph et al., 2016), to name and cite just a few. In this paper we tackle an under-explored problem and apply NMT techniques to translate between language varieties. In previous work (Costa-juss`a, 2017), NMT has been used to translate between Spanish and Catalan, two closely-related Romance languages from the Iberian peninsula, outperforming phrase-based SMT approaches. In this paper we test whether this is also true for national varieties of the same language taking Brazilian and European Portuguese as a ca"
W18-3931,tiedemann-2012-parallel,0,0.0209058,"y corpora for NLP tasks is not trivial. Popular and freely available data sources (e.g. Wikipedia) used in NLP do not account for regional variation. One possible data source that includes national varieties of the same language are technical user manuals which are often localized between countries. However, user manuals contain a very specific technical language with short and idiomatic sentences representing commands. We searched for suitable datasets and we acquired an aligned Brazilian - European Portuguese parallel corpus of film subtitle dialogues from Open Subtitles available at Opus3 (Tiedemann, 2012). We removed all XML tags available in the data. The cleaned corpus, which we will be making available for the community as another contribution of our work4 , comprises 4.3 million sentences in each language for training, with over 33 million tokens for BP and over 34 million tokens for EP. Finally, 2,000 parallel sentences were kept for development and another 2,000 sentences for testing. 3.2 Systems Statistical-based. In a phrase-based system, the main model, which is the translation model, is extracted by statistical co-occurrences from a parallel corpus at the level of sentences. This tra"
W18-3931,P01-1067,0,0.307045,"e one by (Costa-juss`a et al., 2017) who developed a neural-based MT system to translate between Catalan and Spanish. The use of NMT to translate between language varieties is the main contribution of our work. 3 Methods To be able to compare MT approaches, we trained SMT and NMT systems using the same dataset described in Section 3.1. The two systems are described in detail in Section 3.2. Systems within the SMT category use statistical techniques to compose the final translation. There are a variety of alternatives that are state-of-the art, including: n-gram (Mari˜no et al., 2006), syntax (Yamada and Knight, 2001) or hierarchical to name a few. In this paper, we are using the popular phrase-based system (Koehn et al., 2003). Systems within the NMT category use a machine learning architecture based on neural networks to compose the final translation. As mentioned, there are several architectures which have been proven state-of-the-art, all of them based on an encoder-decoder schema but using either recurrent neural networks (Cho et al., 2014), convolutional neural networks (Gehring et al., 2017) or the transformer architecture based only on attention-based mechanisms (Vaswani et al., 2017). These archit"
W18-3931,P98-2238,0,0.0882976,"er Zero Hora, and Ted Talks to evaluate their method. Another example of a system developed to translate between Brazilian and European Portuguese is the one by Fancellu et al. (2014) who presented and SMT system trained on a parallel collection from Intel translation memories. The authors report 0.589 BLEU score using a Moses baseline system. Apart from the two aforementioned studies on translating between Portuguese varieties there have been a few studies published on translating between similar languages, language varieties, and dialects of other languages. Examples of such studies include Zhang (1998) on Mandarin and Cantonese Chinese, Scannell (2006) on Irish and Scottish Gaelic, (Goyal and Lehal, 2010) on Hindi and Punjabi, a few studies 1 In this paper, when talking about language varieties, we use the verbs adapt, edit, and translate interchangeably. In previous work Marujo et al. (2011) used the word adaptation whereas Fancellu et al. (2014) used the word conversion. We consider it, however, as a full-fledged translation task and approach the task as such. 2 The 1990 orthographic agreement has been recently introduced in both countries diminishing these differences. 276 on Afrikaans a"
W18-3931,D16-1163,0,0.0335215,"architecture of NMT systems is constantly evolving. Given the youth of the paradigm and while the main structure of encoder-decoder is still maintained, the implementation of such is done either using recurrent neural networks (RNN) with attention mechanisms (Bahdanau et al., 2015), to convolutional neural networks (CNN) (Gehring et al., 2017) and to only attention mechanisms (Vaswani et al., 2017). For the same reason, research in NMT goes in many directions, including minimal units (Sennrich et al., 2016), unsupervised training and low resources (Artetxe et al., 2018) or transfer learning (Zoph et al., 2016), to name and cite just a few. In this paper we tackle an under-explored problem and apply NMT techniques to translate between language varieties. In previous work (Costa-juss`a, 2017), NMT has been used to translate between Spanish and Catalan, two closely-related Romance languages from the Iberian peninsula, outperforming phrase-based SMT approaches. In this paper we test whether this is also true for national varieties of the same language taking Brazilian and European Portuguese as a case study. To the best of our knowledge the use of NMT to translate between national language varieties ha"
W18-3931,C98-2233,0,\N,Missing
W18-6406,D16-1250,0,0.0218591,"s. Geographical location has also led to differences in the loanwords borrowed by each language. 355 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 355–360 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64033 3 Attention-based NMT with an auxiliary text (denoising) auto-encoding loss, whose internal sentence representation is aligned with the ones from the translation task by means of a discriminator in feature space. Pre-trained cross-lingual embeddings (Artetxe et al., 2016, 2017) can be used complementarily to further reduce the need for parallel data. Finding parallel data from a similar source language and the same target language (or vice versa) and adding it to the original parallel corpus. With such a composite training data set, a wordpiece-level vocabulary can leverage the common word stems between the similar languages and profit from the combined amount of data. This approach is used in the present work, as described in sections 5 and 6.1. Multilingual zero-shot translation (Johnson et al., 2017) also uses parallel corpora from different source and tar"
W18-6406,P17-1042,0,0.0362811,"Missing"
W18-6406,P07-2045,0,0.00689592,"k are constrained using exclusively parallel data provided by the organization. For the English - Finnish language pair the data employed is the Europarl corpus version 7 and 8, Paracrawl corpus, Rapid corpus of EU press releases and Wiki Headlines corpus. For the English - Estonian data the Europarl v8 corpus, Paracrawl and Rapid corpus of EU press releases corpus were employed. All language pairs have been preprocessed following the proposed scripts by the organization of the conference. The pipeline consisted in normalizing punctuation, tokenization and truecasing using the standard Moses (Koehn et al., 2007) 356 scripts. With the addition that, for tokenization, no escaping of special characters was performed. For the language pair of English - Estonian we found that from Paracrawl corpus a considerable number of sentences were not suitable sentences in the intended languages, but apparently random sequences of upper case characters. In order to remove them, an additional step of language detection was performed using library langdetect (Danil´ak, 2017), which is a port to Python of library language-detection (Shuyo, 2010). The criteria for removing noisy sentences from the dataset was that eithe"
W18-6406,D18-1549,0,0.0139989,"e originally available parallel corpus and train on it a new source language to target language translation system. Pivoting approaches use a third resource-rich language as pivot and train translation systems from source language to pivot and from pivot to target language. These auxiliary systems can either be used in cascade to obtain source-to-target translations, or be used to build syntethic parallel source-target corpora (i.e. pseudocorpus approach). A recent application of pivoting techniques to NMT can be found in (Costa-juss`a et al., 2018). Adversarial learning (Lample et al., 2018; Artetxe et al., 2018) in a multi-task learning setup, 5 Corpora and Data preparation All proposed systems in this work are constrained using exclusively parallel data provided by the organization. For the English - Finnish language pair the data employed is the Europarl corpus version 7 and 8, Paracrawl corpus, Rapid corpus of EU press releases and Wiki Headlines corpus. For the English - Estonian data the Europarl v8 corpus, Paracrawl and Rapid corpus of EU press releases corpus were employed. All language pairs have been preprocessed following the proposed scripts by the organization of the conference. The pipel"
W18-6406,D14-1179,0,0.0327078,"Missing"
W18-6406,D15-1166,0,0.0570156,"get language, the source sentence is prefixed with a token that specifies which language the target sentence belongs to. This approach aims at implicitly learning language-independent internal representations, enabling the translation of low resource language pairs (and even language pairs where there is zero parallel data available) to profit from the combined language pair training data. The first competitive NMT systems were based on the sequence-to-sequence architecture (Cho et al., 2014; Sutskever et al., 2014), especially with the addition of attention mechanisms (Bahdanau et al., 2014; Luong et al., 2015), either using Gated Recurrent Units (GRU) (Cho et al., 2014) or LongShort Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997). Sequence-to-sequence with attention was the state of the art NMT model until the Transformer architecture (Vaswani et al., 2017) was proposed. This model does not rely on recurrent units or convolutional networks, but only on attention layers, combining them with several other architectural elements: positional embeddings (Gehring et al., 2017), layer normalization (Ba et al., 2016), residual connections (He et al., 2016) and dropout (Srivastava et al., 2014)."
W18-6406,P16-1009,0,0.0508579,"Missing"
W18-6406,1983.tc-1.13,0,0.656265,"Missing"
W18-6449,P17-4012,0,0.0713179,"Missing"
W18-6449,W17-3204,0,0.0271297,"rent neural networks and attention. After this architecture, new proposals based on convolutional neural networks (Gehring et al., 2017) or only attention-based mechanisms (Vaswani et al., 2017) appeared. The latter architecture has achieved great success in Machine Translation (MT) and it has already been extended to other tasks such as Parsing (Kaiser et al., 2017), Speech Recognition 1 , Speech Translation (Cros et al., 2018), Chatbots (Costa-juss`a et al., 2018) among others. However, training with low resources is still a big drawback for neural architectures and NMT is not an exception (Koehn and Knowles, 2017). To face low resource scenarios, several techniques have been proposed, like using multi-source (Zoph and Knight, 2016), multiple languages (Johnson et al., 2017) or unsupervised techniques (Lample et al., 2018; Artetxe et al., 2018), among many others. 1 https://tensorflow.github.io/ tensor2tensor/tutorials/asr$_$with$_ $transformer.html 667 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 667–670 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64076 and “"
W18-6449,D18-1549,0,0.0233088,"eved great success in Machine Translation (MT) and it has already been extended to other tasks such as Parsing (Kaiser et al., 2017), Speech Recognition 1 , Speech Translation (Cros et al., 2018), Chatbots (Costa-juss`a et al., 2018) among others. However, training with low resources is still a big drawback for neural architectures and NMT is not an exception (Koehn and Knowles, 2017). To face low resource scenarios, several techniques have been proposed, like using multi-source (Zoph and Knight, 2016), multiple languages (Johnson et al., 2017) or unsupervised techniques (Lample et al., 2018; Artetxe et al., 2018), among many others. 1 https://tensorflow.github.io/ tensor2tensor/tutorials/asr$_$with$_ $transformer.html 667 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 667–670 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64076 and “I love her more than you”. The second stage is a multi-head self-attention. Instead of computing a single attention, this stage computes multiple attention blocks over the source, concatenates them and projects them linearly back onto"
W18-6449,N16-1004,0,0.0155647,"et al., 2017) or only attention-based mechanisms (Vaswani et al., 2017) appeared. The latter architecture has achieved great success in Machine Translation (MT) and it has already been extended to other tasks such as Parsing (Kaiser et al., 2017), Speech Recognition 1 , Speech Translation (Cros et al., 2018), Chatbots (Costa-juss`a et al., 2018) among others. However, training with low resources is still a big drawback for neural architectures and NMT is not an exception (Koehn and Knowles, 2017). To face low resource scenarios, several techniques have been proposed, like using multi-source (Zoph and Knight, 2016), multiple languages (Johnson et al., 2017) or unsupervised techniques (Lample et al., 2018; Artetxe et al., 2018), among many others. 1 https://tensorflow.github.io/ tensor2tensor/tutorials/asr$_$with$_ $transformer.html 667 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 667–670 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64076 and “I love her more than you”. The second stage is a multi-head self-attention. Instead of computing a single attention, thi"
W19-3801,W19-3816,0,0.253863,"nder Bias in Natural Language Processing, pages 1–7 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics with separate scores for masculine and feminine examples. To simplify evaluation, we did not disaggregate evaluation for this shared task, but instead encouraged fairness by not releasing the balance of masculine to feminine examples in the final evaluation data.1 The competition was run on Kaggle2 , a wellknown platform for competitive data science and machine learning projects with an active community of participants and support. 2.1 Attree (2019) Wang (2019) Abzaliev (2019) F1 96.2 95.7 95.4 Bias 0.99 0.99 0.99 Table 1: Performance of prize-winning submissions on the blind Kaggle evaluation set. logloss was the official task metric, and correlates well with F1 score, which was used in the original GAP work. 760 clean examples was dispersed in a larger set of 11,599 unlabeled examples to produce a set of 12,359 examples that competing systems had to rate. This augmentation was to discourage submissions based on manual labeling. We note many competing systems used the original GAP evaluation data4 as training data for this task, given that the two have the same fo"
W19-3801,W19-3812,0,0.113421,"all from individual contributors, while academic researchers worked in groups. This correlation is somewhat indicative of performance: individual contributors from industry won all three monetary prizes, and only one academic group featured in the top ten submissions. A possible factor in this was the concurrent timing of the competition with other conference deadlines. 5 https://github.com/allenai/allennlp/blob/ master/allennlp/modules/span_extractors/self_ attentive_span_extractor.py 3 Attree (2019) Wang (2019) Abzaliev (2019) Yang et al. (2019) Ionita et al. (2019)* Liu (2019) Chada (2019) Bao and Qiao (2019) Ionita et al. (2019)* Lois et al. (2019) Xu and Yang (2019) Place 1 2 3 4 5 7 9 14 22 46 67 logloss 0.13667 0.17289 0.18397 0.18498 0.19189 0.19473 0.20238 0.20758 0.22562 0.30151 0.39479 Members 1 1 1 4 1 1 1 2 4 3 2 Affiliation Industry Industry Industry Academic Other Industry Industry Academic Mixed Academic Academic Region USA Asia Europe Asia Africa USA USA Europe Mixed Europe USA Table 2: Teams with accepted system description papers. *Note the two teams placing 5 and 22 submitted a combined system description paper. Rank logloss Fine-tuning Attree (2019) Wang (2019) Abzaliev (2019) Ya"
W19-3801,W19-3809,0,0.0162363,"is one of the typologies of social bias (e.g. race, politics) that is alarming the Natural Language Processing (NLP) community. An illustration of the problematic behaviour are the recurrently appearing occupational stereotypes that homemaker is to woman as programmer is to man (Bolukbasi et al., 2016). Recent studies have aimed to detect, analyse and mitigate gender bias in different NLP tools and applications including word embeddings (Bolukbasi et al., 2016; Gonen and Goldberg, 2019), coreference resolution (Rudinger et al., 2018; Zhao et al., 2018), sentiment analysis (Park et al., 2018; Bhaskaran and Bhallamudi, 2019) and machine translation (Vanmassenhove et al., 2018; Font and Costa-juss`a, 2019). One of the main sources of gender bias is believed to be societal artefacts in the data from which our algorithms learn. To address this, many have created gender-labelled and gender-balanced datasets (Rudinger et al., 2018; Zhao et al., 2018; Vanmassenhove et al., 2018). We present the results of a shared task evaluation conducted at the 1st Workshop on Gender Bias in Natural Language Processing at the 2 Task The goal of our shared task was to encourage research in gender-fair models for NLP by providing a wel"
W19-3801,N19-1063,0,0.0205593,"titividad and the European Regional Development Fund and the Agencia Estatal de Investigaci´on, through the post-doctoral senior grant Ram´on y Cajal and by the Swedish Research Council through grant 2017-930. Gender It is encouraging to see submitted systems improve the gender gap so close to parity at 0.99, particularly as no special modeling strategies were required. Indeed, Abzaliev (2019) reported that a handcrafted pronoun gender feature had no impact. Moreover, Bao and Qiao (2019) report that BERT encodings show no significant gender bias on either WEAT (Caliskan et al., 2017) or SEAT (May et al., 2019). We look forward to studies considering potential biases in BERT across more tasks and dimensions of diversity. References The teams competing in the shared task made effective use of BERT in at least three distinct methods: fine-tuning, feature extraction, and masked language modeling. Many system papers noted the incredible power of the model (see, e.g. Attree (2019) for a good analysis), particularly when compared to hand-crafted features (Abzaliev, 2019). We also believe the widespread use of BERT is related to the low rate of external data usage, as it is easier for most teams to reuse a"
W19-3801,W19-3819,0,0.0589937,"issions were all from individual contributors, while academic researchers worked in groups. This correlation is somewhat indicative of performance: individual contributors from industry won all three monetary prizes, and only one academic group featured in the top ten submissions. A possible factor in this was the concurrent timing of the competition with other conference deadlines. 5 https://github.com/allenai/allennlp/blob/ master/allennlp/modules/span_extractors/self_ attentive_span_extractor.py 3 Attree (2019) Wang (2019) Abzaliev (2019) Yang et al. (2019) Ionita et al. (2019)* Liu (2019) Chada (2019) Bao and Qiao (2019) Ionita et al. (2019)* Lois et al. (2019) Xu and Yang (2019) Place 1 2 3 4 5 7 9 14 22 46 67 logloss 0.13667 0.17289 0.18397 0.18498 0.19189 0.19473 0.20238 0.20758 0.22562 0.30151 0.39479 Members 1 1 1 4 1 1 1 2 4 3 2 Affiliation Industry Industry Industry Academic Other Industry Industry Academic Mixed Academic Academic Region USA Asia Europe Asia Africa USA USA Europe Mixed Europe USA Table 2: Teams with accepted system description papers. *Note the two teams placing 5 and 22 submitted a combined system description paper. Rank logloss Fine-tuning Attree (2019) Wang (2019"
W19-3801,N19-1423,0,0.180018,"rom existing unbalanced datasets. The 1st ACL workshop on Gender Bias in Natural Language Processing included a shared task on gendered ambiguous pronoun (GAP) resolution. This task was based on the coreference challenge defined in Webster et al. (2018), designed to benchmark the ability of systems to resolve pronouns in real-world contexts in a gender-fair way. 263 teams competed via a Kaggle competition, with the winning system achieving logloss of 0.13667 and near gender parity. We review the approaches of eleven systems with accepted description papers, noting their effective use of BERT (Devlin et al., 2019), both via fine-tuning and for feature extraction, as well as ensembling. 1 Introduction Gender bias is one of the typologies of social bias (e.g. race, politics) that is alarming the Natural Language Processing (NLP) community. An illustration of the problematic behaviour are the recurrently appearing occupational stereotypes that homemaker is to woman as programmer is to man (Bolukbasi et al., 2016). Recent studies have aimed to detect, analyse and mitigate gender bias in different NLP tools and applications including word embeddings (Bolukbasi et al., 2016; Gonen and Goldberg, 2019), corefe"
W19-3801,D18-1302,0,0.0229876,"duction Gender bias is one of the typologies of social bias (e.g. race, politics) that is alarming the Natural Language Processing (NLP) community. An illustration of the problematic behaviour are the recurrently appearing occupational stereotypes that homemaker is to woman as programmer is to man (Bolukbasi et al., 2016). Recent studies have aimed to detect, analyse and mitigate gender bias in different NLP tools and applications including word embeddings (Bolukbasi et al., 2016; Gonen and Goldberg, 2019), coreference resolution (Rudinger et al., 2018; Zhao et al., 2018), sentiment analysis (Park et al., 2018; Bhaskaran and Bhallamudi, 2019) and machine translation (Vanmassenhove et al., 2018; Font and Costa-juss`a, 2019). One of the main sources of gender bias is believed to be societal artefacts in the data from which our algorithms learn. To address this, many have created gender-labelled and gender-balanced datasets (Rudinger et al., 2018; Zhao et al., 2018; Vanmassenhove et al., 2018). We present the results of a shared task evaluation conducted at the 1st Workshop on Gender Bias in Natural Language Processing at the 2 Task The goal of our shared task was to encourage research in gender-fair"
W19-3801,W19-3821,1,0.884179,"Missing"
W19-3801,N06-2015,0,0.228659,"Missing"
W19-3801,W19-3621,0,0.0381936,"use of BERT (Devlin et al., 2019), both via fine-tuning and for feature extraction, as well as ensembling. 1 Introduction Gender bias is one of the typologies of social bias (e.g. race, politics) that is alarming the Natural Language Processing (NLP) community. An illustration of the problematic behaviour are the recurrently appearing occupational stereotypes that homemaker is to woman as programmer is to man (Bolukbasi et al., 2016). Recent studies have aimed to detect, analyse and mitigate gender bias in different NLP tools and applications including word embeddings (Bolukbasi et al., 2016; Gonen and Goldberg, 2019), coreference resolution (Rudinger et al., 2018; Zhao et al., 2018), sentiment analysis (Park et al., 2018; Bhaskaran and Bhallamudi, 2019) and machine translation (Vanmassenhove et al., 2018; Font and Costa-juss`a, 2019). One of the main sources of gender bias is believed to be societal artefacts in the data from which our algorithms learn. To address this, many have created gender-labelled and gender-balanced datasets (Rudinger et al., 2018; Zhao et al., 2018; Vanmassenhove et al., 2018). We present the results of a shared task evaluation conducted at the 1st Workshop on Gender Bias in Natur"
W19-3801,D12-1071,0,0.0329911,". The different models built from BERT are summarized in Table 3. Eight of the eleven system descriptions used BERT via fine-tuning, the technique recommended in Devlin et al. (2019). To do this, the original GAP data release was used as a tuning set to learn a classifier on top of BERT to predict whether the target pronoun referred to Name A, Name B, or Neither. Abzaliev (2019) also made use of the available datasets for coreference resolution: OntoNotes 5.0 (Pradhan and Xue, 2009), WinoBias (Zhao et al., 2018), WinoGender (Rudinger et al., 2018), and the Definite Pronoun Resolution Dataset (Rahman and Ng, 2012). Given the multiple BERT models available, it was possible to learn multiple such classifiers; teams marked ensemble fine-tuned multiple base BERT models and ensembled their predictions, while teams marked single produced just one, from a BERT-Large variant. An alternative way to use BERT in NLP modeling is as a feature extractor. Teams using BERT in this capacity represented mention spans as input vectors to a neural structure (typically a linear structure, e.g. feed-forward network) that learned some sort of mention compatibility, via interaction or feature crossing. To derive mention-span"
W19-3801,N18-2002,0,0.0607398,"Missing"
W19-3801,Q19-1026,0,0.02881,"ering a wide diversity of geographic locations and affiliations, see Section 3.1. Table 1 lists results for the three prize-winning systems: Attree (2019), Wang (2019), and Abzaliev (2019). 3 All system descriptions were from teams who used BERT (Devlin et al., 2019), a method to create context-sensitive word embeddings by pretraining a deep self-attention neural network on a training objective optimizing for cloze word prediction and recognition of adjacent sentences. This is perhaps not surprising, given the recent success of BERT for modeling a wide range of NLP tasks (Tenney et al., 2019; Kwiatkowski et al., 2019) and the small amount of training data available for GAP resolution (which makes LM pretraining particularly attractive). The different models built from BERT are summarized in Table 3. Eight of the eleven system descriptions used BERT via fine-tuning, the technique recommended in Devlin et al. (2019). To do this, the original GAP data release was used as a tuning set to learn a classifier on top of BERT to predict whether the target pronoun referred to Name A, Name B, or Neither. Abzaliev (2019) also made use of the available datasets for coreference resolution: OntoNotes 5.0 (Pradhan and Xue"
W19-3801,D18-1334,1,0.824962,"tics) that is alarming the Natural Language Processing (NLP) community. An illustration of the problematic behaviour are the recurrently appearing occupational stereotypes that homemaker is to woman as programmer is to man (Bolukbasi et al., 2016). Recent studies have aimed to detect, analyse and mitigate gender bias in different NLP tools and applications including word embeddings (Bolukbasi et al., 2016; Gonen and Goldberg, 2019), coreference resolution (Rudinger et al., 2018; Zhao et al., 2018), sentiment analysis (Park et al., 2018; Bhaskaran and Bhallamudi, 2019) and machine translation (Vanmassenhove et al., 2018; Font and Costa-juss`a, 2019). One of the main sources of gender bias is believed to be societal artefacts in the data from which our algorithms learn. To address this, many have created gender-labelled and gender-balanced datasets (Rudinger et al., 2018; Zhao et al., 2018; Vanmassenhove et al., 2018). We present the results of a shared task evaluation conducted at the 1st Workshop on Gender Bias in Natural Language Processing at the 2 Task The goal of our shared task was to encourage research in gender-fair models for NLP by providing a well-defined task that is known to be sensitive to gend"
W19-3801,W19-3818,0,0.0152269,"licity. Two other systems stood out as novel in their approach to the task: Chada (2019) reformulated GAP reference resolution as a question answering task, and Lois et al. (2019) used BERT in a third way, directly applying the masked language modeling task to predicting resolutions. Despite the scarcity of data for this challenge, there was little use of extra resources. Only two teams made use of the URL given in the example, with Attree (2019) using it only indirectly as part of a coreference heuristic fed into evidence pooling. Two teams augmented the GAP data by using name substitutions (Liu, 2019; Lois et al., 2019) and two automatically created extra examples of the minority label Neither (Attree, 2019; Bao and Qiao, 2019). 4 Discussion Running the GAP shared task has taught us many valuable things about reference, gender, and BERT models. Based on these, we make recommendations for future work expanding from this shared task into different languages and domains. GAP Given the incredibly strong performance of the submitted systems, it is tempting to ask whether GAP resolution is solved. We suggest the answer is no. Firstly, the shared task only tested one of the four original GAP set"
W19-3801,W19-3813,0,0.168496,"rkshop on Gender Bias in Natural Language Processing, pages 1–7 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics with separate scores for masculine and feminine examples. To simplify evaluation, we did not disaggregate evaluation for this shared task, but instead encouraged fairness by not releasing the balance of masculine to feminine examples in the final evaluation data.1 The competition was run on Kaggle2 , a wellknown platform for competitive data science and machine learning projects with an active community of participants and support. 2.1 Attree (2019) Wang (2019) Abzaliev (2019) F1 96.2 95.7 95.4 Bias 0.99 0.99 0.99 Table 1: Performance of prize-winning submissions on the blind Kaggle evaluation set. logloss was the official task metric, and correlates well with F1 score, which was used in the original GAP work. 760 clean examples was dispersed in a larger set of 11,599 unlabeled examples to produce a set of 12,359 examples that competing systems had to rate. This augmentation was to discourage submissions based on manual labeling. We note many competing systems used the original GAP evaluation data4 as training data for this task, given that the two"
W19-3801,W19-3811,1,0.859601,"Missing"
W19-3801,Q18-1042,1,0.937536,"near-human accuracy while achieving near gender-parity at 0.99, measured by the ratio between F1 scores on feminine and masculine examples. We are excited for future work extending this success to more languages, domains, and tasks. However, we especially note future work in algorithms which achieve fair outcomes given biased data, given the wealth of information from existing unbalanced datasets. The 1st ACL workshop on Gender Bias in Natural Language Processing included a shared task on gendered ambiguous pronoun (GAP) resolution. This task was based on the coreference challenge defined in Webster et al. (2018), designed to benchmark the ability of systems to resolve pronouns in real-world contexts in a gender-fair way. 263 teams competed via a Kaggle competition, with the winning system achieving logloss of 0.13667 and near gender parity. We review the approaches of eleven systems with accepted description papers, noting their effective use of BERT (Devlin et al., 2019), both via fine-tuning and for feature extraction, as well as ensembling. 1 Introduction Gender bias is one of the typologies of social bias (e.g. race, politics) that is alarming the Natural Language Processing (NLP) community. An i"
W19-3801,W19-3814,0,0.0511122,"Missing"
W19-3801,N18-2003,0,0.139092,"traction, as well as ensembling. 1 Introduction Gender bias is one of the typologies of social bias (e.g. race, politics) that is alarming the Natural Language Processing (NLP) community. An illustration of the problematic behaviour are the recurrently appearing occupational stereotypes that homemaker is to woman as programmer is to man (Bolukbasi et al., 2016). Recent studies have aimed to detect, analyse and mitigate gender bias in different NLP tools and applications including word embeddings (Bolukbasi et al., 2016; Gonen and Goldberg, 2019), coreference resolution (Rudinger et al., 2018; Zhao et al., 2018), sentiment analysis (Park et al., 2018; Bhaskaran and Bhallamudi, 2019) and machine translation (Vanmassenhove et al., 2018; Font and Costa-juss`a, 2019). One of the main sources of gender bias is believed to be societal artefacts in the data from which our algorithms learn. To address this, many have created gender-labelled and gender-balanced datasets (Rudinger et al., 2018; Zhao et al., 2018; Vanmassenhove et al., 2018). We present the results of a shared task evaluation conducted at the 1st Workshop on Gender Bias in Natural Language Processing at the 2 Task The goal of our shared task wa"
W19-3801,W19-3815,0,0.0806747,"T in this capacity represented mention spans as input vectors to a neural structure (typically a linear structure, e.g. feed-forward network) that learned some sort of mention compatibility, via interaction or feature crossing. To derive mention-span representations from BERT subtoken encodings, Wang (2019) found that pooling using an attentionmediated process was more effective than simple mean-pooling; most teams pooled using AllenAI’s SelfAttentionSpanExtractor5 . An interesting finding was that certain BERT layers were more suitable for feature extraction than others (see Abzaliev (2019); Yang et al. (2019) for an exploration). The winning solution (Attree, 2019) used a Submissions In this section, we describe the diverse set of teams who competed in the shared task, and the systems they designed for the GAP challenge. We note effective use of BERT (Devlin et al., 2019), both via fine-tuning and for feature extraction, and ensembling. Despite very little modeling targeted at debiasing for gender, the submitted systems narrowed the gender gap to near parity at 0.99, while achieving remarkably strong performance. 3.1 Systems Teams We accepted ten system description papers, from 11 of the 263 teams"
W19-3805,W19-3821,1,0.809984,"Missing"
W19-3805,W19-3621,0,0.478958,"man to computer programmer as woman to homemaker (Bolukbasi et al., 2016). Pre-trained word embeddings are used in many NLP downstream tasks, such as natural language inference (NLI), machine translation (MT) or question answering (QA). Recent progress in word embedding techniques has been achieved with contextualized word embeddings (Peters et al., 2018) which provide different vector representations for the same word in different contexts. While gender bias has been studied, detected and partially addressed for standard word embeddings techniques (Bolukbasi et al., 2016; Zhao et al., 2018a; Gonen and Goldberg, 2019), it is not the case for the latest techniques of contextualized word embeddings. Only just recently, Zhao et al. (2019) present a first analysis on the topic based on the proposed methods in Bolukbasi et al. (2016). In this paper, we further analyse the presence of gender biases in contextualized word embeddings by means of the proposed methods in Gonen and Goldberg (2019). For this, in section 2 we provide an overview of the relevant work on which we build our analysis; in section 3 we state the specific request questions addressed in this work, while in section 4 we describe the experimenta"
W19-3805,P82-1020,0,0.802369,"Missing"
W19-3805,P18-1031,0,0.0184763,"en words. This way, subtracting the vector representations of two related words and adding the result to a third word, results in a representation that is close to the application of the semantic relationship between the two first words to the third one. This application of analogical relationships have been used to showcase the bias present in word embeddings, with the prototypical example that when subtracting the vector representation of man from that of computer and adding it to woman, we obtain homemaker. 2.2 2.3 Contextualized Word Embeddings Pretrained Language Models (LM) like ULMfit (Howard and Ruder, 2018), ELMo (Peters et al., 2018), OpenAI GPT (Radford, 2018; Radford et al., 2019) and BERT (Devlin et al., 2018), proposed different neural language model architectures and made their pre-trained weights available to ease the application of transfer learning to downstream tasks, where they have pushed the state-of-the-art for several benchmarks including question answering on SQuAD, NLI, cross-lingual NLI and named identity recognition (NER). While some of these pre-trained LMs, like BERT, use subword level tokens, ELMo provides word-level representations. Peters et al. (2019) and Liu et al. (201"
W19-3805,D14-1162,0,0.103881,"ector space. Word embeddings representation spaces are known to present geometrical phenomena mimicking relations and analogies between words (e.g. man is to 2 Background In this section we describe the relevant NLP techniques used along the paper, including word embeddings, their debiased version and contextualized word representations. 33 Proceedings of the 1st Workshop on Gender Bias in Natural Language Processing, pages 33–39 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 2.1 Words Embeddings Zhao et al. (2018b) proposed an extension to GloVe embeddings (Pennington et al., 2014) where the loss function used to train the embeddings is enriched with terms that confine the gender information to a specific portion of the embedded vector. The authors refer to these pieces of information as protected attributes. Once the embeddings are trained, the gender protected attribute can be simply removed from the vector representation, therefore eliminating any gender bias present in it. The transformations proposed by both Bolukbasi et al. (2016) and Zhao et al. (2018b) are downstream task-agnostic. This fact is used in the work of Gonen and Goldberg (2019) to showcase that, whil"
W19-3805,S18-2005,0,0.0465688,"ng, in general and in natural language processing (NLP) applications in particular, are raising the alarm of the scientific community. Examples of these biases are evidences such that face recognition systems or speech recognition systems work better for white men than for ethnic minorities (Buolamwini and Gebru, 2018). Examples in the area of NLP are the case of machine translation that systems tend to ignore the coreference information in benefit of a stereotype (Font and Costa-juss`a, 2019) or sentiment analysis where higher sentiment intensity prediction is biased for a particular gender (Kiritchenko and Mohammad, 2018). In this work we focus on the particular NLP area of word embeddings (Mikolov et al., 2010), which represent words in a numerical vector space. Word embeddings representation spaces are known to present geometrical phenomena mimicking relations and analogies between words (e.g. man is to 2 Background In this section we describe the relevant NLP techniques used along the paper, including word embeddings, their debiased version and contextualized word representations. 33 Proceedings of the 1st Workshop on Gender Bias in Natural Language Processing, pages 33–39 c Florence, Italy, August 2, 2019."
W19-3805,N18-1202,0,0.725446,"sas Universitat Polit`ecnica de Catalunya {christine.raouf.saad.basta,marta.ruiz,noe.casas}@upc.edu Abstract woman as king is to queen). Following this property of finding relations or analogies, one popular example of gender bias is the word association between man to computer programmer as woman to homemaker (Bolukbasi et al., 2016). Pre-trained word embeddings are used in many NLP downstream tasks, such as natural language inference (NLI), machine translation (MT) or question answering (QA). Recent progress in word embedding techniques has been achieved with contextualized word embeddings (Peters et al., 2018) which provide different vector representations for the same word in different contexts. While gender bias has been studied, detected and partially addressed for standard word embeddings techniques (Bolukbasi et al., 2016; Zhao et al., 2018a; Gonen and Goldberg, 2019), it is not the case for the latest techniques of contextualized word embeddings. Only just recently, Zhao et al. (2019) present a first analysis on the topic based on the proposed methods in Bolukbasi et al. (2016). In this paper, we further analyse the presence of gender biases in contextualized word embeddings by means of the p"
W19-3805,N19-1112,0,0.0190547,"nd Ruder, 2018), ELMo (Peters et al., 2018), OpenAI GPT (Radford, 2018; Radford et al., 2019) and BERT (Devlin et al., 2018), proposed different neural language model architectures and made their pre-trained weights available to ease the application of transfer learning to downstream tasks, where they have pushed the state-of-the-art for several benchmarks including question answering on SQuAD, NLI, cross-lingual NLI and named identity recognition (NER). While some of these pre-trained LMs, like BERT, use subword level tokens, ELMo provides word-level representations. Peters et al. (2019) and Liu et al. (2019) confirmed the viability of using ELMo representations directly as features for downstream tasks without re-training the full model on the target task. Unlike word2vec vector representations, which are constant regardless of their context, ELMo representations depend on the sentence where the word appears, and therefore the full model has to be fed with each whole sentence to get the word representations. The neural architecture proposed in ELMo (Peters et al., 2018) consists of a character-level convolutional layer processing the characters of each word and creating a word representation that"
W19-3805,W19-4302,0,0.0122038,"LM) like ULMfit (Howard and Ruder, 2018), ELMo (Peters et al., 2018), OpenAI GPT (Radford, 2018; Radford et al., 2019) and BERT (Devlin et al., 2018), proposed different neural language model architectures and made their pre-trained weights available to ease the application of transfer learning to downstream tasks, where they have pushed the state-of-the-art for several benchmarks including question answering on SQuAD, NLI, cross-lingual NLI and named identity recognition (NER). While some of these pre-trained LMs, like BERT, use subword level tokens, ELMo provides word-level representations. Peters et al. (2019) and Liu et al. (2019) confirmed the viability of using ELMo representations directly as features for downstream tasks without re-training the full model on the target task. Unlike word2vec vector representations, which are constant regardless of their context, ELMo representations depend on the sentence where the word appears, and therefore the full model has to be fed with each whole sentence to get the word representations. The neural architecture proposed in ELMo (Peters et al., 2018) consists of a character-level convolutional layer processing the characters of each word and creating a wo"
W19-3805,N19-1064,0,0.412756,"nstream tasks, such as natural language inference (NLI), machine translation (MT) or question answering (QA). Recent progress in word embedding techniques has been achieved with contextualized word embeddings (Peters et al., 2018) which provide different vector representations for the same word in different contexts. While gender bias has been studied, detected and partially addressed for standard word embeddings techniques (Bolukbasi et al., 2016; Zhao et al., 2018a; Gonen and Goldberg, 2019), it is not the case for the latest techniques of contextualized word embeddings. Only just recently, Zhao et al. (2019) present a first analysis on the topic based on the proposed methods in Bolukbasi et al. (2016). In this paper, we further analyse the presence of gender biases in contextualized word embeddings by means of the proposed methods in Gonen and Goldberg (2019). For this, in section 2 we provide an overview of the relevant work on which we build our analysis; in section 3 we state the specific request questions addressed in this work, while in section 4 we describe the experimental framework proposed to address them and in section 5 we present the obtained and discuss the results; finally, in secti"
W19-3805,N18-2003,0,0.20423,"association between man to computer programmer as woman to homemaker (Bolukbasi et al., 2016). Pre-trained word embeddings are used in many NLP downstream tasks, such as natural language inference (NLI), machine translation (MT) or question answering (QA). Recent progress in word embedding techniques has been achieved with contextualized word embeddings (Peters et al., 2018) which provide different vector representations for the same word in different contexts. While gender bias has been studied, detected and partially addressed for standard word embeddings techniques (Bolukbasi et al., 2016; Zhao et al., 2018a; Gonen and Goldberg, 2019), it is not the case for the latest techniques of contextualized word embeddings. Only just recently, Zhao et al. (2019) present a first analysis on the topic based on the proposed methods in Bolukbasi et al. (2016). In this paper, we further analyse the presence of gender biases in contextualized word embeddings by means of the proposed methods in Gonen and Goldberg (2019). For this, in section 2 we provide an overview of the relevant work on which we build our analysis; in section 3 we state the specific request questions addressed in this work, while in section 4"
W19-3805,D18-1521,0,0.341457,"association between man to computer programmer as woman to homemaker (Bolukbasi et al., 2016). Pre-trained word embeddings are used in many NLP downstream tasks, such as natural language inference (NLI), machine translation (MT) or question answering (QA). Recent progress in word embedding techniques has been achieved with contextualized word embeddings (Peters et al., 2018) which provide different vector representations for the same word in different contexts. While gender bias has been studied, detected and partially addressed for standard word embeddings techniques (Bolukbasi et al., 2016; Zhao et al., 2018a; Gonen and Goldberg, 2019), it is not the case for the latest techniques of contextualized word embeddings. Only just recently, Zhao et al. (2019) present a first analysis on the topic based on the proposed methods in Bolukbasi et al. (2016). In this paper, we further analyse the presence of gender biases in contextualized word embeddings by means of the proposed methods in Gonen and Goldberg (2019). For this, in section 2 we provide an overview of the relevant work on which we build our analysis; in section 3 we state the specific request questions addressed in this work, while in section 4"
W19-3811,N06-2015,0,0.136549,"Missing"
W19-3811,Q18-1042,0,0.0610365,"odel 2. replacement names 1 and 2 are switched. So, as figure 3 shows, we get one text with each name in each position. For example lets say we get the text: ”In the late 1980s Jones began working with 78 3 Experimental Framework 3.1 Task details Name A Name B None The objective of the task is that of a classification problem. Where the output for every entry is the probability of the pronoun referencing name A, name B or Neither. 3.2 Stage 1 Train 1105 1060 289 Test 874 925 201 Stage 2 Train 1979 1985 490 Table 1: Dataset distribution for the datasets of stages 1 and 2. Data The GAP dataset (Webster et al., 2018) created by Google AI Language was the dataset used for this task. This dataset consists of 8908 co-reference labeled pairs sampled from Wikipedia, also it’s split perfectly between male and female representation. Each entry of the dataset consists of a short text, a pronoun that is present in the text and its offset and two different names (name A and name B) also present in the text. The pronoun refers to one of these two names and in some cases, none of them. The GAP dataset doesn’t contain any neutral pronouns such as it or they. For the two different stages of the competition different da"
W19-3821,P02-1040,0,0.103891,"words. Finally, a set of crowdsourced male-female equalization pairs such as dad-mom, boy-girl, granpa-grandma that represent gender direction are equalized in the algorithm. In fact, for the English side, the gendered pairs used are the same as identified in the crowdsourcing test by Bolukbasi et al. (2016). For the Spanish side, the sets are translated manually and modified when necessary to avoid non-applicable pairs or unnecessary repetitions. The sets from Zhao et al. (2018b) are similarly adapted to the Spanish language. To evaluate the performance of the models we use the BLEU metric (Papineni et al., 2002). This metric gives a score for a predicted translation set compared to its expected output. word “friend” that can be translated into any of the two words and we are adding context in the same sentence so that the system has enough information to translate them correctly. The list of occupations is from the U.S. Bureau of Labor Statistics2 , which also includes statistical data for gender and race for most professions. We use a pre-processed version of this list from (Prates et al., 2018). 5.2 Models The architecture to train the models for the translation task is the Transformer (Vaswani et"
W19-3821,D14-1162,0,0.0912406,"ring in same contexts share semantic meaning, this continuous vector space representation gathers semantically similar words, thus being more expressive than other discrete representations like one-hot vectors. Arithmetic operations can be performed with these embeddings, in order to find analogies between pairs of nouns with the pattern “A is to B what C is to D” (Mikolov et al., 2013). For nouns, such as countries and their respective capitals or for the conjugations of verbs. While there are many techniques for extracting word embeddings, in this work we are using Global Vectors, or GloVe (Pennington et al., 2014). Glove is an unsupervised method for learning word embeddings. This count-based method, uses statistical information of word occurrences from a given corpus to train a vector space for which each vector is related to a word and their values describes their semantic relations. Background This section presents the models used in this paper. First, we describe the Transformer model which is the state-of-the-art model in MT. Second, we report describe word embeddings and, then, the corresponding techniques to debias them. 2.1 Word embeddings Transformer The Transformer (Vaswani et al., 2017) is a"
W19-3821,N18-2084,0,0.020013,"32M 10k 9k 0.8k Train Dev Test Occupations test 16.6M 3k 3k 1k 477.3M 79k 71k 17k 1.37M 12k 11k 0.8k Table 2: English-Spanish data set. Parameter Vector size Memory Vocab. min. count Max. iter. Window size Num. threads X max. Binary Verbose Value Baseline 512 4.0 5 15 15 8 10 2 2 Pre-trained emb. Enc. Dec. Enc./Dec. GloVe GloVe Hard-Deb. GN-GloVe 30.21 30.16 29.12 30.24 30.09 30.13 30.62 29.95 30.74 Table 4: BLEU scores for the newstest2013 test set. English-Spanish. Pre-trained embeddings are updated during training. In bold best results. translation, which is coherent with previous studies (Qi et al., 2018). Furthermore, debiasing with GN-GloVe embeddings keeps this improvement and even increases it when used in both the encoder and decoder sides. We want to underline that these models do not decrease the quality of translation in terms of BLEU when tested in a standard MT task. Next, we show how each of the models performs on a gender debiasing task. Table 3: Word Embeddings Parameters. 6.1 29.78 Translation For the test set newstest2013, BLUE scores are given in Table 4. Pre-trained embeddings are used for training in three scenarios: in the encoder side (Enc.), in the decoder side (Dec.) and"
W19-3821,2005.mtsummit-papers.11,0,0.0361013,"on correference and stereotypes to evaluate the effectiveness of our technique. 4 5 Experimental framework In this section, we present the experimental framework. We report details on the training of the word embeddings and the translation system. We describe the data related to the training corpus and test sets and the parameters. Also, we comment on the use of computational resources. 5.1 Corpora The language pair used for the experiments is English-Spanish. The training set consists of 16,554,790 sentences from a variety of sources including United Nations (Ziemski et al., 2016), Europarl (Koehn, 2005), CommonCrawl and News available from the Workshop on Machine Translation (WMT) 1 . The validation and test sets used are the newstest2012 (3,003 sentences) and newstest2013 (3,000 sentences), respectively, also from the same WMT workshop. See Table 2 for the corpus statistics. To study gender bias, we have developed an additional test set with custom sentences to evaluate the quality of the translation in the models. We built this test set using a sentence pattern “I’ve known {her, him, &lt;proper noun&gt;} for a long time, my friend works as {a, an} &lt;occupation&gt;.” for a list of occupations from di"
W19-3821,D18-1334,0,0.175352,"many NLP applications, studies of this type in MT are quite limited. Prates et al. (2018) performs a case study on gender bias in machine translation. They build a test set consisting of a list of jobs and genderspecific sentences. Using English as a target language and a variety of gender neutral languages as a source, i.e. languages that do not explicitly give gender information about the subject, they test these sentences on the translating service Google Translate. They find that occupations related to science, engineering and mathematics present a strong stereotype toward male subjects. Vanmassenhove et al. (2018) compile a large multilingual dataset on the politics domain that contains the speaker information. They specifically use this information to incorporate it in a MT system. Adding this information improves the translation quality. Our contribution is different from previous approaches in the sense that we are explicitly proposing a gender-debiased approach for NMT as well as an specific analysis based on correference and stereotypes to evaluate the effectiveness of our technique. 4 5 Experimental framework In this section, we present the experimental framework. We report details on the trainin"
W19-3821,D18-1521,0,0.470671,"s such as attention (Bahdanau et al., 2014) and translation systems algorithms like the Transformer (Vaswani et al., 2017). One downside of models trained with human generated corpora is that social biases and stereotypes from the data are learned (Madaan et al., 2018). A systematic way of showing this bias is by means of word embeddings, a vector representation of words. The presence of biases, such as gender bias, is studied for these representations and evaluated on crowd-sourced tests (Bolukbasi et al., 2016). The presence of biases in the data can directly impact downstream applications (Zhao et al., 2018a) and are at risk of being amplified (Zhao et al., 2017). The objective of this work is to study the presence of gender bias in MT and give insight on the impact of debiasing in such systems. An example of this gender bias is the word “friend” in the English sentence “She works in a hospital, my friend is a nurse” would be correctly translated to “amiga” (girl friend in Spanish) in Spanish, while “She works in a hospital, my friend is a doctor” would be incorrectly translated to “amigo” (boy friend in Spanish) in Spanish. We consider that this translation contains gender bias since it ignores"
W19-3821,D17-1323,0,0.097119,"ion systems algorithms like the Transformer (Vaswani et al., 2017). One downside of models trained with human generated corpora is that social biases and stereotypes from the data are learned (Madaan et al., 2018). A systematic way of showing this bias is by means of word embeddings, a vector representation of words. The presence of biases, such as gender bias, is studied for these representations and evaluated on crowd-sourced tests (Bolukbasi et al., 2016). The presence of biases in the data can directly impact downstream applications (Zhao et al., 2018a) and are at risk of being amplified (Zhao et al., 2017). The objective of this work is to study the presence of gender bias in MT and give insight on the impact of debiasing in such systems. An example of this gender bias is the word “friend” in the English sentence “She works in a hospital, my friend is a nurse” would be correctly translated to “amiga” (girl friend in Spanish) in Spanish, while “She works in a hospital, my friend is a doctor” would be incorrectly translated to “amigo” (boy friend in Spanish) in Spanish. We consider that this translation contains gender bias since it ignores the fact that, for both cases, “friend” is a female and"
W19-3821,L16-1561,0,0.01719,"ll as an specific analysis based on correference and stereotypes to evaluate the effectiveness of our technique. 4 5 Experimental framework In this section, we present the experimental framework. We report details on the training of the word embeddings and the translation system. We describe the data related to the training corpus and test sets and the parameters. Also, we comment on the use of computational resources. 5.1 Corpora The language pair used for the experiments is English-Spanish. The training set consists of 16,554,790 sentences from a variety of sources including United Nations (Ziemski et al., 2016), Europarl (Koehn, 2005), CommonCrawl and News available from the Workshop on Machine Translation (WMT) 1 . The validation and test sets used are the newstest2012 (3,003 sentences) and newstest2013 (3,000 sentences), respectively, also from the same WMT workshop. See Table 2 for the corpus statistics. To study gender bias, we have developed an additional test set with custom sentences to evaluate the quality of the translation in the models. We built this test set using a sentence pattern “I’ve known {her, him, &lt;proper noun&gt;} for a long time, my friend works as {a, an} &lt;occupation&gt;.” for a lis"
W19-3821,D18-1302,0,\N,Missing
W19-5301,W19-5424,1,0.858444,"Missing"
W19-5301,W19-5306,0,0.248769,"al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated"
W19-5301,D18-1549,0,0.116951,"s are available for this system. 2.5.7 BASELINE - RE - RERANK (no associated CUNI-T RANSFORMER -T2T2018 (Popel, 2018) is the exact same system as used last year. paper) BASELINE - RE - RERANK is a standard Transformer, with corpus filtering, pre-processing, postprocessing, averaging and ensembling as well as n-best list reranking. 2.5.8 CUNI-T RANSFORMER -M ARIAN (Popel et al., 2019) is a “reimplementation” of the last year’s system (Popel, 2018) in Marian (JunczysDowmunt et al., 2018). CA I RE (Liu et al., 2019) CUNI-U NSUPERVISED -NER- POST (Kvapilíková et al., 2019) follows the strategy of Artetxe et al. (2018), creating a seed phrase-based system where the phrase table is initialized from cross-lingual embedding mappings trained on monolingual data, followed by a neural machine translation system trained on synthetic parallel corpus. The synthetic corpus is produced by the seed phrase-based MT system or by a such a model refined through iterative back-translation. CUNI-U NSUPERVISED -NER- POST further focuses on the handling of named entities, i.e. the part of vocabulary where the cross-lingual embedding mapping suffer most. CA I RE is a hybrid system that took part only in the unsupervised track."
W19-5301,D18-1332,0,0.0215805,"et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔English translation task are that character-level model on the Chinese side can be used when translating into Chinese to improve the BLEU score. The same does not hold when transl"
W19-5301,W19-5351,0,0.0505622,"Missing"
W19-5301,W19-5423,0,0.0419015,"Missing"
W19-5301,W18-6412,1,0.856623,"Missing"
W19-5301,W19-5305,0,0.0496912,"Missing"
W19-5301,W12-3102,1,0.474924,"Missing"
W19-5301,W19-5310,0,0.0432396,"Missing"
W19-5301,W19-5425,0,0.0236032,"ys-Dowmunt et al., 2018) and Phrase-based machine translation system (implemented with Moses) and for the Spanish-Portuguese task. The system combination included features formerly presented in (Marie and Fujita, 2018), including scores left-to-right and right-to-left, sentence level translation probabilities and language model scores. Also authors provide contrastive results with an unsupervised phrase-based MT system which achieves quite close results to their primary system. Authors associate high performance of the unsupervised system to the language similarity. Incomslav: Team INCOMSLAV (Chen and Avgustinova, 2019) by Saarlad University participated in the Czech to Polish translation task only. The team’s primary submission builds on a transformer-based NMT baseline with back translation which has been submitted one of their contrastive submission. Incomslav’s primary system is a phoneme-based system re-scored using their NMT baseline. A second contrastive submission builds our phrase-based SMT system combined with a joint BPE model. NITS-CNLP: The NITS-CNLP team (Laskar et al., 2019) by the National Institute of Technology Silchar in India submitted results to the HI-NE translation task in both directi"
W19-5301,W07-0718,1,0.530103,"ojar Charles University Yvette Graham Barry Haddow Dublin City University University of Edinburgh Philipp Koehn JHU / University of Edinburgh Mathias Müller University of Zurich Marta R. Costa-jussà Christian Federmann UPC Microsoft Cloud + AI Shervin Malmasi Harvard Medical School Santanu Pal Saarland University Matt Post JHU Abstract Introduction The Fourth Conference on Machine Translation (WMT) held at ACL 20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to tra"
W19-5301,P16-2058,1,0.819865,"ipated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transformer (implemented with Fairseq (Ott et al., 2019)) for the Czechto-Polish task and a Phrase-based system (implemented with Moses (Koehn et al., 2007)) for Spanish-to-Portuguese. They tested adding monolingual data to the NMT system by copying the same data on the source and target sides, with negative results. Also, their system combination based on sentence-level BLEU in back-translation 5.4 Conclusion of Simi"
W19-5301,W08-0309,1,0.659809,"Missing"
W19-5301,W18-3931,1,0.820211,"or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language pairs: Spanish - Portuguese (Romance languages), Czech - Polis"
W19-5301,W19-5312,0,0.0773587,"Missing"
W19-5301,W19-5313,0,0.0931699,"University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of"
W19-5301,W19-5314,0,0.0200465,"Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 2.5.6 BTRANS only the middle sentence was considered for the final translation hypothesis, otherwise shorter context of two sentences or just a single sentence was used. Unfortunately, no details are available for this system. 2.5.7 BASELINE - RE - RERANK (no associ"
W19-5301,D18-1045,0,0.0285693,"xt was morphologically segmented with Apertium. The UEDIN systems are supervised NMT systems based on the transformer architecture and trained using Marian (Junczys-Dowmunt et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔Engl"
W19-5301,W18-6410,0,0.0193718,"ormance can be found in Hindi-Nepali (both directions), where the best performing system is around 50 BLEU (53 for Hindi-to-Nepali and 49.1 for Nepali-toHindi), and the lowest entry is 1.4 for Hindi-toNepali and 0 for Nepali-to-Hindi. The lowest variance is for Polish-to-Czech and it may be because only two teams participated. UHelsinki: The University of Helsinki team (Scherrer et al., 2019) participated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transform"
W19-5301,W19-5317,0,0.114089,"ed paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 201"
W19-5301,W19-5315,0,0.0266353,"Missing"
W19-5301,W19-5318,0,0.198338,"ance of the systems when translating from French to German seems to heavily depend on the 7 http://data.statmt.org/wmt19/ translation-task/dev.tgz 6 Systems MSRA.MADL eTranslation LIUM MLLP-UPV onlineA TartuNLP onlineB onlineY onlineG onlineX FULL 47.3 45.4 43.7 41.5 40.8 39.2 39.1 39.0 38.5 38.1 source FR 38.3 37.4 37.5 36.4 35.4 34.8 35.3 34.7 34.6 35.6 source DE 50.0 47.8 45.5 43.0 42.3 40.5 40.2 40.2 39.7 38.8 evaluations. In the rest of this sub-section, we provide brief details of the submitted systems, for those in cases where the authors provided such details. 2.5.1 AFRL - SYSCOMB 19 (Gwinnup et al., 2019) is a system combination of a Marian ensemble system, two distinct OpenNMT systems, a Sockeyebased Elastic Weight Consolidation system, and one Moses phrase-based system. Table 3: French→German Meteor scores. Systems MSRA.MADL LinguaCustodia MLLP_UPV Kyoto_University_T2T LIUM onlineY onlineB TartuNLP onlineA onlineX onlineG FULL 52.0 51.3 49.5 48.8 48.3 47.5 46.4 46.3 45.3 42.7 41.7 source FR 51.9 52.5 49.9 49.7 46.5 43.7 43.7 45.0 43.7 41.6 40.9 source DE 52.0 51.0 49.4 48.6 48.7 48.4 47.0 46.7 45.8 42.9 41.9 AFRL- EWC (Gwinnup et al., 2019) is a Sockeye Transformer system trained with the de"
W19-5301,W19-5316,0,0.109691,"boratory (Gwinnup et al., 2019) Apertium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of"
W19-5301,E14-1047,1,0.904335,"Missing"
W19-5301,W19-5427,0,0.0467733,"Missing"
W19-5301,W19-5322,1,0.807321,"Missing"
W19-5301,W19-5302,1,0.715203,"20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We 1 Christof Monz University of Amsterdam Marcos Zampieri University of Wolverhampton held 18 translation tasks this year, between English and each of Chinese, Czech (into Czech only), German, Finnish, Lithuanian, and Russian. New this year were Gujarati↔English and Kazakh↔English. B"
W19-5301,W19-5333,0,0.0923169,"Missing"
W19-5301,W19-5353,0,0.0658382,"Missing"
W19-5301,W19-5430,1,0.873847,"Missing"
W19-5301,W19-5431,0,0.0199622,"Universitat Politècnica de València (UPV) participated with a Transformer (implemented with FairSeq (Ott et al., 2019)) and a finetuning strategy for domain adaptaion in the task of Spanish-Portuguese. Fine-tunning on the development data provide improvements of almost 12 BLEU points, which may explain their clear best performance in the task for this language pair. As a contrastive system authors provided only for the Portuguese-to-Spanish a novel 2D alternating RNN model which did not respond so well when fine-tunning. UBC-NLP: Team UBC-NLP from the University of British Columbia in Canada (Przystupa and Abdul-Mageed, 2019) compared the performance of the LSTM plus attention (Bahdanau et al., 2015) and Transformer (Vaswani et al., 2017) (implemented in OpenNMT toolkit22 ) perform for the three tasks at hand. Authors use backtranslation to introduce monolingual data in their systems. LSTM plus attention outperformed Transformer for Hindi-Nepali, and viceversa for the other two tasks. As reported by the authors, Hindi-Nepali task provides much more shorter sentences than KYOTOUNIVERSITY: Kyoto University’s submission, listed simply as KYOTO in Table 25 for PT → ES task is based on transformer NMT system. They used"
W19-5301,P02-1040,0,0.11337,"ation of the source (CS), and a second encoder to encode sub-word (byte-pair-encoding) information of the source (CS). The results obtained by their system in translating from Czech→Polish and comment on the impact of out-of-domain test data in the performance of their system. UDSDFKI ranked second among ten teams in Czech– Polish translation. 5.3 Results We present results for the three language pairs, each of them in the two possible directions. For this first edition of the Similar Translation Task and differently from News task, evaluation was only performed on automatic basis using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) measures. Each language direction is reported in one different table which contain information of the team; type of system, either contrastive (C) or primary (P), and the BLEU and TER results. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. Even if we are presenting 3 pairs of languages each pair belonging to the same family, translation quality in terms of BLEU varies signficantly. While the best systems for Spanish-Portuguese are above 64 BLEU and below 21 TER (see Tables 26 and 27), best syste"
W19-5301,W19-5354,0,0.0611791,"Missing"
W19-5301,W18-6486,0,0.0189853,"the agglutinative nature of Kazakh, (ii) data from an additional language (Russian), given the scarcity of English–Kazakh data and (iii) synthetic data for the source language filtered using language-independent sentence similarity. RUG _ KKEN _ MORFESSOR Tilde developed both constrained and unconstrained NMT systems for English-Lithuanian and Lithuanian-English using the Marian toolkit. All systems feature ensembles of four to five transformer models that were trained using the quasi-hyperbolic Adam optimiser (Ma and Yarats, 2018). Data for the systems were prepared using TildeMT filtering (Pinnis, 2018) and preprocessing (Pinnis et al., 2018) methods. For unconstrained systems, data were additionally filtered using dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018a). All systems were trained using iterative back-translation (Rikters, 2018) and feature synthetic data that allows training NMT systems to support handling of unknown phenomena (Pinnis et al., 2017). During translation, automatic named entity and nontranslatable phrase post-editing were performed. For constrained systems, named entities and nontranslatable phrase lists were extracted from the parallel training data."
W19-5301,W19-5335,0,0.0408704,"Missing"
W19-5301,W19-5344,1,0.904781,"n Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas e"
W19-5301,W19-5346,0,0.197908,"rtium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communicatio"
W19-5301,W19-5341,0,0.0172601,"A,B,G,X,Y. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human AYLIEN _ MULTILINGUAL (Hokamp et al., 2019) The Aylien research team built a Multilingual NMT system which is trained on all WMT2019 language pairs in all directions, then fine-tuned for a small number of iterations on Gujarati-English data only, including some self-backtranslated data. 2.5.5 BAIDU (Sun et al., 2019) Baidu systems are based on the Transformer architecture with several improvements. Data selection, back translation, data augmentation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. 7 Team AFRL A PERTIUM - FIN - ENG A PPRENTICE - C AYLIEN _ MULTILINGUAL BAIDU BTRANS BASELINE - RE - RERANK CA I RE CUNI DBMS-KU DFKI - NMT E T RANSLATION FACEBOOK FAIR GTCOM H ELSINKI NLP IIITH-MT IITP JHU JUMT JU_S AARLAND KSAI K YOTO U NIVERSITY L INGUA C USTODIA LIUM LMU-NMT MLLP-UPV MS T RANSLATOR MSRA N IU T RANS NICT NRC PARFDA"
W19-5301,P16-1162,1,0.310296,"ssible, 2.5.13 E T RANSLATION (Oravecz et al., 2019) E T RANSLATION En-De E T RANSLATION ’s EnDe system is an ensemble of 3 base Transformers and a Transformer-type language model, trained from all available parallel data (cleaned up and filtered with dual conditional cross-entropy filtering) and with additional back-translated data generated 9 2.5.17 from monolingual news. Each Transformer model is fine tuned on previous years’ test sets. H ELSINKI NLP is a Transformer (Vaswani et al., 2017) style model implemented in OpenNMTpy using a variety of corpus filtering techniques, truecasing, BPE (Sennrich et al., 2016), backtranslation, ensembling and fine-tuning for domain adaptation. E T RANSLATION Fr-De The Fr-De system is an ensemble of 2 big Transformers (with size 8192 FFN layers). Back-translation data was selected using topic modelling techniques to tune the model towards the domain defined in the task. 2.5.18 En-Lt The En-Lt system is an ensemble of 2 big Transformers (as for Fr-De) and a Transformer type language model. The training data contains the Rapid corpus and the news domain back-translated data sets 2 times oversampled. E T RANSLATION 2.5.19 FACEBOOK FAIR (Ng et al., 2019) 2.5.20 JHU (Mar"
W19-5301,W19-5339,0,0.0767002,"Missing"
W19-5301,W19-5347,0,0.0328257,"Missing"
W19-5301,W19-5342,1,0.887858,"encia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and"
W19-5301,W19-5355,1,0.869964,"Missing"
W19-5301,W19-5350,0,0.0441915,"Missing"
W19-5301,P98-2238,0,0.38957,"n trained to translate texts from and to English or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language"
W19-5311,D16-1026,0,0.148645,"Missing"
W19-5311,W17-4102,0,0.0231212,"Missing"
W19-5311,W08-0509,0,0.0461152,"re are some precedents for subword tokenization in SMT, like the work by Kunchukuttan and Bhattacharyya (2016, 2017). The use of subword tokenization leads to longer token sequence lengths compared to the usual word-based vocabularies of SMT systems. In order to cope with this fact, we configured the subword-based SMT systems to have longer ngram order for their Language Models (LM) and phrase tables: the typical n-gram order used is 3 and we used 6. All other Moses configuration settings are the standard ones, using KenLM as language model (Heafield, 2011; Heafield et al., 2013) and MGIZA++ (Gao and Vogel, 2008) for alignment. The data used to create the respective target-side LMs consisted of the target side of the parallel data used for training. Some improvement could have been gained by using the available extra monolingual English and Kazakh data for the LMs. pora based on the Russian data and then combine it with the parallel English-Kazakh data. Further justification of the technique used can be found in section 2. In pivoting approaches, the final translation quality does not get influenced significantly if synthetic data is used for the source language side; on the other hand, using syntheti"
W19-5311,N19-4009,0,0.0318804,"vel tokenization (SMT(w)): we trained a Moses system on the parallel Kazakh-English data, using normal word-level tokenization • Statistical Machine Translation with subword-level tokenization (SMT(sw)): we trained a Moses system on the parallel Kazakh-English data, using BPE tokenization with 10K merge operations2 . Moses default values were used for the rest of configuration settings . • Neural Machine Translation (NMT): we trained a Transformer model on the parallel Kazakh-English data, using BPE tokenization with 10K merge operations, separately for source and target. We used the fairseq (Ott et al., 2019) implementation with the same hyperparameters as the IWSLT model, namely an embedding dimensionality of 512, 6 layers of attention, 4 attention heads and 1024 for the feedwordward expansion dimensionality. The translation quality BLEU scores of the aforedescribed baselines were very low, as shown in table 4. In order to evaluate the pivot translation systems described in section 5.1, we also measured the BLEU scores in the respective held out test sets, obtaining 36.05 BLEU for the Russian→English system and 21.06 for the Russian→Kazakh system. With these pivot systems, we created two pseudo-p"
W19-5311,P02-1040,0,0.10634,"In order to evaluate the pivot translation systems described in section 5.1, we also measured the BLEU scores in the respective held out test sets, obtaining 36.05 BLEU for the Russian→English system and 21.06 for the Russian→Kazakh system. With these pivot systems, we created two pseudo-parallel synthetic corpora, merged them with the parallel data and trained a self-attention NMT model that obtained BLEU scores one order of magnitude above the chosen baselines, as shown in table 4. Experiments and Results In order to assess the translation quality of the systems, we computed the BLEU score (Papineni et al., 2002) over the respective held out test sets. As there is not much literature of current NMT approaches being applied to English-Kazakh, we prepared different baselines to gauge the range of BLEU values to expect: • Rule-based machine translation system (RBMT): we used the Apertium system (Forcada et al., 2011; Sundetova et al., 2014; Assem and Aida, 2013), which is based on transfer rules distilled from linguistic knowledge. Using the BLEU score to compare an 2 The low number of BPE merge operations is justified with the low amount of training data 159 When we tested the final Kazakh→English syste"
W19-5311,W11-2123,0,0.0175043,"2K merge operations each. Although not frequent, there are some precedents for subword tokenization in SMT, like the work by Kunchukuttan and Bhattacharyya (2016, 2017). The use of subword tokenization leads to longer token sequence lengths compared to the usual word-based vocabularies of SMT systems. In order to cope with this fact, we configured the subword-based SMT systems to have longer ngram order for their Language Models (LM) and phrase tables: the typical n-gram order used is 3 and we used 6. All other Moses configuration settings are the standard ones, using KenLM as language model (Heafield, 2011; Heafield et al., 2013) and MGIZA++ (Gao and Vogel, 2008) for alignment. The data used to create the respective target-side LMs consisted of the target side of the parallel data used for training. Some improvement could have been gained by using the available extra monolingual English and Kazakh data for the LMs. pora based on the Russian data and then combine it with the parallel English-Kazakh data. Further justification of the technique used can be found in section 2. In pivoting approaches, the final translation quality does not get influenced significantly if synthetic data is used for t"
W19-5311,W17-2619,0,0.0708846,"ion of a synthetic pseudo-parallel corpus of translated data between the source and target language through the pivot, and train a system as done in the back translation approach. Finally, multilingual systems are recently showing nice improvements. Among the different types of multilingual systems there are the many-to-one approaches and the many-to-many approaches. The former is aiming to translate to one single language and can simply concatenate source languages (Zoph and Knight, 2016; Tubay and Costajuss`a, 2018). However, the latter either needs to use independent encoders and decoders (Schwenk and Douze, 2017; Firat et al., 2016; Escolano et al., 2019) or when using universal encoder and decoders (Johnson et al., 2017) needs to add a tag in the source input to let the system know to which language it is translating. This many-to-many systems are an alternative to pivot systems. However, most these multilingual systems are not able to achieve the level of performance of pivot systems yet. In the frame of the WMT19 news translation shared task several of the aforementioned techniques are applicable. An English+Russian→Kakakh multilingual system could be trained, but the amount of Kazakh-Russian data"
W19-5311,P13-2121,0,0.0630838,"Missing"
W19-5311,P16-1009,0,0.339049,"age. This way, we used English-Russian and Kazakh-Russian data to train intermediate translation systems that we then 2 Low-resource NMT There are several different approaches that can improve translation quality in under-resourced scenarios. In this section, we provide an overview of some of the dominant techniques and justify their application in the frame of this shared task. While for low resource languages there is limited parallel data, monolingual data is often available in greater quantities. A common strategy to integrate this monolingual data into the NMT system is back-translation (Sennrich et al., 2016a), which consists in generating synthetic data by translating monolingual data of the target language into the source language that would be then fed to the system to further train it. 155 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 155–162 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics to this scenario. The cascade approach, however, would not allow to profit from the existing parallel English-Kazakh data, making the pseudo-parallel corpus approach the most sensible option. Another common s"
W19-5311,P16-1162,0,0.474151,"age. This way, we used English-Russian and Kazakh-Russian data to train intermediate translation systems that we then 2 Low-resource NMT There are several different approaches that can improve translation quality in under-resourced scenarios. In this section, we provide an overview of some of the dominant techniques and justify their application in the frame of this shared task. While for low resource languages there is limited parallel data, monolingual data is often available in greater quantities. A common strategy to integrate this monolingual data into the NMT system is back-translation (Sennrich et al., 2016a), which consists in generating synthetic data by translating monolingual data of the target language into the source language that would be then fed to the system to further train it. 155 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 155–162 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics to this scenario. The cascade approach, however, would not allow to profit from the existing parallel English-Kazakh data, making the pseudo-parallel corpus approach the most sensible option. Another common s"
W19-5311,P07-2045,0,0.0187399,"Missing"
W19-5311,W18-6449,1,0.89165,"Missing"
W19-5311,P07-1108,0,0.140785,"Missing"
W19-5311,N07-1061,0,0.0969472,"Missing"
W19-5311,L16-1561,0,0.0137195,"ipts for preprocessing, including tokenization, truecasing and cleaning, using the same settings as for the aggressive EnglishRussian data cleaning described before. From the combined corpus, we extracted 4000 lines as development data and 1000 segments as hold out test set, leaving the rest for training. The statistics of the resulting training corpus are shown in table 3. English-Russian The available parallel English-Russian corpora for the shared task included News Commentary v14, Wiki Titles v1, Common Crawl corpus, ParaCrawl v3, Yandex Corpus and the United Nations Parallel Corpus v1.0 (Ziemski et al., 2016). Following the rationale exposed for the EnglishKazakh Wiki Titles data, we also dropped the English-Russian Wiki Titles data. Among the other corpora, some are of very large size. In order to assemble a manageable final training dataset and taking into account the high presence of garbage in the crawled datasets, before combining the individual corpora, we filtered each corpus and selected from each a random sample of segments. For the filtering, we applied heuristic criteria based on our visual inspection of the data, including elimination of lines with repeated separation characters (like"
W19-5311,N16-1004,0,0.030125,"o the pivot to the target system, obtaining a source to target translation. An alternative to this approach could be the generation of a synthetic pseudo-parallel corpus of translated data between the source and target language through the pivot, and train a system as done in the back translation approach. Finally, multilingual systems are recently showing nice improvements. Among the different types of multilingual systems there are the many-to-one approaches and the many-to-many approaches. The former is aiming to translate to one single language and can simply concatenate source languages (Zoph and Knight, 2016; Tubay and Costajuss`a, 2018). However, the latter either needs to use independent encoders and decoders (Schwenk and Douze, 2017; Firat et al., 2016; Escolano et al., 2019) or when using universal encoder and decoders (Johnson et al., 2017) needs to add a tag in the source input to let the system know to which language it is translating. This many-to-many systems are an alternative to pivot systems. However, most these multilingual systems are not able to achieve the level of performance of pivot systems yet. In the frame of the WMT19 news translation shared task several of the aforementione"
W19-5418,L16-1470,0,0.782028,"Missing"
W19-5418,C18-1111,0,0.0156794,"ion Domain adaptation in Neural Machine Translation (NMT) remains one of the main challenges (Koehn and Knowles, 2017). Domain-specific translations are especially relevant for industrial applications where there is a need for achieving both fluency and terminology in translations. Current state-of-the-art NMT systems achieve high performances when trained with large-scale parallel corpora. However, most of the time, largescale parallel corpora are not available for specific domains. Consequently, NMT models perform poorly for domain-specific translation when trained in low-resource scenario (Chu and Wang, 2018). Several works have been proposed to overcome the lack of domain parallel data by leveraging on both monolingual domain data (Domhan and Hieber, 2017; Currey et al., 2017) and parallel out-of-domain data (Wang et al., 2017; van der Wees et al., 2017) to improve the performance of domain-specific systems. Furthermore, some attempts have been made to directly insert external knowledge into NMT models through termi2 BabelNet In our work, in order to collect biomedical terms, the domain category of each word was detected with the help of BabelNet (Navigli and Ponzetto, 2012). Specifically, we ext"
W19-5418,P02-1040,0,0.103767,"r sequence of tokens. It is also important to notice that all the terms that are not present in the terminology list, like ”hypertension” and ”clot” in the examples, might be split into subwords. These examples show how the effectiveness of bpe-term segmentation depends entirely on the size and quality of the terminology list. 4 5 Experiments This section describes the experiments we performed. We first start with the data collection and preprocessing processes. Then, we describe trained systems and their evaluations. Finally, we present the results of the competition in terms of BLEU score. (Papineni et al., 2002). 5.1 Data collection We gathered data from the resources provided in the official WMT19 web page and from the OPUS collection. For our submissions, all the available biomedical parallel sentences for en/es are chosen both in plain text and Dublin Core format. Then, data have been parsed and merged to create the training and validation sets. Finally, we cleaned the datasets by removing empty sentences and duplicates. In particular, we selected Scielo (Soares et al., 2018), (Neves et al., 2016), UFAL, Pubmed, Medline, IBECS (Villegas et al., 2018) and EMEA (Tiedemann, 2012) sources for the trai"
W19-5418,W17-4715,0,0.0137694,"t for industrial applications where there is a need for achieving both fluency and terminology in translations. Current state-of-the-art NMT systems achieve high performances when trained with large-scale parallel corpora. However, most of the time, largescale parallel corpora are not available for specific domains. Consequently, NMT models perform poorly for domain-specific translation when trained in low-resource scenario (Chu and Wang, 2018). Several works have been proposed to overcome the lack of domain parallel data by leveraging on both monolingual domain data (Domhan and Hieber, 2017; Currey et al., 2017) and parallel out-of-domain data (Wang et al., 2017; van der Wees et al., 2017) to improve the performance of domain-specific systems. Furthermore, some attempts have been made to directly insert external knowledge into NMT models through termi2 BabelNet In our work, in order to collect biomedical terms, the domain category of each word was detected with the help of BabelNet (Navigli and Ponzetto, 2012). Specifically, we extracted a list of biomedical terms from our training data using the BabelNet API. To capture biomedical-related domains, we refer to the ”biomedical” definition in the Babel"
W19-5418,D17-1158,0,0.0189004,"ns are especially relevant for industrial applications where there is a need for achieving both fluency and terminology in translations. Current state-of-the-art NMT systems achieve high performances when trained with large-scale parallel corpora. However, most of the time, largescale parallel corpora are not available for specific domains. Consequently, NMT models perform poorly for domain-specific translation when trained in low-resource scenario (Chu and Wang, 2018). Several works have been proposed to overcome the lack of domain parallel data by leveraging on both monolingual domain data (Domhan and Hieber, 2017; Currey et al., 2017) and parallel out-of-domain data (Wang et al., 2017; van der Wees et al., 2017) to improve the performance of domain-specific systems. Furthermore, some attempts have been made to directly insert external knowledge into NMT models through termi2 BabelNet In our work, in order to collect biomedical terms, the domain category of each word was detected with the help of BabelNet (Navigli and Ponzetto, 2012). Specifically, we extracted a list of biomedical terms from our training data using the BabelNet API. To capture biomedical-related domains, we refer to the ”biomedical” d"
W19-5418,L18-1546,0,0.31012,"scribe trained systems and their evaluations. Finally, we present the results of the competition in terms of BLEU score. (Papineni et al., 2002). 5.1 Data collection We gathered data from the resources provided in the official WMT19 web page and from the OPUS collection. For our submissions, all the available biomedical parallel sentences for en/es are chosen both in plain text and Dublin Core format. Then, data have been parsed and merged to create the training and validation sets. Finally, we cleaned the datasets by removing empty sentences and duplicates. In particular, we selected Scielo (Soares et al., 2018), (Neves et al., 2016), UFAL, Pubmed, Medline, IBECS (Villegas et al., 2018) and EMEA (Tiedemann, 2012) sources for the training set and Khresmoi (Duˇsek et al., 2017) for the validation set. Domain features Following the domain control approach (Kobus et al., 2016), we enrich the data with a word-level binary feature by means of the biomedical terminology. Every word belonging to the terminology list has been labelled as biomedical, while all others as a general domain. The resulting binary feature is then embedded into a dense vector and combined with the word vector. The most common combina"
W19-5418,tiedemann-2012-parallel,0,0.0360055,"f BLEU score. (Papineni et al., 2002). 5.1 Data collection We gathered data from the resources provided in the official WMT19 web page and from the OPUS collection. For our submissions, all the available biomedical parallel sentences for en/es are chosen both in plain text and Dublin Core format. Then, data have been parsed and merged to create the training and validation sets. Finally, we cleaned the datasets by removing empty sentences and duplicates. In particular, we selected Scielo (Soares et al., 2018), (Neves et al., 2016), UFAL, Pubmed, Medline, IBECS (Villegas et al., 2018) and EMEA (Tiedemann, 2012) sources for the training set and Khresmoi (Duˇsek et al., 2017) for the validation set. Domain features Following the domain control approach (Kobus et al., 2016), we enrich the data with a word-level binary feature by means of the biomedical terminology. Every word belonging to the terminology list has been labelled as biomedical, while all others as a general domain. The resulting binary feature is then embedded into a dense vector and combined with the word vector. The most common combination strategy consists in concatenating the feature embedding with the word em5.2 Data preprocessing Da"
W19-5418,P07-2045,0,0.0244784,"word belonging to the terminology list has been labelled as biomedical, while all others as a general domain. The resulting binary feature is then embedded into a dense vector and combined with the word vector. The most common combination strategy consists in concatenating the feature embedding with the word em5.2 Data preprocessing Data are preprocessed following the standard pipeline by normalizing punctuation, tokenization and true-casing. We also removed sentences longer than 80 tokens and shorter than 2 tokens. For the previous steps, we used the scripts found in the Moses distribution (Koehn et al., 2007). Eventually, we trained shared byte-pairs encoding (BPE) (Sennrich et al., 2015) on both source and 152 Segmentation Bpe Sentence ”the intr@@ ig@@ u@@ ing pro@@ ble@@ m of cal@@ ci@@ fic@@ ation and os@@ s@@ ific@@ ation ; ne@@ ed to un@@ der@@ st@@ and it for the comp@@ re@@ h@@ ens@@ ion of b@@ one phys@@ io@@ path@@ ology .” ”inhibition of T@@ AF@@ I activity also resulted in a tw@@ of@@ old increase in clot lysis whereas inhibition of both factor XI and T@@ AF@@ I activity had no additional effect . ” ”a 5@@ 7-@@ year-old male with hepatos@@ plen@@ omegaly , p@@ ancy@@ topenia and hyperte"
W19-5418,P17-2089,0,0.0175669,"r achieving both fluency and terminology in translations. Current state-of-the-art NMT systems achieve high performances when trained with large-scale parallel corpora. However, most of the time, largescale parallel corpora are not available for specific domains. Consequently, NMT models perform poorly for domain-specific translation when trained in low-resource scenario (Chu and Wang, 2018). Several works have been proposed to overcome the lack of domain parallel data by leveraging on both monolingual domain data (Domhan and Hieber, 2017; Currey et al., 2017) and parallel out-of-domain data (Wang et al., 2017; van der Wees et al., 2017) to improve the performance of domain-specific systems. Furthermore, some attempts have been made to directly insert external knowledge into NMT models through termi2 BabelNet In our work, in order to collect biomedical terms, the domain category of each word was detected with the help of BabelNet (Navigli and Ponzetto, 2012). Specifically, we extracted a list of biomedical terms from our training data using the BabelNet API. To capture biomedical-related domains, we refer to the ”biomedical” definition in the BabelNet as stated, ”The science of dealing with the mai"
W19-5418,W17-3204,0,0.0133577,"lies only on one ingredient, a biomedical terminology list. We first extracted such a terminology list by labelling biomedical words in our training dataset using the BabelNet API. Then, we designed a data preparation strategy to insert the terms information at a token level. Finally, we trained the Transformer model (Vaswani et al., 2017) with this termsinformed data. Our best-submitted system ranked 2nd and 3rd for Spanish-English and English-Spanish translation directions, respectively. 1 Introduction Domain adaptation in Neural Machine Translation (NMT) remains one of the main challenges (Koehn and Knowles, 2017). Domain-specific translations are especially relevant for industrial applications where there is a need for achieving both fluency and terminology in translations. Current state-of-the-art NMT systems achieve high performances when trained with large-scale parallel corpora. However, most of the time, largescale parallel corpora are not available for specific domains. Consequently, NMT models perform poorly for domain-specific translation when trained in low-resource scenario (Chu and Wang, 2018). Several works have been proposed to overcome the lack of domain parallel data by leveraging on bo"
W19-5424,D14-1179,0,0.0493498,"Missing"
W19-5424,W19-5301,1,0.874415,"Missing"
W19-5424,W17-4715,0,0.038328,"ing over a validation set. Based on these optimized combinations, the decoder uses beam search to find the most probable output given an input. Figure 1 shows a diagram of the phrase-based MT approach. Adding Monolingual Data Although our proposed statistical MT model incorporates monolingual corpora, the supervised neural MT approach is not capable to make use of such data. However, recent studies have reported notable improvements in the translation quality when monolingual corpora were added to the training corpora, either through back-translation (Sennrich et al., 2016b) or copied corpus (Currey et al., 2017). Encouraged by those results and given the similarity of languages at hand, we propose to exploit monolingual data by leveraging back-translation as well as by simply copying target-side monolingual corpus and use it together with the original parallel data. Figure 1: Basic schema of a phrase-based MT system 2.2 Neural Approach Neural networks (NNs) have been successful in many Natural Language Processing (NLP) tasks in recent years. NMT systems, which use end-toend NN models to encode a source sequence in one language and decode a target sequence in the second language, early on demonstrated"
W19-5424,J82-2005,0,0.444283,"Missing"
W19-5424,D15-1166,0,0.0578079,"e, early on demonstrated performance on a par with or even outperformed traditional phrase-based SMT systems (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016a; Zhou et al., 2016; Wu et al., 2016). Previous state-of-the-art NMT models used predominantly bi-directional recurrent neural networks (RNN) equipped with Long-Short Term Memory (LSTM; Hochreiter and Schmidhuber 1997) units or Gated Recurrent Units (GRU; Cho et al. 2014) both in the encoder and the decoder combined with the attention mechanism (Bahdanau et al., 2015; Luong et al., 2015). There were also approaches, although less common, to leverage convolutional neural networks (CNN) for 3 System Combination with Back-translation In this paper, we propose to combine the results of both phrase-based and NMT systems at the sentence level. However, differently from the previous work of Marie and Fujita (2018), we aimed for a conceptually simple combination strategy. In principle, for every sentence generated by the two alternative systems we used the BLEU score (Papineni et al., 2002) to select a sentence with the highest translation quality. Each of the translations was back-t"
W19-5424,W18-1811,0,0.019432,"y bi-directional recurrent neural networks (RNN) equipped with Long-Short Term Memory (LSTM; Hochreiter and Schmidhuber 1997) units or Gated Recurrent Units (GRU; Cho et al. 2014) both in the encoder and the decoder combined with the attention mechanism (Bahdanau et al., 2015; Luong et al., 2015). There were also approaches, although less common, to leverage convolutional neural networks (CNN) for 3 System Combination with Back-translation In this paper, we propose to combine the results of both phrase-based and NMT systems at the sentence level. However, differently from the previous work of Marie and Fujita (2018), we aimed for a conceptually simple combination strategy. In principle, for every sentence generated by the two alternative systems we used the BLEU score (Papineni et al., 2002) to select a sentence with the highest translation quality. Each of the translations was back-translated (i.e. translated from the target language to the source language). In186 malized, tokenized and truecased using Moses1 scripts. Additionally, training data was also cleaned with clean-corpus-n.perl script from Moses. Finally, to allow open-vocabulary, we learned and applied byte-pair encoding (BPE)2 for the concate"
W19-5424,P02-1040,0,0.104426,"both in the encoder and the decoder combined with the attention mechanism (Bahdanau et al., 2015; Luong et al., 2015). There were also approaches, although less common, to leverage convolutional neural networks (CNN) for 3 System Combination with Back-translation In this paper, we propose to combine the results of both phrase-based and NMT systems at the sentence level. However, differently from the previous work of Marie and Fujita (2018), we aimed for a conceptually simple combination strategy. In principle, for every sentence generated by the two alternative systems we used the BLEU score (Papineni et al., 2002) to select a sentence with the highest translation quality. Each of the translations was back-translated (i.e. translated from the target language to the source language). In186 malized, tokenized and truecased using Moses1 scripts. Additionally, training data was also cleaned with clean-corpus-n.perl script from Moses. Finally, to allow open-vocabulary, we learned and applied byte-pair encoding (BPE)2 for the concatenation of the source and target languages with 16k operations. The postprocessing was done in reverse order and included detruecasing and detokenization. stead of using only one s"
W19-5424,D13-1176,0,0.0165273,"e to exploit monolingual data by leveraging back-translation as well as by simply copying target-side monolingual corpus and use it together with the original parallel data. Figure 1: Basic schema of a phrase-based MT system 2.2 Neural Approach Neural networks (NNs) have been successful in many Natural Language Processing (NLP) tasks in recent years. NMT systems, which use end-toend NN models to encode a source sequence in one language and decode a target sequence in the second language, early on demonstrated performance on a par with or even outperformed traditional phrase-based SMT systems (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016a; Zhou et al., 2016; Wu et al., 2016). Previous state-of-the-art NMT models used predominantly bi-directional recurrent neural networks (RNN) equipped with Long-Short Term Memory (LSTM; Hochreiter and Schmidhuber 1997) units or Gated Recurrent Units (GRU; Cho et al. 2014) both in the encoder and the decoder combined with the attention mechanism (Bahdanau et al., 2015; Luong et al., 2015). There were also approaches, although less common, to leverage convolutional neural networks (CNN) for 3 System Combinat"
W19-5424,W16-2323,0,0.0670879,"each of these models are optimized by tuning over a validation set. Based on these optimized combinations, the decoder uses beam search to find the most probable output given an input. Figure 1 shows a diagram of the phrase-based MT approach. Adding Monolingual Data Although our proposed statistical MT model incorporates monolingual corpora, the supervised neural MT approach is not capable to make use of such data. However, recent studies have reported notable improvements in the translation quality when monolingual corpora were added to the training corpora, either through back-translation (Sennrich et al., 2016b) or copied corpus (Currey et al., 2017). Encouraged by those results and given the similarity of languages at hand, we propose to exploit monolingual data by leveraging back-translation as well as by simply copying target-side monolingual corpus and use it together with the original parallel data. Figure 1: Basic schema of a phrase-based MT system 2.2 Neural Approach Neural networks (NNs) have been successful in many Natural Language Processing (NLP) tasks in recent years. NMT systems, which use end-toend NN models to encode a source sequence in one language and decode a target sequence in t"
W19-5424,P16-1009,0,0.138591,"each of these models are optimized by tuning over a validation set. Based on these optimized combinations, the decoder uses beam search to find the most probable output given an input. Figure 1 shows a diagram of the phrase-based MT approach. Adding Monolingual Data Although our proposed statistical MT model incorporates monolingual corpora, the supervised neural MT approach is not capable to make use of such data. However, recent studies have reported notable improvements in the translation quality when monolingual corpora were added to the training corpora, either through back-translation (Sennrich et al., 2016b) or copied corpus (Currey et al., 2017). Encouraged by those results and given the similarity of languages at hand, we propose to exploit monolingual data by leveraging back-translation as well as by simply copying target-side monolingual corpus and use it together with the original parallel data. Figure 1: Basic schema of a phrase-based MT system 2.2 Neural Approach Neural networks (NNs) have been successful in many Natural Language Processing (NLP) tasks in recent years. NMT systems, which use end-toend NN models to encode a source sequence in one language and decode a target sequence in t"
W19-5424,P19-3020,0,0.0160059,"rpus 2nd system PB NMT PB NMT 6 ES-PT 64.96 58.40 52.37 – Discussion Although Czech and Polish belong to the same family of languages (Slavic) and share the same subgroup (Western Slavic), the BLEU score obtained by our winning system is relatively low comparing to other pairs of similar languages (e.g. Spanish and Portuguese). It may seem surprising considering some common characteristics shared As presented in Table 3, our proposed system combinations, employing either MBR or the backtranslation approach, did not achieve any signif188 with other quality measures implemented in the OpenKiwi (Kepler et al., 2019) toolkit4 . by both languages, such as 7 noun cases, 2 number cases, 3 noun gender cases as well as 3 tenses among others. Low performance on this task could be explained by the language distance. Considering the metric proposed by Gamallo et al. (2017), which is based on perplexity as a distance measure between languages, the distance between Czech and Polish is 27 while for Spanish-Portuguese is 7. The very same metric used to evaluate the distance of Czech and Polish from other Slavic languages (i.e. Slovak and Russian) shows that Polish is the most distant language within this group (see T"
W19-5424,P07-2045,0,0.00944681,"cluded detruecasing and detokenization. stead of using only one system to perform backtranslation, we used both PB and neural MT systems and weighted them equally. See Figure 2 for a graphical representation of this strategy. This approach was motivated by the recent success of different uses of back-translation in neural MT studies (Sennrich et al., 2016b; Lample et al., 2018). The final test set was composed of sentences produced by the system that obtained the highest score based on the quality of the combined back-translation. 4 4.2 Phrase-based For the Phrase-based systems we used Moses (Koehn et al., 2007), which is a statistical machine translation system. In order to build our model, we used generally the default parameters which include: grow-diagonal-final-and word alignment, lexical msd-bidirectional-fe reordering model trained, lexical weights, binarized and compacted phrase table with 4 score components and 4 threads used for conversion, 5-gram, binarized, loading-on-demand language model with KneserNey smoothing and trie data structure without pruning; and MERT (Minimum Error Rate Training) optimisation with 100 n-best list generated and 16 threads. Experimental Framework In this sectio"
W19-5424,N03-1017,0,0.058499,"Missing"
W19-5424,N04-1022,0,\N,Missing
W19-5424,D18-1549,0,\N,Missing
