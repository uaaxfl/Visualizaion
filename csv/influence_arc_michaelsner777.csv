2015.lilt-12.5,P13-1035,0,0.104168,"Missing"
2015.lilt-12.5,P14-1035,0,0.161803,"ent from those used here. Elson et al. (2010) also build social networks for characters in novels, which they use to evaluate several questions in literary theory. As in this work, they begin by identifying characters using coreference resolution on mentions. They construct the social network based on conversation structure (O’Keefe et al., 2012, Elson et al., 2010); the experiments here use the simpler, but less precise, heuristic of co-frequency counts. Their work, however, does not use time-varying features, but collapses the novel over time, producing a static picture of a dynamic system. Bamman et al. (2014) use a mixed-effects model to infer latent character types from the text of a large set of novels. Like Elson et al. (2010), they have a feature set that is not time-varying. They build upon previous work by Bamman et al. (2013) who use a Bayesian model to learn a set of character roles such as protagonist, love interest and best friend for movie characters, but using metadata rather than scripts directly. They evaluate against a set of preregistered hypotheses. Alm and Sproat (2005), on the other hand, produce an explicitly temporal structure, a time-varying curve of emotional language over 6"
2015.lilt-12.5,P05-1018,0,0.0376452,"of document coherence. 10 / LiLT volume 12, issue 5 October 2015 The specific rank learner used is SVM-rank (Joachims, 2006), which solves an optimization problem based on ordinal regression to approximately minimize the number of pairs ranked in the wrong order. 2.4 Evaluation by reordering The experiments presented here test the kernel similarity function by challenging it to distinguish novels from artificial “negative examples”. These are created from real texts by permuting the order of the chapters. This procedure originated in the discourse coherence literature (Karamanis et al., 2004, Barzilay and Lapata, 2005), in which it is assumed that most reorderings of a text cause a loss of coherence and are therefore suitable as negative examples. Post (2011) uses a similar idea to evaluate a model of grammaticality. The use of artificial negative examples has both strengths and weaknesses. On one hand, it is highly replicable and objective in domains where annotation would be expensive and unreliable. In this case, human annotators would have to decide which real novels are more or less similar to one another. On an artificial task, systems can be evaluated for at least basic effectiveness without this kin"
2015.lilt-12.5,P09-1068,0,0.0218593,"they tend to be slower, less stable and less scalable than LDA. Since the results in this paper show that topic model features are useful in capturing a global beginning-to-end temporal structure, evaluating these more complex topic models is a promising direction for future work. 2.2 Event-based representations In contrast to these abstract representations are models that stay closer to the level of specific events extracted from sentences in the text. Such models have usually been applied to shorter fictional narratives such as fables. Many of these follow from narrative schema extraction (Chambers and Jurafsky, 2009), which attempts to learn representations for events in news stories. These representations are similar to Schankian scripts (Schank and Abelson, 1977); they are networks of events in temporal sequence, with slots for specific actors. For instance, “terrorist attacks target” can be followed by “terrorist being arrested”. Similar representations for fiction were investigated by McIntyre and Lapata (2009), who use them to generate short fables. In later work (McIntyre and Lapata, 2010), they add a coherence component to ensure smooth sentence-to-sentence transitions. Figure 2 shows a sample outp"
2015.lilt-12.5,N01-1007,0,0.0185165,"ed in (Coll Ardanuy and Sporleder, 2014). The system begins by detecting proper nouns and discarding those which occur fewer than 5 times. Identical mentions longer than two words are merged into a single entity, so that for example all mentions of “George Osborne” are taken to refer to the same person. Next, each name is assigned a gender—masculine, feminine or neuter—using a list of gendered titles, then a list of male and female first names from the 1990 US census. Mentions are then merged when each is longer than one word, the genders do not clash, and first and last names are consistent (Charniak, 2001). This step would cluster “George Osborne”, “Mr. Osborne” and “Mr. George Osborne”. Single-word mentions are then merged, either with matching multiword mentions if they appear in the same paragraph, or else with the multi-word mention that occurs in the most paragraphs—so when “George” appears close to “George Osborne”, the two refer to the same person. Finally, mentions are discared if they still have not been assigned a non-neuter gender, or if they match synset location.n.01 in WordNet (Miller et al., 1993); these are likely to be place names like “London”, which are proper noun phrases bu"
2015.lilt-12.5,W14-0905,0,0.0766212,"Missing"
2015.lilt-12.5,E12-1065,1,0.807007,"Missing"
2015.lilt-12.5,N07-1055,1,0.800423,"1) uses a similar idea to evaluate a model of grammaticality. The use of artificial negative examples has both strengths and weaknesses. On one hand, it is highly replicable and objective in domains where annotation would be expensive and unreliable. In this case, human annotators would have to decide which real novels are more or less similar to one another. On an artificial task, systems can be evaluated for at least basic effectiveness without this kind of resource. On the other hand, performance on artificial reordering tasks can fail to correlate with performance on more realistic tasks (Elsner et al., 2007). Random permutations can cause particular problems, since they often create local structures (for instance, a rarely mentioned minor character who appears in two widely separated chapters) which are uncommon in real documents. The use of novels in reverse order is intended to guard against this to some degree, because reversals destroy the global plot structure of the novel while preserving its local consistency. In any case, failure to correlate is primarily a problem with well-performing systems; systems which fail badly on tests with artificial documents rarely succeed on more complex ones"
2015.lilt-12.5,P10-1015,0,0.122883,"Ardanuy and Sporleder (2014) is the closest to what is presented here. They too use both static and time-varying character similarity features to represent texts as graphs with characters as nodes. They use these graphs to define a text similarity metric, and cluster texts by author and genre. Unlike this work, however, they gather features over the network as a whole (for instance, the proportion of male characters) rather than trying to define text similarity by building up from character similarities. Thus, the feature sets considered in their work are quite different from those used here. Elson et al. (2010) also build social networks for characters in novels, which they use to evaluate several questions in literary theory. As in this work, they begin by identifying characters using coreference resolution on mentions. They construct the social network based on conversation structure (O’Keefe et al., 2012, Elson et al., 2010); the experiments here use the simpler, but less precise, heuristic of co-frequency counts. Their work, however, does not use time-varying features, but collapses the novel over time, producing a static picture of a dynamic system. Bamman et al. (2014) use a mixed-effects mode"
2015.lilt-12.5,elson-mckeown-2010-building,0,0.0306188,"ESOP (Goyal et al., 2010) attempts to modernize this representation by learning which verbs cause positive or negative states automatically. A similar goal-based representation is learned in O’Neill and Riedl’s (2011) work, which performs story generation by abductive plan inference. These representations are sophisticated, but brittle; AESOP’s accuracy is poor even for short fables, and the authors conclude that the approach is unlikely to be scalable. Elson et al. (2010) present Scheherezade, a system for human annotation of AESOP-like goal structures; a small annotated corpus is available (Elson and McKeown, 2010). However, extending this expensive hands-on annotation task to novelistic texts is likely to be prohibitively time-consuming. Kazantseva and Szpakowicz (2010), while relying on sentence-based information, is somewhat different in its objectives. Rather than modeling plot structure, it attempts to produce “spoiler-free” summaries which exclude plot detail while incorporating character and setting description, using features such as stative verbs (“stand, know, be located”) to find appropriate sentences. They analyze the performance of summarization systems on fictional text and find that conve"
2015.lilt-12.5,E12-1032,0,0.0673867,"aining the component models directly to encourage agreement (Liang et al., 2006). When the problem of interest involves multiple sets of features, it may be useful to set kernel parameters weighing the different features to optimize performance. This task is sometimes considered as multiple kernel learning (Gnen and Alpaydın, 2011); a variety of methods have been used. In this paper, kernel parameters are optimized using rank learning: attempting to make pairs of training instances which should be similar score higher under k than less similar ones. The ranking protocol was designed following Feng and Hirst (2012), who used a similar procedure to optimize parameters in a model of document coherence. 10 / LiLT volume 12, issue 5 October 2015 The specific rank learner used is SVM-rank (Joachims, 2006), which solves an optimization problem based on ordinal regression to approximately minimize the number of pairs ranked in the wrong order. 2.4 Evaluation by reordering The experiments presented here test the kernel similarity function by challenging it to distinguish novels from artificial “negative examples”. These are created from real texts by permuting the order of the chapters. This procedure originate"
2015.lilt-12.5,D10-1008,0,0.107464,"folktales and a small corpus of Shakespeare’s plays. Early work by Lehnert (1981) proposes a model which represents emotion and goal information alongside events. This plot-unit model treats positive and negative sentiment as primitives, and builds up a plot as a set of actions which result from, and cause, characters to feel good or bad. For instance, a retaliation has the form: “Because Y’s [action] caused a [negative state] for X, X [acted] to cause a [negative state] for Y”. Early implementations relied heavily on hand-engineered domain knowledge, and thus did not generalize well. AESOP (Goyal et al., 2010) attempts to modernize this representation by learning which verbs cause positive or negative states automatically. A similar goal-based representation is learned in O’Neill and Riedl’s (2011) work, which performs story generation by abductive plan inference. These representations are sophisticated, but brittle; AESOP’s accuracy is poor even for short fables, and the authors conclude that the approach is unlikely to be scalable. Elson et al. (2010) present Scheherezade, a system for human annotation of AESOP-like goal structures; a small annotated corpus is available (Elson and McKeown, 2010)."
2015.lilt-12.5,P04-1050,0,0.240324,"DA induces word clusters directly from the corpus and can therefore learn features which a general-purpose lexicon could not. The experiments discussed in this paper evaluate the strengths and weaknesses of each approach. These representational choices are evaluated through the construc4 / LiLT volume 12, issue 5 October 2015 tion of a similarity function, or kernel, which measures how alike two novels are in a given feature representation. Representation quality can then be compared by drawing on an existing tradition of discourse coherence evaluation using artificial reordering experiments (Karamanis et al., 2004). In these experiments, the kernel is used to distinguish novels that have been randomly permuted, or reversed, chapter-by-chapter, from the originals. The proposed representation and experimental setup implicitly assume that novels share a common sequential structure—an emotional/rhetorical structure for systems using sentiment features or a chronological sequence for LDA. Deviations from these structural principles are a common way to create suspense or draw attention to the narrative as an artifact. In a 19th-century example, the long flashback in The Tenant of Wildfell Hall breaks the chro"
2015.lilt-12.5,J10-1003,0,0.187654,"py ending” in ways that apply across a broad range of texts. Whenever possible, the representation is constructed using lexical distribution rather than requiring text analyses (possibly error-prone) with complex NLP tools. This representation is used to create models capable of distinguishing real novels from artificially disordered texts for which plot structure is missing or incomprehensible. This approach is motivated by the inadequate performance of current general-purpose NLP systems on novelistic text. For instance, generalpurpose extractive summarizers fail to find suitable sentences (Kazantseva and Szpakowicz, 2010). The output of Microsoft Word 2008’s builtin summarizer on Pride and Prejudice is shown in Figure 1. “Bingley.” Elizabeth felt Jane’s pleasure. “Miss Elizabeth Bennet.” Elizabeth looked surprised. “FITZWILLIAM DARCY” Elizabeth was delighted. Elizabeth read on: Elizabeth smiled. “If! “Dearest Jane! Output the built-in summarizer in Microsoft Word 2008, run on the full text of Pride and Prejudice; quoted by Huff (2010). FIGURE 1 Part of the problem is that counting unigrams at the document level identifies character names, rather than themes or plot elements, as the most important content of Pr"
2015.lilt-12.5,N06-1014,0,0.0103979,"holds true for characters—character u from work X (Elizabeth Bennet from Pride and Prejudice) should correspond only to character v from work Y (Jane from Jane Eyre). If Elizabeth took on the roles of a whole set of characters, this would be a point of dissimilarity between the two novels. The simplest technique for computing a symmetric alignment is to run independent one-to-many models in each direction and take the intersection. MT researchers have improved upon this by computing a matching explicitly (Matusov et al., 2004) or training the component models directly to encourage agreement (Liang et al., 2006). When the problem of interest involves multiple sets of features, it may be useful to set kernel parameters weighing the different features to optimize performance. This task is sometimes considered as multiple kernel learning (Gnen and Alpaydın, 2011); a variety of methods have been used. In this paper, kernel parameters are optimized using rank learning: attempting to make pairs of training instances which should be similar score higher under k than less similar ones. The ranking protocol was designed following Feng and Hirst (2012), who used a similar procedure to optimize parameters in a"
2015.lilt-12.5,C04-1032,0,0.0485348,"anslates as v; when aligning in reverse, v should become u again. In this work, the same holds true for characters—character u from work X (Elizabeth Bennet from Pride and Prejudice) should correspond only to character v from work Y (Jane from Jane Eyre). If Elizabeth took on the roles of a whole set of characters, this would be a point of dissimilarity between the two novels. The simplest technique for computing a symmetric alignment is to run independent one-to-many models in each direction and take the intersection. MT researchers have improved upon this by computing a matching explicitly (Matusov et al., 2004) or training the component models directly to encourage agreement (Liang et al., 2006). When the problem of interest involves multiple sets of features, it may be useful to set kernel parameters weighing the different features to optimize performance. This task is sometimes considered as multiple kernel learning (Gnen and Alpaydın, 2011); a variety of methods have been used. In this paper, kernel parameters are optimized using rank learning: attempting to make pairs of training instances which should be similar score higher under k than less similar ones. The ranking protocol was designed foll"
2015.lilt-12.5,P09-1025,0,0.0322728,"cific events extracted from sentences in the text. Such models have usually been applied to shorter fictional narratives such as fables. Many of these follow from narrative schema extraction (Chambers and Jurafsky, 2009), which attempts to learn representations for events in news stories. These representations are similar to Schankian scripts (Schank and Abelson, 1977); they are networks of events in temporal sequence, with slots for specific actors. For instance, “terrorist attacks target” can be followed by “terrorist being arrested”. Similar representations for fiction were investigated by McIntyre and Lapata (2009), who use them to generate short fables. In later work (McIntyre and Lapata, 2010), they add a coherence component to ensure smooth sentence-to-sentence transitions. Figure 2 shows a sample output. The lack of global “plot” structure is an important shortcoming of this work; while the generated stories describe reasonable sequences of events, the stories do not seem to raise or resolve any central conflict. Similar narrative models are considered by Li et al. (2012), who learn event networks from a corpus of short texts elicited through crowdsourcing. These texts focus on specific events such"
2015.lilt-12.5,P10-1158,0,0.103514,"zed by Chang et al. (2009) and Mimno and Blei (2011). 1 Sentiment analysis systems which work on whole clauses or sentences are an active research area (Socher et al., 2011, Yessenalina and Cardie, 2011, among others); to my knowledge, such systems have not been used to construct emotional trajectory models. Abstract Representations of Plot Structure / 7 The emperor rules the kingdom. The kingdom holds on to the emperor. The emperor rides out of the kingdom. The kingdom speaks out against the emperor. The emperor lies. FIGURE 2 Story generated by an event-based model with coherence reranking (McIntyre and Lapata 2010). How these representations compare to sentiment or other features on novelistic text is an open question. This paper uses the simpler method of running standard LDA and measuring temporal patterns on the resulting topics. While more sophisticated methods do have published implementations, they tend to be slower, less stable and less scalable than LDA. Since the results in this paper show that topic model features are useful in capturing a global beginning-to-end temporal structure, evaluating these more complex topic models is a promising direction for future work. 2.2 Event-based representat"
2015.lilt-12.5,D11-1021,0,0.0250419,"in a document can then be used as a coarse-grained approximation of its content. Although the frequencies of LDA topics naturally vary throughout the course of a long text, LDA does not directly model temporal variation. Subsequent work proposes a variety of Bayesian topic models which make more sophisticated use of document metadata, including sequence ordering (Blei and Lafferty, 2006, Kim and Sudderth, 2011). These papers tend to evaluate using a combination of held-out likelihood and eyeball, often on corpora of scientific journal articles—an approach criticized by Chang et al. (2009) and Mimno and Blei (2011). 1 Sentiment analysis systems which work on whole clauses or sentences are an active research area (Socher et al., 2011, Yessenalina and Cardie, 2011, among others); to my knowledge, such systems have not been used to construct emotional trajectory models. Abstract Representations of Plot Structure / 7 The emperor rules the kingdom. The kingdom holds on to the emperor. The emperor rides out of the kingdom. The kingdom speaks out against the emperor. The emperor lies. FIGURE 2 Story generated by an event-based model with coherence reranking (McIntyre and Lapata 2010). How these representations"
2015.lilt-12.5,W11-1514,0,0.0275877,"They produce a single trajectory per story (or several stories). Subsequent work on sentiment analysis of fiction and literary texts has retained the single-trajectory assumption, while focusing on enriching the set of sentiments used by the system and improving techniques for detecting them. Volkova et al. (2010), for instance, describe a protocol for hand-annotation of sentiment in fairy tales which allows non-experts to achieve high agreement. Mohammad and Turney (2010) construct a large emotional lexicon using Mechanical Turk crowd-sourcing, which is used in the systems presented here. In Mohammad (2011, 2012), it is used to construct emotional trajectories for a few literary works such as Hamlet, but, apart from a broad corpus-level comparison between novels and fairy tales, no effort is made to evaluate these systematically. Ang (2012) creates trajectory plots using Mohammad and Turney’s (2010) emotional categories, and evaluates them as part of a toolkit for writers. In a survey, writers find the tool interesting, although they point out that its inferred sentiments can be inaccurate due to negation and other effects of context.1 This paper evaluates both traditional trajectory systems wh"
2015.lilt-12.5,W10-0204,0,0.0107692,"LiLT volume 12, issue 5 October 2015 time, which they call an emotional trajectory. Alm and Sproat (2005) rely on hand-annotation of the trajectory. They produce a single trajectory per story (or several stories). Subsequent work on sentiment analysis of fiction and literary texts has retained the single-trajectory assumption, while focusing on enriching the set of sentiments used by the system and improving techniques for detecting them. Volkova et al. (2010), for instance, describe a protocol for hand-annotation of sentiment in fairy tales which allows non-experts to achieve high agreement. Mohammad and Turney (2010) construct a large emotional lexicon using Mechanical Turk crowd-sourcing, which is used in the systems presented here. In Mohammad (2011, 2012), it is used to construct emotional trajectories for a few literary works such as Hamlet, but, apart from a broad corpus-level comparison between novels and fairy tales, no effort is made to evaluate these systematically. Ang (2012) creates trajectory plots using Mohammad and Turney’s (2010) emotional categories, and evaluates them as part of a toolkit for writers. In a survey, writers find the tool interesting, although they point out that its inferre"
2015.lilt-12.5,D12-1072,0,0.0448186,"Missing"
2015.lilt-12.5,P11-2038,0,0.013556,"blem based on ordinal regression to approximately minimize the number of pairs ranked in the wrong order. 2.4 Evaluation by reordering The experiments presented here test the kernel similarity function by challenging it to distinguish novels from artificial “negative examples”. These are created from real texts by permuting the order of the chapters. This procedure originated in the discourse coherence literature (Karamanis et al., 2004, Barzilay and Lapata, 2005), in which it is assumed that most reorderings of a text cause a loss of coherence and are therefore suitable as negative examples. Post (2011) uses a similar idea to evaluate a model of grammaticality. The use of artificial negative examples has both strengths and weaknesses. On one hand, it is highly replicable and objective in domains where annotation would be expensive and unreliable. In this case, human annotators would have to decide which real novels are more or less similar to one another. On an artificial task, systems can be evaluated for at least basic effectiveness without this kind of resource. On the other hand, performance on artificial reordering tasks can fail to correlate with performance on more realistic tasks (El"
2015.lilt-12.5,D11-1014,0,0.0567495,"turally vary throughout the course of a long text, LDA does not directly model temporal variation. Subsequent work proposes a variety of Bayesian topic models which make more sophisticated use of document metadata, including sequence ordering (Blei and Lafferty, 2006, Kim and Sudderth, 2011). These papers tend to evaluate using a combination of held-out likelihood and eyeball, often on corpora of scientific journal articles—an approach criticized by Chang et al. (2009) and Mimno and Blei (2011). 1 Sentiment analysis systems which work on whole clauses or sentences are an active research area (Socher et al., 2011, Yessenalina and Cardie, 2011, among others); to my knowledge, such systems have not been used to construct emotional trajectory models. Abstract Representations of Plot Structure / 7 The emperor rules the kingdom. The kingdom holds on to the emperor. The emperor rides out of the kingdom. The kingdom speaks out against the emperor. The emperor lies. FIGURE 2 Story generated by an event-based model with coherence reranking (McIntyre and Lapata 2010). How these representations compare to sentiment or other features on novelistic text is an open question. This paper uses the simpler method of ru"
2015.lilt-12.5,W10-0212,0,0.0208847,"gistered hypotheses. Alm and Sproat (2005), on the other hand, produce an explicitly temporal structure, a time-varying curve of emotional language over 6 / LiLT volume 12, issue 5 October 2015 time, which they call an emotional trajectory. Alm and Sproat (2005) rely on hand-annotation of the trajectory. They produce a single trajectory per story (or several stories). Subsequent work on sentiment analysis of fiction and literary texts has retained the single-trajectory assumption, while focusing on enriching the set of sentiments used by the system and improving techniques for detecting them. Volkova et al. (2010), for instance, describe a protocol for hand-annotation of sentiment in fairy tales which allows non-experts to achieve high agreement. Mohammad and Turney (2010) construct a large emotional lexicon using Mechanical Turk crowd-sourcing, which is used in the systems presented here. In Mohammad (2011, 2012), it is used to construct emotional trajectories for a few literary works such as Hamlet, but, apart from a broad corpus-level comparison between novels and fairy tales, no effort is made to evaluate these systematically. Ang (2012) creates trajectory plots using Mohammad and Turney’s (2010) e"
2015.lilt-12.5,N12-1001,0,0.0502567,"Missing"
2015.lilt-12.5,H05-1044,0,0.0708818,"Missing"
2015.lilt-12.5,D11-1016,0,0.0214299,"ut the course of a long text, LDA does not directly model temporal variation. Subsequent work proposes a variety of Bayesian topic models which make more sophisticated use of document metadata, including sequence ordering (Blei and Lafferty, 2006, Kim and Sudderth, 2011). These papers tend to evaluate using a combination of held-out likelihood and eyeball, often on corpora of scientific journal articles—an approach criticized by Chang et al. (2009) and Mimno and Blei (2011). 1 Sentiment analysis systems which work on whole clauses or sentences are an active research area (Socher et al., 2011, Yessenalina and Cardie, 2011, among others); to my knowledge, such systems have not been used to construct emotional trajectory models. Abstract Representations of Plot Structure / 7 The emperor rules the kingdom. The kingdom holds on to the emperor. The emperor rides out of the kingdom. The kingdom speaks out against the emperor. The emperor lies. FIGURE 2 Story generated by an event-based model with coherence reranking (McIntyre and Lapata 2010). How these representations compare to sentiment or other features on novelistic text is an open question. This paper uses the simpler method of running standard LDA and measuri"
2020.acl-main.695,E17-2067,0,0.0305779,"en a corpus and lexicon. First, we cluster lexicon forms into cells. Then we cluster forms into paradigms given their fixed cell membership. To maintain tractability, clustering assumes a one-to-one mapping of forms to slots. Following cell and paradigm clustering, we predict forms to realize empty slots given one of the lexicon forms assigned to a cell in Clustering into Cells We use a heuristic method to determine the number of cells and what lexicon forms to assign to each. Inspired by work on inductive biases in word embeddings (Pennington et al., 2014; Trask et al., 2015; Goldberg, 2016; Avraham and Goldberg, 2017; Tu et al., 2017), we train morphosyntactically biased embeddings on the corpus and use them to k-means cluster lexicon forms into cells. Following Erdmann et al. (2018), we emphasize morphosyntactically salient dimensions in embedding space by manipulating hyperparameters in fastText (Bojanowski et al., 2017). Specifically, to encourage grouping of morphologically related words, fastText computes a word’s embedding as the sum of its subword embeddings for all subword sequences between 3 and 6 characters long (Schütze, 1993). We shorten this range to 2 to 4 to bias the grouping toward shared"
2020.acl-main.695,E17-1032,0,0.183936,"Missing"
2020.acl-main.695,P19-1376,0,0.0196105,"y. Some cognitive works suggest the PCFP cannot be too difficult for any language (Dale et al., 1998; Ackerman and Malouf, 2013, 2015; Blevins et al., 2017; Cotterell et al., 2019). Neural models can test and extend such proposals (Cotterell et al., 2018a; Silfverberg and Hulden, 2018). A related vein of work discusses how speakers inflect nonce words (Berko, 1958; Plunkett and Juola, 1999; Yang, 2015), e.g., is the past tense of sping, spinged or spung? There is a long tradition of modeling past-tense generation with neural networks (Rumelhart and McClelland, 1986; Kirov and Cotterell, 2018; Corkery et al., 2019). On the engineering side, Durrett and DeNero (2013) inspired much recent work, which has since benefited from large inflectional datasets (Kirov et al., 2018) and advances in neural sequence modeling (Bahdanau et al., 2015). Shared tasks have drawn extra attention to the PCFP (Cotterell et al., 2016a, 2017, 2018c; McCarthy et al., 2019). 3 The Paradigm Discovery Problem Paradigm discovery is a natural next step in computational morphology, building on related minimally or indirectly supervised works (§2.2) to bridge the gap between unsupervised traditions (§2.1) and supervised work on the PCF"
2020.acl-main.695,Q19-1021,1,0.85462,"dels can guess a word’s plural having only seen its singular, but the child must bootstrap morphological knowledge from scratch, first learning that singular–plural is a relevant distinction. Thus, the PDP must be at least partially solved before the PCFP can be attempted. Yet, as a supervised task, the PCFP is more easily studied, and has received much attention on its own, especially from the word-and-paradigm camp of morphological theory. Some cognitive works suggest the PCFP cannot be too difficult for any language (Dale et al., 1998; Ackerman and Malouf, 2013, 2015; Blevins et al., 2017; Cotterell et al., 2019). Neural models can test and extend such proposals (Cotterell et al., 2018a; Silfverberg and Hulden, 2018). A related vein of work discusses how speakers inflect nonce words (Berko, 1958; Plunkett and Juola, 1999; Yang, 2015), e.g., is the past tense of sping, spinged or spung? There is a long tradition of modeling past-tense generation with neural networks (Rumelhart and McClelland, 1986; Kirov and Cotterell, 2018; Corkery et al., 2019). On the engineering side, Durrett and DeNero (2013) inspired much recent work, which has since benefited from large inflectional datasets (Kirov et al., 2018)"
2020.acl-main.695,N18-2087,1,0.928989,"must bootstrap morphological knowledge from scratch, first learning that singular–plural is a relevant distinction. Thus, the PDP must be at least partially solved before the PCFP can be attempted. Yet, as a supervised task, the PCFP is more easily studied, and has received much attention on its own, especially from the word-and-paradigm camp of morphological theory. Some cognitive works suggest the PCFP cannot be too difficult for any language (Dale et al., 1998; Ackerman and Malouf, 2013, 2015; Blevins et al., 2017; Cotterell et al., 2019). Neural models can test and extend such proposals (Cotterell et al., 2018a; Silfverberg and Hulden, 2018). A related vein of work discusses how speakers inflect nonce words (Berko, 1958; Plunkett and Juola, 1999; Yang, 2015), e.g., is the past tense of sping, spinged or spung? There is a long tradition of modeling past-tense generation with neural networks (Rumelhart and McClelland, 1986; Kirov and Cotterell, 2018; Corkery et al., 2019). On the engineering side, Durrett and DeNero (2013) inspired much recent work, which has since benefited from large inflectional datasets (Kirov et al., 2018) and advances in neural sequence modeling (Bahdanau et al., 2015). Shared"
2020.acl-main.695,K18-3001,1,0.853843,"rvised morphological paradigm completion. Given only a small corpus and lexicon of verbal lemmata, participating systems must propose full paradigms for each lemma. By contrast, our framework does not reveal how many paradigms should be generated, nor do we privilege a specific form as the lemma, but we do use a larger lexicon of exclusively verbal or nominal forms. Their proposed baseline uses distributional context for POS tagging and features, but does not train embeddings as the corpus is small. 2.2 Subtasks of Paradigm Discovery A few works address subtasks of the PDP. Erdmann and Habash (2018) learn paradigm membership from raw text, but do not sort paradigms into cells. Boyé and Schalchli (2019) discuss the paradigm cell finding problem, identifying the cell (but not paradigm) realized by a given form. Lee (2015) clusters forms into cells across inflection classes. Beniamine et al. (2018) group paradigms into inflection classes, and Eskander et al. (2013) induce inflection classes and lemmata from cell labels. 2.3 The Paradigm Cell Filling Problem The PCFP is the task of predicting unseen inflected forms given morphologically labeled input. PCFP models can guess a word’s plural ha"
2020.acl-main.695,K17-2001,1,0.898483,"Missing"
2020.acl-main.695,P16-1156,1,0.930366,"i.e., they realize the same morphosyntactic properties 3. SG . PRES, but in different paradigms. Acquiring such paradigmatic knowledge enables us to produce unseen inflectional variants of new vocabulary items, i.e. to complete morphological paradigms. Much work has addressed this task, which Ackerman et al. (2009) call the paradigm cell filling problem (PCFP),1 but few have discussed inducing paradigmatic knowledge from scratch, which we call the paradigm discovery problem (PDP).2 1 In the NLP literature, this task is called morphological reinflection or morphological inflection generation (Cotterell et al., 2016a); this is only a difference in nomenclature. 2 Elsner et al. (2019) call the task the paradigm cell discovery problem; we drop cell to distinguish our task from As an unsupervised task, the PDP poses challenges for modeling and evaluation and has yet to be attempted in its full form (Elsner et al., 2019). However, we contend there is much to be gained from formalizing and studying the PDP. There are insights for cognitive modeling to be won (Pinker, 2001; Goldwater, 2007) and intuitions on combating sparse data for language generation (King and White, 2018) to be accrued. Unsupervised langua"
2020.acl-main.695,J98-3001,0,0.192601,"Missing"
2020.acl-main.695,D11-1057,0,0.266604,"(e.g., bring 6= br + ing) with high accuracy, they do not attempt to solve the PDP. They do, however, reveal which forms take the same affixes (e.g., walked, talked), not which forms occupy the same cell (e.g., walked, brought). Indeed, they explicitly struggle with irregular morphology. Segmenters also cannot easily model non-concatenative phenomena like ablaut, vowel harmony and templatic processes. Two works have proposed tasks which can be considered alternative formulations of the PDP, using either minimal or indirect supervision to bootstrap their models. We discuss each in turn. First, Dreyer and Eisner (2011) use a generative model to cluster forms into paradigms and cells with a Bayesian non-parametric mixture of weighted finite-state transducers. They present a PDP framework which, in principle, could be fully unsupervised, but their model requires a small seed of labeled data to get key information like the number of cells distinguished, making it less relevant cognitively. In contrast, our task is not directly supervised and focuses on distributional context. Second, contemporaneous to our work, Jin et al. (2020) propose a similar framework for SIGMORPHON 2020’s shared task on unsupervised mor"
2020.acl-main.695,N13-1138,0,0.166655,"be too difficult for any language (Dale et al., 1998; Ackerman and Malouf, 2013, 2015; Blevins et al., 2017; Cotterell et al., 2019). Neural models can test and extend such proposals (Cotterell et al., 2018a; Silfverberg and Hulden, 2018). A related vein of work discusses how speakers inflect nonce words (Berko, 1958; Plunkett and Juola, 1999; Yang, 2015), e.g., is the past tense of sping, spinged or spung? There is a long tradition of modeling past-tense generation with neural networks (Rumelhart and McClelland, 1986; Kirov and Cotterell, 2018; Corkery et al., 2019). On the engineering side, Durrett and DeNero (2013) inspired much recent work, which has since benefited from large inflectional datasets (Kirov et al., 2018) and advances in neural sequence modeling (Bahdanau et al., 2015). Shared tasks have drawn extra attention to the PCFP (Cotterell et al., 2016a, 2017, 2018c; McCarthy et al., 2019). 3 The Paradigm Discovery Problem Paradigm discovery is a natural next step in computational morphology, building on related minimally or indirectly supervised works (§2.2) to bridge the gap between unsupervised traditions (§2.1) and supervised work on the PCFP (§2.3). In the PCFP, 7779 Corpus The cat watched m"
2020.acl-main.695,W18-5806,1,0.852738,"ared task on unsupervised morphological paradigm completion. Given only a small corpus and lexicon of verbal lemmata, participating systems must propose full paradigms for each lemma. By contrast, our framework does not reveal how many paradigms should be generated, nor do we privilege a specific form as the lemma, but we do use a larger lexicon of exclusively verbal or nominal forms. Their proposed baseline uses distributional context for POS tagging and features, but does not train embeddings as the corpus is small. 2.2 Subtasks of Paradigm Discovery A few works address subtasks of the PDP. Erdmann and Habash (2018) learn paradigm membership from raw text, but do not sort paradigms into cells. Boyé and Schalchli (2019) discuss the paradigm cell finding problem, identifying the cell (but not paradigm) realized by a given form. Lee (2015) clusters forms into cells across inflection classes. Beniamine et al. (2018) group paradigms into inflection classes, and Eskander et al. (2013) induce inflection classes and lemmata from cell labels. 2.3 The Paradigm Cell Filling Problem The PCFP is the task of predicting unseen inflected forms given morphologically labeled input. PCFP models can guess a word’s plural ha"
2020.acl-main.695,W19-4214,1,0.815752,"cell. Our algorithm greedily builds paradigms cell by cell. To gauge the quality of a candidate paradigm, we first identify its base and exponents. Following Beniamine et al. (2018), we define π’s base, bπ , as the longest common subsequence shared by all forms in π.56 For each form f in π, we define the exponent xf as the subsequences of f that remain after removing bπ , i.e., xf is a tuple of affixes. For example, if π contains words wxyxz and axx, bπ is xx and the exponents are (&lt;w, y, z&gt;) and (&lt;a), respectively.7 Inspired by unsupervised maximum matching in greedy tokenization (Guo, 1997; Erdmann et al., 2019), we define the following paradigm score function: score(π) = X   |bπ |− |xf | (2) 1: 2: 3: 4: 5: 6: 7: 8: fj0 ∈Cj 9: 10: 11: 12: 13: 5 The fact that we use a subsequence, instead of a substring, means that we can handle non-concatenative morphology. 6 We note that the longest common subsequence may be found with a polynomial-time dynamic program; however, there will not exist an algorithm whose runtime is polynomial in the number of strings unless P = NP (Maier, 1978). 7 We use word start (&lt;) and end (&gt;) tokens to distinguish exponents; they do not count as exponent characters in eq. (2). 8"
2020.acl-main.695,P18-2089,1,0.840933,"ng assumes a one-to-one mapping of forms to slots. Following cell and paradigm clustering, we predict forms to realize empty slots given one of the lexicon forms assigned to a cell in Clustering into Cells We use a heuristic method to determine the number of cells and what lexicon forms to assign to each. Inspired by work on inductive biases in word embeddings (Pennington et al., 2014; Trask et al., 2015; Goldberg, 2016; Avraham and Goldberg, 2017; Tu et al., 2017), we train morphosyntactically biased embeddings on the corpus and use them to k-means cluster lexicon forms into cells. Following Erdmann et al. (2018), we emphasize morphosyntactically salient dimensions in embedding space by manipulating hyperparameters in fastText (Bojanowski et al., 2017). Specifically, to encourage grouping of morphologically related words, fastText computes a word’s embedding as the sum of its subword embeddings for all subword sequences between 3 and 6 characters long (Schütze, 1993). We shorten this range to 2 to 4 to bias the grouping toward shared affixes rather than (usually longer) shared stems. This helps recognize that the same affix is likely to realize the same cell, e.g., watch +ed and follow +ed. We limit t"
2020.acl-main.695,D13-1105,1,0.825814,"nominal forms. Their proposed baseline uses distributional context for POS tagging and features, but does not train embeddings as the corpus is small. 2.2 Subtasks of Paradigm Discovery A few works address subtasks of the PDP. Erdmann and Habash (2018) learn paradigm membership from raw text, but do not sort paradigms into cells. Boyé and Schalchli (2019) discuss the paradigm cell finding problem, identifying the cell (but not paradigm) realized by a given form. Lee (2015) clusters forms into cells across inflection classes. Beniamine et al. (2018) group paradigms into inflection classes, and Eskander et al. (2013) induce inflection classes and lemmata from cell labels. 2.3 The Paradigm Cell Filling Problem The PCFP is the task of predicting unseen inflected forms given morphologically labeled input. PCFP models can guess a word’s plural having only seen its singular, but the child must bootstrap morphological knowledge from scratch, first learning that singular–plural is a relevant distinction. Thus, the PDP must be at least partially solved before the PCFP can be attempted. Yet, as a supervised task, the PCFP is more easily studied, and has received much attention on its own, especially from the word-"
2020.acl-main.695,W99-0904,0,0.49395,"Missing"
2020.acl-main.695,J01-2001,0,0.284034,"Missing"
2020.acl-main.695,K19-1014,1,0.855013,"onal data. The use of orthographic data for morphological tasks is problematic, but standard in the field, due to scarcity of phonologically transcribed data (Malouf et al., 2020). 7780 Predictions paradigm 1 paradigm 2 paradigm 3 paradigm 4 cell 1 watched followed «seed» «seened» cell 2 watching «following» «seeing» «seening» cell 3 «watches» follows «sees» «seens» cell 4 «watch» «follow» see seen Table 2: Toy predictions made from the corpus and lexicon in Table 1, to be evaluated against the toy gold grid. Again, bracketed «forms» are those not occurring in the lexicon. in UD and UniMorph (Gorman et al., 2019; Malouf et al., 2020). Unlike the gold grid, the lexicon retains overabundant realizations, requiring systems to handle such phenomena. For each language, the raw sentences used to augment the corpus add over 1 million additional words. For German and Russian, we sample sentences from OpenSubtitles (Lison and Tiedemann, 2016), for Latin, the Latin Library (Johnson et al., 2016), and for English and Arabic, Gigaword (Parker et al., 2011a,b). Supplementary sentences are preprocessed via Moses (Koehn et al., 2007) to split punctuation, and, for supported languages, clitics. Table 3 shows corpus"
2020.acl-main.695,J97-4004,0,0.109447,"m the same cell. Our algorithm greedily builds paradigms cell by cell. To gauge the quality of a candidate paradigm, we first identify its base and exponents. Following Beniamine et al. (2018), we define π’s base, bπ , as the longest common subsequence shared by all forms in π.56 For each form f in π, we define the exponent xf as the subsequences of f that remain after removing bπ , i.e., xf is a tuple of affixes. For example, if π contains words wxyxz and axx, bπ is xx and the exponents are (&lt;w, y, z&gt;) and (&lt;a), respectively.7 Inspired by unsupervised maximum matching in greedy tokenization (Guo, 1997; Erdmann et al., 2019), we define the following paradigm score function: score(π) = X   |bπ |− |xf | (2) 1: 2: 3: 4: 5: 6: 7: 8: fj0 ∈Cj 9: 10: 11: 12: 13: 5 The fact that we use a subsequence, instead of a substring, means that we can handle non-concatenative morphology. 6 We note that the longest common subsequence may be found with a polynomial-time dynamic program; however, there will not exist an algorithm whose runtime is polynomial in the number of strings unless P = NP (Maier, 1978). 7 We use word start (&lt;) and end (&gt;) tokens to distinguish exponents; they do not count as exponent c"
2020.acl-main.695,2020.acl-main.598,0,0.247281,"ect supervision to bootstrap their models. We discuss each in turn. First, Dreyer and Eisner (2011) use a generative model to cluster forms into paradigms and cells with a Bayesian non-parametric mixture of weighted finite-state transducers. They present a PDP framework which, in principle, could be fully unsupervised, but their model requires a small seed of labeled data to get key information like the number of cells distinguished, making it less relevant cognitively. In contrast, our task is not directly supervised and focuses on distributional context. Second, contemporaneous to our work, Jin et al. (2020) propose a similar framework for SIGMORPHON 2020’s shared task on unsupervised morphological paradigm completion. Given only a small corpus and lexicon of verbal lemmata, participating systems must propose full paradigms for each lemma. By contrast, our framework does not reveal how many paradigms should be generated, nor do we privilege a specific form as the lemma, but we do use a larger lexicon of exclusively verbal or nominal forms. Their proposed baseline uses distributional context for POS tagging and features, but does not train embeddings as the corpus is small. 2.2 Subtasks of Paradig"
2020.acl-main.695,W18-3605,0,0.0295523,"phological inflection generation (Cotterell et al., 2016a); this is only a difference in nomenclature. 2 Elsner et al. (2019) call the task the paradigm cell discovery problem; we drop cell to distinguish our task from As an unsupervised task, the PDP poses challenges for modeling and evaluation and has yet to be attempted in its full form (Elsner et al., 2019). However, we contend there is much to be gained from formalizing and studying the PDP. There are insights for cognitive modeling to be won (Pinker, 2001; Goldwater, 2007) and intuitions on combating sparse data for language generation (King and White, 2018) to be accrued. Unsupervised language processing also has natural applications in the documentation of endangered languages (Zamaraeva et al., 2019) where a lot of annotated data is never likely to exist. Our formalization of the PDP offers a starting point for future work on unsupervised morphological paradigm completion. Our paper presents a concrete formalization of the PDP. Then, as a baseline for future work, we introduce a heuristic benchmark system. Our benchmark system takes an unannotated text corpus and a lexicon of words from the corpus to be analyzed. It first clusters the lexicon"
2020.acl-main.695,L18-1293,1,0.928002,"erell et al., 2019). Neural models can test and extend such proposals (Cotterell et al., 2018a; Silfverberg and Hulden, 2018). A related vein of work discusses how speakers inflect nonce words (Berko, 1958; Plunkett and Juola, 1999; Yang, 2015), e.g., is the past tense of sping, spinged or spung? There is a long tradition of modeling past-tense generation with neural networks (Rumelhart and McClelland, 1986; Kirov and Cotterell, 2018; Corkery et al., 2019). On the engineering side, Durrett and DeNero (2013) inspired much recent work, which has since benefited from large inflectional datasets (Kirov et al., 2018) and advances in neural sequence modeling (Bahdanau et al., 2015). Shared tasks have drawn extra attention to the PCFP (Cotterell et al., 2016a, 2017, 2018c; McCarthy et al., 2019). 3 The Paradigm Discovery Problem Paradigm discovery is a natural next step in computational morphology, building on related minimally or indirectly supervised works (§2.2) to bridge the gap between unsupervised traditions (§2.1) and supervised work on the PCFP (§2.3). In the PCFP, 7779 Corpus The cat watched me watching it . I followed the show but she had n’t seen it . Let ’s see who follows your logic . Lexicon w"
2020.acl-main.695,P07-2045,0,0.00621406,"ain, bracketed «forms» are those not occurring in the lexicon. in UD and UniMorph (Gorman et al., 2019; Malouf et al., 2020). Unlike the gold grid, the lexicon retains overabundant realizations, requiring systems to handle such phenomena. For each language, the raw sentences used to augment the corpus add over 1 million additional words. For German and Russian, we sample sentences from OpenSubtitles (Lison and Tiedemann, 2016), for Latin, the Latin Library (Johnson et al., 2016), and for English and Arabic, Gigaword (Parker et al., 2011a,b). Supplementary sentences are preprocessed via Moses (Koehn et al., 2007) to split punctuation, and, for supported languages, clitics. Table 3 shows corpus and lexicon sizes. 3.3 Metrics A system attemping the PDP is expected to output a morphologically organized grid in which rows and columns are arbitrarily ordered, but ideally, each row corresponds to a gold paradigm and each column to a gold cell. Aligning rows to paradigms and columns to cells is non-trivial, making it difficult to simply compute accuracy over gold grid slots. Furthermore, cluster-based metrics (Rosenberg and Hirschberg, 2007) are difficult to apply as forms can appear in multiple columns or r"
2020.acl-main.695,N15-2022,0,0.0209498,"digms should be generated, nor do we privilege a specific form as the lemma, but we do use a larger lexicon of exclusively verbal or nominal forms. Their proposed baseline uses distributional context for POS tagging and features, but does not train embeddings as the corpus is small. 2.2 Subtasks of Paradigm Discovery A few works address subtasks of the PDP. Erdmann and Habash (2018) learn paradigm membership from raw text, but do not sort paradigms into cells. Boyé and Schalchli (2019) discuss the paradigm cell finding problem, identifying the cell (but not paradigm) realized by a given form. Lee (2015) clusters forms into cells across inflection classes. Beniamine et al. (2018) group paradigms into inflection classes, and Eskander et al. (2013) induce inflection classes and lemmata from cell labels. 2.3 The Paradigm Cell Filling Problem The PCFP is the task of predicting unseen inflected forms given morphologically labeled input. PCFP models can guess a word’s plural having only seen its singular, but the child must bootstrap morphological knowledge from scratch, first learning that singular–plural is a relevant distinction. Thus, the PDP must be at least partially solved before the PCFP ca"
2020.acl-main.695,L16-1147,0,0.0156269,"eening» cell 3 «watches» follows «sees» «seens» cell 4 «watch» «follow» see seen Table 2: Toy predictions made from the corpus and lexicon in Table 1, to be evaluated against the toy gold grid. Again, bracketed «forms» are those not occurring in the lexicon. in UD and UniMorph (Gorman et al., 2019; Malouf et al., 2020). Unlike the gold grid, the lexicon retains overabundant realizations, requiring systems to handle such phenomena. For each language, the raw sentences used to augment the corpus add over 1 million additional words. For German and Russian, we sample sentences from OpenSubtitles (Lison and Tiedemann, 2016), for Latin, the Latin Library (Johnson et al., 2016), and for English and Arabic, Gigaword (Parker et al., 2011a,b). Supplementary sentences are preprocessed via Moses (Koehn et al., 2007) to split punctuation, and, for supported languages, clitics. Table 3 shows corpus and lexicon sizes. 3.3 Metrics A system attemping the PDP is expected to output a morphologically organized grid in which rows and columns are arbitrarily ordered, but ideally, each row corresponds to a gold paradigm and each column to a gold cell. Aligning rows to paradigms and columns to cells is non-trivial, making it diffi"
2020.acl-main.695,2020.scil-1.52,0,0.0134249,"i.e., slots realized by multiple forms, and remove all but the most frequently attested realization in UD. While some languages permit overabundance (Thornton, 2010), it often indicates typographical or annotation errors 3 Aligning UniMorph and UD requires removing diacritics in (Latin and Arabic) UniMorph corpora to match UD. This can obscure some morphosyntactic distinctions but is more consistent with natural orthography in distributional data. The use of orthographic data for morphological tasks is problematic, but standard in the field, due to scarcity of phonologically transcribed data (Malouf et al., 2020). 7780 Predictions paradigm 1 paradigm 2 paradigm 3 paradigm 4 cell 1 watched followed «seed» «seened» cell 2 watching «following» «seeing» «seening» cell 3 «watches» follows «sees» «seens» cell 4 «watch» «follow» see seen Table 2: Toy predictions made from the corpus and lexicon in Table 1, to be evaluated against the toy gold grid. Again, bracketed «forms» are those not occurring in the lexicon. in UD and UniMorph (Gorman et al., 2019; Malouf et al., 2020). Unlike the gold grid, the lexicon retains overabundant realizations, requiring systems to handle such phenomena. For each language, the"
2020.acl-main.695,W18-6011,1,0.85496,"lar to the corpus created by Vylomova et al. (2019). As a system does not know which lexicon forms will be evaluated in the gold grid, it must model the entire lexicon, which should contain a realistic distribution over rare words and inflection classes having been directly extracted from distributional data (Bybee, 2003; Lignos and Yang, 2018). To ensure the gold grid is reasonably clean, we take all word–lemma–feature tuples from the UD portion of the corpus matching the specified POS and convert the features to a morphosyntactic cell identifier compatible with UniMorph representation as in McCarthy et al. (2018).3 Then we check which word–lemma–cell tuples also occur in UniMorph. For each unique lemma in this intersection, the full paradigm is added as a row to the gold grid. To filter typos and annotation discrepancies, we identify any overabundant slots, i.e., slots realized by multiple forms, and remove all but the most frequently attested realization in UD. While some languages permit overabundance (Thornton, 2010), it often indicates typographical or annotation errors 3 Aligning UniMorph and UD requires removing diacritics in (Latin and Arabic) UniMorph corpora to match UD. This can obscure some"
2020.acl-main.695,W19-4226,1,0.679152,"ect nonce words (Berko, 1958; Plunkett and Juola, 1999; Yang, 2015), e.g., is the past tense of sping, spinged or spung? There is a long tradition of modeling past-tense generation with neural networks (Rumelhart and McClelland, 1986; Kirov and Cotterell, 2018; Corkery et al., 2019). On the engineering side, Durrett and DeNero (2013) inspired much recent work, which has since benefited from large inflectional datasets (Kirov et al., 2018) and advances in neural sequence modeling (Bahdanau et al., 2015). Shared tasks have drawn extra attention to the PCFP (Cotterell et al., 2016a, 2017, 2018c; McCarthy et al., 2019). 3 The Paradigm Discovery Problem Paradigm discovery is a natural next step in computational morphology, building on related minimally or indirectly supervised works (§2.2) to bridge the gap between unsupervised traditions (§2.1) and supervised work on the PCFP (§2.3). In the PCFP, 7779 Corpus The cat watched me watching it . I followed the show but she had n’t seen it . Let ’s see who follows your logic . Lexicon watching, seen, follows, watched, followed, see Gold Grid paradigm 1 paradigm 2 paradigm 3 cell 1 «watch» «follow» see cell 2 «watches» follows «sees» cell 3 watching «following» «s"
2020.acl-main.695,Q15-1012,0,0.338073,"Missing"
2020.acl-main.695,L16-1262,0,0.0610659,"Missing"
2020.acl-main.695,D14-1162,0,0.0838528,"ark system for proposing a morphologically organized grid given a corpus and lexicon. First, we cluster lexicon forms into cells. Then we cluster forms into paradigms given their fixed cell membership. To maintain tractability, clustering assumes a one-to-one mapping of forms to slots. Following cell and paradigm clustering, we predict forms to realize empty slots given one of the lexicon forms assigned to a cell in Clustering into Cells We use a heuristic method to determine the number of cells and what lexicon forms to assign to each. Inspired by work on inductive biases in word embeddings (Pennington et al., 2014; Trask et al., 2015; Goldberg, 2016; Avraham and Goldberg, 2017; Tu et al., 2017), we train morphosyntactically biased embeddings on the corpus and use them to k-means cluster lexicon forms into cells. Following Erdmann et al. (2018), we emphasize morphosyntactically salient dimensions in embedding space by manipulating hyperparameters in fastText (Bojanowski et al., 2017). Specifically, to encourage grouping of morphologically related words, fastText computes a word’s embedding as the sum of its subword embeddings for all subword sequences between 3 and 6 characters long (Schütze, 1993). We"
2020.acl-main.695,D07-1043,0,0.0730775,"d (Parker et al., 2011a,b). Supplementary sentences are preprocessed via Moses (Koehn et al., 2007) to split punctuation, and, for supported languages, clitics. Table 3 shows corpus and lexicon sizes. 3.3 Metrics A system attemping the PDP is expected to output a morphologically organized grid in which rows and columns are arbitrarily ordered, but ideally, each row corresponds to a gold paradigm and each column to a gold cell. Aligning rows to paradigms and columns to cells is non-trivial, making it difficult to simply compute accuracy over gold grid slots. Furthermore, cluster-based metrics (Rosenberg and Hirschberg, 2007) are difficult to apply as forms can appear in multiple columns or rows. Thus, we propose novel metrics that are lexical, based on analogical relationships between forms. We propose a set of PDP metrics, to measure how well organized lexicon forms are in the grid, and a set of PCFP metrics, to measure how well the system anticipates unattested inflectional variants. All metrics support non-canonical phenomena such as defective paradigms and overabundant slots. 3.3.1 PDP Metrics A form f ’s paradigm mates are all those forms that co-occur in at least one paradigm with f . f ’s paradigm F-score"
2020.acl-main.695,D18-1315,0,0.0493291,"ical knowledge from scratch, first learning that singular–plural is a relevant distinction. Thus, the PDP must be at least partially solved before the PCFP can be attempted. Yet, as a supervised task, the PCFP is more easily studied, and has received much attention on its own, especially from the word-and-paradigm camp of morphological theory. Some cognitive works suggest the PCFP cannot be too difficult for any language (Dale et al., 1998; Ackerman and Malouf, 2013, 2015; Blevins et al., 2017; Cotterell et al., 2019). Neural models can test and extend such proposals (Cotterell et al., 2018a; Silfverberg and Hulden, 2018). A related vein of work discusses how speakers inflect nonce words (Berko, 1958; Plunkett and Juola, 1999; Yang, 2015), e.g., is the past tense of sping, spinged or spung? There is a long tradition of modeling past-tense generation with neural networks (Rumelhart and McClelland, 1986; Kirov and Cotterell, 2018; Corkery et al., 2019). On the engineering side, Durrett and DeNero (2013) inspired much recent work, which has since benefited from large inflectional datasets (Kirov et al., 2018) and advances in neural sequence modeling (Bahdanau et al., 2015). Shared tasks have drawn extra attention"
2020.acl-main.695,N19-1203,1,0.854484,"DP For a given language and POS, we create a corpus, lexicon, and gold grid based on a Universal Dependencies (UD) corpus (Nivre et al., 2016). At a high level, the corpus includes raw, non-UD sentences, and UD sentences stripped of annotations. The lexicon includes all forms occurring in the UD sentences with the specified POS (potentially including variant spellings and typographical errors). The gold grid consists of full paradigms for every word which co-occurs in UD and the UniMorph lexicon (Kirov et al., 2018) with a matching lemma–cell analysis; this is similar to the corpus created by Vylomova et al. (2019). As a system does not know which lexicon forms will be evaluated in the gold grid, it must model the entire lexicon, which should contain a realistic distribution over rare words and inflection classes having been directly extracted from distributional data (Bybee, 2003; Lignos and Yang, 2018). To ensure the gold grid is reasonably clean, we take all word–lemma–feature tuples from the UD portion of the corpus matching the specified POS and convert the features to a morphosyntactic cell identifier compatible with UniMorph representation as in McCarthy et al. (2018).3 Then we check which word–l"
2020.acl-main.695,D18-1268,0,0.0934499,"Missing"
2020.acl-main.695,W19-6005,0,0.0193758,"radigm cell discovery problem; we drop cell to distinguish our task from As an unsupervised task, the PDP poses challenges for modeling and evaluation and has yet to be attempted in its full form (Elsner et al., 2019). However, we contend there is much to be gained from formalizing and studying the PDP. There are insights for cognitive modeling to be won (Pinker, 2001; Goldwater, 2007) and intuitions on combating sparse data for language generation (King and White, 2018) to be accrued. Unsupervised language processing also has natural applications in the documentation of endangered languages (Zamaraeva et al., 2019) where a lot of annotated data is never likely to exist. Our formalization of the PDP offers a starting point for future work on unsupervised morphological paradigm completion. Our paper presents a concrete formalization of the PDP. Then, as a baseline for future work, we introduce a heuristic benchmark system. Our benchmark system takes an unannotated text corpus and a lexicon of words from the corpus to be analyzed. It first clusters the lexicon by cell and then by paradigm making use of distributional semantics and string similarity. Finally, it uses this clustering as silver-standard super"
2020.acl-main.695,W17-2632,0,0.0231066,"st, we cluster lexicon forms into cells. Then we cluster forms into paradigms given their fixed cell membership. To maintain tractability, clustering assumes a one-to-one mapping of forms to slots. Following cell and paradigm clustering, we predict forms to realize empty slots given one of the lexicon forms assigned to a cell in Clustering into Cells We use a heuristic method to determine the number of cells and what lexicon forms to assign to each. Inspired by work on inductive biases in word embeddings (Pennington et al., 2014; Trask et al., 2015; Goldberg, 2016; Avraham and Goldberg, 2017; Tu et al., 2017), we train morphosyntactically biased embeddings on the corpus and use them to k-means cluster lexicon forms into cells. Following Erdmann et al. (2018), we emphasize morphosyntactically salient dimensions in embedding space by manipulating hyperparameters in fastText (Bojanowski et al., 2017). Specifically, to encourage grouping of morphologically related words, fastText computes a word’s embedding as the sum of its subword embeddings for all subword sequences between 3 and 6 characters long (Schütze, 1993). We shorten this range to 2 to 4 to bias the grouping toward shared affixes rather tha"
2020.acl-main.695,Q17-1010,0,\N,Missing
2020.scil-1.4,N19-1156,0,0.0145681,"cular, we present firm evidence that variably-realized marking makes fusion less likely while durational vowel reduction has the opposite effect. While authors like Plank (1999) have listed many independent features or elements which characterize prototypically “fusional” morphology, these have typically been discussed as typological clusters, without necessarily providing a causal explanation. Our modeling results give a mechanism in which some of these features precede, and give rise to, others. A variety of researchers have noted (Greenberg, 1966) and attempted to discover (Murawaki, 2018; Bjerva et al., 2019) correlations between typological features. Harris (2008) suggests that in many cases, such correlations reflect precisely this kind of historical mechanism— the likelihood that a language will develop in some typological direction is dependent on the features it already has, some of which may encourage a particular change while others tend to reinforce existing patterns. While the simulations presented here use artificial data, we hope to apply this model to real corpus data from languages in which fusion might be developing, in order to isolate particular changes in the phonology as the “tri"
2020.scil-1.4,Q15-1031,0,0.172813,"ze to other words, as we saw above for Greek, or undergo further diachronic changes. We leave modeling such changes for future work. The model (Figure 1) formalizes our intuitions about agglutinative and fusional analyses of morphological systems. In order to do so, it represents morphemes as invariant underlying representations and applies phonological processes that transform them into surface forms. Because the popular sequence-to-sequence framework for inflection (Kann and Sch¨utze, 2016) conflates these processes within a single neural network, we choose instead to extend an older model, Cotterell et al. (2015), in which these components are separate. While this model may be less capable overall, it is more interpretable in terms of the theoretical questions we are trying to answer. Cotterell et al. model the correspondence between sequences of abstract morphemes and surface strings. The term “abstract morpheme” refers to a set of MSPs that already reflect the effects of fusion— in the context of agglutination, each abstract morpheme is a single MSP, whereas for fusion, the abstract morphemes bundle together many MSPs. The model maps abstract morphemes to surface strings in the following steps: firs"
2020.scil-1.4,W16-2010,0,0.0492304,"Missing"
2020.scil-1.4,D18-1468,0,0.0140366,"stable. In particular, we present firm evidence that variably-realized marking makes fusion less likely while durational vowel reduction has the opposite effect. While authors like Plank (1999) have listed many independent features or elements which characterize prototypically “fusional” morphology, these have typically been discussed as typological clusters, without necessarily providing a causal explanation. Our modeling results give a mechanism in which some of these features precede, and give rise to, others. A variety of researchers have noted (Greenberg, 1966) and attempted to discover (Murawaki, 2018; Bjerva et al., 2019) correlations between typological features. Harris (2008) suggests that in many cases, such correlations reflect precisely this kind of historical mechanism— the likelihood that a language will develop in some typological direction is dependent on the features it already has, some of which may encourage a particular change while others tend to reinforce existing patterns. While the simulations presented here use artificial data, we hope to apply this model to real corpus data from languages in which fusion might be developing, in order to isolate particular changes in the"
2020.scil-1.58,P17-1183,0,0.113599,"introduced the use of attention-based neural sequence-to-sequence learning for the inflection task, building on models from machine translation (Bahdanau et al., 2014). Their model treats input as a linear series where grammatical features and characters are encoded as one-hot embeddings and passed to a bidirectional encoder LSTM; output for each paradigm cell is produced by a separate decoder. Kann and Sch¨utze (2016) extended Faruqui et al.’s architecture by using ensembling and by using a single decoder, shared across all output paradigm cells, to account for data sparsity. Later systems (Aharoni and Goldberg, 2017; Kann and Sch¨utze, 2017) have made changes to the input representation and the architecture, for instance incorporating variants of hard attention and autoencoding. From a theoretical standpoint, all these models are “a-morphous” (Anderson, 1992) or “inferentialrealizational” (Stump, 2001)— rather than assume a concatenative process which stitches discrete morphemes together into surface word forms, they learn a flexible, generalizable transduction, either between a stem and surface form (Anderson, 1992; Stump, 2001), or between pairs of surface forms (Albright, 2002; Blevins, 2006). Some ol"
2020.scil-1.58,N13-1138,0,0.356552,"-to-sequence models excel at learning inflectional paradigms from incomplete input (Table 1 shows an example inflection problem.) These models, originally borrowed from neural machine translation (Bahdanau et al., 2014), read in a series of input tokens (e.g. characters, words) and output, or translate, them as another series. Although these models have become adept at mapping input to output sequences, like all neural models, they are relatively uninterpretable. We present a novel error analysis technique, based on previous systems for learning to inflect which relied on edit rule induction (Durrett and DeNero, 2013). By using this to interpret the output of a neural model, we can group errors into linguistically salient classes such as producing the wrong case form or incorrect inflection class. Our broader linguistic contribution is to reconnect the inflection task to the descriptive literature on morphological systems. Neural models for inflection are now being applied as cognitive models of human learning in a variety of settings (Malouf, 2017; Silfverberg and Hulden, 2018; Kirov and Cotterell, 2018, and others). They are appealing cognitive models partly because of their high performance on benchmark"
2020.scil-1.58,N16-1077,0,0.150328,"n et al., 2009; Ackerman and Malouf, 2013; Albright, 2002; Bonami and Beniamine, 2016; Sims and Parker, 2016). Ackerman et al.’s (2009) formulation of the PCFP relies on a simple concatenative model in which words are divided into stems and affixes, and in which each affix is treated as a discrete value. Cotterell et al. (2018) points out that this model is ill-suited to dealing with phenomena like phonological alterations or stem suppletion. Newer models (Silfverberg and Hulden, 2018; Malouf, 2017; Cotterell et al., 2018) use sequenceto-sequence inflection models to avoid these shortcomings. Faruqui et al. (2016) introduced the use of attention-based neural sequence-to-sequence learning for the inflection task, building on models from machine translation (Bahdanau et al., 2014). Their model treats input as a linear series where grammatical features and characters are encoded as one-hot embeddings and passed to a bidirectional encoder LSTM; output for each paradigm cell is produced by a separate decoder. Kann and Sch¨utze (2016) extended Faruqui et al.’s architecture by using ensembling and by using a single decoder, shared across all output paradigm cells, to account for data sparsity. Later systems ("
2020.scil-1.58,N18-1108,0,0.0309276,", and subsq.), and also because they make few assumptions about the morphological system they are trying to model, dispensing with overly restrictive notions of segmentable morphemes and discrete inflection classes. But while these constructs are theoretically troublesome, they are still important for describing many commonly-studied languages; without them, it is relatively difficult to discover what a particular model has and has not learned about a morphological system. This is often the key question which prevents us from using a general-purpose neural network system as a cognitive model (Gulordava et al., 2018). Our error analysis allows us to understand more clearly how the sequence-to-sequence model diverges from human behavior, giving us new information about its suitability as a cognitive model of the language learner. As a case study, we apply our error analysis technique to Russian, one of the lowestperforming languages in SIGMORPHON 2016. We find a large class of errors in which the model incorrectly selects among lexically- or semantically-conditioned allomorphs. Russian has semantically-conditioned allomorphy in nouns and adjectives, and lexically-conditioned allomorphy (inflection classes)"
2020.scil-1.58,P06-4018,0,0.448835,"Missing"
2020.scil-1.58,P13-1045,0,0.0417162,"by the MED system on the 2016 SIGMORPHON dataset. An 7 is an incorrect prediction and the 3 below is the gold wordform. Empty set symbols (;) indicate an erroneous insertion. 5 Model Improvements In this section, we incorporate a proxy for lexical semantics into the model input representations, leading to improved results. This is useful from a practical standpoint, but also as a clear demonstration that semantic conditioning was responsible for many of the errors which we discussed in the previous section. As our source for semantic information, we use word embeddings (Mikolov et al., 2013; Socher et al., 2013; Xu et al., 2015). We concatenate the output from the bidirectional encoder with the citation form’s embedding. Equipped with this information, the model should be able to learn phenomena like the animacy-dependent syncretism Gold Predicted ABSOLJUTISTA ˇ ABSOLJU Sˇ CISTA ˇ I Sˇ ’ SJA DER Z ˇ AE Sˇ ’ SJA DER Z ABDOMEN ABDOMENA Rule - T + Sˇ + Cˇ -I+A+E +A Annotation +d+i+i +d+i+i +i Category phonological alternation verb class animacy Table 5: An example of the annotation we performed, where ‘-’ indicates ‘missing’ and ‘+’ indicates ‘erroneous’. Additionally, ‘i’ indicates ‘insertion’ and ‘d’"
2020.scil-1.58,L16-1498,0,0.0436251,"on set, and do not represent enough data to conduct statistical analyses by category or paradigm cell. To further break down the improvements quantitatively, we created secondary evaluation sets containing more items. For nouns, we created a secondary evaluation set with the Universal Dependency RusSynTag corpus6 since it annotates both animacy and gender. We removed any nouns that did not have a 1-to-1 feature correspondence with the SIGMORPHON dataset.7 This gave us a new evaluation set of 48,590 wordforms. Similarly, we also built a second evaluation set of 25,000 verb forms from Unimorph (Kirov et al., 2016). Although verb conjugation class is not directly annotated, we extracted that information from the second person singular present indicative form. In both cases, we removed any word form that also occurred in the training data. As seen in Table 11, using word embeddings almost halved the error rate of e-conjugation verbs. It is important to note that the citation form supplied often requires less editing to make an iconjugation verb than an e-conjugation verb since the citation form often has the -i theme vowel. Since the model has a strong preference for reproducing the input, our modificati"
2020.scil-1.58,D13-1006,0,0.0675082,"Missing"
2020.scil-1.58,P15-2119,0,0.0606796,"Missing"
2020.scil-1.58,D18-1315,0,0.0990621,". We present a novel error analysis technique, based on previous systems for learning to inflect which relied on edit rule induction (Durrett and DeNero, 2013). By using this to interpret the output of a neural model, we can group errors into linguistically salient classes such as producing the wrong case form or incorrect inflection class. Our broader linguistic contribution is to reconnect the inflection task to the descriptive literature on morphological systems. Neural models for inflection are now being applied as cognitive models of human learning in a variety of settings (Malouf, 2017; Silfverberg and Hulden, 2018; Kirov and Cotterell, 2018, and others). They are appealing cognitive models partly because of their high performance on benchmark tasks (Cotterell et al., 2016, and subsq.), and also because they make few assumptions about the morphological system they are trying to model, dispensing with overly restrictive notions of segmentable morphemes and discrete inflection classes. But while these constructs are theoretically troublesome, they are still important for describing many commonly-studied languages; without them, it is relatively difficult to discover what a particular model has and has not"
2020.scil-1.58,W18-5423,0,0.0488809,"lay in morphology, namely that no corpus will ever exist that has every wordform from every lexeme. For theoretical morphologists, the difficulty of the PCFP on average is a measure of the learnability of a morphological system, with implications for language typology (Ackerman et al., 2009; Ackerman and Malouf, 2013; Albright, 2002; Bonami and Beniamine, 2016; Sims and Parker, 2016). Ackerman et al.’s (2009) formulation of the PCFP relies on a simple concatenative model in which words are divided into stems and affixes, and in which each affix is treated as a discrete value. Cotterell et al. (2018) points out that this model is ill-suited to dealing with phenomena like phonological alterations or stem suppletion. Newer models (Silfverberg and Hulden, 2018; Malouf, 2017; Cotterell et al., 2018) use sequenceto-sequence inflection models to avoid these shortcomings. Faruqui et al. (2016) introduced the use of attention-based neural sequence-to-sequence learning for the inflection task, building on models from machine translation (Bahdanau et al., 2014). Their model treats input as a linear series where grammatical features and characters are encoded as one-hot embeddings and passed to a bi"
2020.scil-1.58,P15-2041,0,0.0275897,"the 2016 SIGMORPHON dataset. An 7 is an incorrect prediction and the 3 below is the gold wordform. Empty set symbols (;) indicate an erroneous insertion. 5 Model Improvements In this section, we incorporate a proxy for lexical semantics into the model input representations, leading to improved results. This is useful from a practical standpoint, but also as a clear demonstration that semantic conditioning was responsible for many of the errors which we discussed in the previous section. As our source for semantic information, we use word embeddings (Mikolov et al., 2013; Socher et al., 2013; Xu et al., 2015). We concatenate the output from the bidirectional encoder with the citation form’s embedding. Equipped with this information, the model should be able to learn phenomena like the animacy-dependent syncretism Gold Predicted ABSOLJUTISTA ˇ ABSOLJU Sˇ CISTA ˇ I Sˇ ’ SJA DER Z ˇ AE Sˇ ’ SJA DER Z ABDOMEN ABDOMENA Rule - T + Sˇ + Cˇ -I+A+E +A Annotation +d+i+i +d+i+i +i Category phonological alternation verb class animacy Table 5: An example of the annotation we performed, where ‘-’ indicates ‘missing’ and ‘+’ indicates ‘erroneous’. Additionally, ‘i’ indicates ‘insertion’ and ‘d’ deletion, so ‘-i’"
2021.scil-1.10,2021.scil-1.20,0,0.0762441,"Missing"
2021.scil-1.10,Q19-1021,0,0.0279213,"rstand paradigm shape as an organizing principle of inflectional systems, we want to model stem and affix distributions jointly. The second line of research reflects complementary insights and complementary problems. Coming from the literature on inflectional complexity, it consists of work that uses informationtheoretic measures, predominantly conditional entropy, to measure the average uncertainty associated with the unobserved form realizing some paradigm cell, given one or more observed forms of the same lexeme (Ackerman et al., 2009; Ackerman and Malouf, 2013; Bonami and Beniamine, 2016; Cotterell et al., 2019; Mansfield, 2016; Parker and Sims, 2020; Sims and Parker, 2016; Stump and Finkel, 2013). This work tends to be typological in focus. The information-theoretic approach has proven popular for quantifying paradigmatic relations in a gradient way. At the same time, this literature has tended to abstract away stem alternations in order to focus on affixes and other ‘primary’ inflectional exponence (e.g. Ackerman and Malouf, 2013), although there are exceptions (Parker and Sims, 2020). This reflects in part a tendency to rely on hand segmentation of words into morphological exponents, a problemati"
2021.sigmorphon-1.18,2020.scil-1.19,0,0.215307,"udies the system’s ability to apply different types of morphological processes using constructed stimuli, showing that some configurations are capable of learning generic and transferable representations of processes including prefixing, suffixing and reduplication. 2 Related work The overall positive effect of transfer learning is well established (McCarthy et al., 2019). Previous research has also evaluated how the choice of source language affects the performance in the target. While there is a robust trend for related languages to perform better, there are also many reports of exceptions. Kann (2020) finds that Hungarian is a better source for English than German and a better source for Spanish than Italian. She concludes that matching the target language’s default affix placement (prefixing/suffixing) is important, and that agglutinative languages might be beneficial to transfer learning in general, but that genetic relatedness is not always a necessary or sufficient for effective transfer. Lin et al. (2019) also find that Hungarian and Turkish are good source languages for a surprising variety of unrelated targets. Rather than attribute this to agglutination, they propose that these lan"
2021.sigmorphon-1.18,E17-1049,0,0.0455152,"Missing"
2021.sigmorphon-1.18,P17-1182,0,0.0258589,"Missing"
2021.sigmorphon-1.18,2020.coling-main.257,0,0.0606667,"Missing"
2021.sigmorphon-1.18,2020.sigmorphon-1.22,0,0.195172,"tion, aiding its ability to transfer. The paper argues that the degree of generality of these process representations also helps to explain transfer results from previous research. 1 Introduction Morphological transfer learning has proven to be a powerful and effective technique for improving the performance of inflection models on underresourced languages. The beneficial effects of transfer between source and target languages are known to be higher when the two are closely related (Anastasopoulos and Neubig, 2019) or typologically similar (Lin et al., 2019), mediated by the effect of script (Murikinati et al., 2020). But these effects are not always consistent; a variety of researchers report failure of transfer between closely related languages, or surprising successes with rather dissimilar ones (Sec 2). Pushing forward our understanding of these cases requires a more nuanced understanding of what is transferred by morphological transfer learning— that is, what abstract representational concepts do inflection networks acquire and how are these shared across languages? This is a difficult question to address in the standard framework for inflection (Kann and Sch¨utze, 2016), in which morphosyntactic pro"
2021.sigmorphon-1.18,2020.scil-1.22,0,0.0283401,"common in Germanic and Uralic; Oto-Manguean tonal morphology is also often represented via word-final diacritics.10 Prefixing is more common in the Niger-Congo family. Reduplication appears in three of the four Austronesian development languages, Tagalog, Hiligaynon and Cebuano (WAL, 2013), but not in the Maori dataset provided. The probe language has partial reduplication of the first syllable, as found in Tagalog and Hiligaynon. Previous work with artificial data demonstrates that sequence-to-sequence learners can learn fully abstract representations of reduplication (Prickett et al., 2018; Nelson et al., 2020; Haley and Wilson, 2021), but it has not been previously shown that networks trained on real data do this in a transferable way. In one-shot language transfer, reduplication instances are actually ambiguous. Given an instance modi : :: gobu : gogobu, there are two plausible interpretations, reduplicative momodi and affixal gomodi. Thus, analysis of reduplicative instances can be informative about the model’s learned linkage between language family and typology. Gemination is a inflectional process whereby a segment is lengthened to mark some morphological feature (Samek-Lodovici, 1992). The p"
2021.sigmorphon-1.18,W16-2005,0,0.0287481,"tem relies on an exemplar verb as the target specifier; shown here are karanga “call”, which takes a matching suffix, and kaukau “swim”, which mismatches. find that such examples are complementary to data hallucination and yield improved results in datasparse settings. Some earlier non-neural models also rely on stored word forms (Skousen, 1989; Albright and Hayes, 2002). 3 evaluated using instances generated using random selection. To perform similarity-based selection, each lemma is aligned with its target form in the training data in order to extract an edit rule (Durrett and DeNero, 2013; Nicolai et al., 2016). (For the first memory-based example in Figure 1, both words have the same edit rule -+tia.) The selected exemplar/form pair uses the same edit rule, if possible. During training, a lemma is allowed to act as its own exemplar, so that there is always at least one candidate. However, words in the test set must be given exemplars from the training set. If a cell in the test set does not appear in the training set, no prediction can be made; in this case, the system outputs the lemma. Extending the model to cover this case is discussed below as future work.6 Exemplar selection The system uses in"
2021.sigmorphon-1.18,W18-5810,0,0.0266156,"d. Suffixation is more common in Germanic and Uralic; Oto-Manguean tonal morphology is also often represented via word-final diacritics.10 Prefixing is more common in the Niger-Congo family. Reduplication appears in three of the four Austronesian development languages, Tagalog, Hiligaynon and Cebuano (WAL, 2013), but not in the Maori dataset provided. The probe language has partial reduplication of the first syllable, as found in Tagalog and Hiligaynon. Previous work with artificial data demonstrates that sequence-to-sequence learners can learn fully abstract representations of reduplication (Prickett et al., 2018; Nelson et al., 2020; Haley and Wilson, 2021), but it has not been previously shown that networks trained on real data do this in a transferable way. In one-shot language transfer, reduplication instances are actually ambiguous. Given an instance modi : :: gobu : gogobu, there are two plausible interpretations, reduplicative momodi and affixal gomodi. Thus, analysis of reduplicative instances can be informative about the model’s learned linkage between language family and typology. Gemination is a inflectional process whereby a segment is lengthened to mark some morphological feature (Samek-L"
2021.sigmorphon-1.18,2021.naacl-main.435,0,0.0429274,"cated, while reduplication requires phonological information about vowels and consonants. 8 Discussion These experiments with real and synthetic transfer suggest some useful insights into the problematic findings of earlier transfer experiments. Why 12 The random-exemplar model has low accuracy for reduplication in Tagalog because it appends spurious Tagalog prefixes to the output, another example of a language-specific rule. However, the regular expression check confirms that reduplication is performed correctly. 13 Because of this poor performance, Cyrillic gemination was not tested. 14 See Silfverberg et al. (2021) for a fuller investigation of generalizable representations of gradation processes in Finnish noun paradigms. Where transfer between related languages fails, it is conjecturally possible that the source model representations of edit operations are too closely linked to particular phonological and lexical properties of the source. This is clearly shown in the synthetic transfer experiments, where generic suffixation fails in Germanic and Uralic despite these families being strongly suffixing, because the system has learned to remodel its outputs to conform too closely to source-language templa"
2021.sigmorphon-1.18,K17-2010,0,0.0137003,"strings between 2-5 characters long and infixes are 1-2 characters long. (This means that, in some cases, no affix is added and the transformation is the identity, as occurs in cases of morphological syncretism.) An example such instance is mpieˇnjmel:rbeaikkea::zlurbeaikkea ue ¨ ¨ with output zlumpieˇ njmelue. ¨ ¨ The language tags for these examples indicate the kinds of affixation operations which were performed, for example LANG PREFIX SUFFIX; the family tag identifies them as SYNTHETIC. While this synthetic dataset is inspired by hallucination techniques (Anastasopoulos and Neubig, 2019; Silfverberg et al., 2017), note that these synthetic instances are not presented to the model as part of any natural language. The Sigmorphon 2020 data is divided into “development languages” (45 languages in 5 families: Austronesian, Germanic, Niger-Congo, OtoManguean and Uralic) and “surprise languages” (45 more languages, including some members of development families as well as unseen families). Data from all the “development languages”, plus the synthetic examples from the previous stage, is used to train a multilingual model, which is finetuned family. Finally the family models are finetuned by language. During"
2021.sigmorphon-1.18,2021.eacl-main.163,0,0.0471404,"Missing"
D13-1005,P12-2017,0,0.0347966,"Missing"
D13-1005,P13-1148,0,0.105815,"Missing"
D13-1005,W11-0601,0,0.030909,"Missing"
D13-1005,D08-1113,0,0.10302,"Missing"
D13-1005,P12-1020,1,0.511537,"learning word meanings (Bergelson and Swingley, 2012). These developmental cooccurrences lead some researchers to propose that phonetic and word learning occur jointly, each one informing the other (Swingley, 2009; Feldman et al., 2013). Previous computational models capture some aspects of this joint learning problem, but typically simplify the problem considerably, either by assuming an unrealistic degree of phonetic regularity for word segmentation (Goldwater et al., 2009) or assuming pre-segmented input for phonetic and lexical acquisition (Feldman et al., 2009; Feldman et al., in press; Elsner et al., 2012). This paper presents, to our knowledge, the first broadcoverage model that learns to segment phonetically variable input into words, while simultaneously learning an explicit model of phonetic variation that allows it to cluster together segmented tokens with different phonetic realizations (e.g., [ju] and [jI]) into lexical items (/ju/). We base our model on the Bayesian word segmentation model of Goldwater et al. (2009) (henceforth GGJ), using a noisy-channel setup where phonetic variation is introduced by a finite-state transducer (Neubig et al., 2010; Elsner et al., 2012). This integrated"
D13-1005,P08-1016,0,0.588196,"want, ... juwant, ... Probabilities for each word (sparse) G0 0 α p(ði) = .1, p(a) = .05, p(want) = .01... Conditional probabilities for each word after each word Gx 1 ∞ contexts p(ði |want) = .3, p(a |want) = .1, p(want |want) = .0001... Intended forms T x1 x2 ... s1 s2 ... Surface forms n utterances Related Work Nearly all computational models used to explore the problems addressed here have treated the learning tasks in isolation. Examples include models of word segmentation from phonemic input (Christiansen et al., 1998; Brent, 1999; Venkataraman, 2001; Swingley, 2005) or phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2011; Boruta et al., 2011), models of phonetic clustering (Vallabha et al., 2007; Varadarajan et al., 2008; Dupoux et al., 2011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic learning model of Feldman et al. (2009; in press)) their model assumes known word bou"
D13-1005,P08-1046,0,0.0260435,"esembles GGJ’s on clean data, and therefore the behavior of human learners. 5.2 Phonetic variability We next analyze the model’s ability to normalize variations in the pronunciation of tokens, by inspecting the mtk score. The “segment only” baseline is predictably poor, F : 44.8. The pipeline model scores 48.8, and our oracle transducer model matches this exactly. The EM transducer scores better, F : 49.6. Although the confidence intervals overlap slightly, the EM system also outperforms the pipeline on the other F -measures; altogether, these results suggest at least a weak learning synergy (Johnson, 2008) between segmentation and phonetic learning. It is interesting that EM can perform better than the oracle. However, EM is more conservative about which sound changes it will allow, and thus tends to avoid mistakes caused by the simplicity of the transducer model. Since the transducer works segmentby-segment, it can apply rare contextual variations out of context. EM benefits from not learning these variations to begin with. We can also compare the bigram and unigram versions of the model. The unigram model is a reasonable segmenter, though not quite as good as the bigram model, with boundary F"
D13-1005,P09-1012,0,0.61566,"s /2/, the word is likely to be /w2n/ “one” and the next word begins with /t/; if instead we posit that the vowel is /O/, the word is probably /wOnt/ “want”. Thus, inference methods that change only one character at a time are unlikely to mix well. Since they cannot simultaneously change the vowel and resegment the /t/, they must pass through a low-probability intermediate state to get from one state to the other, so will tend to get stuck in a bad local minimum. A Gibbs sampler which inserts or deletes a single segment boundary in each step (Goldwater et al., 2009) suffers from this problem. Mochihashi et al. (2009) describe an inference method with higher mobility: a block sampler for the GGJ model that samples from the posterior over analyses of a whole utterance at once. This method encodes the model as a large HMM, using dynamic programming to select an analysis. We encode our own model in the same way, constructing the HMM and composing it with the transducer (Mohri, 2004) to form a larger finite-state machine which is still amenable to forward-backward sampling. 4.1 Finite-state encoding Following Mochihashi et al. (2009) and Neubig et al. (2010), we can write the original GGJ model as a Hidden Sem"
D13-1005,P08-2042,0,0.0457234,"r each word after each word Gx 1 ∞ contexts p(ði |want) = .3, p(a |want) = .1, p(want |want) = .0001... Intended forms T x1 x2 ... s1 s2 ... Surface forms n utterances Related Work Nearly all computational models used to explore the problems addressed here have treated the learning tasks in isolation. Examples include models of word segmentation from phonemic input (Christiansen et al., 1998; Brent, 1999; Venkataraman, 2001; Swingley, 2005) or phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2011; Boruta et al., 2011), models of phonetic clustering (Vallabha et al., 2007; Varadarajan et al., 2008; Dupoux et al., 2011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic learning model of Feldman et al. (2009; in press)) their model assumes known word boundaries, so to perform both segmentation and lexical-phonetic learning, they use a pipeline that first segments using GGJ and then applies their mode"
D13-1005,J01-3002,0,0.0639611,"GGJ 09 Generator for possible words a, b, ..., ju, ... want, ... juwant, ... Probabilities for each word (sparse) G0 0 α p(ði) = .1, p(a) = .05, p(want) = .01... Conditional probabilities for each word after each word Gx 1 ∞ contexts p(ði |want) = .3, p(a |want) = .1, p(want |want) = .0001... Intended forms T x1 x2 ... s1 s2 ... Surface forms n utterances Related Work Nearly all computational models used to explore the problems addressed here have treated the learning tasks in isolation. Examples include models of word segmentation from phonemic input (Christiansen et al., 1998; Brent, 1999; Venkataraman, 2001; Swingley, 2005) or phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2011; Boruta et al., 2011), models of phonetic clustering (Vallabha et al., 2007; Varadarajan et al., 2008; Dupoux et al., 2011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic learning model of Feldman et al. (200"
D17-1112,Q14-1008,0,0.0382233,"Missing"
D17-1112,D13-1005,1,0.84421,"ave focused on detecting phonological boundaries between words using transitional probabilities (Christiansen et al., 1998, among others) or inducing words procedurally by “subtracting” known word forms from utterances (Lignos, 2011). All these modeling architectures are designed to work with phonemically transcribed input, and require some degree of retrofitting to work with more realistic inputs. In the Bayesian framework, this typically takes the form of a transducer which probabilistically transforms “underlying” lexical items to “surface” acoustics (Lee et al., 2015) or discrete symbols (Elsner et al., 2013); the same framework is used for morphological segmentation in Cotterell et al. (2015). For transitionbased models, the input must be transformed into discrete symbols from which segment-to-segment probabilities can be extracted; this transformation requires an externally trained preprocessor (a phone recognizer). Transition-based models are fairly robust to variation in the symbols (Rytting, 2007; Rytting et al., 2010; Daland and Pierrehumbert, 2011; Fleck, 2008) and can be relatively successful in this framework. Extensions using neural nets (Christiansen et al., 1998; Rytting et al., 2010)"
D17-1112,P08-1016,0,0.225221,"nguage processing domains, yet are still underused in unsupervised learning. Even systems which do use neural nets to model lexical acquisition generally require an auxiliary model for clustering the embeddings, which can make their learning objectives difficult to understand. Our system uses the well-understood autoencoder objective to perform the segmentation task without requiring auxiliary clustering, and thus suggests a new direction for neural unsupervised learning. In an experiment conducted on the widely used Brent corpus (Brent, 1999), our system achieves performance close to that of Fleck (2008), although subsequent systems outperform ours by a wider margin. We show that memory limitations do indeed drive the performance of the system, with smaller LSTM hidden states outperforming larger ones in the development set. In a follow-up experiment designed to ex1 The system is available from https://github. com/melsner/neural-segmentation. 1070 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1070–1080 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics plore the flexibility of our model, we deploy the seg"
D17-1112,D07-1031,0,0.018142,"2003). An example of an alternative approach to representation learning from acoustics is R¨as¨anen et al. (2015). They exploit known acoustic indicators of syllable boundaries to infer syllable segments, cluster those segments using expectationmaximization (EM), and then identify multisyllabic words by searching for recurring cluster ngrams. As a result, their system is constrained to propose word boundaries only at proposed syllable boundaries regardless of the representations acquired downstream. Furthermore, EM is known to find non-optimal solutions for many problems in natural language (Johnson, 2007). To the extent that this inhibits their system’s ability to exploit information in the acoustic feature space, it might lead to misidentification of recurrent syllable n-grams and consequently to segmentation error. Latent underlying representations can also cause search problems, since the model must explore all the possible underlying forms which might map to some utterance on the surface. In a probabilistic system capable of mapping every word to every possible realization, this quickly becomes intractable. Many systems use dynamic programming (Mochihashi et al., 2009; Neubig et al., 2010)"
D17-1112,N09-1036,0,0.191175,"benefits of our adaptation of neural sequence modeling to unsupervised learning. 2 Motivations We begin with a short overview of previous approaches to the word learning problem, then explain each of our main contributions in detail. Many cognitive models of the word learning problem draw on Brent (1999), which used a simple unigram model of the lexicon to discover repeated patterns in phonemically transcribed input. Brent’s model laid the groundwork for later generative models with more sophisticated prior distributions over word frequencies, co-occurrence statistics and phonological shapes (Johnson and Goldwater, 2009, among others). Other modeling architectures for segmentation have focused on detecting phonological boundaries between words using transitional probabilities (Christiansen et al., 1998, among others) or inducing words procedurally by “subtracting” known word forms from utterances (Lignos, 2011). All these modeling architectures are designed to work with phonemically transcribed input, and require some degree of retrofitting to work with more realistic inputs. In the Bayesian framework, this typically takes the form of a transducer which probabilistically transforms “underlying” lexical items"
D17-1112,P12-1005,0,0.033576,"he model. 2.2 Input representations As stated above, traditional segmentation models operate on phonemic transcriptions and must be adapted to cope with phonetic or acoustic input. For models which infer an explicit lexicon (i.e., those which do not simply count segment transitions), this takes the form of a mapping between the data and the space of “underlying” latent word forms. Learning such a mapping can be problematic. Traditional generative learning models use parametric distributions over the data— for acoustics, Gaussians (Vallabha et al., 2007; Feldman et al., 2009) or Gaussian-HMMs (Lee and Glass, 2012; Lee et al., 2015). But these are a notoriously poor fit to real speech sounds (Glass, 2003). An example of an alternative approach to representation learning from acoustics is R¨as¨anen et al. (2015). They exploit known acoustic indicators of syllable boundaries to infer syllable segments, cluster those segments using expectationmaximization (EM), and then identify multisyllabic words by searching for recurring cluster ngrams. As a result, their system is constrained to propose word boundaries only at proposed syllable boundaries regardless of the representations acquired downstream. Further"
D17-1112,Q15-1028,0,0.0298526,"Missing"
D17-1112,W11-0304,0,0.0241269,"h used a simple unigram model of the lexicon to discover repeated patterns in phonemically transcribed input. Brent’s model laid the groundwork for later generative models with more sophisticated prior distributions over word frequencies, co-occurrence statistics and phonological shapes (Johnson and Goldwater, 2009, among others). Other modeling architectures for segmentation have focused on detecting phonological boundaries between words using transitional probabilities (Christiansen et al., 1998, among others) or inducing words procedurally by “subtracting” known word forms from utterances (Lignos, 2011). All these modeling architectures are designed to work with phonemically transcribed input, and require some degree of retrofitting to work with more realistic inputs. In the Bayesian framework, this typically takes the form of a transducer which probabilistically transforms “underlying” lexical items to “surface” acoustics (Lee et al., 2015) or discrete symbols (Elsner et al., 2013); the same framework is used for morphological segmentation in Cotterell et al. (2015). For transitionbased models, the input must be transformed into discrete symbols from which segment-to-segment probabilities c"
D17-1112,P09-1012,0,0.0308418,"problems in natural language (Johnson, 2007). To the extent that this inhibits their system’s ability to exploit information in the acoustic feature space, it might lead to misidentification of recurrent syllable n-grams and consequently to segmentation error. Latent underlying representations can also cause search problems, since the model must explore all the possible underlying forms which might map to some utterance on the surface. In a probabilistic system capable of mapping every word to every possible realization, this quickly becomes intractable. Many systems use dynamic programming (Mochihashi et al., 2009; Neubig et al., 2010), sometimes with pruning (Van Gael et al., 2008). But these algorithms require Markov models with small context windows, and in any case can still be slow and prone to search errors. Neural nets, on the other hand, learn a nonlinear mapping between input and output. This allows them to model speech more flexibly, outcompeting Gaussian/HMMs for supervised speech recognition (Graves et al., 2013; Hinton et al., 2012). Recurrent neural nets also produce hidden representations differently than HMMs. Rather than use dynamic programming to search a latent space, they produce a"
D17-1112,D11-1014,0,0.00831054,"optimized, and the system’s lexical learning does not inform its phonetic representations. Even outside the domain of segmentation, neural networks have been most successful for supervised problems, and are not widely used for unsupervised learning of discrete structures (trees, clusters, segment boundaries). While some researchers have proposed information-theoretic objectives for learning clusters (Klapper-Rybicka et al., 2001), the most widely used unsupervised objective is the one used here: autoencoding. Yet autoencoders are rarely used to learn discrete hidden structures. One exception, Socher et al. (2011), uses autencoders to find a latent tree structure for sentiment analysis by greedily merging adjacent nodes so as to minimize the reconstruction error. Chung et al. (2017) describe a model similar to our own which performs a segmentation task using autoencoders. Both models use multiscale autoencoding to learn a sequence model with unknown segment boundaries. The main difference is the different technique used to deal with the discontinuities caused by switching discrete segment boundary variables. However, they evaluate their model on downstream tasks (notably, character language modeling) w"
D17-1112,Q15-1031,0,\N,Missing
E09-1018,W99-0613,0,0.0972512,"Missing"
E09-1018,N04-4009,0,0.107532,"ion.) The test set has 1119 personal pronouns of which 246 are non-anaphoric. Our selection of this dataset, rather than the widely used MUC-6 corpus, is motivated by this large number of pronouns. We compared our results to four currentlyavailable anaphora programs from the web. These four were selected by sending a request to a commonly used mailing list (the “corpora-list”) asking for such programs. We received four leads: JavaRAP, Open-NLP, BART and GuiTAR. Of course, these systems represent the best available work, not the state of the art. We presume that more recent supervised systems (Kehler et al., 2004b; Yang et al., 2004; Yang et al., 2006) perDefinition of Correctness We evaluate all programs according to Mitkov’s “resolution etiquette” scoring metric (also used in Cherry and Bergsma (2005)), which is defined as follows: if N is the number of non-anaphoric pronouns correctly identified, A the number of anaphoric pronouns correctly linked to their antecedent, and P the total number of pronouns, then a pronoun-anaphora program’s percentage correct is NP+A . Most papers dealing with pronoun coreference use this simple ratio, or the variant that ignores non-anaphoric pronouns. It has appeared"
E09-1018,N04-1037,0,0.0285598,"ion.) The test set has 1119 personal pronouns of which 246 are non-anaphoric. Our selection of this dataset, rather than the widely used MUC-6 corpus, is motivated by this large number of pronouns. We compared our results to four currentlyavailable anaphora programs from the web. These four were selected by sending a request to a commonly used mailing list (the “corpora-list”) asking for such programs. We received four leads: JavaRAP, Open-NLP, BART and GuiTAR. Of course, these systems represent the best available work, not the state of the art. We presume that more recent supervised systems (Kehler et al., 2004b; Yang et al., 2004; Yang et al., 2006) perDefinition of Correctness We evaluate all programs according to Mitkov’s “resolution etiquette” scoring metric (also used in Cherry and Bergsma (2005)), which is defined as follows: if N is the number of non-anaphoric pronouns correctly identified, A the number of anaphoric pronouns correctly linked to their antecedent, and P the total number of pronouns, then a pronoun-anaphora program’s percentage correct is NP+A . Most papers dealing with pronoun coreference use this simple ratio, or the variant that ignores non-anaphoric pronouns. It has appeared"
E09-1018,J94-4002,0,0.0490232,"led error chaining (Walker, 1989)), both x and y are to be scored as wrong, as they both end up in the wrong coreferential chain. We believe this is, in fact, the standard (Mitkov, personal communication), although 3 Of course our system does not attempt NP coreference resolution, nor does JavaRAP. The other three comparison systems do. 153 System GuiTAR JavaRap Open-NLP Our System form better. Unfortunately, we were unable to obtain a comparison unsupervised learning system at all. Only one of the four is explicitly aimed at personal-pronoun anaphora — RAP (Resolution of Anaphora Procedure) (Lappin and Leass, 1994). It is a non-statistical system originally implemented in Prolog. The version we used is JavaRAP, a later reimplementation in Java (Long Qiu and Chua, 2004). It only handles third person pronouns. The other three are more general in that they handle all NP anaphora. The GuiTAR system (Poesio and Kabadjov, 2004) is designed to work in an “off the shelf” fashion on general text GUITAR resolves pronouns using the algorithm of (Mitkov et al., 2002), which filters candidate antecedents and then ranks them using morphosyntactic features. Due to a bug in version 3, GUITAR does not currently handle p"
E09-1018,J93-2003,0,0.0192564,"Missing"
E09-1018,qiu-etal-2004-public,0,0.088315,"e standard (Mitkov, personal communication), although 3 Of course our system does not attempt NP coreference resolution, nor does JavaRAP. The other three comparison systems do. 153 System GuiTAR JavaRap Open-NLP Our System form better. Unfortunately, we were unable to obtain a comparison unsupervised learning system at all. Only one of the four is explicitly aimed at personal-pronoun anaphora — RAP (Resolution of Anaphora Procedure) (Lappin and Leass, 1994). It is a non-statistical system originally implemented in Prolog. The version we used is JavaRAP, a later reimplementation in Java (Long Qiu and Chua, 2004). It only handles third person pronouns. The other three are more general in that they handle all NP anaphora. The GuiTAR system (Poesio and Kabadjov, 2004) is designed to work in an “off the shelf” fashion on general text GUITAR resolves pronouns using the algorithm of (Mitkov et al., 2002), which filters candidate antecedents and then ranks them using morphosyntactic features. Due to a bug in version 3, GUITAR does not currently handle possessive pronouns.GUITAR also has an optional discoursenew classification step, which cannot be used as it requires a discontinued Google search API. OpenNL"
E09-1018,P02-1011,0,0.0180899,"pon a distribution p(anaphora|context). The nature of the “context” is discussed below. Then given the antecedent we generative the pronoun’s person according to p(person|antecedent), the pronoun’s gender according to p(gender|antecedent), number, p(number|antecedent) and governor/relationto-governor from p(governor/relation|antecedent). To generate a non-anaphoric third person singular “it” we first guess that the non-anaphoric pronouns is “it” according to p(“it”|non-anaphoric). 2 Actually, as in most previous work, we only consider referents realized by NPs. For more general approaches see Byron (2002). 150 Word paul paula pig piggy wal-mart waist probability mass, so each one will only get an increase of 0.05. Thus on the first iteration only the first two sentences have the power to move the distributions, but they do, and they make NPs in the current sentence very slightly more likely to generate the pronoun than the sentence one back, which in turn is more likely than the ones two back. This slight imbalance is reflected when EM readjusts the probability distribution at the end of the first iteration. Thus for the second iteration everyone contributes to subsequent imbalances, because i"
E09-1018,W99-0611,0,0.380923,"es. First we only concern ourselves with “personal” pronouns: “I”, “you”, “he”, “she”, “it”, and their variants. We ignore, e.g., relative pronouns (“who”, “which”, etc.), deictic pronouns (“this”, “that”) and others. Personal pronouns come in four basic types: subject “I”, “she”, etc. Used in subject position. object “me”, “her” etc. Used in non-subject position. possessive “my” “her”, and reflexive “myself”, “herself” etc. Required by English grammar in certain constructions — e.g., “I kicked myself.” There are also several papers which treat coference as an unsupervised clustering problem (Cardie and Wagstaff, 1999; Angheluta et al., 2004). In this literature there is no generative model at all, and thus this work is only loosely connected to the above models. The system described here handles all of these cases. Note that the type of a pronoun is not connected with its antecedent, but rather is completely determined by the role it plays in it’s sentence. Personal pronouns are either anaphoric or nonanaphoric. We say that a pronoun is anaphoric when it is coreferent with another piece of text in the same discourse. As is standard in the field we distinguish between a referent and an antecedent. The refe"
E09-1018,P05-1022,1,0.0355759,"ing a non-anaphoric “it”. Lastly we have a probability of generating each of the other nonmonotonic pronouns along with (the nth root of) their governor. These parameters are 6, 0.1, and 0.0004 respectively. 6 7 Evaluation To develop and test our program we use the dataset annotated by Niyu Ge (Ge et al., 1998). This consists of sections 0 and 1 of the Penn treebank. Ge marked every personal pronoun and all noun phrases that were coreferent with these pronouns. We used section 0 as our development set, and section 1 for testing. We reparsed the sentences using the Charniak and Johnson parser (Charniak and Johnson, 2005) rather than using the gold-parses that Ge marked up. We hope thereby to make the results closer to those a user will experience. (Generally the gold trees perform about 0.005 higher than the machine parsed version.) The test set has 1119 personal pronouns of which 246 are non-anaphoric. Our selection of this dataset, rather than the widely used MUC-6 corpus, is motivated by this large number of pronouns. We compared our results to four currentlyavailable anaphora programs from the web. These four were selected by sending a request to a commonly used mailing list (the “corpora-list”) asking fo"
E09-1018,W05-0612,0,0.544311,"to find good models for the tasks to which it is set, in this case it works quite well. We have compared it to several systems available on the web (all we have found so far). Our program significantly outperforms all of them. The algorithm is fast and robust, and has been made publically available for downloading. 1 2 Previous Work The literature on pronominal anaphora is quite large, and we cannot hope to do justice to it here. Rather we limit ourselves to particular papers and systems that have had the greatest impact on, and similarity to, ours. Probably the closest approach to our own is Cherry and Bergsma (2005), which also presents an EM approach to pronoun resolution, and obtains quite successful results. Our work improves upon theirs in several dimensions. Firstly, they do not distinguish antecedents of non-reflexive pronouns based on syntax (for instance, subjects and objects). Both previous work (cf. Tetreault (2001) discussed below) and our present results find these distinctions extremely helpful. Secondly, their system relies on a separate preprocessing stage to classify non-anaphoric pronouns, and mark the gender of certain NPs (Mr., Mrs. and some first names). This allows the incorporation"
E09-1018,poesio-kabadjov-2004-general,0,0.427236,"Missing"
E09-1018,J98-2001,0,0.0144778,"Missing"
E09-1018,J01-4004,0,0.102185,"edents and then ranks them using morphosyntactic features. Due to a bug in version 3, GUITAR does not currently handle possessive pronouns.GUITAR also has an optional discoursenew classification step, which cannot be used as it requires a discontinued Google search API. OpenNLP (Morton et al., 2005) uses a maximum-entropy classifier to rank potential antecedents for pronouns. However despite being the best-performing (on pronouns) of the existing systems, there is a remarkable lack of published information on its innards. BART (Versley et al., 2008) also uses a maximum-entropy model, based on Soon et al. (2001). The BART system also provides a more sophisticated feature set than is available in the basic model, including tree-kernel features and a variety of web-based knowledge sources. Unfortunately we were not able to get the basic version working. More precisely we were able to run the program, but the results we got were substantially lower than any of the other models and we believe that the program as shipped is not working properly. Some of these systems provide their own preprocessing tools. However, these were bypassed, so that all systems ran on the Charniak parse trees (with gold sentence"
E09-1018,J01-4003,0,0.264839,"es it is measure forward from the start of the sentence. • syntactic positions — generally we expect NPs in subject position to be more likely antecedents than those in object position, and those more likely than other positions (e.g., object of a preposition). • position of the pronoun — for example the subject of the previous sentence is very likely to be the antecedent if the pronoun is very early in the sentence, much less likely if it is at the end. • type of pronoun — reflexives can only be bound within the same sentence, while sub152 5.3 Parameters Not Set by EM there are a few papers (Tetreault, 2001; Yang et al., 2006) which do the opposite and many which simply do not discuss this case. One more issue arises in the case of a system attempting to perform complete NP anaphora3 . In these cases the coreferential chains they create may not correspond to any of the original chains. In these cases, we call a pronoun correctly resolved if it is put in a chain including at least one correct non-pronominal antecedent. This definition cannot be used in general, as putting all NPs into the same set would give a perfect score. Fortunately, the systems we compare against do not do this – they seem m"
E09-1018,P08-4003,0,0.0144991,"algorithm of (Mitkov et al., 2002), which filters candidate antecedents and then ranks them using morphosyntactic features. Due to a bug in version 3, GUITAR does not currently handle possessive pronouns.GUITAR also has an optional discoursenew classification step, which cannot be used as it requires a discontinued Google search API. OpenNLP (Morton et al., 2005) uses a maximum-entropy classifier to rank potential antecedents for pronouns. However despite being the best-performing (on pronouns) of the existing systems, there is a remarkable lack of published information on its innards. BART (Versley et al., 2008) also uses a maximum-entropy model, based on Soon et al. (2001). The BART system also provides a more sophisticated feature set than is available in the basic model, including tree-kernel features and a variety of web-based knowledge sources. Unfortunately we were not able to get the basic version working. More precisely we were able to run the program, but the results we got were substantially lower than any of the other models and we believe that the program as shipped is not working properly. Some of these systems provide their own preprocessing tools. However, these were bypassed, so that"
E09-1018,P89-1031,0,0.305948,"c pronouns. It has appeared under a number of names: success (Yang et al., 2006), accuracy (Kehler et al., 2004a; Angheluta et al., 2004) and success rate (Tetreault, 2001). The other occasionally-used metric is the MUC score restricted to pronouns, but this has well-known problems (Bagga and Baldwin, 1998). To make the definition perfectly concrete, however, we must resolve a few special cases. One is the case in which a pronoun x correctly says that it is coreferent with another pronoun y. However, the program misidentifies the antecedent of y. In this case (sometimes called error chaining (Walker, 1989)), both x and y are to be scored as wrong, as they both end up in the wrong coreferential chain. We believe this is, in fact, the standard (Mitkov, personal communication), although 3 Of course our system does not attempt NP coreference resolution, nor does JavaRAP. The other three comparison systems do. 153 System GuiTAR JavaRap Open-NLP Our System form better. Unfortunately, we were unable to obtain a comparison unsupervised learning system at all. Only one of the four is explicitly aimed at personal-pronoun anaphora — RAP (Resolution of Anaphora Procedure) (Lappin and Leass, 1994). It is a"
E09-1018,P04-1017,0,0.0266244,"1119 personal pronouns of which 246 are non-anaphoric. Our selection of this dataset, rather than the widely used MUC-6 corpus, is motivated by this large number of pronouns. We compared our results to four currentlyavailable anaphora programs from the web. These four were selected by sending a request to a commonly used mailing list (the “corpora-list”) asking for such programs. We received four leads: JavaRAP, Open-NLP, BART and GuiTAR. Of course, these systems represent the best available work, not the state of the art. We presume that more recent supervised systems (Kehler et al., 2004b; Yang et al., 2004; Yang et al., 2006) perDefinition of Correctness We evaluate all programs according to Mitkov’s “resolution etiquette” scoring metric (also used in Cherry and Bergsma (2005)), which is defined as follows: if N is the number of non-anaphoric pronouns correctly identified, A the number of anaphoric pronouns correctly linked to their antecedent, and P the total number of pronouns, then a pronoun-anaphora program’s percentage correct is NP+A . Most papers dealing with pronoun coreference use this simple ratio, or the variant that ignores non-anaphoric pronouns. It has appeared under a number of n"
E09-1018,P06-1006,0,0.0654905,"forward from the start of the sentence. • syntactic positions — generally we expect NPs in subject position to be more likely antecedents than those in object position, and those more likely than other positions (e.g., object of a preposition). • position of the pronoun — for example the subject of the previous sentence is very likely to be the antecedent if the pronoun is very early in the sentence, much less likely if it is at the end. • type of pronoun — reflexives can only be bound within the same sentence, while sub152 5.3 Parameters Not Set by EM there are a few papers (Tetreault, 2001; Yang et al., 2006) which do the opposite and many which simply do not discuss this case. One more issue arises in the case of a system attempting to perform complete NP anaphora3 . In these cases the coreferential chains they create may not correspond to any of the original chains. In these cases, we call a pronoun correctly resolved if it is put in a chain including at least one correct non-pronominal antecedent. This definition cannot be used in general, as putting all NPs into the same set would give a perfect score. Fortunately, the systems we compare against do not do this – they seem more likely to over-s"
E09-1018,W98-1119,1,\N,Missing
E09-1018,P07-1107,0,\N,Missing
E12-1065,H05-1073,0,0.133073,"transitions and are coherent. Since it focuses only on events, however, it cannot enforce a global notion of what the characters want or how they relate to one another. Our own work draws on representations that explicitly model emotions rather than events. Alm and Sproat (2005) were the first to describe stories in terms of an emotional trajectory. They annotate emotional states in 22 Grimms’ fairy tales and discover an increase in emotion (mostly positive) toward the ends of stories. They later use this corpus to construct a reasonably accurate classifier for emotional states of sentences (Alm et al., 2005). Volkova et al. (2010) extend the human annotation approach using a larger number of emotion categories and applying them to freelydefined chunks instead of sentences. The largestscale emotional analysis is performed by Mohammad (2011), using crowd-sourcing to construct a large emotional lexicon with which he analyzes adult texts such as plays and novels. In this work, we adopt the concept of emotional trajectory, but apply it to particular characters rather than works as a whole. In focusing on characters, we follow Elson et al. (2010), who analyze narratives by examining their social networ"
E12-1065,P05-1018,0,0.011917,"spoilerfree” summaries focusing on characters, settings and themes, in order to attract potential readers. They do find it useful to detect character mentions, and also use features based on verb aspect to automatically exclude plot events while retaining descriptive passages. They compare their genrespecific system with a few state-of-the-art methods for summarizing news, and find it outperforms them substantially. We evaluate our system by comparing real novels to artificially produced surrogates, a procedure previously used to evaluate models of discourse coherence (Karamanis et al., 2004; Barzilay and Lapata, 2005) and models of syntax (Post, 2011). As in these settings, we anticipate that performance on this kind of task will be correlated with performance in applied settings, so we use it as an easier preliminary test of our capabilities. 3 Dataset We focus on the 19th century novel, partly following Elson et al. (2010) and partly because these texts are freely available via Project Gutenberg. Our main dataset is composed of romances (which we loosely define as novels focusing on a courtship or love affair). We select 41 texts, taking 11 as a development set and the remaining 30 as a test set; a compl"
E12-1065,P09-1068,0,0.204308,"first step toward generation—a recognition model can always be used as part of a generate-and-rank pipeline, and potentially its underlying representation can be used in more sophisticated ways. We show a detailed analysis of the character correspondences discovered by our system, and discuss their potential relevance to summarization, in section 9. 2 Related work Some recent work on story understanding has focused on directly modeling the series of events that occur in the narrative. McIntyre and Lapata (2010) create a story generation system that draws on earlier work on narrative schemas (Chambers and Jurafsky, 2009). Their system ensures that generated stories contain plausible event-to-event transitions and are coherent. Since it focuses only on events, however, it cannot enforce a global notion of what the characters want or how they relate to one another. Our own work draws on representations that explicitly model emotions rather than events. Alm and Sproat (2005) were the first to describe stories in terms of an emotional trajectory. They annotate emotional states in 22 Grimms’ fairy tales and discover an increase in emotion (mostly positive) toward the ends of stories. They later use this corpus to"
E12-1065,N01-1007,0,0.0597319,"ll identical mentions which contain more than two words (leaving bare first or last names unmerged). Next, we heuristically assign each mention a gender (masculine, feminine or neuter) using a list of gendered titles, then a list of male and female first names2 . We then merge mentions where each is longer than one word, the genders do not clash, 2 The most frequent names from the 1990 US census. 17 14 10 7 7 Table 1: Top five stemmed unigram dependency features for “Miss Elizabeth Bennet”, protagonist of Pride and Prejudice, and their frequencies. and the first and last names are consistent (Charniak, 2001). We then merge single-word mentions with matching multiword mentions if they appear in the same paragraph, or if not, with the multiword mention that occurs in the most paragraphs. When this process ends, we have resolved each mention in the novel to some specific character. As in previous work, we discard very infrequent characters and their mentions. For the reasons stated, this method is errorprone. Our intuition is that the simpler method described in Elson et al. (2010), which merges each mention to the most recent possible coreferent, must be even more so. However, due to the expense of"
E12-1065,N09-1042,0,0.0422284,"other by measuring their frequency of occurrence over time and the descriptive and emotional language associated with them. Given a corpus of 19thcentury novels as training data, our method can accurately distinguish held-out novels in their original form from artificially disordered or reversed surrogates, demonstrating its ability to robustly represent important aspects of plot structure. 1 Introduction Every culture has stories, and storytelling is one of the key functions of human language. Yet while we have robust, flexible models for the structure of informative documents (for instance (Chen et al., 2009; Abu Jbara and Radev, 2011)), current approaches have difficulty representing the narrative structure of fictional stories. This causes problems for any task requiring us to model fiction, including summarization and generation of stories; Kazantseva and Szpakowicz (2010) show that state-of-the-art summarizers perform extremely poorly on short fictional texts1 . A major problem with applying models for informative 1 Apart from Kazantseva, we know of one other attempt to apply a modern summarizer to fiction, by the artist Jason Huff, using Microsoft Word 2008’s extractive summary feature: http"
E12-1065,C10-1032,0,0.0174567,"racter, and record character frequencies and relationships over time. 4.1 Identifying characters We create a list of possible character references for each work by extracting all strings of proper nouns (as detected by the parser), then discarding those which occur less than 5 times. Grouping these into a useful character list is a problem of cross-document coreference. Although cross-document coreference has been extensively studied (Bhattacharya and Getoor, 2005) and modern systems can achieve quite high accuracy on the TAC-KBP task, where the list of available entities is given in advance (Dredze et al., 2010), novelistic text poses a significant challenge for the methods normally used. The typical 19th-century novel contains many related characters, often named after one another. There are complicated social conventions determining which titles are used for whom—for instance, the eldest unmarried daughter of a family can be called “Miss Bennet”, while her younger sister must be “Miss Elizabeth Bennet”. And characters often use nicknames, such as “Lizzie”. Our system uses the multi-stage clustering approach outlined in Bhattacharya and Getoor (2005), but with some features specific to 19th century"
E12-1065,elson-mckeown-2010-building,0,0.0490003,"ather than works as a whole. In focusing on characters, we follow Elson et al. (2010), who analyze narratives by examining their social network relationships. They use an automatic method based on quoted speech to find social links between characters in 19th century novels. Their work, designed for computational literary criticism, does not extract any temporal or emotional structure. A few projects attempt to represent story structure in terms of both characters and their emotional states. However, they operate at a very detailed level and so can be applied only to short texts. Scheherazade (Elson and McKeown, 2010) allows human annotators to mark character goals and emotional states in a narrative, and indicate the causal links between them. AESOP (Goyal et al., 2010) attempts to learn a similar structure automatically. AESOP’s accuracy, however, is relatively poor even on short fables, indicating that this fine-grained approach is unlikely to be scalable to novel-length texts; our system relies on a much coarser analysis. Kazantseva and Szpakowicz (2010) summarize short stories, although unlike the other projects we discuss here, they explicitly try to avoid giving away plot details—their goal is to cr"
E12-1065,P10-1015,0,0.104555,"bly accurate classifier for emotional states of sentences (Alm et al., 2005). Volkova et al. (2010) extend the human annotation approach using a larger number of emotion categories and applying them to freelydefined chunks instead of sentences. The largestscale emotional analysis is performed by Mohammad (2011), using crowd-sourcing to construct a large emotional lexicon with which he analyzes adult texts such as plays and novels. In this work, we adopt the concept of emotional trajectory, but apply it to particular characters rather than works as a whole. In focusing on characters, we follow Elson et al. (2010), who analyze narratives by examining their social network relationships. They use an automatic method based on quoted speech to find social links between characters in 19th century novels. Their work, designed for computational literary criticism, does not extract any temporal or emotional structure. A few projects attempt to represent story structure in terms of both characters and their emotional states. However, they operate at a very detailed level and so can be applied only to short texts. Scheherazade (Elson and McKeown, 2010) allows human annotators to mark character goals and emotiona"
E12-1065,D10-1008,0,0.0467304,"Missing"
E12-1065,P04-1050,0,0.126048,"heir goal is to create “spoilerfree” summaries focusing on characters, settings and themes, in order to attract potential readers. They do find it useful to detect character mentions, and also use features based on verb aspect to automatically exclude plot events while retaining descriptive passages. They compare their genrespecific system with a few state-of-the-art methods for summarizing news, and find it outperforms them substantially. We evaluate our system by comparing real novels to artificially produced surrogates, a procedure previously used to evaluate models of discourse coherence (Karamanis et al., 2004; Barzilay and Lapata, 2005) and models of syntax (Post, 2011). As in these settings, we anticipate that performance on this kind of task will be correlated with performance in applied settings, so we use it as an easier preliminary test of our capabilities. 3 Dataset We focus on the 19th century novel, partly following Elson et al. (2010) and partly because these texts are freely available via Project Gutenberg. Our main dataset is composed of romances (which we loosely define as novels focusing on a courtship or love affair). We select 41 texts, taking 11 as a development set and the remaini"
E12-1065,N06-1020,0,0.0119135,". 635 4 reply left-of-[name] right-of-[name] feel right-of-[name] look right-of-[name] mind right-of-[name] make Preprocessing In order to compare two texts, we must first extract the characters in each and some features of their relationships with one another. Our first step is to split the text into chapters, and each chapter into paragraphs; if the text contains a running dialogue where each line begins with a quotation mark, we append it to the previous paragraph. We segment each paragraph with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse it with the self-trained Charniak parser (McClosky et al., 2006). Next, we extract a list of characters, compute dependency tree-based unigram features for each character, and record character frequencies and relationships over time. 4.1 Identifying characters We create a list of possible character references for each work by extracting all strings of proper nouns (as detected by the parser), then discarding those which occur less than 5 times. Grouping these into a useful character list is a problem of cross-document coreference. Although cross-document coreference has been extensively studied (Bhattacharya and Getoor, 2005) and modern systems can achieve"
E12-1065,P10-1158,0,0.228046,"for Computational Linguistics of recognizing acceptable novels (section 6), but recognition is usually a good first step toward generation—a recognition model can always be used as part of a generate-and-rank pipeline, and potentially its underlying representation can be used in more sophisticated ways. We show a detailed analysis of the character correspondences discovered by our system, and discuss their potential relevance to summarization, in section 9. 2 Related work Some recent work on story understanding has focused on directly modeling the series of events that occur in the narrative. McIntyre and Lapata (2010) create a story generation system that draws on earlier work on narrative schemas (Chambers and Jurafsky, 2009). Their system ensures that generated stories contain plausible event-to-event transitions and are coherent. Since it focuses only on events, however, it cannot enforce a global notion of what the characters want or how they relate to one another. Our own work draws on representations that explicitly model emotions rather than events. Alm and Sproat (2005) were the first to describe stories in terms of an emotional trajectory. They annotate emotional states in 22 Grimms’ fairy tales a"
E12-1065,W11-1514,0,0.0234278,"rather than events. Alm and Sproat (2005) were the first to describe stories in terms of an emotional trajectory. They annotate emotional states in 22 Grimms’ fairy tales and discover an increase in emotion (mostly positive) toward the ends of stories. They later use this corpus to construct a reasonably accurate classifier for emotional states of sentences (Alm et al., 2005). Volkova et al. (2010) extend the human annotation approach using a larger number of emotion categories and applying them to freelydefined chunks instead of sentences. The largestscale emotional analysis is performed by Mohammad (2011), using crowd-sourcing to construct a large emotional lexicon with which he analyzes adult texts such as plays and novels. In this work, we adopt the concept of emotional trajectory, but apply it to particular characters rather than works as a whole. In focusing on characters, we follow Elson et al. (2010), who analyze narratives by examining their social network relationships. They use an automatic method based on quoted speech to find social links between characters in 19th century novels. Their work, designed for computational literary criticism, does not extract any temporal or emotional s"
E12-1065,P11-2038,0,0.066005,"ttings and themes, in order to attract potential readers. They do find it useful to detect character mentions, and also use features based on verb aspect to automatically exclude plot events while retaining descriptive passages. They compare their genrespecific system with a few state-of-the-art methods for summarizing news, and find it outperforms them substantially. We evaluate our system by comparing real novels to artificially produced surrogates, a procedure previously used to evaluate models of discourse coherence (Karamanis et al., 2004; Barzilay and Lapata, 2005) and models of syntax (Post, 2011). As in these settings, we anticipate that performance on this kind of task will be correlated with performance in applied settings, so we use it as an easier preliminary test of our capabilities. 3 Dataset We focus on the 19th century novel, partly following Elson et al. (2010) and partly because these texts are freely available via Project Gutenberg. Our main dataset is composed of romances (which we loosely define as novels focusing on a courtship or love affair). We select 41 texts, taking 11 as a development set and the remaining 30 as a test set; a complete list is given in Appendix A. W"
E12-1065,A97-1004,0,0.0386401,"least romantic works as an outof-domain set; experiments on these are in section 8. 635 4 reply left-of-[name] right-of-[name] feel right-of-[name] look right-of-[name] mind right-of-[name] make Preprocessing In order to compare two texts, we must first extract the characters in each and some features of their relationships with one another. Our first step is to split the text into chapters, and each chapter into paragraphs; if the text contains a running dialogue where each line begins with a quotation mark, we append it to the previous paragraph. We segment each paragraph with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse it with the self-trained Charniak parser (McClosky et al., 2006). Next, we extract a list of characters, compute dependency tree-based unigram features for each character, and record character frequencies and relationships over time. 4.1 Identifying characters We create a list of possible character references for each work by extracting all strings of proper nouns (as detected by the parser), then discarding those which occur less than 5 times. Grouping these into a useful character list is a problem of cross-document coreference. Although cross-document coreference has been extensi"
E12-1065,W10-0212,0,0.156645,"Missing"
E12-1065,H05-1044,0,0.0103226,"and emotions associated with “Miss Elizabeth Bennet”, protagonist of Pride and Prejudice, and frequency of paragraphs about her and “Mr. Darcy”, smoothed and projected onto 50 basis points. 4.3 Temporal relationships We record two time-varying features for each character, each taking one value per chapter. The first is the character’s frequency as a proportion of all character mentions in the chapter. The second is the frequency with which the character is associated with emotional language—their emotional trajectory (Alm et al., 2005). We use the strong subjectivity cues from the lexicon of Wilson et al. (2005) as a measurement of emotion. If, in a particular paragraph, only one character is mentioned, we count all emotional words in that paragraph and add them to the character’s total. To render the numbers comparable across works, each paragraph subtotal is normalized by the amount of emotional language in the novel as a whole. Then the chapter score is the average over paragraphs. For pairwise character relationships, we count the number of paragraphs in which only two characters are mentioned, and treat this number (as a proportion of the total) as a measurement of the strength of the relationsh"
E12-1065,J08-1001,0,\N,Missing
E12-1065,P11-1051,0,\N,Missing
E12-1065,J10-1003,0,\N,Missing
E12-1065,P04-1000,0,\N,Missing
E14-1055,P99-1048,0,0.0553006,"which learned precedence relationships between pairs of words, and a template-based approach which learned global orderings over sets of (1) a. b. Obama adopted a dog named Bo. #A dog named Bo was adopted by Obama. Ex. (1-a) demonstrates the standard order (under the assumption that Obama is familiar to a reader of this paper while Bo may not be). (1-b) violates the ordering principles and is likely to be judged less felicitous. Importantly, Obama is hearer-old not because of a preceding discourse mention but due to (assumed) general knowledge; it is an unused (Prince, 1981), or existential (Bean and Riloff, 1999) entity. General knowledge shared by speakers of a community is one way in which an entity enters the common ground. Along with this shared socio-cultural background, speakers may also share physical co-presence and linguistic co-presence (Clark, 1996). They can indicate salient entities, individuals, or entire events by engaging their listener in joint attention via pointing or gaze cueing (Baldwin, 1995; Carpenter et al., 1998); in this paper, we demonstrate that visual prominence is also sufficient. Maienborn (2001) explicitly suggests that this topic-comment structure principle is the moti"
E14-1055,E91-1028,0,0.0243264,"served for old information, while new information is usually placed at the end. For instance, see these (contrived) examples: search on salience and information structure, we show that visually prominent objects are treated as part of common ground despite the lack of previous mention. 2 Related work Computational models of REG (Krahmer and van Deemter, 2012) focus mainly on content selection: Given a list of objects in the scene and their visual attributes, such models decide what information to include in a description so as to specify the target object. Early systems (with the exception of Dale and Haddock (1991)) did not produce relational descriptions. Nor did these systems model the visual salience of the objects or attributes under discussion. Later models (Kelleher et al., 2005; Kelleher and Kruijff, 2006; Duckham et al., 2010) introduce simple models of visual salience, prompted by psycholinguistic research which shows that objects are more likely to be selected as landmarks when they are easy for an observer to find (Beun and Cremers, 1998). Clarke et al. (2013) extend these results with a more complicated model of visual salience (Torralba et al., 2006). Fang et al. (2013) similarly note that"
E14-1055,W08-1133,0,0.0194507,"Missing"
E14-1055,P08-2050,0,0.0451282,"Missing"
E14-1055,D13-1038,0,0.0122524,"he exception of Dale and Haddock (1991)) did not produce relational descriptions. Nor did these systems model the visual salience of the objects or attributes under discussion. Later models (Kelleher et al., 2005; Kelleher and Kruijff, 2006; Duckham et al., 2010) introduce simple models of visual salience, prompted by psycholinguistic research which shows that objects are more likely to be selected as landmarks when they are easy for an observer to find (Beun and Cremers, 1998). Clarke et al. (2013) extend these results with a more complicated model of visual salience (Torralba et al., 2006). Fang et al. (2013) similarly note that generated REs should avoid information that is perceptually expensive to obtain. However, these results focus on content selection rather than surface realization. In comparison to selection, surface realization for REG has received little attention. Many researchers do not even perform realization, but simply compare their systems’ selected content with the gold standard under metrics like the Dice coefficient. The TUNA challenges (Gatt et al., 2008; Gatt et al., 2009; Gatt and Belz, 2010) are an exception; participants were required to provide surface realizations, which"
E14-1055,P07-1041,0,0.0134198,"s leftward movement is expected. Since most of the modifiers in 521 this study are locatives, our data should be taken as endorsing this theoretical position, but supplying felicity conditions in terms of common ground. These principles have been applied to computational surface realization in non-visual domains (Webber, 2004; Nakatsu and White, 2010, and others). Freer-word-order languages such as German also have predictable information structures which have been employed in surface realization systems, but these require a different structural analysis than in English (Zarrieß et al., 2012; Filippova and Strube, 2007). 3 Man closest to the rear tyre of the van. (4) There is a person standing in the water wearing a blue shirt and yellow hat Ex. (2) places the landmark so that it precedes the anchor; Ex. (3) shows the landmark following it. Ex. (4) shows a more complex structure, which we refer to as interleaved, where information about the anchor is given in multiple phrases and the landmark phrase appears between them.2 (These orders are determined with respect to the first mention of the landmark.) We denote these ordering strategies as PRECEDE, FOLLOW and IN TER respectively. We also distinguish between"
E14-1055,W09-0630,0,0.0150649,"realization, but simply compare their systems’ selected content with the gold standard under metrics like the Dice coefficient. The TUNA challenges (Gatt et al., 2008; Gatt et al., 2009; Gatt and Belz, 2010) are an exception; participants were required to provide surface realizations, which were evaluated via NIST, BLEU and string edit distance. Many participants used a template-based realizer written by Irene Langkilde-Geary, which imposes a fixed ordering on attributes like “size” and “color” but has no provisions for relational descriptions. A few participants created their own realizers. Brugman et al. (2009) describe a system with multiple hand-written templates. Di Fabbrizio et al. (2008) propose several learning-based systems; the most effective were a dependency-based approach which learned precedence relationships between pairs of words, and a template-based approach which learned global orderings over sets of (1) a. b. Obama adopted a dog named Bo. #A dog named Bo was adopted by Obama. Ex. (1-a) demonstrates the standard order (under the assumption that Obama is familiar to a reader of this paper while Bo may not be). (1-b) violates the ordering principles and is likely to be judged less fel"
E14-1055,W08-1131,0,0.0515259,"1998). Clarke et al. (2013) extend these results with a more complicated model of visual salience (Torralba et al., 2006). Fang et al. (2013) similarly note that generated REs should avoid information that is perceptually expensive to obtain. However, these results focus on content selection rather than surface realization. In comparison to selection, surface realization for REG has received little attention. Many researchers do not even perform realization, but simply compare their systems’ selected content with the gold standard under metrics like the Dice coefficient. The TUNA challenges (Gatt et al., 2008; Gatt et al., 2009; Gatt and Belz, 2010) are an exception; participants were required to provide surface realizations, which were evaluated via NIST, BLEU and string edit distance. Many participants used a template-based realizer written by Irene Langkilde-Geary, which imposes a fixed ordering on attributes like “size” and “color” but has no provisions for relational descriptions. A few participants created their own realizers. Brugman et al. (2009) describe a system with multiple hand-written templates. Di Fabbrizio et al. (2008) propose several learning-based systems; the most effective wer"
E14-1055,W09-0629,0,0.0130818,"l. (2013) extend these results with a more complicated model of visual salience (Torralba et al., 2006). Fang et al. (2013) similarly note that generated REs should avoid information that is perceptually expensive to obtain. However, these results focus on content selection rather than surface realization. In comparison to selection, surface realization for REG has received little attention. Many researchers do not even perform realization, but simply compare their systems’ selected content with the gold standard under metrics like the Dice coefficient. The TUNA challenges (Gatt et al., 2008; Gatt et al., 2009; Gatt and Belz, 2010) are an exception; participants were required to provide surface realizations, which were evaluated via NIST, BLEU and string edit distance. Many participants used a template-based realizer written by Irene Langkilde-Geary, which imposes a fixed ordering on attributes like “size” and “color” but has no provisions for relational descriptions. A few participants created their own realizers. Brugman et al. (2009) describe a system with multiple hand-written templates. Di Fabbrizio et al. (2008) propose several learning-based systems; the most effective were a dependency-base"
E14-1055,D10-1040,0,0.0344108,"Missing"
E14-1055,J95-2003,0,0.0742585,"e basic principles for information structure in English discourse. Prince (1981) introduces the key distinctions between discourse-old and new entities (previously mentioned vs not mentioned) and hearer-old and new entities (familiar to the listener vs not familiar). Clark and Wilkes-Gibbs (1986) extends the latter distinction to a notion of common ground; entities in the common ground are familiar to both participants in the discourse, and each participant is in turn aware of the other’s familiarity. As noted by Prince (1981) and expanded on by Ward and Birner (2001) and in Centering Theory (Grosz et al., 1995), the first element in an English sentence is generally reserved for old information, while new information is usually placed at the end. For instance, see these (contrived) examples: search on salience and information structure, we show that visually prominent objects are treated as part of common ground despite the lack of previous mention. 2 Related work Computational models of REG (Krahmer and van Deemter, 2012) focus mainly on content selection: Given a list of objects in the scene and their visual attributes, such models decide what information to include in a description so as to specif"
E14-1055,P06-1131,0,0.028923,"inent objects are treated as part of common ground despite the lack of previous mention. 2 Related work Computational models of REG (Krahmer and van Deemter, 2012) focus mainly on content selection: Given a list of objects in the scene and their visual attributes, such models decide what information to include in a description so as to specify the target object. Early systems (with the exception of Dale and Haddock (1991)) did not produce relational descriptions. Nor did these systems model the visual salience of the objects or attributes under discussion. Later models (Kelleher et al., 2005; Kelleher and Kruijff, 2006; Duckham et al., 2010) introduce simple models of visual salience, prompted by psycholinguistic research which shows that objects are more likely to be selected as landmarks when they are easy for an observer to find (Beun and Cremers, 1998). Clarke et al. (2013) extend these results with a more complicated model of visual salience (Torralba et al., 2006). Fang et al. (2013) similarly note that generated REs should avoid information that is perceptually expensive to obtain. However, these results focus on content selection rather than surface realization. In comparison to selection, surface r"
E14-1055,D12-1023,0,0.0129297,"and tends to draw initial gaze fixations (Itti and Koch, 2000). We also include indicators for whether the anchor is the target object, and whether the landmark is an image region (reg) (see section 3). In addition, we give a few non-visual features derived from the content structure. These include the number of dependents (landmarks which relate to each object in the description) and the number of descendants (the direct dependents, their dependents and so forth). When the speaker has to arrange a large number of landmarks, they tend to vary the ordering more, because of heavy-shift effects (White and Rajkumar, 2012) and the difficulty of preposing more than one constituent. 7 Regression analysis To gain some insight into the influence of different features, we conduct a logistic regression analysis. For each pair of (anchor, landmark) occur8 Following Clarke et al. (2013), we attempted to also measuring distinctiveness from the background using a perceptual model of visual salience (Torralba et al., 2006). Although this measure is effective in predicting landmark selection, it proves uninformative here for predicting information structure, yielding no significant effects in any analyses. 9 We use these c"
E14-1055,J12-1006,0,0.0434471,"Missing"
E14-1055,E12-1078,0,0.0163311,"conditions on when this leftward movement is expected. Since most of the modifiers in 521 this study are locatives, our data should be taken as endorsing this theoretical position, but supplying felicity conditions in terms of common ground. These principles have been applied to computational surface realization in non-visual domains (Webber, 2004; Nakatsu and White, 2010, and others). Freer-word-order languages such as German also have predictable information structures which have been employed in surface realization systems, but these require a different structural analysis than in English (Zarrieß et al., 2012; Filippova and Strube, 2007). 3 Man closest to the rear tyre of the van. (4) There is a person standing in the water wearing a blue shirt and yellow hat Ex. (2) places the landmark so that it precedes the anchor; Ex. (3) shows the landmark following it. Ex. (4) shows a more complex structure, which we refer to as interleaved, where information about the anchor is given in multiple phrases and the landmark phrase appears between them.2 (These orders are determined with respect to the first mention of the landmark.) We denote these ordering strategies as PRECEDE, FOLLOW and IN TER respectively."
J10-3004,W09-1803,1,0.889183,"004). Finally, we brieﬂy mention some work which appeared after we developed the system described here. Wang and Oard (2009) is another system that uses TF–IDF unigrams, but augments these feature vectors using the information retrieval technique of message expansion. They report results on our corpus which improve on our own. Adams (2008) attempts to use not only WordNet but Latent Dirichlet Allocation to describe semantic relatedness. He ﬁnds both techniques ineffective. In addition, he annotates a large corpus of Internet Relay Chat and similarly ﬁnds that annotators have trouble agreeing. Elsner and Schudy (2009) explore different partitioning strategies, improving on the greedy algorithm that we present. 3. Data Set Our data set is recorded from IRC channel ## LINUX at free–node.net, using the freely available gaim client. ## LINUX is an unofﬁcial tech support line for the Linux operating system, selected because it is one of the most active chat rooms on freenode, leading to many simultaneous conversations, and because its content is typically inoffensive. Although it is notionally intended only for tech support, it includes large amounts of social chat as well, such as the conversation about factor"
J10-3004,N06-1041,0,0.027446,"l task, and metrics for agreement on supervised classiﬁcation, such as the κ statistic, are not applicable. To measure global similarity between annotations, we use one-to-one accuracy. This measure describes how well we can extract whole conversations intact, as required for summarization or information extraction. To compute it, we pair up conversations from the two annotations to maximize the total overlap by computing an optimal maxweight bipartite matching, then report the percentage of overlap found.3 One-to-one accuracy is a standard metric in unsupervised part-of-speech tagging (e.g., Haghighi and Klein 2006), and is equivalent to mention-based CEAF (Luo 2005) for coreference resolution. If we intend to monitor or participate in the conversation as it occurs, we will care more about local judgments. The local agreement metric is a constrained form of the Rand index for clusterings (Rand 1971) which counts agreements and disagreements for pairs within a context k. We consider a particular utterance: The previous k utterances are each in either the same or a different conversation. The lock score between two annotators is their average agreement on these k same/different judgments, averaged over all"
J10-3004,W04-2317,0,0.15917,"Missing"
J10-3004,H05-1004,0,0.0140731,"as the κ statistic, are not applicable. To measure global similarity between annotations, we use one-to-one accuracy. This measure describes how well we can extract whole conversations intact, as required for summarization or information extraction. To compute it, we pair up conversations from the two annotations to maximize the total overlap by computing an optimal maxweight bipartite matching, then report the percentage of overlap found.3 One-to-one accuracy is a standard metric in unsupervised part-of-speech tagging (e.g., Haghighi and Klein 2006), and is equivalent to mention-based CEAF (Luo 2005) for coreference resolution. If we intend to monitor or participate in the conversation as it occurs, we will care more about local judgments. The local agreement metric is a constrained form of the Rand index for clusterings (Rand 1971) which counts agreements and disagreements for pairs within a context k. We consider a particular utterance: The previous k utterances are each in either the same or a different conversation. The lock score between two annotators is their average agreement on these k same/different judgments, averaged over all utterances. For example, loc1 counts pairs of adjac"
J10-3004,P06-1004,0,0.0145708,"peaker takes part in about 3.3 conversations (the actual number varies for each annotator). The more talkative a speaker is, the more conversations they participate in, as shown by a plot of conversations versus utterances (Figure 4). The assumption is not very accurate, especially for speakers with more than 10 utterances. 4. Model Our model for disentanglement ﬁts into the general class of graph partitioning algorithms (Roth and Yih 2004) that have been used for a variety of tasks in NLP, including coreference resolution (Soon, Ng, and Lim 2001) and the related task of meeting segmentation (Malioutov and Barzilay 2006). These algorithms operate in two stages: First, 397 Computational Linguistics Volume 36, Number 3 Table 2 Feature functions with performance on development data. Chat-speciﬁc (Acc: 73 Prec: 73 Rec: 61 F: 66) Time Speaker Mention x-y Mention same Mention other The time between x and y in seconds, discretized into logarithmically sized bins. x and y have the same speaker. x mentions the speaker of y (or vice versa). For example, this feature is true for a pair such as: Felicia “Gale: ... and any utterance spoken by Gale. Both x and y mention the same name. either x or y mentions a third person’"
J10-3004,C02-1139,0,0.0261248,"Missing"
J10-3004,W04-2401,0,0.00842338,"on about the data set, we test the appropriateness of the assumption (used in previous work) that each speaker takes part in only one conversation. In our data, the average speaker takes part in about 3.3 conversations (the actual number varies for each annotator). The more talkative a speaker is, the more conversations they participate in, as shown by a plot of conversations versus utterances (Figure 4). The assumption is not very accurate, especially for speakers with more than 10 utterances. 4. Model Our model for disentanglement ﬁts into the general class of graph partitioning algorithms (Roth and Yih 2004) that have been used for a variety of tasks in NLP, including coreference resolution (Soon, Ng, and Lim 2001) and the related task of meeting segmentation (Malioutov and Barzilay 2006). These algorithms operate in two stages: First, 397 Computational Linguistics Volume 36, Number 3 Table 2 Feature functions with performance on development data. Chat-speciﬁc (Acc: 73 Prec: 73 Rec: 61 F: 66) Time Speaker Mention x-y Mention same Mention other The time between x and y in seconds, discretized into logarithmically sized bins. x and y have the same speaker. x mentions the speaker of y (or vice versa"
J10-3004,J01-4004,0,0.034712,"Missing"
J10-3004,traum-etal-2004-evaluation,0,0.0634965,"Missing"
J10-3004,N09-1023,0,0.426937,"ell (2008) is based, and uses very similar methodology based on TF-IDF. Our two-stage classiﬁcation and partitioning algorithm draws on work on coreference resolution. Many approaches to coreference (starting with Soon, Ng, and Lim 2001) use such an approach, building global clusters based on pairwise decisions made by a 392 Elsner and Charniak Disentangling Chat classiﬁer. The global partitioning problem was identiﬁed as correlation clustering, an NP-hard problem, by McCallum and Wellner (2004). Finally, we brieﬂy mention some work which appeared after we developed the system described here. Wang and Oard (2009) is another system that uses TF–IDF unigrams, but augments these feature vectors using the information retrieval technique of message expansion. They report results on our corpus which improve on our own. Adams (2008) attempts to use not only WordNet but Latent Dirichlet Allocation to describe semantic relatedness. He ﬁnds both techniques ineffective. In addition, he annotates a large corpus of Internet Relay Chat and similarly ﬁnds that annotators have trouble agreeing. Elsner and Schudy (2009) explore different partitioning strategies, improving on the greedy algorithm that we present. 3. Da"
J10-3004,E06-1022,0,\N,Missing
N06-1022,P04-1006,1,0.79928,"ll too slow for many applications. In some cases researchers need large quantities of parsed data and do not have the hundreds of machines necessary to parse gigaword corpora in a week or two. More pressingly, in real-time applications such as speech recognition, a parser would be only a part of a much larger system, and the system builders are not keen on giving the parser one of the ten seconds available to process, say, a thirty-word sentence. Even worse, some applications require the parsing of multiple candidate strings per sentence (Johnson and Charniak, 2004) or parsing from a lattice (Hall and Johnson, 2004), and in these applications parsing efficiency is even more important. We present here a multilevel coarse-to-fine (mlctf) PCFG parsing algorithm that reduces the complexity of the search involved in finding the best parse. It defines a sequence of increasingly more complex PCFGs, and uses the parse forest produced by one PCFG to prune the search of the next more complex PCFG. We currently use four levels of grammars in our mlctf algorithm. The simplest PCFG, which we call the level-0 grammar, contains only one nontrivial nonterminal and is so simple that minimal time is needed to parse a sent"
N06-1022,P04-1005,1,0.389081,"about a second per sentence. Unfortunately, this is still too slow for many applications. In some cases researchers need large quantities of parsed data and do not have the hundreds of machines necessary to parse gigaword corpora in a week or two. More pressingly, in real-time applications such as speech recognition, a parser would be only a part of a much larger system, and the system builders are not keen on giving the parser one of the ten seconds available to process, say, a thirty-word sentence. Even worse, some applications require the parsing of multiple candidate strings per sentence (Johnson and Charniak, 2004) or parsing from a lattice (Hall and Johnson, 2004), and in these applications parsing efficiency is even more important. We present here a multilevel coarse-to-fine (mlctf) PCFG parsing algorithm that reduces the complexity of the search involved in finding the best parse. It defines a sequence of increasingly more complex PCFGs, and uses the parse forest produced by one PCFG to prune the search of the next more complex PCFG. We currently use four levels of grammars in our mlctf algorithm. The simplest PCFG, which we call the level-0 grammar, contains only one nontrivial nonterminal and is so"
N06-1022,J98-4004,1,0.862318,"stituent type found in “S →NP VP .” is mapped into its generalization at level 1. The probabilities of all rules are computed using maximum likelihood for constituents at that level. The grammar used by the parser can best be described as being influenced by four components: 1. the nonterminals defined at that level of parsing, 2. the binarization scheme, 3. the generalizations defined over the binarization, and 171 Grammars induced in this way tend to be too specific, as the binarization introduce a very large number of very specialized phrasal categories (the Ai ). Following common practice Johnson (1998; Klein and Manning (2003b) we Markovize by replacing these nonterminals with ones that remember less of the immediate rule context. In our version we keep track of only the parent, the head constituent and the constituent immediately to the right or left, depending on which side of the constituent we are processing. With this scheme the above rules now look like this: A →Ad,c e Ad,c →Aa,c d Aa,c →a Ab,c Ab,c →b c So, for example, the rule “A →Ad,c e” would have a high probability if constituents of type A, with c as their head, often have d followed by e at their end. Lastly, we add parent an"
N06-1022,N03-1016,0,0.854396,"f the agenda according to (an estimate of) their probability given the sentence. This probability is computed by estimating Equation 1: p(nki,j |s) = α(nki,j )β(nki,j ) . p(s) (1) It must be estimated because during the bottom-up chart-parsing algorithm, the true outside probability cannot be computed. The results cited in Caraballo and Charniak (1998) cannot be compared directly to ours, but are roughly in the same equivalence class. Those presented in Charniak et al. (1998) are superior, but in Section 5 below we suggest that a combination of the techniques could yield better results still. Klein and Manning (2003a) describe efficient A∗ for the most likely parse, where pruning is accomplished by using Equation 1 and a true upper bound on the outside probability. While their maximum is a looser estimate of the outside probability, it is an admissible heuristic and together with an A∗ search is guaranteed to find the best parse first. One question is if the guarantee is worth the extra search required by the looser estimate of the true outside probability. Tsuruoka and Tsujii (2004) explore the framework developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap mani"
N06-1022,P05-1022,1,0.233451,"parser while the first stage is a regular-expression approximation of the grammar. Again, the second stage is constrained by the parses found in the first stage. Neither stage is smoothed. The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents. The second stage explores all of the constituents not pruned out after the first stage. Related approaches are used in Hall (2004) and Charniak and Johnson (2005). A quite different approach to parsing efficiency is taken in Caraballo and Charniak (1998) (and refined in Charniak et al. (1998)). Here efficiency is gained by using a standard chartparsing algorithm and pulling constituents off the agenda according to (an estimate of) their probability given the sentence. This probability is computed by estimating Equation 1: p(nki,j |s) = α(nki,j )β(nki,j ) . p(s) (1) It must be estimated because during the bottom-up chart-parsing algorithm, the true outside probability cannot be computed. The results cited in Caraballo and Charniak (1998) cannot be compa"
N06-1022,P03-1054,0,0.677716,"f the agenda according to (an estimate of) their probability given the sentence. This probability is computed by estimating Equation 1: p(nki,j |s) = α(nki,j )β(nki,j ) . p(s) (1) It must be estimated because during the bottom-up chart-parsing algorithm, the true outside probability cannot be computed. The results cited in Caraballo and Charniak (1998) cannot be compared directly to ours, but are roughly in the same equivalence class. Those presented in Charniak et al. (1998) are superior, but in Section 5 below we suggest that a combination of the techniques could yield better results still. Klein and Manning (2003a) describe efficient A∗ for the most likely parse, where pruning is accomplished by using Equation 1 and a true upper bound on the outside probability. While their maximum is a looser estimate of the outside probability, it is an admissible heuristic and together with an A∗ search is guaranteed to find the best parse first. One question is if the guarantee is worth the extra search required by the looser estimate of the true outside probability. Tsuruoka and Tsujii (2004) explore the framework developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap mani"
N06-1022,W98-1115,1,0.878022,"found in the first stage. Neither stage is smoothed. The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents. The second stage explores all of the constituents not pruned out after the first stage. Related approaches are used in Hall (2004) and Charniak and Johnson (2005). A quite different approach to parsing efficiency is taken in Caraballo and Charniak (1998) (and refined in Charniak et al. (1998)). Here efficiency is gained by using a standard chartparsing algorithm and pulling constituents off the agenda according to (an estimate of) their probability given the sentence. This probability is computed by estimating Equation 1: p(nki,j |s) = α(nki,j )β(nki,j ) . p(s) (1) It must be estimated because during the bottom-up chart-parsing algorithm, the true outside probability cannot be computed. The results cited in Caraballo and Charniak (1998) cannot be compared directly to ours, but are roughly in the same equivalence class. Those presented in Charniak et al. (1998) are superior, but in"
N06-1022,A00-2018,1,0.744452,"the multilevel dynamic programming algorithm needed for coarse-to-fine analysis (which they apply to decoding rather than parsing), and show how to perform exact coarse-to-fine computation, rather than the heuristic search we perform here. A paper closely related to ours is Goodman (1997). In our terminology, Goodman’s parser is a two-stage ctf parser. The second stage is a standard tree-bank parser while the first stage is a regular-expression approximation of the grammar. Again, the second stage is constrained by the parses found in the first stage. Neither stage is smoothed. The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents. The second stage explores all of the constituents not pruned out after the first stage. Related approaches are used in Hall (2004) and Charniak and Johnson (2005). A quite different approach to parsing efficiency is taken in Caraballo and Charniak (1998) (and refined in Charniak et al. (1998)). Here efficiency is gained by using a standard chartparsin"
N06-1022,P97-1003,0,0.216398,"equired by the heap manipulations necessary in this scheme. They describe an iterative deepening algorithm that does not require a heap. They also speed computation by precomputing more accurate upper bounds on the outside probabilities of various kinds of constituents. They are able to reduce by half the number of constituents required to find the best parse (compared to CKY). Most recently, McDonald et al. (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)). It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bilexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing — after all, there are no phrasal constituents to consider. The current paper can be thought of as a way to take the sting out of the grammar constant for PCFGs by parsing first with very few phrasal constituents and adding them only 170 Level: 0 1 © S1 S1                       HP"
N06-1022,P99-1059,0,0.0120516,"t does not require a heap. They also speed computation by precomputing more accurate upper bounds on the outside probabilities of various kinds of constituents. They are able to reduce by half the number of constituents required to find the best parse (compared to CKY). Most recently, McDonald et al. (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)). It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bilexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing — after all, there are no phrasal constituents to consider. The current paper can be thought of as a way to take the sting out of the grammar constant for PCFGs by parsing first with very few phrasal constituents and adding them only 170 Level: 0 1 © S1 S1                       HP                     P                       MP                    2 ©  S1           S"
N06-1022,J93-2004,0,0.0285293,"S1 S VP UCP SQ SBAR SBARQ SINV NP NAC NX LST X UCP FRAG ADJP QP CONJP ADVP INTJ PRN PRT PP PRT RRC WHADJP WHADVP WHNP WHPP Figure 1: The levels of nonterminal labels after most constituents have been pruned away. 3 Multilevel Course-to-fine Parsing We use as the underlying parsing algorithm a reasonably standard CKY parser, modified to allow unary branching rules. The complete nonterminal clustering is given in Figure 1. We do not cluster preterminals. These remain fixed at all levels to the standard Penn-tree-bank set Marcus et al. (1993). Level-0 makes two distinctions, the root node and everybody else. At level 1 we make one further distinction, between phrases that tend to be heads of constituents (NPs, VPs, and Ss) and those that tend to be modifiers (ADJPs, PPs, etc.). Level-2 has a total of five categories: root, things that are typically headed by nouns, those headed by verbs, things headed by prepositions, and things headed by classical modifiers (adjectives, adverbs, etc.). Finally, level 3 is the S1 S1 P HP P P PRP VBD He P . ate IN P . at DT 4. extra annotation to improve parsing accuracy. HP HP PRP VBD MP He ate IN"
N06-1022,J93-4001,0,0.0119203,"measured by the total number of constituents processed) is decreased by a factor of ten over standard CKY parsing at the final level. We also discuss some fine points of the results therein. Finally in section 5 we suggest that because the search space of mlctf algorithms is, at this point, almost totally unexplored, future work should be able to improve significantly on these results. 2 Previous Research Coarse-to-fine search is an idea that has appeared several times in the literature of computational linguistics and related areas. The 169 first appearance of this idea we are aware of is in Maxwell and Kaplan (1993), where a covering CFG is automatically extracted from a more detailed unification grammar and used to identify the possible locations of constituents in the more detailed parses of the sentence. Maxwell and Kaplan use their covering CFG to prune the search of their unification grammar parser in essentially the same manner as we do here, and demonstrate significant performance improvements by using their coarse-to-fine approach. The basic theory of coarse-to-fine approximations and dynamic programming in a stochastic framework is laid out in Geman and Kochanek (2001). This paper describes the"
N06-1022,P05-1012,0,0.0309759,"extra search required by the looser estimate of the true outside probability. Tsuruoka and Tsujii (2004) explore the framework developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap manipulations necessary in this scheme. They describe an iterative deepening algorithm that does not require a heap. They also speed computation by precomputing more accurate upper bounds on the outside probabilities of various kinds of constituents. They are able to reduce by half the number of constituents required to find the best parse (compared to CKY). Most recently, McDonald et al. (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)). It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bilexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing — after all, there are no phrasal constituents to consider. The current paper can be thought of as a way to take the sting out of the grammar cons"
N06-1022,J98-2004,1,\N,Missing
N06-1022,W97-0302,0,\N,Missing
N07-1055,P04-1051,0,0.0201021,"have little paragraph structure, so τ is an effective metric. Sentence Ordering In the sentence ordering task, (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Soricut and Marcu, 2006), we view a document as an unordered bag of sentences and try to find the ordering of the sentences which maximizes coherence according to our model. This type of ordering process has applications in natural language generation and multi-document summarization. Unfortunately, finding the optimal ordering according to a probabilistic model with local features is NP-complete and non-approximable (Althaus et al., 2004). Moreover, since our model is not Markovian, the relaxation used as a heuristic for A∗ search by Soricut and Marcu (2006) is ineffective. We therefore use simulated annealing to find a high-probability ordering, starting from a random permutation of the sentences. Our search system has few Estimated Search Errors as defined by Soricut and Marcu (2006); it rarely proposes an ordering which has lower proba441 5.2 Discrimination Our second task is the discriminative test used by (Barzilay and Lapata, 2005). In this task we generate random permutations of a test document, and measure how often th"
N07-1055,P05-1018,0,0.148365,"r, Joseph Austerweil, and Eugene Charniak Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University Providence, RI 02912 {melsner,ec}@cs.brown.edu, joseph.austerweil@gmail.com Abstract We present a model for discourse coherence which combines the local entitybased approach of (Barzilay and Lapata, 2005) and the HMM-based content model of (Barzilay and Lee, 2004). Unlike the mixture model of (Soricut and Marcu, 2006), we learn local and global features jointly, providing a better theoretical explanation of how they are useful. As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively. Our model performs the ordering task competitively with (Soricut and Marcu, 2006), and significantly better than either of the models it is based on. 1 Introduction Models of coherent discourse are central to several tasks in natural language processing: such models have been used in text generation (Kibble and Power, 2004) and evaluation of human-produced text in educational applications (Miltsakaki and Kukich, 2004; Higgins et al., 2004). Moreover, an accurate model can reveal information about documen"
N07-1055,N04-1015,0,0.723731,"cal models (Lapata, 2003; Barzilay and Lapata, 2005; Foltz et al., 1998) attempt to capture the generalization that adjacent sentences often have similar content, and therefore tend to contain related words. Models of this type are good at finding sentences that belong near one another in the document. However, they have trouble finding the beginning or end of the document, or recovering from sudden shifts in topic (such as occur at paragraph boundaries). Some local models also have trouble deciding which of a pair of related sentences ought to come first. In contrast, the global HMM model of Barzilay and Lee (2004) tries to track the predictable changes in topic between sentences. This gives it a pronounced advantage in ordering sentences, since it can learn to represent beginnings, ends and boundaries as separate states. However, it has no local features; the particular words in each sentence are generated based only on the current state of the document. Since information can pass from sentence to sentence only in this restricted manner, the model sometimes fails to place sentences next to the correct neighbors. We attempt here to unify the two approaches by constructing a model with both sentence-to-s"
N07-1055,P05-1022,1,0.165188,"actic roles: subject if possible, then object, or finally other. An example text is figure 1, whose grid is figure 2. Nouns are also treated as salient or non-salient, another important concern of Centering Theory. We condition events involving a noun on the frequency of that noun. Unfortunately, this way of representing salience makes our model slightly deficient, since the model conditions on a particular noun occurring e.g. 2 times, but assigns nonzero probabilities to documents where it occurs 3 times. This is theo1 Roles are determined heuristically using trees produced by the parser of (Charniak and Johnson, 2005). Following previous work, we slightly conflate thematic and syntactic roles, marking the subject of a passive verb as O. 2 The numeric token “1300” is removed in preprocessing, and “Nuevo Laredo” is marked as “PROPER”. 437 0 [The commercial pilot]O , [sole occupant of [the airplane]X ]X , was not injured . 1 [The airplane]O was owned and operated by [a private owner]X . 2 [Visual meteorological conditions]S prevailed for [the personal cross country flight for which [a VFR flight plan]O was filed]X . 3 [The flight]S originated at [Nuevo Laredo , Mexico]X , at [approximately 1300]X . Figure 1:"
N07-1055,J95-2003,0,0.894935,". However, mixture models lack explanatory power; since each of the individual component models is known to be flawed, it is difficult to say that the combination is theoretically more sound than the parts, even if it usually works better. Moreover, since the model we describe uses a strict subset of the features used in the component models of (Soricut and Marcu, 2006), we suspect that adding it to the mixture would lead to still further improved results. 2 Naive Entity Grids Entity grids, first described in (Lapata and Barzilay, 2005), are designed to capture some ideas of Centering Theory (Grosz et al., 1995), namely that adjacent utterances in a locally coherent discourses are likely to contain the same nouns, and that important nouns often appear in syntactically important roles such as subject or object. An entity grid represents a document as a matrix with a column for each entity, and a row for each sentence. The entry ri,j describes the syntactic role of entity j in sentence i: these roles are subject (S), object (O), or some other role (X)1 . In addition there is a special marker (-) for nouns which do not appear at all in a given sentence. Each noun appears only once in a given row of the"
N07-1055,N04-1024,0,0.0358159,"ey are useful. As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively. Our model performs the ordering task competitively with (Soricut and Marcu, 2006), and significantly better than either of the models it is based on. 1 Introduction Models of coherent discourse are central to several tasks in natural language processing: such models have been used in text generation (Kibble and Power, 2004) and evaluation of human-produced text in educational applications (Miltsakaki and Kukich, 2004; Higgins et al., 2004). Moreover, an accurate model can reveal information about document structure, aiding in such tasks as supervised summarization (Barzilay and Lapata, 2005). Models of coherence tend to fall into two classes. Local models (Lapata, 2003; Barzilay and Lapata, 2005; Foltz et al., 1998) attempt to capture the generalization that adjacent sentences often have similar content, and therefore tend to contain related words. Models of this type are good at finding sentences that belong near one another in the document. However, they have trouble finding the beginning or end of the document, or recovering"
N07-1055,J04-4001,0,0.0248343,"ricut and Marcu, 2006), we learn local and global features jointly, providing a better theoretical explanation of how they are useful. As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively. Our model performs the ordering task competitively with (Soricut and Marcu, 2006), and significantly better than either of the models it is based on. 1 Introduction Models of coherent discourse are central to several tasks in natural language processing: such models have been used in text generation (Kibble and Power, 2004) and evaluation of human-produced text in educational applications (Miltsakaki and Kukich, 2004; Higgins et al., 2004). Moreover, an accurate model can reveal information about document structure, aiding in such tasks as supervised summarization (Barzilay and Lapata, 2005). Models of coherence tend to fall into two classes. Local models (Lapata, 2003; Barzilay and Lapata, 2005; Foltz et al., 1998) attempt to capture the generalization that adjacent sentences often have similar content, and therefore tend to contain related words. Models of this type are good at finding sentences that belong ne"
N07-1055,P03-1069,0,0.759769,"and Marcu, 2006), and significantly better than either of the models it is based on. 1 Introduction Models of coherent discourse are central to several tasks in natural language processing: such models have been used in text generation (Kibble and Power, 2004) and evaluation of human-produced text in educational applications (Miltsakaki and Kukich, 2004; Higgins et al., 2004). Moreover, an accurate model can reveal information about document structure, aiding in such tasks as supervised summarization (Barzilay and Lapata, 2005). Models of coherence tend to fall into two classes. Local models (Lapata, 2003; Barzilay and Lapata, 2005; Foltz et al., 1998) attempt to capture the generalization that adjacent sentences often have similar content, and therefore tend to contain related words. Models of this type are good at finding sentences that belong near one another in the document. However, they have trouble finding the beginning or end of the document, or recovering from sudden shifts in topic (such as occur at paragraph boundaries). Some local models also have trouble deciding which of a pair of related sentences ought to come first. In contrast, the global HMM model of Barzilay and Lee (2004)"
N07-1055,J06-4002,0,0.0549184,"Missing"
N07-1055,C04-1129,0,0.0185847,"t. 7 Future Work Ordering in the AIRPLANE corpus and similar constrained sets of short documents is by no means a solved problem, but the results so far show a good deal of promise. Unfortunately, in longer and less formulaic corpora, the models, inference algorithms and even evaluation metrics used thus far may prove extremely difficult to scale up. Domains with more natural writing styles will make lexical prediction a much more difficult problem. On the other hand, the wider variety of grammatical constructions used may motivate more complex syntactic features, for instance as proposed by (Siddharthan et al., 2004) in sentence clustering. Finding optimal orderings is a difficult task even for short documents, and will become exponentially more challenging in longer ones. For multiparagraph documents, it is probably impractical to use full-scale coherence models to find optimal orderings directly. A better approach may be a coarseto-fine or hierarchical system which cuts up longer documents into more manageable chunks that can be ordered as a unit. Multi-paragraph documents also pose a problem for the τ metric itself. In documents with clear thematic divisions between their different sections, a good ord"
N07-1055,P06-2103,0,0.426319,"ph.austerweil@gmail.com Abstract We present a model for discourse coherence which combines the local entitybased approach of (Barzilay and Lapata, 2005) and the HMM-based content model of (Barzilay and Lee, 2004). Unlike the mixture model of (Soricut and Marcu, 2006), we learn local and global features jointly, providing a better theoretical explanation of how they are useful. As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively. Our model performs the ordering task competitively with (Soricut and Marcu, 2006), and significantly better than either of the models it is based on. 1 Introduction Models of coherent discourse are central to several tasks in natural language processing: such models have been used in text generation (Kibble and Power, 2004) and evaluation of human-produced text in educational applications (Miltsakaki and Kukich, 2004; Higgins et al., 2004). Moreover, an accurate model can reveal information about document structure, aiding in such tasks as supervised summarization (Barzilay and Lapata, 2005). Models of coherence tend to fall into two classes. Local models (Lapata, 2003; Ba"
N07-1055,J08-1001,0,\N,Missing
N09-1019,M98-1014,0,0.012749,"Missing"
N09-1019,E09-1018,1,0.512187,"e 3). Our pronoun information is derived from an unsupervised coreference algorithm which does not use named entity informa1 We stem modifiers with the Porter stemmer. ROOT →Modifiers 0 # NE 0 # Prepositions 0 # Pronouns 0 # ... Pronouns 0 →Pronoun 0 Pronouns 0 Pronouns 0 → Pronoun 0 →pers|loc|org|any pers →i |he|she|who|me . . . loc →where|which|it|its org →which|it|they|we . . . Figure 3: A fragment of the full grammar. The symbol # represents punctuation between different feature types. The prior for class 0 is concentrated around personal pronouns, although other types are possible. tion (Charniak and Elsner, 2009). This algorithm uses EM to learn a generative model with syntactic, number and gender parameters. Like Haghighi and Klein (2007), we give our model information about the basic types of pronouns in English. By setting up the base grammar so that each named-entity class prefers to associate to a single type of pronoun, we can also determine the correspondence between our named-entity symbols and the actual named-entity labels– for the models without pronoun information, this matching is arbitrary and must be inferred during the evaluation process. 3.4 Data Preparation To prepare data for cluste"
N09-1019,P05-1022,1,0.178168,"yntactic, number and gender parameters. Like Haghighi and Klein (2007), we give our model information about the basic types of pronouns in English. By setting up the base grammar so that each named-entity class prefers to associate to a single type of pronoun, we can also determine the correspondence between our named-entity symbols and the actual named-entity labels– for the models without pronoun information, this matching is arbitrary and must be inferred during the evaluation process. 3.4 Data Preparation To prepare data for clustering with our system, we first parse it with the parser of Charniak and Johnson (2005). We then annotate pronouns with Charniak and Elsner (2009). For the evaluation set, we use the named entity data from MUC-7. Here, we extract all strings in <ne> tags and determine their cores, plus any relevant modifiers, governing prepositions and pronouns, by examining the parse trees. In addition, we supply the system with additional data from the North American News Corpus (NANC). Here we extract all NPs headed by proper nouns. We then process our data by merging all examples with the same core; some merged examples from our dataset are shown in Figure 4. When two examples are merged, we"
N09-1019,N01-1007,1,0.896329,"the use of extremely large corpora which allow very precise, but sparse features. For instance Etzioni et al. (2005) and Pasca (2004) use web queries to count occurrences of “cities such as X” and similar phrases. Although our research makes use of a fairly large amount of data, our method is designed to make better use of relatively common contextual features, rather than searching for high-quality semantic features elsewhere. Models of the internal structure of names have been used for cross-document coreference (Li et al., 2004; Bhattacharya and Getoor, 2006) and a goal in their own right (Charniak, 2001). Li et al. (2004) take named entity classes as a given, and develops both generative and discriminative models to detect coreference between members of each class. Their generative model designates a particular mention of a name as a “representative” and generates all other mentions from it according to an editing process. Bhattacharya and Getoor (2006) operates only on authors of scientific papers. Their model accounts for a wider variety of name variants than ours, including misspellings and initials. In addition, they confirm our intuition that Gibbs sampling for inference has insufficient"
N09-1019,W99-0613,0,0.529894,"he i prior parameters for the entity-specific symbols Exk are fixed so that, with overwhelming probability, only one expansion occurs. We can represent any fixed number of entities Ek with a standard adaptor grammar, but since we do not know the correct number, we must extend the adaptor model slightly to allow for an unbounded number. We generate the Ek from a Chinese Restaurant process prior. (General grammars with infinite numbers of nonterminals were studied by (Liang et al., 2007b)). 3.3 Modifiers, Prepositions and Pronouns Next, we introduce two types of context information derived from Collins and Singer (1999): nominal modifiers and prepositional information. A nominal modifier is either the head of an appositive phrase (“Maury Cooper, a vice president”) or a non-proper prenominal (“spokesman John Smith”)1 . If the entity is the complement of a preposition, we extract the preposition and the head of the governing NP (“a federally funded sewage plant in Georgia”). These are added to the grammar at the named-entity class level (separated from the core by a special punctuation symbol). Finally, we add information about pronouns and wh-complementizers (Figure 3). Our pronoun information is derived from"
N09-1019,D07-1074,0,0.0335999,"Missing"
N09-1019,P07-1107,0,0.5112,"m (using no “seed rules” or initial heuristics); to our knowledge this is the best such system reported on the MUC-7 dataset. In addition, the model clusters the words which appear in named entities, discovering groups of words with similar roles such as first names and types of organization. Finally, the model defines a notion of consistency between different references to the same entity; this component of the model yields a significant increase in performance. The main motivation for our system is the recent success of unsupervised generative models for coreference resolution. The model of Haghighi and Klein (2007) incorporated a latent variable for named entity class. They report a named entity score 164 of 61.2 percent, well above the baseline of 46.4, but still far behind existing named-entity systems. We suspect that better models for named entities could aid in the coreference task. The easiest way to incorporate a better model is simply to run a supervised or semi-supervised system as a preprocess. To perform joint inference, however, requires an unsupervised generative model for named entities. As far as we know, this work is the best such model. Named entities also pose another problem with the"
N09-1019,N07-1018,1,0.413381,"ers) observed throughout a large input corpus while keeping the size of our input file small. To create an input file, we first add all the MUC7 examples. We then draw additional examples from NANC, ranking them by how many features they have, until we reach a specified number (larger datasets take longer, but without enough data, results tend to be poor). 3.5 Inference Our implementation of adaptor grammars is a modified version of the Pitman-Yor adaptor grammar sampler2 , altered to deal with the infinite number of entities. It carries out inference using a Metropoliswithin-Gibbs algorithm (Johnson et al., 2007), in which it repeatedly parses each input line using the CYK algorithm, samples a parse, and proposes this as the new tree. To do Gibbs sampling for our consistencyenforcing model, we would need to sample a parse for an example from the posterior over every possible entity. However, since there are thousands of entities (the number grows roughly linearly with the number of merged examples in the data file), this is not tractable. Instead, we perform a restricted Gibbs sampling search, where we enumerate the posterior only for entities which share a word in their core with the example in quest"
N09-1019,D07-1072,0,0.0410452,"ER,Clinton ∼ EP0 ER , which we intend to be a distribution over titles in general. The resulting grammar is shown in Figure 2; the i prior parameters for the entity-specific symbols Exk are fixed so that, with overwhelming probability, only one expansion occurs. We can represent any fixed number of entities Ek with a standard adaptor grammar, but since we do not know the correct number, we must extend the adaptor model slightly to allow for an unbounded number. We generate the Ek from a Chinese Restaurant process prior. (General grammars with infinite numbers of nonterminals were studied by (Liang et al., 2007b)). 3.3 Modifiers, Prepositions and Pronouns Next, we introduce two types of context information derived from Collins and Singer (1999): nominal modifiers and prepositional information. A nominal modifier is either the head of an appositive phrase (“Maury Cooper, a vice president”) or a non-proper prenominal (“spokesman John Smith”)1 . If the entity is the complement of a preposition, we extract the preposition and the head of the governing NP (“a federally funded sewage plant in Georgia”). These are added to the grammar at the named-entity class level (separated from the core by a special pu"
N09-1019,M98-1021,0,0.0138538,"Missing"
N09-1019,D08-1067,0,0.0360479,"d aid in the coreference task. The easiest way to incorporate a better model is simply to run a supervised or semi-supervised system as a preprocess. To perform joint inference, however, requires an unsupervised generative model for named entities. As far as we know, this work is the best such model. Named entities also pose another problem with the Haghighi and Klein (2007) coreference model; since it models only the heads of NPs, it will fail to resolve some references to named entities: (“Ford Motor Co.”, “Ford”), while erroneously merging others: (“Ford Motor Co.”, “Lockheed Martin Co.”). Ng (2008) showed that better features for matching named entities– exact string match and an “alias detector” looking for acronyms, abbreviations and name variants– improve the model’s performance substantially. Yet building an alias detector is nontrivial (Uryupina, 2004). English speakers know that “President Clinton” is the same person as “Bill Clinton” , not “President Bush”. But this cannot be implemented by simple substring matching. It requires some concept of the role of each word in the string. Our model attempts to learn this role information by clustering the words within named entities. 2 R"
N09-1019,uryupina-2004-evaluating,0,0.0143615,"we know, this work is the best such model. Named entities also pose another problem with the Haghighi and Klein (2007) coreference model; since it models only the heads of NPs, it will fail to resolve some references to named entities: (“Ford Motor Co.”, “Ford”), while erroneously merging others: (“Ford Motor Co.”, “Lockheed Martin Co.”). Ng (2008) showed that better features for matching named entities– exact string match and an “alias detector” looking for acronyms, abbreviations and name variants– improve the model’s performance substantially. Yet building an alias detector is nontrivial (Uryupina, 2004). English speakers know that “President Clinton” is the same person as “Bill Clinton” , not “President Bush”. But this cannot be implemented by simple substring matching. It requires some concept of the role of each word in the string. Our model attempts to learn this role information by clustering the words within named entities. 2 Related Work Supervised named entity recognition now performs almost as well as human annotation in English (Chinchor, 1998) and has excellent performance on other languages (Tjong Kim Sang and De Meulder, 2003). For a survey of the state of the art, Human Language"
N09-1019,M98-1004,0,\N,Missing
N09-1019,M98-1012,0,\N,Missing
N09-1019,W03-0419,0,\N,Missing
N19-1007,P14-1103,0,0.0309518,"a learner faced with the immense challenge of discovering structure in dense perceptual input, do theory-driven phonological features “stand out” or are they swamped by noise? In this paper, we address this question using an unsupervised computational acquisition model. Previous models of phonological category induction have emphasized the importance of topdown information (information about the contexts in which phonemes occur) (Peperkamp et al., 2006; Swingley, 2009; Feldman et al., 2009a, 2013a,b; Moreton and Pater, 2012a,b; Martin et al., 2013; Pater and Moreton, 2014; Frank et al., 2014; Doyle et al., 2014; Doyle and Levy, 2016). But to prevent the acquisition process from being circular, the learner cannot operate solely on top-down information — the acoustic signal must provide some evidence for the phonemic categories. We hypothesize that the same must be true for at least some phonological features (e.g. [±nasal], [±lateral]), but previous work on unsupervised speech processing has inferred phonological structure from spoken utterances using either (1) discrete transition-based architectures (Varadarajan et al., 2008; Jansen and Church, 2011; Lee and Glass, 2012), which do attempt to discov"
N19-1007,P12-1005,0,0.954931,"on, 2014; Frank et al., 2014; Doyle et al., 2014; Doyle and Levy, 2016). But to prevent the acquisition process from being circular, the learner cannot operate solely on top-down information — the acoustic signal must provide some evidence for the phonemic categories. We hypothesize that the same must be true for at least some phonological features (e.g. [±nasal], [±lateral]), but previous work on unsupervised speech processing has inferred phonological structure from spoken utterances using either (1) discrete transition-based architectures (Varadarajan et al., 2008; Jansen and Church, 2011; Lee and Glass, 2012), which do attempt to discover featurally-related natural classes, or (2) continuous deep neural (Kamper et al., 2015, In this paper, we deploy binary stochastic neural autoencoder networks as models of infant language learning in two typologically unrelated languages (Xitsonga and English). We show that the drive to model auditory percepts leads to latent clusters that partially align with theory-driven phonemic categories. We further evaluate the degree to which theorydriven phonological features are encoded in the latent bit patterns, finding that some (e.g. [±approximant]), are well repres"
N19-1007,D07-1043,0,0.0717314,"Missing"
N19-1007,P08-2042,0,0.0514229,"ater, 2012a,b; Martin et al., 2013; Pater and Moreton, 2014; Frank et al., 2014; Doyle et al., 2014; Doyle and Levy, 2016). But to prevent the acquisition process from being circular, the learner cannot operate solely on top-down information — the acoustic signal must provide some evidence for the phonemic categories. We hypothesize that the same must be true for at least some phonological features (e.g. [±nasal], [±lateral]), but previous work on unsupervised speech processing has inferred phonological structure from spoken utterances using either (1) discrete transition-based architectures (Varadarajan et al., 2008; Jansen and Church, 2011; Lee and Glass, 2012), which do attempt to discover featurally-related natural classes, or (2) continuous deep neural (Kamper et al., 2015, In this paper, we deploy binary stochastic neural autoencoder networks as models of infant language learning in two typologically unrelated languages (Xitsonga and English). We show that the drive to model auditory percepts leads to latent clusters that partially align with theory-driven phonemic categories. We further evaluate the degree to which theorydriven phonological features are encoded in the latent bit patterns, finding t"
N19-1231,D16-1153,0,0.0176387,"ctures in low resource environments, many approaches leverage external resources (Al-Rfou et al., 2015). Cotterell and Duh (2017), for instance, harvest silver annotations from structured Wikipedia data and build models for typologically diverse languages, though their approach is limited to specific domains and label sets. Lin and Lu (2018) adapt well-resourced NER systems to low resource target domains, given minimal annotation and word embeddings in domain. Several translation-based approaches leverage better resourced languages by inducing lexical information from multi-lingual resources (Bharadwaj et al., 2016; Nguyen and Chiang, 2017; Xie et al., 2018). In a slightly different vein, Shang et al. (2018) use dictionaries as distant supervision to resolve entity ambiguity. Unfortunately, external resources are not always publicly available. It is in fact impossible to replicate many of the above studies without a government contract and extensive knowledge of linguistic resources, limiting their applicability to many DH scenarios. Mayhew et al. (2017) suggest manually building bilingual dictionaries when no other translation resources are available to facilitate their method, though active learning p"
N19-1231,Q17-1010,0,0.0231081,"ining data is reported as percentage of the entire corpus for inclusive evaluations, and as tokens actively annotated (i.e., not counting the random seed sentences) for exclusive evaluations. For consistency, following seed annotation, we always fetch additional annotation batches at the following intervals, in tokens: 1K, 4K, 5K, 10K, 20K until we reach 100K total tokens, 50K until 300K total, 100K until 500K total, and 250K from there. For all experiments leveraging neural taggers, we use freely available pretrained embeddings (Grave et al., 2018), except for Latin, where we train fasttext (Bojanowski et al., 2017) embeddings on the Perseus (Smith et al., 2000) and Latin Library collections with default parameters (using pretrained embeddings yield small performance boosts that decrease with additional training data). We conclude this section with a direct comparison to the recently proposed active learning pipeline of Shen et al. (2017) and their MNLP ranking algorithm. 4.1 Consistency of Non-deterministic Results Because the active learning pipeline involves taking a random seed and many of the experiments on larger corpora could not be averaged over several runs, we first measure performance variatio"
N19-1231,I17-2016,0,0.0181024,"input data structure and noisy optical character recognition (Van Hooland et al., 2013; Kettunen et al., 2017). Low Resource NER Language agnostic NER is highly desirable, yet limited by the data available in the least resourced languages. Curran and Clark (2003) demonstrate that careful feature engineering can be typologically robust, though data hungry neural architectures have achieved stateof-the-art performance without feature engineering (Lample et al., 2016). To enable neural architectures in low resource environments, many approaches leverage external resources (Al-Rfou et al., 2015). Cotterell and Duh (2017), for instance, harvest silver annotations from structured Wikipedia data and build models for typologically diverse languages, though their approach is limited to specific domains and label sets. Lin and Lu (2018) adapt well-resourced NER systems to low resource target domains, given minimal annotation and word embeddings in domain. Several translation-based approaches leverage better resourced languages by inducing lexical information from multi-lingual resources (Bharadwaj et al., 2016; Nguyen and Chiang, 2017; Xie et al., 2018). In a slightly different vein, Shang et al. (2018) use diction"
N19-1231,W03-0424,0,0.149449,"fine grained NER for English. Erdmann et al. (2016) and Sprugnoli (2018), among others, have shown that such off-the-shelf models can be substantially improved on DH-relevant data. Work such as Smith and Crane (2001) and Simon et al. (2016) represent a large community mining such data for geospatial entities. Additional DH work on NER concerns the impact of input data structure and noisy optical character recognition (Van Hooland et al., 2013; Kettunen et al., 2017). Low Resource NER Language agnostic NER is highly desirable, yet limited by the data available in the least resourced languages. Curran and Clark (2003) demonstrate that careful feature engineering can be typologically robust, though data hungry neural architectures have achieved stateof-the-art performance without feature engineering (Lample et al., 2016). To enable neural architectures in low resource environments, many approaches leverage external resources (Al-Rfou et al., 2015). Cotterell and Duh (2017), for instance, harvest silver annotations from structured Wikipedia data and build models for typologically diverse languages, though their approach is limited to specific domains and label sets. Lin and Lu (2018) adapt well-resourced NER"
N19-1231,W16-4012,1,0.905616,"Missing"
N19-1231,P05-1045,0,0.0310344,"frameworks when evaluating an active learning approach, as the intended application determines which framework is more relevant and thus, which model should be employed. Controlling for the NER model, HER’s active learning sentence ranking component achieves significant improvement over a competitive baseline (Shen et al., 2017). Because HER does not reference the inference model during sentence ranking, this provides counter evidence to Lowell et al. (2018)’s hypothesis that non-native active learning is suboptimal. 2 Related Work The best known NER systems among humanists are Stanford NER (Finkel et al., 2005), with pretrained models in several languages and an interface for building new models, and among researchers interested in NER for spatial research, the Edinburgh Geoparser (Grover et al., 2010), with fine grained NER for English. Erdmann et al. (2016) and Sprugnoli (2018), among others, have shown that such off-the-shelf models can be substantially improved on DH-relevant data. Work such as Smith and Crane (2001) and Simon et al. (2016) represent a large community mining such data for geospatial entities. Additional DH work on NER concerns the impact of input data structure and noisy optical"
N19-1231,L18-1550,0,0.0162553,"OOVs occurring in non-sentence initial position. Quantity of training data is reported as percentage of the entire corpus for inclusive evaluations, and as tokens actively annotated (i.e., not counting the random seed sentences) for exclusive evaluations. For consistency, following seed annotation, we always fetch additional annotation batches at the following intervals, in tokens: 1K, 4K, 5K, 10K, 20K until we reach 100K total tokens, 50K until 300K total, 100K until 500K total, and 250K from there. For all experiments leveraging neural taggers, we use freely available pretrained embeddings (Grave et al., 2018), except for Latin, where we train fasttext (Bojanowski et al., 2017) embeddings on the Perseus (Smith et al., 2000) and Latin Library collections with default parameters (using pretrained embeddings yield small performance boosts that decrease with additional training data). We conclude this section with a direct comparison to the recently proposed active learning pipeline of Shen et al. (2017) and their MNLP ranking algorithm. 4.1 Consistency of Non-deterministic Results Because the active learning pipeline involves taking a random seed and many of the experiments on larger corpora could not"
N19-1231,N16-1030,0,0.21778,"ework is relevant to the user who wants to build an NER tool that can generalize well to other corpora. We conduct extensive experiments comparing several combinations of active learning algorithms and NER model architectures in both frameworks across many typologically diverse languages and domains. The systematic differences between inclusive and exclusive results demonstrate that while deep NER model architectures 1 github.com/alexerdmann/HER. 2223 Proceedings of NAACL-HLT 2019, pages 2223–2234 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics (Lample et al., 2016) are highly preferable for tagging held out sentences, shallow models (Lafferty et al., 2001) perform better on sentences that could have been chosen for manual annotation but were not selected by the active learning algorithm. We argue for the importance of considering both frameworks when evaluating an active learning approach, as the intended application determines which framework is more relevant and thus, which model should be employed. Controlling for the NER model, HER’s active learning sentence ranking component achieves significant improvement over a competitive baseline (Shen et al.,"
N19-1231,D18-1226,0,0.0269341,"sourced languages. Curran and Clark (2003) demonstrate that careful feature engineering can be typologically robust, though data hungry neural architectures have achieved stateof-the-art performance without feature engineering (Lample et al., 2016). To enable neural architectures in low resource environments, many approaches leverage external resources (Al-Rfou et al., 2015). Cotterell and Duh (2017), for instance, harvest silver annotations from structured Wikipedia data and build models for typologically diverse languages, though their approach is limited to specific domains and label sets. Lin and Lu (2018) adapt well-resourced NER systems to low resource target domains, given minimal annotation and word embeddings in domain. Several translation-based approaches leverage better resourced languages by inducing lexical information from multi-lingual resources (Bharadwaj et al., 2016; Nguyen and Chiang, 2017; Xie et al., 2018). In a slightly different vein, Shang et al. (2018) use dictionaries as distant supervision to resolve entity ambiguity. Unfortunately, external resources are not always publicly available. It is in fact impossible to replicate many of the above studies without a government co"
N19-1231,D17-1269,0,0.0611551,"Missing"
N19-1231,I17-2050,0,0.0174354,"nvironments, many approaches leverage external resources (Al-Rfou et al., 2015). Cotterell and Duh (2017), for instance, harvest silver annotations from structured Wikipedia data and build models for typologically diverse languages, though their approach is limited to specific domains and label sets. Lin and Lu (2018) adapt well-resourced NER systems to low resource target domains, given minimal annotation and word embeddings in domain. Several translation-based approaches leverage better resourced languages by inducing lexical information from multi-lingual resources (Bharadwaj et al., 2016; Nguyen and Chiang, 2017; Xie et al., 2018). In a slightly different vein, Shang et al. (2018) use dictionaries as distant supervision to resolve entity ambiguity. Unfortunately, external resources are not always publicly available. It is in fact impossible to replicate many of the above studies without a government contract and extensive knowledge of linguistic resources, limiting their applicability to many DH scenarios. Mayhew et al. (2017) suggest manually building bilingual dictionaries when no other translation resources are available to facilitate their method, though active learning provides a more direct mea"
N19-1231,D08-1112,0,0.0662843,"st informative to the model if annotated: uncertainty, where instances which confuse the model are given priority; diversity, where instances that would expand the model’s coverage are prioritized; and representativeness, prioritizing instances that best approximate the true distribution over all instances. Uncertainty-based approaches outperform other single-criterion approaches, though many works, primarily in Computer Vision, demonstrate that considering diversity reduces repetitive training examples and representativeness reduces outlier sampling (Roy and McCallum, 2001; Zhu et al., 2003; Settles and Craven, 2008; Zhu et al., 2008; Olsson, 2009; Gu et al., 2014; He et al., 2014; Yang et al., 2015; Wang et al., 2018b). For active learning in NER, Shen et al. (2017) propose the uncertainty-based metric maximized normalized log-probability (MNLP). It prioritizes sentences based on the length normalized log probability of the model’s predicted label sequence. To make neural active learning tractable, they shift workload to lighter convolutional neural networks (CNN) and update weights after each manual annotation batch instead of retraining from scratch. They demonstrate state-of-the-art performance with"
N19-1231,D18-1230,0,0.02038,"2015). Cotterell and Duh (2017), for instance, harvest silver annotations from structured Wikipedia data and build models for typologically diverse languages, though their approach is limited to specific domains and label sets. Lin and Lu (2018) adapt well-resourced NER systems to low resource target domains, given minimal annotation and word embeddings in domain. Several translation-based approaches leverage better resourced languages by inducing lexical information from multi-lingual resources (Bharadwaj et al., 2016; Nguyen and Chiang, 2017; Xie et al., 2018). In a slightly different vein, Shang et al. (2018) use dictionaries as distant supervision to resolve entity ambiguity. Unfortunately, external resources are not always publicly available. It is in fact impossible to replicate many of the above studies without a government contract and extensive knowledge of linguistic resources, limiting their applicability to many DH scenarios. Mayhew et al. (2017) suggest manually building bilingual dictionaries when no other translation resources are available to facilitate their method, though active learning provides a more direct means of improving NER quality. Active Learning Active learning seeks to"
N19-1231,P04-1075,0,0.428456,"ical sources, often require extensive funding, relying on considerable manual annotation (Simon et al., 2017). To this end, we introduce the Humanities Entity Recognizer (HER),1 a whitebox toolkit for buildyour-own NER models, freely available for public use. HER robustly handles any domain and userdefined label set, guiding users through an active learning process whereby sentences are chosen for manual annotation that are maximally informative to the model. Informativeness is determined based on novel interpretations of the uncertainty, representativeness, and diversity criteria proposed by Shen et al. (2004). In contrast to literature emphasizing the disproportionate or exclusive importance of uncertainty (Shen et al., 2017; Zhu et al., 2008; Olsson, 2009), we observe significant improvements by integrating all three criteria. In addition to a robust active learning based NER toolkit, we also contribute a novel evaluation framework. This inclusive framework considers the accuracy with which an entire corpus is annotated, regardless of which instances are annotated manually versus automatically, such that no instance is held out when the active learning algorithm considers candidates for annotatio"
N19-1231,W17-2630,0,0.228707,"end, we introduce the Humanities Entity Recognizer (HER),1 a whitebox toolkit for buildyour-own NER models, freely available for public use. HER robustly handles any domain and userdefined label set, guiding users through an active learning process whereby sentences are chosen for manual annotation that are maximally informative to the model. Informativeness is determined based on novel interpretations of the uncertainty, representativeness, and diversity criteria proposed by Shen et al. (2004). In contrast to literature emphasizing the disproportionate or exclusive importance of uncertainty (Shen et al., 2017; Zhu et al., 2008; Olsson, 2009), we observe significant improvements by integrating all three criteria. In addition to a robust active learning based NER toolkit, we also contribute a novel evaluation framework. This inclusive framework considers the accuracy with which an entire corpus is annotated, regardless of which instances are annotated manually versus automatically, such that no instance is held out when the active learning algorithm considers candidates for annotation. The standard, exclusive evaluation framework, by contrast, only measures the accuracy of the final trained model’s"
N19-1231,D18-1309,0,0.0455514,"Missing"
N19-1231,W03-0419,0,0.28682,"Missing"
N19-1231,D18-1124,0,0.054593,"ty; diversity, where instances that would expand the model’s coverage are prioritized; and representativeness, prioritizing instances that best approximate the true distribution over all instances. Uncertainty-based approaches outperform other single-criterion approaches, though many works, primarily in Computer Vision, demonstrate that considering diversity reduces repetitive training examples and representativeness reduces outlier sampling (Roy and McCallum, 2001; Zhu et al., 2003; Settles and Craven, 2008; Zhu et al., 2008; Olsson, 2009; Gu et al., 2014; He et al., 2014; Yang et al., 2015; Wang et al., 2018b). For active learning in NER, Shen et al. (2017) propose the uncertainty-based metric maximized normalized log-probability (MNLP). It prioritizes sentences based on the length normalized log probability of the model’s predicted label sequence. To make neural active learning tractable, they shift workload to lighter convolutional neural networks (CNN) and update weights after each manual annotation batch instead of retraining from scratch. They demonstrate state-of-the-art performance with MNLP, though Lowell et al. (2018) show its improvement above random sampling to be less dramatic, as do"
N19-1231,D18-1034,0,0.0126689,"hes leverage external resources (Al-Rfou et al., 2015). Cotterell and Duh (2017), for instance, harvest silver annotations from structured Wikipedia data and build models for typologically diverse languages, though their approach is limited to specific domains and label sets. Lin and Lu (2018) adapt well-resourced NER systems to low resource target domains, given minimal annotation and word embeddings in domain. Several translation-based approaches leverage better resourced languages by inducing lexical information from multi-lingual resources (Bharadwaj et al., 2016; Nguyen and Chiang, 2017; Xie et al., 2018). In a slightly different vein, Shang et al. (2018) use dictionaries as distant supervision to resolve entity ambiguity. Unfortunately, external resources are not always publicly available. It is in fact impossible to replicate many of the above studies without a government contract and extensive knowledge of linguistic resources, limiting their applicability to many DH scenarios. Mayhew et al. (2017) suggest manually building bilingual dictionaries when no other translation resources are available to facilitate their method, though active learning provides a more direct means of improving NER"
N19-1231,C08-1143,0,0.262902,"he Humanities Entity Recognizer (HER),1 a whitebox toolkit for buildyour-own NER models, freely available for public use. HER robustly handles any domain and userdefined label set, guiding users through an active learning process whereby sentences are chosen for manual annotation that are maximally informative to the model. Informativeness is determined based on novel interpretations of the uncertainty, representativeness, and diversity criteria proposed by Shen et al. (2004). In contrast to literature emphasizing the disproportionate or exclusive importance of uncertainty (Shen et al., 2017; Zhu et al., 2008; Olsson, 2009), we observe significant improvements by integrating all three criteria. In addition to a robust active learning based NER toolkit, we also contribute a novel evaluation framework. This inclusive framework considers the accuracy with which an entire corpus is annotated, regardless of which instances are annotated manually versus automatically, such that no instance is held out when the active learning algorithm considers candidates for annotation. The standard, exclusive evaluation framework, by contrast, only measures the accuracy of the final trained model’s predictions on a h"
P08-1095,P03-1071,0,0.0175328,"Missing"
P08-1095,P06-1004,0,0.0308436,"Missing"
P08-1095,W04-2401,0,0.013538,"g makes at least a reasonable starting point for further efforts, since it is a natural online algorithm– it assigns each utterance as it arrives, without reference to the future. At any rate, we should not take our objective function too seriously. Although it is roughly correlated with performance, the high error rate of the classifier makes it unlikely that small changes in objective will mean much. In fact, the objective value of our output solutions are generally higher than those for true so8 We set up the problem by taking the weight of edge i, j as the classifier’s decision pi,j − .5. Roth and Yih (2004) use log probabilities as weights. Bansal et al. (2004) propose the log odds ratio log(p/(1 − p)). We are unsure of the relative merit of these approaches. 840 lutions, which implies we have already reached the limits of what our classifier can tell us. 5 Experiments We annotate the 800 line test transcript using our system. The annotation obtained has 63 conversations, with mean length 12.70. The average density of conversations is 2.9, and the entropy is 3.79. This places it within the bounds of our human annotations (see table 1), toward the more general end of the spectrum. As a standard o"
P08-2011,P05-1018,0,0.477531,"ble, pronouns must be placed close to appropriate referents with the correct number and gender. Centering theory (Grosz et al., 1995) describes additional constraints about which entities in a discourse can be pronominalized: if there are pronouns in a segment, they must include the backwardlooking center. We use a model which probabilistically attempts to describe these preferences (Ge et al., 1998). These two models can be combined with the entity grid described by Lapata and Barzilay (2005) for significant improvement. The magnitude of the improvement is particularly interesting given that Barzilay and Lapata (2005) do use a coreference system but are unable to derive much advantage from it. 2 Discourse-new Model In the task of discourse-new classification, the model is given a referring expression (as in previous work, we consider only NPs) from a document and must 41 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 41–44, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics determine whether it is a first mention (discoursenew) or a subsequent mention (discourse-old). Features such as full names, appositives, and restrictive relative clauses are associated"
P08-2011,N04-1015,0,0.16387,"Missing"
P08-2011,P99-1048,0,0.0174872,"Missing"
P08-2011,D07-1009,0,0.2172,"un coreference algorithm to the information ordering task, and show significant improvements in performance over the entity grid, a popular model of local coherence. 1 Introduction Models of discourse coherence describe the relationships between nearby sentences, in which previous sentences help make their successors easier to understand. Models of coherence have been used to impose an order on sentences for multidocument summarization (Barzilay et al., 2002), to evaluate the quality of human-authored essays (Miltsakaki and Kukich, 2004), and to insert new information into existing documents (Chen et al., 2007). These models typically view a sentence either as a bag of words (Foltz et al., 1998) or as a bag of entities associated with various syntactic roles (Lapata and Barzilay, 2005). However, a mention of an entity contains more information than just its head and syntactic role. The referring expression itself contains discourse-motivated information distinguishing familiar entities from unfamiliar and salient from non-salient. These patterns have been studied extensively, by linguists (Prince, 1981; Fraurud, 1990) and in the field of coreference resolution. We draw on the coreference work, takin"
P08-2011,W98-1119,1,0.640917,"been previously mentioned in the discourse. As noted by studies since Hawkins (1978), there are marked syntactic differences between the two classes. Our second model describes pronoun coreference. To be intelligible, pronouns must be placed close to appropriate referents with the correct number and gender. Centering theory (Grosz et al., 1995) describes additional constraints about which entities in a discourse can be pronominalized: if there are pronouns in a segment, they must include the backwardlooking center. We use a model which probabilistically attempts to describe these preferences (Ge et al., 1998). These two models can be combined with the entity grid described by Lapata and Barzilay (2005) for significant improvement. The magnitude of the improvement is particularly interesting given that Barzilay and Lapata (2005) do use a coreference system but are unable to derive much advantage from it. 2 Discourse-new Model In the task of discourse-new classification, the model is given a referring expression (as in previous work, we consider only NPs) from a document and must 41 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 41–44, c Columbus, Ohio, USA, June 2008. 2008 Assoc"
P08-2011,J95-2003,0,0.82439,"ference work, taking two standard models from the literature and applying them to coherence modeling. Our first model distinguishes discourse-new from discourse-old noun phrases, using features based on Uryupina (2003). Discourse-new NPs are those whose referents have not been previously mentioned in the discourse. As noted by studies since Hawkins (1978), there are marked syntactic differences between the two classes. Our second model describes pronoun coreference. To be intelligible, pronouns must be placed close to appropriate referents with the correct number and gender. Centering theory (Grosz et al., 1995) describes additional constraints about which entities in a discourse can be pronominalized: if there are pronouns in a segment, they must include the backwardlooking center. We use a model which probabilistically attempts to describe these preferences (Ge et al., 1998). These two models can be combined with the entity grid described by Lapata and Barzilay (2005) for significant improvement. The magnitude of the improvement is particularly interesting given that Barzilay and Lapata (2005) do use a coreference system but are unable to derive much advantage from it. 2 Discourse-new Model In the"
P08-2011,J06-4002,0,0.0598224,"cu (2006). The entity grid represents each entity by tracking the syntactic roles in which it appears throughout the document. The internal syntax of the various referring expressions is ignored. Since it also uses the “same head” coreference heuristic, it also disregards pronouns. Since the three models use very different feature sets, we combine them by assuming independence and multiplying the probabilities. 5 Experiments We evaluate our models using two tasks, both based on the assumption that a human-authored document is coherent, and uses the best possible ordering of its sentences (see Lapata (2006)). In the discrimination task (Barzilay and Lapata, 2005), a document is compared with a random permutation of its sentences, and we score the system correct if it indicates the original as more coherent4 . 4 Since the model might refuse to make a decision by scoring a permutation the same as the original, we also report F-score, where precision is correct/decisions and recall is correct/total. 43 Discrimination becomes easier for longer documents, since a random permutation is likely to be much less similar to the original. Therefore we also test our systems on the task of insertion (Chen et"
P08-2011,N03-2024,0,0.306405,"Missing"
P08-2011,C02-1139,0,0.0558178,"Missing"
P08-2011,J98-2001,0,0.0370675,"Missing"
P08-2011,P06-2103,0,0.705054,"004 WSJ documents. Finding the probability of a document using this model requires us to sum out the antecedents a. Unfortunately, because each ai is conditioned on the previous ones, this cannot be done efficiently. Instead, we use a greedy search, assigning each pronoun left to right. Finally we report the probability of the resulting sequence of pronoun assignments. 4 Baseline Model As a baseline, we adopt the entity grid (Lapata and Barzilay, 2005). This model outperforms a variety of word overlap and semantic similarity models, and is used as a component in the state-of-the-art system of Soricut and Marcu (2006). The entity grid represents each entity by tracking the syntactic roles in which it appears throughout the document. The internal syntax of the various referring expressions is ignored. Since it also uses the “same head” coreference heuristic, it also disregards pronouns. Since the three models use very different feature sets, we combine them by assuming independence and multiplying the probabilities. 5 Experiments We evaluate our models using two tasks, both based on the assumption that a human-authored document is coherent, and uses the best possible ordering of its sentences (see Lapata (2"
P08-2011,P03-2012,0,0.0165502,"ver, a mention of an entity contains more information than just its head and syntactic role. The referring expression itself contains discourse-motivated information distinguishing familiar entities from unfamiliar and salient from non-salient. These patterns have been studied extensively, by linguists (Prince, 1981; Fraurud, 1990) and in the field of coreference resolution. We draw on the coreference work, taking two standard models from the literature and applying them to coherence modeling. Our first model distinguishes discourse-new from discourse-old noun phrases, using features based on Uryupina (2003). Discourse-new NPs are those whose referents have not been previously mentioned in the discourse. As noted by studies since Hawkins (1978), there are marked syntactic differences between the two classes. Our second model describes pronoun coreference. To be intelligible, pronouns must be placed close to appropriate referents with the correct number and gender. Centering theory (Grosz et al., 1995) describes additional constraints about which entities in a discourse can be pronominalized: if there are pronouns in a segment, they must include the backwardlooking center. We use a model which pro"
P08-2011,J00-4003,0,0.0267774,"Missing"
P10-2007,E09-1018,1,0.946536,"ked as mentions. In this paper, we count how often same-head pairs fail to corefer in the MUC-6 corpus, showing that gold mention detection hides most such pairs, but more realistic detection finds large numbers. We also present an unsupervised generative model which learns to make certain samehead pairs non-coreferent. The model is based on the idea that pronoun referents are likely to be salient noun phrases in the discourse, so we can learn about NP antecedents using pronominal antecedents as a starting point. Pronoun anaphora, in turn, is learnable from raw data (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). Since our model links fewer NPs than the baseline, it improves precision but decreases recall. This tradeoff is favorable for CEAF, but not for b3 . We investigate coreference relationships between NPs with the same head noun. It is relatively common in unsupervised work to assume that such pairs are coreferent– but this is not always true, especially if realistic mention detection is used. We describe the distribution of noncoreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some samehead NPs using syntactic features, improving prec"
P10-2007,W05-0612,0,0.0315234,"nal assumption are not marked as mentions. In this paper, we count how often same-head pairs fail to corefer in the MUC-6 corpus, showing that gold mention detection hides most such pairs, but more realistic detection finds large numbers. We also present an unsupervised generative model which learns to make certain samehead pairs non-coreferent. The model is based on the idea that pronoun referents are likely to be salient noun phrases in the discourse, so we can learn about NP antecedents using pronominal antecedents as a starting point. Pronoun anaphora, in turn, is learnable from raw data (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). Since our model links fewer NPs than the baseline, it improves precision but decreases recall. This tradeoff is favorable for CEAF, but not for b3 . We investigate coreference relationships between NPs with the same head noun. It is relatively common in unsupervised work to assume that such pairs are coreferent– but this is not always true, especially if realistic mention detection is used. We describe the distribution of noncoreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some samehead NPs using syntac"
P10-2007,C00-1027,0,0.0139205,"as a starting point. For translation, we use a trivial model, p(ni |gai ) = 1 if the two have the same head, and 0 otherwise, except for the null antecedent, which draws heads from a multinomial distribution over words. While we could learn an alignment and then treat all generators as antecedents, so that only NPs aligned to the null antecedent were not labeled coreferent, in practice this model would align nearly all the same-head pairs. This is true because many words are “bursty”; the probability of a second occurrence given the first is higher than the a priori probability of occurrence (Church, 2000). Therefore, our model is actually a mixture of two IBM models, pC and pN , where pC produces NPs with antecedents and pN produces pairs that share a head, but are not coreferent. To break the symmetry, we allow pC to use any parameters w, while pN uses a uniform alignment, w ≡ ~0. We interpolate between these two models with a constant λ, the single manually set parameter of our system, which we fixed at .9. The full model, therefore, is: ator (the largest term in either of the sums) is from pT and is not the null antecedent are marked as coreferent to the generator. Other NPs are marked not"
P10-2007,P07-1107,0,0.084103,"arlier in the document are the easiest to 2 Related work Unsupervised systems specify the assumption of same-head coreference in several ways: by as1 Gold mention detection means something slightly different in the ACE corpus, where the system input contains every NP annotated with an entity type. 33 Proceedings of the ACL 2010 Conference Short Papers, pages 33–37, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics sumption (Haghighi and Klein, 2009), using a head-prediction clause (Poon and Domingos, 2008), and using a sparse Dirichlet prior on word emissions (Haghighi and Klein, 2007). (These three systems, perhaps not coincidentally, use gold mentions.) An exception is Ng (2008), who points out that head identity is not an entirely reliable cue and instead uses exact string match (minus determiners) for common NPs and an alias detection system for proper NPs. This work uses mentions extracted with an NP chunker. No specific results are reported for same-head NPs. However, while using exact string match raises precision, many non-matching phrases are still coreferent, so this approach cannot be considered a full solution to the problem. Supervised systems do better on the"
P10-2007,D09-1120,0,0.0593013,"ask in a notoriously difficult area; Stoyanov et al. (2009) shows that, among NPs headed by common nouns, those which have an exact match earlier in the document are the easiest to 2 Related work Unsupervised systems specify the assumption of same-head coreference in several ways: by as1 Gold mention detection means something slightly different in the ACE corpus, where the system input contains every NP annotated with an entity type. 33 Proceedings of the ACL 2010 Conference Short Papers, pages 33–37, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics sumption (Haghighi and Klein, 2009), using a head-prediction clause (Poon and Domingos, 2008), and using a sparse Dirichlet prior on word emissions (Haghighi and Klein, 2007). (These three systems, perhaps not coincidentally, use gold mentions.) An exception is Ng (2008), who points out that head identity is not an entirely reliable cue and instead uses exact string match (minus determiners) for common NPs and an alias detection system for proper NPs. This work uses mentions extracted with an NP chunker. No specific results are reported for same-head NPs. However, while using exact string match raises precision, many non-matchi"
P10-2007,N09-1069,0,0.0271473,"process fills in all the NP nodes in order, from left to right. This process ensures that, when generating node ni , we have already filled in all the NPs in the set G (since these all precede ni ). When deciding on a generator for NP ni , we can extract features characterizing its p(ai = j|G, D) ∝ exp(f (ni , gj , D) • w) The weights w are learned by gradient descent on the log-likelihood. To use this model within EM, we alternate an E-step where we calculate the expected alignments E[ai = j], then an Mstep where we run gradient descent. (We have also had some success with stepwise EM as in (Liang and Klein, 2009), but this requires some tuning to work properly.) 4 35 Downloaded from http://bllip.cs.brown.edu. As features, we take the same features as Charniak and Elsner (2009): sentence and word-count distance between ni and gj , sentence position of each, syntactic role of each, and head type of gj (proper, common or pronoun). We add binary features for the nonterminal directly over gj (NP, VP, PP, any S type, or other), the type of phrases modifying gj (proper nouns, phrasals (except QP and PP), QP, PP-of, PP-other, other modifiers, or nothing), and the type of determiner of gj (possessive, definite"
P10-2007,H05-1004,0,0.266784,"Missing"
P10-2007,N06-1020,1,0.855374,"Missing"
P10-2007,D08-1067,0,0.0667463,"coreference in several ways: by as1 Gold mention detection means something slightly different in the ACE corpus, where the system input contains every NP annotated with an entity type. 33 Proceedings of the ACL 2010 Conference Short Papers, pages 33–37, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics sumption (Haghighi and Klein, 2009), using a head-prediction clause (Poon and Domingos, 2008), and using a sparse Dirichlet prior on word emissions (Haghighi and Klein, 2007). (These three systems, perhaps not coincidentally, use gold mentions.) An exception is Ng (2008), who points out that head identity is not an entirely reliable cue and instead uses exact string match (minus determiners) for common NPs and an alias detection system for proper NPs. This work uses mentions extracted with an NP chunker. No specific results are reported for same-head NPs. However, while using exact string match raises precision, many non-matching phrases are still coreferent, so this approach cannot be considered a full solution to the problem. Supervised systems do better on the task, but not perfectly. Recent work (Stoyanov et al., 2009) attempts to determine the contributi"
P10-2007,D08-1068,0,0.0797683,"shows that, among NPs headed by common nouns, those which have an exact match earlier in the document are the easiest to 2 Related work Unsupervised systems specify the assumption of same-head coreference in several ways: by as1 Gold mention detection means something slightly different in the ACE corpus, where the system input contains every NP annotated with an entity type. 33 Proceedings of the ACL 2010 Conference Short Papers, pages 33–37, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics sumption (Haghighi and Klein, 2009), using a head-prediction clause (Poon and Domingos, 2008), and using a sparse Dirichlet prior on word emissions (Haghighi and Klein, 2007). (These three systems, perhaps not coincidentally, use gold mentions.) An exception is Ng (2008), who points out that head identity is not an entirely reliable cue and instead uses exact string match (minus determiners) for common NPs and an alias detection system for proper NPs. This work uses mentions extracted with an NP chunker. No specific results are reported for same-head NPs. However, while using exact string match raises precision, many non-matching phrases are still coreferent, so this approach cannot b"
P10-2007,P09-1074,0,0.187105,"ve not attracted nearly as much attention, and many systems, especially unsupervised ones, operate under the assumption that all same-head pairs corefer. This is by no means always the case– there are several systematic exceptions to the rule. In this paper, we show that these exceptions are fairly common, and describe an unsupervised system which learns to distinguish them from coreferent same-head pairs. There are several reasons why relatively little attention has been paid to same-head pairs. Primarily, this is because they are a comparatively easy subtask in a notoriously difficult area; Stoyanov et al. (2009) shows that, among NPs headed by common nouns, those which have an exact match earlier in the document are the easiest to 2 Related work Unsupervised systems specify the assumption of same-head coreference in several ways: by as1 Gold mention detection means something slightly different in the ACE corpus, where the system input contains every NP annotated with an entity type. 33 Proceedings of the ACL 2010 Conference Short Papers, pages 33–37, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics sumption (Haghighi and Klein, 2009), using a head-prediction clause ("
P10-2007,P07-1031,0,0.0773972,"Missing"
P10-2007,nissim-etal-2004-annotation,0,\N,Missing
P10-2007,W06-1612,0,\N,Missing
P10-2007,N04-4009,0,\N,Missing
P11-1118,P05-1018,0,0.282907,"rk; one, the topical entity grid, is a novel extension of the entity grid. For the experiments below, we train the models on SWBD, sometimes augmented with a larger set of automatically parsed conversations from the F ISHER corpus. Since the two corpora are quite similar, F ISHER is a useful source for extra data; McClosky et al. (2010) uses it for this purpose in parsing experiments. (We continue to use SWBD/F ISHER even for experiments on IRC, because we do not have enough disentangled training data to learn lexical relationships.) 3.1 Entity grid The entity grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005) is an attempt to model some principles of Centering Theory (Grosz et al., 1995) in a statistical manner. It represents a document in terms of entities and their syntactic roles: subject (S), object (O), other (X) and not present (-). In each new 2 cs.brown.edu/melsner utterance, the grid predicts the role in which each entity will appear, given its history of roles in the previous sentences, plus a salience feature counting the total number of times the entity occurs. For instance, for an entity which is the subject of sentence 1, the object of sentence 2, and occurs four times in total, the"
P11-1118,N04-1015,0,0.128303,"apply these improvements to the full task. We suspect that, with better low-level annotation tools for the chat domain and a good way of integrating prior information, our improvements on SWBD could transfer fully to IRC chat. 2 Related work There is extensive previous work on coherence models for text ordering; we describe several specic models below, in section 2. This study focuses on models of local coherence, which relate text to its immediate context. There has also been work on global coherence, the structure of a document as a whole (Chen et al., 2009; Eisenstein and Barzilay, 2008; Barzilay and Lee, 2004), typically modeled in terms of sequential topics. We avoid using them here, because we do not believe topic sequences are predictable in conversation and because such models tend to be algorithmically cumbersome. In addition to text ordering, local coherence models have also been used to score the uency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate previously unknown weaknesses in models. However, the ta"
P11-1118,E09-1018,1,0.736844,"proposed earlier (Lapata, 2003). This model attempts to generate the content words of the next sentence by translating them from the words of the previous sentence, plus a null word; thus, it will learn alignments between pairs of words that tend to occur in adjacent sentences. We learn parameters on the F ISHER corpus, and on NANC for news. 3.4 Pronouns The use of a generative pronoun resolver for coherence modeling originates in Elsner and Charniak (2008a). That paper used a supervised model (Ge et al., 1998), but we adapt a newer, unsupervised model which they also make publicly available (Charniak and Elsner, 2009)4 . They model each pronoun as generated by an antecedent somewhere in the previous two sentences. If a good antecedent is found, the probability of the pronoun's occurrence will be high; otherwise, the probability is low, signaling that the text is less coherent because the pronoun is hard to interpret correctly. We use the model as distributed for news text. For conversation, we adapt it by running a few iterations of their EM training algorithm on the F ISHER data. 4 bllip.cs.brown.edu/resources.shtml #software 3 1181 3.5 Discourse-newness Building on work from summarization (Nenkova and M"
P11-1118,N09-1042,0,0.0112794,"entanglement task, though so far, we are unable to apply these improvements to the full task. We suspect that, with better low-level annotation tools for the chat domain and a good way of integrating prior information, our improvements on SWBD could transfer fully to IRC chat. 2 Related work There is extensive previous work on coherence models for text ordering; we describe several specic models below, in section 2. This study focuses on models of local coherence, which relate text to its immediate context. There has also been work on global coherence, the structure of a document as a whole (Chen et al., 2009; Eisenstein and Barzilay, 2008; Barzilay and Lee, 2004), typically modeled in terms of sequential topics. We avoid using them here, because we do not believe topic sequences are predictable in conversation and because such models tend to be algorithmically cumbersome. In addition to text ordering, local coherence models have also been used to score the uency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate"
P11-1118,D08-1035,0,0.0574798,"though so far, we are unable to apply these improvements to the full task. We suspect that, with better low-level annotation tools for the chat domain and a good way of integrating prior information, our improvements on SWBD could transfer fully to IRC chat. 2 Related work There is extensive previous work on coherence models for text ordering; we describe several specic models below, in section 2. This study focuses on models of local coherence, which relate text to its immediate context. There has also been work on global coherence, the structure of a document as a whole (Chen et al., 2009; Eisenstein and Barzilay, 2008; Barzilay and Lee, 2004), typically modeled in terms of sequential topics. We avoid using them here, because we do not believe topic sequences are predictable in conversation and because such models tend to be algorithmically cumbersome. In addition to text ordering, local coherence models have also been used to score the uency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate previously unknown weaknesses i"
P11-1118,P08-2011,1,0.548915,"adopt the S WITCHBOARD (SWBD) corpus. SWBD contains recorded telephone conversations with known topics and hand-annotated parse trees; this allows us to control for the performance of our parser and other informational resources. To compare the two algorithmic settings, we use SWBD for ordering experiments, and also articially entangle pairs of telephone dialogues to create synthetic transcripts which we can disentangle. Finally, we present results on actual internet chat corpora. On synthetic SWBD transcripts, local coherence models improve performance considerably over our baseline model, Elsner and Charniak (2008b). On 1179 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1179–1189, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics internet chat, we continue to do better on a constrained disentanglement task, though so far, we are unable to apply these improvements to the full task. We suspect that, with better low-level annotation tools for the chat domain and a good way of integrating prior information, our improvements on SWBD could transfer fully to IRC chat. 2 Related work There is extensive previous work on coherenc"
P11-1118,P08-1095,1,0.367747,"Missing"
P11-1118,N10-1060,0,0.01319,"uns +EGrid/Topic/IBM-1 E+C `08b 74.0 79.3 76.8 76.3 73.9 78.3 76.4 +EGrid E+C `08b # IPHONE 92.3 89.0 # PHYSICS 96.6 90.2 # PYTHON 91.1 88.4 Table 5: Average accuracy for disentanglement of a single utterance for 19581 total lines from Adams (2008). Table 4: Accuracy for single utterance disentanglement, averaged over annotations of 800 lines of # LINUX data. In order to use syntactic models like the entity grid, we parse the transcripts using (McClosky et al., 2006). Performance is bad, although the parser does identify most of the NPs; poor results are typical for a standard parser on chat (Foster, 2010). We postprocess the parse trees to retag lol, haha and yes as UH (rather than NN, NNP and JJ). In this section, we use all three of our chatspecic models (sec. 2.0.6; time, speaker and mention) as a baseline. This baseline is relatively strong, so we evaluate our other models in combination with it. 6.1 Disentangling a single sentence As before, we show results on correctly disentangling a single sentence, given the correct structure of the rest of the transcript. We average performance on each transcript over the different annotations, then average the transcripts, weighing them by le"
P11-1118,W98-1119,1,0.516387,"ed for coherence by Soricut and Marcu (2006), although a less probabilistically elegant version was proposed earlier (Lapata, 2003). This model attempts to generate the content words of the next sentence by translating them from the words of the previous sentence, plus a null word; thus, it will learn alignments between pairs of words that tend to occur in adjacent sentences. We learn parameters on the F ISHER corpus, and on NANC for news. 3.4 Pronouns The use of a generative pronoun resolver for coherence modeling originates in Elsner and Charniak (2008a). That paper used a supervised model (Ge et al., 1998), but we adapt a newer, unsupervised model which they also make publicly available (Charniak and Elsner, 2009)4 . They model each pronoun as generated by an antecedent somewhere in the previous two sentences. If a good antecedent is found, the probability of the pronoun's occurrence will be high; otherwise, the probability is low, signaling that the text is less coherent because the pronoun is hard to interpret correctly. We use the model as distributed for news text. For conversation, we adapt it by running a few iterations of their EM training algorithm on the F ISHER data. 4 bllip.cs.brown."
P11-1118,J95-2003,0,0.584922,"riments below, we train the models on SWBD, sometimes augmented with a larger set of automatically parsed conversations from the F ISHER corpus. Since the two corpora are quite similar, F ISHER is a useful source for extra data; McClosky et al. (2010) uses it for this purpose in parsing experiments. (We continue to use SWBD/F ISHER even for experiments on IRC, because we do not have enough disentangled training data to learn lexical relationships.) 3.1 Entity grid The entity grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005) is an attempt to model some principles of Centering Theory (Grosz et al., 1995) in a statistical manner. It represents a document in terms of entities and their syntactic roles: subject (S), object (O), other (X) and not present (-). In each new 2 cs.brown.edu/melsner utterance, the grid predicts the role in which each entity will appear, given its history of roles in the previous sentences, plus a salience feature counting the total number of times the entity occurs. For instance, for an entity which is the subject of sentence 1, the object of sentence 2, and occurs four times in total, the grid predicts its role in sentence 3 according to the conditional P (jS; O; sa"
P11-1118,P04-1050,0,0.0208496,"es probabilities for three outcomes: no name mention, a mention of someone who has previously spoken 1182 The weights  can be learned discriminatively, maximizing the probability of d relative to a taskspecic contrast set. For ordering experiments, the contrast set is a single random permutation of d; we explain the training regime for disentanglement below, in subsection 4.1. 4 Comparing orderings of SWBD To measure the differences in performance caused by moving from news to a conversational domain, we rst compare our models on an ordering task, discrimination (Barzilay and Lapata, 2005; Karamanis et al., 2004). In this task, we take an original document and randomly permute its sentences, creating an articial incoherent document. We then test to see if our model prefers the coherent original. For SWBD, rather than compare permutations of the individual utterances, we permute conversational turns (sets of consecutive utterances by each speaker), since turns are natural discourse units in conversation. We take documents numbered 2000 3999 as training/development and the remainder as test, yielding 505 training and 153 test documents; we evaluate 20 permutations per document. As a comparison, we als"
P11-1118,P03-1069,0,0.139893,"omain exibility. The task of disentanglement or threading for internet chat was introduced by Shen et al. (2006). Elsner and Charniak (2008b) created the publicly available # LINUX corpus; the best published results on this corpus are those of Wang and Oard (2009). These two studies use overlapping unigrams to measure similarity between two sentences; Wang and Oard (2009) use a message expansion technique to incorporate context beyond a single sentence. Unigram overlaps are used to model coherence, but more sophisticated methods using syntax (Lapata and Barzilay, 2005) or lexical features (Lapata, 2003) often outperform them on ordering tasks. This study compares several of these methods with Elsner and Charniak (2008b), which we use as a baseline because there is a publicly available imple1180 mentation2 . Adams (2008) also created and released a disentanglement corpus. They use LDA (Blei et al., 2001) to discover latent topics in their corpus, then measuring similarity by looking for shared topics. These features fail to improve their performance, which is puzzling in light of the success of topic modeling for other coherence and segmentation problems (Eisenstein and Barzilay, 2008; Foltz"
P11-1118,J06-4002,0,0.00561801,"l coherence, which relate text to its immediate context. There has also been work on global coherence, the structure of a document as a whole (Chen et al., 2009; Eisenstein and Barzilay, 2008; Barzilay and Lee, 2004), typically modeled in terms of sequential topics. We avoid using them here, because we do not believe topic sequences are predictable in conversation and because such models tend to be algorithmically cumbersome. In addition to text ordering, local coherence models have also been used to score the uency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate previously unknown weaknesses in models. However, the target genre is still informative writing, so they reveal little about cross-domain exibility. The task of disentanglement or threading for internet chat was introduced by Shen et al. (2006). Elsner and Charniak (2008b) created the publicly available # LINUX corpus; the best published results on this corpus are those of Wang and Oard (2009). These two studies use overlapping unigrams to measu"
P11-1118,N06-1020,1,0.245318,"esting; together, they consist of 19581 lines of chat, with each section containing 500 to 1000 lines. Chat-specic +EGrid +Topical EGrid +IBM-1 +Pronouns +EGrid/Topic/IBM-1 E+C `08b 74.0 79.3 76.8 76.3 73.9 78.3 76.4 +EGrid E+C `08b # IPHONE 92.3 89.0 # PHYSICS 96.6 90.2 # PYTHON 91.1 88.4 Table 5: Average accuracy for disentanglement of a single utterance for 19581 total lines from Adams (2008). Table 4: Accuracy for single utterance disentanglement, averaged over annotations of 800 lines of # LINUX data. In order to use syntactic models like the entity grid, we parse the transcripts using (McClosky et al., 2006). Performance is bad, although the parser does identify most of the NPs; poor results are typical for a standard parser on chat (Foster, 2010). We postprocess the parse trees to retag lol, haha and yes as UH (rather than NN, NNP and JJ). In this section, we use all three of our chatspecic models (sec. 2.0.6; time, speaker and mention) as a baseline. This baseline is relatively strong, so we evaluate our other models in combination with it. 6.1 Disentangling a single sentence As before, we show results on correctly disentangling a single sentence, given the correct structure of the rest"
P11-1118,N03-2024,0,0.0151163,"Elsner, 2009)4 . They model each pronoun as generated by an antecedent somewhere in the previous two sentences. If a good antecedent is found, the probability of the pronoun's occurrence will be high; otherwise, the probability is low, signaling that the text is less coherent because the pronoun is hard to interpret correctly. We use the model as distributed for news text. For conversation, we adapt it by running a few iterations of their EM training algorithm on the F ISHER data. 4 bllip.cs.brown.edu/resources.shtml #software 3 1181 3.5 Discourse-newness Building on work from summarization (Nenkova and McKeown, 2003) and coreference resolution (Poesio et al., 2005), Elsner and Charniak (2008a) use a model which recognizes discourse-new versus old NPs as a coherence model. For instance, the model can learn that President Barack Obama is a more likely rst reference than Obama. Following their work, we score discourse-newness with a maximum-entropy classier using syntactic features counting different types of NP modiers, and we use NP head identity as a proxy for coreference. in the conversation, or a mention of someone else. (The third option is extremely rare; this accounts for most of the model's p"
P11-1118,W06-1612,0,0.0201703,"single model, showing the information provided by the weaker models is not completely redundant. Overall, these results suggest that most previously proposed local coherence models are domaingeneral; they work on conversation as well as news. The exception is the discourse-newness model, which benets most from the specic conventions of a written style. Full names with titles (like President Barack Obama) are more common in news, while conversation tends to involve fewer completely unfamiliar entities and more cases of bridging reference, in which grounding information is given implicitly (Nissim, 2006). Due to its poor performance, we omit the discourse-newness model in our remaining experiments. 5 Disentangling SWBD We now turn to the task of disentanglement, testing whether models that are good at ordering also do well in this new setting. We would like to hold the domain constant, but we do not have any disentanglement data recorded from naturally occurring speech, so we create synthetic instances by merging pairs of SWBD dialogues. Doing so creates an articial transcript in which two pairs of people appear to be talking simultaneously over a shared channel. The situation is somewhat co"
P11-1118,D08-1020,0,0.0317348,"focuses on models of local coherence, which relate text to its immediate context. There has also been work on global coherence, the structure of a document as a whole (Chen et al., 2009; Eisenstein and Barzilay, 2008; Barzilay and Lee, 2004), typically modeled in terms of sequential topics. We avoid using them here, because we do not believe topic sequences are predictable in conversation and because such models tend to be algorithmically cumbersome. In addition to text ordering, local coherence models have also been used to score the uency of texts written by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate previously unknown weaknesses in models. However, the target genre is still informative writing, so they reveal little about cross-domain exibility. The task of disentanglement or threading for internet chat was introduced by Shen et al. (2006). Elsner and Charniak (2008b) created the publicly available # LINUX corpus; the best published results on this corpus are those of Wang and Oard (2009). These two studies use overlapping uni"
P11-1118,P06-2103,0,0.649207,"f the previous sentence. These features can detect a transition like: The House voted yesterday. The Senate will consider the bill today.. If House and Senate have a high similarity, then the feature will have a high value, predicting that Senate is a good subject for the current sentence. As in the previous section, we learn the conditional probabilities with logistic regression; we train in parallel by splitting the data and averaging (Mann et al., 2009). The topics are trained on F ISHER, and on NANC for news. 3.3 IBM-1 The IBM translation model was rst considered for coherence by Soricut and Marcu (2006), although a less probabilistically elegant version was proposed earlier (Lapata, 2003). This model attempts to generate the content words of the next sentence by translating them from the words of the previous sentence, plus a null word; thus, it will learn alignments between pairs of words that tend to occur in adjacent sentences. We learn parameters on the F ISHER corpus, and on NANC for news. 3.4 Pronouns The use of a generative pronoun resolver for coherence modeling originates in Elsner and Charniak (2008a). That paper used a supervised model (Ge et al., 1998), but we adapt a newer, unsu"
P11-1118,N09-1023,0,0.061662,"by humans or produced by machine (Pitler and Nenkova, 2008; Lapata, 2006; Miltsakaki and Kukich, 2004). Like disentanglement, these tasks provide an algorithmic setting that differs from ordering, and so can demonstrate previously unknown weaknesses in models. However, the target genre is still informative writing, so they reveal little about cross-domain exibility. The task of disentanglement or threading for internet chat was introduced by Shen et al. (2006). Elsner and Charniak (2008b) created the publicly available # LINUX corpus; the best published results on this corpus are those of Wang and Oard (2009). These two studies use overlapping unigrams to measure similarity between two sentences; Wang and Oard (2009) use a message expansion technique to incorporate context beyond a single sentence. Unigram overlaps are used to model coherence, but more sophisticated methods using syntax (Lapata and Barzilay, 2005) or lexical features (Lapata, 2003) often outperform them on ordering tasks. This study compares several of these methods with Elsner and Charniak (2008b), which we use as a baseline because there is a publicly available imple1180 mentation2 . Adams (2008) also created and released a dise"
P11-1118,N10-1004,1,\N,Missing
P11-2022,J95-2003,0,0.975848,"Missing"
P11-2022,N10-1061,0,0.0124322,"entation is available via https:// bitbucket.org/melsner/browncoherence. (and what syntactic role it will have) given its history of occurrences in the previous sentences. For instance, it estimates the probability that Clinton will be the subject of sentence 2, given that it was the subject of sentence 1. The standard grid model uses no information about the entity itself the probability is the same whether the entity under discussion is Hillary Clinton or wheat. Plainly, this assumption is too strong. Distinguishing important from unimportant entity types is important in coreference (Haghighi and Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lap"
P11-2022,P05-1018,0,0.687763,"ce 1. The standard grid model uses no information about the entity itself the probability is the same whether the entity under discussion is Hillary Clinton or wheat. Plainly, this assumption is too strong. Distinguishing important from unimportant entity types is important in coreference (Haghighi and Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the ent"
P11-2022,J08-1001,0,0.766763,"and Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but nd no signicant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125–129, c Portland, Oregon, June 19-24, 2011"
P11-2022,N10-1099,0,0.16164,"Missing"
P11-2022,E09-1018,1,0.418722,"target document itself. This avoids the problem that coreference resolvers do not work well for disordered or automatically produced text such as multidocument summary sentences, and also avoids the computational cost associated with coreference resolution. Linkable Was the head word of the entity ever marked as coreferring in MUC6? Unlinkable Did the head word of the entity occur 5 times in MUC6 and never corefer? Has pronouns Were there 5 or more pronouns coreferent with the head word of the entity in the NANC corpus? (Pronouns in NANC are automatically resolved using an unsupervised model (Charniak and Elsner, 2009).) No pronouns Did the head word of the entity occur over 50 times in NANC, and have fewer than 5 coreferent pronouns? To learn probabilities based on these features, we model the conditional probability p(r jF ) using multilabel logistic regression. Our model has a parameter for each combination of syntactic role r , entity-specic feature h and feature vector F : r  h  F . This allows the old and new features to interact while keeping the parameter space tractable7 . In Table 2, we examine the changes in our estimated probability in one particular context: an entity with salience 3 which a"
P11-2022,P05-1022,1,0.353675,"this often fails to improve results; in Barzilay and Lapata (2005), coreference improves results in only one of their target domains, and actually hurts for readability prediction. Their results, moreover, rely on running coreference on the document in its original order; in a summarization task, the correct order is not known, which will cause even more resolver errors. To build a model based on the grid, we treat the columns (entities) as independent, and look at local transitions between sentences. We model the i;j 2 Roles are determined heuristically using trees produced by the parser of (Charniak and Johnson, 2005). 126 ;j i ;j i;j ;f light 4 Experimental design We test our model on two experimental tasks, both testing its ability to distinguish between correct and incorrect orderings for WSJ articles. In document discrimination (Barzilay and Lapata, 2005), we compare a document to a random permutation of its sentences, scoring the system correct if it prefers the original ordering4 . We also evaluate on the more difcult task of sentence insertion (Chen et al., 2007; Elsner and Charniak, 2008). In this task, we remove each sentence from the article and test whether the model prefers to re-insert it at"
P11-2022,D07-1009,0,0.0177826,"ocal transitions between sentences. We model the i;j 2 Roles are determined heuristically using trees produced by the parser of (Charniak and Johnson, 2005). 126 ;j i ;j i;j ;f light 4 Experimental design We test our model on two experimental tasks, both testing its ability to distinguish between correct and incorrect orderings for WSJ articles. In document discrimination (Barzilay and Lapata, 2005), we compare a document to a random permutation of its sentences, scoring the system correct if it prefers the original ordering4 . We also evaluate on the more difcult task of sentence insertion (Chen et al., 2007; Elsner and Charniak, 2008). In this task, we remove each sentence from the article and test whether the model prefers to re-insert it at its original location. We report the average proportion of correct insertions per document. As in Elsner and Charniak (2008), we test on sections 14-24 of the Penn Treebank, for 1004 test documents. We test signicance using the Wilcoxon Sign-rank test, which detects signicant differences in the medians of two distributions5 . 5 Mention detection Our main contribution is to extend the entity grid by adding a large number of entity-specic features. Before"
P11-2022,P10-1020,0,0.0674261,"ity is mentioned in the document. We denote this feature vector F . For example, the vector for ight after the last sentence of the example would be F3 = hX; S; sal = 2i. Using two sentences of context and capping salience at 4, there are only 64 possible vectors, so we can learn an independent multinomial distribution for each F . However, the number of vectors grows exponentially as we add features. i;j i laredo X Figure 1: A short text (using NP-only mention detection), and its corresponding entity grid. The numeric token 1300 is removed in preprocessing. ment over the original model. Cheung and Penn (2010) adapt the grid to German, where focused constituents are indicated by sentence position rather than syntactic role. The best entity grid for English text, however, is still the original. 3 Entity grids The entity grid represents a document as a matrix (Figure 1) with a row for each sentence and a column for each entity. The entry for (sentence i, entity j ), which we write r , represents the syntactic role that entity takes on in that sentence: subject (S), object (O), or some other role (X)2 . In addition, there is a special marker (-) for entities which do not appear at all in a given sente"
P11-2022,P08-2011,1,0.760466,"tistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but nd no signicant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125–129, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 1 [Visual meteorological conditions]S prevailed for [the personal cross country ight for which [a VFR ight plan]O was led]X ."
P11-2022,P10-2007,1,0.882727,"Missing"
P11-2022,W07-2321,0,0.101782,"tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but nd no signicant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125–129, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 1 [Visual meteorological conditions]S prevailed for [the personal cross country ight for which [a VFR ight plan]O was led]X . 2 [The ight]S originated at [Nuevo Laredo , Mexico]X , at [approximately 1300]X . s 1 2 conditions S - plan O - flight X S transitions using the generative approach given in Lapata and Barzilay (2005)3"
P11-2022,P03-1069,0,0.080079,"Missing"
P11-2022,P10-1158,0,0.0282456,"tant from unimportant entity types is important in coreference (Haghighi and Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but nd no signicant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational Lin"
P11-2022,H05-1031,0,0.0190441,"org/melsner/browncoherence. (and what syntactic role it will have) given its history of occurrences in the previous sentences. For instance, it estimates the probability that Clinton will be the subject of sentence 2, given that it was the subject of sentence 1. The standard grid model uses no information about the entity itself the probability is the same whether the entity under discussion is Hillary Clinton or wheat. Plainly, this assumption is too strong. Distinguishing important from unimportant entity types is important in coreference (Haghighi and Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et"
P11-2022,P10-1056,0,0.0216028,"oreference (Haghighi and Klein, 2010) and summarization (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding information from syntax, a named-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but nd no signicant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125–129, c Portla"
P11-2022,P06-2103,0,0.170713,"med-entity tagger and statistics from an external coreference corpus. 2 Related work Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010). It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models. There have been few attempts to improve the entity grid directly by altering its feature representation. Filippova and Strube (2007) incorporate semantic relatedness, but nd no signicant improve125 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125–129, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 1 [Visual meteorological conditions]S prevailed for [the personal cross country ight for which [a VF"
P12-1020,E09-3001,0,0.0289407,"the speech stream, but they are tested on an artificial corpus with only 80 vocabulary items that was constructed so as to “avoid strong word-to-word dependencies” (R¨as¨anen, 2011). Here, we use a naturalistic corpus, demonstrating that lexical-phonetic learning is possible in this more general setting and that word-level context information is important for doing so. Several other related systems work directly from the acoustic signal and many of these do use naturalistic corpora. However, they do not learn at both the lexical and phonetic/acoustic level. For example, Park and Glass (2008), Aimetti (2009), Jansen et al. (2010), and McInnes and Goldwater (2011) present lexicon-building systems that use hard-coded acoustic similarity measures rather than learning about variability, and they only extract and cluster a few frequent words. On the phonetic side, Varadarajan et al. (2008) and Dupoux et al. (2011) describe systems that learn phone-like units but without the benefit of top-down information. A final line of related work is on word segmentation. In addition to the models mentioned in Section 1, which use phonemic input, a few models of word segmentation have been tested using phonetic in"
P12-1020,W11-0601,0,0.0951404,"Missing"
P12-1020,D08-1113,0,0.214064,"Missing"
P12-1020,P08-1016,0,0.0608466,"sen et al. (2010), and McInnes and Goldwater (2011) present lexicon-building systems that use hard-coded acoustic similarity measures rather than learning about variability, and they only extract and cluster a few frequent words. On the phonetic side, Varadarajan et al. (2008) and Dupoux et al. (2011) describe systems that learn phone-like units but without the benefit of top-down information. A final line of related work is on word segmentation. In addition to the models mentioned in Section 1, which use phonemic input, a few models of word segmentation have been tested using phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2010). However, they do not cluster segmented Figure 2: Our generative model of the surface tokens s from intended tokens x, which occur with left and right contexts l and r. word tokens into lexical items (none of these models even maintains an explicit lexicon), nor do they model or learn from phonetic variation in the input. 3 Lexical-phonetic model Our lexical-phonetic model is defined using the standard noisy channel framework: first a sequence of intended word tokens is generated using a language model, and then each token is transformed by a pr"
P12-1020,N06-1041,0,0.0331693,"Missing"
P12-1020,H94-1050,0,0.0377277,"int probability (l, x, r) as p(x)p(l|x)p(r|x) rather than as a leftto-right chain p(l)p(x|l)p(r|x). Given our independence assumption above, these two quantities are mathematically equivalent, so the difference matters only because we are using smoothed estimates. Our factorization leads to a symmetric treatment of left and right contexts, which simplifies implementation: we can store all the context parameters locally as PL (·|x) rather than distributed over various P (x|·). Next, we explain our transducer T . A weighted finite-state transducer (WFST) is a variant of a finitestate automaton (Pereira et al., 1994) that reads an input string symbol-by-symbol and probabilistically produces an output string; thus it can be used to specify a conditional probability on output strings given an input. Our WFST (Figure 3) computes a weighted edit distance, and is implemented using OpenFST (Allauzen et al., 2007). It contains a state for each triplet of (previous, current, next) phones; conditioned on this state, it emits a character output which can be thought of as a possible surface realization of current in its particular environment. The output can be the empty string , in which case current is deleted. T"
P12-1020,W10-2902,0,0.0286415,"nd five manner values (stop, nasal stop, fricative, vowel, other). Empty segments like  and • are assigned a special value “no-value” for all features. 187 Figure 4: Some features generated for (•, D, i) → d. Each black factor node corresponds to a positional template. The features instantiated for the (curr)→out and →out template are shown in full, and we show some of the features for the (curr,next)→out template. tic Optimality Theory models (Goldwater and Johnson, 2003; Hayes and Wilson, 2008). 4 Inference Global optimization of the model posterior is difficult; instead we use Viterbi EM (Spitkovsky et al., 2010; Allahverdyan and Galstyan, 2011). We begin with a simple initial transducer and alternate between two phases: clustering together surface forms, and reestimating the transducer parameters. We iterate this procedure until convergence (when successive clustering phases find nearly the same set of merges); this tends to take about 5 or 6 iterations. In our clustering phase, we improve the model posterior as much as possible by greedily making type merges, where, for a pair of intended word forms u and v, we replace all instances of xi = u with xi = v. We maintain the invariant that each intende"
P12-1020,P06-1124,0,0.323419,"x1 . . . xn . As shown in Figure 2, si is produced from xi by a transducer T : si ∼ T (xi ), which models phonetic changes. Each xi is sampled from a distribution θ which represents word frequencies, and its left and right context words, li and ri , are drawn from distributions conditioned on xi , in order to capture information about the environments in which xi appears: li ∼ PL (xi ), ri ∼ PR (xi ). Because the number of word types is not known in advance, θ is drawn from a Dirichlet process DP (α), and PL (x) and PR (x) have Pitman-Yor priors with concentration parameter 0 and discount d (Teh, 2006). 186 Our generative model of xi is unusual for two reasons. First, we treat each xi independently rather than linking them via a Markov chain. This makes the model deficient, since li overlaps with xi−1 and so forth, generating each token twice. During inference, however, we will never compute the joint probability of all the data at once, only the probabilities of subsets of the variables with particular intended word forms u and v. As long as no two of these words are adjacent, the deficiency will have no effect. We make this independence assumption for computational reasons—when deciding w"
P12-1020,P08-2042,0,0.121566,"ible in this more general setting and that word-level context information is important for doing so. Several other related systems work directly from the acoustic signal and many of these do use naturalistic corpora. However, they do not learn at both the lexical and phonetic/acoustic level. For example, Park and Glass (2008), Aimetti (2009), Jansen et al. (2010), and McInnes and Goldwater (2011) present lexicon-building systems that use hard-coded acoustic similarity measures rather than learning about variability, and they only extract and cluster a few frequent words. On the phonetic side, Varadarajan et al. (2008) and Dupoux et al. (2011) describe systems that learn phone-like units but without the benefit of top-down information. A final line of related work is on word segmentation. In addition to the models mentioned in Section 1, which use phonemic input, a few models of word segmentation have been tested using phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2010). However, they do not cluster segmented Figure 2: Our generative model of the surface tokens s from intended tokens x, which occur with left and right contexts l and r. word tokens into lexical items (none of these mo"
P12-1020,J01-3002,0,0.178409,"iation (many English vowels are reduced to [@] in unstressed positions) and the context—if the next word is “want”, “you” is a plausible choice. To date, most models of infant language learning have focused on either lexicon-building or phonetic learning in isolation. For example, many models of word segmentation implicitly or explicitly build a lexicon while segmenting the input stream of phonemes into word tokens; in nearly all cases the phonemic input is created from an orthographic transcription using a phonemic dictionary, thus abstracting away from any phonetic variability (Brent, 1999; Venkataraman, 2001; Swingley, 2005; Goldwater et al., 2009, among others). As illustrated in Figure 1, these models attempt to infer line (a) from line (d). However, (d) is an idealization: real speech has variability, and behavioral evidence suggests that infants are still learning about the phonetics and phonology of their language even after beginning to segment words, rather than learning to neutralize 184 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 184–193, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics the variat"
P14-1102,W09-1112,0,0.29184,"ve rather than ditransitive. Computational modeling provides a way to test the computational level of processing (Marr, 1982). That is, given the input (child-directed speech, adult-directed speech, and environmental experiences), it is possible to probe the computational processes that result in the observed output. However, previous computational models of grammar induction (Klein and Manning, 2004), including infant grammar induction (Kwiatkowski et al., 2012), have not addressed filler-gap comprehension.4 The closest work to that presented here is the work on BabySRL (Connor et al., 2008; Connor et al., 2009; Connor et al., 2010). BabySRL is a computational model of semantic role acquistion using a similar set of assumptions to the current work. BabySRL learns weights over ordering constraints (e.g. preverbal, second noun, etc.) to acquire semantic role labelling while still exhibiting 1-1 role bias. However, no analysis has evaluated the abil3 There were two actors in each image to avoid biasing the infants to look at the image with more actors. 4 As one reviewer notes, Joshi et al. (1990) and subsequent work show that filler-gap phenomena can be formally captured by mildly context-sensitive gra"
P14-1102,P10-1101,0,0.0341232,"sitive. Computational modeling provides a way to test the computational level of processing (Marr, 1982). That is, given the input (child-directed speech, adult-directed speech, and environmental experiences), it is possible to probe the computational processes that result in the observed output. However, previous computational models of grammar induction (Klein and Manning, 2004), including infant grammar induction (Kwiatkowski et al., 2012), have not addressed filler-gap comprehension.4 The closest work to that presented here is the work on BabySRL (Connor et al., 2008; Connor et al., 2009; Connor et al., 2010). BabySRL is a computational model of semantic role acquistion using a similar set of assumptions to the current work. BabySRL learns weights over ordering constraints (e.g. preverbal, second noun, etc.) to acquire semantic role labelling while still exhibiting 1-1 role bias. However, no analysis has evaluated the abil3 There were two actors in each image to avoid biasing the infants to look at the image with more actors. 4 As one reviewer notes, Joshi et al. (1990) and subsequent work show that filler-gap phenomena can be formally captured by mildly context-sensitive grammar formalisms; these"
P14-1102,P07-1094,0,0.0436243,"rpus, the symmetric model obtains a worse BIC fit than a model that lacks the non-canonical subject Gaussian. Therefore, if one makes the assumption that imperatives are prosodically-marked for learners (e.g. the learner is the implicit subject), the best model is one that lacks a non-canonical subject.7 The remainder of this paper assumes a symmetric model to demonstrate what happens if such an assumption is not made; for the evaluations described in this paper, the results are similar in either case. This model differs from other non-recursive computational models of grammar induction (e.g. Goldwater and Griffiths, 2007) since it is not based on Hidden Markov Models. Instead, it determines the best ordering for the sentence as a whole. This approach bears some similarity to a Generalized Mallows model (Chen et al., 2009), but the current formulation was chosen due to being independently posited as cognitively plausible (Boersma, 1997). Figure 2 (Right) shows the converged, final state of the model. The model expects the first argument (usually agent) to be assigned preverbally and expects the second (say, patient) to be assigned postverbally; however, there is now a larger chance that the second argument will"
P14-1102,W08-2111,0,0.158099,"ed as merely transitive rather than ditransitive. Computational modeling provides a way to test the computational level of processing (Marr, 1982). That is, given the input (child-directed speech, adult-directed speech, and environmental experiences), it is possible to probe the computational processes that result in the observed output. However, previous computational models of grammar induction (Klein and Manning, 2004), including infant grammar induction (Kwiatkowski et al., 2012), have not addressed filler-gap comprehension.4 The closest work to that presented here is the work on BabySRL (Connor et al., 2008; Connor et al., 2009; Connor et al., 2010). BabySRL is a computational model of semantic role acquistion using a similar set of assumptions to the current work. BabySRL learns weights over ordering constraints (e.g. preverbal, second noun, etc.) to acquire semantic role labelling while still exhibiting 1-1 role bias. However, no analysis has evaluated the abil3 There were two actors in each image to avoid biasing the infants to look at the image with more actors. 4 As one reviewer notes, Joshi et al. (1990) and subsequent work show that filler-gap phenomena can be formally captured by mildly"
P14-1102,P04-1061,0,0.0335767,"en do not seem to generalize beyond two arguments until after at least 31 months of age (Goldberg et al., 2004; Bello, 2012), so a predicate occurring with three nouns would still likely be interpreted as merely transitive rather than ditransitive. Computational modeling provides a way to test the computational level of processing (Marr, 1982). That is, given the input (child-directed speech, adult-directed speech, and environmental experiences), it is possible to probe the computational processes that result in the observed output. However, previous computational models of grammar induction (Klein and Manning, 2004), including infant grammar induction (Kwiatkowski et al., 2012), have not addressed filler-gap comprehension.4 The closest work to that presented here is the work on BabySRL (Connor et al., 2008; Connor et al., 2009; Connor et al., 2010). BabySRL is a computational model of semantic role acquistion using a similar set of assumptions to the current work. BabySRL learns weights over ordering constraints (e.g. preverbal, second noun, etc.) to acquire semantic role labelling while still exhibiting 1-1 role bias. However, no analysis has evaluated the abil3 There were two actors in each image to av"
P14-1102,E12-1024,0,0.0734413,"Missing"
P14-1102,J08-2005,0,0.0108751,"ess confidence in during testing. Convergence (see Figure 2) tends to occur after four iterations but can take up to ten iterations depending on the initial parameters. Since the model is unsupervised, it is trained on a given corpus (e.g. Eve) before being tested on the role annotations of that same corpus. The Eve corpus was used for development purposes,8 and the Adam data was used only for testing. For testing, this study uses the semantic role annotations in the BabySRL corpus. These annotations were obtained by automatically semantic role labelling portions of CHILDES with the system of Punyakanok et al. (2008) before roughly hand-correcting them (Connor et al., 2008). The BabySRL corpus is annotated with 5 different roles, but the model described in this paper only uses 2 roles. Therefore, overall accuracy results (see Table 3) are presented both for the raw BabySRL corpus and for a collapsed BabySRL corpus where all non-agent roles are collapsed into a single role (denoted by a subscript c in all tables). Since children do not generalize above two arguments during the modelled age range (Goldberg et al., 2004; Bello, 2012), the collapsed numbers more closely reflect the performance of a learner at"
P14-1102,P12-1068,0,0.0278759,"this work uses syntactic and semantic roles interchangeably (e.g. subject and agent). µ -1 -1 1 1 σ π 0.5 .999 3 .001 0.5 .999 3 .001 .00001 Table 2: Initial values for the mean (µ), standard deviation (σ), and prior (π) of each Gaussian as well as the skip penalty (Φ) used in this paper. Finally, following the finding by Gertner and Fisher (2012) that children interpret intransitives with conjoined subjects as transitives, this work assumes that semantic roles have a one-to-one correspondence with nouns in a sentence (similarly used as a soft constraint in the semantic role labelling work of Titov and Klementiev, 2012). 4 Model The model represents the preferred locations of semantic roles relative to the verb as distributions over real numbers. This idea is adapted from Boersma (1997) who uses it to learn constraint rankings in optimality theory. In this work, the final (main) verb is placed at position 0; words (and chunks) before the verb are given progressively more negative positions, and words after the verb are given progressively more positive positions (see Table 1). Learner expectations of where an argument will appear relative to the verb are modelled as two-component Gaussian mixtures: one mixtu"
P14-1102,C10-1094,0,\N,Missing
P14-1102,C12-1130,1,\N,Missing
P14-2044,P10-1132,0,0.0174697,". A review and comparison of older systems is provided by Christodoulopoulos et al. (2010), who found that imposing a one-tag-per-word-type constraint to reduce model flexibility tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically (Clark, 2003; Berg-Kirkpatrick et al., 2010). Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters (Abend et al., 2010), or as features in a generative model (Lee et al., 2010; Christodoulopoulos et al., 2011; Sirts and Alum¨ae, 2012), or a representation-learning algorithm (Yatbaz et al., 2012). Several of these systems use a small fixed set of orthographic and/or suffix features, sometimes obtained from an unsupervised morphological segmentation system (Abend et al., 2010; Lee et al., 2010; Christodoulopoulos et al., 2011; Yatbaz et al., 2012). Blunsom and Cohn’s (2011) model learns an n-gram character model over the words in each cluster; we learn a loglinear model, which can incorporate arbitrary features."
P14-2044,W13-3520,0,0.0466473,"Missing"
P14-2044,N10-1083,0,0.135339,"Missing"
P14-2044,P06-3002,0,0.0580063,"Missing"
P14-2044,P11-1087,0,0.129669,"Missing"
P14-2044,N06-1041,0,0.0466985,"Missing"
P14-2044,D13-1169,0,0.0235492,"Missing"
P14-2044,D10-1083,0,0.0183087,"hristodoulopoulos et al. (2010), who found that imposing a one-tag-per-word-type constraint to reduce model flexibility tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically (Clark, 2003; Berg-Kirkpatrick et al., 2010). Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters (Abend et al., 2010), or as features in a generative model (Lee et al., 2010; Christodoulopoulos et al., 2011; Sirts and Alum¨ae, 2012), or a representation-learning algorithm (Yatbaz et al., 2012). Several of these systems use a small fixed set of orthographic and/or suffix features, sometimes obtained from an unsupervised morphological segmentation system (Abend et al., 2010; Lee et al., 2010; Christodoulopoulos et al., 2011; Yatbaz et al., 2012). Blunsom and Cohn’s (2011) model learns an n-gram character model over the words in each cluster; we learn a loglinear model, which can incorporate arbitrary features. Berg-Kirkpatrick et al. (2010) also include a log-linea"
P14-2044,W13-3512,0,0.0246285,"be available for rare words; morphological rules can be applied to any word regardless of how often it appears. These observations suggest that a general approach to the induction of syntactic categories should leverage both distributional and morphological features (Clark, 2003; Christodoulopoulos et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and morphology is complex and nonlinear, so that simple metrics such as edit distance will only weakly approximate morphological similarity. In this paper we present a new approach for inducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model based on the distanc"
P14-2044,N13-1090,0,0.0350707,"of data, which may not be available for rare words; morphological rules can be applied to any word regardless of how often it appears. These observations suggest that a general approach to the induction of syntactic categories should leverage both distributional and morphological features (Clark, 2003; Christodoulopoulos et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and morphology is complex and nonlinear, so that simple metrics such as edit distance will only weakly approximate morphological similarity. In this paper we present a new approach for inducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model"
P14-2044,petrov-etal-2012-universal,0,0.0558409,"Missing"
P14-2044,D10-1056,1,0.856919,"unction, so that words with similar syntactic functions should have similar distributional properties. In contrast, there can be multiple paradigms for a single morphological inflection (such as past tense in English). But accurate computation of distributional similarity requires large amounts of data, which may not be available for rare words; morphological rules can be applied to any word regardless of how often it appears. These observations suggest that a general approach to the induction of syntactic categories should leverage both distributional and morphological features (Clark, 2003; Christodoulopoulos et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and morphology is complex and nonlinear,"
P14-2044,W09-1121,0,0.0417273,"Missing"
P14-2044,D11-1059,1,0.889971,"et al. (2010), who found that imposing a one-tag-per-word-type constraint to reduce model flexibility tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically (Clark, 2003; Berg-Kirkpatrick et al., 2010). Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters (Abend et al., 2010), or as features in a generative model (Lee et al., 2010; Christodoulopoulos et al., 2011; Sirts and Alum¨ae, 2012), or a representation-learning algorithm (Yatbaz et al., 2012). Several of these systems use a small fixed set of orthographic and/or suffix features, sometimes obtained from an unsupervised morphological segmentation system (Abend et al., 2010; Lee et al., 2010; Christodoulopoulos et al., 2011; Yatbaz et al., 2012). Blunsom and Cohn’s (2011) model learns an n-gram character model over the words in each cluster; we learn a loglinear model, which can incorporate arbitrary features. Berg-Kirkpatrick et al. (2010) also include a log-linear model of morphology in POS indu"
P14-2044,D07-1043,0,0.363702,"Missing"
P14-2044,E03-1009,0,0.0278881,"h syntactic function, so that words with similar syntactic functions should have similar distributional properties. In contrast, there can be multiple paradigms for a single morphological inflection (such as past tense in English). But accurate computation of distributional similarity requires large amounts of data, which may not be available for rare words; morphological rules can be applied to any word regardless of how often it appears. These observations suggest that a general approach to the induction of syntactic categories should leverage both distributional and morphological features (Clark, 2003; Christodoulopoulos et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and mor"
P14-2044,P12-1020,1,0.853847,"following j is P (ci = j|X, Θ, w, α) ∝ P (fol(i)|Xj , Θ)P (ci = j|w, α). (8) Our non-sequential ddCRP introduces cycles into the follower structure, which are handled in the sampler as described by Socher et al. (2011). Also, the block of customers being moved around can potentially be very large, which makes it easy for the likelihood term to swamp the prior. In practice we found that introducing an additional parameter a (used to exponentiate the prior) improved results— although we report results without this exponent as well. This technique was also used by Titov and Klementiev (2012) and Elsner et al. (2012). Inference also includes optimizing the feature weights for the log-linear model in the ddCRP prior (Titov and Klementiev, 2012). We interleave L-BFGS optimization within sampling, as in Monte Carlo Expectation-Maximization (Wei and Tanner, 1990). We do not apply the exponentiation parameter a when training the weights because this procedure affects the follower structure only, and we do not have to worry about the magnitude of the likelihood. Before the first iteration we initialize the follower structure: for each word, we choose randomly a word to follow from amongst those with the longest"
P14-2044,erjavec-2004-multext,0,0.126496,"clusters. The probability of word w1 following w2 depends on two factors: 1) the distributional similarity between all words in the proposed partition containing w1 and w2 , which is encoded using a Gaussian likelihood function over the word embeddings; and 2) the morphological similarity between w1 and w2 , which acts as a prior distribution on the induced clustering. We use a log-linear model to capture suffix similarities between words, and learn the feature weights by iterating between sampling and weight learning. We apply our model to the English section of the the Multext-East corpus (Erjavec, 2004) in order to evaluate both against the coarse-grained and 265 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 265–271, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics fine-grained tags, where the fine-grained tags encode detailed morphological classes. We find that our model effectively combines morphological features with distributional similarity, outperforming comparable alternative approaches. 2 Related work Unsupervised POS tagging has a long history in NLP. This paper focuses on the"
P14-2044,N12-1045,1,0.820633,"Missing"
P14-2044,E12-1003,0,0.0244364,", the probability of customer i following j is P (ci = j|X, Θ, w, α) ∝ P (fol(i)|Xj , Θ)P (ci = j|w, α). (8) Our non-sequential ddCRP introduces cycles into the follower structure, which are handled in the sampler as described by Socher et al. (2011). Also, the block of customers being moved around can potentially be very large, which makes it easy for the likelihood term to swamp the prior. In practice we found that introducing an additional parameter a (used to exponentiate the prior) improved results— although we report results without this exponent as well. This technique was also used by Titov and Klementiev (2012) and Elsner et al. (2012). Inference also includes optimizing the feature weights for the log-linear model in the ddCRP prior (Titov and Klementiev, 2012). We interleave L-BFGS optimization within sampling, as in Monte Carlo Expectation-Maximization (Wei and Tanner, 1990). We do not apply the exponentiation parameter a when training the weights because this procedure affects the follower structure only, and we do not have to worry about the magnitude of the likelihood. Before the first iteration we initialize the follower structure: for each word, we choose randomly a word to follow from among"
P14-2044,P10-1040,0,0.00921189,"es can be applied to any word regardless of how often it appears. These observations suggest that a general approach to the induction of syntactic categories should leverage both distributional and morphological features (Clark, 2003; Christodoulopoulos et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and morphology is complex and nonlinear, so that simple metrics such as edit distance will only weakly approximate morphological similarity. In this paper we present a new approach for inducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model based on the distance-dependent Chinese restaurant process (ddCRP; Bl"
P14-2044,D09-1071,0,0.0418626,"Missing"
P14-2044,D12-1086,0,0.019225,"ty tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically (Clark, 2003; Berg-Kirkpatrick et al., 2010). Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters (Abend et al., 2010), or as features in a generative model (Lee et al., 2010; Christodoulopoulos et al., 2011; Sirts and Alum¨ae, 2012), or a representation-learning algorithm (Yatbaz et al., 2012). Several of these systems use a small fixed set of orthographic and/or suffix features, sometimes obtained from an unsupervised morphological segmentation system (Abend et al., 2010; Lee et al., 2010; Christodoulopoulos et al., 2011; Yatbaz et al., 2012). Blunsom and Cohn’s (2011) model learns an n-gram character model over the words in each cluster; we learn a loglinear model, which can incorporate arbitrary features. Berg-Kirkpatrick et al. (2010) also include a log-linear model of morphology in POS induction, but they use morphology in the likelihood term of a parametric sequence model, th"
P16-2010,P12-1005,0,0.118842,"tion: a set of lexical items (along with the contexts in which they are likely to occur), and a set of phonetic categories. For instance, an Englishlearning infant must learn that [i] and [I] are different segments, differentiating between words like beat and bit, while for a Spanish-learning infant, [i] and [I]-like tokens represent realizations of the same category. It is clear that these two tasks are intimately related, and that models of language acquisition must solve both together— but how? This problem has inspired much recent work in low-resource speech recognition (Lee et al., 2015; Lee and Glass, 2012; Jansen and Church, 2011; Varadarajan et al., 2008), with impressive results. Nonetheless, many of these researchers conclude that their systems learn too many phonetic categories, a problem they attribute to the presence of contextual variants (allophones) of the different sounds. For instance, the [a] in dog is likely longer than the [a] in dock (Ladefoged and Johnson, 2010), but this difference is not phonologically meaningful in English— it cannot differentiate any 2 Related work This work aims to induce both a set of phonetic vowel categories and a lexical representation from unlabeled d"
P16-2010,D13-1005,1,0.941768,"nts for English vowels read in the context h d. We estimate a multivariate Gaussian distribution for each vowel, and, whenever a monophthongal vowel occurs in the Brent corpus, we replace it with a pair of formants (f1 , f2 ) drawn from the appropriate Gaussian. The ARPABET diphthongs “oy, aw, ay, em, en”, and all the consonants, retain their discrete values. The first three words of the dataset, orthographically “you want to”, are rendered: y[380.53 1251.69] w[811.88 1431.96]n t[532.91 1094.14]. In the segmentation literature, several previous systems learn lexical items from variable input (Elsner et al., 2013; Daland and Pierrehumbert, 2011; Rytting et al., 2010; Neubig et al., 2010; Fleck, 2008). However, these models use pre-processed representations of the acoustics (phonetic transcription or posterior probabilities from a phone recognizer) rather than inducing an acoustic category structure directly. Elsner et al. (2013) and Neubig et al. (2010) use Bayesian models and sampling schemes similar to those presented here. Acquisition models like Elsner et al. (2013),Rytting et al. (2010) and Fleck (2008) are designed to handle phonological variability. In particular, they are designed to cope with"
P16-2010,Q15-1028,0,0.0359384,"Missing"
P16-2010,P08-1016,0,0.0355859,"for each vowel, and, whenever a monophthongal vowel occurs in the Brent corpus, we replace it with a pair of formants (f1 , f2 ) drawn from the appropriate Gaussian. The ARPABET diphthongs “oy, aw, ay, em, en”, and all the consonants, retain their discrete values. The first three words of the dataset, orthographically “you want to”, are rendered: y[380.53 1251.69] w[811.88 1431.96]n t[532.91 1094.14]. In the segmentation literature, several previous systems learn lexical items from variable input (Elsner et al., 2013; Daland and Pierrehumbert, 2011; Rytting et al., 2010; Neubig et al., 2010; Fleck, 2008). However, these models use pre-processed representations of the acoustics (phonetic transcription or posterior probabilities from a phone recognizer) rather than inducing an acoustic category structure directly. Elsner et al. (2013) and Neubig et al. (2010) use Bayesian models and sampling schemes similar to those presented here. Acquisition models like Elsner et al. (2013),Rytting et al. (2010) and Fleck (2008) are designed to handle phonological variability. In particular, they are designed to cope with words which have multiple transcribed pronunciations ([wan] and [want] for “want”); this"
P16-2010,P09-1012,0,0.0680126,"Missing"
P16-2010,P14-1101,1,0.84971,"nds. For instance, the [a] in dog is likely longer than the [a] in dock (Ladefoged and Johnson, 2010), but this difference is not phonologically meaningful in English— it cannot differentiate any 2 Related work This work aims to induce both a set of phonetic vowel categories and a lexical representation from unlabeled data. It extends the closely related model of Feldman et al. (2013a), which performs the same task, but with known word boundaries; this requirement is a significant limitation on the model’s cognitive plausibility. Our model infers a latent word segmentation. Another extension, Frank et al. (2014), uses semantic information to disambiguate words, but still with known word boundaries. A few models learn a lexicon while categorizing all sounds, instead of just vowels. Lee et al. (2015) and Lee and Glass (2012) use hierarchical Bayesian models to induce word and subword units. These models are mathematically very similar to 59 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 59–65, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics our own, differing primarily using more complex acoustic representations and in"
P16-2010,P06-1085,0,0.0493838,"ing, Sij ∼ N ormal(µXij , ΣXij ) The initial prior over word forms, CV (πv , pc , pstop ) is the following: sample a word length ≥ 1 from Geom(pstop ); for each character in the word, choose to sample a consonant with probability pc or a vowel otherwise; sample all consonants uniformally, and all vowels according to the (possibly-infinite) probability vector πv .1 In practice, we integrate out πv , yielding a Chinese restaurant process in which the distribution over vowels in a new word depend on those used in already-seen words. Vowels which occur in many word types are more likely to recur (Goldwater et al., 2006; Teh et al., 2006). The hyperparameters for the model are α0 and α1 (which control the size of the unigram and bigram vocabularies), αv (which weakly affects the number of vowel categories), µ0 , n, Λ and ν (which affect the average location and dispersion of vowel categories in formant space), and pc and pstop (which weakly affect the length and composition of words). We set α0 and α1 to their optimal values for word segmentation (3000 and 100 (Goldwater et al., 2009)) and αv to .001. In practice, no value of αv we tried would produce a useful number of vowels and so we fix the maximum numbe"
P16-2010,N09-1036,0,0.307698,"e parameters to be influential. These hyperparameter values were mostly taken from previous work. The vowel inverse precision and degrees of freedom differ from those in Feldman et al. (2013a), since our approach requires us to sample from the prior, but the uninformative prior used there was too poor a fit for the data. We chose a variance with units on the order of the overall data variance, but did not tune it. 3.2 Inference We conduct inference by Gibbs sampling, including three sampling moves: block sampling of the analyses of a single utterance, table label relabeling of a lexical item (Johnson and Goldwater, 2009) and resampling of the vowel category parameters µv and Σv . We run 1000 iterations of utterance resampling, with table relabeling every 10 iterations.2 Following previous work, we integrate out the mixing weight distributions G0 , G1 and πv , resulting in Chinese restaurant process distributions for unigrams, bigrams and vowel categories in the lexicon (Teh et al., 2006). Unlike Feldman et al. (2013a) and many other variants of the Infinite Mixture of Gaussians (Rasmussen, 1999), we do not integrate out µv and Σv , since this would create long-distance dependencies between different tokens of"
P16-2010,P08-2042,0,0.0817253,"ontexts in which they are likely to occur), and a set of phonetic categories. For instance, an Englishlearning infant must learn that [i] and [I] are different segments, differentiating between words like beat and bit, while for a Spanish-learning infant, [i] and [I]-like tokens represent realizations of the same category. It is clear that these two tasks are intimately related, and that models of language acquisition must solve both together— but how? This problem has inspired much recent work in low-resource speech recognition (Lee et al., 2015; Lee and Glass, 2012; Jansen and Church, 2011; Varadarajan et al., 2008), with impressive results. Nonetheless, many of these researchers conclude that their systems learn too many phonetic categories, a problem they attribute to the presence of contextual variants (allophones) of the different sounds. For instance, the [a] in dog is likely longer than the [a] in dock (Ladefoged and Johnson, 2010), but this difference is not phonologically meaningful in English— it cannot differentiate any 2 Related work This work aims to induce both a set of phonetic vowel categories and a lexical representation from unlabeled data. It extends the closely related model of Feldman"
W05-1526,C04-1055,1,0.772927,"alogue. We show that our augmented parser produces significantly fewer constituents than the baseline system and achieves comparable bracketing accuracy, even yielding slight improvements for longer sentences. 1 Experiments Introduction Unification parsers have problems with efficiency and selecting the best parse. Lexically-conditioned statistics as used by Collins (1999) may provide a solution. They have been used in three ways: as a postprocess for parse selection (Toutanova et al., 2005; Riezler et al., 2000; Riezler et al., 2002), a preprocess to find more probable bracketing structures (Swift et al., 2004), and online to rank each constituent produced, as in Tsuruoka et al. (2004) and this experiment. The TRIPS parser (Allen et al., 1996) is a unification parser using an HPSG-inspired grammar and hand-tuned weights for each rule. In our augmented system (Aug-TRIPS), we replaced these weights with a lexically-conditioned model based on the adaptation of Collins used by Bikel (2002), allowing more efficiency and (in some cases) better selection. Aug-TRIPS retains the same grammar and lexicon as TRIPS, but uses its statistical model to determine the order in which unifications are attempted. (trai"
W05-1526,W04-0214,1,0.849773,"Missing"
W05-1526,P00-1061,0,\N,Missing
W05-1526,W00-1320,0,\N,Missing
W05-1526,J03-4003,0,\N,Missing
W05-1526,P96-1009,1,\N,Missing
W05-1526,P02-1035,0,\N,Missing
W09-1803,N06-1046,0,0.02312,"with better features, and develop the best-link heuristic, which finds better solutions. McCallum and Wellner (2004) explicitly describe the problem as correlation clustering and use an approximate technique (Bansal et al., 2004) to enforce transitivity. Recently Finkel and Manning (2008) show that the optimal ILP solution outperforms the first and best-link methods. Cohen and Richman (2002) experiment with various heuristic solutions for the cross-document coreference task of grouping references to named entities. Finally, correlation clustering has proven useful in several discourse tasks. Barzilay and Lapata (2006) use it for content aggregation in a generation system. In Malioutov and Barzilay (2006), it is used for topic segmentation—since segments must be contiguous, the problem can be solved in polynomial time. Elsner and Charniak (2008) address the related problem of disentanglement (which we explore in Section 5.3), doing inference with the voting greedy algorithm. Bertolacci and Wirth (2007), Goder and Filkov (2008) and Gionis et al. (2007) conduct experiments on the closely related problem of consensus clustering, often solved by reduction to correlation clustering. The input to this problem is"
W09-1803,P08-1095,1,0.832606,"to enforce transitivity. Recently Finkel and Manning (2008) show that the optimal ILP solution outperforms the first and best-link methods. Cohen and Richman (2002) experiment with various heuristic solutions for the cross-document coreference task of grouping references to named entities. Finally, correlation clustering has proven useful in several discourse tasks. Barzilay and Lapata (2006) use it for content aggregation in a generation system. In Malioutov and Barzilay (2006), it is used for topic segmentation—since segments must be contiguous, the problem can be solved in polynomial time. Elsner and Charniak (2008) address the related problem of disentanglement (which we explore in Section 5.3), doing inference with the voting greedy algorithm. Bertolacci and Wirth (2007), Goder and Filkov (2008) and Gionis et al. (2007) conduct experiments on the closely related problem of consensus clustering, often solved by reduction to correlation clustering. The input to this problem is a set of clusterings; the output is a “median” clustering which minimizes the sum of (Rand) distance to the inputs. Although these papers investigate some of the same algorithms we use, they use an unrealistic lower bound, and so c"
W09-1803,P08-2012,0,0.0331229,"t can use messages about cars to learn a similarity function that can then be applied to messages about atheism. Correlation clustering is a standard method for coreference resolution. It was introduced to the area by Soon et al. (2001), who describe the firstlink heuristic method for solving it. Ng and Cardie (2002) extend this work with better features, and develop the best-link heuristic, which finds better solutions. McCallum and Wellner (2004) explicitly describe the problem as correlation clustering and use an approximate technique (Bansal et al., 2004) to enforce transitivity. Recently Finkel and Manning (2008) show that the optimal ILP solution outperforms the first and best-link methods. Cohen and Richman (2002) experiment with various heuristic solutions for the cross-document coreference task of grouping references to named entities. Finally, correlation clustering has proven useful in several discourse tasks. Barzilay and Lapata (2006) use it for content aggregation in a generation system. In Malioutov and Barzilay (2006), it is used for topic segmentation—since segments must be contiguous, the problem can be solved in polynomial time. Elsner and Charniak (2008) address the related problem of d"
W09-1803,P06-1004,0,0.0128736,"ons. McCallum and Wellner (2004) explicitly describe the problem as correlation clustering and use an approximate technique (Bansal et al., 2004) to enforce transitivity. Recently Finkel and Manning (2008) show that the optimal ILP solution outperforms the first and best-link methods. Cohen and Richman (2002) experiment with various heuristic solutions for the cross-document coreference task of grouping references to named entities. Finally, correlation clustering has proven useful in several discourse tasks. Barzilay and Lapata (2006) use it for content aggregation in a generation system. In Malioutov and Barzilay (2006), it is used for topic segmentation—since segments must be contiguous, the problem can be solved in polynomial time. Elsner and Charniak (2008) address the related problem of disentanglement (which we explore in Section 5.3), doing inference with the voting greedy algorithm. Bertolacci and Wirth (2007), Goder and Filkov (2008) and Gionis et al. (2007) conduct experiments on the closely related problem of consensus clustering, often solved by reduction to correlation clustering. The input to this problem is a set of clusterings; the output is a “median” clustering which minimizes the sum of (Ra"
W09-1803,P02-1014,0,0.01167,"rocessing, pages 19–27, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics tering methods, it can use training data to optimize the pairwise classifier, but unlike classification, it does not require samples from the specific clusters found in the test data. For instance, it can use messages about cars to learn a similarity function that can then be applied to messages about atheism. Correlation clustering is a standard method for coreference resolution. It was introduced to the area by Soon et al. (2001), who describe the firstlink heuristic method for solving it. Ng and Cardie (2002) extend this work with better features, and develop the best-link heuristic, which finds better solutions. McCallum and Wellner (2004) explicitly describe the problem as correlation clustering and use an approximate technique (Bansal et al., 2004) to enforce transitivity. Recently Finkel and Manning (2008) show that the optimal ILP solution outperforms the first and best-link methods. Cohen and Richman (2002) experiment with various heuristic solutions for the cross-document coreference task of grouping references to named entities. Finally, correlation clustering has proven useful in several"
W09-1803,J01-4004,0,0.036486,"s of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 19–27, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics tering methods, it can use training data to optimize the pairwise classifier, but unlike classification, it does not require samples from the specific clusters found in the test data. For instance, it can use messages about cars to learn a similarity function that can then be applied to messages about atheism. Correlation clustering is a standard method for coreference resolution. It was introduced to the area by Soon et al. (2001), who describe the firstlink heuristic method for solving it. Ng and Cardie (2002) extend this work with better features, and develop the best-link heuristic, which finds better solutions. McCallum and Wellner (2004) explicitly describe the problem as correlation clustering and use an approximate technique (Bansal et al., 2004) to enforce transitivity. Recently Finkel and Manning (2008) show that the optimal ILP solution outperforms the first and best-link methods. Cohen and Richman (2002) experiment with various heuristic solutions for the cross-document coreference task of grouping reference"
W09-1803,N09-1023,0,0.221942,"Missing"
W11-1607,J05-3002,0,0.746752,"ld contribute to systems for automatic editing. In the remainder of the paper, we first give an overview of related work (Section 2). We next describe our dataset and preprocessing in more detail (Section 3), describe the optimization we perform (Section 4), and explain how we learn parameters for it (Section 5). Finally, we discuss our experimental evaluation and give results (Section 6). 2 Related work Previous work on sentence fusion examines the task in the context of multidocument summarization, targeting groups of sentences with mostly redundant content. The pioneering work on fusion is Barzilay and McKeown (2005), which introduces the framework used by subsequent projects: they represent the inputs by dependency trees, align some words to merge the input trees into a lattice, and then extract a single, connected dependency tree as the output. Our work most closely follows Filippova and Strube (2008), which proposes using Integer Linear Programming (ILP) for extraction of an output dependency tree. ILP allows specification of grammaticality constraints in terms of dependency relationships (Clarke and Lapata, 2008), as opposed to previous fusion methods (Barzilay and McKeown, 2005; Marsi and Krahmer, 20"
W11-1607,W02-1001,0,0.0126631,"use errors. We must also introduce conjunctions between arguments of the same syntactic type; our system always inserts “and”. Finally, we choose a realization for the dummy relative pronoun THAT using a trigram language model (Stolcke, 2002). A more sophisticated approach (Filippova and Strube, 2009) might lead to better results. 5 Learning The solution which the system finds depends on the weights v which we provide for each dependency, word and merger. We set the weights based on a dot product of features φ and parameters α, which we learn from data using a supervised structured technique (Collins, 2002). To do so, we define a loss function L(s, s′ ) → R which measures how poor solution s is when the true solution is s′ . For each of our training examples, we compute the oracle solution, the best solution accessible to our system, by minimizing the loss. Finally, we use the structured averaged perceptron update rule to push our system’s parameters away from bad solutions and towards the oracle solutions for each example. Our loss function is designed to measure the highlevel similarity between two dependency trees containing some aligned regions. (For our system, these are the regions found b"
W11-1607,W04-1016,0,0.476309,"Missing"
W11-1607,D08-1019,0,0.26448,"edited versions. We search this corpus for sentences which were fused (or separated) by the editor; these constitute naturally occurring data for our system. One example from our dataset consists of input sentences (1) and (2) and output (3). We show corresponding regions of the input and output in boldface. We present a system for fusing sentences which are drawn from the same source document but have different content. Unlike previous work, our approach is supervised, training on real-world examples of sentences fused by professional journalists in the process of editing news articles. Like Filippova and Strube (2008), our system merges dependency graphs using Integer Linear Programming. However, instead of aligning the inputs as a preprocess, we integrate the tasks of finding an alignment and selecting a merged sentence into a joint optimization problem, and learn parameters for this optimization using a structured online algorithm. Evaluation by human judges shows that our technique produces fused sentences that are both informative and readable. (1) The bodies showed signs of torture. (2) They were left on the side of a highway in Chilpancingo, about an hour north of the tourist resort of Acapulco in th"
W11-1607,N09-2057,0,0.0119799,"ordering mostly according to the original word order of the input. In the case of a merged node, however, we must also interleave modifiers of the merged heads, which are not ordered with respect to one another. We use a simple heuristic, trying to place dependencies with the same arc label next to one another; this can cause errors. We must also introduce conjunctions between arguments of the same syntactic type; our system always inserts “and”. Finally, we choose a realization for the dummy relative pronoun THAT using a trigram language model (Stolcke, 2002). A more sophisticated approach (Filippova and Strube, 2009) might lead to better results. 5 Learning The solution which the system finds depends on the weights v which we provide for each dependency, word and merger. We set the weights based on a dot product of features φ and parameters α, which we learn from data using a supervised structured technique (Collins, 2002). To do so, we define a loss function L(s, s′ ) → R which measures how poor solution s is when the true solution is s′ . For each of our training examples, we compute the oracle solution, the best solution accessible to our system, by minimizing the loss. Finally, we use the structured a"
W11-1607,P08-2049,0,0.162689,"Missing"
W11-1607,W05-1612,0,0.557769,"ay and McKeown (2005), which introduces the framework used by subsequent projects: they represent the inputs by dependency trees, align some words to merge the input trees into a lattice, and then extract a single, connected dependency tree as the output. Our work most closely follows Filippova and Strube (2008), which proposes using Integer Linear Programming (ILP) for extraction of an output dependency tree. ILP allows specification of grammaticality constraints in terms of dependency relationships (Clarke and Lapata, 2008), as opposed to previous fusion methods (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005) which used language modeling to extract their output. In their ILP, Filippova and Strube (2008) optimize a function based on syntactic importance scores learned from a corpus of general text. While similar methods have been used for the related task of sentence compression, improvements can be obtained using supervised learning (Knight and Marcu, 2000; Turner and Charniak, 2005; Cohn and Lapata, 2009) if a suitable corpus of compressed sentences can be 55 obtained. This paper is the first we know of to adopt the supervised strategy for sentence fusion. For supervised learning to be effective,"
W11-1607,N06-1020,0,0.0221357,"e the editor’s decision to split the sentences probably means the fused version is too long, but is required in this small dataset to avoid sparsity. Out of a total of 9007 sentences in the corpus, our bigram method finds that 175 were split and 132 were merged, for a total of 307. We take 92 examples for testing and 189 for training3 . Following previous work (Barzilay and McKeown, 2005), we adopt a labeled dependency format for our system’s input. To produce this, we segment sentences with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse the corpus with the selftrained Charniak parser (McClosky et al., 2006). We then convert to dependencies and apply rules to simplify and label the graph. An example dependency graph is shown in Figure 1. We augment the dependency tree by adding a potential dependency labeled “relative clause” between each subject and its verb. This allows our system to transform main clauses, like “the bodies showed signs of torture”, into NPs like “the bodies, which showed signs of torture”, a common paraphrase strategy in our dataset. We also add correspondences between the two sentences to the graph, marking nodes which the system might decide to merge while fusing the two sen"
W11-1607,N10-1044,0,0.337963,"le similar methods have been used for the related task of sentence compression, improvements can be obtained using supervised learning (Knight and Marcu, 2000; Turner and Charniak, 2005; Cohn and Lapata, 2009) if a suitable corpus of compressed sentences can be 55 obtained. This paper is the first we know of to adopt the supervised strategy for sentence fusion. For supervised learning to be effective, it is necessary to find or produce example data. Previous work does produce some examples written by humans, though these are used during evaluation, not for learning (a large corpus of fusions (McKeown et al., 2010) was recently compiled as a first step toward a supervised fusion system). However, they elicit these examples by asking experimental subjects to fuse selected input sentences– the choice of which sentences to fuse is made by the system, not the subjects. In contrast, our dataset consists of sentences humans actually chose to fuse as part of a practical writing task. Moreover, our sentences have disparate content, while previous work focuses on sentences whose content mostly overlaps. Input sentences with differing content present a challenge to the models used in previous work. All these mode"
W11-1607,D09-1041,0,0.0413056,"uation. We compare sentences produced by our system to three alternatives: the editor’s fused sentence, a readability upper-bound and a baseline formed by splicing the input sentences together by inserting the word “and” between each one. The readability upper bound is the output of parsing and linearization on the editor’s original sentence (Filippova and Strube, 2008); it is designed to measure the loss in grammaticality due to our preprocessing. Native English speakers rated the fused sentences with respect to readability and content on a scale of 1 to 5 (we give a scoring rubric based on (Nomoto, 2009)). 12 judges participated in the study, for a total of 1062 evaluations7 . Each judge saw the each pair of inputs with the retained regions boldfaced, plus a single fusion drawn randomly from among the four systems. Results are displayed in Table 2. System Editor Readability UB “And”-splice Our System Readability 4.55 3.97 3.65 3.12 Content 4.56 4.27 3.80 3.83 “And”-splice System 1 3 24 2 43 24 3 60 39 4 57 58 5 103 115 Total 266 260 Table 3: Number of times each Content score was assigned by human judges. by that much. It performs quite well on some relatively hard sentences and gets easy fus"
W11-1607,N04-3012,0,0.0161205,"f all coreferent NPs. The example sentence has only a single correspondence arc (“they” and “bodies”) be2 In a few cases, this creates two examples which share a sentence, since the editor sometimes splits content off from one sentence and merges it into another. 3 We originally had 100 testing and 207 training examples, but found 26 of our examples were spurious, caused by faulty sentence segmentation. 4 Words with the same part of speech whose similarity is greater than 3.0 according to the information-theoretic WordNet based similarity measure of Resnik (1995), using the implementation of (Pedersen et al., 2004). 56 cause input sentence (1) is extremely short, but most sentences have more. rel bodies showed sbj obj root signs pp of torture side merge? pp of highway pp in chilpancingo pp by pp about sbj they rel north left aux hour an state police were pp of pp of resort acapulco the rel sbj obj said root Figure 1: The labeled dependency graph for sentences (1) and (2). Dashed lines show a correspondence arc (“bodies” and “they”) and potential relative clauses between subjects and VPs. 3.1 Retained information Sentence fusion can be thought of as a two-part process: first, the editor decides which inf"
W11-1607,A97-1004,0,0.0939008,"our method and attempt to produce the original through fusion2 . This is suboptimal, since the editor’s decision to split the sentences probably means the fused version is too long, but is required in this small dataset to avoid sparsity. Out of a total of 9007 sentences in the corpus, our bigram method finds that 175 were split and 132 were merged, for a total of 307. We take 92 examples for testing and 189 for training3 . Following previous work (Barzilay and McKeown, 2005), we adopt a labeled dependency format for our system’s input. To produce this, we segment sentences with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse the corpus with the selftrained Charniak parser (McClosky et al., 2006). We then convert to dependencies and apply rules to simplify and label the graph. An example dependency graph is shown in Figure 1. We augment the dependency tree by adding a potential dependency labeled “relative clause” between each subject and its verb. This allows our system to transform main clauses, like “the bodies showed signs of torture”, into NPs like “the bodies, which showed signs of torture”, a common paraphrase strategy in our dataset. We also add correspondences between the two sentences to the gr"
W11-1607,P05-1036,0,0.0425088,"ion of an output dependency tree. ILP allows specification of grammaticality constraints in terms of dependency relationships (Clarke and Lapata, 2008), as opposed to previous fusion methods (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005) which used language modeling to extract their output. In their ILP, Filippova and Strube (2008) optimize a function based on syntactic importance scores learned from a corpus of general text. While similar methods have been used for the related task of sentence compression, improvements can be obtained using supervised learning (Knight and Marcu, 2000; Turner and Charniak, 2005; Cohn and Lapata, 2009) if a suitable corpus of compressed sentences can be 55 obtained. This paper is the first we know of to adopt the supervised strategy for sentence fusion. For supervised learning to be effective, it is necessary to find or produce example data. Previous work does produce some examples written by humans, though these are used during evaluation, not for learning (a large corpus of fusions (McKeown et al., 2010) was recently compiled as a first step toward a supervised fusion system). However, they elicit these examples by asking experimental subjects to fuse selected inpu"
W16-2120,P98-1022,0,0.208123,"s (Cohn et al., 2009). Each fragment has a root symbol (analogous to the left-hand-side category in a CFG) and a frontier which can consist of terminals (words) and non-terminal symbols to be filled in later in the derivation. An example tree fragment is shown in figure 1; this fragment describes a particular complement clause structure which can be interpreted as the construction “that X is Y”. A single treebank tree may have multiple TSG derivations (depending on how it is split up into constructional fragments), so TSGs must be induced from the data. The Data-oriented Parsing (DOP) method (Bod and Kaplan, 1998) was criticized by Johnson (2002) for its poor estimation procedure. Newer methods select a set of fragments either using Bayesian models (Cohn et al., 2009; Post and Gildea, 2009) or using so-called Double-DOP (Sangati and Zuidema, 2011), which 1 “He says Cicero is consul” and “That Caesar is a general, we know”. 157 Author Text Sents. Classical (Perseus) Cicero In Catalinam 327 which is claimed to be a regional variant. For instance, Hanssen (1945) claims that mittere pro may be a calque of English “send for”, a claim which L¨ofstedt (1959) rebuts by providing a variety of examples from else"
W16-2120,N09-1062,0,0.196059,"as rejected on the grounds that the supposedly African constructions represented a distinct rhetorical style rather than a dialect. Similar questions have been raised about dialectal differences between France and Spain and the influences of Germanic languages on their local varieties of Latin. A robust computational method could help to resolve controversies like these. In many cases, the dispute is centered around some construction Tree substitution grammars A Tree Substitution Grammar (TSG) generalizes Context-Free Grammar (CFG) by allowing rules to insert arbitrarily large tree fragments (Cohn et al., 2009). Each fragment has a root symbol (analogous to the left-hand-side category in a CFG) and a frontier which can consist of terminals (words) and non-terminal symbols to be filled in later in the derivation. An example tree fragment is shown in figure 1; this fragment describes a particular complement clause structure which can be interpreted as the construction “that X is Y”. A single treebank tree may have multiple TSG derivations (depending on how it is split up into constructional fragments), so TSGs must be induced from the data. The Data-oriented Parsing (DOP) method (Bod and Kaplan, 1998)"
W16-2120,W96-0214,0,0.334432,"Cultural Heritage, Social Sciences, and Humanities (LaTeCH), pages 156–164, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics C creates a TSG rule for every maximal fragment which occurs more than once in the dataset. For instance, the fragment in figure 1 would be extracted from the trees for dicit quod Cicero consul est and quod Caesar dux est scimus,1 since it is shared between them both, but cannot be further expanded without adding an unshared element. TSGs are equivalent in expressive power to CFGs and can be efficiently parsed using the same algorithms (Goodman, 1996). TSGs have been used effectively for native language identification (Swanson and Charniak, 2012): determining the native language of a writer with intermediate proficiency in English, given a sample of their English writing. (Two closely related approaches are Wong and Dras (2011) and Wong et al. (2012).) Swanson and Charniak (2014) show that the rules learned by their system can be interpreted as transferring features or constructions from their native language. In this work, we argue that TSG is also useful for detecting the forms of change which occur in historical corpora. V.IND C quod N."
W16-2120,K15-1011,0,0.0204423,"ative subclause headed by quod which contains two nominals and the verb est “is”. Work on syntactic variation is much rarer. For the most part, it is confirmatory rather than explanatory; computational systems are designed to find examples of specific constructions in order to support investigations driven by pre-existing hypotheses. Such systems do not suggest new hypotheses from the data. Stewart (2014) detects African-American copula deletion and auxiliary verb structures; Doyle (2014) investigates “needs done” and double modals. We know of one exploratory project using syntactic features: Johannsen et al. (2015) use universal dependencies to extract “treelets” correlated with age and gender. Our TSG fragments are similar to their treelet features, but have the potential to be larger and are partly lexicalized. 3 4 Classical and Medieval Latin Lind (1941) divides Latin roughly into Classical (250 BCE to 100 CE), Late (100-600 CE), Medieval (600-1300) and Neo-Latin (1300-1700). Though these divisions are heuristic, they do correspond to episodes of lexical and grammatical change. Medieval Latin was an educated language used by clerics and scholars. It diverges from its Classical roots partly due to the"
W16-2120,J02-1005,0,0.0827291,"a root symbol (analogous to the left-hand-side category in a CFG) and a frontier which can consist of terminals (words) and non-terminal symbols to be filled in later in the derivation. An example tree fragment is shown in figure 1; this fragment describes a particular complement clause structure which can be interpreted as the construction “that X is Y”. A single treebank tree may have multiple TSG derivations (depending on how it is split up into constructional fragments), so TSGs must be induced from the data. The Data-oriented Parsing (DOP) method (Bod and Kaplan, 1998) was criticized by Johnson (2002) for its poor estimation procedure. Newer methods select a set of fragments either using Bayesian models (Cohn et al., 2009; Post and Gildea, 2009) or using so-called Double-DOP (Sangati and Zuidema, 2011), which 1 “He says Cicero is consul” and “That Caesar is a general, we know”. 157 Author Text Sents. Classical (Perseus) Cicero In Catalinam 327 which is claimed to be a regional variant. For instance, Hanssen (1945) claims that mittere pro may be a calque of English “send for”, a claim which L¨ofstedt (1959) rebuts by providing a variety of examples from elsewhere. The constructions involved"
W16-2120,P04-1061,0,0.0520006,"; this represents a weakness of this project, since it will be impossible to distinguish Medieval Latin in general from the specific style of Aquinas. The data is also somewhat unbalanced, with Aquinas representing much more text than any other author. These limitations are imposed by the system’s requirement for parse trees, and the unavailability of other parsed Latin data. Both source treebanks use non-projective dependency trees. To employ the TSG technique, we convert these to constituency trees. Our conversion introduces a phrasal projection over every head word with children; following Klein and Manning (2004), we give this projection the same label as the head word’s part of speech. Nonprojective edges are converted to projective ones by reordering the words so that the descendants of every head are contiguous. When a subtree is moved for this purpose, its tag is marked with a diacritic, so that the grammar can learn separate “sanctified usages such as changes in the use of cases and the subjunctive, and the more frequent use of quod/quia clauses in reported speech. . . . It is linguistically a central text.” But while the Vulgate has a strong influence on Medieval tradition, its compiler, St. Jer"
W16-2120,D11-1008,0,0.123919,"mple tree fragment is shown in figure 1; this fragment describes a particular complement clause structure which can be interpreted as the construction “that X is Y”. A single treebank tree may have multiple TSG derivations (depending on how it is split up into constructional fragments), so TSGs must be induced from the data. The Data-oriented Parsing (DOP) method (Bod and Kaplan, 1998) was criticized by Johnson (2002) for its poor estimation procedure. Newer methods select a set of fragments either using Bayesian models (Cohn et al., 2009; Post and Gildea, 2009) or using so-called Double-DOP (Sangati and Zuidema, 2011), which 1 “He says Cicero is consul” and “That Caesar is a general, we know”. 157 Author Text Sents. Classical (Perseus) Cicero In Catalinam 327 which is claimed to be a regional variant. For instance, Hanssen (1945) claims that mittere pro may be a calque of English “send for”, a claim which L¨ofstedt (1959) rebuts by providing a variety of examples from elsewhere. The constructions involved may be quite rare, and a specialist in one region or period may be unaware that a construction of interest is attested elsewhere, especially in obscure texts. An automatic method for discovering cases whi"
W16-2120,W11-0310,0,0.0280474,"tidimensional part of speech tags, with slightly different tagging conventions. We use only the top-level part of speech tag for most words, converting the Thomisticus tags deterministically into the Perseus tags. We use the remaining dimensions to annotate nominals with their case and verbs with their mood (indicative, subjunctive, imperative or infinitive). Finally, again following Swanson and Charniak (2013), we selectively delexicalize the trees. This prevents the “syntactic” patterns our system learns as markers of variation from being dominated by lexical items marking different topics (Sarawgi et al., 2011). For instance, Aquinas frequently uses the adjective Christiana “Christian”, while the Classical authors do not. But this is a change in culture, rather than in language. We remove all lexical items except prepositions (POS tag R), conjunctions (C), and a short list of adverbials (D), ne, non, tam, tamen, ita, etiam (“lest, not, so, however, thus, besides”). We replace all forms of the verb esse (“to be”), which is often used as an auxiliary, with the Perseus Treebank lemmatized form sum1. For example, the phrase ad quem finem is delexicalized to ad UNK 7.1 3 Unlike the construction of Nivre"
W16-2120,P05-1013,0,0.147867,"Missing"
W16-2120,E14-3004,0,0.123387,"high accuracy. Applied to an intermediate text (the Vulgate Bible), it indicates which changes between the eras were already occurring at this earlier stage. 1 Introduction In recent years, the study of language variation and change has been aided by a variety of computational tools that can automatically infer hypotheses about language change from a corpus (Eisenstein, 2015). In the domain of syntax, however, computational work is still limited by the necessity of manually choosing interesting hypotheses to study. For example, computational research on the syntax of African-American English (Stewart, 2014) is driven by pre-existing scholarly intuitions about the distinctive features of this dialect, but such intuitions are much harder to obtain for dead (or newly-emerging) language varieties. This paper adopts a method for unsupervised learning of syntactic constructions previously found effective for native language identification (Swanson and Charniak, 2012), and shows that it can discover a range of historically varying elements in a Latin corpus. In particular, we conduct a case study comparing classical prose (1st century 2 Variationist research Computational methods for studying language"
W16-2120,P12-2038,0,0.0697075,"om a corpus (Eisenstein, 2015). In the domain of syntax, however, computational work is still limited by the necessity of manually choosing interesting hypotheses to study. For example, computational research on the syntax of African-American English (Stewart, 2014) is driven by pre-existing scholarly intuitions about the distinctive features of this dialect, but such intuitions are much harder to obtain for dead (or newly-emerging) language varieties. This paper adopts a method for unsupervised learning of syntactic constructions previously found effective for native language identification (Swanson and Charniak, 2012), and shows that it can discover a range of historically varying elements in a Latin corpus. In particular, we conduct a case study comparing classical prose (1st century 2 Variationist research Computational methods for studying language variation can enhance both diachronic (historical) and synchronic (sociolinguistic) research. In some cases, the computational contribution is to build a classifier for a particular feature which is already of interest. For instance, Bane et al. (2010) target pre-selected phonetic features for analysis in recorded speech. Other computational systems are explo"
W16-2120,passarotti-dellorletta-2010-improvements,0,0.0603157,"Missing"
W16-2120,N13-1009,0,0.0575106,"de2 velopment, 10 testing and the rest training. Since we do not train or develop on the Vulgate, we set this data aside as a single set. ROOT quem what ad to finem end iactabit hurl.3.FUT V.IND R R ad 7 V.IND N.ACC P.ACCmov N.ACC quem finem Learning and ranking constructions We extract a set of TSG rules using Double-DOP (Sangati and Zuidema, 2011). As stated above, this process yields the set of all maximal TSG fragments which occur in more than one treebank tree. It is usually more exhaustive than the Bayesian extractors (Cohn et al., 2009), although it can be slow for large corpora. As in Swanson and Charniak (2013), we then match each rule against each treebank sentence, deciding whether that rule can occur in any derivation of the sentence. We assemble these decisions into a (rule×sentence) binary co-occurrence matrix. To compute variants which change between Classical and Medieval Latin, we sum across sentences in the training set to compute the four-way contingency table of counts: sentences with and without the rule in each era. We compute the χ2 statistic for each table and use this to rank the rules for feature selection. Swanson and Charniak (2013) recommend χ2 because it tends to retain moderate"
W16-2120,E14-4033,0,0.0130379,"s for dicit quod Cicero consul est and quod Caesar dux est scimus,1 since it is shared between them both, but cannot be further expanded without adding an unshared element. TSGs are equivalent in expressive power to CFGs and can be efficiently parsed using the same algorithms (Goodman, 1996). TSGs have been used effectively for native language identification (Swanson and Charniak, 2012): determining the native language of a writer with intermediate proficiency in English, given a sample of their English writing. (Two closely related approaches are Wong and Dras (2011) and Wong et al. (2012).) Swanson and Charniak (2014) show that the rules learned by their system can be interpreted as transferring features or constructions from their native language. In this work, we argue that TSG is also useful for detecting the forms of change which occur in historical corpora. V.IND C quod N.NOM N.NOM V.IND est Figure 1: A TSG fragment with the root symbol C (complement clause), introducing an indicative subclause headed by quod which contains two nominals and the verb est “is”. Work on syntactic variation is much rarer. For the most part, it is confirmatory rather than explanatory; computational systems are designed to"
W16-2120,P13-1030,0,0.016046,"ce these may not appear in the training data. Parsers for Latin of any kind are rare, although working systems (McGillivray, 2013; Passarotti and Dell’Orletta, 2010) do exist. Secondly, as seen above, the system has trouble unifying different examples of large constructions, such as clauses with and without modifiers. This prevents it from learning constructions larger than one or two context-free rules due to data sparsity. More expressive versions of TSG like Tree Adjoining Grammar (Joshi and Schabes, 1997) have been studied as solutions to this problem, including variants reducible to TSG (Swanson et al., 2013). It seems likely that such a more sophisticated grammatical representation could help to address this problem. Although delexicalization of all content words was effective in controlling for the very different topics represented in our corpus, it also renders the system incapable of recognizing any lexically mediated changes. For instance, the system cannot represent changes in the argument structure 559 507 Table 6: Features important in the classification of Vulgate sentences, ranked by importance M (i). this represents a failure to generalize, or genuine ambiguity, we search the Vulgate Bo"
W16-2120,D11-1148,0,0.0281678,"t in figure 1 would be extracted from the trees for dicit quod Cicero consul est and quod Caesar dux est scimus,1 since it is shared between them both, but cannot be further expanded without adding an unshared element. TSGs are equivalent in expressive power to CFGs and can be efficiently parsed using the same algorithms (Goodman, 1996). TSGs have been used effectively for native language identification (Swanson and Charniak, 2012): determining the native language of a writer with intermediate proficiency in English, given a sample of their English writing. (Two closely related approaches are Wong and Dras (2011) and Wong et al. (2012).) Swanson and Charniak (2014) show that the rules learned by their system can be interpreted as transferring features or constructions from their native language. In this work, we argue that TSG is also useful for detecting the forms of change which occur in historical corpora. V.IND C quod N.NOM N.NOM V.IND est Figure 1: A TSG fragment with the root symbol C (complement clause), introducing an indicative subclause headed by quod which contains two nominals and the verb est “is”. Work on syntactic variation is much rarer. For the most part, it is confirmatory rather tha"
W16-2120,P09-2012,0,0.0302945,"ymbols to be filled in later in the derivation. An example tree fragment is shown in figure 1; this fragment describes a particular complement clause structure which can be interpreted as the construction “that X is Y”. A single treebank tree may have multiple TSG derivations (depending on how it is split up into constructional fragments), so TSGs must be induced from the data. The Data-oriented Parsing (DOP) method (Bod and Kaplan, 1998) was criticized by Johnson (2002) for its poor estimation procedure. Newer methods select a set of fragments either using Bayesian models (Cohn et al., 2009; Post and Gildea, 2009) or using so-called Double-DOP (Sangati and Zuidema, 2011), which 1 “He says Cicero is consul” and “That Caesar is a general, we know”. 157 Author Text Sents. Classical (Perseus) Cicero In Catalinam 327 which is claimed to be a regional variant. For instance, Hanssen (1945) claims that mittere pro may be a calque of English “send for”, a claim which L¨ofstedt (1959) rebuts by providing a variety of examples from elsewhere. The constructions involved may be quite rare, and a specialist in one region or period may be unaware that a construction of interest is attested elsewhere, especially in ob"
W16-2120,D12-1064,0,0.0218811,"tracted from the trees for dicit quod Cicero consul est and quod Caesar dux est scimus,1 since it is shared between them both, but cannot be further expanded without adding an unshared element. TSGs are equivalent in expressive power to CFGs and can be efficiently parsed using the same algorithms (Goodman, 1996). TSGs have been used effectively for native language identification (Swanson and Charniak, 2012): determining the native language of a writer with intermediate proficiency in English, given a sample of their English writing. (Two closely related approaches are Wong and Dras (2011) and Wong et al. (2012).) Swanson and Charniak (2014) show that the rules learned by their system can be interpreted as transferring features or constructions from their native language. In this work, we argue that TSG is also useful for detecting the forms of change which occur in historical corpora. V.IND C quod N.NOM N.NOM V.IND est Figure 1: A TSG fragment with the root symbol C (complement clause), introducing an indicative subclause headed by quod which contains two nominals and the verb est “is”. Work on syntactic variation is much rarer. For the most part, it is confirmatory rather than explanatory; computat"
W16-2120,C98-1022,0,\N,Missing
W16-2120,E14-1011,0,\N,Missing
W16-2120,N10-1004,0,\N,Missing
W16-4012,2011.mtsummit-papers.12,0,0.0303415,"n of Blum and Mitchell’s (1998) co-training (the output of one tagger is used to train another) could be implemented without seed rules, yet, Pierce and Cardie’s (2001) assessment of the limitations of co-training shows that when “all the classes are [not] represented according to their prior probabilities in every region in the feature space”, as when we adapt to new domains in Perseus, we get Charniak’s (1997) result where mistakes are magnified, not smoothed. Active-learning, as opposed to self-training, allows the learner to query the user for additional annotation. Lynn et al. (2012) and Ambati et al. (2011) suggest that this is an effective solution for low-resource languages when self-training fails. Following Cohn et al. (1994), we modify the Query by Uncertainty tactic, where the tagger selects informative sentences based on how uncertain it is of the correct tag sequence and sends these to be annotated. Our modifications ensure that the most useful sentences are 89 Figure 1: Learning Curves for our 3 folds demonstrate a strong negative relationship between Remaining UNK’s and Accuracy. Remaining UNK’s is the number of unknown words in what is left of the test set normalized by the total numb"
W16-4012,W16-2110,0,0.026837,"filtering out any components that the analyzer considers impossible. Furthermore, we combine the output of the tagger and analyzer with our own set of rules, thereby deducing a lemma (if one failed to be identified by the tagger or analyzer), component morphemes, and number (singular/plural), all to be used as features. Like Farber et al. (2008), which uses a similar process to leverage morpho-syntactic information in morphologically rich, low-resource Arabic, we too find that filtering POS tag output cuts down on noise, boosting accuracy. By additionally leveraging the newly enhanced LEMLAT (Budassi and Passarotti, 2016) morphological analyzer, or even substituting it for Whitaker’s (1993), which has a smaller lexical base and struggles with graphical 87 Table 1: Fold 1 tests on Ep, trains on the remaining annotated data. 2 tests on AA, and 3 on books 2 and 7 of BG which, when concatenated, resemble the lengths of the other 2 test sets. 3 tests “in-domain” as the training set is mostly from the same domain, i.e. the other 6 books of BG, the historiography domain. Table 2: UNK’s are words unseen in training, F, F-score, Prec, precision, Rec, recall, and GAZ, gazetteer. variants, we expect even further gains in"
W16-4012,W99-0613,0,0.245566,"ity 1 words. Forms like Video, “I see”, which also do not appear in training but do show uncapitalized variants, are still challenging but much less so as most are non-NE’s, only capitalized when sentence initial. We refer to these as Priority 2 words; however, these tend to be more frequent than priority 1’s. We consider this tradeoff between difficulty and frequency as we tailor our pipeline to better handle capitalized UNK’s. 4 Semi-Supervised Model Semi-supervised learning involves supplementing with unannotated data during training. Liao and Veeramachaneni (2009), Rani et al. (2014), and Collins and Singer (1999) show that self-training, where unannotated data is used for training without querying the user, can overcome data sparsity or gaps between training and testing domains. However, the first two identify high precision unannotated sentences by relying on seed rules which are difficult to develop for Latin. While Liao and Veeramachaneni (2009) can rely on any capitalized word following a PRS to be part of the same NE, Latin’s free word order frequently allows NE’s from entirely different syntactic constituents to appear adjacent to one another, as in Caesar [PRS] Haeduos [GRP] frumentum ... flagi"
W16-4012,P05-1045,0,0.0134187,"NER due to their ability to rapidly learn from potentially large vectors of features belonging to each sequential token. Stanford NER leverages the widely used types of features discussed by McCallum and Li (2003), lexical, orthographic, semantic, conjoined sequences of features, and features of neighbors (we use the default feature set and parameters specified at nlp.stanford.edu/nlp/javadoc/javanlp-3. 6.0/edu/stanford/nlp/ie/NERFeatureFactory.html), but additionally employs a Gibbs Sampling-based penalty system motivating consistency in labels for multiple occurrences of the same word type (Finkel et al., 2005). 3.2 Our Model Our model, like Stanford’s, is a CRF using similar features, though we alter ours to suit our language and corpus. We employ a POS tagger (Schmid, 1999) to leverage the highly informative morphological complexity of Latin. Finkel et al. (2005) claim only a negligible boost from using POS features in English NER and thus do not include them in the off-the-shelf version, yet we find that when implemented with creativity, the output of even a low accuracy POS tagger can be beneficial for Latin NER. For each token, we deconstruct the fine-grained POS tag into component parts rangin"
W16-4012,farber-etal-2008-improving,0,0.0325477,"each token, we deconstruct the fine-grained POS tag into component parts ranging from case and mood distinctions to coarser distinctions between syntactic categories like nouns and verbs. We then run each token through a rule-based morphological analyzer (Whitaker, 1993), filtering out any components that the analyzer considers impossible. Furthermore, we combine the output of the tagger and analyzer with our own set of rules, thereby deducing a lemma (if one failed to be identified by the tagger or analyzer), component morphemes, and number (singular/plural), all to be used as features. Like Farber et al. (2008), which uses a similar process to leverage morpho-syntactic information in morphologically rich, low-resource Arabic, we too find that filtering POS tag output cuts down on noise, boosting accuracy. By additionally leveraging the newly enhanced LEMLAT (Budassi and Passarotti, 2016) morphological analyzer, or even substituting it for Whitaker’s (1993), which has a smaller lexical base and struggles with graphical 87 Table 1: Fold 1 tests on Ep, trains on the remaining annotated data. 2 tests on AA, and 3 on books 2 and 7 of BG which, when concatenated, resemble the lengths of the other 2 test s"
W16-4012,W09-2208,0,0.0843949,"es Institute Latin Libraries corpus (Packard Humanities Institute, 1992). Because no gold data was annotated to train or evaluate this system, our small annotated corpus presents the first opportunity to gauge its performance. Stanford’s NER system is a more sophisticated conditional random field (CRF) model, although, being fully supervised in its off-theshelf implementation, it lacks the extensive vocabulary that the CLTK system has access to. CRF’s are undirected graphical models trained to maximize the conditional probability of a sequence of labels given the corresponding input sequence (Liao and Veeramachaneni, 2009), and are especially effective in NER due to their ability to rapidly learn from potentially large vectors of features belonging to each sequential token. Stanford NER leverages the widely used types of features discussed by McCallum and Li (2003), lexical, orthographic, semantic, conjoined sequences of features, and features of neighbors (we use the default feature set and parameters specified at nlp.stanford.edu/nlp/javadoc/javanlp-3. 6.0/edu/stanford/nlp/ie/NERFeatureFactory.html), but additionally employs a Gibbs Sampling-based penalty system motivating consistency in labels for multiple o"
W16-4012,U12-1005,0,0.0224543,"Missing"
W16-4012,W03-0430,0,0.0441248,"ore sophisticated conditional random field (CRF) model, although, being fully supervised in its off-theshelf implementation, it lacks the extensive vocabulary that the CLTK system has access to. CRF’s are undirected graphical models trained to maximize the conditional probability of a sequence of labels given the corresponding input sequence (Liao and Veeramachaneni, 2009), and are especially effective in NER due to their ability to rapidly learn from potentially large vectors of features belonging to each sequential token. Stanford NER leverages the widely used types of features discussed by McCallum and Li (2003), lexical, orthographic, semantic, conjoined sequences of features, and features of neighbors (we use the default feature set and parameters specified at nlp.stanford.edu/nlp/javadoc/javanlp-3. 6.0/edu/stanford/nlp/ie/NERFeatureFactory.html), but additionally employs a Gibbs Sampling-based penalty system motivating consistency in labels for multiple occurrences of the same word type (Finkel et al., 2005). 3.2 Our Model Our model, like Stanford’s, is a CRF using similar features, though we alter ours to suit our language and corpus. We employ a POS tagger (Schmid, 1999) to leverage the highly i"
W16-4012,W01-0501,0,0.100693,"Missing"
W16-4012,L16-1108,0,0.014621,"es fine-grained distinctions among NE’s, and Perseus’ digital gazetteers, mark-ups of Smith (1854; 1870; 1890), cover just individual persons (PRS) and geographical place names (GEO), not group names (GRP), which we want also to be able to recognize.1 Additionally, the reliability of part-of-speech taggers (e.g. Schmid (1999) trained by Gabriele Brandolini (http://www.cis.uni-muenchen.de/˜schmid/tools/TreeTagger/data/ latin-par-linux-3.2.bin.gz) and Johnson et al. (2014)), dependency parsers (Bamman and Crane, 2009), and semantic models (Johnson et al., 2014) is very low due to data-sparsity. Ponti and Passarotti (2016) actually demonstrate reliable syntactic dependency parsing on Medieval Latin using the Index Thomisticus Treebank (McGillivray et al., 2009), but find that more work is needed to successfully adapt their model to handle other varieties of Latin. In broad terms, generalizing from linguistic patterns recognized in one Latin text when processing another is difficult because “a series of particular historical, geographical and cultural circumstances [led] to an inhomogeneous linguistic system where elements from different areas and registers met and were only partially transmitted by the sources”"
W17-0115,C14-1096,0,0.0252462,"f language loss by quantifying the degree to which the target segments have been lost. Our results reveal new facts about the discriminative features for clicks. For example, although Traill (1997) provided a scale of click burst intensity, shown in (1) above, the variability of the amplitude of the alveolar click bursts relative to the following vowel is so high, that it is clear that the amplitude alone can not be very useful in discriminating the four click types. As mentioned Similar variability in click production is reported in langugage as a stage in click loss due to 113 et al., 2016; Bird et al., 2014) in training a fullscale ASR system, or to bootstrap a learning-based landmark recognition system (Hasegawa-Johnson et al., 2005). above, the relative perceptual weighting of the two temporal measures (click burst duration and rise time to peak amplitude) is completely unknown for clicks. Comparing our results to Fulop et al. (2004) Yeyi results, we can conclude that a combination of manner cues and place of articulation cues results in much better discriminability. Of course, we can not rule out the contribution of click reduction / loss to the poorer discriminability seen in the Yeyi results"
W17-5405,D12-1096,0,0.0134449,"ight the distance which even sophisticated, modern sentiment analysis systems have yet to cover, particularly in terms of semantic and pragmatic analysis. Moreover, changes that broke the systems were often comparatively slight; just as image classification systems can be vulnerable to adversarial examples that look very similar to the originals (Szegedy et al., 2014), sentiment analysis systems may be fooled by changes to single words or morphemes. In many cases, of course, our strategies for constructing these examples drew on previous knowledge about hard problems, for instance in parsing (Kummerfeld et al., 2012) and the detection of irony in text (Wallace et al., 2014). Nonetheless, a concrete set of examples of these problems may help developers to create more robust systems in the future. For sets of constructed examples like ours to be useful, they should contain enough instances of each construction to reliably indicate a system’s capabilities. Looking towards the future, we hope that the next iteration of the contest will use a larger test section so that more examples can be created. Many of our strategies targeted particular constructions or idioms (for instance, right-node raising or concrete"
W17-5405,P14-2084,0,0.022055,"analysis systems have yet to cover, particularly in terms of semantic and pragmatic analysis. Moreover, changes that broke the systems were often comparatively slight; just as image classification systems can be vulnerable to adversarial examples that look very similar to the originals (Szegedy et al., 2014), sentiment analysis systems may be fooled by changes to single words or morphemes. In many cases, of course, our strategies for constructing these examples drew on previous knowledge about hard problems, for instance in parsing (Kummerfeld et al., 2012) and the detection of irony in text (Wallace et al., 2014). Nonetheless, a concrete set of examples of these problems may help developers to create more robust systems in the future. For sets of constructed examples like ours to be useful, they should contain enough instances of each construction to reliably indicate a system’s capabilities. Looking towards the future, we hope that the next iteration of the contest will use a larger test section so that more examples can be created. Many of our strategies targeted particular constructions or idioms (for instance, right-node raising or concrete metaphors), and it was difficult to create many instances"
W18-5802,W16-2018,0,0.355325,"ical Networks in !Xung Syed-Amad Hussain Micha Elsner Department of Computer Science Department of Linguistics The Ohio State University The Ohio State University amadh881@gmail.com melsner0@gmail.com Abstract smaller inventories. In a LN, as shown in Figure 1, nodes represent words and edges between nodes represent minimal pairs (Vitevitch, 2008). Vitevitch (2008) argues that the high connectivity and tendency toward clustering found in the English language lexicon are important aids to word learning and retrieval; later work finds similar properties in other lexicons (Arbesman et al., 2010; Shoemark et al., 2016). Some claims about the linguistic relevance of LNPs have been qualified by experiments showing that certain property values are inherent to the construction process of the network and can be replicated even when words are sampled from simple generative processes (Stella and Brede, 2015; Gruenenfelder and Pisoni, 2009; Turnbull and Peperkamp, 2016; Brown et al., 2018), though all these studies except Brown et al. point out that the LNs of natural languages maintain some distinctive properties. Because !Xung has a very large phoneme inventory, it might in principle have very different network p"
W18-5802,cieri-etal-2004-fisher,0,0.0473538,"Missing"
